{"id": "0704.0897", "contents": "Title: A unified approach to the theory of separately holomorphic mappings Abstract: We extend the theory of separately holomorphic mappings between complex\nanalytic spaces. Our method is based on Poletsky theory of discs, Rosay Theorem\non holomorphic discs and our recent joint-work with Pflug on cross theorems in\ndimension 1. It also relies on our new technique of conformal mappings and a\ngeneralization of Siciak's relative extremal function.\n  Our approach illustrates the unified character: ``From local informations to\nglobal extensions\". Moreover, it avoids systematically the use of the classical\nmethod of doubly orthogonal bases of Bergman type. \n\n"}
{"id": "0706.3115", "contents": "Title: Finite jet determination of CR mappings Abstract: We prove the following finite jet determination result for CR mappings: Given\na smooth generic submanifold M of C^N, N >= 2, which is essentially finite and\nof finite type at each of its points, for every point p on M there exists an\ninteger l(p), depending upper-semicontinuously on p, such that for every smooth\ngeneric submanifold M' of C^N of the same dimension as M, if h_1 and h_2:\n(M,p)->M' are two germs of smooth finite CR mappings with the same l(p) jet at\np, then necessarily their k-jets agree for all positive integers k. In the\nhypersurface case, this result provides several new unique jet determination\nproperties for holomorphic mappings at the boundary in the real-analytic case;\nin particular, it provides the finite jet determination of arbitrary\nreal-analytic CR mappings between real-analytic hypersurfaces in C^N of\nD'Angelo finite type. It also yields a new boundary version of H. Cartan's\nuniqueness theorem: if Omega and Omega' are two bounded domains in C^N with\nsmooth real-analytic boundary, then there exists an integer k, depending only\non the boundary of Omega, such that if H_1 and H_2: Omega -> Omega' are two\nproper holomorphic mappings extending smoothly up to the boundary of Omega near\nsome point boundary point p and agreeing up to order k at p, then necessarily\nH_1=H_2. \n\n"}
{"id": "0707.1097", "contents": "Title: The strong superadditivity conjecture holds for the quantum depolarizing\n  channel in any dimension Abstract: Given a quantum channel $\\Phi $ in a Hilbert space $H$ put $\\hat\nH_{\\Phi}(\\rho)=\\min \\limits_{\\rho_{av}=\\rho}\\Sigma_{j=1}^{k}\\pi_{j}S(\\Phi\n(\\rho_{j}))$, where $\\rho_{av}=\\Sigma_{j=1}^{k}\\pi_{j}\\rho_{j}$, the minimum is\ntaken over all probability distributions $\\pi =\\{\\pi_{j}\\}$ and states\n$\\rho_{j}$ in $H$, $S(\\rho)=-Tr\\rho\\log\\rho$ is the von Neumann entropy of a\nstate $\\rho$. The strong superadditivity conjecture states that $\\hat H_{\\Phi\n\\otimes \\Psi}(\\rho)\\ge \\hat H_{\\Phi}(Tr_{K}(\\rho))+\\hat H_{\\Psi}(Tr_{H}(\\rho))$\nfor two channels $\\Phi $ and $\\Psi $ in Hilbert spaces $H$ and $K$,\nrespectively. We have proved the strong superadditivity conjecture for the\nquantum depolarizing channel in any dimensions. \n\n"}
{"id": "0708.0563", "contents": "Title: Probabilistic implications of symmetries of q-Hermite and\n  Al-Salam-Chihara polynomials Abstract: We prove the existence of stationary random fields with linear regressions\nfor $q>1$ and thus close an open question posed by W. Bryc et al.. We prove\nthis result by describing a discrete 1 dimensional conditional distribution and\nthen checking Chapman-Kolmogorov equation. Support of this distribution consist\nof zeros of certain Al-Salam-Chihara polynomials. To find them we refer to and\nexpose known result concerning addition of $q-$ exponential function. This\nleads to generalization of a well known formula $(x+y)^{n}%\n=\\sum_{i=0}^{n}\\binom{n}{k}i^{k}H_{n-k}(x) H_{k}(-iy) ,$ where $H_{k}(x) $\ndenotes $k-$th Hermite polynomial. \n\n"}
{"id": "0708.0890", "contents": "Title: Operational Semantics and Type Soundness of Quantum Programming Language\n  LanQ Abstract: We present an imperative quantum programming language LanQ which was designed\nto support combination of quantum and classical programming and basic process\noperations - process creation and interprocess communication. The language can\nthus be used for implementing both classical and quantum algorithms and\nprotocols. Its syntax is similar to that of C language what makes it easy to\nlearn for existing programmers. In this paper, we present operational semantics\nof the language and a proof of type soundness of the noncommunicating part of\nthe language. We provide an example run of a quantum random number generator. \n\n"}
{"id": "0708.2754", "contents": "Title: Convergence of random zeros on complex manifolds Abstract: We show that the zeros of random sequences of Gaussian systems of polynomials\nof increasing degree almost surely converge to the expected limit distribution\nunder very general hypotheses. In particular, the normalized distribution of\nzeros of systems of m polynomials of degree N, orthonormalized on a regular\ncompact subset K of C^m, almost surely converge to the equilibrium measure on K\nas the degree N goes to infinity. \n\n"}
{"id": "0710.0417", "contents": "Title: Quantum capacity of lossy channel with additive classical Gaussian noise\n  : a perturbation approach Abstract: For a quantum channel of additive Gaussian noise with loss, in the general\ncase of $n$ copies input, we show that up to first order perturbation, any\nnon-Gaussian perturbation to the product thermal state input has a less quantum\ninformation transmission rate when the input energy tend to infinitive. \n\n"}
{"id": "0710.0868", "contents": "Title: Peak points for pseudoconvex domains: a survey Abstract: This paper surveys results concerning peak points for pseudoconvex domains.\nIt includes results of Laszlo that have not been published elsewhere. \n\n"}
{"id": "0710.3579", "contents": "Title: Holomorphic correspondences between CR manifolds Abstract: It is proved that a germ of a real analytic CR map from a smooth\nreal-analytic minimal CR manifold M to an essentially finite real-algebraic\ngeneric submanifold M' of P^N of the same CR-dimension extends as a holomorphic\ncorrespondence along M. Applications are given for pseudoconcave submanifolds\nof P^N. \n\n"}
{"id": "0710.5776", "contents": "Title: Entanglement Generation in the Scattering of One-Dimensional Particles Abstract: This article provides a convenient framework for quantitative evaluation of\nthe entanglement generated when two structureless, distinguishable particles\nscatter non-relativistically in one dimension. It explores how three factors\ndetermine the amount of entanglement generated: the momentum distributions of\nthe incoming particles, their masses, and the interaction potential. Two\nimportant scales emerge, one set by the kinematics and one set by the dynamics.\nThis method also provides two approximate analytic formulas useful for\nnumerical evaluation of entanglement and reveals an interesting connection\nbetween purity, linear coordinate transformations, and momentum uncertainties. \n\n"}
{"id": "0711.0429", "contents": "Title: Equivalence of types and Catlin boundary systems Abstract: The D'Angelo finite type is shown to be equivalent to the Kohn finite ideal\ntype on smooth, pseudoconvex domains in complex n space. This is known as the\nKohn Conjecture. The argument uses Catlin's notion of a boundary system as well\nas methods from subanalytic and semialgebraic geometry. When a subset of the\nboundary contains only two level sets of the Catlin multitype, a lower bound\nfor the subelliptic gain in the \\bar\\partial-Neumann problem is obtained in\nterms of the D'Angelo type, the dimension of the ambient space, and the level\nof forms. \n\n"}
{"id": "0711.1483", "contents": "Title: Dynamical creation of bosonic Cooper-like pairs Abstract: We propose a scheme to create a metastable state of paired bosonic atoms in\nan optical lattice. The most salient features of this state are that the\nwavefunction of each pair is a Bell state and that the pair size spans half the\nlattice, similar to fermionic Cooper pairs. This mesoscopic state can be\ncreated with a dynamical process that involves crossing a quantum phase\ntransition and which is supported by the symmetries of the physical system. We\ncharacterize the final state by means of a measurable two-particle correlator\nthat detects both the presence of the pairs and their size. \n\n"}
{"id": "0711.3939", "contents": "Title: Proposal of a Cold-atom Realization of Quantum Maps with Hofstadter's\n  Butterfly Spectrum Abstract: Quantum systems with Hofstadter's butterfly spectrum are of fundamental\ninterest to many research areas. Based upon slight modifications of existing\ncold-atom experiments, a cold-atom realization of quantum maps with\nHofstadter's butterfly spectrum is proposed. Connections and differences\nbetween our realization and the kicked Harper model are identified. This work\nalso exposes, for the first time, a simple connection between the kicked Harper\nmodel and the kicked rotor model, the two paradigms of classical and quantum\nchaos. \n\n"}
{"id": "0712.1656", "contents": "Title: Special Values of Generalized Polylogarithms Abstract: We study values of generalized polylogarithms at various points and\nrelationships among them. Polylogarithms of small weight at the points 1/2 and\n-1 are completely investigated. We formulate a conjecture about the structure\nof the linear space generated by values of generalized polylogarithms. \n\n"}
{"id": "0801.0710", "contents": "Title: Koppelman formulas and the $\\dbar$-equation on an analytic space Abstract: Let $X$ be an analytic space of pure dimension. We introduce a formalism to\ngenerate intrinsic weighted Koppelman formulas on $X$ that provide solutions to\nthe $\\dbar$-equation. We prove that if $\\phi$ is a smooth $(0,q+1)$-form on a\nStein space $X$ with $\\dbar\\phi=0$, then there is a smooth $(0,q)$-form $\\psi$\non $X_{reg}$ with at most polynomial growth at $X_{sing}$ such that\n$\\dbar\\psi=\\phi$. The integral formulas also give other new existence results\nfor the $\\dbar$-equation and Hartogs theorems, as well as new proofs of various\nknown results. \n\n"}
{"id": "0801.0861", "contents": "Title: Quantum Graphity: a model of emergent locality Abstract: Quantum graphity is a background independent model for emergent locality,\nspatial geometry and matter. The states of the system correspond to dynamical\ngraphs on N vertices. At high energy, the graph describing the system is highly\nconnected and the physics is invariant under the full symmetric group acting on\nthe vertices. We present evidence that the model also has a low-energy phase in\nwhich the graph describing the system breaks permutation symmetry and appears\nto be ordered, low-dimensional and local. Consideration of the free energy\nassociated with the dominant terms in the dynamics shows that this low-energy\nstate is thermodynamically stable under local perturbations. The model can also\ngive rise to an emergent U(1) gauge theory in the ground state by the\nstring-net condensation mechanism of Levin and Wen. We also reformulate the\nmodel in graph-theoretic terms and compare its dynamics to some common graph\nprocesses. \n\n"}
{"id": "0801.3994", "contents": "Title: The mystical formula and the mystery of Khronos Abstract: In 1908, Minkowski put forward the idea that invariance under what we call\ntoday the Lorentz group, $GL(1,3, {\\bf R})$, would be more meaningful in a\nfour-dimensional space-time continuum. This suggestion implies that space and\ntime are intertwined entities so that, kinematic and dynamical quantities can\nbe expressed as vectors, or more generally by tensors, in the four-dimensional\nspace-time. Minkowski also showed how causality should be structured in the\nfour-dimensional vector space. The mathematical formulation proposed by\nMinkowski made its generalization to curved spaces quite natural, leaving the\ndoors to the General Theory of Relativity and many other developments ajar.\n  Nevertheless, it is remarkable that this deceptively simple formulation\neluded many researchers of space and time, and goes against our every day\nexperience and perception, according to which space and time are distinct\nentities. In this contribution, we discuss these contradictory views, analyze\nhow they are seen in contemporary physics and comment on the challenges that\nspace-time explorers face. \n\n"}
{"id": "0803.1512", "contents": "Title: A Protocol for Quantum Energy Distribution Abstract: A new protocol, quantum energy distribution (QED), is proposed in which\nmultiple parties can simultaneously extract positive energy from spin chains by\ncommon secret keys shared by an energy supplier. QED is robust against\nimpersonation; an adversary, who does not have a common secret key and attempts\nto get energy, will instead give energy to the spin chains. The total amount of\nenergy transfer gives a lower bound of the residual energy of any local cooling\nprocess by the energy supplier. \n\n"}
{"id": "0804.2986", "contents": "Title: Higher order invariants of Levi degenerate hypersurfaces Abstract: The first part of this paper considers higher order CR invariants of three\ndimensional hypersurfaces of finite type. Using a full normal form we give a\ncomplete characterization of hypersurfaces with trivial local automorphism\ngroup, and analogous results for finite groups. The second part considers\nhypersurfaces of finite Catlin multitype and the Kohn-Nirenberg phenomenon in\nhigher dimensions. We give a necessary condition for local convexifiability of\na class of pseudoconvex hypersurfaces in $\\mathbb C^{n+1}$. \n\n"}
{"id": "0806.0145", "contents": "Title: Lasso-type recovery of sparse representations for high-dimensional data Abstract: The Lasso is an attractive technique for regularization and variable\nselection for high-dimensional data, where the number of predictor variables\n$p_n$ is potentially much larger than the number of samples $n$. However, it\nwas recently discovered that the sparsity pattern of the Lasso estimator can\nonly be asymptotically identical to the true sparsity pattern if the design\nmatrix satisfies the so-called irrepresentable condition. The latter condition\ncan easily be violated in the presence of highly correlated variables. Here we\nexamine the behavior of the Lasso estimators if the irrepresentable condition\nis relaxed. Even though the Lasso cannot recover the correct sparsity pattern,\nwe show that the estimator is still consistent in the $\\ell_2$-norm sense for\nfixed designs under conditions on (a) the number $s_n$ of nonzero components of\nthe vector $\\beta_n$ and (b) the minimal singular values of design matrices\nthat are induced by selecting small subsets of variables. Furthermore, a rate\nof convergence result is obtained on the $\\ell_2$ error with an appropriate\nchoice of the smoothing parameter. The rate is shown to be optimal under the\ncondition of bounded maximal and minimal sparse eigenvalues. Our results imply\nthat, with high probability, all important variables are selected. The set of\nselected variables is a meaningful reduction on the original set of variables.\nFinally, our results are illustrated with the detection of closely adjacent\nfrequencies, a problem encountered in astrophysics. \n\n"}
{"id": "0807.0775", "contents": "Title: Efficiency of Producing Random Unitary Matrices with Quantum Circuits Abstract: We study the scaling of the convergence of several statistical properties of\na recently introduced random unitary circuit ensemble towards their limits\ngiven by the circular unitary ensemble (CUE). Our study includes the full\ndistribution of the absolute square of a matrix element, moments of that\ndistribution up to order eight, as well as correlators containing up to 16\nmatrix elements in a given column of the unitary matrices. Our numerical\nscaling analysis shows that all of these quantities can be reproduced\nefficiently, with a number of random gates which scales at most as $n_q\\log\n(n_q/\\epsilon)$ with the number of qubits $n_q$ for a given fixed precision\n$\\epsilon$. This suggests that quantities which require an exponentially large\nnumber of gates are of more complex nature. \n\n"}
{"id": "0807.1429", "contents": "Title: The Weil--Petersson geometry of the moduli space of Riemann surfaces Abstract: In [4], Z. Huang showed that in the thick part of the moduli space\n$\\mathcal{M}_g$ of compact Riemann surfaces of genus $g$, the sectional\ncurvature of the Weil--Petersson metric is bounded below by a constant\ndepending on injectivity radius, but independent of the genus $g$. In this\narticle, we prove this result by a different method. We also show that the same\nresult holds for Ricci curvature. For the universal Teichm\\\"uller space\nequipped with Hilbert structure induced by Weil--Petersson metric, we prove\nthat its sectional curvature is bounded below by a universal constant. \n\n"}
{"id": "0808.1770", "contents": "Title: Superharmonic Perturbations of a Gaussian Measure, Equilibrium Measures\n  and Orthogonal Polynomials Abstract: This work concerns superharmonic perturbations of a Gaussian measure given by\na special class of positive weights in the complex plane of the form $w(z) =\n\\exp(-|z|^2 + U^{\\mu}(z))$, where $U^{\\mu}(z)$ is the logarithmic potential of\na compactly supported positive measure $\\mu$. The equilibrium measure of the\ncorresponding weighted energy problem is shown to be supported on subharmonic\ngeneralized quadrature domains for a large class of perturbing potentials\n$U^{\\mu}(z)$. It is also shown that the $2\\times 2$ matrix d-bar problem for\northogonal polynomials with respect to such weights is well-defined and has a\nunique solution given explicitly by Cauchy transforms. Numerical evidence is\npresented supporting a conjectured relation between the asymptotic distribution\nof the zeroes of the orthogonal polynomials in a semi-classical scaling limit\nand the Schwarz function of the curve bounding the support of the equilibrium\nmeasure, extending the previously studied case of harmonic polynomial\nperturbations with weights $w(z)$ supported on a compact domain. \n\n"}
{"id": "0809.2311", "contents": "Title: Reply to Comment on \"Exposed-key weakness of $\\alpha \\eta$\" Abstract: We address criticism of the Letter \"Exposed-Key Weakness of $\\alpha \\eta$\" in\nthe Comment by Nair and Yuen. The Comment claims that the Letter does not show\ninsecurity of $\\alpha \\eta$ because our approximation for the eavesdropper's\nentropy on the encrypted key is invalid. We present simulations which show\nthat, on the contrary, our estimate is in close agreement with numerical\ncalculations of the actual entropy over the applicable domain. We additionally\ndiscuss some ways in which our views on security requirements differ from the\nviews given in the Comment. \n\n"}
{"id": "0809.4128", "contents": "Title: Hilbert transforms and the Cauchy integral in euclidean space Abstract: We generalize the notion of harmonic conjugate functions and Hilbert\ntransforms to higher dimensional euclidean spaces, in the setting of\ndifferential forms and the Hodge-Dirac system. These conjugate functions are in\ngeneral far from being unique, but under suitable boundary conditions we prove\nexistence and uniqueness of conjugates. The proof also yields invertibility\nresults for a new class of generalized double layer potential operators on\nLipschitz surfaces and boundedness of related Hilbert transforms. \n\n"}
{"id": "0810.4878", "contents": "Title: Shcherbina's Theorem for Finely Holomorphic Functions Abstract: We prove an analogue of Sadullaev's theorem concerning the size of the set\nwhere a maximal totally real manifold can meet a pluripolar set. The manifold\nhas to be of class C-1 only. This readily leads to a version of Shcherbina's\ntheorem for C-1 functions f that are defined in a neighborhood of certain\ncompact sets K in the complex plane. If the graph of f on K is pluripolar, then\nf satisfies the Cauchy Riemann equations in the closure of the fine interior of\nK. \n\n"}
{"id": "0811.2352", "contents": "Title: A supplement to a theorem of Merker and Porten: a short proof of\n  Hartogs' extension theorem for $(n-1)$-complete complex spaces Abstract: We give a short proof for the Hartogs's extension theorem on (n-1)-complete\ncomplex spaces. \n\n"}
{"id": "0812.3579", "contents": "Title: Simultaneous linearization of holomorphic germs in presence of\n  resonances Abstract: Let $f_1, ..., f_m$ be $m\\ge 2$ germs of biholomorphisms of $\\C^n$, fixing\nthe origin, with $(\\d f_1)_O$ diagonalizable and such that $f_1$ commutes with\n$f_h$ for any $h=2,..., m$. We prove that, under certain arithmetic conditions\non the eigenvalues of $(\\d f_1)_O$ and some restrictions on their resonances,\n$f_1, ..., f_m$ are simultaneously holomorphically linearizable if and only if\nthere exists a particular complex manifold invariant under $f_1,..., f_m$. \n\n"}
{"id": "0812.4403", "contents": "Title: Geometry of splice-quotient singularities Abstract: We obtain a new important basic result on splice-quotient singularities in an\nelegant combinatorial-geometric way: every level of the divisorial filtration\nof the ring of functions is generated by monomials of the defining coordinate\nfunctions. The elegant way is the language of of line bundles based on Okuma's\ndescription of the function ring of the universal abelian cover. As an easy\napplication, we obtain a new proof of the End Curve Theorem of Neumann and\nWahl. \n\n"}
{"id": "0901.1504", "contents": "Title: A D.C. Programming Approach to the Sparse Generalized Eigenvalue Problem Abstract: In this paper, we consider the sparse eigenvalue problem wherein the goal is\nto obtain a sparse solution to the generalized eigenvalue problem. We achieve\nthis by constraining the cardinality of the solution to the generalized\neigenvalue problem and obtain sparse principal component analysis (PCA), sparse\ncanonical correlation analysis (CCA) and sparse Fisher discriminant analysis\n(FDA) as special cases. Unlike the $\\ell_1$-norm approximation to the\ncardinality constraint, which previous methods have used in the context of\nsparse PCA, we propose a tighter approximation that is related to the negative\nlog-likelihood of a Student's t-distribution. The problem is then framed as a\nd.c. (difference of convex functions) program and is solved as a sequence of\nconvex programs by invoking the majorization-minimization method. The resulting\nalgorithm is proved to exhibit \\emph{global convergence} behavior, i.e., for\nany random initialization, the sequence (subsequence) of iterates generated by\nthe algorithm converges to a stationary point of the d.c. program. The\nperformance of the algorithm is empirically demonstrated on both sparse PCA\n(finding few relevant genes that explain as much variance as possible in a\nhigh-dimensional gene dataset) and sparse CCA (cross-language document\nretrieval and vocabulary selection for music retrieval) applications. \n\n"}
{"id": "0901.1840", "contents": "Title: Polyhedral Kahler Manifolds Abstract: In this article we introduce the notion of Polyhedral Kahler manifolds, even\ndimensional polyhedral manifolds with unitary holonomy. We concentrate on the\n4-dimensional case, prove that such manifolds are smooth complex surfaces, and\nclassify the singularities of the metric. The singularities form a divisor and\nthe residues of the flat connection on the complement of the divisor give us a\nsystem of cohomological equations. Parabolic version of Kobayshi-Hitchin\ncorrespondence of T. Mochizuki permits us to characterize polyhedral Kahler\nmetrics of non-negative curvature on CP^2 with singularities at complex line\narrangements. \n\n"}
{"id": "0902.0193", "contents": "Title: Critical measures, quadratic differentials, and weak limits of zeros of\n  Stieltjes polynomials Abstract: We investigate the asymptotic zero distribution of Heine-Stieltjes\npolynomials - polynomial solutions of a second order differential equations\nwith complex polynomial coefficients. In the case when all zeros of the leading\ncoefficients are all real, zeros of the Heine-Stieltjes polynomials were\ninterpreted by Stieltjes as discrete distributions minimizing an energy\nfunctional. In a general complex situation one deals instead with a critical\npoint of the energy. We introduce the notion of discrete and continuous\ncritical measures (saddle points of the weighted logarithmic energy on the\nplane), and prove that a weak-* limit of a sequence of discrete critical\nmeasures is a continuous critical measure. Thus, the limit zero distributions\nof the Heine-Stieltjes polynomials are given by continuous critical measures.\nWe give a detailed description of such measures, showing their connections with\nquadratic differentials. In doing that, we obtain some results on the global\nstructure of rational quadratic differentials on the Riemann sphere that have\nan independent interest. \n\n"}
{"id": "0902.3455", "contents": "Title: Non-resonant dot-cavity coupling and its applications in resonant\n  quantum dot spectroscopy Abstract: We present experimental investigations on the non-resonant dot-cavity\ncoupling of a single quantum dot inside a micro-pillar where the dot has been\nresonantly excited in the s-shell, thereby avoiding the generation of\nadditional charges in the QD and its surrounding. As a direct proof of the pure\nsingle dot-cavity system, strong photon anti-bunching is consistently observed\nin the autocorrelation functions of the QD and the mode emission, as well as in\nthe cross-correlation function between the dot and mode signals. Strong Stokes\nand anti-Stokes-like emission is observed for energetic QD-mode detunings of up\nto ~100 times the QD linewidth. Furthermore, we demonstrate that non-resonant\ndot-cavity coupling can be utilized to directly monitor and study relevant QD\ns-shell properties like fine-structure splittings, emission saturation and\npower broadening, as well as photon statistics with negligible background\ncontributions. Our results open a new perspective on the understanding and\nimplementation of dot-cavity systems for single-photon sources, single and\nmultiple quantum dot lasers, semiconductor cavity quantum electrodynamics, and\ntheir implementation, e.g. in quantum information technology. \n\n"}
{"id": "0904.4081", "contents": "Title: Topological characterization of the hyperbolic maps in the Sine family Abstract: The purpose of this paper is to establish a topological characterization of\nall the hyperbolic maps in the Sine family $\\{\\lambda \\sin(z)\n\\:\\big{|}\\:\\lambda \\ne 0\\}$ which have super-attracting cycles. \n\n"}
{"id": "0904.4845", "contents": "Title: Magnetism, entropy, and the first nano-machines Abstract: The efficiency of bio-molecular motors stems from reversible interactions\n$\\sim$ $k_B T$; weak bonds stabilizing intermediate states (enabling $direct$\nconversion of chemical into mechanical energy). For their (unknown) origins, we\nsuggest that a magnetically structured phase (MSP) formed via accretion of\nsuper-paramagnetic particles (S-PPs) by magnetic rocks on the Hadean Ocean\nfloor had hosted motor-like diffusion of ligand-bound S-PPs through its\ntemplate-layers; its ramifications range from optical activity to quantum\ncoherence. A gentle flux gradient offers both detailed-balance breaking\nnon-equilibrium and $asymmetry$ to a magnetic dipole, undergoing infinitesimal\nspin-alignment changes. Periodic perturbation of this background by local\nH-fields of template-partners can lead to periodic high and low-template\naffinity states, due to the dipole's magnetic degree of freedom. An\naccompanying magnetocaloric effect allows interchange between system-entropy\nand bath temperature. We speculate on a magnetic reproducer in a setting close\nto the mound-scenario of Russell and coworkers that could evolve bio- ratchets. \n\n"}
{"id": "0907.2820", "contents": "Title: Fekete points and convergence towards equilibrium measures on complex\n  manifolds Abstract: Building on the first two authors' previous results, we prove a general\ncriterion for convergence of (possibly singular) Bergman measures towards\nequilibrium measures on complex manifolds. The criterion may be formulated in\nterms of growth properties of balls of holomorphic sections, or equivalently as\nan asymptotic minimization of generalized Donaldson L-functionals. Our result\nyields in particular the proof of a well-known conjecture in pluripotential\ntheory concerning the equidistribution of Fekete points, and it also gives the\nconvergence of Bergman measures towards equilibrium for Bernstein-Markov\nmeasures. Applications to interpolation of holomorphic sections are also\ndiscussed. \n\n"}
{"id": "0909.0106", "contents": "Title: On the canonical line bundle and negative holomorphic sectional\n  curvature Abstract: We prove that a smooth complex projective threefold with a K\\\"ahler metric of\nnegative holomorphic sectional curvature has ample canonical line bundle. In\ndimensions greater than three, we prove that, under equal assumptions, the nef\ndimension of the canonical line bundle is maximal. With certain additional\nassumptions, ampleness is again obtained. The methods used come from both\ncomplex differential geometry and complex algebraic geometry. \n\n"}
{"id": "0909.3600", "contents": "Title: Discrete Riemann Surfaces and the Ising model Abstract: We define a new theory of discrete Riemann surfaces and present its basic\nresults. The key idea is to consider not only a cellular decomposition of a\nsurface, but the union with its dual. Discrete holomorphy is defined by a\nstraightforward discretisation of the Cauchy-Riemann equation. A lot of\nclassical results in Riemann theory have a discrete counterpart, Hodge star,\nharmonicity, Hodge theorem, Weyl's lemma, Cauchy integral formula, existence of\nholomorphic forms with prescribed holonomies. Giving a geometrical meaning to\nthe construction on a Riemann surface, we define a notion of criticality on\nwhich we prove a continuous limit theorem. We investigate its connection with\ncriticality in the Ising model. We set up a Dirac equation on a discrete\nuniversal spin structure and we prove that the existence of a Dirac spinor is\nequivalent to criticality. \n\n"}
{"id": "0910.0346", "contents": "Title: Counting zeros of holomorphic functions of exponential growth Abstract: We consider the number of zeros of holomorphic functions in a bounded domain\nthat depend on a small parameter and satisfy an exponential upper bound near\nthe boundary of the domain and similar lower bounds at finitely many points\nalong the boundary. Roughly the number of such zeros is $(2\\pi h)^{-1}$ times\nthe integral over the domain of the laplacian of the exponent of the dominating\nexponential. Such results have already been obtained by M. Hager and by Hager\nand the author and they are of importance when studying the asymptotic\ndistribution of eigenvalues of elliptic operators with small random\nperturbations. In this paper we generalize these results and arrive at\ngeometrically natural statements and natural remainder estimates. \n\n"}
{"id": "0910.2875", "contents": "Title: A Non-Autonomous Version Of The Denjoy-Wolff Theorem Abstract: The aim of this work is to establish the celebrated Denjoy-Wolff Theorem in\nthe context of generalized Loewner chains. In contrast with the classical\nsituation where essentially convergence to a certain point in the closed unit\ndisk is the unique possibility, several new dynamical phenomena appear in this\nframework. Indeed, $\\omega$-limits formed by suitable closed arcs of\ncircumferences appear now as natural possibilities of asymptotic dynamical\nbehavior. \n\n"}
{"id": "0910.3144", "contents": "Title: Abstract Physical Traces Abstract: We revise our \"Physical Traces\" paper in the light of the results in \"A\nCategorical Semantics of Quantum Protocols\". The key fact is that the notion of\na strongly compact closed category allows abstract notions of adjoint,\nbipartite projector and inner product to be defined, and their key properties\nto be proved. In this paper we improve on the definition of strong compact\nclosure as compared to the one presented in Categorical Semantics of Quantum\nProtocols. This modification enables an elegant characterization of strong\ncompact closure in terms of adjoints and a Yanking axiom, and a better\ntreatment of bipartite projectors. \n\n"}
{"id": "0910.3452", "contents": "Title: Adiabatic quantum computation along quasienergies Abstract: The parametric deformations of quasienergies and eigenvectors of unitary\noperators are applied to the design of quantum adiabatic algorithms. The\nconventional, standard adiabatic quantum computation proceeds along\neigenenergies of parameter-dependent Hamiltonians. By contrast, discrete\nadiabatic computation utilizes adiabatic passage along the quasienergies of\nparameter-dependent unitary operators. For example, such computation can be\nrealized by a concatenation of parameterized quantum circuits, with an\nadiabatic though inevitably discrete change of the parameter. A design\nprinciple of adiabatic passage along quasienergy is recently proposed: Cheon's\nquasienergy and eigenspace anholonomies on unitary operators is available to\nrealize anholonomic adiabatic algorithms [Tanaka and Miyamoto, Phys. Rev. Lett.\n98, 160407 (2007)], which compose a nontrivial family of discrete adiabatic\nalgorithms. It is straightforward to port a standard adiabatic algorithm to an\nanholonomic adiabatic one, except an introduction of a parameter |v>, which is\navailable to adjust the gaps of the quasienergies to control the running time\nsteps. In Grover's database search problem, the costs to prepare |v> for the\nqualitatively different, i.e., power or exponential, running time steps are\nshown to be qualitatively different. Curiously, in establishing the equivalence\nbetween the standard quantum computation based on the circuit model and the\nanholonomic adiabatic quantum computation model, it is shown that the cost for\n|v> to enlarge the gaps of the eigenvalue is qualitatively negligible. \n\n"}
{"id": "0911.2361", "contents": "Title: Heavy electrons: Electron droplets generated by photogalvanic and\n  pyroelectric effects Abstract: Electron clusters, X-rays and nanosecond radio-frequency pulses are produced\nby 100 mW continuous-wave laser illuminating ferroelectric crystal of LiNbO_3.\nA long-living stable electron droplet with the size of about 100 mcm has freely\nmoved with the velocity 0.5 cm/s in the air near the surface of the crystal\nexperiencing the Earth gravitational field. The microscopic model of cluster\nstability, which is based on submicroscopic mechanics developed in the real\nphysical space, is suggested. The role of a restraining force plays the inerton\nfield, a substructure of the particles' matter waves, which a solitary one can\nelastically withstand the Coulomb repulsion of electrons. It is shown that\nelectrons in the droplet are heavy electrons whose mass at least 1 million of\ntimes exceeds the rest mass of free electron. Application for X-ray imaging and\nlithography is discussed. \n\n"}
{"id": "0911.4675", "contents": "Title: Large entropy measures for endomorphisms of CP(k) Abstract: Let $f$ be an holomorphic endomorphism of $\\mathbb{C}\\mathbb{P}^k$. We\nconstruct by using coding techniques a class of ergodic measures as limits of\nnon-uniform probability measures on preimages of points. We show that they have\nlarge metric entropy, close to $\\log d^k$. We establish for them strong\nstochastic properties and prove the positivity of their Lyapunov exponents.\nSince they have large entropy, those measures are supported in the support of\nthe maximal entropy measure of $f$. They in particular provide lower bounds for\nthe Hausdorff dimension of the Julia set. \n\n"}
{"id": "0911.5460", "contents": "Title: An Iterative Algorithm for Fitting Nonconvex Penalized Generalized\n  Linear Models with Grouped Predictors Abstract: High-dimensional data pose challenges in statistical learning and modeling.\nSometimes the predictors can be naturally grouped where pursuing the\nbetween-group sparsity is desired. Collinearity may occur in real-world\nhigh-dimensional applications where the popular $l_1$ technique suffers from\nboth selection inconsistency and prediction inaccuracy. Moreover, the problems\nof interest often go beyond Gaussian models. To meet these challenges,\nnonconvex penalized generalized linear models with grouped predictors are\ninvestigated and a simple-to-implement algorithm is proposed for computation. A\nrigorous theoretical result guarantees its convergence and provides tight\npreliminary scaling. This framework allows for grouped predictors and nonconvex\npenalties, including the discrete $l_0$ and the `$l_0+l_2$' type penalties.\nPenalty design and parameter tuning for nonconvex penalties are examined.\nApplications of super-resolution spectrum estimation in signal processing and\ncancer classification with joint gene selection in bioinformatics show the\nperformance improvement by nonconvex penalized estimation. \n\n"}
{"id": "0912.0457", "contents": "Title: Explicit expression for the generating function counting Gessel's walks Abstract: Gessel's walks are the planar walks that move within the positive quadrant\n$\\mathbb{Z}_{+}^{2}$ by unit steps in any of the following directions: West,\nNorth-East, East and South-West. In this paper, we find an explicit expression\nfor the trivariate generating function counting the Gessel's walks with $k\\geq\n0$ steps, which start at $(0,0)$ and end at a given point $(i,j) \\in\n\\mathbb{Z}^2_+$. \n\n"}
{"id": "0912.1064", "contents": "Title: On the numeric stability of the SFA implementation sfa-tk Abstract: Slow feature analysis (SFA) is a method for extracting slowly varying\nfeatures from a quickly varying multidimensional signal. An open source\nMatlab-implementation sfa-tk makes SFA easily useable. We show here that under\ncertain circumstances, namely when the covariance matrix of the nonlinearly\nexpanded data does not have full rank, this implementation runs into numerical\ninstabilities. We propse a modified algorithm based on singular value\ndecomposition (SVD) which is free of those instabilities even in the case where\nthe rank of the matrix is only less than 10% of its size. Furthermore we show\nthat an alternative way of handling the numerical problems is to inject a small\namount of noise into the multidimensional input signal which can restore a\nrank-deficient covariance matrix to full rank, however at the price of\nmodifying the original data and the need for noise parameter tuning. \n\n"}
{"id": "0912.4271", "contents": "Title: A Quantum Monte Carlo Method at Fixed Energy Abstract: In this paper we explore new ways to study the zero temperature limit of\nquantum statistical mechanics using Quantum Monte Carlo simulations. We develop\na Quantum Monte Carlo method in which one fixes the ground state energy as a\nparameter. The Hamiltonians we consider are of the form $H=H_{0}+\\lambda V$\nwith ground state energy E. For fixed $H_{0}$ and V, one can view E as a\nfunction of $\\lambda$ whereas we view $\\lambda$ as a function of E. We fix E\nand define a path integral Quantum Monte Carlo method in which a path makes no\nreference to the times (discrete or continuous) at which transitions occur\nbetween states. For fixed E we can determine $\\lambda(E)$ and other ground\nstate properties of H. \n\n"}
{"id": "1001.3400", "contents": "Title: A New Generating Function of (q-) Bernstein Type Polynomials and their\n  Interpolation Function Abstract: The main object of this paper is to construct a new generating function of\nthe (q-) Bernstein type polynomials. We establish elementary properties of this\nfunction. By using this generating function, we derive recurrence relation and\nderivative of the (q-) Bernstein type polynomials. We also give relations\nbetween the (q-) Bernstein type polynomials, Hermite polynomials, Bernoulli\npolynomials of higher-order and the second kind Stirling numbers. By applying\nMellin transformation to this generating function, we define interpolation of\nthe (q-) Bernstein type polynomials. Moreover, we give some applications and\nquestions on approximations of (q-) Bernstein type polynomials, moments of some\ndistributions in Statistics. \n\n"}
{"id": "1001.3515", "contents": "Title: On integral conditions in the mapping theory Abstract: It is established interconnections between various integral conditions that\nplay an important role in the theory of space mappings and in the theory of\ndegenerate Beltrami equations in the plane. \n\n"}
{"id": "1001.4369", "contents": "Title: Minimizing measures on condensers of infinitely many plates Abstract: The study deals with a minimal energy problem over noncompact classes of\ninfinite dimensional vector measures in a locally compact space. The components\nare positive measures (charges) satisfying certain normalizing assumptions and\nsupported by given closed sets (plates) with the sign +1 or -1 prescribed such\nthat oppositely signed sets are mutually disjoint, and the interaction matrix\nfor the charges corresponds to an electrostatic interpretation of a condenser.\nFor all positive definite kernels satisfying Fuglede's condition of consistency\nbetween the weak* and strong topologies, sufficient conditions for the\nexistence of equilibrium measures are established and properties of their\nuniqueness, vague compactness, and continuity are studied. \n\n"}
{"id": "1002.3003", "contents": "Title: Two-particle quantum walks applied to the graph isomorphism problem Abstract: We show that the quantum dynamics of interacting and noninteracting quantum\nparticles are fundamentally different in the context of solving a particular\ncomputational problem. Specifically, we consider the graph isomorphism problem,\nin which one wishes to determine whether two graphs are isomorphic (related to\neach other by a relabeling of the graph vertices), and focus on a class of\ngraphs with particularly high symmetry called strongly regular graphs (SRG's).\nWe study the Green's functions that characterize the dynamical evolution\nsingle-particle and two-particle quantum walks on pairs of non-isomorphic SRG's\nand show that interacting particles can distinguish non-isomorphic graphs that\nnoninteracting particles cannot. We obtain the following specific results: (1)\nWe prove that quantum walks of two noninteracting particles, Fermions or\nBosons, cannot distinguish certain pairs of non-isomorphic SRG's. (2) We\ndemonstrate numerically that two interacting Bosons are more powerful than\nsingle particles and two noninteracting particles, in that quantum walks of\ninteracting bosons distinguish all non-isomorphic pairs of SRGs that we\nexamined. By utilizing high-throughput computing to perform over 500 million\ndirect comparisons between evolution operators, we checked all tabulated pairs\nof non-isomorphic SRGs, including graphs with up to 64 vertices. (3) By\nperforming a short-time expansion of the evolution operator, we derive\ndistinguishing operators that provide analytic insight into the power of the\ninteracting two-particle quantum walk. \n\n"}
{"id": "1002.4603", "contents": "Title: Causality and the effective range expansion Abstract: We derive the generalization of Wigner's causality bounds and Bethe's\nintegral formula for the effective range parameter to arbitrary dimension and\narbitrary angular momentum. We also discuss the impact of these constraints on\nthe separation of low- and high-momentum scales and universality in low-energy\nscattering. Some of our results were summarized earlier in a letter\npublication. In this work, we present full derivations and several detailed\nexamples. \n\n"}
{"id": "1003.4439", "contents": "Title: Feuilletage lisse de $S^5$ par surfaces complexes Abstract: In 2002 Meersseman-Verjovsky [2] constructed a smooth, codimension-one,\nfoliation on 5-sphere by complex surfaces with two compact leaves. The aim of\nthis note is to improve their construction in order to give a smooth foliation\non 5-sphere by complex surfaces with only one compact leaf. \n\n"}
{"id": "1004.1484", "contents": "Title: Value distribution of the Gauss map of improper affine spheres Abstract: We give the best possible upper bound for the number of exceptional values of\nthe Lagrangian Gauss map of complete improper affine fronts in the affine\nthree-space. We also obtain the sharp estimate for weakly complete case. As an\napplication of this result, we provide a new and simple proof of the parametric\naffine Bernstein problem for improper affine spheres. Moreover we get the same\nestimate for the ratio of canonical forms of weakly complete flat fronts in\nhyperbolic three-space. \n\n"}
{"id": "1005.2775", "contents": "Title: Quantum circuits for spin and flavor degrees of freedom of quarks\n  forming nucleons Abstract: We discuss the quantum-circuit realization of the state of a nucleon in the\nscope of simple symmetry groups. Explicit algorithms are presented for the\npreparation of the state of a neutron or a proton as resulting from the\ncomposition of their quark constituents. We estimate the computational\nresources required for such a simulation and design a photonic network for its\nimplementation. Moreover, we highlight that current work on three-body\ninteractions in lattices of interacting qubits, combined with the\nmeasurement-based paradigm for quantum information processing, may also be\nsuitable for the implementation of these nucleonic spin states. \n\n"}
{"id": "1005.2938", "contents": "Title: Correlation versus commensurability effects for finite bosonic systems\n  in one-dimensional lattices Abstract: We investigate few-boson systems in finite one-dimensional multi-well traps\ncovering the full interaction crossover from uncorrelated to fermionized\nparticles. Our treatment of the ground state properties is based on the\nnumerically exact Multi-Configurational Time-Dependent Hartree method. For\ncommensurate filling we trace the fingerprints of localisation, as the\ninteraction strength increases, in several observables like reduced density\nmatrices, fluctuations and momentum distribution. For filling factor larger\nthan one we observe on-site repulsion effects in the densities and\nfragmentation of particles beyond the validity of the Bose-Hubbard model upon\napproaching the Tonks-Girardeau limit. The presence of an incommensurate\nfraction of particles induces incomplete localisation and spatial modulations\nof the density profiles, taking into account the finite size of the system. \n\n"}
{"id": "1006.3972", "contents": "Title: Graph-Valued Regression Abstract: Undirected graphical models encode in a graph $G$ the dependency structure of\na random vector $Y$. In many applications, it is of interest to model $Y$ given\nanother random vector $X$ as input. We refer to the problem of estimating the\ngraph $G(x)$ of $Y$ conditioned on $X=x$ as ``graph-valued regression.'' In\nthis paper, we propose a semiparametric method for estimating $G(x)$ that\nbuilds a tree on the $X$ space just as in CART (classification and regression\ntrees), but at each leaf of the tree estimates a graph. We call the method\n``Graph-optimized CART,'' or Go-CART. We study the theoretical properties of\nGo-CART using dyadic partitioning trees, establishing oracle inequalities on\nrisk minimization and tree partition consistency. We also demonstrate the\napplication of Go-CART to a meteorological dataset, showing how graph-valued\nregression can provide a useful tool for analyzing complex data. \n\n"}
{"id": "1007.5006", "contents": "Title: SU(m) non-Abelian anyons in the Jain hierarchy of quantum Hall states Abstract: We show that different classes of topological order can be distinguished by\nthe dynamical symmetry algebra of edge excitations. Fundamental topological\norder is realized when this algebra is the largest possible, the algebra of\nquantum area-preserving diffeomorphisms, called $W_{1+\\infty}$. We argue that\nthis order is realized in the Jain hierarchy of fractional quantum Hall states\nand show that it is more robust than the standard Abelian Chern-Simons order\nsince it has a lower entanglement entropy due to the non-Abelian character of\nthe quasi-particle anyon excitations. These behave as SU($m$) quarks, where $m$\nis the number of components in the hierarchy. We propose the topological\nentanglement entropy as the experimental measure to detect the existence of\nthese quantum Hall quarks. Non-Abelian anyons in the $\\nu = 2/5$ fractional\nquantum Hall states could be the primary candidates to realize qbits for\ntopological quantum computation. \n\n"}
{"id": "1008.2147", "contents": "Title: Quantum Tagging: Authenticating Location via Quantum Information and\n  Relativistic Signalling Constraints Abstract: We define the task of {\\it quantum tagging}, that is, authenticating the\nclassical location of a classical tagging device by sending and receiving\nquantum signals from suitably located distant sites, in an environment\ncontrolled by an adversary whose quantum information processing and\ntransmitting power is unbounded. We define simple security models for this task\nand briefly discuss alternatives.\n  We illustrate the pitfalls of naive quantum cryptographic reasoning in this\ncontext by describing several protocols which at first sight appear\nunconditionally secure but which, as we show, can in fact be broken by\nteleportation-based attacks. We also describe some protocols which cannot be\nbroken by these specific attacks, but do not prove they are unconditionally\nsecure.\n  We review the history of quantum tagging protocols, which we first discussed\nin 2002 and described in a 2006 patent (for an insecure protocol). The\npossibility has recently been reconsidered by other authors. All the more\nrecently discussed protocols of which we are aware were either previously\nconsidered by us in 2002-3 or are variants of schemes then considered, and all\nare provably insecure. \n\n"}
{"id": "1009.6077", "contents": "Title: Discrete Complex Analysis and Probability Abstract: We discuss possible discretizations of complex analysis and some of their\napplications to probability and mathematical physics, following our recent work\nwith Dmitry Chelkak, Hugo Duminil-Copin and Cl\\'ement Hongler. \n\n"}
{"id": "1010.1292", "contents": "Title: A Viscosity Approach to the Dirichlet Problem for Complex Monge-Amp\\`ere\n  Equations Abstract: The Dirichlet problem for complex Monge-Amp\\'ere equations with continuous\ndata is considered. In particular, a notion of viscosity solutions is\nintroduced; a comparison principle and a solvability theorem are proved; the\nequivalence between viscosity and pluripotential solutions is established; and\nan ABP-type of $L^{\\infty}$-estimate is achieved. \n\n"}
{"id": "1010.3548", "contents": "Title: The positive real lemma and construction of all realizations of\n  generalized positive rational functions Abstract: We here extend the well known Positive Real Lemma (also known as the\nKalman-Yakubovich-Popov Lemma) to complex matrix-valued generalized positive\nrational function, when non-minimal realizations are considered. We then\nexploit this result to provide an easy construction procedure of all (not\nnecessarily minimal) state space realizations of generalized positive\nfunctions. As a by-product, we partition all state space realizations into\nsubsets: Each is identified with a set of matrices satisfying the same Lyapunov\ninclusion and thus form a convex invertible cone, cic in short. Moreover, this\napproach enables us to characterize systems which may be brought to be\ngeneralized positive through static output feedback. The formulation through\nLyapunov inclusions suggests the introduction of an equivalence class of\nrational functions of various dimensions associated with the same system\nmatrix. \n\n"}
{"id": "1010.4237", "contents": "Title: Robust PCA via Outlier Pursuit Abstract: Singular Value Decomposition (and Principal Component Analysis) is one of the\nmost widely used techniques for dimensionality reduction: successful and\nefficiently computable, it is nevertheless plagued by a well-known,\nwell-documented sensitivity to outliers. Recent work has considered the setting\nwhere each point has a few arbitrarily corrupted components. Yet, in\napplications of SVD or PCA such as robust collaborative filtering or\nbioinformatics, malicious agents, defective genes, or simply corrupted or\ncontaminated experiments may effectively yield entire points that are\ncompletely corrupted.\n  We present an efficient convex optimization-based algorithm we call Outlier\nPursuit, that under some mild assumptions on the uncorrupted points (satisfied,\ne.g., by the standard generative assumption in PCA problems) recovers the exact\noptimal low-dimensional subspace, and identifies the corrupted points. Such\nidentification of corrupted points that do not conform to the low-dimensional\napproximation, is of paramount interest in bioinformatics and financial\napplications, and beyond. Our techniques involve matrix decomposition using\nnuclear norm minimization, however, our results, setup, and approach,\nnecessarily differ considerably from the existing line of work in matrix\ncompletion and matrix decomposition, since we develop an approach to recover\nthe correct column space of the uncorrupted matrix, rather than the exact\nmatrix itself. In any problem where one seeks to recover a structure rather\nthan the exact initial matrices, techniques developed thus far relying on\ncertificates of optimality, will fail. We present an important extension of\nthese methods, that allows the treatment of such problems. \n\n"}
{"id": "1011.0205", "contents": "Title: Reply to Comment by Wolfgang Ketterle on \"Electromagnetic Wave Dynamics\n  in Matter-Wave Superradiant Scattering\" (see arXiv:1010.3915) Abstract: The Comment by Wolfgang Ketterle (Ref.[1]) purports to present a viable model\nof superradiance in condensates. However, Ref.[1] is not able to explain the\nred/blue pump detuning asymmetry that was first observed recently by us\n(Ref.[2]). It is clear from our original paper (Ref.[3]) that the\nrate-equation-based theories of Ref.[1] are incomplete since they only model\nthe final growth stage of the process when a red-detuned pump is used. Our\ntheoretical framework (Ref.[3]), on the other hand, also treats the initial\ngrowth stage of superradiance and is therefore also capable of explaining the\ngenesis of the red/blue detuning asymmetry (Ref.[2]). This is the key message\nof our response, which we frame in terms of reference to the specific points\nraised in Ref. [1]. \n\n"}
{"id": "1011.3310", "contents": "Title: On homeomorphisms with finite distortion in the plane Abstract: It is shown that every homeomorphism f of finite distortion in the plane is\nthe so-called lower Q-homeomorphism with Q(z)=K_f(z), and, on this base, it is\ndeveloped the theory of the boundary behavior of such homeomorphisms. \n\n"}
{"id": "1012.1397", "contents": "Title: Discrete-Time Controllability for Feedback Quantum Dynamics Abstract: Controllability properties for discrete-time, Markovian quantum dynamics are\ninvestigated. We find that, while in general the controlled system is not\nfinite-time controllable, feedback control allows for arbitrary asymptotic\nstate-to-state transitions. Under further assumption on the form of the\nmeasurement, we show that finite-time controllability can be achieved in a time\nthat scales linearly with the dimension of the system, and we provide an\niterative procedure to design the unitary control actions. \n\n"}
{"id": "1101.0136", "contents": "Title: Meromorphic extensions from small families of circles and holomorphic\n  extensions from spheres Abstract: Let B be the open unit ball in C^2 and let a, b, c be three points in C^2\nwhich do not lie in a complex line, such that the complex line through a and b\nmeets B and such that <a|b> is different from 1 if one of the points a, b is in\nB and the other in the complement of B and such that at least one of the\nnumbers <a|c>, <b|c> is different from 1. We prove that if a continuous\nfunction f on the sphere bB extends holomorphically into B along each complex\nline which passes through one of the points a, b, c then f extends\nholomorphically through B. This generalizes recent work of L.Baracco who proved\nsuch a result in the case when the points a, b, c are contained in B. The proof\nis different from the one of Baracco and uses the following one variable result\nwhich we also prove in the paper and which in the real analytic case follows\nfrom the work of M.Agranovsky: Let D be the open unit disc in C. Given a in D\nlet C(a) be the family of all circles in D obtained as the images of circles\ncentered at the origin under an automorphism of D that maps the origin to a.\nGiven distinct points a, b in D and a positive integer n, a continuous function\nf on the closed unit disc extends meromorphically from every circle T in either\nC(a) or C(b) through the disc bounded by T with the only pole at the center of\nT of degree not exceeding n if and only if f is of the form f(z) =\ng_0(z)+g_1(z)\\bar z +...+ g_n(z)\\bar z^n where the functions g_0, g_1, ..., g_n\nare holomorphic on D. \n\n"}
{"id": "1101.3712", "contents": "Title: Generic identification of binary-valued hidden Markov processes Abstract: The generic identification problem is to decide whether a stochastic process\n$(X_t)$ is a hidden Markov process and if yes to infer its parameters for all\nbut a subset of parametrizations that form a lower-dimensional subvariety in\nparameter space. Partial answers so far available depend on extra assumptions\non the processes, which are usually centered around stationarity. Here we\npresent a general solution for binary-valued hidden Markov processes. Our\napproach is rooted in algebraic statistics hence it is geometric in nature. We\nfind that the algebraic varieties associated with the probability distributions\nof binary-valued hidden Markov processes are zero sets of determinantal\nequations which draws a connection to well-studied objects from algebra. As a\nconsequence, our solution allows for algorithmic implementation based on\nelementary (linear) algebraic routines. \n\n"}
{"id": "1102.1191", "contents": "Title: Smoothed log-concave maximum likelihood estimation with applications Abstract: We study the smoothed log-concave maximum likelihood estimator of a\nprobability distribution on $\\mathbb{R}^d$. This is a fully automatic\nnonparametric density estimator, obtained as a canonical smoothing of the\nlog-concave maximum likelihood estimator. We demonstrate its attractive\nfeatures both through an analysis of its theoretical properties and a\nsimulation study. Moreover, we use our methodology to develop a new test of\nlog-concavity, and show how the estimator can be used as an intermediate stage\nof more involved procedures, such as constructing a classifier or estimating a\nfunctional of the density. Here again, the use of these procedures can be\njustified both on theoretical grounds and through its finite sample\nperformance, and we illustrate its use in a breast cancer diagnosis\n(classification) problem. \n\n"}
{"id": "1102.1910", "contents": "Title: Fatou-Julia theory for non-uniformly quasiregular maps Abstract: Many results of the Fatou-Julia iteration theory of rational functions extend\nto uniformly quasiregular maps in higher dimensions. We obtain results of this\ntype for certain classes of quasiregular maps which are not uniformly\nquasiregular. \n\n"}
{"id": "1102.2530", "contents": "Title: Bi-Harmonic mappings and J. C. C. Nitsche type conjecture Abstract: In this note it is formulated the J. C. C. Nitsche type conjecture for\nbi-harmonic mappings. The conjecture has been motivated by the radial\nbi-harmonic mappings between annuli. \n\n"}
{"id": "1102.5472", "contents": "Title: Dynamical properties of a trapped dipolar Fermi gas at finite\n  temperature Abstract: We investigate the dynamical properties of a trapped finite-temperature\nnormal Fermi gas with dipole-dipole interaction. For the free expansion\ndynamics, we show that the expanded gas always becomes stretched along the\ndirection of the dipole moment. In addition, we present the temperature and\ninteraction dependences of the asymptotical aspect ratio. We further study the\ncollapse dynamics of the system by suddenly increasing the dipolar interaction\nstrength. We show that, in contrast to the anisotropic collapse of a dipolar\nBose-Einstein condensate, a dipolar Fermi gas always collapses isotropically\nwhen the system becomes globally unstable. We also explore the interaction and\ntemperature dependences for the frequencies of the low-lying collective\nexcitations. \n\n"}
{"id": "1103.1686", "contents": "Title: Fractional Quantum Hall Effect of Hard-Core Bosons in Topological Flat\n  Bands Abstract: Recent proposals of topological flat band (TFB) models have provided a new\nroute to realize the fractional quantum Hall effect (FQHE) without Landau\nlevels. We study hard-core bosons with short-range interactions in two\nrepresentative TFB models, one of which is the well known Haldane model (but\nwith different parameters). We demonstrate that FQHE states emerge with\nsignatures of even number of quasi-degenerate ground states on a torus and a\nrobust spectrum gap separating these states from higher energy spectrum. We\nalso establish quantum phase diagrams for the filling factor 1/2 and illustrate\nquantum phase transitions to other competing symmetry-breaking phases. \n\n"}
{"id": "1103.4199", "contents": "Title: Demonstrating various quantum effects with two entangled laser beams Abstract: We report on the preparation of entangled two mode squeezed states of yet\nunseen quality. Based on a measurement of the covariance matrix we found a\nviolation of the Reid and Drummond EPR-criterion at a value of only 0.36\\pm0.03\ncompared to the threshold of 1. Furthermore, quantum state tomography was used\nto extract a single photon Fock state solely based on homodyne detection,\ndemonstrating the strong quantum features of this pair of laser-beams. The\nprobability for a single photon in this ensemble measurement exceeded 2/3. \n\n"}
{"id": "1103.5640", "contents": "Title: Stieltjes, Poisson and other integral representations for functions of\n  Lambert $W$ Abstract: We show that many functions containing $W$ are Stieltjes functions. Explicit\nStieltjes integrals are given for functions $1/W(z)$, $W(z)/z$, and others. We\nalso prove a generalization of a conjecture of Jackson, Procacci & Sokal.\nIntegral representations of $W$ and related functions are also given which are\nassociated with the properties of their being Pick or Bernstein functions.\nRepresentations based on Poisson and Burniston--Siewert integrals are given as\nwell. \n\n"}
{"id": "1104.0191", "contents": "Title: Normal form backward induction for decision trees with coherent lower\n  previsions Abstract: We examine normal form solutions of decision trees under typical choice\nfunctions induced by lower previsions. For large trees, finding such solutions\nis hard as very many strategies must be considered. In an earlier paper, we\nextended backward induction to arbitrary choice functions, yielding far more\nefficient solutions, and we identified simple necessary and sufficient\nconditions for this to work. In this paper, we show that backward induction\nworks for maximality and E-admissibility, but not for interval dominance and\nGamma-maximin. We also show that, in some situations, a computationally cheap\napproximation of a choice function can be used, even if the approximation\nviolates the conditions for backward induction; for instance, interval\ndominance with backward induction will yield at least all maximal normal form\nsolutions. \n\n"}
{"id": "1104.3924", "contents": "Title: K\\\"ahler-Ricci Flow on Projective Bundles over K\\\"ahler-Einstein\n  Manifolds Abstract: We study the K\\\"ahler-Ricci flow on a class of projective bundles\n$\\mathbb{P}(\\mathcal{O}_\\Sigma \\oplus L)$ over compact K\\\"ahler-Einstein\nmanifold $\\Sigma^n$. Assuming the initial K\\\"ahler metric $\\omega_0$ admits a\nU(1)-invariant momentum profile, we give a criterion, characterized by the\ntriple $(\\Sigma, L, [\\omega_0])$, under which the $\\mathbb{P}^1$-fiber\ncollapses along the K\\\"ahler-Ricci flow and the projective bundle converges to\n$\\Sigma$ in Gromov-Hausdorff sense. Furthermore, the K\\\"ahler-Ricci flow must\nhave Type I singularity and is of $(\\C^n \\times \\mathbb{P}^1)$-type. This\ngeneralizes and extends part of Song-Weinkove's work \\cite{SgWk09} on\nHirzebruch surfaces. \n\n"}
{"id": "1104.4175", "contents": "Title: Partial Fraction Expansions for Newton's and Halley's Iterations for\n  Square Roots Abstract: When Newton's method, or Halley's method is used to approximate the $p${th}\nroot of $1-z$, a sequence of rational functions is obtained. In this paper, a\nbeautiful formula for these rational functions is proved in the square root\ncase, using an interesting link to Chebyshev's polynomials. It allows the\ndetermination of the sign of the coefficients of the power series expansion of\nthese rational functions. This answers positively the square root case of a\nproposed conjecture by Guo(2010). \n\n"}
{"id": "1105.0760", "contents": "Title: Variational Bayes approach for model aggregation in unsupervised\n  classification with Markovian dependency Abstract: We consider a binary unsupervised classification problem where each\nobservation is associated with an unobserved label that we want to retrieve.\nMore precisely, we assume that there are two groups of observation: normal and\nabnormal. The `normal' observations are coming from a known distribution\nwhereas the distribution of the `abnormal' observations is unknown. Several\nmodels have been developed to fit this unknown distribution. In this paper, we\npropose an alternative based on a mixture of Gaussian distributions. The\ninference is done within a variational Bayesian framework and our aim is to\ninfer the posterior probability of belonging to the class of interest. To this\nend, it makes no sense to estimate the mixture component number since each\nmixture model provides more or less relevant information to the posterior\nprobability estimation. By computing a weighted average (named aggregated\nestimator) over the model collection, Bayesian Model Averaging (BMA) is one way\nof combining models in order to account for information provided by each model.\nThe aim is then the estimation of the weights and the posterior probability for\none specific model. In this work, we derive optimal approximations of these\nquantities from the variational theory and propose other approximations of the\nweights. To perform our method, we consider that the data are dependent\n(Markovian dependency) and hence we consider a Hidden Markov Model. A\nsimulation study is carried out to evaluate the accuracy of the estimates in\nterms of classification. We also present an application to the analysis of\npublic health surveillance systems. \n\n"}
{"id": "1105.1051", "contents": "Title: Bound Molecules in an Interacting Quantum Walk Abstract: We investigate a system of two atoms in an optical lattice, performing a\nquantum walk by state-dependent shift operations and a coin operation acting on\nthe internal states. The atoms interact, e.g., by cold collisions, whenever\nthey are in the same potential well of the lattice. Under such conditions they\ntypically develop a bound state, so that the two atoms effectively perform a\nquantum walk together, rarely moving further from each other than a few lattice\nsites. The theoretical analysis is based on a theory of quantum walks with a\npoint defect, applied to the difference variable. We also discuss the\nfeasibility of an experimental realization in existing quantum walk\nexperiments. \n\n"}
{"id": "1105.5834", "contents": "Title: Orbital excitation blockade and algorithmic cooling in quantum gases Abstract: Interaction blockade occurs when strong interactions in a confined few-body\nsystem prevent a particle from occupying an otherwise accessible quantum state.\nBlockade phenomena reveal the underlying granular nature of quantum systems and\nallow the detection and manipulation of the constituent particles, whether they\nare electrons, spins, atoms, or photons. The diverse applications range from\nsingle-electron transistors based on electronic Coulomb blockade to quantum\nlogic gates in Rydberg atoms. We have observed a new kind of interaction\nblockade in transferring ultracold atoms between orbitals in an optical\nlattice. In this system, atoms on the same lattice site undergo coherent\ncollisions described by a contact interaction whose strength depends strongly\non the orbital wavefunctions of the atoms. We induce coherent orbital\nexcitations by modulating the lattice depth and observe a staircase-type\nexcitation behavior as we cross the interaction-split resonances by tuning the\nmodulation frequency. As an application of orbital excitation blockade (OEB),\nwe demonstrate a novel algorithmic route for cooling quantum gases. Our\nrealization of algorithmic cooling utilizes a sequence of reversible OEB-based\nquantum operations that isolate the entropy in one part of the system, followed\nby an irreversible step that removes the entropy from the gas. This work opens\nthe door to cooling quantum gases down to ultralow entropies, with implications\nfor developing a microscopic understanding of strongly correlated electron\nsystems that can be simulated in optical lattices. In addition, the close\nanalogy between OEB and dipole blockade in Rydberg atoms provides a roadmap for\nthe implementation of two-qubit gates in a quantum computing architecture with\nnatural scalability. \n\n"}
{"id": "1106.6024", "contents": "Title: The Rate of Convergence of AdaBoost Abstract: The AdaBoost algorithm was designed to combine many \"weak\" hypotheses that\nperform slightly better than random guessing into a \"strong\" hypothesis that\nhas very low error. We study the rate at which AdaBoost iteratively converges\nto the minimum of the \"exponential loss.\" Unlike previous work, our proofs do\nnot require a weak-learning assumption, nor do they require that minimizers of\nthe exponential loss are finite. Our first result shows that at iteration $t$,\nthe exponential loss of AdaBoost's computed parameter vector will be at most\n$\\epsilon$ more than that of any parameter vector of $\\ell_1$-norm bounded by\n$B$ in a number of rounds that is at most a polynomial in $B$ and $1/\\epsilon$.\nWe also provide lower bounds showing that a polynomial dependence on these\nparameters is necessary. Our second result is that within $C/\\epsilon$\niterations, AdaBoost achieves a value of the exponential loss that is at most\n$\\epsilon$ more than the best possible value, where $C$ depends on the dataset.\nWe show that this dependence of the rate on $\\epsilon$ is optimal up to\nconstant factors, i.e., at least $\\Omega(1/\\epsilon)$ rounds are necessary to\nachieve within $\\epsilon$ of the optimal exponential loss. \n\n"}
{"id": "1107.0102", "contents": "Title: Acyclic embeddings of open Riemann surfaces into new examples of\n  elliptic manifolds Abstract: The geometric notion of ellipticity for complex manifolds was introduced by\nGromov in his seminal 1989 paper on the Oka principle, and is a sufficient\ncondition for a manifold to be Oka. In the current paper we present\ncontributions to three open questions involving elliptic and Oka manifolds. We\nshow that quotients of C^n by discrete groups of affine transformations are\nelliptic. Combined with an example of Margulis, this yields new examples of\nelliptic manifolds with free fundamental groups and vanishing higher homotopy.\nFinally we show that every open Riemann surface embeds acyclically into an\nelliptic manifold, giving a partial answer to a question of Larusson. \n\n"}
{"id": "1107.4390", "contents": "Title: Multi-Task Averaging Abstract: We present a multi-task learning approach to jointly estimate the means of\nmultiple independent data sets. The proposed multi-task averaging (MTA)\nalgorithm results in a convex combination of the single-task maximum likelihood\nestimates. We derive the optimal minimum risk estimator and the minimax\nestimator, and show that these estimators can be efficiently estimated.\nSimulations and real data experiments demonstrate that MTA estimators often\noutperform both single-task and James-Stein estimators. \n\n"}
{"id": "1108.2327", "contents": "Title: SU(N) magnetism in chains of ultracold alkaline-earth-metal atoms: Mott\n  transitions and quantum correlations Abstract: We investigate one dimensional SU$(N)$ Hubbard chains at zero temperature,\nwhich can be emulated with ultracold alkaline earth atoms, by using the density\nmatrix renormalization group (DMRG), Bethe ansatz (BA), and bosonization. We\ncompute experimental observables and use the DMRG to benchmark the accuracy of\nthe Bethe ansatz for $N>2$ where the BA is only approximate. In the worst case,\nwe find a relative error $\\epsilon \\lesssim 4%$ in the BA ground state energy\nfor $N \\leq 4$ at filling 1/N, which is due to the fact that BA improperly\ntreats the triply and higher occupied states. Using the DMRG for $N \\leq 4$ and\nthe BA for large $N$, we determine the regimes of validity of strong- and\nweak-coupling perturbation theory for all values of $N$ and in particular, the\nparameter range in which the system is well described by a SU$(N)$ Heisenberg\nmodel at filling 1/N. We find this depends only weakly on $N$. We investigate\nthe Berezinskii-Kosterlitz-Thouless phase transition from a Luttinger liquid to\na Mott-insulator by computing the fidelity susceptibility and the Luttinger\nparameter $K_\\rho$ at 1/N filling. The numerical findings give strong evidence\nthat the fidelity susceptibility develops a minimum at a critical interaction\nstrength which is found to occur at a finite positive value for $N>2$. \n\n"}
{"id": "1108.5244", "contents": "Title: Semi-supervised logistic discrimination via labeled data and unlabeled\n  data from different sampling distributions Abstract: This article addresses the problem of classification method based on both\nlabeled and unlabeled data, where we assume that a density function for labeled\ndata is different from that for unlabeled data. We propose a semi-supervised\nlogistic regression model for classification problem along with the technique\nof covariate shift adaptation. Unknown parameters involved in proposed models\nare estimated by regularization with EM algorithm. A crucial issue in the\nmodeling process is the choices of tuning parameters in our semi-supervised\nlogistic models. In order to select the parameters, a model selection criterion\nis derived from an information-theoretic approach. Some numerical studies show\nthat our modeling procedure performs well in various cases. \n\n"}
{"id": "1109.3142", "contents": "Title: On principles of inductive inference Abstract: We propose an intersubjective epistemic approach to foundations of\nprobability theory and statistical inference, based on relative entropy and\ncategory theory, and aimed to bypass the mathematical and conceptual problems\nof existing foundational approaches. \n\n"}
{"id": "1109.4540", "contents": "Title: Manifold estimation and singular deconvolution under Hausdorff loss Abstract: We find lower and upper bounds for the risk of estimating a manifold in\nHausdorff distance under several models. We also show that there are close\nconnections between manifold estimation and the problem of deconvolving a\nsingular measure. \n\n"}
{"id": "1109.6090", "contents": "Title: Robust Parametric Classification and Variable Selection by a Minimum\n  Distance Criterion Abstract: We investigate a robust penalized logistic regression algorithm based on a\nminimum distance criterion. Influential outliers are often associated with the\nexplosion of parameter vector estimates, but in the context of standard\nlogistic regression, the bias due to outliers always causes the parameter\nvector to implode, that is shrink towards the zero vector. Thus, using\nLASSO-like penalties to perform variable selection in the presence of outliers\ncan result in missed detections of relevant covariates. We show that by\nchoosing a minimum distance criterion together with an Elastic Net penalty, we\ncan simultaneously find a parsimonious model and avoid estimation implosion\neven in the presence of many outliers in the important small $n$ large $p$\nsituation. Implementation using an MM algorithm is described and performance\nevaluated. \n\n"}
{"id": "1110.6084", "contents": "Title: The multi-armed bandit problem with covariates Abstract: We consider a multi-armed bandit problem in a setting where each arm produces\na noisy reward realization which depends on an observable random covariate. As\nopposed to the traditional static multi-armed bandit problem, this setting\nallows for dynamically changing rewards that better describe applications where\nside information is available. We adopt a nonparametric model where the\nexpected rewards are smooth functions of the covariate and where the hardness\nof the problem is captured by a margin parameter. To maximize the expected\ncumulative reward, we introduce a policy called Adaptively Binned Successive\nElimination (abse) that adaptively decomposes the global problem into suitably\n\"localized\" static bandit problems. This policy constructs an adaptive\npartition using a variant of the Successive Elimination (se) policy. Our\nresults include sharper regret bounds for the se policy in a static bandit\nproblem and minimax optimal regret bounds for the abse policy in the dynamic\nproblem. \n\n"}
{"id": "1111.3743", "contents": "Title: Experimental fully contextual correlations Abstract: Quantum correlations are contextual yet, in general, nothing prevents the\nexistence of even more contextual correlations. We identify and test a\nnoncontextuality inequality in which the quantum violation cannot be improved\nby any hypothetical postquantum theory, and use it to experimentally obtain\ncorrelations in which the fraction of noncontextual correlations is less than\n0.06. Our correlations are experimentally generated from the results of\nsequential compatible tests on a four-state quantum system encoded in the\npolarization and path of a single photon. \n\n"}
{"id": "1111.5379", "contents": "Title: Self-Avoiding Random Dynamics on Integer Complex Systems Abstract: This paper introduces a new specialized algorithm for equilibrium Monte Carlo\nsampling of binary-valued systems, which allows for large moves in the state\nspace. This is achieved by constructing self-avoiding walks (SAWs) in the state\nspace. As a consequence, many bits are flipped in a single MCMC step. We name\nthe algorithm SARDONICS, an acronym for Self-Avoiding Random Dynamics on\nInteger Complex Systems. The algorithm has several free parameters, but we show\nthat Bayesian optimization can be used to automatically tune them. SARDONICS\nperforms remarkably well in a broad number of sampling tasks: toroidal\nferromagnetic and frustrated Ising models, 3D Ising models, restricted\nBoltzmann machines and chimera graphs arising in the design of quantum\ncomputers. \n\n"}
{"id": "1112.3605", "contents": "Title: Beta-Negative Binomial Process and Poisson Factor Analysis Abstract: A beta-negative binomial (BNB) process is proposed, leading to a\nbeta-gamma-Poisson process, which may be viewed as a \"multi-scoop\"\ngeneralization of the beta-Bernoulli process. The BNB process is augmented into\na beta-gamma-gamma-Poisson hierarchical structure, and applied as a\nnonparametric Bayesian prior for an infinite Poisson factor analysis model. A\nfinite approximation for the beta process Levy random measure is constructed\nfor convenient implementation. Efficient MCMC computations are performed with\ndata augmentation and marginalization techniques. Encouraging results are shown\non document count matrix factorization. \n\n"}
{"id": "1112.6141", "contents": "Title: Thermodynamics of Dipolar Chain Systems Abstract: The thermodynamics of a quantum system of layers containing perpendicularly\noriented dipolar molecules is studied within an oscillator approximation for\nboth bosonic and fermionic species. The system is assumed to be built from\nchains with one molecule in each layer. We consider the effects of the\nintralayer repulsion and quantum statistical requirements in systems with more\nthan one chain. Specifically, we consider the case of two chains and solve the\nproblem analytically within the harmonic Hamiltonian approach which is accurate\nfor large dipole moments. The case of three chains is calculated numerically.\nOur findings indicate that thermodynamic observables, such as the heat\ncapacity, can be used to probe the signatures of the intralayer interaction\nbetween chains. This should be relevant for near future experiments on polar\nmolecules with strong dipole moments. \n\n"}
{"id": "1201.0285", "contents": "Title: Ferromagnetic Ordering in Carbon Nanotubes, Incorporated in Diamond\n  Single Crystals Abstract: The physical origin of the mechanism of the formation of ferromagnetic\nordering in carbon nanotubes (NTs), produced by high energy ion beam\nmodification of diamond single crystals in $\\langle{110}\\rangle$ and\n$\\langle{111}\\rangle$ directions has been found. It is concluded from analysis\nof experimental results on ferromagnetic spin wave resonance observed, that the\nonly $\\pi$-electronic subsystem of given NTs is responsible for the appearance\nof ferromagnetism. It is determined by asymmetry in spin density distribution\nin Su-Schrieffer-Heeger (SSH) topological soliton lattice. The formation of SSH\ntopological soliton lattice is considered in the frames of generalized\nSSH-model of organic conductors, in which $\\pi$-electronic subsystem is\nrepresented being to be 1D quantum Fermi liquid.\n  The phenomenon of formation of uncompensated antiferromagnetic ordering\ncoexisting with superconductivity at room temperature in carbon nanotubes,\nproduced by high energy ion beam modification of diamond single crystals in\n$\\langle{100}\\rangle$ direction is argued. \n\n"}
{"id": "1201.0862", "contents": "Title: Extension of SBL Algorithms for the Recovery of Block Sparse Signals\n  with Intra-Block Correlation Abstract: We examine the recovery of block sparse signals and extend the framework in\ntwo important directions; one by exploiting signals' intra-block correlation\nand the other by generalizing signals' block structure. We propose two families\nof algorithms based on the framework of block sparse Bayesian learning (BSBL).\nOne family, directly derived from the BSBL framework, requires knowledge of the\nblock structure. Another family, derived from an expanded BSBL framework, is\nbased on a weaker assumption on the block structure, and can be used when the\nblock structure is completely unknown. Using these algorithms we show that\nexploiting intra-block correlation is very helpful in improving recovery\nperformance. These algorithms also shed light on how to modify existing\nalgorithms or design new ones to exploit such correlation and improve\nperformance. \n\n"}
{"id": "1201.0970", "contents": "Title: A second variation formula for Perelman's $\\mathcal{W}$-functional along\n  the modified K\\\"ahler-Ricci flow Abstract: We show a quite simple second variation formula for Perelman's\n$\\mathcal{W}$-functional along the modified K\\\"ahler-Ricci flow over Fano\nmanifolds. \n\n"}
{"id": "1201.1212", "contents": "Title: Measuring quantumness via anticommutators Abstract: We introduce a method to witness the quantumness of a system. The method\nrelies on the fact that the anticommutator of two classical states is always\npositive. We show that there is always a nonpositive anticommutator due to any\ntwo quantum states. We notice that interference depends on the trace of the\nanticommutator of two states and it is therefore more natural to detect\nquantumness by looking at anticommutators of states rather than their\ncommutators. \n\n"}
{"id": "1201.3286", "contents": "Title: Does every contractive analytic function in a polydisk have a\n  dissipative n-dimensional scattering realization? Abstract: No. The title question was posed by D. Kalyuzhnyi-Verbovetskyi [1, Problem\n1.3]. Let \\mathcal{L(H,K)} denote the set of all bounded linear operators\nbetween a pair of Hilbert spaces \\mathcal{H,K}, and let \\mathbb{D}^{n} and\n\\mathbb{T}^{n} denote the open unit polydisk, and the unit n-torus,\nrespectively. \n\n"}
{"id": "1201.3784", "contents": "Title: Hodge Bundles on Smooth Compactifications of Siegel Varieties and\n  Applications Abstract: We study Hodge bundles on Siegel varieties and their various extensions to\nsmooth toroidal compactifications. Precisely, we construct a canonical Hodge\nbundle on an arbitrary Siegel variety so that the holomorphic tangent bundle\ncan be embedded into the Hodge bundle, and we observe that the Bergman metric\non the Siegel variety is compatible with the induced Hodge metric. Therefore we\nobtain the asymptotic estimate of the Bergman metric explicitly. Depending on\nthese properties and the uniformitarian of K\\\"ahler-Einstein manifold, we study\nextensions of the tangent bundle over any smooth toroidal compactification. We\nalso apply this result, together with Siegel cusp modular forms, to study\ngeneral type for Siegel varieties. \n\n"}
{"id": "1202.4850", "contents": "Title: Estimation in functional linear quantile regression Abstract: This paper studies estimation in functional linear quantile regression in\nwhich the dependent variable is scalar while the covariate is a function, and\nthe conditional quantile for each fixed quantile index is modeled as a linear\nfunctional of the covariate. Here we suppose that covariates are discretely\nobserved and sampling points may differ across subjects, where the number of\nmeasurements per subject increases as the sample size. Also, we allow the\nquantile index to vary over a given subset of the open unit interval, so the\nslope function is a function of two variables: (typically) time and quantile\nindex. Likewise, the conditional quantile function is a function of the\nquantile index and the covariate. We consider an estimator for the slope\nfunction based on the principal component basis. An estimator for the\nconditional quantile function is obtained by a plug-in method. Since the\nso-constructed plug-in estimator not necessarily satisfies the monotonicity\nconstraint with respect to the quantile index, we also consider a class of\nmonotonized estimators for the conditional quantile function. We establish\nrates of convergence for these estimators under suitable norms, showing that\nthese rates are optimal in a minimax sense under some smoothness assumptions on\nthe covariance kernel of the covariate and the slope function. Empirical choice\nof the cutoff level is studied by using simulations. \n\n"}
{"id": "1203.2507", "contents": "Title: Deviation optimal learning using greedy Q-aggregation Abstract: Given a finite family of functions, the goal of model selection aggregation\nis to construct a procedure that mimics the function from this family that is\nthe closest to an unknown regression function. More precisely, we consider a\ngeneral regression model with fixed design and measure the distance between\nfunctions by the mean squared error at the design points. While procedures\nbased on exponential weights are known to solve the problem of model selection\naggregation in expectation, they are, surprisingly, sub-optimal in deviation.\nWe propose a new formulation called Q-aggregation that addresses this\nlimitation; namely, its solution leads to sharp oracle inequalities that are\noptimal in a minimax sense. Moreover, based on the new formulation, we design\ngreedy Q-aggregation procedures that produce sparse aggregation models\nachieving the optimal rate. The convergence and performance of these greedy\nprocedures are illustrated and compared with other standard methods on\nsimulated examples. \n\n"}
{"id": "1203.3829", "contents": "Title: Analytic Continuation of Holomorphic Mappings From Non-minimal\n  Hypersurfaces Abstract: We study the analytic continuation problem for a germ of a biholomorphic\nmapping from a non-minimal real hypersurface $M\\subset\\CC{n}$ into a real\nhyperquadric $\\mathcal Q\\subset\\CP{n}$ and prove that under certain\nnon-degeneracy conditions any such germ extends locally biholomorphically along\nany path lying in the complement $U\\setminus X$ of the complex hypersurface $X$\ncontained in $M$ for an appropriate neighborhood $U\\supset X$. Using the\nmonodromy representation for the multiple-valued mapping obtained by the\nanalytic continuation we establish a connection between nonminimal real\nhypersurfaces and singular complex ODEs. \n\n"}
{"id": "1203.3995", "contents": "Title: Liouville and Calabi-Yau type theorems for complex Hessian equations Abstract: We prove a Liouville type theorem for entire maximal $m$-subharmonic\nfunctions in $\\mathbb C^n$ with bounded gradient. This result, coupled with a\nstandard blow-up argument, yields a (non-explicit) a priori gradient estimate\nfor the complex Hessian equation on a compact K\\\"ahler manifold. This\nterminates the program, initiated by Hou, Ma and Wu, of solving the\nnon-degenerate Hessian equation on such manifolds in full generality. We also\nobtain, using our previous work, continuous weak solutions in the degenerate\ncase for the right hand side in some $L^p, $ with sharp bound on $p$. \n\n"}
{"id": "1203.4301", "contents": "Title: Conformal Fractals for Normal Subgroups of Free Groups Abstract: We investigate subsets of a multifractal decomposition of the limit set of a\nconformal graph directed Markov system, which is constructed from the Cayley\ngraph of a free group with at least two generators. The subsets we consider are\nparametrised by a normal subgroup $N$ of the free group and mimic the radial\nlimit set of a Kleinian group. Our main results show that, regarding the\nHausdorff dimension of these sets, various results for Kleinian groups can be\ngeneralised. Namely, under certain natural symmetry assumptions on the\nmultifractal decomposition, we prove that, for a subset parametrised by $N$,\nthe Hausdorff dimension is maximal if and only if $\\F_{d}/N$ is amenable and\nthat the dimension is greater than half of the maximal value. We also give a\ncriterion for amenability via the divergence of the Poincar\\'{e} series of $N$.\nOur results are applied to the Lyapunov spectrum for normal subgroups of\nKleinian groups of Schottky type. \n\n"}
{"id": "1203.6038", "contents": "Title: Branched projective structures with quasi-Fuchsian holonomy Abstract: We prove that if S is a closed compact surface of negative Euler\ncharacteristic, and if R is a quasi-Fuchsian representation in PSL(2,C), then\nthe deformation space M(k,R) of branched projective structures on S with total\nbranching order k and holonomy R is connected, as soon as k>0. Equivalently,\ntwo branched projective structures with the same quasi-Fuchsian holonomy and\nthe same number of branch points are related by a movement of branch points. In\nparticular grafting annuli are obtained by moving branch points. In the\nappendix we give an explicit atlas for the space M(k,R). It is shown to be a\nsmooth complex manifold modeled on Hurwitz spaces. \n\n"}
{"id": "1204.1685", "contents": "Title: Density-sensitive semisupervised inference Abstract: Semisupervised methods are techniques for using labeled data\n$(X_1,Y_1),\\ldots,(X_n,Y_n)$ together with unlabeled data $X_{n+1},\\ldots,X_N$\nto make predictions. These methods invoke some assumptions that link the\nmarginal distribution $P_X$ of X to the regression function f(x). For example,\nit is common to assume that f is very smooth over high density regions of\n$P_X$. Many of the methods are ad-hoc and have been shown to work in specific\nexamples but are lacking a theoretical foundation. We provide a minimax\nframework for analyzing semisupervised methods. In particular, we study methods\nbased on metrics that are sensitive to the distribution $P_X$. Our model\nincludes a parameter $\\alpha$ that controls the strength of the semisupervised\nassumption. We then use the data to adapt to $\\alpha$. \n\n"}
{"id": "1204.3005", "contents": "Title: Collaboration and Coordination in Secondary Networks for Opportunistic\n  Spectrum Access Abstract: In this paper, we address the general case of a coordinated secondary network\nwilling to exploit communication opportunities left vacant by a licensed\nprimary network. Since secondary users (SU) usually have no prior knowledge on\nthe environment, they need to learn the availability of each channel through\nsensing techniques, which however can be prone to detection errors. We argue\nthat cooperation among secondary users can enable efficient learning and\ncoordination mechanisms in order to maximize the spectrum exploitation by SUs,\nwhile minimizing the impact on the primary network. To this goal, we provide\nthree novel contributions in this paper. First, we formulate the spectrum\nselection in secondary networks as an instance of the Multi-Armed Bandit (MAB)\nproblem, and we extend the analysis to the collaboration learning case, in\nwhich each SU learns the spectrum occupation, and shares this information with\nother SUs. We show that collaboration among SUs can mitigate the impact of\nsensing errors on system performance, and improve the convergence of the\nlearning process to the optimal solution. Second, we integrate the learning\nalgorithms with two collaboration techniques based on modified versions of the\nHungarian algorithm and of the Round Robin algorithm that allows reducing the\ninterference among SUs. Third, we derive fundamental limits to the performance\nof cooperative learning algorithms based on Upper Confidence Bound (UCB)\npolicies in a symmetric scenario where all SU have the same perception of the\nquality of the resources. Extensive simulation results confirm the\neffectiveness of our joint learning-collaboration algorithm in protecting the\noperations of Primary Users (PUs), while maximizing the performance of SUs. \n\n"}
{"id": "1204.3573", "contents": "Title: Learning Sets with Separating Kernels Abstract: We consider the problem of learning a set from random samples. We show how\nrelevant geometric and topological properties of a set can be studied\nanalytically using concepts from the theory of reproducing kernel Hilbert\nspaces. A new kind of reproducing kernel, that we call separating kernel, plays\na crucial role in our study and is analyzed in detail. We prove a new analytic\ncharacterization of the support of a distribution, that naturally leads to a\nfamily of provably consistent regularized learning algorithms and we discuss\nthe stability of these methods with respect to random sampling. Numerical\nexperiments show that the approach is competitive, and often better, than other\nstate of the art techniques. \n\n"}
{"id": "1204.6379", "contents": "Title: Discrete Quantum Control - State Preparation Abstract: A discrete-time method for solving problems in optimal quantum control is\npresented. Controlling the time discretized markovian dynamics of a quantum\nsystem can be reduced to a Markov-decision process. We demonstrate this method\nin this with a class of simple one qubit systems, which are also discretized in\nspace. For the task of state preparation we solve the examples both numerically\nand analytically with dynamic programming techniques. \n\n"}
{"id": "1205.2300", "contents": "Title: Quantum Tomography via Compressed Sensing: Error Bounds, Sample\n  Complexity, and Efficient Estimators Abstract: Intuitively, if a density operator has small rank, then it should be easier\nto estimate from experimental data, since in this case only a few eigenvectors\nneed to be learned. We prove two complementary results that confirm this\nintuition. First, we show that a low-rank density matrix can be estimated using\nfewer copies of the state, i.e., the sample complexity of tomography decreases\nwith the rank. Second, we show that unknown low-rank states can be\nreconstructed from an incomplete set of measurements, using techniques from\ncompressed sensing and matrix completion. These techniques use simple Pauli\nmeasurements, and their output can be certified without making any assumptions\nabout the unknown state.\n  We give a new theoretical analysis of compressed tomography, based on the\nrestricted isometry property (RIP) for low-rank matrices. Using these tools, we\nobtain near-optimal error bounds, for the realistic situation where the data\ncontains noise due to finite statistics, and the density matrix is full-rank\nwith decaying eigenvalues. We also obtain upper-bounds on the sample complexity\nof compressed tomography, and almost-matching lower bounds on the sample\ncomplexity of any procedure using adaptive sequences of Pauli measurements.\n  Using numerical simulations, we compare the performance of two compressed\nsensing estimators with standard maximum-likelihood estimation (MLE). We find\nthat, given comparable experimental resources, the compressed sensing\nestimators consistently produce higher-fidelity state reconstructions than MLE.\nIn addition, the use of an incomplete set of measurements leads to faster\nclassical processing with no loss of accuracy.\n  Finally, we show how to certify the accuracy of a low rank estimate using\ndirect fidelity estimation and we describe a method for compressed quantum\nprocess tomography that works for processes with small Kraus rank. \n\n"}
{"id": "1205.3439", "contents": "Title: Continued Fractions and the Rabi Model Abstract: Techniques based on continued fractions to compute numerically the spectrum\nof the quantum Rabi model are reviewed. They are of two essentially different\ntypes. In the first case, the spectral condition is implemented using a\nrepresentation in the infinite-dimensional Bargmann space of analytic\nfunctions. This approach is shown to approximate the correct spectrum of the\nfull model if the continued fraction is truncated at sufficiently high order.\nIn the second case, one considers the limit of a sequence of models defined in\nfinite-dimensional state spaces. Contrary to the first, the second approach is\nambiguous and can be justified only through recourse to the analyticity\nargument from the first method. \n\n"}
{"id": "1205.3511", "contents": "Title: Second Josephson excitations beyond mean field as a toy model for\n  thermal pressure: exact quantum dynamics and the quantum phase model Abstract: A simple four-mode Bose-Hubbard model with intrinsic time scale separation\ncan be considered as a paradigm for mesoscopic quantum systems in thermal\ncontact. In our previous work we showed that in addition to coherent particle\nexchange, a novel slow collective excitation can be identified by a series of\nHolstein-Primakoff transformations. This resonant energy exchange mode is not\npredicted by linear Bogoliubov theory, and its frequency is sensitive to\ninteractions among Bogoliubov quasi-particles; it may be referred to as a\nsecond Josephson oscillation, in analogy to the second sound mode of liquid\nHelium II. In this paper we will explore this system beyond the\nGross-Pitaevskii mean field regime. We directly compare the classical mean\nfield dynamics to the exact full quantum many-particle dynamics and show good\nagreement over a large range of the system parameters. The second Josephson\nfrequency becomes imaginary for stronger interactions, however, indicating\ndynamical instability of the symmetric state. By means of a generalized quantum\nphase model for the full four-mode system, we then show that, in this regime,\nhigh-energy Bogoliubov quasiparticles tend to accumulate in one pair of sites,\nwhile the actual particles preferentially occupy the opposite pair. We\ninterpret this as a simple model for thermal pressure. \n\n"}
{"id": "1205.4074", "contents": "Title: Complex polynomial vector fields with many algebraic orbits Abstract: We state some generalizations of a theorem due to G. Darboux, which\noriginally states that a polynomial vector field in the complex plane exhibits\na rational first integral and has all its orbits algebraic provided that it\nexhibits infinitely many algebraic orbits. In this paper, we give an\ninterpretation of this result in terms of the classical Reeb stability\ntheorems, for compact leaves of (non-singular) smooth foliations. Then we give\nversions of Darboux's theorem, assuring, for a (non-singular) holomorphic\nfoliation of any codimension, the existence of an open set of compact leaves\nprovided that the measure of the set of compact leaves is not zero. As for the\ncase of polynomial vector fields in the complex affine space of dimenion\n$m\\geq2$, we prove suitable versions of the above results, based also on the\nvery special geometry of the complex projective space of dimension $m$, and on\nthe nature of the singularities of such vector fields we consider. \n\n"}
{"id": "1205.4524", "contents": "Title: Homodyne detection of matter-wave fields (shortened) Abstract: A scheme is discussed that allows one for performing homodyne detection of\nthe matter-wave field of ultracold bosonic atoms. It is based on a pump-probe\nlasers setup, that both illuminates a Bose-Einstein condensate, acting as\nreference system, and a second ultracold gas, composed by the same atoms but in\na quantum phase to determine. Photon scattering outcouples atoms from both\nsystems, which then propagate freely. Under appropriate conditions, when the\nsame photon can either be scattered by the Bose-Einstein condensate or by the\nother quantum gas, both flux of outcoupled atoms and scattered photons exhibit\noscillations, whose amplitude is proportional to the condensate fraction of the\nquantum gas. The setup can be extended to measure the first-order correlation\nfunction of a quantum gas. The dynamics here discussed make use of the\nentanglement between atoms and photons which is established by the scattering\nprocess in order to access detailed information on the quantum state of matter. \n\n"}
{"id": "1205.4770", "contents": "Title: Variance function estimation in high-dimensions Abstract: We consider the high-dimensional heteroscedastic regression model, where the\nmean and the log variance are modeled as a linear combination of input\nvariables. Existing literature on high-dimensional linear regres- sion models\nhas largely ignored non-constant error variances, even though they commonly\noccur in a variety of applications ranging from biostatis- tics to finance. In\nthis paper we study a class of non-convex penalized pseudolikelihood estimators\nfor both the mean and variance parameters. We show that the Heteroscedastic\nIterative Penalized Pseudolikelihood Optimizer (HIPPO) achieves the oracle\nproperty, that is, we prove that the rates of convergence are the same as if\nthe true model was known. We demonstrate numerical properties of the procedure\non a simulation study and real world data. \n\n"}
{"id": "1205.5012", "contents": "Title: Learning Mixed Graphical Models Abstract: We consider the problem of learning the structure of a pairwise graphical\nmodel over continuous and discrete variables. We present a new pairwise model\nfor graphical models with both continuous and discrete variables that is\namenable to structure learning. In previous work, authors have considered\nstructure learning of Gaussian graphical models and structure learning of\ndiscrete models. Our approach is a natural generalization of these two lines of\nwork to the mixed case. The penalization scheme involves a novel symmetric use\nof the group-lasso norm and follows naturally from a particular parametrization\nof the model. \n\n"}
{"id": "1205.5075", "contents": "Title: Efficient Sparse Group Feature Selection via Nonconvex Optimization Abstract: Sparse feature selection has been demonstrated to be effective in handling\nhigh-dimensional data. While promising, most of the existing works use convex\nmethods, which may be suboptimal in terms of the accuracy of feature selection\nand parameter estimation. In this paper, we expand a nonconvex paradigm to\nsparse group feature selection, which is motivated by applications that require\nidentifying the underlying group structure and performing feature selection\nsimultaneously. The main contributions of this article are twofold: (1)\nstatistically, we introduce a nonconvex sparse group feature selection model\nwhich can reconstruct the oracle estimator. Therefore, consistent feature\nselection and parameter estimation can be achieved; (2) computationally, we\npropose an efficient algorithm that is applicable to large-scale problems.\nNumerical results suggest that the proposed nonconvex method compares favorably\nagainst its competitors on synthetic data and real-world applications, thus\nachieving desired goal of delivering high performance. \n\n"}
{"id": "1206.2380", "contents": "Title: The Highest Dimensional Stochastic Blockmodel with a Regularized\n  Estimator Abstract: In the high dimensional Stochastic Blockmodel for a random network, the\nnumber of clusters (or blocks) K grows with the number of nodes N. Two previous\nstudies have examined the statistical estimation performance of spectral\nclustering and the maximum likelihood estimator under the high dimensional\nmodel; neither of these results allow K to grow faster than N^{1/2}. We study a\nmodel where, ignoring log terms, K can grow proportionally to N. Since the\nnumber of clusters must be smaller than the number of nodes, no reasonable\nmodel allows K to grow faster; thus, our asymptotic results are the \"highest\"\ndimensional. To push the asymptotic setting to this extreme, we make additional\nassumptions that are motivated by empirical observations in physical\nanthropology (Dunbar, 1992), and an in depth study of massive empirical\nnetworks (Leskovec et al 2008). Furthermore, we develop a regularized maximum\nlikelihood estimator that leverages these insights and we prove that, under\ncertain conditions, the proportion of nodes that the regularized estimator\nmisclusters converges to zero. This is the first paper to explicitly introduce\nand demonstrate the advantages of statistical regularization in a parametric\nform for network analysis. \n\n"}
{"id": "1206.6702", "contents": "Title: Unsharp continuous measurement of a Bose-Einstein condensate: full\n  quantum state estimation and the transition to classicality Abstract: We study a Bose-Einstein condensate (BEC) in a double-well potential subject\nto an unsharp continuous measurement of the atom number in one of the two\nwells. We investigate the back action of the measurement on the quantum\ndynamics and the viability to monitor the ensuing time evolution. For vanishing\ninter-atomic interactions, mainly the expectation values of the measured local\nobservable can be inferred from the measurement record. Conversely, in the\npresence of moderate inter-atomic interactions, the entire many-body state\n--modified by the measurement-- is monitored with unit fidelity and, at the\nsame time, the measurement effects a transition from quantum to mean-field\n(classical) behavior of the BEC. We show that this perfect state estimation is\npossible because the inter-atomic interactions enhance the information gained\nvia the measurement. \n\n"}
{"id": "1207.0789", "contents": "Title: Bifurcation currents in holomorphic families of rational maps Abstract: The aim of these lectures is the study of bifurcations within holomorphic\nfamilies of polynomials or rational maps by mean of ergodic and pluripotential\ntheoretic tools. \n\n"}
{"id": "1207.2011", "contents": "Title: Estimates in the Hardy-Sobolev space of the annulus and stability result Abstract: The main purpose of this work is to establish some logarithmic estimates of\noptimal type in the Hardy-Sobolev space $H^{k, \\infty}; k \\in {\\mathbb{N}}^*$\nof an annular domain. These results are considered as a continuation of a\nprevious study in the setting of the unit disk by L. Baratchart and M. Zerner:\nOn the recovery of functions from pointwise boundary values in a Hardy-sobolev\nclass of the disk. J.Comput.Apll.Math 46(1993), 255-69 and by S. Chaabane and\nI. Feki: Logarithmic stability estimates in Hardy-Sobolev spaces\n$H^{k,\\infty}$. C.R. Acad. Sci. Paris, Ser. I 347(2009), 1001-1006.\n  As an application, we prove a logarithmic stability result for the inverse\nproblem of identifying a Robin parameter on a part of the boundary of an\nannular domain starting from its behavior on the complementary boundary part. \n\n"}
{"id": "1207.3772", "contents": "Title: Surrogate Losses in Passive and Active Learning Abstract: Active learning is a type of sequential design for supervised machine\nlearning, in which the learning algorithm sequentially requests the labels of\nselected instances from a large pool of unlabeled data points. The objective is\nto produce a classifier of relatively low risk, as measured under the 0-1 loss,\nideally using fewer label requests than the number of random labeled data\npoints sufficient to achieve the same. This work investigates the potential\nuses of surrogate loss functions in the context of active learning.\nSpecifically, it presents an active learning algorithm based on an arbitrary\nclassification-calibrated surrogate loss function, along with an analysis of\nthe number of label requests sufficient for the classifier returned by the\nalgorithm to achieve a given risk under the 0-1 loss. Interestingly, these\nresults cannot be obtained by simply optimizing the surrogate risk via active\nlearning to an extent sufficient to provide a guarantee on the 0-1 loss, as is\ncommon practice in the analysis of surrogate losses for passive learning. Some\nof the results have additional implications for the use of surrogate losses in\npassive learning. \n\n"}
{"id": "1207.6505", "contents": "Title: Rotational properties of non-dipolar and dipolar Bose-Einstein\n  condensates confined in annular potentials Abstract: We investigate the rotational response of both non-dipolar and dipolar\nBose-Einstein condensates confined in an annular potential. For the non-dipolar\ncase we identify certain critical rotational frequencies associated with the\nformation of vortices. For the dipolar case, assuming that the dipoles are\naligned along some arbitrary and tunable direction, we study the same problem\nas a function of the orientation angle of the dipole moment of the atoms. \n\n"}
{"id": "1208.1687", "contents": "Title: Convergence and compactness of the Sobolev mappings Abstract: First of all, we establish compactness of continuous mappings of the\nOrlicz--Sobolev classes $W^{1,\\varphi}_{\\rm loc}$ with the Calderon type\ncondition on $\\varphi$ and, in particular, of the Sobolev classes $W^{1,p}_{\\rm\nloc}$ for $p>n-1$ in ${\\Bbb R}^n\\,,$ $n\\ge 3\\,,$ with one fixed point. Then we\ngive a series of theorems on convergence of the Orlicz--Sobolev homeomorphisms\nand on semicontinuity in the mean of dilatations of the Sobolev homeomorphisms.\nThese results lead us to closeness of the corresponding classes of\nhomeomorpisms. Finally, we come on this basis to criteria of compactness of\nclasses of Sobolev's homeomorphisms with the corresponding conditions on\ndilatations and two fixed points. \n\n"}
{"id": "1208.3692", "contents": "Title: Connectedness properties of the set where the iterates of an entire\n  function are bounded Abstract: We investigate some connectedness properties of the set of points K(f) where\nthe iterates of an entire function f are bounded. In particular, we describe a\nclass of transcendental entire functions for which an analogue of the\nBranner-Hubbard conjecture holds and show that, for such functions, if K(f) is\ndisconnected then it has uncountably many components. We give examples to show\nthat K(f) can be totally disconnected, and we use quasiconformal surgery to\nconstruct a function for which K(f) has a component with empty interior that is\nnot a singleton. \n\n"}
{"id": "1208.4491", "contents": "Title: Macrorealism from entropic Leggett-Garg inequalities Abstract: We formulate entropic Leggett-Garg inequalities, which place constraints on\nthe statistical outcomes of temporal correlations of observables. The\ninformation theoretic inequalities are satisfied if macrorealism holds. We show\nthat the quantum statistics underlying correlations between time-separated spin\ncomponent of a quantum rotor mimics that of spin correlations in two spatially\nseparated spin-$s$ particles sharing a state of zero total spin. This brings\nforth the violation of the entropic Leggett-Garg inequality by a rotating\nquantum spin-$s$ system in similar manner as does the entropic Bell inequality\n(Phys. Rev. Lett. 61, 662 (1988)) by a pair of spin-$s$ particles forming a\ncomposite spin singlet state. \n\n"}
{"id": "1208.5467", "contents": "Title: Censored quantile regression processes under dependence and penalization Abstract: We consider quantile regression processes from censored data under dependent\ndata structures and derive a uniform Bahadur representation for those\nprocesses. We also consider cases where the dimension of the parameter in the\nquantile regression model is large. It is demonstrated that traditional\npenalized estimators such as the adaptive lasso yield sub-optimal rates if the\ncoefficients of the quantile regression cross zero. New penalization techniques\nare introduced which are able to deal with specific problems of censored data\nand yield estimates with an optimal rate. In contrast to most of the\nliterature, the asymptotic analysis does not require the assumption of\nindependent observations, but is based on rather weak assumptions, which are\nsatisfied for many kinds of dependent data. \n\n"}
{"id": "1208.6535", "contents": "Title: Genuine Multiparty Quantum Entanglement Suppresses Multiport Classical\n  Information Transmission Abstract: We establish a universal complementarity relation between the capacity of\nclassical information transmission by employing a multiparty quantum state as a\nmultiport quantum channel, and the genuine multipartite entanglement of the\nquantum state. The classical information transfer is from a sender to several\nreceivers by using the quantum dense coding protocol with the multiparty\nquantum state shared between the sender and the receivers. The relation holds\nfor arbitrary pure or mixed quantum states of an arbitrary number of parties in\narbitrary dimensions. \n\n"}
{"id": "1209.1066", "contents": "Title: The degeneration of the boundary of the Milnor fibre to the link of\n  complex and real non-isolated singularities Abstract: We study the boundary of the Milnor fibre of real analytic singularities $f:\n(\\bR^m,0) \\to (\\bR^k,0)$, $m\\geq k$, with an isolated critical value and the\nThom $a_f$-property. We define the vanishing zone for $f$ and we give necessary\nand sufficient conditions for it to be a fibre bundle over the link of the\nsingular set of $f^{-1}(0)$. In the case of singularities of the type $\\fgbar:\n(\\bC^n,0) \\to (\\bC,0)$ with an isoalted critical value, $f, g$ holomorphic, we\nfurther describe the degeneration of the boundary of the Milnor fibre to the\nlink of $\\fgbar$. As a milestone, we also construct a L\\^e's polyhedron for\nreal analytic singularities of the type $\\fgbar: (\\bC^2,0) \\to (\\bC,0)$ such\nthat either $f$ or $g$ depends only on one variable. \n\n"}
{"id": "1209.1119", "contents": "Title: Augment-and-Conquer Negative Binomial Processes Abstract: By developing data augmentation methods unique to the negative binomial (NB)\ndistribution, we unite seemingly disjoint count and mixture models under the NB\nprocess framework. We develop fundamental properties of the models and derive\nefficient Gibbs sampling inference. We show that the gamma-NB process can be\nreduced to the hierarchical Dirichlet process with normalization, highlighting\nits unique theoretical, structural and computational advantages. A variety of\nNB processes with distinct sharing mechanisms are constructed and applied to\ntopic modeling, with connections to existing algorithms, showing the importance\nof inferring both the NB dispersion and probability parameters. \n\n"}
{"id": "1209.1126", "contents": "Title: Measuring topology in a laser-coupled honeycomb lattice: From Chern\n  insulators to topological semi-metals Abstract: Ultracold fermions trapped in a honeycomb optical lattice constitute a\nversatile setup to experimentally realize the Haldane model [Phys. Rev. Lett.\n61, 2015 (1988)]. In this system, a non-uniform synthetic magnetic flux can be\nengineered through laser-induced methods, explicitly breaking time-reversal\nsymmetry. This potentially opens a bulk gap in the energy spectrum, which is\nassociated with a non-trivial topological order, i.e., a non-zero Chern number.\nIn this work, we consider the possibility of producing and identifying such a\nrobust Chern insulator in the laser-coupled honeycomb lattice. We explore a\nlarge parameter space spanned by experimentally controllable parameters and\nobtain a variety of phase diagrams, clearly identifying the accessible\ntopologically non-trivial regimes. We discuss the signatures of Chern\ninsulators in cold-atom systems, considering available detection methods. We\nalso highlight the existence of topological semi-metals in this system, which\nare gapless phases characterized by non-zero winding numbers, not present in\nHaldane's original model. \n\n"}
{"id": "1209.1688", "contents": "Title: Rank Centrality: Ranking from Pair-wise Comparisons Abstract: The question of aggregating pair-wise comparisons to obtain a global ranking\nover a collection of objects has been of interest for a very long time: be it\nranking of online gamers (e.g. MSR's TrueSkill system) and chess players,\naggregating social opinions, or deciding which product to sell based on\ntransactions. In most settings, in addition to obtaining a ranking, finding\n`scores' for each object (e.g. player's rating) is of interest for\nunderstanding the intensity of the preferences.\n  In this paper, we propose Rank Centrality, an iterative rank aggregation\nalgorithm for discovering scores for objects (or items) from pair-wise\ncomparisons. The algorithm has a natural random walk interpretation over the\ngraph of objects with an edge present between a pair of objects if they are\ncompared; the score, which we call Rank Centrality, of an object turns out to\nbe its stationary probability under this random walk. To study the efficacy of\nthe algorithm, we consider the popular Bradley-Terry-Luce (BTL) model\n(equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in which\neach object has an associated score which determines the probabilistic outcomes\nof pair-wise comparisons between objects. In terms of the pair-wise marginal\nprobabilities, which is the main subject of this paper, the MNL model and the\nBTL model are identical. We bound the finite sample error rates between the\nscores assumed by the BTL model and those estimated by our algorithm. In\nparticular, the number of samples required to learn the score well with high\nprobability depends on the structure of the comparison graph. When the\nLaplacian of the comparison graph has a strictly positive spectral gap, e.g.\neach item is compared to a subset of randomly chosen items, this leads to\ndependence on the number of samples that is nearly order-optimal. \n\n"}
{"id": "1209.2437", "contents": "Title: TIGER: A Tuning-Insensitive Approach for Optimally Estimating Gaussian\n  Graphical Models Abstract: We propose a new procedure for estimating high dimensional Gaussian graphical\nmodels. Our approach is asymptotically tuning-free and non-asymptotically\ntuning-insensitive: it requires very few efforts to choose the tuning parameter\nin finite sample settings. Computationally, our procedure is significantly\nfaster than existing methods due to its tuning-insensitive property.\nTheoretically, the obtained estimator is simultaneously minimax optimal for\nprecision matrix estimation under different norms. Empirically, we illustrate\nthe advantages of our method using thorough simulated and real examples. The R\npackage bigmatrix implementing the proposed methods is available on the\nComprehensive R Archive Network: http://cran.r-project.org/. \n\n"}
{"id": "1210.7069", "contents": "Title: Kotani-Last problem and Hardy spaces on surfaces of Widom type Abstract: It is a small theory of non almost periodic ergodic families of Jacobi\nmatrices with pure (however) absolutely continuous spectrum. And the reason why\nthis effect may happen: under our \"axioms\" we found an analytic condition on\nthe resolvent set that is responsible for (exactly equivalent to) this effect. \n\n"}
{"id": "1210.7369", "contents": "Title: On h-principle and specialness for complex projective manifolds Abstract: We show that a complex projective manifold X which satisfies the Gromov's\nh-principle is `special', and raise some questions about the reverse\nimplication, the extension to the quasi-K\\\" ahler case, and the relationships\nof these properties to the `Oka' property. The guiding principle is that the\nexistence of many Stein manifolds which have degenerate Kobayashi pseudometric\ngives strong obstructions to the complex hyperbolicity of X satisfying the\nh-principle. \n\n"}
{"id": "1210.7426", "contents": "Title: Asymptotic conformal welding via Loewner-Kufarev evolution Abstract: The Loewner-Kufarev evolution produces asymptotics for mappings onto domains\nclose to the unit disk or the exterior of the unit disk. We deduce variational\nformulae which lead to the asymptotic conformal welding for such domains. The\ncomparison of mappings onto bounded and unbounded components of the Jordan\ncurve establishes an asymptotic connection between driving functions in both\nversions of the Loewner-Kufarev equation and conformal radii of the two\ndomains. \n\n"}
{"id": "1211.0203", "contents": "Title: Remarks about bubbles Abstract: We make some remarks about bubbling on, not necessarily proper, champs de\nDeligne-Mumford, i.e. compactification of the space of mappings from a given\n(wholly scheme like) curve, so, in particular, on quasi-projective projective\nvarieties. Under hypothesis on both the interior and the boundary such as\nRemark \\ref{rmk:interior} below, this implies an optimal logarithmic variant of\nMori's Bend-and-Break. The main technical remark is \\ref{thm:logMM}, while our\nfinal remark, the cone theorem, \\ref{rmk:cone}, is a variant. \n\n"}
{"id": "1211.0817", "contents": "Title: Discussion: Latent variable graphical model selection via convex\n  optimization Abstract: Discussion of \"Latent variable graphical model selection via convex\noptimization\" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky\n[arXiv:1008.1290]. \n\n"}
{"id": "1211.1043", "contents": "Title: Soft (Gaussian CDE) regression models and loss functions Abstract: Regression, unlike classification, has lacked a comprehensive and effective\napproach to deal with cost-sensitive problems by the reuse (and not a\nre-training) of general regression models. In this paper, a wide variety of\ncost-sensitive problems in regression (such as bids, asymmetric losses and\nrejection rules) can be solved effectively by a lightweight but powerful\napproach, consisting of: (1) the conversion of any traditional one-parameter\ncrisp regression model into a two-parameter soft regression model, seen as a\nnormal conditional density estimator, by the use of newly-introduced enrichment\nmethods; and (2) the reframing of an enriched soft regression model to new\ncontexts by an instance-dependent optimisation of the expected loss derived\nfrom the conditional normal distribution. \n\n"}
{"id": "1211.1547", "contents": "Title: A note on p-values interpreted as plausibilities Abstract: P-values are a mainstay in statistics but are often misinterpreted. We\npropose a new interpretation of p-value as a meaningful plausibility, where\nthis is to be interpreted formally within the inferential model framework. We\nshow that, for most practical hypothesis testing problems, there exists an\ninferential model such that the corresponding plausibility function, evaluated\nat the null hypothesis, is exactly the p-value. The advantages of this\nrepresentation are that the notion of plausibility is consistent with the way\npractitioners use and interpret p-values, and the plausibility calculation\navoids the troublesome conditioning on the truthfulness of the null. This\nconnection with plausibilities also reveals a shortcoming of standard p-values\nin problems with non-trivial parameter constraints. \n\n"}
{"id": "1211.1642", "contents": "Title: Randomized Dimension Reduction on Massive Data Abstract: Scalability of statistical estimators is of increasing importance in modern\napplications and dimension reduction is often used to extract relevant\ninformation from data. A variety of popular dimension reduction approaches can\nbe framed as symmetric generalized eigendecomposition problems. In this paper\nwe outline how taking into account the low rank structure assumption implicit\nin these dimension reduction approaches provides both computational and\nstatistical advantages. We adapt recent randomized low-rank approximation\nalgorithms to provide efficient solutions to three dimension reduction methods:\nPrincipal Component Analysis (PCA), Sliced Inverse Regression (SIR), and\nLocalized Sliced Inverse Regression (LSIR). A key observation in this paper is\nthat randomization serves a dual role, improving both computational and\nstatistical performance. This point is highlighted in our experiments on real\nand simulated data. \n\n"}
{"id": "1211.2884", "contents": "Title: A generalized Stoilow decomposition for pairs of mappings of integrable\n  dilatation Abstract: We prove a rigidity result for pairs of mappings of integrable dilatation\nwhose gradients pointwise deform the unit ball to similar ellipses. Our result\nimplies as corollaries a version of the generalized Stoilow decomposition\nprovided by Theorem 5.5.1 of a recent monograph of Astala-Iwaniec-Martin and\nthe two dimensional rigidity result of our previous paper for mappings whose\nsymmetric part of gradient agrees.\n  Specifically let $u,v\\in W^{1,2}(\\Omega,\\mathbb{R}^2)$ where $\\det(Du)>0$,\n$\\det(Dv)>0$ a.e. and $u$ is a mapping of integrable dilatation. Suppose for\na.e. $z\\in \\Omega$ we have $Du(z)^T Du(z)=\\lambda Dv(z)^T Dv(z)$ for some\n$\\lambda>0$. Then there exists a meromorphic function $\\psi$ and a\nhomeomorphism $w\\in W^{1,1}(\\Omega:\\mathbb{R}^2)$ such that\n$Du(z)=\\mathcal{P}(\\psi(w(z)))Dv(z)$ where\n$\\mathcal{P}(a+ib)=(\\begin{smallmatrix} a & -b \\\\ b & a \\end{smallmatrix})$.\n  We show by example that this result is sharp in the sense that there can be\nno continuous relation between the gradients of $Du$ and $Dv$ on a dense open\nconnected subset of $\\Omega$ unless one of the mappings is of integrable\ndilatation. \n\n"}
{"id": "1212.2423", "contents": "Title: Spatial correlations of Rydberg excitations in optically driven atomic\n  ensembles Abstract: We study the emergence of many-body correlations in the stationary state of\ncontinuously-driven, strongly-interacting dissipative system. Specifically, we\nexamine resonant optical excitations of Rydberg states of atoms interacting via\nlong-range dipole-dipole and van der Waals potentials employing exact numerical\nsolutions of the density matrix equations and Monte-Carlo simulations.\nCollection of atoms within a blockade distance form a \"superatom\" that can\naccommodate at most one Rydberg excitation. The superatom excitation\nprobability saturates to 1/2 for coherently driven atoms, but is significantly\nhigher for incoherent driving, approaching unity as the number of atoms\nincreases. In the steady state of uniformly-driven, extended one-dimensional\nsystem, the saturation of superatoms leads to quasi-crystallization of Rydberg\nexcitations whose correlations exhibit damped spatial oscillations. The\nbehavior of the system under the van der Waals interaction potential can be\napproximated by an analytically soluble model based on a \"hard-rod\" interatomic\npotential. \n\n"}
{"id": "1212.5156", "contents": "Title: Nonparametric ridge estimation Abstract: We study the problem of estimating the ridges of a density function. Ridge\nestimation is an extension of mode finding and is useful for understanding the\nstructure of a density. It can also be used to find hidden structure in point\ncloud data. We show that, under mild regularity conditions, the ridges of the\nkernel density estimator consistently estimate the ridges of the true density.\nWhen the data are noisy measurements of a manifold, we show that the ridges are\nclose and topologically similar to the hidden manifold. To find the estimated\nridges in practice, we adapt the modified mean-shift algorithm proposed by\nOzertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numerical\nexperiments verify that the algorithm is accurate. \n\n"}
{"id": "1301.0771", "contents": "Title: Quantum interference in attosecond transient absorption of laser-dressed\n  helium atoms Abstract: We calculate the transient absorption of an isolated attosecond pulse by\nhelium atoms subject to a delayed infrared (\\ir) laser pulse. With the central\nfrequency of the broad attosecond spectrum near the ionization threshold, the\nabsorption spectrum is strongly modulated at the sub-\\ir-cycle level. Given\nthat the absorption spectrum results from a time-integrated measurement, we\ninvestigate the extent to which the delay-dependence of the absorption yields\ninformation about the attosecond dynamics of the atom-field energy exchange. We\nfind two configurations in which this is possible. The first involves multi\nphoton transitions between bound states that result in interference between\ndifferent excitation pathways. The other involves the modification of the bound\nstate absorption lines by the IR field, which we find can result in a sub-cycle\ntime dependence only when ionization limits the duration of the strong field\ninteraction. \n\n"}
{"id": "1301.3541", "contents": "Title: Deep Predictive Coding Networks Abstract: The quality of data representation in deep learning methods is directly\nrelated to the prior model imposed on the representations; however, generally\nused fixed priors are not capable of adjusting to the context in the data. To\naddress this issue, we propose deep predictive coding networks, a hierarchical\ngenerative model that empirically alters priors on the latent representations\nin a dynamic and context-sensitive manner. This model captures the temporal\ndependencies in time-varying signals and uses top-down information to modulate\nthe representation in lower layers. The centerpiece of our model is a novel\nprocedure to infer sparse states of a dynamic model which is used for feature\nextraction. We also extend this feature extraction block to introduce a pooling\nfunction that captures locally invariant representations. When applied on a\nnatural video data, we show that our method is able to learn high-level visual\nfeatures. We also demonstrate the role of the top-down connections by showing\nthe robustness of the proposed model to structured noise. \n\n"}
{"id": "1301.4168", "contents": "Title: Herded Gibbs Sampling Abstract: The Gibbs sampler is one of the most popular algorithms for inference in\nstatistical models. In this paper, we introduce a herding variant of this\nalgorithm, called herded Gibbs, that is entirely deterministic. We prove that\nherded Gibbs has an $O(1/T)$ convergence rate for models with independent\nvariables and for fully connected probabilistic graphical models. Herded Gibbs\nis shown to outperform Gibbs in the tasks of image denoising with MRFs and\nnamed entity recognition with CRFs. However, the convergence for herded Gibbs\nfor sparsely connected probabilistic graphical models is still an open problem. \n\n"}
{"id": "1301.4183", "contents": "Title: On Graphical Models via Univariate Exponential Family Distributions Abstract: Undirected graphical models, or Markov networks, are a popular class of\nstatistical models, used in a wide variety of applications. Popular instances\nof this class include Gaussian graphical models and Ising models. In many\nsettings, however, it might not be clear which subclass of graphical models to\nuse, particularly for non-Gaussian and non-categorical data. In this paper, we\nconsider a general sub-class of graphical models where the node-wise\nconditional distributions arise from exponential families. This allows us to\nderive multivariate graphical model distributions from univariate exponential\nfamily distributions, such as the Poisson, negative binomial, and exponential\ndistributions. Our key contributions include a class of M-estimators to fit\nthese graphical model distributions; and rigorous statistical analysis showing\nthat these M-estimators recover the true graphical model structure exactly,\nwith high probability. We provide examples of genomic and proteomic networks\nlearned via instances of our class of graphical models derived from Poisson and\nexponential distributions. \n\n"}
{"id": "1302.1611", "contents": "Title: Bounded regret in stochastic multi-armed bandits Abstract: We study the stochastic multi-armed bandit problem when one knows the value\n$\\mu^{(\\star)}$ of an optimal arm, as a well as a positive lower bound on the\nsmallest positive gap $\\Delta$. We propose a new randomized policy that attains\na regret {\\em uniformly bounded over time} in this setting. We also prove\nseveral lower bounds, which show in particular that bounded regret is not\npossible if one only knows $\\Delta$, and bounded regret of order $1/\\Delta$ is\nnot possible if one only knows $\\mu^{(\\star)}$ \n\n"}
{"id": "1302.4141", "contents": "Title: Canonical dual solutions to nonconvex radial basis neural network\n  optimization problem Abstract: Radial Basis Functions Neural Networks (RBFNNs) are tools widely used in\nregression problems. One of their principal drawbacks is that the formulation\ncorresponding to the training with the supervision of both the centers and the\nweights is a highly non-convex optimization problem, which leads to some\nfundamentally difficulties for traditional optimization theory and methods.\n  This paper presents a generalized canonical duality theory for solving this\nchallenging problem. We demonstrate that by sequential canonical dual\ntransformations, the nonconvex optimization problem of the RBFNN can be\nreformulated as a canonical dual problem (without duality gap). Both global\noptimal solution and local extrema can be classified. Several applications to\none of the most used Radial Basis Functions, the Gaussian function, are\nillustrated. Our results show that even for one-dimensional case, the global\nminimizer of the nonconvex problem may not be the best solution to the RBFNNs,\nand the canonical dual theory is a promising tool for solving general neural\nnetworks training problems. \n\n"}
{"id": "1303.5145", "contents": "Title: Node-Based Learning of Multiple Gaussian Graphical Models Abstract: We consider the problem of estimating high-dimensional Gaussian graphical\nmodels corresponding to a single set of variables under several distinct\nconditions. This problem is motivated by the task of recovering transcriptional\nregulatory networks on the basis of gene expression data {containing\nheterogeneous samples, such as different disease states, multiple species, or\ndifferent developmental stages}. We assume that most aspects of the conditional\ndependence networks are shared, but that there are some structured differences\nbetween them. Rather than assuming that similarities and differences between\nnetworks are driven by individual edges, we take a node-based approach, which\nin many cases provides a more intuitive interpretation of the network\ndifferences. We consider estimation under two distinct assumptions: (1)\ndifferences between the K networks are due to individual nodes that are\nperturbed across conditions, or (2) similarities among the K networks are due\nto the presence of common hub nodes that are shared across all K networks.\nUsing a row-column overlap norm penalty function, we formulate two convex\noptimization problems that correspond to these two assumptions. We solve these\nproblems using an alternating direction method of multipliers algorithm, and we\nderive a set of necessary and sufficient conditions that allows us to decompose\nthe problem into independent subproblems so that our algorithm can be scaled to\nhigh-dimensional settings. Our proposal is illustrated on synthetic data, a\nwebpage data set, and a brain cancer gene expression data set. \n\n"}
{"id": "1303.5343", "contents": "Title: Entanglement's Benefit Survives an Entanglement-Breaking Channel Abstract: Entanglement is essential to many quantum information applications, but it is\neasily destroyed by quantum decoherence arising from interaction with the\nenvironment. We report the first experimental demonstration of an\nentanglement-based protocol that is resilient to loss and noise which destroy\nentanglement. Specifically, despite channel noise 8.3 dB beyond the threshold\nfor entanglement breaking, eavesdropping-immune communication is achieved\nbetween Alice and Bob when an entangled source is used, but no such immunity is\nobtainable when their source is classical. The results prove that entanglement\ncan be utilized beneficially in lossy and noisy situations, i.e., in practical\nscenarios. \n\n"}
{"id": "1304.0277", "contents": "Title: Rotational Symmetry of Conical K\\\"ahler-Ricci Solitons Abstract: We show that expanding K\\\"ahler-Ricci solitons which have positive\nholomorphic bisectional curvature and are asymptotic to K\\\"ahler cones at\ninfinity must be the U(n)-rotationally symmetric expanding solitons constructed\nby Cao. \n\n"}
{"id": "1304.2810", "contents": "Title: High-dimensional Mixed Graphical Models Abstract: While graphical models for continuous data (Gaussian graphical models) and\ndiscrete data (Ising models) have been extensively studied, there is little\nwork on graphical models linking both continuous and discrete variables (mixed\ndata), which are common in many scientific applications. We propose a novel\ngraphical model for mixed data, which is simple enough to be suitable for\nhigh-dimensional data, yet flexible enough to represent all possible graph\nstructures. We develop a computationally efficient regression-based algorithm\nfor fitting the model by focusing on the conditional log-likelihood of each\nvariable given the rest. The parameters have a natural group structure, and\nsparsity in the fitted graph is attained by incorporating a group lasso\npenalty, approximated by a weighted $\\ell_1$ penalty for computational\nefficiency. We demonstrate the effectiveness of our method through an extensive\nsimulation study and apply it to a music annotation data set (CAL500),\nobtaining a sparse and interpretable graphical model relating the continuous\nfeatures of the audio signal to categorical variables such as genre, emotions,\nand usage associated with particular songs. While we focus on binary discrete\nvariables, we also show that the proposed methodology can be easily extended to\ngeneral discrete variables. \n\n"}
{"id": "1304.2810", "contents": "Title: High-dimensional Mixed Graphical Models Abstract: While graphical models for continuous data (Gaussian graphical models) and\ndiscrete data (Ising models) have been extensively studied, there is little\nwork on graphical models linking both continuous and discrete variables (mixed\ndata), which are common in many scientific applications. We propose a novel\ngraphical model for mixed data, which is simple enough to be suitable for\nhigh-dimensional data, yet flexible enough to represent all possible graph\nstructures. We develop a computationally efficient regression-based algorithm\nfor fitting the model by focusing on the conditional log-likelihood of each\nvariable given the rest. The parameters have a natural group structure, and\nsparsity in the fitted graph is attained by incorporating a group lasso\npenalty, approximated by a weighted $\\ell_1$ penalty for computational\nefficiency. We demonstrate the effectiveness of our method through an extensive\nsimulation study and apply it to a music annotation data set (CAL500),\nobtaining a sparse and interpretable graphical model relating the continuous\nfeatures of the audio signal to categorical variables such as genre, emotions,\nand usage associated with particular songs. While we focus on binary discrete\nvariables, we also show that the proposed methodology can be easily extended to\ngeneral discrete variables. \n\n"}
{"id": "1304.3628", "contents": "Title: Dynamical probing of a topological phase of bosons in one dimension Abstract: We study the linear response to time-dependent probes of a symmetry-protected\ntopological phase of bosons in one-dimension, the Haldane insulator (HI). This\nphase is separated from the ordinary Mott insulator (MI) and density-wave (DW)\nphases by continuous transitions, whose field theoretical description is here\nreviewed. Using this technique, we compute the absorption spectrum to two types\nof periodic perturbations and relate the findings to the nature of the critical\nexcitations at the transition between the different phases. The HI-MI phase\ntransition is topological and the critical excitations possess trivial quantum\nnumbers: they correspond to particles and holes at zero momentum. Our findings\nare corroborated by a non-local mean-field approach, which allows us to\ndirectly relate the predicted spectrum to the known microscopic theory. \n\n"}
{"id": "1304.5583", "contents": "Title: Distributed Low-rank Subspace Segmentation Abstract: Vision problems ranging from image clustering to motion segmentation to\nsemi-supervised learning can naturally be framed as subspace segmentation\nproblems, in which one aims to recover multiple low-dimensional subspaces from\nnoisy and corrupted input data. Low-Rank Representation (LRR), a convex\nformulation of the subspace segmentation problem, is provably and empirically\naccurate on small problems but does not scale to the massive sizes of modern\nvision datasets. Moreover, past work aimed at scaling up low-rank matrix\nfactorization is not applicable to LRR given its non-decomposable constraints.\nIn this work, we propose a novel divide-and-conquer algorithm for large-scale\nsubspace segmentation that can cope with LRR's non-decomposable constraints and\nmaintains LRR's strong recovery guarantees. This has immediate implications for\nthe scalability of subspace segmentation, which we demonstrate on a benchmark\nface recognition dataset and in simulations. We then introduce novel\napplications of LRR-based subspace segmentation to large-scale semi-supervised\nlearning for multimedia event detection, concept detection, and image tagging.\nIn each case, we obtain state-of-the-art results and order-of-magnitude speed\nups. \n\n"}
{"id": "1305.3292", "contents": "Title: Discrete Quantum Theories Abstract: We explore finite-field frameworks for quantum theory and quantum\ncomputation. The simplest theory, defined over unrestricted finite fields, is\nunnaturally strong. A second framework employs only finite fields with no\nsolution to x^2+1=0, and thus permits an elegant complex representation of the\nextended field by adjoining i=\\sqrt{-1}. Quantum theories over these fields\nrecover much of the structure of conventional quantum theory except for the\ncondition that vanishing inner products arise only from null states;\nunnaturally strong computational power may still occur. Finally, we are led to\nconsider one more framework, with further restrictions on the finite fields,\nthat recovers a local transitive order and a locally-consistent notion of inner\nproduct with a new notion of cardinal probability. In this framework,\nconventional quantum mechanics and quantum computation emerge locally (though\nnot globally) as the size of the underlying field increases. Interestingly, the\nframework allows one to choose separate finite fields for system description\nand for measurement: the size of the first field quantifies the resources\nneeded to describe the system and the size of the second quantifies the\nresources used by the observer. This resource-based perspective potentially\nprovides insights into quantitative measures for actual computational power,\nthe complexity of quantum system definition and evolution, and the independent\nquestion of the cost of the measurement process. \n\n"}
{"id": "1305.4451", "contents": "Title: Flows and a tangency condition for embeddable CR structures in dimension\n  3 Abstract: We study the fillability (or embeddability) of 3-dimensional $CR$ structures\nunder the geometric flows. Suppose we can solve a certain second order equation\nfor the geometric quantity associated to the flow. Then we prove that if the\ninitial $CR$ structure is fillable, then it keeps having the same property as\nlong as the flow has a solution. We discuss the situation for the torsion flow\nand the Cartan flow. In the second part, we show that the above mentioned\nsecond order operator is used to express a tangency condition for the space of\nall fillable or embeddable $CR$ structures at one embedded in $\\mathbb{C}^{2}.$ \n\n"}
{"id": "1305.5730", "contents": "Title: Adiabatic quantum metrology with strongly correlated quantum optical\n  systems Abstract: We show that the quasi-adiabatic evolution of a system governed by the Dicke\nHamiltonian can be described in terms of a self-induced quantum many-body\nmetrological protocol. This effect relies on the sensitivity of the ground\nstate to a small symmetry-breaking perturbation at the quantum phase\ntransition, that leads to the collapse of the wavefunciton into one of two\npossible ground states. The scaling of the final state properties with the\nnumber of atoms and with the intensity of the symmetry breaking field, can be\ninterpreted in terms of the precession time of an effective quantum\nmetrological protocol. We show that our ideas can be tested with spin-phonon\ninteractions in trapped ion setups. Our work points to a classification of\nquantum phase transitions in terms of the capability of many-body quantum\nsystems for parameter estimation. \n\n"}
{"id": "1305.6526", "contents": "Title: Adaptive estimation of the copula correlation matrix for semiparametric\n  elliptical copulas Abstract: We study the adaptive estimation of copula correlation matrix $\\Sigma$ for\nthe semi-parametric elliptical copula model. In this context, the correlations\nare connected to Kendall's tau through a sine function transformation. Hence, a\nnatural estimate for $\\Sigma$ is the plug-in estimator $\\hat{\\Sigma}$ with\nKendall's tau statistic. We first obtain a sharp bound on the operator norm of\n$\\hat{\\Sigma}-\\Sigma$. Then we study a factor model of $\\Sigma$, for which we\npropose a refined estimator $\\widetilde{\\Sigma}$ by fitting a low-rank matrix\nplus a diagonal matrix to $\\hat{\\Sigma}$ using least squares with a nuclear\nnorm penalty on the low-rank matrix. The bound on the operator norm of\n$\\hat{\\Sigma}-\\Sigma$ serves to scale the penalty term, and we obtain finite\nsample oracle inequalities for $\\widetilde{\\Sigma}$. We also consider an\nelementary factor copula model of $\\Sigma$, for which we propose closed-form\nestimators. All of our estimation procedures are entirely data-driven. \n\n"}
{"id": "1305.6916", "contents": "Title: Statistical analysis of latent generalized correlation matrix estimation\n  in transelliptical distribution Abstract: Correlation matrices play a key role in many multivariate methods (e.g.,\ngraphical model estimation and factor analysis). The current state-of-the-art\nin estimating large correlation matrices focuses on the use of Pearson's sample\ncorrelation matrix. Although Pearson's sample correlation matrix enjoys various\ngood properties under Gaussian models, it is not an effective estimator when\nfacing heavy-tailed distributions. As a robust alternative, Han and Liu [J. Am.\nStat. Assoc. 109 (2015) 275-287] advocated the use of a transformed version of\nthe Kendall's tau sample correlation matrix in estimating high dimensional\nlatent generalized correlation matrix under the transelliptical distribution\nfamily (or elliptical copula). The transelliptical family assumes that after\nunspecified marginal monotone transformations, the data follow an elliptical\ndistribution. In this paper, we study the theoretical properties of the\nKendall's tau sample correlation matrix and its transformed version proposed in\nHan and Liu [J. Am. Stat. Assoc. 109 (2015) 275-287] for estimating the\npopulation Kendall's tau correlation matrix and the latent Pearson's\ncorrelation matrix under both spectral and restricted spectral norms. With\nregard to the spectral norm, we highlight the role of \"effective rank\" in\nquantifying the rate of convergence. With regard to the restricted spectral\nnorm, we for the first time present a \"sign sub-Gaussian condition\" which is\nsufficient to guarantee that the rank-based correlation matrix estimator\nattains the fast rate of convergence. In both cases, we do not need any moment\ncondition. \n\n"}
{"id": "1306.0849", "contents": "Title: Efficient separation of the orbital angular momentum eigenstates of\n  light Abstract: Orbital angular momentum (OAM) of light is an attractive degree of freedom\nfor funda- mentals studies in quantum mechanics. In addition, the discrete\nunbounded state-space of OAM has been used to enhance classical and quantum\ncommunications. Unambiguous mea- surement of OAM is a key part of all such\nexperiments. However, state-of-the-art methods for separating single photons\ncarrying a large number of different OAM values are limited to a theoretical\nseparation efficiency of about 77 percent. Here we demonstrate a method which\nuses a series of unitary optical transformations to enable the measurement of\nlights OAM with an experimental separation efficiency of more than 92 percent.\nFurther, we demonstrate the separation of modes in the angular position basis,\nwhich is mutually unbiased with respect to the OAM basis. The high degree of\ncertainty achieved by our method makes it particu- larly attractive for\nenhancing the information capacity of multi-level quantum cryptography systems. \n\n"}
{"id": "1306.2733", "contents": "Title: Copula Mixed-Membership Stochastic Blockmodel for Intra-Subgroup\n  Correlations Abstract: The \\emph{Mixed-Membership Stochastic Blockmodel (MMSB)} is a popular\nframework for modeling social network relationships. It can fully exploit each\nindividual node's participation (or membership) in a social structure. Despite\nits powerful representations, this model makes an assumption that the\ndistributions of relational membership indicators between two nodes are\nindependent. Under many social network settings, however, it is possible that\ncertain known subgroups of people may have high or low correlations in terms of\ntheir membership categories towards each other, and such prior information\nshould be incorporated into the model. To this end, we introduce a \\emph{Copula\nMixed-Membership Stochastic Blockmodel (cMMSB)} where an individual Copula\nfunction is employed to jointly model the membership pairs of those nodes\nwithin the subgroup of interest. The model enables the use of various Copula\nfunctions to suit the scenario, while maintaining the membership's marginal\ndistribution, as needed, for modeling membership indicators with other nodes\noutside of the subgroup of interest. We describe the proposed model and its\ninference algorithm in detail for both the finite and infinite cases. In the\nexperiment section, we compare our algorithms with other popular models in\nterms of link prediction, using both synthetic and real world data. \n\n"}
{"id": "1306.3494", "contents": "Title: Randomized maximum-contrast selection: subagging for large-scale\n  regression Abstract: We introduce a very general method for sparse and large-scale variable\nselection. The large-scale regression settings is such that both the number of\nparameters and the number of samples are extremely large. The proposed method\nis based on careful combination of penalized estimators, each applied to a\nrandom projection of the sample space into a low-dimensional space. In one\nspecial case that we study in detail, the random projections are divided into\nnon-overlapping blocks; each consisting of only a small portion of the original\ndata. Within each block we select the projection yielding the smallest\nout-of-sample error. Our random ensemble estimator then aggregates the results\naccording to new maximal-contrast voting scheme to determine the final selected\nset. Our theoretical results illuminate the effect on performance of increasing\nthe number of non-overlapping blocks. Moreover, we demonstrate that statistical\noptimality is retained along with the computational speedup. The proposed\nmethod achieves minimax rates for approximate recovery over all estimators\nusing the full set of samples. Furthermore, our theoretical results allow the\nnumber of subsamples to grow with the subsample size and do not require\nirrepresentable condition. The estimator is also compared empirically with\nseveral other popular high-dimensional estimators via an extensive simulation\nstudy, which reveals its excellent finite-sample performance. \n\n"}
{"id": "1306.4529", "contents": "Title: Conditional Least Squares and Copulae in Claims Reserving for a Single\n  Line of Business Abstract: One of the main goals in non-life insurance is to estimate the claims reserve\ndistribution. A generalized time series model, that allows for modeling the\nconditional mean and variance of the claim amounts, is proposed for the claims\ndevelopment. On contrary to the classical stochastic reserving techniques, the\nnumber of model parameters does not depend on the number of development\nperiods, which leads to a more precise forecasting.\n  Moreover, the time series innovations for the consecutive claims are not\nconsidered to be independent anymore. Conditional least squares are used for\nmodel parameter estimation and consistency of such estimate is proved. Copula\napproach is used for modeling the dependence structure, which improves the\nprecision of the reserve distribution estimate as well.\n  Real data examples are provided as an illustration of the potential benefits\nof the presented approach. \n\n"}
{"id": "1307.0781", "contents": "Title: Distributed Online Big Data Classification Using Context Information Abstract: Distributed, online data mining systems have emerged as a result of\napplications requiring analysis of large amounts of correlated and\nhigh-dimensional data produced by multiple distributed data sources. We propose\na distributed online data classification framework where data is gathered by\ndistributed data sources and processed by a heterogeneous set of distributed\nlearners which learn online, at run-time, how to classify the different data\nstreams either by using their locally available classification functions or by\nhelping each other by classifying each other's data. Importantly, since the\ndata is gathered at different locations, sending the data to another learner to\nprocess incurs additional costs such as delays, and hence this will be only\nbeneficial if the benefits obtained from a better classification will exceed\nthe costs. We model the problem of joint classification by the distributed and\nheterogeneous learners from multiple data sources as a distributed contextual\nbandit problem where each data is characterized by a specific context. We\ndevelop a distributed online learning algorithm for which we can prove\nsublinear regret. Compared to prior work in distributed online data mining, our\nwork is the first to provide analytic regret results characterizing the\nperformance of the proposed algorithm. \n\n"}
{"id": "1307.5339", "contents": "Title: The Cluster Graphical Lasso for improved estimation of Gaussian\n  graphical models Abstract: We consider the task of estimating a Gaussian graphical model in the\nhigh-dimensional setting. The graphical lasso, which involves maximizing the\nGaussian log likelihood subject to an l1 penalty, is a well-studied approach\nfor this task. We begin by introducing a surprising connection between the\ngraphical lasso and hierarchical clustering: the graphical lasso in effect\nperforms a two-step procedure, in which (1) single linkage hierarchical\nclustering is performed on the variables in order to identify connected\ncomponents, and then (2) an l1-penalized log likelihood is maximized on the\nsubset of variables within each connected component. In other words, the\ngraphical lasso determines the connected components of the estimated network\nvia single linkage clustering. Unfortunately, single linkage clustering is\nknown to perform poorly in certain settings. Therefore, we propose the cluster\ngraphical lasso, which involves clustering the features using an alternative to\nsingle linkage clustering, and then performing the graphical lasso on the\nsubset of variables within each cluster. We establish model selection\nconsistency for this technique, and demonstrate its improved performance\nrelative to the graphical lasso in a simulation study, as well as in\napplications to an equities data set, a university webpage data set, and a gene\nexpression data set. \n\n"}
{"id": "1307.6852", "contents": "Title: One-dimensional Bose-Hubbard model with pure three-body interactions Abstract: The extended Bose-Hubbard model with pure three-body local interactions is\nstudied using the Density Matrix Renormalization Group approach. The shapes of\nthe first two insulating lobes are discussed, and the values of the critical\ntunneling for which the system undergoes the quantum phase transition from\ninsulating to superfluid phase are predicted. It is shown that stability of\ninsulating phases, in contrast to the standard Bose-Hubbard model, is enhanced\nfor larger fillings. It is also shown that, on the tip of the boundary of the\ninsulating phase, the model under consideration belongs to the\nBerenzinskii-Kosterlitz-Thouless universality class. \n\n"}
{"id": "1308.2348", "contents": "Title: Integral quantizations with two basic examples Abstract: The paper is devoted to integral quantization, a procedure based on\noperator-valued measure and resolution of the identity. We insist on covariance\nproperties in the important case where group representation theory is involved.\nWe also insist on the inherent probabilistic aspects of this classical-quantum\nmap. The approach includes and generalizes coherent state quantization. Two\napplications based on group representation are carried out. The first one\nconcerns the Weyl-Heisenberg group and the euclidean plane viewed as the\ncorresponding phase space. We show that there exists a world of quantizations\nwhich yield the canonical commutation rule and the usual quantum spectrum of\nthe harmonic oscillator. The second one concerns the affine group of the real\nline and gives rise to an interesting regularization of the dilation origin in\nthe half-plane viewed as the corresponding phase space. \n\n"}
{"id": "1308.2655", "contents": "Title: KL-based Control of the Learning Schedule for Surrogate Black-Box\n  Optimization Abstract: This paper investigates the control of an ML component within the Covariance\nMatrix Adaptation Evolution Strategy (CMA-ES) devoted to black-box\noptimization. The known CMA-ES weakness is its sample complexity, the number of\nevaluations of the objective function needed to approximate the global optimum.\nThis weakness is commonly addressed through surrogate optimization, learning an\nestimate of the objective function a.k.a. surrogate model, and replacing most\nevaluations of the true objective function with the (inexpensive) evaluation of\nthe surrogate model. This paper presents a principled control of the learning\nschedule (when to relearn the surrogate model), based on the Kullback-Leibler\ndivergence of the current search distribution and the training distribution of\nthe former surrogate model. The experimental validation of the proposed\napproach shows significant performance gains on a comprehensive set of\nill-conditioned benchmark problems, compared to the best state of the art\nincluding the quasi-Newton high-precision BFGS method. \n\n"}
{"id": "1308.2878", "contents": "Title: Weak transcendental holomorphic Morse inequalities on compact K\\\"ahler\n  manifolds Abstract: Transcendental holomorphic Morse inequalities aim at characterizing the\npositivity of transcendental cohomology classes of type $(1,1)$. In this paper,\nwe prove a weak version of Demailly's conjecture on transcendental Morse\ninequalities on compact K\\\"ahler manifolds. And as a consequence, we partially\nimprove a result of Boucksom-Demailly-Paun-Peternell. \n\n"}
{"id": "1308.6069", "contents": "Title: Compound Poisson Processes, Latent Shrinkage Priors and Bayesian\n  Nonconvex Penalization Abstract: In this paper we discuss Bayesian nonconvex penalization for sparse learning\nproblems. We explore a nonparametric formulation for latent shrinkage\nparameters using subordinators which are one-dimensional L\\'{e}vy processes. We\nparticularly study a family of continuous compound Poisson subordinators and a\nfamily of discrete compound Poisson subordinators. We exemplify four specific\nsubordinators: Gamma, Poisson, negative binomial and squared Bessel\nsubordinators. The Laplace exponents of the subordinators are Bernstein\nfunctions, so they can be used as sparsity-inducing nonconvex penalty\nfunctions. We exploit these subordinators in regression problems, yielding a\nhierarchical model with multiple regularization parameters. We devise ECME\n(Expectation/Conditional Maximization Either) algorithms to simultaneously\nestimate regression coefficients and regularization parameters. The empirical\nevaluation of simulated data shows that our approach is feasible and effective\nin high-dimensional data analysis. \n\n"}
{"id": "1309.0626", "contents": "Title: Genuine Multipartite Entanglement in the $XY$ Model Abstract: We analyze the $XY$ model characterized by an anisotropy $\\gamma$ in an\nexternal magnetic field $h$ with respect to its genuine multipartite\nentanglement content (in the thermodynamic and finite size case). Despite its\nsimplicity we show that the quantity -detecting genuine multipartite\nentanglement through permutation operators and being a lower bound on measures-\nwitnesses the presence of genuine multipartite entanglement for nearly all\nvalues of $\\gamma$ and $h$. We further show that the phase transition and\nscaling properties are fully characterized by this multipartite quantity.\nConsequently, we provide a useful toolbox for other condensed matter systems,\nwhere bipartite entanglement measures are known to fail. \n\n"}
{"id": "1309.2074", "contents": "Title: Learning Transformations for Clustering and Classification Abstract: A low-rank transformation learning framework for subspace clustering and\nclassification is here proposed. Many high-dimensional data, such as face\nimages and motion sequences, approximately lie in a union of low-dimensional\nsubspaces. The corresponding subspace clustering problem has been extensively\nstudied in the literature to partition such high-dimensional data into clusters\ncorresponding to their underlying low-dimensional subspaces. However,\nlow-dimensional intrinsic structures are often violated for real-world\nobservations, as they can be corrupted by errors or deviate from ideal models.\nWe propose to address this by learning a linear transformation on subspaces\nusing matrix rank, via its convex surrogate nuclear norm, as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same subspace, and, at the same time, forces a a maximally\nseparated structure for data from different subspaces. In this way, we reduce\nvariations within subspaces, and increase separation between subspaces for a\nmore robust subspace clustering. This proposed learned robust subspace\nclustering framework significantly enhances the performance of existing\nsubspace clustering methods. Basic theoretical results here presented help to\nfurther support the underlying framework. To exploit the low-rank structures of\nthe transformed subspaces, we further introduce a fast subspace clustering\ntechnique, which efficiently combines robust PCA with sparse modeling. When\nclass labels are present at the training stage, we show this low-rank\ntransformation framework also significantly enhances classification\nperformance. Extensive experiments using public datasets are presented, showing\nthat the proposed approach significantly outperforms state-of-the-art methods\nfor subspace clustering and classification. \n\n"}
{"id": "1309.2350", "contents": "Title: Exponentially Fast Parameter Estimation in Networks Using Distributed\n  Dual Averaging Abstract: In this paper we present an optimization-based view of distributed parameter\nestimation and observational social learning in networks. Agents receive a\nsequence of random, independent and identically distributed (i.i.d.) signals,\neach of which individually may not be informative about the underlying true\nstate, but the signals together are globally informative enough to make the\ntrue state identifiable. Using an optimization-based characterization of\nBayesian learning as proximal stochastic gradient descent (with\nKullback-Leibler divergence from a prior as a proximal function), we show how\nto efficiently use a distributed, online variant of Nesterov's dual averaging\nmethod to solve the estimation with purely local information. When the true\nstate is globally identifiable, and the network is connected, we prove that\nagents eventually learn the true parameter using a randomized gossip scheme. We\ndemonstrate that with high probability the convergence is exponentially fast\nwith a rate dependent on the KL divergence of observations under the true state\nfrom observations under the second likeliest state. Furthermore, our work also\nhighlights the possibility of learning under continuous adaptation of network\nwhich is a consequence of employing constant, unit stepsize for the algorithm. \n\n"}
{"id": "1309.3256", "contents": "Title: Recovery guarantees for exemplar-based clustering Abstract: For a certain class of distributions, we prove that the linear programming\nrelaxation of $k$-medoids clustering---a variant of $k$-means clustering where\nmeans are replaced by exemplars from within the dataset---distinguishes points\ndrawn from nonoverlapping balls with high probability once the number of points\ndrawn and the separation distance between any two balls are sufficiently large.\nOur results hold in the nontrivial regime where the separation distance is\nsmall enough that points drawn from different balls may be closer to each other\nthan points drawn from the same ball; in this case, clustering by thresholding\npairwise distances between points can fail. We also exhibit numerical evidence\nof high-probability recovery in a substantially more permissive regime. \n\n"}
{"id": "1309.3489", "contents": "Title: Group-bound: confidence intervals for groups of variables in sparse\n  high-dimensional regression without assumptions on the design Abstract: It is in general challenging to provide confidence intervals for individual\nvariables in high-dimensional regression without making strict or unverifiable\nassumptions on the design matrix. We show here that a \"group-bound\" confidence\ninterval can be derived without making any assumptions on the design matrix.\nThe lower bound for the regression coefficient of individual variables can be\nderived via linear programming. The idea also generalises naturally to groups\nof variables, where we can derive a one-sided confidence interval for the joint\neffect of a group. While the confidence intervals of individual variables are\nby the nature of the problem often very wide, it is shown to be possible to\ndetect the contribution of groups of highly correlated predictor variables even\nwhen no variable individually shows a significant effect. The assumptions\nnecessary to detect the effect of groups of variables are shown to be weaker\nthan the weakest known assumptions to detect the effect of individual\nvariables. \n\n"}
{"id": "1309.4097", "contents": "Title: Seeing Hofstadter's Butterfly in Atomic Fermi Gases Abstract: We propose a novel way to detect the fractal energy spectrum of the\nHofstadter model from the density distributions of ultracold fermions in an\nexternal trap. At low temperature, the local compressibility is proportional to\nthe density of states of the system which reveals the fractal energy spectrum.\nHowever, thermal broadening and noises in the real experimental situation\ninevitably smear out fine features in the density distribution. To overcome\nthis difficulty, we use the maximum entropy method to extract the density of\nstates directly from the noisy thermal density distributions. Simulations show\nthat one is able to restore the core feature of the Hofstadter's butterfly\nspectrum with current experimental techniques. By further reducing the noise or\nthe temperature, one can refine the resolution and observe fine structures of\nthe butterfly spectrum. \n\n"}
{"id": "1309.4686", "contents": "Title: Robust Inference on Average Treatment Effects with Possibly More\n  Covariates than Observations Abstract: This paper concerns robust inference on average treatment effects following\nmodel selection. In the selection on observables framework, we show how to\nconstruct confidence intervals based on a doubly-robust estimator that are\nrobust to model selection errors and prove that they are valid uniformly over a\nlarge class of treatment effect models. The class allows for multivalued\ntreatments with heterogeneous effects (in observables), general\nheteroskedasticity, and selection amongst (possibly) more covariates than\nobservations. Our estimator attains the semiparametric efficiency bound under\nappropriate conditions. Precise conditions are given for any model selector to\nyield these results, and we show how to combine data-driven selection with\neconomic theory. For implementation, we give a specific proposal for selection\nbased on the group lasso, which is particularly well-suited to treatment\neffects data, and derive new results for high-dimensional, sparse multinomial\nlogistic regression. A simulation study shows our estimator performs very well\nin finite samples over a wide range of models. Revisiting the National\nSupported Work demonstration data, our method yields accurate estimates and\ntight confidence intervals. \n\n"}
{"id": "1309.5352", "contents": "Title: Sequential Selection Procedures and False Discovery Rate Control Abstract: We consider a multiple hypothesis testing setting where the hypotheses are\nordered and one is only permitted to reject an initial contiguous block,\nH_1,\\dots,H_k, of hypotheses. A rejection rule in this setting amounts to a\nprocedure for choosing the stopping point k. This setting is inspired by the\nsequential nature of many model selection problems, where choosing a stopping\npoint or a model is equivalent to rejecting all hypotheses up to that point and\nnone thereafter. We propose two new testing procedures, and prove that they\ncontrol the false discovery rate in the ordered testing setting. We also show\nhow the methods can be applied to model selection using recent results on\np-values in sequential model selection settings. \n\n"}
{"id": "1309.5744", "contents": "Title: Globalizations of infinitesimal actions on supermanifolds Abstract: Let $\\mathcal G$ be a Lie supergroup with Lie superalgebra $\\mathfrak g$,\n$\\mathcal M$ a supermanifold and $\\mathrm{Vec}(\\mathcal M)$ the set of vector\nfields on $\\mathcal M$. Let $\\lambda:\\mathfrak g\\rightarrow\n\\mathrm{Vec}(\\mathcal M)$ be an infinitesimal action, i.e. a homomorphism of\nLie superalgebras.\n  We show the existence of a local $\\mathcal G$-action on $\\mathcal M$ inducing\nthe infinitesimal action $\\lambda$ and find necessary and sufficient conditions\nfor the existence of a globalization in the sense of Palais. \n\n"}
{"id": "1309.6933", "contents": "Title: Estimating Undirected Graphs Under Weak Assumptions Abstract: We consider the problem of providing nonparametric confidence guarantees for\nundirected graphs under weak assumptions. In particular, we do not assume\nsparsity, incoherence or Normality. We allow the dimension $D$ to increase with\nthe sample size $n$. First, we prove lower bounds that show that if we want\naccurate inferences with low assumptions then there are limitations on the\ndimension as a function of sample size. When the dimension increases slowly\nwith sample size, we show that methods based on Normal approximations and on\nthe bootstrap lead to valid inferences and we provide Berry-Esseen bounds on\nthe accuracy of the Normal approximation. When the dimension is large relative\nto sample size, accurate inferences for graphs under low assumptions are not\npossible. Instead we propose to estimate something less demanding than the\nentire partial correlation graph. In particular, we consider: cluster graphs,\nrestricted partial correlation graphs and correlation graphs. \n\n"}
{"id": "1310.8302", "contents": "Title: No $\\psi$-epistemic model can fully explain the indistinguishability of\n  quantum states Abstract: According to a recent no-go theorem (M. Pusey, J. Barrett and T. Rudolph,\nNature Physics 8, 475 (2012)), models in which quantum states correspond to\nprobability distributions over the values of some underlying physical variables\nmust have the following feature: the distributions corresponding to distinct\nquantum states do not overlap. This is significant because if the distributions\ndo not overlap, then the quantum state itself is encoded by the physical\nvariables. In such a model, it cannot coherently be maintained that the quantum\nstate merely encodes information about underlying physical variables. The\ntheorem, however, considers only models in which the physical variables\ncorresponding to independently prepared systems are independent. This work\nconsiders models that are defined for a single quantum system of dimension $d$,\nsuch that the independence condition does not arise. We prove a result in a\nsimilar spirit to the original no-go theorem, in the form of an upper bound on\nthe extent to which the probability distributions can overlap, consistently\nwith reproducing quantum predictions. In particular, models in which the\nquantum overlap between pure states is equal to the classical overlap between\nthe corresponding probability distributions cannot reproduce the quantum\npredictions in any dimension $d \\geq 3$. The result is noise tolerant, and an\nexperiment is motivated to distinguish the class of models ruled out from\nquantum theory. \n\n"}
{"id": "1311.0274", "contents": "Title: Nearly Optimal Sample Size in Hypothesis Testing for High-Dimensional\n  Regression Abstract: We consider the problem of fitting the parameters of a high-dimensional\nlinear regression model. In the regime where the number of parameters $p$ is\ncomparable to or exceeds the sample size $n$, a successful approach uses an\n$\\ell_1$-penalized least squares estimator, known as Lasso. Unfortunately,\nunlike for linear estimators (e.g., ordinary least squares), no\nwell-established method exists to compute confidence intervals or p-values on\nthe basis of the Lasso estimator. Very recently, a line of work\n\\cite{javanmard2013hypothesis, confidenceJM, GBR-hypothesis} has addressed this\nproblem by constructing a debiased version of the Lasso estimator. In this\npaper, we study this approach for random design model, under the assumption\nthat a good estimator exists for the precision matrix of the design. Our\nanalysis improves over the state of the art in that it establishes nearly\noptimal \\emph{average} testing power if the sample size $n$ asymptotically\ndominates $s_0 (\\log p)^2$, with $s_0$ being the sparsity level (number of\nnon-zero coefficients). Earlier work obtains provable guarantees only for much\nlarger sample size, namely it requires $n$ to asymptotically dominate $(s_0\n\\log p)^2$.\n  In particular, for random designs with a sparse precision matrix we show that\nan estimator thereof having the required properties can be computed\nefficiently. Finally, we evaluate this approach on synthetic data and compare\nit with earlier proposals. \n\n"}
{"id": "1311.0513", "contents": "Title: Maximal Plurifinely Plurisubharmonic functions Abstract: The main purpose of this paper is to introduce and study the notion of\nplurifinely-maximal plurifinely plurisubharmonic functions, which extends the\nnotion of maximal plurisubharmonic functions on a Euclidean domain to a\nplurifine domain of C^n in a natural way. Our main result is that a finite\nplurifinely plurisubharmonic function u on a plurifine domain U satisfies (dd^c\nu)^n=0 if and only if u is plurifinely-locally plurifinely-maximal outside some\npluripolar set. In particular, a finite plurifinely-maximal plurisubharmonic\nfunction u satisfies (dd^c u)^n=0. \n\n"}
{"id": "1311.0562", "contents": "Title: LP Mixed Data Science : Outline of Theory Abstract: This article presents the theoretical foundation of a new frontier of\nresearch-`LP Mixed Data Science'-that simultaneously extends and integrates the\npractice of traditional and novel statistical methods for nonparametric\nexploratory data modeling, and is applicable to the teaching and training of\nstatistics.\n  Statistics journals have great difficulty accepting papers unlike those\npreviously published. For statisticians with new big ideas a practical strategy\nis to publish them in many small applied studies which enables one to provide\nreferences to work of others. This essay outlines the many concepts, new\ntheory, and important algorithms of our new culture of statistical science\ncalled LP MIXED DATA SCIENCE. It provides comprehensive solutions to problems\nof data analysis and nonparametric modeling of many variables that are\ncontinuous or discrete, which does not yet have a large literature. It develops\na new modeling approach to nonparametric estimation of the multivariate copula\ndensity. We discuss the theory which we believe is very elegant (and can\nprovide a framework for United Statistical Algorithms, for traditional Small\nData methods and Big Data methods). \n\n"}
{"id": "1311.1595", "contents": "Title: Testing for a General Class of Functional Inequalities Abstract: In this paper, we propose a general method for testing inequality\nrestrictions on nonparametric functions. Our framework includes many\nnonparametric testing problems in a unified framework, with a number of\npossible applications in auction models, game theoretic models, wage\ninequality, and revealed preferences. Our test involves a one-sided version of\n$L_{p}$ functionals of kernel-type estimators $(1\\leq p <\\infty )$ and is easy\nto implement in general, mainly due to its recourse to the bootstrap method.\nThe bootstrap procedure is based on nonparametric bootstrap applied to\nkernel-based test statistics, with estimated \"contact sets.\" We provide\nregularity conditions under which the bootstrap test is asymptotically valid\nuniformly over a large class of distributions, including the cases that the\nlimiting distribution of the test statistic is degenerate. Our bootstrap test\nis shown to exhibit good power properties in Monte Carlo experiments, and we\nprovide a general form of the local power function. As an illustration, we\nconsider testing implications from auction theory, provide primitive conditions\nfor our test, and demonstrate its usefulness by applying our test to real data.\nWe supplement this example with the second empirical illustration in the\ncontext of wage inequality. \n\n"}
{"id": "1311.2483", "contents": "Title: Global Sensitivity Analysis with Dependence Measures Abstract: Global sensitivity analysis with variance-based measures suffers from several\ntheoretical and practical limitations, since they focus only on the variance of\nthe output and handle multivariate variables in a limited way. In this paper,\nwe introduce a new class of sensitivity indices based on dependence measures\nwhich overcomes these insufficiencies. Our approach originates from the idea to\ncompare the output distribution with its conditional counterpart when one of\nthe input variables is fixed. We establish that this comparison yields\npreviously proposed indices when it is performed with Csiszar f-divergences, as\nwell as sensitivity indices which are well-known dependence measures between\nrandom variables. This leads us to investigate completely new sensitivity\nindices based on recent state-of-the-art dependence measures, such as distance\ncorrelation and the Hilbert-Schmidt independence criterion. We also emphasize\nthe potential of feature selection techniques relying on such dependence\nmeasures as alternatives to screening in high dimension. \n\n"}
{"id": "1311.2520", "contents": "Title: The Infinite Degree Corrected Stochastic Block Model Abstract: In Stochastic blockmodels, which are among the most prominent statistical\nmodels for cluster analysis of complex networks, clusters are defined as groups\nof nodes with statistically similar link probabilities within and between\ngroups. A recent extension by Karrer and Newman incorporates a node degree\ncorrection to model degree heterogeneity within each group. Although this\ndemonstrably leads to better performance on several networks it is not obvious\nwhether modelling node degree is always appropriate or necessary. We formulate\nthe degree corrected stochastic blockmodel as a non-parametric Bayesian model,\nincorporating a parameter to control the amount of degree correction which can\nthen be inferred from data. Additionally, our formulation yields principled\nways of inferring the number of groups as well as predicting missing links in\nthe network which can be used to quantify the model's predictive performance.\nOn synthetic data we demonstrate that including the degree correction yields\nbetter performance both on recovering the true group structure and predicting\nmissing links when degree heterogeneity is present, whereas performance is on\npar for data with no degree heterogeneity within clusters. On seven real\nnetworks (with no ground truth group structure available) we show that\npredictive performance is about equal whether or not degree correction is\nincluded; however, for some networks significantly fewer clusters are\ndiscovered when correcting for degree indicating that the data can be more\ncompactly explained by clusters of heterogenous degree nodes. \n\n"}
{"id": "1311.4643", "contents": "Title: Near-Optimal Entrywise Sampling for Data Matrices Abstract: We consider the problem of selecting non-zero entries of a matrix $A$ in\norder to produce a sparse sketch of it, $B$, that minimizes $\\|A-B\\|_2$. For\nlarge $m \\times n$ matrices, such that $n \\gg m$ (for example, representing $n$\nobservations over $m$ attributes) we give sampling distributions that exhibit\nfour important properties. First, they have closed forms computable from\nminimal information regarding $A$. Second, they allow sketching of matrices\nwhose non-zeros are presented to the algorithm in arbitrary order as a stream,\nwith $O(1)$ computation per non-zero. Third, the resulting sketch matrices are\nnot only sparse, but their non-zero entries are highly compressible. Lastly,\nand most importantly, under mild assumptions, our distributions are provably\ncompetitive with the optimal offline distribution. Note that the probabilities\nin the optimal offline distribution may be complex functions of all the entries\nin the matrix. Therefore, regardless of computational complexity, the optimal\ndistribution might be impossible to compute in the streaming model. \n\n"}
{"id": "1311.6238", "contents": "Title: Exact post-selection inference, with application to the lasso Abstract: We develop a general approach to valid inference after model selection. At\nthe core of our framework is a result that characterizes the distribution of a\npost-selection estimator conditioned on the selection event. We specialize the\napproach to model selection by the lasso to form valid confidence intervals for\nthe selected coefficients and test whether all relevant variables have been\nincluded in the model. \n\n"}
{"id": "1311.7343", "contents": "Title: Log concavity for matrix-valued functions and a matrix-valued Pr\\'ekopa\n  theorem Abstract: We give two different definitions of what it means for a matrix-valued\nfunction to be log concave, guided by similar notions in complex differential\ngeometry. After discussing a few simple examples, we proceed to develop some of\nthe basic properties associated with these new concepts. Finally, we prove a\nmatrix-valued Pr\\'ekopa theorem using a weighted, vector-valued Paley-Wiener\ntheorem, and positivity properties of direct image bundles. \n\n"}
{"id": "1312.0887", "contents": "Title: Beyond the Spin Model Approximation for Ramsey Spectroscopy Abstract: Ramsey spectroscopy has become a powerful technique for probing\nnon-equilibrium dynamics of internal (pseudospin) degrees of freedom of\ninteracting systems. In many theoretical treatments, the key to understanding\nthe dynamics has been to assume the external (motional) degrees of freedom are\ndecoupled from the pseudospin degrees of freedom. Determining the validity of\nthis approximation -- known as the spin model approximation -- is complicated,\nand has not been addressed in detail. Here we shed light in this direction by\ncalculating Ramsey dynamics exactly for two interacting spin-1/2 particles in a\nharmonic trap. We focus on $s$-wave-interacting fermions in quasi-one and\ntwo-dimensional geometries. We find that in 1D the spin model assumption works\nwell over a wide range of experimentally-relevant conditions, but can fail at\ntime scales longer than those set by the mean interaction energy. Surprisingly,\nin 2D a modified version of the spin model is exact to first order in the\ninteraction strength. This analysis is important for a correct interpretation\nof Ramsey spectroscopy and has broad applications ranging from precision\nmeasurements to quantum information and to fundamental probes of many-body\nsystems. \n\n"}
{"id": "1312.2903", "contents": "Title: The lower tail of random quadratic forms, with applications to ordinary\n  least squares and restricted eigenvalue properties Abstract: Finite sample properties of random covariance-type matrices have been the\nsubject of much research. In this paper we focus on the \"lower tail\" of such a\nmatrix, and prove that it is subgaussian under a simple fourth moment\nassumption on the one-dimensional marginals of the random vectors. A similar\nresult holds for more general sums of random positive semidefinite matrices,\nand the (relatively simple) proof uses a variant of the so-called PAC-Bayesian\nmethod for bounding empirical processes.\n  We give two applications of the main result. In the first one we obtain a new\nfinite-sample bound for ordinary least squares estimator in linear regression\nwith random design. Our result is model-free, requires fairly weak moment\nassumptions and is almost optimal. Our second application is to bounding\nrestricted eigenvalue constants of certain random ensembles with \"heavy tails\".\nThese constants are important in the analysis of problems in Compressed Sensing\nand High Dimensional Statistics, where one recovers a sparse vector from a\nsmall umber of linear measurements. Our result implies that heavy tails still\nallow for the fast recovery rates found in efficient methods such as the LASSO\nand the Dantzig selector. Along the way we strengthen, with a fairly short\nargument, a recent result of Rudelson and Zhou on the restricted eigenvalue\nproperty. \n\n"}
{"id": "1312.3429", "contents": "Title: Unsupervised learning of depth and motion Abstract: We present a model for the joint estimation of disparity and motion. The\nmodel is based on learning about the interrelations between images from\nmultiple cameras, multiple frames in a video, or the combination of both. We\nshow that learning depth and motion cues, as well as their combinations, from\ndata is possible within a single type of architecture and a single type of\nlearning algorithm, by using biologically inspired \"complex cell\" like units,\nwhich encode correlations between the pixels across image pairs. Our\nexperimental results show that the learning of depth and motion makes it\npossible to achieve state-of-the-art performance in 3-D activity analysis, and\nto outperform existing hand-engineered 3-D motion features by a very large\nmargin. \n\n"}
{"id": "1312.4605", "contents": "Title: Parallelizing MCMC via Weierstrass Sampler Abstract: With the rapidly growing scales of statistical problems, subset based\ncommunication-free parallel MCMC methods are a promising future for large scale\nBayesian analysis. In this article, we propose a new Weierstrass sampler for\nparallel MCMC based on independent subsets. The new sampler approximates the\nfull data posterior samples via combining the posterior draws from independent\nsubset MCMC chains, and thus enjoys a higher computational efficiency. We show\nthat the approximation error for the Weierstrass sampler is bounded by some\ntuning parameters and provide suggestions for choice of the values. Simulation\nstudy shows the Weierstrass sampler is very competitive compared to other\nmethods for combining MCMC chains generated for subsets, including averaging\nand kernel smoothing. \n\n"}
{"id": "1312.7559", "contents": "Title: A model selection approach for clustering a multinomial sequence with\n  non-negative factorization Abstract: We consider a problem of clustering a sequence of multinomial observations by\nway of a model selection criterion. We propose a form of a penalty term for the\nmodel selection procedure. Our approach subsumes both the conventional AIC and\nBIC criteria but also extends the conventional criteria in a way that it can be\napplicable also to a sequence of sparse multinomial observations, where even\nwithin a same cluster, the number of multinomial trials may be different for\ndifferent observations. In addition, as a preliminary estimation step to\nmaximum likelihood estimation, and more generally, to maximum $L_{q}$\nestimation, we propose to use reduced rank projection in combination with\nnon-negative factorization. We motivate our approach by showing that our model\nselection criterion and preliminary estimation step yield consistent estimates\nunder simplifying assumptions. We also illustrate our approach through\nnumerical experiments using real and simulated data. \n\n"}
{"id": "1401.2561", "contents": "Title: An Operational Interpretation of Negative Probabilities and\n  No-Signalling Models Abstract: Negative probabilities have long been discussed in connection with the\nfoundations of quantum mechanics. We have recently shown that, if signed\nmeasures are allowed on the hidden variables, the class of probability models\nwhich can be captured by local hidden-variable models are exactly the\nno-signalling models. However, the question remains of how negative\nprobabilities are to be interpreted. In this paper, we present an operational\ninterpretation of negative probabilities as arising from standard probabilities\non signed events. This leads, by virtue of our previous result, to a systematic\nscheme for simulating arbitrary no-signalling models. \n\n"}
{"id": "1401.4724", "contents": "Title: Analytic Differential Equations and Spherical Real Hypersurfaces Abstract: We establish an injective correspondence $M\\longrightarrow\\mathcal E(M)$\nbetween real-analytic nonminimal hypersurfaces $M\\subset\\mathbb{C}^{2}$,\nspherical at a generic point, and a class of second order complex ODEs with a\nmeromorphic singularity. We apply this result to the proof of the bound\n$\\mbox{dim}\\,\\mathfrak{hol}(M,p)\\leq 5$ for the infinitesimal automorphism\nalgebra of an \\it arbitrary \\rm germ $(M,p)\\not\\sim(S^3,p')$ of a real-analytic\nLevi nonflat hypersurface $M\\subset\\mathbb{C}^2$ (the Dimension Conjecture).\nThis bound gives the first proof of the dimension gap\n$\\mbox{dim}\\,\\mathfrak{hol}(M,p)=\\{8,5,4,3,2,1,0\\}$ for the dimension of the\nautomorphism algebra of a real-analytic Levi nonflat hypersurface. As another\napplication we obtain a new regularity condition for CR-mappings of nonminimal\nhypersurfaces, that we call \\it Fuchsian type, \\rm and prove its optimality for\nextension of CR-mappings to nonminimal points. \\\\ We also obtain an existence\ntheorem for solutions of a class of singular complex ODEs (Theorem 3.5). \n\n"}
{"id": "1401.7278", "contents": "Title: Minimax-optimal nonparametric regression in high dimensions Abstract: Minimax $L_2$ risks for high-dimensional nonparametric regression are derived\nunder two sparsity assumptions: (1) the true regression surface is a sparse\nfunction that depends only on $d=O(\\log n)$ important predictors among a list\nof $p$ predictors, with $\\log p=o(n)$; (2) the true regression surface depends\non $O(n)$ predictors but is an additive function where each additive component\nis sparse but may contain two or more interacting predictors and may have a\nsmoothness level different from other components. For either modeling\nassumption, a practicable extension of the widely used Bayesian Gaussian\nprocess regression method is shown to adaptively attain the optimal minimax\nrate (up to $\\log n$ terms) asymptotically as both $n,p\\to\\infty$ with $\\log\np=o(n)$. \n\n"}
{"id": "1402.0119", "contents": "Title: Randomized Nonlinear Component Analysis Abstract: Classical methods such as Principal Component Analysis (PCA) and Canonical\nCorrelation Analysis (CCA) are ubiquitous in statistics. However, these\ntechniques are only able to reveal linear relationships in data. Although\nnonlinear variants of PCA and CCA have been proposed, these are computationally\nprohibitive in the large scale.\n  In a separate strand of recent research, randomized methods have been\nproposed to construct features that help reveal nonlinear patterns in data. For\nbasic tasks such as regression or classification, random features exhibit\nlittle or no loss in performance, while achieving drastic savings in\ncomputational requirements.\n  In this paper we leverage randomness to design scalable new variants of\nnonlinear PCA and CCA; our ideas extend to key multivariate analysis tools such\nas spectral clustering or LDA. We demonstrate our algorithms through\nexperiments on real-world data, on which we compare against the\nstate-of-the-art. A simple R implementation of the presented algorithms is\nprovided. \n\n"}
{"id": "1402.1954", "contents": "Title: On the $\\partial\\overline{\\partial}$-Lemma and Bott-Chern cohomology Abstract: On a compact complex manifold $X$, we prove a Fr\\\"olicher-type inequality for\nBott-Chern cohomology and we show that the equality holds if and only if $X$\nsatisfies the $\\partial\\overline{\\partial}$-Lemma. \n\n"}
{"id": "1402.2085", "contents": "Title: Large degree asymptotics of orthogonal polynomials with respect to an\n  oscillatory weight on a bounded interval Abstract: We consider polynomials $p_n^{\\omega}(x)$ that are orthogonal with respect to\nthe oscillatory weight $w(x)=e^{i\\omega x}$ on $[-1,1]$, where $\\omega>0$ is a\nreal parameter. A first analysis of $p_n^{\\omega}(x)$ for large values of\n$\\omega$ was carried out in connection with complex Gaussian quadrature rules\nwith uniform good properties in $\\omega$. In this contribution we study the\nexistence, asymptotic behavior and asymptotic distribution of the roots of\n$p_n^{\\omega}(x)$ in the complex plane as $n\\to\\infty$. The parameter $\\omega$\ngrows with $n$ linearly. The tools used are logarithmic potential theory and\nthe $S$-property, together with the Riemann--Hilbert formulation and the\nDeift--Zhou steepest descent method. \n\n"}
{"id": "1402.2383", "contents": "Title: Sequential Quantum Secret Sharing in a Noisy Environment aided with Weak\n  Measurements Abstract: In this work we give a $(n,n)$-threshold protocol for sequential secret\nsharing of quantum information for the first time.\n  By sequential secret sharing we refer to a situation where the dealer is not\nhaving all the secrets at the same time, at the beginning of the protocol;\nhowever if the dealer wishes to share secrets at subsequent phases she/he can\nrealize it with the help of our protocol. First of all we present our protocol\nfor three parties and later we generalize it for the situation where we have\n$(n>3)$ parties.\n  Further in a much more realistic situation, we consider the sharing of qubits\nthrough two kinds of noisy channels, namely the phase damping channel (PDC) and\nthe amplitude damping channel (ADC). When we carry out the sequential secret\nsharing in the presence of noise we observe that the fidelity of secret sharing\nat the $k^{th}$ iteration is independent of the effect of noise at the\n$(k-1)^{th}$ iteration. In case of ADC we have seen that the average fidelity\nof secret sharing drops down to $\\frac{1}{2}$ which is equivalent to a random\nguess of the quantum secret. Interestingly, we find that by applying weak\nmeasurements one can enhance the average fidelity. This increase of the average\nfidelity can be achieved with certain trade off with the success probability of\nthe weak measurements. \n\n"}
{"id": "1402.2679", "contents": "Title: Equivalence of Kernel Machine Regression and Kernel Distance Covariance\n  for Multidimensional Trait Association Studies Abstract: Associating genetic markers with a multidimensional phenotype is an important\nyet challenging problem. In this work, we establish the equivalence between two\npopular methods: kernel-machine regression (KMR), and kernel distance\ncovariance (KDC). KMR is a semiparametric regression frameworks that models the\ncovariate effects parametrically, while the genetic markers are considered\nnon-parametrically. KDC represents a class of methods that includes distance\ncovariance (DC) and Hilbert-Schmidt Independence Criterion (HSIC), which are\nnonparametric tests of independence. We show the equivalence between the score\ntest of KMR and the KDC statistic under certain conditions. This result leads\nto a novel generalization of the KDC test that incorporates the covariates. Our\ncontributions are three-fold: (1) establishing the equivalence between KMR and\nKDC; (2) showing that the principles of kernel machine regression can be\napplied to the interpretation of KDC; (3) the development of a broader class of\nKDC statistics, that the members are the quantities of different kernels. We\ndemonstrate the proposals using simulation studies. Data from the Alzheimer's\nDisease Neuroimaging Initiative (ADNI) is used to explore the association\nbetween the genetic variants on gene \\emph{FLJ16124} and phenotypes represented\nin 3D structural brain MR images adjusting for age and gender. The results\nsuggest that SNPs of \\emph{FLJ16124} exhibit strong pairwise interaction\neffects that are correlated to the changes of brain region volumes. \n\n"}
{"id": "1402.3093", "contents": "Title: Bayesian nonparametric dependent model for partially replicated data:\n  the influence of fuel spills on species diversity Abstract: We introduce a dependent Bayesian nonparametric model for the probabilistic\nmodeling of membership of subgroups in a community based on partially\nreplicated data. The focus here is on species-by-site data, i.e. community data\nwhere observations at different sites are classified in distinct species. Our\naim is to study the impact of additional covariates, for instance environmental\nvariables, on the data structure, and in particular on the community diversity.\nTo that purpose, we introduce dependence a priori across the covariates, and\nshow that it improves posterior inference. We use a dependent version of the\nGriffiths-Engen-McCloskey distribution defined via the stick-breaking\nconstruction. This distribution is obtained by transforming a Gaussian process\nwhose covariance function controls the desired dependence. The resulting\nposterior distribution is sampled by Markov chain Monte Carlo. We illustrate\nthe application of our model to a soil microbial dataset acquired across a\nhydrocarbon contamination gradient at the site of a fuel spill in Antarctica.\nThis method allows for inference on a number of quantities of interest in\necotoxicology, such as diversity or effective concentrations, and is broadly\napplicable to the general problem of communities response to environmental\nvariables. \n\n"}
{"id": "1402.3722", "contents": "Title: word2vec Explained: deriving Mikolov et al.'s negative-sampling\n  word-embedding method Abstract: The word2vec software of Tomas Mikolov and colleagues\n(https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and\nprovides state-of-the-art word embeddings. The learning models behind the\nsoftware are described in two research papers. We found the description of the\nmodels in these papers to be somewhat cryptic and hard to follow. While the\nmotivations and presentation may be obvious to the neural-networks\nlanguage-modeling crowd, we had to struggle quite a bit to figure out the\nrationale behind the equations.\n  This note is an attempt to explain equation (4) (negative sampling) in\n\"Distributed Representations of Words and Phrases and their Compositionality\"\nby Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean. \n\n"}
{"id": "1402.5329", "contents": "Title: Single-atom trapping in holographic 2D arrays of microtraps with\n  arbitrary geometries Abstract: We demonstrate single-atom trapping in two-dimensional arrays of microtraps\nwith arbitrary geometries. We generate the arrays using a Spatial Light\nModulator (SLM), with which we imprint an appropriate phase pattern on an\noptical dipole trap beam prior to focusing. We trap single $^{87}{\\rm Rb}$\natoms in the sites of arrays containing up to $\\sim100$ microtraps separated by\ndistances as small as $3\\;\\mu$m, with complex structures such as triangular,\nhoneycomb or kagome lattices. Using a closed-loop optimization of the\nuniformity of the trap depths ensures that all trapping sites are equivalent.\nThis versatile system opens appealing applications in quantum information\nprocessing and quantum simulation, e.g. for simulating frustrated quantum\nmagnetism using Rydberg atoms. \n\n"}
{"id": "1402.6951", "contents": "Title: Modeling the Complex Dynamics and Changing Correlations of Epileptic\n  Events Abstract: Patients with epilepsy can manifest short, sub-clinical epileptic \"bursts\" in\naddition to full-blown clinical seizures. We believe the relationship between\nthese two classes of events---something not previously studied\nquantitatively---could yield important insights into the nature and intrinsic\ndynamics of seizures. A goal of our work is to parse these complex epileptic\nevents into distinct dynamic regimes. A challenge posed by the intracranial EEG\n(iEEG) data we study is the fact that the number and placement of electrodes\ncan vary between patients. We develop a Bayesian nonparametric Markov switching\nprocess that allows for (i) shared dynamic regimes between a variable number of\nchannels, (ii) asynchronous regime-switching, and (iii) an unknown dictionary\nof dynamic regimes. We encode a sparse and changing set of dependencies between\nthe channels using a Markov-switching Gaussian graphical model for the\ninnovations process driving the channel dynamics and demonstrate the importance\nof this model in parsing and out-of-sample predictions of iEEG data. We show\nthat our model produces intuitive state assignments that can help automate\nclinical analysis of seizures and enable the comparison of sub-clinical bursts\nand full clinical seizures. \n\n"}
{"id": "1403.0611", "contents": "Title: The data aggregation problem in quantum hypothesis testing Abstract: We discuss the implications of quantum-classical Yule-Simpson effect for\nquantum hypothesis testing in the presence of noise, and provide an\nexperimental demonstration of its occurrence in the problem of discriminating\nwhich polarization quantum measurement has been actually performed by a\ndetector box designed to measure linear polarization of single-photon states\nalong a fixed but unknown direction. \n\n"}
{"id": "1403.1553", "contents": "Title: On the mixed Hodge structure associated to hypersurface singularities Abstract: Let $f:\\mathbb{C}^{n+1} \\to \\mathbb{C}$ be a germ of hypersurface with\nisolated singularity. One can associate to $f$ a polarized variation of mixed\nHodge structure $\\mathcal{H}$ over the punctured disc, where the Hodge\nfiltration is the limit Hodge filtration of W. Schmid and J. Steenbrink. By the\nwork of M. Saito and P. Deligne the VMHS associated to cohomologies of the\nfibers of $f$ can be extended over the degenerate point $0$ of disc. The new\nfiber obtained in this way is isomorphic to the module of relative\ndifferentials of $f$ denoted $\\Omega_f$. A mixed Hodge structure can be defined\non $\\Omega_f$ in this way. The polarization on $\\mathcal{H}$ deforms to\nGrothendieck residue pairing modified by a varying sign on the Hodge graded\npieces in this process. This also proves the existence of a Riemann-Hodge\nbilinear relation for Grothendieck pairing and allow to calculate the Hodge\nsignature of Grothendieck pairing. \n\n"}
{"id": "1403.1922", "contents": "Title: Adaptive Estimation in Two-way Sparse Reduced-rank Regression Abstract: This paper studies the problem of estimating a large coefficient matrix in a\nmultiple response linear regression model when the coefficient matrix could be\nboth of low rank and sparse in the sense that most nonzero entries concentrate\non a few rows and columns. We are especially interested in the high dimensional\nsettings where the number of predictors and/or response variables can be much\nlarger than the number of observations. We propose a new estimation scheme,\nwhich achieves competitive numerical performance and at the same time allows\nfast computation. Moreover, we show that (a slight variant of) the proposed\nestimator achieves near optimal non-asymptotic minimax rates of estimation\nunder a collection of squared Schatten norm losses simultaneously by providing\nboth the error bounds for the estimator and minimax lower bounds. The\neffectiveness of the proposed algorithm is also demonstrated on an \\textit{in\nvivo} calcium imaging dataset. \n\n"}
{"id": "1403.6095", "contents": "Title: Simultaneous sparse estimation of canonical vectors in the p>>N setting Abstract: This article considers the problem of sparse estimation of canonical vectors\nin linear discriminant analysis when $p\\gg N$. Several methods have been\nproposed in the literature that estimate one canonical vector in the two-group\ncase. However, $G-1$ canonical vectors can be considered if the number of\ngroups is $G$. In the multi-group context, it is common to estimate canonical\nvectors in a sequential fashion. Moreover, separate prior estimation of the\ncovariance structure is often required. We propose a novel methodology for\ndirect estimation of canonical vectors. In contrast to existing techniques, the\nproposed method estimates all canonical vectors at once, performs variable\nselection across all the vectors and comes with theoretical guarantees on the\nvariable selection and classification consistency. First, we highlight the fact\nthat in the $N>p$ setting the canonical vectors can be expressed in a closed\nform up to an orthogonal transformation. Secondly, we propose an extension of\nthis form to the $p\\gg N$ setting and achieve feature selection by using a\ngroup penalty. The resulting optimization problem is convex and can be solved\nusing a block-coordinate descent algorithm. The practical performance of the\nmethod is evaluated through simulation studies as well as real data\napplications. \n\n"}
{"id": "1403.6195", "contents": "Title: Multivariate Analysis of Nonparametric Estimates of Large Correlation\n  Matrices Abstract: We study concentration in spectral norm of nonparametric estimates of\ncorrelation matrices. We work within the confine of a Gaussian copula model.\nTwo nonparametric estimators of the correlation matrix, the sine\ntransformations of the Kendall's tau and Spearman's rho correlation\ncoefficient, are studied. Expected spectrum error bound is obtained for both\nthe estimators. A general large deviation bound for the maximum spectral error\nof a collection of submatrices of a given dimension is also established. These\nresults prove that when both the number of variables and sample size are large,\nthe spectral error of the nonparametric estimators is of no greater order than\nthat of the latent sample covariance matrix, at least when compared with some\nof the sharpest known error bounds for the later. As an application, we\nestablish the minimax optimal convergence rate in the estimation of\nhigh-dimensional bandable correlation matrices via tapering off of these\nnonparametric estimators. An optimal convergence rate for sparse principal\ncomponent analysis is also established as another example of possible\napplications of the main results. \n\n"}
{"id": "1404.0983", "contents": "Title: On the exceptional set in a conditional theorem of Littlewood Abstract: In 1952, Littlewood stated a conjecture about the average growth of spherical\nderivatives of polynomials, and showed that it would imply that for entire\nfunction of finite order, \"most\" preimages of almost all points are\nconcentrated in a small subset of the plane. In 1988, Lewis and Wu proved\nLittlewood's conjecture. Using techniques from complex dynamics, we construct\nentire functions of finite order with a bounded set of singular values for\nwhich the set of exceptional preimages is infinite, with logarithmically\ngrowing cardinality. \n\n"}
{"id": "1404.3030", "contents": "Title: Antiholomorphic involutions and spherical subgroups of reductive groups Abstract: We investigate qualitative and quantitative properties of anti-holomorphic\ninvolutions on some spherical varieties. \n\n"}
{"id": "1404.3120", "contents": "Title: Landau-Toeplitz theorems for slice regular functions over quaternions Abstract: The theory of slice regular functions of a quaternionic variable extends the\nnotion of holomorphic function to the quaternionic setting. This theory,\nalready rich of results, is sometimes surprisingly different from the theory of\nholomorphic functions of a complex variable. However, several fundamental\nresults in the two environments are similar, even if their proofs for the case\nof quaternions need new technical tools. In this paper we prove the\nLandau-Toeplitz Theorem for slice regular functions, in a formulation that\ninvolves an appropriate notion of regular $2$-diameter. We then show that the\nLandau-Toeplitz inequalities hold in the case of the regular $n$-diameter, for\nall $n\\geq 2$. Finally, a $3$-diameter version of the Landau-Toeplitz Theorem\nis proved using the notion of slice $3$-diameter. \n\n"}
{"id": "1404.3441", "contents": "Title: Bayesian nonparametric estimation of Tsallis diversity indices under\n  Gnedin-Pitman priors Abstract: Tsallis entropy is a generalized diversity index first derived in Patil and\nTaillie (1982) and then rediscovered in community ecology by Keylock (2005).\nBayesian nonparametric estimation of Shannon entropy and Simpson's diversity\nunder uniform and symmetric Dirichlet priors has been already advocated as an\nalternative to maximum likelihood estimation based on frequency counts, which\nis negatively biased in the undersampled regime. Here we present a fully\ngeneral Bayesian nonparametric estimation of the whole class of Tsallis\ndiversity indices under Gnedin-Pitman priors, a large family of random discrete\ndistributions recently deeply investigated in posterior predictive species\nrichness and discovery probability estimation. We provide both prior and\nposterior analysis. The results, illustrated through examples and an\napplication to a real dataset, show the procedure is easily implementable,\nflexible and overcomes limitations of previous frequentist and Bayesian\nsolutions. \n\n"}
{"id": "1404.4408", "contents": "Title: Geometric Inference for General High-Dimensional Linear Inverse Problems Abstract: This paper presents a unified geometric framework for the statistical\nanalysis of a general ill-posed linear inverse model which includes as special\ncases noisy compressed sensing, sign vector recovery, trace regression,\northogonal matrix estimation, and noisy matrix completion. We propose\ncomputationally feasible convex programs for statistical inference including\nestimation, confidence intervals and hypothesis testing. A theoretical\nframework is developed to characterize the local estimation rate of convergence\nand to provide statistical inference guarantees. Our results are built based on\nthe local conic geometry and duality. The difficulty of statistical inference\nis captured by the geometric characterization of the local tangent cone through\nthe Gaussian width and Sudakov minoration estimate. \n\n"}
{"id": "1405.0502", "contents": "Title: Weak compactness of operators acting on o-O type spaces Abstract: We consider operators T : M_0 -> Z and T : M -> Z, where Z is a Banach space\nand (M_0, M) is a pair of Banach spaces belonging to a general construction in\nwhich M is defined by a \"big-O\" condition and M_0 is given by the corresponding\n\"little-o\" condition. Prototype examples of such spaces M are given by\n$\\ell^\\infty$, weighted spaces of functions or their derivatives, bounded mean\noscillation, Lipschitz-H\\\"older spaces, and many others. The main result\ncharacterizes the weakly compact operators T in terms of a certain norm\nnaturally attached to M, weaker than the M-norm, and shows that weakly compact\noperators T : M_0 -> Z are already quite close to being completely continuous.\nFurther, we develop a method to extract c_0-subsequences from sequences in M_0.\nApplications are given to the characterizations of the weakly compact\ncomposition and Volterra-type integral operators on weighted spaces of analytic\nfunctions, BMOA, VMOA, and the Bloch space. \n\n"}
{"id": "1405.0782", "contents": "Title: Optimality guarantees for distributed statistical estimation Abstract: Large data sets often require performing distributed statistical estimation,\nwith a full data set split across multiple machines and limited communication\nbetween machines. To study such scenarios, we define and study some refinements\nof the classical minimax risk that apply to distributed settings, comparing to\nthe performance of estimators with access to the entire data. Lower bounds on\nthese quantities provide a precise characterization of the minimum amount of\ncommunication required to achieve the centralized minimax risk. We study two\nclasses of distributed protocols: one in which machines send messages\nindependently over channels without feedback, and a second allowing for\ninteractive communication, in which a central server broadcasts the messages\nfrom a given machine to all other machines. We establish lower bounds for a\nvariety of problems, including location estimation in several families and\nparameter estimation in different types of regression models. Our results\ninclude a novel class of quantitative data-processing inequalities used to\ncharacterize the effects of limited communication. \n\n"}
{"id": "1405.1969", "contents": "Title: Tunable and Switchable Coupling Between Two Superconducting Resonators Abstract: We realize a device allowing for tunable and switchable coupling between two\nsuperconducting resonators mediated by an artificial atom. For the latter, we\nutilize a persistent current flux qubit. We characterize the tunable and\nswitchable coupling in frequency and time domain and find that the coupling\nbetween the relevant modes can be varied in a controlled way. Specifically, the\ncoupling can be tuned by adjusting the flux through the qubit loop or by\nsaturating the qubit. Our time domain measurements allow us to find parameter\nregimes for optimal switch performance with respect to qubit drive power and\nthe dynamic range of the resonator input power. \n\n"}
{"id": "1405.3224", "contents": "Title: On the Complexity of A/B Testing Abstract: A/B testing refers to the task of determining the best option among two\nalternatives that yield random outcomes. We provide distribution-dependent\nlower bounds for the performance of A/B testing that improve over the results\ncurrently available both in the fixed-confidence (or delta-PAC) and\nfixed-budget settings. When the distribution of the outcomes are Gaussian, we\nprove that the complexity of the fixed-confidence and fixed-budget settings are\nequivalent, and that uniform sampling of both alternatives is optimal only in\nthe case of equal variances. In the common variance case, we also provide a\nstopping rule that terminates faster than existing fixed-confidence algorithms.\nIn the case of Bernoulli distributions, we show that the complexity of\nfixed-budget setting is smaller than that of fixed-confidence setting and that\nuniform sampling of both alternatives -though not optimal- is advisable in\npractice when combined with an appropriate stopping criterion. \n\n"}
{"id": "1406.0052", "contents": "Title: Variable selection in high-dimensional additive models based on norms of\n  projections Abstract: We consider the problem of variable selection in high-dimensional sparse\nadditive models. We focus on the case that the components belong to\nnonparametric classes of functions. The proposed method is motivated by\ngeometric considerations in Hilbert spaces and consists of comparing the norms\nof the projections of the data onto various additive subspaces. Under minimal\ngeometric assumptions, we prove concentration inequalities which lead to new\nconditions under which consistent variable selection is possible. As an\napplication, we establish conditions under which a single component can be\nestimated with the rate of convergence corresponding to the situation in which\nthe other components are known. \n\n"}
{"id": "1406.2541", "contents": "Title: Predictive Entropy Search for Efficient Global Optimization of Black-box\n  Functions Abstract: We propose a novel information-theoretic approach for Bayesian optimization\ncalled Predictive Entropy Search (PES). At each iteration, PES selects the next\nevaluation point that maximizes the expected information gained with respect to\nthe global maximum. PES codifies this intractable acquisition function in terms\nof the expected reduction in the differential entropy of the predictive\ndistribution. This reformulation allows PES to obtain approximations that are\nboth more accurate and efficient than other alternatives such as Entropy Search\n(ES). Furthermore, PES can easily perform a fully Bayesian treatment of the\nmodel hyperparameters while ES cannot. We evaluate PES in both synthetic and\nreal-world applications, including optimization problems in machine learning,\nfinance, biotechnology, and robotics. We show that the increased accuracy of\nPES leads to significant gains in optimization performance. \n\n"}
{"id": "1406.3469", "contents": "Title: LOCO: Distributing Ridge Regression with Random Projections Abstract: We propose LOCO, an algorithm for large-scale ridge regression which\ndistributes the features across workers on a cluster. Important dependencies\nbetween variables are preserved using structured random projections which are\ncheap to compute and must only be communicated once. We show that LOCO obtains\na solution which is close to the exact ridge regression solution in the fixed\ndesign setting. We verify this experimentally in a simulation study as well as\nan application to climate prediction. Furthermore, we show that LOCO achieves\nsignificant speedups compared with a state-of-the-art distributed algorithm on\na large-scale regression problem. \n\n"}
{"id": "1406.4444", "contents": "Title: PRISM: Person Re-Identification via Structured Matching Abstract: Person re-identification (re-id), an emerging problem in visual surveillance,\ndeals with maintaining entities of individuals whilst they traverse various\nlocations surveilled by a camera network. From a visual perspective re-id is\nchallenging due to significant changes in visual appearance of individuals in\ncameras with different pose, illumination and calibration. Globally the\nchallenge arises from the need to maintain structurally consistent matches\namong all the individual entities across different camera views. We propose\nPRISM, a structured matching method to jointly account for these challenges. We\nview the global problem as a weighted graph matching problem and estimate edge\nweights by learning to predict them based on the co-occurrences of visual\npatterns in the training examples. These co-occurrence based scores in turn\naccount for appearance changes by inferring likely and unlikely visual\nco-occurrences appearing in training instances. We implement PRISM on single\nshot and multi-shot scenarios. PRISM uniformly outperforms state-of-the-art in\nterms of matching rate while being computationally efficient. \n\n"}
{"id": "1406.6471", "contents": "Title: Order of Starlikeness and Convexity of certain integral transforms using\n  duality techniques Abstract: For $\\alpha\\geq 0$, $\\beta<1$ and $\\gamma\\geq 0$, the class\n$\\mathcal{W}_{\\beta}(\\alpha,\\gamma)$ satisfies the condition \\begin{align*}\n{\\rm Re\\,} \\left( e^{i\\phi}\\left((1-\\alpha+2\\gamma)f/z+(\\alpha-2\\gamma)f'+\n\\gamma zf''-\\beta\\right)\\frac{}{}\\right)>0, \\quad \\phi\\in {\\mathbb{R}},{\\,}z\\in\n{\\mathbb{D}}; \\end{align*} is taken into consideration. The Pascu class of\n$\\xi$-convex functions of order $\\sigma$ $(M(\\sigma,{\\,}\\xi))$, having analytic\ncharacterization \\begin{align*} {\\rm Re\\,}\\frac{\\xi\nz(zf'(z))'+(1-\\xi)zf'(z)}{\\xi zf'(z)+(1-\\xi)f(z)}>\\sigma,\\quad 0\\leq \\sigma<\n1,\\quad z\\in {\\mathbb{D}}, \\end{align*} unifies starlike and convex functions\nclass of order $\\sigma$.The admissible and sufficient conditions on\n$\\lambda(t)$ are investigated so that the integral transforms \\begin{align*}\nV_{\\lambda}(f)(z)= \\int_0^1 \\lambda(t) \\frac{f(tz)}{t} dt, \\end{align*} maps\nthe function from $\\mathcal{W}_{\\beta}(\\alpha,\\gamma)$ into\n$M(\\sigma,{\\,}\\xi)$. Further several interesting applications, for specific\nchoice of $\\lambda(t)$ are discussed which are related to the classical\nintegral transform. \n\n"}
{"id": "1406.7321", "contents": "Title: Proximal Quasi-Newton for Computationally Intensive L1-regularized\n  M-estimators Abstract: We consider the class of optimization problems arising from computationally\nintensive L1-regularized M-estimators, where the function or gradient values\nare very expensive to compute. A particular instance of interest is the\nL1-regularized MLE for learning Conditional Random Fields (CRFs), which are a\npopular class of statistical models for varied structured prediction problems\nsuch as sequence labeling, alignment, and classification with label taxonomy.\nL1-regularized MLEs for CRFs are particularly expensive to optimize since\ncomputing the gradient values requires an expensive inference step. In this\nwork, we propose the use of a carefully constructed proximal quasi-Newton\nalgorithm for such computationally intensive M-estimation problems, where we\nemploy an aggressive active set selection technique. In a key contribution of\nthe paper, we show that the proximal quasi-Newton method is provably\nsuper-linearly convergent, even in the absence of strong convexity, by\nleveraging a restricted variant of strong convexity. In our experiments, the\nproposed algorithm converges considerably faster than current state-of-the-art\non the problems of sequence labeling and hierarchical classification. \n\n"}
{"id": "1407.0202", "contents": "Title: SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly\n  Convex Composite Objectives Abstract: In this work we introduce a new optimisation method called SAGA in the spirit\nof SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient\nalgorithms with fast linear convergence rates. SAGA improves on the theory\nbehind SAG and SVRG, with better theoretical convergence rates, and has support\nfor composite objectives where a proximal operator is used on the regulariser.\nUnlike SDCA, SAGA supports non-strongly convex problems directly, and is\nadaptive to any inherent strong convexity of the problem. We give experimental\nresults showing the effectiveness of our method. \n\n"}
{"id": "1407.4729", "contents": "Title: Sparse Partially Linear Additive Models Abstract: The generalized partially linear additive model (GPLAM) is a flexible and\ninterpretable approach to building predictive models. It combines features in\nan additive manner, allowing each to have either a linear or nonlinear effect\non the response. However, the choice of which features to treat as linear or\nnonlinear is typically assumed known. Thus, to make a GPLAM a viable approach\nin situations in which little is known $a~priori$ about the features, one must\novercome two primary model selection challenges: deciding which features to\ninclude in the model and determining which of these features to treat\nnonlinearly. We introduce the sparse partially linear additive model (SPLAM),\nwhich combines model fitting and $both$ of these model selection challenges\ninto a single convex optimization problem. SPLAM provides a bridge between the\nlasso and sparse additive models. Through a statistical oracle inequality and\nthorough simulation, we demonstrate that SPLAM can outperform other methods\nacross a broad spectrum of statistical regimes, including the high-dimensional\n($p\\gg N$) setting. We develop efficient algorithms that are applied to real\ndata sets with half a million samples and over 45,000 features with excellent\npredictive performance. \n\n"}
{"id": "1407.6533", "contents": "Title: Rice-Mele model with topological solitons in an optical lattice Abstract: Attractive ultra-cold fermions trapped in a one-dimensional periodically\nshaken opticla lattices are considered. For an appropriate resonant shaking the\nsystem realizes paradigmatic dimes physics described by Rice-Mele model. The\nimportant feature of our system is the possible presence of controlled defects.\nThey result in the creation of topologically protected loclaized modes carrying\nfractional particle number. Their possible experimental signatures are\ndiscussed. \n\n"}
{"id": "1408.1717", "contents": "Title: Matrix Completion on Graphs Abstract: The problem of finding the missing values of a matrix given a few of its\nentries, called matrix completion, has gathered a lot of attention in the\nrecent years. Although the problem under the standard low rank assumption is\nNP-hard, Cand\\`es and Recht showed that it can be exactly relaxed if the number\nof observed entries is sufficiently large. In this work, we introduce a novel\nmatrix completion model that makes use of proximity information about rows and\ncolumns by assuming they form communities. This assumption makes sense in\nseveral real-world problems like in recommender systems, where there are\ncommunities of people sharing preferences, while products form clusters that\nreceive similar ratings. Our main goal is thus to find a low-rank solution that\nis structured by the proximities of rows and columns encoded by graphs. We\nborrow ideas from manifold learning to constrain our solution to be smooth on\nthese graphs, in order to implicitly force row and column proximities. Our\nmatrix recovery model is formulated as a convex non-smooth optimization\nproblem, for which a well-posed iterative scheme is provided. We study and\nevaluate the proposed matrix completion on synthetic and real data, showing\nthat the proposed structured low-rank recovery model outperforms the standard\nmatrix completion model in many situations. \n\n"}
{"id": "1408.2156", "contents": "Title: Statistical guarantees for the EM algorithm: From population to\n  sample-based analysis Abstract: We develop a general framework for proving rigorous guarantees on the\nperformance of the EM algorithm and a variant known as gradient EM. Our\nanalysis is divided into two parts: a treatment of these algorithms at the\npopulation level (in the limit of infinite data), followed by results that\napply to updates based on a finite set of samples. First, we characterize the\ndomain of attraction of any global maximizer of the population likelihood. This\ncharacterization is based on a novel view of the EM updates as a perturbed form\nof likelihood ascent, or in parallel, of the gradient EM updates as a perturbed\nform of standard gradient ascent. Leveraging this characterization, we then\nprovide non-asymptotic guarantees on the EM and gradient EM algorithms when\napplied to a finite set of samples. We develop consequences of our general\ntheory for three canonical examples of incomplete-data problems: mixture of\nGaussians, mixture of regressions, and linear regression with covariates\nmissing completely at random. In each case, our theory guarantees that with a\nsuitable initialization, a relatively small number of EM (or gradient EM) steps\nwill yield (with high probability) an estimate that is within statistical error\nof the MLE. We provide simulations to confirm this theoretically predicted\nbehavior. \n\n"}
{"id": "1408.5352", "contents": "Title: Nonconvex Statistical Optimization: Minimax-Optimal Sparse PCA in\n  Polynomial Time Abstract: Sparse principal component analysis (PCA) involves nonconvex optimization for\nwhich the global solution is hard to obtain. To address this issue, one popular\napproach is convex relaxation. However, such an approach may produce suboptimal\nestimators due to the relaxation effect. To optimally estimate sparse principal\nsubspaces, we propose a two-stage computational framework named \"tighten after\nrelax\": Within the 'relax' stage, we approximately solve a convex relaxation of\nsparse PCA with early stopping to obtain a desired initial estimator; For the\n'tighten' stage, we propose a novel algorithm called sparse orthogonal\niteration pursuit (SOAP), which iteratively refines the initial estimator by\ndirectly solving the underlying nonconvex problem. A key concept of this\ntwo-stage framework is the basin of attraction. It represents a local region\nwithin which the `tighten' stage has desired computational and statistical\nguarantees. We prove that, the initial estimator obtained from the 'relax'\nstage falls into such a region, and hence SOAP geometrically converges to a\nprincipal subspace estimator which is minimax-optimal within a certain model\nclass. Unlike most existing sparse PCA estimators, our approach applies to the\nnon-spiked covariance models, and adapts to non-Gaussianity as well as\ndependent data settings. Moreover, through analyzing the computational\ncomplexity of the two stages, we illustrate an interesting phenomenon that\nlarger sample size can reduce the total iteration complexity. Our framework\nmotivates a general paradigm for solving many complex statistical problems\nwhich involve nonconvex optimization with provable guarantees. \n\n"}
{"id": "1409.1259", "contents": "Title: On the Properties of Neural Machine Translation: Encoder-Decoder\n  Approaches Abstract: Neural machine translation is a relatively new approach to statistical\nmachine translation based purely on neural networks. The neural machine\ntranslation models often consist of an encoder and a decoder. The encoder\nextracts a fixed-length representation from a variable-length input sentence,\nand the decoder generates a correct translation from this representation. In\nthis paper, we focus on analyzing the properties of the neural machine\ntranslation using two models; RNN Encoder--Decoder and a newly proposed gated\nrecursive convolutional neural network. We show that the neural machine\ntranslation performs relatively well on short sentences without unknown words,\nbut its performance degrades rapidly as the length of the sentence and the\nnumber of unknown words increase. Furthermore, we find that the proposed gated\nrecursive convolutional network learns a grammatical structure of a sentence\nautomatically. \n\n"}
{"id": "1409.5023", "contents": "Title: On the Suita conjecture for some convex ellipsoids in $\\mathbb C^2$ Abstract: It has been recently shown that for a convex domain $\\Omega$ in $\\mathbb C^n$\nand $w\\in\\Omega$ the function\n$F_\\Omega(w):=\\big(K_\\Omega(w)\\lambda(I_\\Omega(w))\\big)^{1/n}$, where\n$K_\\Omega$ is the Bergman kernel on the diagonal and $I_\\Omega(w)$ the\nKobayashi indicatrix, satisfies $1\\leq F_\\Omega\\leq 4$. While the lower bound\nis optimal, not much more is known about the upper bound. In general it is\nquite difficult to compute $F_\\Omega$ even numerically and the highest value of\nit obtained so far is $1.010182\\dots$ In this paper we present precise,\nalthough rather complicated formulas for the ellipsoids\n$\\Omega=\\{|z_1|^{2m}+|z_2|^2<1\\}$ (with $m\\geq 1/2$) and all $w$, as well as\nfor $\\Omega=\\{|z_1|+|z_2|<1\\}$ and $w$ on the diagonal. The Bergman kernel for\nthose ellipsoids had been known, the main point is to compute the volume of the\nKobayashi indicatrix. It turns out that in the second case the function\n$\\lambda(I_\\Omega(w))$ is not $C^{3,1}$. \n\n"}
{"id": "1409.5391", "contents": "Title: Fused Lasso Additive Model Abstract: We consider the problem of predicting an outcome variable using $p$\ncovariates that are measured on $n$ independent observations, in the setting in\nwhich flexible and interpretable fits are desirable. We propose the fused lasso\nadditive model (FLAM), in which each additive function is estimated to be\npiecewise constant with a small number of adaptively-chosen knots. FLAM is the\nsolution to a convex optimization problem, for which a simple algorithm with\nguaranteed convergence to the global optimum is provided. FLAM is shown to be\nconsistent in high dimensions, and an unbiased estimator of its degrees of\nfreedom is proposed. We evaluate the performance of FLAM in a simulation study\nand on two data sets. \n\n"}
{"id": "1410.0518", "contents": "Title: Thin Sequences and Their Role in Model Spaces and Douglas Algebras Abstract: We study thin interpolating sequences $\\{\\lambda_n\\}$ and their relationship\nto interpolation in the Hardy space $H^2$ and the model spaces $K_\\Theta = H^2\n\\ominus \\Theta H^2$, where $\\Theta$ is an inner function. Our results, phrased\nin terms of the functions that do the interpolation as well as Carleson\nmeasures, show that under the assumption that $\\Theta(\\lambda_n) \\to 0$ the\ninterpolation properties in $H^2$ are essentially the same as those in\n$K_\\Theta$. \n\n"}
{"id": "1410.3517", "contents": "Title: Convex Modeling of Interactions with Strong Heredity Abstract: We consider the task of fitting a regression model involving interactions\namong a potentially large set of covariates, in which we wish to enforce strong\nheredity. We propose FAMILY, a very general framework for this task. Our\nproposal is a generalization of several existing methods, such as VANISH\n[Radchenko and James, 2010], hierNet [Bien et al., 2013], the all-pairs lasso,\nand the lasso using only main effects. It can be formulated as the solution to\na convex optimization problem, which we solve using an efficient alternating\ndirections method of multipliers (ADMM) algorithm. This algorithm has\nguaranteed convergence to the global optimum, can be easily specialized to any\nconvex penalty function of interest, and allows for a straightforward extension\nto the setting of generalized linear models. We derive an unbiased estimator of\nthe degrees of freedom of FAMILY, and explore its performance in a simulation\nstudy and on an HIV sequence data set. \n\n"}
{"id": "1410.4744", "contents": "Title: mS2GD: Mini-Batch Semi-Stochastic Gradient Descent in the Proximal\n  Setting Abstract: We propose a mini-batching scheme for improving the theoretical complexity\nand practical performance of semi-stochastic gradient descent applied to the\nproblem of minimizing a strongly convex composite function represented as the\nsum of an average of a large number of smooth convex functions, and simple\nnonsmooth convex function. Our method first performs a deterministic step\n(computation of the gradient of the objective function at the starting point),\nfollowed by a large number of stochastic steps. The process is repeated a few\ntimes with the last iterate becoming the new starting point. The novelty of our\nmethod is in introduction of mini-batching into the computation of stochastic\nsteps. In each step, instead of choosing a single function, we sample $b$\nfunctions, compute their gradients, and compute the direction based on this. We\nanalyze the complexity of the method and show that the method benefits from two\nspeedup effects. First, we prove that as long as $b$ is below a certain\nthreshold, we can reach predefined accuracy with less overall work than without\nmini-batching. Second, our mini-batching scheme admits a simple parallel\nimplementation, and hence is suitable for further acceleration by\nparallelization. \n\n"}
{"id": "1410.7876", "contents": "Title: Collaborative Multi-sensor Classification via Sparsity-based\n  Representation Abstract: In this paper, we propose a general collaborative sparse representation\nframework for multi-sensor classification, which takes into account the\ncorrelations as well as complementary information between heterogeneous sensors\nsimultaneously while considering joint sparsity within each sensor's\nobservations. We also robustify our models to deal with the presence of sparse\nnoise and low-rank interference signals. Specifically, we demonstrate that\nincorporating the noise or interference signal as a low-rank component in our\nmodels is essential in a multi-sensor classification problem when multiple\nco-located sources/sensors simultaneously record the same physical event. We\nfurther extend our frameworks to kernelized models which rely on sparsely\nrepresenting a test sample in terms of all the training samples in a feature\nspace induced by a kernel function. A fast and efficient algorithm based on\nalternative direction method is proposed where its convergence to an optimal\nsolution is guaranteed. Extensive experiments are conducted on several real\nmulti-sensor data sets and results are compared with the conventional\nclassifiers to verify the effectiveness of the proposed methods. \n\n"}
{"id": "1410.8288", "contents": "Title: Additivity of the approximation functional of currents induced by\n  Bergman kernels Abstract: In this note, we give a positive answer to a question raised by Jean-Pierre\nDemailly in 2013, and show the additivity of the approximation functional of\nclosed positive $(1,1)$-currents induced by Bergman kernels. \n\n"}
{"id": "1410.8570", "contents": "Title: A Partially Linear Framework for Massive Heterogeneous Data Abstract: We consider a partially linear framework for modelling massive heterogeneous\ndata. The major goal is to extract common features across all sub-populations\nwhile exploring heterogeneity of each sub-population. In particular, we propose\nan aggregation type estimator for the commonality parameter that possesses the\n(non-asymptotic) minimax optimal bound and asymptotic distribution as if there\nwere no heterogeneity. This oracular result holds when the number of\nsub-populations does not grow too fast. A plug-in estimator for the\nheterogeneity parameter is further constructed, and shown to possess the\nasymptotic distribution as if the commonality information were available. We\nalso test the heterogeneity among a large number of sub-populations. All the\nabove results require to regularize each sub-estimation as though it had the\nentire sample size. Our general theory applies to the divide-and-conquer\napproach that is often used to deal with massive homogeneous data. A technical\nby-product of this paper is the statistical inferences for the general kernel\nridge regression. Thorough numerical results are also provided to back up our\ntheory. \n\n"}
{"id": "1411.0972", "contents": "Title: Convex Optimization for Big Data Abstract: This article reviews recent advances in convex optimization algorithms for\nBig Data, which aim to reduce the computational, storage, and communications\nbottlenecks. We provide an overview of this emerging field, describe\ncontemporary approximation techniques like first-order methods and\nrandomization for scalability, and survey the important role of parallel and\ndistributed computation. The new Big Data algorithms are based on surprisingly\nsimple principles and attain staggering accelerations even on classical\nproblems. \n\n"}
{"id": "1411.1437", "contents": "Title: Higher criticism: $p$-values and criticism Abstract: This paper compares the higher criticism statistic (Donoho and Jin [Ann.\nStatist. 32 (2004) 962-994]), a modification of the higher criticism statistic\nalso suggested by Donoho and Jin, and two statistics of the Berk-Jones [Z.\nWahrsch. Verw. Gebiete 47 (1979) 47-59] type. New approximations to the\nsignificance levels of the statistics are derived, and their accuracy is\nstudied by simulations. By numerical examples it is shown that over a broad\nrange of sample sizes the Berk-Jones statistics have a better power function\nthan the higher criticism statistics to detect sparse mixtures. The\napplications suggested by Meinshausen and Rice [Ann. Statist. 34 (2006)\n373-393], to find lower confidence bounds for the number of false hypotheses,\nand by Jeng, Cai and Li [Biometrika 100 (2013) 157-172], to detect copy number\nvariants, are also studied. \n\n"}
{"id": "1411.1804", "contents": "Title: Beta Process Non-negative Matrix Factorization with Stochastic\n  Structured Mean-Field Variational Inference Abstract: Beta process is the standard nonparametric Bayesian prior for latent factor\nmodel. In this paper, we derive a structured mean-field variational inference\nalgorithm for a beta process non-negative matrix factorization (NMF) model with\nPoisson likelihood. Unlike the linear Gaussian model, which is well-studied in\nthe nonparametric Bayesian literature, NMF model with beta process prior does\nnot enjoy the conjugacy. We leverage the recently developed stochastic\nstructured mean-field variational inference to relax the conjugacy constraint\nand restore the dependencies among the latent variables in the approximating\nvariational distribution. Preliminary results on both synthetic and real\nexamples demonstrate that the proposed inference algorithm can reasonably\nrecover the hidden structure of the data. \n\n"}
{"id": "1411.1805", "contents": "Title: Faithful Variable Screening for High-Dimensional Convex Regression Abstract: We study the problem of variable selection in convex nonparametric\nregression. Under the assumption that the true regression function is convex\nand sparse, we develop a screening procedure to select a subset of variables\nthat contains the relevant variables. Our approach is a two-stage quadratic\nprogramming method that estimates a sum of one-dimensional convex functions,\nfollowed by one-dimensional concave regression fits on the residuals. In\ncontrast to previous methods for sparse additive models, the optimization is\nfinite dimensional and requires no tuning parameters for smoothness. Under\nappropriate assumptions, we prove that the procedure is faithful in the\npopulation setting, yielding no false negatives. We give a finite sample\nstatistical analysis, and introduce algorithms for efficiently carrying out the\nrequired quadratic programs. The approach leads to computational and\nstatistical advantages over fitting a full model, and provides an effective,\npractical approach to variable screening in convex regression. \n\n"}
{"id": "1411.4357", "contents": "Title: Sketching as a Tool for Numerical Linear Algebra Abstract: This survey highlights the recent advances in algorithms for numerical linear\nalgebra that have come from the technique of linear sketching, whereby given a\nmatrix, one first compresses it to a much smaller matrix by multiplying it by a\n(usually) random matrix with certain properties. Much of the expensive\ncomputation can then be performed on the smaller matrix, thereby accelerating\nthe solution for the original problem. In this survey we consider least squares\nas well as robust regression problems, low rank approximation, and graph\nsparsification. We also discuss a number of variants of these problems.\nFinally, we discuss the limitations of sketching methods. \n\n"}
{"id": "1411.4512", "contents": "Title: Maximization of Extractable Randomness in a Quantum Random-Number\n  Generator Abstract: The generation of random numbers via quantum processes is an efficient and\nreliable method to obtain true indeterministic random numbers that are of vital\nimportance to cryptographic communication and large-scale computer modeling.\nHowever, in realistic scenarios, the raw output of a quantum random-number\ngenerator is inevitably tainted by classical technical noise. The integrity of\nthe device can be compromised if this noise is tampered with, or even\ncontrolled by some malicious party. To safeguard against this, we propose and\nexperimentally demonstrate an approach that produces side-information\nindependent randomness that is quantified by min-entropy conditioned on this\nclassical noise. We present a method for maximizing the conditional min-entropy\nof the number sequence generated from a given quantum-to-classical-noise ratio.\nThe detected photocurrent in our experiment is shown to have a real-time\nrandom-number generation rate of 14 (Mbit/s)/MHz. The spectral response of the\ndetection system shows the potential to deliver more than 70 Gbit/s of random\nnumbers in our experimental setup. \n\n"}
{"id": "1411.5733", "contents": "Title: Fractal tube formulas and a Minkowski measurability criterion for\n  compact subsets of Euclidean spaces Abstract: We establish pointwise and distributional fractal tube formulas for a large\nclass of compact subsets of Euclidean spaces of arbitrary dimensions. These\nformulas are expressed as sums of residues of suitable meromorphic functions\nover the complex dimensions of the compact set under consideration (i.e., over\nthe poles of its fractal zeta function). Our results generalize to higher\ndimensions (and in a significant way) the corresponding ones previously\nobtained for fractal strings by the first author and van Frankenhuijsen. They\nare illustrated by several examples and applied to yield a new Minkowski\nmeasurability criterion. \n\n"}
{"id": "1411.7405", "contents": "Title: A note relating ridge regression and OLS p-values to preconditioned\n  sparse penalized regression Abstract: When the design matrix has orthonormal columns, \"soft thresholding\" the\nordinary least squares (OLS) solution produces the Lasso solution [Tibshirani,\n1996]. If one uses the Puffer preconditioned Lasso [Jia and Rohe, 2012], then\nthis result generalizes from orthonormal designs to full rank designs (Theorem\n1). Theorem 2 refines the Puffer preconditioner to make the Lasso select the\nsame model as removing the elements of the OLS solution with the largest\np-values. Using a generalized Puffer preconditioner, Theorem 3 relates ridge\nregression to the preconditioned Lasso; this result is for the high dimensional\nsetting, p > n. Where the standard Lasso is akin to forward selection [Efron et\nal., 2004], Theorems 1, 2, and 3 suggest that the preconditioned Lasso is more\nakin to backward elimination. These results hold for sparse penalties beyond\nl1; for a broad class of sparse and non-convex techniques (e.g. SCAD and MC+),\nthe results hold for all local minima. \n\n"}
{"id": "1411.7783", "contents": "Title: From neural PCA to deep unsupervised learning Abstract: A network supporting deep unsupervised learning is presented. The network is\nan autoencoder with lateral shortcut connections from the encoder to decoder at\neach level of the hierarchy. The lateral shortcut connections allow the higher\nlevels of the hierarchy to focus on abstract invariant features. While standard\nautoencoders are analogous to latent variable models with a single layer of\nstochastic variables, the proposed network is analogous to hierarchical latent\nvariables models. Learning combines denoising autoencoder and denoising sources\nseparation frameworks. Each layer of the network contributes to the cost\nfunction a term which measures the distance of the representations produced by\nthe encoder and the decoder. Since training signals originate from all levels\nof the network, all layers can learn efficiently even in deep networks. The\nspeedup offered by cost terms from higher levels of the hierarchy and the\nability to learn invariant features are demonstrated in experiments. \n\n"}
{"id": "1412.1426", "contents": "Title: Alpha invariants and coercivity of the Mabuchi functional on Fano\n  manifolds Abstract: We give a criterion for the coercivity of the Mabuchi functional for general\nK\\\"ahler classes on Fano manifolds in terms of Tian's alpha invariant. This\ngeneralises a result of Tian in the anti-canonical case implying the existence\nof a K\\\"ahler-Einstein metric. We also prove the alpha invariant is a\ncontinuous function on the K\\\"ahler cone. As an application, we provide new\nK\\\"ahler classes on a general degree one del Pezzo surface for which the\nMabuchi functional is coercive. \n\n"}
{"id": "1412.1927", "contents": "Title: Quantile universal threshold: model selection at the detection edge for\n  high-dimensional linear regression Abstract: To estimate a sparse linear model from data with Gaussian noise, consilience\nfrom lasso and compressed sensing literatures is that thresholding estimators\nlike lasso and the Dantzig selector have the ability in some situations to\nidentify with high probability part of the significant covariates\nasymptotically, and are numerically tractable thanks to convexity.\n  Yet, the selection of a threshold parameter $\\lambda$ remains crucial in\npractice. To that aim we propose Quantile Universal Thresholding, a selection\nof $\\lambda$ at the detection edge. We show with extensive simulations and real\ndata that an excellent compromise between high true positive rate and low false\ndiscovery rate is achieved, leading also to good predictive risk. \n\n"}
{"id": "1412.2863", "contents": "Title: Score Function Features for Discriminative Learning: Matrix and Tensor\n  Framework Abstract: Feature learning forms the cornerstone for tackling challenging learning\nproblems in domains such as speech, computer vision and natural language\nprocessing. In this paper, we consider a novel class of matrix and\ntensor-valued features, which can be pre-trained using unlabeled samples. We\npresent efficient algorithms for extracting discriminative information, given\nthese pre-trained features and labeled samples for any related task. Our class\nof features are based on higher-order score functions, which capture local\nvariations in the probability density function of the input. We establish a\ntheoretical framework to characterize the nature of discriminative information\nthat can be extracted from score-function features, when used in conjunction\nwith labeled samples. We employ efficient spectral decomposition algorithms (on\nmatrices and tensors) for extracting discriminative components. The advantage\nof employing tensor-valued features is that we can extract richer\ndiscriminative information in the form of an overcomplete representations.\nThus, we present a novel framework for employing generative models of the input\nfor discriminative learning. \n\n"}
{"id": "1412.4182", "contents": "Title: The Statistics of Streaming Sparse Regression Abstract: We present a sparse analogue to stochastic gradient descent that is\nguaranteed to perform well under similar conditions to the lasso. In the linear\nregression setup with irrepresentable noise features, our algorithm recovers\nthe support set of the optimal parameter vector with high probability, and\nachieves a statistically quasi-optimal rate of convergence of Op(k log(d)/T),\nwhere k is the sparsity of the solution, d is the number of features, and T is\nthe number of training examples. Meanwhile, our algorithm does not require any\nmore computational resources than stochastic gradient descent. In our\nexperiments, we find that our method substantially out-performs existing\nstreaming algorithms on both real and simulated data. \n\n"}
{"id": "1412.5604", "contents": "Title: Matrix product operators for symmetry-protected topological phases:\n  Gauging and edge theories Abstract: Projected entangled pair states (PEPS) provide a natural ansatz for the\nground states of gapped, local Hamiltonians in which global characteristics of\na quantum state are encoded in properties of local tensors. We develop a\nframework to describe on-site symmetries, as occurring in systems exhibiting\nsymmetry-protected topological (SPT) quantum order, in terms of virtual\nsymmetries of the local tensors expressed as a set of matrix product operators\n(MPOs) labeled by distinct group elements. These MPOs describe the possibly\nanomalous symmetry of the edge theory, whose local degrees of freedom are\nconcretely identified in a PEPS. A classification of SPT phases is obtained by\nstudying the obstructions to continuously deforming one set of MPOs into\nanother, recovering the results derived for fixed-point models [X. Chen et al.,\nPhys. Rev. B 87, 155114 (2013)]. Our formalism accommodates perturbations away\nfrom fixed point models, opening the possibility of studying phase transitions\nbetween different SPT phases. We also demonstrate that applying the recently\ndeveloped quantum state gauging procedure to a SPT PEPS yields a PEPS with\ntopological order determined by the initial symmetry MPOs. The MPO framework\nthus unifies the different approaches to classifying SPT phases, via\nfixed-points models, boundary anomalies, or gauging the symmetry, into the\nsingle problem of classifying inequivalent sets of matrix product operator\nsymmetries that are defined purely in terms of a PEPS. \n\n"}
{"id": "1412.5903", "contents": "Title: Deep Structured Output Learning for Unconstrained Text Recognition Abstract: We develop a representation suitable for the unconstrained recognition of\nwords in natural images: the general case of no fixed lexicon and unknown\nlength.\n  To this end we propose a convolutional neural network (CNN) based\narchitecture which incorporates a Conditional Random Field (CRF) graphical\nmodel, taking the whole word image as a single input. The unaries of the CRF\nare provided by a CNN that predicts characters at each position of the output,\nwhile higher order terms are provided by another CNN that detects the presence\nof N-grams. We show that this entire model (CRF, character predictor, N-gram\npredictor) can be jointly optimised by back-propagating the structured output\nloss, essentially requiring the system to perform multi-task learning, and\ntraining uses purely synthetically generated data. The resulting model is a\nmore accurate system on standard real-world text recognition benchmarks than\ncharacter prediction alone, setting a benchmark for systems that have not been\ntrained on a particular lexicon. In addition, our model achieves\nstate-of-the-art accuracy in lexicon-constrained scenarios, without being\nspecifically modelled for constrained recognition. To test the generalisation\nof our model, we also perform experiments with random alpha-numeric strings to\nevaluate the method when no visual language model is applicable. \n\n"}
{"id": "1412.6111", "contents": "Title: Quantum computation speedup limits from quantum metrological precision\n  bounds Abstract: We propose a scheme for translating metrological precision bounds into lower\nbounds on query complexity of quantum search algorithms. Within the scheme the\nlink between quadratic performance enhancement in idealized quantum\nmetrological and quantum computing schemes becomes clear. More importantly, we\nutilize results from the field of quantum metrology on a generic loss of\nquadratic quantum precision enhancement in presence of decoherence to infer an\nanalogous generic loss of quadratic speed-up in oracle based quantum computing.\nWhile most of our reasoning is rigorous, at one of the final steps, we need to\nmake use of an unproven technical conjecture. We hope that we will be able to\namend this deficiency in the near future, but we are convinced that even\nwithout the conjecture proven our results provide a novel and deep insight into\nrelationship between quantum algorithms and quantum metrology protocols. \n\n"}
{"id": "1412.6418", "contents": "Title: Inducing Semantic Representation from Text by Jointly Predicting and\n  Factorizing Relations Abstract: In this work, we propose a new method to integrate two recent lines of work:\nunsupervised induction of shallow semantics (e.g., semantic roles) and\nfactorization of relations in text and knowledge bases. Our model consists of\ntwo components: (1) an encoding component: a semantic role labeling model which\npredicts roles given a rich set of syntactic and lexical features; (2) a\nreconstruction component: a tensor factorization model which relies on roles to\npredict argument fillers. When the components are estimated jointly to minimize\nerrors in argument reconstruction, the induced roles largely correspond to\nroles defined in annotated resources. Our method performs on par with most\naccurate role induction methods on English, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguage. \n\n"}
{"id": "1412.8697", "contents": "Title: On Semiparametric Exponential Family Graphical Models Abstract: We propose a new class of semiparametric exponential family graphical models\nfor the analysis of high dimensional mixed data. Different from the existing\nmixed graphical models, we allow the nodewise conditional distributions to be\nsemiparametric generalized linear models with unspecified base measure\nfunctions. Thus, one advantage of our method is that it is unnecessary to\nspecify the type of each node and the method is more convenient to apply in\npractice. Under the proposed model, we consider both problems of parameter\nestimation and hypothesis testing in high dimensions. In particular, we propose\na symmetric pairwise score test for the presence of a single edge in the graph.\nCompared to the existing methods for hypothesis tests, our approach takes into\naccount of the symmetry of the parameters, such that the inferential results\nare invariant with respect to the different parametrizations of the same edge.\nThorough numerical simulations and a real data example are provided to back up\nour results. \n\n"}
{"id": "1412.8724", "contents": "Title: A General Framework for Robust Testing and Confidence Regions in\n  High-Dimensional Quantile Regression Abstract: We propose a robust inferential procedure for assessing uncertainties of\nparameter estimation in high-dimensional linear models, where the dimension $p$\ncan grow exponentially fast with the sample size $n$. Our method combines the\nde-biasing technique with the composite quantile function to construct an\nestimator that is asymptotically normal. Hence it can be used to construct\nvalid confidence intervals and conduct hypothesis tests. Our estimator is\nrobust and does not require the existence of first or second moment of the\nnoise distribution. It also preserves efficiency in the sense that the worst\ncase efficiency loss is less than 30\\% compared to the square-loss-based\nde-biased Lasso estimator. In many cases our estimator is close to or better\nthan the latter, especially when the noise is heavy-tailed. Our de-biasing\nprocedure does not require solving the $L_1$-penalized composite quantile\nregression. Instead, it allows for any first-stage estimator with desired\nconvergence rate and empirical sparsity. The paper also provides new proof\ntechniques for developing theoretical guarantees of inferential procedures with\nnon-smooth loss functions. To establish the main results, we exploit the local\ncurvature of the conditional expectation of composite quantile loss and apply\nempirical process theories to control the difference between empirical\nquantities and their conditional expectations. Our results are established\nunder weaker assumptions compared to existing work on inference for\nhigh-dimensional quantile regression. Furthermore, we consider a\nhigh-dimensional simultaneous test for the regression parameters by applying\nthe Gaussian approximation and multiplier bootstrap theories. We also study\ndistributed learning and exploit the divide-and-conquer estimator to reduce\ncomputation complexity when the sample size is massive. Finally, we provide\nempirical results to verify the theory. \n\n"}
{"id": "1412.8765", "contents": "Title: A General Theory of Hypothesis Tests and Confidence Regions for Sparse\n  High Dimensional Models Abstract: We consider the problem of uncertainty assessment for low dimensional\ncomponents in high dimensional models. Specifically, we propose a decorrelated\nscore function to handle the impact of high dimensional nuisance parameters. We\nconsider both hypothesis tests and confidence regions for generic penalized\nM-estimators. Unlike most existing inferential methods which are tailored for\nindividual models, our approach provides a general framework for high\ndimensional inference and is applicable to a wide range of applications. From\nthe testing perspective, we develop general theorems to characterize the\nlimiting distributions of the decorrelated score test statistic under both null\nhypothesis and local alternatives. These results provide asymptotic guarantees\non the type I errors and local powers of the proposed test. Furthermore, we\nshow that the decorrelated score function can be used to construct point and\nconfidence region estimators that are semiparametrically efficient. We also\ngeneralize this framework to broaden its applications. First, we extend it to\nhandle high dimensional null hypothesis, where the number of parameters of\ninterest can increase exponentially fast with the sample size. Second, we\nestablish the theory for model misspecification. Third, we go beyond the\nlikelihood framework, by introducing the generalized score test based on\ngeneral loss functions. Thorough numerical studies are conducted to back up the\ndeveloped theoretical results. \n\n"}
{"id": "1501.02176", "contents": "Title: Fatou flowers and parabolic curves Abstract: In this survey we shall collect the main results known up to now (July 2015)\nregarding possible generalizations to several complex variables of the\nclassical Leau-Fatou flower theorem in holomorphic parabolic dynamics. \n\n"}
{"id": "1501.03514", "contents": "Title: Phases of d-orbital bosons in optical lattices Abstract: We explore the properties of bosonic atoms loaded into the d bands of an\nisotropic square optical lattice. Following the recent experimental success\nreported in [Y. Zhai et al., Phys. Rev. A 87, 063638 (2013)], in which\npopulating d bands with a 99% fidelity was demonstrated, we present a\ntheoretical study of the possible phases that can appear in this system. Using\nthe Gutzwiller ansatz for the three d band orbitals we map the boundaries of\nthe Mott insulating phases. For not too large occupation, two of the orbitals\nare predominantly occupied, while the third, of a slightly higher energy,\nremains almost unpopulated. In this regime, in the superfluid phase we find the\nformation of a vortex lattice, where the vortices come in vortex/anti-vortex\npairs with two pairs locked to every site. Due to the orientation of the\nvortices time-reversal symmetry is spontaneously broken. This state also breaks\na discrete Z2-symmetry. We further derive an effective spin-1/2 model that\ndescribe the relevant physics of the lowest Mott-phase with unit filling. We\nargue that the corresponding two dimensional phase diagram should be rich with\nseveral different phases. We also explain how to generate antisymmetric spin\ninteractions that can give rise to novel effects like spin canting. \n\n"}
{"id": "1501.03854", "contents": "Title: Understanding Kernel Ridge Regression: Common behaviors from simple\n  functions to density functionals Abstract: Accurate approximations to density functionals have recently been obtained\nvia machine learning (ML). By applying ML to a simple function of one variable\nwithout any random sampling, we extract the qualitative dependence of errors on\nhyperparameters. We find universal features of the behavior in extreme limits,\nincluding both very small and very large length scales, and the noise-free\nlimit. We show how such features arise in ML models of density functionals. \n\n"}
{"id": "1501.05527", "contents": "Title: Matrix-valued Hermitian Positivstellensatz, lurking contractions, and\n  contractive determinantal representations of stable polynomials Abstract: We prove that every matrix-valued rational function $F$, which is regular on\nthe closure of a bounded domain $\\mathcal{D}_\\mathbf{P}$ in $\\mathbb{C}^d$ and\nwhich has the associated Agler norm strictly less than 1, admits a\nfinite-dimensional contractive realization $$F(z)= D +\nC\\mathbf{P}(z)_n(I-A\\mathbf{P}(z)_n)^{-1} B. $$ Here $\\mathcal{D}_\\mathbf{P}$\nis defined by the inequality $\\|\\mathbf{P}(z)\\|<1$, where $\\mathbf{P}(z)$ is a\ndirect sum of matrix polynomials $\\mathbf{P}_i(z)$ (so that appropriate\nArchimedean and approximation conditions are satisfied), and\n$\\mathbf{P}(z)_n=\\bigoplus_{i=1}^k\\mathbf{P}_i(z)\\otimes I_{n_i}$, with some\n$k$-tuple $n$ of multiplicities $n_i$; special cases include the open unit\npolydisk and the classical Cartan domains. The proof uses a matrix-valued\nversion of a Hermitian Positivstellensatz by Putinar, and a lurking contraction\nargument. As a consequence, we show that every polynomial with no zeros on the\nclosure of $\\mathcal{D}_\\mathbf{P}$ is a factor of $\\det (I -\nK\\mathbf{P}(z)_n)$, with a contractive matrix $K$. \n\n"}
{"id": "1501.06195", "contents": "Title: Randomized sketches for kernels: Fast and optimal non-parametric\n  regression Abstract: Kernel ridge regression (KRR) is a standard method for performing\nnon-parametric regression over reproducing kernel Hilbert spaces. Given $n$\nsamples, the time and space complexity of computing the KRR estimate scale as\n$\\mathcal{O}(n^3)$ and $\\mathcal{O}(n^2)$ respectively, and so is prohibitive\nin many cases. We propose approximations of KRR based on $m$-dimensional\nrandomized sketches of the kernel matrix, and study how small the projection\ndimension $m$ can be chosen while still preserving minimax optimality of the\napproximate KRR estimate. For various classes of randomized sketches, including\nthose based on Gaussian and randomized Hadamard matrices, we prove that it\nsuffices to choose the sketch dimension $m$ proportional to the statistical\ndimension (modulo logarithmic factors). Thus, we obtain fast and minimax\noptimal approximations to the KRR estimate for non-parametric regression. \n\n"}
{"id": "1501.06276", "contents": "Title: Prospects for the formation of ultracold polar ground state KCs\n  molecules via an optical process Abstract: Heteronuclear alkali-metal dimers represent the class of molecules of choice\nfor creating samples of ultracold molecules exhibiting an intrinsic large\npermanent electric dipole moment. Among them, the KCs molecule, with a\npermanent dipole moment of 1.92~Debye still remains to be observed in ultracold\nconditions. Based on spectroscopic studies available in the literature\ncompleted by accurate quantum chemistry calculations, we propose several\noptical coherent schemes to create ultracold bosonic and fermionic KCs\nmolecules in their absolute rovibrational ground level, starting from a weakly\nbound level of their electronic ground state manifold. The processes rely on\nthe existence of convenient electronically excited states allowing an efficient\nstimulated Raman adiabatic transfer of the level population. \n\n"}
{"id": "1501.06545", "contents": "Title: Jarzynski equality in $\\mathcal{PT}$-symmetric quantum mechanics Abstract: We show that the quantum Jarzynski equality generalizes to\n$\\mathcal{PT}$-symmetric quantum mechanics with unbroken\n$\\mathcal{PT}$-symmetry. In the regime of broken $\\mathcal{PT}$-symmetry the\nJarzynski equality does not hold as also the $\\mathcal{CPT}$-norm is not\npreserved during the dynamics. These findings are illustrated for an\nexperimentally relevant system -- two coupled optical waveguides. It turns out\nthat for these systems the phase transition between the regimes of unbroken and\nbroken $\\mathcal{PT}$-symmetry is thermodynamically inhibited as the\nirreversible work diverges at the critical point. \n\n"}
{"id": "1501.06955", "contents": "Title: Polynomial automorphisms of C^n preserving the Markoff-Hurwitz\n  polynomial Abstract: We study the action of the group of polynomial automorphisms of C^n (n>2)\nwhich preserve the Markoff-Hurwitz polynomial H(x):= x_1^2 + x_2^2 + ... +\nx_n^2 - x_1 x_2 ... x_n. Our main results include the determination of the\ngroup, the description of a non-empty open subset of C^n on which the group\nacts properly discontinuously (domain of discontinuity), and identities for the\norbit of points in the domain of discontinuity. \n\n"}
{"id": "1501.07098", "contents": "Title: Dynamical many-body phases of the parametrically driven, dissipative\n  Dicke model Abstract: The dissipative Dicke model exhibits a fascinating out-of-equilibrium\nmany-body phase transition as a function of a coupling between a driven\nphotonic cavity and numerous two-level atoms. We study the effect of a\ntime-dependent parametric modulation of this coupling, and discover a rich\nphase diagram as a function of the modulation strength. We find that in\naddition to the established normal and super-radiant phases, a new phase with\npulsed superradiance which we term dynamical normal phase appears when the\nsystem is parametrically driven. Employing different methods, we characterize\nthe different phases and the transitions between them. Specific heed is paid to\nthe role of dissipation in determining the phase boundaries. Our analysis paves\nthe road for the experimental study of dynamically stabilized phases of\ninteracting light and matter. \n\n"}
{"id": "1502.02347", "contents": "Title: Local and Global Inference for High Dimensional Nonparanormal Graphical\n  Models Abstract: This paper proposes a unified framework to quantify local and global\ninferential uncertainty for high dimensional nonparanormal graphical models. In\nparticular, we consider the problems of testing the presence of a single edge\nand constructing a uniform confidence subgraph. Due to the presence of unknown\nmarginal transformations, we propose a pseudo likelihood based inferential\napproach. In sharp contrast to the existing high dimensional score test method,\nour method is free of tuning parameters given an initial estimator, and extends\nthe scope of the existing likelihood based inferential framework. Furthermore,\nwe propose a U-statistic multiplier bootstrap method to construct the\nconfidence subgraph. We show that the constructed subgraph is contained in the\ntrue graph with probability greater than a given nominal level. Compared with\nexisting methods for constructing confidence subgraphs, our method does not\nrely on Gaussian or sub-Gaussian assumptions. The theoretical properties of the\nproposed inferential methods are verified by thorough numerical experiments and\nreal data analysis. \n\n"}
{"id": "1502.03391", "contents": "Title: Fast Embedding for JOFC Using the Raw Stress Criterion Abstract: The Joint Optimization of Fidelity and Commensurability (JOFC) manifold\nmatching methodology embeds an omnibus dissimilarity matrix consisting of\nmultiple dissimilarities on the same set of objects. One approach to this\nembedding optimizes the preservation of fidelity to each individual\ndissimilarity matrix together with commensurability of each given observation\nacross modalities via iterative majorization of a raw stress error criterion by\nsuccessive Guttman transforms. In this paper, we exploit the special structure\ninherent to JOFC to exactly and efficiently compute the successive Guttman\ntransforms, and as a result we are able to greatly speed up the JOFC procedure\nfor both in-sample and out-of-sample embedding. We demonstrate the scalability\nof our implementation on both real and simulated data examples. \n\n"}
{"id": "1502.03508", "contents": "Title: Adding vs. Averaging in Distributed Primal-Dual Optimization Abstract: Distributed optimization methods for large-scale machine learning suffer from\na communication bottleneck. It is difficult to reduce this bottleneck while\nstill efficiently and accurately aggregating partial work from different\nmachines. In this paper, we present a novel generalization of the recent\ncommunication-efficient primal-dual framework (CoCoA) for distributed\noptimization. Our framework, CoCoA+, allows for additive combination of local\nupdates to the global parameters at each iteration, whereas previous schemes\nwith convergence guarantees only allow conservative averaging. We give stronger\n(primal-dual) convergence rate guarantees for both CoCoA as well as our new\nvariants, and generalize the theory for both methods to cover non-smooth convex\nloss functions. We provide an extensive experimental comparison that shows the\nmarkedly improved performance of CoCoA+ on several real-world distributed\ndatasets, especially when scaling up the number of machines. \n\n"}
{"id": "1502.04282", "contents": "Title: Weighted Sobolev regularity of the Bergman projection on the Hartogs\n  triangle Abstract: We prove a weighted Sobolev estimate of the Bergman projection on the Hartogs\ntriangle, where the weight is some power of the distance to the singularity at\nthe boundary. This method also applies to the $n$-dimensional generalization of\nthe Hartogs triangle. \n\n"}
{"id": "1502.06411", "contents": "Title: Additive bounds of minimum output entropies for unital channels and an\n  exact qubit formula Abstract: We investigate minimum output (R\\'enyi) entropy of qubit channels and unital\nquantum channels. We obtain an exact formula for the minimum output entropy of\nqubit channels, and bounds for unital quantum channels. Interestingly, our\nbounds depend only on the operator norm of the matrix representation of the\nchannels on the space of trace-less Hermitian operators. Moreover, since these\nbounds respect tensor products, we get bounds for the capacity of unital\nquantum channels, which is saturated by the Werner-Holevo channel. Furthermore,\nwe construct an orthonormal basis, besides the Gell-Mann basis, for the space\nof trace-less Hermitian operators by using discrete Weyl operators. We apply\nour bounds to discrete Weyl covariant channels with this basis, and find new\nexamples in which the minimum output R\\'enyi $2$-entropy is additive. \n\n"}
{"id": "1502.06590", "contents": "Title: Improved Sum-of-Squares Lower Bounds for Hidden Clique and Hidden\n  Submatrix Problems Abstract: Given a large data matrix $A\\in\\mathbb{R}^{n\\times n}$, we consider the\nproblem of determining whether its entries are i.i.d. with some known marginal\ndistribution $A_{ij}\\sim P_0$, or instead $A$ contains a principal submatrix\n$A_{{\\sf Q},{\\sf Q}}$ whose entries have marginal distribution $A_{ij}\\sim\nP_1\\neq P_0$. As a special case, the hidden (or planted) clique problem\nrequires to find a planted clique in an otherwise uniformly random graph.\n  Assuming unbounded computational resources, this hypothesis testing problem\nis statistically solvable provided $|{\\sf Q}|\\ge C \\log n$ for a suitable\nconstant $C$. However, despite substantial effort, no polynomial time algorithm\nis known that succeeds with high probability when $|{\\sf Q}| = o(\\sqrt{n})$.\nRecently Meka and Wigderson \\cite{meka2013association}, proposed a method to\nestablish lower bounds within the Sum of Squares (SOS) semidefinite hierarchy.\n  Here we consider the degree-$4$ SOS relaxation, and study the construction of\n\\cite{meka2013association} to prove that SOS fails unless $k\\ge C\\,\nn^{1/3}/\\log n$. An argument presented by Barak implies that this lower bound\ncannot be substantially improved unless the witness construction is changed in\nthe proof. Our proof uses the moments method to bound the spectrum of a certain\nrandom association scheme, i.e. a symmetric random matrix whose rows and\ncolumns are indexed by the edges of an Erd\\\"os-Renyi random graph. \n\n"}
{"id": "1502.06895", "contents": "Title: On the consistency theory of high dimensional variable screening Abstract: Variable screening is a fast dimension reduction technique for assisting high\ndimensional feature selection. As a preselection method, it selects a moderate\nsize subset of candidate variables for further refining via feature selection\nto produce the final model. The performance of variable screening depends on\nboth computational efficiency and the ability to dramatically reduce the number\nof variables without discarding the important ones. When the data dimension $p$\nis substantially larger than the sample size $n$, variable screening becomes\ncrucial as 1) Faster feature selection algorithms are needed; 2) Conditions\nguaranteeing selection consistency might fail to hold. This article studies a\nclass of linear screening methods and establishes consistency theory for this\nspecial class. In particular, we prove the restricted diagonally dominant (RDD)\ncondition is a necessary and sufficient condition for strong screening\nconsistency. As concrete examples, we show two screening methods $SIS$ and\n$HOLP$ are both strong screening consistent (subject to additional constraints)\nwith large probability if $n > O((\\rho s + \\sigma/\\tau)^2\\log p)$ under random\ndesigns. In addition, we relate the RDD condition to the irrepresentable\ncondition, and highlight limitations of $SIS$. \n\n"}
{"id": "1502.07641", "contents": "Title: ROCKET: Robust Confidence Intervals via Kendall's Tau for\n  Transelliptical Graphical Models Abstract: Undirected graphical models are used extensively in the biological and social\nsciences to encode a pattern of conditional independences between variables,\nwhere the absence of an edge between two nodes $a$ and $b$ indicates that the\ncorresponding two variables $X_a$ and $X_b$ are believed to be conditionally\nindependent, after controlling for all other measured variables. In the\nGaussian case, conditional independence corresponds to a zero entry in the\nprecision matrix $\\Omega$ (the inverse of the covariance matrix $\\Sigma$). Real\ndata often exhibits heavy tail dependence between variables, which cannot be\ncaptured by the commonly-used Gaussian or nonparanormal (Gaussian copula)\ngraphical models. In this paper, we study the transelliptical model, an\nelliptical copula model that generalizes Gaussian and nonparanormal models to a\nbroader family of distributions. We propose the ROCKET method, which constructs\nan estimator of $\\Omega_{ab}$ that we prove to be asymptotically normal under\nmild assumptions. Empirically, ROCKET outperforms the nonparanormal and\nGaussian models in terms of achieving accurate inference on simulated data. We\nalso compare the three methods on real data (daily stock returns), and find\nthat the ROCKET estimator is the only method whose behavior across subsamples\nagrees with the distribution predicted by the theory. \n\n"}
{"id": "1503.01401", "contents": "Title: Quantifying Uncertainty in Stochastic Models with Parametric Variability Abstract: We present a method to quantify uncertainty in the predictions made by\nsimulations of mathematical models that can be applied to a broad class of\nstochastic, discrete, and differential equation models. Quantifying uncertainty\nis crucial for determining how accurate the model predictions are and\nidentifying which input parameters affect the outputs of interest. Most of the\nexisting methods for uncertainty quantification require many samples to\ngenerate accurate results, are unable to differentiate where the uncertainty is\ncoming from (e.g., parameters or model assumptions), or require a lot of\ncomputational resources. Our approach addresses these challenges and\nopportunities by allowing different types of uncertainty, that is, uncertainty\nin input parameters as well as uncertainty created through stochastic model\ncomponents. This is done by combining the Karhunen-Loeve decomposition,\npolynomial chaos expansion, and Bayesian Gaussian process regression to create\na statistical surrogate for the stochastic model. The surrogate separates the\nanalysis of variation arising through stochastic simulation and variation\narising through uncertainty in the model parameterization. We illustrate our\napproach by quantifying the uncertainty in a stochastic ordinary differential\nequation epidemic model. Specifically, we estimate four quantities of interest\nfor the epidemic model and show agreement between the surrogate and the actual\nmodel results. \n\n"}
{"id": "1503.01445", "contents": "Title: Toxicity Prediction using Deep Learning Abstract: Everyday we are exposed to various chemicals via food additives, cleaning and\ncosmetic products and medicines -- and some of them might be toxic. However\ntesting the toxicity of all existing compounds by biological experiments is\nneither financially nor logistically feasible. Therefore the government\nagencies NIH, EPA and FDA launched the Tox21 Data Challenge within the\n\"Toxicology in the 21st Century\" (Tox21) initiative. The goal of this challenge\nwas to assess the performance of computational methods in predicting the\ntoxicity of chemical compounds. State of the art toxicity prediction methods\nbuild upon specifically-designed chemical descriptors developed over decades.\nThough Deep Learning is new to the field and was never applied to toxicity\nprediction before, it clearly outperformed all other participating methods. In\nthis application paper we show that deep nets automatically learn features\nresembling well-established toxicophores. In total, our Deep Learning approach\nwon both of the panel-challenges (nuclear receptors and stress response) as\nwell as the overall Grand Challenge, and thereby sets a new standard in tox\nprediction. \n\n"}
{"id": "1503.02978", "contents": "Title: Kernel Meets Sieve: Post-Regularization Confidence Bands for Sparse\n  Additive Model Abstract: We develop a novel procedure for constructing confidence bands for components\nof a sparse additive model. Our procedure is based on a new kernel-sieve hybrid\nestimator that combines two most popular nonparametric estimation methods in\nthe literature, the kernel regression and the spline method, and is of interest\nin its own right. Existing methods for fitting sparse additive model are\nprimarily based on sieve estimators, while the literature on confidence bands\nfor nonparametric models are primarily based upon kernel or local polynomial\nestimators. Our kernel-sieve hybrid estimator combines the best of both worlds\nand allows us to provide a simple procedure for constructing confidence bands\nin high-dimensional sparse additive models. We prove that the confidence bands\nare asymptotically honest by studying approximation with a Gaussian process.\nThorough numerical results on both synthetic data and real-world neuroscience\ndata are provided to demonstrate the efficacy of the theory. \n\n"}
{"id": "1503.03613", "contents": "Title: On the Impossibility of Learning the Missing Mass Abstract: This paper shows that one cannot learn the probability of rare events without\nimposing further structural assumptions. The event of interest is that of\nobtaining an outcome outside the coverage of an i.i.d. sample from a discrete\ndistribution. The probability of this event is referred to as the \"missing\nmass\". The impossibility result can then be stated as: the missing mass is not\ndistribution-free PAC-learnable in relative error. The proof is\nsemi-constructive and relies on a coupling argument using a dithered geometric\ndistribution. This result formalizes the folklore that in order to predict rare\nevents, one necessarily needs distributions with \"heavy tails\". \n\n"}
{"id": "1503.04474", "contents": "Title: Statistical Estimation and Clustering of Group-invariant Orientation\n  Parameters Abstract: We treat the problem of estimation of orientation parameters whose values are\ninvariant to transformations from a spherical symmetry group. Previous work has\nshown that any such group-invariant distribution must satisfy a restricted\nfinite mixture representation, which allows the orientation parameter to be\nestimated using an Expectation Maximization (EM) maximum likelihood (ML)\nestimation algorithm. In this paper, we introduce two parametric models for\nthis spherical symmetry group estimation problem: 1) the hyperbolic Von Mises\nFisher (VMF) mixture distribution and 2) the Watson mixture distribution. We\nalso introduce a new EM-ML algorithm for clustering samples that come from\nmixtures of group-invariant distributions with different parameters. We apply\nthe models to the problem of mean crystal orientation estimation under the\nspherically symmetric group associated with the crystal form, e.g., cubic or\noctahedral or hexahedral. Simulations and experiments establish the advantages\nof the extended EM-VMF and EM-Watson estimators for data acquired by Electron\nBackscatter Diffraction (EBSD) microscopy of a polycrystalline Nickel alloy\nsample. \n\n"}
{"id": "1503.05459", "contents": "Title: Hypoelliptic Diffusion Maps I: Tangent Bundles Abstract: We introduce the concept of Hypoelliptic Diffusion Maps (HDM), a framework\ngeneralizing Diffusion Maps in the context of manifold learning and\ndimensionality reduction. Standard non-linear dimensionality reduction methods\n(e.g., LLE, ISOMAP, Laplacian Eigenmaps, Diffusion Maps) focus on mining\nmassive data sets using weighted affinity graphs; Orientable Diffusion Maps and\nVector Diffusion Maps enrich these graphs by attaching to each node also some\nlocal geometry. HDM likewise considers a scenario where each node possesses\nadditional structure, which is now itself of interest to investigate.\nVirtually, HDM augments the original data set with attached structures, and\nprovides tools for studying and organizing the augmented ensemble. The goal is\nto obtain information on individual structures attached to the nodes and on the\nrelationship between structures attached to nearby nodes, so as to study the\nunderlying manifold from which the nodes are sampled. In this paper, we analyze\nHDM on tangent bundles, revealing its intimate connection with sub-Riemannian\ngeometry and a family of hypoelliptic differential operators. In a later paper,\nwe shall consider more general fibre bundles. \n\n"}
{"id": "1503.08701", "contents": "Title: Blow-up analysis of a nonlocal Liouville-type equation Abstract: In this paper we perform a blow-up and quantization analysis of the following\nnonlocal Liouville-type equation \\begin{equation}(-\\Delta)^\\frac12 u= \\kappa\ne^u-1~\\mbox{in $S^1$,} \\end{equation} where $(-\\Delta)^\\frac{1}{2}$ stands for\nthe fractional Laplacian and $\\kappa$ is a bounded function. We interpret the\nabove equation as the prescribed curvature equation to a curve in conformal\nparametrization. We also establish a relation between this equation and the\nanalogous equation in $\\mathbb{R}$ \\begin{equation}\n  (-\\Delta)^\\frac{1}{2} u =Ke^u \\quad \\text{in }\\mathbb{R}, \\end{equation} with\n$K$ bounded on $\\mathbb{R}$. \n\n"}
{"id": "1504.01108", "contents": "Title: Stability Analysis of Matrix Wiener--Hopf Factorisation of\n  Daniele--Khrapkov Class and Reliable Approximate Factorisation Abstract: This paper presents new stability results for matrix Wiener--Hopf\nfactorisation. The first part of the paper examines conditions for stability of\nWiener-Hopf factorisation in Daniele--Khrapkov class. The second part of the\npaper concerns the class of matrix functions which can be exactly or\napproximately reduced to the factorisation of the Daniele--Khrapkov matrices.\nThe results of the paper are demonstrated by numerical examples with partial\nindices \\(\\{1,-1\\}\\), \\(\\{0,0\\}\\) and \\(\\{-1,-1\\}\\). \n\n"}
{"id": "1504.02183", "contents": "Title: Quantum statistics and the performance of engine cycles Abstract: We study the role of quantum statistics in the performance of Otto cycles.\nFirst, we show analytically that the work distributions for bosonic and\nfermionic working fluids are identical for cycles driven by harmonic trapping\npotentials. Subsequently, in the case of non-harmonic potentials, we find that\nthe interplay between different energy level spacings and particle statistics\nstrongly affects the performances of the engine cycle. To demonstrate this, we\nexamine three trapping potentials which induce different (single particle)\nenergy level spacings: monotonically decreasing with the level number,\nmonotonically increasing, and the case in which the level spacing does not vary\nmonotonically. \n\n"}
{"id": "1504.03767", "contents": "Title: $\\mathcal{PT}$-breaking threshold in spatially asymmetric Aubry-Andre\n  Harper models: hidden symmetry and topological states Abstract: Aubry-Andre Harper (AAH) lattice models, characterized by\nreflection-asymmetric, sinusoidally varying nearest-neighbor tunneling profile,\nare well-known for their topological properties. We consider the fate of such\nmodels in the presence of balanced gain and loss potentials $\\pm i\\gamma$\nlocated at reflection-symmetric sites. We predict that these models have a\nfinite $\\mathcal{PT}$ breaking threshold only for {\\it specific locations} of\nthe gain-loss potential, and uncover a hidden symmetry that is instrumental to\nthe finite threshold strength. We also show that the topological edge-states\nremain robust in the $\\mathcal{PT}$-symmetry broken phase. Our predictions\nsubstantially broaden the possible realizations of a $\\mathcal{PT}$-symmetric\nsystem. \n\n"}
{"id": "1504.05487", "contents": "Title: Deep Convolutional Neural Networks Based on Semi-Discrete Frames Abstract: Deep convolutional neural networks have led to breakthrough results in\npractical feature extraction applications. The mathematical analysis of these\nnetworks was pioneered by Mallat, 2012. Specifically, Mallat considered\nso-called scattering networks based on identical semi-discrete wavelet frames\nin each network layer, and proved translation-invariance as well as deformation\nstability of the resulting feature extractor. The purpose of this paper is to\ndevelop Mallat's theory further by allowing for different and, most\nimportantly, general semi-discrete frames (such as, e.g., Gabor frames,\nwavelets, curvelets, shearlets, ridgelets) in distinct network layers. This\nallows to extract wider classes of features than point singularities resolved\nby the wavelet transform. Our generalized feature extractor is proven to be\ntranslation-invariant, and we develop deformation stability results for a\nlarger class of deformations than those considered by Mallat. For Mallat's\nwavelet-based feature extractor, we get rid of a number of technical\nconditions. The mathematical engine behind our results is continuous frame\ntheory, which allows us to completely detach the invariance and deformation\nstability proofs from the particular algebraic structure of the underlying\nframes. \n\n"}
{"id": "1504.07460", "contents": "Title: Identifying Reliable Annotations for Large Scale Image Segmentation Abstract: Challenging computer vision tasks, in particular semantic image segmentation,\nrequire large training sets of annotated images. While obtaining the actual\nimages is often unproblematic, creating the necessary annotation is a tedious\nand costly process. Therefore, one often has to work with unreliable annotation\nsources, such as Amazon Mechanical Turk or (semi-)automatic algorithmic\ntechniques. In this work, we present a Gaussian process (GP) based technique\nfor simultaneously identifying which images of a training set have unreliable\nannotation and learning a segmentation model in which the negative effect of\nthese images is suppressed. Alternatively, the model can also just be used to\nidentify the most reliably annotated images from the training set, which can\nthen be used for training any other segmentation method. By relying on \"deep\nfeatures\" in combination with a linear covariance function, our GP can be\nlearned and its hyperparameter determined efficiently using only matrix\noperations and gradient-based optimization. This makes our method scalable even\nto large datasets with several million training instances. \n\n"}
{"id": "1505.00477", "contents": "Title: Kernel Spectral Clustering and applications Abstract: In this chapter we review the main literature related to kernel spectral\nclustering (KSC), an approach to clustering cast within a kernel-based\noptimization setting. KSC represents a least-squares support vector machine\nbased formulation of spectral clustering described by a weighted kernel PCA\nobjective. Just as in the classifier case, the binary clustering model is\nexpressed by a hyperplane in a high dimensional space induced by a kernel. In\naddition, the multi-way clustering can be obtained by combining a set of binary\ndecision functions via an Error Correcting Output Codes (ECOC) encoding scheme.\nBecause of its model-based nature, the KSC method encompasses three main steps:\ntraining, validation, testing. In the validation stage model selection is\nperformed to obtain tuning parameters, like the number of clusters present in\nthe data. This is a major advantage compared to classical spectral clustering\nwhere the determination of the clustering parameters is unclear and relies on\nheuristics. Once a KSC model is trained on a small subset of the entire data,\nit is able to generalize well to unseen test points. Beyond the basic\nformulation, sparse KSC algorithms based on the Incomplete Cholesky\nDecomposition (ICD) and $L_0$, $L_1, L_0 + L_1$, Group Lasso regularization are\nreviewed. In that respect, we show how it is possible to handle large scale\ndata. Also, two possible ways to perform hierarchical clustering and a soft\nclustering method are presented. Finally, real-world applications such as image\nsegmentation, power load time-series clustering, document clustering and big\ndata learning are considered. \n\n"}
{"id": "1505.00482", "contents": "Title: Risk Bounds For Mode Clustering Abstract: Density mode clustering is a nonparametric clustering method. The clusters\nare the basins of attraction of the modes of a density estimator. We study the\nrisk of mode-based clustering. We show that the clustering risk over the\ncluster cores --- the regions where the density is high --- is very small even\nin high dimensions. And under a low noise condition, the overall cluster risk\nis small even beyond the cores, in high dimensions. \n\n"}
{"id": "1505.02009", "contents": "Title: Synchronization and Phase Noise Reduction in Micromechanical Oscillators\n  Arrays Coupled through Light Abstract: Synchronization of many coupled oscillators is widely found in nature and has\nthe potential to revolutionize timing technologies. Here we demonstrate\nsynchronization in arrays of silicon nitride micromechanical oscillators\ncoupled purely through optical radiation field. We show that the phase noise of\nthe synchronized oscillators can be improved by almost 10 dB below the phase\nnoise limit for each individual oscillator. These results open a practical\nroute towards synchronized oscillator networks. \n\n"}
{"id": "1505.02250", "contents": "Title: Newton Sketch: A Linear-time Optimization Algorithm with\n  Linear-Quadratic Convergence Abstract: We propose a randomized second-order method for optimization known as the\nNewton Sketch: it is based on performing an approximate Newton step using a\nrandomly projected or sub-sampled Hessian. For self-concordant functions, we\nprove that the algorithm has super-linear convergence with exponentially high\nprobability, with convergence and complexity guarantees that are independent of\ncondition numbers and related problem-dependent quantities. Given a suitable\ninitialization, similar guarantees also hold for strongly convex and smooth\nobjectives without self-concordance. When implemented using randomized\nprojections based on a sub-sampled Hadamard basis, the algorithm typically has\nsubstantially lower complexity than Newton's method. We also describe\nextensions of our methods to programs involving convex constraints that are\nequipped with self-concordant barriers. We discuss and illustrate applications\nto linear programs, quadratic programs with convex constraints, logistic\nregression and other generalized linear models, as well as semidefinite\nprograms. \n\n"}
{"id": "1505.03036", "contents": "Title: Removing systematic errors for exoplanet search via latent causes Abstract: We describe a method for removing the effect of confounders in order to\nreconstruct a latent quantity of interest. The method, referred to as\nhalf-sibling regression, is inspired by recent work in causal inference using\nadditive noise models. We provide a theoretical justification and illustrate\nthe potential of the method in a challenging astronomy application. \n\n"}
{"id": "1505.05654", "contents": "Title: Slope heuristics and V-Fold model selection in heteroscedastic\n  regression using strongly localized bases Abstract: We investigate the optimality for model selection of the so-called slope\nheuristics, $V$-fold cross-validation and $V$-fold penalization in a\nheteroscedastic with random design regression context. We consider a new class\nof linear models that we call strongly localized bases and that generalize\nhistograms, piecewise polynomials and compactly supported wavelets. We derive\nsharp oracle inequalities that prove the asymptotic optimality of the slope\nheuristics---when the optimal penalty shape is known---and $V$ -fold\npenalization. Furthermore, $V$-fold cross-validation seems to be suboptimal for\na fixed value of $V$ since it recovers asymptotically the oracle learned from a\nsample size equal to $1-V^{-1}$ of the original amount of data. Our results are\nbased on genuine concentration inequalities for the true and empirical excess\nrisks that are of independent interest. We show in our experiments the good\nbehavior of the slope heuristics for the selection of linear wavelet models.\nFurthermore, $V$-fold cross-validation and $V$-fold penalization have\ncomparable efficiency. \n\n"}
{"id": "1505.06694", "contents": "Title: A dynamic viscoelastic analogy for fluid-filled elastic tubes Abstract: In this paper we evaluate the dynamic effects of the fluid viscosity for\nfluid filled elastic tubes in the framework of a linear uni-axial theory.\n  Because of the linear approximation, the effects on the fluid inside the\nelastic tube are taken into account according to the Womersley theory for a\npulsatile flow in a rigid tube.\n  The evolution equations for the response variables are derived by means of\nthe Laplace transform technique and they all turn out to be very same\nintegro-differential equation of the convolution type.\n  This equation has the same structure as the one describing uni-axial waves in\nlinear viscoelastic solids characterized by a relaxation modulus or by a creep\ncompliance. In our case, the analogy is connected with a peculiar viscoelastic\nsolid which exhibits creep properties similar to those of a fractional Maxwell\nmodel (of order 1/2) for short times, and of a standard Maxwell model for long\ntimes.\n  The present analysis could find applications in biophysics concerning the\npropagation of pressure waves within large arteries. \n\n"}
{"id": "1506.00354", "contents": "Title: Learning with hidden variables Abstract: Learning and inferring features that generate sensory input is a task\ncontinuously performed by cortex. In recent years, novel algorithms and\nlearning rules have been proposed that allow neural network models to learn\nsuch features from natural images, written text, audio signals, etc. These\nnetworks usually involve deep architectures with many layers of hidden neurons.\nHere we review recent advancements in this area emphasizing, amongst other\nthings, the processing of dynamical inputs by networks with hidden nodes and\nthe role of single neuron models. These points and the questions they arise can\nprovide conceptual advancements in understanding of learning in the cortex and\nthe relationship between machine learning approaches to learning with hidden\nnodes and those in cortical circuits. \n\n"}
{"id": "1506.01900", "contents": "Title: Communication Complexity of Distributed Convex Learning and Optimization Abstract: We study the fundamental limits to communication-efficient distributed\nmethods for convex learning and optimization, under different assumptions on\nthe information available to individual machines, and the types of functions\nconsidered. We identify cases where existing algorithms are already worst-case\noptimal, as well as cases where room for further improvement is still possible.\nAmong other things, our results indicate that without similarity between the\nlocal objective functions (due to statistical data similarity or otherwise)\nmany communication rounds may be required, even if the machines have unbounded\ncomputational power. \n\n"}
{"id": "1506.02142", "contents": "Title: Dropout as a Bayesian Approximation: Representing Model Uncertainty in\n  Deep Learning Abstract: Deep learning tools have gained tremendous attention in applied machine\nlearning. However such tools for regression and classification do not capture\nmodel uncertainty. In comparison, Bayesian models offer a mathematically\ngrounded framework to reason about model uncertainty, but usually come with a\nprohibitive computational cost. In this paper we develop a new theoretical\nframework casting dropout training in deep neural networks (NNs) as approximate\nBayesian inference in deep Gaussian processes. A direct result of this theory\ngives us tools to model uncertainty with dropout NNs -- extracting information\nfrom existing models that has been thrown away so far. This mitigates the\nproblem of representing uncertainty in deep learning without sacrificing either\ncomputational complexity or test accuracy. We perform an extensive study of the\nproperties of dropout's uncertainty. Various network architectures and\nnon-linearities are assessed on tasks of regression and classification, using\nMNIST as an example. We show a considerable improvement in predictive\nlog-likelihood and RMSE compared to existing state-of-the-art methods, and\nfinish by using dropout's uncertainty in deep reinforcement learning. \n\n"}
{"id": "1506.02221", "contents": "Title: An indefinite-proximal-based strictly contractive Peaceman-Rachford\n  splitting method Abstract: The Peaceman-Rachford splitting method is efficient for minimizing a convex\noptimization problem with a separable objective function and linear\nconstraints. However, its convergence was not guaranteed without extra\nrequirements. He {\\it et al.} (SIAM J. Optim. 24: 1011 - 1040, 2014) proved the\nconvergence of a strictly contractive Peaceman-Rachford splitting method by\nemploying a suitable underdetermined relaxation factor. In this paper, we\nfurther extend the so-called strictly contractive Peaceman-Rachford splitting\nmethod by using two different relaxation factors. Besides, motivated by the\nrecent advances on the ADMM type method with indefinite proximal terms, we\nemploy the indefinite proximal term in the strictly contractive\nPeaceman-Rachford splitting method. We show that the proposed\nindefinite-proximal strictly contractive Peaceman-Rachford splitting method is\nconvergent and also prove the $o(1/t)$ convergence rate in the nonergodic\nsense. The numerical tests on the $l_1$ regularized least square problem\ndemonstrate the efficiency of the proposed method. \n\n"}
{"id": "1506.02554", "contents": "Title: DUAL-LOCO: Distributing Statistical Estimation Using Random Projections Abstract: We present DUAL-LOCO, a communication-efficient algorithm for distributed\nstatistical estimation. DUAL-LOCO assumes that the data is distributed\naccording to the features rather than the samples. It requires only a single\nround of communication where low-dimensional random projections are used to\napproximate the dependences between features available to different workers. We\nshow that DUAL-LOCO has bounded approximation error which only depends weakly\non the number of workers. We compare DUAL-LOCO against a state-of-the-art\ndistributed optimization method on a variety of real world datasets and show\nthat it obtains better speedups while retaining good accuracy. \n\n"}
{"id": "1506.03164", "contents": "Title: Parallelizing MCMC with Random Partition Trees Abstract: The modern scale of data has brought new challenges to Bayesian inference. In\nparticular, conventional MCMC algorithms are computationally very expensive for\nlarge data sets. A promising approach to solve this problem is embarrassingly\nparallel MCMC (EP-MCMC), which first partitions the data into multiple subsets\nand runs independent sampling algorithms on each subset. The subset posterior\ndraws are then aggregated via some combining rules to obtain the final\napproximation. Existing EP-MCMC algorithms are limited by approximation\naccuracy and difficulty in resampling. In this article, we propose a new\nEP-MCMC algorithm PART that solves these problems. The new algorithm applies\nrandom partition trees to combine the subset posterior draws, which is\ndistribution-free, easy to resample from and can adapt to multiple scales. We\nprovide theoretical justification and extensive experiments illustrating\nempirical performance. \n\n"}
{"id": "1506.03521", "contents": "Title: Isometric sketching of any set via the Restricted Isometry Property Abstract: In this paper we show that for the purposes of dimensionality reduction\ncertain class of structured random matrices behave similarly to random Gaussian\nmatrices. This class includes several matrices for which matrix-vector multiply\ncan be computed in log-linear time, providing efficient dimensionality\nreduction of general sets. In particular, we show that using such matrices any\nset from high dimensions can be embedded into lower dimensions with near\noptimal distortion. We obtain our results by connecting dimensionality\nreduction of any set to dimensionality reduction of sparse vectors via a\nchaining argument. \n\n"}
{"id": "1506.05446", "contents": "Title: Communication-Efficient False Discovery Rate Control via Knockoff\n  Aggregation Abstract: The false discovery rate (FDR)---the expected fraction of spurious\ndiscoveries among all the discoveries---provides a popular statistical\nassessment of the reproducibility of scientific studies in various disciplines.\nIn this work, we introduce a new method for controlling the FDR in\nmeta-analysis of many decentralized linear models. Our method targets the\nscenario where many research groups---possibly the number of which is\nrandom---are independently testing a common set of hypotheses and then sending\nsummary statistics to a coordinating center in an online manner. Built on the\nknockoffs framework introduced by Barber and Candes (2015), our procedure\nstarts by applying the knockoff filter to each linear model and then aggregates\nthe summary statistics via one-shot communication in a novel way. This method\ngives exact FDR control non-asymptotically without any knowledge of the noise\nvariances or making any assumption about sparsity of the signal. In certain\nsettings, it has a communication complexity that is optimal up to a logarithmic\nfactor. \n\n"}
{"id": "1506.07216", "contents": "Title: Communication Lower Bounds for Statistical Estimation Problems via a\n  Distributed Data Processing Inequality Abstract: We study the tradeoff between the statistical error and communication cost of\ndistributed statistical estimation problems in high dimensions. In the\ndistributed sparse Gaussian mean estimation problem, each of the $m$ machines\nreceives $n$ data points from a $d$-dimensional Gaussian distribution with\nunknown mean $\\theta$ which is promised to be $k$-sparse. The machines\ncommunicate by message passing and aim to estimate the mean $\\theta$. We\nprovide a tight (up to logarithmic factors) tradeoff between the estimation\nerror and the number of bits communicated between the machines. This directly\nleads to a lower bound for the distributed \\textit{sparse linear regression}\nproblem: to achieve the statistical minimax error, the total communication is\nat least $\\Omega(\\min\\{n,d\\}m)$, where $n$ is the number of observations that\neach machine receives and $d$ is the ambient dimension. These lower results\nimprove upon [Sha14,SD'14] by allowing multi-round iterative communication\nmodel. We also give the first optimal simultaneous protocol in the dense case\nfor mean estimation.\n  As our main technique, we prove a \\textit{distributed data processing\ninequality}, as a generalization of usual data processing inequalities, which\nmight be of independent interest and useful for other problems. \n\n"}
{"id": "1506.08836", "contents": "Title: Interaction stabilized steady states in the driven O(N) model Abstract: We study periodically driven bosonic scalar field theories in the infinite N\nlimit. It is well-known that the free theory can undergo parametric resonance\nunder monochromatic modulation of the mass term and thereby absorb energy\nindefinitely. Interactions in the infinite N limit terminate this increase for\nany choice of the UV cutoff and driving frequency. The steady state has\nnon-trivial correlations and is synchronized with the drive. The O(N) model at\ninfinite N provides the first example of a clean interacting quantum system\nthat does not heat to infinite temperature at any drive frequency. \n\n"}
{"id": "1507.00562", "contents": "Title: Several Complex Variables Abstract: These are notes from a basic course in Several Complex Variables \n\n"}
{"id": "1507.00677", "contents": "Title: Distributional Smoothing with Virtual Adversarial Training Abstract: We propose local distributional smoothness (LDS), a new notion of smoothness\nfor statistical model that can be used as a regularization term to promote the\nsmoothness of the model distribution. We named the LDS based regularization as\nvirtual adversarial training (VAT). The LDS of a model at an input datapoint is\ndefined as the KL-divergence based robustness of the model distribution against\nlocal perturbation around the datapoint. VAT resembles adversarial training,\nbut distinguishes itself in that it determines the adversarial direction from\nthe model distribution alone without using the label information, making it\napplicable to semi-supervised learning. The computational cost for VAT is\nrelatively low. For neural network, the approximated gradient of the LDS can be\ncomputed with no more than three pairs of forward and back propagations. When\nwe applied our technique to supervised and semi-supervised learning for the\nMNIST dataset, it outperformed all the training methods other than the current\nstate of the art method, which is based on a highly advanced generative model.\nWe also applied our method to SVHN and NORB, and confirmed our method's\nsuperior performance over the current state of the art semi-supervised method\napplied to these datasets. \n\n"}
{"id": "1507.00720", "contents": "Title: Correlated Random Measures Abstract: We develop correlated random measures, random measures where the atom weights\ncan exhibit a flexible pattern of dependence, and use them to develop powerful\nhierarchical Bayesian nonparametric models. Hierarchical Bayesian nonparametric\nmodels are usually built from completely random measures, a Poisson-process\nbased construction in which the atom weights are independent. Completely random\nmeasures imply strong independence assumptions in the corresponding\nhierarchical model, and these assumptions are often misplaced in real-world\nsettings. Correlated random measures address this limitation. They model\ncorrelation within the measure by using a Gaussian process in concert with the\nPoisson process. With correlated random measures, for example, we can develop a\nlatent feature model for which we can infer both the properties of the latent\nfeatures and their dependency pattern. We develop several other examples as\nwell. We study a correlated random measure model of pairwise count data. We\nderive an efficient variational inference algorithm and show improved\npredictive performance on large data sets of documents, web clicks, and\nelectronic health records. \n\n"}
{"id": "1507.05780", "contents": "Title: Geometric ergodicity of the Random Walk Metropolis with\n  position-dependent proposal covariance Abstract: We consider a Metropolis--Hastings method with proposal $\\mathcal{N}(x,\nhG(x)^{-1})$, where $x$ is the current state, and study its ergodicity\nproperties. We show that suitable choices of $G(x)$ can change these compared\nto the Random Walk Metropolis case $\\mathcal{N}(x, h\\Sigma)$, either for better\nor worse. We find that if the proposal variance is allowed to grow unboundedly\nin the tails of the distribution then geometric ergodicity can be established\nwhen the target distribution for the algorithm has tails that are heavier than\nexponential, but that the growth rate must be carefully controlled to prevent\nthe rejection rate approaching unity. We also illustrate that a judicious\nchoice of $G(x)$ can result in a geometrically ergodic chain when probability\nconcentrates on an ever narrower ridge in the tails, something that is not true\nfor the Random Walk Metropolis. \n\n"}
{"id": "1507.06597", "contents": "Title: Cox's Theorem and the Jaynesian Interpretation of Probability Abstract: There are multiple proposed interpretations of probability theory: one such\ninterpretation is true-false logic under uncertainty. Cox's Theorem is a\nrepresentation theorem that states, under a certain set of axioms describing\nthe meaning of uncertainty, that every true-false logic under uncertainty is\nisomorphic to conditional probability theory. This result was used by Jaynes to\ndevelop a philosophical framework in which statistical inference under\nuncertainty should be conducted through the use of probability, via Bayes'\nRule. Unfortunately, most existing correct proofs of Cox's Theorem require\nrestrictive assumptions: for instance, many do not apply even to the simple\nexample of rolling a pair of fair dice. We offer a new axiomatization by\nreplacing various technical conditions with an axiom stating that our theory\nmust be consistent with respect to repeated events. We discuss the implications\nof our results, both for the philosophy of probability and for the philosophy\nof statistics. \n\n"}
{"id": "1507.06755", "contents": "Title: Weak solutions of complex Hessian equations on compact Hermitian\n  manifolds Abstract: We prove the existence of weak solutions of complex $m-$Hessian equations on\ncompact Hermitian manifolds for the nonnegative right hand side belonging to\n$L^p, p>n/m$ ($n$ is the dimension of the manifold). For smooth, positive data\nthe equation has been recently solved by Szekelyhidi and Zhang. We also give a\nstability result for such solutions. \n\n"}
{"id": "1507.07595", "contents": "Title: Distributed Stochastic Variance Reduced Gradient Methods and A Lower\n  Bound for Communication Complexity Abstract: We study distributed optimization algorithms for minimizing the average of\nconvex functions. The applications include empirical risk minimization problems\nin statistical machine learning where the datasets are large and have to be\nstored on different machines. We design a distributed stochastic variance\nreduced gradient algorithm that, under certain conditions on the condition\nnumber, simultaneously achieves the optimal parallel runtime, amount of\ncommunication and rounds of communication among all distributed first-order\nmethods up to constant factors. Our method and its accelerated extension also\noutperform existing distributed algorithms in terms of the rounds of\ncommunication as long as the condition number is not too large compared to the\nsize of data in each machine. We also prove a lower bound for the number of\nrounds of communication for a broad class of distributed first-order methods\nincluding the proposed algorithms in this paper. We show that our accelerated\ndistributed stochastic variance reduced gradient algorithm achieves this lower\nbound so that it uses the fewest rounds of communication among all distributed\nfirst-order algorithms. \n\n"}
{"id": "1507.07604", "contents": "Title: Algebraic (Volume) Density Property for Affine Homogeneous Spaces Abstract: Let $X$ be a connected affine homogenous space of a linear algebraic group\n$G$ over $\\C$. (1) If $X$ is different from a line or a torus we show that the\nspace of all algebraic vector fields on $X$ coincides with the Lie algebra\ngenerated by complete algebraic vector fields on $X$. (2) Suppose that $X$ has\na $G$-invariant volume form $\\omega$. We prove that the space of all\ndivergence-free (with respect to $\\omega$) algebraic vector fields on $X$\ncoincides with the Lie algebra generated by divergence-free complete algebraic\nvector fields on $X$ (including the cases when $X$ is a line or a torus). The\nproof of these results requires new criteria for algebraic (volume) density\nproperty based on so called module generating pairs. \n\n"}
{"id": "1508.01211", "contents": "Title: Listen, Attend and Spell Abstract: We present Listen, Attend and Spell (LAS), a neural network that learns to\ntranscribe speech utterances to characters. Unlike traditional DNN-HMM models,\nthis model learns all the components of a speech recognizer jointly. Our system\nhas two components: a listener and a speller. The listener is a pyramidal\nrecurrent network encoder that accepts filter bank spectra as inputs. The\nspeller is an attention-based recurrent network decoder that emits characters\nas outputs. The network produces character sequences without making any\nindependence assumptions between the characters. This is the key improvement of\nLAS over previous end-to-end CTC models. On a subset of the Google voice search\ntask, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a\nlanguage model, and 10.3% with language model rescoring over the top 32 beams.\nBy comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%. \n\n"}
{"id": "1508.01587", "contents": "Title: Experimental demonstration of a quantum router Abstract: The router is a key element for a network. We describe a scheme to realize\ngenuine quantum routing of single-photon pulses based on cascading of\nconditional quantum gates in a Mach-Zehnder interferometer and report a\nproof-of-principle experiment for its demonstration using linear optics quantum\ngates. The polarization of the control photon routes in a coherent way the path\nof the signal photon while preserving the qubit state of the signal photon\nrepresented by its polarization. We demonstrate quantum nature of this router\nby showing entanglement generated between the initially unentangled control and\nsignal photons, and confirm that the qubit state of the signal photon is well\npreserved by the router through quantum process tomography. \n\n"}
{"id": "1508.01774", "contents": "Title: An End-to-End Neural Network for Polyphonic Piano Music Transcription Abstract: We present a supervised neural network model for polyphonic piano music\ntranscription. The architecture of the proposed model is analogous to speech\nrecognition systems and comprises an acoustic model and a music language model.\nThe acoustic model is a neural network used for estimating the probabilities of\npitches in a frame of audio. The language model is a recurrent neural network\nthat models the correlations between pitch combinations over time. The proposed\nmodel is general and can be used to transcribe polyphonic music without\nimposing any constraints on the polyphony. The acoustic and language model\npredictions are combined using a probabilistic graphical model. Inference over\nthe output variables is performed using the beam search algorithm. We perform\ntwo sets of experiments. We investigate various neural network architectures\nfor the acoustic models and also investigate the effect of combining acoustic\nand music language model predictions using the proposed architecture. We\ncompare performance of the neural network based acoustic models with two\npopular unsupervised acoustic models. Results show that convolutional neural\nnetwork acoustic models yields the best performance across all evaluation\nmetrics. We also observe improved performance with the application of the music\nlanguage models. Finally, we present an efficient variant of beam search that\nimproves performance and reduces run-times by an order of magnitude, making the\nmodel suitable for real-time applications. \n\n"}
{"id": "1508.01928", "contents": "Title: A variational approach to the consistency of spectral clustering Abstract: This paper establishes the consistency of spectral approaches to data\nclustering. We consider clustering of point clouds obtained as samples of a\nground-truth measure. A graph representing the point cloud is obtained by\nassigning weights to edges based on the distance between the points they\nconnect. We investigate the spectral convergence of both unnormalized and\nnormalized graph Laplacians towards the appropriate operators in the continuum\ndomain. We obtain sharp conditions on how the connectivity radius can be scaled\nwith respect to the number of sample points for the spectral convergence to\nhold.\n  We also show that the discrete clusters obtained via spectral clustering\nconverge towards a continuum partition of the ground truth measure. Such\ncontinuum partition minimizes a functional describing the continuum analogue of\nthe graph-based spectral partitioning. Our approach, based on variational\nconvergence, is general and flexible. \n\n"}
{"id": "1508.02114", "contents": "Title: Excitation and dynamics in the extended bose-hubbard model Abstract: The one-dimensional extended bosonic Hubbard model has been shown to exhibit\na variety of phases ranging from Mott insulator and superfluid to exotic\nsupersolids and Haldane insulators depending on the filling and the relative\nvalue of the contact ($U$) and near neighbor ($V$) interaction strengths. In\nthis paper we use the density matrix renormalization group and the time\nevolving block decimation numerical methods to study in detail the dynamics and\nexcitation spectra of this model in its various phases. In particular, we study\nin detail the behavior of the charge and neutral gaps which characterize the\nMott, charge density and Haldane insulating phases. We also show that in\naddition to the gapless modes at $k=0$, the supersolid phase exhibits gapless\nmodes at a finite $k$ which depends on the filling. \n\n"}
{"id": "1508.02887", "contents": "Title: Toeplitz operators on doubling Fock spaces Abstract: We study Toeplitz operator theory on the doubling Fock spaces, which are Fock\nspaces whose exponential weight is associated to a subharmonic function with\ndoubling Riesz measure. Namely, we characterize the boundedness, compactness\nand membership in the Schatten class of Toeplitz operators on doubling Fock\nspaces whose symbol is a positive Radon measure. \n\n"}
{"id": "1508.03390", "contents": "Title: Doubly Stochastic Primal-Dual Coordinate Method for Bilinear\n  Saddle-Point Problem Abstract: We propose a doubly stochastic primal-dual coordinate optimization algorithm\nfor empirical risk minimization, which can be formulated as a bilinear\nsaddle-point problem. In each iteration, our method randomly samples a block of\ncoordinates of the primal and dual solutions to update. The linear convergence\nof our method could be established in terms of 1) the distance from the current\niterate to the optimal solution and 2) the primal-dual objective gap. We show\nthat the proposed method has a lower overall complexity than existing\ncoordinate methods when either the data matrix has a factorized structure or\nthe proximal mapping on each block is computationally expensive, e.g.,\ninvolving an eigenvalue decomposition. The efficiency of the proposed method is\nconfirmed by empirical studies on several real applications, such as the\nmulti-task large margin nearest neighbor problem. \n\n"}
{"id": "1508.04211", "contents": "Title: Scalable Bayesian Non-Negative Tensor Factorization for Massive Count\n  Data Abstract: We present a Bayesian non-negative tensor factorization model for\ncount-valued tensor data, and develop scalable inference algorithms (both batch\nand online) for dealing with massive tensors. Our generative model can handle\noverdispersed counts as well as infer the rank of the decomposition. Moreover,\nleveraging a reparameterization of the Poisson distribution as a multinomial\nfacilitates conjugacy in the model and enables simple and efficient Gibbs\nsampling and variational Bayes (VB) inference updates, with a computational\ncost that only depends on the number of nonzeros in the tensor. The model also\nprovides a nice interpretability for the factors; in our model, each factor\ncorresponds to a \"topic\". We develop a set of online inference algorithms that\nallow further scaling up the model to massive tensors, for which batch\ninference methods may be infeasible. We apply our framework on diverse\nreal-world applications, such as \\emph{multiway} topic modeling on a scientific\npublications database, analyzing a political science data set, and analyzing a\nmassive household transactions data set. \n\n"}
{"id": "1508.06182", "contents": "Title: Solving the Optimal Trading Trajectory Problem Using a Quantum Annealer Abstract: We solve a multi-period portfolio optimization problem using D-Wave Systems'\nquantum annealer. We derive a formulation of the problem, discuss several\npossible integer encoding schemes, and present numerical examples that show\nhigh success rates. The formulation incorporates transaction costs (including\npermanent and temporary market impact), and, significantly, the solution does\nnot require the inversion of a covariance matrix. The discrete multi-period\nportfolio optimization problem we solve is significantly harder than the\ncontinuous variable problem. We present insight into how results may be\nimproved using suitable software enhancements, and why current quantum\nannealing technology limits the size of problem that can be successfully solved\ntoday. The formulation presented is specifically designed to be scalable, with\nthe expectation that as quantum annealing technology improves, larger problems\nwill be solvable using the same techniques. \n\n"}
{"id": "1509.03025", "contents": "Title: Fast low-rank estimation by projected gradient descent: General\n  statistical and algorithmic guarantees Abstract: Optimization problems with rank constraints arise in many applications,\nincluding matrix regression, structured PCA, matrix completion and matrix\ndecomposition problems. An attractive heuristic for solving such problems is to\nfactorize the low-rank matrix, and to run projected gradient descent on the\nnonconvex factorized optimization problem. The goal of this problem is to\nprovide a general theoretical framework for understanding when such methods\nwork well, and to characterize the nature of the resulting fixed point. We\nprovide a simple set of conditions under which projected gradient descent, when\ngiven a suitable initialization, converges geometrically to a statistically\nuseful solution. Our results are applicable even when the initial solution is\noutside any region of local convexity, and even when the problem is globally\nconcave. Working in a non-asymptotic framework, we show that our conditions are\nsatisfied for a wide range of concrete models, including matrix regression,\nstructured PCA, matrix completion with real and quantized observations, matrix\ndecomposition, and graph clustering problems. Simulation results show excellent\nagreement with the theoretical predictions. \n\n"}
{"id": "1509.03025", "contents": "Title: Fast low-rank estimation by projected gradient descent: General\n  statistical and algorithmic guarantees Abstract: Optimization problems with rank constraints arise in many applications,\nincluding matrix regression, structured PCA, matrix completion and matrix\ndecomposition problems. An attractive heuristic for solving such problems is to\nfactorize the low-rank matrix, and to run projected gradient descent on the\nnonconvex factorized optimization problem. The goal of this problem is to\nprovide a general theoretical framework for understanding when such methods\nwork well, and to characterize the nature of the resulting fixed point. We\nprovide a simple set of conditions under which projected gradient descent, when\ngiven a suitable initialization, converges geometrically to a statistically\nuseful solution. Our results are applicable even when the initial solution is\noutside any region of local convexity, and even when the problem is globally\nconcave. Working in a non-asymptotic framework, we show that our conditions are\nsatisfied for a wide range of concrete models, including matrix regression,\nstructured PCA, matrix completion with real and quantized observations, matrix\ndecomposition, and graph clustering problems. Simulation results show excellent\nagreement with the theoretical predictions. \n\n"}
{"id": "1509.05457", "contents": "Title: Distributed Estimation and Inference with Statistical Guarantees Abstract: This paper studies hypothesis testing and parameter estimation in the context\nof the divide and conquer algorithm. In a unified likelihood based framework,\nwe propose new test statistics and point estimators obtained by aggregating\nvarious statistics from $k$ subsamples of size $n/k$, where $n$ is the sample\nsize. In both low dimensional and high dimensional settings, we address the\nimportant question of how to choose $k$ as $n$ grows large, providing a\ntheoretical upper bound on $k$ such that the information loss due to the divide\nand conquer algorithm is negligible. In other words, the resulting estimators\nhave the same inferential efficiencies and estimation rates as a practically\ninfeasible oracle with access to the full sample. Thorough numerical results\nare provided to back up the theory. \n\n"}
{"id": "1510.05149", "contents": "Title: Robust Non-linear Wiener-Granger Causality For Large High-dimensional\n  Data Abstract: Wiener-Granger causality is a widely used framework of causal analysis for\ntemporally resolved events. We introduce a new measure of Wiener-Granger\ncausality based on kernelization of partial canonical correlation analysis with\nspecific advantages in the context of large high-dimensional data. The\nintroduced measure is able to detect non-linear and non-monotonous signals, is\ndesigned to be immune to noise, and offers tunability in terms of computational\ncomplexity in its estimations. Furthermore, we show that, under specified\nconditions, the introduced measure can be regarded as an estimate of\nconditional mutual information (transfer entropy). The functionality of this\nmeasure is assessed using comparative simulations where it outperforms other\nexisting methods. The paper is concluded with an application to climatological\ndata. \n\n"}
{"id": "1510.07471", "contents": "Title: A Parallel algorithm for $\\mathcal{X}$-Armed bandits Abstract: The target of $\\mathcal{X}$-armed bandit problem is to find the global\nmaximum of an unknown stochastic function $f$, given a finite budget of $n$\nevaluations. Recently, $\\mathcal{X}$-armed bandits have been widely used in\nmany situations. Many of these applications need to deal with large-scale data\nsets. To deal with these large-scale data sets, we study a distributed setting\nof $\\mathcal{X}$-armed bandits, where $m$ players collaborate to find the\nmaximum of the unknown function. We develop a novel anytime distributed\n$\\mathcal{X}$-armed bandit algorithm. Compared with prior work on\n$\\mathcal{X}$-armed bandits, our algorithm uses a quite different searching\nstrategy so as to fit distributed learning scenarios. Our theoretical analysis\nshows that our distributed algorithm is $m$ times faster than the classical\nsingle-player algorithm. Moreover, the number of communication rounds of our\nalgorithm is only logarithmic in $mn$. The numerical results show that our\nmethod can make effective use of every players to minimize the loss. Thus, our\ndistributed approach is attractive and useful. \n\n"}
{"id": "1510.08692", "contents": "Title: Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale\n  Bayesian Sampling Abstract: Monte Carlo sampling for Bayesian posterior inference is a common approach\nused in machine learning. The Markov Chain Monte Carlo procedures that are used\nare often discrete-time analogues of associated stochastic differential\nequations (SDEs). These SDEs are guaranteed to leave invariant the required\nposterior distribution. An area of current research addresses the computational\nbenefits of stochastic gradient methods in this setting. Existing techniques\nrely on estimating the variance or covariance of the subsampling error, and\ntypically assume constant variance. In this article, we propose a\ncovariance-controlled adaptive Langevin thermostat that can effectively\ndissipate parameter-dependent noise while maintaining a desired target\ndistribution. The proposed method achieves a substantial speedup over popular\nalternative schemes for large-scale machine learning applications. \n\n"}
{"id": "1510.08986", "contents": "Title: A Unified Theory of Confidence Regions and Testing for High Dimensional\n  Estimating Equations Abstract: We propose a new inferential framework for constructing confidence regions\nand testing hypotheses in statistical models specified by a system of high\ndimensional estimating equations. We construct an influence function by\nprojecting the fitted estimating equations to a sparse direction obtained by\nsolving a large-scale linear program. Our main theoretical contribution is to\nestablish a unified Z-estimation theory of confidence regions for high\ndimensional problems.\n  Different from existing methods, all of which require the specification of\nthe likelihood or pseudo-likelihood, our framework is likelihood-free. As a\nresult, our approach provides valid inference for a broad class of high\ndimensional constrained estimating equation problems, which are not covered by\nexisting methods.\n  Such examples include, noisy compressed sensing, instrumental variable\nregression, undirected graphical models, discriminant analysis and vector\nautoregressive models. We present detailed theoretical results for all these\nexamples. Finally, we conduct thorough numerical simulations, and a real\ndataset analysis to back up the developed theoretical results. \n\n"}
{"id": "1510.09020", "contents": "Title: Entanglement Renormalization and Two Dimensional String Theory Abstract: The entanglement renormalization flow of a (1+1) free boson is formulated as\na path integral over some auxiliary scalar fields. The resulting effective\ntheory for these fields amounts to the dilaton term of non-critical string\ntheory in two spacetime dimensions. A connection between the scalar fields in\nthe two theories is provided, allowing to acquire novel insights into how a\ntheory of gravity emerges from the entanglement structure of another one\nwithout gravity. \n\n"}
{"id": "1510.09050", "contents": "Title: Phase dependent loading of Bloch bands and Quantum simulation of\n  relativistic wave equation predictions with ultracold atoms in variably\n  shaped optical lattice potentials Abstract: The dispersion relation of ultracold atoms in variably shaped optical\nlattices can be tuned to resemble that of a relativistic particle, i.e. be\nlinear instead of the usual nonrelativistic quadratic dispersion relation of a\nfree atom. Cold atoms in such a lattice can be used to carry out quantum\nsimulations of relativistic wave equation predictions. We begin this article by\ndescribing a Raman technique that allows to selectively load atoms into a\ndesired Bloch band of the lattice near a band crossing. Subsequently, we review\ntwo recent experiments with quasirelativistic rubidium atoms in a bichromatic\nlattice, demonstrating the analogs of Klein tunneling and Veselago lensing with\nultracold atoms respectively. \n\n"}
{"id": "1511.00652", "contents": "Title: Discrete Riemann surfaces based on quadrilateral cellular decompositions Abstract: Our aim in this paper is to provide a theory of discrete Riemann surfaces\nbased on quadrilateral cellular decompositions of Riemann surfaces together\nwith their complex structure encoded by complex weights. Previous work, in\nparticular of Mercat, mainly focused on real weights corresponding to\nquadrilateral cells having orthogonal diagonals. We discuss discrete coverings,\ndiscrete exterior calculus, and discrete Abelian integrals. Our presentation\nincludes several new notions and results such as branched coverings of discrete\nRiemann surfaces, the discrete Riemann-Hurwitz Formula, double poles of\ndiscrete one-forms and double values of discrete meromorphic functions that\nenter the discrete Riemann-Roch Theorem, and a discrete Abel-Jacobi map. \n\n"}
{"id": "1511.01443", "contents": "Title: A Distributed One-Step Estimator Abstract: Distributed statistical inference has recently attracted enormous attention.\nMany existing work focuses on the averaging estimator. We propose a one-step\napproach to enhance a simple-averaging based distributed estimator. We derive\nthe corresponding asymptotic properties of the newly proposed estimator. We\nfind that the proposed one-step estimator enjoys the same asymptotic properties\nas the centralized estimator. The proposed one-step approach merely requires\none additional round of communication in relative to the averaging estimator;\nso the extra communication burden is insignificant. In finite sample cases,\nnumerical examples show that the proposed estimator outperforms the simple\naveraging estimator with a large margin in terms of the mean squared errors. A\npotential application of the one-step approach is that one can use multiple\nmachines to speed up large scale statistical inference with little compromise\nin the quality of estimators. The proposed method becomes more valuable when\ndata can only be available at distributed machines with limited communication\nbandwidth. \n\n"}
{"id": "1511.03243", "contents": "Title: Black-box $\\alpha$-divergence Minimization Abstract: Black-box alpha (BB-$\\alpha$) is a new approximate inference method based on\nthe minimization of $\\alpha$-divergences. BB-$\\alpha$ scales to large datasets\nbecause it can be implemented using stochastic gradient descent. BB-$\\alpha$\ncan be applied to complex probabilistic models with little effort since it only\nrequires as input the likelihood function and its gradients. These gradients\ncan be easily obtained using automatic differentiation. By changing the\ndivergence parameter $\\alpha$, the method is able to interpolate between\nvariational Bayes (VB) ($\\alpha \\rightarrow 0$) and an algorithm similar to\nexpectation propagation (EP) ($\\alpha = 1$). Experiments on probit regression\nand neural network regression and classification problems show that BB-$\\alpha$\nwith non-standard settings of $\\alpha$, such as $\\alpha = 0.5$, usually\nproduces better predictions than with $\\alpha \\rightarrow 0$ (VB) or $\\alpha =\n1$ (EP). \n\n"}
{"id": "1511.03575", "contents": "Title: Federated Optimization:Distributed Optimization Beyond the Datacenter Abstract: We introduce a new and increasingly relevant setting for distributed\noptimization in machine learning, where the data defining the optimization are\ndistributed (unevenly) over an extremely large number of \\nodes, but the goal\nremains to train a high-quality centralized model. We refer to this setting as\nFederated Optimization. In this setting, communication efficiency is of utmost\nimportance.\n  A motivating example for federated optimization arises when we keep the\ntraining data locally on users' mobile devices rather than logging it to a data\ncenter for training. Instead, the mobile devices are used as nodes performing\ncomputation on their local data in order to update a global model. We suppose\nthat we have an extremely large number of devices in our network, each of which\nhas only a tiny fraction of data available totally; in particular, we expect\nthe number of data points available locally to be much smaller than the number\nof devices. Additionally, since different users generate data with different\npatterns, we assume that no device has a representative sample of the overall\ndistribution.\n  We show that existing algorithms are not suitable for this setting, and\npropose a new algorithm which shows encouraging experimental results. This work\nalso sets a path for future research needed in the context of federated\noptimization. \n\n"}
{"id": "1511.03962", "contents": "Title: Document Context Language Models Abstract: Text documents are structured on multiple levels of detail: individual words\nare related by syntax, but larger units of text are related by discourse\nstructure. Existing language models generally fail to account for discourse\nstructure, but it is crucial if we are to have language models that reward\ncoherence and generate coherent texts. We present and empirically evaluate a\nset of multi-level recurrent neural network language models, called\nDocument-Context Language Models (DCLM), which incorporate contextual\ninformation both within and beyond the sentence. In comparison with word-level\nrecurrent neural network language models, the DCLM models obtain slightly\nbetter predictive likelihoods, and considerably better assessments of document\ncoherence. \n\n"}
{"id": "1511.05286", "contents": "Title: Classifying and Segmenting Microscopy Images Using Convolutional\n  Multiple Instance Learning Abstract: Convolutional neural networks (CNN) have achieved state of the art\nperformance on both classification and segmentation tasks. Applying CNNs to\nmicroscopy images is challenging due to the lack of datasets labeled at the\nsingle cell level. We extend the application of CNNs to microscopy image\nclassification and segmentation using multiple instance learning (MIL). We\npresent the adaptive Noisy-AND MIL pooling function, a new MIL operator that is\nrobust to outliers. Combining CNNs with MIL enables training CNNs using full\nresolution microscopy images with global labels. We base our approach on the\nsimilarity between the aggregation function used in MIL and pooling layers used\nin CNNs. We show that training MIL CNNs end-to-end outperforms several previous\nmethods on both mammalian and yeast microscopy images without requiring any\nsegmentation steps. \n\n"}
{"id": "1511.05680", "contents": "Title: Wishart Mechanism for Differentially Private Principal Components\n  Analysis Abstract: We propose a new input perturbation mechanism for publishing a covariance\nmatrix to achieve $(\\epsilon,0)$-differential privacy. Our mechanism uses a\nWishart distribution to generate matrix noise. In particular, We apply this\nmechanism to principal component analysis. Our mechanism is able to keep the\npositive semi-definiteness of the published covariance matrix. Thus, our\napproach gives rise to a general publishing framework for input perturbation of\na symmetric positive semidefinite matrix. Moreover, compared with the classic\nLaplace mechanism, our method has better utility guarantee. To the best of our\nknowledge, Wishart mechanism is the best input perturbation approach for\n$(\\epsilon,0)$-differentially private PCA. We also compare our work with\nprevious exponential mechanism algorithms in the literature and provide near\noptimal bound while having more flexibility and less computational\nintractability. \n\n"}
{"id": "1511.08611", "contents": "Title: Squeezer-based pulsed optomechanical interface Abstract: We prove feasibility of high-fidelity pulsed optomechanical interface based\non all-optical presqueezing of non-Gaussian quantum states of light before they\nenter the optomechanical system. We demonstrate that feasible presqueezing of\noptical states effectively increases the low noise transfer of them to\nmechanical oscillator. It allows one to surpass the limit necessary to transfer\nhighly nonclassical states with negative Wigner function. In particular, we\nverify that with this help single photon states of light can be efficiently\nturned to single phonon states of mechanical oscillator, keeping the negativity\nof the Wigner function. It opens the possibility to merge quantum optomechanics\nwith the recent methods of quantum optics. \n\n"}
{"id": "1511.08963", "contents": "Title: Learning Directed Acyclic Graphs with Penalized Neighbourhood Regression Abstract: We study a family of regularized score-based estimators for learning the\nstructure of a directed acyclic graph (DAG) for a multivariate normal\ndistribution from high-dimensional data with $p\\gg n$. Our main results\nestablish support recovery guarantees and deviation bounds for a family of\npenalized least-squares estimators under concave regularization without\nassuming prior knowledge of a variable ordering. These results apply to a\nvariety of practical situations that allow for arbitrary nondegenerate\ncovariance structures as well as many popular regularizers including the MCP,\nSCAD, $\\ell_{0}$ and $\\ell_{1}$. The proof relies on interpreting a DAG as a\nrecursive linear structural equation model, which reduces the estimation\nproblem to a series of neighbourhood regressions. We provide a novel\nstatistical analysis of these neighbourhood problems, establishing uniform\ncontrol over the superexponential family of neighbourhoods associated with a\nGaussian distribution. We then apply these results to study the statistical\nproperties of score-based DAG estimators, learning causal DAGs, and inferring\nconditional independence relations via graphical models. Our results\nyield---for the first time---finite-sample guarantees for structure learning of\nGaussian DAGs in high-dimensions via score-based estimation. \n\n"}
{"id": "1511.09433", "contents": "Title: Universality laws for randomized dimension reduction, with applications Abstract: Dimension reduction is the process of embedding high-dimensional data into a\nlower dimensional space to facilitate its analysis. In the Euclidean setting,\none fundamental technique for dimension reduction is to apply a random linear\nmap to the data. This dimension reduction procedure succeeds when it preserves\ncertain geometric features of the set.\n  The question is how large the embedding dimension must be to ensure that\nrandomized dimension reduction succeeds with high probability.\n  This paper studies a natural family of randomized dimension reduction maps\nand a large class of data sets. It proves that there is a phase transition in\nthe success probability of the dimension reduction map as the embedding\ndimension increases. For a given data set, the location of the phase transition\nis the same for all maps in this family. Furthermore, each map has the same\nstability properties, as quantified through the restricted minimum singular\nvalue. These results can be viewed as new universality laws in high-dimensional\nstochastic geometry.\n  Universality laws for randomized dimension reduction have many applications\nin applied mathematics, signal processing, and statistics. They yield design\nprinciples for numerical linear algebra algorithms, for compressed sensing\nmeasurement ensembles, and for random linear codes. Furthermore, these results\nhave implications for the performance of statistical estimation methods under a\nlarge class of random experimental designs. \n\n"}
{"id": "1512.00209", "contents": "Title: Equivalence Classes of Staged Trees Abstract: In this paper we give a complete characterization of the statistical\nequivalence classes of CEGs and of staged trees. We are able to show that all\ngraphical representations of the same model share a common polynomial\ndescription. Then, simple transformations on that polynomial enable us to\ntraverse the corresponding class of graphs. We illustrate our results with a\nreal analysis of the implicit dependence relationships within a previously\nstudied dataset. \n\n"}
{"id": "1512.00218", "contents": "Title: Minimax theory for a class of non-linear statistical inverse problems Abstract: We study a class of statistical inverse problems with non-linear pointwise\noperators motivated by concrete statistical applications. A two-step procedure\nis proposed, where the first step smoothes the data and inverts the\nnon-linearity. This reduces the initial non-linear problem to a linear inverse\nproblem with deterministic noise, which is then solved in a second step. The\nnoise reduction step is based on wavelet thresholding and is shown to be\nminimax optimal (up to logarithmic factors) in a pointwise function-dependent\nsense. Our analysis is based on a modified notion of H\\\"older smoothness scales\nthat are natural in this setting. \n\n"}
{"id": "1512.02523", "contents": "Title: A remark on boundary estimates on unbounded $Z(q)$ domains in\n  $\\mathbb{C}^n$ Abstract: The goal of this note is to explore the relationship between the Folland-Kohn\nbasic estimate and the $Z(q)$-condition. In particular, on unbounded\npseudoconvex (resp., pseudoconcave) domains, we prove that the Folland-Kohn\nbasic estimate is equivalent to uniform strict pseudoconvexity (resp.,\npseudoconcavity). As a corollary, we observe that despite the Siegel upper half\nspace being strictly pseudoconvex and biholomorphic to a the unit ball, it\nfails to satisfy uniform strict pseudoconvexity and hence the Folland-Kohn\nbasic estimate fails.\n  On unbounded non-pseudoconvex domains, we show that the Folland-Kohn basic\nestimate on $(0,q)$-forms implies a uniform $Z(q)$ condition, and conversely,\nthat a uniform $Z(q)$ condition with some additional hypotheses implies the\nFolland-Kohn basic estimate for $(0,q)$-forms. \n\n"}
{"id": "1512.03081", "contents": "Title: Gamma Belief Networks Abstract: To infer multilayer deep representations of high-dimensional discrete and\nnonnegative real vectors, we propose an augmentable gamma belief network (GBN)\nthat factorizes each of its hidden layers into the product of a sparse\nconnection weight matrix and the nonnegative real hidden units of the next\nlayer. The GBN's hidden layers are jointly trained with an upward-downward\nGibbs sampler that solves each layer with the same subroutine. The\ngamma-negative binomial process combined with a layer-wise training strategy\nallows inferring the width of each layer given a fixed budget on the width of\nthe first layer. Example results illustrate interesting relationships between\nthe width of the first layer and the inferred network structure, and\ndemonstrate that the GBN can add more layers to improve its performance in both\nunsupervisedly extracting features and predicting heldout data. For exploratory\ndata analysis, we extract trees and subnetworks from the learned deep network\nto visualize how the very specific factors discovered at the first hidden layer\nand the increasingly more general factors discovered at deeper hidden layers\nare related to each other, and we generate synthetic data by propagating random\nvariables through the deep network from the top hidden layer back to the bottom\ndata layer. \n\n"}
{"id": "1512.03883", "contents": "Title: Sparse Generalized Principal Component Analysis for Large-scale\n  Applications beyond Gaussianity Abstract: Principal Component Analysis (PCA) is a dimension reduction technique. It\nproduces inconsistent estimators when the dimensionality is moderate to high,\nwhich is often the problem in modern large-scale applications where algorithm\nscalability and model interpretability are difficult to achieve, not to mention\nthe prevalence of missing values. While existing sparse PCA methods alleviate\ninconsistency, they are constrained to the Gaussian assumption of classical PCA\nand fail to address algorithm scalability issues. We generalize sparse PCA to\nthe broad exponential family distributions under high-dimensional setup, with\nbuilt-in treatment for missing values. Meanwhile we propose a family of\niterative sparse generalized PCA (SG-PCA) algorithms such that despite the\nnon-convexity and non-smoothness of the optimization task, the loss function\ndecreases in every iteration. In terms of ease and intuitive parameter tuning,\nour sparsity-inducing regularization is far superior to the popular Lasso.\nFurthermore, to promote overall scalability, accelerated gradient is integrated\nfor fast convergence, while a progressive screening technique gradually\nsqueezes out nuisance dimensions of a large-scale problem for feasible\noptimization. High-dimensional simulation and real data experiments demonstrate\nthe efficiency and efficacy of SG-PCA. \n\n"}
{"id": "1512.03952", "contents": "Title: Szeg\\H{o} kernel expansion and equivariant embedding of CR manifolds\n  with circle action Abstract: Let $X$ be a compact strongly pseudoconvex CR manifold with a transversal CR\n$S^1$-action. In this paper, we establish the asymptotic expansion of Szeg\\H{o}\nkernels of positive Fourier components and by using the asymptotics, we show\nthat $X$ can be equivariant CR embedded into some $\\mathbb C^N$ equipped with a\nsimple $S^1$-action. An equivariant embedding of quasi-regular Sasakian\nmanifold is also derived. \n\n"}
{"id": "1512.06086", "contents": "Title: Bayesian anti-sparse coding Abstract: Sparse representations have proven their efficiency in solving a wide class\nof inverse problems encountered in signal and image processing. Conversely,\nenforcing the information to be spread uniformly over representation\ncoefficients exhibits relevant properties in various applications such as\ndigital communications. Anti-sparse regularization can be naturally expressed\nthrough an $\\ell_{\\infty}$-norm penalty. This paper derives a probabilistic\nformulation of such representations. A new probability distribution, referred\nto as the democratic prior, is first introduced. Its main properties as well as\nthree random variate generators for this distribution are derived. Then this\nprobability distribution is used as a prior to promote anti-sparsity in a\nGaussian linear inverse problem, yielding a fully Bayesian formulation of\nanti-sparse coding. Two Markov chain Monte Carlo (MCMC) algorithms are proposed\nto generate samples according to the posterior distribution. The first one is a\nstandard Gibbs sampler. The second one uses Metropolis-Hastings moves that\nexploit the proximity mapping of the log-posterior distribution. These samples\nare used to approximate maximum a posteriori and minimum mean square error\nestimators of both parameters and hyperparameters. Simulations on synthetic\ndata illustrate the performances of the two proposed samplers, for both\ncomplete and over-complete dictionaries. All results are compared to the recent\ndeterministic variational FITRA algorithm. \n\n"}
{"id": "1512.06639", "contents": "Title: A remark on the Ueno-Campana's threefold Abstract: We show that the Ueno-Campana's threefold cannot be obtained as the blow-up\nof any smooth threefold along a smooth centre, answering negatively a question\nraised by Oguiso and Truong. \n\n"}
{"id": "1512.09103", "contents": "Title: Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling Abstract: Accelerated coordinate descent is widely used in optimization due to its\ncheap per-iteration cost and scalability to large-scale problems. Up to a\nprimal-dual transformation, it is also the same as accelerated stochastic\ngradient descent that is one of the central methods used in machine learning.\n  In this paper, we improve the best known running time of accelerated\ncoordinate descent by a factor up to $\\sqrt{n}$. Our improvement is based on a\nclean, novel non-uniform sampling that selects each coordinate with a\nprobability proportional to the square root of its smoothness parameter. Our\nproof technique also deviates from the classical estimation sequence technique\nused in prior work. Our speed-up applies to important problems such as\nempirical risk minimization and solving linear systems, both in theory and in\npractice. \n\n"}
{"id": "1512.09226", "contents": "Title: Computational Limits of A Distributed Algorithm For Smoothing Spline Abstract: In this paper, we explore statistical versus computational trade-off to\naddress a basic question in the application of a distributed algorithm: what is\nthe minimal computational cost in obtaining statistical optimality? In\nsmoothing spline setup, we observe a phase transition phenomenon for the number\nof deployed machines that ends up being a simple proxy for computing cost.\nSpecifically, a sharp upper bound for the number of machines is established:\nwhen the number is below this bound, statistical optimality (in terms of\nnonparametric estimation or testing) is achievable; otherwise, statistical\noptimality becomes impossible. These sharp bounds partly capture intrinsic\ncomputational limits of the distributed algorithm considered in this paper, and\nturn out to be fully determined by the smoothness of the regression function.\nAs a side remark, we argue that sample splitting may be viewed as an\nalternative form of regularization, playing a similar role as smoothing\nparameter. \n\n"}
{"id": "1601.00511", "contents": "Title: Gaussian perturbations of hard edge random matrix ensembles Abstract: We study the eigenvalue correlations of random Hermitian $n\\times n$ matrices\nof the form $S=M+\\epsilon H$, where $H$ is a GUE matrix, $\\epsilon>0$, and $M$\nis a positive-definite Hermitian random matrix, independent of $H$, whose\neigenvalue density is a polynomial ensemble. We show that there is a\nsoft-to-hard edge transition in the microscopic behaviour of the eigenvalues of\n$S$ close to $0$ if $\\epsilon$ tends to $0$ together with $n\\to +\\infty$ at a\ncritical speed, depending on the random matrix $M$. In a double scaling limit,\nwe obtain a new family of limiting eigenvalue correlation kernels. We apply our\ngeneral results to the cases where (i) $M$ is a Laguerre/Wishart random matrix,\n(ii) $M=G^*G$ with $G$ a product of Ginibre matrices, (iii) $M=T^*T$ with $T$ a\nproduct of truncations of Haar distributed unitary matrices, and (iv) the\neigenvalues of $M$ follow a Muttalib-Borodin biorthogonal ensemble. \n\n"}
{"id": "1601.00563", "contents": "Title: On infinite series concerning zeros of Bessel functions of the first\n  kind Abstract: A relevant result independently obtained by Rayleigh and Sneddon on an\nidentity on series involving the zeros of Bessel functions of the first kind is\nderived by an alternative method based on Laplace transforms. Our method leads\nto a Bernstein function of time, expressed by Dirichlet series, that allows us\nto recover the Rayleigh-Sneddon sum. We also consider another method arriving\nat the same result based on a relevant formula by Calogero. Moreover, we also\nprovide an electrical example in which this sum results to be extremely useful\nin order to recover the analytical expression for the response of the system to\na certain external input. \n\n"}
{"id": "1601.01073", "contents": "Title: Multi-Way, Multilingual Neural Machine Translation with a Shared\n  Attention Mechanism Abstract: We propose multi-way, multilingual neural machine translation. The proposed\napproach enables a single neural translation model to translate between\nmultiple languages, with a number of parameters that grows only linearly with\nthe number of languages. This is made possible by having a single attention\nmechanism that is shared across all language pairs. We train the proposed\nmulti-way, multilingual model on ten language pairs from WMT'15 simultaneously\nand observe clear performance improvements over models trained on only one\nlanguage pair. In particular, we observe that the proposed model significantly\nimproves the translation quality of low-resource language pairs. \n\n"}
{"id": "1601.01663", "contents": "Title: Measurement-induced macroscopic superposition states in cavity\n  optomechanics Abstract: We present a novel proposal for generating quantum superpositions of\nmacroscopically distinct states of a bulk mechanical oscillator, compatible\nwith existing optomechanical devices operating in the readily achievable\nbad-cavity limit. The scheme is based on a pulsed cavity optomechanical quantum\nnon-demolition (QND) interaction, driven by displaced non-Gaussian states, and\nmeasurement-induced feedback, avoiding the need for strong single-photon\noptomechanical coupling. Furthermore, we show that single-quadrature cooling of\nthe mechanical oscillator is sufficient for efficient state preparation, and we\noutline a three-pulse protocol comprising a sequence of QND interactions for\nsqueezing-enhanced cooling, state preparation, and tomography. \n\n"}
{"id": "1601.02522", "contents": "Title: Stationary signal processing on graphs Abstract: Graphs are a central tool in machine learning and information processing as\nthey allow to conveniently capture the structure of complex datasets. In this\ncontext, it is of high importance to develop flexible models of signals defined\nover graphs or networks. In this paper, we generalize the traditional concept\nof wide sense stationarity to signals defined over the vertices of arbitrary\nweighted undirected graphs. We show that stationarity is expressed through the\ngraph localization operator reminiscent of translation. We prove that\nstationary graph signals are characterized by a well-defined Power Spectral\nDensity that can be efficiently estimated even for large graphs. We leverage\nthis new concept to derive Wiener-type estimation procedures of noisy and\npartially observed signals and illustrate the performance of this new model for\ndenoising and regression. \n\n"}
{"id": "1601.03822", "contents": "Title: On the consistency of inversion-free parameter estimation for Gaussian\n  random fields Abstract: Gaussian random fields are a powerful tool for modeling environmental\nprocesses. For high dimensional samples, classical approaches for estimating\nthe covariance parameters require highly challenging and massive computations,\nsuch as the evaluation of the Cholesky factorization or solving linear systems.\nRecently, Anitescu, Chen and Stein \\cite{M.Anitescu} proposed a fast and\nscalable algorithm which does not need such burdensome computations. The main\nfocus of this article is to study the asymptotic behavior of the algorithm of\nAnitescu et al. (ACS) for regular and irregular grids in the increasing domain\nsetting. Consistency, minimax optimality and asymptotic normality of this\nalgorithm are proved under mild differentiability conditions on the covariance\nfunction. Despite the fact that ACS's method entails a non-concave\nmaximization, our results hold for any stationary point of the objective\nfunction. A numerical study is presented to evaluate the efficiency of this\nalgorithm for large data sets. \n\n"}
{"id": "1601.04742", "contents": "Title: Evolution of coherence and non-classicality under global environmental\n  interaction Abstract: A master equation has been constructed for a global system-bath interaction\nboth in the absence as well as presence of non-Markovian noise. For the\nmemoryless case, it has been exactly solved for a paradigmatic class of two\nqubit states in high and zero temperature thermal environment. For the\nnon-Markovian model, it has been solved for zero temperature bath. The\nevolution of quantum coherence and entanglement has been observed in presence\nof the above mentioned interactions. We show that the global part of the\nsystem-bath interaction compensates for the decoherence, resulting in slow down\nof coherence and entanglement decay. For an appropriately defined limiting\ncase, both coherence and entanglement show freezing behaviour for the high\ntemperature bath. In case of zero temperature bath, the mentioned interaction\nnot only stabilizes the non-classical correlations, but also enhances them for\na finite period. For the memory dependent case, we have seen that the global\ninteraction enhances the back-flow of information from environment to the\nsystem, as it enhances the regeneration of coherence and entanglement. Also we\nhave studied the generation of Quantum Fisher information by the mentioned\nprocess. An intuitive measure of non-classicality based on non-commutativity of\nquantum states has been considered. Bounds on generated quantum Fisher\ninformation has been found in terms of quantumness and coherence. This gives us\na novel understanding of Quantum Fisher information as a measure of\nnon-classicality. \n\n"}
{"id": "1601.06212", "contents": "Title: Nonparametric Heterogeneity Testing For Massive Data Abstract: A massive dataset often consists of a growing number of (potentially)\nheterogeneous sub-populations. This paper is concerned about testing various\nforms of heterogeneity arising from massive data. In a general nonparametric\nframework, a set of testing procedures are designed to accommodate a growing\nnumber of sub-populations, denoted as $s$, with computational feasibility. In\ntheory, their null limit distributions are derived as being nearly Chi-square\nwith diverging degrees of freedom as long as $s$ does not grow too fast.\nInterestingly, we find that a lower bound on $s$ needs to be set for obtaining\na sufficiently powerful testing result, so-called \"blessing of aggregation.\" As\na by-produc, a type of homogeneity testing is also proposed with a test\nstatistic being aggregated over all sub-populations. Numerical results are\npresented to support our theory. \n\n"}
{"id": "1602.02615", "contents": "Title: Regularity of the Szeg\\\"o projection on model worm domains Abstract: In this paper we study the regularity of the Szeg\\\"o projection on Lebesgue\nand Sobolev spaces on the boundary of the unbounded model worm domain\n$D'_\\beta$.\n  We consider the Hardy space $H^2(D'_\\beta)$. Denoting by $bD'_\\beta$ the\nboundary of $D'_\\beta$, it is classical that $H^2(D'_\\beta)$ can be identified\nwith the closed subspace of $L^2(bD'_\\beta,d\\sigma)$, denoted by\n$H^2(bD'_\\beta)$, consisting of the boundary values of functions in\n$H^2(D'_\\beta)$, where $d\\sigma$ is the induced Lebesgue measure. The\northogonal Hilbert space projection $P: L^2(D'_\\beta,d\\sigma)\\to\nH^2(bD'_\\beta)$ is called the Szeg\\\"o projection.\n  Let $W^{s,p}(bD'_\\beta)$ denote the Lebesgue--Sobolev space on $bD'_\\beta$.\nWe prove that $P$, initially defined on the dense subspace\n$W^{s,p}(bD'_\\beta)\\cap L^2(bD'_\\beta,d\\sigma)$, extends to a bounded operator\n$P: W^{s,p}(bD'_\\beta)\\to\n  W^{s,p}(bD'_\\beta)$, for $1<p<\\infty$ and $s\\ge0$. \n\n"}
{"id": "1602.02738", "contents": "Title: Divergent integrals, residues of Dolbeault forms, and asymptotic Riemann\n  mappings Abstract: We describe the asymptotic behaviour and the dependence on the regularization\nof logarithmically divergent integrals of products of meromorphic and\nantimeromorphic forms on complex manifolds. Our formula is expressed in terms\nof residues of Dolbeault forms, a notion introduced in this paper. The proof is\nbased on a result on the asymptotic behaviour of Riemann mappings of small\ndomains. \n\n"}
{"id": "1602.03114", "contents": "Title: Regularity of weak minimizers of the K-energy and applications to\n  properness and K-stability Abstract: Let $(X,\\omega)$ be a compact K\\\"ahler manifold and $\\mathcal H$ the space of\nK\\\"ahler metrics cohomologous to $\\omega$. If a cscK metric exists in $\\mathcal\nH$, we show that all finite energy minimizers of the extended K-energy are\nsmooth cscK metrics, partially confirming a conjecture of Y.A. Rubinstein and\nthe second author. As an immediate application, we obtain that existence of a\ncscK metric in $\\mathcal H$ implies J-properness of the K-energy, thus\nconfirming one direction of a conjecture of Tian. Exploiting this properness\nresult we prove that an ample line bundle $(X,L)$ admitting a cscK metric in\n$c_1(L)$ is $K$-polystable. \n\n"}
{"id": "1602.03670", "contents": "Title: Online Low-Rank Subspace Learning from Incomplete Data: A Bayesian View Abstract: Extracting the underlying low-dimensional space where high-dimensional\nsignals often reside has long been at the center of numerous algorithms in the\nsignal processing and machine learning literature during the past few decades.\nAt the same time, working with incomplete (partly observed) large scale\ndatasets has recently been commonplace for diverse reasons. This so called {\\it\nbig data era} we are currently living calls for devising online subspace\nlearning algorithms that can suitably handle incomplete data. Their envisaged\nobjective is to {\\it recursively} estimate the unknown subspace by processing\nstreaming data sequentially, thus reducing computational complexity, while\nobviating the need for storing the whole dataset in memory. In this paper, an\nonline variational Bayes subspace learning algorithm from partial observations\nis presented. To account for the unawareness of the true rank of the subspace,\ncommonly met in practice, low-rankness is explicitly imposed on the sought\nsubspace data matrix by exploiting sparse Bayesian learning principles.\nMoreover, sparsity, {\\it simultaneously} to low-rankness, is favored on the\nsubspace matrix by the sophisticated hierarchical Bayesian scheme that is\nadopted. In doing so, the proposed algorithm becomes adept in dealing with\napplications whereby the underlying subspace may be also sparse, as, e.g., in\nsparse dictionary learning problems. As shown, the new subspace tracking scheme\noutperforms its state-of-the-art counterparts in terms of estimation accuracy,\nin a variety of experiments conducted on simulated and real data. \n\n"}
{"id": "1602.04589", "contents": "Title: Optimal Best Arm Identification with Fixed Confidence Abstract: We give a complete characterization of the complexity of best-arm\nidentification in one-parameter bandit problems. We prove a new, tight lower\nbound on the sample complexity. We propose the `Track-and-Stop' strategy, which\nwe prove to be asymptotically optimal. It consists in a new sampling rule\n(which tracks the optimal proportions of arm draws highlighted by the lower\nbound) and in a stopping rule named after Chernoff, for which we give a new\nanalysis. \n\n"}
{"id": "1602.04729", "contents": "Title: Volterra operators on Hardy spaces of Dirichlet series Abstract: For a Dirichlet series symbol $g(s) = \\sum_{n \\geq 1} b_n n^{-s}$, the\nassociated Volterra operator $\\mathbf{T}_g$ acting on a Dirichlet series\n$f(s)=\\sum_{n\\ge 1} a_n n^{-s}$ is defined by the integral $f\\mapsto\n-\\int_{s}^{+\\infty} f(w)g'(w)\\,dw$. We show that $\\mathbf{T}_g$ is a bounded\noperator on the Hardy space $\\mathcal{H}^p$ of Dirichlet series with $0 < p <\n\\infty$ if and only if the symbol $g$ satisfies a Carleson measure condition.\nWhen appropriately restricted to one complex variable, our condition coincides\nwith the standard Carleson measure characterization of\n${\\operatorname{BMOA}}(\\mathbb{D})$. A further analogy with classical\n${\\operatorname{BMO}}$ is that $\\exp(c|g|)$ is integrable (on the infinite\npolytorus) for some $c > 0$ whenever $\\mathbf{T}_g$ is bounded. In particular,\nsuch $g$ belong to $\\mathcal{H}^p$ for every $p < \\infty$. We relate the\nboundedness of $\\mathbf{T}_g$ to several other ${\\operatorname{BMO}}$ type\nspaces: ${\\operatorname{BMOA}}$ in half-planes, the dual of $\\mathcal{H}^1$,\nand the space of symbols of bounded Hankel forms. Moreover, we study symbols\nwhose coefficients enjoy a multiplicative structure and obtain coefficient\nestimates for $m$-homogeneous symbols as well as for general symbols. Finally,\nwe consider the action of $\\mathbf{T}_g$ on reproducing kernels for appropriate\nsequences of subspaces of $\\mathcal{H}^2$. Our proofs employ function and\noperator theoretic techniques in one and several variables; a variety of number\ntheoretic arguments are used throughout the paper in our study of special\nclasses of symbols $g$. \n\n"}
{"id": "1602.07905", "contents": "Title: Thompson Sampling is Asymptotically Optimal in General Environments Abstract: We discuss a variant of Thompson sampling for nonparametric reinforcement\nlearning in a countable classes of general stochastic environments. These\nenvironments can be non-Markov, non-ergodic, and partially observable. We show\nthat Thompson sampling learns the environment class in the sense that (1)\nasymptotically its value converges to the optimal value in mean and (2) given a\nrecoverability assumption regret is sublinear. \n\n"}
{"id": "1603.00370", "contents": "Title: Scalable Metric Learning via Weighted Approximate Rank Component\n  Analysis Abstract: We are interested in the large-scale learning of Mahalanobis distances, with\na particular focus on person re-identification.\n  We propose a metric learning formulation called Weighted Approximate Rank\nComponent Analysis (WARCA). WARCA optimizes the precision at top ranks by\ncombining the WARP loss with a regularizer that favors orthonormal linear\nmappings, and avoids rank-deficient embeddings. Using this new regularizer\nallows us to adapt the large-scale WSABIE procedure and to leverage the Adam\nstochastic optimization algorithm, which results in an algorithm that scales\ngracefully to very large data-sets. Also, we derive a kernelized version which\nallows to take advantage of state-of-the-art features for re-identification\nwhen data-set size permits kernel computation.\n  Benchmarks on recent and standard re-identification data-sets show that our\nmethod beats existing state-of-the-art techniques both in term of accuracy and\nspeed. We also provide experimental analysis to shade lights on the properties\nof the regularizer we use, and how it improves performance. \n\n"}
{"id": "1603.02185", "contents": "Title: Distributed Multi-Task Learning with Shared Representation Abstract: We study the problem of distributed multi-task learning with shared\nrepresentation, where each machine aims to learn a separate, but related, task\nin an unknown shared low-dimensional subspaces, i.e. when the predictor matrix\nhas low rank. We consider a setting where each task is handled by a different\nmachine, with samples for the task available locally on the machine, and study\ncommunication-efficient methods for exploiting the shared structure. \n\n"}
{"id": "1603.03980", "contents": "Title: On Learning High Dimensional Structured Single Index Models Abstract: Single Index Models (SIMs) are simple yet flexible semi-parametric models for\nmachine learning, where the response variable is modeled as a monotonic\nfunction of a linear combination of features. Estimation in this context\nrequires learning both the feature weights and the nonlinear function that\nrelates features to observations. While methods have been described to learn\nSIMs in the low dimensional regime, a method that can efficiently learn SIMs in\nhigh dimensions, and under general structural assumptions, has not been\nforthcoming. In this paper, we propose computationally efficient algorithms for\nSIM inference in high dimensions with structural constraints. Our general\napproach specializes to sparsity, group sparsity, and low-rank assumptions\namong others. Experiments show that the proposed method enjoys superior\npredictive performance when compared to generalized linear models, and achieves\nresults comparable to or better than single layer feedforward neural networks\nwith significantly less computational cost. \n\n"}
{"id": "1603.04095", "contents": "Title: The Rudin-Shapiro polynomials and The Fekete polynomials are not\n  $L^\\alpha$-flat Abstract: We establish that the Rudin-Shapiro polynomials are not $L^\\alpha$-flat, for\nany $\\alpha \\geq 0$. We further prove that the \"truncated\" Rudin-Shapiro\nsequence cannot generate a sequence of $L^\\alpha$-flat polynomials, for any\n$\\alpha \\geq 0$. In the appendix, we present a simple proof of the fact that\nthe Fekete polynomials and the modified or shifted Fekete polynomials are not\n$L^\\alpha$-flat, for any $\\alpha \\geq 0$. \n\n"}
{"id": "1603.04845", "contents": "Title: The Illusory Appeal of Decoherence in the Everettian Picture: Affirming\n  the Consequent Abstract: The idea that decoherence in a unitary-only quantum theory suffices to\nexplain emergence of classical phenomena has been shown in the peer-reviewed\nliterature to be seriously flawed due to circularity. However, claims continue\nto be made that this approach, also known as \"Quantum Darwinism,\" is the\ncorrect way to understand classical emergence. This Letter reviews the basic\nproblem and points out an additional logical flaw in the argument. It is\nconcluded that the \"Quantum Darwinism\" program fails. \n\n"}
{"id": "1603.05728", "contents": "Title: Lelong numbers, complex singularity exponents, and Siu's semicontinuity\n  theorem Abstract: In this note, we present a relationship between Lelong numbers and complex\nsingularity exponents. As an application, we obtain a new proof of Siu's\nsemicontinuity theorem for Lelong numbers. \n\n"}
{"id": "1603.06125", "contents": "Title: The Computational Power of Dynamic Bayesian Networks Abstract: This paper considers the computational power of constant size, dynamic\nBayesian networks. Although discrete dynamic Bayesian networks are no more\npowerful than hidden Markov models, dynamic Bayesian networks with continuous\nrandom variables and discrete children of continuous parents are capable of\nperforming Turing-complete computation. With modified versions of existing\nalgorithms for belief propagation, such a simulation can be carried out in real\ntime. This result suggests that dynamic Bayesian networks may be more powerful\nthan previously considered. Relationships to causal models and recurrent neural\nnetworks are also discussed. \n\n"}
{"id": "1603.08486", "contents": "Title: Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for\n  Automated Image Annotation Abstract: Despite the recent advances in automatically describing image contents, their\napplications have been mostly limited to image caption datasets containing\nnatural images (e.g., Flickr 30k, MSCOCO). In this paper, we present a deep\nlearning model to efficiently detect a disease from an image and annotate its\ncontexts (e.g., location, severity and the affected organs). We employ a\npublicly available radiology dataset of chest x-rays and their reports, and use\nits image annotations to mine disease names to train convolutional neural\nnetworks (CNNs). In doing so, we adopt various regularization techniques to\ncircumvent the large normal-vs-diseased cases bias. Recurrent neural networks\n(RNNs) are then trained to describe the contexts of a detected disease, based\non the deep CNN features. Moreover, we introduce a novel approach to use the\nweights of the already trained pair of CNN/RNN on the domain-specific\nimage/text dataset, to infer the joint image/text contexts for composite image\nlabeling. Significantly improved image annotation results are demonstrated\nusing the recurrent neural cascade model by taking the joint image/text\ncontexts into account. \n\n"}
{"id": "1603.08631", "contents": "Title: Classification of Alzheimer's Disease using fMRI Data and Deep Learning\n  Convolutional Neural Networks Abstract: Over the past decade, machine learning techniques especially predictive\nmodeling and pattern recognition in biomedical sciences from drug delivery\nsystem to medical imaging has become one of the important methods which are\nassisting researchers to have deeper understanding of entire issue and to solve\ncomplex medical problems. Deep learning is power learning machine learning\nalgorithm in classification while extracting high-level features. In this\npaper, we used convolutional neural network to classify Alzheimer's brain from\nnormal healthy brain. The importance of classifying this kind of medical data\nis to potentially develop a predict model or system in order to recognize the\ntype disease from normal subjects or to estimate the stage of the disease.\nClassification of clinical data such as Alzheimer's disease has been always\nchallenging and most problematic part has been always selecting the most\ndiscriminative features. Using Convolutional Neural Network (CNN) and the\nfamous architecture LeNet-5, we successfully classified functional MRI data of\nAlzheimer's subjects from normal controls where the accuracy of test data on\ntrained data reached 96.85%. This experiment suggests us the shift and scale\ninvariant features extracted by CNN followed by deep learning classification is\nmost powerful method to distinguish clinical data from healthy data in fMRI.\nThis approach also enables us to expand our methodology to predict more\ncomplicated systems. \n\n"}
{"id": "1604.00289", "contents": "Title: Building Machines That Learn and Think Like People Abstract: Recent progress in artificial intelligence (AI) has renewed interest in\nbuilding systems that learn and think like people. Many advances have come from\nusing deep neural networks trained end-to-end in tasks such as object\nrecognition, video games, and board games, achieving performance that equals or\neven beats humans in some respects. Despite their biological inspiration and\nperformance achievements, these systems differ from human intelligence in\ncrucial ways. We review progress in cognitive science suggesting that truly\nhuman-like learning and thinking machines will have to reach beyond current\nengineering trends in both what they learn, and how they learn it.\nSpecifically, we argue that these machines should (a) build causal models of\nthe world that support explanation and understanding, rather than merely\nsolving pattern recognition problems; (b) ground learning in intuitive theories\nof physics and psychology, to support and enrich the knowledge that is learned;\nand (c) harness compositionality and learning-to-learn to rapidly acquire and\ngeneralize knowledge to new tasks and situations. We suggest concrete\nchallenges and promising routes towards these goals that can combine the\nstrengths of recent neural network advances with more structured cognitive\nmodels. \n\n"}
{"id": "1604.01480", "contents": "Title: A domain with non-plurisubharmonic squeezing function Abstract: We construct a strictly pseudoconvex domain with smooth boundary whose\nsqueezing function is not plurisubharmonic. \n\n"}
{"id": "1604.06654", "contents": "Title: Numerically stable conditions on rational and essential singularities Abstract: This paper demonstrates some connections between the coefficients of a Taylor\nseries $f(z)=\\ds\\sum_{n=0}^\\infty a_n z^n$ and singularities of the function.\nThere are many known results of this type, for example, counting the number of\npoles on the circle of convergence, and doing convergence or overconvergence\nfor $f$ on any arc of holomorphy. A new approach proposed here is that these\nkinds of results are extended by relaxing the classical conditions for\nsingularities and convergence theorems. This is done by allowing the\ncoefficients to be sufficiently small instead of being zero. \n\n"}
{"id": "1604.07653", "contents": "Title: Ultra-precise holographic beam shaping for microscopic quantum control Abstract: High-resolution addressing of individual ultracold atoms, trapped ions or\nsolid state emitters allows for exquisite control in quantum optics\nexperiments. This becomes possible through large aperture magnifying optics\nthat project microscopic light patterns with diffraction limited performance.\nWe use programmable amplitude holograms generated on a digital micromirror\ndevice to create arbitrary microscopic beam shapes with full phase and\namplitude control. The system self-corrects for aberrations of up to several\n$\\lambda$ and reduces them to $\\lambda/50$, leading to light patterns with a\nprecision on the $10^{-4}$ level. We demonstrate aberration-compensated beam\nshaping in an optical lattice experiment and perform single-site addressing in\na quantum gas microscope for $^{87}$Rb. \n\n"}
{"id": "1604.08522", "contents": "Title: Spin evolution of cold atomic gases in SU(2)$\\otimes $U(1) fields Abstract: We consider response function and spin evolution in spin-orbit coupled cold\natomic gases in a synthetic gauge magnetic field influencing solely the orbital\nmotion of atoms. We demonstrate that various regimes of spin-orbit coupling\nstrength, magnetic field, and disorder can be treated within a single approach\nbased on the representation of atomic motion in terms of auxiliary collective\nclassical trajectories. Our approach allows for a unified description of\nfermionic and bosonic gases. \n\n"}
{"id": "1605.03707", "contents": "Title: Optimal Bayes Classifiers for Functional Data and Density Ratios Abstract: Bayes classifiers for functional data pose a challenge. This is because\nprobability density functions do not exist for functional data. As a\nconsequence, the classical Bayes classifier using density quotients needs to be\nmodified. We propose to use density ratios of projections on a sequence of\neigenfunctions that are common to the groups to be classified. The density\nratios can then be factored into density ratios of individual functional\nprincipal components whence the classification problem is reduced to a sequence\nof nonparametric one-dimensional density estimates. This is an extension to\nfunctional data of some of the very earliest nonparametric Bayes classifiers\nthat were based on simple density ratios in the one-dimensional case. By means\nof the factorization of the density quotients the curse of dimensionality that\nwould otherwise severely affect Bayes classifiers for functional data can be\navoided. We demonstrate that in the case of Gaussian functional data, the\nproposed functional Bayes classifier reduces to a functional version of the\nclassical quadratic discriminant. A study of the asymptotic behavior of the\nproposed classifiers in the large sample limit shows that under certain\nconditions the misclassification rate converges to zero, a phenomenon that has\nbeen referred to as \"perfect classification\". The proposed classifiers also\nperform favorably in finite sample applications, as we demonstrate in\ncomparisons with other functional classifiers in simulations and various data\napplications, including wine spectral data, functional magnetic resonance\nimaging (fMRI) data for attention deficit hyperactivity disorder (ADHD)\npatients, and yeast gene expression data. \n\n"}
{"id": "1605.07127", "contents": "Title: Learning and Policy Search in Stochastic Dynamical Systems with Bayesian\n  Neural Networks Abstract: We present an algorithm for model-based reinforcement learning that combines\nBayesian neural networks (BNNs) with random roll-outs and stochastic\noptimization for policy learning. The BNNs are trained by minimizing\n$\\alpha$-divergences, allowing us to capture complicated statistical patterns\nin the transition dynamics, e.g. multi-modality and heteroskedasticity, which\nare usually missed by other common modeling approaches. We illustrate the\nperformance of our method by solving a challenging benchmark where model-based\napproaches usually fail and by obtaining promising results in a real-world\nscenario for controlling a gas turbine. \n\n"}
{"id": "1605.07422", "contents": "Title: Computing Web-scale Topic Models using an Asynchronous Parameter Server Abstract: Topic models such as Latent Dirichlet Allocation (LDA) have been widely used\nin information retrieval for tasks ranging from smoothing and feedback methods\nto tools for exploratory search and discovery. However, classical methods for\ninferring topic models do not scale up to the massive size of today's publicly\navailable Web-scale data sets. The state-of-the-art approaches rely on custom\nstrategies, implementations and hardware to facilitate their asynchronous,\ncommunication-intensive workloads.\n  We present APS-LDA, which integrates state-of-the-art topic modeling with\ncluster computing frameworks such as Spark using a novel asynchronous parameter\nserver. Advantages of this integration include convenient usage of existing\ndata processing pipelines and eliminating the need for disk writes as data can\nbe kept in memory from start to finish. Our goal is not to outperform highly\ncustomized implementations, but to propose a general high-performance topic\nmodeling framework that can easily be used in today's data processing\npipelines. We compare APS-LDA to the existing Spark LDA implementations and\nshow that our system can, on a 480-core cluster, process up to 135 times more\ndata and 10 times more topics without sacrificing model quality. \n\n"}
{"id": "1605.07478", "contents": "Title: Storage enhanced nonlinearities in a cold atomic Rydberg ensemble Abstract: The combination of electromagnetically induced transparency (EIT) with the\nnonlinear interaction between Rydberg atoms provides an effective interaction\nbetween photons. In this paper, we investigate the storage of optical pulses as\ncollective Rydberg atomic excitations in a cold atomic ensemble. By measuring\nthe dynamics of the stored Rydberg polaritons, we experimentally demonstrate\nthat storing a probe pulse as Rydberg polaritons strongly enhances the Rydberg\nmediated interaction compared to the slow propagation case. We show that the\nprocess is characterized by two time scales. At short storage times, we observe\na strong enhancement of the interaction due to the reduction of the Rydberg\npolariton group velocity down to zero. For longer storage times, we observe a\nfurther, weaker enhancement dominated by Rydberg induced dephasing of the\nmultiparticle components of the state. In this regime, we observe a non-linear\ndependence of the Rydberg polariton coherence time with the input photon\nnumber. Our results have direct consequences in Rydberg quantum optics and\nenable the test of new theories of strongly interacting Rydberg systems. \n\n"}
{"id": "1605.08361", "contents": "Title: No bad local minima: Data independent training error guarantees for\n  multilayer neural networks Abstract: We use smoothed analysis techniques to provide guarantees on the training\nloss of Multilayer Neural Networks (MNNs) at differentiable local minima.\nSpecifically, we examine MNNs with piecewise linear activation functions,\nquadratic loss and a single output, under mild over-parametrization. We prove\nthat for a MNN with one hidden layer, the training error is zero at every\ndifferentiable local minimum, for almost every dataset and dropout-like noise\nrealization. We then extend these results to the case of more than one hidden\nlayer. Our theoretical guarantees assume essentially nothing on the training\ndata, and are verified numerically. These results suggest why the highly\nnon-convex loss of such MNNs can be easily optimized using local updates (e.g.,\nstochastic gradient descent), as observed empirically. \n\n"}
{"id": "1605.08375", "contents": "Title: Generalization Properties and Implicit Regularization for Multiple\n  Passes SGM Abstract: We study the generalization properties of stochastic gradient methods for\nlearning with convex loss functions and linearly parameterized functions. We\nshow that, in the absence of penalizations or constraints, the stability and\napproximation properties of the algorithm can be controlled by tuning either\nthe step-size or the number of passes over the data. In this view, these\nparameters can be seen to control a form of implicit regularization. Numerical\nresults complement the theoretical findings. \n\n"}
{"id": "1605.08656", "contents": "Title: Twistor interpretation of slice regular functions Abstract: Given a slice regular function $f:\\Omega\\subset\\mathbb{H}\\to \\mathbb{H}$,\nwith $\\Omega\\cap\\mathbb{R}\\neq \\emptyset$, it is possible to lift it to a\nsurface in the twistor space $\\mathbb{CP}^{3}$ of $\\mathbb{S}^4\\simeq\n\\mathbb{H}\\cup \\{\\infty\\}$ (see~\\cite{gensalsto}). In this paper we show that\nthe same result is true if one removes the hypothesis $\\Omega\\cap\\mathbb{R}\\neq\n\\emptyset$ on the domain of the function $f$. Moreover we find that if a\nsurface $\\mathcal{S}\\subset\\mathbb{CP}^{3}$ contains the image of the twistor\nlift of a slice regular function, then $\\mathcal{S}$ has to be ruled by lines.\nStarting from these results we find all the projective classes of algebraic\nsurfaces up to degree 3 in $\\mathbb{CP}^{3}$ that contain the lift of a slice\nregular function. In addition we extend and further explore the so-called\ntwistor transform, that is a curve in $\\mathbb{G}r_2(\\mathbb{C}^4)$ which,\ngiven a slice regular function, returns the arrangement of lines whose lift\ncarries on. With the explicit expression of the twistor lift and of the twistor\ntransform of a slice regular function we exhibit the set of slice regular\nfunctions whose twistor transform describes a rational line inside\n$\\mathbb{G}r_2(\\mathbb{C}^4)$, showing the role of slice regular functions not\ndefined on $\\mathbb{R}$. At the end we study the twistor lift of a particular\nslice regular function not defined over the reals. This example shows the\neffectiveness of our approach and opens some questions. \n\n"}
{"id": "1605.09499", "contents": "Title: Extreme Stochastic Variational Inference: Distributed and Asynchronous Abstract: Stochastic variational inference (SVI), the state-of-the-art algorithm for\nscaling variational inference to large-datasets, is inherently serial.\nMoreover, it requires the parameters to fit in the memory of a single\nprocessor; this is problematic when the number of parameters is in billions. In\nthis paper, we propose extreme stochastic variational inference (ESVI), an\nasynchronous and lock-free algorithm to perform variational inference for\nmixture models on massive real world datasets. ESVI overcomes the limitations\nof SVI by requiring that each processor only access a subset of the data and a\nsubset of the parameters, thus providing data and model parallelism\nsimultaneously. We demonstrate the effectiveness of ESVI by running Latent\nDirichlet Allocation (LDA) on UMBC-3B, a dataset that has a vocabulary of 3\nmillion and a token size of 3 billion. In our experiments, we found that ESVI\nnot only outperforms VI and SVI in wallclock-time, but also achieves a better\nquality solution. In addition, we propose a strategy to speed up computation\nand save memory when fitting large number of topics. \n\n"}
{"id": "1606.00256", "contents": "Title: From Graphs to Keyed Quantum Hash Functions Abstract: We present two new constructions of quantum hash functions: the first based\non expander graphs and the second based on extractor functions and estimate the\namount of randomness that is needed to construct them. We also propose a keyed\nquantum hash function based on extractor function that can be used in quantum\nmessage authentication codes and assess its security in a limited attacker\nmodel. \n\n"}
{"id": "1606.01865", "contents": "Title: Recurrent Neural Networks for Multivariate Time Series with Missing\n  Values Abstract: Multivariate time series data in practical applications, such as health care,\ngeoscience, and biology, are characterized by a variety of missing values. In\ntime series prediction and other related tasks, it has been noted that missing\nvalues and their missing patterns are often correlated with the target labels,\na.k.a., informative missingness. There is very limited work on exploiting the\nmissing patterns for effective imputation and improving prediction performance.\nIn this paper, we develop novel deep learning models, namely GRU-D, as one of\nthe early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a\nstate-of-the-art recurrent neural network. It takes two representations of\nmissing patterns, i.e., masking and time interval, and effectively incorporates\nthem into a deep model architecture so that it not only captures the long-term\ntemporal dependencies in time series, but also utilizes the missing patterns to\nachieve better prediction results. Experiments of time series classification\ntasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic\ndatasets demonstrate that our models achieve state-of-the-art performance and\nprovides useful insights for better understanding and utilization of missing\nvalues in time series analysis. \n\n"}
{"id": "1606.02401", "contents": "Title: On clustering network-valued data Abstract: Community detection, which focuses on clustering nodes or detecting\ncommunities in (mostly) a single network, is a problem of considerable\npractical interest and has received a great deal of attention in the research\ncommunity. While being able to cluster within a network is important, there are\nemerging needs to be able to cluster multiple networks. This is largely\nmotivated by the routine collection of network data that are generated from\npotentially different populations. These networks may or may not have node\ncorrespondence. When node correspondence is present, we cluster networks by\nsummarizing a network by its graphon estimate, whereas when node correspondence\nis not present, we propose a novel solution for clustering such networks by\nassociating a computationally feasible feature vector to each network based on\ntrace of powers of the adjacency matrix. We illustrate our methods using both\nsimulated and real data sets, and theoretical justifications are provided in\nterms of consistency. \n\n"}
{"id": "1606.03168", "contents": "Title: Finding Low-Rank Solutions via Non-Convex Matrix Factorization,\n  Efficiently and Provably Abstract: A rank-$r$ matrix $X \\in \\mathbb{R}^{m \\times n}$ can be written as a product\n$U V^\\top$, where $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n\n\\times r}$. One could exploit this observation in optimization: e.g., consider\nthe minimization of a convex function $f(X)$ over rank-$r$ matrices, where the\nset of rank-$r$ matrices is modeled via the factorization $UV^\\top$. Though\nsuch parameterization reduces the number of variables, and is more\ncomputationally efficient (of particular interest is the case $r \\ll \\min\\{m,\nn\\}$), it comes at a cost: $f(UV^\\top)$ becomes a non-convex function w.r.t.\n$U$ and $V$.\n  We study such parameterization for optimization of generic convex objectives\n$f$, and focus on first-order, gradient descent algorithmic solutions. We\npropose the Bi-Factored Gradient Descent (BFGD) algorithm, an efficient\nfirst-order method that operates on the $U, V$ factors. We show that when $f$\nis (restricted) smooth, BFGD has local sublinear convergence, and linear\nconvergence when $f$ is both (restricted) smooth and (restricted) strongly\nconvex. For several key applications, we provide simple and efficient\ninitialization schemes that provide approximate solutions good enough for the\nabove convergence results to hold. \n\n"}
{"id": "1606.04478", "contents": "Title: Bayesian Inference on Matrix Manifolds for Linear Dimensionality\n  Reduction Abstract: We reframe linear dimensionality reduction as a problem of Bayesian inference\non matrix manifolds. This natural paradigm extends the Bayesian framework to\ndimensionality reduction tasks in higher dimensions with simpler models at\ngreater speeds. Here an orthogonal basis is treated as a single point on a\nmanifold and is associated with a linear subspace on which observations vary\nmaximally. Throughout this paper, we employ the Grassmann and Stiefel manifolds\nfor various dimensionality reduction problems, explore the connection between\nthe two manifolds, and use Hybrid Monte Carlo for posterior sampling on the\nGrassmannian for the first time. We delineate in which situations either\nmanifold should be considered. Further, matrix manifold models are used to\nyield scientific insight in the context of cognitive neuroscience, and we\nconclude that our methods are suitable for basic inference as well as accurate\nprediction. \n\n"}
{"id": "1606.06962", "contents": "Title: Towards stationary time-vertex signal processing Abstract: Graph-based methods for signal processing have shown promise for the analysis\nof data exhibiting irregular structure, such as those found in social,\ntransportation, and sensor networks. Yet, though these systems are often\ndynamic, state-of-the-art methods for signal processing on graphs ignore the\ndimension of time, treating successive graph signals independently or taking a\nglobal average. To address this shortcoming, this paper considers the\nstatistical analysis of time-varying graph signals. We introduce a novel\ndefinition of joint (time-vertex) stationarity, which generalizes the classical\ndefinition of time stationarity and the more recent definition appropriate for\ngraphs. Joint stationarity gives rise to a scalable Wiener optimization\nframework for joint denoising, semi-supervised learning, or more generally\ninversing a linear operator, that is provably optimal. Experimental results on\nreal weather data demonstrate that taking into account graph and time\ndimensions jointly can yield significant accuracy improvements in the\nreconstruction effort. \n\n"}
{"id": "1606.09505", "contents": "Title: Synthetic Unruh effect in cold atoms Abstract: We propose to simulate a Dirac field near an event horizon using ultracold\natoms in an optical lattice. Such a quantum simulator allows for the\nobservation of the celebrated Unruh effect. Our proposal involves three stages:\n(1) preparation of the ground state of a massless 2D Dirac field in Minkowski\nspacetime; (2) quench of the optical lattice setup to simulate how an\naccelerated observer would view that state; (3) measurement of the local\nquantum fluctuation spectra by one-particle excitation spectroscopy in order to\nsimulate a De Witt detector. According to Unruh's prediction, fluctuations\nmeasured in such a way must be thermal. Moreover, following Takagi's inversion\ntheorem, they will obey the Bose-Einstein distribution, which will smoothly\ntransform into the Fermi-Dirac as one of the dimensions of the lattice is\nreduced. \n\n"}
{"id": "1607.00893", "contents": "Title: Differential tests for plurisubharmonic functions and Koch curves Abstract: We study minimum sets of singular plurisubharmonic functions and their\nrelation to upper contact sets. In particular we develop an algorithm checking\nwhen a naturally parametrized curve is such a minimum set. The case of Koch\ncurves is studied in detail. We also study the size of the set of upper\nnon-contact points. We show that this set is always of Lebesgue measure zero\nthus answering an open problem in the viscosity approach to the complex\nMonge-Amp\\`ere equation. Finally, we prove that similarly to the case of convex\nfunctions, strictly plurisubharmonic lower tests yield existence of upper tests\nwith a control on the opening. \n\n"}
{"id": "1607.02665", "contents": "Title: Classifier Risk Estimation under Limited Labeling Resources Abstract: In this paper we propose strategies for estimating performance of a\nclassifier when labels cannot be obtained for the whole test set. The number of\ntest instances which can be labeled is very small compared to the whole test\ndata size. The goal then is to obtain a precise estimate of classifier\nperformance using as little labeling resource as possible. Specifically, we try\nto answer, how to select a subset of the large test set for labeling such that\nthe performance of a classifier estimated on this subset is as close as\npossible to the one on the whole test set. We propose strategies based on\nstratified sampling for selecting this subset. We show that these strategies\ncan reduce the variance in estimation of classifier accuracy by a significant\namount compared to simple random sampling (over 65% in several cases). Hence,\nour proposed methods are much more precise compared to random sampling for\naccuracy estimation under restricted labeling resources. The reduction in\nnumber of samples required (compared to random sampling) to estimate the\nclassifier accuracy with only 1% error is high as 60% in some cases. \n\n"}
{"id": "1607.02801", "contents": "Title: Bounds on the Number of Measurements for Reliable Compressive\n  Classification Abstract: This paper studies the classification of high-dimensional Gaussian signals\nfrom low-dimensional noisy, linear measurements. In particular, it provides\nupper bounds (sufficient conditions) on the number of measurements required to\ndrive the probability of misclassification to zero in the low-noise regime,\nboth for random measurements and designed ones. Such bounds reveal two\nimportant operational regimes that are a function of the characteristics of the\nsource: i) when the number of classes is less than or equal to the dimension of\nthe space spanned by signals in each class, reliable classification is possible\nin the low-noise regime by using a one-vs-all measurement design; ii) when the\ndimension of the spaces spanned by signals in each class is lower than the\nnumber of classes, reliable classification is guaranteed in the low-noise\nregime by using a simple random measurement design. Simulation results both\nwith synthetic and real data show that our analysis is sharp, in the sense that\nit is able to gauge the number of measurements required to drive the\nmisclassification probability to zero in the low-noise regime. \n\n"}
{"id": "1607.02852", "contents": "Title: Exact Casimir interaction of perfectly conducting three-spheres in four\n  euclidean dimensions Abstract: Exploiting conformal symmetry, we derive a simple exact formula for the\nclassical electromagnetic Casimir interaction of two perfectly conducting\nthree-spheres, including the sphere-plate geometry as a special case, in four\neuclidean dimensions. We verify that the short distance expansion of the\nCasimir energy agrees to leading order with the Proximity Force Approximation\n(PFA), while the next-to-leading-order is in agreement with a recently proposed\nderivative expansion of the Casimir energy. At the next-to-next-to-leading\norder we find a non-analytic correction to PFA, which for a sphere-plate system\nis of the order of $(d/R)^{3/2} \\log(d/R)$, where $d$ is the separation and $R$\nthe sphere radius. \n\n"}
{"id": "1607.06072", "contents": "Title: On the symmetry algebras of 5-dimensional CR-manifolds Abstract: We show that for a real-analytic connected holomorphically nondegenerate\n5-dimensional CR-hypersurface $M$ and its symmetry algebra $\\mathfrak{s}$ one\nhas either: (i) $\\dim\\mathfrak{s}=15$ and $M$ is spherical (with Levi form of\nsignature either $(2,0)$ or $(1,1)$ everywhere), or (ii)\n$\\dim\\mathfrak{s}\\le11$ where $\\dim\\mathfrak{s}=11$ can only occur if on a\ndense open subset $M$ is spherical with Levi form of signature $(1,1)$.\nFurthermore, we construct a series of examples of pairwise nonequivalent\nCR-hypersurfaces with $\\dim\\mathfrak{s}=11$. \n\n"}
{"id": "1607.06317", "contents": "Title: A Multi-cut Formulation for Joint Segmentation and Tracking of Multiple\n  Objects Abstract: Recently, Minimum Cost Multicut Formulations have been proposed and proven to\nbe successful in both motion trajectory segmentation and multi-target tracking\nscenarios. Both tasks benefit from decomposing a graphical model into an\noptimal number of connected components based on attractive and repulsive\npairwise terms. The two tasks are formulated on different levels of granularity\nand, accordingly, leverage mostly local information for motion segmentation and\nmostly high-level information for multi-target tracking. In this paper we argue\nthat point trajectories and their local relationships can contribute to the\nhigh-level task of multi-target tracking and also argue that high-level cues\nfrom object detection and tracking are helpful to solve motion segmentation. We\npropose a joint graphical model for point trajectories and object detections\nwhose Multicuts are solutions to motion segmentation {\\it and} multi-target\ntracking problems at once. Results on the FBMS59 motion segmentation benchmark\nas well as on pedestrian tracking sequences from the 2D MOT 2015 benchmark\ndemonstrate the promise of this joint approach. \n\n"}
{"id": "1608.00027", "contents": "Title: gLOP: the global and Local Penalty for Capturing Predictive\n  Heterogeneity Abstract: When faced with a supervised learning problem, we hope to have rich enough\ndata to build a model that predicts future instances well. However, in\npractice, problems can exhibit predictive heterogeneity: most instances might\nbe relatively easy to predict, while others might be predictive outliers for\nwhich a model trained on the entire dataset does not perform well. Identifying\nthese can help focus future data collection. We present gLOP, the global and\nLocal Penalty, a framework for capturing predictive heterogeneity and\nidentifying predictive outliers. gLOP is based on penalized regression for\nmultitask learning, which improves learning by leveraging training signal\ninformation from related tasks. We give two optimization algorithms for gLOP,\none space-efficient, and another giving the full regularization path. We also\ncharacterize uniqueness in terms of the data and tuning parameters, and present\nempirical results on synthetic data and on two health research problems. \n\n"}
{"id": "1608.02459", "contents": "Title: Efficient electrical spin readout of NV- centers in diamond Abstract: Using pulsed photoionization the coherent spin manipulation and echo\nformation of ensembles of NV- centers in diamond are detected electrically\nrealizing contrasts of up to 17 %. The underlying spin-dependent ionization\ndynamics are investigated experimentally and compared to Monte-Carlo\nsimulations. This allows the identification of the conditions optimizing\ncontrast and sensitivity which compare favorably with respect to optical\ndetection. \n\n"}
{"id": "1608.03022", "contents": "Title: Dynamic Principal Component Analysis: Identifying the Relationship\n  between Multiple Air Pollutants Abstract: The dynamic nature of air quality chemistry and transport makes it difficult\nto identify the mixture of air pollutants for a region. In this study of air\nquality in the Houston metropolitan area we apply dynamic principal component\nanalysis (DPCA) to a normalized multivariate time series of daily concentration\nmeasurements of five pollutants (O3, CO, NO2, SO2, PM2.5) from January 1, 2009\nthrough December 31, 2011 for each of the 24 hours in a day. The resulting\ndynamic components are examined by hour across days for the 3 year period.\nDiurnal and seasonal patterns are revealed underlining times when DPCA performs\nbest and two principal components (PCs) explain most variability in the\nmultivariate series. DPCA is shown to be superior to static principal component\nanalysis (PCA) in discovery of linear relations among transformed pollutant\nmeasurements. DPCA captures the time-dependent correlation structure of the\nunderlying pollutants recorded at up to 34 monitoring sites in the region. In\nwinter mornings the first principal component (PC1) (mainly CO and NO2)\nexplains up to 70% of variability. Augmenting with the second principal\ncomponent (PC2) (mainly driven by SO2) the explained variability rises to 90%.\nIn the afternoon, O3 gains prominence in the second principal component. The\nseasonal profile of PCs' contribution to variance loses its distinction in the\nafternoon, yet cumulatively PC1 and PC2 still explain up to 65% of variability\nin ambient air data. DPCA provides a strategy for identifying the changing air\nquality profile for the region studied. \n\n"}
{"id": "1608.03045", "contents": "Title: Combinatorial Inference for Graphical Models Abstract: We propose a new family of combinatorial inference problems for graphical\nmodels. Unlike classical statistical inference where the main interest is point\nestimation or parameter testing, combinatorial inference aims at testing the\nglobal structure of the underlying graph. Examples include testing the graph\nconnectivity, the presence of a cycle of certain size, or the maximum degree of\nthe graph. To begin with, we develop a unified theory for the fundamental\nlimits of a large family of combinatorial inference problems. We propose new\nconcepts including structural packing and buffer entropies to characterize how\nthe complexity of combinatorial graph structures impacts the corresponding\nminimax lower bounds. On the other hand, we propose a family of novel and\npractical structural testing algorithms to match the lower bounds. We provide\nthorough numerical results on both synthetic graphical models and brain\nnetworks to illustrate the usefulness of these proposed methods. \n\n"}
{"id": "1608.05872", "contents": "Title: Sparse Beltrami coefficients, integral means of conformal mappings and\n  the Feynman-Kac formula Abstract: In this note, we give an estimate for the dimension of the image of the unit\ncircle under a quasiconformal mapping whose dilatation has small support. We\nalso prove an analogous estimate for the rate of growth of a solution of a\nsecond-order parabolic equation given by the Feynman-Kac formula (with a\nsparsely supported potential) and introduce a dictionary between the two\nsettings. \n\n"}
{"id": "1608.05983", "contents": "Title: Inverting Variational Autoencoders for Improved Generative Accuracy Abstract: Recent advances in semi-supervised learning with deep generative models have\nshown promise in generalizing from small labeled datasets\n($\\mathbf{x},\\mathbf{y}$) to large unlabeled ones ($\\mathbf{x}$). In the case\nwhere the codomain has known structure, a large unfeatured dataset\n($\\mathbf{y}$) is potentially available. We develop a parameter-efficient, deep\nsemi-supervised generative model for the purpose of exploiting this untapped\ndata source. Empirical results show improved performance in disentangling\nlatent variable semantics as well as improved discriminative prediction on\nMartian spectroscopic and handwritten digit domains. \n\n"}
{"id": "1608.06366", "contents": "Title: Accelerated Alternating Direction Method of Multipliers: an Optimal\n  $O(1/K)$ Nonergodic Analysis Abstract: The Alternating Direction Method of Multipliers (ADMM) is widely used for\nlinearly constrained convex problems. It is proven to have an $o(1/\\sqrt{K})$\nnonergodic convergence rate and a faster $O(1/K)$ ergodic rate after ergodic\naveraging, which may destroy the sparsity and low-rankness in sparse and\nlow-rank learning, where $K$ is the number of iterations. In this paper, we\nmodify the accelerated ADMM proposed in [Y. Ouyang, Y. Chen, G. Lan, and E.\nPasiliao, An Accelerated Linearized Alternating Direction Method of\nMultipliers, SIAM J. on Imaging Sciences, 2015, 1588-1623] and give an $O(1/K)$\nnonergodic convergence rate analysis, which satisfies $|F(x^K)-F(x^*)|\\leq\nO(1/K)$, $\\|Ax^K-b\\|\\leq O(1/K)$ and $x^K$ has a more favorable sparseness and\nlow-rankness than the ergodic result. As far as we know, this is the first\n$O(1/K)$ nonergodic convergent ADMM type method for general linearly\nconstrained convex problems. Moreover, we show that the lower complexity bound\nof ADMM type methods for the separable linearly constrained nonsmooth convex\nproblems is $O(1/K)$, which means that our method is optimal. \n\n"}
{"id": "1608.06383", "contents": "Title: Softplus Regressions and Convex Polytopes Abstract: To construct flexible nonlinear predictive distributions, the paper\nintroduces a family of softplus function based regression models that convolve,\nstack, or combine both operations by convolving countably infinite stacked\ngamma distributions, whose scales depend on the covariates. Generalizing\nlogistic regression that uses a single hyperplane to partition the covariate\nspace into two halves, softplus regressions employ multiple hyperplanes to\nconstruct a confined space, related to a single convex polytope defined by the\nintersection of multiple half-spaces or a union of multiple convex polytopes,\nto separate one class from the other. The gamma process is introduced to\nsupport the convolution of countably infinite (stacked) covariate-dependent\ngamma distributions. For Bayesian inference, Gibbs sampling derived via novel\ndata augmentation and marginalization techniques is used to deconvolve and/or\ndemix the highly complex nonlinear predictive distribution. Example results\ndemonstrate that softplus regressions provide flexible nonlinear decision\nboundaries, achieving classification accuracies comparable to that of kernel\nsupport vector machine while requiring significant less computation for\nout-of-sample prediction. \n\n"}
{"id": "1608.07217", "contents": "Title: The Poincar\\'e problem in the dicritical case Abstract: We develop a study on local polar invariants of planar complex analytic\nfoliations at $(\\mathbb{C}^{2},0)$, which leads to the characterization of\nsecond type foliations and of generalized curve foliations, as well as a\ndescription of the $GSV$-index. We apply it to the Poincar\\'e problem for\nfoliations on the complex projective plane $\\mathbb{P}^{2}_{\\mathbb{C}}$,\nestablishing, in the dicritical case, conditions for the existence of a bound\nfor the degree of an invariant algebraic curve $S$ in terms of the degree of\nthe foliation $\\mathcal{F}$. We characterize the existence of a solution for\nthe Poincar\\'e problem in terms of the structure of the set of local\nseparatrices of $\\mathcal{F}$ over the curve $S$. Our method, in particular,\nrecovers the known solution for the non-dicritical case, ${\\rm deg}(S) \\leq\n{\\rm deg}(\\mathcal{F}) + 2$. \n\n"}
{"id": "1608.08329", "contents": "Title: Qudit-Based Measurement-Device-Independent Quantum Key Distribution\n  Using Linear Optics Abstract: Measurement-device-independent (MDI) method is a way to solve all detector\nside-channel attacks in quantum key distribution (QKD). However, very little\nwork has been done on experimentally feasible qudit-based MDI-QKD scheme\nalthough the famous (qudit-based) round-robin differential-phase-shift (RRDPS)\nscheme is vulnerable to attacks on uncharacterized detectors. Here we report a\nmother-of-all QKD protocol on which all provably secure qubit-based QKD schemes\nknown to date including the RRDPS and the so-called Chau15 schemes are based.\nWe also report an experimentally feasible MDI system via optical implementation\nof entanglement swapping based on a recent qudit teleportation proposal by\nGoyal et al. In this way, we show that all provably secure qudit-based QKD\nschemes discovered to date can be made MDI. \n\n"}
{"id": "1609.01025", "contents": "Title: Structured signal recovery from non-linear and heavy-tailed measurements Abstract: We study high-dimensional signal recovery from non-linear measurements with\ndesign vectors having elliptically symmetric distribution. Special attention is\ndevoted to the situation when the unknown signal belongs to a set of low\nstatistical complexity, while both the measurements and the design vectors are\nheavy-tailed. We propose and analyze a new estimator that adapts to the\nstructure of the problem, while being robust both to the possible model\nmisspecification characterized by arbitrary non-linearity of the measurements\nas well as to data corruption modeled by the heavy-tailed distributions.\nMoreover, this estimator has low computational complexity. Our results are\nexpressed in the form of exponential concentration inequalities for the error\nof the proposed estimator. On the technical side, our proofs rely on the\ngeneric chaining methods, and illustrate the power of this approach for\nstatistical applications. Theory is supported by numerical experiments\ndemonstrating that our estimator outperforms existing alternatives when data is\nheavy-tailed. \n\n"}
{"id": "1609.01596", "contents": "Title: Direct Feedback Alignment Provides Learning in Deep Neural Networks Abstract: Artificial neural networks are most commonly trained with the\nback-propagation algorithm, where the gradient for learning is provided by\nback-propagating the error, layer by layer, from the output layer to the hidden\nlayers. A recently discovered method called feedback-alignment shows that the\nweights used for propagating the error backward don't have to be symmetric with\nthe weights used for propagation the activation forward. In fact, random\nfeedback weights work evenly well, because the network learns how to make the\nfeedback useful. In this work, the feedback alignment principle is used for\ntraining hidden layers more independently from the rest of the network, and\nfrom a zero initial condition. The error is propagated through fixed random\nfeedback connections directly from the output layer to each hidden layer. This\nsimple method is able to achieve zero training error even in convolutional\nnetworks and very deep networks, completely without error back-propagation. The\nmethod is a step towards biologically plausible machine learning because the\nerror signal is almost local, and no symmetric or reciprocal weights are\nrequired. Experiments show that the test performance on MNIST and CIFAR is\nalmost as good as those obtained with back-propagation for fully connected\nnetworks. If combined with dropout, the method achieves 1.45% error on the\npermutation invariant MNIST task. \n\n"}
{"id": "1609.02116", "contents": "Title: Ask the GRU: Multi-Task Learning for Deep Text Recommendations Abstract: In a variety of application domains the content to be recommended to users is\nassociated with text. This includes research papers, movies with associated\nplot summaries, news articles, blog posts, etc. Recommendation approaches based\non latent factor models can be extended naturally to leverage text by employing\nan explicit mapping from text to factors. This enables recommendations for new,\nunseen content, and may generalize better, since the factors for all items are\nproduced by a compactly-parametrized model. Previous work has used topic models\nor averages of word embeddings for this mapping. In this paper we present a\nmethod leveraging deep recurrent neural networks to encode the text sequence\ninto a latent vector, specifically gated recurrent units (GRUs) trained\nend-to-end on the collaborative filtering task. For the task of scientific\npaper recommendation, this yields models with significantly higher accuracy. In\ncold-start scenarios, we beat the previous state-of-the-art, all of which\nignore word order. Performance is further improved by multi-task learning,\nwhere the text encoder network is trained for a combination of content\nrecommendation and item metadata prediction. This regularizes the collaborative\nfiltering model, ameliorating the problem of sparsity of the observed rating\nmatrix. \n\n"}
{"id": "1609.03544", "contents": "Title: Online Data Thinning via Multi-Subspace Tracking Abstract: In an era of ubiquitous large-scale streaming data, the availability of data\nfar exceeds the capacity of expert human analysts. In many settings, such data\nis either discarded or stored unprocessed in datacenters. This paper proposes a\nmethod of online data thinning, in which large-scale streaming datasets are\nwinnowed to preserve unique, anomalous, or salient elements for timely expert\nanalysis. At the heart of this proposed approach is an online anomaly detection\nmethod based on dynamic, low-rank Gaussian mixture models. Specifically, the\nhigh-dimensional covariances matrices associated with the Gaussian components\nare associated with low-rank models. According to this model, most observations\nlie near a union of subspaces. The low-rank modeling mitigates the curse of\ndimensionality associated with anomaly detection for high-dimensional data, and\nrecent advances in subspace clustering and subspace tracking allow the proposed\nmethod to adapt to dynamic environments. Furthermore, the proposed method\nallows subsampling, is robust to missing data, and uses a mini-batch online\noptimization approach. The resulting algorithms are scalable, efficient, and\nare capable of operating in real time. Experiments on wide-area motion imagery\nand e-mail databases illustrate the efficacy of the proposed approach. \n\n"}
{"id": "1609.04522", "contents": "Title: Tensor Graphical Model: Non-convex Optimization and Statistical\n  Inference Abstract: We consider the estimation and inference of graphical models that\ncharacterize the dependency structure of high-dimensional tensor-valued data.\nTo facilitate the estimation of the precision matrix corresponding to each way\nof the tensor, we assume the data follow a tensor normal distribution whose\ncovariance has a Kronecker product structure. A critical challenge in the\nestimation and inference of this model is the fact that its penalized maximum\nlikelihood estimation involves minimizing a non-convex objective function. To\naddress it, this paper makes two contributions: (i) In spite of the\nnon-convexity of this estimation problem, we prove that an alternating\nminimization algorithm, which iteratively estimates each sparse precision\nmatrix while fixing the others, attains an estimator with an optimal\nstatistical rate of convergence. (ii) We propose a de-biased statistical\ninference procedure for testing hypotheses on the true support of the sparse\nprecision matrices, and employ it for testing a growing number of hypothesis\nwith false discovery rate (FDR) control. The asymptotic normality of our test\nstatistic and the consistency of FDR control procedure are established. Our\ntheoretical results are backed up by thorough numerical studies and our real\napplications on neuroimaging studies of Autism spectrum disorder and users'\nadvertising click analysis bring new scientific findings and business insights.\nThe proposed methods are encoded into a publicly available R package Tlasso. \n\n"}
{"id": "1609.07236", "contents": "Title: On the (im)possibility of fairness Abstract: What does it mean for an algorithm to be fair? Different papers use different\nnotions of algorithmic fairness, and although these appear internally\nconsistent, they also seem mutually incompatible. We present a mathematical\nsetting in which the distinctions in previous papers can be made formal. In\naddition to characterizing the spaces of inputs (the \"observed\" space) and\noutputs (the \"decision\" space), we introduce the notion of a construct space: a\nspace that captures unobservable, but meaningful variables for the prediction.\n  We show that in order to prove desirable properties of the entire\ndecision-making process, different mechanisms for fairness require different\nassumptions about the nature of the mapping from construct space to decision\nspace. The results in this paper imply that future treatments of algorithmic\nfairness should more explicitly state assumptions about the relationship\nbetween constructs and observations. \n\n"}
{"id": "1610.00217", "contents": "Title: Coherence generating power of quantum unitary maps and beyond Abstract: Given a preferred orthonormal basis $B$ in the Hilbert space of a quantum\nsystem we define a measure of the coherence generating power of a unitary\noperation with respect to $B$. This measure is the average coherence generated\nby the operation acting on a uniform ensemble of incoherent states. We give its\nexplicit analytical form in any dimension and provide an operational protocol\nto directly detect it. We characterize the set of unitaries with maximal\ncoherence generating power and study the properties of our measure when the\nunitary is drawn at random from the Haar distribution. For large state-space\ndimension a random unitary has, with overwhelming probability, nearly maximal\ncoherence generating power with respect to any basis. Finally, extensions to\ngeneral unital quantum operations and the relation to the concept of asymmetry\nare discussed. \n\n"}
{"id": "1610.00425", "contents": "Title: Rational dilation on the symmetrized tridisc: failure, success and\n  unknown Abstract: The closed symmetrized tridisc $\\Gamma_3$ and its distinguished boundary\n$b\\Gamma_3$ are the sets $\\Gamma_3=\\{\n(z_1+z_2+z_3,z_1z_2+z_2z_3+z_3z_1,z_1z_2z_3): \\,|z_i|\\leq 1, i=1,2,3\n\\}\\subseteq \\mathbb C^3$ $b\\Gamma_3=\\{\n(z_1+z_2+z_3,z_1z_2+z_2z_3+z_3z_1,z_1z_2z_3): \\,|z_i|= 1, i=1,2,3 \\}\\subseteq\n\\Gamma_3.$ A triple of commuting operators $(S_1,S_2,P)$ defined on a Hilbert\nspace $\\mathcal H$ for which $\\Gamma_3$ is a spectral set is called a\n$\\Gamma_3$-contraction. In this article we show by a counter example that there\nare $\\Gamma_3$-contractions which do not dilate. It is also shown that under\ncertain conditions a $\\Gamma_3$-contraction can have normal $b\\Gamma_3$\ndilation. We determine several classes of $\\Gamma_3$-contractions which dilate\nand show explicit construction of their dilations. A concrete functional model\nis provided for the $\\Gamma_3$-contractions which dilate. Various\ncharacterizations for $\\Gamma_3$-unitaries and $\\Gamma_3$-isometries are\nobtained; the classes of $\\Gamma_3$-unitaries and $\\Gamma_3$-isometries are\nanalogous to the unitaries and isometries in one variable operator theory. Also\nwe find out a model for the class of pure $\\Gamma_3$-isometries. En route we\nstudy the geometry of the sets $\\Gamma_3$ and $b\\Gamma_3$ and provide variety\nof characterizations for them. \n\n"}
{"id": "1610.00970", "contents": "Title: Stochastic Optimization with Variance Reduction for Infinite Datasets\n  with Finite-Sum Structure Abstract: Stochastic optimization algorithms with variance reduction have proven\nsuccessful for minimizing large finite sums of functions. Unfortunately, these\ntechniques are unable to deal with stochastic perturbations of input data,\ninduced for example by data augmentation. In such cases, the objective is no\nlonger a finite sum, and the main candidate for optimization is the stochastic\ngradient descent method (SGD). In this paper, we introduce a variance reduction\napproach for these settings when the objective is composite and strongly\nconvex. The convergence rate outperforms SGD with a typically much smaller\nconstant factor, which depends on the variance of gradient estimates only due\nto perturbations on a single example. \n\n"}
{"id": "1610.01032", "contents": "Title: On $(H,\\widetilde{H})$-harmonic Maps between pseudo-Hermitian manifolds Abstract: In this paper, we investigate critical maps of the horizontal energy\nfunctional $E_{H,\\widetilde{H}}(f)$ for maps between two pseudo-Hermitian\nmanifolds $(M^{2m+1},H(M),J,\\theta )$ and $(N^{2n+1},\\widetilde{H}(N),\n\\widetilde{J},\\widetilde{\\theta})$. These critical maps are referred to as\n$(H,\\widetilde{H})$-harmonic maps. We derive a CR Bochner formula for the\nhorizontal energy density $|df_{H, \\widetilde{H}}|^{2}$, and introduce a\nPaneitz type operator acting on maps to refine the Bochner formula. As a\nresult, we are able to establish some Bochner type theorems for\n$(H,\\widetilde{H})$-harmonic maps. We also introduce\n$(H,\\widetilde{H})$-pluriharmonic, $(H,\\widetilde{H})$-holomorphic maps between\nthese manifolds, which provide us examples of $(H,\\widetilde{H})$-harmonic\nmaps. Moreover, a Lichnerowicz type result is established to show that foliated\n$(H,\\widetilde{ H})$-holomorphic maps are actually minimizers of\n$E_{H,\\widetilde{H}}(f)$ in their foliated homotopy classes. We also prove some\nunique continuation results for characterizing either horizontally constant\nmaps or foliated $(H,\\widetilde{H})$-holomorphic maps. Furthermore,\nEells-Sampson type existence results for $(H,\\widetilde{H})$-harmonic maps are\nestablished if both manifolds are compact Sasakian and the target is regular\nwith non-positive horizontal sectional curvature. Finally, we give a foliated\nrigidity result for $(H,\\widetilde{H})$-harmonic maps and Siu type strong\nrigidity results for compact regular Sasakian manifolds with either strongly\nnegative horizontal curvature or adequately negative horizontal curvature. \n\n"}
{"id": "1610.02492", "contents": "Title: A Schwarz lemma for the symmetrized tridisc and description of\n  interpolating functions Abstract: We produce a Schwarz lemma for the symmetrized tridisc \\[ \\mathbb G_3 =\\{\n(z_1+z_2+z_3,z_1z_2+z_2z_3+z_3z_1,z_1z_2z_3): \\,|z_i|< 1, i=1,2,3 \\}. \\] We\nshow that an interpolating function related to the Schward lemma for $\\mathbb\nG_3$ is not unique and present an explicit description of all such\ninterpolating functions. We also study the complex geometry of $\\mathbb G_3$\nand present a variety of new characterizations for the open and closed\nsymmetrized tridisc. \n\n"}
{"id": "1610.02776", "contents": "Title: Otto refrigerator based on a superconducting qubit: classical and\n  quantum performance Abstract: We analyse a quantum Otto refrigerator based on a superconducting qubit\ncoupled to two LC-resonators each including a resistor acting as a reservoir.\nWe find various operation regimes: nearly adiabatic (low driving frequency),\nideal Otto cycle (intermediate frequency), and non-adiabatic coherent regime\n(high frequency). In the nearly adiabatic regime, the cooling power is\nquadratic in frequency, and we find substantially enhanced coefficient of\nperformance $\\epsilon$, as compared to that of an ideal Otto cycle. Quantum\ncoherent effects lead invariably to decrease in both cooling power and\n$\\epsilon$ as compared to purely classical dynamics. In the non-adiabatic\nregime we observe strong coherent oscillations of the cooling power as a\nfunction of frequency. We investigate various driving waveforms: compared to\nthe standard sinusoidal drive, truncated trapezoidal drive with optimized rise\nand dwell times yields higher cooling power and efficiency. \n\n"}
{"id": "1610.03725", "contents": "Title: Post Selection Inference with Kernels Abstract: We propose a novel kernel based post selection inference (PSI) algorithm,\nwhich can not only handle non-linearity in data but also structured output such\nas multi-dimensional and multi-label outputs. Specifically, we develop a PSI\nalgorithm for independence measures, and propose the Hilbert-Schmidt\nIndependence Criterion (HSIC) based PSI algorithm (hsicInf). The novelty of the\nproposed algorithm is that it can handle non-linearity and/or structured data\nthrough kernels. Namely, the proposed algorithm can be used for wider range of\napplications including nonlinear multi-class classification and multi-variate\nregressions, while existing PSI algorithms cannot handle them. Through\nsynthetic experiments, we show that the proposed approach can find a set of\nstatistically significant features for both regression and classification\nproblems. Moreover, we apply the hsicInf algorithm to a real-world data, and\nshow that hsicInf can successfully identify important features. \n\n"}
{"id": "1610.04019", "contents": "Title: Voice Conversion from Non-parallel Corpora Using Variational\n  Auto-encoder Abstract: We propose a flexible framework for spectral conversion (SC) that facilitates\ntraining with unaligned corpora. Many SC frameworks require parallel corpora,\nphonetic alignments, or explicit frame-wise correspondence for learning\nconversion functions or for synthesizing a target spectrum with the aid of\nalignments. However, these requirements gravely limit the scope of practical\napplications of SC due to scarcity or even unavailability of parallel corpora.\nWe propose an SC framework based on variational auto-encoder which enables us\nto exploit non-parallel corpora. The framework comprises an encoder that learns\nspeaker-independent phonetic representations and a decoder that learns to\nreconstruct the designated speaker. It removes the requirement of parallel\ncorpora or phonetic alignments to train a spectral conversion system. We report\nobjective and subjective evaluations to validate our proposed method and\ncompare it to SC methods that have access to aligned corpora. \n\n"}
{"id": "1610.04045", "contents": "Title: Rigidity of Oeljeklaus-Toma manifolds Abstract: We prove that Oeljeklaus-Toma manifolds of simple type are rigid, and that\nany line bundle on an Oeljeklaus-Toma manifold is flat. \n\n"}
{"id": "1610.04583", "contents": "Title: Message-passing algorithms for synchronization problems over compact\n  groups Abstract: Various alignment problems arising in cryo-electron microscopy, community\ndetection, time synchronization, computer vision, and other fields fall into a\ncommon framework of synchronization problems over compact groups such as Z/L,\nU(1), or SO(3). The goal of such problems is to estimate an unknown vector of\ngroup elements given noisy relative observations. We present an efficient\niterative algorithm to solve a large class of these problems, allowing for any\ncompact group, with measurements on multiple 'frequency channels' (Fourier\nmodes, or more generally, irreducible representations of the group). Our\nalgorithm is a highly efficient iterative method following the blueprint of\napproximate message passing (AMP), which has recently arisen as a central\ntechnique for inference problems such as structured low-rank estimation and\ncompressed sensing. We augment the standard ideas of AMP with ideas from\nrepresentation theory so that the algorithm can work with distributions over\ncompact groups. Using standard but non-rigorous methods from statistical\nphysics we analyze the behavior of our algorithm on a Gaussian noise model,\nidentifying phases where the problem is easy, (computationally) hard, and\n(statistically) impossible. In particular, such evidence predicts that our\nalgorithm is information-theoretically optimal in many cases, and that the\nremaining cases show evidence of statistical-to-computational gaps. \n\n"}
{"id": "1610.05683", "contents": "Title: Reparameterization Gradients through Acceptance-Rejection Sampling\n  Algorithms Abstract: Variational inference using the reparameterization trick has enabled\nlarge-scale approximate Bayesian inference in complex probabilistic models,\nleveraging stochastic optimization to sidestep intractable expectations. The\nreparameterization trick is applicable when we can simulate a random variable\nby applying a differentiable deterministic function on an auxiliary random\nvariable whose distribution is fixed. For many distributions of interest (such\nas the gamma or Dirichlet), simulation of random variables relies on\nacceptance-rejection sampling. The discontinuity introduced by the\naccept-reject step means that standard reparameterization tricks are not\napplicable. We propose a new method that lets us leverage reparameterization\ngradients even when variables are outputs of a acceptance-rejection sampling\nalgorithm. Our approach enables reparameterization on a larger class of\nvariational distributions. In several studies of real and synthetic data, we\nshow that the variance of the estimator of the gradient is significantly lower\nthan other state-of-the-art methods. This leads to faster convergence of\nstochastic gradient variational inference. \n\n"}
{"id": "1610.05755", "contents": "Title: Semi-supervised Knowledge Transfer for Deep Learning from Private\n  Training Data Abstract: Some machine learning applications involve training data that is sensitive,\nsuch as the medical histories of patients in a clinical trial. A model may\ninadvertently and implicitly store some of its training data; careful analysis\nof the model may therefore reveal sensitive information.\n  To address this problem, we demonstrate a generally applicable approach to\nproviding strong privacy guarantees for training data: Private Aggregation of\nTeacher Ensembles (PATE). The approach combines, in a black-box fashion,\nmultiple models trained with disjoint datasets, such as records from different\nsubsets of users. Because they rely directly on sensitive data, these models\nare not published, but instead used as \"teachers\" for a \"student\" model. The\nstudent learns to predict an output chosen by noisy voting among all of the\nteachers, and cannot directly access an individual teacher or the underlying\ndata or parameters. The student's privacy properties can be understood both\nintuitively (since no single teacher and thus no single dataset dictates the\nstudent's training) and formally, in terms of differential privacy. These\nproperties hold even if an adversary can not only query the student but also\ninspect its internal workings.\n  Compared with previous work, the approach imposes only weak assumptions on\nhow teachers are trained: it applies to any model, including non-convex models\nlike DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and\nSVHN thanks to an improved privacy analysis and semi-supervised learning. \n\n"}
{"id": "1610.05794", "contents": "Title: Surjective holomorphic maps onto Oka manifolds Abstract: Let $X$ be a connected Oka manifold, and let $S$ be a Stein manifold with\n$\\mathrm{dim} S \\geq \\mathrm{dim} X$. We show that every continuous map $S\\to\nX$ is homotopic to a surjective strongly dominating holomorphic map $S\\to X$.\nWe also find strongly dominating algebraic morphisms from the affine $n$-space\nonto any compact $n$-dimensional algebraically subelliptic manifold. Motivated\nby these results, we propose a new holomorphic flexibility property of complex\nmanifolds, the basic Oka property with surjectivity, which could potentially\nprovide another characterization of the class of Oka manifolds. \n\n"}
{"id": "1610.06447", "contents": "Title: Regularized Optimal Transport and the Rot Mover's Distance Abstract: This paper presents a unified framework for smooth convex regularization of\ndiscrete optimal transport problems. In this context, the regularized optimal\ntransport turns out to be equivalent to a matrix nearness problem with respect\nto Bregman divergences. Our framework thus naturally generalizes a previously\nproposed regularization based on the Boltzmann-Shannon entropy related to the\nKullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We\ncall the regularized optimal transport distance the rot mover's distance in\nreference to the classical earth mover's distance. We develop two generic\nschemes that we respectively call the alternate scaling algorithm and the\nnon-negative alternate scaling algorithm, to compute efficiently the\nregularized optimal plans depending on whether the domain of the regularizer\nlies within the non-negative orthant or not. These schemes are based on\nDykstra's algorithm with alternate Bregman projections, and further exploit the\nNewton-Raphson method when applied to separable divergences. We enhance the\nseparable case with a sparse extension to deal with high data dimensions. We\nalso instantiate our proposed framework and discuss the inherent specificities\nfor well-known regularizers and statistical divergences in the machine learning\nand information geometry communities. Finally, we demonstrate the merits of our\nmethods with experiments using synthetic data to illustrate the effect of\ndifferent regularizers and penalties on the solutions, as well as real-world\ndata for a pattern recognition application to audio scene classification. \n\n"}
{"id": "1611.03001", "contents": "Title: On Lang's conjecture for some product-quotient surfaces Abstract: We prove effective versions of algebraic and analytic Lang's conjectures for\nproduct-quotient surfaces of general type with $P_g=0$ and $c_1^2=c_2$. \n\n"}
{"id": "1611.03028", "contents": "Title: Node Embedding via Word Embedding for Network Community Discovery Abstract: Neural node embeddings have recently emerged as a powerful representation for\nsupervised learning tasks involving graph-structured data. We leverage this\nrecent advance to develop a novel algorithm for unsupervised community\ndiscovery in graphs. Through extensive experimental studies on simulated and\nreal-world data, we demonstrate that the proposed approach consistently\nimproves over the current state-of-the-art. Specifically, our approach\nempirically attains the information-theoretic limits for community recovery\nunder the benchmark Stochastic Block Models for graph generation and exhibits\nbetter stability and accuracy over both Spectral Clustering and Acyclic Belief\nPropagation in the community recovery limits. \n\n"}
{"id": "1611.04460", "contents": "Title: Predictive, finite-sample model choice for time series under\n  stationarity and non-stationarity Abstract: In statistical research there usually exists a choice between structurally\nsimpler or more complex models. We argue that, even if a more complex, locally\nstationary time series model were true, then a simple, stationary time series\nmodel may be advantageous to work with under parameter uncertainty. We present\na new model choice methodology, where one of two competing approaches is chosen\nbased on its empirical, finite-sample performance with respect to prediction,\nin a manner that ensures interpretability. A rigorous, theoretical analysis of\nthe procedure is provided. As an important side result we prove, for possibly\ndiverging model order, that the localised Yule-Walker estimator is strongly,\nuniformly consistent under local stationarity. An R package, forecastSNSTS, is\nprovided and used to apply the methodology to financial and meteorological data\nin empirical examples. We further provide an extensive simulation study and\ndiscuss when it is preferable to base forecasts on the more volatile\ntime-varying estimates and when it is advantageous to forecast as if the data\nwere from a stationary process, even though they might not be. \n\n"}
{"id": "1611.05209", "contents": "Title: Deep Variational Inference Without Pixel-Wise Reconstruction Abstract: Variational autoencoders (VAEs), that are built upon deep neural networks\nhave emerged as popular generative models in computer vision. Most of the work\ntowards improving variational autoencoders has focused mainly on making the\napproximations to the posterior flexible and accurate, leading to tremendous\nprogress. However, there have been limited efforts to replace pixel-wise\nreconstruction, which have known shortcomings. In this work, we use real-valued\nnon-volume preserving transformations (real NVP) to exactly compute the\nconditional likelihood of the data given the latent distribution. We show that\na simple VAE with this form of reconstruction is competitive with complicated\nVAE structures, on image modeling tasks. As part of our model, we develop\npowerful conditional coupling layers that enable real NVP to learn with fewer\nintermediate layers. \n\n"}
{"id": "1611.06655", "contents": "Title: Sparse Sliced Inverse Regression Via Lasso Abstract: For multiple index models, it has recently been shown that the sliced inverse\nregression (SIR) is consistent for estimating the sufficient dimension\nreduction (SDR) space if and only if $\\rho=\\lim\\frac{p}{n}=0$, where $p$ is the\ndimension and $n$ is the sample size. Thus, when $p$ is of the same or a higher\norder of $n$, additional assumptions such as sparsity must be imposed in order\nto ensure consistency for SIR. By constructing artificial response variables\nmade up from top eigenvectors of the estimated conditional covariance matrix,\nwe introduce a simple Lasso regression method to obtain an estimate of the SDR\nspace. The resulting algorithm, Lasso-SIR, is shown to be consistent and\nachieve the optimal convergence rate under certain sparsity conditions when $p$\nis of order $o(n^2\\lambda^2)$, where $\\lambda$ is the generalized\nsignal-to-noise ratio. We also demonstrate the superior performance of\nLasso-SIR compared with existing approaches via extensive numerical studies and\nseveral real data examples. \n\n"}
{"id": "1611.07093", "contents": "Title: Using Empirical Covariance Matrix in Enhancing Prediction Accuracy of\n  Linear Models with Missing Information Abstract: Inference and Estimation in Missing Information (MI) scenarios are important\ntopics in Statistical Learning Theory and Machine Learning (ML). In ML\nliterature, attempts have been made to enhance prediction through precise\nfeature selection methods. In sparse linear models, LASSO is well-known in\nextracting the desired support of the signal and resisting against noisy\nsystems. When sparse models are also suffering from MI, the sparse recovery and\ninference of the missing models are taken into account simultaneously. In this\npaper, we will introduce an approach which enjoys sparse regression and\ncovariance matrix estimation to improve matrix completion accuracy, and as a\nresult enhancing feature selection preciseness which leads to reduction in\nprediction Mean Squared Error (MSE). We will compare the effect of employing\ncovariance matrix in enhancing estimation accuracy to the case it is not used\nin feature selection. Simulations show the improvement in the performance as\ncompared to the case where the covariance matrix estimation is not used. \n\n"}
{"id": "1611.07308", "contents": "Title: Variational Graph Auto-Encoders Abstract: We introduce the variational graph auto-encoder (VGAE), a framework for\nunsupervised learning on graph-structured data based on the variational\nauto-encoder (VAE). This model makes use of latent variables and is capable of\nlearning interpretable latent representations for undirected graphs. We\ndemonstrate this model using a graph convolutional network (GCN) encoder and a\nsimple inner product decoder. Our model achieves competitive results on a link\nprediction task in citation networks. In contrast to most existing models for\nunsupervised learning on graph-structured data and link prediction, our model\ncan naturally incorporate node features, which significantly improves\npredictive performance on a number of benchmark datasets. \n\n"}
{"id": "1611.07460", "contents": "Title: Poisson Random Fields for Dynamic Feature Models Abstract: We present the Wright-Fisher Indian buffet process (WF-IBP), a probabilistic\nmodel for time-dependent data assumed to have been generated by an unknown\nnumber of latent features. This model is suitable as a prior in Bayesian\nnonparametric feature allocation models in which the features underlying the\nobserved data exhibit a dependency structure over time. More specifically, we\nestablish a new framework for generating dependent Indian buffet processes,\nwhere the Poisson random field model from population genetics is used as a way\nof constructing dependent beta processes. Inference in the model is complex,\nand we describe a sophisticated Markov Chain Monte Carlo algorithm for exact\nposterior simulation. We apply our construction to develop a nonparametric\nfocused topic model for collections of time-stamped text documents and test it\non the full corpus of NIPS papers published from 1987 to 2015. \n\n"}
{"id": "1611.07675", "contents": "Title: Video Captioning with Transferred Semantic Attributes Abstract: Automatically generating natural language descriptions of videos plays a\nfundamental challenge for computer vision community. Most recent progress in\nthis problem has been achieved through employing 2-D and/or 3-D Convolutional\nNeural Networks (CNN) to encode video content and Recurrent Neural Networks\n(RNN) to decode a sentence. In this paper, we present Long Short-Term Memory\nwith Transferred Semantic Attributes (LSTM-TSA)---a novel deep architecture\nthat incorporates the transferred semantic attributes learnt from images and\nvideos into the CNN plus RNN framework, by training them in an end-to-end\nmanner. The design of LSTM-TSA is highly inspired by the facts that 1) semantic\nattributes play a significant contribution to captioning, and 2) images and\nvideos carry complementary semantics and thus can reinforce each other for\ncaptioning. To boost video captioning, we propose a novel transfer unit to\nmodel the mutually correlated attributes learnt from images and videos.\nExtensive experiments are conducted on three public datasets, i.e., MSVD, M-VAD\nand MPII-MD. Our proposed LSTM-TSA achieves to-date the best published\nperformance in sentence generation on MSVD: 52.8% and 74.0% in terms of BLEU@4\nand CIDEr-D. Superior results when compared to state-of-the-art methods are\nalso reported on M-VAD and MPII-MD. \n\n"}
{"id": "1611.08292", "contents": "Title: Identifying Significant Predictive Bias in Classifiers Abstract: We present a novel subset scan method to detect if a probabilistic binary\nclassifier has statistically significant bias -- over or under predicting the\nrisk -- for some subgroup, and identify the characteristics of this subgroup.\nThis form of model checking and goodness-of-fit test provides a way to\ninterpretably detect the presence of classifier bias or regions of poor\nclassifier fit. This allows consideration of not just subgroups of a priori\ninterest or small dimensions, but the space of all possible subgroups of\nfeatures. To address the difficulty of considering these exponentially many\npossible subgroups, we use subset scan and parametric bootstrap-based methods.\nExtending this method, we can penalize the complexity of the detected subgroup\nand also identify subgroups with high classification errors. We demonstrate\nthese methods and find interesting results on the COMPAS crime recidivism and\ncredit delinquency data. \n\n"}
{"id": "1611.08331", "contents": "Title: An Overview on Data Representation Learning: From Traditional Feature\n  Learning to Recent Deep Learning Abstract: Since about 100 years ago, to learn the intrinsic structure of data, many\nrepresentation learning approaches have been proposed, including both linear\nones and nonlinear ones, supervised ones and unsupervised ones. Particularly,\ndeep architectures are widely applied for representation learning in recent\nyears, and have delivered top results in many tasks, such as image\nclassification, object detection and speech recognition. In this paper, we\nreview the development of data representation learning methods. Specifically,\nwe investigate both traditional feature learning algorithms and\nstate-of-the-art deep learning models. The history of data representation\nlearning is introduced, while available resources (e.g. online course, tutorial\nand book information) and toolboxes are provided. Finally, we conclude this\npaper with remarks and some interesting research directions on data\nrepresentation learning. \n\n"}
{"id": "1611.08402", "contents": "Title: Geometric deep learning on graphs and manifolds using mixture model CNNs Abstract: Deep learning has achieved a remarkable performance breakthrough in several\nfields, most notably in speech recognition, natural language processing, and\ncomputer vision. In particular, convolutional neural network (CNN)\narchitectures currently produce state-of-the-art performance on a variety of\nimage analysis tasks such as object detection and recognition. Most of deep\nlearning research has so far focused on dealing with 1D, 2D, or 3D\nEuclidean-structured data such as acoustic signals, images, or videos.\nRecently, there has been an increasing interest in geometric deep learning,\nattempting to generalize deep learning methods to non-Euclidean structured data\nsuch as graphs and manifolds, with a variety of applications from the domains\nof network analysis, computational social science, or computer graphics. In\nthis paper, we propose a unified framework allowing to generalize CNN\narchitectures to non-Euclidean domains (graphs and manifolds) and learn local,\nstationary, and compositional task-specific features. We show that various\nnon-Euclidean CNN methods previously proposed in the literature can be\nconsidered as particular instances of our framework. We test the proposed\nmethod on standard tasks from the realms of image-, graph- and 3D shape\nanalysis and show that it consistently outperforms previous approaches. \n\n"}
{"id": "1611.08568", "contents": "Title: Bottleneck Conditional Density Estimation Abstract: We introduce a new framework for training deep generative models for\nhigh-dimensional conditional density estimation. The Bottleneck Conditional\nDensity Estimator (BCDE) is a variant of the conditional variational\nautoencoder (CVAE) that employs layer(s) of stochastic variables as the\nbottleneck between the input $x$ and target $y$, where both are\nhigh-dimensional. Crucially, we propose a new hybrid training method that\nblends the conditional generative model with a joint generative model. Hybrid\nblending is the key to effective training of the BCDE, which avoids overfitting\nand provides a novel mechanism for leveraging unlabeled data. We show that our\nhybrid training procedure enables models to achieve competitive results in the\nMNIST quadrant prediction task in the fully-supervised setting, and sets new\nbenchmarks in the semi-supervised regime for MNIST, SVHN, and CelebA. \n\n"}
{"id": "1611.08663", "contents": "Title: Multi-Task Zero-Shot Action Recognition with Prioritised Data\n  Augmentation Abstract: Zero-Shot Learning (ZSL) promises to scale visual recognition by bypassing\nthe conventional model training requirement of annotated examples for every\ncategory. This is achieved by establishing a mapping connecting low-level\nfeatures and a semantic description of the label space, referred as\nvisual-semantic mapping, on auxiliary data. Reusing the learned mapping to\nproject target videos into an embedding space thus allows novel-classes to be\nrecognised by nearest neighbour inference. However, existing ZSL methods suffer\nfrom auxiliary-target domain shift intrinsically induced by assuming the same\nmapping for the disjoint auxiliary and target classes. This compromises the\ngeneralisation accuracy of ZSL recognition on the target data. In this work, we\nimprove the ability of ZSL to generalise across this domain shift in both\nmodel- and data-centric ways by formulating a visual-semantic mapping with\nbetter generalisation properties and a dynamic data re-weighting method to\nprioritise auxiliary data that are relevant to the target classes.\nSpecifically: (1) We introduce a multi-task visual-semantic mapping to improve\ngeneralisation by constraining the semantic mapping parameters to lie on a\nlow-dimensional manifold, (2) We explore prioritised data augmentation by\nexpanding the pool of auxiliary data with additional instances weighted by\nrelevance to the target domain. The proposed new model is applied to the\nchallenging zero-shot action recognition problem to demonstrate its advantages\nover existing ZSL models. \n\n"}
{"id": "1611.09444", "contents": "Title: The empirical size of trained neural networks Abstract: ReLU neural networks define piecewise linear functions of their inputs.\nHowever, initializing and training a neural network is very different from\nfitting a linear spline. In this paper, we expand empirically upon previous\ntheoretical work to demonstrate features of trained neural networks. Standard\nnetwork initialization and training produce networks vastly simpler than a\nnaive parameter count would suggest and can impart odd features to the trained\nnetwork. However, we also show the forced simplicity is beneficial and, indeed,\ncritical for the wide success of these networks. \n\n"}
{"id": "1611.09913", "contents": "Title: Capacity and Trainability in Recurrent Neural Networks Abstract: Two potential bottlenecks on the expressiveness of recurrent neural networks\n(RNNs) are their ability to store information about the task in their\nparameters, and to store information about the input history in their units. We\nshow experimentally that all common RNN architectures achieve nearly the same\nper-task and per-unit capacity bounds with careful training, for a variety of\ntasks and stacking depths. They can store an amount of task information which\nis linear in the number of parameters, and is approximately 5 bits per\nparameter. They can additionally store approximately one real number from their\ninput history per hidden unit. We further find that for several tasks it is the\nper-task parameter capacity bound that determines performance. These results\nsuggest that many previous results comparing RNN architectures are driven\nprimarily by differences in training effectiveness, rather than differences in\ncapacity. Supporting this observation, we compare training difficulty for\nseveral architectures, and show that vanilla RNNs are far more difficult to\ntrain, yet have slightly higher capacity. Finally, we propose two novel RNN\narchitectures, one of which is easier to train than the LSTM or GRU for deeply\nstacked architectures. \n\n"}
{"id": "1611.09960", "contents": "Title: Attend in groups: a weakly-supervised deep learning framework for\n  learning from web data Abstract: Large-scale datasets have driven the rapid development of deep neural\nnetworks for visual recognition. However, annotating a massive dataset is\nexpensive and time-consuming. Web images and their labels are, in comparison,\nmuch easier to obtain, but direct training on such automatically harvested\nimages can lead to unsatisfactory performance, because the noisy labels of Web\nimages adversely affect the learned recognition models. To address this\ndrawback we propose an end-to-end weakly-supervised deep learning framework\nwhich is robust to the label noise in Web images. The proposed framework relies\non two unified strategies -- random grouping and attention -- to effectively\nreduce the negative impact of noisy web image annotations. Specifically, random\ngrouping stacks multiple images into a single training instance and thus\nincreases the labeling accuracy at the instance level. Attention, on the other\nhand, suppresses the noisy signals from both incorrectly labeled images and\nless discriminative image regions. By conducting intensive experiments on two\nchallenging datasets, including a newly collected fine-grained dataset with Web\nimages of different car models, the superior performance of the proposed\nmethods over competitive baselines is clearly demonstrated. \n\n"}
{"id": "1611.10234", "contents": "Title: Slice starlike functions over quaternions Abstract: In this paper, we initiate the study of the geometric function theory for\nslice starlike functions over quaternions and its subclasses. This allows us to\nanswer negatively some questions about the Bieberbach conjecture, the growth,\ndistortion, and covering theorems for slice regular functions. Precisely, we\nfind that the Bieberbach conjecture holds true for slice starlike functions in\ncontrast to the fact that the Bieberbach conjecture fails for biholomorphic\nstarlike mappings in higher dimensions. We also establish some sharp versions\nof the growth, distortion, and covering theorems for quaternions. \n\n"}
{"id": "1612.00212", "contents": "Title: Training Bit Fully Convolutional Network for Fast Semantic Segmentation Abstract: Fully convolutional neural networks give accurate, per-pixel prediction for\ninput images and have applications like semantic segmentation. However, a\ntypical FCN usually requires lots of floating point computation and large\nrun-time memory, which effectively limits its usability. We propose a method to\ntrain Bit Fully Convolution Network (BFCN), a fully convolutional neural\nnetwork that has low bit-width weights and activations. Because most of its\ncomputation-intensive convolutions are accomplished between low bit-width\nnumbers, a BFCN can be accelerated by an efficient bit-convolution\nimplementation. On CPU, the dot product operation between two bit vectors can\nbe reduced to bitwise operations and popcounts, which can offer much higher\nthroughput than 32-bit multiplications and additions.\n  To validate the effectiveness of BFCN, we conduct experiments on the PASCAL\nVOC 2012 semantic segmentation task and Cityscapes. Our BFCN with 1-bit weights\nand 2-bit activations, which runs 7.8x faster on CPU or requires less than 1\\%\nresources on FPGA, can achieve comparable performance as the 32-bit\ncounterpart. \n\n"}
{"id": "1612.00550", "contents": "Title: Haldane phase in the sawtooth lattice: Edge states, entanglement\n  spectrum and the flat band Abstract: Using density matrix renormalization group numerical calculations, we study\nthe phase diagram of the half filled Bose-Hubbard system in the sawtooth\nlattice with strong frustration in the kinetic energy term. We focus in\nparticular on values of the hopping terms which produce a flat band and show\nthat, in the presence of contact and near neighbor repulsion, three phases\nexist: Mott insulator (MI), charge density wave (CDW), and the topological\nHaldane insulating (HI) phase which displays edge states and particle imbalance\nbetween the two ends of the system. We find that, even though the entanglement\nspectrum in the Haldane phase is not doubly degenerate, it is in excellent\nagreement with the entanglement spectrum of the Affleck-Kennedy-Lieb-Tasaki\n(AKLT) state built in the Wannier basis associated with the flat band. This\nemphasizes that the absence of degeneracy in the entanglement spectrum is not\nnecessarily a signature of a non-topological phase, but rather that the\n(hidden) protecting symmetry involves non-local states. Finally, we also show\nthat the HI phase is stable against small departure from flatness of the band\nbut is destroyed for larger ones. \n\n"}
{"id": "1612.00614", "contents": "Title: Out-of-time-ordered density correlators in Luttinger liquids Abstract: Information scrambling and the butterfly effect in chaotic quantum systems\ncan be diagnosed by out-of-time-ordered (OTO) commutators through an\nexponential growth and large late time value. We show that the latter feature\nshows up in a strongly correlated many-body system, a Luttinger liquid, whose\ndensity fluctuations we study at long and short wavelengths, both in\nequilibrium and after a quantum quench. We find rich behaviour combining\nrobustly universal and non-universal features. The OTO commutators display\ntemperature and initial state independent behaviour, and grow as $t^2$ for\nshort times. For the short wavelength density operator, they reach a sizeable\nvalue after the light cone only in an interacting Luttinger liquid, where the\nbare excitations break up into collective modes. We benchmark our findings\nnumerically on an interacting spinless fermion model in 1D, and find\npersistence of central features even in the non-integrable case. As a\nnon-universal feature, the short time growth exhibits a distance dependent\npower. \n\n"}
{"id": "1612.02099", "contents": "Title: Statistical and Computational Guarantees of Lloyd's Algorithm and its\n  Variants Abstract: Clustering is a fundamental problem in statistics and machine learning.\nLloyd's algorithm, proposed in 1957, is still possibly the most widely used\nclustering algorithm in practice due to its simplicity and empirical\nperformance. However, there has been little theoretical investigation on the\nstatistical and computational guarantees of Lloyd's algorithm. This paper is an\nattempt to bridge this gap between practice and theory. We investigate the\nperformance of Lloyd's algorithm on clustering sub-Gaussian mixtures. Under an\nappropriate initialization for labels or centers, we show that Lloyd's\nalgorithm converges to an exponentially small clustering error after an order\nof $\\log n$ iterations, where $n$ is the sample size. The error rate is shown\nto be minimax optimal. For the two-mixture case, we only require the\ninitializer to be slightly better than random guess.\n  In addition, we extend the Lloyd's algorithm and its analysis to community\ndetection and crowdsourcing, two problems that have received a lot of attention\nrecently in statistics and machine learning. Two variants of Lloyd's algorithm\nare proposed respectively for community detection and crowdsourcing. On the\ntheoretical side, we provide statistical and computational guarantees of the\ntwo algorithms, and the results improve upon some previous signal-to-noise\nratio conditions in literature for both problems. Experimental results on\nsimulated and real data sets demonstrate competitive performance of our\nalgorithms to the state-of-the-art methods. \n\n"}
{"id": "1612.02699", "contents": "Title: Deep Supervision with Shape Concepts for Occlusion-Aware 3D Object\n  Parsing Abstract: Monocular 3D object parsing is highly desirable in various scenarios\nincluding occlusion reasoning and holistic scene interpretation. We present a\ndeep convolutional neural network (CNN) architecture to localize semantic parts\nin 2D image and 3D space while inferring their visibility states, given a\nsingle RGB image. Our key insight is to exploit domain knowledge to regularize\nthe network by deeply supervising its hidden layers, in order to sequentially\ninfer intermediate concepts associated with the final task. To acquire training\ndata in desired quantities with ground truth 3D shape and relevant concepts, we\nrender 3D object CAD models to generate large-scale synthetic data and simulate\nchallenging occlusion configurations between objects. We train the network only\non synthetic data and demonstrate state-of-the-art performances on real image\nbenchmarks including an extended version of KITTI, PASCAL VOC, PASCAL3D+ and\nIKEA for 2D and 3D keypoint localization and instance segmentation. The\nempirical results substantiate the utility of our deep supervision scheme by\ndemonstrating effective transfer of knowledge from synthetic data to real\nimages, resulting in less overfitting compared to standard end-to-end training. \n\n"}
{"id": "1612.03425", "contents": "Title: Auxiliary-Field Monte Carlo for lattice bosons: tackling strong\n  interactions and frustration Abstract: We introduce a new numerical technique -- bosonic auxiliary-field Monte Carlo\n(bAFMC) -- which allows to calculate the thermal properties of large\nlattice-boson systems within a systematically improvable semiclassical\napproach, and which is virtually applicable to any bosonic model. Our method\namounts to a decomposition of the lattice into clusters, and to an Ansatz for\nthe density matrix of the system in the form of a cluster-separable state --\nwith non-entangled, yet classically correlated clusters. This approximation\neliminates any sign problem, and can be systematically improved upon by using\nclusters of growing size. Extrapolation in the cluster size allows to reproduce\nnumerically exact results for the superfluid transition of hardcore bosons on\nthe square lattice, and to provide a solid quantitative prediction for the\nsuperfluid and chiral transition of hardcore bosons on the frustrated\ntriangular lattice. \n\n"}
{"id": "1612.03663", "contents": "Title: Analysis and Optimization of Loss Functions for Multiclass, Top-k, and\n  Multilabel Classification Abstract: Top-k error is currently a popular performance measure on large scale image\nclassification benchmarks such as ImageNet and Places. Despite its wide\nacceptance, our understanding of this metric is limited as most of the previous\nresearch is focused on its special case, the top-1 error. In this work, we\nexplore two directions that shed more light on the top-k error. First, we\nprovide an in-depth analysis of established and recently proposed single-label\nmulticlass methods along with a detailed account of efficient optimization\nalgorithms for them. Our results indicate that the softmax loss and the smooth\nmulticlass SVM are surprisingly competitive in top-k error uniformly across all\nk, which can be explained by our analysis of multiclass top-k calibration.\nFurther improvements for a specific k are possible with a number of proposed\ntop-k loss functions. Second, we use the top-k methods to explore the\ntransition from multiclass to multilabel learning. In particular, we find that\nit is possible to obtain effective multilabel classifiers on Pascal VOC using a\nsingle label per image for training, while the gap between multiclass and\nmultilabel methods on MS COCO is more significant. Finally, our contribution of\nefficient algorithms for training with the considered top-k and multilabel loss\nfunctions is of independent interest. \n\n"}
{"id": "1612.03738", "contents": "Title: On the multivariate Fujiwara bound for exponential sums Abstract: We prove the multivariate Fujiwara bound for exponential sums: for a\n$d$-variate exponential sum $f$ with scaling parameter $\\mu$, if $x$ is\ncontained in the amoeba $\\mathscr{A}(f)$, then the distance from $x$ to the\nArchimedean tropical variety associated to $f$ is at most $d \\sqrt{d}\\, 2\\log(2\n+ \\sqrt{3})/ \\mu$. If $f$ is polynomial, then the bound can be improved to $d\n\\log(2 + \\sqrt{3})$. \n\n"}
{"id": "1612.04022", "contents": "Title: Distributed Multi-Task Relationship Learning Abstract: Multi-task learning aims to learn multiple tasks jointly by exploiting their\nrelatedness to improve the generalization performance for each task.\nTraditionally, to perform multi-task learning, one needs to centralize data\nfrom all the tasks to a single machine. However, in many real-world\napplications, data of different tasks may be geo-distributed over different\nlocal machines. Due to heavy communication caused by transmitting the data and\nthe issue of data privacy and security, it is impossible to send data of\ndifferent task to a master machine to perform multi-task learning. Therefore,\nin this paper, we propose a distributed multi-task learning framework that\nsimultaneously learns predictive models for each task as well as task\nrelationships between tasks alternatingly in the parameter server paradigm. In\nour framework, we first offer a general dual form for a family of regularized\nmulti-task relationship learning methods. Subsequently, we propose a\ncommunication-efficient primal-dual distributed optimization algorithm to solve\nthe dual problem by carefully designing local subproblems to make the dual\nproblem decomposable. Moreover, we provide a theoretical convergence analysis\nfor the proposed algorithm, which is specific for distributed multi-task\nrelationship learning. We conduct extensive experiments on both synthetic and\nreal-world datasets to evaluate our proposed framework in terms of\neffectiveness and convergence. \n\n"}
{"id": "1612.06321", "contents": "Title: Large-Scale Image Retrieval with Attentive Deep Local Features Abstract: We propose an attentive local feature descriptor suitable for large-scale\nimage retrieval, referred to as DELF (DEep Local Feature). The new feature is\nbased on convolutional neural networks, which are trained only with image-level\nannotations on a landmark image dataset. To identify semantically useful local\nfeatures for image retrieval, we also propose an attention mechanism for\nkeypoint selection, which shares most network layers with the descriptor. This\nframework can be used for image retrieval as a drop-in replacement for other\nkeypoint detectors and descriptors, enabling more accurate feature matching and\ngeometric verification. Our system produces reliable confidence scores to\nreject false positives---in particular, it is robust against queries that have\nno correct match in the database. To evaluate the proposed descriptor, we\nintroduce a new large-scale dataset, referred to as Google-Landmarks dataset,\nwhich involves challenges in both database and query such as background\nclutter, partial occlusion, multiple landmarks, objects in variable scales,\netc. We show that DELF outperforms the state-of-the-art global and local\ndescriptors in the large-scale setting by significant margins. Code and dataset\ncan be found at the project webpage:\nhttps://github.com/tensorflow/models/tree/master/research/delf . \n\n"}
{"id": "1612.06743", "contents": "Title: Light shifts in atomic Bragg diffraction Abstract: Bragg diffraction of an atomic wave packet in a retroreflective geometry with\ntwo counterpropagating optical lattices exhibits a light shift induced phase.\nWe show that the temporal shape of the light pulse determines the behavior of\nthis phase shift: In contrast to Raman diffraction, Bragg diffraction with\nGaussian pulses leads to a significant suppression of the intrinsic phase shift\ndue to a scaling with the third power of the inverse Doppler frequency.\nHowever, for box-shaped laser pulses, the corresponding shift is twice as large\nas for Raman diffraction. Our results are based on approximate, but analytical\nexpressions as well as a numerical integration of the corresponding\nSchr\\\"odinger equation. \n\n"}
{"id": "1612.08155", "contents": "Title: Long-range topological insulators and weakened bulk-boundary\n  correspondence Abstract: We formalize the appearance of new types of insulators in long-range (LR)\nfermionic systems. These phases are not included in the \"ten-fold way\nclassification\" (TWC) for the short-range (SR) topological insulators. This\nconclusion is obtained studying at first specific one-dimensional LR examples,\nin particular their phase diagrams and contents in symmetries and entanglement.\nThe purely long-range phases (LRP) are signaled by the violation of the\narea-law for the Von Neumann entropy and by corresponding peculiar\ndistributions for the entanglement spectrum (ES). The origin of the deviations\nfrom the TWC is analyzed from a more general point of view and in any\ndimension. In particular, it is found related with a particular type of\ndivergences occurring in the spectrum, due to the LR couplings. A satisfying\ncharacterization for the LRP can be achieved at least for one-dimensional\nsystems, as well as the connected definition of a nontrivial topology, provided\na careful evaluation of the LR contributions. Our results lead to reconsider\nthe definition of correlation length in LR systems. The same analysis also\nallows to infer, at least for one-dimensional models, the weakening of the\nbulk-boundary correspondence, due to the important correlations between bulk\nand edges, and consequently to clarify the nature of the massive edge states\nappearing in the topological LR. The emergence of this peculiar edge structure\nis signaled by the bulk ES. The stability of the LRP against finite-size\neffects, relevant in current experiments, and against local disorder is\ndiscussed, showing that the latter ingredient can even strengthen the effect of\nthe LR couplings. Finally, we analyze the entanglement content of the\nparadigmatic LR Ising spin chain, inferring again important deviations from the\nSR regime, and the limitations of bulk-boundary (tensor-network based)\napproaches to classify LR spin models. \n\n"}
{"id": "1701.00081", "contents": "Title: Dissipative stabilization of quantum-feedback-based multipartite\n  entanglement with Rydberg atoms Abstract: A quantum-feedback-based scheme is proposed for generating multipartite\nentanglements of Rydberg atoms in a dissipative optical cavity. The Rydberg\nblockade mechanism efficiently prevents double excitations of the system, which\nis further exploited to speed up the stabilization of an entangled state with a\nsingle Rydberg state excitation. The corresponding feedback operations are\ngreatly simplified, since only one regular atom needs to be controlled during\nthe whole process, irrespective of the number of particles. The form of\nentangled state is also adjustable via regulating the Rabi frequencies of\ndriving fields. Moreover, a relatively long-life time of the high-lying Rydberg\nlevel guarantees a high fidelity in a realistic situation. \n\n"}
{"id": "1701.01081", "contents": "Title: SalGAN: Visual Saliency Prediction with Generative Adversarial Networks Abstract: We introduce SalGAN, a deep convolutional neural network for visual saliency\nprediction trained with adversarial examples. The first stage of the network\nconsists of a generator model whose weights are learned by back-propagation\ncomputed from a binary cross entropy (BCE) loss over downsampled versions of\nthe saliency maps. The resulting prediction is processed by a discriminator\nnetwork trained to solve a binary classification task between the saliency maps\ngenerated by the generative stage and the ground truth ones. Our experiments\nshow how adversarial training allows reaching state-of-the-art performance\nacross different metrics when combined with a widely-used loss function like\nBCE. Our results can be reproduced with the source code and trained models\navailable at https://imatge-upc.github.io/saliency-salgan-2017/. \n\n"}
{"id": "1701.03068", "contents": "Title: A generalization of the Lomnitz logarithmic creep law via Hadamard\n  fractional calculus Abstract: We present a new approach based on linear integro-differential operators with\nlogarithmic kernel related to the Hadamard fractional calculus in order to\ngeneralize, by a parameter $\\nu \\in (0,1]$, the logarithmic creep law known in\nrheology as Lomnitz law (obtained for $\\nu=1$). We derive the constitutive\nstress-strain relation of this generalized model in a form that couples memory\neffects and time-varying viscosity. Then, based on the hereditary theory of\nlinear viscoelasticity, we also derive the corresponding relaxation function by\nsolving numerically a Volterra integral equation of the second kind. So doing\nwe provide a full characterization of the new model both in creep and in\nrelaxation representation, where the slow varying functions of logarithmic type\nplay a fundamental role as required in processes of ultra slow kinetics. \n\n"}
{"id": "1701.06009", "contents": "Title: On the optimality of sliced inverse regression in high dimensions Abstract: The central subspace of a pair of random variables $(y,x) \\in\n\\mathbb{R}^{p+1}$ is the minimal subspace $\\mathcal{S}$ such that $y \\perp\n\\hspace{-2mm} \\perp x\\mid P_{\\mathcal{S}}x$. In this paper, we consider the\nminimax rate of estimating the central space of the multiple index models\n$y=f(\\beta_{1}^{\\tau}x,\\beta_{2}^{\\tau}x,...,\\beta_{d}^{\\tau}x,\\epsilon)$ with\nat most $s$ active predictors where $x \\sim N(0,I_{p})$. We first introduce a\nlarge class of models depending on the smallest non-zero eigenvalue $\\lambda$\nof $var(\\mathbb{E}[x|y])$, over which we show that an aggregated estimator\nbased on the SIR procedure converges at rate\n$d\\wedge((sd+s\\log(ep/s))/(n\\lambda))$. We then show that this rate is optimal\nin two scenarios: the single index models; and the multiple index models with\nfixed central dimension $d$ and fixed $\\lambda$. By assuming a technical\nconjecture, we can show that this rate is also optimal for multiple index\nmodels with bounded dimension of the central space. We believe that these\n(conditional) optimal rate results bring us meaningful insights of general SDR\nproblems in high dimensions. \n\n"}
{"id": "1701.06511", "contents": "Title: Aggressive Sampling for Multi-class to Binary Reduction with\n  Applications to Text Classification Abstract: We address the problem of multi-class classification in the case where the\nnumber of classes is very large. We propose a double sampling strategy on top\nof a multi-class to binary reduction strategy, which transforms the original\nmulti-class problem into a binary classification problem over pairs of\nexamples. The aim of the sampling strategy is to overcome the curse of\nlong-tailed class distributions exhibited in majority of large-scale\nmulti-class classification problems and to reduce the number of pairs of\nexamples in the expanded data. We show that this strategy does not alter the\nconsistency of the empirical risk minimization principle defined over the\ndouble sample reduction. Experiments are carried out on DMOZ and Wikipedia\ncollections with 10,000 to 100,000 classes where we show the efficiency of the\nproposed approach in terms of training and prediction time, memory consumption,\nand predictive performance with respect to state-of-the-art approaches. \n\n"}
{"id": "1701.06876", "contents": "Title: Fr\\'echet Means and Procrustes Analysis in Wasserstein Space Abstract: We consider two statistical problems at the intersection of functional and\nnon-Euclidean data analysis: the determination of a Fr\\'echet mean in the\nWasserstein space of multivariate distributions; and the optimal registration\nof deformed random measures and point processes. We elucidate how the two\nproblems are linked, each being in a sense dual to the other. We first study\nthe finite sample version of the problem in the continuum. Exploiting the\ntangent bundle structure of Wasserstein space, we deduce the Fr\\'echet mean via\ngradient descent. We show that this is equivalent to a Procrustes analysis for\nthe registration maps, thus only requiring successive solutions to pairwise\noptimal coupling problems. We then study the population version of the problem,\nfocussing on inference and stability: in practice, the data are i.i.d.\nrealisations from a law on Wasserstein space, and indeed their observation is\ndiscrete, where one observes a proxy finite sample or point process. We\nconstruct regularised nonparametric estimators, and prove their consistency for\nthe population mean, and uniform consistency for the population Procrustes\nregistration maps. \n\n"}
{"id": "1701.08304", "contents": "Title: On Quaternionic Tori and their Moduli Spaces Abstract: Quaternionic tori are defined as quotients of the skew field $\\mathbb{H}$ of\nquaternions by rank-4 lattices. Using slice regular functions, these tori are\nendowed with natural structures of quaternionic manifolds (in fact quaternionic\ncurves), and a fundamental region in a $12$-dimensional real subspace is then\nconstructed to classify them up to biregular diffeomorphisms. The points of the\nmoduli space correspond to suitable \\emph{special} bases of rank-4 lattices,\nwhich are studied with respect to the action of the group $GL(4, \\mathbb{Z})$,\nand up to biregular diffeomeorphisms. All tori with a non trivial group of\nbiregular automorphisms - and all possible groups of their biregular\nautomorphisms - are then identified, and recognized to correspond to five\ndifferent subsets of boundary points of the moduli space. \n\n"}
{"id": "1702.00985", "contents": "Title: Embedding of LCK manifolds with potential into Hopf manifolds using\n  Riesz-Schauder theorem Abstract: An locally conformally Kahler (LCK) manifold with potential is a complex\nmanifold with a cover which admits an automorphic Kahler potential. An LCK\nmanifold with potential can be embedded to a Hopf manifold, if its dimension is\nat least 3. We give a functional-analytic proof of this result based on\nRiesz-Schauder theorem and Montel theorem. We give an alternative argument for\ncomplex surfaces, deducing embedding theorem from the Spherical Shell\nConjecture. \n\n"}
{"id": "1702.01992", "contents": "Title: Gated Multimodal Units for Information Fusion Abstract: This paper presents a novel model for multimodal learning based on gated\nneural networks. The Gated Multimodal Unit (GMU) model is intended to be used\nas an internal unit in a neural network architecture whose purpose is to find\nan intermediate representation based on a combination of data from different\nmodalities. The GMU learns to decide how modalities influence the activation of\nthe unit using multiplicative gates. It was evaluated on a multilabel scenario\nfor genre classification of movies using the plot and the poster. The GMU\nimproved the macro f-score performance of single-modality approaches and\noutperformed other fusion strategies, including mixture of experts models.\nAlong with this work, the MM-IMDb dataset is released which, to the best of our\nknowledge, is the largest publicly available multimodal dataset for genre\nprediction on movies. \n\n"}
{"id": "1702.05056", "contents": "Title: An Empirical Bayes Approach for High Dimensional Classification Abstract: We propose an empirical Bayes estimator based on Dirichlet process mixture\nmodel for estimating the sparse normalized mean difference, which could be\ndirectly applied to the high dimensional linear classification. In theory, we\nbuild a bridge to connect the estimation error of the mean difference and the\nmisclassification error, also provide sufficient conditions of sub-optimal\nclassifiers and optimal classifiers. In implementation, a variational Bayes\nalgorithm is developed to compute the posterior efficiently and could be\nparallelized to deal with the ultra-high dimensional case. \n\n"}
{"id": "1702.05960", "contents": "Title: A Statistical Learning Approach to Modal Regression Abstract: This paper studies the nonparametric modal regression problem systematically\nfrom a statistical learning view. Originally motivated by pursuing a\ntheoretical understanding of the maximum correntropy criterion based regression\n(MCCR), our study reveals that MCCR with a tending-to-zero scale parameter is\nessentially modal regression. We show that nonparametric modal regression\nproblem can be approached via the classical empirical risk minimization. Some\nefforts are then made to develop a framework for analyzing and implementing\nmodal regression. For instance, the modal regression function is described, the\nmodal regression risk is defined explicitly and its \\textit{Bayes} rule is\ncharacterized; for the sake of computational tractability, the surrogate modal\nregression risk, which is termed as the generalization risk in our study, is\nintroduced. On the theoretical side, the excess modal regression risk, the\nexcess generalization risk, the function estimation error, and the relations\namong the above three quantities are studied rigorously. It turns out that\nunder mild conditions, function estimation consistency and convergence may be\npursued in modal regression as in vanilla regression protocols, such as mean\nregression, median regression, and quantile regression. However, it outperforms\nthese regression models in terms of robustness as shown in our study from a\nre-descending M-estimation view. This coincides with and in return explains the\nmerits of MCCR on robustness. On the practical side, the implementation issues\nof modal regression including the computational algorithm and the tuning\nparameters selection are discussed. Numerical assessments on modal regression\nare also conducted to verify our findings empirically. \n\n"}
{"id": "1702.06525", "contents": "Title: A Unified Framework for Low-Rank plus Sparse Matrix Recovery Abstract: We propose a unified framework to solve general low-rank plus sparse matrix\nrecovery problems based on matrix factorization, which covers a broad family of\nobjective functions satisfying the restricted strong convexity and smoothness\nconditions. Based on projected gradient descent and the double thresholding\noperator, our proposed generic algorithm is guaranteed to converge to the\nunknown low-rank and sparse matrices at a locally linear rate, while matching\nthe best-known robustness guarantee (i.e., tolerance for sparsity). At the core\nof our theory is a novel structural Lipschitz gradient condition for low-rank\nplus sparse matrices, which is essential for proving the linear convergence\nrate of our algorithm, and we believe is of independent interest to prove fast\nrates for general superposition-structured models. We illustrate the\napplication of our framework through two concrete examples: robust matrix\nsensing and robust PCA. Experiments on both synthetic and real datasets\ncorroborate our theory. \n\n"}
{"id": "1702.07945", "contents": "Title: Global Optimality in Low-rank Matrix Optimization Abstract: This paper considers the minimization of a general objective function $f(X)$\nover the set of rectangular $n\\times m$ matrices that have rank at most $r$. To\nreduce the computational burden, we factorize the variable $X$ into a product\nof two smaller matrices and optimize over these two matrices instead of $X$.\nDespite the resulting nonconvexity, recent studies in matrix completion and\nsensing have shown that the factored problem has no spurious local minima and\nobeys the so-called strict saddle property (the function has a directional\nnegative curvature at all critical points but local minima). We analyze the\nglobal geometry for a general and yet well-conditioned objective function\n$f(X)$ whose restricted strong convexity and restricted strong smoothness\nconstants are comparable. In particular, we show that the reformulated\nobjective function has no spurious local minima and obeys the strict saddle\nproperty. These geometric properties imply that a number of iterative\noptimization algorithms (such as gradient descent) can provably solve the\nfactored problem with global convergence. \n\n"}
{"id": "1702.08651", "contents": "Title: Speeding Up Latent Variable Gaussian Graphical Model Estimation via\n  Nonconvex Optimizations Abstract: We study the estimation of the latent variable Gaussian graphical model\n(LVGGM), where the precision matrix is the superposition of a sparse matrix and\na low-rank matrix. In order to speed up the estimation of the sparse plus\nlow-rank components, we propose a sparsity constrained maximum likelihood\nestimator based on matrix factorization, and an efficient alternating gradient\ndescent algorithm with hard thresholding to solve it. Our algorithm is orders\nof magnitude faster than the convex relaxation based methods for LVGGM. In\naddition, we prove that our algorithm is guaranteed to linearly converge to the\nunknown sparse and low-rank components up to the optimal statistical precision.\nExperiments on both synthetic and genomic data demonstrate the superiority of\nour algorithm over the state-of-the-art algorithms and corroborate our theory. \n\n"}
{"id": "1702.08840", "contents": "Title: Iterative Bayesian Learning for Crowdsourced Regression Abstract: Crowdsourcing platforms emerged as popular venues for purchasing human\nintelligence at low cost for large volume of tasks. As many low-paid workers\nare prone to give noisy answers, a common practice is to add redundancy by\nassigning multiple workers to each task and then simply average out these\nanswers. However, to fully harness the wisdom of the crowd, one needs to learn\nthe heterogeneous quality of each worker. We resolve this fundamental challenge\nin crowdsourced regression tasks, i.e., the answer takes continuous labels,\nwhere identifying good or bad workers becomes much more non-trivial compared to\na classification setting of discrete labels. In particular, we introduce a\nBayesian iterative scheme and show that it provably achieves the optimal mean\nsquared error. Our evaluations on synthetic and real-world datasets support our\ntheoretical results and show the superiority of the proposed scheme. \n\n"}
{"id": "1703.00410", "contents": "Title: Detecting Adversarial Samples from Artifacts Abstract: Deep neural networks (DNNs) are powerful nonlinear architectures that are\nknown to be robust to random perturbations of the input. However, these models\nare vulnerable to adversarial perturbations--small input changes crafted\nexplicitly to fool the model. In this paper, we ask whether a DNN can\ndistinguish adversarial samples from their normal and noisy counterparts. We\ninvestigate model confidence on adversarial samples by looking at Bayesian\nuncertainty estimates, available in dropout neural networks, and by performing\ndensity estimation in the subspace of deep features learned by the model. The\nresult is a method for implicit adversarial detection that is oblivious to the\nattack algorithm. We evaluate this method on a variety of standard datasets\nincluding MNIST and CIFAR-10 and show that it generalizes well across different\narchitectures and attacks. Our findings report that 85-93% ROC-AUC can be\nachieved on a number of standard classification tasks with a negative class\nthat consists of both normal and noisy samples. \n\n"}
{"id": "1703.00579", "contents": "Title: Active Learning for Accurate Estimation of Linear Models Abstract: We explore the sequential decision making problem where the goal is to\nestimate uniformly well a number of linear models, given a shared budget of\nrandom contexts independently sampled from a known distribution. The decision\nmaker must query one of the linear models for each incoming context, and\nreceives an observation corrupted by noise levels that are unknown, and depend\non the model instance. We present Trace-UCB, an adaptive allocation algorithm\nthat learns the noise levels while balancing contexts accordingly across the\ndifferent linear functions, and derive guarantees for simple regret in both\nexpectation and high-probability. Finally, we extend the algorithm and its\nguarantees to high dimensional settings, where the number of linear models\ntimes the dimension of the contextual space is higher than the total budget of\nsamples. Simulations with real data suggest that Trace-UCB is remarkably\nrobust, outperforming a number of baselines even when its assumptions are\nviolated. \n\n"}
{"id": "1703.00593", "contents": "Title: Positive-Unlabeled Learning with Non-Negative Risk Estimator Abstract: From only positive (P) and unlabeled (U) data, a binary classifier could be\ntrained with PU learning, in which the state of the art is unbiased PU\nlearning. However, if its model is very flexible, empirical risks on training\ndata will go negative, and we will suffer from serious overfitting. In this\npaper, we propose a non-negative risk estimator for PU learning: when getting\nminimized, it is more robust against overfitting, and thus we are able to use\nvery flexible models (such as deep neural networks) given limited P data.\nMoreover, we analyze the bias, consistency, and mean-squared-error reduction of\nthe proposed risk estimator, and bound the estimation error of the resulting\nempirical risk minimizer. Experiments demonstrate that our risk estimator fixes\nthe overfitting problem of its unbiased counterparts. \n\n"}
{"id": "1703.00837", "contents": "Title: Meta Networks Abstract: Neural networks have been successfully applied in applications with a large\namount of labeled data. However, the task of rapid generalization on new\nconcepts with small training data while preserving performances on previously\nlearned ones still presents a significant challenge to neural network models.\nIn this work, we introduce a novel meta learning method, Meta Networks\n(MetaNet), that learns a meta-level knowledge across tasks and shifts its\ninductive biases via fast parameterization for rapid generalization. When\nevaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve\na near human-level performance and outperform the baseline approaches by up to\n6% accuracy. We demonstrate several appealing properties of MetaNet relating to\ngeneralization and continual learning. \n\n"}
{"id": "1703.01256", "contents": "Title: The Global Optimization Geometry of Low-Rank Matrix Optimization Abstract: This paper considers general rank-constrained optimization problems that\nminimize a general objective function $f(X)$ over the set of rectangular\n$n\\times m$ matrices that have rank at most $r$. To tackle the rank constraint\nand also to reduce the computational burden, we factorize $X$ into $UV^T$ where\n$U$ and $V$ are $n\\times r$ and $m\\times r$ matrices, respectively, and then\noptimize over the small matrices $U$ and $V$. We characterize the global\noptimization geometry of the nonconvex factored problem and show that the\ncorresponding objective function satisfies the robust strict saddle property as\nlong as the original objective function $f$ satisfies restricted strong\nconvexity and smoothness properties, ensuring global convergence of many local\nsearch algorithms (such as noisy gradient descent) in polynomial time for\nsolving the factored problem. We also provide a comprehensive analysis for the\noptimization geometry of a matrix factorization problem where we aim to find\n$n\\times r$ and $m\\times r$ matrices $U$ and $V$ such that $UV^T$ approximates\na given matrix $X^\\star$. Aside from the robust strict saddle property, we show\nthat the objective function of the matrix factorization problem has no spurious\nlocal minima and obeys the strict saddle property not only for the\nexact-parameterization case where $rank(X^\\star) = r$, but also for the\nover-parameterization case where $rank(X^\\star) < r$ and the\nunder-parameterization case where $rank(X^\\star) > r$. These geometric\nproperties imply that a number of iterative optimization algorithms (such as\ngradient descent) converge to a global solution with random initialization. \n\n"}
{"id": "1703.02317", "contents": "Title: Convolutional Recurrent Neural Networks for Bird Audio Detection Abstract: Bird sounds possess distinctive spectral structure which may exhibit small\nshifts in spectrum depending on the bird species and environmental conditions.\nIn this paper, we propose using convolutional recurrent neural networks on the\ntask of automated bird audio detection in real-life environments. In the\nproposed method, convolutional layers extract high dimensional, local frequency\nshift invariant features, while recurrent layers capture longer term\ndependencies between the features extracted from short time frames. This method\nachieves 88.5% Area Under ROC Curve (AUC) score on the unseen evaluation data\nand obtains the second place in the Bird Audio Detection challenge. \n\n"}
{"id": "1703.04697", "contents": "Title: On the benefits of output sparsity for multi-label classification Abstract: The multi-label classification framework, where each observation can be\nassociated with a set of labels, has generated a tremendous amount of attention\nover recent years. The modern multi-label problems are typically large-scale in\nterms of number of observations, features and labels, and the amount of labels\ncan even be comparable with the amount of observations. In this context,\ndifferent remedies have been proposed to overcome the curse of dimensionality.\nIn this work, we aim at exploiting the output sparsity by introducing a new\nloss, called the sparse weighted Hamming loss. This proposed loss can be seen\nas a weighted version of classical ones, where active and inactive labels are\nweighted separately. Leveraging the influence of sparsity in the loss function,\nwe provide improved generalization bounds for the empirical risk minimizer, a\nsuitable property for large-scale problems. For this new loss, we derive rates\nof convergence linear in the underlying output-sparsity rather than linear in\nthe number of labels. In practice, minimizing the associated risk can be\nperformed efficiently by using convex surrogates and modern convex optimization\nalgorithms. We provide experiments on various real-world datasets demonstrating\nthe pertinence of our approach when compared to non-weighted techniques. \n\n"}
{"id": "1703.04757", "contents": "Title: Separation of time scales and direct computation of weights in deep\n  neural networks Abstract: Artificial intelligence is revolutionizing our lives at an ever increasing\npace. At the heart of this revolution is the recent advancements in deep neural\nnetworks (DNN), learning to perform sophisticated, high-level tasks. However,\ntraining DNNs requires massive amounts of data and is very computationally\nintensive. Gaining analytical understanding of the solutions found by DNNs can\nhelp us devise more efficient training algorithms, replacing the commonly used\nmthod of stochastic gradient descent (SGD). We analyze the dynamics of SGD and\nshow that, indeed, direct computation of the solutions is possible in many\ncases. We show that a high performing setup used in DNNs introduces a\nseparation of time-scales in the training dynamics, allowing SGD to train\nlayers from the lowest (closest to input) to the highest. We then show that for\neach layer, the distribution of solutions found by SGD can be estimated using a\nclass-based principal component analysis (PCA) of the layer's input. This\nfinding allows us to forgo SGD entirely and directly derive the DNN parameters\nusing this class-based PCA, which can be well estimated using significantly\nless data than SGD. We implement these results on image datasets MNIST, CIFAR10\nand CIFAR100 and find that, in fact, layers derived using our class-based PCA\nperform comparable or superior to neural networks of the same size and\narchitecture trained using SGD. We also confirm that the class-based PCA often\nconverges using a fraction of the data required for SGD. Thus, using our method\ntraining time can be reduced both by requiring less training data than SGD, and\nby eliminating layers in the costly backpropagation step of the training. \n\n"}
{"id": "1703.05849", "contents": "Title: Estimation and Inference on Nonlinear and Heterogeneous Effects Abstract: Multiple regression has been the go-to method for data analysis for\ngenerations of scholars due to its transparency, interpretability, and\ndesirable theoretical properties. However, the method's simplicity precludes\nthe discovery of complex heterogeneities in the data. We introduce the Method\nof Direct Estimation and Inference (MDEI) that embraces these potential\ncomplexities, is interpretable, has desirable theoretical guarantees, and,\nunlike some existing methods, returns appropriate uncertainty estimates. The\nproposed method uses a machine learning regression methodology to estimate the\nobservation-level effect of a treatment variable. Importantly, we introduce a\nrobust approach to uncertainty estimates. We provide simulation evidence and an\napplication illustrating the performance of the method. \n\n"}
{"id": "1703.07755", "contents": "Title: Gradient descent with nonconvex constraints: local concavity determines\n  convergence Abstract: Many problems in high-dimensional statistics and optimization involve\nminimization over nonconvex constraints-for instance, a rank constraint for a\nmatrix estimation problem-but little is known about the theoretical properties\nof such optimization problems for a general nonconvex constraint set. In this\npaper we study the interplay between the geometric properties of the constraint\nset and the convergence behavior of gradient descent for minimization over this\nset. We develop the notion of local concavity coefficients of the constraint\nset, measuring the extent to which convexity is violated, which govern the\nbehavior of projected gradient descent over this set. We demonstrate the\nversatility of these concavity coefficients by computing them for a range of\nproblems in low-rank estimation, sparse estimation, and other examples. Through\nour understanding of the role of these geometric properties in optimization, we\nthen provide a convergence analysis when projections are calculated only\napproximately, leading to a more efficient method for projected gradient\ndescent in low-rank estimation problems. \n\n"}
{"id": "1703.08052", "contents": "Title: Dynamic Bernoulli Embeddings for Language Evolution Abstract: Word embeddings are a powerful approach for unsupervised analysis of\nlanguage. Recently, Rudolph et al. (2016) developed exponential family\nembeddings, which cast word embeddings in a probabilistic framework. Here, we\ndevelop dynamic embeddings, building on exponential family embeddings to\ncapture how the meanings of words change over time. We use dynamic embeddings\nto analyze three large collections of historical texts: the U.S. Senate\nspeeches from 1858 to 2009, the history of computer science ACM abstracts from\n1951 to 2014, and machine learning papers on the Arxiv from 2007 to 2015. We\nfind dynamic embeddings provide better fits than classical embeddings and\ncapture interesting patterns about how language changes. \n\n"}
{"id": "1703.08667", "contents": "Title: Exploration--Exploitation in MDPs with Options Abstract: While a large body of empirical results show that temporally-extended actions\nand options may significantly affect the learning performance of an agent, the\ntheoretical understanding of how and when options can be beneficial in online\nreinforcement learning is relatively limited. In this paper, we derive an upper\nand lower bound on the regret of a variant of UCRL using options. While we\nfirst analyze the algorithm in the general case of semi-Markov decision\nprocesses (SMDPs), we show how these results can be translated to the specific\ncase of MDPs with options and we illustrate simple scenarios in which the\nregret of learning with options can be \\textit{provably} much smaller than the\nregret suffered when learning with primitive actions. \n\n"}
{"id": "1703.09477", "contents": "Title: Convergence of the Forward-Backward Algorithm: Beyond the Worst Case\n  with the Help of Geometry Abstract: We provide a comprehensive study of the convergence of the forward-backward\nalgorithm under suitable geometric conditions, such as conditioning or\n{\\L}ojasiewicz properties. These geometrical notions are usually local by\nnature, and may fail to describe the fine geometry of objective functions\nrelevant in inverse problems and signal processing, that have a nice behaviour\non manifolds, or sets open with respect to a weak topology. Motivated by this\nobservation, we revisit those geometric notions over arbitrary sets. In turn,\nthis allows us to present several new results as well as collect in a unified\nview a variety of results scattered in the literature. Our contributions\ninclude the analysis of infinite dimensional convex minimization problems,\nshowing the first {\\L}ojasiewicz inequality for a quadratic function associated\nto a compact operator, and the derivation of new linear rates for problems\narising from inverse problems with low-complexity priors. Our approach allows\nto establish unexpected connections between geometry and a priori conditions in\ninverse problems, such as source conditions, or restricted isometry properties. \n\n"}
{"id": "1703.09956", "contents": "Title: Marginal likelihood based model comparison in Fuzzy Bayesian Learning Abstract: In a recent paper [1] we introduced the Fuzzy Bayesian Learning (FBL)\nparadigm where expert opinions can be encoded in the form of fuzzy rule bases\nand the hyper-parameters of the fuzzy sets can be learned from data using a\nBayesian approach. The present paper extends this work for selecting the most\nappropriate rule base among a set of competing alternatives, which best\nexplains the data, by calculating the model evidence or marginal likelihood. We\nexplain why this is an attractive alternative over simply minimizing a mean\nsquared error metric of prediction and show the validity of the proposition\nusing synthetic examples and a real world case study in the financial services\nsector. \n\n"}
{"id": "1703.10553", "contents": "Title: Learning Convolutional Networks for Content-weighted Image Compression Abstract: Lossy image compression is generally formulated as a joint rate-distortion\noptimization to learn encoder, quantizer, and decoder. However, the quantizer\nis non-differentiable, and discrete entropy estimation usually is required for\nrate control. These make it very challenging to develop a convolutional network\n(CNN)-based image compression system. In this paper, motivated by that the\nlocal information content is spatially variant in an image, we suggest that the\nbit rate of the different parts of the image should be adapted to local\ncontent. And the content aware bit rate is allocated under the guidance of a\ncontent-weighted importance map. Thus, the sum of the importance map can serve\nas a continuous alternative of discrete entropy estimation to control\ncompression rate. And binarizer is adopted to quantize the output of encoder\ndue to the binarization scheme is also directly defined by the importance map.\nFurthermore, a proxy function is introduced for binary operation in backward\npropagation to make it differentiable. Therefore, the encoder, decoder,\nbinarizer and importance map can be jointly optimized in an end-to-end manner\nby using a subset of the ImageNet database. In low bit rate image compression,\nexperiments show that our system significantly outperforms JPEG and JPEG 2000\nby structural similarity (SSIM) index, and can produce the much better visual\nresult with sharp edges, rich textures, and fewer artifacts. \n\n"}
{"id": "1704.00028", "contents": "Title: Improved Training of Wasserstein GANs Abstract: Generative Adversarial Networks (GANs) are powerful generative models, but\nsuffer from training instability. The recently proposed Wasserstein GAN (WGAN)\nmakes progress toward stable training of GANs, but sometimes can still generate\nonly low-quality samples or fail to converge. We find that these problems are\noften due to the use of weight clipping in WGAN to enforce a Lipschitz\nconstraint on the critic, which can lead to undesired behavior. We propose an\nalternative to clipping weights: penalize the norm of gradient of the critic\nwith respect to its input. Our proposed method performs better than standard\nWGAN and enables stable training of a wide variety of GAN architectures with\nalmost no hyperparameter tuning, including 101-layer ResNets and language\nmodels over discrete data. We also achieve high quality generations on CIFAR-10\nand LSUN bedrooms. \n\n"}
{"id": "1704.00738", "contents": "Title: Many-body Localization Transition: Schmidt Gap, Entanglement Length &\n  Scaling Abstract: Many-body localization has become an important phenomenon for illuminating a\npotential rift between non-equilibrium quantum systems and statistical\nmechanics. However, the nature of the transition between ergodic and localized\nphases in models displaying many-body localization is not yet well understood.\nAssuming that this is a continuous transition, analytic results show that the\nlength scale should diverge with a critical exponent $\\nu \\ge 2$ in one\ndimensional systems. Interestingly, this is in stark contrast with all exact\nnumerical studies which find $\\nu \\sim 1$. We introduce the Schmidt gap, new in\nthis context, which scales near the transition with a exponent $\\nu > 2$\ncompatible with the analytical bound. We attribute this to an insensitivity to\ncertain finite size fluctuations, which remain significant in other quantities\nat the sizes accessible to exact numerical methods. Additionally, we find that\na physical manifestation of the diverging length scale is apparent in the\nentanglement length computed using the logarithmic negativity between disjoint\nblocks. \n\n"}
{"id": "1704.01265", "contents": "Title: Geometry of Factored Nuclear Norm Regularization Abstract: This work investigates the geometry of a nonconvex reformulation of\nminimizing a general convex loss function $f(X)$ regularized by the matrix\nnuclear norm $\\|X\\|_*$. Nuclear-norm regularized matrix inverse problems are at\nthe heart of many applications in machine learning, signal processing, and\ncontrol. The statistical performance of nuclear norm regularization has been\nstudied extensively in literature using convex analysis techniques. Despite its\noptimal performance, the resulting optimization has high computational\ncomplexity when solved using standard or even tailored fast convex solvers. To\ndevelop faster and more scalable algorithms, we follow the proposal of\nBurer-Monteiro to factor the matrix variable $X$ into the product of two\nsmaller rectangular matrices $X=UV^T$ and also replace the nuclear norm\n$\\|X\\|_*$ with $(\\|U\\|_F^2+\\|V\\|_F^2)/2$. In spite of the nonconvexity of the\nfactored formulation, we prove that when the convex loss function $f(X)$ is\n$(2r,4r)$-restricted well-conditioned, each critical point of the factored\nproblem either corresponds to the optimal solution $X^\\star$ of the original\nconvex optimization or is a strict saddle point where the Hessian matrix has a\nstrictly negative eigenvalue. Such a geometric structure of the factored\nformulation allows many local search algorithms to converge to the global\noptimum with random initializations. \n\n"}
{"id": "1704.02492", "contents": "Title: Metric Learning in Codebook Generation of Bag-of-Words for Person\n  Re-identification Abstract: Person re-identification is generally divided into two part: first how to\nrepresent a pedestrian by discriminative visual descriptors and second how to\ncompare them by suitable distance metrics. Conventional methods isolate these\ntwo parts, the first part usually unsupervised and the second part supervised.\nThe Bag-of-Words (BoW) model is a widely used image representing descriptor in\npart one. Its codebook is simply generated by clustering visual features in\nEuclidian space. In this paper, we propose to use part two metric learning\ntechniques in the codebook generation phase of BoW. In particular, the proposed\ncodebook is clustered under Mahalanobis distance which is learned supervised.\nExtensive experiments prove that our proposed method is effective. With several\nlow level features extracted on superpixel and fused together, our method\noutperforms state-of-the-art on person re-identification benchmarks including\nVIPeR, PRID450S, and Market1501. \n\n"}
{"id": "1704.03995", "contents": "Title: Optimal experimental design that minimizes the width of simultaneous\n  confidence bands Abstract: We propose an optimal experimental design for a curvilinear regression model\nthat minimizes the band-width of simultaneous confidence bands. Simultaneous\nconfidence bands for curvilinear regression are constructed by evaluating the\nvolume of a tube about a curve that is defined as a trajectory of a regression\nbasis vector (Naiman, 1986). The proposed criterion is constructed based on the\nvolume of a tube, and the corresponding optimal design that minimizes the\nvolume of tube is referred to as the tube-volume optimal (TV-optimal) design.\nFor Fourier and weighted polynomial regressions, the problem is formalized as\none of minimization over the cone of Hankel positive definite matrices, and the\ncriterion to minimize is expressed as an elliptic integral. We show that the\nM\\\"obius group keeps our problem invariant, and hence, minimization can be\nconducted over cross-sections of orbits. We demonstrate that for the weighted\npolynomial regression and the Fourier regression with three bases, the\ntube-volume optimal design forms an orbit of the M\\\"obius group containing\nD-optimal designs as representative elements. \n\n"}
{"id": "1704.04134", "contents": "Title: Anisotropic optical trapping as a manifestation of the complex\n  electronic structure of ultracold lanthanide atoms: the example of holmium Abstract: The efficiency of optical trapping is determined by the atomic dynamic dipole\npolarizability, whose real and imaginary parts are associated with the\npotential energy and photon-scattering rate respectively. In this article we\ndevelop a formalism to calculate analytically the real and imaginary parts of\nthe scalar, vector and tensor polarizabilities of lanthanide atoms. We assume\nthat the sum-over-state formula only comprises transitions involving electrons\nin the valence orbitals like $6s$, $5d$, $6p$ or $7s$, while transitions\ninvolving $4f$ core electrons are neglected. Applying this formalism to the\nground level of configuration $4f^q6s^2$, we restrict the sum to transitions\nimplying the $4f^q6s6p$ configuration, which yields polarizabilities depending\non two parameters: an effective transition energy and an effective transition\ndipole moment. Then, by introducing configuration-interaction mixing between\n$4f^q6s6p$ and other configurations, we demonstrate that the imaginary part of\nthe scalar, vector and tensor polarizabilities is very sensitive to\nconfiguration-interaction coefficients, whereas the real part is not. The\nmagnitude and anisotropy of the photon-scattering rate is thus strongly related\nto the details of the atomic electronic structure. Those analytical results\nagree with our detailed electronic-structure calculations of energy levels,\nLand\\'e $g$-factors, transition probabilities, polarizabilities and van der\nWaals $C_6$ coefficients, previously performed on erbium and dysprosium, and\npresently performed on holmium. Our results show that, although the density of\nstates decreases with increasing $q$, the configuration interaction between\n$4f^q6s6p$, $4f^{q-1}5d6s^2$ and $4f^{q-1}5d^26s$ is surprisingly stronger in\nerbium ($q=12$), than in holmium ($q=11$), itself stronger than in dysprosium\n($q=10$). \n\n"}
{"id": "1704.05147", "contents": "Title: O$^2$TD: (Near)-Optimal Off-Policy TD Learning Abstract: Temporal difference learning and Residual Gradient methods are the most\nwidely used temporal difference based learning algorithms; however, it has been\nshown that none of their objective functions is optimal w.r.t approximating the\ntrue value function $V$. Two novel algorithms are proposed to approximate the\ntrue value function $V$. This paper makes the following contributions: (1) A\nbatch algorithm that can help find the approximate optimal off-policy\nprediction of the true value function $V$. (2) A linear computational cost (per\nstep) near-optimal algorithm that can learn from a collection of off-policy\nsamples. (3) A new perspective of the emphatic temporal difference learning\nwhich bridges the gap between off-policy optimality and off-policy stability. \n\n"}
{"id": "1704.05982", "contents": "Title: Retrospective Higher-Order Markov Processes for User Trails Abstract: Users form information trails as they browse the web, checkin with a\ngeolocation, rate items, or consume media. A common problem is to predict what\na user might do next for the purposes of guidance, recommendation, or\nprefetching. First-order and higher-order Markov chains have been widely used\nmethods to study such sequences of data. First-order Markov chains are easy to\nestimate, but lack accuracy when history matters. Higher-order Markov chains,\nin contrast, have too many parameters and suffer from overfitting the training\ndata. Fitting these parameters with regularization and smoothing only offers\nmild improvements. In this paper we propose the retrospective higher-order\nMarkov process (RHOMP) as a low-parameter model for such sequences. This model\nis a special case of a higher-order Markov chain where the transitions depend\nretrospectively on a single history state instead of an arbitrary combination\nof history states. There are two immediate computational advantages: the number\nof parameters is linear in the order of the Markov chain and the model can be\nfit to large state spaces. Furthermore, by providing a specific structure to\nthe higher-order chain, RHOMPs improve the model accuracy by efficiently\nutilizing history states without risks of overfitting the data. We demonstrate\nhow to estimate a RHOMP from data and we demonstrate the effectiveness of our\nmethod on various real application datasets spanning geolocation data, review\nsequences, and business locations. The RHOMP model uniformly outperforms\nhigher-order Markov chains, Kneser-Ney regularization, and tensor\nfactorizations in terms of prediction accuracy. \n\n"}
{"id": "1704.06587", "contents": "Title: Transmission time and resonant tunneling through barriers using\n  localized quantum density soliton waves Abstract: In this paper, the interaction and transmission time of quantum density\nsolitons waves representing particles passing through finite barrier potentials\nis investigated. Using the conservation of energy and of quantum density, it is\nfirst demonstrated that these waves have finite de Broglie wavelength and\nrepresent particles in quantum theory. The passage of the quantum density\nsolitons (particles) through barriers of finite energies is then shown to lead\nto the phenomena of resonant tunneling and, in Josephson-like configurations,\nto the quantization of magnetic flux. A precise general measure for barrier\ntunneling time is derived which is found to give a new interpretation of the\nquantum indeterminacy principles. \n\n"}
{"id": "1704.07228", "contents": "Title: Learning from Comparisons and Choices Abstract: When tracking user-specific online activities, each user's preference is\nrevealed in the form of choices and comparisons. For example, a user's purchase\nhistory is a record of her choices, i.e. which item was chosen among a subset\nof offerings. A user's preferences can be observed either explicitly as in\nmovie ratings or implicitly as in viewing times of news articles. Given such\nindividualized ordinal data in the form of comparisons and choices, we address\nthe problem of collaboratively learning representations of the users and the\nitems. The learned features can be used to predict a user's preference of an\nunseen item to be used in recommendation systems. This also allows one to\ncompute similarities among users and items to be used for categorization and\nsearch. Motivated by the empirical successes of the MultiNomial Logit (MNL)\nmodel in marketing and transportation, and also more recent successes in word\nembedding and crowdsourced image embedding, we pose this problem as learning\nthe MNL model parameters that best explain the data. We propose a convex\nrelaxation for learning the MNL model, and show that it is minimax optimal up\nto a logarithmic factor by comparing its performance to a fundamental lower\nbound. This characterizes the minimax sample complexity of the problem, and\nproves that the proposed estimator cannot be improved upon other than by a\nlogarithmic factor. Further, the analysis identifies how the accuracy depends\non the topology of sampling via the spectrum of the sampling graph. This\nprovides a guideline for designing surveys when one can choose which items are\nto be compared. This is accompanied by numerical simulations on synthetic and\nreal data sets, confirming our theoretical predictions. \n\n"}
{"id": "1704.07535", "contents": "Title: Abstract Syntax Networks for Code Generation and Semantic Parsing Abstract: Tasks like code generation and semantic parsing require mapping unstructured\n(or partially structured) inputs to well-formed, executable outputs. We\nintroduce abstract syntax networks, a modeling framework for these problems.\nThe outputs are represented as abstract syntax trees (ASTs) and constructed by\na decoder with a dynamically-determined modular structure paralleling the\nstructure of the output tree. On the benchmark Hearthstone dataset for code\ngeneration, our model obtains 79.2 BLEU and 22.7% exact match accuracy,\ncompared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we\nperform competitively on the Atis, Jobs, and Geo semantic parsing datasets with\nno task-specific engineering. \n\n"}
{"id": "1704.07564", "contents": "Title: State protection by quantum control before and after noise Abstract: We discuss the possibility of protecting the state of a quantum system that\ngoes through noise by measurements and operations before and after the noise\nprocess. We extend our previous result on nonexistence of \"truly quantum\"\nprotocols that protect an unknown qubit state against the depolarizing noise\nbetter than \"classical\" ones [Phys. Rev. A, 95, 022321 (2017)] in two\ndirections. First, we show that the statement is also true in any\nfinite-dimensional Hilbert spaces, which was previously conjectured, the\noptimal protocol is either the do nothing protocol or the discriminate and\nreprepare protocol, depending on the strength of the noise. Second, in the case\nof a qubit, we show that essentially the same conclusion holds for any unital\nnoise. These results describe the fundamental limitations in quantum mechanics\nfrom the viewpoint of control theory. \n\n"}
{"id": "1704.08349", "contents": "Title: SOFAR: large-scale association network learning Abstract: Many modern big data applications feature large scale in both numbers of\nresponses and predictors. Better statistical efficiency and scientific insights\ncan be enabled by understanding the large-scale response-predictor association\nnetwork structures via layers of sparse latent factors ranked by importance.\nYet sparsity and orthogonality have been two largely incompatible goals. To\naccommodate both features, in this paper we suggest the method of sparse\northogonal factor regression (SOFAR) via the sparse singular value\ndecomposition with orthogonality constrained optimization to learn the\nunderlying association networks, with broad applications to both unsupervised\nand supervised learning tasks such as biclustering with sparse singular value\ndecomposition, sparse principal component analysis, sparse factor analysis, and\nspare vector autoregression analysis. Exploiting the framework of\nconvexity-assisted nonconvex optimization, we derive nonasymptotic error bounds\nfor the suggested procedure characterizing the theoretical advantages. The\nstatistical guarantees are powered by an efficient SOFAR algorithm with\nconvergence property. Both computational and theoretical advantages of our\nprocedure are demonstrated with several simulation and real data examples. \n\n"}
{"id": "1704.09011", "contents": "Title: Mostly Exploration-Free Algorithms for Contextual Bandits Abstract: The contextual bandit literature has traditionally focused on algorithms that\naddress the exploration-exploitation tradeoff. In particular, greedy algorithms\nthat exploit current estimates without any exploration may be sub-optimal in\ngeneral. However, exploration-free greedy algorithms are desirable in practical\nsettings where exploration may be costly or unethical (e.g., clinical trials).\nSurprisingly, we find that a simple greedy algorithm can be rate optimal\n(achieves asymptotically optimal regret) if there is sufficient randomness in\nthe observed contexts (covariates). We prove that this is always the case for a\ntwo-armed bandit under a general class of context distributions that satisfy a\ncondition we term covariate diversity. Furthermore, even absent this condition,\nwe show that a greedy algorithm can be rate optimal with positive probability.\nThus, standard bandit algorithms may unnecessarily explore. Motivated by these\nresults, we introduce Greedy-First, a new algorithm that uses only observed\ncontexts and rewards to determine whether to follow a greedy algorithm or to\nexplore. We prove that this algorithm is rate optimal without any additional\nassumptions on the context distribution or the number of arms. Extensive\nsimulations demonstrate that Greedy-First successfully reduces exploration and\noutperforms existing (exploration-based) contextual bandit algorithms such as\nThompson sampling or upper confidence bound (UCB). \n\n"}
{"id": "1705.01351", "contents": "Title: Teichm\\\"uller spaces of Generalized Hyperelliptic Manifolds Abstract: In this paper we achieve a description of the connected components of\nTeichm\\\"uller space corresponding to Generalized Hyperelliptic Manifolds $X$.\nThese are the quotients $ X = T/G$ of a complex torus $T$ by the free action of\na finite group $G$, and they are also the K\\\"ahler classifying spaces for a\ncertain class of Euclidean cristallographic groups $\\Gamma$, the ones which are\ntorsion free and even. \n\n"}
{"id": "1705.02082", "contents": "Title: Motion Prediction Under Multimodality with Conditional Stochastic\n  Networks Abstract: Given a visual history, multiple future outcomes for a video scene are\nequally probable, in other words, the distribution of future outcomes has\nmultiple modes. Multimodality is notoriously hard to handle by standard\nregressors or classifiers: the former regress to the mean and the latter\ndiscretize a continuous high dimensional output space. In this work, we present\nstochastic neural network architectures that handle such multimodality through\nstochasticity: future trajectories of objects, body joints or frames are\nrepresented as deep, non-linear transformations of random (as opposed to\ndeterministic) variables. Such random variables are sampled from simple\nGaussian distributions whose means and variances are parametrized by the output\nof convolutional encoders over the visual history. We introduce novel\nconvolutional architectures for predicting future body joint trajectories that\noutperform fully connected alternatives \\cite{DBLP:journals/corr/WalkerDGH16}.\nWe introduce stochastic spatial transformers through optical flow warping for\npredicting future frames, which outperform their deterministic equivalents\n\\cite{DBLP:journals/corr/PatrauceanHC15}. Training stochastic networks involves\nan intractable marginalization over stochastic variables. We compare various\ntraining schemes that handle such marginalization through a) straightforward\nsampling from the prior, b) conditional variational autoencoders\n\\cite{NIPS2015_5775,DBLP:journals/corr/WalkerDGH16}, and, c) a proposed\nK-best-sample loss that penalizes the best prediction under a fixed \"prediction\nbudget\". We show experimental results on object trajectory prediction, human\nbody joint trajectory prediction and video prediction under varying future\nuncertainty, validating quantitatively and qualitatively our architectural\nchoices and training schemes. \n\n"}
{"id": "1705.02425", "contents": "Title: Trapping of Bose-Einstein condensates in a three-dimensional dark focus\n  generated by conical refraction Abstract: We present an efficient three-dimensional dark-focus optical trapping\npotential for neutral atoms and Bose-Einstein condensates. This \"optical\nbottle\" is created by a single blue-detuned light field exploiting the\nphenomenon of conical refraction occurring in biaxial crystals. The conversion\nof a Gaussian input beam to the bottle beam has an efficiency of close to 100 %\nand the optical setup requires the addition of the biaxial crystal and a\ncircular polarizer only. Based on the conical-refraction theory, we derive the\ngeneral form of the potential, the trapping frequencies, and the potential\nbarrier heights. We present experiments on confining a $^{87}$Rb Bose-Einstein\ncondensate in three dimensions. We determine the trap shape, the vibrational\nfrequencies along the weak axis, as well as the lifetime of ultracold atoms in\nthis type of potential. \n\n"}
{"id": "1705.02727", "contents": "Title: Automatic Recognition of Mammal Genera on Camera-Trap Images using\n  Multi-Layer Robust Principal Component Analysis and Mixture Neural Networks Abstract: The segmentation and classification of animals from camera-trap images is due\nto the conditions under which the images are taken, a difficult task. This work\npresents a method for classifying and segmenting mammal genera from camera-trap\nimages. Our method uses Multi-Layer Robust Principal Component Analysis (RPCA)\nfor segmenting, Convolutional Neural Networks (CNNs) for extracting features,\nLeast Absolute Shrinkage and Selection Operator (LASSO) for selecting features,\nand Artificial Neural Networks (ANNs) or Support Vector Machines (SVM) for\nclassifying mammal genera present in the Colombian forest. We evaluated our\nmethod with the camera-trap images from the Alexander von Humboldt Biological\nResources Research Institute. We obtained an accuracy of 92.65% classifying 8\nmammal genera and a False Positive (FP) class, using automatic-segmented\nimages. On the other hand, we reached 90.32% of accuracy classifying 10 mammal\ngenera, using ground-truth images only. Unlike almost all previous works, we\nconfront the animal segmentation and genera classification in the camera-trap\nrecognition. This method shows a new approach toward a fully-automatic\ndetection of animals from camera-trap images. \n\n"}
{"id": "1705.02891", "contents": "Title: Geometry and Dynamics for Markov Chain Monte Carlo Abstract: Markov Chain Monte Carlo methods have revolutionised mathematical computation\nand enabled statistical inference within many previously intractable models. In\nthis context, Hamiltonian dynamics have been proposed as an efficient way of\nbuilding chains which can explore probability densities efficiently. The method\nemerges from physics and geometry and these links have been extensively studied\nby a series of authors through the last thirty years. However, there is\ncurrently a gap between the intuitions and knowledge of users of the\nmethodology and our deep understanding of these theoretical foundations. The\naim of this review is to provide a comprehensive introduction to the geometric\ntools used in Hamiltonian Monte Carlo at a level accessible to statisticians,\nmachine learners and other users of the methodology with only a basic\nunderstanding of Monte Carlo methods. This will be complemented with some\ndiscussion of the most recent advances in the field which we believe will\nbecome increasingly relevant to applied scientists. \n\n"}
{"id": "1705.03290", "contents": "Title: Improving drug sensitivity predictions in precision medicine through\n  active expert knowledge elicitation Abstract: Predicting the efficacy of a drug for a given individual, using\nhigh-dimensional genomic measurements, is at the core of precision medicine.\nHowever, identifying features on which to base the predictions remains a\nchallenge, especially when the sample size is small. Incorporating expert\nknowledge offers a promising alternative to improve a prediction model, but\ncollecting such knowledge is laborious to the expert if the number of candidate\nfeatures is very large. We introduce a probabilistic model that can incorporate\nexpert feedback about the impact of genomic measurements on the sensitivity of\na cancer cell for a given drug. We also present two methods to intelligently\ncollect this feedback from the expert, using experimental design and\nmulti-armed bandit models. In a multiple myeloma blood cancer data set (n=51),\nexpert knowledge decreased the prediction error by 8%. Furthermore, the\nintelligent approaches can be used to reduce the workload of feedback\ncollection to less than 30% on average compared to a naive approach. \n\n"}
{"id": "1705.03483", "contents": "Title: Notwithstanding Bohr, the Reasons for QBism Abstract: Without Niels Bohr, QBism would be nothing. But QBism is not Bohr. This paper\nattempts to show that, despite a popular misconception, QBism is no minor tweak\nto Bohr's interpretation of quantum mechanics. It is something quite distinct.\nAlong the way, we lay out three tenets of QBism in some detail: 1) The Born\nRule---the foundation of what quantum theory means for QBism---is a normative\nstatement. It is about the decision-making behavior any individual agent should\nstrive for; it is not a descriptive \"law of nature\" in the usual sense. 2) All\nprobabilities, including all quantum probabilities, are so subjective they\nnever tell nature what to do. This includes probability-1 assignments. Quantum\nstates thus have no \"ontic hold\" on the world. 3) Quantum measurement outcomes\njust are personal experiences for the agent gambling upon them. Particularly,\nquantum measurement outcomes are not, to paraphrase Bohr, instances of\n\"irreversible amplification in devices whose design is communicable in common\nlanguage suitably refined by the terminology of classical physics.\" Finally, an\nexplicit comparison is given between QBism and Bohr with regard to three\nsubjects: a) The issue of the \"detached observer\" as it arose in a debate\nbetween Pauli and Bohr, b) Bohr's reply to Einstein, Podolsky, and Rosen, and\nc) Bohr's mature notion of \"quantum phenomena.\" At the end, we discuss how\nBohr's notion of phenomena may have something to offer the philosophy of\nWilliam James: A physics from which to further develop his vision of the\nworld---call it an ontology if you will---in which \"new being comes in local\nspots and patches.\" \n\n"}
{"id": "1705.04379", "contents": "Title: The Network Nullspace Property for Compressed Sensing of Big Data over\n  Networks Abstract: We present a novel condition, which we term the net- work nullspace property,\nwhich ensures accurate recovery of graph signals representing massive\nnetwork-structured datasets from few signal values. The network nullspace\nproperty couples the cluster structure of the underlying network-structure with\nthe geometry of the sampling set. Our results can be used to design efficient\nsampling strategies based on the network topology. \n\n"}
{"id": "1705.05671", "contents": "Title: Weakly quasisymmetric maps and uniform spaces Abstract: Suppose that $X$ and $Y$ are quasiconvex and complete metric spaces, that\n$G\\subset X$ and $G'\\subset Y$ are domains, and that $f: G\\to G'$ is a\nhomeomorphism. In this paper, we first give some basic properties of short\narcs, and then we show that: if $f$ is a weakly quasisymmetric mapping and $G'$\nis a quasiconvex domain, then the image $f(D)$ of every uniform subdomain $D$\nin $G$ is uniform. As an application, we get that if $f$ is a weakly\nquasisymmetric mapping and $G'$ is an uniform domain, then the images of the\nshort arcs in $G$ under $f$ are uniform arcs in the sense of diameter. \n\n"}
{"id": "1705.06499", "contents": "Title: A Non-monotone Alternating Updating Method for A Class of Matrix\n  Factorization Problems Abstract: In this paper we consider a general matrix factorization model which covers a\nlarge class of existing models with many applications in areas such as machine\nlearning and imaging sciences. To solve this possibly nonconvex, nonsmooth and\nnon-Lipschitz problem, we develop a non-monotone alternating updating method\nbased on a potential function. Our method essentially updates two blocks of\nvariables in turn by inexactly minimizing this potential function, and updates\nanother auxiliary block of variables using an explicit formula. The special\nstructure of our potential function allows us to take advantage of efficient\ncomputational strategies for non-negative matrix factorization to perform the\nalternating minimization over the two blocks of variables. A suitable line\nsearch criterion is also incorporated to improve the numerical performance.\nUnder some mild conditions, we show that the line search criterion is well\ndefined, and establish that the sequence generated is bounded and any cluster\npoint of the sequence is a stationary point. Finally, we conduct some numerical\nexperiments using real datasets to compare our method with some existing\nefficient methods for non-negative matrix factorization and matrix completion.\nThe numerical results show that our method can outperform these methods for\nthese specific applications. \n\n"}
{"id": "1705.06808", "contents": "Title: Adaptive Rate of Convergence of Thompson Sampling for Gaussian Process\n  Optimization Abstract: We consider the problem of global optimization of a function over a\ncontinuous domain. In our setup, we can evaluate the function sequentially at\npoints of our choice and the evaluations are noisy. We frame it as a\ncontinuum-armed bandit problem with a Gaussian Process prior on the function.\nIn this regime, most algorithms have been developed to minimize some form of\nregret. In this paper, we study the convergence of the sequential point $x^t$\nto the global optimizer $x^*$ for the Thompson Sampling approach. Under some\nassumptions and regularity conditions, we prove concentration bounds for $x^t$\nwhere the probability that $x^t$ is bounded away from $x^*$ decays\nexponentially fast in $t$. Moreover, the result allows us to derive adaptive\nconvergence rates depending on the function structure. \n\n"}
{"id": "1705.07019", "contents": "Title: Model-Robust Counterfactual Prediction Method Abstract: We develop a novel method for counterfactual analysis based on observational\ndata using prediction intervals for units under different exposures. Unlike\nmethods that target heterogeneous or conditional average treatment effects of\nan exposure, the proposed approach aims to take into account the irreducible\ndispersions of counterfactual outcomes so as to quantify the relative impact of\ndifferent exposures. The prediction intervals are constructed in a\ndistribution-free and model-robust manner based on the conformal prediction\napproach. The computational obstacles to this approach are circumvented by\nleveraging properties of a tuning-free method that learns sparse additive\npredictor models for counterfactual outcomes. The method is illustrated using\nboth real and synthetic data. \n\n"}
{"id": "1705.08030", "contents": "Title: Parallel Stochastic Gradient Descent with Sound Combiners Abstract: Stochastic gradient descent (SGD) is a well known method for regression and\nclassification tasks. However, it is an inherently sequential algorithm at each\nstep, the processing of the current example depends on the parameters learned\nfrom the previous examples. Prior approaches to parallelizing linear learners\nusing SGD, such as HOGWILD! and ALLREDUCE, do not honor these dependencies\nacross threads and thus can potentially suffer poor convergence rates and/or\npoor scalability. This paper proposes SYMSGD, a parallel SGD algorithm that, to\na first-order approximation, retains the sequential semantics of SGD. Each\nthread learns a local model in addition to a model combiner, which allows local\nmodels to be combined to produce the same result as what a sequential SGD would\nhave produced. This paper evaluates SYMSGD's accuracy and performance on 6\ndatasets on a shared-memory machine shows upto 11x speedup over our heavily\noptimized sequential baseline on 16 cores and 2.2x, on average, faster than\nHOGWILD!. \n\n"}
{"id": "1705.08580", "contents": "Title: Provable Estimation of the Number of Blocks in Block Models Abstract: Community detection is a fundamental unsupervised learning problem for\nunlabeled networks which has a broad range of applications. Many community\ndetection algorithms assume that the number of clusters $r$ is known apriori.\nIn this paper, we propose an approach based on semi-definite relaxations, which\ndoes not require prior knowledge of model parameters like many existing convex\nrelaxation methods and recovers the number of clusters and the clustering\nmatrix exactly under a broad parameter regime, with probability tending to one.\nOn a variety of simulated and real data experiments, we show that the proposed\nmethod often outperforms state-of-the-art techniques for estimating the number\nof clusters. \n\n"}
{"id": "1705.08865", "contents": "Title: Anti-spoofing Methods for Automatic SpeakerVerification System Abstract: Growing interest in automatic speaker verification (ASV)systems has lead to\nsignificant quality improvement of spoofing attackson them. Many research works\nconfirm that despite the low equal er-ror rate (EER) ASV systems are still\nvulnerable to spoofing attacks. Inthis work we overview different acoustic\nfeature spaces and classifiersto determine reliable and robust countermeasures\nagainst spoofing at-tacks. We compared several spoofing detection systems,\npresented so far,on the development and evaluation datasets of the Automatic\nSpeakerVerification Spoofing and Countermeasures (ASVspoof) Challenge\n2015.Experimental results presented in this paper demonstrate that the useof\nmagnitude and phase information combination provides a substantialinput into\nthe efficiency of the spoofing detection systems. Also wavelet-based features\nshow impressive results in terms of equal error rate. Inour overview we compare\nspoofing performance for systems based on dif-ferent classifiers. Comparison\nresults demonstrate that the linear SVMclassifier outperforms the conventional\nGMM approach. However, manyresearchers inspired by the great success of deep\nneural networks (DNN)approaches in the automatic speech recognition, applied\nDNN in thespoofing detection task and obtained quite low EER for known and\nun-known type of spoofing attacks. \n\n"}
{"id": "1705.08991", "contents": "Title: Approximation and Convergence Properties of Generative Adversarial\n  Learning Abstract: Generative adversarial networks (GAN) approximate a target data distribution\nby jointly optimizing an objective function through a \"two-player game\" between\na generator and a discriminator. Despite their empirical success, however, two\nvery basic questions on how well they can approximate the target distribution\nremain unanswered. First, it is not known how restricting the discriminator\nfamily affects the approximation quality. Second, while a number of different\nobjective functions have been proposed, we do not understand when convergence\nto the global minima of the objective function leads to convergence to the\ntarget distribution under various notions of distributional convergence.\n  In this paper, we address these questions in a broad and unified setting by\ndefining a notion of adversarial divergences that includes a number of recently\nproposed objective functions. We show that if the objective function is an\nadversarial divergence with some additional conditions, then using a restricted\ndiscriminator family has a moment-matching effect. Additionally, we show that\nfor objective functions that are strict adversarial divergences, convergence in\nthe objective function implies weak convergence, thus generalizing previous\nresults. \n\n"}
{"id": "1705.09407", "contents": "Title: An Efficient Algorithm for Bayesian Nearest Neighbours Abstract: K-Nearest Neighbours (k-NN) is a popular classification and regression\nalgorithm, yet one of its main limitations is the difficulty in choosing the\nnumber of neighbours. We present a Bayesian algorithm to compute the posterior\nprobability distribution for k given a target point within a data-set,\nefficiently and without the use of Markov Chain Monte Carlo (MCMC) methods or\nsimulation - alongside an exact solution for distributions within the\nexponential family. The central idea is that data points around our target are\ngenerated by the same probability distribution, extending outwards over the\nappropriate, though unknown, number of neighbours. Once the data is projected\nonto a distance metric of choice, we can transform the choice of k into a\nchange-point detection problem, for which there is an efficient solution: we\nrecursively compute the probability of the last change-point as we move towards\nour target, and thus de facto compute the posterior probability distribution\nover k. Applying this approach to both a classification and a regression UCI\ndata-sets, we compare favourably and, most importantly, by removing the need\nfor simulation, we are able to compute the posterior probability of k exactly\nand rapidly. As an example, the computational time for the Ripley data-set is a\nfew milliseconds compared to a few hours when using a MCMC approach. \n\n"}
{"id": "1705.10182", "contents": "Title: Fast learning rate of deep learning via a kernel perspective Abstract: We develop a new theoretical framework to analyze the generalization error of\ndeep learning, and derive a new fast learning rate for two representative\nalgorithms: empirical risk minimization and Bayesian deep learning. The series\nof theoretical analyses of deep learning has revealed its high expressive power\nand universal approximation capability. Although these analyses are highly\nnonparametric, existing generalization error analyses have been developed\nmainly in a fixed dimensional parametric model. To compensate this gap, we\ndevelop an infinite dimensional model that is based on an integral form as\nperformed in the analysis of the universal approximation capability. This\nallows us to define a reproducing kernel Hilbert space corresponding to each\nlayer. Our point of view is to deal with the ordinary finite dimensional deep\nneural network as a finite approximation of the infinite dimensional one. The\napproximation error is evaluated by the degree of freedom of the reproducing\nkernel Hilbert space in each layer. To estimate a good finite dimensional\nmodel, we consider both of empirical risk minimization and Bayesian deep\nlearning. We derive its generalization error bound and it is shown that there\nappears bias-variance trade-off in terms of the number of parameters of the\nfinite dimensional approximation. We show that the optimal width of the\ninternal layers can be determined through the degree of freedom and the\nconvergence rate can be faster than $O(1/\\sqrt{n})$ rate which has been shown\nin the existing studies. \n\n"}
{"id": "1705.10298", "contents": "Title: Construction of and efficient sampling from the simplicial configuration\n  model Abstract: Simplicial complexes are now a popular alternative to networks when it comes\nto describing the structure of complex systems, primarily because they encode\nmulti-node interactions explicitly. With this new description comes the need\nfor principled null models that allow for easy comparison with empirical data.\nWe propose a natural candidate, the simplicial configuration model. The core of\nour contribution is an efficient and uniform Markov chain Monte Carlo sampler\nfor this model. We demonstrate its usefulness in a short case study by\ninvestigating the topology of three real systems and their randomized\ncounterparts (using their Betti numbers). For two out of three systems, the\nmodel allows us to reject the hypothesis that there is no organization beyond\nthe local scale. \n\n"}
{"id": "1705.10466", "contents": "Title: Estimation of the lead-lag parameter between two stochastic processes\n  driven by fractional Brownian motions Abstract: In this paper, we consider the problem of estimating the lead-lag parameter\nbetween two stochastic processes driven by fractional Brownian motions (fBMs)\nof the Hurst parameter greater than 1/2. First we propose a lead-lag model\nbetween two stochastic processes involving fBMs, and then construct a\nconsistent estimator of the lead-lag parameter with possible convergence rate.\nOur estimator has the following two features. Firstly, we can construct the\nlead-lag estimator without using the Hurst parameters of the underlying fBMs.\nSecondly, our estimator can deal with some non-synchronous and irregular\nobservations. We explicitly calculate possible convergence rate when the\nobservation times are (1) synchronous and equidistant, and (2) given by the\nPoisson sampling scheme. We also present numerical simulations of our results\nusing the R package YUIMA. \n\n"}
{"id": "1705.10762", "contents": "Title: Generative Models of Visually Grounded Imagination Abstract: It is easy for people to imagine what a man with pink hair looks like, even\nif they have never seen such a person before. We call the ability to create\nimages of novel semantic concepts visually grounded imagination. In this paper,\nwe show how we can modify variational auto-encoders to perform this task. Our\nmethod uses a novel training objective, and a novel product-of-experts\ninference network, which can handle partially specified (abstract) concepts in\na principled and efficient way. We also propose a set of easy-to-compute\nevaluation metrics that capture our intuitive notions of what it means to have\ngood visual imagination, namely correctness, coverage, and compositionality\n(the 3 C's). Finally, we perform a detailed comparison of our method with two\nexisting joint image-attribute VAE methods (the JMVAE method of Suzuki et.al.\nand the BiVCCA method of Wang et.al.) by applying them to two datasets: the\nMNIST-with-attributes dataset (which we introduce here), and the CelebA\ndataset. \n\n"}
{"id": "1705.10883", "contents": "Title: Optimization of Tree Ensembles Abstract: Tree ensemble models such as random forests and boosted trees are among the\nmost widely used and practically successful predictive models in applied\nmachine learning and business analytics. Although such models have been used to\nmake predictions based on exogenous, uncontrollable independent variables, they\nare increasingly being used to make predictions where the independent variables\nare controllable and are also decision variables. In this paper, we study the\nproblem of tree ensemble optimization: given a tree ensemble that predicts some\ndependent variable using controllable independent variables, how should we set\nthese variables so as to maximize the predicted value? We formulate the problem\nas a mixed-integer optimization problem. We theoretically examine the strength\nof our formulation, provide a hierarchy of approximate formulations with bounds\non approximation quality and exploit the structure of the problem to develop\ntwo large-scale solution methods, one based on Benders decomposition and one\nbased on iteratively generating tree split constraints. We test our methodology\non real data sets, including two case studies in drug design and customized\npricing, and show that our methodology can efficiently solve large-scale\ninstances to near or full optimality, and outperforms solutions obtained by\nheuristic approaches. In our drug design case, we show how our approach can\nidentify compounds that efficiently trade-off predicted performance and novelty\nwith respect to existing, known compounds. In our customized pricing case, we\nshow how our approach can efficiently determine optimal store-level prices\nunder a random forest model that delivers excellent predictive accuracy. \n\n"}
{"id": "1706.01824", "contents": "Title: Robust Online Multi-Task Learning with Correlative and Personalized\n  Structures Abstract: Multi-Task Learning (MTL) can enhance a classifier's generalization\nperformance by learning multiple related tasks simultaneously. Conventional MTL\nworks under the offline or batch setting, and suffers from expensive training\ncost and poor scalability. To address such inefficiency issues, online learning\ntechniques have been applied to solve MTL problems. However, most existing\nalgorithms of online MTL constrain task relatedness into a presumed structure\nvia a single weight matrix, which is a strict restriction that does not always\nhold in practice. In this paper, we propose a robust online MTL framework that\novercomes this restriction by decomposing the weight matrix into two\ncomponents: the first one captures the low-rank common structure among tasks\nvia a nuclear norm and the second one identifies the personalized patterns of\noutlier tasks via a group lasso. Theoretical analysis shows the proposed\nalgorithm can achieve a sub-linear regret with respect to the best linear model\nin hindsight. Even though the above framework achieves good performance, the\nnuclear norm that simply adds all nonzero singular values together may not be a\ngood low-rank approximation. To improve the results, we use a log-determinant\nfunction as a non-convex rank approximation. The gradient scheme is applied to\noptimize log-determinant function and can obtain a closed-form solution for\nthis refined problem. Experimental results on a number of real-world\napplications verify the efficacy of our method. \n\n"}
{"id": "1706.05376", "contents": "Title: Wandering Montel Theorems for Hilbert Space Valued Holomorphic Functions Abstract: We prove a Montel theorem for Hilbert space valued functions, and a\nnon-commutative version of this theorem, by composing with unitaries to achieve\nconvergence. \n\n"}
{"id": "1706.06258", "contents": "Title: Learning-based Ensemble Average Propagator Estimation Abstract: By capturing the anisotropic water diffusion in tissue, diffusion magnetic\nresonance imaging (dMRI) provides a unique tool for noninvasively probing the\ntissue microstructure and orientation in the human brain. The diffusion profile\ncan be described by the ensemble average propagator (EAP), which is inferred\nfrom observed diffusion signals. However, accurate EAP estimation using the\nnumber of diffusion gradients that is clinically practical can be challenging.\nIn this work, we propose a deep learning algorithm for EAP estimation, which is\nnamed learning-based ensemble average propagator estimation (LEAPE). The EAP is\ncommonly represented by a basis and its associated coefficients, and here we\nchoose the SHORE basis and design a deep network to estimate the coefficients.\nThe network comprises two cascaded components. The first component is a\nmultiple layer perceptron (MLP) that simultaneously predicts the unknown\ncoefficients. However, typical training loss functions, such as mean squared\nerrors, may not properly represent the geometry of the possibly non-Euclidean\nspace of the coefficients, which in particular causes problems for the\nextraction of directional information from the EAP. Therefore, to regularize\nthe training, in the second component we compute an auxiliary output of\napproximated fiber orientation (FO) errors with the aid of a second MLP that is\ntrained separately. We performed experiments using dMRI data that resemble\nclinically achievable $q$-space sampling, and observed promising results\ncompared with the conventional EAP estimation method. \n\n"}
{"id": "1706.08470", "contents": "Title: Efficiency of quantum versus classical annealing in non-convex learning\n  problems Abstract: Quantum annealers aim at solving non-convex optimization problems by\nexploiting cooperative tunneling effects to escape local minima. The underlying\nidea consists in designing a classical energy function whose ground states are\nthe sought optimal solutions of the original optimization problem and add a\ncontrollable quantum transverse field to generate tunneling processes. A key\nchallenge is to identify classes of non-convex optimization problems for which\nquantum annealing remains efficient while thermal annealing fails. We show that\nthis happens for a wide class of problems which are central to machine\nlearning. Their energy landscapes is dominated by local minima that cause\nexponential slow down of classical thermal annealers while simulated quantum\nannealing converges efficiently to rare dense regions of optimal solutions. \n\n"}
{"id": "1706.08894", "contents": "Title: Unsupervised Feature Selection Based on Space Filling Concept Abstract: The paper deals with the adaptation of a new measure for the unsupervised\nfeature selection problems. The proposed measure is based on space filling\nconcept and is called the coverage measure. This measure was used for judging\nthe quality of an experimental space filling design. In the present work, the\ncoverage measure is adapted for selecting the smallest informative subset of\nvariables by reducing redundancy in data. This paper proposes a simple analogy\nto apply this measure. It is implemented in a filter algorithm for unsupervised\nfeature selection problems.\n  The proposed filter algorithm is robust with high dimensional data and can be\nimplemented without extra parameters. Further, it is tested with simulated data\nand real world case studies including environmental data and hyperspectral\nimage. Finally, the results are evaluated by using random forest algorithm. \n\n"}
{"id": "1706.08916", "contents": "Title: Complex variable approach to analysis of a fractional differential\n  equation in the real line Abstract: The first aim of this work is to establish a Peano type existence theorem for\nan initial value problem involving complex fractional derivative and the second\nis, as a consequence of this theorem, to give a partial answer to the local\nexistence of the continuous solution for the following problem with\nRiemann-Liouville fractional derivative: \\begin{equation*} \\begin{cases}\n&D^{q}u(x) = f\\big(x,u(x)\\big), \\\\ &u(0)=b, \\ \\ \\ (b\\neq 0). \\\\ \\end{cases}\n\\end{equation*} Moreover, in the special cases of considered problem, we\ninvestigate some geometric properties of the solutions. \n\n"}
{"id": "1707.01327", "contents": "Title: Hyperbolicity of varieties supporting a variation of Hodge structure Abstract: We generalize former results of Zuo and the first author showing some\nhyperbolicity properties of varieties supporting a variation of Hodge\nstructure. Our proof only uses the special curvature properties of period\ndomains. In particular, in contrast to the former approaches, it does not use\nany result on the asymptotic behaviour of the Hodge metric. \n\n"}
{"id": "1707.02850", "contents": "Title: Adaptive Binarization for Weakly Supervised Affordance Segmentation Abstract: The concept of affordance is important to understand the relevance of object\nparts for a certain functional interaction. Affordance types generalize across\nobject categories and are not mutually exclusive. This makes the segmentation\nof affordance regions of objects in images a difficult task. In this work, we\nbuild on an iterative approach that learns a convolutional neural network for\naffordance segmentation from sparse keypoints. During this process, the\npredictions of the network need to be binarized. In this work, we propose an\nadaptive approach for binarization and estimate the parameters for\ninitialization by approximated cross validation. We evaluate our approach on\ntwo affordance datasets where our approach outperforms the state-of-the-art for\nweakly supervised affordance segmentation. \n\n"}
{"id": "1707.03134", "contents": "Title: Least Square Variational Bayesian Autoencoder with Regularization Abstract: In recent years Variation Autoencoders have become one of the most popular\nunsupervised learning of complicated distributions.Variational Autoencoder\n(VAE) provides more efficient reconstructive performance over a traditional\nautoencoder. Variational auto enocders make better approximaiton than MCMC. The\nVAE defines a generative process in terms of ancestral sampling through a\ncascade of hidden stochastic layers. They are a directed graphic models.\nVariational autoencoder is trained to maximise the variational lower bound.\nHere we are trying maximise the likelihood and also at the same time we are\ntrying to make a good approximation of the data. Its basically trading of the\ndata log-likelihood and the KL divergence from the true posterior. This paper\ndescribes the scenario in which we wish to find a point-estimate to the\nparameters $\\theta$ of some parametric model in which we generate each\nobservations by first sampling a local latent variable and then sampling the\nassociated observation. Here we use least square loss function with\nregularization in the the reconstruction of the image, the least square loss\nfunction was found to give better reconstructed images and had a faster\ntraining time. \n\n"}
{"id": "1707.06209", "contents": "Title: Crowdsourcing Multiple Choice Science Questions Abstract: We present a novel method for obtaining high-quality, domain-targeted\nmultiple choice questions from crowd workers. Generating these questions can be\ndifficult without trading away originality, relevance or diversity in the\nanswer options. Our method addresses these problems by leveraging a large\ncorpus of domain-specific text and a small set of existing questions. It\nproduces model suggestions for document selection and answer distractor choice\nwhich aid the human question generation process. With this method we have\nassembled SciQ, a dataset of 13.7K multiple choice science exam questions\n(Dataset available at http://allenai.org/data.html). We demonstrate that the\nmethod produces in-domain questions by providing an analysis of this new\ndataset and by showing that humans cannot distinguish the crowdsourced\nquestions from original questions. When using SciQ as additional training data\nto existing questions, we observe accuracy improvements on real science exams. \n\n"}
{"id": "1707.06213", "contents": "Title: Analysis of $p$-Laplacian Regularization in Semi-Supervised Learning Abstract: We investigate a family of regression problems in a semi-supervised setting.\nThe task is to assign real-valued labels to a set of $n$ sample points,\nprovided a small training subset of $N$ labeled points. A goal of\nsemi-supervised learning is to take advantage of the (geometric) structure\nprovided by the large number of unlabeled data when assigning labels. We\nconsider random geometric graphs, with connection radius $\\epsilon(n)$, to\nrepresent the geometry of the data set. Functionals which model the task reward\nthe regularity of the estimator function and impose or reward the agreement\nwith the training data. Here we consider the discrete $p$-Laplacian\nregularization.\n  We investigate asymptotic behavior when the number of unlabeled points\nincreases, while the number of training points remains fixed. We uncover a\ndelicate interplay between the regularizing nature of the functionals\nconsidered and the nonlocality inherent to the graph constructions. We\nrigorously obtain almost optimal ranges on the scaling of $\\epsilon(n)$ for the\nasymptotic consistency to hold. We prove that the minimizers of the discrete\nfunctionals in random setting converge uniformly to the desired continuum\nlimit. Furthermore we discover that for the standard model used there is a\nrestrictive upper bound on how quickly $\\epsilon(n)$ must converge to zero as\n$n \\to \\infty$. We introduce a new model which is as simple as the original\nmodel, but overcomes this restriction. \n\n"}
{"id": "1707.06218", "contents": "Title: Analytic solution of the Domain Wall non-equilibrium stationary state Abstract: We consider the out-of-equilibrium dynamics generated by joining two domains\nwith arbitrary opposite magnetisations. We study the stationary state which\nemerges by the unitary evolution via the spin $1/2$ XXZ Hamiltonian, in the\ngapless regime, where the system develops a stationary spin current. Using the\ngeneralized hydrodynamic approach, we present a simple formula for the\nspace-time profile of the spin current and the magnetisation exact in the limit\nof large times. As a remarkable effect, we show that the stationary state has a\nstrongly discontinuous dependence on the strength of interaction. This feature\nallows us to give a qualitative estimation for the transient behavior of the\ncurrent which is compared with numerical simulations. Moreover, we analyse the\nbehavior around the edge of the magnetisation profile and we argue that, unlike\nthe XX free-fermionic point, interactions always prevent the emergence of a\nTracy-Widom scaling. \n\n"}
{"id": "1707.06487", "contents": "Title: A Nonlinear Kernel Support Matrix Machine for Matrix Learning Abstract: In many problems of supervised tensor learning (STL), real world data such as\nface images or MRI scans are naturally represented as matrices, which are also\ncalled as second order tensors. Most existing classifiers based on tensor\nrepresentation, such as support tensor machine (STM) need to solve iteratively\nwhich occupy much time and may suffer from local minima. In this paper, we\npresent a kernel support matrix machine (KSMM) to perform supervised learning\nwhen data are represented as matrices. KSMM is a general framework for the\nconstruction of matrix-based hyperplane to exploit structural information. We\nanalyze a unifying optimization problem for which we propose an asymptotically\nconvergent algorithm. Theoretical analysis for the generalization bounds is\nderived based on Rademacher complexity with respect to a probability\ndistribution. We demonstrate the merits of the proposed method by exhaustive\nexperiments on both simulation study and a number of real-word datasets from a\nvariety of application domains. \n\n"}
{"id": "1707.06611", "contents": "Title: Prolongation of SMAP to Spatio-temporally Seamless Coverage of\n  Continental US Using a Deep Learning Neural Network Abstract: The Soil Moisture Active Passive (SMAP) mission has delivered valuable\nsensing of surface soil moisture since 2015. However, it has a short time span\nand irregular revisit schedule. Utilizing a state-of-the-art time-series deep\nlearning neural network, Long Short-Term Memory (LSTM), we created a system\nthat predicts SMAP level-3 soil moisture data with atmospheric forcing,\nmodel-simulated moisture, and static physiographic attributes as inputs. The\nsystem removes most of the bias with model simulations and improves predicted\nmoisture climatology, achieving small test root-mean-squared error (<0.035) and\nhigh correlation coefficient >0.87 for over 75\\% of Continental United States,\nincluding the forested Southeast. As the first application of LSTM in\nhydrology, we show the proposed network avoids overfitting and is robust for\nboth temporal and spatial extrapolation tests. LSTM generalizes well across\nregions with distinct climates and physiography. With high fidelity to SMAP,\nLSTM shows great potential for hindcasting, data assimilation, and weather\nforecasting. \n\n"}
{"id": "1707.06787", "contents": "Title: Parametric CR-umbilical Locus of Ellipsoids in $\\mathbb{C}^2$ Abstract: For every real numbers $a \\geqslant 1$, $b \\geqslant 1$ with $(a,b) \\neq\n(1,1)$, the curve parametrized by $\\theta \\in \\mathbb{R}$ valued in\n$\\mathbb{C}^2 \\cong \\mathbb{R}^4$ \\[ \\gamma\\, \\colon \\ \\ \\ \\theta\n\\,\\,\\,\\longmapsto\\,\\,\\, \\big(\nx(\\theta)+{\\scriptstyle{\\sqrt{-1}}}\\,y(\\theta),\\,\\,\nu(\\theta)+{\\scriptstyle{\\sqrt{-1}}}\\,v(\\theta) \\big) \\] with components: \\[\nx(\\theta) \\,:=\\, {\\textstyle{\\sqrt{\\frac{a-1}{a\\,(ab-1)}}}}\\, \\cos\\,\\theta, \\ \\\n\\ \\ \\ y(\\theta) \\,:=\\, {\\textstyle{\\sqrt{\\frac{b\\,(a-1)}{ab-1}}}}\\,\n\\sin\\,\\theta, \\ \\ \\ \\ \\ u(\\theta) \\,:=\\,\n{\\textstyle{\\sqrt{\\frac{b-1}{b\\,(ab-1)}}}}\\, \\sin\\,\\theta, \\ \\ \\ \\ \\ v(\\theta)\n\\,:=\\, -\\, {\\textstyle{\\sqrt{\\frac{a\\,(b-1)}{ab-1}}}}\\, \\cos\\,\\theta, \\] has\nimage contained in the CR-umbilical locus: \\[ \\gamma(\\mathbb{R}) \\,\\subset\\,\n{\\sf UmbCR} \\big({\\sf E}_{a,b}\\big) \\,\\subset\\, {\\sf E}_{a,b} \\] of the\nellipsoid ${\\sf E}_{a,b} \\subset \\mathbb{C}^2$ of equation\n$a\\,x^2+y^2+b\\,u^2+y^2 = 1$. \n\n"}
{"id": "1707.06903", "contents": "Title: A New Family of Near-metrics for Universal Similarity Abstract: We propose a family of near-metrics based on local graph diffusion to capture\nsimilarity for a wide class of data sets. These quasi-metametrics, as their\nnames suggest, dispense with one or two standard axioms of metric spaces,\nspecifically distinguishability and symmetry, so that similarity between data\npoints of arbitrary type and form could be measured broadly and effectively.\nThe proposed near-metric family includes the forward k-step diffusion and its\nreverse, typically on the graph consisting of data objects and their features.\nBy construction, this family of near-metrics is particularly appropriate for\ncategorical data, continuous data, and vector representations of images and\ntext extracted via deep learning approaches. We conduct extensive experiments\nto evaluate the performance of this family of similarity measures and compare\nand contrast with traditional measures of similarity used for each specific\napplication and with the ground truth when available. We show that for\nstructured data including categorical and continuous data, the near-metrics\ncorresponding to normalized forward k-step diffusion (k small) work as one of\nthe best performing similarity measures; for vector representations of text and\nimages including those extracted from deep learning, the near-metrics derived\nfrom normalized and reverse k-step graph diffusion (k very small) exhibit\noutstanding ability to distinguish data points from different classes. \n\n"}
{"id": "1707.08040", "contents": "Title: A Simple Exponential Family Framework for Zero-Shot Learning Abstract: We present a simple generative framework for learning to predict previously\nunseen classes, based on estimating class-attribute-gated class-conditional\ndistributions. We model each class-conditional distribution as an exponential\nfamily distribution and the parameters of the distribution of each seen/unseen\nclass are defined as functions of the respective observed class attributes.\nThese functions can be learned using only the seen class data and can be used\nto predict the parameters of the class-conditional distribution of each unseen\nclass. Unlike most existing methods for zero-shot learning that represent\nclasses as fixed embeddings in some vector space, our generative model\nnaturally represents each class as a probability distribution. It is simple to\nimplement and also allows leveraging additional unlabeled data from unseen\nclasses to improve the estimates of their class-conditional distributions using\ntransductive/semi-supervised learning. Moreover, it extends seamlessly to\nfew-shot learning by easily updating these distributions when provided with a\nsmall number of additional labelled examples from unseen classes. Through a\ncomprehensive set of experiments on several benchmark data sets, we demonstrate\nthe efficacy of our framework. \n\n"}
{"id": "1707.09430", "contents": "Title: Human in the Loop: Interactive Passive Automata Learning via\n  Evidence-Driven State-Merging Algorithms Abstract: We present an interactive version of an evidence-driven state-merging (EDSM)\nalgorithm for learning variants of finite state automata. Learning these\nautomata often amounts to recovering or reverse engineering the model\ngenerating the data despite noisy, incomplete, or imperfectly sampled data\nsources rather than optimizing a purely numeric target function. Domain\nexpertise and human knowledge about the target domain can guide this process,\nand typically is captured in parameter settings. Often, domain expertise is\nsubconscious and not expressed explicitly. Directly interacting with the\nlearning algorithm makes it easier to utilize this knowledge effectively. \n\n"}
{"id": "1708.00260", "contents": "Title: Deep Asymmetric Multi-task Feature Learning Abstract: We propose Deep Asymmetric Multitask Feature Learning (Deep-AMTFL) which can\nlearn deep representations shared across multiple tasks while effectively\npreventing negative transfer that may happen in the feature sharing process.\nSpecifically, we introduce an asymmetric autoencoder term that allows reliable\npredictors for the easy tasks to have high contribution to the feature learning\nwhile suppressing the influences of unreliable predictors for more difficult\ntasks. This allows the learning of less noisy representations, and enables\nunreliable predictors to exploit knowledge from the reliable predictors via the\nshared latent features. Such asymmetric knowledge transfer through shared\nfeatures is also more scalable and efficient than inter-task asymmetric\ntransfer. We validate our Deep-AMTFL model on multiple benchmark datasets for\nmultitask learning and image classification, on which it significantly\noutperforms existing symmetric and asymmetric multitask learning models, by\neffectively preventing negative transfer in deep feature learning. \n\n"}
{"id": "1708.00955", "contents": "Title: Hamiltonian Monte Carlo with Energy Conserving Subsampling Abstract: Hamiltonian Monte Carlo (HMC) samples efficiently from high-dimensional\nposterior distributions with proposed parameter draws obtained by iterating on\na discretized version of the Hamiltonian dynamics. The iterations make HMC\ncomputationally costly, especially in problems with large datasets, since it is\nnecessary to compute posterior densities and their derivatives with respect to\nthe parameters. Naively computing the Hamiltonian dynamics on a subset of the\ndata causes HMC to lose its key ability to generate distant parameter proposals\nwith high acceptance probability. The key insight in our article is that\nefficient subsampling HMC for the parameters is possible if both the dynamics\nand the acceptance probability are computed from the same data subsample in\neach complete HMC iteration. We show that this is possible to do in a\nprincipled way in a HMC-within-Gibbs framework where the subsample is updated\nusing a pseudo marginal MH step and the parameters are then updated using an\nHMC step, based on the current subsample. We show that our subsampling methods\nare fast and compare favorably to two popular sampling algorithms that utilize\ngradient estimates from data subsampling. We also explore the current\nlimitations of subsampling HMC algorithms by varying the quality of the\nvariance reducing control variates used in the estimators of the posterior\ndensity and its gradients. \n\n"}
{"id": "1708.01840", "contents": "Title: Quantum difference parametric amplification and oscillation Abstract: A recent article [W.C.W. Huang and H. Batelaan, arXiv:1708.0057v1] analysed\nthe dualism between optical and difference parametric amplification, performing\na classical analysis of a system where two electromagnetic fields are produced\nby another of a frequency which is the difference of the frequency of the other\ntwo. The authors claimed that this process would not violate energy\nconservation at the classical level, but that a quantum description would\nnecessarily require a non-Hermitian Hamiltonian and therefore would not exist.\nIn this work we show that the process can proceed quantum mechanically if\ndescribed by the correct Hamiltonian, that energy conservation is not violated,\nand that fields are produced with interesting quantum statistics. Furthermore,\nwe show that the process can be thought of as different types of already known\nthree-wave mixing processes, with the actual type depending on either initial\nconditions or personal preference. \n\n"}
{"id": "1708.02999", "contents": "Title: Demixing Structured Superposition Signals from Periodic and Aperiodic\n  Nonlinear Observations Abstract: We consider the demixing problem of two (or more) structured high-dimensional\nvectors from a limited number of nonlinear observations where this nonlinearity\nis due to either a periodic or an aperiodic function. We study certain families\nof structured superposition models, and propose a method which provably\nrecovers the components given (nearly) $m = \\mathcal{O}(s)$ samples where $s$\ndenotes the sparsity level of the underlying components. This strictly improves\nupon previous nonlinear demixing techniques and asymptotically matches the best\npossible sample complexity. We also provide a range of simulations to\nillustrate the performance of the proposed algorithms. \n\n"}
{"id": "1708.03020", "contents": "Title: Non-stationary Stochastic Optimization under $L_{p,q}$-Variation\n  Measures Abstract: We consider a non-stationary sequential stochastic optimization problem, in\nwhich the underlying cost functions change over time under a variation budget\nconstraint. We propose an $L_{p,q}$-variation functional to quantify the\nchange, which yields less variation for dynamic function sequences whose\nchanges are constrained to short time periods or small subsets of input domain.\nUnder the $L_{p,q}$-variation constraint, we derive both upper and matching\nlower regret bounds for smooth and strongly convex function sequences, which\ngeneralize previous results in Besbes et al. (2015). Furthermore, we provide an\nupper bound for general convex function sequences with noisy gradient feedback,\nwhich matches the optimal rate as $p\\to\\infty$. Our results reveal some\nsurprising phenomena under this general variation functional, such as the curse\nof dimensionality of the function domain. The key technical novelties in our\nanalysis include affinity lemmas that characterize the distance of the\nminimizers of two convex functions with bounded Lp difference, and a cubic\nspline based construction that attains matching lower bounds. \n\n"}
{"id": "1708.04676", "contents": "Title: Analysis of the Jun Ishiwara's \"The universal meaning of the quantum of\n  action\" Abstract: Here we present an analysis of the paper \"Universelle Bedeutung des\nWirkungsquantums\" (The universal meaning of the quantum of action), published\nby Jun Ishiwara in German in the \"Proceedings of Tokyo Mathematico-Physical\nSociety 8 (1915) 106-116\". In his work, Ishiwara, established in the Sendai\nUniversity, Japan, proposed - simultaneously with Arnold Sommerfeld, William\nWilson and Niels Bohr in Europe - the phase-space-integral quantization, a rule\nthat would be incorporated into the old-quantum-theory formalism. The\ndiscussions and analysis render this paper fully accessible to undergraduate\nstudents of physics with elementary knowledge of quantum mechanics. \n\n"}
{"id": "1708.04781", "contents": "Title: Racing Thompson: an Efficient Algorithm for Thompson Sampling with\n  Non-conjugate Priors Abstract: Thompson sampling has impressive empirical performance for many multi-armed\nbandit problems. But current algorithms for Thompson sampling only work for the\ncase of conjugate priors since these algorithms require to infer the posterior,\nwhich is often computationally intractable when the prior is not conjugate. In\nthis paper, we propose a novel algorithm for Thompson sampling which only\nrequires to draw samples from a tractable distribution, so our algorithm is\nefficient even when the prior is non-conjugate. To do this, we reformulate\nThompson sampling as an optimization problem via the Gumbel-Max trick. After\nthat we construct a set of random variables and our goal is to identify the one\nwith highest mean. Finally, we solve it with techniques in best arm\nidentification. \n\n"}
{"id": "1708.04891", "contents": "Title: Distribution on Warp Maps for Alignment of Open and Closed Curves Abstract: Alignment of curve data is an integral part of their statistical analysis,\nand can be achieved using model- or optimization-based approaches. The\nparameter space is usually the set of monotone, continuous warp maps of a\ndomain. Infinite-dimensional nature of the parameter space encourages sampling\nbased approaches, which require a distribution on the set of warp maps.\nMoreover, the distribution should also enable sampling in the presence of\nimportant landmark information on the curves which constrain the warp maps. For\nalignment of closed and open curves in $\\mathbb{R}^d, d=1,2,3$, possibly with\nlandmark information, we provide a constructive, point-process based definition\nof a distribution on the set of warp maps of $[0,1]$ and the unit circle\n$\\mathbb{S}^1$ that is (1) simple to sample from, and (2) possesses the\ndesiderata for decomposition of the alignment problem with landmark constraints\ninto multiple unconstrained ones. For warp maps on $[0,1]$, the distribution is\nrelated to the Dirichlet process. We demonstrate its utility by using it as a\nprior distribution on warp maps in a Bayesian model for alignment of two\nunivariate curves, and as a proposal distribution in a stochastic algorithm\nthat optimizes a suitable alignment functional for higher-dimensional curves.\nSeveral examples from simulated and real datasets are provided. \n\n"}
{"id": "1708.05254", "contents": "Title: Adaptive Clustering Using Kernel Density Estimators Abstract: We derive and analyze a generic, recursive algorithm for estimating all\nsplits in a finite cluster tree as well as the corresponding clusters. We\nfurther investigate statistical properties of this generic clustering algorithm\nwhen it receives level set estimates from a kernel density estimator. In\nparticular, we derive finite sample guarantees, consistency, rates of\nconvergence, and an adaptive data-driven strategy for choosing the kernel\nbandwidth. For these results we do not need continuity assumptions on the\ndensity such as H\\\"{o}lder continuity, but only require intuitive geometric\nassumptions of non-parametric nature. \n\n"}
{"id": "1708.05851", "contents": "Title: Image2song: Song Retrieval via Bridging Image Content and Lyric Words Abstract: Image is usually taken for expressing some kinds of emotions or purposes,\nsuch as love, celebrating Christmas. There is another better way that combines\nthe image and relevant song to amplify the expression, which has drawn much\nattention in the social network recently. Hence, the automatic selection of\nsongs should be expected. In this paper, we propose to retrieve semantic\nrelevant songs just by an image query, which is named as the image2song\nproblem. Motivated by the requirements of establishing correlation in\nsemantic/content, we build a semantic-based song retrieval framework, which\nlearns the correlation between image content and lyric words. This model uses a\nconvolutional neural network to generate rich tags from image regions, a\nrecurrent neural network to model lyric, and then establishes correlation via a\nmulti-layer perceptron. To reduce the content gap between image and lyric, we\npropose to make the lyric modeling focus on the main image content via a tag\nattention. We collect a dataset from the social-sharing multimodal data to\nstudy the proposed problem, which consists of (image, music clip, lyric)\ntriplets. We demonstrate that our proposed model shows noticeable results in\nthe image2song retrieval task and provides suitable songs. Besides, the\nsong2image task is also performed. \n\n"}
{"id": "1708.05929", "contents": "Title: Explaining Anomalies in Groups with Characterizing Subspace Rules Abstract: Anomaly detection has numerous applications and has been studied vastly. We\nconsider a complementary problem that has a much sparser literature: anomaly\ndescription. Interpretation of anomalies is crucial for practitioners for\nsense-making, troubleshooting, and planning actions. To this end, we present a\nnew approach called x-PACS (for eXplaining Patterns of Anomalies with\nCharacterizing Subspaces), which \"reverse-engineers\" the known anomalies by\nidentifying (1) the groups (or patterns) that they form, and (2) the\ncharacterizing subspace and feature rules that separate each anomalous pattern\nfrom normal instances. Explaining anomalies in groups not only saves analyst\ntime and gives insight into various types of anomalies, but also draws\nattention to potentially critical, repeating anomalies.\n  In developing x-PACS, we first construct a desiderata for the anomaly\ndescription problem. From a descriptive data mining perspective, our method\nexhibits five desired properties in our desiderata. Namely, it can unearth\nanomalous patterns (i) of multiple different types, (ii) hidden in arbitrary\nsubspaces of a high dimensional space, (iii) interpretable by the analysts,\n(iv) different from normal patterns of the data, and finally (v) succinct,\nproviding the shortest data description. Furthermore, x-PACS is highly\nparallelizable and scales linearly in terms of data size.\n  No existing work on anomaly description satisfies all of these properties\nsimultaneously. While not our primary goal, the anomalous patterns we find\nserve as interpretable \"signatures\" and can be used for detection. We show the\neffectiveness of x-PACS in explanation as well as detection on real-world\ndatasets as compared to state-of-the-art. \n\n"}
{"id": "1708.06516", "contents": "Title: H\\\"older continuous solutions of the Monge-Amp\\`ere equation on compact\n  Hermitian manifolds Abstract: We show that a positive Borel measure of positive finite total mass, on\ncompact Hermitian manifolds, admits a Holder continuous quasi-plurisubharmonic\nsolution to the Monge-Ampere equation if and only if it is dominated locally by\nMonge-Ampere measures of Holder continuous plurisubharmonic functions. \n\n"}
{"id": "1708.06822", "contents": "Title: Deep EndoVO: A Recurrent Convolutional Neural Network (RCNN) based\n  Visual Odometry Approach for Endoscopic Capsule Robots Abstract: Ingestible wireless capsule endoscopy is an emerging minimally invasive\ndiagnostic technology for inspection of the GI tract and diagnosis of a wide\nrange of diseases and pathologies. Medical device companies and many research\ngroups have recently made substantial progresses in converting passive capsule\nendoscopes to active capsule robots, enabling more accurate, precise, and\nintuitive detection of the location and size of the diseased areas. Since a\nreliable real time pose estimation functionality is crucial for actively\ncontrolled endoscopic capsule robots, in this study, we propose a monocular\nvisual odometry (VO) method for endoscopic capsule robot operations. Our method\nlies on the application of the deep Recurrent Convolutional Neural Networks\n(RCNNs) for the visual odometry task, where Convolutional Neural Networks\n(CNNs) and Recurrent Neural Networks (RNNs) are used for the feature extraction\nand inference of dynamics across the frames, respectively. Detailed analyses\nand evaluations made on a real pig stomach dataset proves that our system\nachieves high translational and rotational accuracies for different types of\nendoscopic capsule robot trajectories. \n\n"}
{"id": "1708.07852", "contents": "Title: Mixed Membership Estimation for Social Networks Abstract: In economics and social science, network data are regularly observed, and a\nthorough understanding of the network community structure facilitates the\ncomprehension of economic patterns and activities. Consider an undirected\nnetwork with $n$ nodes and $K$ communities. We model the network using the\nDegree-Corrected Mixed-Membership (DCMM) model, where for each node $i$, there\nexists a membership vector $\\pi_i = (\\pi_i(1), \\pi_i(2), \\ldots, \\pi_i(K))'$,\nwhere $\\pi_i(k)$ is the weight that node $i$ puts in community $k$, $1 \\leq k\n\\leq K$. In comparison to the well-known stochastic block model (SBM), the DCMM\npermits both severe degree heterogeneity and mixed memberships, making it\nconsiderably more realistic and general. We present an efficient approach,\nMixed-SCORE, for estimating the mixed membership vectors of all nodes and the\nother DCMM parameters. This approach is inspired by the discovery of a delicate\nsimplex structure in the spectral domain. We derive explicit error rates for\nthe Mixed-SCORE algorithm and demonstrate that it is rate-optimal over a broad\nparameter space. Our findings provide a novel statistical tool for network\ncommunity analysis, which can be used to understand network formations, extract\nnodal features, identify unobserved covariates in dyadic regressions, and\nestimate peer effects. We applied Mixed-SCORE to a political blog network, two\ntrade networks, a co-authorship network, and a citee network, and obtained\ninterpretable results. \n\n"}
{"id": "1708.08682", "contents": "Title: Log-Convexity of Weighted Area Integral Means of $H^p$ Functions on the\n  Upper Half-plan Abstract: In the present work weighted area integral means $M_{p,\\varphi}(f;{\\mathrm\n{Im}}z)$ are studied and it is proved that the function $y\\to \\log\nM_{p,\\varphi}(f;y)$ is convex in the case when $f$ belongs to a Hardy space on\nthe upper half-plane. \n\n"}
{"id": "1708.08687", "contents": "Title: Performance Guaranteed Network Acceleration via High-Order Residual\n  Quantization Abstract: Input binarization has shown to be an effective way for network acceleration.\nHowever, previous binarization scheme could be regarded as simple pixel-wise\nthresholding operations (i.e., order-one approximation) and suffers a big\naccuracy loss. In this paper, we propose a highorder binarization scheme, which\nachieves more accurate approximation while still possesses the advantage of\nbinary operation. In particular, the proposed scheme recursively performs\nresidual quantization and yields a series of binary input images with\ndecreasing magnitude scales. Accordingly, we propose high-order binary\nfiltering and gradient propagation operations for both forward and backward\ncomputations. Theoretical analysis shows approximation error guarantee property\nof proposed method. Extensive experimental results demonstrate that the\nproposed scheme yields great recognition accuracy while being accelerated. \n\n"}
{"id": "1709.00141", "contents": "Title: Context Based Visual Content Verification Abstract: In this paper the intermediary visual content verification method based on\nmulti-level co-occurrences is studied. The co-occurrence statistics are in\ngeneral used to determine relational properties between objects based on\ninformation collected from data. As such these measures are heavily subject to\nrelative number of occurrences and give only limited amount of accuracy when\npredicting objects in real world. In order to improve the accuracy of this\nmethod in the verification task, we include the context information such as\nlocation, type of environment etc. In order to train our model we provide new\nannotated dataset the Advanced Attribute VOC (AAVOC) that contains additional\nproperties of the image. We show that the usage of context greatly improve the\naccuracy of verification with up to 16% improvement. \n\n"}
{"id": "1709.01919", "contents": "Title: Estimation of a Low-rank Topic-Based Model for Information Cascades Abstract: We consider the problem of estimating the latent structure of a social\nnetwork based on the observed information diffusion events, or cascades, where\nthe observations for a given cascade consist of only the timestamps of\ninfection for infected nodes but not the source of the infection. Most of the\nexisting work on this problem has focused on estimating a diffusion matrix\nwithout any structural assumptions on it. In this paper, we propose a novel\nmodel based on the intuition that an information is more likely to propagate\namong two nodes if they are interested in similar topics which are also\nprominent in the information content. In particular, our model endows each node\nwith an influence vector (which measures how authoritative the node is on each\ntopic) and a receptivity vector (which measures how susceptible the node is for\neach topic). We show how this node-topic structure can be estimated from the\nobserved cascades, and prove the consistency of the estimator. Experiments on\nsynthetic and real data demonstrate the improved performance and better\ninterpretability of our model compared to existing state-of-the-art methods. \n\n"}
{"id": "1709.04384", "contents": "Title: Generating Music Medleys via Playing Music Puzzle Games Abstract: Generating music medleys is about finding an optimal permutation of a given\nset of music clips. Toward this goal, we propose a self-supervised learning\ntask, called the music puzzle game, to train neural network models to learn the\nsequential patterns in music. In essence, such a game requires machines to\ncorrectly sort a few multisecond music fragments. In the training stage, we\nlearn the model by sampling multiple non-overlapping fragment pairs from the\nsame songs and seeking to predict whether a given pair is consecutive and is in\nthe correct chronological order. For testing, we design a number of puzzle\ngames with different difficulty levels, the most difficult one being music\nmedley, which requiring sorting fragments from different songs. On the basis of\nstate-of-the-art Siamese convolutional network, we propose an improved\narchitecture that learns to embed frame-level similarity scores computed from\nthe input fragment pairs to a common space, where fragment pairs in the correct\norder can be more easily identified. Our result shows that the resulting model,\ndubbed as the similarity embedding network (SEN), performs better than\ncompeting models across different games, including music jigsaw puzzle, music\nsequencing, and music medley. Example results can be found at our project\nwebsite, https://remyhuang.github.io/DJnet. \n\n"}
{"id": "1709.04451", "contents": "Title: Alternating minimization and alternating descent over nonconvex sets Abstract: We analyze the performance of alternating minimization for loss functions\noptimized over two variables, where each variable may be restricted to lie in\nsome potentially nonconvex constraint set. This type of setting arises\nnaturally in high-dimensional statistics and signal processing, where the\nvariables often reflect different structures or components within the signals\nbeing considered. Our analysis relies on the notion of local concavity\ncoefficients, which has been proposed in Barber and Ha to measure and quantify\nthe concavity of a general nonconvex set. Our results further reveal important\ndistinctions between alternating and non-alternating methods. Since computing\nthe alternating minimization steps may not be tractable for some problems, we\nalso consider an inexact version of the algorithm and provide a set of\nsufficient conditions to ensure fast convergence of the inexact algorithms. We\ndemonstrate our framework on several examples, including low rank + sparse\ndecomposition and multitask regression, and provide numerical experiments to\nvalidate our theoretical results. \n\n"}
{"id": "1709.05328", "contents": "Title: Granger Mediation Analysis of Multiple Time Series with an Application\n  to fMRI Abstract: It becomes increasingly popular to perform mediation analysis for complex\ndata from sophisticated experimental studies. In this paper, we present Granger\nMediation Analysis (GMA), a new framework for causal mediation analysis of\nmultiple time series. This framework is motivated by a functional magnetic\nresonance imaging (fMRI) experiment where we are interested in estimating the\nmediation effects between a randomized stimulus time series and brain activity\ntime series from two brain regions. The stable unit treatment assumption for\ncausal mediation analysis is thus unrealistic for this type of time series\ndata. To address this challenge, our framework integrates two types of models:\ncausal mediation analysis across the variables and vector autoregressive models\nacross the temporal observations. We further extend this framework to handle\nmultilevel data to address individual variability and correlated errors between\nthe mediator and the outcome variables. These models not only provide valid\ncausal mediation for time series data but also model the causal dynamics across\ntime. We show that the modeling parameters in our models are identifiable, and\nwe develop computationally efficient methods to maximize the likelihood-based\noptimization criteria. Simulation studies show that our method reduces the\nestimation bias and improve statistical power, compared to existing approaches.\nOn a real fMRI data set, our approach not only infers the causal effects of\nbrain pathways but accurately captures the feedback effect of the outcome\nregion on the mediator region. \n\n"}
{"id": "1709.06489", "contents": "Title: Accurate Genomic Prediction Of Human Height Abstract: We construct genomic predictors for heritable and extremely complex human\nquantitative traits (height, heel bone density, and educational attainment)\nusing modern methods in high dimensional statistics (i.e., machine learning).\nReplication tests show that these predictors capture, respectively, $\\sim$40,\n20, and 9 percent of total variance for the three traits. For example,\npredicted heights correlate $\\sim$0.65 with actual height; actual heights of\nmost individuals in validation samples are within a few cm of the prediction.\nThe variance captured for height is comparable to the estimated SNP\nheritability from GCTA (GREML) analysis, and seems to be close to its\nasymptotic value (i.e., as sample size goes to infinity), suggesting that we\nhave captured most of the heritability for the SNPs used. Thus, our results\nresolve the common SNP portion of the \"missing heritability\" problem -- i.e.,\nthe gap between prediction R-squared and SNP heritability. The $\\sim$20k\nactivated SNPs in our height predictor reveal the genetic architecture of human\nheight, at least for common SNPs. Our primary dataset is the UK Biobank cohort,\ncomprised of almost 500k individual genotypes with multiple phenotypes. We also\nuse other datasets and SNPs found in earlier GWAS for out-of-sample validation\nof our results. \n\n"}
{"id": "1709.08568", "contents": "Title: The Consciousness Prior Abstract: A new prior is proposed for learning representations of high-level concepts\nof the kind we manipulate with language. This prior can be combined with other\npriors in order to help disentangling abstract factors from each other. It is\ninspired by cognitive neuroscience theories of consciousness, seen as a\nbottleneck through which just a few elements, after having been selected by\nattention from a broader pool, are then broadcast and condition further\nprocessing, both in perception and decision-making. The set of recently\nselected elements one becomes aware of is seen as forming a low-dimensional\nconscious state. This conscious state is combining the few concepts\nconstituting a conscious thought, i.e., what one is immediately conscious of at\na particular moment. We claim that this architectural and\ninformation-processing constraint corresponds to assumptions about the joint\ndistribution between high-level concepts. To the extent that these assumptions\nare generally true (and the form of natural language seems consistent with\nthem), they can form a useful prior for representation learning. A\nlow-dimensional thought or conscious state is analogous to a sentence: it\ninvolves only a few variables and yet can make a statement with very high\nprobability of being true. This is consistent with a joint distribution (over\nhigh-level concepts) which has the form of a sparse factor graph, i.e., where\nthe dependencies captured by each factor of the factor graph involve only very\nfew variables while creating a strong dip in the overall energy function. The\nconsciousness prior also makes it natural to map conscious states to natural\nlanguage utterances or to express classical AI knowledge in a form similar to\nfacts and rules, albeit capturing uncertainty as well as efficient search\nmechanisms implemented by attention mechanisms. \n\n"}
{"id": "1709.08894", "contents": "Title: On the regularization of Wasserstein GANs Abstract: Since their invention, generative adversarial networks (GANs) have become a\npopular approach for learning to model a distribution of real (unlabeled) data.\nConvergence problems during training are overcome by Wasserstein GANs which\nminimize the distance between the model and the empirical distribution in terms\nof a different metric, but thereby introduce a Lipschitz constraint into the\noptimization problem. A simple way to enforce the Lipschitz constraint on the\nclass of functions, which can be modeled by the neural network, is weight\nclipping. It was proposed that training can be improved by instead augmenting\nthe loss by a regularization term that penalizes the deviation of the gradient\nof the critic (as a function of the network's input) from one. We present\ntheoretical arguments why using a weaker regularization term enforcing the\nLipschitz constraint is preferable. These arguments are supported by\nexperimental results on toy data sets. \n\n"}
{"id": "1709.09263", "contents": "Title: Quantum spreading of a self-gravitating wave-packet in singularity free\n  gravity Abstract: In this paper we will study for the first time how the wave-packet of a\nself-gravitating meso-scopic system spreads in theories beyond Einstein's\ngeneral relativity. In particular, we will consider a ghost-free infinite\nderivative gravity, which resolves the $1/r$ singularity in the potential -\nsuch that the gradient of the potential vanishes within the scale of\nnon-locality. We will show that a quantum wave-packet spreads faster for a\nghost-free and singularity-free gravity as compared to the Newtonian case,\ntherefore providing us a unique scenario for testing classical and quantum\nproperties of short-distance gravity in a laboratory in the near future. \n\n"}
{"id": "1709.09616", "contents": "Title: Geodesics in the space of Kahler cone metrics, II. Uniqueness of\n  constant scalar curvature Kahler cone metrics Abstract: This is a continuation of the previous articles on Kahler cone metrics. In\nthis article, we introduce weighted function spaces and provide a\nself-contained treatment on cone angles in the whole interval $(0,1]$. We first\nconstruct geodesics in the space of Kahler cone metrics (cone geodesics). We\nnext determine the very detailed asymptotic behaviour of constant scalar\ncurvature Kahler (cscK) cone metrics, which leads to the reductivity of the\nautomorphism group. Then we establish the linear theory for the Lichnerowicz\noperator, which immediately implies the openness of the path deforming the cone\nangles of cscK cone metrics. Finally, we address the problem on the uniqueness\nof cscK cone metrics and show that the cscK cone metric is unique up to\nautomorphisms. \n\n"}
{"id": "1710.01788", "contents": "Title: Multitask Learning using Task Clustering with Applications to Predictive\n  Modeling and GWAS of Plant Varieties Abstract: Inferring predictive maps between multiple input and multiple output\nvariables or tasks has innumerable applications in data science. Multi-task\nlearning attempts to learn the maps to several output tasks simultaneously with\ninformation sharing between them. We propose a novel multi-task learning\nframework for sparse linear regression, where a full task hierarchy is\nautomatically inferred from the data, with the assumption that the task\nparameters follow a hierarchical tree structure. The leaves of the tree are the\nparameters for individual tasks, and the root is the global model that\napproximates all the tasks. We apply the proposed approach to develop and\nevaluate: (a) predictive models of plant traits using large-scale and automated\nremote sensing data, and (b) GWAS methodologies mapping such derived phenotypes\nin lieu of hand-measured traits. We demonstrate the superior performance of our\napproach compared to other methods, as well as the usefulness of discovering\nhierarchical groupings between tasks. Our results suggest that richer genetic\nmapping can indeed be obtained from the remote sensing data. In addition, our\ndiscovered groupings reveal interesting insights from a plant science\nperspective. \n\n"}
{"id": "1710.01931", "contents": "Title: Forecasting Player Behavioral Data and Simulating in-Game Events Abstract: Understanding player behavior is fundamental in game data science. Video\ngames evolve as players interact with the game, so being able to foresee player\nexperience would help to ensure a successful game development. In particular,\ngame developers need to evaluate beforehand the impact of in-game events.\nSimulation optimization of these events is crucial to increase player\nengagement and maximize monetization. We present an experimental analysis of\nseveral methods to forecast game-related variables, with two main aims: to\nobtain accurate predictions of in-app purchases and playtime in an operational\nproduction environment, and to perform simulations of in-game events in order\nto maximize sales and playtime. Our ultimate purpose is to take a step towards\nthe data-driven development of games. The results suggest that, even though the\nperformance of traditional approaches such as ARIMA is still better, the\noutcomes of state-of-the-art techniques like deep learning are promising. Deep\nlearning comes up as a well-suited general model that could be used to forecast\na variety of time series with different dynamic behaviors. \n\n"}
{"id": "1710.02844", "contents": "Title: Reconstruction of Hidden Representation for Robust Feature Extraction Abstract: This paper aims to develop a new and robust approach to feature\nrepresentation. Motivated by the success of Auto-Encoders, we first theoretical\nsummarize the general properties of all algorithms that are based on\ntraditional Auto-Encoders: 1) The reconstruction error of the input can not be\nlower than a lower bound, which can be viewed as a guiding principle for\nreconstructing the input. Additionally, when the input is corrupted with\nnoises, the reconstruction error of the corrupted input also can not be lower\nthan a lower bound. 2) The reconstruction of a hidden representation achieving\nits ideal situation is the necessary condition for the reconstruction of the\ninput to reach the ideal state. 3) Minimizing the Frobenius norm of the\nJacobian matrix of the hidden representation has a deficiency and may result in\na much worse local optimum value. We believe that minimizing the reconstruction\nerror of the hidden representation is more robust than minimizing the Frobenius\nnorm of the Jacobian matrix of the hidden representation. Based on the above\nanalysis, we propose a new model termed Double Denoising Auto-Encoders (DDAEs),\nwhich uses corruption and reconstruction on both the input and the hidden\nrepresentation. We demonstrate that the proposed model is highly flexible and\nextensible and has a potentially better capability to learn invariant and\nrobust feature representations. We also show that our model is more robust than\nDenoising Auto-Encoders (DAEs) for dealing with noises or inessential features.\nFurthermore, we detail how to train DDAEs with two different pre-training\nmethods by optimizing the objective function in a combined and separate manner,\nrespectively. Comparative experiments illustrate that the proposed model is\nsignificantly better for representation learning than the state-of-the-art\nmodels. \n\n"}
{"id": "1710.03300", "contents": "Title: Holomorphic Jacobi Manifolds and Holomorphic Contact Groupoids Abstract: This is the second part of a series of two papers dedicated to a systematic\nstudy of holomorphic Jacobi structures. In the first part, we introduced and\nstudy the concept of a holomorphic Jacobi manifold in a very natural way as\nwell as various tools. In the present paper, we solve the integration problem\nfor holomorphic Jacobi manifolds by proving that they integrate to complex\ncontact groupoids. A crucial tool in our proof is what we call the\n\"homogenization scheme\", which allows us to identify holomorphic Jacobi\nmanifolds with homogeneous holomorphic Poisson manifolds and holomorphic\ncontact groupoids with homogeneous complex symplectic groupoids. \n\n"}
{"id": "1710.03667", "contents": "Title: High-dimensional dynamics of generalization error in neural networks Abstract: We perform an average case analysis of the generalization dynamics of large\nneural networks trained using gradient descent. We study the\npractically-relevant \"high-dimensional\" regime where the number of free\nparameters in the network is on the order of or even larger than the number of\nexamples in the dataset. Using random matrix theory and exact solutions in\nlinear models, we derive the generalization error and training error dynamics\nof learning and analyze how they depend on the dimensionality of data and\nsignal to noise ratio of the learning problem. We find that the dynamics of\ngradient descent learning naturally protect against overtraining and\noverfitting in large networks. Overtraining is worst at intermediate network\nsizes, when the effective number of free parameters equals the number of\nsamples, and thus can be reduced by making a network smaller or larger.\nAdditionally, in the high-dimensional regime, low generalization error requires\nstarting with small initial weights. We then turn to non-linear neural\nnetworks, and show that making networks very large does not harm their\ngeneralization performance. On the contrary, it can in fact reduce\novertraining, even without early stopping or regularization of any sort. We\nidentify two novel phenomena underlying this behavior in overcomplete models:\nfirst, there is a frozen subspace of the weights in which no learning occurs\nunder gradient descent; and second, the statistical properties of the\nhigh-dimensional regime yield better-conditioned input correlations which\nprotect against overtraining. We demonstrate that naive application of\nworst-case theories such as Rademacher complexity are inaccurate in predicting\nthe generalization performance of deep neural networks, and derive an\nalternative bound which incorporates the frozen subspace and conditioning\neffects and qualitatively matches the behavior observed in simulation. \n\n"}
{"id": "1710.03955", "contents": "Title: Hyperbolic components and cubic polynomials Abstract: In the space of cubic polynomials, Milnor defined a notable curve $\\mathcal\nS_p$, consisting of cubic polynomials with a periodic critical point, whose\nperiod is exactly $p$. In this paper, we show that for any integer $p\\geq 1$,\nany bounded hyperbolic component on $\\mathcal{S}_p$ is a Jordan disk. \n\n"}
{"id": "1710.05241", "contents": "Title: Robust Decentralized Learning Using ADMM with Unreliable Agents Abstract: Many machine learning problems can be formulated as consensus optimization\nproblems which can be solved efficiently via a cooperative multi-agent system.\nHowever, the agents in the system can be unreliable due to a variety of\nreasons: noise, faults and attacks. Providing erroneous updates leads the\noptimization process in a wrong direction, and degrades the performance of\ndistributed machine learning algorithms. This paper considers the problem of\ndecentralized learning using ADMM in the presence of unreliable agents. First,\nwe rigorously analyze the effect of erroneous updates (in ADMM learning\niterations) on the convergence behavior of multi-agent system. We show that the\nalgorithm linearly converges to a neighborhood of the optimal solution under\ncertain conditions and characterize the neighborhood size analytically. Next,\nwe provide guidelines for network design to achieve a faster convergence. We\nalso provide conditions on the erroneous updates for exact convergence to the\noptimal solution. Finally, to mitigate the influence of unreliable agents, we\npropose \\textsf{ROAD}, a robust variant of ADMM, and show its resilience to\nunreliable agents with an exact convergence to the optimum. \n\n"}
{"id": "1710.05758", "contents": "Title: TensorQuant - A Simulation Toolbox for Deep Neural Network Quantization Abstract: Recent research implies that training and inference of deep neural networks\n(DNN) can be computed with low precision numerical representations of the\ntraining/test data, weights and gradients without a general loss in accuracy.\nThe benefit of such compact representations is twofold: they allow a\nsignificant reduction of the communication bottleneck in distributed DNN\ntraining and faster neural network implementations on hardware accelerators\nlike FPGAs. Several quantization methods have been proposed to map the original\n32-bit floating point problem to low-bit representations. While most related\npublications validate the proposed approach on a single DNN topology, it\nappears to be evident, that the optimal choice of the quantization method and\nnumber of coding bits is topology dependent. To this end, there is no general\ntheory available, which would allow users to derive the optimal quantization\nduring the design of a DNN topology. In this paper, we present a quantization\ntool box for the TensorFlow framework. TensorQuant allows a transparent\nquantization simulation of existing DNN topologies during training and\ninference. TensorQuant supports generic quantization methods and allows\nexperimental evaluation of the impact of the quantization on single layers as\nwell as on the full topology. In a first series of experiments with\nTensorQuant, we show an analysis of fix-point quantizations of popular CNN\ntopologies. \n\n"}
{"id": "1710.05937", "contents": "Title: Analytical description of the survival probability of coherent states in\n  regular regimes Abstract: Using coherent states as initial states, we investigate the quantum dynamics\nof the Lipkin-Meshkov-Glick (LMG) and Dicke models in the semi-classical limit.\nThey are representative models of bounded systems with one- and two-degrees of\nfreedom, respectively. The first model is integrable, while the second one has\nboth regular and chaotic regimes. Our analysis is based on the survival\nprobability. Within the regular regime, the energy distribution of the initial\ncoherent states consists of quasi-harmonic sub-sequences of energies with\nGaussian weights. This allows for the derivation of analytical expressions that\naccurately describe the entire evolution of the survival probability, from\n$t=0$ to the saturation of the dynamics. The evolution shows decaying\noscillations with a rate that depends on the anharmonicity of the spectrum and,\nin the case of the Dicke model, on interference terms coming from the\nsimultaneous excitation of its two-degrees of freedom. As we move away from the\nregular regime, the complexity of the survival probability is shown to be\nclosely connected with the properties of the corresponding classical phase\nspace. Our approach has broad applicability, since its central assumptions are\nnot particular of the studied models. \n\n"}
{"id": "1710.07781", "contents": "Title: Functional data analysis in the Banach space of continuous functions Abstract: Functional data analysis is typically conducted within the $L^2$-Hilbert\nspace framework. There is by now a fully developed statistical toolbox allowing\nfor the principled application of the functional data machinery to real-world\nproblems, often based on dimension reduction techniques such as functional\nprincipal component analysis. At the same time, there have recently been a\nnumber of publications that sidestep dimension reduction steps and focus on a\nfully functional $L^2$-methodology. This paper goes one step further and\ndevelops data analysis methodology for functional time series in the space of\nall continuous functions. The work is motivated by the fact that objects with\nrather different shapes may still have a small $L^2$-distance and are therefore\nidentified as similar when using an $L^2$-metric. However, in applications it\nis often desirable to use metrics reflecting the visualization of the curves in\nthe statistical analysis. The methodological contributions are focused on\ndeveloping two-sample and change-point tests as well as confidence bands, as\nthese procedures appear do be conducive to the proposed setting. Particular\ninterest is put on relevant differences; that is, on not trying to test for\nexact equality, but rather for pre-specified deviations under the null\nhypothesis.\n  The procedures are justified through large-sample theory. To ensure\npracticability, non-standard bootstrap procedures are developed and\ninvestigated addressing particular features that arise in the problem of\ntesting relevant hypotheses. The finite sample properties are explored through\na simulation study and an application to annual temperature profiles. \n\n"}
{"id": "1710.07804", "contents": "Title: Zeroth-Order Online Alternating Direction Method of Multipliers:\n  Convergence Analysis and Applications Abstract: In this paper, we design and analyze a new zeroth-order online algorithm,\nnamely, the zeroth-order online alternating direction method of multipliers\n(ZOO-ADMM), which enjoys dual advantages of being gradient-free operation and\nemploying the ADMM to accommodate complex structured regularizers. Compared to\nthe first-order gradient-based online algorithm, we show that ZOO-ADMM requires\n$\\sqrt{m}$ times more iterations, leading to a convergence rate of\n$O(\\sqrt{m}/\\sqrt{T})$, where $m$ is the number of optimization variables, and\n$T$ is the number of iterations. To accelerate ZOO-ADMM, we propose two\nminibatch strategies: gradient sample averaging and observation averaging,\nresulting in an improved convergence rate of $O(\\sqrt{1+q^{-1}m}/\\sqrt{T})$,\nwhere $q$ is the minibatch size. In addition to convergence analysis, we also\ndemonstrate ZOO-ADMM to applications in signal processing, statistics, and\nmachine learning. \n\n"}
{"id": "1710.08637", "contents": "Title: Improving Accuracy of Nonparametric Transfer Learning via Vector\n  Segmentation Abstract: Transfer learning using deep neural networks as feature extractors has become\nincreasingly popular over the past few years. It allows to obtain\nstate-of-the-art accuracy on datasets too small to train a deep neural network\non its own, and it provides cutting edge descriptors that, combined with\nnonparametric learning methods, allow rapid and flexible deployment of\nperforming solutions in computationally restricted settings. In this paper, we\nare interested in showing that the features extracted using deep neural\nnetworks have specific properties which can be used to improve accuracy of\ndownstream nonparametric learning methods. Namely, we demonstrate that for some\ndistributions where information is embedded in a few coordinates, segmenting\nfeature vectors can lead to better accuracy. We show how this model can be\napplied to real datasets by performing experiments using three mainstream deep\nneural network feature extractors and four databases, in vision and audio. \n\n"}
{"id": "1710.09180", "contents": "Title: Anatomical labeling of brain CT scan anomalies using multi-context\n  nearest neighbor relation networks Abstract: This work is an endeavor to develop a deep learning methodology for automated\nanatomical labeling of a given region of interest (ROI) in brain computed\ntomography (CT) scans. We combine both local and global context to obtain a\nrepresentation of the ROI. We then use Relation Networks (RNs) to predict the\ncorresponding anatomy of the ROI based on its relationship score for each\nclass. Further, we propose a novel strategy employing nearest neighbors\napproach for training RNs. We train RNs to learn the relationship of the target\nROI with the joint representation of its nearest neighbors in each class\ninstead of all data-points in each class. The proposed strategy leads to better\ntraining of RNs along with increased performance as compared to training\nbaseline RN network. \n\n"}
{"id": "1710.09404", "contents": "Title: Steady states and edge state transport in topological Floquet-Bloch\n  systems Abstract: We study the open system dynamics and steady states of two dimensional\nFloquet topological insulators: systems in which a topological Floquet-Bloch\nspectrum is induced by an external periodic drive. We solve for the bulk and\nedge state carrier distributions, taking into account energy and momentum\nrelaxation through radiative recombination and electron-phonon interactions, as\nwell as coupling to an external lead. We show that the resulting steady state\nresembles a topological insulator in the Floquet basis. The particle\ndistribution in the Floquet edge modes exhibits a sharp feature akin to the\nFermi level in equilibrium systems, while the bulk hosts a small density of\nexcitations. We discuss two-terminal transport and describe the regimes where\nedge-state transport can be observed. Our results show that signatures of the\nnon-trivial topology persist in the non-equilibrium steady state. \n\n"}
{"id": "1710.10016", "contents": "Title: Regularization via Mass Transportation Abstract: The goal of regression and classification methods in supervised learning is\nto minimize the empirical risk, that is, the expectation of some loss function\nquantifying the prediction error under the empirical distribution. When facing\nscarce training data, overfitting is typically mitigated by adding\nregularization terms to the objective that penalize hypothesis complexity. In\nthis paper we introduce new regularization techniques using ideas from\ndistributionally robust optimization, and we give new probabilistic\ninterpretations to existing techniques. Specifically, we propose to minimize\nthe worst-case expected loss, where the worst case is taken over the ball of\nall (continuous or discrete) distributions that have a bounded transportation\ndistance from the (discrete) empirical distribution. By choosing the radius of\nthis ball judiciously, we can guarantee that the worst-case expected loss\nprovides an upper confidence bound on the loss on test data, thus offering new\ngeneralization bounds. We prove that the resulting regularized learning\nproblems are tractable and can be tractably kernelized for many popular loss\nfunctions. We validate our theoretical out-of-sample guarantees through\nsimulated and empirical experiments. \n\n"}
{"id": "1710.10403", "contents": "Title: Trainable back-propagated functional transfer matrices Abstract: Connections between nodes of fully connected neural networks are usually\nrepresented by weight matrices. In this article, functional transfer matrices\nare introduced as alternatives to the weight matrices: Instead of using real\nweights, a functional transfer matrix uses real functions with trainable\nparameters to represent connections between nodes. Multiple functional transfer\nmatrices are then stacked together with bias vectors and activations to form\ndeep functional transfer neural networks. These neural networks can be trained\nwithin the framework of back-propagation, based on a revision of the delta\nrules and the error transmission rule for functional connections. In\nexperiments, it is demonstrated that the revised rules can be used to train a\nrange of functional connections: 20 different functions are applied to neural\nnetworks with up to 10 hidden layers, and most of them gain high test\naccuracies on the MNIST database. It is also demonstrated that a functional\ntransfer matrix with a memory function can roughly memorise a non-cyclical\nsequence of 400 digits. \n\n"}
{"id": "1710.10518", "contents": "Title: Quantum state engineering using one-dimensional discrete-time quantum\n  walks Abstract: Quantum state preparation in high-dimensional systems is an essential\nrequirement for many quantum-technology applications. The engineering of an\narbitrary quantum state is, however, typically strongly dependent on the\nexperimental platform chosen for implementation, and a general framework is\nstill missing. Here we show that coined quantum walks on a line, which\nrepresent a framework general enough to encompass a variety of different\nplatforms, can be used for quantum state engineering of arbitrary\nsuperpositions of the walker's sites. We achieve this goal by identifying a set\nof conditions that fully characterize the reachable states in the space\ncomprising walker and coin, and providing a method to efficiently compute the\ncorresponding set of coin parameters. We assess the feasibility of our proposal\nby identifying a linear optics experiment based on photonic orbital angular\nmomentum technology. \n\n"}
{"id": "1710.11052", "contents": "Title: A Connection between Feed-Forward Neural Networks and Probabilistic\n  Graphical Models Abstract: Two of the most popular modelling paradigms in computer vision are\nfeed-forward neural networks (FFNs) and probabilistic graphical models (GMs).\nVarious connections between the two have been studied in recent works, such as\ne.g. expressing mean-field based inference in a GM as an FFN. This paper\nestablishes a new connection between FFNs and GMs. Our key observation is that\nany FFN implements a certain approximation of a corresponding Bayesian network\n(BN). We characterize various benefits of having this connection. In\nparticular, it results in a new learning algorithm for BNs. We validate the\nproposed methods for a classification problem on CIFAR-10 dataset and for\nbinary image segmentation on Weizmann Horse dataset. We show that statistically\nlearned BNs improve performance, having at the same time essentially better\ngeneralization capability, than their FFN counterparts. \n\n"}
{"id": "1710.11239", "contents": "Title: Time-lagged autoencoders: Deep learning of slow collective variables for\n  molecular kinetics Abstract: Inspired by the success of deep learning techniques in the physical and\nchemical sciences, we apply a modification of an autoencoder type deep neural\nnetwork to the task of dimension reduction of molecular dynamics data. We can\nshow that our time-lagged autoencoder reliably finds low-dimensional embeddings\nfor high-dimensional feature spaces which capture the slow dynamics of the\nunderlying stochastic processes - beyond the capabilities of linear dimension\nreduction techniques. \n\n"}
{"id": "1711.00950", "contents": "Title: Beyond normality: Learning sparse probabilistic graphical models in the\n  non-Gaussian setting Abstract: We present an algorithm to identify sparse dependence structure in continuous\nand non-Gaussian probability distributions, given a corresponding set of data.\nThe conditional independence structure of an arbitrary distribution can be\nrepresented as an undirected graph (or Markov random field), but most\nalgorithms for learning this structure are restricted to the discrete or\nGaussian cases. Our new approach allows for more realistic and accurate\ndescriptions of the distribution in question, and in turn better estimates of\nits sparse Markov structure. Sparsity in the graph is of interest as it can\naccelerate inference, improve sampling methods, and reveal important\ndependencies between variables. The algorithm relies on exploiting the\nconnection between the sparsity of the graph and the sparsity of transport\nmaps, which deterministically couple one probability measure to another. \n\n"}
{"id": "1711.00991", "contents": "Title: The neighborhood lattice for encoding partial correlations in a Hilbert\n  space Abstract: Neighborhood regression has been a successful approach in graphical and\nstructural equation modeling, with applications to learning undirected and\ndirected graphical models. We extend these ideas by defining and studying an\nalgebraic structure called the neighborhood lattice based on a generalized\nnotion of neighborhood regression. We show that this algebraic structure has\nthe potential to provide an economic encoding of all conditional independence\nstatements in a Gaussian distribution (or conditional uncorrelatedness in\ngeneral), even in the cases where no graphical model exists that could\n\"perfectly\" encode all such statements. We study the computational complexity\nof computing these structures and show that under a sparsity assumption, they\ncan be computed in polynomial time, even in the absence of the assumption of\nperfectness to a graph. On the other hand, assuming perfectness, we show how\nthese neighborhood lattices may be \"graphically\" computed using the separation\nproperties of the so-called partial correlation graph. We also draw connections\nwith directed acyclic graphical models and Bayesian networks. We derive these\nresults using an abstract generalization of partial uncorrelatedness, called\npartial orthogonality, which allows us to use algebraic properties of\nprojection operators on Hilbert spaces to significantly simplify and extend\nexisting ideas and arguments. Consequently, our results apply to a wide range\nof random objects and data structures, such as random vectors, data matrices,\nand functions. \n\n"}
{"id": "1711.01204", "contents": "Title: Metrics for Deep Generative Models Abstract: Neural samplers such as variational autoencoders (VAEs) or generative\nadversarial networks (GANs) approximate distributions by transforming samples\nfrom a simple random source---the latent space---to samples from a more complex\ndistribution represented by a dataset. While the manifold hypothesis implies\nthat the density induced by a dataset contains large regions of low density,\nthe training criterions of VAEs and GANs will make the latent space densely\ncovered. Consequently points that are separated by low-density regions in\nobservation space will be pushed together in latent space, making stationary\ndistances poor proxies for similarity. We transfer ideas from Riemannian\ngeometry to this setting, letting the distance between two points be the\nshortest path on a Riemannian manifold induced by the transformation. The\nmethod yields a principled distance measure, provides a tool for visual\ninspection of deep generative models, and an alternative to linear\ninterpolation in latent space. In addition, it can be applied for robot\nmovement generalization using previously learned skills. The method is\nevaluated on a synthetic dataset with known ground truth; on a simulated robot\narm dataset; on human motion capture data; and on a generative model of\nhandwritten digits. \n\n"}
{"id": "1711.01341", "contents": "Title: Generalized Linear Model Regression under Distance-to-set Penalties Abstract: Estimation in generalized linear models (GLM) is complicated by the presence\nof constraints. One can handle constraints by maximizing a penalized\nlog-likelihood. Penalties such as the lasso are effective in high dimensions,\nbut often lead to unwanted shrinkage. This paper explores instead penalizing\nthe squared distance to constraint sets. Distance penalties are more flexible\nthan algebraic and regularization penalties, and avoid the drawback of\nshrinkage. To optimize distance penalized objectives, we make use of the\nmajorization-minimization principle. Resulting algorithms constructed within\nthis framework are amenable to acceleration and come with global convergence\nguarantees. Applications to shape constraints, sparse regression, and\nrank-restricted matrix regression on synthetic and real data showcase strong\nempirical performance, even under non-convex constraints. \n\n"}
{"id": "1711.01655", "contents": "Title: Approximating Partition Functions in Constant Time Abstract: We study approximations of the partition function of dense graphical models.\nPartition functions of graphical models play a fundamental role is statistical\nphysics, in statistics and in machine learning. Two of the main methods for\napproximating the partition function are Markov Chain Monte Carlo and\nVariational Methods. An impressive body of work in mathematics, physics and\ntheoretical computer science provides conditions under which Markov Chain Monte\nCarlo methods converge in polynomial time. These methods often lead to\npolynomial time approximation algorithms for the partition function in cases\nwhere the underlying model exhibits correlation decay. There are very few\ntheoretical guarantees for the performance of variational methods. One\nexception is recent results by Risteski (2016) who considered dense graphical\nmodels and showed that using variational methods, it is possible to find an\n$O(\\epsilon n)$ additive approximation to the log partition function in time\n$n^{O(1/\\epsilon^2)}$ even in a regime where correlation decay does not hold.\n  We show that under essentially the same conditions, an $O(\\epsilon n)$\nadditive approximation of the log partition function can be found in constant\ntime, independent of $n$. In particular, our results cover dense Ising and\nPotts models as well as dense graphical models with $k$-wise interaction. They\nalso apply for low threshold rank models. \n\n"}
{"id": "1711.02233", "contents": "Title: Rudin-Shapiro-Like Sequences with Maximum Asymptotic Merit Factor Abstract: Borwein and Mossinghoff investigated the Rudin-Shapiro-like sequences, which\nare infinite families of binary sequences, usually represented as polynomials.\nEach family of Rudin-Shapiro-like sequences is obtained from a starting\nsequence (which we call the seed) by a recursive construction that doubles the\nlength of the sequence at each step, and many sequences produced in this manner\nhave exceptionally low aperiodic autocorrelation. Borwein and Mossinghoff\nshowed that the asymptotic autocorrelation merit factor for any such family is\nat most $3$, and found the seeds of length $40$ or less that produce the\nmaximum asymptotic merit factor of $3$. The definition of Rudin-Shapiro-like\nsequences was generalized by Katz, Lee, and Trunov to include sequences with\narbitrary complex coefficients, among which are families of low autocorrelation\npolyphase sequences. Katz, Lee, and Trunov proved that the maximum asymptotic\nmerit factor is also $3$ for this larger class. Here we show that a family of\nsuch Rudin-Shapiro-like sequences achieves asymptotic merit factor $3$ if and\nonly if the seed is either of length $1$ or is the interleaving of a pair of\nGolay complementary sequences. For small seed lengths where this is not\npossible, the optimal seeds are interleavings of pairs that are as close as\npossible to being complementary pairs, and the idea of an almost-complementary\npair makes sense of remarkable patterns in previously unexplained data on\noptimal seeds for binary Rudin-Shapiro-like sequences. \n\n"}
{"id": "1711.03946", "contents": "Title: Bayesian Paragraph Vectors Abstract: Word2vec (Mikolov et al., 2013) has proven to be successful in natural\nlanguage processing by capturing the semantic relationships between different\nwords. Built on top of single-word embeddings, paragraph vectors (Le and\nMikolov, 2014) find fixed-length representations for pieces of text with\narbitrary lengths, such as documents, paragraphs, and sentences. In this work,\nwe propose a novel interpretation for neural-network-based paragraph vectors by\ndeveloping an unsupervised generative model whose maximum likelihood solution\ncorresponds to traditional paragraph vectors. This probabilistic formulation\nallows us to go beyond point estimates of parameters and to perform Bayesian\nposterior inference. We find that the entropy of paragraph vectors decreases\nwith the length of documents, and that information about posterior uncertainty\nimproves performance in supervised learning tasks such as sentiment analysis\nand paraphrase detection. \n\n"}
{"id": "1711.04340", "contents": "Title: Data Augmentation Generative Adversarial Networks Abstract: Effective training of neural networks requires much data. In the low-data\nregime, parameters are underdetermined, and learnt networks generalise poorly.\nData Augmentation alleviates this by using existing data more effectively.\nHowever standard data augmentation produces only limited plausible alternative\ndata. Given there is potential to generate a much broader set of augmentations,\nwe design and train a generative model to do data augmentation. The model,\nbased on image conditional Generative Adversarial Networks, takes data from a\nsource domain and learns to take any data item and generalise it to generate\nother within-class data items. As this generative process does not depend on\nthe classes themselves, it can be applied to novel unseen classes of data. We\nshow that a Data Augmentation Generative Adversarial Network (DAGAN) augments\nstandard vanilla classifiers well. We also show a DAGAN can enhance few-shot\nlearning systems such as Matching Networks. We demonstrate these approaches on\nOmniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In\nour experiments we can see over 13% increase in accuracy in the low-data regime\nexperiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face\n(4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5%\n(from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%). \n\n"}
{"id": "1711.04511", "contents": "Title: Toeplitz kernels and model spaces Abstract: We review some classical and more recent results concerning kernels of\nToeplitz operators and their relations with model spaces, which are themselves\nToeplitz kernels of a special kind. We highlight the fundamental role played by\nthe existence of maximal vectors for every nontrivial Toeplitz kernel. \n\n"}
{"id": "1711.04817", "contents": "Title: Sparse quadratic classification rules via linear dimension reduction Abstract: We consider the problem of high-dimensional classification between the two\ngroups with unequal covariance matrices. Rather than estimating the full\nquadratic discriminant rule, we propose to perform simultaneous variable\nselection and linear dimension reduction on original data, with the subsequent\napplication of quadratic discriminant analysis on the reduced space. In\ncontrast to quadratic discriminant analysis, the proposed framework doesn't\nrequire estimation of precision matrices and scales linearly with the number of\nmeasurements, making it especially attractive for the use on high-dimensional\ndatasets. We support the methodology with theoretical guarantees on variable\nselection consistency, and empirical comparison with competing approaches. We\napply the method to gene expression data of breast cancer patients, and confirm\nthe crucial importance of ESR1 gene in differentiating estrogen receptor\nstatus. \n\n"}
{"id": "1711.04965", "contents": "Title: Near-optimal sample complexity for convex tensor completion Abstract: We analyze low rank tensor completion (TC) using noisy measurements of a\nsubset of the tensor. Assuming a rank-$r$, order-$d$, $N \\times N \\times \\cdots\n\\times N$ tensor where $r=O(1)$, the best sampling complexity that was achieved\nis $O(N^{\\frac{d}{2}})$, which is obtained by solving a tensor nuclear-norm\nminimization problem. However, this bound is significantly larger than the\nnumber of free variables in a low rank tensor which is $O(dN)$. In this paper,\nwe show that by using an atomic-norm whose atoms are rank-$1$ sign tensors, one\ncan obtain a sample complexity of $O(dN)$. Moreover, we generalize the matrix\nmax-norm definition to tensors, which results in a max-quasi-norm (max-qnorm)\nwhose unit ball has small Rademacher complexity. We prove that solving a\nconstrained least squares estimation using either the convex atomic-norm or the\nnonconvex max-qnorm results in optimal sample complexity for the problem of\nlow-rank tensor completion. Furthermore, we show that these bounds are nearly\nminimax rate-optimal. We also provide promising numerical results for max-qnorm\nconstrained tensor completion, showing improved recovery results compared to\nmatricization and alternating least squares. \n\n"}
{"id": "1711.05401", "contents": "Title: Revisiting Simple Neural Networks for Learning Representations of\n  Knowledge Graphs Abstract: We address the problem of learning vector representations for entities and\nrelations in Knowledge Graphs (KGs) for Knowledge Base Completion (KBC). This\nproblem has received significant attention in the past few years and multiple\nmethods have been proposed. Most of the existing methods in the literature use\na predefined characteristic scoring function for evaluating the correctness of\nKG triples. These scoring functions distinguish correct triples (high score)\nfrom incorrect ones (low score). However, their performance vary across\ndifferent datasets. In this work, we demonstrate that a simple neural network\nbased score function can consistently achieve near start-of-the-art performance\non multiple datasets. We also quantitatively demonstrate biases in standard\nbenchmark datasets, and highlight the need to perform evaluation spanning\nvarious datasets. \n\n"}
{"id": "1711.05657", "contents": "Title: Approximation of $m-$subharmonic functions on bounded domains in\n  $\\mathbb C^n$ Abstract: Let $D$ be a bounded domain in $\\mathbb C^n$. We study approximation of (not\nnecessarily bounded from above) $m-$subharmonic function $D$ by continuous\n$m-$subharmonic ones defined on neighborhoods of $\\overline{D}$. We also\nconsider the existence of a $m-$subharmonic function on $D$ whose boundary\nvalues coincides with a given real valued continuous function on $\\partial D$\nexcept for a sufficiently small subset of $\\partial D.$ \n\n"}
{"id": "1711.06350", "contents": "Title: Towards Deep Learning Models for Psychological State Prediction using\n  Smartphone Data: Challenges and Opportunities Abstract: There is an increasing interest in exploiting mobile sensing technologies and\nmachine learning techniques for mental health monitoring and intervention.\nResearchers have effectively used contextual information, such as mobility,\ncommunication and mobile phone usage patterns for quantifying individuals' mood\nand wellbeing. In this paper, we investigate the effectiveness of neural\nnetwork models for predicting users' level of stress by using the location\ninformation collected by smartphones. We characterize the mobility patterns of\nindividuals using the GPS metrics presented in the literature and employ these\nmetrics as input to the network. We evaluate our approach on the open-source\nStudentLife dataset. Moreover, we discuss the challenges and trade-offs\ninvolved in building machine learning models for digital mental health and\nhighlight potential future work in this direction. \n\n"}
{"id": "1711.06709", "contents": "Title: Homotopy groups of generic leaves of logarithmic foliations Abstract: We study the homotopy groups of generic leaves of logarithmic foliations on\ncomplex projective manifolds. We exhibit a relation between the homotopy groups\nof a generic leaf and of the complement of the polar divisor of the logarithmic\nfoliation. \n\n"}
{"id": "1711.07479", "contents": "Title: Teaching a Machine to Read Maps with Deep Reinforcement Learning Abstract: The ability to use a 2D map to navigate a complex 3D environment is quite\nremarkable, and even difficult for many humans. Localization and navigation is\nalso an important problem in domains such as robotics, and has recently become\na focus of the deep reinforcement learning community. In this paper we teach a\nreinforcement learning agent to read a map in order to find the shortest way\nout of a random maze it has never seen before. Our system combines several\nstate-of-the-art methods such as A3C and incorporates novel elements such as a\nrecurrent localization cell. Our agent learns to localize itself based on 3D\nfirst person images and an approximate orientation angle. The agent generalizes\nwell to bigger mazes, showing that it learned useful localization and\nnavigation capabilities. \n\n"}
{"id": "1711.07839", "contents": "Title: Application of generative autoencoder in de novo molecular design Abstract: A major challenge in computational chemistry is the generation of novel\nmolecular structures with desirable pharmacological and physiochemical\nproperties. In this work, we investigate the potential use of autoencoder, a\ndeep learning methodology, for de novo molecular design. Various generative\nautoencoders were used to map molecule structures into a continuous latent\nspace and vice versa and their performance as structure generator was assessed.\nOur results show that the latent space preserves chemical similarity principle\nand thus can be used for the generation of analogue structures. Furthermore,\nthe latent space created by autoencoders were searched systematically to\ngenerate novel compounds with predicted activity against dopamine receptor type\n2 and compounds similar to known active compounds not included in the training\nset were identified. \n\n"}
{"id": "1711.08310", "contents": "Title: The Local Structure of Generalized Contact Bundles Abstract: Generalized contact bundles are odd dimensional analogues of generalized\ncomplex manifolds. They have been introduced recently and very little is known\nabout them. In this paper we study their local structure. Specifically, we\nprove a local splitting theorem similar to those appearing in Poisson geometry.\nIn particular, in a neighborhood of a regular point, a generalized contact\nbundle is either the product of a contact and a complex manifold or the product\nof a symplectic manifold and a manifold equipped with an integrable complex\nstructure on the gauge algebroid of the trivial line bundle. \n\n"}
{"id": "1711.08513", "contents": "Title: Calibration for the (Computationally-Identifiable) Masses Abstract: As algorithms increasingly inform and influence decisions made about\nindividuals, it becomes increasingly important to address concerns that these\nalgorithms might be discriminatory. The output of an algorithm can be\ndiscriminatory for many reasons, most notably: (1) the data used to train the\nalgorithm might be biased (in various ways) to favor certain populations over\nothers; (2) the analysis of this training data might inadvertently or\nmaliciously introduce biases that are not borne out in the data. This work\nfocuses on the latter concern.\n  We develop and study multicalbration -- a new measure of algorithmic fairness\nthat aims to mitigate concerns about discrimination that is introduced in the\nprocess of learning a predictor from data. Multicalibration guarantees accurate\n(calibrated) predictions for every subpopulation that can be identified within\na specified class of computations. We think of the class as being quite rich;\nin particular, it can contain many overlapping subgroups of a protected group.\n  We show that in many settings this strong notion of protection from\ndiscrimination is both attainable and aligned with the goal of obtaining\naccurate predictions. Along the way, we present new algorithms for learning a\nmulticalibrated predictor, study the computational complexity of this task, and\ndraw new connections to computational learning models such as agnostic\nlearning. \n\n"}
{"id": "1711.08588", "contents": "Title: SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance\n  Segmentation Abstract: We introduce Similarity Group Proposal Network (SGPN), a simple and intuitive\ndeep learning framework for 3D object instance segmentation on point clouds.\nSGPN uses a single network to predict point grouping proposals and a\ncorresponding semantic class for each proposal, from which we can directly\nextract instance segmentation results. Important to the effectiveness of SGPN\nis its novel representation of 3D instance segmentation results in the form of\na similarity matrix that indicates the similarity between each pair of points\nin embedded feature space, thus producing an accurate grouping proposal for\neach point. To the best of our knowledge, SGPN is the first framework to learn\n3D instance-aware semantic segmentation on point clouds. Experimental results\non various 3D scenes show the effectiveness of our method on 3D instance\nsegmentation, and we also evaluate the capability of SGPN to improve 3D object\ndetection and semantic segmentation results. We also demonstrate its\nflexibility by seamlessly incorporating 2D CNN features into the framework to\nboost performance. \n\n"}
{"id": "1711.09347", "contents": "Title: HashGAN:Attention-aware Deep Adversarial Hashing for Cross Modal\n  Retrieval Abstract: As the rapid growth of multi-modal data, hashing methods for cross-modal\nretrieval have received considerable attention. Deep-networks-based cross-modal\nhashing methods are appealing as they can integrate feature learning and hash\ncoding into end-to-end trainable frameworks. However, it is still challenging\nto find content similarities between different modalities of data due to the\nheterogeneity gap. To further address this problem, we propose an adversarial\nhashing network with attention mechanism to enhance the measurement of content\nsimilarities by selectively focusing on informative parts of multi-modal data.\nThe proposed new adversarial network, HashGAN, consists of three building\nblocks: 1) the feature learning module to obtain feature representations, 2)\nthe generative attention module to generate an attention mask, which is used to\nobtain the attended (foreground) and the unattended (background) feature\nrepresentations, 3) the discriminative hash coding module to learn hash\nfunctions that preserve the similarities between different modalities. In our\nframework, the generative module and the discriminative module are trained in\nan adversarial way: the generator is learned to make the discriminator cannot\npreserve the similarities of multi-modal data w.r.t. the background feature\nrepresentations, while the discriminator aims to preserve the similarities of\nmulti-modal data w.r.t. both the foreground and the background feature\nrepresentations. Extensive evaluations on several benchmark datasets\ndemonstrate that the proposed HashGAN brings substantial improvements over\nother state-of-the-art cross-modal hashing methods. \n\n"}
{"id": "1711.10938", "contents": "Title: Extreme Dimension Reduction for Handling Covariate Shift Abstract: In the covariate shift learning scenario, the training and test covariate\ndistributions differ, so that a predictor's average loss over the training and\ntest distributions also differ. In this work, we explore the potential of\nextreme dimension reduction, i.e. to very low dimensions, in improving the\nperformance of importance weighting methods for handling covariate shift, which\nfail in high dimensions due to potentially high train/test covariate divergence\nand the inability to accurately estimate the requisite density ratios. We first\nformulate and solve a problem optimizing over linear subspaces a combination of\ntheir predictive utility and train/test divergence within. Applying it to\nsimulated and real data, we show extreme dimension reduction helps sometimes\nbut not always, due to a bias introduced by dimension reduction. \n\n"}
{"id": "1711.11279", "contents": "Title: Interpretability Beyond Feature Attribution: Quantitative Testing with\n  Concept Activation Vectors (TCAV) Abstract: The interpretation of deep learning models is a challenge due to their size,\ncomplexity, and often opaque internal state. In addition, many systems, such as\nimage classifiers, operate on low-level features rather than high-level\nconcepts. To address these challenges, we introduce Concept Activation Vectors\n(CAVs), which provide an interpretation of a neural net's internal state in\nterms of human-friendly concepts. The key idea is to view the high-dimensional\ninternal state of a neural net as an aid, not an obstacle. We show how to use\nCAVs as part of a technique, Testing with CAVs (TCAV), that uses directional\nderivatives to quantify the degree to which a user-defined concept is important\nto a classification result--for example, how sensitive a prediction of \"zebra\"\nis to the presence of stripes. Using the domain of image classification as a\ntesting ground, we describe how CAVs may be used to explore hypotheses and\ngenerate insights for a standard image classification network as well as a\nmedical application. \n\n"}
{"id": "1711.11423", "contents": "Title: On reducing the communication cost of the diffusion LMS algorithm Abstract: The rise of digital and mobile communications has recently made the world\nmore connected and networked, resulting in an unprecedented volume of data\nflowing between sources, data centers, or processes. While these data may be\nprocessed in a centralized manner, it is often more suitable to consider\ndistributed strategies such as diffusion as they are scalable and can handle\nlarge amounts of data by distributing tasks over networked agents. Although it\nis relatively simple to implement diffusion strategies over a cluster, it\nappears to be challenging to deploy them in an ad-hoc network with limited\nenergy budget for communication. In this paper, we introduce a diffusion LMS\nstrategy that significantly reduces communication costs without compromising\nthe performance. Then, we analyze the proposed algorithm in the mean and\nmean-square sense. Next, we conduct numerical experiments to confirm the\ntheoretical findings. Finally, we perform large scale simulations to test the\nalgorithm efficiency in a scenario where energy is limited. \n\n"}
{"id": "1712.00311", "contents": "Title: Folded Recurrent Neural Networks for Future Video Prediction Abstract: Future video prediction is an ill-posed Computer Vision problem that recently\nreceived much attention. Its main challenges are the high variability in video\ncontent, the propagation of errors through time, and the non-specificity of the\nfuture frames: given a sequence of past frames there is a continuous\ndistribution of possible futures. This work introduces bijective Gated\nRecurrent Units, a double mapping between the input and output of a GRU layer.\nThis allows for recurrent auto-encoders with state sharing between encoder and\ndecoder, stratifying the sequence representation and helping to prevent\ncapacity problems. We show how with this topology only the encoder or decoder\nneeds to be applied for input encoding and prediction, respectively. This\nreduces the computational cost and avoids re-encoding the predictions when\ngenerating a sequence of frames, mitigating the propagation of errors.\nFurthermore, it is possible to remove layers from an already trained model,\ngiving an insight to the role performed by each layer and making the model more\nexplainable. We evaluate our approach on three video datasets, outperforming\nstate of the art prediction results on MMNIST and UCF101, and obtaining\ncompetitive results on KTH with 2 and 3 times less memory usage and\ncomputational cost than the best scored approach. \n\n"}
{"id": "1712.02009", "contents": "Title: On the nonparametric maximum likelihood estimator for Gaussian location\n  mixture densities with application to Gaussian denoising Abstract: We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for\nestimating Gaussian location mixture densities in $d$-dimensions from\nindependent observations. Unlike usual likelihood-based methods for fitting\nmixtures, NPMLEs are based on convex optimization. We prove finite sample\nresults on the Hellinger accuracy of every NPMLE. Our results imply, in\nparticular, that every NPMLE achieves near parametric risk (up to logarithmic\nmultiplicative factors) when the true density is a discrete Gaussian mixture\nwithout any prior information on the number of mixture components. NPMLEs can\nnaturally be used to yield empirical Bayes estimates of the Oracle Bayes\nestimator in the Gaussian denoising problem. We prove bounds for the accuracy\nof the empirical Bayes estimate as an approximation to the Oracle Bayes\nestimator. Here our results imply that the empirical Bayes estimator performs\nat nearly the optimal level (up to logarithmic multiplicative factors) for\ndenoising in clustering situations without any prior knowledge of the number of\nclusters. \n\n"}
{"id": "1712.02629", "contents": "Title: Differentially Private Variational Dropout Abstract: Deep neural networks with their large number of parameters are highly\nflexible learning systems. The high flexibility in such networks brings with\nsome serious problems such as overfitting, and regularization is used to\naddress this problem. A currently popular and effective regularization\ntechnique for controlling the overfitting is dropout. Often, large data\ncollections required for neural networks contain sensitive information such as\nthe medical histories of patients, and the privacy of the training data should\nbe protected. In this paper, we modify the recently proposed variational\ndropout technique which provided an elegant Bayesian interpretation to dropout,\nand show that the intrinsic noise in the variational dropout can be exploited\nto obtain a degree of differential privacy. The iterative nature of training\nneural networks presents a challenge for privacy-preserving estimation since\nmultiple iterations increase the amount of noise added. We overcome this by\nusing a relaxed notion of differential privacy, called concentrated\ndifferential privacy, which provides tighter estimates on the overall privacy\nloss. We demonstrate the accuracy of our privacy-preserving variational dropout\nalgorithm on benchmark datasets. \n\n"}
{"id": "1712.02902", "contents": "Title: Multiple Adaptive Bayesian Linear Regression for Scalable Bayesian\n  Optimization with Warm Start Abstract: Bayesian optimization (BO) is a model-based approach for gradient-free\nblack-box function optimization. Typically, BO is powered by a Gaussian process\n(GP), whose algorithmic complexity is cubic in the number of evaluations.\nHence, GP-based BO cannot leverage large amounts of past or related function\nevaluations, for example, to warm start the BO procedure. We develop a multiple\nadaptive Bayesian linear regression model as a scalable alternative whose\ncomplexity is linear in the number of observations. The multiple Bayesian\nlinear regression models are coupled through a shared feedforward neural\nnetwork, which learns a joint representation and transfers knowledge across\nmachine learning problems. \n\n"}
{"id": "1712.03337", "contents": "Title: Bayesian Joint Matrix Decomposition for Data Integration with\n  Heterogeneous Noise Abstract: Matrix decomposition is a popular and fundamental approach in machine\nlearning and data mining. It has been successfully applied into various fields.\nMost matrix decomposition methods focus on decomposing a data matrix from one\nsingle source. However, it is common that data are from different sources with\nheterogeneous noise. A few of matrix decomposition methods have been extended\nfor such multi-view data integration and pattern discovery. While only few\nmethods were designed to consider the heterogeneity of noise in such multi-view\ndata for data integration explicitly. To this end, we propose a joint matrix\ndecomposition framework (BJMD), which models the heterogeneity of noise by\nGaussian distribution in a Bayesian framework. We develop two algorithms to\nsolve this model: one is a variational Bayesian inference algorithm, which\nmakes full use of the posterior distribution; and another is a maximum a\nposterior algorithm, which is more scalable and can be easily paralleled.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nBJMD considering the heterogeneity of noise is superior or competitive to the\nstate-of-the-art methods. \n\n"}
{"id": "1712.04332", "contents": "Title: Scaling Limit: Exact and Tractable Analysis of Online Learning\n  Algorithms with Applications to Regularized Regression and PCA Abstract: We present a framework for analyzing the exact dynamics of a class of online\nlearning algorithms in the high-dimensional scaling limit. Our results are\napplied to two concrete examples: online regularized linear regression and\nprincipal component analysis. As the ambient dimension tends to infinity, and\nwith proper time scaling, we show that the time-varying joint empirical\nmeasures of the target feature vector and its estimates provided by the\nalgorithms will converge weakly to a deterministic measured-valued process that\ncan be characterized as the unique solution of a nonlinear PDE. Numerical\nsolutions of this PDE can be efficiently obtained. These solutions lead to\nprecise predictions of the performance of the algorithms, as many practical\nperformance metrics are linear functionals of the joint empirical measures. In\naddition to characterizing the dynamic performance of online learning\nalgorithms, our asymptotic analysis also provides useful insights. In\nparticular, in the high-dimensional limit, and due to exchangeability, the\noriginal coupled dynamics associated with the algorithms will be asymptotically\n\"decoupled\", with each coordinate independently solving a 1-D effective\nminimization problem via stochastic gradient descent. Exploiting this insight\nfor nonconvex optimization problems may prove an interesting line of future\nresearch. \n\n"}
{"id": "1712.05877", "contents": "Title: Quantization and Training of Neural Networks for Efficient\n  Integer-Arithmetic-Only Inference Abstract: The rising popularity of intelligent mobile devices and the daunting\ncomputational cost of deep learning-based models call for efficient and\naccurate on-device inference schemes. We propose a quantization scheme that\nallows inference to be carried out using integer-only arithmetic, which can be\nimplemented more efficiently than floating point inference on commonly\navailable integer-only hardware. We also co-design a training procedure to\npreserve end-to-end model accuracy post quantization. As a result, the proposed\nquantization scheme improves the tradeoff between accuracy and on-device\nlatency. The improvements are significant even on MobileNets, a model family\nknown for run-time efficiency, and are demonstrated in ImageNet classification\nand COCO detection on popular CPUs. \n\n"}
{"id": "1712.06245", "contents": "Title: Misspecified Nonconvex Statistical Optimization for Phase Retrieval Abstract: Existing nonconvex statistical optimization theory and methods crucially rely\non the correct specification of the underlying \"true\" statistical models. To\naddress this issue, we take a first step towards taming model misspecification\nby studying the high-dimensional sparse phase retrieval problem with\nmisspecified link functions. In particular, we propose a simple variant of the\nthresholded Wirtinger flow algorithm that, given a proper initialization,\nlinearly converges to an estimator with optimal statistical accuracy for a\nbroad family of unknown link functions. We further provide extensive numerical\nexperiments to support our theoretical findings. \n\n"}
{"id": "1712.06911", "contents": "Title: The Dirichlet problem for the complex Hessian operator in the class\n  $\\mathcal{N}_m(H)$ Abstract: We prove that, in a $m$-hyperconvex domain in $\\mathbb{C}^{n},$ if a\nnon-negative Borel measure is dominated by a complex Hessian measure, then it\nis a complex Hessian measure of a function in the class $\\mathcal{N}_m(H)$.\nThis is an extension of P. {\\AA}hg, U. Cegrell, R. Czy\\.z and P.H. Hiep's\nresult in \\cite{ACCH}. \n\n"}
{"id": "1712.08085", "contents": "Title: Multipartite Entanglement Swapping and Mechanical Cluster States Abstract: We present a protocol for generating multipartite quantum correlations across\na quantum network with a continuous-variable architecture. An arbitrary number\nof users possess two-mode entangled states, keeping one mode while sending the\nother to a central relay. Here a suitable multipartite Bell detection is\nperformed which conditionally generates a cluster state on the retained modes.\nThis cluster state can be suitably manipulated by the parties and used for\ntasks of quantum communication in a fully optical scenario. More interestingly,\nthe protocol can be used to create a purely-mechanical cluster state starting\nfrom a supply of optomechanical systems. We show that detecting the optical\nparts of optomechanical cavities may efficiently swap entanglement into their\nmechanical modes, creating cluster states up to 5 modes under suitable\ncryogenic conditions. \n\n"}
{"id": "1712.09483", "contents": "Title: Minimax Estimation of Large Precision Matrices with Bandable Cholesky\n  Factor Abstract: Last decade witnesses significant methodological and theoretical advances in\nestimating large precision matrices. In particular, there are scientific\napplications such as longitudinal data, meteorology and spectroscopy in which\nthe ordering of the variables can be interpreted through a bandable structure\non the Cholesky factor of the precision matrix. However, the minimax theory has\nstill been largely unknown, as opposed to the well established minimax results\nover the corresponding bandable covariance matrices. In this paper, we focus on\ntwo commonly used types of parameter spaces, and develop the optimal rates of\nconvergence under both the operator norm and the Frobenius norm. A striking\nphenomenon is found: two types of parameter spaces are fundamentally different\nunder the operator norm but enjoy the same rate optimality under the Frobenius\nnorm, which is in sharp contrast to the equivalence of corresponding two types\nof bandable covariance matrices under both norms. This fundamental difference\nis established by carefully constructing the corresponding minimax lower\nbounds. Two new estimation procedures are developed: for the operator norm, our\noptimal procedure is based on a novel local cropping estimator targeting on all\nprinciple submatrices of the precision matrix while for the Frobenius norm, our\noptimal procedure relies on a delicate regression-based thresholding rule.\nLepski's method is considered to achieve optimal adaptation. We further\nestablish rate optimality in the nonparanormal model. Numerical studies are\ncarried out to confirm our theoretical findings. \n\n"}
{"id": "1712.09520", "contents": "Title: Tensor Regression Networks with various Low-Rank Tensor Approximations Abstract: Tensor regression networks achieve high compression rate of neural networks\nwhile having slight impact on performances. They do so by imposing low tensor\nrank structure on the weight matrices of fully connected layers. In recent\nyears, tensor regression networks have been investigated from the perspective\nof their compressive power, however, the regularization effect of enforcing\nlow-rank tensor structure has not been investigated enough. We study tensor\nregression networks using various low-rank tensor approximations, aiming to\ncompare the compressive and regularization power of different low-rank\nconstraints. We evaluate the compressive and regularization performances of the\nproposed model with both deep and shallow convolutional neural networks. The\noutcome of our experiment suggests the superiority of Global Average Pooling\nLayer over Tensor Regression Layer when applied to deep convolutional neural\nnetwork with CIFAR-10 dataset. On the contrary, shallow convolutional neural\nnetworks with tensor regression layer and dropout achieved lower test error\nthan both Global Average Pooling and fully-connected layer with dropout\nfunction when trained with a small number of samples. \n\n"}
{"id": "1801.01167", "contents": "Title: Random Kleinian Groups, II : Two parabolic generators Abstract: In earlier work we introduced geometrically natural probability measures on\nthe group of all M\\\"obius transformations in order to study \"random\" groups of\nM\\\"obius transformations, random surfaces, and in particular random\ntwo-generator groups, that is groups where the generators are selected\nrandomly, with a view to estimating the likely-hood that such groups are\ndiscrete and then to make calculations of the expectation of their associated\nparameters, geometry and topology. In this paper we continue that study and\nidentify the precise probability that a Fuchsian group generated by two\nparabolic M\\\"obius transformations is discrete, and give estimates for the case\nof Kleinian groups generated by a pair of random parabolic elements which we\nsupport with a computational investigation into of the Riley slice as\nidentified by Bowditch's condition, and establish rigorous bounds. \n\n"}
{"id": "1801.01469", "contents": "Title: PHOENICS: A universal deep Bayesian optimizer Abstract: In this work we introduce PHOENICS, a probabilistic global optimization\nalgorithm combining ideas from Bayesian optimization with concepts from\nBayesian kernel density estimation. We propose an inexpensive acquisition\nfunction balancing the explorative and exploitative behavior of the algorithm.\nThis acquisition function enables intuitive sampling strategies for an\nefficient parallel search of global minima. The performance of PHOENICS is\nassessed via an exhaustive benchmark study on a set of 15 discrete,\nquasi-discrete and continuous multidimensional functions. Unlike optimization\nmethods based on Gaussian processes (GP) and random forests (RF), we show that\nPHOENICS is less sensitive to the nature of the co-domain, and outperforms GP\nand RF optimizations. We illustrate the performance of PHOENICS on the\nOregonator, a difficult case-study describing a complex chemical reaction\nnetwork. We demonstrate that only PHOENICS was able to reproduce qualitatively\nand quantitatively the target dynamic behavior of this nonlinear reaction\ndynamics. We recommend PHOENICS for rapid optimization of scalar, possibly\nnon-convex, black-box unknown objective functions. \n\n"}
{"id": "1801.01950", "contents": "Title: High Dimensional Elliptical Sliced Inverse Regression in non-Gaussian\n  Distributions Abstract: Sliced inverse regression (SIR) is the most widely-used sufficient dimension\nreduction method due to its simplicity, generality and computational\nefficiency. However, when the distribution of the covariates deviates from the\nmultivariate normal distribution, the estimation efficiency of SIR is rather\nlow. In this paper, we propose a robust alternative to SIR - called elliptical\nsliced inverse regression (ESIR) for analysing high dimensional, elliptically\ndistributed data. There are wide range of applications of the elliptically\ndistributed data, especially in finance and economics where the distribution of\nthe data is often heavy-tailed. To tackle the heavy-tailed elliptically\ndistributed covariates, we novelly utilize the multivariate Kendall's tau\nmatrix in a framework of so-called generalized eigenvector problem for\nsufficient dimension reduction. Methodologically, we present a practical\nalgorithm for our method. Theoretically, we investigate the asymptotic behavior\nof the ESIR estimator and obtain the corresponding convergence rate under high\ndimensional setting. Quantities of simulation results show that ESIR\nsignificantly improves the estimation efficiency in heavy-tailed scenarios. A\nstock exchange data analysis also demonstrates the effectiveness of our method.\nMoreover, ESIR can be easily extended to most other sufficient dimension\nreduction methods. \n\n"}
{"id": "1801.03491", "contents": "Title: Note on the absence of remainders in the Wiener-Ikehara theorem Abstract: We show that it is impossible to get a better remainder than the classical\none in the Wiener-Ikehara theorem even if one assumes analytic continuation of\nthe Mellin transform after subtraction of the pole to a half-plane. We also\nprove a similar result for the Ingham-Karamata theorem. \n\n"}
{"id": "1801.05141", "contents": "Title: Image denoising and restoration with CNN-LSTM Encoder Decoder with\n  Direct Attention Abstract: Image denoising is always a challenging task in the field of computer vision\nand image processing. In this paper, we have proposed an encoder-decoder model\nwith direct attention, which is capable of denoising and reconstruct highly\ncorrupted images. Our model consists of an encoder and a decoder, where the\nencoder is a convolutional neural network and decoder is a multilayer Long\nShort-Term memory network. In the proposed model, the encoder reads an image\nand catches the abstraction of that image in a vector, where decoder takes that\nvector as well as the corrupted image to reconstruct a clean image. We have\ntrained our model on MNIST handwritten digit database after making lower half\nof every image as black as well as adding noise top of that. After a massive\ndestruction of the images where it is hard for a human to understand the\ncontent of those images, our model can retrieve that image with minimal error.\nOur proposed model has been compared with convolutional encoder-decoder, where\nour model has performed better at generating missing part of the images than\nconvolutional autoencoder. \n\n"}
{"id": "1801.07172", "contents": "Title: Scale-invariant Feature Extraction of Neural Network and Renormalization\n  Group Flow Abstract: Theoretical understanding of how deep neural network (DNN) extracts features\nfrom input images is still unclear, but it is widely believed that the\nextraction is performed hierarchically through a process of coarse-graining. It\nreminds us of the basic concept of renormalization group (RG) in statistical\nphysics. In order to explore possible relations between DNN and RG, we use the\nRestricted Boltzmann machine (RBM) applied to Ising model and construct a flow\nof model parameters (in particular, temperature) generated by the RBM. We show\nthat the unsupervised RBM trained by spin configurations at various\ntemperatures from $T=0$ to $T=6$ generates a flow along which the temperature\napproaches the critical value $T_c=2.27$. This behavior is opposite to the\ntypical RG flow of the Ising model. By analyzing various properties of the\nweight matrices of the trained RBM, we discuss why it flows towards $T_c$ and\nhow the RBM learns to extract features of spin configurations. \n\n"}
{"id": "1801.07756", "contents": "Title: Deep Learning for Electromyographic Hand Gesture Signal Classification\n  Using Transfer Learning Abstract: In recent years, deep learning algorithms have become increasingly more\nprominent for their unparalleled ability to automatically learn discriminant\nfeatures from large amounts of data. However, within the field of\nelectromyography-based gesture recognition, deep learning algorithms are seldom\nemployed as they require an unreasonable amount of effort from a single person,\nto generate tens of thousands of examples.\n  This work's hypothesis is that general, informative features can be learned\nfrom the large amounts of data generated by aggregating the signals of multiple\nusers, thus reducing the recording burden while enhancing gesture recognition.\nConsequently, this paper proposes applying transfer learning on aggregated data\nfrom multiple users, while leveraging the capacity of deep learning algorithms\nto learn discriminant features from large datasets. Two datasets comprised of\n19 and 17 able-bodied participants respectively (the first one is employed for\npre-training) were recorded for this work, using the Myo Armband. A third Myo\nArmband dataset was taken from the NinaPro database and is comprised of 10\nable-bodied participants. Three different deep learning networks employing\nthree different modalities as input (raw EMG, Spectrograms and Continuous\nWavelet Transform (CWT)) are tested on the second and third dataset. The\nproposed transfer learning scheme is shown to systematically and significantly\nenhance the performance for all three networks on the two datasets, achieving\nan offline accuracy of 98.31% for 7 gestures over 17 participants for the\nCWT-based ConvNet and 68.98% for 18 gestures over 10 participants for the raw\nEMG-based ConvNet. Finally, a use-case study employing eight able-bodied\nparticipants suggests that real-time feedback allows users to adapt their\nmuscle activation strategy which reduces the degradation in accuracy normally\nexperienced over time. \n\n"}
{"id": "1801.08234", "contents": "Title: When Vehicles See Pedestrians with Phones:A Multi-Cue Framework for\n  Recognizing Phone-based Activities of Pedestrians Abstract: The intelligent vehicle community has devoted considerable efforts to model\ndriver behavior, and in particular to detect and overcome driver distraction in\nan effort to reduce accidents caused by driver negligence. However, as the\ndomain increasingly shifts towards autonomous and semi-autonomous solutions,\nthe driver is no longer integral to the decision making process, indicating a\nneed to refocus efforts elsewhere. To this end, we propose to study pedestrian\ndistraction instead. In particular, we focus on detecting pedestrians who are\nengaged in secondary activities involving their cellphones and similar handheld\nmultimedia devices from a purely vision-based standpoint. To achieve this\nobjective, we propose a pipeline incorporating articulated human pose\nestimation, followed by a soft object label transfer from an ensemble of\nexemplar SVMs trained on the nearest neighbors in pose feature space. We\nadditionally incorporate head gaze features and prior pose information to carry\nout cellphone related pedestrian activity recognition. Finally, we offer a\nmethod to reliably track the articulated pose of a pedestrian through a\nsequence of images using a particle filter with a Gaussian Process Dynamical\nModel (GPDM), which can then be used to estimate sequentially varying activity\nscores at a very low computational cost. The entire framework is fast\n(especially for sequential data) and accurate, and easily extensible to include\nother secondary activities and sources of distraction. \n\n"}
{"id": "1801.09842", "contents": "Title: Fu-Yau Hessian Equations Abstract: We solve the Fu-Yau equation for arbitrary dimension and arbitrary slope\n$\\alpha'$. Actually we obtain at the same time a solution of the open case\n$\\alpha'>0$, an improved solution of the known case $\\alpha'<0$, and solutions\nfor a family of Hessian equations which includes the Fu-Yau equation as a\nspecial case. The method is based on the introduction of a more stringent\nellipticity condition than the usual $\\Gamma_k$ admissible cone condition, and\nwhich can be shown to be preserved by precise estimates with scale. \n\n"}
{"id": "1802.00926", "contents": "Title: On the Minimax Misclassification Ratio of Hypergraph Community Detection Abstract: Community detection in hypergraphs is explored. Under a generative hypergraph\nmodel called \"d-wise hypergraph stochastic block model\" (d-hSBM) which\nnaturally extends the Stochastic Block Model from graphs to d-uniform\nhypergraphs, the asymptotic minimax mismatch ratio is characterized. For\nproving the achievability, we propose a two-step polynomial time algorithm that\nachieves the fundamental limit. The first step of the algorithm is a hypergraph\nspectral clustering method which achieves partial recovery to a certain\nprecision level. The second step is a local refinement method which leverages\nthe underlying probabilistic model along with parameter estimation from the\noutcome of the first step. To characterize the asymptotic performance of the\nproposed algorithm, we first derive a sufficient condition for attaining weak\nconsistency in the hypergraph spectral clustering step. Then, under the\nguarantee of weak consistency in the first step, we upper bound the worst-case\nrisk attained in the local refinement step by an exponentially decaying\nfunction of the size of the hypergraph and characterize the decaying rate. For\nproving the converse, the lower bound of the minimax mismatch ratio is set by\nfinding a smaller parameter space which contains the most dominant error\nevents, inspired by the analysis in the achievability part. It turns out that\nthe minimax mismatch ratio decays exponentially fast to zero as the number of\nnodes tends to infinity, and the rate function is a weighted combination of\nseveral divergence terms, each of which is the Renyi divergence of order 1/2\nbetween two Bernoulli's. The Bernoulli's involved in the characterization of\nthe rate function are those governing the random instantiation of hyperedges in\nd-hSBM. Experimental results on synthetic data validate our theoretical finding\nthat the refinement step is critical in achieving the optimal statistical\nlimit. \n\n"}
{"id": "1802.01223", "contents": "Title: Learning Compact Neural Networks with Regularization Abstract: Proper regularization is critical for speeding up training, improving\ngeneralization performance, and learning compact models that are cost\nefficient. We propose and analyze regularized gradient descent algorithms for\nlearning shallow neural networks. Our framework is general and covers\nweight-sharing (convolutional networks), sparsity (network pruning), and\nlow-rank constraints among others. We first introduce covering dimension to\nquantify the complexity of the constraint set and provide insights on the\ngeneralization properties. Then, we show that proposed algorithms become\nwell-behaved and local linear convergence occurs once the amount of data\nexceeds the covering dimension. Overall, our results demonstrate that\nnear-optimal sample complexity is sufficient for efficient learning and\nillustrate how regularization can be beneficial to learn over-parameterized\nnetworks. \n\n"}
{"id": "1802.01864", "contents": "Title: Conformal Parametrisation of Loxodromes by Triples of Circles Abstract: We provide a parametrisation of a loxodrome by three specially arranged\ncycles. The parametrisation is covariant under fractional linear\ntransformations of the complex plane and naturally encodes conformal properties\nof loxodromes. Selected geometrical examples illustrate the usage of\nparametrisation. Our work extends the set of objects in Lie sphere\ngeometry---circle, lines and points---to the natural maximal\nconformally-invariant family, which also includes loxodromes. \n\n"}
{"id": "1802.02242", "contents": "Title: Full-pulse Tomographic Reconstruction with Deep Neural Networks Abstract: Plasma tomography consists in reconstructing the 2D radiation profile in a\npoloidal cross-section of a fusion device, based on line-integrated\nmeasurements along several lines of sight. The reconstruction process is\ncomputationally intensive and, in practice, only a few reconstructions are\nusually computed per pulse. In this work, we trained a deep neural network\nbased on a large collection of sample tomograms that have been produced at JET\nover several years. Once trained, the network is able to reproduce those\nresults with high accuracy. More importantly, it can compute all the\ntomographic reconstructions for a given pulse in just a few seconds. This makes\nit possible to visualize several phenomena -- such as plasma heating,\ndisruptions and impurity transport -- over the course of a discharge. \n\n"}
{"id": "1802.02550", "contents": "Title: Semi-Amortized Variational Autoencoders Abstract: Amortized variational inference (AVI) replaces instance-specific local\ninference with a global inference network. While AVI has enabled efficient\ntraining of deep generative models such as variational autoencoders (VAE),\nrecent empirical work suggests that inference networks can produce suboptimal\nvariational parameters. We propose a hybrid approach, to use AVI to initialize\nthe variational parameters and run stochastic variational inference (SVI) to\nrefine them. Crucially, the local SVI procedure is itself differentiable, so\nthe inference network and generative model can be trained end-to-end with\ngradient-based optimization. This semi-amortized approach enables the use of\nrich generative models without experiencing the posterior-collapse phenomenon\ncommon in training VAEs for problems like text generation. Experiments show\nthis approach outperforms strong autoregressive and variational baselines on\nstandard text and image datasets. \n\n"}
{"id": "1802.02643", "contents": "Title: Gradient conjugate priors and multi-layer neural networks Abstract: The paper deals with learning probability distributions of observed data by\nartificial neural networks. We suggest a so-called gradient conjugate prior\n(GCP) update appropriate for neural networks, which is a modification of the\nclassical Bayesian update for conjugate priors. We establish a connection\nbetween the gradient conjugate prior update and the maximization of the\nlog-likelihood of the predictive distribution. Unlike for the Bayesian neural\nnetworks, we use deterministic weights of neural networks, but rather assume\nthat the ground truth distribution is normal with unknown mean and variance and\nlearn by the neural networks the parameters of a prior (normal-gamma\ndistribution) for these unknown mean and variance. The update of the parameters\nis done, using the gradient that, at each step, directs towards minimizing the\nKullback--Leibler divergence from the prior to the posterior distribution (both\nbeing normal-gamma). We obtain a corresponding dynamical system for the prior's\nparameters and analyze its properties. In particular, we study the limiting\nbehavior of all the prior's parameters and show how it differs from the case of\nthe classical full Bayesian update. The results are validated on synthetic and\nreal world data sets. \n\n"}
{"id": "1802.02920", "contents": "Title: Spectral State Compression of Markov Processes Abstract: Model reduction of Markov processes is a basic problem in modeling\nstate-transition systems. Motivated by the state aggregation approach rooted in\ncontrol theory, we study the statistical state compression of a discrete-state\nMarkov chain from empirical trajectories. Through the lens of spectral\ndecomposition, we study the rank and features of Markov processes, as well as\nproperties like representability, aggregability, and lumpability. We develop\nspectral methods for estimating the transition matrix of a low-rank Markov\nmodel, estimating the leading subspace spanned by Markov features, and\nrecovering latent structures like state aggregation and lumpable partition of\nthe state space. We prove statistical upper bounds for the estimation errors\nand nearly matching minimax lower bounds. Numerical studies are performed on\nsynthetic data and a dataset of New York City taxi trips. \n\n"}
{"id": "1802.04307", "contents": "Title: A Fast Proximal Point Method for Computing Exact Wasserstein Distance Abstract: Wasserstein distance plays increasingly important roles in machine learning,\nstochastic programming and image processing. Major efforts have been under way\nto address its high computational complexity, some leading to approximate or\nregularized variations such as Sinkhorn distance. However, as we will\ndemonstrate, regularized variations with large regularization parameter will\ndegradate the performance in several important machine learning applications,\nand small regularization parameter will fail due to numerical stability issues\nwith existing algorithms. We address this challenge by developing an Inexact\nProximal point method for exact Optimal Transport problem (IPOT) with the\nproximal operator approximately evaluated at each iteration using projections\nto the probability simplex. The algorithm (a) converges to exact Wasserstein\ndistance with theoretical guarantee and robust regularization parameter\nselection, (b) alleviates numerical stability issue, (c) has similar\ncomputational complexity to Sinkhorn, and (d) avoids the shrinking problem when\napply to generative models. Furthermore, a new algorithm is proposed based on\nIPOT to obtain sharper Wasserstein barycenter. \n\n"}
{"id": "1802.04364", "contents": "Title: Junction Tree Variational Autoencoder for Molecular Graph Generation Abstract: We seek to automate the design of molecules based on specific chemical\nproperties. In computational terms, this task involves continuous embedding and\ngeneration of molecular graphs. Our primary contribution is the direct\nrealization of molecular graphs, a task previously approached by generating\nlinear SMILES strings instead of graphs. Our junction tree variational\nautoencoder generates molecular graphs in two phases, by first generating a\ntree-structured scaffold over chemical substructures, and then combining them\ninto a molecule with a graph message passing network. This approach allows us\nto incrementally expand molecules while maintaining chemical validity at every\nstep. We evaluate our model on multiple tasks ranging from molecular generation\nto optimization. Across these tasks, our model outperforms previous\nstate-of-the-art baselines by a significant margin. \n\n"}
{"id": "1802.04712", "contents": "Title: Attention-based Deep Multiple Instance Learning Abstract: Multiple instance learning (MIL) is a variation of supervised learning where\na single class label is assigned to a bag of instances. In this paper, we state\nthe MIL problem as learning the Bernoulli distribution of the bag label where\nthe bag label probability is fully parameterized by neural networks.\nFurthermore, we propose a neural network-based permutation-invariant\naggregation operator that corresponds to the attention mechanism. Notably, an\napplication of the proposed attention-based operator provides insight into the\ncontribution of each instance to the bag label. We show empirically that our\napproach achieves comparable performance to the best MIL methods on benchmark\nMIL datasets and it outperforms other methods on a MNIST-based MIL dataset and\ntwo real-life histopathology datasets without sacrificing interpretability. \n\n"}
{"id": "1802.04942", "contents": "Title: Isolating Sources of Disentanglement in Variational Autoencoders Abstract: We decompose the evidence lower bound to show the existence of a term\nmeasuring the total correlation between latent variables. We use this to\nmotivate our $\\beta$-TCVAE (Total Correlation Variational Autoencoder), a\nrefinement of the state-of-the-art $\\beta$-VAE objective for learning\ndisentangled representations, requiring no additional hyperparameters during\ntraining. We further propose a principled classifier-free measure of\ndisentanglement called the mutual information gap (MIG). We perform extensive\nquantitative and qualitative experiments, in both restricted and non-restricted\nsettings, and show a strong relation between total correlation and\ndisentanglement, when the latent variables model is trained using our\nframework. \n\n"}
{"id": "1802.05478", "contents": "Title: Enhanced non-Markovian behavior in quantum walks with Markovian disorder Abstract: Non-Markovian quantum effects are typically observed in systems interacting\nwith structured reservoirs. Discrete-time quantum walks are prime example of\nsuch systems in which, quantum memory arises due to the controlled interaction\nbetween the coin and position degrees of freedom. Here we show that the\ninformation backflow that quantifies memory effects can be enhanced when the\nparticle is subjected to uncorrelated static or dynamic disorder. The presence\nof disorder in the system leads to localization effects in 1-dimensional\nquantum walks. We shown that it is possible to infer about the nature of\nlocalization in position space by monitoring the information backflow in the\nreduced system. Further, we study other useful properties of quantum walk such\nas entanglement, interference and its connection to quantum non-Markovianity. \n\n"}
{"id": "1802.05637", "contents": "Title: cGANs with Projection Discriminator Abstract: We propose a novel, projection based way to incorporate the conditional\ninformation into the discriminator of GANs that respects the role of the\nconditional information in the underlining probabilistic model. This approach\nis in contrast with most frameworks of conditional GANs used in application\ntoday, which use the conditional information by concatenating the (embedded)\nconditional vector to the feature vectors. With this modification, we were able\nto significantly improve the quality of the class conditional image generation\non ILSVRC2012 (ImageNet) 1000-class image dataset from the current\nstate-of-the-art result, and we achieved this with a single pair of a\ndiscriminator and a generator. We were also able to extend the application to\nsuper-resolution and succeeded in producing highly discriminative\nsuper-resolution images. This new structure also enabled high quality category\ntransformation based on parametric functional transformation of conditional\nbatch normalization layers in the generator. \n\n"}
{"id": "1802.05800", "contents": "Title: Tree-CNN: A Hierarchical Deep Convolutional Neural Network for\n  Incremental Learning Abstract: Over the past decade, Deep Convolutional Neural Networks (DCNNs) have shown\nremarkable performance in most computer vision tasks. These tasks traditionally\nuse a fixed dataset, and the model, once trained, is deployed as is. Adding new\ninformation to such a model presents a challenge due to complex training\nissues, such as \"catastrophic forgetting\", and sensitivity to hyper-parameter\ntuning. However, in this modern world, data is constantly evolving, and our\ndeep learning models are required to adapt to these changes. In this paper, we\npropose an adaptive hierarchical network structure composed of DCNNs that can\ngrow and learn as new data becomes available. The network grows in a tree-like\nfashion to accommodate new classes of data, while preserving the ability to\ndistinguish the previously trained classes. The network organizes the\nincrementally available data into feature-driven super-classes and improves\nupon existing hierarchical CNN models by adding the capability of self-growth.\nThe proposed hierarchical model, when compared against fine-tuning a deep\nnetwork, achieves significant reduction of training effort, while maintaining\ncompetitive accuracy on CIFAR-10 and CIFAR-100. \n\n"}
{"id": "1802.06967", "contents": "Title: Recovery of simultaneous low rank and two-way sparse coefficient\n  matrices, a nonconvex approach Abstract: We study the problem of recovery of matrices that are simultaneously low rank\nand row and/or column sparse. Such matrices appear in recent applications in\ncognitive neuroscience, imaging, computer vision, macroeconomics, and genetics.\nWe propose a GDT (Gradient Descent with hard Thresholding) algorithm to\nefficiently recover matrices with such structure, by minimizing a bi-convex\nfunction over a nonconvex set of constraints. We show linear convergence of the\niterates obtained by GDT to a region within statistical error of an optimal\nsolution. As an application of our method, we consider multi-task learning\nproblems and show that the statistical error rate obtained by GDT is near\noptimal compared to minimax rate. Experiments demonstrate competitive\nperformance and much faster running speed compared to existing methods, on both\nsimulations and real data sets. \n\n"}
{"id": "1802.06967", "contents": "Title: Recovery of simultaneous low rank and two-way sparse coefficient\n  matrices, a nonconvex approach Abstract: We study the problem of recovery of matrices that are simultaneously low rank\nand row and/or column sparse. Such matrices appear in recent applications in\ncognitive neuroscience, imaging, computer vision, macroeconomics, and genetics.\nWe propose a GDT (Gradient Descent with hard Thresholding) algorithm to\nefficiently recover matrices with such structure, by minimizing a bi-convex\nfunction over a nonconvex set of constraints. We show linear convergence of the\niterates obtained by GDT to a region within statistical error of an optimal\nsolution. As an application of our method, we consider multi-task learning\nproblems and show that the statistical error rate obtained by GDT is near\noptimal compared to minimax rate. Experiments demonstrate competitive\nperformance and much faster running speed compared to existing methods, on both\nsimulations and real data sets. \n\n"}
{"id": "1802.07107", "contents": "Title: Learning of Optimal Forecast Aggregation in Partial Evidence\n  Environments Abstract: We consider the forecast aggregation problem in repeated settings, where the\nforecasts are done on a binary event. At each period multiple experts provide\nforecasts about an event. The goal of the aggregator is to aggregate those\nforecasts into a subjective accurate forecast. We assume that experts are\nBayesian; namely they share a common prior, each expert is exposed to some\nevidence, and each expert applies Bayes rule to deduce his forecast. The\naggregator is ignorant with respect to the information structure (i.e.,\ndistribution over evidence) according to which experts make their prediction.\nThe aggregator observes the experts' forecasts only. At the end of each period\nthe actual state is realized. We focus on the question whether the aggregator\ncan learn to aggregate optimally the forecasts of the experts, where the\noptimal aggregation is the Bayesian aggregation that takes into account all the\ninformation (evidence) in the system.\n  We consider the class of partial evidence information structures, where each\nexpert is exposed to a different subset of conditionally independent signals.\nOur main results are positive; We show that optimal aggregation can be learned\nin polynomial time in a quite wide range of instances of the partial evidence\nenvironments. We provide a tight characterization of the instances where\nlearning is possible and impossible. \n\n"}
{"id": "1802.07384", "contents": "Title: Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic\n  Corrections Abstract: We present a new algorithm to generate minimal, stable, and symbolic\ncorrections to an input that will cause a neural network with ReLU activations\nto change its output. We argue that such a correction is a useful way to\nprovide feedback to a user when the network's output is different from a\ndesired output. Our algorithm generates such a correction by solving a series\nof linear constraint satisfaction problems. The technique is evaluated on three\nneural network models: one predicting whether an applicant will pay a mortgage,\none predicting whether a first-order theorem can be proved efficiently by a\nsolver using certain heuristics, and the final one judging whether a drawing is\nan accurate rendition of a canonical drawing of a cat. \n\n"}
{"id": "1802.07434", "contents": "Title: Nonparametric Bayesian Sparse Graph Linear Dynamical Systems Abstract: A nonparametric Bayesian sparse graph linear dynamical system (SGLDS) is\nproposed to model sequentially observed multivariate data. SGLDS uses the\nBernoulli-Poisson link together with a gamma process to generate an infinite\ndimensional sparse random graph to model state transitions. Depending on the\nsparsity pattern of the corresponding row and column of the graph affinity\nmatrix, a latent state of SGLDS can be categorized as either a non-dynamic\nstate or a dynamic one. A normal-gamma construction is used to shrink the\nenergy captured by the non-dynamic states, while the dynamic states can be\nfurther categorized into live, absorbing, or noise-injection states, which\ncapture different types of dynamical components of the underlying time series.\nThe state-of-the-art performance of SGLDS is demonstrated with experiments on\nboth synthetic and real data. \n\n"}
{"id": "1802.08195", "contents": "Title: Adversarial Examples that Fool both Computer Vision and Time-Limited\n  Humans Abstract: Machine learning models are vulnerable to adversarial examples: small changes\nto images can cause computer vision models to make mistakes such as identifying\na school bus as an ostrich. However, it is still an open question whether\nhumans are prone to similar mistakes. Here, we address this question by\nleveraging recent techniques that transfer adversarial examples from computer\nvision models with known parameters and architecture to other models with\nunknown parameters and architecture, and by matching the initial processing of\nthe human visual system. We find that adversarial examples that strongly\ntransfer across computer vision models influence the classifications made by\ntime-limited human observers. \n\n"}
{"id": "1802.08372", "contents": "Title: Approximation Algorithms for D-optimal Design Abstract: Experimental design is a classical statistics problem and its aim is to\nestimate an unknown $m$-dimensional vector $\\beta$ from linear measurements\nwhere a Gaussian noise is introduced in each measurement. For the combinatorial\nexperimental design problem, the goal is to pick $k$ out of the given $n$\nexperiments so as to make the most accurate estimate of the unknown parameters,\ndenoted as $\\hat{\\beta}$. In this paper, we will study one of the most robust\nmeasures of error estimation - $D$-optimality criterion, which corresponds to\nminimizing the volume of the confidence ellipsoid for the estimation error\n$\\beta-\\hat{\\beta}$. The problem gives rise to two natural variants depending\non whether repetitions of experiments are allowed or not. We first propose an\napproximation algorithm with a $\\frac1e$-approximation for the $D$-optimal\ndesign problem with and without repetitions, giving the first constant factor\napproximation for the problem. We then analyze another sampling approximation\nalgorithm and prove that it is $(1-\\epsilon)$-approximation if $k\\geq\n\\frac{4m}{\\epsilon}+\\frac{12}{\\epsilon^2}\\log(\\frac{1}{\\epsilon})$ for any\n$\\epsilon \\in (0,1)$. Finally, for $D$-optimal design with repetitions, we\nstudy a different algorithm proposed by literature and show that it can improve\nthis asymptotic approximation ratio. \n\n"}
{"id": "1802.08429", "contents": "Title: Exact Sampling of Determinantal Point Processes without\n  Eigendecomposition Abstract: Determinantal point processes (DPPs) enable the modeling of repulsion: they\nprovide diverse sets of points. The repulsion is encoded in a kernel $K$ that\ncan be seen as a matrix storing the similarity between points. The diversity\ncomes from the fact that the inclusion probability of a subset is equal to the\ndeterminant of a submatrice of $K$. The exact algorithm to sample DPPs uses the\nspectral decomposition of $K$, a computation that becomes costly when dealing\nwith a high number of points. Here, we present an alternative exact algorithm\nin the discrete setting that avoids the eigenvalues and the eigenvectors\ncomputation. Instead, it relies on Cholesky decompositions. This is a two steps\nstrategy: first, it samples a Bernoulli point process with an appropriate\ndistribution, then it samples the target DPP distribution through a thinning\nprocedure. Not only is the method used here innovative, but this algorithm can\nbe competitive with the original algorithm or even faster for some applications\nspecified here. \n\n"}
{"id": "1802.08567", "contents": "Title: Adversarial Training for Probabilistic Spiking Neural Networks Abstract: Classifiers trained using conventional empirical risk minimization or maximum\nlikelihood methods are known to suffer dramatic performance degradations when\ntested over examples adversarially selected based on knowledge of the\nclassifier's decision rule. Due to the prominence of Artificial Neural Networks\n(ANNs) as classifiers, their sensitivity to adversarial examples, as well as\nrobust training schemes, have been recently the subject of intense\ninvestigation. In this paper, for the first time, the sensitivity of spiking\nneural networks (SNNs), or third-generation neural networks, to adversarial\nexamples is studied. The study considers rate and time encoding, as well as\nrate and first-to-spike decoding. Furthermore, a robust training mechanism is\nproposed that is demonstrated to enhance the performance of SNNs under\nwhite-box attacks. \n\n"}
{"id": "1802.09514", "contents": "Title: Best Arm Identification for Contaminated Bandits Abstract: This paper studies active learning in the context of robust statistics.\nSpecifically, we propose a variant of the Best Arm Identification problem for\n\\emph{contaminated bandits}, where each arm pull has probability $\\varepsilon$\nof generating a sample from an arbitrary contamination distribution instead of\nthe true underlying distribution. The goal is to identify the best (or\napproximately best) true distribution with high probability, with a secondary\ngoal of providing guarantees on the quality of this distribution. The primary\nchallenge of the contaminated bandit setting is that the true distributions are\nonly partially identifiable, even with infinite samples. To address this, we\ndevelop tight, non-asymptotic sample complexity bounds for high-probability\nestimation of the first two robust moments (median and median absolute\ndeviation) from contaminated samples. These concentration inequalities are the\nmain technical contributions of the paper and may be of independent interest.\nUsing these results, we adapt several classical Best Arm Identification\nalgorithms to the contaminated bandit setting and derive sample complexity\nupper bounds for our problem. Finally, we provide matching\ninformation-theoretic lower bounds on the sample complexity (up to a small\nlogarithmic factor). \n\n"}
{"id": "1802.09684", "contents": "Title: Network Representation Using Graph Root Distributions Abstract: Exchangeable random graphs serve as an important probabilistic framework for\nthe statistical analysis of network data. In this work we develop an\nalternative parameterization for a large class of exchangeable random graphs,\nwhere the nodes are independent random vectors in a linear space equipped with\nan indefinite inner product, and the edge probability between two nodes equals\nthe inner product of the corresponding node vectors. Therefore, the\ndistribution of exchangeable random graphs in this subclass can be represented\nby a node sampling distribution on this linear space, which we call the graph\nroot distribution. We study existence and identifiability of such\nrepresentations, the topological relationship between the graph root\ndistribution and the exchangeable random graph sampling distribution, and\nestimation of graph root distributions. \n\n"}
{"id": "1802.09777", "contents": "Title: Gaussian meta-embeddings for efficient scoring of a heavy-tailed PLDA\n  model Abstract: Embeddings in machine learning are low-dimensional representations of complex\ninput patterns, with the property that simple geometric operations like\nEuclidean distances and dot products can be used for classification and\ncomparison tasks. The proposed meta-embeddings are special embeddings that live\nin more general inner product spaces. They are designed to propagate\nuncertainty to the final output in speaker recognition and similar\napplications. The familiar Gaussian PLDA model (GPLDA) can be re-formulated as\nan extractor for Gaussian meta-embeddings (GMEs), such that likelihood ratio\nscores are given by Hilbert space inner products between Gaussian likelihood\nfunctions. GMEs extracted by the GPLDA model have fixed precisions and do not\npropagate uncertainty. We show that a generalization to heavy-tailed PLDA gives\nGMEs with variable precisions, which do propagate uncertainty. Experiments on\nNIST SRE 2010 and 2016 show that the proposed method applied to i-vectors\nwithout length normalization is up to 20% more accurate than GPLDA applied to\nlength-normalized ivectors. \n\n"}
{"id": "1803.00194", "contents": "Title: Chordal Komatu-Loewner equation for a family of continuously growing\n  hulls Abstract: In this paper, we discuss the chordal Komatu-Loewner equation on standard\nslit domains in a manner applicable not just to a simple curve but also a\nfamily of continuously growing hulls. Especially a conformally invariant\ncharacterization of the Komatu-Loewner evolution is obtained. As an\napplication, we prove a sort of conformal invariance, or locality, of the\nstochastic Komatu-Loewner evolution $\\mathrm{SKLE}_{\\sqrt{6},\n-b_{\\mathrm{BMD}}}$ in a fully general setting, which solves an open problem\nposed by Chen, Fukushima and Suzuki [Stochastic Komatu-Loewner evolutions and\nSLEs, Stoch. Proc. Appl. 127 (2017), 2068-2087]. \n\n"}
{"id": "1803.00491", "contents": "Title: The Power Mean Laplacian for Multilayer Graph Clustering Abstract: Multilayer graphs encode different kind of interactions between the same set\nof entities. When one wants to cluster such a multilayer graph, the natural\nquestion arises how one should merge the information different layers. We\nintroduce in this paper a one-parameter family of matrix power means for\nmerging the Laplacians from different layers and analyze it in expectation in\nthe stochastic block model. We show that this family allows to recover ground\ntruth clusters under different settings and verify this in real world data.\nWhile computing the matrix power mean can be very expensive for large graphs,\nwe introduce a numerical scheme to efficiently compute its eigenvectors for the\ncase of large sparse graphs. \n\n"}
{"id": "1803.00650", "contents": "Title: Kernel Embedding Approaches to Orbit Determination of Spacecraft\n  Clusters Abstract: This paper presents a novel formulation and solution of orbit determination\nover finite time horizons as a learning problem. We present an approach to\norbit determination under very broad conditions that are satisfied for n-body\nproblems. These weak conditions allow us to perform orbit determination with\nnoisy and highly non-linear observations such as those presented by range-rate\nonly (Doppler only) observations. We show that domain generalization and\ndistribution regression techniques can learn to estimate orbits of a group of\nsatellites and identify individual satellites especially with prior\nunderstanding of correlations between orbits and provide asymptotic convergence\nconditions. The approach presented requires only visibility and observability\nof the underlying state from observations and is particularly useful for\nautonomous spacecraft operations using low-cost ground stations or sensors. We\nvalidate the orbit determination approach using observations of two spacecraft\n(GRIFEX and MCubed-2) along with synthetic datasets of multiple spacecraft\ndeployments and lunar orbits. We also provide a comparison with the standard\ntechniques (EKF) under highly noisy conditions. \n\n"}
{"id": "1803.01347", "contents": "Title: Greedy stochastic algorithms for entropy-regularized optimal transport\n  problems Abstract: Optimal transport (OT) distances are finding evermore applications in machine\nlearning and computer vision, but their wide spread use in larger-scale\nproblems is impeded by their high computational cost. In this work we develop a\nfamily of fast and practical stochastic algorithms for solving the optimal\ntransport problem with an entropic penalization. This work extends the recently\ndeveloped Greenkhorn algorithm, in the sense that, the Greenkhorn algorithm is\na limiting case of this family. We also provide a simple and general\nconvergence theorem for all algorithms in the class, with rates that match the\nbest known rates of Greenkorn and the Sinkhorn algorithm, and conclude with\nnumerical experiments that show under what regime of penalization the new\nstochastic methods are faster than the aforementioned methods. \n\n"}
{"id": "1803.02596", "contents": "Title: Revisiting differentially private linear regression: optimal and\n  adaptive prediction & estimation in unbounded domain Abstract: We revisit the problem of linear regression under a differential privacy\nconstraint. By consolidating existing pieces in the literature, we clarify the\ncorrect dependence of the feature, label and coefficient domains in the\noptimization error and estimation error, hence revealing the delicate price of\ndifferential privacy in statistical estimation and statistical learning.\nMoreover, we propose simple modifications of two existing DP algorithms: (a)\nposterior sampling, (b) sufficient statistics perturbation, and show that they\ncan be upgraded into **adaptive** algorithms that are able to exploit\ndata-dependent quantities and behave nearly optimally **for every instance**.\nExtensive experiments are conducted on both simulated data and real data, which\nconclude that both AdaOPS and AdaSSP outperform the existing techniques on\nnearly all 36 data sets that we test on. \n\n"}
{"id": "1803.02642", "contents": "Title: Learning Spectral-Spatial-Temporal Features via a Recurrent\n  Convolutional Neural Network for Change Detection in Multispectral Imagery Abstract: Change detection is one of the central problems in earth observation and was\nextensively investigated over recent decades. In this paper, we propose a novel\nrecurrent convolutional neural network (ReCNN) architecture, which is trained\nto learn a joint spectral-spatial-temporal feature representation in a unified\nframework for change detection in multispectral images. To this end, we bring\ntogether a convolutional neural network (CNN) and a recurrent neural network\n(RNN) into one end-to-end network. The former is able to generate rich\nspectral-spatial feature representations, while the latter effectively analyzes\ntemporal dependency in bi-temporal images. In comparison with previous\napproaches to change detection, the proposed network architecture possesses\nthree distinctive properties: 1) It is end-to-end trainable, in contrast to\nmost existing methods whose components are separately trained or computed; 2)\nit naturally harnesses spatial information that has been proven to be\nbeneficial to change detection task; 3) it is capable of adaptively learning\nthe temporal dependency between multitemporal images, unlike most of algorithms\nthat use fairly simple operation like image differencing or stacking. As far as\nwe know, this is the first time that a recurrent convolutional network\narchitecture has been proposed for multitemporal remote sensing image analysis.\nThe proposed network is validated on real multispectral data sets. Both visual\nand quantitative analysis of experimental results demonstrates competitive\nperformance in the proposed mode. \n\n"}
{"id": "1803.02922", "contents": "Title: Fast Convergence for Stochastic and Distributed Gradient Descent in the\n  Interpolation Limit Abstract: Modern supervised learning techniques, particularly those using deep nets,\ninvolve fitting high dimensional labelled data sets with functions containing\nvery large numbers of parameters. Much of this work is empirical. Interesting\nphenomena have been observed that require theoretical explanations; however the\nnon-convexity of the loss functions complicates the analysis. Recently it has\nbeen proposed that the success of these techniques rests partly in the\neffectiveness of the simple stochastic gradient descent algorithm in the so\ncalled interpolation limit in which all labels are fit perfectly. This analysis\nis made possible since the SGD algorithm reduces to a stochastic linear system\nnear the interpolating minimum of the loss function. Here we exploit this\ninsight by presenting and analyzing a new distributed algorithm for gradient\ndescent, also in the interpolating limit. The distributed SGD algorithm\npresented in the paper corresponds to gradient descent applied to a simple\npenalized distributed loss function, $L({\\bf w}_1,...,{\\bf w}_n) = \\Sigma_i\nl_i({\\bf w}_i) + \\mu \\sum_{<i,j>}|{\\bf w}_i-{\\bf w}_j|^2$. Here each node holds\nonly one sample, and its own parameter vector. The notation $<i,j>$ denotes\nedges of a connected graph defining the links between nodes. It is shown that\nthis distributed algorithm converges linearly (ie the error reduces\nexponentially with iteration number), with a rate\n$1-\\frac{\\eta}{n}\\lambda_{min}(H)<R<1$ where $\\lambda_{min}(H)$ is the smallest\nnonzero eigenvalue of the sample covariance or the Hessian H. In contrast with\nprevious usage of similar penalty functions to enforce consensus between nodes,\nin the interpolating limit it is not required to take the penalty parameter to\ninfinity for consensus to occur. The analysis further reinforces the utility of\nthe interpolation limit in the theoretical treatment of modern machine learning\nalgorithms. \n\n"}
{"id": "1803.03075", "contents": "Title: Probing the collective dynamics of nuclear spin bath in a rare-earth ion\n  doped crystal Abstract: Probing collective spin dynamics is a current challenge in the field of\nmagnetic resonance spectroscopy and has important applications in material\nanalysis and quantum information protocols. Recently, the rare-earth ion doped\ncrystals are an attractive candidate for making long-lived quantum memory.\nFurther enhancement of its performance would benefit from the direct knowledge\non the dynamics of nuclear-spin bath in the crystal. Here we detect the\ncollective dynamics of nuclear-spin bath located around the rare-earth ions in\na crystal using dynamical decoupling spectroscopy method. From the measured\nspectrum, we analyze the configuration of the spin bath and characterize the\nflip-flop time between two correlated nuclear spins in a long time scale ($\\sim\n$1s). Furthermore, we experimentally demonstrate that the rare-earth ions can\nserve as a magnetic quantum sensor for external magnetic field. These results\nsuggest that the rare-earth ion is a useful probe for complex spin dynamics in\nsolids and enable quantum sensing in the low-frequency regime, revealing\npromising possibilities for applications in diverse fields. \n\n"}
{"id": "1803.04469", "contents": "Title: An Introduction to Image Synthesis with Generative Adversarial Nets Abstract: There has been a drastic growth of research in Generative Adversarial Nets\n(GANs) in the past few years. Proposed in 2014, GAN has been applied to various\napplications such as computer vision and natural language processing, and\nachieves impressive performance. Among the many applications of GAN, image\nsynthesis is the most well-studied one, and research in this area has already\ndemonstrated the great potential of using GAN in image synthesis. In this\npaper, we provide a taxonomy of methods used in image synthesis, review\ndifferent models for text-to-image synthesis and image-to-image translation,\nand discuss some evaluation metrics as well as possible future research\ndirections in image synthesis with GAN. \n\n"}
{"id": "1803.04489", "contents": "Title: Probabilistic and Regularized Graph Convolutional Networks Abstract: This paper explores the recently proposed Graph Convolutional Network\narchitecture proposed in (Kipf & Welling, 2016) The key points of their work is\nsummarized and their results are reproduced. Graph regularization and\nalternative graph convolution approaches are explored. I find that explicit\ngraph regularization was correctly rejected by (Kipf & Welling, 2016). I\nattempt to improve the performance of GCN by approximating a k-step transition\nmatrix in place of the normalized graph laplacian, but I fail to find positive\nresults. Nonetheless, the performance of several configurations of this GCN\nvariation is shown for the Cora, Citeseer, and Pubmed datasets. \n\n"}
{"id": "1803.06272", "contents": "Title: Graph Partition Neural Networks for Semi-Supervised Classification Abstract: We present graph partition neural networks (GPNN), an extension of graph\nneural networks (GNNs) able to handle extremely large graphs. GPNNs alternate\nbetween locally propagating information between nodes in small subgraphs and\nglobally propagating information between the subgraphs. To efficiently\npartition graphs, we experiment with several partitioning algorithms and also\npropose a novel variant for fast processing of large scale graphs. We\nextensively test our model on a variety of semi-supervised node classification\ntasks. Experimental results indicate that GPNNs are either superior or\ncomparable to state-of-the-art methods on a wide variety of datasets for\ngraph-based semi-supervised classification. We also show that GPNNs can achieve\nsimilar performance as standard GNNs with fewer propagation steps. \n\n"}
{"id": "1803.06791", "contents": "Title: Depth-aware CNN for RGB-D Segmentation Abstract: Convolutional neural networks (CNN) are limited by the lack of capability to\nhandle geometric information due to the fixed grid kernel structure. The\navailability of depth data enables progress in RGB-D semantic segmentation with\nCNNs. State-of-the-art methods either use depth as additional images or process\nspatial information in 3D volumes or point clouds. These methods suffer from\nhigh computation and memory cost. To address these issues, we present\nDepth-aware CNN by introducing two intuitive, flexible and effective\noperations: depth-aware convolution and depth-aware average pooling. By\nleveraging depth similarity between pixels in the process of information\npropagation, geometry is seamlessly incorporated into CNN. Without introducing\nany additional parameters, both operators can be easily integrated into\nexisting CNNs. Extensive experiments and ablation studies on challenging RGB-D\nsemantic segmentation benchmarks validate the effectiveness and flexibility of\nour approach. \n\n"}
{"id": "1803.06978", "contents": "Title: Improving Transferability of Adversarial Examples with Input Diversity Abstract: Though CNNs have achieved the state-of-the-art performance on various vision\ntasks, they are vulnerable to adversarial examples --- crafted by adding\nhuman-imperceptible perturbations to clean images. However, most of the\nexisting adversarial attacks only achieve relatively low success rates under\nthe challenging black-box setting, where the attackers have no knowledge of the\nmodel structure and parameters. To this end, we propose to improve the\ntransferability of adversarial examples by creating diverse input patterns.\nInstead of only using the original images to generate adversarial examples, our\nmethod applies random transformations to the input images at each iteration.\nExtensive experiments on ImageNet show that the proposed attack method can\ngenerate adversarial examples that transfer much better to different networks\nthan existing baselines. By evaluating our method against top defense solutions\nand official baselines from NIPS 2017 adversarial competition, the enhanced\nattack reaches an average success rate of 73.0%, which outperforms the top-1\nattack submission in the NIPS competition by a large margin of 6.6%. We hope\nthat our proposed attack strategy can serve as a strong benchmark baseline for\nevaluating the robustness of networks to adversaries and the effectiveness of\ndifferent defense methods in the future. Code is available at\nhttps://github.com/cihangxie/DI-2-FGSM. \n\n"}
{"id": "1803.07012", "contents": "Title: Conditional $\\pi$-Phase Shift of Single-Photon-Level Pulses at Room\n  Temperature Abstract: The development of useful photon-photon interactions can trigger numerous\nbreakthroughs in quantum information science, however this has remained a\nconsiderable challenge spanning several decades. Here we demonstrate the first\nroom-temperature implementation of large phase shifts ($\\approx\\pi$) on a\nsingle-photon level probe pulse (1.5us) triggered by a\nsimultaneously-propagating few-photon-level signal field. This process is\nmediated by $Rb^{87}$ vapor in a double-$\\Lambda$ atomic configuration. We use\nhomodyne tomography to obtain the quadrature statistics of the phase-shifted\nquantum fields and perform maximum-likelihood estimation to reconstruct their\nquantum state in the Fock state basis. For the probe field, we have observed\ninput-output fidelities higher than 90$\\%$ for phase-shifted output states, and\nhigh overlap (over 90\\%) with a theoretically perfect coherent state. Our\nnoise-free, four-wave-mixing-mediated photon-photon interface is a key\nmilestone towards developing quantum logic and nondemolition photon detection\nusing schemes such as coherent photon conversion. \n\n"}
{"id": "1803.07054", "contents": "Title: Optimal link prediction with matrix logistic regression Abstract: We consider the problem of link prediction, based on partial observation of a\nlarge network, and on side information associated to its vertices. The\ngenerative model is formulated as a matrix logistic regression. The performance\nof the model is analysed in a high-dimensional regime under a structural\nassumption. The minimax rate for the Frobenius-norm risk is established and a\ncombinatorial estimator based on the penalised maximum likelihood approach is\nshown to achieve it. Furthermore, it is shown that this rate cannot be attained\nby any (randomised) algorithm computable in polynomial time under a\ncomputational complexity assumption. \n\n"}
{"id": "1803.07089", "contents": "Title: Device-independent quantum key distribution with single-photon sources Abstract: Device-independent quantum key distribution protocols allow two honest users\nto establish a secret key with minimal levels of trust on the provider, as\nsecurity is proven without any assumption on the inner working of the devices\nused for the distribution. Unfortunately, the implementation of these protocols\nis challenging, as it requires the observation of a large Bell-inequality\nviolation between the two distant users. Here, we introduce novel photonic\nprotocols for device-independent quantum key distribution exploiting\nsingle-photon sources and heralding-type architectures. The heralding process\nis designed so that transmission losses become irrelevant for security. We then\nshow how the use of single-photon sources for entanglement distribution in\nthese architectures, instead of standard entangled-pair generation schemes,\nprovides significant improvements on the attainable key rates and distances\nover previous proposals. Given the current progress in single-photon sources,\nour work opens up a promising avenue for device-independent quantum key\ndistribution implementations. \n\n"}
{"id": "1803.07469", "contents": "Title: MAGSAC: marginalizing sample consensus Abstract: A method called, sigma-consensus, is proposed to eliminate the need for a\nuser-defined inlier-outlier threshold in RANSAC. Instead of estimating the\nnoise sigma, it is marginalized over a range of noise scales. The optimized\nmodel is obtained by weighted least-squares fitting where the weights come from\nthe marginalization over sigma of the point likelihoods of being inliers. A new\nquality function is proposed not requiring sigma and, thus, a set of inliers to\ndetermine the model quality. Also, a new termination criterion for RANSAC is\nbuilt on the proposed marginalization approach. Applying sigma-consensus,\nMAGSAC is proposed with no need for a user-defined sigma and improving the\naccuracy of robust estimation significantly. It is superior to the\nstate-of-the-art in terms of geometric accuracy on publicly available\nreal-world datasets for epipolar geometry (F and E) and homography estimation.\nIn addition, applying sigma-consensus only once as a post-processing step to\nthe RANSAC output always improved the model quality on a wide range of vision\nproblems without noticeable deterioration in processing time, adding a few\nmilliseconds. The source code is at https://github.com/danini/magsac. \n\n"}
{"id": "1803.08591", "contents": "Title: End-to-End Learning for the Deep Multivariate Probit Model Abstract: The multivariate probit model (MVP) is a popular classic model for studying\nbinary responses of multiple entities. Nevertheless, the computational\nchallenge of learning the MVP model, given that its likelihood involves\nintegrating over a multidimensional constrained space of latent variables,\nsignificantly limits its application in practice. We propose a flexible deep\ngeneralization of the classic MVP, the Deep Multivariate Probit Model (DMVP),\nwhich is an end-to-end learning scheme that uses an efficient parallel sampling\nprocess of the multivariate probit model to exploit GPU-boosted deep neural\nnetworks. We present both theoretical and empirical analysis of the convergence\nbehavior of DMVP's sampling process with respect to the resolution of the\ncorrelation structure. We provide convergence guarantees for DMVP and our\nempirical analysis demonstrates the advantages of DMVP's sampling compared with\nstandard MCMC-based methods. We also show that when applied to multi-entity\nmodelling problems, which are natural DMVP applications, DMVP trains faster\nthan classical MVP, by at least an order of magnitude, captures rich\ncorrelations among entities, and further improves the joint likelihood of\nentities compared with several competitive models. \n\n"}
{"id": "1803.09053", "contents": "Title: Bounded strictly pseudoconvex domains in $\\mathbb{C}^2$ with obstruction\n  flat boundary Abstract: On a bounded strictly pseudoconvex domain in $\\mathbb{C}^n$, $n>1$, the\nsmoothness of the Cheng-Yau solution to Fefferman's complex Monge-Ampere\nequation up to the boundary is obstructed by a local curvature invariant of the\nboundary. For bounded strictly pseudoconvex domains in $\\mathbb{C}^2$ which are\ndiffeomorphic to the ball, we motivate and consider the problem of determining\nwhether the global vanishing of this obstruction implies biholomorphic\nequivalence to the unit ball. In particular we observe that, up to\nbiholomorphism, the unit ball in $\\mathbb{C}^2$ is rigid with respect to\ndeformations in the class of strictly pseudoconvex domains with obstruction\nflat boundary. We further show that for more general deformations of the unit\nball, the order of vanishing of the obstruction equals the order of vanishing\nof the CR curvature. Finally, we give a generalization of the recent result of\nthe second author that for an abstract CR manifold with transverse symmetry,\nobstruction flatness implies local equivalence to the CR $3$-sphere. \n\n"}
{"id": "1803.10397", "contents": "Title: Supervising Unsupervised Learning with Evolutionary Algorithm in Deep\n  Neural Network Abstract: A method to control results of gradient descent unsupervised learning in a\ndeep neural network by using evolutionary algorithm is proposed. To process\ncrossover of unsupervisedly trained models, the algorithm evaluates pointwise\nfitness of individual nodes in neural network. Labeled training data is\nrandomly sampled and breeding process selects nodes by calculating degree of\ntheir consistency on different sets of sampled data. This method supervises\nunsupervised training by evolutionary process. We also introduce modified\nRestricted Boltzmann Machine which contains repulsive force among nodes in a\nneural network and it contributes to isolate network nodes each other to avoid\naccidental degeneration of nodes by evolutionary process. These new methods are\napplied to document classification problem and it results better accuracy than\na traditional fully supervised classifier implemented with linear regression\nalgorithm. \n\n"}
{"id": "1803.10484", "contents": "Title: Complementary metal-oxide semiconductor compatible source of single\n  photons at near-visible wavelengths Abstract: We demonstrate on chip generation of correlated pairs of photons in the\nnear-visible spectrum using a CMOS compatible PECVD Silicon Nitride photonic\ndevice. Photons are generated via spontaneous four wave mixing enhanced by a\nring resonator with high quality Q-factor of 320,000 resulting in a generation\nrate of 950,000 $\\frac{pairs}{mW}$. The high brightness of this source offers\nthe opportunity to expand photonic quantum technologies over a broad wavelength\nrange and provides a path to develop fully integrated quantum chips working at\nroom temperature. \n\n"}
{"id": "1803.10554", "contents": "Title: Joint PLDA for Simultaneous Modeling of Two Factors Abstract: Probabilistic linear discriminant analysis (PLDA) is a method used for\nbiometric problems like speaker or face recognition that models the variability\nof the samples using two latent variables, one that depends on the class of the\nsample and another one that is assumed independent across samples and models\nthe within-class variability. In this work, we propose a generalization of PLDA\nthat enables joint modeling of two sample-dependent factors: the class of\ninterest and a nuisance condition. The approach does not change the basic form\nof PLDA but rather modifies the training procedure to consider the dependency\nacross samples of the latent variable that models within-class variability.\nWhile the identity of the nuisance condition is needed during training, it is\nnot needed during testing since we propose a scoring procedure that\nmarginalizes over the corresponding latent variable. We show results on a\nmultilingual speaker-verification task, where the language spoken is considered\na nuisance condition. We show that the proposed joint PLDA approach leads to\nsignificant performance gains in this task for two different datasets, in\nparticular when the training data contains mostly or only monolingual speakers. \n\n"}
{"id": "1803.10647", "contents": "Title: Active Metric Learning for Supervised Classification Abstract: Clustering and classification critically rely on distance metrics that\nprovide meaningful comparisons between data points. We present mixed-integer\noptimization approaches to find optimal distance metrics that generalize the\nMahalanobis metric extensively studied in the literature. Additionally, we\ngeneralize and improve upon leading methods by removing reliance on\npre-designated \"target neighbors,\" \"triplets,\" and \"similarity pairs.\" Another\nsalient feature of our method is its ability to enable active learning by\nrecommending precise regions to sample after an optimal metric is computed to\nimprove classification performance. This targeted acquisition can significantly\nreduce computational burden by ensuring training data completeness,\nrepresentativeness, and economy. We demonstrate classification and\ncomputational performance of the algorithms through several simple and\nintuitive examples, followed by results on real image and medical datasets. \n\n"}
{"id": "1804.00323", "contents": "Title: The Jordan property for Lie groups and automorphism groups of complex\n  spaces Abstract: We prove that the family of all connected n-dimensional real Lie groups is\nuniformly Jordan for every n. This implies that all algebraic groups (not\nnecessarily affine) over fields of characteristic zero and some transformation\ngroups of complex spaces and Riemannian manifods are Jordan. \n\n"}
{"id": "1804.00414", "contents": "Title: Unbounded Weighted Composition Operators on Fock space Abstract: In this paper, we consider \\emph{unbounded} weighted composition operators\nacting on Fock space, and investigate some important properties of these\noperators, such as $\\calC$-selfadjoint (with respect to weighted composition\nconjugations), Hermitian, normal, cohyponormal, and invertible. In addition,\nthe paper shows that unbounded normal weighted composition operators are\ncontained properly in the class of $\\calC$-selfadjoint operators with respect\nto weighted composition conjugations. The computation of the spectrum is\ncarried out in detail. \n\n"}
{"id": "1804.00432", "contents": "Title: Deep Residual Learning for Accelerated MRI using Magnitude and Phase\n  Networks Abstract: Accelerated magnetic resonance (MR) scan acquisition with compressed sensing\n(CS) and parallel imaging is a powerful method to reduce MR imaging scan time.\nHowever, many reconstruction algorithms have high computational costs. To\naddress this, we investigate deep residual learning networks to remove aliasing\nartifacts from artifact corrupted images. The proposed deep residual learning\nnetworks are composed of magnitude and phase networks that are separately\ntrained. If both phase and magnitude information are available, the proposed\nalgorithm can work as an iterative k-space interpolation algorithm using\nframelet representation. When only magnitude data is available, the proposed\napproach works as an image domain post-processing algorithm. Even with strong\ncoherent aliasing artifacts, the proposed network successfully learned and\nremoved the aliasing artifacts, whereas current parallel and CS reconstruction\nmethods were unable to remove these artifacts. Comparisons using single and\nmultiple coil show that the proposed residual network provides good\nreconstruction results with orders of magnitude faster computational time than\nexisting compressed sensing methods. The proposed deep learning framework may\nhave a great potential for accelerated MR reconstruction by generating accurate\nresults immediately. \n\n"}
{"id": "1804.00504", "contents": "Title: Generalizability vs. Robustness: Adversarial Examples for Medical\n  Imaging Abstract: In this paper, for the first time, we propose an evaluation method for deep\nlearning models that assesses the performance of a model not only in an unseen\ntest scenario, but also in extreme cases of noise, outliers and ambiguous input\ndata. To this end, we utilize adversarial examples, images that fool machine\nlearning models, while looking imperceptibly different from original data, as a\nmeasure to evaluate the robustness of a variety of medical imaging models.\nThrough extensive experiments on skin lesion classification and whole brain\nsegmentation with state-of-the-art networks such as Inception and UNet, we show\nthat models that achieve comparable performance regarding generalizability may\nhave significant variations in their perception of the underlying data\nmanifold, leading to an extensive performance gap in their robustness. \n\n"}
{"id": "1804.01071", "contents": "Title: Average performance analysis of the stochastic gradient method for\n  online PCA Abstract: This paper studies the complexity of the stochastic gradient algorithm for\nPCA when the data are observed in a streaming setting. We also propose an\nonline approach for selecting the learning rate. Simulation experiments confirm\nthe practical relevance of the plain stochastic gradient approach and that\ndrastic improvements can be achieved by learning the learning rate. \n\n"}
{"id": "1804.01864", "contents": "Title: Adaptive test for ergodic diffusions plus noise Abstract: We propose some parametric tests for ergodic diffusion-plus-noise model,\nwhich is a version of state-space modelling in statistics for stochastic\ndiffusion equations. The test statistics are classified into three types:\nlikelihood-ratio-type test statistic; Wald-type one; and Rao-type one. All the\ntest statistics are constructed with quasi-likelihood-functions for local mean\nsequence of noised observation. We also simulate the behaviour of them for\nseveral practical hypothesis tests and check the convergence in law of test\nstatistics under null hypotheses and consistency of the test under alternative\nones. We apply the method for real data analysis of wind data, and examine some\nsets of the hypotheses mainly with respect to the structure of diffusion\ncoefficient. \n\n"}
{"id": "1804.02086", "contents": "Title: Structured Disentangled Representations Abstract: Deep latent-variable models learn representations of high-dimensional data in\nan unsupervised manner. A number of recent efforts have focused on learning\nrepresentations that disentangle statistically independent axes of variation by\nintroducing modifications to the standard objective function. These approaches\ngenerally assume a simple diagonal Gaussian prior and as a result are not able\nto reliably disentangle discrete factors of variation. We propose a two-level\nhierarchical objective to control relative degree of statistical independence\nbetween blocks of variables and individual variables within blocks. We derive\nthis objective as a generalization of the evidence lower bound, which allows us\nto explicitly represent the trade-offs between mutual information between data\nand representation, KL divergence between representation and prior, and\ncoverage of the support of the empirical data distribution. Experiments on a\nvariety of datasets demonstrate that our objective can not only disentangle\ndiscrete variables, but that doing so also improves disentanglement of other\nvariables and, importantly, generalization even to unseen combinations of\nfactors. \n\n"}
{"id": "1804.02147", "contents": "Title: Fast phase-modulated optical lattice for wave packet engineering Abstract: We investigate experimentally a Bose Einstein condensate placed in a 1D\noptical lattice whose phase is modulated at a frequency large compared to all\ncharacteristic frequencies. As a result, the depth of the periodic potential is\nrenormalized by a Bessel function which only depends on the amplitude of\nmodulation, a prediction that we have checked quantitatively using a careful\ncalibration scheme. This renormalization provides an interesting tool to\nengineer in time optical lattices. For instance, we have used it to perform\nsimultaneously a sudden $\\pi$-phase shift (without phase residual errors)\ncombined with a change of lattice depth, and to study the subsequent\nout-of-equilibrium dynamics. \n\n"}
{"id": "1804.02181", "contents": "Title: Generative adversarial network-based approach to signal reconstruction\n  from magnitude spectrograms Abstract: In this paper, we address the problem of reconstructing a time-domain signal\n(or a phase spectrogram) solely from a magnitude spectrogram. Since magnitude\nspectrograms do not contain phase information, we must restore or infer phase\ninformation to reconstruct a time-domain signal. One widely used approach for\ndealing with the signal reconstruction problem was proposed by Griffin and Lim.\nThis method usually requires many iterations for the signal reconstruction\nprocess and depending on the inputs, it does not always produce high-quality\naudio signals. To overcome these shortcomings, we apply a learning-based\napproach to the signal reconstruction problem by modeling the signal\nreconstruction process using a deep neural network and training it using the\nidea of a generative adversarial network. Experimental evaluations revealed\nthat our method was able to reconstruct signals faster with higher quality than\nthe Griffin-Lim method. \n\n"}
{"id": "1804.03105", "contents": "Title: Central limit theorems via Stein's method for randomized experiments\n  under interference Abstract: We study conditions under which treatment effect estimators constructed under\nthe no-interference assumption in randomized experiments are asymptotically\nnormal in the presence of interference. We prove that the standard\nHorvitz-Thompson estimator is asymptotically normal under a restricted\ninterference condition characterized by limiting the degree of the dependency\ngraph. The amount of interference is allowed to grow with the population size.\nWe then provide a central limit theorem for the difference-in-means estimator\nthat can handle interference that exists between all pairs of units, provided\nmost of the interference is captured by a restricted-degree dependency graph.\nThe asymptotic variance admits a decomposition into two terms: (a) the variance\nthat is expected under no-interference and (b) the additional variance\ncontributed by interference. We propose a conservative variance estimator based\non this variance decomposition. The results arise as an application of Stein's\nmethod. For practitioners, our results show that standard estimators continue\nto exhibit normality in large sample sizes and that inference can be made\nrobust to mild forms of interference. \n\n"}
{"id": "1804.04524", "contents": "Title: Chern scalar curvature and symmetric products of compact Riemann\n  surfaces Abstract: Let $X$ be a compact connected Riemann surface of genus $g\\geq 0$, and let\n${\\rm Sym}^d(X)$, $d \\ge 1$, denote the $d$-fold symmetric product of $X$. We\nshow that ${\\rm Sym}^d(X)$ admits a Hermitian metric with negative Chern scalar\ncurvature if and only if $g \\geq 2$, and positive Chern scalar curvature if and\nonly if $d > g$. \n\n"}
{"id": "1804.04656", "contents": "Title: 3D G-CNNs for Pulmonary Nodule Detection Abstract: Convolutional Neural Networks (CNNs) require a large amount of annotated data\nto learn from, which is often difficult to obtain in the medical domain. In\nthis paper we show that the sample complexity of CNNs can be significantly\nimproved by using 3D roto-translation group convolutions (G-Convs) instead of\nthe more conventional translational convolutions. These 3D G-CNNs were applied\nto the problem of false positive reduction for pulmonary nodule detection, and\nproved to be substantially more effective in terms of performance, sensitivity\nto malignant nodules, and speed of convergence compared to a strong and\ncomparable baseline architecture with regular convolutions, data augmentation\nand a similar number of parameters. For every dataset size tested, the G-CNN\nachieved a FROC score close to the CNN trained on ten times more data. \n\n"}
{"id": "1804.05688", "contents": "Title: Notes on Non-Generic Isomonodromy Deformations Abstract: Some of the main results of [Cotti G., Dubrovin B., Guzzetti D., Duke Math.\nJ., to appear, arXiv:1706.04808], concerning non-generic isomonodromy\ndeformations of a certain linear differential system with irregular singularity\nand coalescing eigenvalues, are reviewed from the point of view of Pfaffian\nsystems, making a distinction between weak and strong isomonodromic\ndeformations. Such distinction has a counterpart in the case of Fuchsian\nsystems, which is well known as Schlesinger and non-Schlesinger deformations,\nreviewed in Appendix A. \n\n"}
{"id": "1804.05862", "contents": "Title: Non-Vacuous Generalization Bounds at the ImageNet Scale: A PAC-Bayesian\n  Compression Approach Abstract: Modern neural networks are highly overparameterized, with capacity to\nsubstantially overfit to training data. Nevertheless, these networks often\ngeneralize well in practice. It has also been observed that trained networks\ncan often be \"compressed\" to much smaller representations. The purpose of this\npaper is to connect these two empirical observations. Our main technical result\nis a generalization bound for compressed networks based on the compressed size.\nCombined with off-the-shelf compression algorithms, the bound leads to state of\nthe art generalization guarantees; in particular, we provide the first\nnon-vacuous generalization guarantees for realistic architectures applied to\nthe ImageNet classification problem. As additional evidence connecting\ncompression and generalization, we show that compressibility of models that\ntend to overfit is limited: We establish an absolute limit on expected\ncompressibility as a function of expected generalization error, where the\nexpectations are over the random choice of training examples. The bounds are\ncomplemented by empirical results that show an increase in overfitting implies\nan increase in the number of bits required to describe a trained network. \n\n"}
{"id": "1804.06352", "contents": "Title: High Dimensional Time Series Generators Abstract: Multidimensional time series are sequences of real valued vectors. They occur\nin different areas, for example handwritten characters, GPS tracking, and\ngestures of modern virtual reality motion controllers. Within these areas, a\ncommon task is to search for similar time series. Dynamic Time Warping (DTW) is\na common distance function to compare two time series. The Edit Distance with\nReal Penalty (ERP) and the Dog Keeper Distance (DK) are two more distance\nfunctions on time series. Their behaviour has been analyzed on 1-dimensional\ntime series. However, it is not easy to evaluate their behaviour in relation to\ngrowing dimensionality. For this reason we propose two new data synthesizers\ngenerating multidimensional time series. The first synthesizer extends the well\nknown cylinder-bell-funnel (CBF) dataset to multidimensional time series. Here,\neach time series has an arbitrary type (cylinder, bell, or funnel) in each\ndimension, thus for $d$-dimensional time series there are $3^{d}$ different\nclasses. The second synthesizer (RAM) creates time series with ideas adapted\nfrom Brownian motions which is a common model of movement in physics. Finally,\nwe evaluate the applicability of a 1-nearest neighbor classifier using DTW on\ndatasets generated by our synthesizers. \n\n"}
{"id": "1804.08369", "contents": "Title: Gaussian Material Synthesis Abstract: We present a learning-based system for rapid mass-scale material synthesis\nthat is useful for novice and expert users alike. The user preferences are\nlearned via Gaussian Process Regression and can be easily sampled for new\nrecommendations. Typically, each recommendation takes 40-60 seconds to render\nwith global illumination, which makes this process impracticable for real-world\nworkflows. Our neural network eliminates this bottleneck by providing\nhigh-quality image predictions in real time, after which it is possible to pick\nthe desired materials from a gallery and assign them to a scene in an intuitive\nmanner. Workflow timings against Disney's \"principled\" shader reveal that our\nsystem scales well with the number of sought materials, thus empowering even\nnovice users to generate hundreds of high-quality material models without any\nexpertise in material modeling. Similarly, expert users experience a\nsignificant decrease in the total modeling time when populating a scene with\nmaterials. Furthermore, our proposed solution also offers controllable\nrecommendations and a novel latent space variant generation step to enable the\nreal-time fine-tuning of materials without requiring any domain expertise. \n\n"}
{"id": "1804.09569", "contents": "Title: On a hyperconvex manifold without non-constant bounded holomorphic\n  functions Abstract: An example is given of a hyperconvex manifold without non-constant bounded\nholomorphic functions, which is realized as a domain with real-analytic\nLevi-flat boundary in a projective surface. \n\n"}
{"id": "1804.10916", "contents": "Title: Unsupervised Cross-Modality Domain Adaptation of ConvNets for Biomedical\n  Image Segmentations with Adversarial Loss Abstract: Convolutional networks (ConvNets) have achieved great successes in various\nchallenging vision tasks. However, the performance of ConvNets would degrade\nwhen encountering the domain shift. The domain adaptation is more significant\nwhile challenging in the field of biomedical image analysis, where\ncross-modality data have largely different distributions. Given that annotating\nthe medical data is especially expensive, the supervised transfer learning\napproaches are not quite optimal. In this paper, we propose an unsupervised\ndomain adaptation framework with adversarial learning for cross-modality\nbiomedical image segmentations. Specifically, our model is based on a dilated\nfully convolutional network for pixel-wise prediction. Moreover, we build a\nplug-and-play domain adaptation module (DAM) to map the target input to\nfeatures which are aligned with source domain feature space. A domain critic\nmodule (DCM) is set up for discriminating the feature space of both domains. We\noptimize the DAM and DCM via an adversarial loss without using any target\ndomain label. Our proposed method is validated by adapting a ConvNet trained\nwith MRI images to unpaired CT data for cardiac structures segmentations, and\nachieved very promising results. \n\n"}
{"id": "1804.10928", "contents": "Title: A Nonparametric Ensemble Binary Classifier and its Statistical\n  Properties Abstract: In this work, we propose an ensemble of classification trees (CT) and\nartificial neural networks (ANN). Several statistical properties including\nuniversal consistency and upper bound of an important parameter of the proposed\nclassifier are shown. Numerical evidence is also provided using various real\nlife data sets to assess the performance of the model. Our proposed\nnonparametric ensemble classifier doesn't suffer from the `curse of\ndimensionality' and can be used in a wide variety of feature selection cum\nclassification problems. Performance of the proposed model is quite better when\ncompared to many other state-of-the-art models used for similar situations. \n\n"}
{"id": "1805.00071", "contents": "Title: Understanding Regularization to Visualize Convolutional Neural Networks Abstract: Variational methods for revealing visual concepts learned by convolutional\nneural networks have gained significant attention during the last years. Being\nbased on noisy gradients obtained via back-propagation such methods require the\napplication of regularization strategies. We present a mathematical framework\nunifying previously employed regularization methods. Within this framework, we\npropose a novel technique based on Sobolev gradients which can be implemented\nvia convolutions and does not require specialized numerical treatment, such as\ntotal variation regularization. The experiments performed on feature inversion\nand activation maximization demonstrate the benefit of a unified approach to\nregularization, such as sharper reconstructions via the proposed Sobolev\nfilters and a better control over reconstructed scales. \n\n"}
{"id": "1805.00079", "contents": "Title: Coherence, entanglement and quantumness in closed and open systems with\n  conserved charge, with an application to many-body localisation Abstract: While the scaling of entanglement in a quantum system can be used to\ndistinguish many-body quantum phases, it is usually hard to quantify the amount\nof entanglement in mixed states of open quantum systems, while measuring\nentanglement experimentally, even for the closed systems, requires in general\nquantum state tomography. In this work we show how to remedy this situation in\nsystem with a fixed or conserved charge, e.g., density or magnetization, due to\nan emerging relation between quantum correlations and coherence. First, we show\nhow, in these cases, the presence of multipartite entanglement or quantumness\ncan be faithfully witnessed simply by detecting coherence in the quantum\nsystem, while bipartite entanglement or bipartite quantum discord are implied\nby asymmetry (block coherence) in the system. Second, we prove that the\nrelation between quantum correlations and coherence is also quantitative.\nNamely, we establish upper and lower bounds on the amount of multipartite and\nbipartite entanglement in a many-body system with a fixed local charge, in\nterms of the amount of coherence and asymmetry present in the system.\nImportantly, both for pure and mixed quantum states, these bounds are expressed\nas closed formulas, and furthermore, for bipartite entanglement, are\nexperimentally accessible by means of the multiple quantum coherence spectra.\nIn particular, in one-dimensional systems, our bounds may detect breaking of\nthe area law of entanglement entropy. We illustrate our results on the example\nof a many-body localized system, also in the presence of dephasing. \n\n"}
{"id": "1805.00216", "contents": "Title: Privately Learning High-Dimensional Distributions Abstract: We present novel, computationally efficient, and differentially private\nalgorithms for two fundamental high-dimensional learning problems: learning a\nmultivariate Gaussian and learning a product distribution over the Boolean\nhypercube in total variation distance. The sample complexity of our algorithms\nnearly matches the sample complexity of the optimal non-private learners for\nthese tasks in a wide range of parameters, showing that privacy comes\nessentially for free for these problems. In particular, in contrast to previous\napproaches, our algorithm for learning Gaussians does not require strong a\npriori bounds on the range of the parameters. Our algorithms introduce a novel\ntechnical approach to reducing the sensitivity of the estimation procedure that\nwe call recursive private preconditioning. \n\n"}
{"id": "1805.00361", "contents": "Title: Ultra Power-Efficient CNN Domain Specific Accelerator with 9.3TOPS/Watt\n  for Mobile and Embedded Applications Abstract: Computer vision performances have been significantly improved in recent years\nby Convolutional Neural Networks(CNN). Currently, applications using CNN\nalgorithms are deployed mainly on general purpose hardwares, such as CPUs, GPUs\nor FPGAs. However, power consumption, speed, accuracy, memory footprint, and\ndie size should all be taken into consideration for mobile and embedded\napplications. Domain Specific Architecture (DSA) for CNN is the efficient and\npractical solution for CNN deployment and implementation. We designed and\nproduced a 28nm Two-Dimensional CNN-DSA accelerator with an ultra\npower-efficient performance of 9.3TOPS/Watt and with all processing done in the\ninternal memory instead of outside DRAM. It classifies 224x224 RGB image inputs\nat more than 140fps with peak power consumption at less than 300mW and an\naccuracy comparable to the VGG benchmark. The CNN-DSA accelerator is\nreconfigurable to support CNN model coefficients of various layer sizes and\nlayer types, including convolution, depth-wise convolution, short-cut\nconnections, max pooling, and ReLU. Furthermore, in order to better support\nreal-world deployment for various application scenarios, especially with\nlow-end mobile and embedded platforms and MCUs (Microcontroller Units), we also\ndesigned algorithms to fully utilize the CNN-DSA accelerator efficiently by\nreducing the dependency on external accelerator computation resources,\nincluding implementation of Fully-Connected (FC) layers within the accelerator\nand compression of extracted features from the CNN-DSA accelerator. Live demos\nwith our CNN-DSA accelerator on mobile and embedded systems show its\ncapabilities to be widely and practically applied in the real world. \n\n"}
{"id": "1805.00899", "contents": "Title: AI safety via debate Abstract: To make AI systems broadly useful for challenging real-world tasks, we need\nthem to learn complex human goals and preferences. One approach to specifying\ncomplex goals asks humans to judge during training which agent behaviors are\nsafe and useful, but this approach can fail if the task is too complicated for\na human to directly judge. To help address this concern, we propose training\nagents via self play on a zero sum debate game. Given a question or proposed\naction, two agents take turns making short statements up to a limit, then a\nhuman judges which of the agents gave the most true, useful information. In an\nanalogy to complexity theory, debate with optimal play can answer any question\nin PSPACE given polynomial time judges (direct judging answers only NP\nquestions). In practice, whether debate works involves empirical questions\nabout humans and the tasks we want AIs to perform, plus theoretical questions\nabout the meaning of AI alignment. We report results on an initial MNIST\nexperiment where agents compete to convince a sparse classifier, boosting the\nclassifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to\n85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of\nthe debate model, focusing on potential weaknesses as the model scales up, and\nwe propose future human and computer experiments to test these properties. \n\n"}
{"id": "1805.01516", "contents": "Title: How deep should be the depth of convolutional neural networks: a\n  backyard dog case study Abstract: The work concerns the problem of reducing a pre-trained deep neuronal network\nto a smaller network, with just few layers, whilst retaining the network's\nfunctionality on a given task\n  The proposed approach is motivated by the observation that the aim to deliver\nthe highest accuracy possible in the broadest range of operational conditions,\nwhich many deep neural networks models strive to achieve, may not necessarily\nbe always needed, desired, or even achievable due to the lack of data or\ntechnical constraints. In relation to the face recognition problem, we\nformulated an example of such a usecase, the `backyard dog' problem. The\n`backyard dog', implemented by a lean network, should correctly identify\nmembers from a limited group of individuals, a `family', and should distinguish\nbetween them. At the same time, the network must produce an alarm to an image\nof an individual who is not in a member of the family. To produce such a\nnetwork, we propose a shallowing algorithm. The algorithm takes an existing\ndeep learning model on its input and outputs a shallowed version of it. The\nalgorithm is non-iterative and is based on the Advanced Supervised Principal\nComponent Analysis. Performance of the algorithm is assessed in exhaustive\nnumerical experiments. In the above usecase, the `backyard dog' problem, the\nmethod is capable of drastically reducing the depth of deep learning neural\nnetworks, albeit at the cost of mild performance deterioration.\n  We developed a simple non-iterative method for shallowing down pre-trained\ndeep networks. The method is generic in the sense that it applies to a broad\nclass of feed-forward networks, and is based on the Advanced Supervise\nPrincipal Component Analysis. The method enables generation of families of\nsmaller-size shallower specialized networks tuned for specific operational\nconditions and tasks from a single larger and more universal legacy network. \n\n"}
{"id": "1805.02587", "contents": "Title: Sharp Analysis of a Simple Model for Random Forests Abstract: Random forests have become an important tool for improving accuracy in\nregression and classification problems since their inception by Leo Breiman in\n2001. In this paper, we revisit a historically important random forest model\noriginally proposed by Breiman in 2004 and later studied by G\\'erard Biau in\n2012, where a feature is selected at random and the splits occurs at the\nmidpoint of the node along the chosen feature. If the regression function is\nLipschitz and depends only on a small subset of $ S $ out of $ d $ features, we\nshow that, given access to $ n $ observations and properly tuned split\nprobabilities, the mean-squared prediction error is $ O((n(\\log\nn)^{(S-1)/2})^{-\\frac{1}{S\\log2+1}}) $. This positively answers an outstanding\nquestion of Biau about whether the rate of convergence for this random forest\nmodel could be improved. Furthermore, by a refined analysis of the\napproximation and estimation errors for linear models, we show that this rate\ncannot be improved in general. Finally, we generalize our analysis and improve\nextant prediction error bounds for another random forest model in which each\ntree is constructed from subsampled data and the splits are performed at the\nempirical median along a chosen feature. \n\n"}
{"id": "1805.02953", "contents": "Title: Universality and models for semigroups of operators on a Hilbert space Abstract: This paper considers universal Hilbert space operators in the sense of Rota,\nand gives criteria for universality of semigroups in the context of uniformly\ncontinuous semigroups and contraction semigroups. Specific examples are given.\nUniversal semigroups provide models for these classes of semigroups: following\na line of research initiated by Shimorin, models for concave semigroups are\ndeveloped, in terms of shifts on reproducing kernel Hilbert spaces. \n\n"}
{"id": "1805.03697", "contents": "Title: Which-Way Measurement and Momentum Kicks Abstract: Two-slit interference experiment with a which-way detector has been a topic\nof intense debate. Scientific community is divided on the question whether the\nparticle receives a momentum kick because of the process of which-way\nmeasurement. It is shown here that the same experiment can be viewed in two\ndifferent ways, depending on which basis of the which-way detector states one\nchooses to look at. In one view, the loss of interference arises due to the\nentanglement of the two paths of the particle with two orthogonal states of the\nwhich-way detector. In another view, the loss of interference can be\ninterpreted as arising from random momentum kicks of magnitude $h/2d$ received\nby the particle, $d$ being the slit separation. The same scenario is shown to\nhold for a three-slit interference experiment. The random momentum kicks for\nthe three-slit case are of two kinds, of magnitude $\\pm h/3d$. The analysis is\nalso generalized to the case of n-slit interference. The two alternate views\nare described by the same quantum state, and hence are completely equivalent.\nThe concept of \"local\" versus \"nonlocal\" kicks, much discussed in the\nliterature, is not needed here. \n\n"}
{"id": "1805.05396", "contents": "Title: Confidence Scoring Using Whitebox Meta-models with Linear Classifier\n  Probes Abstract: We propose a novel confidence scoring mechanism for deep neural networks\nbased on a two-model paradigm involving a base model and a meta-model. The\nconfidence score is learned by the meta-model observing the base model\nsucceeding/failing at its task. As features to the meta-model, we investigate\nlinear classifier probes inserted between the various layers of the base model.\nOur experiments demonstrate that this approach outperforms various baselines in\na filtering task, i.e., task of rejecting samples with low confidence.\nExperimental results are presented using CIFAR-10 and CIFAR-100 dataset with\nand without added noise. We discuss the importance of confidence scoring to\nbridge the gap between experimental and real-world applications. \n\n"}
{"id": "1805.05827", "contents": "Title: Graph Signal Sampling via Reinforcement Learning Abstract: We formulate the problem of sampling and recovering clustered graph signal as\na multi-armed bandit (MAB) problem. This formulation lends naturally to\nlearning sampling strategies using the well-known gradient MAB algorithm. In\nparticular, the sampling strategy is represented as a probability distribution\nover the individual arms of the MAB and optimized using gradient ascent. Some\nillustrative numerical experiments indicate that the sampling strategies based\non the gradient MAB algorithm outperform existing sampling methods. \n\n"}
{"id": "1805.05997", "contents": "Title: A Thurston boundary for infinite-dimensional Teichm\\\"uller spaces Abstract: For a compact surface $X_0$, Thurston introduced a compactification of its\nTeichm\\\"uller space $\\mathcal T(X_0)$ by completing it with a boundary\n$\\mathcal{PML}(X_0)$ consisting of projective measured geodesic laminations. We\nintroduce a similar bordification for the Teichm\\\"uller space $\\mathcal T(X_0)$\nof a noncompact Riemann surface $X_0$, using the technical tool of geodesic\ncurrents. The lack of compactness requires the introduction of certain\nuniformity conditions which were unnecessary for compact surfaces. A technical\nstep, providing a convergence result for earthquake paths in $\\mathcal T(X_0)$,\nmay be of independent interest. \n\n"}
{"id": "1805.07410", "contents": "Title: Learning to Collaborate for User-Controlled Privacy Abstract: It is becoming increasingly clear that users should own and control their\ndata. Utility providers are also becoming more interested in guaranteeing data\nprivacy. As such, users and utility providers should collaborate in data\nprivacy, a paradigm that has not yet been developed in the privacy research\ncommunity. We introduce this concept and present explicit architectures where\nthe user controls what characteristics of the data she/he wants to share and\nwhat she/he wants to keep private. This is achieved by collaborative learning a\nsensitization function, either a deterministic or a stochastic one, that\nretains valuable information for the utility tasks but it also eliminates\nnecessary information for the privacy ones. As illustration examples, we\nimplement them using a plug-and-play approach, where no algorithm is changed at\nthe system provider end, and an adversarial approach, where minor re-training\nof the privacy inferring engine is allowed. In both cases the learned\nsanitization function keeps the data in the original domain, thereby allowing\nthe system to use the same algorithms it was using before for both original and\nprivatized data. We show how we can maintain utility while fully protecting\nprivate information if the user chooses to do so, even when the first is harder\nthan the second, as in the case here illustrated of identity detection while\nhiding gender. \n\n"}
{"id": "1805.07440", "contents": "Title: Neural Architecture Search using Deep Neural Networks and Monte Carlo\n  Tree Search Abstract: Neural Architecture Search (NAS) has shown great success in automating the\ndesign of neural networks, but the prohibitive amount of computations behind\ncurrent NAS methods requires further investigations in improving the sample\nefficiency and the network evaluation cost to get better results in a shorter\ntime. In this paper, we present a novel scalable Monte Carlo Tree Search (MCTS)\nbased NAS agent, named AlphaX, to tackle these two aspects. AlphaX improves the\nsearch efficiency by adaptively balancing the exploration and exploitation at\nthe state level, and by a Meta-Deep Neural Network (DNN) to predict network\naccuracies for biasing the search toward a promising region. To amortize the\nnetwork evaluation cost, AlphaX accelerates MCTS rollouts with a distributed\ndesign and reduces the number of epochs in evaluating a network by transfer\nlearning, which is guided with the tree structure in MCTS. In 12 GPU days and\n1000 samples, AlphaX found an architecture that reaches 97.84\\% top-1 accuracy\non CIFAR-10, and 75.5\\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods\nin both the accuracy and sampling efficiency. Particularly, we also evaluate\nAlphaX on NASBench-101, a large scale NAS dataset; AlphaX is 3x and 2.8x more\nsample efficient than Random Search and Regularized Evolution in finding the\nglobal optimum. Finally, we show the searched architecture improves a variety\nof vision applications from Neural Style Transfer, to Image Captioning and\nObject Detection. \n\n"}
{"id": "1805.07588", "contents": "Title: Robust Optimization over Multiple Domains Abstract: In this work, we study the problem of learning a single model for multiple\ndomains. Unlike the conventional machine learning scenario where each domain\ncan have the corresponding model, multiple domains (i.e., applications/users)\nmay share the same machine learning model due to maintenance loads in cloud\ncomputing services. For example, a digit-recognition model should be applicable\nto hand-written digits, house numbers, car plates, etc. Therefore, an ideal\nmodel for cloud computing has to perform well at each applicable domain. To\naddress this new challenge from cloud computing, we develop a framework of\nrobust optimization over multiple domains. In lieu of minimizing the empirical\nrisk, we aim to learn a model optimized to the adversarial distribution over\nmultiple domains. Hence, we propose to learn the model and the adversarial\ndistribution simultaneously with the stochastic algorithm for efficiency.\nTheoretically, we analyze the convergence rate for convex and non-convex\nmodels. To our best knowledge, we first study the convergence rate of learning\na robust non-convex model with a practical algorithm. Furthermore, we\ndemonstrate that the robustness of the framework and the convergence rate can\nbe further enhanced by appropriate regularizers over the adversarial\ndistribution. The empirical study on real-world fine-grained visual\ncategorization and digits recognition tasks verifies the effectiveness and\nefficiency of the proposed framework. \n\n"}
{"id": "1805.07813", "contents": "Title: Learning Real-World Robot Policies by Dreaming Abstract: Learning to control robots directly based on images is a primary challenge in\nrobotics. However, many existing reinforcement learning approaches require\niteratively obtaining millions of robot samples to learn a policy, which can\ntake significant time. In this paper, we focus on learning a realistic world\nmodel capturing the dynamics of scene changes conditioned on robot actions. Our\ndreaming model can emulate samples equivalent to a sequence of images from the\nactual environment, technically by learning an action-conditioned future\nrepresentation/scene regressor. This allows the agent to learn action policies\n(i.e., visuomotor policies) by interacting with the dreaming model rather than\nthe real-world. We experimentally confirm that our dreaming model enables robot\nlearning of policies that transfer to the real-world. \n\n"}
{"id": "1805.07820", "contents": "Title: Targeted Adversarial Examples for Black Box Audio Systems Abstract: The application of deep recurrent networks to audio transcription has led to\nimpressive gains in automatic speech recognition (ASR) systems. Many have\ndemonstrated that small adversarial perturbations can fool deep neural networks\ninto incorrectly predicting a specified target with high confidence. Current\nwork on fooling ASR systems have focused on white-box attacks, in which the\nmodel architecture and parameters are known. In this paper, we adopt a\nblack-box approach to adversarial generation, combining the approaches of both\ngenetic algorithms and gradient estimation to solve the task. We achieve a\n89.25% targeted attack similarity after 3000 generations while maintaining\n94.6% audio file similarity. \n\n"}
{"id": "1805.08095", "contents": "Title: Small steps and giant leaps: Minimal Newton solvers for Deep Learning Abstract: We propose a fast second-order method that can be used as a drop-in\nreplacement for current deep learning solvers. Compared to stochastic gradient\ndescent (SGD), it only requires two additional forward-mode automatic\ndifferentiation operations per iteration, which has a computational cost\ncomparable to two standard forward passes and is easy to implement. Our method\naddresses long-standing issues with current second-order solvers, which invert\nan approximate Hessian matrix every iteration exactly or by conjugate-gradient\nmethods, a procedure that is both costly and sensitive to noise. Instead, we\npropose to keep a single estimate of the gradient projected by the inverse\nHessian matrix, and update it once per iteration. This estimate has the same\nsize and is similar to the momentum variable that is commonly used in SGD. No\nestimate of the Hessian is maintained. We first validate our method, called\nCurveBall, on small problems with known closed-form solutions (noisy Rosenbrock\nfunction and degenerate 2-layer linear networks), where current deep learning\nsolvers seem to struggle. We then train several large models on CIFAR and\nImageNet, including ResNet and VGG-f networks, where we demonstrate faster\nconvergence with no hyperparameter tuning. Code is available. \n\n"}
{"id": "1805.08751", "contents": "Title: FAKEDETECTOR: Effective Fake News Detection with Deep Diffusive Neural\n  Network Abstract: In recent years, due to the booming development of online social networks,\nfake news for various commercial and political purposes has been appearing in\nlarge numbers and widespread in the online world. With deceptive words, online\nsocial network users can get infected by these online fake news easily, which\nhas brought about tremendous effects on the offline society already. An\nimportant goal in improving the trustworthiness of information in online social\nnetworks is to identify the fake news timely. This paper aims at investigating\nthe principles, methodologies and algorithms for detecting fake news articles,\ncreators and subjects from online social networks and evaluating the\ncorresponding performance. This paper addresses the challenges introduced by\nthe unknown characteristics of fake news and diverse connections among news\narticles, creators and subjects. This paper introduces a novel automatic fake\nnews credibility inference model, namely FAKEDETECTOR. Based on a set of\nexplicit and latent features extracted from the textual information,\nFAKEDETECTOR builds a deep diffusive network model to learn the representations\nof news articles, creators and subjects simultaneously. Extensive experiments\nhave been done on a real-world fake news dataset to compare FAKEDETECTOR with\nseveral state-of-the-art models, and the experimental results have demonstrated\nthe effectiveness of the proposed model. \n\n"}
{"id": "1805.08956", "contents": "Title: Hypergraph Spectral Clustering in the Weighted Stochastic Block Model Abstract: Spectral clustering is a celebrated algorithm that partitions objects based\non pairwise similarity information. While this approach has been successfully\napplied to a variety of domains, it comes with limitations. The reason is that\nthere are many other applications in which only \\emph{multi}-way similarity\nmeasures are available. This motivates us to explore the multi-way measurement\nsetting. In this work, we develop two algorithms intended for such setting:\nHypergraph Spectral Clustering (HSC) and Hypergraph Spectral Clustering with\nLocal Refinement (HSCLR). Our main contribution lies in performance analysis of\nthe poly-time algorithms under a random hypergraph model, which we name the\nweighted stochastic block model, in which objects and multi-way measures are\nmodeled as nodes and weights of hyperedges, respectively. Denoting by $n$ the\nnumber of nodes, our analysis reveals the following: (1) HSC outputs a\npartition which is better than a random guess if the sum of edge weights (to be\nexplained later) is $\\Omega(n)$; (2) HSC outputs a partition which coincides\nwith the hidden partition except for a vanishing fraction of nodes if the sum\nof edge weights is $\\omega(n)$; and (3) HSCLR exactly recovers the hidden\npartition if the sum of edge weights is on the order of $n \\log n$. Our results\nimprove upon the state of the arts recently established under the model and\nthey firstly settle the order-wise optimal results for the binary edge weight\ncase. Moreover, we show that our results lead to efficient sketching algorithms\nfor subspace clustering, a computer vision application. Lastly, we show that\nHSCLR achieves the information-theoretic limits for a special yet practically\nrelevant model, thereby showing no computational barrier for the case. \n\n"}
{"id": "1805.09122", "contents": "Title: Probabilistic Riemannian submanifold learning with wrapped Gaussian\n  process latent variable models Abstract: Latent variable models (LVMs) learn probabilistic models of data manifolds\nlying in an \\emph{ambient} Euclidean space. In a number of applications, a\npriori known spatial constraints can shrink the ambient space into a\nconsiderably smaller manifold. Additionally, in these applications the\nEuclidean geometry might induce a suboptimal similarity measure, which could be\nimproved by choosing a different metric. Euclidean models ignore such\ninformation and assign probability mass to data points that can never appear as\ndata, and vastly different likelihoods to points that are similar under the\ndesired metric. We propose the wrapped Gaussian process latent variable model\n(WGPLVM), that extends Gaussian process latent variable models to take values\nstrictly on a given ambient Riemannian manifold, making the model blind to\nimpossible data points. This allows non-linear, probabilistic inference of\nlow-dimensional Riemannian submanifolds from data. Our evaluation on diverse\ndatasets show that we improve performance on several tasks, including encoding,\nvisualization and uncertainty quantification. \n\n"}
{"id": "1805.09294", "contents": "Title: Likelihood-free inference with emulator networks Abstract: Approximate Bayesian Computation (ABC) provides methods for Bayesian\ninference in simulation-based stochastic models which do not permit tractable\nlikelihoods. We present a new ABC method which uses probabilistic neural\nemulator networks to learn synthetic likelihoods on simulated data -- both\nlocal emulators which approximate the likelihood for specific observed data, as\nwell as global ones which are applicable to a range of data. Simulations are\nchosen adaptively using an acquisition function which takes into account\nuncertainty about either the posterior distribution of interest, or the\nparameters of the emulator. Our approach does not rely on user-defined\nrejection thresholds or distance functions. We illustrate inference with\nemulator networks on synthetic examples and on a biophysical neuron model, and\nshow that emulators allow accurate and efficient inference even on\nhigh-dimensional problems which are challenging for conventional ABC\napproaches. \n\n"}
{"id": "1805.10111", "contents": "Title: Double Quantization for Communication-Efficient Distributed Optimization Abstract: Modern distributed training of machine learning models suffers from high\ncommunication overhead for synchronizing stochastic gradients and model\nparameters. In this paper, to reduce the communication complexity, we propose\n\\emph{double quantization}, a general scheme for quantizing both model\nparameters and gradients. Three communication-efficient algorithms are proposed\nunder this general scheme. Specifically, (i) we propose a low-precision\nalgorithm AsyLPG with asynchronous parallelism, (ii) we explore integrating\ngradient sparsification with double quantization and develop Sparse-AsyLPG,\n(iii) we show that double quantization can also be accelerated by momentum\ntechnique and design accelerated AsyLPG. We establish rigorous performance\nguarantees for the algorithms, and conduct experiments on a multi-server\ntest-bed to demonstrate that our algorithms can effectively save transmitted\nbits without performance degradation. \n\n"}
{"id": "1805.10130", "contents": "Title: Cross Domain Image Generation through Latent Space Exploration with\n  Adversarial Loss Abstract: Conditional domain generation is a good way to interactively control sample\ngeneration process of deep generative models. However, once a conditional\ngenerative model has been created, it is often expensive to allow it to adapt\nto new conditional controls, especially the network structure is relatively\ndeep. We propose a conditioned latent domain transfer framework across latent\nspaces of unconditional variational autoencoders(VAE). With this framework, we\ncan allow unconditionally trained VAEs to generate images in its domain with\nconditionals provided by a latent representation of another domain. This\nframework does not assume commonalities between two domains. We demonstrate\neffectiveness and robustness of our model under widely used image datasets. \n\n"}
{"id": "1805.10886", "contents": "Title: Importance Weighted Transfer of Samples in Reinforcement Learning Abstract: We consider the transfer of experience samples (i.e., tuples < s, a, s', r >)\nin reinforcement learning (RL), collected from a set of source tasks to improve\nthe learning process in a given target task. Most of the related approaches\nfocus on selecting the most relevant source samples for solving the target\ntask, but then all the transferred samples are used without considering anymore\nthe discrepancies between the task models. In this paper, we propose a\nmodel-based technique that automatically estimates the relevance (importance\nweight) of each source sample for solving the target task. In the proposed\napproach, all the samples are transferred and used by a batch RL algorithm to\nsolve the target task, but their contribution to the learning process is\nproportional to their importance weight. By extending the results for\nimportance weighting provided in supervised learning literature, we develop a\nfinite-sample analysis of the proposed batch RL algorithm. Furthermore, we\nempirically compare the proposed algorithm to state-of-the-art approaches,\nshowing that it achieves better learning performance and is very robust to\nnegative transfer, even when some source tasks are significantly different from\nthe target task. \n\n"}
{"id": "1805.10915", "contents": "Title: Dirichlet-based Gaussian Processes for Large-scale Calibrated\n  Classification Abstract: In this paper, we study the problem of deriving fast and accurate\nclassification algorithms with uncertainty quantification. Gaussian process\nclassification provides a principled approach, but the corresponding\ncomputational burden is hardly sustainable in large-scale problems and devising\nefficient alternatives is a challenge. In this work, we investigate if and how\nGaussian process regression directly applied to the classification labels can\nbe used to tackle this question. While in this case training time is remarkably\nfaster, predictions need be calibrated for classification and uncertainty\nestimation. To this aim, we propose a novel approach based on interpreting the\nlabels as the output of a Dirichlet distribution. Extensive experimental\nresults show that the proposed approach provides essentially the same accuracy\nand uncertainty quantification of Gaussian process classification while\nrequiring only a fraction of computational resources. \n\n"}
{"id": "1805.11240", "contents": "Title: Truncated Horizon Policy Search: Combining Reinforcement Learning &\n  Imitation Learning Abstract: In this paper, we propose to combine imitation and reinforcement learning via\nthe idea of reward shaping using an oracle. We study the effectiveness of the\nnear-optimal cost-to-go oracle on the planning horizon and demonstrate that the\ncost-to-go oracle shortens the learner's planning horizon as function of its\naccuracy: a globally optimal oracle can shorten the planning horizon to one,\nleading to a one-step greedy Markov Decision Process which is much easier to\noptimize, while an oracle that is far away from the optimality requires\nplanning over a longer horizon to achieve near-optimal performance. Hence our\nnew insight bridges the gap and interpolates between imitation learning and\nreinforcement learning. Motivated by the above mentioned insights, we propose\nTruncated HORizon Policy Search (THOR), a method that focuses on searching for\npolicies that maximize the total reshaped reward over a finite planning horizon\nwhen the oracle is sub-optimal. We experimentally demonstrate that a\ngradient-based implementation of THOR can achieve superior performance compared\nto RL baselines and IL baselines even when the oracle is sub-optimal. \n\n"}
{"id": "1805.11324", "contents": "Title: Bayesian Inference with Anchored Ensembles of Neural Networks, and\n  Application to Exploration in Reinforcement Learning Abstract: The use of ensembles of neural networks (NNs) for the quantification of\npredictive uncertainty is widespread. However, the current justification is\nintuitive rather than analytical. This work proposes one minor modification to\nthe normal ensembling methodology, which we prove allows the ensemble to\nperform Bayesian inference, hence converging to the corresponding Gaussian\nProcess as both the total number of NNs, and the size of each, tend to\ninfinity. This working paper provides early-stage results in a reinforcement\nlearning setting, analysing the practicality of the technique for an ensemble\nof small, finite number. Using the uncertainty estimates produced by anchored\nensembles to govern the exploration-exploitation process results in steadier,\nmore stable learning. \n\n"}
{"id": "1805.11686", "contents": "Title: Variational Inverse Control with Events: A General Framework for\n  Data-Driven Reward Definition Abstract: The design of a reward function often poses a major practical challenge to\nreal-world applications of reinforcement learning. Approaches such as inverse\nreinforcement learning attempt to overcome this challenge, but require expert\ndemonstrations, which can be difficult or expensive to obtain in practice. We\npropose variational inverse control with events (VICE), which generalizes\ninverse reinforcement learning methods to cases where full demonstrations are\nnot needed, such as when only samples of desired goal states are available. Our\nmethod is grounded in an alternative perspective on control and reinforcement\nlearning, where an agent's goal is to maximize the probability that one or more\nevents will happen at some point in the future, rather than maximizing\ncumulative rewards. We demonstrate the effectiveness of our methods on\ncontinuous control tasks, with a focus on high-dimensional observations like\nimages where rewards are hard or even impossible to specify. \n\n"}
{"id": "1805.11711", "contents": "Title: Depth and nonlinearity induce implicit exploration for RL Abstract: The question of how to explore, i.e., take actions with uncertain outcomes to\nlearn about possible future rewards, is a key question in reinforcement\nlearning (RL). Here, we show a surprising result: We show that Q-learning with\nnonlinear Q-function and no explicit exploration (i.e., a purely greedy policy)\ncan learn several standard benchmark tasks, including mountain car, equally\nwell as, or better than, the most commonly-used $\\epsilon$-greedy exploration.\nWe carefully examine this result and show that both the depth of the Q-network\nand the type of nonlinearity are important to induce such deterministic\nexploration. \n\n"}
{"id": "1805.11770", "contents": "Title: AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for\n  Attacking Black-box Neural Networks Abstract: Recent studies have shown that adversarial examples in state-of-the-art image\nclassifiers trained by deep neural networks (DNN) can be easily generated when\nthe target model is transparent to an attacker, known as the white-box setting.\nHowever, when attacking a deployed machine learning service, one can only\nacquire the input-output correspondences of the target model; this is the\nso-called black-box attack setting. The major drawback of existing black-box\nattacks is the need for excessive model queries, which may give a false sense\nof model robustness due to inefficient query designs. To bridge this gap, we\npropose a generic framework for query-efficient black-box attacks. Our\nframework, AutoZOOM, which is short for Autoencoder-based Zeroth Order\nOptimization Method, has two novel building blocks towards efficient black-box\nattacks: (i) an adaptive random gradient estimation strategy to balance query\ncounts and distortion, and (ii) an autoencoder that is either trained offline\nwith unlabeled data or a bilinear resizing operation for attack acceleration.\nExperimental results suggest that, by applying AutoZOOM to a state-of-the-art\nblack-box attack (ZOO), a significant reduction in model queries can be\nachieved without sacrificing the attack success rate and the visual quality of\nthe resulting adversarial examples. In particular, when compared to the\nstandard ZOO method, AutoZOOM can consistently reduce the mean query counts in\nfinding successful adversarial examples (or reaching the same distortion level)\nby at least 93% on MNIST, CIFAR-10 and ImageNet datasets, leading to novel\ninsights on adversarial robustness. \n\n"}
{"id": "1805.11792", "contents": "Title: Tight Regret Bounds for Bayesian Optimization in One Dimension Abstract: We consider the problem of Bayesian optimization (BO) in one dimension, under\na Gaussian process prior and Gaussian sampling noise. We provide a theoretical\nanalysis showing that, under fairly mild technical assumptions on the kernel,\nthe best possible cumulative regret up to time $T$ behaves as\n$\\Omega(\\sqrt{T})$ and $O(\\sqrt{T\\log T})$. This gives a tight characterization\nup to a $\\sqrt{\\log T}$ factor, and includes the first non-trivial lower bound\nfor noisy BO. Our assumptions are satisfied, for example, by the squared\nexponential and Mat\\'ern-$\\nu$ kernels, with the latter requiring $\\nu > 2$.\nOur results certify the near-optimality of existing bounds (Srinivas {\\em et\nal.}, 2009) for the SE kernel, while proving them to be strictly suboptimal for\nthe Mat\\'ern kernel with $\\nu > 2$. \n\n"}
{"id": "1806.00250", "contents": "Title: TAPAS: Train-less Accuracy Predictor for Architecture Search Abstract: In recent years an increasing number of researchers and practitioners have\nbeen suggesting algorithms for large-scale neural network architecture search:\ngenetic algorithms, reinforcement learning, learning curve extrapolation, and\naccuracy predictors. None of them, however, demonstrated high-performance\nwithout training new experiments in the presence of unseen datasets. We propose\na new deep neural network accuracy predictor, that estimates in fractions of a\nsecond classification performance for unseen input datasets, without training.\nIn contrast to previously proposed approaches, our prediction is not only\ncalibrated on the topological network information, but also on the\ncharacterization of the dataset-difficulty which allows us to re-tune the\nprediction without any training. Our predictor achieves a performance which\nexceeds 100 networks per second on a single GPU, thus creating the opportunity\nto perform large-scale architecture search within a few minutes. We present\nresults of two searches performed in 400 seconds on a single GPU. Our best\ndiscovered networks reach 93.67% accuracy for CIFAR-10 and 81.01% for\nCIFAR-100, verified by training. These networks are performance competitive\nwith other automatically discovered state-of-the-art networks however we only\nneeded a small fraction of the time to solution and computational resources. \n\n"}
{"id": "1806.01159", "contents": "Title: Efficient and Scalable Batch Bayesian Optimization Using K-Means Abstract: We present K-Means Batch Bayesian Optimization (KMBBO), a novel batch\nsampling algorithm for Bayesian Optimization (BO). KMBBO uses unsupervised\nlearning to efficiently estimate peaks of the model acquisition function. We\nshow in empirical experiments that our method outperforms the current\nstate-of-the-art batch allocation algorithms on a variety of test problems\nincluding tuning of algorithm hyper-parameters and a challenging drug discovery\nproblem. In order to accommodate the real-world problem of high dimensional\ndata, we propose a modification to KMBBO by combining it with compressed\nsensing to project the optimization into a lower dimensional subspace. We\ndemonstrate empirically that this 2-step method outperforms algorithms where no\ndimensionality reduction has taken place. \n\n"}
{"id": "1806.01754", "contents": "Title: Neural-Kernelized Conditional Density Estimation Abstract: Conditional density estimation is a general framework for solving various\nproblems in machine learning. Among existing methods, non-parametric and/or\nkernel-based methods are often difficult to use on large datasets, while\nmethods based on neural networks usually make restrictive parametric\nassumptions on the probability densities. Here, we propose a novel method for\nestimating the conditional density based on score matching. In contrast to\nexisting methods, we employ scalable neural networks, but do not make explicit\nparametric assumptions on densities. The key challenge in applying score\nmatching to neural networks is computation of the first- and second-order\nderivatives of a model for the log-density. We tackle this challenge by\ndeveloping a new neural-kernelized approach, which can be applied on large\ndatasets with stochastic gradient descent, while the reproducing kernels allow\nfor easy computation of the derivatives needed in score matching. We show that\nthe neural-kernelized function approximator has universal approximation\ncapability and that our method is consistent in conditional density estimation.\nWe numerically demonstrate that our method is useful in high-dimensional\nconditional density estimation, and compares favourably with existing methods.\nFinally, we prove that the proposed method has interesting connections to two\nprobabilistically principled frameworks of representation learning: Nonlinear\nsufficient dimension reduction and nonlinear independent component analysis. \n\n"}
{"id": "1806.01811", "contents": "Title: AdaGrad stepsizes: Sharp convergence over nonconvex landscapes Abstract: Adaptive gradient methods such as AdaGrad and its variants update the\nstepsize in stochastic gradient descent on the fly according to the gradients\nreceived along the way; such methods have gained widespread use in large-scale\noptimization for their ability to converge robustly, without the need to\nfine-tune the stepsize schedule. Yet, the theoretical guarantees to date for\nAdaGrad are for online and convex optimization. We bridge this gap by providing\ntheoretical guarantees for the convergence of AdaGrad for smooth, nonconvex\nfunctions. We show that the norm version of AdaGrad (AdaGrad-Norm) converges to\na stationary point at the $\\mathcal{O}(\\log(N)/\\sqrt{N})$ rate in the\nstochastic setting, and at the optimal $\\mathcal{O}(1/N)$ rate in the batch\n(non-stochastic) setting -- in this sense, our convergence guarantees are\n'sharp'. In particular, the convergence of AdaGrad-Norm is robust to the choice\nof all hyper-parameters of the algorithm, in contrast to stochastic gradient\ndescent whose convergence depends crucially on tuning the step-size to the\n(generally unknown) Lipschitz smoothness constant and level of stochastic noise\non the gradient. Extensive numerical experiments are provided to corroborate\nour theory; moreover, the experiments suggest that the robustness of\nAdaGrad-Norm extends to state-of-the-art models in deep learning, without\nsacrificing generalization. \n\n"}
{"id": "1806.01969", "contents": "Title: Reverse iterative volume sampling for linear regression Abstract: We study the following basic machine learning task: Given a fixed set of\n$d$-dimensional input points for a linear regression problem, we wish to\npredict a hidden response value for each of the points. We can only afford to\nattain the responses for a small subset of the points that are then used to\nconstruct linear predictions for all points in the dataset. The performance of\nthe predictions is evaluated by the total square loss on all responses (the\nattained as well as the hidden ones). We show that a good approximate solution\nto this least squares problem can be obtained from just dimension $d$ many\nresponses by using a joint sampling technique called volume sampling. Moreover,\nthe least squares solution obtained for the volume sampled subproblem is an\nunbiased estimator of optimal solution based on all n responses. This\nunbiasedness is a desirable property that is not shared by other common subset\nselection techniques.\n  Motivated by these basic properties, we develop a theoretical framework for\nstudying volume sampling, resulting in a number of new matrix expectation\nequalities and statistical guarantees which are of importance not only to least\nsquares regression but also to numerical linear algebra in general. Our methods\nalso lead to a regularized variant of volume sampling, and we propose the first\nefficient algorithms for volume sampling which make this technique a practical\ntool in the machine learning toolbox. Finally, we provide experimental evidence\nwhich confirms our theoretical findings. \n\n"}
{"id": "1806.02813", "contents": "Title: Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement\n  Learning with Trajectory Embeddings Abstract: In this work, we take a representation learning perspective on hierarchical\nreinforcement learning, where the problem of learning lower layers in a\nhierarchy is transformed into the problem of learning trajectory-level\ngenerative models. We show that we can learn continuous latent representations\nof trajectories, which are effective in solving temporally extended and\nmulti-stage problems. Our proposed model, SeCTAR, draws inspiration from\nvariational autoencoders, and learns latent representations of trajectories. A\nkey component of this method is to learn both a latent-conditioned policy and a\nlatent-conditioned model which are consistent with each other. Given the same\nlatent, the policy generates a trajectory which should match the trajectory\npredicted by the model. This model provides a built-in prediction mechanism, by\npredicting the outcome of closed loop policy behavior. We propose a novel\nalgorithm for performing hierarchical RL with this model, combining model-based\nplanning in the learned latent space with an unsupervised exploration\nobjective. We show that our model is effective at reasoning over long horizons\nwith sparse rewards for several simulated tasks, outperforming standard\nreinforcement learning methods and prior methods for hierarchical reasoning,\nmodel-based planning, and exploration. \n\n"}
{"id": "1806.03417", "contents": "Title: Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic\n  Geometry Abstract: We are concerned with the discovery of hierarchical relationships from\nlarge-scale unstructured similarity scores. For this purpose, we study\ndifferent models of hyperbolic space and find that learning embeddings in the\nLorentz model is substantially more efficient than in the Poincar\\'e-ball\nmodel. We show that the proposed approach allows us to learn high-quality\nembeddings of large taxonomies which yield improvements over Poincar\\'e\nembeddings, especially in low dimensions. Lastly, we apply our model to\ndiscover hierarchies in two real-world datasets: we show that an embedding in\nhyperbolic space can reveal important aspects of a company's organizational\nstructure as well as reveal historical relationships between language families. \n\n"}
{"id": "1806.03536", "contents": "Title: Representation Learning on Graphs with Jumping Knowledge Networks Abstract: Recent deep learning approaches for representation learning on graphs follow\na neighborhood aggregation procedure. We analyze some important properties of\nthese models, and propose a strategy to overcome those. In particular, the\nrange of \"neighboring\" nodes that a node's representation draws from strongly\ndepends on the graph structure, analogous to the spread of a random walk. To\nadapt to local neighborhood properties and tasks, we explore an architecture --\njumping knowledge (JK) networks -- that flexibly leverages, for each node,\ndifferent neighborhood ranges to enable better structure-aware representation.\nIn a number of experiments on social, bioinformatics and citation networks, we\ndemonstrate that our model achieves state-of-the-art performance. Furthermore,\ncombining the JK framework with models like Graph Convolutional Networks,\nGraphSAGE and Graph Attention Networks consistently improves those models'\nperformance. \n\n"}
{"id": "1806.03832", "contents": "Title: Covariance matrix entanglement criterion for an arbitrary set of\n  operators Abstract: We generalize entanglement detection with covariance matrices for an\narbitrary set of observables. A generalized uncertainty relation is constructed\nusing the covariance and commutation matrices, then a criterion is established\nby performing a partial transposition on the operators. The method is highly\nefficient and versatile in the sense that the set of measurement operators can\nbe freely chosen, do not need to be complete, and there is no constraint on the\ncommutation relations. The method is particularly suited for systems with\nhigher dimensionality since the computations do not scale with the dimension of\nthe Hilbert space rather they scale with the number of chosen observables which\ncan always be kept small. We illustrate the approach by examining the\nentanglement between two spin ensembles, and show that it detects entanglement\nin a basis independent way. \n\n"}
{"id": "1806.04326", "contents": "Title: Differentiable Compositional Kernel Learning for Gaussian Processes Abstract: The generalization properties of Gaussian processes depend heavily on the\nchoice of kernel, and this choice remains a dark art. We present the Neural\nKernel Network (NKN), a flexible family of kernels represented by a neural\nnetwork. The NKN architecture is based on the composition rules for kernels, so\nthat each unit of the network corresponds to a valid kernel. It can compactly\napproximate compositional kernel structures such as those used by the Automatic\nStatistician (Lloyd et al., 2014), but because the architecture is\ndifferentiable, it is end-to-end trainable with gradient-based optimization. We\nshow that the NKN is universal for the class of stationary kernels. Empirically\nwe demonstrate pattern discovery and extrapolation abilities of NKN on several\ntasks that depend crucially on identifying the underlying structure, including\ntime series and texture extrapolation, as well as Bayesian optimization. \n\n"}
{"id": "1806.04819", "contents": "Title: Integral Privacy for Sampling Abstract: Differential privacy is a leading protection setting, focused by design on\nindividual privacy. Many applications, in medical / pharmaceutical domains or\nsocial networks, rather posit privacy at a group level, a setting we call\nintegral privacy. We aim for the strongest form of privacy: the group size is\nin particular not known in advance. We study a problem with related\napplications in domains cited above that have recently met with substantial\nrecent press: sampling.\n  Keeping correct utility levels in such a strong model of statistical\nindistinguishability looks difficult to be achieved with the usual differential\nprivacy toolbox because it would typically scale in the worst case the\nsensitivity by the sample size and so the noise variance by up to its square.\nWe introduce a trick specific to sampling that bypasses the sensitivity\nanalysis. Privacy enforces an information theoretic barrier on approximation,\nand we show how to reach this barrier with guarantees on the approximation of\nthe target non private density. We do so using a recent approach to non private\ndensity estimation relying on the original boosting theory, learning the\nsufficient statistics of an exponential family with classifiers. Approximation\nguarantees cover the mode capture problem. In the context of learning, the\nsampling problem is particularly important: because integral privacy enjoys the\nsame closure under post-processing as differential privacy does, any algorithm\nusing integrally privacy sampled data would result in an output equally\nintegrally private. We also show that this brings fairness guarantees on\npost-processing that would eventually elude classical differential privacy: any\ndecision process has bounded data-dependent bias when the data is integrally\nprivately sampled. Experimental results against private kernel density\nestimation and private GANs displays the quality of our results. \n\n"}
{"id": "1806.05393", "contents": "Title: Dynamical Isometry and a Mean Field Theory of CNNs: How to Train\n  10,000-Layer Vanilla Convolutional Neural Networks Abstract: In recent years, state-of-the-art methods in computer vision have utilized\nincreasingly deep convolutional neural network architectures (CNNs), with some\nof the most successful models employing hundreds or even thousands of layers. A\nvariety of pathologies such as vanishing/exploding gradients make training such\ndeep networks challenging. While residual connections and batch normalization\ndo enable training at these depths, it has remained unclear whether such\nspecialized architecture designs are truly necessary to train deep CNNs. In\nthis work, we demonstrate that it is possible to train vanilla CNNs with ten\nthousand layers or more simply by using an appropriate initialization scheme.\nWe derive this initialization scheme theoretically by developing a mean field\ntheory for signal propagation and by characterizing the conditions for\ndynamical isometry, the equilibration of singular values of the input-output\nJacobian matrix. These conditions require that the convolution operator be an\northogonal transformation in the sense that it is norm-preserving. We present\nan algorithm for generating such random initial orthogonal convolution kernels\nand demonstrate empirically that they enable efficient training of extremely\ndeep architectures. \n\n"}
{"id": "1806.05490", "contents": "Title: Inference in Deep Gaussian Processes using Stochastic Gradient\n  Hamiltonian Monte Carlo Abstract: Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian\nProcesses that combine well calibrated uncertainty estimates with the high\nflexibility of multilayer models. One of the biggest challenges with these\nmodels is that exact inference is intractable. The current state-of-the-art\ninference method, Variational Inference (VI), employs a Gaussian approximation\nto the posterior distribution. This can be a potentially poor unimodal\napproximation of the generally multimodal posterior. In this work, we provide\nevidence for the non-Gaussian nature of the posterior and we apply the\nStochastic Gradient Hamiltonian Monte Carlo method to generate samples. To\nefficiently optimize the hyperparameters, we introduce the Moving Window MCEM\nalgorithm. This results in significantly better predictions at a lower\ncomputational cost than its VI counterpart. Thus our method establishes a new\nstate-of-the-art for inference in DGPs. \n\n"}
{"id": "1806.05730", "contents": "Title: Learning Influence-Receptivity Network Structure with Guarantee Abstract: Traditional works on community detection from observations of information\ncascade assume that a single adjacency matrix parametrizes all the observed\ncascades. However, in reality the connection structure usually does not stay\nthe same across cascades. For example, different people have different topics\nof interest, therefore the connection structure depends on the\ninformation/topic content of the cascade. In this paper we consider the case\nwhere we observe a sequence of noisy adjacency matrices triggered by\ninformation/event with different topic distributions. We propose a novel latent\nmodel using the intuition that a connection is more likely to exist between two\nnodes if they are interested in similar topics, which are common with the\ninformation/event. Specifically, we endow each node with two node-topic\nvectors: an influence vector that measures how influential/authoritative they\nare on each topic; and a receptivity vector that measures how\nreceptive/susceptible they are to each topic. We show how these two node-topic\nstructures can be estimated from observed adjacency matrices with theoretical\nguarantee on estimation error, in cases where the topic distributions of the\ninformation/event are known, as well as when they are unknown. Experiments on\nsynthetic and real data demonstrate the effectiveness of our model and superior\nperformance compared to state-of-the-art methods. \n\n"}
{"id": "1806.06392", "contents": "Title: Task-Relevant Object Discovery and Categorization for Playing\n  First-person Shooter Games Abstract: We consider the problem of learning to play first-person shooter (FPS) video\ngames using raw screen images as observations and keyboard inputs as actions.\nThe high-dimensionality of the observations in this type of applications leads\nto prohibitive needs of training data for model-free methods, such as the deep\nQ-network (DQN), and its recurrent variant DRQN. Thus, recent works focused on\nlearning low-dimensional representations that may reduce the need for data.\nThis paper presents a new and efficient method for learning such\nrepresentations. Salient segments of consecutive frames are detected from their\noptical flow, and clustered based on their feature descriptors. The clusters\ntypically correspond to different discovered categories of objects. Segments\ndetected in new frames are then classified based on their nearest clusters.\nBecause only a few categories are relevant to a given task, the importance of a\ncategory is defined as the correlation between its occurrence and the agent's\nperformance. The result is encoded as a vector indicating objects that are in\nthe frame and their locations, and used as a side input to DRQN. Experiments on\nthe game Doom provide a good evidence for the benefit of this approach. \n\n"}
{"id": "1806.06950", "contents": "Title: GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model\n  Shrinking Abstract: Model compression is essential for serving large deep neural nets on devices\nwith limited resources or applications that require real-time responses. As a\ncase study, a state-of-the-art neural language model usually consists of one or\nmore recurrent layers sandwiched between an embedding layer used for\nrepresenting input tokens and a softmax layer for generating output tokens. For\nproblems with a very large vocabulary size, the embedding and the softmax\nmatrices can account for more than half of the model size. For instance, the\nbigLSTM model achieves state-of- the-art performance on the One-Billion-Word\n(OBW) dataset with around 800k vocabulary, and its word embedding and softmax\nmatrices use more than 6GBytes space, and are responsible for over 90% of the\nmodel parameters. In this paper, we propose GroupReduce, a novel compression\nmethod for neural language models, based on vocabulary-partition (block) based\nlow-rank matrix approximation and the inherent frequency distribution of tokens\n(the power-law distribution of words). The experimental results show our method\ncan significantly outperform traditional compression methods such as low-rank\napproximation and pruning. On the OBW dataset, our method achieved 6.6 times\ncompression rate for the embedding and softmax matrices, and when combined with\nquantization, our method can achieve 26 times compression rate, which\ntranslates to a factor of 12.8 times compression for the entire model with very\nlittle degradation in perplexity. \n\n"}
{"id": "1806.07307", "contents": "Title: Estimation from Non-Linear Observations via Convex Programming with\n  Application to Bilinear Regression Abstract: We propose a computationally efficient estimator, formulated as a convex\nprogram, for a broad class of non-linear regression problems that involve\ndifference of convex (DC) non-linearities. The proposed method can be viewed as\na significant extension of the \"anchored regression\" method formulated and\nanalyzed in [10] for regression with convex non-linearities. Our main\nassumption, in addition to other mild statistical and computational\nassumptions, is availability of a certain approximation oracle for the average\nof the gradients of the observation functions at a ground truth. Under this\nassumption and using a PAC-Bayesian analysis we show that the proposed\nestimator produces an accurate estimate with high probability. As a concrete\nexample, we study the proposed framework in the bilinear regression problem\nwith Gaussian factors and quantify a sufficient sample complexity for exact\nrecovery. Furthermore, we describe a computationally tractable scheme that\nprovably produces the required approximation oracle in the considered bilinear\nregression problem. \n\n"}
{"id": "1806.07506", "contents": "Title: A Simple Fusion of Deep and Shallow Learning for Acoustic Scene\n  Classification Abstract: In the past, Acoustic Scene Classification systems have been based on hand\ncrafting audio features that are input to a classifier. Nowadays, the common\ntrend is to adopt data driven techniques, e.g., deep learning, where audio\nrepresentations are learned from data. In this paper, we propose a system that\nconsists of a simple fusion of two methods of the aforementioned types: a deep\nlearning approach where log-scaled mel-spectrograms are input to a\nconvolutional neural network, and a feature engineering approach, where a\ncollection of hand-crafted features is input to a gradient boosting machine. We\nfirst show that both methods provide complementary information to some extent.\nThen, we use a simple late fusion strategy to combine both methods. We report\nclassification accuracy of each method individually and the combined system on\nthe TUT Acoustic Scenes 2017 dataset. The proposed fused system outperforms\neach of the individual methods and attains a classification accuracy of 72.8%\non the evaluation set, improving the baseline system by 11.8%. \n\n"}
{"id": "1806.08834", "contents": "Title: Smart Inverter Grid Probing for Learning Loads: Part I - Identifiability\n  Analysis Abstract: Distribution grids currently lack comprehensive real-time metering.\nNevertheless, grid operators require precise knowledge of loads and renewable\ngeneration to accomplish any feeder optimization task. At the same time, new\ngrid technologies, such as solar photovoltaics and energy storage units are\ninterfaced via inverters with advanced sensing and actuation capabilities. In\nthis context, this two-part work puts forth the idea of engaging power\nelectronics to probe an electric grid and record its voltage response at\nactuated and metered buses, to infer non-metered loads. Probing can be\naccomplished by commanding inverters to momentarily perturb their power\ninjections. Multiple probing actions can be induced within a few tens of\nseconds. In Part I, load inference via grid probing is formulated as an\nimplicit nonlinear system identification task, which is shown to be\ntopologically observable under certain conditions. The conditions can be\nreadily checked upon solving a max-flow problem on a bipartite graph derived\nfrom the feeder topology and the placement of actuated and non-metered buses.\nThe analysis holds for single- and multi-phase grids, radial or meshed, and\napplies to phasor or magnitude-only voltage data. The topological observability\nof distribution systems using smart meter or phasor data is cast and analyzed a\nspecial case. \n\n"}
{"id": "1806.08867", "contents": "Title: xGEMs: Generating Examplars to Explain Black-Box Models Abstract: This work proposes xGEMs or manifold guided exemplars, a framework to\nunderstand black-box classifier behavior by exploring the landscape of the\nunderlying data manifold as data points cross decision boundaries. To do so, we\ntrain an unsupervised implicit generative model -- treated as a proxy to the\ndata manifold. We summarize black-box model behavior quantitatively by\nperturbing data samples along the manifold. We demonstrate xGEMs' ability to\ndetect and quantify bias in model learning and also for understanding the\nchanges in model behavior as training progresses. \n\n"}
{"id": "1806.09614", "contents": "Title: Accuracy-based Curriculum Learning in Deep Reinforcement Learning Abstract: In this paper, we investigate a new form of automated curriculum learning\nbased on adaptive selection of accuracy requirements, called accuracy-based\ncurriculum learning. Using a reinforcement learning agent based on the Deep\nDeterministic Policy Gradient algorithm and addressing the Reacher environment,\nwe first show that an agent trained with various accuracy requirements sampled\nrandomly learns more efficiently than when asked to be very accurate at all\ntimes. Then we show that adaptive selection of accuracy requirements, based on\na local measure of competence progress, automatically generates a curriculum\nwhere difficulty progressively increases, resulting in a better learning\nefficiency than sampling randomly. \n\n"}
{"id": "1806.09777", "contents": "Title: On the Implicit Bias of Dropout Abstract: Algorithmic approaches endow deep learning systems with implicit bias that\nhelps them generalize even in over-parametrized settings. In this paper, we\nfocus on understanding such a bias induced in learning through dropout, a\npopular technique to avoid overfitting in deep learning. For single\nhidden-layer linear neural networks, we show that dropout tends to make the\nnorm of incoming/outgoing weight vectors of all the hidden nodes equal. In\naddition, we provide a complete characterization of the optimization landscape\ninduced by dropout. \n\n"}
{"id": "1806.10040", "contents": "Title: Crowd Counting with Density Adaption Networks Abstract: Crowd counting is one of the core tasks in various surveillance applications.\nA practical system involves estimating accurate head counts in dynamic\nscenarios under different lightning, camera perspective and occlusion states.\nPrevious approaches estimate head counts despite that they can vary\ndramatically in different density settings; the crowd is often unevenly\ndistributed and the results are therefore unsatisfactory. In this paper, we\npropose a lightweight deep learning framework that can automatically estimate\nthe crowd density level and adaptively choose between different counter\nnetworks that are explicitly trained for different density domains. Experiments\non two recent crowd counting datasets, UCF_CC_50 and ShanghaiTech, show that\nthe proposed mechanism achieves promising improvements over state-of-the-art\nmethods. Moreover, runtime speed is 20 FPS on a single GPU. \n\n"}
{"id": "1806.10268", "contents": "Title: Fixed points and emergent topological phenomena in a\n  parity-time-symmetric quantum quench Abstract: We identify emergent topological phenomena such as dynamic Chern numbers and\ndynamic quantum phase transitions in quantum quenches of the non-Hermitian\nSu-Schrieffer-Heeger Hamiltonian with parity-time ($\\mathcal{PT}$) symmetry.\nTheir occurrence in the non-unitary dynamics are intimately connected with\nfixed points in the Brillouin zone, where the states do not evolve in time. We\nconstruct a theoretical formalism for characterizing topological properties in\nnon-unitary dynamics within the framework of biorthogonal quantum mechanics,\nand prove the existence of fixed points for quenches between distinct static\ntopological phases in the $\\mathcal{PT}$-symmetry-preserving regime. We then\nreveal the interesting relation between different dynamic topological phenomena\nthrough the momentum-time spin texture characterizing the dynamic process. For\nquenches involving Hamiltonians in the $\\mathcal{PT}$-symmetry-broken regime,\nthese topological phenomena are not ensured. \n\n"}
{"id": "1806.10448", "contents": "Title: Learning Simon's quantum algorithm Abstract: We consider whether trainable quantum unitaries can be used to discover\nquantum speed-ups for classical problems. Using methods recently developed for\ntraining quantum neural nets, we consider Simon's problem, for which there is a\nknown quantum algorithm which performs exponentially faster in the number of\nbits, relative to the best known classical algorithm. We give the problem to a\nrandomly chosen but trainable unitary circuit, and find that the training\nrecovers Simon's algorithm as hoped. \n\n"}
{"id": "1806.10758", "contents": "Title: A Benchmark for Interpretability Methods in Deep Neural Networks Abstract: We propose an empirical measure of the approximate accuracy of feature\nimportance estimates in deep neural networks. Our results across several\nlarge-scale image classification datasets show that many popular\ninterpretability methods produce estimates of feature importance that are not\nbetter than a random designation of feature importance. Only certain ensemble\nbased approaches---VarGrad and SmoothGrad-Squared---outperform such a random\nassignment of importance. The manner of ensembling remains critical, we show\nthat some approaches do no better then the underlying method but carry a far\nhigher computational burden. \n\n"}
{"id": "1806.11212", "contents": "Title: Proxy Fairness Abstract: We consider the problem of improving fairness when one lacks access to a\ndataset labeled with protected groups, making it difficult to take advantage of\nstrategies that can improve fairness but require protected group labels, either\nat training or runtime. To address this, we investigate improving fairness\nmetrics for proxy groups, and test whether doing so results in improved\nfairness for the true sensitive groups. Results on benchmark and real-world\ndatasets demonstrate that such a proxy fairness strategy can work well in\npractice. However, we caution that the effectiveness likely depends on the\nchoice of fairness metric, as well as how aligned the proxy groups are with the\ntrue protected groups in terms of the constrained model parameters. \n\n"}
{"id": "1807.00042", "contents": "Title: Neural Networks Trained to Solve Differential Equations Learn General\n  Representations Abstract: We introduce a technique based on the singular vector canonical correlation\nanalysis (SVCCA) for measuring the generality of neural network layers across a\ncontinuously-parametrized set of tasks. We illustrate this method by studying\ngenerality in neural networks trained to solve parametrized boundary value\nproblems based on the Poisson partial differential equation. We find that the\nfirst hidden layer is general, and that deeper layers are successively more\nspecific. Next, we validate our method against an existing technique that\nmeasures layer generality using transfer learning experiments. We find\nexcellent agreement between the two methods, and note that our method is much\nfaster, particularly for continuously-parametrized problems. Finally, we\nvisualize the general representations of the first layers, and interpret them\nas generalized coordinates over the input domain. \n\n"}
{"id": "1807.00051", "contents": "Title: Adversarial Examples in Deep Learning: Characterization and Divergence Abstract: The burgeoning success of deep learning has raised the security and privacy\nconcerns as more and more tasks are accompanied with sensitive data.\nAdversarial attacks in deep learning have emerged as one of the dominating\nsecurity threat to a range of mission-critical deep learning systems and\napplications. This paper takes a holistic and principled approach to perform\nstatistical characterization of adversarial examples in deep learning. We\nprovide a general formulation of adversarial examples and elaborate on the\nbasic principle for adversarial attack algorithm design. We introduce easy and\nhard categorization of adversarial attacks to analyze the effectiveness of\nadversarial examples in terms of attack success rate, degree of change in\nadversarial perturbation, average entropy of prediction qualities, and fraction\nof adversarial examples that lead to successful attacks. We conduct extensive\nexperimental study on adversarial behavior in easy and hard attacks under deep\nlearning models with different hyperparameters and different deep learning\nframeworks. We show that the same adversarial attack behaves differently under\ndifferent hyperparameters and across different frameworks due to the different\nfeatures learned under different deep learning model training process. Our\nstatistical characterization with strong empirical evidence provides a\ntransformative enlightenment on mitigation strategies towards effective\ncountermeasures against present and future adversarial attacks. \n\n"}
{"id": "1807.00598", "contents": "Title: A Pulmonary Nodule Detection Model Based on Progressive Resolution and\n  Hierarchical Saliency Abstract: Detection of pulmonary nodules on chest CT is an essential step in the early\ndiagnosis of lung cancer, which is critical for best patient care. Although a\nnumber of computer-aided nodule detection methods have been published in the\nliterature, these methods still have two major drawbacks: missing out true\nnodules during the detection of nodule candidates and less-accurate\nidentification of nodules from non-nodule. In this paper, we propose an\nautomated pulmonary nodule detection algorithm that jointly combines\nprogressive resolution and hierarchical saliency. Specifically, we design a 3D\nprogressive resolution-based densely dilated FCN, namely the progressive\nresolution network (PRN), to detect nodule candidates inside the lung, and\nconstruct a densely dilated 3D CNN with hierarchical saliency, namely the\nhierarchical saliency network (HSN), to simultaneously identify genuine nodules\nfrom those candidates and estimate the diameters of nodules. We evaluated our\nalgorithm on the benchmark LUng Nodule Analysis 2016 (LUNA16) dataset and\nachieved a state-of-the-art detection score. Our results suggest that the\nproposed algorithm can effectively detect pulmonary nodules on chest CT and\naccurately estimate their diameters. \n\n"}
{"id": "1807.01281", "contents": "Title: Human-level performance in first-person multiplayer games with\n  population-based deep reinforcement learning Abstract: Recent progress in artificial intelligence through reinforcement learning\n(RL) has shown great success on increasingly complex single-agent environments\nand two-player turn-based games. However, the real-world contains multiple\nagents, each learning and acting independently to cooperate and compete with\nother agents, and environments reflecting this degree of complexity remain an\nopen challenge. In this work, we demonstrate for the first time that an agent\ncan achieve human-level in a popular 3D multiplayer first-person video game,\nQuake III Arena Capture the Flag, using only pixels and game points as input.\nThese results were achieved by a novel two-tier optimisation process in which a\npopulation of independent RL agents are trained concurrently from thousands of\nparallel matches with agents playing in teams together and against each other\non randomly generated environments. Each agent in the population learns its own\ninternal reward signal to complement the sparse delayed reward from winning,\nand selects actions using a novel temporally hierarchical representation that\nenables the agent to reason at multiple timescales. During game-play, these\nagents display human-like behaviours such as navigating, following, and\ndefending based on a rich learned representation that is shown to encode\nhigh-level game knowledge. In an extensive tournament-style evaluation the\ntrained agents exceeded the win-rate of strong human players both as teammates\nand opponents, and proved far stronger than existing state-of-the-art agents.\nThese results demonstrate a significant jump in the capabilities of artificial\nagents, bringing us closer to the goal of human-level intelligence. \n\n"}
{"id": "1807.01972", "contents": "Title: Beef Cattle Instance Segmentation Using Fully Convolutional Neural\n  Network Abstract: We present an instance segmentation algorithm trained and applied to a CCTV\nrecording of beef cattle during a winter finishing period. A fully\nconvolutional network was transformed into an instance segmentation network\nthat learns to label each instance of an animal separately. We introduce a\nconceptually simple framework that the network uses to output a single\nprediction for every animal. These results are a contribution towards behaviour\nanalysis in winter finishing beef cattle for early detection of animal\nwelfare-related problems. \n\n"}
{"id": "1807.02188", "contents": "Title: Implicit Generative Modeling of Random Noise during Training for\n  Adversarial Robustness Abstract: We introduce a Noise-based prior Learning (NoL) approach for training neural\nnetworks that are intrinsically robust to adversarial attacks. We find that the\nimplicit generative modeling of random noise with the same loss function used\nduring posterior maximization, improves a model's understanding of the data\nmanifold furthering adversarial robustness. We evaluate our approach's efficacy\nand provide a simplistic visualization tool for understanding adversarial data,\nusing Principal Component Analysis. Our analysis reveals that adversarial\nrobustness, in general, manifests in models with higher variance along the\nhigh-ranked principal components. We show that models learnt with our approach\nperform remarkably well against a wide-range of attacks. Furthermore, combining\nNoL with state-of-the-art adversarial training extends the robustness of a\nmodel, even beyond what it is adversarially trained for, in both white-box and\nblack-box attack scenarios. \n\n"}
{"id": "1807.02811", "contents": "Title: A Tutorial on Bayesian Optimization Abstract: Bayesian optimization is an approach to optimizing objective functions that\ntake a long time (minutes or hours) to evaluate. It is best-suited for\noptimization over continuous domains of less than 20 dimensions, and tolerates\nstochastic noise in function evaluations. It builds a surrogate for the\nobjective and quantifies the uncertainty in that surrogate using a Bayesian\nmachine learning technique, Gaussian process regression, and then uses an\nacquisition function defined from this surrogate to decide where to sample. In\nthis tutorial, we describe how Bayesian optimization works, including Gaussian\nprocess regression and three common acquisition functions: expected\nimprovement, entropy search, and knowledge gradient. We then discuss more\nadvanced techniques, including running multiple function evaluations in\nparallel, multi-fidelity and multi-information source optimization,\nexpensive-to-evaluate constraints, random environmental conditions, multi-task\nBayesian optimization, and the inclusion of derivative information. We conclude\nwith a discussion of Bayesian optimization software and future research\ndirections in the field. Within our tutorial material we provide a\ngeneralization of expected improvement to noisy evaluations, beyond the\nnoise-free setting where it is more commonly applied. This generalization is\njustified by a formal decision-theoretic argument, standing in contrast to\nprevious ad hoc modifications. \n\n"}
{"id": "1807.02816", "contents": "Title: Improving Deep Learning through Automatic Programming Abstract: Deep learning and deep architectures are emerging as the best machine\nlearning methods so far in many practical applications such as reducing the\ndimensionality of data, image classification, speech recognition or object\nsegmentation. In fact, many leading technology companies such as Google,\nMicrosoft or IBM are researching and using deep architectures in their systems\nto replace other traditional models. Therefore, improving the performance of\nthese models could make a strong impact in the area of machine learning.\nHowever, deep learning is a very fast-growing research domain with many core\nmethodologies and paradigms just discovered over the last few years. This\nthesis will first serve as a short summary of deep learning, which tries to\ninclude all of the most important ideas in this research area. Based on this\nknowledge, we suggested, and conducted some experiments to investigate the\npossibility of improving the deep learning based on automatic programming\n(ADATE). Although our experiments did produce good results, there are still\nmany more possibilities that we could not try due to limited time as well as\nsome limitations of the current ADATE version. I hope that this thesis can\npromote future work on this topic, especially when the next version of ADATE\ncomes out. This thesis also includes a short analysis of the power of ADATE\nsystem, which could be useful for other researchers who want to know what it is\ncapable of. \n\n"}
{"id": "1807.03746", "contents": "Title: Scalable Sparse Subspace Clustering via Ordered Weighted $\\ell_1$\n  Regression Abstract: The main contribution of the paper is a new approach to subspace clustering\nthat is significantly more computationally efficient and scalable than existing\nstate-of-the-art methods. The central idea is to modify the regression\ntechnique in sparse subspace clustering (SSC) by replacing the $\\ell_1$\nminimization with a generalization called Ordered Weighted $\\ell_1$ (OWL)\nminimization which performs simultaneous regression and clustering of\ncorrelated variables. Using random geometric graph theory, we prove that OWL\nregression selects more points within each subspace, resulting in better\nclustering results. This allows for accurate subspace clustering based on\nregression solutions for only a small subset of the total dataset,\nsignificantly reducing the computational complexity compared to SSC. In\nexperiments, we find that our OWL approach can achieve a speedup of 20$\\times$\nto 30$\\times$ for synthetic problems and 4$\\times$ to 8$\\times$ on real data\nproblems. \n\n"}
{"id": "1807.04429", "contents": "Title: Bootstrapping Max Statistics in High Dimensions: Near-Parametric Rates\n  Under Weak Variance Decay and Application to Functional and Multinomial Data Abstract: In recent years, bootstrap methods have drawn attention for their ability to\napproximate the laws of \"max statistics\" in high-dimensional problems. A\nleading example of such a statistic is the coordinate-wise maximum of a sample\naverage of $n$ random vectors in $\\mathbb{R}^p$. Existing results for this\nstatistic show that the bootstrap can work when $n\\ll p$, and rates of\napproximation (in Kolmogorov distance) have been obtained with only logarithmic\ndependence in $p$. Nevertheless, one of the challenging aspects of this setting\nis that established rates tend to scale like $n^{-1/6}$ as a function of $n$.\n  The main purpose of this paper is to demonstrate that improvement in rate is\npossible when extra model structure is available. Specifically, we show that if\nthe coordinate-wise variances of the observations exhibit decay, then a nearly\n$n^{-1/2}$ rate can be achieved, independent of $p$. Furthermore, a surprising\naspect of this dimension-free rate is that it holds even when the decay is very\nweak. Lastly, we provide examples showing how these ideas can be applied to\ninference problems dealing with functional and multinomial data. \n\n"}
{"id": "1807.06214", "contents": "Title: Knockoffs for the mass: new feature importance statistics with false\n  discovery guarantees Abstract: An important problem in machine learning and statistics is to identify\nfeatures that causally affect the outcome. This is often impossible to do from\npurely observational data, and a natural relaxation is to identify features\nthat are correlated with the outcome even conditioned on all other observed\nfeatures. For example, we want to identify that smoking really is correlated\nwith cancer conditioned on demographics. The knockoff procedure is a recent\nbreakthrough in statistics that, in theory, can identify truly correlated\nfeatures while guaranteeing that the false discovery is limited. The idea is to\ncreate synthetic data -- knockoffs -- that captures correlations amongst the\nfeatures. However there are substantial computational and practical challenges\nto generating and using knockoffs. This paper makes several key advances that\nenable knockoff application to be more efficient and powerful. We develop an\nefficient algorithm to generate valid knockoffs from Bayesian Networks. Then we\nsystematically evaluate knockoff test statistics and develop new statistics\nwith improved power. The paper combines new mathematical guarantees with\nsystematic experiments on real and synthetic data. \n\n"}
{"id": "1807.06466", "contents": "Title: Automatic Skin Lesion Segmentation Using Deep Fully Convolutional\n  Networks Abstract: This paper summarizes our method and validation results for the ISIC\nChallenge 2018 - Skin Lesion Analysis Towards Melanoma Detection - Task 1:\nLesion Segmentation \n\n"}
{"id": "1807.06481", "contents": "Title: Dynamic Sampling from Graphical Models Abstract: In this paper, we study the problem of sampling from a graphical model when\nthe model itself is changing dynamically with time. This problem derives its\ninterest from a variety of inference, learning, and sampling settings in\nmachine learning, computer vision, statistical physics, and theoretical\ncomputer science. While the problem of sampling from a static graphical model\nhas received considerable attention, theoretical works for its dynamic variants\nhave been largely lacking. The main contribution of this paper is an algorithm\nthat can sample dynamically from a broad class of graphical models over\ndiscrete random variables. Our algorithm is parallel and Las Vegas: it knows\nwhen to stop and it outputs samples from the exact distribution. We also\nprovide sufficient conditions under which this algorithm runs in time\nproportional to the size of the update, on general graphical models as well as\nwell-studied specific spin systems. In particular we obtain, for the Ising\nmodel (ferromagnetic or anti-ferromagnetic) and for the hardcore model the\nfirst dynamic sampling algorithms that can handle both edge and vertex updates\n(addition, deletion, change of functions), both efficient within regimes that\nare close to the respective uniqueness regimes, beyond which, even for the\nstatic and approximate sampling, no local algorithms were known or the problem\nitself is intractable. Our dynamic sampling algorithm relies on a local\nresampling algorithm and a new \"equilibrium\" property that is shown to be\nsatisfied by our algorithm at each step, and enables us to prove its\ncorrectness. This equilibrium property is robust enough to guarantee the\ncorrectness of our algorithm, helps us improve bounds on fast convergence on\nspecific models, and should be of independent interest. \n\n"}
{"id": "1807.07217", "contents": "Title: Deconfounding age effects with fair representation learning when\n  assessing dementia Abstract: One of the most prevalent symptoms among the elderly population, dementia,\ncan be detected by classifiers trained on linguistic features extracted from\nnarrative transcripts. However, these linguistic features are impacted in a\nsimilar but different fashion by the normal aging process. Aging is therefore a\nconfounding factor, whose effects have been hard for machine learning\nclassifiers (especially deep neural network based models) to ignore. We show\nDNN models are capable of estimating ages based on linguistic features.\nPredicting dementia based on this aging bias could lead to potentially\nnon-generalizable accuracies on clinical datasets, if not properly\ndeconfounded.\n  In this paper, we propose to address this deconfounding problem with fair\nrepresentation learning. We build neural network classifiers that learn\nlow-dimensional representations reflecting the impacts of dementia yet\ndiscarding the effects of age. To evaluate these classifiers, we specify a\nmodel-agnostic score $\\Delta_{eo}^{(N)}$ measuring how classifier results are\ndeconfounded from age. Our best models compromise accuracy by only 2.56\\% and\n1.54\\% on two clinical datasets compared to DNNs, and their $\\Delta_{eo}^{(2)}$\nscores are better than statistical (residulization and inverse probability\nweight) adjustments. \n\n"}
{"id": "1807.07270", "contents": "Title: Singularities of inner functions associated with hyperbolic maps Abstract: Let $f$ be a function in the Eremenko-Lyubich class $\\mathcal{B}$, and let\n$U$ be an unbounded, forward invariant Fatou component of $f$. We relate the\nnumber of singularities of an inner function associated to $f|_U$ with the\nnumber of tracts of $f$. In particular, we show that if $f$ lies in either of\ntwo large classes of functions in $\\mathcal{B}$, and also has finitely many\ntracts, then the number of singularities of an associated inner function is at\nmost equal to the number of tracts of $f$.\n  Our results imply that for hyperbolic functions of finite order there is an\nupper bound -- related to the order -- on the number of singularities of an\nassociated inner function. \n\n"}
{"id": "1807.10363", "contents": "Title: Message-passing neural networks for high-throughput polymer screening Abstract: Machine learning methods have shown promise in predicting molecular\nproperties, and given sufficient training data machine learning approaches can\nenable rapid high-throughput virtual screening of large libraries of compounds.\nGraph-based neural network architectures have emerged in recent years as the\nmost successful approach for predictions based on molecular structure, and have\nconsistently achieved the best performance on benchmark quantum chemical\ndatasets. However, these models have typically required optimized 3D structural\ninformation for the molecule to achieve the highest accuracy. These 3D\ngeometries are costly to compute for high levels of theory, limiting the\napplicability and practicality of machine learning methods in high-throughput\nscreening applications. In this study, we present a new database of candidate\nmolecules for organic photovoltaic applications, comprising approximately\n91,000 unique chemical structures.Compared to existing datasets, this dataset\ncontains substantially larger molecules (up to 200 atoms) as well as\nextrapolated properties for long polymer chains. We show that message-passing\nneural networks trained with and without 3D structural information for these\nmolecules achieve similar accuracy, comparable to state-of-the-art methods on\nexisting benchmark datasets. These results therefore emphasize that for larger\nmolecules with practical applications, near-optimal prediction results can be\nobtained without using optimized 3D geometry as an input. We further show that\nlearned molecular representations can be leveraged to reduce the training data\nrequired to transfer predictions to a new DFT functional. \n\n"}
{"id": "1807.10678", "contents": "Title: Survival of the Fittest Group: Factorial Analyses of Treatment Effects\n  under Independent Right-Censoring Abstract: This paper introduces new effect parameters for factorial survival designs\nwith possibly right-censored time-to-event data. In the special case of a\ntwo-sample design it coincides with the concordance or Wilcoxon parameter in\nsurvival analysis. More generally, the new parameters describe treatment or\ninteraction effects and we develop estimates and tests to infer their presence.\nWe rigorously study the asymptotic properties by means of empirical process\ntechniques and additionally suggest wild bootstrapping for a consistent and\ndistribution-free application of the inference procedures. The small sample\nperformance is discussed based on simulation results. The practical usefulness\nof the developed methodology is exemplified on a data example about patients\nwith colon cancer by conducting one- and two-factorial analyses. \n\n"}
{"id": "1807.11228", "contents": "Title: Predicting Conversion of Mild Cognitive Impairments to Alzheimer's\n  Disease and Exploring Impact of Neuroimaging Abstract: Nowadays, a lot of scientific efforts are concentrated on the diagnosis of\nAlzheimer's Disease (AD) applying deep learning methods to neuroimaging data.\nEven for 2017, there were published more than a hundred papers dedicated to AD\ndiagnosis, whereas only a few works considered a problem of mild cognitive\nimpairments (MCI) conversion to the AD. However, the conversion prediction is\nan important problem since approximately 15% of patients with MCI converges to\nthe AD every year. In the current work, we are focusing on the conversion\nprediction using brain Magnetic Resonance Imaging and clinical data, such as\ndemographics, cognitive assessments, genetic, and biochemical markers. First of\nall, we applied state-of-the-art deep learning algorithms on the neuroimaging\ndata and compared these results with two machine learning algorithms that we\nfit using the clinical data. As a result, the models trained on the clinical\ndata outperform the deep learning algorithms applied to the MR images. To\nexplore the impact of neuroimaging further, we trained a deep feed-forward\nembedding using similarity learning with Histogram loss on all available MRIs\nand obtained 64-dimensional vector representation of neuroimaging data. The use\nof learned representation from the deep embedding allowed to increase the\nquality of prediction based on the neuroimaging. Finally, the current results\non this dataset show that the neuroimaging does affect conversion prediction,\nhowever, cannot noticeably increase the quality of the prediction. The best\nresults of predicting MCI-to-AD conversion are provided by XGBoost algorithm\ntrained on the clinical and embedding data. The resulting accuracy is 0.76 +-\n0.01 and the area under the ROC curve - 0.86 +- 0.01. \n\n"}
{"id": "1807.11342", "contents": "Title: A Dissipatively Stabilized Mott Insulator of Photons Abstract: Superconducting circuits are a competitive platform for quantum computation\nbecause they offer controllability, long coherence times and strong\ninteractions - properties that are essential for the study of quantum materials\ncomprising microwave photons. However, intrinsic photon losses in these\ncircuits hinder the realization of quantum many-body phases. Here we use\nsuperconducting circuits to explore strongly correlated quantum matter by\nbuilding a Bose-Hubbard lattice for photons in the strongly interacting regime.\nWe develop a versatile method for dissipative preparation of incompressible\nmany-body phases through reservoir engineering and apply it to our system to\nstabilize a Mott insulator of photons against losses. Site- and time-resolved\nreadout of the lattice allows us to investigate the microscopic details of the\nthermalization process through the dynamics of defect propagation and removal\nin the Mott phase. Our experiments demonstrate the power of superconducting\ncircuits for studying strongly correlated matter in both coherent and\nengineered dissipative settings. In conjunction with recently demonstrated\nsuperconducting microwave Chern insulators, we expect that our approach will\nenable the exploration of topologically ordered phases of matter. \n\n"}
{"id": "1808.00200", "contents": "Title: Anomaly Detection via Minimum Likelihood Generative Adversarial Networks Abstract: Anomaly detection aims to detect abnormal events by a model of normality. It\nplays an important role in many domains such as network intrusion detection,\ncriminal activity identity and so on. With the rapidly growing size of\naccessible training data and high computation capacities, deep learning based\nanomaly detection has become more and more popular. In this paper, a new\ndomain-based anomaly detection method based on generative adversarial networks\n(GAN) is proposed. Minimum likelihood regularization is proposed to make the\ngenerator produce more anomalies and prevent it from converging to normal data\ndistribution. Proper ensemble of anomaly scores is shown to improve the\nstability of discriminator effectively. The proposed method has achieved\nsignificant improvement than other anomaly detection methods on Cifar10 and UCI\ndatasets. \n\n"}
{"id": "1808.00209", "contents": "Title: Binarized Convolutional Neural Networks for Efficient Inference on GPUs Abstract: Convolutional neural networks have recently achieved significant\nbreakthroughs in various image classification tasks. However, they are\ncomputationally expensive,which can make their feasible mplementation on\nembedded and low-power devices difficult. In this paper convolutional neural\nnetwork binarization is implemented on GPU-based platforms for real-time\ninference on resource constrained devices. In binarized networks, all weights\nand intermediate computations between layers are quantized to +1 and -1,\nallowing multiplications and additions to be replaced with bit-wise operations\nbetween 32-bit words. This representation completely eliminates the need for\nfloating point multiplications and additions and decreases both the\ncomputational load and the memory footprint compared to a full-precision\nnetwork implemented in floating point, making it well-suited for\nresource-constrained environments. We compare the performance of our\nimplementation with an equivalent floating point implementation on one desktop\nand two embedded GPU platforms. Our implementation achieves a maximum speed up\nof 7. 4X with only 4.4% loss in accuracy compared to a reference\nimplementation. \n\n"}
{"id": "1808.00380", "contents": "Title: A Differentially Private Kernel Two-Sample Test Abstract: Kernel two-sample testing is a useful statistical tool in determining whether\ndata samples arise from different distributions without imposing any parametric\nassumptions on those distributions. However, raw data samples can expose\nsensitive information about individuals who participate in scientific studies,\nwhich makes the current tests vulnerable to privacy breaches. Hence, we design\na new framework for kernel two-sample testing conforming to differential\nprivacy constraints, in order to guarantee the privacy of subjects in the data.\nUnlike existing differentially private parametric tests that simply add noise\nto data, kernel-based testing imposes a challenge due to a complex dependence\nof test statistics on the raw data, as these statistics correspond to\nestimators of distances between representations of probability measures in\nHilbert spaces. Our approach considers finite dimensional approximations to\nthose representations. As a result, a simple chi-squared test is obtained,\nwhere a test statistic depends on a mean and covariance of empirical\ndifferences between the samples, which we perturb for a privacy guarantee. We\ninvestigate the utility of our framework in two realistic settings and conclude\nthat our method requires only a relatively modest increase in sample size to\nachieve a similar level of power to the non-private tests in both settings. \n\n"}
{"id": "1808.00387", "contents": "Title: Just Interpolate: Kernel \"Ridgeless\" Regression Can Generalize Abstract: In the absence of explicit regularization, Kernel \"Ridgeless\" Regression with\nnonlinear kernels has the potential to fit the training data perfectly. It has\nbeen observed empirically, however, that such interpolated solutions can still\ngeneralize well on test data. We isolate a phenomenon of implicit\nregularization for minimum-norm interpolated solutions which is due to a\ncombination of high dimensionality of the input data, curvature of the kernel\nfunction, and favorable geometric properties of the data such as an eigenvalue\ndecay of the empirical covariance and kernel matrices. In addition to deriving\na data-dependent upper bound on the out-of-sample error, we present\nexperimental evidence suggesting that the phenomenon occurs in the MNIST\ndataset. \n\n"}
{"id": "1808.01427", "contents": "Title: Structure-Aware Shape Synthesis Abstract: We propose a new procedure to guide training of a data-driven shape\ngenerative model using a structure-aware loss function. Complex 3D shapes often\ncan be summarized using a coarsely defined structure which is consistent and\nrobust across variety of observations. However, existing synthesis techniques\ndo not account for structure during training, and thus often generate\nimplausible and structurally unrealistic shapes. During training, we enforce\nstructural constraints in order to enforce consistency and structure across the\nentire manifold. We propose a novel methodology for training 3D generative\nmodels that incorporates structural information into an end-to-end training\npipeline. \n\n"}
{"id": "1808.01990", "contents": "Title: Hashing with Binary Matrix Pursuit Abstract: We propose theoretical and empirical improvements for two-stage hashing\nmethods. We first provide a theoretical analysis on the quality of the binary\ncodes and show that, under mild assumptions, a residual learning scheme can\nconstruct binary codes that fit any neighborhood structure with arbitrary\naccuracy. Secondly, we show that with high-capacity hash functions such as\nCNNs, binary code inference can be greatly simplified for many standard\nneighborhood definitions, yielding smaller optimization problems and more\nrobust codes. Incorporating our findings, we propose a novel two-stage hashing\nmethod that significantly outperforms previous hashing studies on widely used\nimage retrieval benchmarks. \n\n"}
{"id": "1808.03064", "contents": "Title: Gradient and Newton Boosting for Classification and Regression Abstract: Boosting algorithms are frequently used in applied data science and in\nresearch. To date, the distinction between boosting with either gradient\ndescent or second-order Newton updates is often not made in both applied and\nmethodological research, and it is thus implicitly assumed that the difference\nis irrelevant. The goal of this article is to clarify this situation. In\nparticular, we present gradient and Newton boosting, as well as a hybrid\nvariant of the two, in a unified framework. We compare these boosting\nalgorithms with trees as base learners using various datasets and loss\nfunctions. Our experiments show that Newton boosting outperforms gradient and\nhybrid gradient-Newton boosting in terms of predictive accuracy on the majority\nof datasets. We also present evidence that the reason for this is not faster\nconvergence of Newton boosting. In addition, we introduce a novel tuning\nparameter for tree-based Newton boosting which is interpretable and important\nfor predictive accuracy. \n\n"}
{"id": "1808.03533", "contents": "Title: Measuring azimuthal and radial modes of photons Abstract: With the emergence of the field of quantum communications, the appropriate\nchoice of photonic degrees of freedom used for encoding information is of\nparamount importance. Highly precise techniques for measuring the polarisation,\nfrequency, and arrival time of a photon have been developed. However, the\ntransverse spatial degree of freedom still lacks a measurement scheme that\nallows the reconstruction of its full transverse structure with a simple\nimplementation and a high level of accuracy. Here we show a method to measure\nthe azimuthal and radial modes of Laguerre-Gaussian beams with a greater than\n99% accuracy, using a single phase screen. We compare our technique with\nprevious commonly used methods and demonstrate the significant improvements it\npresents for quantum key distribution and state tomography of high-dimensional\nquantum states of light. Moreover, our technique can be readily extended to any\narbitrary family of spatial modes, such as mutually unbiased bases,\nHermite-Gauss, and Ince-Gauss. Our scheme will significantly enhance existing\nquantum and classical communication protocols that use the spatial structure of\nlight, as well as enable fundamental experiments on spatial-mode entanglement\nto reach their full potential. \n\n"}
{"id": "1808.03698", "contents": "Title: BooST: Boosting Smooth Trees for Partial Effect Estimation in Nonlinear\n  Regressions Abstract: In this paper, we introduce a new machine learning (ML) model for nonlinear\nregression called the Boosted Smooth Transition Regression Trees (BooST), which\nis a combination of boosting algorithms with smooth transition regression\ntrees. The main advantage of the BooST model is the estimation of the\nderivatives (partial effects) of very general nonlinear models. Therefore, the\nmodel can provide more interpretation about the mapping between the covariates\nand the dependent variable than other tree-based models, such as Random\nForests. We present several examples with both simulated and real data. \n\n"}
{"id": "1808.03873", "contents": "Title: A Consistent Method for Learning OOMs from Asymptotically Stationary\n  Time Series Data Containing Missing Values Abstract: In the traditional framework of spectral learning of stochastic time series\nmodels, model parameters are estimated based on trajectories of fully recorded\nobservations. However, real-world time series data often contain missing\nvalues, and worse, the distributions of missingness events over time are often\nnot independent of the visible process. Recently, a spectral OOM learning\nalgorithm for time series with missing data was introduced and proved to be\nconsistent, albeit under quite strong conditions. Here we refine the algorithm\nand prove that the original strong conditions can be very much relaxed. We\nvalidate our theoretical findings by numerical experiments, showing that the\nalgorithm can consistently handle missingness patterns whose dynamic interacts\nwith the visible process. \n\n"}
{"id": "1808.03880", "contents": "Title: Parallelization does not Accelerate Convex Optimization: Adaptivity\n  Lower Bounds for Non-smooth Convex Minimization Abstract: In this paper we study the limitations of parallelization in convex\noptimization. A convenient approach to study parallelization is through the\nprism of \\emph{adaptivity} which is an information theoretic measure of the\nparallel runtime of an algorithm [BS18]. Informally, adaptivity is the number\nof sequential rounds an algorithm needs to make when it can execute\npolynomially-many queries in parallel at every round. For combinatorial\noptimization with black-box oracle access, the study of adaptivity has recently\nled to exponential accelerations in parallel runtime and the natural question\nis whether dramatic accelerations are achievable for convex optimization.\n  For the problem of minimizing a non-smooth convex function $f:[0,1]^n\\to\n\\mathbb{R}$ over the unit Euclidean ball, we give a tight lower bound that\nshows that even when $\\texttt{poly}(n)$ queries can be executed in parallel,\nthere is no randomized algorithm with $\\tilde{o}(n^{1/3})$ rounds of adaptivity\nthat has convergence rate that is better than those achievable with a\none-query-per-round algorithm. A similar lower bound was obtained by Nemirovski\n[Nem94], however that result holds for the $\\ell_{\\infty}$-setting instead of\n$\\ell_2$. In addition, we also show a tight lower bound that holds for\nLipschitz and strongly convex functions.\n  At the time of writing this manuscript we were not aware of Nemirovski's\nresult. The construction we use is similar to the one in [Nem94], though our\nanalysis is different. Due to the close relationship between this work and\n[Nem94], we view the research contribution of this manuscript limited and it\nshould serve as an instructful approach to understanding lower bounds for\nparallel optimization. \n\n"}
{"id": "1808.05377", "contents": "Title: Neural Architecture Search: A Survey Abstract: Deep Learning has enabled remarkable progress over the last years on a\nvariety of tasks, such as image recognition, speech recognition, and machine\ntranslation. One crucial aspect for this progress are novel neural\narchitectures. Currently employed architectures have mostly been developed\nmanually by human experts, which is a time-consuming and error-prone process.\nBecause of this, there is growing interest in automated neural architecture\nsearch methods. We provide an overview of existing work in this field of\nresearch and categorize them according to three dimensions: search space,\nsearch strategy, and performance estimation strategy. \n\n"}
{"id": "1808.05889", "contents": "Title: Data Consistency Approach to Model Validation Abstract: In scientific inference problems, the underlying statistical modeling\nassumptions have a crucial impact on the end results. There exist, however,\nonly a few automatic means for validating these fundamental modelling\nassumptions. The contribution in this paper is a general criterion to evaluate\nthe consistency of a set of statistical models with respect to observed data.\nThis is achieved by automatically gauging the models' ability to generate data\nthat is similar to the observed data. Importantly, the criterion follows from\nthe model class itself and is therefore directly applicable to a broad range of\ninference problems with varying data types, ranging from independent univariate\ndata to high-dimensional time-series. The proposed data consistency criterion\nis illustrated, evaluated and compared to several well-established methods\nusing three synthetic and two real data sets. \n\n"}
{"id": "1808.06645", "contents": "Title: Stochastic Combinatorial Ensembles for Defending Against Adversarial\n  Examples Abstract: Many deep learning algorithms can be easily fooled with simple adversarial\nexamples. To address the limitations of existing defenses, we devised a\nprobabilistic framework that can generate an exponentially large ensemble of\nmodels from a single model with just a linear cost. This framework takes\nadvantage of neural network depth and stochastically decides whether or not to\ninsert noise removal operators such as VAEs between layers. We show empirically\nthe important role that model gradients have when it comes to determining\ntransferability of adversarial examples, and take advantage of this result to\ndemonstrate that it is possible to train models with limited adversarial attack\ntransferability. Additionally, we propose a detection method based on metric\nlearning in order to detect adversarial examples that have no hope of being\ncleaned of maliciously engineered noise. \n\n"}
{"id": "1808.07431", "contents": "Title: Deep Boosted Regression for MR to CT Synthesis Abstract: Attenuation correction is an essential requirement of positron emission\ntomography (PET) image reconstruction to allow for accurate quantification.\nHowever, attenuation correction is particularly challenging for PET-MRI as\nneither PET nor magnetic resonance imaging (MRI) can directly image tissue\nattenuation properties. MRI-based computed tomography (CT) synthesis has been\nproposed as an alternative to physics based and segmentation-based approaches\nthat assign a population-based tissue density value in order to generate an\nattenuation map. We propose a novel deep fully convolutional neural network\nthat generates synthetic CTs in a recursive manner by gradually reducing the\nresiduals of the previous network, increasing the overall accuracy and\ngeneralisability, while keeping the number of trainable parameters within\nreasonable limits. The model is trained on a database of 20 pre-acquired MRI/CT\npairs and a four-fold random bootstrapped validation with a 80:20 split is\nperformed. Quantitative results show that the proposed framework outperforms a\nstate-of-the-art atlas-based approach decreasing the Mean Absolute Error (MAE)\nfrom 131HU to 68HU for the synthetic CTs and reducing the PET reconstruction\nerror from 14.3% to 7.2%. \n\n"}
{"id": "1808.07784", "contents": "Title: Time-Agnostic Prediction: Predicting Predictable Video Frames Abstract: Prediction is arguably one of the most basic functions of an intelligent\nsystem. In general, the problem of predicting events in the future or between\ntwo waypoints is exceedingly difficult. However, most phenomena naturally pass\nthrough relatively predictable bottlenecks---while we cannot predict the\nprecise trajectory of a robot arm between being at rest and holding an object\nup, we can be certain that it must have picked the object up. To exploit this,\nwe decouple visual prediction from a rigid notion of time. While conventional\napproaches predict frames at regularly spaced temporal intervals, our\ntime-agnostic predictors (TAP) are not tied to specific times so that they may\ninstead discover predictable \"bottleneck\" frames no matter when they occur. We\nevaluate our approach for future and intermediate frame prediction across three\nrobotic manipulation tasks. Our predictions are not only of higher visual\nquality, but also correspond to coherent semantic subgoals in temporally\nextended tasks. \n\n"}
{"id": "1808.07945", "contents": "Title: Maximal Jacobian-based Saliency Map Attack Abstract: The Jacobian-based Saliency Map Attack is a family of adversarial attack\nmethods for fooling classification models, such as deep neural networks for\nimage classification tasks. By saturating a few pixels in a given image to\ntheir maximum or minimum values, JSMA can cause the model to misclassify the\nresulting adversarial image as a specified erroneous target class. We propose\ntwo variants of JSMA, one which removes the requirement to specify a target\nclass, and another that additionally does not need to specify whether to only\nincrease or decrease pixel intensities. Our experiments highlight the\ncompetitive speeds and qualities of these variants when applied to datasets of\nhand-written digits and natural scenes. \n\n"}
{"id": "1808.08366", "contents": "Title: Parameter-wise co-clustering for high-dimensional data Abstract: In recent years, data dimensionality has increasingly become a concern,\nleading to many parameter and dimension reduction techniques being proposed in\nthe literature. A parameter-wise co-clustering model, for data modelled via\ncontinuous random variables, is presented. The proposed model, although\nallowing more flexibility, still maintains the very high degree of parsimony\nachieved by traditional co-clustering. A stochastic expectation-maximization\n(SEM) algorithm along with a Gibbs sampler is used for parameter estimation and\nan integrated complete log-likelihood criterion is used for model selection.\nSimulated and real datasets are used for illustration and comparison with\ntraditional co-clustering. \n\n"}
{"id": "1808.09645", "contents": "Title: Diffusion Approximations for Online Principal Component Estimation and\n  Global Convergence Abstract: In this paper, we propose to adopt the diffusion approximation tools to study\nthe dynamics of Oja's iteration which is an online stochastic gradient descent\nmethod for the principal component analysis. Oja's iteration maintains a\nrunning estimate of the true principal component from streaming data and enjoys\nless temporal and spatial complexities. We show that the Oja's iteration for\nthe top eigenvector generates a continuous-state discrete-time Markov chain\nover the unit sphere. We characterize the Oja's iteration in three phases using\ndiffusion approximation and weak convergence tools. Our three-phase analysis\nfurther provides a finite-sample error bound for the running estimate, which\nmatches the minimax information lower bound for principal component analysis\nunder the additional assumption of bounded samples. \n\n"}
{"id": "1808.09940", "contents": "Title: Adversarial Deep Reinforcement Learning in Portfolio Management Abstract: In this paper, we implement three state-of-art continuous reinforcement\nlearning algorithms, Deep Deterministic Policy Gradient (DDPG), Proximal Policy\nOptimization (PPO) and Policy Gradient (PG)in portfolio management. All of them\nare widely-used in game playing and robot control. What's more, PPO has\nappealing theoretical propeties which is hopefully potential in portfolio\nmanagement. We present the performances of them under different settings,\nincluding different learning rates, objective functions, feature combinations,\nin order to provide insights for parameters tuning, features selection and data\npreparation. We also conduct intensive experiments in China Stock market and\nshow that PG is more desirable in financial market than DDPG and PPO, although\nboth of them are more advanced. What's more, we propose a so called Adversarial\nTraining method and show that it can greatly improve the training efficiency\nand significantly promote average daily return and sharpe ratio in back test.\nBased on this new modification, our experiments results show that our agent\nbased on Policy Gradient can outperform UCRP. \n\n"}
{"id": "1808.10430", "contents": "Title: Nested multi-instance classification Abstract: There are classification tasks that take as inputs groups of images rather\nthan single images. In order to address such situations, we introduce a nested\nmulti-instance deep network. The approach is generic in that it is applicable\nto general data instances, not just images. The network has several\nconvolutional neural networks grouped together at different stages. This\nprimarily differs from other previous works in that we organize instances into\nrelevant groups that are treated differently. We also introduce a method to\nreplace instances that are missing which successfully creates neutral input\ninstances and consistently outperforms standard fill-in methods in real world\nuse cases. In addition, we propose a method for manual dropout when a whole\ngroup of instances is missing that allows us to use richer training data and\nobtain higher accuracy at the end of training. With specific pretraining, we\nfind that the model works to great effect on our real world and pub-lic\ndatasets in comparison to baseline methods, justifying the different treatment\namong groups of instances. \n\n"}
{"id": "1809.00082", "contents": "Title: NEU: A Meta-Algorithm for Universal UAP-Invariant Feature Representation Abstract: Effective feature representation is key to the predictive performance of any\nalgorithm. This paper introduces a meta-procedure, called Non-Euclidean\nUpgrading (NEU), which learns feature maps that are expressive enough to embed\nthe universal approximation property (UAP) into most model classes while only\noutputting feature maps that preserve any model class's UAP. We show that NEU\ncan learn any feature map with these two properties if that feature map is\nasymptotically deformable into the identity. We also find that the\nfeature-representations learned by NEU are always submanifolds of the feature\nspace. NEU's properties are derived from a new deep neural model that is\nuniversal amongst all orientation-preserving homeomorphisms on the input space.\nWe derive qualitative and quantitative approximation guarantees for this\narchitecture. We quantify the number of parameters required for this new\narchitecture to memorize any set of input-output pairs while simultaneously\nfixing every point of the input space lying outside some compact set, and we\nquantify the size of this set as a function of our model's depth. Moreover, we\nshow that no deep feed-forward network with commonly used activation function\nhas all these properties. NEU's performance is evaluated against competing\nmachine learning methods on various regression and dimension reduction tasks\nboth with financial and simulated data. \n\n"}
{"id": "1809.00366", "contents": "Title: Cold-start recommendations in Collective Matrix Factorization Abstract: This work explores the ability of collective matrix factorization models in\nrecommender systems to make predictions about users and items for which there\nis side information available but no feedback or interactions data, and\nproposes a new formulation with a faster cold-start prediction formula that can\nbe used in real-time systems. While these cold-start recommendations are not as\ngood as warm-start ones, they were found to be of better quality than\nnon-personalized recommendations, and predictions about new users were found to\nbe more reliable than those about new items. The formulation proposed here\nresulted in improved cold-start recommendations in many scenarios, at the\nexpense of worse warm-start ones. \n\n"}
{"id": "1809.02963", "contents": "Title: Variational Approximation Error in Bayesian Non-negative Matrix\n  Factorization Abstract: Non-negative matrix factorization (NMF) is a knowledge discovery method that\nis used in many fields. Variational inference and Gibbs sampling methods for it\nare also wellknown. However, the variational approximation error has not been\nclarified yet, because NMF is not statistically regular and the prior\ndistribution used in variational Bayesian NMF (VBNMF) has zero or divergence\npoints. In this paper, using algebraic geometrical methods, we theoretically\nanalyze the difference in negative log evidence (a.k.a. free energy) between\nVBNMF and Bayesian NMF, i.e., the Kullback-Leibler divergence between the\nvariational posterior and the true posterior. We derive an upper bound for the\nlearning coefficient (a.k.a. the real log canonical threshold) in Bayesian NMF.\nBy using the upper bound, we find a lower bound for the approximation error,\nasymptotically. The result quantitatively shows how well the VBNMF algorithm\ncan approximate Bayesian NMF; the lower bound depends on the hyperparameters\nand the true nonnegative rank. A numerical experiment demonstrates the\ntheoretical result. \n\n"}
{"id": "1809.03833", "contents": "Title: Statistical periodicity in driven quantum systems: General formalism and\n  application to noisy Floquet topological chains Abstract: Much recent experimental effort has focused on the realization of exotic\nquantum states and dynamics predicted to occur in periodically driven systems.\nBut how robust are the sought-after features, such as Floquet topological\nsurface states, against unavoidable imperfections in the periodic driving? In\nthis work, we address this question in a broader context and study the dynamics\nof quantum systems subject to noise with periodically recurring statistics. We\nshow that the stroboscopic time evolution of such systems is described by a\nnoise-averaged Floquet superoperator. The eigenvectors and -values of this\nsuperoperator generalize the familiar concepts of Floquet states and\nquasienergies and allow us to describe decoherence due to noise efficiently.\nApplying the general formalism to the example of a noisy Floquet topological\nchain, we re-derive and corroborate our recent findings on the noise-induced\ndecay of topologically protected end states. These results follow directly from\nan expansion of the end state in eigenvectors of the Floquet superoperator. \n\n"}
{"id": "1809.04110", "contents": "Title: Joint Embedding of Meta-Path and Meta-Graph for Heterogeneous\n  Information Networks Abstract: Meta-graph is currently the most powerful tool for similarity search on\nheterogeneous information networks,where a meta-graph is a composition of\nmeta-paths that captures the complex structural information. However, current\nrelevance computing based on meta-graph only considers the complex structural\ninformation, but ignores its embedded meta-paths information. To address this\nproblem, we proposeMEta-GrAph-based network embedding models, called MEGA and\nMEGA++, respectively. The MEGA model uses normalized relevance or similarity\nmeasures that are derived from a meta-graph and its embedded meta-paths between\nnodes simultaneously, and then leverages tensor decomposition method to perform\nnode embedding. The MEGA++ further facilitates the use of coupled tensor-matrix\ndecomposition method to obtain a joint embedding for nodes, which\nsimultaneously considers the hidden relations of all meta information of a\nmeta-graph.Extensive experiments on two real datasets demonstrate thatMEGA and\nMEGA++ are more effective than state-of-the-art approaches. \n\n"}
{"id": "1809.04423", "contents": "Title: Can a Compact Neuronal Circuit Policy be Re-purposed to Learn Simple\n  Robotic Control? Abstract: We propose a neural information processing system which is obtained by\nre-purposing the function of a biological neural circuit model, to govern\nsimulated and real-world control tasks. Inspired by the structure of the\nnervous system of the soil-worm, C. elegans, we introduce Neuronal Circuit\nPolicies (NCPs), defined as the model of biological neural circuits\nreparameterized for the control of an alternative task. We learn instances of\nNCPs to control a series of robotic tasks, including the autonomous parking of\na real-world rover robot. For reconfiguration of the purpose of the neural\ncircuit, we adopt a search-based optimization algorithm. Neuronal circuit\npolicies perform on par and in some cases surpass the performance of\ncontemporary deep learning models with the advantage leveraging significantly\nfewer learnable parameters and realizing interpretable dynamics at the\ncell-level. \n\n"}
{"id": "1809.05032", "contents": "Title: IPAD: Stable Interpretable Forecasting with Knockoffs Inference Abstract: Interpretability and stability are two important features that are desired in\nmany contemporary big data applications arising in economics and finance. While\nthe former is enjoyed to some extent by many existing forecasting approaches,\nthe latter in the sense of controlling the fraction of wrongly discovered\nfeatures which can enhance greatly the interpretability is still largely\nunderdeveloped in the econometric settings. To this end, in this paper we\nexploit the general framework of model-X knockoffs introduced recently in\nCand\\`{e}s, Fan, Janson and Lv (2018), which is nonconventional for\nreproducible large-scale inference in that the framework is completely free of\nthe use of p-values for significance testing, and suggest a new method of\nintertwined probabilistic factors decoupling (IPAD) for stable interpretable\nforecasting with knockoffs inference in high-dimensional models. The recipe of\nthe method is constructing the knockoff variables by assuming a latent factor\nmodel that is exploited widely in economics and finance for the association\nstructure of covariates. Our method and work are distinct from the existing\nliterature in that we estimate the covariate distribution from data instead of\nassuming that it is known when constructing the knockoff variables, our\nprocedure does not require any sample splitting, we provide theoretical\njustifications on the asymptotic false discovery rate control, and the theory\nfor the power analysis is also established. Several simulation examples and the\nreal data analysis further demonstrate that the newly suggested method has\nappealing finite-sample performance with desired interpretability and stability\ncompared to some popularly used forecasting methods. \n\n"}
{"id": "1809.06019", "contents": "Title: Statistically and Computationally Efficient Variance Estimator for\n  Kernel Ridge Regression Abstract: In this paper, we propose a random projection approach to estimate variance\nin kernel ridge regression. Our approach leads to a consistent estimator of the\ntrue variance, while being computationally more efficient. Our variance\nestimator is optimal for a large family of kernels, including cubic splines and\nGaussian kernels. Simulation analysis is conducted to support our theory. \n\n"}
{"id": "1809.06243", "contents": "Title: Morse inequalities for Fourier components of Kohn-Rossi cohomology of CR\n  covering manifolds with $S^1$-action Abstract: Let $X$ be a compact connected CR manifold of dimension $2n+1, n \\geq 1$. Let\n$\\widetilde{X}$ be a paracompact CR manifold with a transversal CR\n$S^1$-action, such that there is a discrete group $\\Gamma$ acting freely on\n$\\widetilde{X}$ having $X \\, = \\, \\widetilde{X}/\\Gamma$. Based on an asymptotic\nformula for the Fourier components of the heat kernel with respect to the\n$S^1$-action, we establish the Morse inequalities for Fourier components of\nreduced $L^2$-Kohn-Rossi cohomology with values in a rigid CR vector bundle\nover $\\widetilde{X}$. As a corollary, we obtain the Morse inequalities for\nFourier components of Kohn-Rossi cohomology on $X$ which were obtained by\nHsiao-Li by using Szeg\\\"{o} kernel method. \n\n"}
{"id": "1809.08330", "contents": "Title: Estimating minimum effect with outlier selection Abstract: We introduce one-sided versions of Huber's contamination model, in which\ncorrupted samples tend to take larger values than uncorrupted ones. Two\nintertwined problems are addressed: estimation of the mean of uncorrupted\nsamples (minimum effect) and selection of corrupted samples (outliers).\nRegarding the minimum effect estimation, we derive the minimax risks and\nintroduce adaptive estimators to the unknown number of contaminations.\nInterestingly, the optimal convergence rate highly differs from that in\nclassical Huber's contamination model. Also, our analysis uncovers the effect\nof particular structural assumptions on the distribution of the contaminated\nsamples. As for the problem of selecting the outliers, we formulate the problem\nin a multiple testing framework for which the location/scaling of the null\nhypotheses are unknown. We rigorously prove how estimating the null hypothesis\nis possible while maintaining a theoretical guarantee on the amount of the\nfalsely selected outliers, both through false discovery rate (FDR) or post hoc\nbounds. As a by-product, we address a long-standing open issue on FDR control\nunder equi-correlation, which reinforces the interest of removing dependency\nwhen making multiple testing. \n\n"}
{"id": "1809.09129", "contents": "Title: Cavity-quantum-electrodynamical toolbox for quantum magnetism Abstract: The recent experimental observation of spinor self-ordering of ultracold\natoms in optical resonators has set the stage for the exploration of emergent\nmagnetic orders in quantum-gas--cavity systems. Based on this platform, we\nintroduce a generic scheme for the implementation of long-range quantum spin\nHamiltonians composed of various types of couplings, including Heisenberg and\nDzyaloshinskii-Moriya interactions. Our model is comprised of an effective\ntwo-component Bose-Einstein condensate, driven by two classical pump lasers and\ncoupled to a single dynamic mode of a linear cavity in a double $\\Lambda$\nscheme. Cavity photons mediate the long-range spin-spin interactions with\nspatially modulated coupling coefficients, where the latter ones can be tuned\nby modifying spatial profiles of the pump lasers. As experimentally relevant\nexamples, we demonstrate that by properly choosing the spatial profiles of the\npump lasers achiral domain-wall antiferromagnetic and chiral spin-spiral orders\nemerge beyond critical laser strengths. The transition between these two phases\ncan be observed in a single experimental setup by tuning the reflectivity of a\nmirror. We also discuss extensions of our scheme for the implementation of\nother classes of spin Hamiltonians. \n\n"}
{"id": "1809.09365", "contents": "Title: A Comprehensive Subclass of Bi-Univalent Functions Associated with\n  Chebyshev Polynomials of the Second Kind Abstract: Our objective in this paper is to introduce and investigate a\nnewly-constructed subclass of normalized analytic and bi-univalent functions by\nmeans of the Chebyshev polynomials of the second kind. Upper bounds for the\nsecond and third Taylor-Maclaurin coefficients, and also Fekete-Szego\ninequalities of functions belonging to this subclass are founded. Several\nconnections to some of the earlier known results are also pointed out. \n\n"}
{"id": "1809.10284", "contents": "Title: When is there a Representer Theorem? Reflexive Banach spaces Abstract: We consider a general regularised interpolation problem for learning a\nparameter vector from data. The well known representer theorem says that under\ncertain conditions on the regulariser there exists a solution in the linear\nspan of the data points. This is at the core of kernel methods in machine\nlearning as it makes the problem computationally tractable. Most literature\ndeals only with sufficient conditions for representer theorems in Hilbert\nspaces. We prove necessary and sufficient conditions for the existence of\nrepresenter theorems in reflexive Banach spaces and illustrate why in a sense\nreflexivity is the minimal requirement on the function space. We further show\nthat if the learning relies on the linear representer theorem, then the\nsolution is independent of the regulariser and in fact determined by the\nfunction space alone. This in particular shows the value of generalising\nHilbert space learning theory to Banach spaces. \n\n"}
{"id": "1809.10658", "contents": "Title: Learning to Coordinate Multiple Reinforcement Learning Agents for\n  Diverse Query Reformulation Abstract: We propose a method to efficiently learn diverse strategies in reinforcement\nlearning for query reformulation in the tasks of document retrieval and\nquestion answering. In the proposed framework an agent consists of multiple\nspecialized sub-agents and a meta-agent that learns to aggregate the answers\nfrom sub-agents to produce a final answer. Sub-agents are trained on disjoint\npartitions of the training data, while the meta-agent is trained on the full\ntraining set. Our method makes learning faster, because it is highly\nparallelizable, and has better generalization performance than strong\nbaselines, such as an ensemble of agents trained on the full data. We show that\nthe improved performance is due to the increased diversity of reformulation\nstrategies. \n\n"}
{"id": "1809.10827", "contents": "Title: Weak detection in the spiked Wigner model Abstract: We consider the weak detection problem in a rank-one spiked Wigner data\nmatrix where the signal-to-noise ratio is small so that reliable detection is\nimpossible. We propose a hypothesis test on the presence of the signal by\nutilizing the linear spectral statistics of the data matrix. The test is\ndata-driven and does not require prior knowledge about the distribution of the\nsignal or the noise. When the noise is Gaussian, the proposed test is optimal\nin the sense that its error matches that of the likelihood ratio test, which\nminimizes the sum of the Type-I and Type-II errors. If the density of the noise\nis known and non-Gaussian, the error of the test can be lowered by applying an\nentrywise transformation to the data matrix. We establish a central limit\ntheorem for the linear spectral statistics of general rank-one spiked Wigner\nmatrices as an intermediate step. \n\n"}
{"id": "1809.11108", "contents": "Title: Online Inference with Multi-modal Likelihood Functions Abstract: Let $(Y_t)_{t\\geq 1}$ be a sequence of i.i.d.\\ observations and\n$\\{f_\\theta,\\theta\\in \\mathbb{R}^d\\}$ be a parametric model. We introduce a new\nonline algorithm for computing a sequence $(\\hat{\\theta}_t)_{t\\geq 1}$ which is\nshown to converge almost surely to $\\text{argmax}_{\\theta\\in\n\\mathbb{R}^d}\\mathbb{E}[\\log f_\\theta(Y_1)]$ at rate $ \\mathcal{O}(\\log\n(t)^{(1+\\varepsilon)/2}t^{-1/2})$, with $\\varepsilon>0$ a user specified\nparameter. This convergence result is obtained under standard conditions on the\nstatistical model and, most notably, we allow the mapping $\\theta\\mapsto\n\\mathbb{E}[\\log f_\\theta(Y_1)]$ to be multi-modal. However, the computational\ncost to process each observation grows exponentially with the dimension of\n$\\theta$, which makes the proposed approach applicable to low or moderate\ndimensional problems only. We also derive a version of the estimator\n$\\hat{\\theta}_t$ which is well suited to Student-t linear regression models.\nThe corresponding estimator of the regression coefficients is robust to the\npresence of outliers, as shown by experiments on simulated and real data, and\nthus, as a by-product of this work, we obtain a new online and adaptive robust\nestimation method for linear regression models. \n\n"}
{"id": "1810.00240", "contents": "Title: Reinforcement Learning in R Abstract: Reinforcement learning refers to a group of methods from artificial\nintelligence where an agent performs learning through trial and error. It\ndiffers from supervised learning, since reinforcement learning requires no\nexplicit labels; instead, the agent interacts continuously with its\nenvironment. That is, the agent starts in a specific state and then performs an\naction, based on which it transitions to a new state and, depending on the\noutcome, receives a reward. Different strategies (e.g. Q-learning) have been\nproposed to maximize the overall reward, resulting in a so-called policy, which\ndefines the best possible action in each state. Mathematically, this process\ncan be formalized by a Markov decision process and it has been implemented by\npackages in R; however, there is currently no package available for\nreinforcement learning. As a remedy, this paper demonstrates how to perform\nreinforcement learning in R and, for this purpose, introduces the\nReinforcementLearning package. The package provides a remarkably flexible\nframework and is easily applied to a wide range of different problems. We\ndemonstrate its use by drawing upon common examples from the literature (e.g.\nfinding optimal game strategies). \n\n"}
{"id": "1810.00490", "contents": "Title: Learning Deep Representations from Clinical Data for Chronic Kidney\n  Disease Abstract: We study the behavior of a Time-Aware Long Short-Term Memory Autoencoder, a\nstate-of-the-art method, in the context of learning latent representations from\nirregularly sampled patient data. We identify a key issue in the way such\nrecurrent neural network models are being currently used and show that the\nsolution of the issue leads to significant improvements in the learnt\nrepresentations on both synthetic and real datasets. A detailed analysis of the\nimproved methodology for representing patients suffering from Chronic Kidney\nDisease (CKD) using clinical data is provided. Experimental results show that\nthe proposed T-LSTM model is able to capture the long-term trends in the data,\nwhile effectively handling the noise in the signal. Finally, we show that by\nusing the latent representations of the CKD patients obtained from the T-LSTM\nautoencoder, one can identify unusual patient profiles from the target\npopulation. \n\n"}
{"id": "1810.01765", "contents": "Title: Predicting Factuality of Reporting and Bias of News Media Sources Abstract: We present a study on predicting the factuality of reporting and bias of news\nmedia. While previous work has focused on studying the veracity of claims or\ndocuments, here we are interested in characterizing entire news media. These\nare under-studied but arguably important research problems, both in their own\nright and as a prior for fact-checking systems. We experiment with a large list\nof news websites and with a rich set of features derived from (i) a sample of\narticles from the target news medium, (ii) its Wikipedia page, (iii) its\nTwitter account, (iv) the structure of its URL, and (v) information about the\nWeb traffic it attracts. The experimental results show sizable performance\ngains over the baselines, and confirm the importance of each feature type. \n\n"}
{"id": "1810.01849", "contents": "Title: SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation Abstract: Recent techniques in self-supervised monocular depth estimation are\napproaching the performance of supervised methods, but operate in low\nresolution only. We show that high resolution is key towards high-fidelity\nself-supervised monocular depth prediction. Inspired by recent deep learning\nmethods for Single-Image Super-Resolution, we propose a sub-pixel convolutional\nlayer extension for depth super-resolution that accurately synthesizes\nhigh-resolution disparities from their corresponding low-resolution\nconvolutional features. In addition, we introduce a differentiable\nflip-augmentation layer that accurately fuses predictions from the image and\nits horizontally flipped version, reducing the effect of left and right shadow\nregions generated in the disparity map due to occlusions. Both contributions\nprovide significant performance gains over the state-of-the-art in\nself-supervised depth and pose estimation on the public KITTI benchmark. A\nvideo of our approach can be found at https://youtu.be/jKNgBeBMx0I. \n\n"}
{"id": "1810.01876", "contents": "Title: Spurious samples in deep generative models: bug or feature? Abstract: Traditional wisdom in generative modeling literature is that spurious samples\nthat a model can generate are errors and they should be avoided. Recent\nresearch, however, has shown interest in studying or even exploiting such\nsamples instead of eliminating them. In this paper, we ask the question whether\nsuch samples can be eliminated all together without sacrificing coverage of the\ngenerating distribution. For the class of models we consider, we experimentally\ndemonstrate that this is not possible without losing the ability to model some\nof the test samples. While our results need to be confirmed on a broader set of\nmodel families, these initial findings provide partial evidence that spurious\nsamples share structural properties with the learned dataset, which, in turn,\nsuggests they are not simply errors but a feature of deep generative nets. \n\n"}
{"id": "1810.02528", "contents": "Title: Local Stability and Performance of Simple Gradient Penalty\n  mu-Wasserstein GAN Abstract: Wasserstein GAN(WGAN) is a model that minimizes the Wasserstein distance\nbetween a data distribution and sample distribution. Recent studies have\nproposed stabilizing the training process for the WGAN and implementing the\nLipschitz constraint. In this study, we prove the local stability of optimizing\nthe simple gradient penalty $\\mu$-WGAN(SGP $\\mu$-WGAN) under suitable\nassumptions regarding the equilibrium and penalty measure $\\mu$. The measure\nvalued differentiation concept is employed to deal with the derivative of the\npenalty terms, which is helpful for handling abstract singular measures with\nlower dimensional support. Based on this analysis, we claim that penalizing the\ndata manifold or sample manifold is the key to regularizing the original WGAN\nwith a gradient penalty. Experimental results obtained with unintuitive penalty\nmeasures that satisfy our assumptions are also provided to support our\ntheoretical results. \n\n"}
{"id": "1810.02814", "contents": "Title: Statistical Optimality of Interpolated Nearest Neighbor Algorithms Abstract: In the era of deep learning, understanding over-fitting phenomenon becomes\nincreasingly important. It is observed that carefully designed deep neural\nnetworks achieve small testing error even when the training error is close to\nzero. One possible explanation is that for many modern machine learning\nalgorithms, over-fitting can greatly reduce the estimation bias, while not\nincreasing the estimation variance too much. To illustrate the above idea, we\nprove that the proposed interpolated nearest neighbor algorithm achieves the\nminimax optimal rate in both regression and classification regimes, and observe\nthat they are empirically better than the traditional $k$ nearest neighbor\nmethod in some cases. \n\n"}
{"id": "1810.03068", "contents": "Title: Geometric Scattering for Graph Data Analysis Abstract: We explore the generalization of scattering transforms from traditional\n(e.g., image or audio) signals to graph data, analogous to the generalization\nof ConvNets in geometric deep learning, and the utility of extracted graph\nfeatures in graph data analysis. In particular, we focus on the capacity of\nthese features to retain informative variability and relations in the data\n(e.g., between individual graphs, or in aggregate), while relating our\nconstruction to previous theoretical results that establish the stability of\nsimilar transforms to families of graph deformations. We demonstrate the\napplication the our geometric scattering features in graph classification of\nsocial network data, and in data exploration of biochemistry data. \n\n"}
{"id": "1810.03402", "contents": "Title: Deep LDA Hashing Abstract: The conventional supervised hashing methods based on classification do not\nentirely meet the requirements of hashing technique, but Linear Discriminant\nAnalysis (LDA) does. In this paper, we propose to perform a revised LDA\nobjective over deep networks to learn efficient hashing codes in a truly\nend-to-end fashion. However, the complicated eigenvalue decomposition within\neach mini-batch in every epoch has to be faced with when simply optimizing the\ndeep network w.r.t. the LDA objective. In this work, the revised LDA objective\nis transformed into a simple least square problem, which naturally overcomes\nthe intractable problems and can be easily solved by the off-the-shelf\noptimizer. Such deep extension can also overcome the weakness of LDA Hashing in\nthe limited linear projection and feature learning. Amounts of experiments are\nconducted on three benchmark datasets. The proposed Deep LDA Hashing shows\nnearly 70 points improvement over the conventional one on the CIFAR-10 dataset.\nIt also beats several state-of-the-art methods on various metrics. \n\n"}
{"id": "1810.03527", "contents": "Title: CHOPT : Automated Hyperparameter Optimization Framework for Cloud-Based\n  Machine Learning Platforms Abstract: Many hyperparameter optimization (HyperOpt) methods assume restricted\ncomputing resources and mainly focus on enhancing performance. Here we propose\na novel cloud-based HyperOpt (CHOPT) framework which can efficiently utilize\nshared computing resources while supporting various HyperOpt algorithms. We\nincorporate convenient web-based user interfaces, visualization, and analysis\ntools, enabling users to easily control optimization procedures and build up\nvaluable insights with an iterative analysis procedure. Furthermore, our\nframework can be incorporated with any cloud platform, thus complementarily\nincreasing the efficiency of conventional deep learning frameworks. We\ndemonstrate applications of CHOPT with tasks such as image recognition and\nquestion-answering, showing that our framework can find hyperparameter\nconfigurations competitive with previous work. We also show CHOPT is capable of\nproviding interesting observations through its analysing tools \n\n"}
{"id": "1810.04246", "contents": "Title: Deep clustering: On the link between discriminative models and K-means Abstract: In the context of recent deep clustering studies, discriminative models\ndominate the literature and report the most competitive performances. These\nmodels learn a deep discriminative neural network classifier in which the\nlabels are latent. Typically, they use multinomial logistic regression\nposteriors and parameter regularization, as is very common in supervised\nlearning. It is generally acknowledged that discriminative objective functions\n(e.g., those based on the mutual information or the KL divergence) are more\nflexible than generative approaches (e.g., K-means) in the sense that they make\nfewer assumptions about the data distributions and, typically, yield much\nbetter unsupervised deep learning results. On the surface, several recent\ndiscriminative models may seem unrelated to K-means. This study shows that\nthese models are, in fact, equivalent to K-means under mild conditions and\ncommon posterior models and parameter regularization. We prove that, for the\ncommonly used logistic regression posteriors, maximizing the $L_2$ regularized\nmutual information via an approximate alternating direction method (ADM) is\nequivalent to a soft and regularized K-means loss. Our theoretical analysis not\nonly connects directly several recent state-of-the-art discriminative models to\nK-means, but also leads to a new soft and regularized deep K-means algorithm,\nwhich yields competitive performance on several image clustering benchmarks. \n\n"}
{"id": "1810.05158", "contents": "Title: The local image problem for complex analytic maps Abstract: We address the question \"when the local image of a map is well defined\" and\nanswer it in case of holomorphic map germs with target $(\\bC^{2}, 0)$. We prove\na criterion for holomorphic map germs $(X, x)\\to (Y, y)$ to be locally open,\nsolving a conjecture by Huckleberry in all dimensions. \n\n"}
{"id": "1810.05259", "contents": "Title: Energy distribution of harmonic 1-forms and Jacobians of Riemann\n  surfaces with a short closed geodesic Abstract: We study the energy distribution of harmonic 1-forms on a compact hyperbolic\nRiemann surface $S$ where a short closed geodesic is pinched. If the geodesic\nseparates the surface into two parts, then the Jacobian torus of $S$ develops\ninto a torus that splits. If the geodesic is nonseparating then the Jacobian\ntorus of $S$ degenerates. The aim of this work is to get insight into this\nprocess and give estimates in terms of geometric data of both the initial\nsurface $S$ and the final surface, such as its injectivity radius and the\nlengths of geodesics that form a homology basis. As an invariant we introduce\nnew families of symplectic matrices that compensate for the lack of full\ndimensional Gram-period matrices in the noncompact case. \n\n"}
{"id": "1810.05746", "contents": "Title: On the nonlinearity of quantum dynamical entropy Abstract: Linearity of a dynamical entropy means that the dynamical entropy of the\nn-fold composition of a dynamical map with itself is equal to n times the\ndynamical entropy of the map for every positive integer n. We show that the\nquantum dynamical entropy introduced by Slomczynski and Zyczkowski is nonlinear\nin the time interval between successive measurements of a quantum dynamical\nsystem. This is in contrast to Kolmogorov-Sinai dynamical entropy for classical\ndynamical systems, which is linear in time. We also compute the exact values of\nquantum dynamical entropy for the Hadamard walk with varying Luders-von Neumann\ninstruments and partitions. \n\n"}
{"id": "1810.06474", "contents": "Title: Association measures for interval variables Abstract: Symbolic Data Analysis (SDA) is a relatively new field of statistics that\nextends conventional data analysis by taking into account intrinsic data\nvariability and structure. Unlike conventional data analysis, in SDA the\nfeatures characterizing the data can be multi-valued, such as intervals or\nhistograms. SDA has been mainly approached from a sampling perspective. In this\nwork, we propose a model that links the micro-data and macro-data of\ninterval-valued symbolic variables, which takes a populational perspective.\nUsing this model, we derive the micro-data assumptions underlying the various\ndefinitions of symbolic covariance matrices proposed in the literature, and\nshow that these assumptions can be too restrictive, raising applicability\nconcerns. We analyze the various definitions using worked examples and four\ndatasets. Our results show that the existence/absence of correlations in the\nmacro-data may not be correctly captured by the definitions of symbolic\ncovariance matrices and that, in real data, there can be a strong divergence\nbetween these definitions. Thus, in order to select the most appropriate\ndefinition, one must have some knowledge about the micro-data structure. \n\n"}
{"id": "1810.06695", "contents": "Title: Exploring the Use of Attention within an Neural Machine Translation\n  Decoder States to Translate Idioms Abstract: Idioms pose problems to almost all Machine Translation systems. This type of\nlanguage is very frequent in day-to-day language use and cannot be simply\nignored. The recent interest in memory augmented models in the field of\nLanguage Modelling has aided the systems to achieve good results by bridging\nlong-distance dependencies. In this paper we explore the use of such techniques\ninto a Neural Machine Translation system to help in translation of idiomatic\nlanguage. \n\n"}
{"id": "1810.06801", "contents": "Title: Quasi-hyperbolic momentum and Adam for deep learning Abstract: Momentum-based acceleration of stochastic gradient descent (SGD) is widely\nused in deep learning. We propose the quasi-hyperbolic momentum algorithm (QHM)\nas an extremely simple alteration of momentum SGD, averaging a plain SGD step\nwith a momentum step. We describe numerous connections to and identities with\nother algorithms, and we characterize the set of two-state optimization\nalgorithms that QHM can recover. Finally, we propose a QH variant of Adam\ncalled QHAdam, and we empirically demonstrate that our algorithms lead to\nsignificantly improved training in a variety of settings, including a new\nstate-of-the-art result on WMT16 EN-DE. We hope that these empirical results,\ncombined with the conceptual and practical simplicity of QHM and QHAdam, will\nspur interest from both practitioners and researchers. Code is immediately\navailable. \n\n"}
{"id": "1810.07128", "contents": "Title: High-dimensional Varying Index Coefficient Models via Stein's Identity Abstract: We study the parameter estimation problem for a varying index coefficient\nmodel in high dimensions. Unlike the most existing works that iteratively\nestimate the parameters and link functions, based on the generalized Stein's\nidentity, we propose computationally efficient estimators for the\nhigh-dimensional parameters without estimating the link functions. We consider\ntwo different setups where we either estimate each sparse parameter vector\nindividually or estimate the parameters simultaneously as a sparse or low-rank\nmatrix. For all these cases, our estimators are shown to achieve optimal\nstatistical rates of convergence (up to logarithmic terms in the low-rank\nsetting). Moreover, throughout our analysis, we only require the covariate to\nsatisfy certain moment conditions, which is significantly weaker than the\nGaussian or elliptically symmetric assumptions that are commonly made in the\nexisting literature. Finally, we conduct extensive numerical experiments to\ncorroborate the theoretical results. \n\n"}
{"id": "1810.07291", "contents": "Title: Deep Neural Maps Abstract: We introduce a new unsupervised representation learning and visualization\nusing deep convolutional networks and self organizing maps called Deep Neural\nMaps (DNM). DNM jointly learns an embedding of the input data and a mapping\nfrom the embedding space to a two-dimensional lattice. We compare\nvisualizations of DNM with those of t-SNE and LLE on the MNIST and COIL-20 data\nsets. Our experiments show that the DNM can learn efficient representations of\nthe input data, which reflects characteristics of each class. This is shown via\nback-projecting the neurons of the map on the data space. \n\n"}
{"id": "1810.08322", "contents": "Title: Sequenced-Replacement Sampling for Deep Learning Abstract: We propose sequenced-replacement sampling (SRS) for training deep neural\nnetworks. The basic idea is to assign a fixed sequence index to each sample in\nthe dataset. Once a mini-batch is randomly drawn in each training iteration, we\nrefill the original dataset by successively adding samples according to their\nsequence index. Thus we carry out replacement sampling but in a batched and\nsequenced way. In a sense, SRS could be viewed as a way of performing\n\"mini-batch augmentation\". It is particularly useful for a task where we have a\nrelatively small images-per-class such as CIFAR-100. Together with a longer\nperiod of initial large learning rate, it significantly improves the\nclassification accuracy in CIFAR-100 over the current state-of-the-art results.\nOur experiments indicate that training deeper networks with SRS is less prone\nto over-fitting. In the best case, we achieve an error rate as low as 10.10%. \n\n"}
{"id": "1810.08452", "contents": "Title: Multitask Learning for Large-scale Semantic Change Detection Abstract: Change detection is one of the main problems in remote sensing, and is\nessential to the accurate processing and understanding of the large scale Earth\nobservation data available through programs such as Sentinel and Landsat. Most\nof the recently proposed change detection methods bring deep learning to this\ncontext, but openly available change detection datasets are still very scarce,\nwhich limits the methods that can be proposed and tested. In this paper we\npresent the first large scale high resolution semantic change detection (HRSCD)\ndataset, which enables the usage of deep learning methods for semantic change\ndetection. The dataset contains coregistered RGB image pairs, pixel-wise change\ninformation and land cover information. We then propose several methods using\nfully convolutional neural networks to perform semantic change detection. Most\nnotably, we present a network architecture that performs change detection and\nland cover mapping simultaneously, while using the predicted land cover\ninformation to help to predict changes. We also describe a sequential training\nscheme that allows this network to be trained without setting a hyperparameter\nthat balances different loss functions and achieves the best overall results. \n\n"}
{"id": "1810.08678", "contents": "Title: Optimization of Molecules via Deep Reinforcement Learning Abstract: We present a framework, which we call Molecule Deep $Q$-Networks (MolDQN),\nfor molecule optimization by combining domain knowledge of chemistry and\nstate-of-the-art reinforcement learning techniques (double $Q$-learning and\nrandomized value functions). We directly define modifications on molecules,\nthereby ensuring 100\\% chemical validity. Further, we operate without\npre-training on any dataset to avoid possible bias from the choice of that set.\nInspired by problems faced during medicinal chemistry lead optimization, we\nextend our model with multi-objective reinforcement learning, which maximizes\ndrug-likeness while maintaining similarity to the original molecule. We further\nshow the path through chemical space to achieve optimization for a molecule to\nunderstand how the model works. \n\n"}
{"id": "1810.08742", "contents": "Title: Some remarks on the correspondence between elliptic curves and four\n  points in the Riemann sphere Abstract: In this paper we relate some classical normal forms for complex elliptic\ncurves in terms of 4-point sets in the Riemann sphere. Our main result is an\nalternative proof that every elliptic curve is isomorphic as a Riemann surface\nto one in the Hesse normal form. In this setting, we give an alternative proof\nof the equivalence between the Edwards and the Jacobi normal forms. Also, we\ngive a geometric construction of the cross ratios for 4-point sets in general\nposition. \n\n"}
{"id": "1810.09167", "contents": "Title: Optimal arrangements of hyperplanes for multiclass classification Abstract: In this paper, we present a novel approach to construct multiclass\nclassifiers by means of arrangements of hyperplanes. We propose different mixed\ninteger (linear and non linear) programming formulations for the problem using\nextensions of widely used measures for misclassifying observations where the\n\\textit{kernel trick} can be adapted to be applicable. Some dimensionality\nreductions and variable fixing strategies are also developed for these models.\nAn extensive battery of experiments has been run which reveal the powerfulness\nof our proposal as compared with other previously proposed methodologies. \n\n"}
{"id": "1810.09480", "contents": "Title: Many-body exceptional points in colliding condensates Abstract: Exceptional points describe the coalescence of the eigenmodes of a\nnon-Hermitian matrix. When an exceptional point occurs in the unitary evolution\nof a many-body system, it generically leads to a dynamical instability with a\nfinite wavevector [N. Bernier \\etal, Phys. Rev. Lett. 113, 065303 (2014)].\nHere, we study exceptional points in the context of the counterflow instability\nof colliding Bose-Einstein condensates. We show that the instability of this\nsystem is due to an exceptional point in the Bogoliubov spectrum. We further\nclarify the connection of this effect to the Landau criterion of superfluidity\nand to the scattering of classical particles. We propose an experimental set-up\nto directly probe this exceptional point, and demonstrate its feasibility with\nthe aid of numerical calculations. Our work fosters the observation of\nexceptional points in nonequilibrium many-body quantum systems. \n\n"}
{"id": "1810.09538", "contents": "Title: Pyro: Deep Universal Probabilistic Programming Abstract: Pyro is a probabilistic programming language built on Python as a platform\nfor developing advanced probabilistic models in AI research. To scale to large\ndatasets and high-dimensional models, Pyro uses stochastic variational\ninference algorithms and probability distributions built on top of PyTorch, a\nmodern GPU-accelerated deep learning framework. To accommodate complex or\nmodel-specific algorithmic behavior, Pyro leverages Poutine, a library of\ncomposable building blocks for modifying the behavior of probabilistic\nprograms. \n\n"}
{"id": "1810.09631", "contents": "Title: Point-cloud-based place recognition using CNN feature extraction Abstract: This paper proposes a novel point-cloud-based place recognition system that\nadopts a deep learning approach for feature extraction. By using a\nconvolutional neural network pre-trained on color images to extract features\nfrom a range image without fine-tuning on extra range images, significant\nimprovement has been observed when compared to using hand-crafted features. The\nresulting system is illumination invariant, rotation invariant and robust\nagainst moving objects that are unrelated to the place identity. Apart from the\nsystem itself, we also bring to the community a new place recognition dataset\ncontaining both point cloud and grayscale images covering a full $360^\\circ$\nenvironmental view. In addition, the dataset is organized in such a way that it\nfacilitates experimental validation with respect to rotation invariance or\nrobustness against unrelated moving objects separately. \n\n"}
{"id": "1810.09821", "contents": "Title: Self-Erasing Network for Integral Object Attention Abstract: Recently, adversarial erasing for weakly-supervised object attention has been\ndeeply studied due to its capability in localizing integral object regions.\nHowever, such a strategy raises one key problem that attention regions will\ngradually expand to non-object regions as training iterations continue, which\nsignificantly decreases the quality of the produced attention maps. To tackle\nsuch an issue as well as promote the quality of object attention, we introduce\na simple yet effective Self-Erasing Network (SeeNet) to prohibit attentions\nfrom spreading to unexpected background regions. In particular, SeeNet\nleverages two self-erasing strategies to encourage networks to use reliable\nobject and background cues for learning to attention. In this way, integral\nobject regions can be effectively highlighted without including much more\nbackground regions. To test the quality of the generated attention maps, we\nemploy the mined object regions as heuristic cues for learning semantic\nsegmentation models. Experiments on Pascal VOC well demonstrate the superiority\nof our SeeNet over other state-of-the-art methods. \n\n"}
{"id": "1810.09854", "contents": "Title: Deep Neural Network inference with reduced word length Abstract: Deep neural networks (DNN) are powerful models for many pattern recognition\ntasks, yet their high computational complexity and memory requirement limit\nthem to applications on high-performance computing platforms. In this paper, we\npropose a new method to evaluate DNNs trained with 32bit floating point\n(float32) accuracy using only low precision integer arithmetics in combination\nwith binary shift and clipping operations. Because hardware implementation of\nthese operations is much simpler than high precision floating point\ncalculation, our method can be used for an efficient DNN inference on dedicated\nhardware. In experiments on MNIST, we demonstrate that DNNs trained with\nfloat32 can be evaluated using a combination of 2bit integer arithmetics and a\nfew float32 calculations in each layer or only 3bit integer arithmetics in\ncombination with binary shift and clipping without significant performance\ndegradation. \n\n"}
{"id": "1810.09868", "contents": "Title: Automatic Full Compilation of Julia Programs and ML Models to Cloud TPUs Abstract: Google's Cloud TPUs are a promising new hardware architecture for machine\nlearning workloads. They have powered many of Google's milestone machine\nlearning achievements in recent years. Google has now made TPUs available for\ngeneral use on their cloud platform and as of very recently has opened them up\nfurther to allow use by non-TensorFlow frontends. We describe a method and\nimplementation for offloading suitable sections of Julia programs to TPUs via\nthis new API and the Google XLA compiler. Our method is able to completely fuse\nthe forward pass of a VGG19 model expressed as a Julia program into a single\nTPU executable to be offloaded to the device. Our method composes well with\nexisting compiler-based automatic differentiation techniques on Julia code, and\nwe are thus able to also automatically obtain the VGG19 backwards pass and\nsimilarly offload it to the TPU. Targeting TPUs using our compiler, we are able\nto evaluate the VGG19 forward pass on a batch of 100 images in 0.23s which\ncompares favorably to the 52.4s required for the original model on the CPU. Our\nimplementation is less than 1000 lines of Julia, with no TPU specific changes\nmade to the core Julia compiler or any other Julia packages. \n\n"}
{"id": "1810.10627", "contents": "Title: Streaming Graph Neural Networks Abstract: Graphs are essential representations of many real-world data such as social\nnetworks. Recent years have witnessed the increasing efforts made to extend the\nneural network models to graph-structured data. These methods, which are\nusually known as the graph neural networks, have been applied to advance many\ngraphs related tasks such as reasoning dynamics of the physical system, graph\nclassification, and node classification. Most of the existing graph neural\nnetwork models have been designed for static graphs, while many real-world\ngraphs are inherently dynamic. For example, social networks are naturally\nevolving as new users joining and new relations being created. Current graph\nneural network models cannot utilize the dynamic information in dynamic graphs.\nHowever, the dynamic information has been proven to enhance the performance of\nmany graph analytic tasks such as community detection and link prediction.\nHence, it is necessary to design dedicated graph neural networks for dynamic\ngraphs. In this paper, we propose DGNN, a new {\\bf D}ynamic {\\bf G}raph {\\bf\nN}eural {\\bf N}etwork model, which can model the dynamic information as the\ngraph evolving. In particular, the proposed framework can keep updating node\ninformation by capturing the sequential information of edges (interactions),\nthe time intervals between edges and information propagation coherently.\nExperimental results on various dynamic graphs demonstrate the effectiveness of\nthe proposed framework. \n\n"}
{"id": "1810.11344", "contents": "Title: Benefits of over-parameterization with EM Abstract: Expectation Maximization (EM) is among the most popular algorithms for\nmaximum likelihood estimation, but it is generally only guaranteed to find its\nstationary points of the log-likelihood objective. The goal of this article is\nto present theoretical and empirical evidence that over-parameterization can\nhelp EM avoid spurious local optima in the log-likelihood. We consider the\nproblem of estimating the mean vectors of a Gaussian mixture model in a\nscenario where the mixing weights are known. Our study shows that the global\nbehavior of EM, when one uses an over-parameterized model in which the mixing\nweights are treated as unknown, is better than that when one uses the (correct)\nmodel with the mixing weights fixed to the known values. For symmetric\nGaussians mixtures with two components, we prove that introducing the\n(statistically redundant) weight parameters enables EM to find the global\nmaximizer of the log-likelihood starting from almost any initial mean\nparameters, whereas EM without this over-parameterization may very often fail.\nFor other Gaussian mixtures, we provide empirical evidence that shows similar\nbehavior. Our results corroborate the value of over-parameterization in solving\nnon-convex optimization problems, previously observed in other domains. \n\n"}
{"id": "1810.11389", "contents": "Title: The geometry of domains with negatively pinched K\\\"ahler metrics Abstract: We study how the existence of a negatively pinched K\\\"ahler metric on a\ndomain in complex Euclidean space restricts the geometry of its boundary. In\nparticular, we show that if a convex domain admits a complete K\\\"ahler metric,\nwith pinched negative holomorphic bisectional curvature outside a compact set,\nthen the boundary of the domain does not contain any complex subvariety of\npositive domain. Moreover, if the boundary of the domain is smooth, then it is\nof finite type in the sense of D'Angelo. We also use curvature to provide a\ncharacterization of strong pseudoconvexity amongst convex domains. In\nparticular, we show that a convex domain with $C^{2,\\alpha}$ boundary is\nstrongly pseudoconvex if and only if it admits a complete K\\\"ahler metric with\nsufficiently tight pinched negative holomorphic sectional curvature outside a\ncompact set. \n\n"}
{"id": "1810.11520", "contents": "Title: Spectrogram-channels u-net: a source separation model viewing each\n  channel as the spectrogram of each source Abstract: Sound source separation has attracted attention from Music Information\nRetrieval(MIR) researchers, since it is related to many MIR tasks such as\nautomatic lyric transcription, singer identification, and voice conversion. In\nthis paper, we propose an intuitive spectrogram-based model for source\nseparation by adapting U-Net. We call it Spectrogram-Channels U-Net, which\nmeans each channel of the output corresponds to the spectrogram of separated\nsource itself. The proposed model can be used for not only singing voice\nseparation but also multi-instrument separation by changing only the number of\noutput channels. In addition, we propose a loss function that balances volumes\nbetween different sources. Finally, we yield performance that is\nstate-of-the-art on both separation tasks. \n\n"}
{"id": "1810.11755", "contents": "Title: Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo\n  Tree Search Abstract: Monte Carlo Tree Search (MCTS) algorithms have achieved great success on many\nchallenging benchmarks (e.g., Computer Go). However, they generally require a\nlarge number of rollouts, making their applications costly. Furthermore, it is\nalso extremely challenging to parallelize MCTS due to its inherent sequential\nnature: each rollout heavily relies on the statistics (e.g., node visitation\ncounts) estimated from previous simulations to achieve an effective\nexploration-exploitation tradeoff. In spite of these difficulties, we develop\nan algorithm, WU-UCT, to effectively parallelize MCTS, which achieves linear\nspeedup and exhibits only limited performance loss with an increasing number of\nworkers. The key idea in WU-UCT is a set of statistics that we introduce to\ntrack the number of on-going yet incomplete simulation queries (named as\nunobserved samples). These statistics are used to modify the UCT tree policy in\nthe selection steps in a principled manner to retain effective\nexploration-exploitation tradeoff when we parallelize the most time-consuming\nexpansion and simulation steps. Experiments on a proprietary benchmark and the\nAtari Game benchmark demonstrate the linear speedup and the superior\nperformance of WU-UCT comparing to existing techniques. \n\n"}
{"id": "1810.12030", "contents": "Title: Simon's problem for linear functions Abstract: Simon's problem asks the following: determine if a function $f: \\{0,1\\}^n\n\\rightarrow \\{0,1\\}^n$ is one-to-one or if there exists a unique $s \\in\n\\{0,1\\}^n$ such that $f(x) = f(x \\oplus s)$ for all $x \\in \\{0,1\\}^n$, given\nthe promise that exactly one of the two holds. A classical algorithm that can\nsolve this problem for every $f$ requires $2^{\\Omega(n)}$ queries to $f$. Simon\nshowed that there is a quantum algorithm that can solve this promise problem\nfor every $f$ using only $\\mathcal O(n)$ quantum queries to $f$. A matching\nlower bound on the number of quantum queries was given by Koiran et al., even\nfor functions $f: {\\mathbb{F}_p^n} \\to {\\mathbb{F}_p^n}$. We give a short proof\nthat $\\mathcal O(n)$ quantum queries is optimal even when we are additionally\npromised that $f$ is linear. This is somewhat surprising because for linear\nfunctions there even exists a classical $n$-query algorithm. \n\n"}
{"id": "1810.12161", "contents": "Title: Regularized Maximum Likelihood Estimation and Feature Selection in\n  Mixtures-of-Experts Models Abstract: Mixture of Experts (MoE) are successful models for modeling heterogeneous\ndata in many statistical learning problems including regression, clustering and\nclassification. Generally fitted by maximum likelihood estimation via the\nwell-known EM algorithm, their application to high-dimensional problems is\nstill therefore challenging. We consider the problem of fitting and feature\nselection in MoE models, and propose a regularized maximum likelihood\nestimation approach that encourages sparse solutions for heterogeneous\nregression data models with potentially high-dimensional predictors. Unlike\nstate-of-the art regularized MLE for MoE, the proposed modelings do not require\nan approximate of the penalty function. We develop two hybrid EM algorithms: an\nExpectation-Majorization-Maximization (EM/MM) algorithm, and an EM algorithm\nwith coordinate ascent algorithm. The proposed algorithms allow to\nautomatically obtaining sparse solutions without thresholding, and avoid matrix\ninversion by allowing univariate parameter updates. An experimental study shows\nthe good performance of the algorithms in terms of recovering the actual sparse\nsolutions, parameter estimation, and clustering of heterogeneous regression\ndata. \n\n"}
{"id": "1810.12169", "contents": "Title: Fast Computation of Genome-Metagenome Interaction Effects Abstract: Motivation. Association studies have been widely used to search for\nassociations between common genetic variants observations and a given\nphenotype. However, it is now generally accepted that genes and environment\nmust be examined jointly when estimating phenotypic variance. In this work we\nconsider two types of biological markers: genotypic markers, which characterize\nan observation in terms of inherited genetic information, and metagenomic\nmarker which are related to the environment. Both types of markers are\navailable in their millions and can be used to characterize any observation\nuniquely. Objective. Our focus is on detecting interactions between groups of\ngenetic and metagenomic markers in order to gain a better understanding of the\ncomplex relationship between environment and genome in the expression of a\ngiven phenotype. Contributions. We propose a novel approach for efficiently\ndetecting interactions between complementary datasets in a high-dimensional\nsetting with a reduced computational cost. The method, named SICOMORE, reduces\nthe dimension of the search space by selecting a subset of supervariables in\nthe two complementary datasets. These supervariables are given by a weighted\ngroup structure defined on sets of variables at different scales. A Lasso\nselection is then applied on each type of supervariable to obtain a subset of\npotential interactions that will be explored via linear model testing. Results.\nWe compare SICOMORE with other approaches in simulations, with varying sample\nsizes, noise, and numbers of true interactions. SICOMORE exhibits convincing\nresults in terms of recall, as well as competitive performances with respect to\nrunning time. The method is also used to detect interaction between genomic\nmarkers in Medicago truncatula and metagenomic markers in its rhizosphere\nbacterial community. Software availability. A R package is available, along\nwith its documentation and associated scripts, allowing the reader to reproduce\nthe results presented in the paper. \n\n"}
{"id": "1810.12361", "contents": "Title: Global Non-convex Optimization with Discretized Diffusions Abstract: An Euler discretization of the Langevin diffusion is known to converge to the\nglobal minimizers of certain convex and non-convex optimization problems. We\nshow that this property holds for any suitably smooth diffusion and that\ndifferent diffusions are suitable for optimizing different classes of convex\nand non-convex functions. This allows us to design diffusions suitable for\nglobally optimizing convex and non-convex functions not covered by the existing\nLangevin theory. Our non-asymptotic analysis delivers computable optimization\nand integration error bounds based on easily accessed properties of the\nobjective and chosen diffusion. Central to our approach are new explicit Stein\nfactor bounds on the solutions of Poisson equations. We complement these\nresults with improved optimization guarantees for targets other than the\nstandard Gibbs measure. \n\n"}
{"id": "1810.12881", "contents": "Title: Data Poisoning Attack against Unsupervised Node Embedding Methods Abstract: Unsupervised node embedding methods (e.g., DeepWalk, LINE, and node2vec) have\nattracted growing interests given their simplicity and effectiveness. However,\nalthough these methods have been proved effective in a variety of applications,\nnone of the existing work has analyzed the robustness of them. This could be\nvery risky if these methods are attacked by an adversarial party. In this\npaper, we take the task of link prediction as an example, which is one of the\nmost fundamental problems for graph analysis, and introduce a data positioning\nattack to node embedding methods. We give a complete characterization of\nattacker's utilities and present efficient solutions to adversarial attacks for\ntwo popular node embedding methods: DeepWalk and LINE. We evaluate our proposed\nattack model on multiple real-world graphs. Experimental results show that our\nproposed model can significantly affect the results of link prediction by\nslightly changing the graph structures (e.g., adding or removing a few edges).\nWe also show that our proposed model is very general and can be transferable\nacross different embedding methods. Finally, we conduct a case study on a\ncoauthor network to better understand our attack method. \n\n"}
{"id": "1810.12997", "contents": "Title: An Online-Learning Approach to Inverse Optimization Abstract: In this paper, we demonstrate how to learn the objective function of a\ndecision-maker while only observing the problem input data and the\ndecision-maker's corresponding decisions over multiple rounds. We present exact\nalgorithms for this online version of inverse optimization which converge at a\nrate of $ \\mathcal{O}(1/\\sqrt{T}) $ in the number of observations~$T$ and\ncompare their further properties. Especially, they all allow taking decisions\nwhich are essentially as good as those of the observed decision-maker already\nafter relatively few iterations, but are suited best for different settings\neach. Our approach is based on online learning and works for linear objectives\nover arbitrary feasible sets for which we have a linear optimization oracle. As\nsuch, it generalizes previous approaches based on KKT-system decomposition and\ndualization. We also introduce several generalizations, such as the approximate\nlearning of non-linear objective functions, dynamically changing as well as\nparameterized objectives and the case of suboptimal observed decisions. When\napplied to the stochastic offline case, our algorithms are able to give\nguarantees on the quality of the learned objectives in expectation. Finally, we\nshow the effectiveness and possible applications of our methods in indicative\ncomputational experiments. \n\n"}
{"id": "1810.13155", "contents": "Title: Structure Learning of Deep Neural Networks with Q-Learning Abstract: Recently, with convolutional neural networks gaining significant achievements\nin many challenging machine learning fields, hand-crafted neural networks no\nlonger satisfy our requirements as designing a network will cost a lot, and\nautomatically generating architectures has attracted increasingly more\nattention and focus. Some research on auto-generated networks has achieved\npromising results. However, they mainly aim at picking a series of single\nlayers such as convolution or pooling layers one by one. There are many elegant\nand creative designs in the carefully hand-crafted neural networks, such as\nInception-block in GoogLeNet, residual block in residual network and dense\nblock in dense convolutional network. Based on reinforcement learning and\ntaking advantages of the superiority of these networks, we propose a novel\nautomatic process to design a multi-block neural network, whose architecture\ncontains multiple types of blocks mentioned above, with the purpose to do\nstructure learning of deep neural networks and explore the possibility whether\ndifferent blocks can be composed together to form a well-behaved neural\nnetwork. The optimal network is created by the Q-learning agent who is trained\nto sequentially pick different types of blocks. To verify the validity of our\nproposed method, we use the auto-generated multi-block neural network to\nconduct experiments on image benchmark datasets MNIST, SVHN and CIFAR-10 image\nclassification task with restricted computational resources. The results\ndemonstrate that our method is very effective, achieving comparable or better\nperformance than hand-crafted networks and advanced auto-generated neural\nnetworks. \n\n"}
{"id": "1811.00755", "contents": "Title: A General Framework for Multi-fidelity Bayesian Optimization with\n  Gaussian Processes Abstract: How can we efficiently gather information to optimize an unknown function,\nwhen presented with multiple, mutually dependent information sources with\ndifferent costs? For example, when optimizing a robotic system, intelligently\ntrading off computer simulations and real robot testings can lead to\nsignificant savings. Existing methods, such as multi-fidelity GP-UCB or Entropy\nSearch-based approaches, either make simplistic assumptions on the interaction\namong different fidelities or use simple heuristics that lack theoretical\nguarantees. In this paper, we study multi-fidelity Bayesian optimization with\ncomplex structural dependencies among multiple outputs, and propose\nMF-MI-Greedy, a principled algorithmic framework for addressing this problem.\nIn particular, we model different fidelities using additive Gaussian processes\nbased on shared latent structures with the target function. Then we use\ncost-sensitive mutual information gain for efficient Bayesian global\noptimization. We propose a simple notion of regret which incorporates the cost\nof different fidelities, and prove that MF-MI-Greedy achieves low regret. We\ndemonstrate the strong empirical performance of our algorithm on both synthetic\nand real-world datasets. \n\n"}
{"id": "1811.01305", "contents": "Title: Block-wise Partitioning for Extreme Multi-label Classification Abstract: Extreme multi-label classification aims to learn a classifier that annotates\nan instance with a relevant subset of labels from an extremely large label set.\nMany existing solutions embed the label matrix to a low-dimensional linear\nsubspace, or examine the relevance of a test instance to every label via a\nlinear scan. In practice, however, those approaches can be computationally\nexorbitant. To alleviate this drawback, we propose a Block-wise Partitioning\n(BP) pretreatment that divides all instances into disjoint clusters, to each of\nwhich the most frequently tagged label subset is attached. One multi-label\nclassifier is trained on one pair of instance and label clusters, and the label\nset of a test instance is predicted by first delivering it to the most\nappropriate instance cluster. Experiments on benchmark multi-label data sets\nreveal that BP pretreatment significantly reduces prediction time, and retains\nalmost the same level of prediction accuracy. \n\n"}
{"id": "1811.01558", "contents": "Title: Stochastic Modified Equations and Dynamics of Stochastic Gradient\n  Algorithms I: Mathematical Foundations Abstract: We develop the mathematical foundations of the stochastic modified equations\n(SME) framework for analyzing the dynamics of stochastic gradient algorithms,\nwhere the latter is approximated by a class of stochastic differential\nequations with small noise parameters. We prove that this approximation can be\nunderstood mathematically as an weak approximation, which leads to a number of\nprecise and useful results on the approximations of stochastic gradient descent\n(SGD), momentum SGD and stochastic Nesterov's accelerated gradient method in\nthe general setting of stochastic objectives. We also demonstrate through\nexplicit calculations that this continuous-time approach can uncover important\nanalytical insights into the stochastic gradient algorithms under consideration\nthat may not be easy to obtain in a purely discrete-time setting. \n\n"}
{"id": "1811.01713", "contents": "Title: Word Mover's Embedding: From Word2Vec to Document Embedding Abstract: While the celebrated Word2Vec technique yields semantically rich\nrepresentations for individual words, there has been relatively less success in\nextending to generate unsupervised sentences or documents embeddings. Recent\nwork has demonstrated that a distance measure between documents called\n\\emph{Word Mover's Distance} (WMD) that aligns semantically similar words,\nyields unprecedented KNN classification accuracy. However, WMD is expensive to\ncompute, and it is hard to extend its use beyond a KNN classifier. In this\npaper, we propose the \\emph{Word Mover's Embedding } (WME), a novel approach to\nbuilding an unsupervised document (sentence) embedding from pre-trained word\nembeddings. In our experiments on 9 benchmark text classification datasets and\n22 textual similarity tasks, the proposed technique consistently matches or\noutperforms state-of-the-art techniques, with significantly higher accuracy on\nproblems of short length. \n\n"}
{"id": "1811.02096", "contents": "Title: Scale calibration for high-dimensional robust regression Abstract: We present a new method for high-dimensional linear regression when a scale\nparameter of the additive errors is unknown. The proposed estimator is based on\na penalized Huber $M$-estimator, for which theoretical results on estimation\nerror have recently been proposed in high-dimensional statistics literature.\nHowever, the variance of the error term in the linear model is intricately\nconnected to the optimal parameter used to define the shape of the Huber loss.\nOur main idea is to use an adaptive technique, based on Lepski's method, to\novercome the difficulties in solving a joint nonconvex optimization problem\nwith respect to the location and scale parameters. \n\n"}
{"id": "1811.03179", "contents": "Title: How Well Generative Adversarial Networks Learn Distributions Abstract: This paper studies the rates of convergence for learning distributions\nimplicitly with the adversarial framework and Generative Adversarial Networks\n(GANs), which subsume Wasserstein, Sobolev, MMD GAN, and Generalized/Simulated\nMethod of Moments (GMM/SMM) as special cases. We study a wide range of\nparametric and nonparametric target distributions under a host of objective\nevaluation metrics. We investigate how to obtain valid statistical guarantees\nfor GANs through the lens of regularization. On the nonparametric end, we\nderive the optimal minimax rates for distribution estimation under the\nadversarial framework. On the parametric end, we establish a theory for general\nneural network classes (including deep leaky ReLU networks) that characterizes\nthe interplay on the choice of generator and discriminator pair. We discover\nand isolate a new notion of regularization, called the\ngenerator-discriminator-pair regularization, that sheds light on the advantage\nof GANs compared to classical parametric and nonparametric approaches for\nexplicit distribution estimation. We develop novel oracle inequalities as the\nmain technical tools for analyzing GANs, which are of independent interest. \n\n"}
{"id": "1811.03305", "contents": "Title: BAR: Bayesian Activity Recognition using variational inference Abstract: Uncertainty estimation in deep neural networks is essential for designing\nreliable and robust AI systems. Applications such as video surveillance for\nidentifying suspicious activities are designed with deep neural networks\n(DNNs), but DNNs do not provide uncertainty estimates. Capturing reliable\nuncertainty estimates in safety and security critical applications will help to\nestablish trust in the AI system. Our contribution is to apply Bayesian deep\nlearning framework to visual activity recognition application and quantify\nmodel uncertainty along with principled confidence. We utilize the stochastic\nvariational inference technique while training the Bayesian DNNs to infer the\napproximate posterior distribution around model parameters and perform Monte\nCarlo sampling on the posterior of model parameters to obtain the predictive\ndistribution. We show that the Bayesian inference applied to DNNs provide\nreliable confidence measures for visual activity recognition task as compared\nto conventional DNNs. We also show that our method improves the visual activity\nrecognition precision-recall AUC by 6.2% compared to non-Bayesian baseline. We\nevaluate our models on Moments-In-Time (MiT) activity recognition dataset by\nselecting a subset of in- and out-of-distribution video samples. \n\n"}
{"id": "1811.04477", "contents": "Title: Unifying Gaussian LWF and AMP Chain Graphs to Model Interference Abstract: An intervention may have an effect on units other than those to which it was\nadministered. This phenomenon is called interference and it usually goes\nunmodeled. In this paper, we propose to combine Lauritzen-Wermuth-Frydenberg\nand Andersson-Madigan-Perlman chain graphs to create a new class of causal\nmodels that can represent both interference and non-interference relationships\nfor Gaussian distributions. Specifically, we define the new class of models,\nintroduce global and local and pairwise Markov properties for them, and prove\ntheir equivalence. We also propose an algorithm for maximum likelihood\nparameter estimation for the new models, and report experimental results.\nFinally, we show how to compute the effects of interventions in the new models. \n\n"}
{"id": "1811.04770", "contents": "Title: Packing Sparse Convolutional Neural Networks for Efficient Systolic\n  Array Implementations: Column Combining Under Joint Optimization Abstract: This paper describes a novel approach of packing sparse convolutional neural\nnetworks for their efficient systolic array implementations. By combining\nsubsets of columns in the original filter matrix associated with a\nconvolutional layer, we increase the utilization efficiency of the systolic\narray substantially (e.g., ~4x) due to the increased density of nonzeros in the\nresulting packed filter matrix. In combining columns, for each row, all filter\nweights but one with the largest magnitude are pruned. We retrain the remaining\nweights to preserve high accuracy. We demonstrate that in mitigating data\nprivacy concerns the retraining can be accomplished with only fractions of the\noriginal dataset (e.g., 10\\% for CIFAR-10). We study the effectiveness of this\njoint optimization for both high utilization and classification accuracy with\nASIC and FPGA designs based on efficient bit-serial implementations of\nmultiplier-accumulators. We present analysis and empirical evidence on the\nsuperior performance of our column combining approach against prior arts under\nmetrics such as energy efficiency (3x) and inference latency (12x). \n\n"}
{"id": "1811.05370", "contents": "Title: Unsupervised Transfer Learning for Spoken Language Understanding in\n  Intelligent Agents Abstract: User interaction with voice-powered agents generates large amounts of\nunlabeled utterances. In this paper, we explore techniques to efficiently\ntransfer the knowledge from these unlabeled utterances to improve model\nperformance on Spoken Language Understanding (SLU) tasks. We use Embeddings\nfrom Language Model (ELMo) to take advantage of unlabeled data by learning\ncontextualized word representations. Additionally, we propose ELMo-Light\n(ELMoL), a faster and simpler unsupervised pre-training method for SLU. Our\nfindings suggest unsupervised pre-training on a large corpora of unlabeled\nutterances leads to significantly better SLU performance compared to training\nfrom scratch and it can even outperform conventional supervised transfer.\nAdditionally, we show that the gains from unsupervised transfer techniques can\nbe further improved by supervised transfer. The improvements are more\npronounced in low resource settings and when using only 1000 labeled in-domain\nsamples, our techniques match the performance of training from scratch on\n10-15x more labeled in-domain data. \n\n"}
{"id": "1811.05850", "contents": "Title: Drop-Activation: Implicit Parameter Reduction and Harmonic\n  Regularization Abstract: Overfitting frequently occurs in deep learning. In this paper, we propose a\nnovel regularization method called Drop-Activation to reduce overfitting and\nimprove generalization. The key idea is to drop nonlinear activation functions\nby setting them to be identity functions randomly during training time. During\ntesting, we use a deterministic network with a new activation function to\nencode the average effect of dropping activations randomly. Our theoretical\nanalyses support the regularization effect of Drop-Activation as implicit\nparameter reduction and verify its capability to be used together with Batch\nNormalization (Ioffe and Szegedy 2015). The experimental results on CIFAR-10,\nCIFAR-100, SVHN, EMNIST, and ImageNet show that Drop-Activation generally\nimproves the performance of popular neural network architectures for the image\nclassification task. Furthermore, as a regularizer Drop-Activation can be used\nin harmony with standard training and regularization techniques such as Batch\nNormalization and Auto Augment (Cubuk et al. 2019). The code is available at\n\\url{https://github.com/LeungSamWai/Drop-Activation}. \n\n"}
{"id": "1811.05868", "contents": "Title: Pitfalls of Graph Neural Network Evaluation Abstract: Semi-supervised node classification in graphs is a fundamental problem in\ngraph mining, and the recently proposed graph neural networks (GNNs) have\nachieved unparalleled results on this task. Due to their massive success, GNNs\nhave attracted a lot of attention, and many novel architectures have been put\nforward. In this paper we show that existing evaluation strategies for GNN\nmodels have serious shortcomings. We show that using the same\ntrain/validation/test splits of the same datasets, as well as making\nsignificant changes to the training procedure (e.g. early stopping criteria)\nprecludes a fair comparison of different architectures. We perform a thorough\nempirical evaluation of four prominent GNN models and show that considering\ndifferent splits of the data leads to dramatically different rankings of\nmodels. Even more importantly, our findings suggest that simpler GNN\narchitectures are able to outperform the more sophisticated ones if the\nhyperparameters and the training procedure are tuned fairly for all models. \n\n"}
{"id": "1811.05933", "contents": "Title: Deep Nonlinear Non-Gaussian Filtering for Dynamical Systems Abstract: Filtering is a general name for inferring the states of a dynamical system\ngiven observations. The most common filtering approach is Gaussian Filtering\n(GF) where the distribution of the inferred states is a Gaussian whose mean is\nan affine function of the observations. There are two restrictions in this\nmodel: Gaussianity and Affinity. We propose a model to relax both these\nassumptions based on recent advances in implicit generative models. Empirical\nresults show that the proposed method gives a significant advantage over GF and\nnonlinear methods based on fixed nonlinear kernels. \n\n"}
{"id": "1811.06524", "contents": "Title: Exploiting Class Learnability in Noisy Data Abstract: In many domains, collecting sufficient labeled training data for supervised\nmachine learning requires easily accessible but noisy sources, such as\ncrowdsourcing services or tagged Web data. Noisy labels occur frequently in\ndata sets harvested via these means, sometimes resulting in entire classes of\ndata on which learned classifiers generalize poorly. For real world\napplications, we argue that it can be beneficial to avoid training on such\nclasses entirely. In this work, we aim to explore the classes in a given data\nset, and guide supervised training to spend time on a class proportional to its\nlearnability. By focusing the training process, we aim to improve model\ngeneralization on classes with a strong signal. To that end, we develop an\nonline algorithm that works in conjunction with classifier and training\nalgorithm, iteratively selecting training data for the classifier based on how\nwell it appears to generalize on each class. Testing our approach on a variety\nof data sets, we show our algorithm learns to focus on classes for which the\nmodel has low generalization error relative to strong baselines, yielding a\nclassifier with good performance on learnable classes. \n\n"}
{"id": "1811.06909", "contents": "Title: Dynamics of fibered endomorphisms of $\\mathbb P^k$ Abstract: We study the structure and the Lyapunov exponents of the equilibrium measure\nof endomorphisms of $\\mathbb P^k$ preserving a fibration. We extend the\ndecomposition of the equilibrium measure obtained by Jonsson for polynomial\nskew products of $\\mathbb C^2$. We also show that the sum of the sectional\nexponents satisfies a Bedford-Jonsson formula when the fibration is linear, and\nthat this function is plurisubharmonic on families of fibered endomorphisms. In\nparticular, the sectional part of the bifurcation current is a closed positive\ncurrent on the parameter space. \n\n"}
{"id": "1811.06930", "contents": "Title: Pre-training Graph Neural Networks with Kernels Abstract: Many machine learning techniques have been proposed in the last few years to\nprocess data represented in graph-structured form. Graphs can be used to model\nseveral scenarios, from molecules and materials to RNA secondary structures.\nSeveral kernel functions have been defined on graphs that coupled with\nkernelized learning algorithms, have shown state-of-the-art performances on\nmany tasks. Recently, several definitions of Neural Networks for Graph (GNNs)\nhave been proposed, but their accuracy is not yet satisfying. In this paper, we\npropose a task-independent pre-training methodology that allows a GNN to learn\nthe representation induced by state-of-the-art graph kernels. Then, the\nsupervised learning phase will fine-tune this representation for the task at\nhand. The proposed technique is agnostic on the adopted GNN architecture and\nkernel function, and shows consistent improvements in the predictive\nperformance of GNNs in our preliminary experimental results. \n\n"}
{"id": "1811.07209", "contents": "Title: A Statistical Approach to Assessing Neural Network Robustness Abstract: We present a new approach to assessing the robustness of neural networks\nbased on estimating the proportion of inputs for which a property is violated.\nSpecifically, we estimate the probability of the event that the property is\nviolated under an input model. Our approach critically varies from the formal\nverification framework in that when the property can be violated, it provides\nan informative notion of how robust the network is, rather than just the\nconventional assertion that the network is not verifiable. Furthermore, it\nprovides an ability to scale to larger networks than formal verification\napproaches. Though the framework still provides a formal guarantee of\nsatisfiability whenever it successfully finds one or more violations, these\nadvantages do come at the cost of only providing a statistical estimate of\nunsatisfiability whenever no violation is found. Key to the practical success\nof our approach is an adaptation of multi-level splitting, a Monte Carlo\napproach for estimating the probability of rare events, to our statistical\nrobustness framework. We demonstrate that our approach is able to emulate\nformal verification procedures on benchmark problems, while scaling to larger\nnetworks and providing reliable additional information in the form of accurate\nestimates of the violation probability. \n\n"}
{"id": "1811.07988", "contents": "Title: Automated Pain Detection from Facial Expressions using FACS: A Review Abstract: Facial pain expression is an important modality for assessing pain,\nespecially when the patient's verbal ability to communicate is impaired. The\nfacial muscle-based action units (AUs), which are defined by the Facial Action\nCoding System (FACS), have been widely studied and are highly reliable as a\nmethod for detecting facial expressions (FE) including valid detection of pain.\nUnfortunately, FACS coding by humans is a very time-consuming task that makes\nits clinical use prohibitive. Significant progress on automated facial\nexpression recognition (AFER) has led to its numerous successful applications\nin FACS-based affective computing problems. However, only a handful of studies\nhave been reported on automated pain detection (APD), and its application in\nclinical settings is still far from a reality. In this paper, we review the\nprogress in research that has contributed to automated pain detection, with\nfocus on 1) the framework-level similarity between spontaneous AFER and APD\nproblems; 2) the evolution of system design including the recent development of\ndeep learning methods; 3) the strategies and considerations in developing a\nFACS-based pain detection framework from existing research; and 4) introduction\nof the most relevant databases that are available for AFER and APD studies. We\nattempt to present key considerations in extending a general AFER framework to\nan APD framework in clinical settings. In addition, the performance metrics are\nalso highlighted in evaluating an AFER or an APD system. \n\n"}
{"id": "1811.08290", "contents": "Title: An Efficient Optical Flow Based Motion Detection Method for\n  Non-stationary Scenes Abstract: Real-time motion detection in non-stationary scenes is a difficult task due\nto dynamic background, changing foreground appearance and limited computational\nresource. These challenges degrade the performance of the existing methods in\npractical applications. In this paper, an optical flow based framework is\nproposed to address this problem. By applying a novel strategy to utilize\noptical flow, we enable our method being free of model constructing, training\nor updating and can be performed efficiently. Besides, a dual judgment\nmechanism with adaptive intervals and adaptive thresholds is designed to\nheighten the system's adaptation to different situations. In experiment part,\nwe quantitatively and qualitatively validate the effectiveness and feasibility\nof our method with videos in various scene conditions. The experimental results\nshow that our method adapts itself to different situations and outperforms the\nstate-of-the-art real-time methods, indicating the advantages of our optical\nflow based method. \n\n"}
{"id": "1811.08634", "contents": "Title: Synetgy: Algorithm-hardware Co-design for ConvNet Accelerators on\n  Embedded FPGAs Abstract: Using FPGAs to accelerate ConvNets has attracted significant attention in\nrecent years. However, FPGA accelerator design has not leveraged the latest\nprogress of ConvNets. As a result, the key application characteristics such as\nframes-per-second (FPS) are ignored in favor of simply counting GOPs, and\nresults on accuracy, which is critical to application success, are often not\neven reported. In this work, we adopt an algorithm-hardware co-design approach\nto develop a ConvNet accelerator called Synetgy and a novel ConvNet model\ncalled DiracDeltaNet$^{\\dagger}$. Both the accelerator and ConvNet are tailored\nto FPGA requirements. DiracDeltaNet, as the name suggests, is a ConvNet with\nonly $1\\times 1$ convolutions while spatial convolutions are replaced by more\nefficient shift operations. DiracDeltaNet achieves competitive accuracy on\nImageNet (88.7\\% top-5), but with 42$\\times$ fewer parameters and 48$\\times$\nfewer OPs than VGG16. We further quantize DiracDeltaNet's weights to 4-bit and\nactivations to 4-bits, with less than 1\\% accuracy loss. These quantizations\nexploit well the nature of FPGA hardware. In short, DiracDeltaNet's small model\nsize, low computational OP count, low precision and simplified operators allow\nus to co-design a highly customized computing unit for an FPGA. We implement\nthe computing units for DiracDeltaNet on an Ultra96 SoC system through\nhigh-level synthesis. Our accelerator's final top-5 accuracy of 88.1\\% on\nImageNet, is higher than all the previously reported embedded FPGA\naccelerators. In addition, the accelerator reaches an inference speed of 66.3\nFPS on the ImageNet classification task, surpassing prior works with similar\naccuracy by at least 11.6$\\times$. \n\n"}
{"id": "1811.08996", "contents": "Title: HyperAdam: A Learnable Task-Adaptive Adam for Network Training Abstract: Deep neural networks are traditionally trained using human-designed\nstochastic optimization algorithms, such as SGD and Adam. Recently, the\napproach of learning to optimize network parameters has emerged as a promising\nresearch topic. However, these learned black-box optimizers sometimes do not\nfully utilize the experience in human-designed optimizers, therefore have\nlimitation in generalization ability. In this paper, a new optimizer, dubbed as\n\\textit{HyperAdam}, is proposed that combines the idea of \"learning to\noptimize\" and traditional Adam optimizer. Given a network for training, its\nparameter update in each iteration generated by HyperAdam is an adaptive\ncombination of multiple updates generated by Adam with varying decay rates. The\ncombination weights and decay rates in HyperAdam are adaptively learned\ndepending on the task. HyperAdam is modeled as a recurrent neural network with\nAdamCell, WeightCell and StateCell. It is justified to be state-of-the-art for\nvarious network training, such as multilayer perceptron, CNN and LSTM. \n\n"}
{"id": "1811.09083", "contents": "Title: Learning Goal Embeddings via Self-Play for Hierarchical Reinforcement\n  Learning Abstract: In hierarchical reinforcement learning a major challenge is determining\nappropriate low-level policies. We propose an unsupervised learning scheme,\nbased on asymmetric self-play from Sukhbaatar et al. (2018), that automatically\nlearns a good representation of sub-goals in the environment and a low-level\npolicy that can execute them. A high-level policy can then direct the lower one\nby generating a sequence of continuous sub-goal vectors. We evaluate our model\nusing Mazebase and Mujoco environments, including the challenging AntGather\ntask. Visualizations of the sub-goal embeddings reveal a logical decomposition\nof tasks within the environment. Quantitatively, our approach obtains\ncompelling performance gains over non-hierarchical approaches. \n\n"}
{"id": "1811.09098", "contents": "Title: A pointwise norm on a non-reduced analytic space Abstract: Let $X$ be a possibly non-reduced space of pure dimension. We introduce an\nessentially intrinsic pointwise Hermitian norm on smooth $(0,*)$-forms, in\nparticular on holomorphic functions, on $X$. We prove that the space of\nholomorphic functions is complete with respect to the natural topology induced\nby this norm. \n\n"}
{"id": "1811.09236", "contents": "Title: Copy the Old or Paint Anew? An Adversarial Framework for (non-)\n  Parametric Image Stylization Abstract: Parametric generative deep models are state-of-the-art for photo and\nnon-photo realistic image stylization. However, learning complicated image\nrepresentations requires compute-intense models parametrized by a huge number\nof weights, which in turn requires large datasets to make learning successful.\nNon-parametric exemplar-based generation is a technique that works well to\nreproduce style from small datasets, but is also compute-intensive. These\naspects are a drawback for the practice of digital AI artists: typically one\nwants to use a small set of stylization images, and needs a fast flexible model\nin order to experiment with it. With this motivation, our work has these\ncontributions: (i) a novel stylization method called Fully Adversarial Mosaics\n(FAMOS) that combines the strengths of both parametric and non-parametric\napproaches; (ii) multiple ablations and image examples that analyze the method\nand show its capabilities; (iii) source code that will empower artists and\nmachine learning researchers to use and modify FAMOS. \n\n"}
{"id": "1811.09350", "contents": "Title: Predicting Diabetes Disease Evolution Using Financial Records and\n  Recurrent Neural Networks Abstract: Managing patients with chronic diseases is a major and growing healthcare\nchallenge in several countries. A chronic condition, such as diabetes, is an\nillness that lasts a long time and does not go away, and often leads to the\npatient's health gradually getting worse. While recent works involve raw\nelectronic health record (EHR) from hospitals, this work uses only financial\nrecords from health plan providers (medical claims) to predict diabetes disease\nevolution with a self-attentive recurrent neural network. The use of financial\ndata is due to the possibility of being an interface to international\nstandards, as the records standard encodes medical procedures. The main goal\nwas to assess high risk diabetics, so we predict records related to diabetes\nacute complications such as amputations and debridements, revascularization and\nhemodialysis. Our work succeeds to anticipate complications between 60 to 240\ndays with an area under ROC curve ranging from 0.81 to 0.94. In this paper we\ndescribe the first half of a work-in-progress developed within a health plan\nprovider with ROC curve ranging from 0.81 to 0.83. This assessment will give\nhealthcare providers the chance to intervene earlier and head off\nhospitalizations. We are aiming to deliver personalized predictions and\npersonalized recommendations to individual patients, with the goal of improving\noutcomes and reducing costs \n\n"}
{"id": "1811.10106", "contents": "Title: Sparse PCA from Sparse Linear Regression Abstract: Sparse Principal Component Analysis (SPCA) and Sparse Linear Regression (SLR)\nhave a wide range of applications and have attracted a tremendous amount of\nattention in the last two decades as canonical examples of statistical problems\nin high dimension. A variety of algorithms have been proposed for both SPCA and\nSLR, but an explicit connection between the two had not been made. We show how\nto efficiently transform a black-box solver for SLR into an algorithm for SPCA:\nassuming the SLR solver satisfies prediction error guarantees achieved by\nexisting efficient algorithms such as those based on the Lasso, the SPCA\nalgorithm derived from it achieves near state of the art guarantees for testing\nand for support recovery for the single spiked covariance model as obtained by\nthe current best polynomialtime algorithms. Our reduction not only highlights\nthe inherent similarity between the two problems, but also, from a practical\nstandpoint, allows one to obtain a collection of algorithms for SPCA directly\nfrom known algorithms for SLR. We provide experimental results on simulated\ndata comparing our proposed framework to other algorithms for SPCA. \n\n"}
{"id": "1811.10154", "contents": "Title: Stop Explaining Black Box Machine Learning Models for High Stakes\n  Decisions and Use Interpretable Models Instead Abstract: Black box machine learning models are currently being used for high stakes\ndecision-making throughout society, causing problems throughout healthcare,\ncriminal justice, and in other domains. People have hoped that creating methods\nfor explaining these black box models will alleviate some of these problems,\nbut trying to \\textit{explain} black box models, rather than creating models\nthat are \\textit{interpretable} in the first place, is likely to perpetuate bad\npractices and can potentially cause catastrophic harm to society. There is a\nway forward -- it is to design models that are inherently interpretable. This\nmanuscript clarifies the chasm between explaining black boxes and using\ninherently interpretable models, outlines several key reasons why explainable\nblack boxes should be avoided in high-stakes decisions, identifies challenges\nto interpretable machine learning, and provides several example applications\nwhere interpretable models could potentially replace black box models in\ncriminal justice, healthcare, and computer vision. \n\n"}
{"id": "1811.10382", "contents": "Title: Heterogeneous multireference alignment for images with application to\n  2-D classification in single particle reconstruction Abstract: Motivated by the task of 2-D classification in single particle reconstruction\nby cryo-electron microscopy (cryo-EM), we consider the problem of heterogeneous\nmultireference alignment of images. In this problem, the goal is to estimate a\n(typically small) set of target images from a (typically large) collection of\nobservations. Each observation is a rotated, noisy version of one of the target\nimages. For each individual observation, neither the rotation nor which target\nimage has been rotated are known. As the noise level in cryo-EM data is high,\nclustering the observations and estimating individual rotations is challenging.\nWe propose a framework to estimate the target images directly from the\nobservations, completely bypassing the need to cluster or register the images.\nThe framework consists of two steps. First, we estimate rotation-invariant\nfeatures of the images, such as the bispectrum. These features can be estimated\nto any desired accuracy, at any noise level, provided sufficiently many\nobservations are collected. Then, we estimate the images from the invariant\nfeatures. Numerical experiments on synthetic cryo-EM datasets demonstrate the\neffectiveness of the method. Ultimately, we outline future developments\nrequired to apply this method to experimental data. \n\n"}
{"id": "1811.10790", "contents": "Title: High-dimensional Index Volatility Models via Stein's Identity Abstract: We study the estimation of the parametric components of single and multiple\nindex volatility models. Using the first- and second-order Stein's identities,\nwe develop methods that are applicable for the estimation of the variance index\nin the high-dimensional setting requiring finite moment condition, which allows\nfor heavy-tailed data. Our approach complements the existing literature in the\nlow-dimensional setting, while relaxing the conditions on estimation, and\nprovides a novel approach in the high-dimensional setting. We prove that the\nstatistical rate of convergence of our variance index estimators consists of a\nparametric rate and a nonparametric rate, where the latter appears from the\nestimation of the mean link function. However, under standard assumptions, the\nparametric rate dominates the rate of convergence and our results match the\nminimax optimal rate for the mean index estimation. Simulation results\nillustrate finite sample properties of our methodology and back our theoretical\nconclusions. \n\n"}
{"id": "1811.10978", "contents": "Title: Neural Non-Stationary Spectral Kernel Abstract: Standard kernels such as Mat\\'ern or RBF kernels only encode simple monotonic\ndependencies within the input space. Spectral mixture kernels have been\nproposed as general-purpose, flexible kernels for learning and discovering more\ncomplicated patterns in the data. Spectral mixture kernels have recently been\ngeneralized into non-stationary kernels by replacing the mixture weights,\nfrequency means and variances by input-dependent functions. These functions\nhave also been modelled as Gaussian processes on their own. In this paper we\npropose modelling the hyperparameter functions with neural networks, and\nprovide an experimental comparison between the stationary spectral mixture and\nthe two non-stationary spectral mixtures. Scalable Gaussian process inference\nis implemented within the sparse variational framework for all the kernels\nconsidered. We show that the neural variant of the kernel is able to achieve\nthe best performance, among alternatives, on several benchmark datasets. \n\n"}
{"id": "1811.11079", "contents": "Title: Robust Classification of Financial Risk Abstract: Algorithms are increasingly common components of high-impact decision-making,\nand a growing body of literature on adversarial examples in laboratory settings\nindicates that standard machine learning models are not robust. This suggests\nthat real-world systems are also susceptible to manipulation or\nmisclassification, which especially poses a challenge to machine learning\nmodels used in financial services. We use the loan grade classification problem\nto explore how machine learning models are sensitive to small changes in\nuser-reported data, using adversarial attacks documented in the literature and\nan original, domain-specific attack. Our work shows that a robust optimization\nalgorithm can build models for financial services that are resistant to\nmisclassification on perturbations. To the best of our knowledge, this is the\nfirst study of adversarial attacks and defenses for deep learning in financial\nservices. \n\n"}
{"id": "1811.11325", "contents": "Title: CyLKs: Unsupervised Cycle Lucas-Kanade Network for Landmark Tracking Abstract: Across a majority of modern learning-based tracking systems, expensive\nannotations are needed to achieve state-of-the-art performance. In contrast,\nthe Lucas-Kanade (LK) algorithm works well without any annotation. However, LK\nhas a strong assumption of photometric (brightness) consistency on image\nintensity and is easy to drift because of large motion, occlusion, and aperture\nproblem. To relax the assumption and alleviate the drift problem, we propose\nCyLKs, a data-driven way of training Lucas-Kanade in an unsupervised manner.\nCyLKs learns a feature transformation through CNNs, transforming the input\nimages to a feature space which is especially favorable to LK tracking. During\ntraining, we perform differentiable Lucas-Kanade forward and backward on the\nconvolutional feature maps, and then minimize the re-projection error. During\ntesting, we perform the LK tracking on the learned features. We apply our model\nto the task of landmark tracking and perform experiments on datasets of THUMOS\nand 300VW. \n\n"}
{"id": "1811.12234", "contents": "Title: Machine Learning on Electronic Health Records: Models and Features\n  Usages to predict Medication Non-Adherence Abstract: Adherence can be defined as \"the extent to which patients take their\nmedications as prescribed by their healthcare providers\"[Osterberg and\nBlaschke, 2005]. World Health Organization's reports point out that, in\ndeveloped countries, only about 50% of patients with chronic diseases correctly\nfollow their treatments. This severely compromises the efficiency of long-term\ntherapy and increases the cost of health services. We propose in this paper\ndifferent models of patient drug consumption in breast cancer treatments. The\naim of these different approaches is to predict medication non-adherence while\ngiving insights to doctors of the underlying reasons of these illegitimate\ndrop-outs. Working with oncologists, we show the interest of Machine- Learning\nalgorithms fined tune by the feedback of experts to estimate a risk score of a\npatient's non-adherence and thus improve support throughout their care path. \n\n"}
{"id": "1811.12556", "contents": "Title: How to Organize your Deep Reinforcement Learning Agents: The Importance\n  of Communication Topology Abstract: In this empirical paper, we investigate how learning agents can be arranged\nin more efficient communication topologies for improved learning. This is an\nimportant problem because a common technique to improve speed and robustness of\nlearning in deep reinforcement learning and many other machine learning\nalgorithms is to run multiple learning agents in parallel. The standard\ncommunication architecture typically involves all agents intermittently\ncommunicating with each other (fully connected topology) or with a centralized\nserver (star topology). Unfortunately, optimizing the topology of communication\nover the space of all possible graphs is a hard problem, so we borrow results\nfrom the networked optimization and collective intelligence literatures which\nsuggest that certain families of network topologies can lead to strong\nimprovements over fully-connected networks. We start by introducing alternative\nnetwork topologies to DRL benchmark tasks under the Evolution Strategies\nparadigm which we call Network Evolution Strategies. We explore the relative\nperformance of the four main graph families and observe that one such family\n(Erdos-Renyi random graphs) empirically outperforms all other families,\nincluding the de facto fully-connected communication topologies. Additionally,\nthe use of alternative network topologies has a multiplicative performance\neffect: we observe that when 1000 learning agents are arranged in a carefully\ndesigned communication topology, they can compete with 3000 agents arranged in\nthe de facto fully-connected topology. Overall, our work suggests that\ndistributed machine learning algorithms would learn more efficiently if the\ncommunication topology between learning agents was optimized. \n\n"}
{"id": "1811.12852", "contents": "Title: Optimal Data Driven Resource Allocation under Multi-Armed Bandit\n  Observations Abstract: This paper introduces the first asymptotically optimal strategy for a multi\narmed bandit (MAB) model under side constraints. The side constraints model\nsituations in which bandit activations are limited by the availability of\ncertain resources that are replenished at a constant rate. The main result\ninvolves the derivation of an asymptotic lower bound for the regret of feasible\nuniformly fast policies and the construction of policies that achieve this\nlower bound, under pertinent conditions. Further, we provide the explicit form\nof such policies for the case in which the unknown distributions are Normal\nwith unknown means and known variances, for the case of Normal distributions\nwith unknown means and unknown variances and for the case of arbitrary discrete\ndistributions with finite support. \n\n"}
{"id": "1812.00490", "contents": "Title: Improving Clinical Predictions through Unsupervised Time Series\n  Representation Learning Abstract: In this work, we investigate unsupervised representation learning on medical\ntime series, which bears the promise of leveraging copious amounts of existing\nunlabeled data in order to eventually assist clinical decision making. By\nevaluating on the prediction of clinically relevant outcomes, we show that in a\npractical setting, unsupervised representation learning can offer clear\nperformance benefits over end-to-end supervised architectures. We experiment\nwith using sequence-to-sequence (Seq2Seq) models in two different ways, as an\nautoencoder and as a forecaster, and show that the best performance is achieved\nby a forecasting Seq2Seq model with an integrated attention mechanism, proposed\nhere for the first time in the setting of unsupervised learning for medical\ntime series. \n\n"}
{"id": "1812.00547", "contents": "Title: Semi-supervised Rare Disease Detection Using Generative Adversarial\n  Network Abstract: Rare diseases affect a relatively small number of people, which limits\ninvestment in research for treatments and cures. Developing an efficient method\nfor rare disease detection is a crucial first step towards subsequent clinical\nresearch. In this paper, we present a semi-supervised learning framework for\nrare disease detection using generative adversarial networks. Our method takes\nadvantage of the large amount of unlabeled data for disease detection and\nachieves the best results in terms of precision-recall score compared to\nbaseline techniques. \n\n"}
{"id": "1812.00893", "contents": "Title: Domain Alignment with Triplets Abstract: Deep domain adaptation methods can reduce the distribution discrepancy by\nlearning domain-invariant embedddings. However, these methods only focus on\naligning the whole data distributions, without considering the class-level\nrelations among source and target images. Thus, a target embeddings of a bird\nmight be aligned to source embeddings of an airplane. This semantic\nmisalignment can directly degrade the classifier performance on the target\ndataset. To alleviate this problem, we present a similarity constrained\nalignment (SCA) method for unsupervised domain adaptation. When aligning the\ndistributions in the embedding space, SCA enforces a similarity-preserving\nconstraint to maintain class-level relations among the source and target\nimages, i.e., if a source image and a target image are of the same class label,\ntheir corresponding embeddings are supposed to be aligned nearby, and vise\nversa. In the absence of target labels, we assign pseudo labels for target\nimages. Given labeled source images and pseudo-labeled target images, the\nsimilarity-preserving constraint can be implemented by minimizing the triplet\nloss. With the joint supervision of domain alignment loss and\nsimilarity-preserving constraint, we train a network to obtain domain-invariant\nembeddings with two critical characteristics, intra-class compactness and\ninter-class separability. Extensive experiments conducted on the two datasets\nwell demonstrate the effectiveness of SCA. \n\n"}
{"id": "1812.01045", "contents": "Title: First-Order Topological Quantum Phase Transition in a Strongly\n  Correlated Ladder Abstract: We report on the discovery of a quantum tri-critical point (QTP) separating a\nline of first-order topological quantum phase transitions from a continuous\ntransition regime in a strongly correlated one-dimensional lattice system.\nSpecifically, we study a fermionic four-leg ladder supporting a\nsymmetry-protected topological phase in the presence of on-site interaction,\nwhich is driven towards a trivial gapped phase by a nearest-neighbor\ninteraction. Based on DMRG simulations, we show that, as a function of the\ninteraction strength, the phase transition between the topological and the\ntrivial phase switches from being continuous to exhibiting a first-order\ncharacter. Remarkably, the QTP as well as the first-order character of the\ntopological transition in the strongly correlated regime are found to clearly\nmanifest in simple local observables. \n\n"}
{"id": "1812.01664", "contents": "Title: A Stable Cardinality Distance for Topological Classification Abstract: This work incorporates topological features via persistence diagrams to\nclassify point cloud data arising from materials science. Persistence diagrams\nare multisets summarizing the connectedness and holes of given data. A new\ndistance on the space of persistence diagrams generates relevant input features\nfor a classification algorithm for materials science data. This distance\nmeasures the similarity of persistence diagrams using the cost of matching\npoints and a regularization term corresponding to cardinality differences\nbetween diagrams. Establishing stability properties of this distance provides\ntheoretical justification for the use of the distance in comparisons of such\ndiagrams. The classification scheme succeeds in determining the crystal\nstructure of materials on noisy and sparse data retrieved from synthetic atom\nprobe tomography experiments. \n\n"}
{"id": "1812.02633", "contents": "Title: MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Abstract: We consider the problem of handling missing data with deep latent variable\nmodels (DLVMs). First, we present a simple technique to train DLVMs when the\ntraining set contains missing-at-random data. Our approach, called MIWAE, is\nbased on the importance-weighted autoencoder (IWAE), and maximises a\npotentially tight lower bound of the log-likelihood of the observed data.\nCompared to the original IWAE, our algorithm does not induce any additional\ncomputational overhead due to the missing data. We also develop Monte Carlo\ntechniques for single and multiple imputation using a DLVM trained on an\nincomplete data set. We illustrate our approach by training a convolutional\nDLVM on a static binarisation of MNIST that contains 50% of missing pixels.\nLeveraging multiple imputation, a convolutional network trained on these\nincomplete digits has a test performance similar to one trained on complete\ndata. On various continuous and binary data sets, we also show that MIWAE\nprovides accurate single imputations, and is highly competitive with\nstate-of-the-art methods. \n\n"}
{"id": "1812.03090", "contents": "Title: Change Point Estimation in a Dynamic Stochastic Block Model Abstract: We consider the problem of estimating the location of a single change point\nin a dynamic stochastic block model. We propose two methods of estimating the\nchange point, together with the model parameters. The first employs a least\nsquares criterion function and takes into consideration the full structure of\nthe stochastic block model and is evaluated at each point in time. Hence, as an\nintermediate step, it requires estimating the community structure based on a\nclustering algorithm at every time point. The second method comprises of the\nfollowing two steps: in the first one, a least squares function is used and\nevaluated at each time point, but ignores the community structures and just\nconsiders a random graph generating mechanism exhibiting a change point. Once\nthe change point is identified, in the second step, all network data before and\nafter it are used together with a clustering algorithm to obtain the\ncorresponding community structures and subsequently estimate the generating\nstochastic block model parameters. A comparison between these two methods is\nillustrated. Further, for both methods under their respective identifiability\nand certain additional regularity conditions, we establish rates of convergence\nand derive the asymptotic distributions of the change point estimators. The\nresults are illustrated on synthetic data. \n\n"}
{"id": "1812.04407", "contents": "Title: Learning Item-Interaction Embeddings for User Recommendations Abstract: Industry-scale recommendation systems have become a cornerstone of the\ne-commerce shopping experience. For Etsy, an online marketplace with over 50\nmillion handmade and vintage items, users come to rely on personalized\nrecommendations to surface relevant items from its massive inventory. One\nhallmark of Etsy's shopping experience is the multitude of ways in which a user\ncan interact with an item they are interested in: they can view it, favorite\nit, add it to a collection, add it to cart, purchase it, etc. We hypothesize\nthat the different ways in which a user interacts with an item indicates\ndifferent kinds of intent. Consequently, a user's recommendations should be\nbased not only on the item from their past activity, but also the way in which\nthey interacted with that item. In this paper, we propose a novel method for\nlearning interaction-based item embeddings that encode the co-occurrence\npatterns of not only the item itself, but also the interaction type. The\nlearned embeddings give us a convenient way of approximating the likelihood\nthat one item-interaction pair would co-occur with another by way of a simple\ninner product. Because of its computational efficiency, our model lends itself\nnaturally as a candidate set selection method, and we evaluate it as such in an\nindustry-scale recommendation system that serves live traffic on Etsy.com. Our\nexperiments reveal that taking interaction type into account shows promising\nresults in improving the accuracy of modeling user shopping behavior. \n\n"}
{"id": "1812.04821", "contents": "Title: Efficient Super Resolution For Large-Scale Images Using Attentional GAN Abstract: Single Image Super Resolution (SISR) is a well-researched problem with broad\ncommercial relevance. However, most of the SISR literature focuses on\nsmall-size images under 500px, whereas business needs can mandate the\ngeneration of very high resolution images. At Expedia Group, we were tasked\nwith generating images of at least 2000px for display on the website, four\ntimes greater than the sizes typically reported in the literature. This\nrequirement poses a challenge that state-of-the-art models, validated on small\nimages, have not been proven to handle. In this paper, we investigate solutions\nto the problem of generating high-quality images for large-scale super\nresolution in a commercial setting. We find that training a generative\nadversarial network (GAN) with attention from scratch using a large-scale\nlodging image data set generates images with high PSNR and SSIM scores. We\ndescribe a novel attentional SISR model for large-scale images, A-SRGAN, that\nuses a Flexible Self Attention layer to enable processing of large-scale\nimages. We also describe a distributed algorithm which speeds up training by\naround a factor of five. \n\n"}
{"id": "1812.05796", "contents": "Title: AdaFlow: Domain-Adaptive Density Estimator with Application to Anomaly\n  Detection and Unpaired Cross-Domain Translation Abstract: We tackle unsupervised anomaly detection (UAD), a problem of detecting data\nthat significantly differ from normal data. UAD is typically solved by using\ndensity estimation. Recently, deep neural network (DNN)-based density\nestimators, such as Normalizing Flows, have been attracting attention. However,\none of their drawbacks is the difficulty in adapting them to the change in the\nnormal data's distribution. To address this difficulty, we propose AdaFlow, a\nnew DNN-based density estimator that can be easily adapted to the change of the\ndistribution. AdaFlow is a unified model of a Normalizing Flow and Adaptive\nBatch-Normalizations, a module that enables DNNs to adapt to new distributions.\nAdaFlow can be adapted to a new distribution by just conducting forward\npropagation once per sample; hence, it can be used on devices that have limited\ncomputational resources. We have confirmed the effectiveness of the proposed\nmodel through an anomaly detection in a sound task. We also propose a method of\napplying AdaFlow to the unpaired cross-domain translation problem, in which one\nhas to train a cross-domain translation model with only unpaired samples. We\nhave confirmed that our model can be used for the cross-domain translation\nproblem through experiments on image datasets. \n\n"}
{"id": "1812.05988", "contents": "Title: Class Mean Vector Component and Discriminant Analysis Abstract: The kernel matrix used in kernel methods encodes all the information required\nfor solving complex nonlinear problems defined on data representations in the\ninput space using simple, but implicitly defined, solutions. Spectral analysis\non the kernel matrix defines an explicit nonlinear mapping of the input data\nrepresentations to a subspace of the kernel space, which can be used for\ndirectly applying linear methods. However, the selection of the kernel subspace\nis crucial for the performance of the proceeding processing steps. In this\npaper, we propose a component analysis method for kernel-based dimensionality\nreduction that optimally preserves the pair-wise distances of the class means\nin the feature space. We provide extensive analysis on the connection of the\nproposed criterion to those used in kernel principal component analysis and\nkernel discriminant analysis, leading to a discriminant analysis version of the\nproposed method. Our analysis also provides more insights on the properties of\nthe feature spaces obtained by applying these methods. \n\n"}
{"id": "1812.06135", "contents": "Title: Bias Mitigation Post-processing for Individual and Group Fairness Abstract: Whereas previous post-processing approaches for increasing the fairness of\npredictions of biased classifiers address only group fairness, we propose a\nmethod for increasing both individual and group fairness. Our novel framework\nincludes an individual bias detector used to prioritize data samples in a bias\nmitigation algorithm aiming to improve the group fairness measure of disparate\nimpact. We show superior performance to previous work in the combination of\nclassification accuracy, individual fairness and group fairness on several\nreal-world datasets in applications such as credit, employment, and criminal\njustice. \n\n"}
{"id": "1812.06562", "contents": "Title: A Robust Deep Learning Approach for Automatic Classification of Seizures\n  Against Non-seizures Abstract: Identifying epileptic seizures through analysis of the electroencephalography\n(EEG) signal becomes a standard method for the diagnosis of epilepsy. Manual\nseizure identification on EEG by trained neurologists is time-consuming,\nlabor-intensive and error-prone, and a reliable automatic seizure/non-seizure\nclassification method is needed. One of the challenges in automatic\nseizure/non-seizure classification is that seizure morphologies exhibit\nconsiderable variabilities. In order to capture essential seizure patterns,\nthis paper leverages an attention mechanism and a bidirectional long short-term\nmemory (BiLSTM) to exploit both spatial and temporal discriminating features\nand overcome seizure variabilities. The attention mechanism is to capture\nspatial features according to the contributions of different brain regions to\nseizures. The BiLSTM is to extract discriminating temporal features in the\nforward and the backward directions. Cross-validation experiments and\ncross-patient experiments over the noisy data of CHB-MIT are performed to\nevaluate our proposed approach. The obtained average sensitivity of 87.00%,\nspecificity of 88.60% and precision of 88.63% in cross-validation experiments\nare higher than using the current state-of-the-art methods, and the standard\ndeviations of our approach are lower. The evaluation results of cross-patient\nexperiments indicate that, our approach has better performance compared with\nthe current state-of-the-art methods and is more robust across patients. \n\n"}
{"id": "1812.07242", "contents": "Title: A finite geometric toy model of space-time as an error correcting code Abstract: A finite geometric model of space-time (which we call the bulk) is shown to\nemerge as a set of error correcting codes. The bulk is encoding a set of\nmessages located in a blow up of the Gibbons-Hoffman-Wootters (GHW) discrete\nphase space for $n$-qubits (which we call the boundary). Our error correcting\ncode is a geometric subspace code known from network coding, and the\ncorrespondence map is the finite geometric analogue of the Pl\\\"ucker map\nwell-known form twistor theory. The $n=2$ case of the bulk-boundary\ncorrespondence is precisely the twistor correspondence where the boundary is\nplaying the role of the twistor space and the bulk is a finite geometric\nversion of compactified Minkowski space-time. For $n\\geq 3$ the bulk is\nidentified with the finite geometric version of the Brody-Hughston quantum\nspace-time. For special regions on both sides of the correspondence we\nassociate certain collections of qubit observables. On the boundary side this\nassociation gives rise to the well-known GHW quantum net structure. In this\npicture the messages are complete sets of commuting observables associated to\nLagrangian subspaces giving a partition of the boundary. Incomplete subsets of\nobservables corresponding to subspaces of the Lagrangian ones are regarded as\ncorrupted messages. Such a partition of the boundary is represented on the bulk\nside as a special collection of space-time points. For a particular message\nresiding in the boundary, the set of possible errors is described by the fine\ndetails of the light-cone structure of its representative space-time point in\nthe bulk. The geometric arrangement of representative space-time points,\nplaying the role of the variety of codewords, encapsulates an algebraic\nalgorithm for recovery from errors on the boundary side. \n\n"}
{"id": "1812.07497", "contents": "Title: Hybrid estimation for ergodic diffusion processes based on noisy\n  discrete observations Abstract: We consider parametric estimation for ergodic diffusion processes with noisy\nsampled data based on the hybrid method, that is, the multi-step estimation\nwith the initial Bayes type estimators. In order to select proper initial\nvalues for optimisation of the quasi likelihood function of ergodic diffusion\nprocesses with noisy observations, we construct the initial Bayes type\nestimator based on the local means of the noisy observations. The asymptotic\nproperties of the initial Bayes type estimators and the hybrid multi-step\nestimators with the initial Bayes type estimators are shown, and a concrete\nexample and the simulation results are given. \n\n"}
{"id": "1812.08265", "contents": "Title: Statistical learning of geometric characteristics of wireless networks Abstract: Motivated by the prediction of cell loads in cellular networks, we formulate\nthe following new, fundamental problem of statistical learning of geometric\nmarks of point processes: An unknown marking function, depending on the\ngeometry of point patterns, produces characteristics (marks) of the points. One\naims at learning this function from the examples of marked point patterns in\norder to predict the marks of new point patterns. To approximate (interpolate)\nthe marking function, in our baseline approach, we build a statistical\nregression model of the marks with respect some local point distance\nrepresentation. In a more advanced approach, we use a global data\nrepresentation via the scattering moments of random measures, which build\ninformative and stable to deformations data representation, already proven\nuseful in image analysis and related application domains. In this case, the\nregression of the scattering moments of the marked point patterns with respect\nto the non-marked ones is combined with the numerical solution of the inverse\nproblem, where the marks are recovered from the estimated scattering moments.\nConsidering some simple, generic marks, often appearing in the modeling of\nwireless networks, such as the shot-noise values, nearest neighbour distance,\nand some characteristics of the Voronoi cells, we show that the scattering\nmoments can capture similar geometry information as the baseline approach, and\ncan reach even better performance, especially for non-local marking functions.\nOur results motivate further development of statistical learning tools for\nstochastic geometry and analysis of wireless networks, in particular to predict\ncell loads in cellular networks from the locations of base stations and traffic\ndemand. \n\n"}
{"id": "1812.09659", "contents": "Title: Artificial neural networks condensation: A strategy to facilitate\n  adaption of machine learning in medical settings by reducing computational\n  burden Abstract: Machine Learning (ML) applications on healthcare can have a great impact on\npeople's lives helping deliver better and timely treatment to those in need. At\nthe same time, medical data is usually big and sparse requiring important\ncomputational resources. Although it might not be a problem for wide-adoption\nof ML tools in developed nations, availability of computational resource can\nvery well be limited in third-world nations. This can prevent the less favored\npeople from benefiting of the advancement in ML applications for healthcare. In\nthis project we explored methods to increase computational efficiency of ML\nalgorithms, in particular Artificial Neural Nets (NN), while not compromising\nthe accuracy of the predicted results. We used in-hospital mortality prediction\nas our case analysis based on the MIMIC III publicly available dataset. We\nexplored three methods on two different NN architectures. We reduced the size\nof recurrent neural net (RNN) and dense neural net (DNN) by applying pruning of\n\"unused\" neurons. Additionally, we modified the RNN structure by adding a\nhidden-layer to the LSTM cell allowing to use less recurrent layers for the\nmodel. Finally, we implemented quantization on DNN forcing the weights to be\n8-bits instead of 32-bits. We found that all our methods increased\ncomputational efficiency without compromising accuracy and some of them even\nachieved higher accuracy than the pre-condensed baseline models. \n\n"}
{"id": "1812.10387", "contents": "Title: Same but Different: Distant Supervision for Predicting and Understanding\n  Entity Linking Difficulty Abstract: Entity Linking (EL) is the task of automatically identifying entity mentions\nin a piece of text and resolving them to a corresponding entity in a reference\nknowledge base like Wikipedia. There is a large number of EL tools available\nfor different types of documents and domains, yet EL remains a challenging task\nwhere the lack of precision on particularly ambiguous mentions often spoils the\nusefulness of automated disambiguation results in real applications. A priori\napproximations of the difficulty to link a particular entity mention can\nfacilitate flagging of critical cases as part of semi-automated EL systems,\nwhile detecting latent factors that affect the EL performance, like\ncorpus-specific features, can provide insights on how to improve a system based\non the special characteristics of the underlying corpus. In this paper, we\nfirst introduce a consensus-based method to generate difficulty labels for\nentity mentions on arbitrary corpora. The difficulty labels are then exploited\nas training data for a supervised classification task able to predict the EL\ndifficulty of entity mentions using a variety of features. Experiments over a\ncorpus of news articles show that EL difficulty can be estimated with high\naccuracy, revealing also latent features that affect EL performance. Finally,\nevaluation results demonstrate the effectiveness of the proposed method to\ninform semi-automated EL pipelines. \n\n"}
{"id": "1812.11092", "contents": "Title: Multi-resolution neural networks for tracking seismic horizons from few\n  training images Abstract: Detecting a specific horizon in seismic images is a valuable tool for\ngeological interpretation. Because hand-picking the locations of the horizon is\na time-consuming process, automated computational methods were developed\nstarting three decades ago. Older techniques for such picking include\ninterpolation of control points however, in recent years neural networks have\nbeen used for this task. Until now, most networks trained on small patches from\nlarger images. This limits the networks ability to learn from large-scale\ngeologic structures. Moreover, currently available networks and training\nstrategies require label patches that have full and continuous annotations,\nwhich are also time-consuming to generate.\n  We propose a projected loss-function for training convolutional networks with\na multi-resolution structure, including variants of the U-net. Our networks\nlearn from a small number of large seismic images without creating patches. The\nprojected loss-function enables training on labels with just a few annotated\npixels and has no issue with the other unknown label pixels. Training uses all\ndata without reserving some for validation. Only the labels are split into\ntraining/testing. Contrary to other work on horizon tracking, we train the\nnetwork to perform non-linear regression, and not classification. As such, we\npropose labels as the convolution of a Gaussian kernel and the known horizon\nlocations that indicate uncertainty in the labels. The network output is the\nprobability of the horizon location. We demonstrate the proposed computational\ningredients on two different datasets, for horizon extrapolation and\ninterpolation. We show that the predictions of our methodology are accurate\neven in areas far from known horizon locations because our learning strategy\nexploits all data in large seismic images. \n\n"}
{"id": "1901.00615", "contents": "Title: Structure learning via unstructured kernel-based M-regression Abstract: In statistical learning, identifying underlying structures of true target\nfunctions based on observed data plays a crucial role to facilitate subsequent\nmodeling and analysis. Unlike most of those existing methods that focus on some\nspecific settings under certain model assumptions, this paper proposes a\ngeneral and novel framework for recovering true structures of target functions\nby using unstructured M-regression in a reproducing kernel Hilbert space\n(RKHS). The proposed framework is inspired by the fact that gradient functions\ncan be employed as a valid tool to learn underlying structures, including\nsparse learning, interaction selection and model identification, and it is easy\nto implement by taking advantage of the nice properties of the RKHS. More\nimportantly, it admits a wide range of loss functions, and thus includes many\ncommonly used methods, such as mean regression, quantile regression,\nlikelihood-based classification, and margin-based classification, which is also\ncomputationally efficient by solving convex optimization tasks. The asymptotic\nresults of the proposed framework are established within a rich family of loss\nfunctions without any explicit model specifications. The superior performance\nof the proposed framework is also demonstrated by a variety of simulated\nexamples and a real case study. \n\n"}
{"id": "1901.00630", "contents": "Title: Projecting \"better than randomly\": How to reduce the dimensionality of\n  very large datasets in a way that outperforms random projections Abstract: For very large datasets, random projections (RP) have become the tool of\nchoice for dimensionality reduction. This is due to the computational\ncomplexity of principal component analysis. However, the recent development of\nrandomized principal component analysis (RPCA) has opened up the possibility of\nobtaining approximate principal components on very large datasets. In this\npaper, we compare the performance of RPCA and RP in dimensionality reduction\nfor supervised learning. In Experiment 1, study a malware classification task\non a dataset with over 10 million samples, almost 100,000 features, and over 25\nbillion non-zero values, with the goal of reducing the dimensionality to a\ncompressed representation of 5,000 features. In order to apply RPCA to this\ndataset, we develop a new algorithm called large sample RPCA (LS-RPCA), which\nextends the RPCA algorithm to work on datasets with arbitrarily many samples.\nWe find that classification performance is much higher when using LS-RPCA for\ndimensionality reduction than when using random projections. In particular,\nacross a range of target dimensionalities, we find that using LS-RPCA reduces\nclassification error by between 37% and 54%. Experiment 2 generalizes the\nphenomenon to multiple datasets, feature representations, and classifiers.\nThese findings have implications for a large number of research projects in\nwhich random projections were used as a preprocessing step for dimensionality\nreduction. As long as accuracy is at a premium and the target dimensionality is\nsufficiently less than the numeric rank of the dataset, randomized PCA may be a\nsuperior choice. Moreover, if the dataset has a large number of samples, then\nLS-RPCA will provide a method for obtaining the approximate principal\ncomponents. \n\n"}
{"id": "1901.01163", "contents": "Title: Approximating high-dimensional infinite-order $U$-statistics:\n  statistical and computational guarantees Abstract: We study the problem of distributional approximations to high-dimensional\nnon-degenerate $U$-statistics with random kernels of diverging orders.\nInfinite-order $U$-statistics (IOUS) are a useful tool for constructing\nsimultaneous prediction intervals that quantify the uncertainty of ensemble\nmethods such as subbagging and random forests. A major obstacle in using the\nIOUS is their computational intractability when the sample size and/or order\nare large. In this article, we derive non-asymptotic Gaussian approximation\nerror bounds for an incomplete version of the IOUS with a random kernel. We\nalso study data-driven inferential methods for the incomplete IOUS via\nbootstraps and develop their statistical and computational guarantees. \n\n"}
{"id": "1901.02078", "contents": "Title: All Graphs Lead to Rome: Learning Geometric and Cycle-Consistent\n  Representations with Graph Convolutional Networks Abstract: Image feature matching is a fundamental part of many geometric computer\nvision applications, and using multiple images can improve performance. In this\nwork, we formulate multi-image matching as a graph embedding problem then use a\nGraph Convolutional Network to learn an appropriate embedding function for\naligning image features. We use cycle consistency to train our network in an\nunsupervised fashion, since ground truth correspondence is difficult or\nexpensive to aquire. In addition, geometric consistency losses can be added at\ntraining time, even if the information is not available in the test set, unlike\nprevious approaches that optimize cycle consistency directly. To the best of\nour knowledge, no other works have used learning for multi-image feature\nmatching. Our experiments show that our method is competitive with other\noptimization based approaches. \n\n"}
{"id": "1901.02217", "contents": "Title: Tree Tensor Networks for Generative Modeling Abstract: Matrix product states (MPS), a tensor network designed for one-dimensional\nquantum systems, has been recently proposed for generative modeling of natural\ndata (such as images) in terms of `Born machine'. However, the exponential\ndecay of correlation in MPS restricts its representation power heavily for\nmodeling complex data such as natural images. In this work, we push forward the\neffort of applying tensor networks to machine learning by employing the Tree\nTensor Network (TTN) which exhibits balanced performance in expressibility and\nefficient training and sampling. We design the tree tensor network to utilize\nthe 2-dimensional prior of the natural images and develop sweeping learning and\nsampling algorithms which can be efficiently implemented utilizing Graphical\nProcessing Units (GPU). We apply our model to random binary patterns and the\nbinary MNIST datasets of handwritten digits. We show that TTN is superior to\nMPS for generative modeling in keeping correlation of pixels in natural images,\nas well as giving better log-likelihood scores in standard datasets of\nhandwritten digits. We also compare its performance with state-of-the-art\ngenerative models such as the Variational AutoEncoders, Restricted Boltzmann\nmachines, and PixelCNN. Finally, we discuss the future development of Tensor\nNetwork States in machine learning problems. \n\n"}
{"id": "1901.02374", "contents": "Title: Graphical model inference: Sequential Monte Carlo meets deterministic\n  approximations Abstract: Approximate inference in probabilistic graphical models (PGMs) can be grouped\ninto deterministic methods and Monte-Carlo-based methods. The former can often\nprovide accurate and rapid inferences, but are typically associated with biases\nthat are hard to quantify. The latter enjoy asymptotic consistency, but can\nsuffer from high computational costs. In this paper we present a way of\nbridging the gap between deterministic and stochastic inference. Specifically,\nwe suggest an efficient sequential Monte Carlo (SMC) algorithm for PGMs which\ncan leverage the output from deterministic inference methods. While generally\napplicable, we show explicitly how this can be done with loopy belief\npropagation, expectation propagation, and Laplace approximations. The resulting\nalgorithm can be viewed as a post-correction of the biases associated with\nthese methods and, indeed, numerical results show clear improvements over the\nbaseline deterministic methods as well as over \"plain\" SMC. \n\n"}
{"id": "1901.03317", "contents": "Title: Accelerated Flow for Probability Distributions Abstract: This paper presents a methodology and numerical algorithms for constructing\naccelerated gradient flows on the space of probability distributions. In\nparticular, we extend the recent variational formulation of accelerated\ngradient methods in (wibisono, et. al. 2016) from vector valued variables to\nprobability distributions. The variational problem is modeled as a mean-field\noptimal control problem. The maximum principle of optimal control theory is\nused to derive Hamilton's equations for the optimal gradient flow. The\nHamilton's equation are shown to achieve the accelerated form of density\ntransport from any initial probability distribution to a target probability\ndistribution. A quantitative estimate on the asymptotic convergence rate is\nprovided based on a Lyapunov function construction, when the objective\nfunctional is displacement convex. Two numerical approximations are presented\nto implement the Hamilton's equations as a system of $N$ interacting particles.\nThe continuous limit of the Nesterov's algorithm is shown to be a special case\nwith $N=1$. The algorithm is illustrated with numerical examples. \n\n"}
{"id": "1901.03357", "contents": "Title: No-Regret Bayesian Optimization with Unknown Hyperparameters Abstract: Bayesian optimization (BO) based on Gaussian process models is a powerful\nparadigm to optimize black-box functions that are expensive to evaluate. While\nseveral BO algorithms provably converge to the global optimum of the unknown\nfunction, they assume that the hyperparameters of the kernel are known in\nadvance. This is not the case in practice and misspecification often causes\nthese algorithms to converge to poor local optima. In this paper, we present\nthe first BO algorithm that is provably no-regret and converges to the optimum\nwithout knowledge of the hyperparameters. During optimization we slowly adapt\nthe hyperparameters of stationary kernels and thereby expand the associated\nfunction class over time, so that the BO algorithm considers more complex\nfunction candidates. Based on the theoretical insights, we propose several\npractical algorithms that achieve the empirical sample efficiency of BO with\nonline hyperparameter estimation, but retain theoretical convergence\nguarantees. We evaluate our method on several benchmark problems. \n\n"}
{"id": "1901.05995", "contents": "Title: Activation Functions for Generalized Learning Vector Quantization - A\n  Performance Comparison Abstract: An appropriate choice of the activation function (like ReLU, sigmoid or\nswish) plays an important role in the performance of (deep) multilayer\nperceptrons (MLP) for classification and regression learning. Prototype-based\nclassification learning methods like (generalized) learning vector quantization\n(GLVQ) are powerful alternatives. These models also deal with activation\nfunctions but here they are applied to the so-called classifier function\ninstead. In this paper we investigate successful candidates of activation\nfunctions known for MLPs for application in GLVQ and their influence on the\nperformance. \n\n"}
{"id": "1901.06983", "contents": "Title: Dynamical Fractal in Quantum Gases with Discrete Scaling Symmetry Abstract: Inspired by the similarity between the fractal Weierstrass function and\nquantum systems with discrete scaling symmetry, we establish general conditions\nunder which the dynamics of a quantum system will exhibit fractal structure in\nthe time domain. As an example, we discuss the dynamics of the Loschmidt\namplitude and the zero-momentum occupation of a single particle moving in a\nscale invariant $1/r^2$ potential. In order to show these conditions can be\nrealized in ultracold atomic gases, we perform numerical simulations with\npractical experimental parameters, which shows that the dynamical fractal can\nbe observed in realistic time scales. The predication can be directly verified\nin current cold atom experiments. \n\n"}
{"id": "1901.07295", "contents": "Title: Adversarial Pseudo Healthy Synthesis Needs Pathology Factorization Abstract: Pseudo healthy synthesis, i.e. the creation of a subject-specific `healthy'\nimage from a pathological one, could be helpful in tasks such as anomaly\ndetection, understanding changes induced by pathology and disease or even as\ndata augmentation. We treat this task as a factor decomposition problem: we aim\nto separate what appears to be healthy and where disease is (as a map). The two\nfactors are then recombined (by a network) to reconstruct the input disease\nimage. We train our models in an adversarial way using either paired or\nunpaired settings, where we pair disease images and maps (as segmentation\nmasks) when available. We quantitatively evaluate the quality of pseudo healthy\nimages. We show in a series of experiments, performed in ISLES and BraTS\ndatasets, that our method is better than conditional GAN and CycleGAN,\nhighlighting challenges in using adversarial methods in the image translation\ntask of pseudo healthy image generation. \n\n"}
{"id": "1901.08177", "contents": "Title: Generating and Aligning from Data Geometries with Generative Adversarial\n  Networks Abstract: Unsupervised domain mapping has attracted substantial attention in recent\nyears due to the success of models based on the cycle-consistency assumption.\nThese models map between two domains by fooling a probabilistic discriminator,\nthereby matching the probability distributions of the real and generated data.\nInstead of this probabilistic approach, we cast the problem in terms of\naligning the geometry of the manifolds of the two domains. We introduce the\nManifold Geometry Matching Generative Adversarial Network (MGM GAN), which adds\ntwo novel mechanisms to facilitate GANs sampling from the geometry of the\nmanifold rather than the density and then aligning two manifold geometries: (1)\nan importance sampling technique that reweights points based on their density\non the manifold, making the discriminator only able to discern geometry and (2)\na penalty adapted from traditional manifold alignment literature that\nexplicitly enforces the geometry to be preserved. The MGM GAN leverages the\nmanifolds arising from a pre-trained autoencoder to bridge the gap between\nformal manifold alignment literature and existing GAN work, and demonstrate the\nadvantages of modeling the manifold geometry over its density. \n\n"}
{"id": "1901.08396", "contents": "Title: Self-Supervised Deep Learning on Point Clouds by Reconstructing Space Abstract: Point clouds provide a flexible and natural representation usable in\ncountless applications such as robotics or self-driving cars. Recently, deep\nneural networks operating on raw point cloud data have shown promising results\non supervised learning tasks such as object classification and semantic\nsegmentation. While massive point cloud datasets can be captured using modern\nscanning technology, manually labelling such large 3D point clouds for\nsupervised learning tasks is a cumbersome process. This necessitates methods\nthat can learn from unlabelled data to significantly reduce the number of\nannotated samples needed in supervised learning. We propose a self-supervised\nlearning task for deep learning on raw point cloud data in which a neural\nnetwork is trained to reconstruct point clouds whose parts have been randomly\nrearranged. While solving this task, representations that capture semantic\nproperties of the point cloud are learned. Our method is agnostic of network\narchitecture and outperforms current unsupervised learning approaches in\ndownstream object classification tasks. We show experimentally, that\npre-training with our method before supervised training improves the\nperformance of state-of-the-art models and significantly improves sample\nefficiency. \n\n"}
{"id": "1901.08571", "contents": "Title: Nonparametric Inference under B-bits Quantization Abstract: Statistical inference based on lossy or incomplete samples is often needed in\nresearch areas such as signal/image processing, medical image storage, remote\nsensing, signal transmission. In this paper, we propose a nonparametric testing\nprocedure based on samples quantized to $B$ bits through a computationally\nefficient algorithm. Under mild technical conditions, we establish the\nasymptotic properties of the proposed test statistic and investigate how the\ntesting power changes as $B$ increases. In particular, we show that if $B$\nexceeds a certain threshold, the proposed nonparametric testing procedure\nachieves the classical minimax rate of testing (Shang and Cheng, 2015) for\nspline models. We further extend our theoretical investigations to a\nnonparametric linearity test and an adaptive nonparametric test, expanding the\napplicability of the proposed methods. Extensive simulation studies {together\nwith a real-data analysis} are used to demonstrate the validity and\neffectiveness of the proposed tests. \n\n"}
{"id": "1901.09087", "contents": "Title: Optimality Implies Kernel Sum Classifiers are Statistically Efficient Abstract: We propose a novel combination of optimization tools with learning theory\nbounds in order to analyze the sample complexity of optimal kernel sum\nclassifiers. This contrasts the typical learning theoretic results which hold\nfor all (potentially suboptimal) classifiers. Our work also justifies\nassumptions made in prior work on multiple kernel learning. As a byproduct of\nour analysis, we also provide a new form of Rademacher complexity for\nhypothesis classes containing only optimal classifiers. \n\n"}
{"id": "1901.10061", "contents": "Title: A Framework for Deep Constrained Clustering -- Algorithms and Advances Abstract: The area of constrained clustering has been extensively explored by\nresearchers and used by practitioners. Constrained clustering formulations\nexist for popular algorithms such as k-means, mixture models, and spectral\nclustering but have several limitations. A fundamental strength of deep\nlearning is its flexibility, and here we explore a deep learning framework for\nconstrained clustering and in particular explore how it can extend the field of\nconstrained clustering. We show that our framework can not only handle standard\ntogether/apart constraints (without the well documented negative effects\nreported earlier) generated from labeled side information but more complex\nconstraints generated from new types of side information such as continuous\nvalues and high-level domain knowledge. \n\n"}
{"id": "1901.10495", "contents": "Title: Generating a second-order topological insulator with multiple corner\n  states by periodic driving Abstract: We study the effects of periodic driving on a variant of the\nBernevig-Hughes-Zhang (BHZ) model defined on a square lattice. In the absence\nof driving, the model has both topological and nontopological phases depending\non the different parameter values. We also study the anisotropic BHZ model and\nshow that, unlike the isotropic model, it has a nontopological phase which has\nstates localized on only two of the four edges of a finite-sized square. When\nan appropriate term is added, the edge states get gapped and gapless states\nappear at the four corners of a square; we have shown that these corner states\ncan be labeled by the eigenvalues of a certain operator. When the system is\ndriven periodically by a sequence of two pulses, multiple corner states may\nappear depending on the driving frequency and other parameters. We discuss to\nwhat extent the system can be characterized by topological invariants such as\nthe Chern number and a diagonal winding number. We have shown that the\nlocations of the jumps in these invariants can be understood in terms of the\nFloquet operator at both the time-reversal invariant momenta and other momenta\nwhich have no special symmetries. \n\n"}
{"id": "1901.10774", "contents": "Title: Constructing Strebel differentials via Belyi maps on the Riemann sphere Abstract: In this manuscript, by using Belyi maps and dessin d'enfants, we construct\nsome concrete examples of Strebel differentials with four double poles on the\nRiemann sphere. As an application, we could give some explicit cone spherical\nmetrics on the Riemann sphere. \n\n"}
{"id": "1901.11164", "contents": "Title: Spatial-Temporal Graph Convolutional Networks for Sign Language\n  Recognition Abstract: The recognition of sign language is a challenging task with an important role\nin society to facilitate the communication of deaf persons. We propose a new\napproach of Spatial-Temporal Graph Convolutional Network to sign language\nrecognition based on the human skeletal movements. The method uses graphs to\ncapture the signs dynamics in two dimensions, spatial and temporal, considering\nthe complex aspects of the language. Additionally, we present a new dataset of\nhuman skeletons for sign language based on ASLLVD to contribute to future\nrelated studies. \n\n"}
{"id": "1901.11515", "contents": "Title: ProBO: Versatile Bayesian Optimization Using Any Probabilistic\n  Programming Language Abstract: Optimizing an expensive-to-query function is a common task in science and\nengineering, where it is beneficial to keep the number of queries to a minimum.\nA popular strategy is Bayesian optimization (BO), which leverages probabilistic\nmodels for this task. Most BO today uses Gaussian processes (GPs), or a few\nother surrogate models. However, there is a broad set of Bayesian modeling\ntechniques that could be used to capture complex systems and reduce the number\nof queries in BO. Probabilistic programming languages (PPLs) are modern tools\nthat allow for flexible model definition, prior specification, model\ncomposition, and automatic inference. In this paper, we develop ProBO, a BO\nprocedure that uses only standard operations common to most PPLs. This allows a\nuser to drop in a model built with an arbitrary PPL and use it directly in BO.\nWe describe acquisition functions for ProBO, and strategies for efficiently\noptimizing these functions given complex models or costly inference procedures.\nUsing existing PPLs, we implement new models to aid in a few challenging\noptimization settings, and demonstrate these on model hyperparameter and\narchitecture search tasks. \n\n"}
{"id": "astro-ph/0407013", "contents": "Title: Quantum cosmological effects from the high redshift supernova\n  observations Abstract: Subject of this contribution is to demonstrate that the observed faintness of\nthe supernovae at the high redshift can be considered as a manifestation of\nquantum effects at cosmological scales. We show that observed redshift\ndistribution of coordinate distances to the type Ia supernovae can be explained\nby the local manifestations of quantum fluctuations of the cosmological scale\nfactor about its average value. These fluctuations can arise in the early\nuniverse, grow with time, and produce observed accelerating or decelerating\nexpansions of space subdomains containing separate supernovae with high\nredshift whereas the universe as a whole expands at a steady rate. \n\n"}
{"id": "cond-mat/0009083", "contents": "Title: Double-Occupancy Errors, Adiabaticity, and Entanglement of Spin-Qubits\n  in Quantum Dots Abstract: Quantum gates that temporarily increase singlet-triplet splitting in order to\nswap electronic spins in coupled quantum dots, lead inevitably to a finite\ndouble-occupancy probability for both dots. By solving the time-dependent\nSchr\\\"odinger equation for a coupled dot model, we demonstrate that this does\nnot necessarily lead to quantum computation errors. Instead, the coupled dot\nground state evolves quasi-adiabatically for typical system parameters so that\nthe double-occupancy probability at the completion of swapping is negligibly\nsmall. We introduce a measure of entanglement which explicitly takes into\naccount the possibilty of double occupancies and provides a necessary and\nsufficient criterion for entangled states. \n\n"}
{"id": "cond-mat/0204612", "contents": "Title: How the Quasispecies Evolution Depends on the Topology of the Genome\n  Space Abstract: We compared the properties of the error threshold transition in quasispecies\nevolution for three different topologies of the genome space. They are a)\nhypercube b) rugged landscape modelled by an ultrametric space, and c) holey\nlandscape modelled by Bethe lattice. In all studied topologies the phase\ntransition exists. We calculated the critical exponents in all the cases. For\nthe critical exponent corresponding to appropriately defined susceptibility we\nfound super-universal value. \n\n"}
{"id": "cond-mat/0301271", "contents": "Title: Solving satisfiability problems by fluctuations: The dynamics of\n  stochastic local search algorithms Abstract: Stochastic local search algorithms are frequently used to numerically solve\nhard combinatorial optimization or decision problems. We give numerical and\napproximate analytical descriptions of the dynamics of such algorithms applied\nto random satisfiability problems. We find two different dynamical regimes,\ndepending on the number of constraints per variable: For low constraintness,\nthe problems are solved efficiently, i.e. in linear time. For higher\nconstraintness, the solution times become exponential. We observe that the\ndynamical behavior is characterized by a fast equilibration and fluctuations\naround this equilibrium. If the algorithm runs long enough, an exponentially\nrare fluctuation towards a solution appears. \n\n"}
{"id": "cond-mat/0301272", "contents": "Title: Relaxation and Metastability in the RandomWalkSAT search procedure Abstract: An analysis of the average properties of a local search resolution procedure\nfor the satisfaction of random Boolean constraints is presented. Depending on\nthe ratio alpha of constraints per variable, resolution takes a time T_res\ngrowing linearly (T_res \\sim tau(alpha) N, alpha < alpha_d) or exponentially\n(T_res \\sim exp(N zeta(alpha)), alpha > alpha_d) with the size N of the\ninstance. The relaxation time tau(alpha) in the linear phase is calculated\nthrough a systematic expansion scheme based on a quantum formulation of the\nevolution operator. For alpha > alpha_d, the system is trapped in some\nmetastable state, and resolution occurs from escape from this state through\ncrossing of a large barrier. An annealed calculation of the height zeta(alpha)\nof this barrier is proposed. The polynomial/exponentiel cross-over alpha_d is\nnot related to the onset of clustering among solutions. \n\n"}
{"id": "cond-mat/0308510", "contents": "Title: A backtracking survey propagation algorithm for K-satisfiability Abstract: In this paper we present a backtracking version of the survey propagation\nalgorithm. We show that the introduction of the simplest form of backtracking\ngreatly improves the ability of the original survey propagation algorithm in\nsolving difficult random problems near the sat-unsat transition. \n\n"}
{"id": "cond-mat/0401649", "contents": "Title: On the cooling-schedule dependence of the dynamics of mean-field glasses Abstract: The low temperature phase of discontinuous mean-field spin glasses is\ncharacterized by the appearance of an exponential number of metastable states.\nWhich ones among these states dominate the out-of-equilibrium dynamics of these\nsystems?\n  In order to answer this question, we compare high-precision numerical\nsimulations of a diluted $p$-spin model with a cavity computation of the\nthreshold energy. Our main conclusion is that the aging dynamics is dominated\nby different layers of metastable states depending on the cooling schedule. In\norder to perform our analysis, we present a method for computing the\nmarginality condition of diluted spin glasses at non-zero temperature. \n\n"}
{"id": "cond-mat/0403612", "contents": "Title: Ultracold atomic Fermi-Bose mixtures in bichromatic optical dipole\n  traps: a novel route to study fermion superfluidity Abstract: The study of low density, ultracold atomic Fermi gases is a promising avenue\nto understand fermion superfluidity from first principles. One technique\ncurrently used to bring Fermi gases in the degenerate regime is sympathetic\ncooling through a reservoir made of an ultracold Bose gas. We discuss a\nproposal for trapping and cooling of two-species Fermi-Bose mixtures into\noptical dipole traps made from combinations of laser beams having two different\nwavelengths. In these bichromatic traps it is possible, by a proper choice of\nthe relative laser powers, to selectively trap the two species in such a way\nthat fermions experience a stronger confinement than bosons. As a consequence,\na deep Fermi degeneracy can be reached having at the same time a softer\ndegenerate regime for the Bose gas. This leads to an increase in the\nsympathetic cooling efficiency and allows for higher precision thermometry of\nthe Fermi-Bose mixture. \n\n"}
{"id": "cond-mat/0406502", "contents": "Title: Coherent Nuclear Radiation Abstract: The main part of this review is devoted to the comprehensive description of\ncoherent radiation by nuclear spins. The theory of nuclear spin superradiance\nis developed and the experimental observations of this phenomenon are\nconsidered. The intriguing problem of how coherence develops from initially\nincoherent quantum fluctuations is analysed. All main types of coherent\nradiation by nuclear spins are discussed, which are: free nuclear induction,\ncollective induction, maser generation, pure superradiance, triggered\nsuperradiance, pulsing superradiance, punctuated superradiance, and induced\nemission. The influence of electron-nuclear hyperfine interactions and the role\nof magnetic anisotropy are studied. Conditions for realizing spin superradiance\nby magnetic molecules are investigated. The possibility of nuclear matter\nlasing, accompanied by pion or dibaryon radiation, is briefly touched. \n\n"}
{"id": "cond-mat/0603363", "contents": "Title: Spin entanglement induced by spin-orbit interactions in coupled quantum\n  dots Abstract: We theoretically explore the possibility of creating spin quantum\nentanglement in a system of two electrons confined respectively in two\nvertically coupled quantum dots in the presence of Rashba type spin-orbit\ncoupling. We find that the system can be described by a generalized Jaynes -\nCummings model of two modes bosons interacting with two spins. The lower\nexcitation states of this model are calculated to reveal the underlying physics\nof the far infrared absorption spectra. The analytic perturbation approach\nshows that an effective transverse coupling of spins can be obtained by\neliminating the orbital degrees of freedom in the large detuning limit. Here,\nthe orbital degrees of freedom of the two electrons, which are described by two\nmodes of bosons, serve as a quantized data bus to exchange the quantum\ninformation between two electrons. Then a nontrivial two-qubit logic gate is\nrealized and spin entanglement between the two electrons is created by virtue\nof spin-orbit coupling. \n\n"}
{"id": "cs/0301015", "contents": "Title: Some remarks on the survey decimation algorithm for K-satisfiability Abstract: In this note we study the convergence of the survey decimation algorithm. An\nanalytic formula for the reduction of the complexity during the decimation is\nderived. The limit of the converge of the algorithm are estimated in the random\ncase: interesting phenomena appear near the boundary of convergence. \n\n"}
{"id": "cs/0302003", "contents": "Title: Approximate analysis of search algorithms with \"physical\" methods Abstract: An overview of some methods of statistical physics applied to the analysis of\nalgorithms for optimization problems (satisfiability of Boolean constraints,\nvertex cover of graphs, decoding, ...) with distributions of random inputs is\nproposed. Two types of algorithms are analyzed: complete procedures with\nbacktracking (Davis-Putnam-Loveland-Logeman algorithm) and incomplete, local\nsearch procedures (gradient descent, random walksat, ...). The study of\ncomplete algorithms makes use of physical concepts such as phase transitions,\ndynamical renormalization flow, growth processes, ... As for local search\nprocedures, the connection between computational complexity and the structure\nof the cost function landscape is questioned, with emphasis on the notion of\nmetastability. \n\n"}
{"id": "cs/0702082", "contents": "Title: Invariant template matching in systems with spatiotemporal coding: a\n  vote for instability Abstract: We consider the design of a pattern recognition that matches templates to\nimages, both of which are spatially sampled and encoded as temporal sequences.\nThe image is subject to a combination of various perturbations. These include\nones that can be modeled as parameterized uncertainties such as image blur,\nluminance, translation, and rotation as well as unmodeled ones. Biological and\nneural systems require that these perturbations be processed through a minimal\nnumber of channels by simple adaptation mechanisms. We found that the most\nsuitable mathematical framework to meet this requirement is that of weakly\nattracting sets. This framework provides us with a normative and unifying\nsolution to the pattern recognition problem. We analyze the consequences of its\nexplicit implementation in neural systems. Several properties inherent to the\nsystems designed in accordance with our normative mathematical argument\ncoincide with known empirical facts. This is illustrated in mental rotation,\nvisual search and blur/intensity adaptation. We demonstrate how our results can\nbe applied to a range of practical problems in template matching and pattern\nrecognition. \n\n"}
{"id": "hep-ph/0208032", "contents": "Title: CPT and effective Hamiltonians for neutral kaon and similar complexes Abstract: We begin with a discussion of the general form and general CP-- and CPT--\ntransformation properties of the Lee--Oehme--Yang (LOY) effective Hamiltonian\nfor the neutral kaon complex. Next, the properties of the exact effective\nHamiltonian for this complex are discussed. Using the Khalfin Theorem we show\nthat the diagonal matrix elements of the effective Hamiltonian governing the\ntime evolution in the subspace of states of an unstable particle and its\nantiparticle need not be equal at for t > t_0 (t_0 is the instant of creation\nof the pair) when the total system under consideration is CPT invariant but CP\nnoninvariant. The unusual consequence of this result is that, contrary to the\nproperties of stable particles, the masses of the unstable particle \"1\" and its\nantiparticle \"2\" need not be equal for t >> t_0 in the case of preserved CPT\nand violated CP symmetries. We also show that there exists an approximation\nwhich is more accurate than the LOY, and which leads to an effective\nHamiltonian whose diagonal matrix elements posses properties consistent with\nthe conclusions for the exact effective Hamiltonian described above. \n\n"}
{"id": "hep-th/0510268", "contents": "Title: Extension of the Poincar\\'e Symmetry and Its Field Theoretical\n  Implementation Abstract: We define a new algebraic extension of the Poincar\\'e symmetry; this algebra\nis used to implement a field theoretical model. Free Lagrangians are explicitly\nconstructed; several discussions regarding degrees of freedom, compatibility\nwith Abelian gauge invariance etc. are done. Finally we analyse the\npossibilities of interaction terms for this model. \n\n"}
{"id": "math-ph/9906026", "contents": "Title: Spectral statistics for quantized skew translations on the torus Abstract: We study the spectral statistics for quantized skew translations on the\ntorus, which are ergodic but not mixing for irrational parameters. It is shown\nexplicitly that in this case the level--spacing distribution and other common\nspectral statistics, like the number variance, do not exist in the\nsemiclassical limit. \n\n"}
{"id": "math/0101179", "contents": "Title: Quantum matrix ball: the Cauchi-Szeg\\\"o kernel and the Shilov boundary Abstract: This work produces a q-analogue of the Cauchi-Szeg\\\"o integral representation\nthat retrieves a holomorphic function in the matrix ball from its values on the\nShilov boundary. Besides that, the Shilov boundary of the quantum matrix ball\nis described and the U_q su(m,n)-covariance of the U_q s(u(m)x u(n))-invariant\nintegral on this boundary is established. The latter result allows one to\nobtain a q-analogue for the principal degenerate series of unitary\nrepresentations related to the Shilov boundary of the matrix ball. \n\n"}
{"id": "math/0104209", "contents": "Title: Non-commutative linear algebra and plurisubharmonic functions of\n  quaternionic variables Abstract: We recall known and establish new properties of the Dieudonn\\'e and Moore\ndeterminants of quaternionic matrices.Using these linear algebraic results we\ndevelop a basic theory of plurisubharmonic functions of quaternionic variables.\nThen we introduce and briefly discuss quaternionic Monge-Amp\\'ere equations. \n\n"}
{"id": "math/0108144", "contents": "Title: A normal form algorithm for the Brieskorn lattice Abstract: This article describes a normal form algorithm for the Brieskorn lattice of\nan isolated hypersurface singularity. It is the basis of efficient algorithms\nto compute the Bernstein-Sato polynomial, the complex monodromy, and\nHodge-theoretic invariants of the singularity such as the spectral pairs and\ngood bases of the Brieskorn lattice. The algorithm is a variant of Buchberger's\nnormal form algorithm for power series rings using the idea of partial standard\nbases and adic convergence replacing termination. \n\n"}
{"id": "math/0108211", "contents": "Title: One-arm exponent for critical 2D percolation Abstract: The probability that the cluster of the origin in critical site percolation\non the triangular grid has diameter larger than $R$ is proved to decay like\n$R^{-5/48}$ as $R\\to\\infty$. \n\n"}
{"id": "math/0209343", "contents": "Title: Conformal restriction: the chordal case Abstract: We characterize and describe all random subsets $K$ of a given simply\nconnected planar domain (the upper half-plane $\\H$, say) which satisfy the\n``conformal restriction'' property, i.e., $K$ connects two fixed boundary\npoints (0 and $\\infty$, say) and the law of $K$ conditioned to remain in a\nsimply connected open subset $D$ of $\\H$ is identical to that of $\\Phi(K)$,\nwhere $\\Phi$ is a conformal map from $\\H$ onto $D$ with $\\Phi(0)=0$ and\n$\\Phi(\\infty)=\\infty$. The construction of this family relies on the stochastic\nLoewner evolution (SLE) processes with parameter $\\kappa \\le 8/3$ and on their\ndistortion under conformal maps. We show in particular that SLE(8/3) is the\nonly random simple curve satisfying conformal restriction and relate it to the\nouter boundaries of planar Brownian motion and SLE(6). \n\n"}
{"id": "math/0301282", "contents": "Title: Hexagonal circle patterns with constant intersection angles and discrete\n  Painleve and Riccati equations Abstract: Hexagonal circle patterns with constant intersection angles mimicking\nholomorphic maps z^c and log(z) are studied. It is shown that the corresponding\ncircle patterns are immersed and described by special separatrix solutions of\ndiscrete Painleve and Riccati equations. The general solution of the Riccati\nequation is expressed in terms of the hypergeometric function. Global\nproperties of these solutions, as well as of the discrete z^c and log(z), are\nestablished. \n\n"}
{"id": "math/0306376", "contents": "Title: Equivalence of summatory conditions along sequences for bounded\n  holomorphic functions Abstract: A sequence of points $z_k$ in the unit disk is said to be thin for a given\ndecrease function $\\rho$, if there is a nontrivial bounded holomorphic function\nsuch that the infinite series $\\sum_k \\rho(1-|z_k|)|f(z_k)|$ converges. All\nsequences will be assumed hyperbolically separated. We give necessary and\nsufficient conditions for the problem of thinness of a sequence to be\nnon-trivial (one way or the other), and for two different decrease functions to\ngive rise to the same thin sequences. Along the way, some concrete conditions\n(necessary or sufficient) for a sequence to be thin are obtained. \n\n"}
{"id": "math/0406408", "contents": "Title: Weil-Petersson metric on the universal Teichmuller space II. Kahler\n  potential and period mapping Abstract: We study the Hilbert manifold structure on $T_{0}(1)$ -- the connected\ncomponent of the identity of the Hilbert manifold T(1). We characterize points\non $T_{0}(1)$ in terms of Bers and pre-Bers embeddings, and prove that the\nGrunsky operators $B_{1}$ and $B_{4}$, associated with the points in $T_{0}(1)$\nvia conformal welding, are Hilbert-Schmidt. We define a ``universal Liouville\naction'' -- a real-valued function $\\SSS_{1}$ on $T_{0}(1)$, and prove that it\nis a K\\\"{a}hler potential of the Weil-Petersson metric on $T_{0}(1)$. We also\nprove that $\\SSS_{1}$ is $-\\tfrac{1}{12\\pi}$ times the logarithm of the\nFredholm determinant of associated quasi-circle, which generalizes classical\nresults of Schiffer and Hawley. We define the universal period mapping\n$\\hat{\\cP}: T(1)\\to\\cB(\\ell^{2})$ of T(1) into the Banach space of bounded\noperators on the Hilbert space $\\ell^{2}$, prove that $\\hat{\\cP}$ is a\nholomorphic mapping of Banach manifolds, and show that $\\hat{\\cP}$ coincides\nwith the period mapping introduced by Kurillov and Yuriev and Nag and Sullivan.\nWe prove that the restriction of $\\hat{\\cP}$ to $T_{0}(1)$ is an inclusion of\n$T_{0}(1)$ into the Segal-Wilson universal Grassmannian, which is a holomorphic\nmapping of Hilbert manifolds. We also prove that the image of the topological\ngroup $S$ of symmetric homeomorphisms of $S^{1}$ under the mapping $\\hat{\\cP}$\nconsists of compact operators on $\\ell^{2}$. \n\n"}
{"id": "math/0409353", "contents": "Title: On rational approximation of algebraic functions Abstract: We construct a new scheme of approximation of any multivalued algebraic\nfunction $f(z)$ by a sequence $\\{r_{n}(z)\\}_{n\\in \\mathbb{N}}$ of rational\nfunctions. The latter sequence is generated by a recurrence relation which is\ncompletely determined by the algebraic equation satisfied by $f(z)$. Compared\nto the usual Pad\\'e approximation our scheme has a number of advantages, such\nas simple computational procedures that allow us to prove natural analogs of\nthe Pad\\'e Conjecture and Nuttall's Conjecture for the sequence\n$\\{r_{n}(z)\\}_{n\\in \\mathbb{N}}$ in the complement $\\mathbb{CP}^1\\setminus\n\\D_{f}$, where $\\D_{f}$ is the union of a finite number of segments of real\nalgebraic curves and finitely many isolated points. In particular, our\nconstruction makes it possible to control the behavior of spurious poles and to\ndescribe the asymptotic ratio distribution of the family $\\{r_{n}(z)\\}_{n\\in\n\\mathbb{N}}$. As an application we settle the so-called 3-conjecture of\nEgecioglu {\\em et al} dealing with a 4-term recursion related to a polynomial\nRiemann Hypothesis. \n\n"}
{"id": "math/0410390", "contents": "Title: Holomorphic discs with dense images Abstract: We prove that for any complex manifold X, the set of all holomorphic maps\nfrom the unit disc to X whose images are everywhere dense in X forms a dense\nsubset in the space of all holomorphic maps from the disc to X. We show by an\nexample that this need not hold for maps from the disc to complex spaces with\nsingularities. \n\n"}
{"id": "math/0503490", "contents": "Title: Cohomology and extension problems for semi q-coronae Abstract: We prove some extension theorems for analytic objects, in particular sections\nof a coherent sheaf, defined in semi q-coronae of a complex space. Semi\nq-coronae are domains whose boundary is the union of a Levi flat part, a\nq-pseudoconvex part and a q-pseudoconcave part. Such results are obtained\nmainly using cohomological techniques. \n\n"}
{"id": "math/0602030", "contents": "Title: Homogeneous Levi degenerate CR-manifolds in dimension 5 Abstract: We investigate CR-manifolds which are tubes M:= F x iV over general bases F\nin a real vector space V and characterize the k-nondegeneracy of M in terms of\nthe real affine geometry of F. We give a method for an explicit computation of\nthe Lie algebra hol(M,a) of all local infinitesimal CR-transformations at a and\nuse these local invariants to establish the CR-inequivalence of certain\nfamilies of CR-manifolds. In dimension 5 we present, apart from the well known\ntube over the future light cone, new examples of 2-nondegenerate homogeneous\nCR-manifolds that are mutually locally CR-inequivalent. \n\n"}
{"id": "math/0603545", "contents": "Title: Algebraic degrees for iterates of meromorphic self-maps of $\\P^k$ Abstract: We first introduce the class of quasi-algebraically stable meromorphic maps\nof $\\P^k.$ This class is strictly larger than that of algebraically stable\nmeromorphic self-maps of $\\P^k.$\n  Then we prove that all maps in the new class enjoy a recurrent property. In\nparticular, the algebraic degrees for iterates of these maps can be computed\nand their first dynamical degrees are always algebraic integers. \n\n"}
{"id": "math/0604588", "contents": "Title: Elliptic gamma functions, triptic curves and SL_3(Z) Abstract: This is a condensed exposition of the results of math.QA/0601337, based on a\ntalk of the first author at the Oberwolfach workshop \"Deformations and\nContractions in Mathematics and Physics\", 15-21 January 2006. \n\n"}
{"id": "math/0610611", "contents": "Title: Remarks on the Alexander-Wermer Theorem for Curves Abstract: We give a new proof of the Alexander-Wermer Theorem that characterizes the\noriented curves in C^n which bound positive holomorphic chains, in terms of the\nlinking numbers of the curve with algebraic cycles in the complement. In fact,\nwe establish a slightly stronger version which applies to a wider class of\nboundary 1-cycles. Arguments here are based on the Hahn-Banach Theorem and some\ngeometric measure theory. Several ingredients in the original proof have been\neliminated. \n\n"}
{"id": "math/0612630", "contents": "Title: The weigthed Monge-Amp\\`ere energy of quasiplurisubharmonic functions Abstract: We study degenerate complex Monge-Amp\\`ere equations on a compact K\\\"ahler\nmanifold $(X,\\omega)$. We show that the complex Monge-Amp\\`ere operator\n$(\\omega + dd^c \\cdot)^n$ is well-defined on the class ${\\mathcal E}(X,\\omega)$\nof $\\omega$-plurisubharmonic functions with finite weighted Monge-Amp\\`ere\nenergy. The class ${\\mathcal E}(X,\\omega)$ is the largest class of $\\omega$-psh\nfunctions on which the Monge-Amp\\`ere operator is well-defined and the\ncomparison principle is valid. It contains several functions whose gradient is\nnot square integrable. We give a complete description of the range of the\nMonge-Amp\\`ere operator $(\\omega +dd^c \\cdot)^n$ on ${\\mathcal E}(X,\\omega)$,\nas well as on some of its subclasses.\n  We also study uniqueness properties, extending Calabi's result to this\nunbounded and degenerate situation, and we give applications to complex\ndynamics and to the existence of singular K\\\"ahler-Einstein metrics. \n\n"}
{"id": "math/0703788", "contents": "Title: Quasi-conformal functions of quaternion and octonion variables, their\n  integral transformations Abstract: The article is devoted to holomorphic and meromorphic functions of quaternion\nand octonion variables. New classes of quasi-conformal and quasi-meromorphic\nmappings are defined and investigated. Properties of such functions such as\ntheir residues and argument principle are studied. It is proved, that the\nfamily of all quasi-conformal diffeomorphisms of a domain form a topological\ngroup relative to composition of mappings. Cases when it is a\nfinite-dimensional Lie group over $\\bf R$ are studied. Relations between\nquasi-conformal functions and integral transformations of functions over\nquaternions and octonions are established. For this, in particular,\nnoncommutative analogs of the Laplace and Mellin transformations are studied\nand used. Examples of such functions are given. Applications to problems of\ncomplex analysis are demonstrated. \n\n"}
{"id": "quant-ph/0009084", "contents": "Title: Emergence of Fermi-Dirac Thermalization in the Quantum Computer Core Abstract: We model an isolated quantum computer as a two-dimensional lattice of qubits\n(spin halves) with fluctuations in individual qubit energies and residual\nshort-range inter-qubit couplings. In the limit when fluctuations and couplings\nare small compared to the one-qubit energy spacing, the spectrum has a band\nstructure and we study the quantum computer core (central band) with the\nhighest density of states. Above a critical inter-qubit coupling strength,\nquantum chaos sets in, leading to quantum ergodicity of eigenstates in an\nisolated quantum computer. The onset of chaos results in the interaction\ninduced dynamical thermalization and the occupation numbers well described by\nthe Fermi-Dirac distribution. This thermalization destroys the noninteracting\nqubit structure and sets serious requirements for the quantum computer\noperability. \n\n"}
{"id": "quant-ph/0010030", "contents": "Title: An extension of \"Popper's experiment\" can test interpretations of\n  quantum mechanics Abstract: Karl Popper proposed a way to test whether a proposed relation of a\nquantum-mechanical state to perceived reality in the Copenhagen interpretation\n(CI) of quantum mechanics - namely that the state of a particle is merely an\nexpression of ``what is known'' about the system - is in agreement with all\nexperimental facts. A conceptual flaw in Popper's proposal is identified and an\nimproved version of his experiment (called ``Extension step 1'') - which fully\nserves its original purpose - is suggested. The main purpose of this paper is\nto suggest to perform this experiment. The results of this experiment predicted\nunder the alternative assumptions that the CI - together with the above\nconnection of the state function with reality - or the ``many-worlds''\ninterpretation (MWI) is correct are shown to be identical. Only after a further\nmodification (called ``Extension step 2'') - the use of an ion isolated from\nthe macroscopic environment as particle detector - the predictions using the\nrespective interpretations become qualitatively different. \n\n"}
{"id": "quant-ph/0102061", "contents": "Title: Gravitational decoherence of planetary motions Abstract: We study the effect of the scattering of gravitational waves on planetary\nmotions, say the motion of the Moon around the Earth. Though this effect has a\nnegligible influence on dissipation, it dominates fluctuations and the\nassociated decoherence mechanism, due to the very high effective temperature of\nthe background of gravitational waves in our galactic environment. \n\n"}
{"id": "quant-ph/0104011", "contents": "Title: Multipartite entangled coherent states Abstract: We propose a scheme for generating multipartite entangled coherent states via\nentanglement swapping, with an example of a physical realization in ion traps.\nBipartite entanglement of these multipartite states is quantified by the\nconcurrence. We also use the $N$--tangle to compute multipartite entanglement\nfor certain systems. Finally we establish that these results for entanglement\ncan be applied to more general multipartite entangled nonorthogonal states. \n\n"}
{"id": "quant-ph/0105099", "contents": "Title: Maximal entanglement of nonorthogonal states: classification Abstract: A necessary and sufficient condition for the maximal entanglement of\nbipartite nonorthogonal pure states is found. The condition is applied to the\nmaximal entanglement of coherent states. Some new classes of maximally\nentangled coherent states are explicited constructed; their limits give rise to\nmaximally entangled Bell-like states. \n\n"}
{"id": "quant-ph/0106023", "contents": "Title: Typical entanglement in multi-qubit systems Abstract: Quantum entanglement and its paradoxical properties hold the key to an\ninformation processing revolution. Much attention has focused recently on the\nchallenging problem of characterizing entanglement. Entanglement for a two\nqubit system is reasonably well understood, however, the nature and properties\nof multiple qubit systems are largely unexplored. Motivated by the importance\nof such systems in quantum computing, we show that typical pure states of N\nqubits are highly entangled but have decreasing amounts of pairwise\nentanglement (measured using the Wootter's concurrence formula) as N increases.\nAbove six qubits very few states have any pairwise entanglement, and generally,\nfor a typical pure state of N qubits there is a sharp cut-off where its\nsubsystems of size m become PPT (positive partial transpose i.e., separable or\nonly bound entangled) around N >~ 2m + 3, based on numerical analysis up to\nN=13. \n\n"}
{"id": "quant-ph/0207016", "contents": "Title: Quantum trajectory approach to stochastically-induced quantum\n  interference effects in coherently-driven two-level atoms Abstract: Stochastic perturbation of two-level atoms strongly driven by a coherent\nlight field is analyzed by the quantum trajectory method. A new method is\ndeveloped for calculating the resonance fluorescence spectra from numerical\nsimulations. It is shown that in the case of dominant incoherent perturbation,\nthe stochastic noise can unexpectedly create phase correlation between the\nneighboring atomic dressed states. This phase correlation is responsible for\nquantum interference between the related transitions resulting in anomalous\nmodifications of the resonance fluorescence spectra. \n\n"}
{"id": "quant-ph/0210001", "contents": "Title: Towards quantum superpositions of a mirror Abstract: We propose a scheme for creating quantum superposition states involving of\norder $10^{14}$ atoms via the interaction of a single photon with a tiny\nmirror. This mirror, mounted on a high-quality mechanical oscillator, is part\nof a high-finesse optical cavity which forms one arm of a Michelson\ninterferometer. By observing the interference of the photon only, one can study\nthe creation and decoherence of superpositions involving the mirror. All\nexperimental requirements appear to be within reach of current technology. \n\n"}
{"id": "quant-ph/0307169", "contents": "Title: Wehrl entropy, Lieb conjecture and entanglement monotones Abstract: We propose to quantify the entanglement of pure states of $N \\times N$\nbipartite quantum system by defining its Husimi distribution with respect to\n$SU(N)\\times SU(N)$ coherent states. The Wehrl entropy is minimal if and only\nif the pure state analyzed is separable. The excess of the Wehrl entropy is\nshown to be equal to the subentropy of the mixed state obtained by partial\ntrace of the bipartite pure state. This quantity, as well as the generalized\n(R{\\'e}nyi) subentropies, are proved to be Schur--convex, so they are\nentanglement monotones and may be used as alternative measures of entanglement. \n\n"}
{"id": "quant-ph/0311116", "contents": "Title: Quantum Error Correction on Linear Nearest Neighbor Qubit Arrays Abstract: A minimal depth quantum circuit implementing 5-qubit quantum error correction\nin a manner optimized for a linear nearest neighbor architecture is described.\nThe canonical decomposition is used to construct fast and simple gates that\nincorporate the necessary swap operations. Simulations of the circuit's\nperformance when subjected to discrete and continuous errors are presented. The\nrelationship between the error rate of a physical qubit and that of a logical\nqubit is investigated with emphasis on determining the concatenated error\ncorrection threshold. \n\n"}
{"id": "quant-ph/0407033", "contents": "Title: Note on multiple additivity of minimal Renyi entropy output of the\n  Werner-Holevo channels Abstract: We give an elementary self-contained proof that the minimal entropy output of\narbitrary products of channels $\\rho \\mapsto \\frac{1}{d-1}(1-\\rho^T)$ is\nadditive. \n\n"}
{"id": "quant-ph/0407184", "contents": "Title: Quantum polarization properties of two-mode energy eigenstates Abstract: We show that any pure, two-mode, $N$-photon state with $N$ odd or equal to\ntwo can be transformed into an orthogonal state using only linear optics.\nAccording to a recently suggested definition of polarization degree, this\nimplies that all such states are fully polarized. This is also found to be true\nfor any pure, two-mode, energy eigenstate belonging to a two-dimensional SU(2)\norbit. Complete two- and three-photon bases whose basis states are related by\nonly phase shifts or geometrical rotations are also derived. \n\n"}
{"id": "quant-ph/0503054", "contents": "Title: Extended Cahill-Glauber formalism for finite dimensional spaces: I.\n  Fundamentals Abstract: The Cahill-Glauber approach for quantum mechanics on phase-space is extended\nto the finite dimensional case through the use of discrete coherent states. All\nproperties and features of the continuous formalism are appropriately\ngeneralized. The continuum results are promptly recovered as a limiting case.\nThe Jacobi Theta functions are shown to have a prominent role in the context. \n\n"}
{"id": "quant-ph/0507010", "contents": "Title: The quantum adiabatic search with decoherence in the instantaneous\n  energy eigenbasis Abstract: In Phys. Rev. A {\\bf 71}, 060312(R) (2005) the robustness of the local\nadiabatic quantum search to decoherence in the instantaneous eigenbasis of the\nsearch Hamiltonian was examined. We expand this analysis to include the case of\nthe global adiabatic quantum search. As in the case of the local search the\nasymptotic time complexity for the global search is the same as for the ideal\nclosed case, as long as the Hamiltonian dynamics is present. In the case of\npure decoherence, where the environment monitors the search Hamiltonian, we\nfind that the time complexity of the global quantum adiabatic search scales\nlike $N^{3/2}$, where $N$ is the list length. We moreover extend the analysis\nto include success probabilities $p<1$ and prove bounds on the run time with\nthe same scaling as in the conditions for the $p\\to 1$ limit. We supplement the\nanalytical results by numerical simulations of the global and local search. \n\n"}
{"id": "quant-ph/0507185", "contents": "Title: Mean-field dynamics of a Bose-Einstein condensate in a time-dependent\n  triple-well trap: Nonlinear eigenstates, Landau-Zener models and STIRAP Abstract: We investigate the dynamics of a Bose--Einstein condensate (BEC) in a\ntriple-well trap in a three-level approximation. The inter-atomic interactions\nare taken into account in a mean-field approximation (Gross-Pitaevskii\nequation), leading to a nonlinear three-level model. New eigenstates emerge due\nto the nonlinearity, depending on the system parameters. Adiabaticity breaks\ndown if such a nonlinear eigenstate disappears when the parameters are varied.\nThe dynamical implications of this loss of adiabaticity are analyzed for two\nimportant special cases: A three level Landau-Zener model and the STIRAP\nscheme. We discuss the emergence of looped levels for an equal-slope\nLandau-Zener model. The Zener tunneling probability does not tend to zero in\nthe adiabatic limit and shows pronounced oscillations as a function of the\nvelocity of the parameter variation. Furthermore we generalize the STIRAP\nscheme for adiabatic coherent population transfer between atomic states to the\nnonlinear case. It is shown that STIRAP breaks down if the nonlinearity exceeds\nthe detuning. \n\n"}
{"id": "quant-ph/0511063", "contents": "Title: Accounting Principles are Simulated on Quantum Computers Abstract: The paper is devoted to a new idea of simulation of accounting by quantum\ncomputing. We expose the actual accounting principles in a pure mathematics\nlanguage. After that we simulated the accounting principles on quantum\ncomputers. We show that all arbitrary accounting actions are exhausted by the\ndescribed basic actions. The main problem of accounting are reduced to some\nsystem of linear equations in the economic model of Leontief. In this\nsimulation we use our constructed quantum Gau\\ss-Jordan Elimination to solve\nthe problem and the time of quantum computing is some square root order faster\nthan the time in classical computing. \n\n"}
{"id": "quant-ph/0511099", "contents": "Title: Modeling photo-detectors in quantum optics Abstract: Photo-detection plays a fundamental role in experimental quantum optics and\nis of particular importance in the emerging field of linear optics quantum\ncomputing. Present theoretical treatment of photo-detectors is highly idealized\nand fails to consider many important physical effects. We present a physically\nmotivated model for photo-detectors which accommodates for the effects of\nfinite resolution, bandwidth and efficiency, as well as dark-counts and\ndead-time. We apply our model to two simple well known applications, which\nillustrates the significance of these characteristics. \n\n"}
{"id": "quant-ph/0512125", "contents": "Title: Quantum information and computation Abstract: This article deals with theoretical developments in the subject of quantum\ninformation and quantum computation, and includes an overview of classical\ninformation and some relevant quantum mechanics. The discussion covers topics\nin quantum communication, quantum cryptography, and quantum computation, and\nconcludes by considering whether a perspective in terms of quantum information\nsheds new light on the conceptual problems of quantum mechanics. \n\n"}
{"id": "quant-ph/0603258", "contents": "Title: Ultrafast Coherent Coupling of Atomic Hyperfine and Photon Frequency\n  Qubits Abstract: We demonstrate ultrafast coherent coupling between an atomic qubit stored in\na single trapped cadmium ion and a photonic qubit represented by two resolved\nfrequencies of a photon. Such ultrafast coupling is crucial for entangling\nnetworks of remotely-located trapped ions through photon interference, and is\nalso a key component for realizing ultrafast quantum gates between\nCoulomb-coupled ions. \n\n"}
{"id": "quant-ph/0606183", "contents": "Title: On the quantum analogue of Galileo's leaning tower experiment Abstract: The quantum analogue of Galileo's leaning tower experiment is revisited using\nwave packets evolving under the gravitational potential. We first calculate the\nposition detection probabilities for particles projected upwards against\ngravity around the classical turning point and also around the point of initial\nprojection, which exhibit mass dependence at both these points. We then compute\nthe mean arrival time of freely falling particles using the quantum probability\ncurrent, which also turns out to be mass dependent. The mass dependence of both\nthe position detection probabilities and the mean arrival time vanish in the\nlimit of large mass. Thus, compatibility between the weak equivalence principle\nand quantum mechanics is recovered in the macroscopic limit of the latter. \n\n"}
{"id": "quant-ph/0607030", "contents": "Title: First-order intertwining operators with position dependent mass and\n  $\\eta$- weak-psuedo-Hermiticity generators Abstract: A Hermitian and an anti-Hermitian first-order intertwining operators are\nintroduced and a class of $\\eta$-weak-pseudo-Hermitian position-dependent mass\n(PDM) Hamiltonians are constructed. A corresponding reference-target\n$\\eta$-weak-pseudo-Hermitian PDM -- Hamiltonians' map is suggested. Some\n$\\eta$-weak-pseudo-Hermitian PT -symmetric Scarf II and periodic-type models\nare used as illustrative examples. Energy-levels crossing and flown-away states\nphenomena are reported for the resulting Scarf II spectrum. Some of the\ncorresponding $\\eta$-weak-pseudo-Hermitian Scarf II- and\nperiodic-type-isospectral models (PT -symmetric and non-PT -symmetric) are\ngiven as products of the reference-target map. \n\n"}
{"id": "quant-ph/0703112", "contents": "Title: Graphs, Quadratic Forms, and Quantum Codes Abstract: We show that any stabilizer code over a finite field is equivalent to a\ngraphical quantum code. Furthermore we prove that a graphical quantum code over\na finite field is a stabilizer code. The technique used in the proof\nestablishes a new connection between quantum codes and quadratic forms. We\nprovide some simple examples to illustrate our results. \n\n"}
{"id": "quant-ph/9706013", "contents": "Title: Quantum Statistical Thermodynamics of Two-Level Systems Abstract: We study four distinct families of Gibbs canonical distributions defined on\nthe standard complex, quaternionic, real and classical (nonquantum) two-level\nsystems. The structure function or density of states for any two-level system\nis a simple power (1, 3, 0 or -1) of the length of its polarization vector,\nwhile the magnitude of the energy of the system, in all four cases, is the\nnegative of the logarithm of the determinant of the corresponding\ntwo-dimensional density matrix. Functional relationships (proportional to\nratios of gamma functions) are found between the average polarizations with\nrespect to the Gibbs distributions and the effective polarization temperature\nparameters. In the standard complex case, this yields an interesting\nalternative, meeting certain probabilistic requirements recently set forth by\nLavenda, to the more conventional (hyperbolic tangent) Brillouin function of\nparamagnetism (which, Lavenda argues, fails to meet such specifications). \n\n"}
{"id": "quant-ph/9901030", "contents": "Title: Some general bounds for 1-D scattering Abstract: One-dimensional scattering problems are of wide physical interest and are\nencountered in many diverse applications. In this article I establish some very\ngeneral bounds for reflection and transmission coefficients for one-dimensional\npotential scattering. Equivalently, these results may be phrased as general\nbounds on the Bogolubov coefficients, or statements about the transfer matrix.\nA similar analysis can be provided for the parametric change of frequency of a\nharmonic oscillator. A number of specific examples are discussed---in\nparticular I provide a general proof that sharp step function potentials always\nscatter more effectively than the corresponding smoothed potentials. The\nanalysis also serves to collect together and unify what would otherwise appear\nto be quite unrelated results. \n\n"}
{"id": "quant-ph/9912008", "contents": "Title: Quantum Logic with a Single Trapped Electron Abstract: We propose the use of a trapped electron to implement quantum logic\noperations. The fundamental controlled-NOT gate is shown to be feasible. The\ntwo quantum bits are stored in the internal and external (motional) degrees of\nfreedom. \n\n"}
