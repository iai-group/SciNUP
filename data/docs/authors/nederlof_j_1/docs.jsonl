{"id": "0704.0229", "contents": "Title: Geometric Complexity Theory VI: the flip via saturated and positive\n  integer programming in representation theory and algebraic geometry Abstract: This article belongs to a series on geometric complexity theory (GCT), an\napproach to the P vs. NP and related problems through algebraic geometry and\nrepresentation theory. The basic principle behind this approach is called the\nflip. In essence, it reduces the negative hypothesis in complexity theory (the\nlower bound problems), such as the P vs. NP problem in characteristic zero, to\nthe positive hypothesis in complexity theory (the upper bound problems):\nspecifically, to showing that the problems of deciding nonvanishing of the\nfundamental structural constants in representation theory and algebraic\ngeometry, such as the well known plethysm constants--or rather certain relaxed\nforms of these decision probelms--belong to the complexity class P. In this\narticle, we suggest a plan for implementing the flip, i.e., for showing that\nthese relaxed decision problems belong to P. This is based on the reduction of\nthe preceding complexity-theoretic positive hypotheses to mathematical\npositivity hypotheses: specifically, to showing that there exist positive\nformulae--i.e. formulae with nonnegative coefficients--for the structural\nconstants under consideration and certain functions associated with them. These\nturn out be intimately related to the similar positivity properties of the\nKazhdan-Lusztig polynomials and the multiplicative structural constants of the\ncanonical (global crystal) bases in the theory of Drinfeld-Jimbo quantum\ngroups. The known proofs of these positivity properties depend on the Riemann\nhypothesis over finite fields and the related results. Thus the reduction here,\nin conjunction with the flip, in essence, says that the validity of the P vs.\nNP conjecture in characteristic zero is intimately linked to the Riemann\nhypothesis over finite fields and related problems. \n\n"}
{"id": "0704.1269", "contents": "Title: Phase Transitions in the Coloring of Random Graphs Abstract: We consider the problem of coloring the vertices of a large sparse random\ngraph with a given number of colors so that no adjacent vertices have the same\ncolor. Using the cavity method, we present a detailed and systematic analytical\nstudy of the space of proper colorings (solutions).\n  We show that for a fixed number of colors and as the average vertex degree\n(number of constraints) increases, the set of solutions undergoes several phase\ntransitions similar to those observed in the mean field theory of glasses.\nFirst, at the clustering transition, the entropically dominant part of the\nphase space decomposes into an exponential number of pure states so that beyond\nthis transition a uniform sampling of solutions becomes hard. Afterward, the\nspace of solutions condenses over a finite number of the largest states and\nconsequently the total entropy of solutions becomes smaller than the annealed\none. Another transition takes place when in all the entropically dominant\nstates a finite fraction of nodes freezes so that each of these nodes is\nallowed a single color in all the solutions inside the state. Eventually, above\nthe coloring threshold, no more solutions are available. We compute all the\ncritical connectivities for Erdos-Renyi and regular random graphs and determine\ntheir asymptotic values for large number of colors.\n  Finally, we discuss the algorithmic consequences of our findings. We argue\nthat the onset of computational hardness is not associated with the clustering\ntransition and we suggest instead that the freezing transition might be the\nrelevant phenomenon. We also discuss the performance of a simple local Walk-COL\nalgorithm and of the belief propagation algorithm in the light of our results. \n\n"}
{"id": "0704.1694", "contents": "Title: Locally Decodable Codes From Nice Subsets of Finite Fields and Prime\n  Factors of Mersenne Numbers Abstract: A k-query Locally Decodable Code (LDC) encodes an n-bit message x as an N-bit\ncodeword C(x), such that one can probabilistically recover any bit x_i of the\nmessage by querying only k bits of the codeword C(x), even after some constant\nfraction of codeword bits has been corrupted. The major goal of LDC related\nresearch is to establish the optimal trade-off between length and query\ncomplexity of such codes.\n  Recently [Y] introduced a novel technique for constructing locally decodable\ncodes and vastly improved the upper bounds for code length. The technique is\nbased on Mersenne primes. In this paper we extend the work of [Y] and argue\nthat further progress via these methods is tied to progress on an old number\ntheory question regarding the size of the largest prime factors of Mersenne\nnumbers.\n  Specifically, we show that every Mersenne number m=2^t-1 that has a prime\nfactor p>m^\\gamma yields a family of k(\\gamma)-query locally decodable codes of\nlength Exp(n^{1/t}). Conversely, if for some fixed k and all \\epsilon > 0 one\ncan use the technique of [Y] to obtain a family of k-query LDCs of length\nExp(n^\\epsilon); then infinitely many Mersenne numbers have prime factors arger\nthan known currently. \n\n"}
{"id": "0704.2505", "contents": "Title: Algebraic Distributed Space-Time Codes with Low ML Decoding Complexity Abstract: \"Extended Clifford algebras\" are introduced as a means to obtain low ML\ndecoding complexity space-time block codes. Using left regular matrix\nrepresentations of two specific classes of extended Clifford algebras, two\nsystematic algebraic constructions of full diversity Distributed Space-Time\nCodes (DSTCs) are provided for any power of two number of relays. The left\nregular matrix representation has been shown to naturally result in space-time\ncodes meeting the additional constraints required for DSTCs. The DSTCs so\nconstructed have the salient feature of reduced Maximum Likelihood (ML)\ndecoding complexity. In particular, the ML decoding of these codes can be\nperformed by applying the lattice decoder algorithm on a lattice of four times\nlesser dimension than what is required in general. Moreover these codes have a\nuniform distribution of power among the relays and in time, thus leading to a\nlow Peak to Average Power Ratio at the relays. \n\n"}
{"id": "0704.2507", "contents": "Title: STBCs from Representation of Extended Clifford Algebras Abstract: A set of sufficient conditions to construct $\\lambda$-real symbol Maximum\nLikelihood (ML) decodable STBCs have recently been provided by Karmakar et al.\nSTBCs satisfying these sufficient conditions were named as Clifford Unitary\nWeight (CUW) codes. In this paper, the maximal rate (as measured in complex\nsymbols per channel use) of CUW codes for $\\lambda=2^a,a\\in\\mathbb{N}$ is\nobtained using tools from representation theory. Two algebraic constructions of\ncodes achieving this maximal rate are also provided. One of the constructions\nis obtained using linear representation of finite groups whereas the other\nconstruction is based on the concept of right module algebra over\nnon-commutative rings. To the knowledge of the authors, this is the first paper\nin which matrices over non-commutative rings is used to construct STBCs. An\nalgebraic explanation is provided for the 'ABBA' construction first proposed by\nTirkkonen et al and the tensor product construction proposed by Karmakar et al.\nFurthermore, it is established that the 4 transmit antenna STBC originally\nproposed by Tirkkonen et al based on the ABBA construction is actually a single\ncomplex symbol ML decodable code if the design variables are permuted and\nsignal sets of appropriate dimensions are chosen. \n\n"}
{"id": "0704.2596", "contents": "Title: Computing Extensions of Linear Codes Abstract: This paper deals with the problem of increasing the minimum distance of a\nlinear code by adding one or more columns to the generator matrix. Several\nmethods to compute extensions of linear codes are presented. Many codes\nimproving the previously known lower bounds on the minimum distance have been\nfound. \n\n"}
{"id": "0705.2876", "contents": "Title: An online algorithm for generating fractal hash chains applied to\n  digital chains of custody Abstract: This paper gives an online algorithm for generating Jakobsson's fractal hash\nchains. Our new algorithm compliments Jakobsson's fractal hash chain algorithm\nfor preimage traversal since his algorithm assumes the entire hash chain is\nprecomputed and a particular list of Ceiling(log n) hash elements or pebbles\nare saved. Our online algorithm for hash chain traversal incrementally\ngenerates a hash chain of n hash elements without knowledge of n before it\nstarts. For any n, our algorithm stores only the Ceiling(log n) pebbles which\nare precisely the inputs for Jakobsson's amortized hash chain preimage\ntraversal algorithm. This compact representation is useful to generate,\ntraverse, and store a number of large digital hash chains on a small and\nconstrained device. We also give an application using both Jakobsson's and our\nnew algorithm applied to digital chains of custody for validating dynamically\nchanging forensics data. \n\n"}
{"id": "0708.1909", "contents": "Title: Lower Bounds for the Complexity of the Voronoi Diagram of Polygonal\n  Curves under the Discrete Frechet Distance Abstract: We give lower bounds for the combinatorial complexity of the Voronoi diagram\nof polygonal curves under the discrete Frechet distance. We show that the\nVoronoi diagram of n curves in R^d with k vertices each, has complexity\nOmega(n^{dk}) for dimension d=1,2 and Omega(n^{d(k-1)+2}) for d>2. \n\n"}
{"id": "0709.4273", "contents": "Title: Set Matrices and The Path/Cycle Problem Abstract: Presentation of set matrices and demonstration of their efficiency as a tool\nusing the path/cycle problem. \n\n"}
{"id": "0712.1402", "contents": "Title: Reconstruction of Markov Random Fields from Samples: Some Easy\n  Observations and Algorithms Abstract: Markov random fields are used to model high dimensional distributions in a\nnumber of applied areas. Much recent interest has been devoted to the\nreconstruction of the dependency structure from independent samples from the\nMarkov random fields. We analyze a simple algorithm for reconstructing the\nunderlying graph defining a Markov random field on $n$ nodes and maximum degree\n$d$ given observations. We show that under mild non-degeneracy conditions it\nreconstructs the generating graph with high probability using $\\Theta(d\n\\epsilon^{-2}\\delta^{-4} \\log n)$ samples where $\\epsilon,\\delta$ depend on the\nlocal interactions. For most local interaction $\\eps,\\delta$ are of order\n$\\exp(-O(d))$.\n  Our results are optimal as a function of $n$ up to a multiplicative constant\ndepending on $d$ and the strength of the local interactions. Our results seem\nto be the first results for general models that guarantee that {\\em the}\ngenerating model is reconstructed. Furthermore, we provide explicit $O(n^{d+2}\n\\epsilon^{-2}\\delta^{-4} \\log n)$ running time bound. In cases where the\nmeasure on the graph has correlation decay, the running time is $O(n^2 \\log n)$\nfor all fixed $d$. We also discuss the effect of observing noisy samples and\nshow that as long as the noise level is low, our algorithm is effective. On the\nother hand, we construct an example where large noise implies\nnon-identifiability even for generic noise and interactions. Finally, we\nbriefly show that in some simple cases, models with hidden nodes can also be\nrecovered. \n\n"}
{"id": "0712.2384", "contents": "Title: Multi-group ML Decodable Collocated and Distributed Space Time Block\n  Codes Abstract: In this paper, collocated and distributed space-time block codes (DSTBCs)\nwhich admit multi-group maximum likelihood (ML) decoding are studied. First the\ncollocated case is considered and the problem of constructing space-time block\ncodes (STBCs) which optimally tradeoff rate and ML decoding complexity is\nposed. Recently, sufficient conditions for multi-group ML decodability have\nbeen provided in the literature and codes meeting these sufficient conditions\nwere called Clifford Unitary Weight (CUW) STBCs. An algebraic framework based\non extended Clifford algebras is proposed to study CUW STBCs and using this\nframework, the optimal tradeoff between rate and ML decoding complexity of CUW\nSTBCs is obtained for few specific cases. Code constructions meeting this\ntradeoff optimally are also provided. The paper then focuses on multi-group ML\ndecodable DSTBCs for application in synchronous wireless relay networks and\nthree constructions of four-group ML decodable DSTBCs are provided. Finally,\nthe OFDM based Alamouti space-time coded scheme proposed by Li-Xia for a 2\nrelay asynchronous relay network is extended to a more general transmission\nscheme that can achieve full asynchronous cooperative diversity for arbitrary\nnumber of relays. It is then shown how differential encoding at the source can\nbe combined with the proposed transmission scheme to arrive at a new\ntransmission scheme that can achieve full cooperative diversity in asynchronous\nwireless relay networks with no channel information and also no timing error\nknowledge at the destination node. Four-group decodable DSTBCs applicable in\nthe proposed OFDM based transmission scheme are also given. \n\n"}
{"id": "0712.4213", "contents": "Title: Exact Quantum Algorithms for the Leader Election Problem Abstract: This paper gives the first separation of quantum and classical pure (i.e.,\nnon-cryptographic) computing abilities with no restriction on the amount of\navailable computing resources, by considering the exact solvability of a\ncelebrated unsolvable problem in classical distributed computing, the ``leader\nelection problem'' on anonymous networks. The goal of the leader election\nproblem is to elect a unique leader from among distributed parties. The paper\nconsiders this problem for anonymous networks, in which each party has the same\nidentifier. It is well-known that no classical algorithm can solve exactly\n(i.e., in bounded time without error) the leader election problem in anonymous\nnetworks, even if it is given the number of parties. This paper gives two\nquantum algorithms that, given the number of parties, can exactly solve the\nproblem for any network topology in polynomial rounds and polynomial\ncommunication/time complexity with respect to the number of parties, when the\nparties are connected by quantum communication links. \n\n"}
{"id": "0801.0857", "contents": "Title: Period-Different $m$-Sequences With At Most A Four-Valued Cross\n  Correlation Abstract: In this paper, we follow the recent work of Helleseth, Kholosha, Johanssen\nand Ness to study the cross correlation between an $m$-sequence of period\n$2^m-1$ and the $d$-decimation of an $m$-sequence of shorter period $2^{n}-1$\nfor an even number $m=2n$. Assuming that $d$ satisfies $d(2^l+1)=2^i({\\rm mod}\n2^n-1)$ for some $l$ and $i$, we prove the cross correlation takes exactly\neither three or four values, depending on ${\\rm gcd}(l,n)$ is equal to or\nlarger than 1. The distribution of the correlation values is also completely\ndetermined. Our result confirms the numerical phenomenon Helleseth et al found.\nIt is conjectured that there are no more other cases of $d$ that give at most a\nfour-valued cross correlation apart from the ones proved here. \n\n"}
{"id": "0802.2612", "contents": "Title: On Subgraph Isomorphism Abstract: Article explicitly expresses Subgraph Isomorphism by a polynomial size\nasymmetric linear system. \n\n"}
{"id": "0803.0731", "contents": "Title: Complexity Analysis of Reed-Solomon Decoding over GF(2^m) Without Using\n  Syndromes Abstract: For the majority of the applications of Reed-Solomon (RS) codes, hard\ndecision decoding is based on syndromes. Recently, there has been renewed\ninterest in decoding RS codes without using syndromes. In this paper, we\ninvestigate the complexity of syndromeless decoding for RS codes, and compare\nit to that of syndrome-based decoding. Aiming to provide guidelines to\npractical applications, our complexity analysis differs in several aspects from\nexisting asymptotic complexity analysis, which is typically based on\nmultiplicative fast Fourier transform (FFT) techniques and is usually in big O\nnotation. First, we focus on RS codes over characteristic-2 fields, over which\nsome multiplicative FFT techniques are not applicable. Secondly, due to\nmoderate block lengths of RS codes in practice, our analysis is complete since\nall terms in the complexities are accounted for. Finally, in addition to fast\nimplementation using additive FFT techniques, we also consider direct\nimplementation, which is still relevant for RS codes with moderate lengths.\nComparing the complexities of both syndromeless and syndrome-based decoding\nalgorithms based on direct and fast implementations, we show that syndromeless\ndecoding algorithms have higher complexities than syndrome-based ones for high\nrate RS codes regardless of the implementation. Both errors-only and\nerrors-and-erasures decoding are considered in this paper. We also derive\ntighter bounds on the complexities of fast polynomial multiplications based on\nCantor's approach and the fast extended Euclidean algorithm. \n\n"}
{"id": "0803.2639", "contents": "Title: Maximal Orders in the Design of Dense Space-Time Lattice Codes Abstract: We construct explicit rate-one, full-diversity, geometrically dense matrix\nlattices with large, non-vanishing determinants (NVD) for four transmit antenna\nmultiple-input single-output (MISO) space-time (ST) applications. The\nconstructions are based on the theory of rings of algebraic integers and\nrelated subrings of the Hamiltonian quaternions and can be extended to a larger\nnumber of Tx antennas. The usage of ideals guarantees a non-vanishing\ndeterminant larger than one and an easy way to present the exact proofs for the\nminimum determinants. The idea of finding denser sublattices within a given\ndivision algebra is then generalized to a multiple-input multiple-output (MIMO)\ncase with an arbitrary number of Tx antennas by using the theory of cyclic\ndivision algebras (CDA) and maximal orders. It is also shown that the explicit\nconstructions in this paper all have a simple decoding method based on sphere\ndecoding. Related to the decoding complexity, the notion of sensitivity is\nintroduced, and experimental evidence indicating a connection between\nsensitivity, decoding complexity and performance is provided. Simulations in a\nquasi-static Rayleigh fading channel show that our dense quaternionic\nconstructions outperform both the earlier rectangular lattices and the rotated\nABBA lattice as well as the DAST lattice. We also show that our quaternionic\nlattice is better than the DAST lattice in terms of the diversity-multiplexing\ngain tradeoff. \n\n"}
{"id": "0804.0362", "contents": "Title: Exhaustive enumeration unveils clustering and freezing in random 3-SAT Abstract: We study geometrical properties of the complete set of solutions of the\nrandom 3-satisfiability problem. We show that even for moderate system sizes\nthe number of clusters corresponds surprisingly well with the theoretic\nasymptotic prediction. We locate the freezing transition in the space of\nsolutions which has been conjectured to be relevant in explaining the onset of\ncomputational hardness in random constraint satisfaction problems. \n\n"}
{"id": "0804.2288", "contents": "Title: Parimutuel Betting on Permutations Abstract: We focus on a permutation betting market under parimutuel call auction model\nwhere traders bet on the final ranking of n candidates. We present a\nProportional Betting mechanism for this market. Our mechanism allows the\ntraders to bet on any subset of the n x n 'candidate-rank' pairs, and rewards\nthem proportionally to the number of pairs that appear in the final outcome. We\nshow that market organizer's decision problem for this mechanism can be\nformulated as a convex program of polynomial size. More importantly, the\nformulation yields a set of n x n unique marginal prices that are sufficient to\nprice the bets in this mechanism, and are computable in polynomial-time. The\nmarginal prices reflect the traders' beliefs about the marginal distributions\nover outcomes. We also propose techniques to compute the joint distribution\nover n! permutations from these marginal distributions. We show that using a\nmaximum entropy criterion, we can obtain a concise parametric form (with only n\nx n parameters) for the joint distribution which is defined over an\nexponentially large state space. We then present an approximation algorithm for\ncomputing the parameters of this distribution. In fact, the algorithm addresses\nthe generic problem of finding the maximum entropy distribution over\npermutations that has a given mean, and may be of independent interest. \n\n"}
{"id": "0804.2699", "contents": "Title: A Critique of a Polynomial-time SAT Solver Devised by Sergey Gubin Abstract: This paper refutes the validity of the polynomial-time algorithm for solving\nsatisfiability proposed by Sergey Gubin. Gubin introduces the algorithm using\n3-SAT and eventually expands it to accept a broad range of forms of the Boolean\nsatisfiability problem. Because 3-SAT is NP-complete, the algorithm would have\nimplied P = NP, had it been correct. Additionally, this paper refutes the\ncorrectness of his polynomial-time reduction of SAT to 2-SAT. \n\n"}
{"id": "0804.4666", "contents": "Title: Combining geometry and combinatorics: A unified approach to sparse\n  signal recovery Abstract: There are two main algorithmic approaches to sparse signal recovery:\ngeometric and combinatorial. The geometric approach starts with a geometric\nconstraint on the measurement matrix and then uses linear programming to decode\ninformation about the signal from its measurements. The combinatorial approach\nconstructs the measurement matrix and a combinatorial decoding algorithm to\nmatch. We present a unified approach to these two classes of sparse signal\nrecovery algorithms.\n  The unifying elements are the adjacency matrices of high-quality unbalanced\nexpanders. We generalize the notion of Restricted Isometry Property (RIP),\ncrucial to compressed sensing results for signal recovery, from the Euclidean\nnorm to the l_p norm for p about 1, and then show that unbalanced expanders are\nessentially equivalent to RIP-p matrices.\n  From known deterministic constructions for such matrices, we obtain new\ndeterministic measurement matrix constructions and algorithms for signal\nrecovery which, compared to previous deterministic algorithms, are superior in\neither the number of measurements or in noise tolerance. \n\n"}
{"id": "0805.1401", "contents": "Title: Approximation Algorithms for Shortest Descending Paths in Terrains Abstract: A path from s to t on a polyhedral terrain is descending if the height of a\npoint p never increases while we move p along the path from s to t. No\nefficient algorithm is known to find a shortest descending path (SDP) from s to\nt in a polyhedral terrain. We give two approximation algorithms (more\nprecisely, FPTASs) that solve the SDP problem on general terrains. Both\nalgorithms are simple, robust and easy to implement. \n\n"}
{"id": "0806.2274", "contents": "Title: Exposing Multi-Relational Networks to Single-Relational Network Analysis\n  Algorithms Abstract: Many, if not most network analysis algorithms have been designed specifically\nfor single-relational networks; that is, networks in which all edges are of the\nsame type. For example, edges may either represent \"friendship,\" \"kinship,\" or\n\"collaboration,\" but not all of them together. In contrast, a multi-relational\nnetwork is a network with a heterogeneous set of edge labels which can\nrepresent relationships of various types in a single data structure. While\nmulti-relational networks are more expressive in terms of the variety of\nrelationships they can capture, there is a need for a general framework for\ntransferring the many single-relational network analysis algorithms to the\nmulti-relational domain. It is not sufficient to execute a single-relational\nnetwork analysis algorithm on a multi-relational network by simply ignoring\nedge labels. This article presents an algebra for mapping multi-relational\nnetworks to single-relational networks, thereby exposing them to\nsingle-relational network analysis algorithms. \n\n"}
{"id": "0806.4790", "contents": "Title: AMS Without 4-Wise Independence on Product Domains Abstract: In their seminal work, Alon, Matias, and Szegedy introduced several sketching\ntechniques, including showing that 4-wise independence is sufficient to obtain\ngood approximations of the second frequency moment. In this work, we show that\ntheir sketching technique can be extended to product domains $[n]^k$ by using\nthe product of 4-wise independent functions on $[n]$. Our work extends that of\nIndyk and McGregor, who showed the result for $k = 2$. Their primary motivation\nwas the problem of identifying correlations in data streams. In their model, a\nstream of pairs $(i,j) \\in [n]^2$ arrive, giving a joint distribution $(X,Y)$,\nand they find approximation algorithms for how close the joint distribution is\nto the product of the marginal distributions under various metrics, which\nnaturally corresponds to how close $X$ and $Y$ are to being independent. By\nusing our technique, we obtain a new result for the problem of approximating\nthe $\\ell_2$ distance between the joint distribution and the product of the\nmarginal distributions for $k$-ary vectors, instead of just pairs, in a single\npass. Our analysis gives a randomized algorithm that is a $(1 \\pm \\epsilon)$\napproximation (with probability $1-\\delta$) that requires space logarithmic in\n$n$ and $m$ and proportional to $3^k$. \n\n"}
{"id": "0807.4247", "contents": "Title: Z2Z4-linear codes: rank and kernel Abstract: A code C is Z2Z4-additive if the set of coordinates can be partitioned into\ntwo subsets X and Y such that the punctured code of C by deleting the\ncoordinates outside X (respectively, Y) is a binary linear code (respectively,\na quaternary linear code). In this paper, the rank and dimension of the kernel\nfor Z2Z4-linear codes, which are the corresponding binary codes of\nZ2Z4-additive codes, are studied. The possible values of these two parameters\nfor Z2Z4-linear codes, giving lower and upper bounds, are established. For each\npossible rank r between these bounds, the construction of a Z2Z4-linear code\nwith rank r is given. Equivalently, for each possible dimension of the kernel\nk, the construction of a Z2Z4-linear code with dimension of the kernel k is\ngiven. Finally, the bounds on the rank, once the kernel dimension is fixed, are\nestablished and the construction of a Z2Z4-additive code for each possible pair\n(r,k) is given. \n\n"}
{"id": "0808.1549", "contents": "Title: The Peculiar Phase Structure of Random Graph Bisection Abstract: The mincut graph bisection problem involves partitioning the n vertices of a\ngraph into disjoint subsets, each containing exactly n/2 vertices, while\nminimizing the number of \"cut\" edges with an endpoint in each subset. When\nconsidered over sparse random graphs, the phase structure of the graph\nbisection problem displays certain familiar properties, but also some\nsurprises. It is known that when the mean degree is below the critical value of\n2 log 2, the cutsize is zero with high probability. We study how the minimum\ncutsize increases with mean degree above this critical threshold, finding a new\nanalytical upper bound that improves considerably upon previous bounds.\nCombined with recent results on expander graphs, our bound suggests the unusual\nscenario that random graph bisection is replica symmetric up to and beyond the\ncritical threshold, with a replica symmetry breaking transition possibly taking\nplace above the threshold. An intriguing algorithmic consequence is that\nalthough the problem is NP-hard, we can find near-optimal cutsizes (whose ratio\nto the optimal value approaches 1 asymptotically) in polynomial time for\ntypical instances near the phase transition. \n\n"}
{"id": "0808.1762", "contents": "Title: Communication Complexities of XOR functions Abstract: We call $F:\\{0, 1\\}^n\\times \\{0, 1\\}^n\\to\\{0, 1\\}$ a symmetric XOR function\nif for a function $S:\\{0, 1, ..., n\\}\\to\\{0, 1\\}$, $F(x, y)=S(|x\\oplus y|)$,\nfor any $x, y\\in\\{0, 1\\}^n$, where $|x\\oplus y|$ is the Hamming weight of the\nbit-wise XOR of $x$ and $y$.\n  We show that for any such function, (a) the deterministic communication\ncomplexity is always $\\Theta(n)$ except for four simple functions that have a\nconstant complexity, and (b) up to a polylog factor, the error-bounded\nrandomized and quantum communication complexities are $\\Theta(r_0+r_1)$, where\n$r_0$ and $r_1$ are the minimum integers such that $r_0, r_1\\leq n/2$ and\n$S(k)=S(k+2)$ for all $k\\in[r_0, n-r_1)$. \n\n"}
{"id": "0808.3281", "contents": "Title: On the diagonalization of the discrete Fourier transform Abstract: The discrete Fourier transform (DFT) is an important operator which acts on\nthe Hilbert space of complex valued functions on the ring Z/NZ. In the case\nwhere N=p is an odd prime number, we exhibit a canonical basis of eigenvectors\nfor the DFT. The transition matrix from the standard basis to the canonical\nbasis defines a novel transform which we call the discrete oscillator transform\n(DOT for short). Finally, we describe a fast algorithm for computing the\ndiscrete oscillator transform in certain cases. \n\n"}
{"id": "0809.1687", "contents": "Title: Incoherent dictionaries and the statistical restricted isometry property Abstract: In this article we present a statistical version of the Candes-Tao restricted\nisometry property (SRIP for short) which holds in general for any incoherent\ndictionary which is a disjoint union of orthonormal bases. In addition, under\nappropriate normalization, the eigenvalues of the associated Gram matrix\nfluctuate around 1 according to the Wigner semicircle distribution. The result\nis then applied to various dictionaries that arise naturally in the setting of\nfinite harmonic analysis, giving, in particular, a better understanding on a\nremark of Applebaum-Howard-Searle-Calderbank concerning RIP for the Heisenberg\ndictionary of chirp like functions. \n\n"}
{"id": "0809.3232", "contents": "Title: A Local Clustering Algorithm for Massive Graphs and its Application to\n  Nearly-Linear Time Graph Partitioning Abstract: We study the design of local algorithms for massive graphs. A local algorithm\nis one that finds a solution containing or near a given vertex without looking\nat the whole graph. We present a local clustering algorithm. Our algorithm\nfinds a good cluster--a subset of vertices whose internal connections are\nsignificantly richer than its external connections--near a given vertex. The\nrunning time of our algorithm, when it finds a non-empty local cluster, is\nnearly linear in the size of the cluster it outputs.\n  Our clustering algorithm could be a useful primitive for handling massive\ngraphs, such as social networks and web-graphs. As an application of this\nclustering algorithm, we present a partitioning algorithm that finds an\napproximate sparsest cut with nearly optimal balance. Our algorithm takes time\nnearly linear in the number edges of the graph.\n  Using the partitioning algorithm of this paper, we have designed a\nnearly-linear time algorithm for constructing spectral sparsifiers of graphs,\nwhich we in turn use in a nearly-linear time algorithm for solving linear\nsystems in symmetric, diagonally-dominant matrices. The linear system solver\nalso leads to a nearly linear-time algorithm for approximating the\nsecond-smallest eigenvalue and corresponding eigenvector of the Laplacian\nmatrix of a graph. These other results are presented in two companion papers. \n\n"}
{"id": "0809.4332", "contents": "Title: From one solution of a 3-satisfiability formula to a solution cluster:\n  Frozen variables and entropy Abstract: A solution to a 3-satisfiability (3-SAT) formula can be expanded into a\ncluster, all other solutions of which are reachable from this one through a\nsequence of single-spin flips. Some variables in the solution cluster are\nfrozen to the same spin values by one of two different mechanisms: frozen-core\nformation and long-range frustrations. While frozen cores are identified by a\nlocal whitening algorithm, long-range frustrations are very difficult to trace,\nand they make an entropic belief-propagation (BP) algorithm fail to converge.\nFor BP to reach a fixed point the spin values of a tiny fraction of variables\n(chosen according to the whitening algorithm) are externally fixed during the\niteration. From the calculated entropy values, we infer that, for a large\nrandom 3-SAT formula with constraint density close to the satisfiability\nthreshold, the solutions obtained by the survey-propagation or the walksat\nalgorithm belong neither to the most dominating clusters of the formula nor to\nthe most abundant clusters. This work indicates that a single solution cluster\nof a random 3-SAT formula may have further community structures. \n\n"}
{"id": "0810.2067", "contents": "Title: Divisibility, Smoothness and Cryptographic Applications Abstract: This paper deals with products of moderate-size primes, familiarly known as\nsmooth numbers. Smooth numbers play a crucial role in information theory,\nsignal processing and cryptography.\n  We present various properties of smooth numbers relating to their\nenumeration, distribution and occurrence in various integer sequences. We then\nturn our attention to cryptographic applications in which smooth numbers play a\npivotal role. \n\n"}
{"id": "0810.4946", "contents": "Title: FPT Algorithms and Kernels for the Directed $k$-Leaf Problem Abstract: A subgraph $T$ of a digraph $D$ is an {\\em out-branching} if $T$ is an\noriented spanning tree with only one vertex of in-degree zero (called the {\\em\nroot}). The vertices of $T$ of out-degree zero are {\\em leaves}. In the {\\sc\nDirected $k$-Leaf} Problem, we are given a digraph $D$ and an integral\nparameter $k$, and we are to decide whether $D$ has an out-branching with at\nleast $k$ leaves. Recently, Kneis et al. (2008) obtained an algorithm for the\nproblem of running time $4^{k}\\cdot n^{O(1)}$. We describe a new algorithm for\nthe problem of running time $3.72^{k}\\cdot n^{O(1)}$. In {\\sc Rooted Directed\n$k$-Leaf} Problem, apart from $D$ and $k$, we are given a vertex $r$ of $D$ and\nwe are to decide whether $D$ has an out-branching rooted at $r$ with at least\n$k$ leaves. Very recently, Fernau et al. (2008) found an $O(k^3)$-size kernel\nfor {\\sc Rooted Directed $k$-Leaf}. In this paper, we obtain an $O(k)$ kernel\nfor {\\sc Rooted Directed $k$-Leaf} restricted to acyclic digraphs. \n\n"}
{"id": "0811.4186", "contents": "Title: Search Result Clustering via Randomized Partitioning of Query-Induced\n  Subgraphs Abstract: In this paper, we present an approach to search result clustering, using\npartitioning of underlying link graph. We define the notion of \"query-induced\nsubgraph\" and formulate the problem of search result clustering as a problem of\nefficient partitioning of given subgraph into topic-related clusters. Also, we\npropose a novel algorithm for approximative partitioning of such graph, which\nresults in cluster quality comparable to the one obtained by deterministic\nalgorithms, while operating in more efficient computation time, suitable for\npractical implementations. Finally, we present a practical clustering search\nengine developed as a part of this research and use it to get results about\nreal-world performance of proposed concepts. \n\n"}
{"id": "0812.2602", "contents": "Title: The statistical restricted isometry property and the Wigner semicircle\n  distribution of incoherent dictionaries Abstract: In this article we present a statistical version of the Candes-Tao restricted\nisometry property (SRIP for short) which holds in general for any incoherent\ndictionary which is a disjoint union of orthonormal bases. In addition, we show\nthat, under appropriate normalization, the eigenvalues of the associated Gram\nmatrix fluctuate around 1 according to the Wigner semicircle distribution. The\nresult is then applied to various dictionaries that arise naturally in the\nsetting of finite harmonic analysis, giving, in particular, a better\nunderstanding on a remark of Applebaum-Howard-Searle-Calderbank concerning RIP\nfor the Heisenberg dictionary of chirp like functions. \n\n"}
{"id": "0901.0373", "contents": "Title: Highly Undecidable Problems For Infinite Computations Abstract: We show that many classical decision problems about 1-counter\nomega-languages, context free omega-languages, or infinitary rational\nrelations, are $\\Pi_2^1$-complete, hence located at the second level of the\nanalytical hierarchy, and \"highly undecidable\". In particular, the universality\nproblem, the inclusion problem, the equivalence problem, the determinizability\nproblem, the complementability problem, and the unambiguity problem are all\n$\\Pi_2^1$-complete for context-free omega-languages or for infinitary rational\nrelations. Topological and arithmetical properties of 1-counter\nomega-languages, context free omega-languages, or infinitary rational\nrelations, are also highly undecidable. These very surprising results provide\nthe first examples of highly undecidable problems about the behaviour of very\nsimple finite machines like 1-counter automata or 2-tape automata. \n\n"}
{"id": "0901.2391", "contents": "Title: Weight Distribution of A p-ary Cyclic Code Abstract: For an odd prime $p$ and two positive integers $n\\geq 3$ and $k$ with\n$\\frac{n}{{\\rm gcd}(n,k)}$ being odd, the paper determines the weight\ndistribution of a $p$-ary cyclic code $\\mathcal{C}$ over $\\mathbb{F}_{p}$ with\nnonzeros $\\alpha^{-1}$, $\\alpha^{-(p^k+1)}$ and $\\alpha^{-(p^{3k}+1)}$, where\n$\\alpha$ is a primitive element of $\\mathbb{F}_{p^n}$ \n\n"}
{"id": "0901.3348", "contents": "Title: Nuclear norm minimization for the planted clique and biclique problems Abstract: We consider the problems of finding a maximum clique in a graph and finding a\nmaximum-edge biclique in a bipartite graph. Both problems are NP-hard. We write\nboth problems as matrix-rank minimization and then relax them using the nuclear\nnorm. This technique, which may be regarded as a generalization of compressive\nsensing, has recently been shown to be an effective way to solve rank\noptimization problems. In the special cases that the input graph has a planted\nclique or biclique (i.e., a single large clique or biclique plus diversionary\nedges), our algorithm successfully provides an exact solution to the original\ninstance. For each problem, we provide two analyses of when our algorithm\nsucceeds. In the first analysis, the diversionary edges are placed by an\nadversary. In the second, they are placed at random. In the case of random\nedges for the planted clique problem, we obtain the same bound as Alon,\nKrivelevich and Sudakov as well as Feige and Krauthgamer, but we use different\ntechniques. \n\n"}
{"id": "0901.4129", "contents": "Title: Quasi-Cyclic LDPC Codes: Influence of Proto- and Tanner-Graph Structure\n  on Minimum Hamming Distance Upper Bounds Abstract: Quasi-cyclic (QC) low-density parity-check (LDPC) codes are an important\ninstance of proto-graph-based LDPC codes. In this paper we present upper bounds\non the minimum Hamming distance of QC LDPC codes and study how these upper\nbounds depend on graph structure parameters (like variable degrees, check node\ndegrees, girth) of the Tanner graph and of the underlying proto-graph.\nMoreover, for several classes of proto-graphs we present explicit QC LDPC code\nconstructions that achieve (or come close to) the respective minimum Hamming\ndistance upper bounds. Because of the tight algebraic connection between QC\ncodes and convolutional codes, we can state similar results for the free\nHamming distance of convolutional codes. In fact, some QC code statements are\nestablished by first proving the corresponding convolutional code statements\nand then using a result by Tanner that says that the minimum Hamming distance\nof a QC code is upper bounded by the free Hamming distance of the convolutional\ncode that is obtained by \"unwrapping\" the QC code. \n\n"}
{"id": "0902.0668", "contents": "Title: Application of the Weil representation: diagonalization of the discrete\n  Fourier transform Abstract: We survey a new application of the Weil representation to construct a\ncanonical basis of eigenvectors for the discrete Fourier transform (DFT). The\ntransition matrix from the standard basis to the canonical basis defines a\nnovel transform which we call the discrete oscillator transform (DOT for\nshort). In addition, we describe a fast algorithm for computing the DOT in\ncertain cases. \n\n"}
{"id": "0902.1267", "contents": "Title: A Note on the Diagonalization of the Discrete Fourier Transform Abstract: Following the approach developed by S. Gurevich and R. Hadani, an analytical\nformula of the canonical basis of the DFT is given for the case $N=p$ where $p$\nis a prime number and $p\\equiv 1$ (mod 4). \n\n"}
{"id": "0902.1351", "contents": "Title: On the minimum distance graph of an extended Preparata code Abstract: The minimum distance graph of an extended Preparata code P(m) has vertices\ncorresponding to codewords and edges corresponding to pairs of codewords that\nare distance 6 apart. The clique structure of this graph is investigated and it\nis established that the minimum distance graphs of two extended Preparata codes\nare isomorphic if and only if the codes are equivalent. \n\n"}
{"id": "0902.2537", "contents": "Title: Communication-optimal Parallel and Sequential Cholesky Decomposition Abstract: Numerical algorithms have two kinds of costs: arithmetic and communication,\nby which we mean either moving data between levels of a memory hierarchy (in\nthe sequential case) or over a network connecting processors (in the parallel\ncase). Communication costs often dominate arithmetic costs, so it is of\ninterest to design algorithms minimizing communication. In this paper we first\nextend known lower bounds on the communication cost (both for bandwidth and for\nlatency) of conventional (O(n^3)) matrix multiplication to Cholesky\nfactorization, which is used for solving dense symmetric positive definite\nlinear systems. Second, we compare the costs of various Cholesky decomposition\nimplementations to these lower bounds and identify the algorithms and data\nstructures that attain them. In the sequential case, we consider both the\ntwo-level and hierarchical memory models. Combined with prior results in [13,\n14, 15], this gives a set of communication-optimal algorithms for O(n^3)\nimplementations of the three basic factorizations of dense linear algebra: LU\nwith pivoting, QR and Cholesky. But it goes beyond this prior work on\nsequential LU by optimizing communication for any number of levels of memory\nhierarchy. \n\n"}
{"id": "0902.2795", "contents": "Title: A Graph Reduction Step Preserving Element-Connectivity and Applications Abstract: Given an undirected graph G=(V,E) and subset of terminals T \\subseteq V, the\nelement-connectivity of two terminals u,v \\in T is the maximum number of u-v\npaths that are pairwise disjoint in both edges and non-terminals V \\setminus T\n(the paths need not be disjoint in terminals). Element-connectivity is more\ngeneral than edge-connectivity and less general than vertex-connectivity. Hind\nand Oellermann gave a graph reduction step that preserves the global\nelement-connectivity of the graph. We show that this step also preserves local\nconnectivity, that is, all the pairwise element-connectivities of the\nterminals. We give two applications of this reduction step to connectivity and\nnetwork design problems:\n  1. Given a graph G and disjoint terminal sets T_1, T_2, ..., T_m, we seek a\nmaximum number of element-disjoint Steiner forests where each forest connects\neach T_i. We prove that if each T_i is k-element-connected then there exist\n\\Omega(\\frac{k}{\\log h \\log m}) element-disjoint Steiner forests, where h =\n|\\bigcup_i T_i|. If G is planar (or more generally, has fixed genus), we show\nthat there exist \\Omega(k) Steiner forests. Our proofs are constructive, giving\npoly-time algorithms to find these forests; these are the first non-trivial\nalgorithms for packing element-disjoint Steiner Forests.\n  2. We give a very short and intuitive proof of a spider-decomposition theorem\nof Chuzhoy and Khanna in the context of the single-sink k-vertex-connectivity\nproblem; this yields a simple and alternative analysis of an O(k \\log n)\napproximation.\n  Our results highlight the effectiveness of the element-connectivity reduction\nstep; we believe it will find more applications in the future. \n\n"}
{"id": "0903.3627", "contents": "Title: Statistical RIP and Semi-Circle Distribution of Incoherent Dictionaries Abstract: In this paper we formulate and prove a statistical version of the Candes-Tao\nrestricted isometry property (SRIP for short) which holds in general for any\nincoherent dictionary which is a disjoint union of orthonormal bases. In\naddition, we prove that, under appropriate normalization, the eigenvalues of\nthe associated Gram matrix fluctuate around 1 according to the Wigner\nsemicircle distribution. The result is then applied to various dictionaries\nthat arise naturally in the setting of finite harmonic analysis, giving, in\nparticular, a better understanding on a remark of\nApplebaum-Howard-Searle-Calderbank concerning RIP for the Heisenberg dictionary\nof chirp like functions. \n\n"}
{"id": "0904.1630", "contents": "Title: Self-Assembly of a Statistically Self-Similar Fractal Abstract: We demonstrate existence of a tile assembly system that self-assembles the\nstatistically self-similar Sierpinski Triangle in the Winfree-Rothemund Tile\nAssembly Model. This appears to be the first paper that considers self-assembly\nof a random fractal, instead of a deterministic fractal or a finite, bounded\nshape. Our technical contributions include a way to remember, and use,\nunboundedly-long prefixes of an infinite coding sequence at each stage of\nfractal construction; a tile assembly mechanism for nested recursion; and a\ndefinition of \"almost-everywhere local determinism,\" to describe a tileset\nwhose assembly is locally determined, conditional upon a zeta-dimension zero\nset of (infinitely many) \"input\" tiles. This last is similar to the definition\nof randomized computation for Turing machines, in which an algorithm is\ndeterministic relative to an oracle sequence of coin flips that provides advice\nbut does not itself compute. Keywords: tile self-assembly, statistically\nself-similar Sierpinski Triangle. \n\n"}
{"id": "0904.2022", "contents": "Title: Absdet-Pseudo-Codewords and Perm-Pseudo-Codewords: Definitions and\n  Properties Abstract: The linear-programming decoding performance of a binary linear code crucially\ndepends on the structure of the fundamental cone of the parity-check matrix\nthat describes the code. Towards a better understanding of fundamental cones\nand the vectors therein, we introduce the notion of absdet-pseudo-codewords and\nperm-pseudo-codewords: we give the definitions, we discuss some simple\nexamples, and we list some of their properties. \n\n"}
{"id": "0904.2576", "contents": "Title: PTAS for k-tour cover problem on the plane for moderately large values\n  of k Abstract: Let P be a set of n points in the Euclidean plane and let O be the origin\npoint in the plane. In the k-tour cover problem (called frequently the\ncapacitated vehicle routing problem), the goal is to minimize the total length\nof tours that cover all points in P, such that each tour starts and ends in O\nand covers at most k points from P.\n  The k-tour cover problem is known to be NP-hard. It is also known to admit\nconstant factor approximation algorithms for all values of k and even a\npolynomial-time approximation scheme (PTAS) for small values of k, i.e.,\nk=O(log n / log log n).\n  We significantly enlarge the set of values of k for which a PTAS is provable.\nWe present a new PTAS for all values of k <= 2^{log^{\\delta}n}, where \\delta =\n\\delta(\\epsilon). The main technical result proved in the paper is a novel\nreduction of the k-tour cover problem with a set of n points to a small set of\ninstances of the problem, each with O((k/\\epsilon)^O(1)) points. \n\n"}
{"id": "0904.3251", "contents": "Title: On evaluation of permanents Abstract: We study the time and space complexity of matrix permanents over rings and\nsemirings. \n\n"}
{"id": "0905.2485", "contents": "Title: Minimizing Communication in Linear Algebra Abstract: In 1981 Hong and Kung proved a lower bound on the amount of communication\nneeded to perform dense, matrix-multiplication using the conventional $O(n^3)$\nalgorithm, where the input matrices were too large to fit in the small, fast\nmemory. In 2004 Irony, Toledo and Tiskin gave a new proof of this result and\nextended it to the parallel case. In both cases the lower bound may be\nexpressed as $\\Omega$(#arithmetic operations / $\\sqrt{M}$), where M is the size\nof the fast memory (or local memory in the parallel case). Here we generalize\nthese results to a much wider variety of algorithms, including LU\nfactorization, Cholesky factorization, $LDL^T$ factorization, QR factorization,\nalgorithms for eigenvalues and singular values, i.e., essentially all direct\nmethods of linear algebra. The proof works for dense or sparse matrices, and\nfor sequential or parallel algorithms. In addition to lower bounds on the\namount of data moved (bandwidth) we get lower bounds on the number of messages\nrequired to move it (latency). We illustrate how to extend our lower bound\ntechnique to compositions of linear algebra operations (like computing powers\nof a matrix), to decide whether it is enough to call a sequence of simpler\noptimal algorithms (like matrix multiplication) to minimize communication, or\nif we can do better. We give examples of both. We also show how to extend our\nlower bounds to certain graph theoretic problems.\n  We point out recently designed algorithms for dense LU, Cholesky, QR,\neigenvalue and the SVD problems that attain these lower bounds; implementations\nof LU and QR show large speedups over conventional linear algebra algorithms in\nstandard libraries like LAPACK and ScaLAPACK. Many open problems remain. \n\n"}
{"id": "0907.1297", "contents": "Title: Bounds on the quantum satisfiability threshold Abstract: Quantum k-SAT is the problem of deciding whether there is a n-qubit state\nwhich is perpendicular to a set of vectors, each of which lies in the Hilbert\nspace of k qubits. Equivalently, the problem is to decide whether a particular\ntype of local Hamiltonian has a ground state with zero energy. We consider\nrandom quantum k-SAT formulas with n variables and m = \\alpha n clauses, and\nask at what value of \\alpha these formulas cease to be satisfiable. We show\nthat the threshold for random quantum 3-SAT is at most 3.594. For comparison,\nconvincing arguments from statistical physics suggest that the classical 3-SAT\nthreshold is \\alpha \\approx 4.267. For larger k, we show that the quantum\nthreshold is a constant factor smaller than the classical one. Our bounds work\nby determining the generic rank of the satisfying subspace for certain gadgets,\nand then using the technique of differential equations to analyze various\nalgorithms that partition the hypergraph into a collection of these gadgets.\nOur use of differential equation to establish upper bounds on a satisfiability\nthreshold appears to be novel, and our techniques may apply to various\nclassical problems as well. \n\n"}
{"id": "0907.3583", "contents": "Title: Web of Lossy Adapters for Interface Interoperability: An Algorithm and\n  NP-completeness of Minimization Abstract: By using different interface adapters for different methods, it is possible\nto construct a maximally covering web of interface adapters which incurs\nminimum loss during interface adaptation. We introduce a polynomial-time\nalgorithm that can achieve this. However, we also show that minimizing the\nnumber of adapters included in a maximally covering web of interface adapters\nis an NP-complete problem. \n\n"}
{"id": "0907.4283", "contents": "Title: Domination Problems in Nowhere-Dense Classes of Graphs Abstract: We investigate the parameterized complexity of generalisations and variations\nof the dominating set problem on classes of graphs that are nowhere dense. In\nparticular, we show that the distance-d dominating-set problem, also known as\nthe (k,d)-centres problem, is fixed-parameter tractable on any class that is\nnowhere dense and closed under induced subgraphs. This generalises known\nresults about the dominating set problem on H-minor free classes, classes with\nlocally excluded minors and classes of graphs of bounded expansion. A key\nfeature of our proof is that it is based simply on the fact that these graph\nclasses are uniformly quasi-wide, and does not rely on a structural\ndecomposition. Our result also establishes that the distance-d dominating-set\nproblem is FPT on classes of bounded expansion, answering a question of Ne{\\v\ns}et{\\v{r}}il and Ossona de Mendez. \n\n"}
{"id": "0908.2940", "contents": "Title: A Strong Direct Product Theorem for Disjointness Abstract: A strong direct product theorem states that if we want to compute $k$\nindependent instances of a function, using less than $k$ times the resources\nneeded for one instance, then the overall success probability will be\nexponentially small in $k$. We establish such a theorem for the randomized\ncommunication complexity of the Disjointness problem, i.e., with communication\n$const\\cdot kn$ the success probability of solving $k$ instances of size $n$\ncan only be exponentially small in $k$. We show that this bound even holds for\n$AM$ communication protocols with limited ambiguity. This also implies a new\nlower bound for Disjointness in a restricted 3-player NOF protocol, and optimal\ncommunication-space tradeoffs for Boolean matrix product. Our main result\nfollows from a solution to the dual of a linear programming problem, whose\nfeasibility comes from a so-called Intersection Sampling Lemma that generalizes\na result by Razborov. \n\n"}
{"id": "0908.4499", "contents": "Title: Monadic second-order model-checking on decomposable matroids Abstract: A notion of branch-width, which generalizes the one known for graphs, can be\ndefined for matroids. We first give a proof of the polynomial time\nmodel-checking of monadic second-order formulas on representable matroids of\nbounded branch-width, by reduction to monadic second-order formulas on trees.\nThis proof is much simpler than the one previously known. We also provide a\nlink between our logical approach and a grammar that allows to build matroids\nof bounded branch-width. Finally, we introduce a new class of non-necessarily\nrepresentable matroids, described by a grammar and on which monadic\nsecond-order formulas can be checked in linear time. \n\n"}
{"id": "0909.3005", "contents": "Title: A simple encoding of a quantum circuit amplitude as a matrix permanent Abstract: A simple construction is presented which allows computing the transition\namplitude of a quantum circuit to be encoded as computing the permanent of a\nmatrix which is of size proportional to the number of quantum gates in the\ncircuit. This opens up some interesting classical monte-carlo algorithms for\napproximating quantum circuits. \n\n"}
{"id": "0909.3508", "contents": "Title: Compressed Sensing with Probabilistic Measurements: A Group Testing\n  Solution Abstract: Detection of defective members of large populations has been widely studied\nin the statistics community under the name \"group testing\", a problem which\ndates back to World War II when it was suggested for syphilis screening. There\nthe main interest is to identify a small number of infected people among a\nlarge population using collective samples. In viral epidemics, one way to\nacquire collective samples is by sending agents inside the population. While in\nclassical group testing, it is assumed that the sampling procedure is fully\nknown to the reconstruction algorithm, in this work we assume that the decoder\npossesses only partial knowledge about the sampling process. This assumption is\njustified by observing the fact that in a viral sickness, there is a chance\nthat an agent remains healthy despite having contact with an infected person.\nTherefore, the reconstruction method has to cope with two different types of\nuncertainty; namely, identification of the infected population and the\npartially unknown sampling procedure.\n  In this work, by using a natural probabilistic model for \"viral infections\",\nwe design non-adaptive sampling procedures that allow successful identification\nof the infected population with overwhelming probability 1-o(1). We propose\nboth probabilistic and explicit design procedures that require a \"small\" number\nof agents to single out the infected individuals. More precisely, for a\ncontamination probability p, the number of agents required by the probabilistic\nand explicit designs for identification of up to k infected members is bounded\nby m = O(k^2 (log n)/p^2) and m = O(k^2 (log n)^2 /p^2), respectively. In both\ncases, a simple decoder is able to successfully identify the infected\npopulation in time O(mn). \n\n"}
{"id": "0909.4766", "contents": "Title: Quantum Adiabatic Algorithms, Small Gaps, and Different Paths Abstract: We construct a set of instances of 3SAT which are not solved efficiently\nusing the simplest quantum adiabatic algorithm. These instances are obtained by\npicking random clauses all consistent with two disparate planted solutions and\nthen penalizing one of them with a single additional clause. We argue that by\nrandomly modifying the beginning Hamiltonian, one obtains (with substantial\nprobability) an adiabatic path that removes this difficulty. This suggests that\nthe quantum adiabatic algorithm should in general be run on each instance with\nmany different random paths leading to the problem Hamiltonian. We do not know\nwhether this trick will help for a random instance of 3SAT (as opposed to an\ninstance from the particular set we consider), especially if the instance has\nan exponential number of disparate assignments that violate few clauses. We use\na continuous imaginary time Quantum Monte Carlo algorithm in a novel way to\nnumerically investigate the ground state as well as the first excited state of\nour system. Our arguments are supplemented by Quantum Monte Carlo data from\nsimulations with up to 150 spins. \n\n"}
{"id": "0909.4969", "contents": "Title: MACH: Fast Randomized Tensor Decompositions Abstract: Tensors naturally model many real world processes which generate multi-aspect\ndata. Such processes appear in many different research disciplines, e.g,\nchemometrics, computer vision, psychometrics and neuroimaging analysis. Tensor\ndecompositions such as the Tucker decomposition are used to analyze\nmulti-aspect data and extract latent factors, which capture the multilinear\ndata structure. Such decompositions are powerful mining tools, for extracting\npatterns from large data volumes. However, most frequently used algorithms for\nsuch decompositions involve the computationally expensive Singular Value\nDecomposition.\n  In this paper we propose MACH, a new sampling algorithm to compute such\ndecompositions. Our method is of significant practical value for tensor\nstreams, such as environmental monitoring systems, IP traffic matrices over\ntime, where large amounts of data are accumulated and the analysis is\ncomputationally intensive but also in \"post-mortem\" data analysis cases where\nthe tensor does not fit in the available memory. We provide the theoretical\nanalysis of our proposed method, and verify its efficacy in monitoring system\napplications. \n\n"}
{"id": "0909.5097", "contents": "Title: On the Scope of the Universal-Algebraic Approach to Constraint\n  Satisfaction Abstract: The universal-algebraic approach has proved a powerful tool in the study of\nthe complexity of CSPs. This approach has previously been applied to the study\nof CSPs with finite or (infinite) omega-categorical templates, and relies on\ntwo facts. The first is that in finite or omega-categorical structures A, a\nrelation is primitive positive definable if and only if it is preserved by the\npolymorphisms of A. The second is that every finite or omega-categorical\nstructure is homomorphically equivalent to a core structure. In this paper, we\npresent generalizations of these facts to infinite structures that are not\nnecessarily omega-categorical. (This abstract has been severely curtailed by\nthe space constraints of arXiv -- please read the full abstract in the\narticle.) Finally, we present applications of our general results to the\ndescription and analysis of the complexity of CSPs. In particular, we give\ngeneral hardness criteria based on the absence of polymorphisms that depend on\nmore than one argument, and we present a polymorphism-based description of\nthose CSPs that are first-order definable (and therefore can be solved in\npolynomial time). \n\n"}
{"id": "0910.2370", "contents": "Title: On the hardness of the noncommutative determinant Abstract: In this paper we study the computational complexity of computing the\nnoncommutative determinant. We first consider the arithmetic circuit complexity\nof computing the noncommutative determinant polynomial. Then, more generally,\nwe also examine the complexity of computing the determinant (as a function)\nover noncommutative domains. Our hardness results are summarized below:\n  1. We show that if the noncommutative determinant polynomial has small\nnoncommutative arithmetic circuits then so does the noncommutative permanent.\nConsequently, the commutative permanent polynomial has small commutative\narithmetic circuits. 2. For any field F we show that computing the n X n\npermanent over F is polynomial-time reducible to computing the 2n X 2n\n(noncommutative) determinant whose entries are O(n^2) X O(n^2) matrices over\nthe field F. 3. We also derive as a consequence that computing the n X n\npermanent over nonnegative rationals is polynomial-time reducible to computing\nthe noncommutative determinant over Clifford algebras of n^{O(1)} dimension.\n  Our techniques are elementary and use primarily the notion of the Hadamard\nProduct of noncommutative polynomials. \n\n"}
{"id": "0910.2443", "contents": "Title: P versus NP and geometry Abstract: I describe three geometric approaches to resolving variants of P v. NP,\npresent several results that illustrate the role of group actions in complexity\ntheory, and make a first step towards completely geometric definitions of\ncomplexity classes. \n\n"}
{"id": "0910.3376", "contents": "Title: Quantum Proofs for Classical Theorems Abstract: Alongside the development of quantum algorithms and quantum complexity theory\nin recent years, quantum techniques have also proved instrumental in obtaining\nresults in classical (non-quantum) areas. In this paper we survey these results\nand the quantum toolbox they use. \n\n"}
{"id": "0911.0996", "contents": "Title: The Need for Structure in Quantum Speedups Abstract: Is there a general theorem that tells us when we can hope for exponential\nspeedups from quantum algorithms, and when we cannot? In this paper, we make\ntwo advances toward such a theorem, in the black-box model where most quantum\nalgorithms operate.\n  First, we show that for any problem that is invariant under permuting inputs\nand outputs (like the collision or the element distinctness problems), the\nquantum query complexity is at least the 7th root of the classical randomized\nquery complexity. (An earlier version of this paper gave the 9th root.) This\nresolves a conjecture of Watrous from 2002.\n  Second, inspired by recent work of O'Donnell et al. (2005) and Dinur et al.\n(2006), we conjecture that every bounded low-degree polynomial has a \"highly\ninfluential\" variable. Assuming this conjecture, we show that every T-query\nquantum algorithm can be simulated on most inputs by a poly(T)-query classical\nalgorithm, and that one essentially cannot hope to prove P!=BQP relative to a\nrandom oracle. \n\n"}
{"id": "0911.2317", "contents": "Title: Algorithms for Quantum Branching Programs Based on Fingerprinting Abstract: In the paper we develop a method for constructing quantum algorithms for\ncomputing Boolean functions by quantum ordered read-once branching programs\n(quantum OBDDs). Our method is based on fingerprinting technique and\nrepresentation of Boolean functions by their characteristic polynomials. We use\ncircuit notation for branching programs for desired algorithms presentation.\nFor several known functions our approach provides optimal QOBDDs. Namely we\nconsider such functions as Equality, Palindrome, and Permutation Matrix Test.\nWe also propose a generalization of our method and apply it to the Boolean\nvariant of the Hidden Subgroup Problem. \n\n"}
{"id": "0911.3195", "contents": "Title: Efficient Distributed Random Walks with Applications Abstract: We focus on the problem of performing random walks efficiently in a\ndistributed network. Given bandwidth constraints, the goal is to minimize the\nnumber of rounds required to obtain a random walk sample. We first present a\nfast sublinear time distributed algorithm for performing random walks whose\ntime complexity is sublinear in the length of the walk. Our algorithm performs\na random walk of length $\\ell$ in $\\tilde{O}(\\sqrt{\\ell D})$ rounds (with high\nprobability) on an undirected network, where $D$ is the diameter of the\nnetwork. This improves over the previous best algorithm that ran in\n$\\tilde{O}(\\ell^{2/3}D^{1/3})$ rounds (Das Sarma et al., PODC 2009). We further\nextend our algorithms to efficiently perform $k$ independent random walks in\n$\\tilde{O}(\\sqrt{k\\ell D} + k)$ rounds. We then show that there is a\nfundamental difficulty in improving the dependence on $\\ell$ any further by\nproving a lower bound of $\\Omega(\\sqrt{\\frac{\\ell}{\\log \\ell}} + D)$ under a\ngeneral model of distributed random walk algorithms. Our random walk algorithms\nare useful in speeding up distributed algorithms for a variety of applications\nthat use random walks as a subroutine. We present two main applications. First,\nwe give a fast distributed algorithm for computing a random spanning tree (RST)\nin an arbitrary (undirected) network which runs in $\\tilde{O}(\\sqrt{m}D)$\nrounds (with high probability; here $m$ is the number of edges). Our second\napplication is a fast decentralized algorithm for estimating mixing time and\nrelated parameters of the underlying network. Our algorithm is fully\ndecentralized and can serve as a building block in the design of\ntopologically-aware networks. \n\n"}
{"id": "0911.3438", "contents": "Title: First-order transitions and the performance of quantum algorithms in\n  random optimization problems Abstract: We present a study of the phase diagram of a random optimization problem in\npresence of quantum fluctuations. Our main result is the characterization of\nthe nature of the phase transition, which we find to be a first-order quantum\nphase transition. We provide evidence that the gap vanishes exponentially with\nthe system size at the transition. This indicates that the Quantum Adiabatic\nAlgorithm requires a time growing exponentially with system size to find the\nground state of this problem. \n\n"}
{"id": "0911.5487", "contents": "Title: Strong Spatial Mixing for Binary Markov Random Fields Abstract: Gibbs distribution of binary Markov random fields on a sparse on average\ngraph is considered in this paper. The strong spatial mixing is proved under\nthe condition that the `external field' is uniformly large or small. Such\ncondition on `external field' is meaningful in physics. \n\n"}
{"id": "0912.0850", "contents": "Title: Grammar-Based Compression in a Streaming Model Abstract: We show that, given a string $s$ of length $n$, with constant memory and\nlogarithmic passes over a constant number of streams we can build a\ncontext-free grammar that generates $s$ and only $s$ and whose size is within\nan $\\Oh{\\min (g \\log g, \\sqrt{n \\log g})}$-factor of the minimum $g$. This\nstands in contrast to our previous result that, with polylogarithmic memory and\npolylogarithmic passes over a single stream, we cannot build such a grammar\nwhose size is within any polynomial of $g$. \n\n"}
{"id": "0912.1412", "contents": "Title: Topological Properties of an Exponential Random Geometric Graph Process Abstract: In this paper, we consider a one-dimensional random geometric graph process\nwith the inter-nodal gaps evolving according to an exponential AR(1) process,\nwhich may serve as a mobile wireless network model. The transition probability\nmatrix and stationary distribution are derived for the Markov chains in terms\nof network connectivity and the number of components. We characterize an\nalgorithm for the hitting time regarding disconnectivity. In addition, we also\nstudy topological properties for static snapshots. We obtain the degree\ndistributions as well as asymptotic precise bounds and strong law of large\nnumbers for connectivity threshold distance and the largest nearest neighbor\ndistance amongst others. Both closed form results and limit theorems are\nprovided. \n\n"}
{"id": "0912.5009", "contents": "Title: The MacWilliams Theorem for Four-Dimensional Modulo Metrics Abstract: In this paper, the MacWilliams theorem is stated for codes over finite field\nwith four-dimensional modulo metrics. \n\n"}
{"id": "1001.0018", "contents": "Title: Nonadaptive quantum query complexity Abstract: We study the power of nonadaptive quantum query algorithms, which are\nalgorithms whose queries to the input do not depend on the result of previous\nqueries. First, we show that any bounded-error nonadaptive quantum query\nalgorithm that computes some total boolean function depending on n variables\nmust make Omega(n) queries to the input in total. Second, we show that, if\nthere exists a quantum algorithm that uses k nonadaptive oracle queries to\nlearn which one of a set of m boolean functions it has been given, there exists\na nonadaptive classical algorithm using O(k log m) queries to solve the same\nproblem. Thus, in the nonadaptive setting, quantum algorithms can achieve at\nmost a very limited speed-up over classical query algorithms. \n\n"}
{"id": "1001.1139", "contents": "Title: Spatial search in a honeycomb network Abstract: The spatial search problem consists in minimizing the number of steps\nrequired to find a given site in a network, under the restriction that only\noracle queries or translations to neighboring sites are allowed. In this paper,\na quantum algorithm for the spatial search problem on a honeycomb lattice with\n$N$ sites and torus-like boundary conditions. The search algorithm is based on\na modified quantum walk on a hexagonal lattice and the general framework\nproposed by Ambainis, Kempe and Rivosh is used to show that the time complexity\nof this quantum search algorithm is $O(\\sqrt{N \\log N})$. \n\n"}
{"id": "1001.2951", "contents": "Title: Solution space heterogeneity of the random K-satisfiability problem:\n  Theory and simulations Abstract: The random K-satisfiability (K-SAT) problem is an important problem for\nstudying typical-case complexity of NP-complete combinatorial satisfaction; it\nis also a representative model of finite-connectivity spin-glasses. In this\npaper we review our recent efforts on the solution space fine structures of the\nrandom K-SAT problem. A heterogeneity transition is predicted to occur in the\nsolution space as the constraint density alpha reaches a critical value\nalpha_cm. This transition marks the emergency of exponentially many solution\ncommunities in the solution space. After the heterogeneity transition the\nsolution space is still ergodic until alpha reaches a larger threshold value\nalpha_d, at which the solution communities disconnect from each other to become\ndifferent solution clusters (ergodicity-breaking). The existence of solution\ncommunities in the solution space is confirmed by numerical simulations of\nsolution space random walking, and the effect of solution space heterogeneity\non a stochastic local search algorithm SEQSAT, which performs a random walk of\nsingle-spin flips, is investigated. The relevance of this work to glassy\ndynamics studies is briefly mentioned. \n\n"}
{"id": "1002.3864", "contents": "Title: Limits of Approximation Algorithms: PCPs and Unique Games (DIMACS\n  Tutorial Lecture Notes) Abstract: These are the lecture notes for the DIMACS Tutorial \"Limits of Approximation\nAlgorithms: PCPs and Unique Games\" held at the DIMACS Center, CoRE Building,\nRutgers University on 20-21 July, 2009. This tutorial was jointly sponsored by\nthe DIMACS Special Focus on Hardness of Approximation, the DIMACS Special Focus\non Algorithmic Foundations of the Internet, and the Center for Computational\nIntractability with support from the National Security Agency and the National\nScience Foundation.\n  The speakers at the tutorial were Matthew Andrews, Sanjeev Arora, Moses\nCharikar, Prahladh Harsha, Subhash Khot, Dana Moshkovitz and Lisa Zhang. The\nsribes were Ashkan Aazami, Dev Desai, Igor Gorodezky, Geetha Jagannathan,\nAlexander S. Kulikov, Darakhshan J. Mir, Alantha Newman, Aleksandar Nikolov,\nDavid Pritchard and Gwen Spencer. \n\n"}
{"id": "1002.4464", "contents": "Title: Deterministic Sample Sort For GPUs Abstract: We present and evaluate GPU Bucket Sort, a parallel deterministic sample sort\nalgorithm for many-core GPUs. Our method is considerably faster than Thrust\nMerge (Satish et.al., Proc. IPDPS 2009), the best comparison-based sorting\nalgorithm for GPUs, and it is as fast as the new randomized sample sort for\nGPUs by Leischner et.al. (to appear in Proc. IPDPS 2010). Our deterministic\nsample sort has the advantage that bucket sizes are guaranteed and therefore\nits running time does not have the input data dependent fluctuations that can\noccur for randomized sample sort. \n\n"}
{"id": "1002.4577", "contents": "Title: Bounded Rationality, Strategy Simplification, and Equilibrium Abstract: It is frequently suggested that predictions made by game theory could be\nimproved by considering computational restrictions when modeling agents. Under\nthe supposition that players in a game may desire to balance maximization of\npayoff with minimization of strategy complexity, Rubinstein and co-authors\nstudied forms of Nash equilibrium where strategies are maximally simplified in\nthat no strategy can be further simplified without sacrificing payoff. Inspired\nby this line of work, we introduce a notion of equilibrium whereby strategies\nare also maximally simplified, but with respect to a simplification procedure\nthat is more careful in that a player will not simplify if the simplification\nincents other players to deviate. We study such equilibria in two-player\nmachine games in which players choose finite automata that succinctly represent\nstrategies for repeated games; in this context, we present techniques for\nestablishing that an outcome is at equilibrium and present results on the\nstructure of equilibria. \n\n"}
{"id": "1003.3508", "contents": "Title: Independent Sets from an Algebraic Perspective Abstract: In this paper, we study the basic problem of counting independent sets in a\ngraph and, in particular, the problem of counting antichains in a finite poset,\nfrom an algebraic perspective. We show that neither independence polynomials of\nbipartite Cohen-Macaulay graphs nor Hilbert series of initial ideals of radical\nzero-dimensional complete intersections ideals, can be evaluated in polynomial\ntime, unless #P=P. Moreover, we present a family of radical zero-dimensional\ncomplete intersection ideals J_P associated to a finite poset P, for which we\ndescribe a universal Gr\\\"obner basis. This implies that the bottleneck in\ncomputing the dimension of the quotient by J_P (that is, the number of zeros of\nJ_P) using Gr\\\"obner methods lies in the description of the standard monomials. \n\n"}
{"id": "1003.4847", "contents": "Title: A tree-decomposed transfer matrix for computing exact Potts model\n  partition functions for arbitrary graphs, with applications to planar graph\n  colourings Abstract: Combining tree decomposition and transfer matrix techniques provides a very\ngeneral algorithm for computing exact partition functions of statistical models\ndefined on arbitrary graphs. The algorithm is particularly efficient in the\ncase of planar graphs. We illustrate it by computing the Potts model partition\nfunctions and chromatic polynomials (the number of proper vertex colourings\nusing Q colours) for large samples of random planar graphs with up to N=100\nvertices. In the latter case, our algorithm yields a sub-exponential average\nrunning time of ~ exp(1.516 sqrt(N)), a substantial improvement over the\nexponential running time ~ exp(0.245 N) provided by the hitherto best known\nalgorithm. We study the statistics of chromatic roots of random planar graphs\nin some detail, comparing the findings with results for finite pieces of a\nregular lattice. \n\n"}
{"id": "1004.0992", "contents": "Title: The Complexity of Partition Functions on Hermitian Matrices Abstract: Partition functions of certain classes of \"spin glass\" models in statistical\nphysics show strong connections to combinatorial graph invariants. Also known\nas homomorphism functions they allow for the representation of many such\ninvariants, for example, the number of independent sets of a graph or the\nnumber nowhere zero k-flows. Contributing to recent developments on the\ncomplexity of partition functions we study the complexity of partition\nfunctions with complex values. These functions are usually determined by a\nsquare matrix A and it was shown by Goldberg, Grohe, Jerrum, and Thurley that\nfor each real-valued symmetric matrix, the corresponding partition function is\neither polynomial time computable or #P-hard. Extending this result, we give a\ncomplete description of the complexity of partition functions definable by\nHermitian matrices. These can also be classified into polynomial time\ncomputable and #P-hard ones. Although the criterion for polynomial time\ncomputability is not describable in a single line, we give a clear account of\nit in terms of structures associated with Abelian groups. \n\n"}
{"id": "1004.0995", "contents": "Title: Strong Fault-Tolerance for Self-Assembly with Fuzzy Temperature Abstract: We consider the problem of fault-tolerance in nanoscale algorithmic\nself-assembly. We employ a variant of Winfree's abstract Tile Assembly Model\n(aTAM), the two-handed aTAM, in which square \"tiles\" -- a model of molecules\nconstructed from DNA for the purpose of engineering self-assembled\nnanostructures -- aggregate according to specific binding sites of varying\nstrengths, and in which large aggregations of tiles may attach to each other,\nin contrast to the seeded aTAM, in which tiles aggregate one at a time to a\nsingle specially-designated \"seed\" assembly. We focus on a major cause of\nerrors in tile-based self-assembly: that of unintended growth due to \"weak\"\nstrength-1 bonds, which if allowed to persist, may be stabilized by subsequent\nattachment of neighboring tiles in the sense that at least energy 2 is now\nrequired to break apart the resulting assembly; i.e., the errant assembly is\nstable at temperature 2. We study a common self-assembly benchmark problem,\nthat of assembling an n x n square using O(log n) unique tile types, under the\ntwo-handed model of self-assembly. Our main result achieves a much stronger\nnotion of fault-tolerance than those achieved previously. Arbitrary strength-1\ngrowth is allowed (i.e., the temperature is \"fuzzy\" and may drift from 2 to 1\nfor arbitrarily long); however, any assembly that grows sufficiently to become\nstable at temperature 2 is guaranteed to assemble at temperature 2 into the\ncorrect final assembly of an n x n square. In other words, errors due to\ninsufficient attachment, which is the cause of errors studied in earlier papers\non fault-tolerance, are prevented absolutely in our main construction, rather\nthan only with high probability and for sufficiently small structures, as in\nprevious fault-tolerance studies. \n\n"}
{"id": "1004.3169", "contents": "Title: Factorizations of Cunningham numbers with bases 13 to 99 Abstract: This Report updates the tables of factorizations of a^n +- 1 for 13 < a <\n100, previously published as CWI Report NM-R9212 (June 1992) and updated in CWI\nReport NM-R9419 (Update 1, September 1994) and CWI Report NM-R9609 (Update 2,\nMarch 1996). A total of 951 new entries in the tables are given here. The\nfactorizations are now complete for n < 76, and there are no composite\ncofactors smaller than 10^102. \n\n"}
{"id": "1004.4383", "contents": "Title: Self-Assembly of Arbitrary Shapes Using RNAse Enzymes: Meeting the\n  Kolmogorov Bound with Small Scale Factor (extended abstract) Abstract: We consider a model of algorithmic self-assembly of geometric shapes out of\nsquare Wang tiles studied in SODA 2010, in which there are two types of tiles\n(e.g., constructed out of DNA and RNA material) and one operation that destroys\nall tiles of a particular type (e.g., an RNAse enzyme destroys all RNA tiles).\nWe show that a single use of this destruction operation enables much more\nefficient construction of arbitrary shapes. In particular, an arbitrary shape\ncan be constructed using an asymptotically optimal number of distinct tile\ntypes (related to the shape's Kolmogorov complexity), after scaling the shape\nby only a logarithmic factor. By contrast, without the destruction operation,\nthe best such result has a scale factor at least linear in the size of the\nshape, and is connected only by a spanning tree of the scaled tiles. We also\ncharacterize a large collection of shapes that can be constructed efficiently\nwithout any scaling. \n\n"}
{"id": "1004.4802", "contents": "Title: Hypersurfaces with degenerate duals and the Geometric Complexity Theory\n  Program Abstract: We determine set-theoretic defining equations for the variety of\nhypersurfaces of degree d in an N-dimensional complex vector space that have\ndual variety of dimension at most k. We apply these equations to the\nMulmuley-Sohoni variety, the GL_{n^2} orbit closure of the determinant, showing\nit is an irreducible component of the variety of hypersurfaces of degree $n$ in\nC^{n^2} with dual of dimension at most 2n-2. We establish additional geometric\nproperties of the Mulmuley-Sohoni variety and prove a quadratic lower bound for\nthe determinental border-complexity of the permanent. \n\n"}
{"id": "1005.2724", "contents": "Title: Low Rank Matrix-Valued Chernoff Bounds and Approximate Matrix\n  Multiplication Abstract: In this paper we develop algorithms for approximating matrix multiplication\nwith respect to the spectral norm. Let A\\in{\\RR^{n\\times m}} and B\\in\\RR^{n\n\\times p} be two matrices and \\eps>0. We approximate the product A^\\top B using\ntwo down-sampled sketches, \\tilde{A}\\in\\RR^{t\\times m} and\n\\tilde{B}\\in\\RR^{t\\times p}, where t\\ll n such that \\norm{\\tilde{A}^\\top\n\\tilde{B} - A^\\top B} \\leq \\eps \\norm{A}\\norm{B} with high probability. We use\ntwo different sampling procedures for constructing \\tilde{A} and \\tilde{B}; one\nof them is done by i.i.d. non-uniform sampling rows from A and B and the other\nis done by taking random linear combinations of their rows. We prove bounds\nthat depend only on the intrinsic dimensionality of A and B, that is their rank\nand their stable rank; namely the squared ratio between their Frobenius and\noperator norm. For achieving bounds that depend on rank we employ standard\ntools from high-dimensional geometry such as concentration of measure arguments\ncombined with elaborate \\eps-net constructions. For bounds that depend on the\nsmaller parameter of stable rank this technology itself seems weak. However, we\nshow that in combination with a simple truncation argument is amenable to\nprovide such bounds. To handle similar bounds for row sampling, we develop a\nnovel matrix-valued Chernoff bound inequality which we call low rank\nmatrix-valued Chernoff bound. Thanks to this inequality, we are able to give\nbounds that depend only on the stable rank of the input matrices... \n\n"}
{"id": "1005.3729", "contents": "Title: Compressive Sensing over the Grassmann Manifold: a Unified Geometric\n  Framework Abstract: $\\ell_1$ minimization is often used for finding the sparse solutions of an\nunder-determined linear system. In this paper we focus on finding sharp\nperformance bounds on recovering approximately sparse signals using $\\ell_1$\nminimization, possibly under noisy measurements. While the restricted isometry\nproperty is powerful for the analysis of recovering approximately sparse\nsignals with noisy measurements, the known bounds on the achievable sparsity\n(The \"sparsity\" in this paper means the size of the set of nonzero or\nsignificant elements in a signal vector.) level can be quite loose. The\nneighborly polytope analysis which yields sharp bounds for ideally sparse\nsignals cannot be readily generalized to approximately sparse signals. Starting\nfrom a necessary and sufficient condition, the \"balancedness\" property of\nlinear subspaces, for achieving a certain signal recovery accuracy, we give a\nunified \\emph{null space Grassmann angle}-based geometric framework for\nanalyzing the performance of $\\ell_1$ minimization. By investigating the\n\"balancedness\" property, this unified framework characterizes sharp\nquantitative tradeoffs between the considered sparsity and the recovery\naccuracy of the $\\ell_{1}$ optimization. As a consequence, this generalizes the\nneighborly polytope result for ideally sparse signals. Besides the robustness\nin the \"strong\" sense for \\emph{all} sparse signals, we also discuss the\nnotions of \"weak\" and \"sectional\" robustness. Our results concern fundamental\nproperties of linear subspaces and so may be of independent mathematical\ninterest. \n\n"}
{"id": "1005.3730", "contents": "Title: Obtaining the Quantum Fourier Transform from the Classical FFT with QR\n  Decomposition Abstract: We present the detailed process of converting the classical Fourier Transform\nalgorithm into the quantum one by using QR decomposition. This provides an\nexample of a technique for building quantum algorithms using classical ones.\nThe Quantum Fourier Transform is one of the most important quantum subroutines\nknown at present, used in most algorithms that have exponential speed up\ncompared to the classical ones. We briefly review Fast Fourier Transform and\nthen make explicit all the steps that led to the quantum formulation of the\nalgorithm, generalizing Coppersmith's work. \n\n"}
{"id": "1006.1423", "contents": "Title: Quantum algorithms for testing Boolean functions Abstract: We discuss quantum algorithms, based on the Bernstein-Vazirani algorithm, for\nfinding which variables a Boolean function depends on. There are 2^n possible\nlinear Boolean functions of n variables; given a linear Boolean function, the\nBernstein-Vazirani quantum algorithm can deterministically identify which one\nof these Boolean functions we are given using just one single function query.\nThe same quantum algorithm can also be used to learn which input variables\nother types of Boolean functions depend on, with a success probability that\ndepends on the form of the Boolean function that is tested, but does not depend\non the total number of input variables. We also outline a procedure to futher\namplify the success probability, based on another quantum algorithm, the Grover\nsearch. \n\n"}
{"id": "1006.3046", "contents": "Title: Identifying Shapes Using Self-Assembly (extended abstract) Abstract: In this paper, we introduce the following problem in the theory of\nalgorithmic self-assembly: given an input shape as the seed of a tile-based\nself-assembly system, design a finite tile set that can, in some sense,\nuniquely identify whether or not the given input shape--drawn from a very\ngeneral class of shapes--matches a particular target shape. We first study the\ncomplexity of correctly identifying squares. Then we investigate the complexity\nassociated with the identification of a considerably more general class of\nnon-square, hole-free shapes. \n\n"}
{"id": "1006.3651", "contents": "Title: Quantum algorithms for formula evaluation Abstract: We survey the recent sequence of algorithms for evaluating Boolean formulas\nconsisting of NAND gates. \n\n"}
{"id": "1006.4014", "contents": "Title: New Developments in Quantum Algorithms Abstract: In this survey, we describe two recent developments in quantum algorithms.\n  The first new development is a quantum algorithm for evaluating a Boolean\nformula consisting of AND and OR gates of size N in time O(\\sqrt{N}). This\nprovides quantum speedups for any problem that can be expressed via Boolean\nformulas. This result can be also extended to span problems, a generalization\nof Boolean formulas. This provides an optimal quantum algorithm for any Boolean\nfunction in the black-box query model.\n  The second new development is a quantum algorithm for solving systems of\nlinear equations. In contrast with traditional algorithms that run in time\nO(N^{2.37...}) where N is the size of the system, the quantum algorithm runs in\ntime O(\\log^c N). It outputs a quantum state describing the solution of the\nsystem. \n\n"}
{"id": "1006.4586", "contents": "Title: Vertex Sparsifiers: New Results from Old Techniques Abstract: Given a capacitated graph $G = (V,E)$ and a set of terminals $K \\subseteq V$,\nhow should we produce a graph $H$ only on the terminals $K$ so that every\n(multicommodity) flow between the terminals in $G$ could be supported in $H$\nwith low congestion, and vice versa? (Such a graph $H$ is called a\nflow-sparsifier for $G$.) What if we want $H$ to be a \"simple\" graph? What if\nwe allow $H$ to be a convex combination of simple graphs?\n  Improving on results of Moitra [FOCS 2009] and Leighton and Moitra [STOC\n2010], we give efficient algorithms for constructing: (a) a flow-sparsifier $H$\nthat maintains congestion up to a factor of $O(\\log k/\\log \\log k)$, where $k =\n|K|$, (b) a convex combination of trees over the terminals $K$ that maintains\ncongestion up to a factor of $O(\\log k)$, and (c) for a planar graph $G$, a\nconvex combination of planar graphs that maintains congestion up to a constant\nfactor. This requires us to give a new algorithm for the 0-extension problem,\nthe first one in which the preimages of each terminal are connected in $G$.\nMoreover, this result extends to minor-closed families of graphs.\n  Our improved bounds immediately imply improved approximation guarantees for\nseveral terminal-based cut and ordering problems. \n\n"}
{"id": "1006.5234", "contents": "Title: Approximating the Tutte polynomial of a binary matroid and other related\n  combinatorial polynomials Abstract: We consider the problem of approximating certain combinatorial polynomials.\nFirst, we consider the problem of approximating the Tutte polynomial of a\nbinary matroid with parameters q>= 2 and gamma. (Relative to the classical\n(x,y) parameterisation, q=(x-1)(y-1) and gamma=y-1.) A graph is a special case\nof a binary matroid, so earlier work by the authors shows inapproximability\n(subject to certain complexity assumptions) for q>2, apart from the trivial\ncase gamma=0. The situation for q=2 is different. Previous results for graphs\nimply inapproximability in the region -2<=gamma<0, apart from at two \"special\npoints\" where the polynomial can be computed exactly in polynomial time. For\nbinary matroids, we extend this result by showing (i) there is no FPRAS in the\nregion gamma<-2 unless NP=RP, and (ii) in the region gamma>0, the approximation\nproblem is hard for the complexity class #RHPi_1 under approximation-preserving\n(AP) reducibility. The latter result indicates a gap in approximation\ncomplexity at q=2: whereas an FPRAS is known in the graphical case, there can\nbe none in the binary matroid case, unless there is an FPRAS for all of\n#RHPi_1. The result also implies that it is computationally difficult to\napproximate the weight enumerator of a binary linear code, apart from at the\nspecial weights at which the problem is exactly solvable in polynomial time. As\na consequence, we show that approximating the cycle index polynomial of a\npermutation group is hard for #RHPi_1 under AP-reducibility, partially\nresolving a question that we first posed in 1992. \n\n"}
{"id": "1007.1673", "contents": "Title: Online Stochastic Matching: Online Actions Based on Offline Statistics Abstract: We consider the online stochastic matching problem proposed by Feldman et al.\n[FMMM09] as a model of display ad allocation. We are given a bipartite graph;\none side of the graph corresponds to a fixed set of bins and the other side\nrepresents the set of possible ball types. At each time step, a ball is sampled\nindependently from the given distribution and it needs to be matched upon its\narrival to an empty bin. The goal is to maximize the number of allocations.\n  We present an online algorithm for this problem with a competitive ratio of\n0.702. Before our result, algorithms with a competitive ratio better than\n$1-1/e$ were known under the assumption that the expected number of arriving\nballs of each type is integral. A key idea of the algorithm is to collect\nstatistics about the decisions of the optimum offline solution using Monte\nCarlo sampling and use those statistics to guide the decisions of the online\nalgorithm. We also show that our algorithm achieves a competitive ratio of\n0.705 when the rates are integral.\n  On the hardness side, we prove that no online algorithm can have a\ncompetitive ratio better than 0.823 under the known distribution model (and\nhenceforth under the permutation model). This improves upon the 5/6 hardness\nresult proved by Goel and Mehta \\cite{GM08} for the permutation model. \n\n"}
{"id": "1007.5032", "contents": "Title: Approximation Algorithms for Secondary Spectrum Auctions Abstract: We study combinatorial auctions for the secondary spectrum market. In this\nmarket, short-term licenses shall be given to wireless nodes for communication\nin their local neighborhood. In contrast to the primary market, channels can be\nassigned to multiple bidders, provided that the corresponding devices are well\nseparated such that the interference is sufficiently low. Interference\nconflicts are described in terms of a conflict graph in which the nodes\nrepresent the bidders and the edges represent conflicts such that the feasible\nallocations for a channel correspond to the independent sets in the conflict\ngraph.\n  In this paper, we suggest a novel LP formulation for combinatorial auctions\nwith conflict graph using a non-standard graph parameter, the so-called\ninductive independence number. Taking into account this parameter enables us to\nbypass the well-known lower bound of \\Omega(n^{1-\\epsilon}) on the\napproximability of independent set in general graphs with n nodes (bidders). We\nachieve significantly better approximation results by showing that interference\nconstraints for wireless networks yield conflict graphs with bounded inductive\nindependence number.\n  Our framework covers various established models of wireless communication,\ne.g., the protocol or the physical model. For the protocol model, we achieve an\nO(\\sqrt{k})-approximation, where k is the number of available channels. For the\nmore realistic physical model, we achieve an O(\\sqrt{k} \\log^2 n) approximation\nbased on edge-weighted conflict graphs. Combining our approach with the the\nLP-based framework of Lavi and Swamy, we obtain incentive compatible mechanisms\nfor general bidders with arbitrary valuations on bundles of channels specified\nin terms of demand oracles. \n\n"}
{"id": "1008.0683", "contents": "Title: Holographic Algorithms with Matchgates Capture Precisely Tractable\n  Planar #CSP Abstract: Valiant introduced matchgate computation and holographic algorithms. A number\nof seemingly exponential time problems can be solved by this novel algorithmic\nparadigm in polynomial time. We show that, in a very strong sense, matchgate\ncomputations and holographic algorithms based on them provide a universal\nmethodology to a broad class of counting problems studied in statistical\nphysics community for decades. They capture precisely those problems which are\n#P-hard on general graphs but computable in polynomial time on planar graphs.\n  More precisely, we prove complexity dichotomy theorems in the framework of\ncounting CSP problems. The local constraint functions take Boolean inputs, and\ncan be arbitrary real-valued symmetric functions. We prove that, every problem\nin this class belongs to precisely three categories: (1) those which are\ntractable (i.e., polynomial time computable) on general graphs, or (2) those\nwhich are \\#P-hard on general graphs but ractable on planar graphs, or (3)\nthose which are #P-hard even on planar graphs. The classification criteria are\nexplicit. Moreover, problems in category (2) are tractable on planar graphs\nprecisely by holographic algorithms with matchgates. \n\n"}
{"id": "1009.1254", "contents": "Title: Multiuser broadcast erasure channel with feedback - capacity and\n  algorithms Abstract: We consider the $N$-user broadcast erasure channel with $N$ unicast sessions\n(one for each user) where receiver feedback is regularly sent to the\ntransmitter in the form of ACK/NACK messages. We first provide a generic outer\nbound to the capacity of this system; we then propose a virtual-queue-based\ninter-session mixing coding algorithm, determine its rate region and show that\nit achieves capacity under certain conditions on channel statistics, assuming\nthat instantaneous feedback is known to all users. Removing this assumption\nresults in a rate region that asymptotically differs from the outer bound by 1\nbit as $L\\to \\infty$, where $L$ is the number of bits per packet (packet\nlength). For the case of arbitrary channel statistics, we present a\nmodification of the previous algorithm whose rate region is identical to the\nouter bound for N=3, when instant feedback is known to all users, and differs\nfrom the bound by 1 bit as $L\\to \\infty$, when the 3 users know only their own\nACK. The proposed algorithms do not require any prior knowledge of channel\nstatistics. \n\n"}
{"id": "1009.2242", "contents": "Title: Minimal-memory realization of pearl-necklace encoders of general quantum\n  convolutional codes Abstract: Quantum convolutional codes, like their classical counterparts, promise to\noffer higher error correction performance than block codes of equivalent\nencoding complexity, and are expected to find important applications in\nreliable quantum communication where a continuous stream of qubits is\ntransmitted. Grassl and Roetteler devised an algorithm to encode a quantum\nconvolutional code with a \"pearl-necklace encoder.\" Despite their theoretical\nsignificance as a neat way of representing quantum convolutional codes, they\nare not well-suited to practical realization. In fact, there is no\nstraightforward way to implement any given pearl-necklace structure. This paper\ncloses the gap between theoretical representation and practical implementation.\nIn our previous work, we presented an efficient algorithm for finding a\nminimal-memory realization of a pearl-necklace encoder for\nCalderbank-Shor-Steane (CSS) convolutional codes. This work extends our\nprevious work and presents an algorithm for turning a pearl-necklace encoder\nfor a general (non-CSS) quantum convolutional code into a realizable quantum\nconvolutional encoder. We show that a minimal-memory realization depends on the\ncommutativity relations between the gate strings in the pearl-necklace encoder.\nWe find a realization by means of a weighted graph which details the\nnon-commutative paths through the pearl-necklace. The weight of the longest\npath in this graph is equal to the minimal amount of memory needed to implement\nthe encoder. The algorithm has a polynomial-time complexity in the number of\ngate strings in the pearl-necklace encoder. \n\n"}
{"id": "1009.2452", "contents": "Title: Facility Location with Client Latencies: Linear-Programming based\n  Techniques for Minimum-Latency Problems Abstract: We introduce a problem that is a common generalization of the uncapacitated\nfacility location and minimum latency (ML) problems, where facilities need to\nbe opened to serve clients and also need to be sequentially activated before\nthey can provide service. Formally, we are given a set \\F of n facilities with\nfacility-opening costs {f_i}, a set of m clients, and connection costs {c_{ij}}\nspecifying the cost of assigning a client j to a facility i, a root node r\ndenoting the depot, and a time metric d on \\F\\cup{r}. Our goal is to open a\nsubset F of facilities, find a path P starting at r and spanning F to activate\nthe open facilities, and connect each client j to a facility \\phi(j)\\in F, so\nas to minimize \\sum_{i\\in F}f_i +\\sum_{clients j}(c_{\\phi(j),j}+t_j), where t_j\nis the time taken to reach \\phi(j) along path P. We call this the minimum\nlatency uncapacitated facility location (MLUFL) problem.\n  Our main result is an O(\\log n\\max{\\log n,\\log m})-approximation for MLUFL.\nWe also show that any improvement in this approximation guarantee, implies an\nimprovement in the (current-best) approximation factor for group Steiner tree.\nWe obtain constant approximations for two natural special cases of the problem:\n(a) related MLUFL (metric connection costs that are a scalar multiple of the\ntime metric); (b) metric uniform MLUFL (metric connection costs, unform\ntime-metric). Our LP-based methods are versatile and easily adapted to yield\napproximation guarantees for MLUFL in various more general settings, such as\n(i) when the latency-cost of a client is a function of the delay faced by the\nfacility to which it is connected; and (ii) the k-route version, where k\nvehicles are routed in parallel to activate the open facilities. Our LP-based\nunderstanding of MLUFL also offers some LP-based insights into ML, which we\nbelieve is a promising direction for obtaining improvements for ML. \n\n"}
{"id": "1009.5108", "contents": "Title: Improving the Space-Bounded Version of Muchnik's Conditional Complexity\n  Theorem via \"Naive\" Derandomization Abstract: Many theorems about Kolmogorov complexity rely on existence of combinatorial\nobjects with specific properties. Usually the probabilistic method gives such\nobjects with better parameters than explicit constructions do. But the\nprobabilistic method does not give \"effective\" variants of such theorems, i.e.\nvariants for resource-bounded Kolmogorov complexity. We show that a \"naive\nderandomization\" approach of replacing these objects by the output of\nNisan-Wigderson pseudo-random generator may give polynomial-space variants of\nsuch theorems.\n  Specifically, we improve the preceding polynomial-space analogue of Muchnik's\nconditional complexity theorem. I.e., for all $a$ and $b$ there exists a\nprogram $p$ of least possible length that transforms $a$ to $b$ and is simple\nconditional on $b$. Here all programs work in polynomial space and all\ncomplexities are measured with logarithmic accuracy instead of polylogarithmic\none in the previous work. \n\n"}
{"id": "1010.1221", "contents": "Title: Different Adiabatic Quantum Optimization Algorithms for the NP-Complete\n  Exact Cover and 3SAT Problems Abstract: One of the most important questions in studying quantum computation is:\nwhether a quantum computer can solve NP-complete problems more efficiently than\na classical computer? In 2000, Farhi, et al. (Science, 292(5516):472--476,\n2001) proposed the adiabatic quantum optimization (AQO), a paradigm that\ndirectly attacks NP-hard optimization problems. How powerful is AQO? Early on,\nvan Dam and Vazirani claimed that AQO failed (i.e. would take exponential time)\nfor a family of 3SAT instances they constructed. More recently, Altshuler, et\nal. (Proc Natl Acad Sci USA, 107(28): 12446--12450, 2010) claimed that AQO\nfailed also for random instances of the NP-complete Exact Cover problem. In\nthis paper, we make clear that all these negative results are only for a\nspecific AQO algorithm. We do so by demonstrating different AQO algorithms for\nthe same problem for which their arguments no longer hold. Whether AQO fails or\nsucceeds for solving the NP-complete problems (either the worst case or the\naverage case) requires further investigation. Our AQO algorithms for Exact\nCover and 3SAT are based on the polynomial reductions to the NP-complete\nMaximum-weight Independent Set (MIS) problem. \n\n"}
{"id": "1010.4458", "contents": "Title: Variable time amplitude amplification and a faster quantum algorithm for\n  solving systems of linear equations Abstract: We present two new quantum algorithms. Our first algorithm is a\ngeneralization of amplitude amplification to the case when parts of the quantum\nalgorithm that is being amplified stop at different times.\n  Our second algorithm uses the first algorithm to improve the running time of\nHarrow et al. algorithm for solving systems of linear equations from O(kappa^2\nlog N) to O(kappa log^3 kappa log N) where \\kappa is the condition number of\nthe system of equations. \n\n"}
{"id": "1010.5197", "contents": "Title: Multicut is FPT Abstract: Let $G=(V,E)$ be a graph on $n$ vertices and $R$ be a set of pairs of\nvertices in $V$ called \\emph{requests}. A \\emph{multicut} is a subset $F$ of\n$E$ such that every request $xy$ of $R$ is cut by $F$, \\i.e. every $xy$-path of\n$G$ intersects $F$. We show that there exists an $O(f(k)n^c)$ algorithm which\ndecides if there exists a multicut of size at most $k$. In other words, the\n\\M{} problem parameterized by the solution size $k$ is Fixed-Parameter\nTractable. The proof extends to vertex multicuts. \n\n"}
{"id": "1010.6231", "contents": "Title: A polynomial-time algorithm for estimating the partition function of the\n  ferromagnetic Ising model on a regular matroid Abstract: We investigate the computational difficulty of approximating the partition\nfunction of the ferromagnetic Ising model on a regular matroid. Jerrum and\nSinclair have shown that there is a fully polynomial randomised approximation\nscheme (FPRAS) for the class of graphic matroids. On the other hand, the\nauthors have previously shown, subject to a complexity-theoretic assumption,\nthat there is no FPRAS for the class of binary matroids, which is a proper\nsuperset of the class of graphic matroids. In order to map out the region where\napproximation is feasible, we focus on the class of regular matroids, an\nimportant class of matroids which properly includes the class of graphic\nmatroids, and is properly included in the class of binary matroids. Using\nSeymour's decomposition theorem, we give an FPRAS for the class of regular\nmatroids. \n\n"}
{"id": "1011.0972", "contents": "Title: A recombination algorithm for the decomposition of multivariate rational\n  functions Abstract: In this paper we show how we can compute in a deterministic way the\ndecomposition of a multivariate rational function with a recombination\nstrategy. The key point of our recombination strategy is the used of Darboux\npolynomials. We study the complexity of this strategy and we show that this\nmethod improves the previous ones. In appendix, we explain how the strategy\nproposed recently by J. Berthomieu and G. Lecerf for the sparse factorization\ncan be used in the decomposition setting. Then we deduce a decomposition\nalgorithm in the sparse bivariate case and we give its complexity \n\n"}
{"id": "1011.1273", "contents": "Title: Adversarial Satisfiability Problem Abstract: We study the adversarial satisfiability problem, where the adversary can\nchoose whether variables are negated in clauses or not in order to make the\nresulting formula unsatisfiable. This is one case of a general class of\nadversarial optimization problems that often arise in practice and are\nalgorithmically much harder than the standard optimization problems. We use the\ncavity method to compute large deviations of the entropy in the random\nsatisfiability problem with respect to the negation-configurations. We conclude\nthat in the thermodynamic limit the best strategy the adversary can adopt is\nextremely close to simply balancing the number of times every variable is and\nis not negated. We also conduct a numerical study of the problem, and find that\nthere are very strong pre-asymptotic effects that are due to the fact that for\nsmall sizes exponential and factorial growth is hardly distinguishable. \n\n"}
{"id": "1011.1350", "contents": "Title: Geometric Complexity Theory and Tensor Rank Abstract: Mulmuley and Sohoni (GCT1 in SICOMP 2001, GCT2 in SICOMP 2008) proposed to\nview the permanent versus determinant problem as a specific orbit closure\nproblem and to attack it by methods from geometric invariant and representation\ntheory. We adopt these ideas towards the goal of showing lower bounds on the\nborder rank of specific tensors, in particular for matrix multiplication. We\nthus study specific orbit closure problems for the group $G = GL(W_1)\\times\nGL(W_2)\\times GL(W_3)$ acting on the tensor product $W=W_1\\otimes W_2\\otimes\nW_3$ of complex finite dimensional vector spaces. Let $G_s = SL(W_1)\\times\nSL(W_2)\\times SL(W_3)$. A key idea from GCT2 is that the irreducible\n$G_s$-representations occurring in the coordinate ring of the $G$-orbit closure\nof a stable tensor $w\\in W$ are exactly those having a nonzero invariant with\nrespect to the stabilizer group of $w$.\n  However, we prove that by considering $G_s$-representations, as suggested in\nGCT1-2, only trivial lower bounds on border rank can be shown. It is thus\nnecessary to study $G$-representations, which leads to geometric extension\nproblems that are beyond the scope of the subgroup restriction problems\nemphasized in GCT1-2. We prove a very modest lower bound on the border rank of\nmatrix multiplication tensors using $G$-representations. This shows at least\nthat the barrier for $G_s$-representations can be overcome. To advance, we\nsuggest the coarser approach to replace the semigroup of representations of a\ntensor by its moment polytope. We prove first results towards determining the\nmoment polytopes of matrix multiplication and unit tensors. \n\n"}
{"id": "1011.2887", "contents": "Title: Constructing elusive functions with help of evaluation mappings Abstract: We develop a method to construct elusive functions using techniques of\ncommutative algebra and algebraic geometry. The key notions of this method are\nelusive subsets and evaluation mappings. We also develop the effective\nelimination theory combined with algebraic number field theory in order to\nconstruct concrete points outside the image of a polynomial mapping. Using the\ndeveloped methods, for $F = C \\text{or} F = R$, we construct examples of\n$(s,r)$-elusive functions whose monomial coefficients are algebraic numbers,\nwhich give polynomials with algebraic number coefficients of large circuit\nsize. \n\n"}
{"id": "1011.3493", "contents": "Title: Program Size and Temperature in Self-Assembly Abstract: Winfree's abstract Tile Assembly Model (aTAM) is a model of molecular\nself-assembly of DNA complexes known as tiles, which float freely in solution\nand attach one at a time to a growing \"seed\" assembly based on specific binding\nsites on their four sides. We show that there is a polynomial-time algorithm\nthat, given an n x n square, finds the minimal tile system (i.e., the system\nwith the smallest number of distinct tile types) that uniquely self-assembles\nthe square, answering an open question of Adleman, Cheng, Goel, Huang, Kempe,\nMoisset de Espanes, and Rothemund (\"Combinatorial Optimization Problems in\nSelf-Assembly\", STOC 2002). Our investigation leading to this algorithm reveals\nother positive and negative results about the relationship between the size of\na tile system and its \"temperature\" (the binding strength threshold required\nfor a tile to attach). \n\n"}
{"id": "1011.3843", "contents": "Title: Magnetic Towers of Hanoi and their Optimal Solutions Abstract: The Magnetic Tower of Hanoi puzzle - a modified \"base 3\" version of the\nclassical Tower of Hanoi puzzle as described in earlier papers, is actually a\nsmall set of independent sister-puzzles, depending on the \"pre-coloring\"\ncombination of the tower's posts. Starting with Red facing up on a Source post,\nworking through an Intermediate - colored or Neutral post, and ending Blue\nfacing up on a Destination post, we identify the different pre-coloring\ncombinations in (S,I,D) order. The Tower's pre-coloring combinations are\n{[(R,B,B) / (R,R,B)] ; [(R,B,N) / (N,R,B)] ; [(N,B,N) / (N,R,N)] ; [R,N,B] ;\n[(R,N,N) / (N,N,B)] ; [N,N,N]}. In this paper we investigate these\nsister-puzzles, identify the algorithm that optimally solves each pre-colored\npuzzle, and prove its Optimality. As it turns out, five of the six algorithms,\nchallenging on their own, are part of the algorithm solving the \"natural\", Free\nMagnetic Tower of Hanoi puzzle [N,N,N]. We start by showing that the N-disk\nColored Tower [(R,B,B) / (R,R,B)] is solved by (3^N - 1)/2 moves. Defining\n\"Algorithm Duration\" as the ratio of number of algorithm-moves solving the\npuzzle to the number of algorithm-moves solving the Colored Tower, we find the\nDuration-Limits for all sister-puzzles. In the order of the list above they are\n{[1] ; [10/11] ; [10/11] ; [8/11] ; [7/11] ; [20/33]}. Thus, the Duration-Limit\nof the Optimal Algorithm solving the Free Magnetic Tower of Hanoi puzzle is\n20/33 or 606 0/00. On the road to optimally solve this colorful Magnetic\npuzzle, we hit other \"forward-moving\" puzzle-solving algorithms. Overall we\nlooked at 10 pairs of integer sequences. Of the twenty integer sequences, five\nare listed in the On-line Encyclopedia of Integer Sequences, the other fifteen\n- not yet. The large set of different solutions is a clear indication to the\nfreedom-of-wondering that makes this Magnetic Tower of Hanoi puzzle so\ncolorful. \n\n"}
{"id": "1011.4955", "contents": "Title: Reverse Nearest Neighbors Search in High Dimensions using\n  Locality-Sensitive Hashing Abstract: We investigate the problem of finding reverse nearest neighbors efficiently.\nAlthough provably good solutions exist for this problem in low or fixed\ndimensions, to this date the methods proposed in high dimensions are mostly\nheuristic. We introduce a method that is both provably correct and efficient in\nall dimensions, based on a reduction of the problem to one instance of\n$\\e$-nearest neighbor search plus a controlled number of instances of {\\em\nexhaustive $r$-\\pleb}, a variant of {\\em Point Location among Equal Balls}\nwhere all the $r$-balls centered at the data points that contain the query\npoint are sought for, not just one. The former problem has been extensively\nstudied and elegantly solved in high dimensions using Locality-Sensitive\nHashing (LSH) techniques. By contrast, the latter problem has a complexity that\nis still not fully understood. We revisit the analysis of the LSH scheme for\nexhaustive $r$-\\pleb using a somewhat refined notion of locality-sensitive\nfamily of hash function, which brings out a meaningful output-sensitive term in\nthe complexity of the problem. Our analysis, combined with a non-isometric\nlifting of the data, enables us to answer exhaustive $r$-\\pleb queries (and\ndown the road reverse nearest neighbors queries) efficiently. Along the way, we\nobtain a simple algorithm for answering exact nearest neighbor queries, whose\ncomplexity is parametrized by some {\\em condition number} measuring the\ninherent difficulty of a given instance of the problem. \n\n"}
{"id": "1011.6267", "contents": "Title: Computing multiway cut within the given excess over the largest minimum\n  isolating cut Abstract: Let $(G,T)$ be an instance of the (vertex) multiway cut problem where $G$ is\na graph and $T$ is a set of terminals. For $t \\in T$, a set of nonterminal\nvertices separating $t$ from $T \\setminus \\{T\\}$ is called an \\emph{isolating\ncut} of $t$. The largest among all the smallest isolating cuts is a natural\nlower bound for a multiway cut of $(G,T)$. Denote this lower bound by $m$ and\nlet $k$ be an integer.\n  In this paper we propose an $O(kn^{k+3})$ algorithm that computes a multiway\ncut of $(G,T)$ of size at most $m+k$ or reports that there is no such multiway\ncut. The core of the proposed algorithm is the following combinatorial result.\nLet $G$ be a graph and let $X,Y$ be two disjoint subsets of vertices of $G$.\nLet $m$ be the smallest size of a vertex $X-Y$ separator. Then, for the given\ninteger $k$, the number of \\emph{important} $X-Y$ separators \\cite{MarxTCS} of\nsize at most $m+k$ is at most $\\sum_{i=0}^k{n \\choose i}$. \n\n"}
{"id": "1012.2825", "contents": "Title: Improved distance queries in planar graphs Abstract: There are several known data structures that answer distance queries between\ntwo arbitrary vertices in a planar graph. The tradeoff is among preprocessing\ntime, storage space and query time. In this paper we present three data\nstructures that answer such queries, each with its own advantage over previous\ndata structures. The first one improves the query time of data structures of\nlinear space. The second improves the preprocessing time of data structures\nwith a space bound of O(n^(4/3)) or higher while matching the best known query\ntime. The third data structure improves the query time for a similar range of\nspace bounds, at the expense of a longer preprocessing time. The techniques\nthat we use include modifying the parameters of planar graph decompositions,\ncombining the different advantages of existing data structures, and using the\nMonge property for finding minimum elements of matrices. \n\n"}
{"id": "1012.3018", "contents": "Title: On the size of data structures used in symbolic model checking Abstract: Temporal Logic Model Checking is a verification method in which we describe a\nsystem, the model, and then we verify whether some properties, expressed in a\ntemporal logic formula, hold in the system. It has many industrial\napplications. In order to improve performance, some tools allow preprocessing\nof the model, verifying on-line a set of properties reusing the same compiled\nmodel; we prove that the complexity of the Model Checking problem, without any\npreprocessing or preprocessing the model or the formula in a polynomial data\nstructure, is the same. As a result preprocessing does not always exponentially\nimprove performance.\n  Symbolic Model Checking algorithms work by manipulating sets of states, and\nthese sets are often represented by BDDs. It has been observed that the size of\nBDDs may grow exponentially as the model and formula increase in size. As a\nside result, we formally prove that a superpolynomial increase of the size of\nthese BDDs is unavoidable in the worst case. While this exponential growth has\nbeen empirically observed, to the best of our knowledge it has never been\nproved so far in general terms. This result not only holds for all types of\nBDDs regardless of the variable ordering, but also for more powerful data\nstructures, such as BEDs, RBCs, MTBDDs, and ADDs. \n\n"}
{"id": "1012.4404", "contents": "Title: Multicolored Dynamos on Toroidal Meshes Abstract: Detecting on a graph the presence of the minimum number of nodes (target set)\nthat will be able to \"activate\" a prescribed number of vertices in the graph is\ncalled the target set selection problem (TSS) proposed by Kempe, Kleinberg, and\nTardos. In TSS's settings, nodes have two possible states (active or\nnon-active) and the threshold triggering the activation of a node is given by\nthe number of its active neighbors. Dealing with fault tolerance in a majority\nbased system the two possible states are used to denote faulty or non-faulty\nnodes, and the threshold is given by the state of the majority of neighbors.\nHere, the major effort was in determining the distribution of initial faults\nleading the entire system to a faulty behavior. Such an activation pattern,\nalso known as dynamic monopoly (or shortly dynamo), was introduced by Peleg in\n1996. In this paper we extend the TSS problem's settings by representing nodes'\nstates with a \"multicolored\" set. The extended version of the problem can be\ndescribed as follows: let G be a simple connected graph where every node is\nassigned a color from a finite ordered set C = {1, . . ., k} of colors. At each\nlocal time step, each node can recolor itself, depending on the local\nconfigurations, with the color held by the majority of its neighbors. Given G,\nwe study the initial distributions of colors leading the system to a k\nmonochromatic configuration in toroidal meshes, focusing on the minimum number\nof initial k-colored nodes. We find upper and lower bounds to the size of a\ndynamo, and then special classes of dynamos, outlined by means of a new\napproach based on recoloring patterns, are characterized. \n\n"}
{"id": "1101.0523", "contents": "Title: On Arthur Merlin Games in Communication Complexity Abstract: We show several results related to interactive proof modes of communication\ncomplexity. First we show lower bounds for the QMA-communication complexity of\nthe functions Inner Product and Disjointness. We describe a general method to\nprove lower bounds for QMA-communication complexity, and show how one can\n'transfer' hardness under an analogous measure in the query complexity model to\nthe communication model using Sherstov's pattern matrix method. Combining a\nresult by Vereshchagin and the pattern matrix method we find a communication\nproblem with AM-communication complexity $O(\\log n)$, PP-communication\ncomplexity $\\Omega(n^{1/3})$, and QMA-communication complexity\n$\\Omega(n^{1/6})$. Hence in the world of communication complexity\nnoninteractive quantum proof systems are not able to efficiently simulate\nco-nondeterminism or interaction. These results imply that the related\nquestions in Turing machine complexity theory cannot be resolved by\n'algebrizing' techniques. Finally we show that in MA-protocols there is an\nexponential gap between one-way protocols and two-way protocols (this refers to\nthe interaction between Alice and Bob). This is in contrast to\nnondeterministic, AM-, and QMA-protocols, where one-way communication is\nessentially optimal. \n\n"}
{"id": "1101.2170", "contents": "Title: The Complexity of Finding Multiple Solutions to Betweenness and Quartet\n  Compatibility Abstract: We show that two important problems that have applications in computational\nbiology are ASP-complete, which implies that, given a solution to a problem, it\nis NP-complete to decide if another solution exists. We show first that a\nvariation of Betweenness, which is the underlying problem of questions related\nto radiation hybrid mapping, is ASP-complete. Subsequently, we use that result\nto show that Quartet Compatibility, a fundamental problem in phylogenetics that\nasks whether a set of quartets can be represented by a parent tree, is also\nASP-complete. The latter result shows that Steel's \\sc Quartet Challenge, which\nasks whether a solution to Quartet Compatibility is unique, is coNP-complete. \n\n"}
{"id": "1101.2973", "contents": "Title: Maximizing Non-monotone Submodular Set Functions Subject to Different\n  Constraints: Combined Algorithms Abstract: We study the problem of maximizing constrained non-monotone submodular\nfunctions and provide approximation algorithms that improve existing algorithms\nin terms of either the approximation factor or simplicity. Our algorithms\ncombine existing local search and greedy based algorithms. Different\nconstraints that we study are exact cardinality and multiple knapsack\nconstraints. For the multiple-knapsack constraints we achieve a\n$(0.25-2\\epsilon)$-factor algorithm.\n  We also show, as our main contribution, how to use the continuous greedy\nprocess for non-monotone functions and, as a result, obtain a $0.13$-factor\napproximation algorithm for maximization over any solvable down-monotone\npolytope. The continuous greedy process has been previously used for maximizing\nsmooth monotone submodular function over a down-monotone polytope\n\\cite{CCPV08}. This implies a 0.13-approximation for several discrete problems,\nsuch as maximizing a non-negative submodular function subject to a matroid\nconstraint and/or multiple knapsack constraints. \n\n"}
{"id": "1101.3682", "contents": "Title: Diversification improves interpolation Abstract: We consider the problem of interpolating an unknown multivariate polynomial\nwith coefficients taken from a finite field or as numerical approximations of\ncomplex numbers. Building on the recent work of Garg and Schost, we improve on\nthe best-known algorithm for interpolation over large finite fields by\npresenting a Las Vegas randomized algorithm that uses fewer black box\nevaluations. Using related techniques, we also address numerical interpolation\nof sparse polynomials with complex coefficients, and provide the first provably\nstable algorithm (in the sense of relative error) for this problem, at the cost\nof modestly more evaluations. A key new technique is a randomization which\nmakes all coefficients of the unknown polynomial distinguishable, producing\nwhat we call a diverse polynomial. Another departure from most previous\napproaches is that our algorithms do not rely on root finding as a subroutine.\nWe show how these improvements affect the practical performance with trial\nimplementations. \n\n"}
{"id": "1102.2593", "contents": "Title: Codes and Designs Related to Lifted MRD Codes Abstract: Lifted maximum rank distance (MRD) codes, which are constant dimension codes,\nare considered. It is shown that a lifted MRD code can be represented in such a\nway that it forms a block design known as a transversal design. A slightly\ndifferent representation of this design makes it similar to a $q-$analog of a\ntransversal design. The structure of these designs is used to obtain upper\nbounds on the sizes of constant dimension codes which contain a lifted MRD\ncode. Codes which attain these bounds are constructed. These codes are the\nlargest known codes for the given parameters. These transversal designs can be\nalso used to derive a new family of linear codes in the Hamming space. Bounds\non the minimum distance and the dimension of such codes are given. \n\n"}
{"id": "1102.3749", "contents": "Title: Approximation Algorithms for Correlated Knapsacks and Non-Martingale\n  Bandits Abstract: In the stochastic knapsack problem, we are given a knapsack of size B, and a\nset of jobs whose sizes and rewards are drawn from a known probability\ndistribution. However, we know the actual size and reward only when the job\ncompletes. How should we schedule jobs to maximize the expected total reward?\nWe know O(1)-approximations when we assume that (i) rewards and sizes are\nindependent random variables, and (ii) we cannot prematurely cancel jobs. What\ncan we say when either or both of these assumptions are changed?\n  The stochastic knapsack problem is of interest in its own right, but\ntechniques developed for it are applicable to other stochastic packing\nproblems. Indeed, ideas for this problem have been useful for budgeted learning\nproblems, where one is given several arms which evolve in a specified\nstochastic fashion with each pull, and the goal is to pull the arms a total of\nB times to maximize the reward obtained. Much recent work on this problem focus\non the case when the evolution of the arms follows a martingale, i.e., when the\nexpected reward from the future is the same as the reward at the current state.\nWhat can we say when the rewards do not form a martingale?\n  In this paper, we give constant-factor approximation algorithms for the\nstochastic knapsack problem with correlations and/or cancellations, and also\nfor budgeted learning problems where the martingale condition is not satisfied.\nIndeed, we can show that previously proposed LP relaxations have large\nintegrality gaps. We propose new time-indexed LP relaxations, and convert the\nfractional solutions into distributions over strategies, and then use the LP\nvalues and the time ordering information from these strategies to devise a\nrandomized adaptive scheduling algorithm. We hope our LP formulation and\ndecomposition methods may provide a new way to address other correlated bandit\nproblems with more general contexts. \n\n"}
{"id": "1102.3766", "contents": "Title: Derandomizing HSSW Algorithm for 3-SAT Abstract: We present a (full) derandomization of HSSW algorithm for 3-SAT, proposed by\nHofmeister, Sch\\\"oning, Schuler, and Watanabe in [STACS'02]. Thereby, we obtain\nan O(1.3303^n)-time deterministic algorithm for 3-SAT, which is currently\nfastest. \n\n"}
{"id": "1102.4005", "contents": "Title: Approximating the Online Set Multicover Problems Via Randomized\n  Winnowing Abstract: In this paper, we consider the weighted online set k-multicover problem. In\nthis problem, we have a universe V of elements, a family S of subsets of V with\na positive real cost for every set in S and a \"coverage factor\" (positive\ninteger) k. A subset of elements are presented online in an arbitrary order.\nWhen each element, say i, is presented, we are also told the collection of all\n(at least k) sets and their costs to which i belongs and we need to select\nadditional sets from these sets containing i, if necessary, such that our\ncollection of selected sets contains at least k sets that contain the element\ni. The goal is to minimize the total cost of the selected sets (our algorithm\nand competitive ratio bounds can be extended to the case when a set can be\nselected at most a pre-specified number of times instead of just once; we do\nnot report these extensions for simplicity and also because they have no\nrelevance to the biological applications that motivated our work). In this\npaper, we describe a new randomized algorithm for the online multicover problem\nbased on a randomized version of the winnowing approach of Littlestone. This\nalgorithm generalizes and improves some earlier results by N. Alon, B.\nAwerbuch, Y. Azar, N. Buchbinder, and J. Naor. We also discuss lower bounds on\ncompetitive ratios for deterministic algorithms for general $k$. \n\n"}
{"id": "1102.5538", "contents": "Title: Pseudo-random graphs and bit probe schemes with one-sided error Abstract: We study probabilistic bit-probe schemes for the membership problem. Given a\nset A of at most n elements from the universe of size m we organize such a\nstructure that queries of type \"Is x in A?\" can be answered very quickly.\nH.Buhrman, P.B.Miltersen, J.Radhakrishnan, and S.Venkatesh proposed a bit-probe\nscheme based on expanders. Their scheme needs space of $O(n\\log m)$ bits, and\nrequires to read only one randomly chosen bit from the memory to answer a\nquery. The answer is correct with high probability with two-sided errors. In\nthis paper we show that for the same problem there exists a bit-probe scheme\nwith one-sided error that needs space of $O(n\\log^2 m+\\poly(\\log m))$ bits. The\ndifference with the model of Buhrman, Miltersen, Radhakrishnan, and Venkatesh\nis that we consider a bit-probe scheme with an auxiliary word. This means that\nin our scheme the memory is split into two parts of different size: the main\nstorage of $O(n\\log^2 m)$ bits and a short word of $\\log^{O(1)}m$ bits that is\npre-computed once for the stored set A and `cached'. To answer a query \"Is x in\nA?\" we allow to read the whole cached word and only one bit from the main\nstorage. For some reasonable values of parameters our space bound is better\nthan what can be achieved by any scheme without cached data. \n\n"}
{"id": "1103.0040", "contents": "Title: From Convex Optimization to Randomized Mechanisms: Toward Optimal\n  Combinatorial Auctions Abstract: We design an expected polynomial-time, truthful-in-expectation,\n(1-1/e)-approximation mechanism for welfare maximization in a fundamental class\nof combinatorial auctions. Our results apply to bidders with valuations that\nare m matroid rank sums (MRS), which encompass most concrete examples of\nsubmodular functions studied in this context, including coverage functions,\nmatroid weighted-rank functions, and convex combinations thereof. Our\napproximation factor is the best possible, even for known and explicitly given\ncoverage valuations, assuming P != NP. Ours is the first\ntruthful-in-expectation and polynomial-time mechanism to achieve a\nconstant-factor approximation for an NP-hard welfare maximization problem in\ncombinatorial auctions with heterogeneous goods and restricted valuations.\n  Our mechanism is an instantiation of a new framework for designing\napproximation mechanisms based on randomized rounding algorithms. A typical\nsuch algorithm first optimizes over a fractional relaxation of the original\nproblem, and then randomly rounds the fractional solution to an integral one.\nWith rare exceptions, such algorithms cannot be converted into truthful\nmechanisms. The high-level idea of our mechanism design framework is to\noptimize directly over the (random) output of the rounding algorithm, rather\nthan over the input to the rounding algorithm. This approach leads to\ntruthful-in-expectation mechanisms, and these mechanisms can be implemented\nefficiently when the corresponding objective function is concave. For bidders\nwith MRS valuations, we give a novel randomized rounding algorithm that leads\nto both a concave objective function and a (1-1/e)-approximation of the optimal\nwelfare. \n\n"}
{"id": "1103.0041", "contents": "Title: A Truthful Randomized Mechanism for Combinatorial Public Projects via\n  Convex Optimization Abstract: In Combinatorial Public Projects, there is a set of projects that may be\nundertaken, and a set of self-interested players with a stake in the set of\nprojects chosen. A public planner must choose a subset of these projects,\nsubject to a resource constraint, with the goal of maximizing social welfare.\nCombinatorial Public Projects has emerged as one of the paradigmatic problems\nin Algorithmic Mechanism Design, a field concerned with solving fundamental\nresource allocation problems in the presence of both selfish behavior and the\ncomputational constraint of polynomial-time.\n  We design a polynomial-time, truthful-in-expectation, (1-1/e)-approximation\nmechanism for welfare maximization in a fundamental variant of combinatorial\npublic projects. Our results apply to combinatorial public projects when\nplayers have valuations that are matroid rank sums (MRS), which encompass most\nconcrete examples of submodular functions studied in this context, including\ncoverage functions, matroid weighted-rank functions, and convex combinations\nthereof. Our approximation factor is the best possible, assuming P != NP. Ours\nis the first mechanism that achieves a constant factor approximation for a\nnatural NP-hard variant of combinatorial public projects. \n\n"}
{"id": "1103.0318", "contents": "Title: Listing All Maximal Cliques in Large Sparse Real-World Graphs Abstract: We implement a new algorithm for listing all maximal cliques in sparse graphs\ndue to Eppstein, L\\\"offler, and Strash (ISAAC 2010) and analyze its performance\non a large corpus of real-world graphs. Our analysis shows that this algorithm\nis the first to offer a practical solution to listing all maximal cliques in\nlarge sparse graphs. All other theoretically-fast algorithms for sparse graphs\nhave been shown to be significantly slower than the algorithm of Tomita et al.\n(Theoretical Computer Science, 2006) in practice. However, the algorithm of\nTomita et al. uses an adjacency matrix, which requires too much space for large\nsparse graphs. Our new algorithm opens the door for fast analysis of large\nsparse graphs whose adjacency matrix will not fit into working memory. \n\n"}
{"id": "1103.1542", "contents": "Title: The tractability of CSP classes defined by forbidden patterns Abstract: The constraint satisfaction problem (CSP) is a general problem central to\ncomputer science and artificial intelligence. Although the CSP is NP-hard in\ngeneral, considerable effort has been spent on identifying tractable\nsubclasses. The main two approaches consider structural properties\n(restrictions on the hypergraph of constraint scopes) and relational properties\n(restrictions on the language of constraint relations). Recently, some authors\nhave considered hybrid properties that restrict the constraint hypergraph and\nthe relations simultaneously.\n  Our key contribution is the novel concept of a CSP pattern and classes of\nproblems defined by forbidden patterns (which can be viewed as forbidding\ngeneric subproblems). We describe the theoretical framework which can be used\nto reason about classes of problems defined by forbidden patterns. We show that\nthis framework generalises relational properties and allows us to capture known\nhybrid tractable classes.\n  Although we are not close to obtaining a dichotomy concerning the\ntractability of general forbidden patterns, we are able to make some progress\nin a special case: classes of problems that arise when we can only forbid\nbinary negative patterns (generic subproblems in which only inconsistent tuples\nare specified). In this case we are able to characterise very large classes of\ntractable and NP-hard forbidden patterns. This leaves the complexity of just\none case unresolved and we conjecture that this last case is tractable. \n\n"}
{"id": "1103.4195", "contents": "Title: Gossip PCA Abstract: Eigenvectors of data matrices play an important role in many computational\nproblems, ranging from signal processing to machine learning and control. For\ninstance, algorithms that compute positions of the nodes of a wireless network\non the basis of pairwise distance measurements require a few leading\neigenvectors of the distances matrix. While eigenvector calculation is a\nstandard topic in numerical linear algebra, it becomes challenging under severe\ncommunication or computation constraints, or in absence of central scheduling.\nIn this paper we investigate the possibility of computing the leading\neigenvectors of a large data matrix through gossip algorithms.\n  The proposed algorithm amounts to iteratively multiplying a vector by\nindependent random sparsification of the original matrix and averaging the\nresulting normalized vectors. This can be viewed as a generalization of gossip\nalgorithms for consensus, but the resulting dynamics is significantly more\nintricate. Our analysis is based on controlling the convergence to stationarity\nof the associated Kesten-Furstenberg Markov chain. \n\n"}
{"id": "1103.5102", "contents": "Title: Data-Oblivious External-Memory Algorithms for the Compaction, Selection,\n  and Sorting of Outsourced Data Abstract: We present data-oblivious algorithms in the external-memory model for\ncompaction, selection, and sorting. Motivation for such problems comes from\nclients who use outsourced data storage services and wish to mask their data\naccess patterns. We show that compaction and selection can be done\ndata-obliviously using $O(N/B)$ I/Os, and sorting can be done, with a high\nprobability of success, using $O((N/B)\\log_{M/B} (N/B))$ I/Os. Our methods use\na number of new algorithmic techniques, including data-oblivious uses of\ninvertible Bloom lookup tables, a butterfly-like compression network,\nrandomized data thinning, and \"shuffle-and-deal\" data perturbation. In\naddition, since data-oblivious sorting is the bottleneck in the \"inner loop\" in\nexisting oblivious RAM simulations, our sorting result improves the amortized\ntime overhead to do oblivious RAM simulation by a logarithmic factor in the\nexternal-memory model. \n\n"}
{"id": "1103.6246", "contents": "Title: Sparse Vector Distributions and Recovery from Compressed Sensing Abstract: It is well known that the performance of sparse vector recovery algorithms\nfrom compressive measurements can depend on the distribution underlying the\nnon-zero elements of a sparse vector. However, the extent of these effects has\nyet to be explored, and formally presented. In this paper, I empirically\ninvestigate this dependence for seven distributions and fifteen recovery\nalgorithms. The two morals of this work are: 1) any judgement of the recovery\nperformance of one algorithm over that of another must be prefaced by the\nconditions for which this is observed to be true, including sparse vector\ndistributions, and the criterion for exact recovery; and 2) a recovery\nalgorithm must be selected carefully based on what distribution one expects to\nunderlie the sensed sparse signal. \n\n"}
{"id": "1104.1204", "contents": "Title: Planar Cycle Covering Graphs Abstract: We describe a new variational lower-bound on the minimum energy configuration\nof a planar binary Markov Random Field (MRF). Our method is based on adding\nauxiliary nodes to every face of a planar embedding of the graph in order to\ncapture the effect of unary potentials. A ground state of the resulting\napproximation can be computed efficiently by reduction to minimum-weight\nperfect matching. We show that optimization of variational parameters achieves\nthe same lower-bound as dual-decomposition into the set of all cycles of the\noriginal graph. We demonstrate that our variational optimization converges\nquickly and provides high-quality solutions to hard combinatorial problems\n10-100x faster than competing algorithms that optimize the same bound. \n\n"}
{"id": "1104.1732", "contents": "Title: Optimal Column-Based Low-Rank Matrix Reconstruction Abstract: We prove that for any real-valued matrix $X \\in \\R^{m \\times n}$, and\npositive integers $r \\ge k$, there is a subset of $r$ columns of $X$ such that\nprojecting $X$ onto their span gives a $\\sqrt{\\frac{r+1}{r-k+1}}$-approximation\nto best rank-$k$ approximation of $X$ in Frobenius norm. We show that the\ntrade-off we achieve between the number of columns and the approximation ratio\nis optimal up to lower order terms. Furthermore, there is a deterministic\nalgorithm to find such a subset of columns that runs in $O(r n m^{\\omega} \\log\nm)$ arithmetic operations where $\\omega$ is the exponent of matrix\nmultiplication. We also give a faster randomized algorithm that runs in $O(r n\nm^2)$ arithmetic operations. \n\n"}
{"id": "1104.2486", "contents": "Title: Dynamic Programming for Graphs on Surfaces Abstract: We provide a framework for the design and analysis of dynamic programming\nalgorithms for surface-embedded graphs on n vertices and branchwidth at most k.\nOur technique applies to general families of problems where standard dynamic\nprogramming runs in 2^{O(k log k)} n steps. Our approach combines tools from\ntopological graph theory and analytic combinatorics. In particular, we\nintroduce a new type of branch decomposition called \"surface cut\ndecomposition\", generalizing sphere cut decompositions of planar graphs\nintroduced by Seymour and Thomas, which has nice combinatorial properties.\nNamely, the number of partial solutions that can be arranged on a surface cut\ndecomposition can be upper-bounded by the number of non-crossing partitions on\nsurfaces with boundary. It follows that partial solutions can be represented by\na single-exponential (in the branchwidth k) number of configurations. This\nproves that, when applied on surface cut decompositions, dynamic programming\nruns in 2^{O(k)} n steps. That way, we considerably extend the class of\nproblems that can be solved in running times with a single-exponential\ndependence on branchwidth and unify/improve most previous results in this\ndirection. \n\n"}
{"id": "1104.4597", "contents": "Title: The Entropy Rounding Method in Approximation Algorithms Abstract: Let A be a matrix, c be any linear objective function and x be a fractional\nvector, say an LP solution to some discrete optimization problem. Then a\nrecurring task in theoretical computer science (and in approximation algorithms\nin particular) is to obtain an integral vector y such that Ax is roughly Ay and\nc*y exceeds c*x by only a moderate factor.\n  We give a new randomized rounding procedure for this task, provided that A\nhas bounded Delta-approximate entropy. This property means that for uniformly\nchosen random signs chi(j) in {-1,+1} on any subset of the columns, the outcome\nA*chi can be approximately described using a sub-linear number of bits in\nexpectation.\n  To achieve this result, we modify well-known techniques from the field of\ndiscrepancy theory, especially we rely on Beck's entropy method, which to the\nbest of our knowledge has never been used before in the context of\napproximation algorithms. Our result can be made constructive using the Bansal\nframework based on semidefinite programming.\n  We demonstrate the versatility of our procedure by rounding fractional\nsolutions to column-based linear programs for some generalizations of Bin\nPacking. For example we obtain a polynomial time OPT + O(log^2 OPT)\napproximation for Bin Packing With Rejection and the first AFPTAS for the Train\nDelivery problem. \n\n"}
{"id": "1104.4746", "contents": "Title: Lasserre Hierarchy, Higher Eigenvalues, and Approximation Schemes for\n  Quadratic Integer Programming with PSD Objectives Abstract: We present an approximation scheme for optimizing certain Quadratic Integer\nProgramming problems with positive semidefinite objective functions and global\nlinear constraints. This framework includes well known graph problems such as\nMinimum graph bisection, Edge expansion, Uniform sparsest cut, and Small Set\nexpansion, as well as the Unique Games problem. These problems are notorious\nfor the existence of huge gaps between the known algorithmic results and\nNP-hardness results. Our algorithm is based on rounding semidefinite programs\nfrom the Lasserre hierarchy, and the analysis uses bounds for low-rank\napproximations of a matrix in Frobenius norm using columns of the matrix.\n  For all the above graph problems, we give an algorithm running in time\n$n^{O(r/\\epsilon^2)}$ with approximation ratio\n$\\frac{1+\\epsilon}{\\min\\{1,\\lambda_r\\}}$, where $\\lambda_r$ is the $r$'th\nsmallest eigenvalue of the normalized graph Laplacian $\\mathcal{L}$. In the\ncase of graph bisection and small set expansion, the number of vertices in the\ncut is within lower-order terms of the stipulated bound. Our results imply\n$(1+O(\\epsilon))$ factor approximation in time $n^{O(r^\\ast/\\epsilon^2)}$ where\n$r^\\ast$ is the number of eigenvalues of $\\mathcal{L}$ smaller than\n$1-\\epsilon$.\n  For Unique Games, we give a factor $(1+\\frac{2+\\epsilon}{\\lambda_r})$\napproximation for minimizing the number of unsatisfied constraints in\n$n^{O(r/\\epsilon)}$ time. This improves an earlier bound for solving Unique\nGames on expanders, and also shows that Lasserre SDPs are powerful enough to\nsolve well-known integrality gap instances for the basic SDP.\n  We also give an algorithm for independent sets in graphs that performs well\nwhen the Laplacian does not have too many eigenvalues bigger than $1+o(1)$. \n\n"}
{"id": "1104.5226", "contents": "Title: Parallelism and Time in Hierarchical Self-Assembly Abstract: We study the role that parallelism plays in time complexity of Winfree's\nabstract Tile Assembly Model (aTAM), a model of molecular algorithmic\nself-assembly. In the \"hierarchical\" aTAM, two assemblies, both consisting of\nmultiple tiles, are allowed to aggregate together, whereas in the \"seeded\"\naTAM, tiles attach one at a time to a growing assembly. Adleman, Cheng, Goel,\nand Huang (\"Running Time and Program Size for Self-Assembled Squares\", STOC\n2001) showed how to assemble an n x n square in O(n) time in the seeded aTAM\nusing O(log n / log log n) unique tile types, where both of these parameters\nare optimal. They asked whether the hierarchical aTAM could allow a tile system\nto use the ability to form large assemblies in parallel before they attach to\nbreak the Omega(n) lower bound for assembly time. We show that there is a tile\nsystem with the optimal O(log n / log log n) tile types that assembles an n x n\nsquare using O(log^2 n) parallel \"stages\", which is close to the optimal\nOmega(log n) stages, forming the final n x n square from four n/2 x n/2\nsquares, which are themselves recursively formed from n/4 x n/4 squares, etc.\nHowever, despite this nearly maximal parallelism, the system requires\nsuperlinear time to assemble the square. We extend the definition of *partial\norder tile systems* studied by Adleman et al. in a natural way to hierarchical\nassembly and show that no hierarchical partial order tile system can build any\nshape with diameter N in less than time Omega(N), demonstrating that in this\ncase the hierarchical model affords no speedup whatsoever over the seeded\nmodel. We strengthen the Omega(N) time lower bound for deterministic seeded\nsystems of Adleman et al. to nondeterministic seeded systems. Finally, we show\nthat for infinitely many n, a tile system can assemble an n x n' rectangle,\nwith n > n', in time O(n^{4/5} log n), breaking the linear-time lower bound. \n\n"}
{"id": "1105.2769", "contents": "Title: Multibody Multipole Methods Abstract: A three-body potential function can account for interactions among triples of\nparticles which are uncaptured by pairwise interaction functions such as\nCoulombic or Lennard-Jones potentials. Likewise, a multibody potential of order\n$n$ can account for interactions among $n$-tuples of particles uncaptured by\ninteraction functions of lower orders. To date, the computation of multibody\npotential functions for a large number of particles has not been possible due\nto its $O(N^n)$ scaling cost. In this paper we describe a fast tree-code for\nefficiently approximating multibody potentials that can be factorized as\nproducts of functions of pairwise distances. For the first time, we show how to\nderive a Barnes-Hut type algorithm for handling interactions among more than\ntwo particles. Our algorithm uses two approximation schemes: 1) a deterministic\nseries expansion-based method; 2) a Monte Carlo-based approximation based on\nthe central limit theorem. Our approach guarantees a user-specified bound on\nthe absolute or relative error in the computed potential with an asymptotic\nprobability guarantee. We provide speedup results on a three-body dispersion\npotential, the Axilrod-Teller potential. \n\n"}
{"id": "1105.3201", "contents": "Title: An exact tensor network for the 3SAT problem Abstract: We construct a tensor network that delivers an unnormalized quantum state\nwhose coefficients are the solutions to a given instance of 3SAT, an\nNP-complete problem. The tensor network contraction that corresponds to the\nnorm of the state counts the number of solutions to the instance. It follows\nthat exact contractions of this tensor network are in the #P-complete\ncomputational complexity class, thus believed to be a hard task. Furthermore,\nwe show that for a 3SAT instance with n bits, it is enough to perform a\npolynomial number of contractions of the tensor network structure associated to\nthe computation of local observables to obtain one of the explicit solutions to\nthe problem, if any. Physical realization of a state described by a generic\ntensor network is equivalent to finding the satisfying assignment of a 3SAT\ninstance and, consequently, this experimental task is expected to be hard. \n\n"}
{"id": "1105.5032", "contents": "Title: The Complexity of Manipulative Attacks in Nearly Single-Peaked\n  Electorates Abstract: Many electoral bribery, control, and manipulation problems (which we will\nrefer to in general as \"manipulative actions\" problems) are NP-hard in the\ngeneral case. It has recently been noted that many of these problems fall into\npolynomial time if the electorate is single-peaked (i.e., is polarized along\nsome axis/issue). However, real-world electorates are not truly single-peaked.\nThere are usually some mavericks, and so real-world electorates tend to merely\nbe nearly single-peaked. This paper studies the complexity of\nmanipulative-action algorithms for elections over nearly single-peaked\nelectorates, for various notions of nearness and various election systems. We\nprovide instances where even one maverick jumps the manipulative-action\ncomplexity up to $\\np$-hardness, but we also provide many instances where a\nreasonable number of mavericks can be tolerated without increasing the\nmanipulative-action complexity. \n\n"}
{"id": "1106.0436", "contents": "Title: Linear-algebraic list decoding of folded Reed-Solomon codes Abstract: Folded Reed-Solomon codes are an explicit family of codes that achieve the\noptimal trade-off between rate and error-correction capability: specifically,\nfor any $\\eps > 0$, the author and Rudra (2006,08) presented an $n^{O(1/\\eps)}$\ntime algorithm to list decode appropriate folded RS codes of rate $R$ from a\nfraction $1-R-\\eps$ of errors. The algorithm is based on multivariate\npolynomial interpolation and root-finding over extension fields. It was noted\nby Vadhan that interpolating a linear polynomial suffices if one settles for a\nsmaller decoding radius (but still enough for a statement of the above form).\nHere we give a simple linear-algebra based analysis of this variant that\neliminates the need for the computationally expensive root-finding step over\nextension fields (and indeed any mention of extension fields). The entire list\ndecoding algorithm is linear-algebraic, solving one linear system for the\ninterpolation step, and another linear system to find a small subspace of\ncandidate solutions. Except for the step of pruning this subspace, the\nalgorithm can be implemented to run in {\\em quadratic} time. The theoretical\ndrawback of folded RS codes are that both the decoding complexity and proven\nworst-case list-size bound are $n^{\\Omega(1/\\eps)}$. By combining the above\nidea with a pseudorandom subset of all polynomials as messages, we get a Monte\nCarlo construction achieving a list size bound of $O(1/\\eps^2)$ which is quite\nclose to the existential $O(1/\\eps)$ bound (however, the decoding complexity\nremains $n^{\\Omega(1/\\eps)}$). Our work highlights that constructing an\nexplicit {\\em subspace-evasive} subset that has small intersection with\nlow-dimensional subspaces could lead to explicit codes with better\nlist-decoding guarantees. \n\n"}
{"id": "1106.2122", "contents": "Title: Parameterized complexity results for 1-safe Petri nets Abstract: We associate a graph with a 1-safe Petri net and study the parameterized\ncomplexity of various problems with parameters derived from the graph. With\ntreewidth as the parameter, we give W[1]-hardness results for many problems\nabout 1-safe Petri nets. As a corollary, this proves a conjecture of Downey et.\nal. about the hardness of some graph pebbling problems. We consider the\nparameter benefit depth (that is known to be helpful in getting better\nalgorithms for general Petri nets) and again give W[1]-hardness results for\nvarious problems on 1-safe Petri nets. We also consider the stronger parameter\nvertex cover number. Combining the well known automata-theoretic method and a\npowerful fixed parameter tractability (FPT) result about Integer Linear\nProgramming, we give a FPT algorithm for model checking Monadic Second Order\n(MSO) formulas on 1-safe Petri nets, with parameters vertex cover number and\nthe size of the formula. \n\n"}
{"id": "1106.4649", "contents": "Title: Space-Efficient Data-Analysis Queries on Grids Abstract: We consider various data-analysis queries on two-dimensional points. We give\nnew space/time tradeoffs over previous work on geometric queries such as\ndominance and rectangle visibility, and on semigroup and group queries such as\nsum, average, variance, minimum and maximum. We also introduce new solutions to\nqueries less frequently considered in the literature such as two-dimensional\nquantiles, majorities, successor/predecessor, mode, and various top-$k$\nqueries, considering static and dynamic scenarios. \n\n"}
{"id": "1107.2188", "contents": "Title: The Simulated Greedy Algorithm for Several Submodular Matroid Secretary\n  Problems Abstract: We study the matroid secretary problems with submodular valuation functions.\nIn these problems, the elements arrive in random order. When one element\narrives, we have to make an immediate and irrevocable decision on whether to\naccept it or not. The set of accepted elements must form an {\\em independent\nset} in a predefined matroid. Our objective is to maximize the value of the\naccepted elements. In this paper, we focus on the case that the valuation\nfunction is a non-negative and monotonically non-decreasing submodular\nfunction.\n  We introduce a general algorithm for such {\\em submodular matroid secretary\nproblems}. In particular, we obtain constant competitive algorithms for the\ncases of laminar matroids and transversal matroids. Our algorithms can be\nfurther applied to any independent set system defined by the intersection of a\n{\\em constant} number of laminar matroids, while still achieving constant\ncompetitive ratios. Notice that laminar matroids generalize uniform matroids\nand partition matroids.\n  On the other hand, when the underlying valuation function is linear, our\nalgorithm achieves a competitive ratio of 9.6 for laminar matroids, which\nsignificantly improves the previous result. \n\n"}
{"id": "1107.2722", "contents": "Title: On the Feasibility of Maintenance Algorithms in Dynamic Graphs Abstract: Near ubiquitous mobile computing has led to intense interest in dynamic graph\ntheory. This provides a new and challenging setting for algorithmics and\ncomplexity theory. For any graph-based problem, the rapid evolution of a\n(possibly disconnected) graph over time naturally leads to the important\ncomplexity question: is it better to calculate a new solution from scratch or\nto adapt the known solution on the prior graph to quickly provide a solution of\nguaranteed quality for the changed graph?\n  In this paper, we demonstrate that the former is the best approach in some\ncases, but that there are cases where the latter is feasible. We prove that,\nunder certain conditions, hard problems cannot even be approximated in any\nreasonable complexity bound --- i.e., even with a large amount of time, having\na solution to a very similar graph does not help in computing a solution to the\ncurrent graph. To achieve this, we formalize the idea as a maintenance\nalgorithm. Using r-Regular Subgraph as the primary example we show that\nW[1]-hardness for the parameterized approximation problem implies the\nnon-existence of a maintenance algorithm for the given approximation ratio.\nConversely we show that Vertex Cover, which is fixed-parameter tractable, has a\n2-approximate maintenance algorithm. The implications of NP-hardness and\nNPO-hardness are also explored. \n\n"}
{"id": "1108.2290", "contents": "Title: Dimension reduction for finite trees in L_1 Abstract: We show that every n-point tree metric admits a (1+eps)-embedding into a\nC(eps) log n-dimensional L_1 space, for every eps > 0, where C(eps) =\nO((1/eps)^4 log(1/eps)). This matches the natural volume lower bound up to a\nfactor depending only on eps. Previously, it was unknown whether even complete\nbinary trees on n nodes could be embedded in O(log n) dimensions with O(1)\ndistortion. For complete d-ary trees, our construction achieves C(eps) =\nO(1/eps^2). \n\n"}
{"id": "1108.2464", "contents": "Title: Grothendieck-type inequalities in combinatorial optimization Abstract: We survey connections of the Grothendieck inequality and its variants to\ncombinatorial optimization and computational complexity. \n\n"}
{"id": "1109.0964", "contents": "Title: On quantum interactive proofs with short messages Abstract: This paper proves one of the open problem posed by Beigi et al. in\narXiv:1004.0411v2. We consider quantum interactive proof systems where in the\nbeginning the verifier and prover send messages to each other with the combined\nlength of all messages being at most logarithmic (in the input length); and at\nthe end the prover sends a polynomial-length message to the verifier. We show\nthat this class has the same expressive power as QMA. \n\n"}
{"id": "1110.4765", "contents": "Title: Finding small separators in linear time via treewidth reduction Abstract: We present a method for reducing the treewidth of a graph while preserving\nall of its minimal $s-t$ separators up to a certain fixed size $k$. This\ntechnique allows us to solve $s-t$ Cut and Multicut problems with various\nadditional restrictions (e.g., the vertices being removed from the graph form\nan independent set or induce a connected graph) in linear time for every fixed\nnumber $k$ of removed vertices.\n  Our results have applications for problems that are not directly defined by\nseparators, but the known solution methods depend on some variant of\nseparation. for example, we can solve similarly restricted generalizations of\nBipartization (delete at most $k$ vertices from $G$ to make it bipartite) in\nalmost linear time for every fixed number $k$ of removed vertices. These\nresults answer a number of open questions in the area of parameterized\ncomplexity. Furthermore, our technique turns out to be relevant for $(H,C,K)$-\nand $(H,C,\\le K)$-coloring problems as well, which are cardinality constrained\nvariants of the classical $H$-coloring problem. We make progress in the\nclassification of the parameterized complexity of these problems by identifying\nnew cases that can be solved in almost linear time for every fixed cardinality\nbound. \n\n"}
{"id": "1111.0897", "contents": "Title: Active Property Testing Abstract: One of the motivations for property testing of boolean functions is the idea\nthat testing can serve as a preprocessing step before learning. However, in\nmost machine learning applications, it is not possible to request for labels of\nfictitious examples constructed by the algorithm. Instead, the dominant query\nparadigm in applied machine learning, called active learning, is one where the\nalgorithm may query for labels, but only on points in a given polynomial-sized\n(unlabeled) sample, drawn from some underlying distribution D. In this work, we\nbring this well-studied model in learning to the domain of testing.\n  We show that for a number of important properties, testing can still yield\nsubstantial benefits in this setting. This includes testing unions of\nintervals, testing linear separators, and testing various assumptions used in\nsemi-supervised learning. In addition to these specific results, we also\ndevelop a general notion of the testing dimension of a given property with\nrespect to a given distribution. We show this dimension characterizes (up to\nconstant factors) the intrinsic number of label requests needed to test that\nproperty. We develop such notions for both the active and passive testing\nmodels. We then use these dimensions to prove a number of lower bounds,\nincluding for linear separators and the class of dictator functions.\n  Our results show that testing can be a powerful tool in realistic models for\nlearning, and further that active testing exhibits an interesting and rich\nstructure. Our work in addition brings together tools from a range of areas\nincluding U-statistics, noise-sensitivity, self-correction, and spectral\nanalysis of random matrices, and develops new tools that may be of independent\ninterest. \n\n"}
{"id": "1111.2109", "contents": "Title: A Flow-dependent Quadratic Steiner Tree Problem in the Euclidean Plane Abstract: We introduce a flow-dependent version of the quadratic Steiner tree problem\nin the plane. An instance of the problem on a set of embedded sources and a\nsink asks for a directed tree $T$ spanning these nodes and a bounded number of\nSteiner points, such that $\\displaystyle\\sum_{e \\in E(T)}f(e)|e|^2$ is a\nminimum, where $f(e)$ is the flow on edge $e$. The edges are uncapacitated and\nthe flows are determined additively, i.e., the flow on an edge leaving a node\n$u$ will be the sum of the flows on all edges entering $u$. Our motivation for\nstudying this problem is its utility as a model for relay augmentation of\nwireless sensor networks. In these scenarios one seeks to optimise power\nconsumption -- which is predominantly due to communication and, in free space,\nis proportional to the square of transmission distance -- in the network by\nintroducing additional relays. We prove several geometric and combinatorial\nresults on the structure of optimal and locally optimal solution-trees (under\nvarious strategies for bounding the number of Steiner points) and describe a\ngeometric linear-time algorithm for constructing such trees with known\ntopologies. \n\n"}
{"id": "1111.2127", "contents": "Title: Monotone switching networks for directed connectivity are strictly more\n  powerful than certain-knowledge switching networks Abstract: L (Logarithmic space) versus NL (Non-deterministic logarithmic space) is one\nof the great open problems in computational complexity theory. In the paper\n\"Bounds on monotone switching networks for directed connectivity\", we separated\nmonotone analogues of L and NL using a model called the switching network\nmodel. In particular, by considering inputs consisting of just a path and\nisolated vertices, we proved that any monotone switching network solving\ndirected connectivity on $N$ vertices must have size at least\n$N^{\\Omega(\\lg(N))}$ and this bound is tight. If we could show a similar result\nfor general switching networks solving directed connectivity, then this would\nprove that $L \\neq NL$. However, proving lower bounds for general switching\nnetworks solving directed connectivity requires proving stronger lower bounds\non monotone switching networks for directed connectivity. To work towards this\ngoal, we investigated a different set of inputs which we believed to be hard\nfor monotone switching networks to solve and attempted to prove similar lower\nsize bounds. Instead, we found that this set of inputs is actually easy for\nmonotone switching networks for directed connectivity to solve, yet if we\nrestrict ourselves to certain-knowledge switching networks, which are a simple\nand intuitive subclass of monotone switching networks for directed\nconnectivity, then these inputs are indeed hard to solve. In this paper, we\ngive this set of inputs, demonstrate a \"weird\" polynomially-sized monotone\nswitching network for directed connectivity which solves this set of inputs,\nand prove that no polynomially-sized certain-knowledge switching network can\nsolve this set of inputs, thus proving that monotone switching networks for\ndirected connectivity are strictly more powerful than certain-knowledge\nswitching networks. \n\n"}
{"id": "1111.5247", "contents": "Title: The Complexity of the Separable Hamiltonian Problem Abstract: In this paper, we study variants of the canonical Local-Hamiltonian problem\nwhere, in addition, the witness is promised to be separable. We define two\nvariants of the Local-Hamiltonian problem. The input for the\nSeparable-Local-Hamiltonian problem is the same as the Local-Hamiltonian\nproblem, i.e. a local Hamiltonian and two energies a and b, but the question is\nsomewhat different: the answer is YES if there is a separable quantum state\nwith energy at most a, and the answer is NO if all separable quantum states\nhave energy at least b. The Separable-Sparse-Hamiltonian problem is defined\nsimilarly, but the Hamiltonian is not necessarily local, but rather sparse. We\nshow that the Separable-Sparse-Hamiltonian problem is QMA(2)-Complete, while\nSeparable-Local-Hamiltonian is in QMA. This should be compared to the\nLocal-Hamiltonian problem, and the Sparse-Hamiltonian problem which are both\nQMA-Complete. To the best of our knowledge, Separable-SPARSE-Hamiltonian is the\nfirst non-trivial problem shown to be QMA(2)-Complete. \n\n"}
{"id": "1111.5979", "contents": "Title: Erd\\H{o}s-Szekeres and Testing Weak epsilon-Nets are NP-hard in 3\n  dimensions - and what now? Abstract: We consider the computational versions of the Erd\\H os-Szekeres theorem and\nrelated problems in 3 dimensions. We show that, in constrast to the planar\ncase, no polynomial time algorithm exists for determining the largest (empty)\nconvex subset (unless P=NP) among a set of points, by proving that the\ncorresponding decision problem is NP-hard. This answers a question by Dobkin,\nEdelsbrunner and Overmars from 1990.\n  As a corollary, we derive a similar result for the closely related problem of\ntesting weak epsilon-nets in R^3. Answering a question by Chazelle et al. from\n1995, our reduction shows that the problem is co-NP-hard.\n  This is work in progress - we are still trying to find a smart approximation\nalgorithm for the problems. \n\n"}
{"id": "1111.6321", "contents": "Title: Shape and Trajectory Tracking of Moving Obstacles Abstract: This work presents new methods and algorithms for tracking the shape and\ntrajectory of moving reflecting obstacles with broken rays, or rays reflecting\nat an obstacle. While in tomography the focus of the reconstruction method is\nto recover the velocity structure of the domain, the shape and trajectory\nreconstruction procedure directly finds the shape and trajectory of the\nobstacle. The physical signal carrier for this innovative method are ultrasonic\nbeams. When the speed of sound is constant, the rays are straight line segments\nand the shape and trajectory of moving objects will be reconstructed with\nmethods based on the travel time equation and ellipsoid geometry. For variable\nspeed of sound, we start with the eikonal equation and a system of differential\nequations that has its origins in acoustics and seismology. In this case, the\nrays are curves that are not necessarily straight line segments and we develop\nalgorithms for shape and trajectory tracking based on the numerical solution of\nthese equations. We present methods and algorithms for shape and trajectory\ntracking of moving obstacles with reflected rays when the location of the\nreceiver of the reflected ray is not known in advance. The shape and trajectory\ntracking method is very efficient because it is not necessary for the reflected\nsignal to traverse the whole domain or the same path back to the transmitter.\nIt could be received close to the point of reflection or far away from the\ntransmitter. This optimizes the energy spent by transmitters for tracking the\nobject, reduces signal attenuation and improves image resolution. It is a safe\nand secure method. We also present algorithms for tracking the shape and\ntrajectory of absorbing obstacles. The new methods and algorithms for shape and\ntrajectory tracking enable new applications and an application to one-hop\nInternet routing is presented. \n\n"}
{"id": "1112.2000", "contents": "Title: A discrepancy lower bound for information complexity Abstract: This paper provides the first general technique for proving information lower\nbounds on two-party unbounded-rounds communication problems. We show that the\ndiscrepancy lower bound, which applies to randomized communication complexity,\nalso applies to information complexity. More precisely, if the discrepancy of a\ntwo-party function $f$ with respect to a distribution $\\mu$ is $Disc_\\mu f$,\nthen any two party randomized protocol computing $f$ must reveal at least\n$\\Omega(\\log (1/Disc_\\mu f))$ bits of information to the participants. As a\ncorollary, we obtain that any two-party protocol for computing a random\nfunction on $\\{0,1\\}^n\\times\\{0,1\\}^n$ must reveal $\\Omega(n)$ bits of\ninformation to the participants.\n  In addition, we prove that the discrepancy of the Greater-Than function is\n$\\Omega(1/\\sqrt{n})$, which provides an alternative proof to the recent proof\nof Viola \\cite{Viola11} of the $\\Omega(\\log n)$ lower bound on the\ncommunication complexity of this well-studied function and, combined with our\nmain result, proves the tight $\\Omega(\\log n)$ lower bound on its information\ncomplexity.\n  The proof of our main result develops a new simulation procedure that may be\nof an independent interest. In a very recent breakthrough work of Kerenidis et\nal. \\cite{kerenidis2012lower}, this simulation procedure was the main building\nblock for proving that almost all known lower bound techniques for\ncommunication complexity (and not just discrepancy) apply to information\ncomplexity. \n\n"}
{"id": "1112.3337", "contents": "Title: Search by quantum walks on two-dimensional grid without amplitude\n  amplification Abstract: We study search by quantum walk on a finite two dimensional grid. The\nalgorithm of Ambainis, Kempe, Rivosh (quant-ph/0402107) takes O(\\sqrt{N log N})\nsteps and finds a marked location with probability O(1/log N) for grid of size\n\\sqrt{N} * \\sqrt{N}. This probability is small, thus amplitude amplification is\nneeded to achieve \\Theta(1) success probability. The amplitude amplification\nadds an additional O(\\sqrt{log N}) factor to the number of steps, making it\nO(\\sqrt{N} log N).\n  In this paper, we show that despite a small probability to find a marked\nlocation, the probability to be within an O(\\sqrt{N}) neighbourhood (at an\nO(\\sqrt[4]{N}) distance) of the marked location is \\Theta(1). This allows to\nskip amplitude amplification step and leads to an O(\\sqrt{log N}) speed-up.\n  We describe the results of numerical experiments supporting this idea, and we\nprove this fact analytically. \n\n"}
{"id": "1112.3765", "contents": "Title: The Efficiency of MapReduce in Parallel External Memory Abstract: Since its introduction in 2004, the MapReduce framework has become one of the\nstandard approaches in massive distributed and parallel computation. In\ncontrast to its intensive use in practise, theoretical footing is still limited\nand only little work has been done yet to put MapReduce on a par with the major\ncomputational models. Following pioneer work that relates the MapReduce\nframework with PRAM and BSP in their macroscopic structure, we focus on the\nfunctionality provided by the framework itself, considered in the parallel\nexternal memory model (PEM). In this, we present upper and lower bounds on the\nparallel I/O-complexity that are matching up to constant factors for the\nshuffle step. The shuffle step is the single communication phase where all\ninformation of one MapReduce invocation gets transferred from map workers to\nreduce workers. Hence, we move the focus towards the internal communication\nstep in contrast to previous work. The results we obtain further carry over to\nthe BSP* model. On the one hand, this shows how much complexity can be \"hidden\"\nfor an algorithm expressed in MapReduce compared to PEM. On the other hand, our\nresults bound the worst-case performance loss of the MapReduce approach in\nterms of I/O-efficiency. \n\n"}
{"id": "1201.0253", "contents": "Title: A Lower Bound for Estimating High Moments of a Data Stream Abstract: We show an improved lower bound for the Fp estimation problem in a data\nstream setting for p>2. A data stream is a sequence of items from the domain\n[n] with possible repetitions. The frequency vector x is an n-dimensional\nnon-negative integer vector x such that x(i) is the number of occurrences of i\nin the sequence. Given an accuracy parameter Omega(n^{-1/p}) < \\epsilon < 1,\nthe problem of estimating Fp is to estimate \\norm{x}_p^p = \\sum_{i \\in [n]}\n\\abs{x(i)}^p correctly to within a relative accuracy of 1\\pm \\epsilon with high\nconstant probability in an online fashion and using as little space as\npossible. The current space lower bound for this problem is Omega(n^{1-2/p}\n\\epsilon^{-2/p}+ n^{1-2/p}\\epsilon^{-4/p}/ \\log^{O(1)}(n)+ (\\epsilon^{-2} +\n\\log (n))). The first term in the lower bound expression was proved in\n\\cite{B-YJKS:stoc02,cks:ccc03}, the second in \\cite{wz:arxiv11} and the third\nin \\cite{wood:soda04}. In this note, we show an Omega(p^2 n^{1-2/p}\n\\epsilon^{-2}/\\log (n)) bits space bound, for Omega(pn^{-1/p}) \\le \\epsilon \\le\n1/10. \n\n"}
{"id": "1201.0749", "contents": "Title: There is no 16-Clue Sudoku: Solving the Sudoku Minimum Number of Clues\n  Problem Abstract: The sudoku minimum number of clues problem is the following question: what is\nthe smallest number of clues that a sudoku puzzle can have? For several years\nit had been conjectured that the answer is 17. We have performed an exhaustive\ncomputer search for 16-clue sudoku puzzles, and did not find any, thus proving\nthat the answer is indeed 17. In this article we describe our method and the\nactual search. As a part of this project we developed a novel way for\nenumerating hitting sets. The hitting set problem is computationally hard; it\nis one of Karp's 21 classic NP-complete problems. A standard backtracking\nalgorithm for finding hitting sets would not be fast enough to search for a\n16-clue sudoku puzzle exhaustively, even at today's supercomputer speeds. To\nmake an exhaustive search possible, we designed an algorithm that allowed us to\nefficiently enumerate hitting sets of a suitable size. \n\n"}
{"id": "1201.2892", "contents": "Title: Algebraic Relaxations and Hardness Results in Polynomial Optimization\n  and Lyapunov Analysis Abstract: This thesis settles a number of questions related to computational complexity\nand algebraic, semidefinite programming based relaxations in optimization and\ncontrol. \n\n"}
{"id": "1201.3318", "contents": "Title: Notes on Bit-reversal Broadcast Scheduling Abstract: This report contains revision and extension of some results about RBO\n[arXiv:1108.5095]. RBO is a simple and efficient broadcast scheduling of $n =\n2^k$ uniform frames for battery powered radio receivers. Each frame contains a\nkey from some arbitrary linearly ordered universe. The broadcast cycle -- a\nsequence of frames sorted by the keys and permuted by $k$-bit reversal -- is\ntransmitted in a round robin fashion by the broadcaster. At arbitrary time\nduring the transmission, the receiver may start a simple protocol that reports\nto him all the frames with the keys that are contained in a specified interval\nof the key values $[K', K\"]$. RBO receives at most $2 k + 1$ other frames' keys\nbefore receiving the first key from $[K', K\"]$ or noticing that there are no\nsuch keys in the broadcast cycle. As a simple corollary, $4 k + 2$ is upper\nbound the number of keys outside $[K', K\"]$ that will ever be received. In\nunreliable network the expected number of efforts to receive such frames is\nbounded by $(8 k + 4) / p + 2 (1 - p) / p^2$, where $p$ is probability of\nsuccessful reception, and the reception rate of the requested frames is $p$ --\nthe highest possible. The receiver's protocol state consists of the values $k$,\n$K'$ and $K\"$, one wake-up timer and two other $k$-bit variables. Its only\nnontrivial computation -- the computation of the next wake-up time slot -- can\nbe performed in $O (k)$ simple operations, such as arithmetic/bit-wise\noperations on $k$-bit numbers, using only constant number of $k$-bit variables. \n\n"}
{"id": "1202.0313", "contents": "Title: The Complexity of Computing the Sign of the Tutte Polynomial Abstract: We study the complexity of computing the sign of the Tutte polynomial of a\ngraph. As there are only three possible outcomes (positive, negative, and\nzero), this seems at first sight more like a decision problem than a counting\nproblem. Surprisingly, however, there are large regions of the parameter space\nfor which computing the sign of the Tutte polynomial is actually #P-hard. As a\ntrivial consequence, approximating the polynomial is also #P-hard in this case.\nThus, approximately evaluating the Tutte polynomial in these regions is as hard\nas exactly counting the satisfying assignments to a CNF Boolean formula. For\nmost other points in the parameter space, we show that computing the sign of\nthe polynomial is in FP, whereas approximating the polynomial can be done in\npolynomial time with an NP oracle. As a special case, we completely resolve the\ncomplexity of computing the sign of the chromatic polynomial - this is easily\ncomputable at q=2 and when q is less than or equal to 32/27, and is NP-hard to\ncompute for all other values of the parameter q. \n\n"}
{"id": "1202.0535", "contents": "Title: List decoding subspace codes from insertions and deletions Abstract: We present a construction of subspace codes along with an efficient algorithm\nfor list decoding from both insertions and deletions, handling an\ninformation-theoretically maximum fraction of these with polynomially small\nrate. Our construction is based on a variant of the folded Reed-Solomon codes\nin the world of linearized polynomials, and the algorithm is inspired by the\nrecent linear-algebraic approach to list decoding. Ours is the first list\ndecoding algorithm for subspace codes that can handle deletions; even one\ndeletion can totally distort the structure of the basis of a subspace and is\nthus challenging to handle. When there are only insertions, we also present\nresults for list decoding subspace codes that are the linearized analog of\nReed-Solomon codes (proposed previously, and closely related to the Gabidulin\ncodes for rank-metric), obtaining some improvements over similar results in\nprevious work. \n\n"}
{"id": "1202.1370", "contents": "Title: On a functional contraction method Abstract: Methods for proving functional limit laws are developed for sequences of\nstochastic processes which allow a recursive distributional decomposition\neither in time or space. Our approach is an extension of the so-called\ncontraction method to the space $\\mathcal{C}[0,1]$ of continuous functions\nendowed with uniform topology and the space $\\mathcal {D}[0,1]$ of\nc\\`{a}dl\\`{a}g functions with the Skorokhod topology. The contraction method\noriginated from the probabilistic analysis of algorithms and random trees where\ncharacteristics satisfy natural distributional recurrences. It is based on\nstochastic fixed-point equations, where probability metrics can be used to\nobtain contraction properties and allow the application of Banach's fixed-point\ntheorem. We develop the use of the Zolotarev metrics on the spaces\n$\\mathcal{C}[0,1]$ and $\\mathcal{D}[0,1]$ in this context. Applications are\ngiven, in particular, a short proof of Donsker's functional limit theorem is\nderived and recurrences arising in the probabilistic analysis of algorithms are\ndiscussed. \n\n"}
{"id": "1202.3505", "contents": "Title: Near-optimal Coresets For Least-Squares Regression Abstract: We study (constrained) least-squares regression as well as multiple response\nleast-squares regression and ask the question of whether a subset of the data,\na coreset, suffices to compute a good approximate solution to the regression.\nWe give deterministic, low order polynomial-time algorithms to construct such\ncoresets with approximation guarantees, together with lower bounds indicating\nthat there is not much room for improvement upon our results. \n\n"}
{"id": "1202.4665", "contents": "Title: Algorithms and Almost Tight Results for 3-Colorability of Small Diameter\n  Graphs Abstract: In spite of the extensive studies of the 3-coloring problem with respect to\nseveral basic parameters, the complexity status of the 3-coloring problem on\ngraphs with small diameter, i.e. with diameter 2 or 3, has been a longstanding\nand challenging open question. For graphs with diameter 2 we provide the first\nsubexponential algorithm with complexity $2^{O(\\sqrt{n\\log n})}$, which is\nasymptotically the same as the currently best known time complexity for the\ngraph isomorphism (GI) problem. Moreover, we prove that the graph isomorphism\nproblem on 3-colorable graphs with diameter 2 is GI-complete. Furthermore we\npresent a subclass of graphs with diameter 2 that admits a polynomial algorithm\nfor 3-coloring. For graphs with diameter 3 we establish the complexity of\n3-coloring by proving that for every $\\varepsilon \\in [0,1)$, 3-coloring is\nNP-complete on triangle-free graphs of diameter 3 and radius 2 with $n$\nvertices and minimum degree $\\delta=\\Theta(n^{\\varepsilon})$. Moreover,\nassuming ETH, we provide three different amplifications of our hardness results\nto obtain for every $\\varepsilon \\in [0,1)$ subexponential lower bounds for the\ncomplexity of 3-coloring on triangle-free graphs with diameter 3 and minimum\ndegree $\\delta=\\Theta(n^{\\varepsilon})$. Finally, we provide a 3-coloring\nalgorithm with running time\n$2^{O(\\min\\{\\delta\\Delta,\\frac{n}{\\delta}\\log\\delta\\})}$ for graphs with\ndiameter 3, where $\\delta$ (resp. $\\Delta $) is the minimum (resp. maximum)\ndegree of the input graph. To the best of our knowledge, this algorithm is the\nfirst subexponential algorithm for graphs with $\\delta=\\omega(1)$ and for\ngraphs with $\\delta=O(1)$ and $\\Delta=o(n)$. Due to the above lower bounds of\nthe complexity of 3-coloring, the running time of this algorithm is\nasymptotically almost tight when the minimum degree if the input graph is\n$\\delta=\\Theta(n^{\\varepsilon})$, where $\\varepsilon \\in [1/2,1)$. \n\n"}
{"id": "1202.5258", "contents": "Title: Explicit Optimal Hardness via Gaussian stability results Abstract: The results of Raghavendra (2008) show that assuming Khot's Unique Games\nConjecture (2002), for every constraint satisfaction problem there exists a\ngeneric semi-definite program that achieves the optimal approximation factor.\nThis result is existential as it does not provide an explicit optimal rounding\nprocedure nor does it allow to calculate exactly the Unique Games hardness of\nthe problem.\n  Obtaining an explicit optimal approximation scheme and the corresponding\napproximation factor is a difficult challenge for each specific approximation\nproblem. An approach for determining the exact approximation factor and the\ncorresponding optimal rounding was established in the analysis of MAX-CUT (KKMO\n2004) and the use of the Invariance Principle (MOO 2005). However, this\napproach crucially relies on results explicitly proving optimal partitions in\nGaussian space. Until recently, Borell's result (Borell 1985) was the only\nnon-trivial Gaussian partition result known.\n  In this paper we derive the first explicit optimal approximation algorithm\nand the corresponding approximation factor using a new result on Gaussian\npartitions due to Isaksson and Mossel (2012). This Gaussian result allows us to\ndetermine exactly the Unique Games Hardness of MAX-3-EQUAL. In particular, our\nresults show that Zwick algorithm for this problem achieves the optimal\napproximation factor and prove that the approximation achieved by the algorithm\nis $\\approx 0.796$ as conjectured by Zwick.\n  We further use the previously known optimal Gaussian partitions results to\nobtain a new Unique Games Hardness factor for MAX-k-CSP : Using the well known\nfact that jointly normal pairwise independent random variables are fully\nindependent, we show that the the UGC hardness of Max-k-CSP is $\\frac{\\lceil\n(k+1)/2 \\rceil}{2^{k-1}}$, improving on results of Austrin and Mossel (2009). \n\n"}
{"id": "1202.5749", "contents": "Title: Fixed-parameter tractability of multicut in directed acyclic graphs Abstract: The MULTICUT problem, given a graph G, a set of terminal pairs T={(s_i,t_i) |\n1 <= i <= r} and an integer p, asks whether one can find a cutset consisting of\nat most p non-terminal vertices that separates all the terminal pairs, i.e.,\nafter removing the cutset, t_i is not reachable from s_i for each 1 <= i <= r.\nThe fixed-parameter tractability of MULTICUT in undirected graphs,\nparameterized by the size of the cutset only, has been recently proven by Marx\nand Razgon (STOC'11) and, independently, by Bousquet et al. (STOC'11), after\nresisting attacks as a long-standing open problem. In this paper we prove that\nMULTICUT is fixed-parameter tractable on directed acyclic graphs, when\nparameterized both by the size of the cutset and the number of terminal pairs.\nWe complement this result by showing that this is implausible for\nparameterization by the size of the cutset only, as this version of the problem\nremains W[1]-hard. \n\n"}
{"id": "1203.0586", "contents": "Title: An Undecidable Nested Recurrence Relation Abstract: Roughly speaking, a recurrence relation is nested if it contains a\nsubexpression of the form ... A(...A(...)...). Many nested recurrence relations\noccur in the literature, and determining their behavior seems to be quite\ndifficult and highly dependent on their initial conditions. A nested recurrence\nrelation A(n) is said to be undecidable if the following problem is\nundecidable: given a finite set of initial conditions for A(n), is the\nrecurrence relation calculable? Here calculable means that for every n >= 0,\neither A(n) is an initial condition or the calculation of A(n) involves only\ninvocations of A on arguments in {0,1,...,n-1}. We show that the recurrence\nrelation A(n) = A(n-4-A(A(n-4)))+4A(A(n-4)) +A(2A(n-4-A(n-2))+A(n-2)). is\nundecidable by showing how it can be used, together with carefully chosen\ninitial conditions, to simulate Post 2-tag systems, a known Turing complete\nproblem. \n\n"}
{"id": "1203.0833", "contents": "Title: Faster Parameterized Algorithms using Linear Programming Abstract: We investigate the parameterized complexity of Vertex Cover parameterized by\nthe difference between the size of the optimal solution and the value of the\nlinear programming (LP) relaxation of the problem. By carefully analyzing the\nchange in the LP value in the branching steps, we argue that combining\npreviously known preprocessing rules with the most straightforward branching\nalgorithm yields an $O^*((2.618)^k)$ algorithm for the problem. Here $k$ is the\nexcess of the vertex cover size over the LP optimum, and we write $O^*(f(k))$\nfor a time complexity of the form $O(f(k)n^{O(1)})$, where $f (k)$ grows\nexponentially with $k$. We proceed to show that a more sophisticated branching\nalgorithm achieves a runtime of $O^*(2.3146^k)$.\n  Following this, using known and new reductions, we give $O^*(2.3146^k)$\nalgorithms for the parameterized versions of Above Guarantee Vertex Cover, Odd\nCycle Transversal, Split Vertex Deletion and Almost 2-SAT, and an\n$O^*(1.5214^k)$ algorithm for Ko\\\"nig Vertex Deletion, Vertex Cover Param by\nOCT and Vertex Cover Param by KVD. These algorithms significantly improve the\nbest known bounds for these problems. The most notable improvement is the new\nbound for Odd Cycle Transversal - this is the first algorithm which beats the\ndependence on $k$ of the seminal $O^*(3^k)$ algorithm of Reed, Smith and Vetta.\nFinally, using our algorithm, we obtain a kernel for the standard\nparameterization of Vertex Cover with at most $2k - c \\log k$ vertices. Our\nkernel is simpler than previously known kernels achieving the same size bound. \n\n"}
{"id": "1203.2538", "contents": "Title: Spanning trees and the complexity of flood-filling games Abstract: We consider problems related to the combinatorial game (Free-)Flood-It, in\nwhich players aim to make a coloured graph monochromatic with the minimum\npossible number of flooding operations. We show that the minimum number of\nmoves required to flood any given graph G is equal to the minimum, taken over\nall spanning trees T of G, of the number of moves required to flood T. This\nresult is then applied to give two polynomial-time algorithms for flood-filling\nproblems. Firstly, we can compute in polynomial time the minimum number of\nmoves required to flood a graph with only a polynomial number of connected\nsubgraphs. Secondly, given any coloured connected graph and a subset of the\nvertices of bounded size, the number of moves required to connect this subset\ncan be computed in polynomial time. \n\n"}
{"id": "1203.2801", "contents": "Title: On Exact Algorithms for Permutation CSP Abstract: In the Permutation Constraint Satisfaction Problem (Permutation CSP) we are\ngiven a set of variables $V$ and a set of constraints C, in which constraints\nare tuples of elements of V. The goal is to find a total ordering of the\nvariables, $\\pi\\ : V \\rightarrow [1,...,|V|]$, which satisfies as many\nconstraints as possible. A constraint $(v_1,v_2,...,v_k)$ is satisfied by an\nordering $\\pi$ when $\\pi(v_1)<\\pi(v_2)<...<\\pi(v_k)$. An instance has arity $k$\nif all the constraints involve at most $k$ elements.\n  This problem expresses a variety of permutation problems including {\\sc\nFeedback Arc Set} and {\\sc Betweenness} problems. A naive algorithm, listing\nall the $n!$ permutations, requires $2^{O(n\\log{n})}$ time. Interestingly, {\\sc\nPermutation CSP} for arity 2 or 3 can be solved by Held-Karp type algorithms in\ntime $O^*(2^n)$, but no algorithm is known for arity at least 4 with running\ntime significantly better than $2^{O(n\\log{n})}$. In this paper we resolve the\ngap by showing that {\\sc Arity 4 Permutation CSP} cannot be solved in time\n$2^{o(n\\log{n})}$ unless ETH fails. \n\n"}
{"id": "1203.3578", "contents": "Title: Iterative rounding approximation algorithms for degree-bounded\n  node-connectivity network design Abstract: We consider the problem of finding a minimum edge cost subgraph of a graph\nsatisfying both given node-connectivity requirements and degree upper bounds on\nnodes. We present an iterative rounding algorithm of the biset LP relaxation\nfor this problem. For directed graphs and $k$-out-connectivity requirements\nfrom a root, our algorithm computes a solution that is a 2-approximation on the\ncost, and the degree of each node $v$ in the solution is at most $2b(v) + O(k)$\nwhere $b(v)$ is the degree upper bound on $v$. For undirected graphs and\nelement-connectivity requirements with maximum connectivity requirement $k$,\nour algorithm computes a solution that is a $4$-approximation on the cost, and\nthe degree of each node $v$ in the solution is at most $4b(v)+O(k)$. These\nratios improve the previous $O(\\log k)$-approximation on the cost and $O(2^k\nb(v))$ approximation on the degrees. Our algorithms can be used to improve\napproximation ratios for other node-connectivity problems such as undirected\n$k$-out-connectivity, directed and undirected $k$-connectivity, and undirected\nrooted $k$-connectivity and subset $k$-connectivity. \n\n"}
{"id": "1203.3636", "contents": "Title: How to Attack the NP-complete Dag Realization Problem in Practice Abstract: We study the following fundamental realization problem of directed acyclic\ngraphs (dags). Given a sequence S:=(a_1,b_1),...,(a_n, b_n) with a_i, b_i in\nZ_0^+, does there exist a dag (no parallel arcs allowed) with labeled vertex\nset V:= {v_1,...,v_n} such that for all v_i in V indegree and outdegree of v_i\nmatch exactly the given numbers a_i and b_i, respectively? Recently this\ndecision problem has been shown to be NP-complete by Nichterlein (2011).\nHowever, we can show that several important classes of sequences are\nefficiently solvable.\n  In previous work (Berger and Mueller-Hannemann, FCT2011), we have proved that\nyes-instances always have a special kind of topological order which allows us\nto reduce the number of possible topological orderings in most cases\ndrastically. This leads to an exact exponential-time algorithm which\nsignificantly improves upon a straightforward approach. Moreover, a combination\nof this exponential-time algorithm with a special strategy gives a linear-time\nalgorithm. Interestingly, in systematic experiments we observed that we could\nsolve a huge majority of all instances by the linear-time heuristic. This\nmotivates us to develop characteristics like dag density and \"distance to\nprovably easy sequences\" which can give us an indicator how easy or difficult a\ngiven sequence can be realized.\n  Furthermore, we propose a randomized algorithm which exploits our structural\ninsight on topological sortings and uses a number of reduction rules. We\nobserve that it clearly outperforms all other variants and behaves surprisingly\nwell for almost all instances. Another striking observation is that our simple\nlinear-time algorithm solves a set of real-world instances from different\ndomains, namely ordered binary decision diagrams (OBDDs), train and flight\nschedules, as well as instances derived from food-web networks without any\nexception. \n\n"}
{"id": "1203.5927", "contents": "Title: Adaptive group testing as channel coding with feedback Abstract: Group testing is the combinatorial problem of identifying the defective items\nin a population by grouping items into test pools. Recently, nonadaptive group\ntesting - where all the test pools must be decided on at the start - has been\nstudied from an information theory point of view. Using techniques from channel\ncoding, upper and lower bounds have been given on the number of tests required\nto accurately recover the defective set, even when the test outcomes can be\nnoisy.\n  In this paper, we give the first information theoretic result on adaptive\ngroup testing - where the outcome of previous tests can influence the makeup of\nfuture tests. We show that adaptive testing does not help much, as the number\nof tests required obeys the same lower bound as nonadaptive testing. Our proof\nuses similar techniques to the proof that feedback does not improve channel\ncapacity. \n\n"}
{"id": "1204.0062", "contents": "Title: Improved matrix algorithms via the Subsampled Randomized Hadamard\n  Transform Abstract: Several recent randomized linear algebra algorithms rely upon fast dimension\nreduction methods. A popular choice is the Subsampled Randomized Hadamard\nTransform (SRHT). In this article, we address the efficacy, in the Frobenius\nand spectral norms, of an SRHT-based low-rank matrix approximation technique\nintroduced by Woolfe, Liberty, Rohklin, and Tygert. We establish a slightly\nbetter Frobenius norm error bound than currently available, and a much sharper\nspectral norm error bound (in the presence of reasonable decay of the singular\nvalues). Along the way, we produce several results on matrix operations with\nSRHTs (such as approximate matrix multiplication) that may be of independent\ninterest. Our approach builds upon Tropp's in \"Improved analysis of the\nSubsampled Randomized Hadamard Transform\". \n\n"}
{"id": "1204.2021", "contents": "Title: Approximating the Expansion Profile and Almost Optimal Local Graph\n  Clustering Abstract: Spectral partitioning is a simple, nearly-linear time, algorithm to find\nsparse cuts, and the Cheeger inequalities provide a worst-case guarantee for\nthe quality of the approximation found by the algorithm. Local graph\npartitioning algorithms [ST08,ACL06,AP09] run in time that is nearly linear in\nthe size of the output set, and their approximation guarantee is worse than the\nguarantee provided by the Cheeger inequalities by a polylogarithmic\n$\\log^{\\Omega(1)} n$ factor. It has been a long standing open problem to design\na local graph clustering algorithm with an approximation guarantee close to the\nguarantee of the Cheeger inequalities and with a running time nearly linear in\nthe size of the output.\n  In this paper we solve this problem; we design an algorithm with the same\nguarantee (up to a constant factor) as the Cheeger inequality, that runs in\ntime slightly super linear in the size of the output. This is the first\nsublinear (in the size of the input) time algorithm with almost the same\nguarantee as the Cheeger's inequality. As a byproduct of our results, we prove\na bicriteria approximation algorithm for the expansion profile of any graph.\nLet $\\phi(\\gamma) = \\min_{\\mu(S) \\leq \\gamma}\\phi(S)$. There is a polynomial\ntime algorithm that, for any $\\gamma,\\epsilon>0$, finds a set $S$ of measure\n$\\mu(S)\\leq 2\\gamma^{1+\\epsilon}$, and expansion $\\phi(S)\\leq\n\\sqrt{2\\phi(\\gamma)/\\epsilon}$. Our proof techniques also provide a simpler\nproof of the structural result of Arora, Barak, Steurer [ABS10], that can be\napplied to irregular graphs.\n  Our main technical tool is that for any set $S$ of vertices of a graph, a\nlazy $t$-step random walk started from a randomly chosen vertex of $S$, will\nremain entirely inside $S$ with probability at least $(1-\\phi(S)/2)^t$. This\nitself provides a new lower bound to the uniform mixing time of any finite\nstates reversible markov chain. \n\n"}
{"id": "1204.5613", "contents": "Title: Rerouting shortest paths in planar graphs Abstract: A rerouting sequence is a sequence of shortest st-paths such that consecutive\npaths differ in one vertex. We study the the Shortest Path Rerouting Problem,\nwhich asks, given two shortest st-paths P and Q in a graph G, whether a\nrerouting sequence exists from P to Q. This problem is PSPACE-hard in general,\nbut we show that it can be solved in polynomial time if G is planar. To this\nend, we introduce a dynamic programming method for reconfiguration problems. \n\n"}
{"id": "1204.5810", "contents": "Title: Geometry of Online Packing Linear Programs Abstract: We consider packing LP's with $m$ rows where all constraint coefficients are\nnormalized to be in the unit interval. The n columns arrive in random order and\nthe goal is to set the corresponding decision variables irrevocably when they\narrive so as to obtain a feasible solution maximizing the expected reward.\nPrevious (1 - \\epsilon)-competitive algorithms require the right-hand side of\nthe LP to be Omega((m/\\epsilon^2) log (n/\\epsilon)), a bound that worsens with\nthe number of columns and rows. However, the dependence on the number of\ncolumns is not required in the single-row case and known lower bounds for the\ngeneral case are also independent of n.\n  Our goal is to understand whether the dependence on n is required in the\nmulti-row case, making it fundamentally harder than the single-row version. We\nrefute this by exhibiting an algorithm which is (1 - \\epsilon)-competitive as\nlong as the right-hand sides are Omega((m^2/\\epsilon^2) log (m/\\epsilon)). Our\ntechniques refine previous PAC-learning based approaches which interpret the\nonline decisions as linear classifications of the columns based on sampled dual\nprices. The key ingredient of our improvement comes from a non-standard\ncovering argument together with the realization that only when the columns of\nthe LP belong to few 1-d subspaces we can obtain small such covers; bounding\nthe size of the cover constructed also relies on the geometry of linear\nclassifiers. General packing LP's are handled by perturbing the input columns,\nwhich can be seen as making the learning problem more robust. \n\n"}
{"id": "1204.5952", "contents": "Title: Clustering by hypergraphs and dimensionality of cluster systems Abstract: In the present paper we discuss the clustering procedure in the case where\ninstead of a single metric we have a family of metrics. In this case we can\nobtain a partially ordered graph of clusters which is not necessarily a tree.\nWe discuss a structure of a hypergraph above this graph. We propose two\ndefinitions of dimension for hyperedges of this hypergraph and show that for\nthe multidimensional p-adic case both dimensions are reduced to the number of\np-adic parameters.\n  We discuss the application of the hypergraph clustering procedure to the\nconstruction of phylogenetic graphs in biology. In this case the dimension of a\nhyperedge will describe the number of sources of genetic diversity. \n\n"}
{"id": "1204.6233", "contents": "Title: Strong Backdoors to Bounded Treewidth SAT Abstract: There are various approaches to exploiting \"hidden structure\" in instances of\nhard combinatorial problems to allow faster algorithms than for general\nunstructured or random instances. For SAT and its counting version #SAT, hidden\nstructure has been exploited in terms of decomposability and strong backdoor\nsets. Decomposability can be considered in terms of the treewidth of a graph\nthat is associated with the given CNF formula, for instance by considering\nclauses and variables as vertices of the graph, and making a variable adjacent\nwith all the clauses it appears in. On the other hand, a strong backdoor set of\na CNF formula is a set of variables such that each possible partial assignment\nto this set moves the formula into a fixed class for which (#)SAT can be solved\nin polynomial time.\n  In this paper we combine the two above approaches. In particular, we study\nthe algorithmic question of finding a small strong backdoor set into the class\nW_t of CNF formulas whose associated graphs have treewidth at most t. The main\nresults are positive:\n  (1) There is a cubic-time algorithm that, given a CNF formula F and two\nconstants k,t\\ge 0, either finds a strong W_t-backdoor set of size at most 2^k,\nor concludes that F has no strong W_t-backdoor set of size at most k.\n  (2) There is a cubic-time algorithm that, given a CNF formula F, computes the\nnumber of satisfying assignments of F or concludes that sb_t(F)>k, for any pair\nof constants k,t\\ge 0. Here, sb_t(F) denotes the size of a smallest strong\nW_t-backdoor set of F.\n  The significance of our results lies in the fact that they allow us to\nexploit algorithmically a hidden structure in formulas that is not accessible\nby any one of the two approaches (decomposability, backdoors) alone. Already a\nbackdoor size 1 on top of treewidth 1 (i.e., sb_1(F)=1) entails formulas of\narbitrarily large treewidth and arbitrarily large cycle cutsets. \n\n"}
{"id": "1204.6391", "contents": "Title: Extending partial representations of function graphs and permutation\n  graphs Abstract: Function graphs are graphs representable by intersections of continuous\nreal-valued functions on the interval [0,1] and are known to be exactly the\ncomplements of comparability graphs. As such they are recognizable in\npolynomial time. Function graphs generalize permutation graphs, which arise\nwhen all functions considered are linear.\n  We focus on the problem of extending partial representations, which\ngeneralizes the recognition problem. We observe that for permutation graphs an\neasy extension of Golumbic's comparability graph recognition algorithm can be\nexploited. This approach fails for function graphs. Nevertheless, we present a\npolynomial-time algorithm for extending a partial representation of a graph by\nfunctions defined on the entire interval [0,1] provided for some of the\nvertices. On the other hand, we show that if a partial representation consists\nof functions defined on subintervals of [0,1], then the problem of extending\nthis representation to functions on the entire interval [0,1] becomes\nNP-complete. \n\n"}
{"id": "1205.0036", "contents": "Title: Optimal Quantum Circuits for Nearest-Neighbor Architectures Abstract: We show that the depth of quantum circuits in the realistic architecture\nwhere a classical controller determines which local interactions to apply on\nthe kD grid Z^k where k >= 2 is the same (up to a constant factor) as in the\nstandard model where arbitrary interactions are allowed. This allows\nminimum-depth circuits (up to a constant factor) for the nearest-neighbor\narchitecture to be obtained from minimum-depth circuits in the standard\nabstract model. Our work therefore justifies the standard assumption that\ninteractions can be performed between arbitrary pairs of qubits. In particular,\nour results imply that Shor's algorithm, controlled operations and fanouts can\nbe implemented in constant depth, polynomial size and polynomial width in this\narchitecture.\n  We also present optimal non-adaptive quantum circuits for controlled\noperations and fanouts on a kD grid. These circuits have depth Theta(n^(1 /\nk)), size Theta(n) and width Theta(n). Our lower bound also applies to a more\ngeneral class of operations. \n\n"}
{"id": "1205.1183", "contents": "Title: On the Complexity of Trial and Error Abstract: Motivated by certain applications from physics, biochemistry, economics, and\ncomputer science, in which the objects under investigation are not accessible\nbecause of various limitations, we propose a trial-and-error model to examine\nalgorithmic issues in such situations. Given a search problem with a hidden\ninput, we are asked to find a valid solution, to find which we can propose\ncandidate solutions (trials), and use observed violations (errors), to prepare\nfuture proposals. In accordance with our motivating applications, we consider\nthe fairly broad class of constraint satisfaction problems, and assume that\nerrors are signaled by a verification oracle in the format of the index of a\nviolated constraint (with the content of the constraint still hidden).\n  Our discoveries are summarized as follows. On one hand, despite the seemingly\nvery little information provided by the verification oracle, efficient\nalgorithms do exist for a number of important problems. For the Nash, Core,\nStable Matching, and SAT problems, the unknown-input versions are as hard as\nthe corresponding known-input versions, up to a factor of polynomial. We\nfurther give almost tight bounds on the latter two problems' trial\ncomplexities. On the other hand, there are problems whose complexities are\nsubstantially increased in the unknown-input model. In particular, no\ntime-efficient algorithms exist (under standard hardness assumptions) for Graph\nIsomorphism and Group Isomorphism problems. The tools used to achieve these\nresults include order theory, strong ellipsoid method, and some non-standard\nreductions.\n  Our model investigates the value of information, and our results demonstrate\nthat the lack of input information can introduce various levels of extra\ndifficulty. The model exhibits intimate connections with (and we hope can also\nserve as a useful supplement to) certain existing learning and complexity\ntheories. \n\n"}
{"id": "1205.1331", "contents": "Title: Approximation Algorithms for Wireless Link Scheduling with Flexible Data\n  Rates Abstract: We consider scheduling problems in wireless networks with respect to flexible\ndata rates. That is, more or less data can be transmitted per time depending on\nthe signal quality, which is determined by the\nsignal-to-interference-plus-noise ratio (SINR). Each wireless link has a\nutility function mapping SINR values to the respective data rates. We have to\ndecide which transmissions are performed simultaneously and (depending on the\nproblem variant) also which transmission powers are used.\n  In the capacity-maximization problem, one strives to maximize the overall\nnetwork throughput, i.e., the summed utility of all links. For arbitrary\nutility functions (not necessarily continuous ones), we present an O(log\nn)-approximation when having n communication requests. This algorithm is built\non a constant-factor approximation for the special case of the respective\nproblem where utility functions only consist of a single step. In other words,\neach link has an individual threshold and we aim at maximizing the number of\nlinks whose threshold is satisfied. On the way, this improves the result in\n[Kesselheim, SODA 2011] by not only extending it to individual thresholds but\nalso showing a constant approximation factor independent of assumptions on the\nunderlying metric space or the network parameters.\n  In addition, we consider the latency-minimization problem. Here, each link\nhas a demand, e.g., representing an amount of data. We have to compute a\nschedule of shortest possible length such that for each link the demand is\nfulfilled, that is the overall summed utility (or data transferred) is at least\nas large as its demand. Based on the capacity-maximization algorithm, we show\nan O(log^2 n)-approximation for this problem. \n\n"}
{"id": "1205.1670", "contents": "Title: Rainbow Colouring of Split and Threshold Graphs Abstract: A rainbow colouring of a connected graph is a colouring of the edges of the\ngraph, such that every pair of vertices is connected by at least one path in\nwhich no two edges are coloured the same. Such a colouring using minimum\npossible number of colours is called an optimal rainbow colouring, and the\nminimum number of colours required is called the rainbow connection number of\nthe graph. In this article, we show the following:\n  1. The problem of deciding whether a graph can be rainbow coloured using 3\ncolours remains NP-complete even when restricted to the class of split graphs.\nHowever, any split graph can be rainbow coloured in linear time using at most\none more colour than the optimum.\n  2. For every integer k larger than 2, the problem of deciding whether a graph\ncan be rainbow coloured using k colours remains NP-complete even when\nrestricted to the class of chordal graphs.\n  3. For every positive integer k, threshold graphs with rainbow connection\nnumber k can be characterised based on their degree sequence alone. Further, we\ncan optimally rainbow colour a threshold graph in linear time. \n\n"}
{"id": "1205.4484", "contents": "Title: Hypercontractivity, Sum-of-Squares Proofs, and their Applications Abstract: We study the computational complexity of approximating the 2->q norm of\nlinear operators (defined as ||A||_{2->q} = sup_v ||Av||_q/||v||_2), as well as\nconnections between this question and issues arising in quantum information\ntheory and the study of Khot's Unique Games Conjecture (UGC). We show the\nfollowing:\n  1. For any constant even integer q>=4, a graph $G$ is a \"small-set expander\"\nif and only if the projector into the span of the top eigenvectors of G's\nadjacency matrix has bounded 2->q norm. As a corollary, a good approximation to\nthe 2->q norm will refute the Small-Set Expansion Conjecture--a close variant\nof the UGC. We also show that such a good approximation can be obtained in\nexp(n^(2/q)) time, thus obtaining a different proof of the known subexponential\nalgorithm for Small Set Expansion.\n  2. Constant rounds of the \"Sum of Squares\" semidefinite programing hierarchy\ncertify an upper bound on the 2->4 norm of the projector to low-degree\npolynomials over the Boolean cube, as well certify the unsatisfiability of the\n\"noisy cube\" and \"short code\" based instances of Unique Games considered by\nprior works. This improves on the previous upper bound of exp(poly log n)\nrounds (for the \"short code\"), as well as separates the \"Sum of\nSquares\"/\"Lasserre\" hierarchy from weaker hierarchies that were known to\nrequire omega(1) rounds.\n  3. We show reductions between computing the 2->4 norm and computing the\ninjective tensor norm of a tensor, a problem with connections to quantum\ninformation theory. Three corollaries are: (i) the 2->4 norm is NP-hard to\napproximate to precision inverse-polynomial in the dimension, (ii) the 2->4\nnorm does not have a good approximation (in the sense above) unless 3-SAT can\nbe solved in time exp(sqrt(n) polylog(n)), and (iii) known algorithms for the\nquantum separability problem imply a non-trivial additive approximation for the\n2->4 norm. \n\n"}
{"id": "1205.4605", "contents": "Title: No Sublogarithmic-time Approximation Scheme for Bipartite Vertex Cover Abstract: K\\\"onig's theorem states that on bipartite graphs the size of a maximum\nmatching equals the size of a minimum vertex cover. It is known from prior work\nthat for every \\epsilon > 0 there exists a constant-time distributed algorithm\nthat finds a (1+\\epsilon)-approximation of a maximum matching on 2-coloured\ngraphs of bounded degree. In this work, we show---somewhat surprisingly---that\nno sublogarithmic-time approximation scheme exists for the dual problem: there\nis a constant \\delta > 0 so that no randomised distributed algorithm with\nrunning time o(\\log n) can find a (1+\\delta)-approximation of a minimum vertex\ncover on 2-coloured graphs of maximum degree 3. In fact, a simple application\nof the Linial--Saks (1993) decomposition demonstrates that this lower bound is\ntight.\n  Our lower-bound construction is simple and, to some extent, independent of\nprevious techniques. Along the way we prove that a certain cut minimisation\nproblem, which might be of independent interest, is hard to approximate locally\non expander graphs. \n\n"}
{"id": "1206.0206", "contents": "Title: Streaming algorithms for recognizing nearly well-parenthesized\n  expressions Abstract: We study the streaming complexity of the membership problem of 1-turn-Dyck2\nand Dyck2 when there are a few errors in the input string.\n  1-turn-Dyck2 with errors: We prove that there exists a randomized one-pass\nalgorithm that given x checks whether there exists a string x' in 1-turn-Dyck2\nsuch that x is obtained by flipping at most $k$ locations of x' using:\n  - O(k log n) space, O(k log n) randomness, and poly(k log n) time per item\nand with error at most 1/poly(n). - O(k^{1+epsilon} + log n) space for every 0\n<= epsilon <= 1, O(log n) randomness, O(polylog(n) + poly(k)) time per item,\nwith error at most 1/8.\n  Here, we also prove that any randomized one-pass algorithm that makes error\nat most k/n requires at least Omega(k log(n/k)) space to accept strings which\nare exactly k-away from strings in 1-turn-Dyck2 and to reject strings which are\nexactly (k+2)-away from strings in 1-turn-Dyck2. Since 1-turn-Dyck2 and the\nHamming Distance problem are closely related we also obtain new upper and lower\nbounds for this problem.\n  Dyck2 with errors: We prove that there exists a randomized one-pass algorithm\nthat given x checks whether there exists a string x' in Dyck2 such that x is\nobtained from x' by changing (in some restricted manner) at most k positions\nusing:\n  - O(k log n + sqrt(n log n)) space, O(k log n) randomness, poly(k log n) time\nper element and with error at most 1/poly(n). - O(k^(1+epsilon)+ sqrt(n log n))\nspace for every 0 <= epsilon <= 1, O(log n) randomness, O(polylog(n) + poly(k))\ntime per element, with error at most 1/8. \n\n"}
{"id": "1206.3204", "contents": "Title: Improved Spectral-Norm Bounds for Clustering Abstract: Aiming to unify known results about clustering mixtures of distributions\nunder separation conditions, Kumar and Kannan[2010] introduced a deterministic\ncondition for clustering datasets. They showed that this single deterministic\ncondition encompasses many previously studied clustering assumptions. More\nspecifically, their proximity condition requires that in the target\n$k$-clustering, the projection of a point $x$ onto the line joining its cluster\ncenter $\\mu$ and some other center $\\mu'$, is a large additive factor closer to\n$\\mu$ than to $\\mu'$. This additive factor can be roughly described as $k$\ntimes the spectral norm of the matrix representing the differences between the\ngiven (known) dataset and the means of the (unknown) target clustering.\nClearly, the proximity condition implies center separation -- the distance\nbetween any two centers must be as large as the above mentioned bound.\n  In this paper we improve upon the work of Kumar and Kannan along several\naxes. First, we weaken the center separation bound by a factor of $\\sqrt{k}$,\nand secondly we weaken the proximity condition by a factor of $k$. Using these\nweaker bounds we still achieve the same guarantees when all points satisfy the\nproximity condition. We also achieve better guarantees when only\n$(1-\\epsilon)$-fraction of the points satisfy the weaker proximity condition.\nThe bulk of our analysis relies only on center separation under which one can\nproduce a clustering which (i) has low error, (ii) has low $k$-means cost, and\n(iii) has centers very close to the target centers.\n  Our improved separation condition allows us to match the results of the\nPlanted Partition Model of McSherry[2001], improve upon the results of\nOstrovsky et al[2006], and improve separation results for mixture of Gaussian\nmodels in a particular setting. \n\n"}
{"id": "1206.4224", "contents": "Title: Factoring bivariate lacunary polynomials without heights Abstract: We present an algorithm which computes the multilinear factors of bivariate\nlacunary polynomials. It is based on a new Gap Theorem which allows to test\nwhether a polynomial of the form P(X,X+1) is identically zero in time\npolynomial in the number of terms of P(X,Y). The algorithm we obtain is more\nelementary than the one by Kaltofen and Koiran (ISSAC'05) since it relies on\nthe valuation of polynomials of the previous form instead of the height of the\ncoefficients. As a result, it can be used to find some linear factors of\nbivariate lacunary polynomials over a field of large finite characteristic in\nprobabilistic polynomial time. \n\n"}
{"id": "1206.4555", "contents": "Title: Optimal compression of hash-origin prefix trees Abstract: There is a common problem of operating on hash values of elements of some\ndatabase. In this paper there will be analyzed informational content of such\ngeneral task and how to practically approach such found lower boundaries.\nMinimal prefix tree which distinguish elements turns out to require\nasymptotically only about 2.77544 bits per element, while standard approaches\nuse a few times more. While being certain of working inside the database, the\ncost of distinguishability can be reduced further to about 2.33275 bits per\nelements. Increasing minimal depth of nodes to reduce probability of false\npositives leads to simple relation with average depth of such random tree,\nwhich is asymptotically larger by about 1.33275 bits than lg(n) of the perfect\nbinary tree. This asymptotic case can be also seen as a way to optimally encode\nn large unordered numbers - saving lg(n!) bits of information about their\nordering, which can be the major part of contained information. This ability\nitself allows to reduce memory requirements even to about 0.693 of required in\nBloom filter for the same false positive probability. \n\n"}
{"id": "1206.6528", "contents": "Title: Adversary Lower Bound for the k-sum Problem Abstract: We prove a tight quantum query lower bound $\\Omega(n^{k/(k+1)})$ for the\nproblem of deciding whether there exist $k$ numbers among $n$ that sum up to a\nprescribed number, provided that the alphabet size is sufficiently large.\n  This is an extended and simplified version of an earlier preprint of one of\nthe authors arXiv:1204.5074. \n\n"}
{"id": "1206.6982", "contents": "Title: Optimal Dynamic Sequence Representations Abstract: We describe a data structure that supports access, rank and select queries,\nas well as symbol insertions and deletions, on a string $S[1,n]$ over alphabet\n$[1..\\sigma]$ in time $O(\\lg n/\\lg\\lg n)$, which is optimal even on binary\nsequences and in the amortized sense. Our time is worst-case for the queries\nand amortized for the updates. This complexity is better than the best previous\nones by a $\\Theta(1+\\lg\\sigma/\\lg\\lg n)$ factor. We also design a variant where\ntimes are worst-case, yet rank and updates take $O(\\lg n)$ time. Our structure\nuses $nH_0(S)+o(n\\lg\\sigma) + O(\\sigma\\lg n)$ bits, where $H_0(S)$ is the\nzero-order entropy of $S$. Finally, we pursue various extensions and\napplications of the result. \n\n"}
{"id": "1207.2598", "contents": "Title: Hitting Sets Online and Unique-Max Coloring Abstract: We consider the problem of hitting sets online. The hypergraph (i.e.,\nrange-space consisting of points and ranges) is known in advance, and the\nranges to be stabbed are input one-by-one in an online fashion. The online\nalgorithm must stab each range upon arrival. An online algorithm may add points\nto the hitting set but may not remove already chosen points. The goal is to use\nthe smallest number of points. The best known competitive ratio for hitting\nsets online by Alon et al. \\cite{alon2009online} is $O(\\log n \\cdot \\log m)$\nfor general hypergraphs, where $n$ and $m$ denote the number of points and the\nnumber of ranges, respectively. We consider hypergraphs in which the union of\ntwo intersecting ranges is also a range. Our main result for such hypergraphs\nis as follows. The competitive ratio of the online hitting set problem is at\nmost the unique-max number and at least this number minus one. \n\n"}
{"id": "1207.5211", "contents": "Title: Can Quantum Communication Speed Up Distributed Computation? Abstract: The focus of this paper is on {\\em quantum distributed} computation, where we\ninvestigate whether quantum communication can help in {\\em speeding up}\ndistributed network algorithms. Our main result is that for certain fundamental\nnetwork problems such as minimum spanning tree, minimum cut, and shortest\npaths, quantum communication {\\em does not} help in substantially speeding up\ndistributed algorithms for these problems compared to the classical setting.\n  In order to obtain this result, we extend the technique of Das Sarma et al.\n[SICOMP 2012] to obtain a uniform approach to prove non-trivial lower bounds\nfor quantum distributed algorithms for several graph optimization (both exact\nand approximate versions) as well as verification problems, some of which are\nnew even in the classical setting, e.g. tight randomized lower bounds for\nHamiltonian cycle and spanning tree verification, answering an open problem of\nDas Sarma et al., and a lower bound in terms of the weight aspect ratio,\nmatching the upper bounds of Elkin [STOC 2004]. Our approach introduces the\n{\\em Server model} and {\\em Quantum Simulation Theorem} which together provide\na connection between distributed algorithms and communication complexity. The\nServer model is the standard two-party communication complexity model augmented\nwith additional power; yet, most of the hardness in the two-party model is\ncarried over to this new model. The Quantum Simulation Theorem carries this\nhardness further to quantum distributed computing. Our techniques, except the\nproof of the hardness in the Server model, require very little knowledge in\nquantum computing, and this can help overcoming a usual impediment in proving\nbounds on quantum distributed algorithms. \n\n"}
{"id": "1207.5215", "contents": "Title: Density Functions subject to a Co-Matroid Constraint Abstract: In this paper we consider the problem of finding the {\\em densest} subset\nsubject to {\\em co-matroid constraints}. We are given a {\\em monotone\nsupermodular} set function $f$ defined over a universe $U$, and the density of\na subset $S$ is defined to be $f(S)/\\crd{S}$. This generalizes the concept of\ngraph density. Co-matroid constraints are the following: given matroid $\\calM$\na set $S$ is feasible, iff the complement of $S$ is {\\em independent} in the\nmatroid. Under such constraints, the problem becomes $\\np$-hard. The specific\ncase of graph density has been considered in literature under specific\nco-matroid constraints, for example, the cardinality matroid and the partition\nmatroid. We show a 2-approximation for finding the densest subset subject to\nco-matroid constraints. Thus, for instance, we improve the approximation\nguarantees for the result for partition matroids in the literature. \n\n"}
{"id": "1207.5742", "contents": "Title: Conditional Information Inequalities for Entropic and Almost Entropic\n  Points Abstract: We study conditional linear information inequalities, i.e., linear\ninequalities for Shannon entropy that hold for distributions whose entropies\nmeet some linear constraints. We prove that some conditional information\ninequalities cannot be extended to any unconditional linear inequalities. Some\nof these conditional inequalities hold for almost entropic points, while others\ndo not. We also discuss some counterparts of conditional information\ninequalities for Kolmogorov complexity. \n\n"}
{"id": "1208.1565", "contents": "Title: Fuel Efficient Computation in Passive Self-Assembly Abstract: In this paper we show that passive self-assembly in the context of the tile\nself-assembly model is capable of performing fuel efficient, universal\ncomputation. The tile self-assembly model is a premiere model of self-assembly\nin which particles are modeled by four-sided squares with glue types assigned\nto each tile edge. The assembly process is driven by positive and negative\nforce interactions between glue types, allowing for tile assemblies floating in\nthe plane to combine and break apart over time. We refer to this type of\nassembly model as passive in that the constituent parts remain unchanged\nthroughout the assembly process regardless of their interactions. A\ncomputationally universal system is said to be fuel efficient if the number of\ntiles used up per computation step is bounded by a constant. Work within this\nmodel has shown how fuel guzzling tile systems can perform universal\ncomputation with only positive strength glue interactions. Recent work has\nintroduced space-efficient, fuel-guzzling universal computation with the\naddition of negative glue interactions and the use of a powerful non-diagonal\nclass of glue interactions. Other recent work has shown how to achieve fuel\nefficient computation within active tile self-assembly. In this paper we\nutilize negative interactions in the tile self-assembly model to achieve the\nfirst computationally universal passive tile self-assembly system that is both\nspace and fuel-efficient. In addition, we achieve this result using a limited\ndiagonal class of glue interactions. \n\n"}
{"id": "1209.4605", "contents": "Title: One-side Energy costs of the RBO receiver Abstract: Let $n = 2^k$ be the length of the broadcast cycle of the RBO broadcast\nscheduling protocol (see [arXiv:1108.5095] and [arXiv:1201.3318]). Let $lb$ and\n$ub$ be the variables of the RBO receiver as defined in [ arXiv:1201.3318 ]. We\nshow that the number of changes of $lb$ (the \"left-side energy\") is not greater\nthan $k + 1$. We also show that the number of changes of $rb$ (the \"right-side\nenergy\") is not greater than $k + 2$. Thus the \"extra energy\" (defined in\n[arXiv:1201.3318]) is bounded by $2 k + 3$. This updates the previous bound\nfrom [arXiv:1201.3318], which was $4 k + 2$. \n\n"}
{"id": "1209.6348", "contents": "Title: Efficient quantum circuits for binary elliptic curve arithmetic:\n  reducing T-gate complexity Abstract: Elliptic curves over finite fields GF(2^n) play a prominent role in modern\ncryptography. Published quantum algorithms dealing with such curves build on a\nshort Weierstrass form in combination with affine or projective coordinates. In\nthis paper we show that changing the curve representation allows a substantial\nreduction in the number of T-gates needed to implement the curve arithmetic. As\na tool, we present a quantum circuit for computing multiplicative inverses in\nGF(2^n) in depth O(n log n) using a polynomial basis representation, which may\nbe of independent interest. \n\n"}
{"id": "1210.2698", "contents": "Title: Improved Approximation Lower Bounds for Vertex Cover on Power Law Graphs\n  and Some Generalizations Abstract: We prove new explicit inapproximability results for the Vertex Cover Problem\non the Power Law Graphs and some functional generalizations of that class of\ngraphs. Our results depend on special bounded degree amplifier constructions\nfor those classes of graphs and could be also of independent interest. \n\n"}
{"id": "1210.3277", "contents": "Title: Shortest, Fastest, and Foremost Broadcast in Dynamic Networks Abstract: Highly dynamic networks rarely offer end-to-end connectivity at a given time.\nYet, connectivity in these networks can be established over time and space,\nbased on temporal analogues of multi-hop paths (also called {\\em journeys}).\nAttempting to optimize the selection of the journeys in these networks\nnaturally leads to the study of three cases: shortest (minimum hop), fastest\n(minimum duration), and foremost (earliest arrival) journeys. Efficient\ncentralized algorithms exists to compute all cases, when the full knowledge of\nthe network evolution is given.\n  In this paper, we study the {\\em distributed} counterparts of these problems,\ni.e. shortest, fastest, and foremost broadcast with termination detection\n(TDB), with minimal knowledge on the topology.\n  We show that the feasibility of each of these problems requires distinct\nfeatures on the evolution, through identifying three classes of dynamic graphs\nwherein the problems become gradually feasible: graphs in which the\nre-appearance of edges is {\\em recurrent} (class R), {\\em bounded-recurrent}\n(B), or {\\em periodic} (P), together with specific knowledge that are\nrespectively $n$ (the number of nodes), $\\Delta$ (a bound on the recurrence\ntime), and $p$ (the period). In these classes it is not required that all pairs\nof nodes get in contact -- only that the overall {\\em footprint} of the graph\nis connected over time.\n  Our results, together with the strict inclusion between $P$, $B$, and $R$,\nimplies a feasibility order among the three variants of the problem, i.e.\nTDB[foremost] requires weaker assumptions on the topology dynamics than\nTDB[shortest], which itself requires less than TDB[fastest]. Reversely, these\ndifferences in feasibility imply that the computational powers of $R_n$,\n$B_\\Delta$, and $P_p$ also form a strict hierarchy. \n\n"}
{"id": "1210.3978", "contents": "Title: Fixed-Parameter Tractability of Workflow Satisfiability in the Presence\n  of Seniority Constraints Abstract: The workflow satisfiability problem is concerned with determining whether it\nis possible to find an allocation of authorized users to the steps in a\nworkflow in such a way that all constraints are satisfied. The problem is\nNP-hard in general, but is known to be fixed-parameter tractable for certain\nclasses of constraints. The known results on fixed-parameter tractability rely\non the symmetry (in some sense) of the constraints. In this paper, we provide\nthe first results that establish fixed-parameter tractability of the\nsatisfiability problem when the constraints are asymmetric. In particular, we\nintroduce the notion of seniority constraints, in which the execution of steps\nis determined, in part, by the relative seniority of the users that perform\nthem. Our results require new techniques, which make use of tree decompositions\nof the graph of the binary relation defining the constraint. Finally, we\nestablish a lower bound for the hardness of the workflow satisfiability\nproblem. \n\n"}
{"id": "1210.5048", "contents": "Title: Convergence of SDP hierarchies for polynomial optimization on the\n  hypersphere Abstract: We show how to bound the accuracy of a family of semi-definite programming\nrelaxations for the problem of polynomial optimization on the hypersphere. Our\nmethod is inspired by a set of results from quantum information known as\nquantum de Finetti theorems. In particular, we prove a de Finetti theorem for a\nspecial class of real symmetric matrices to establish the existence of\napproximate representing measures for moment matrix relaxations. \n\n"}
{"id": "1210.5879", "contents": "Title: Symmetric Determinantal Representations in Characteristic 2 Abstract: This paper studies Symmetric Determinantal Representations (SDR) in\ncharacteristic 2, that is the representation of a multivariate polynomial P by\na symmetric matrix M such that P=det(M), and where each entry of M is either a\nconstant or a variable.\n  We first give some sufficient conditions for a polynomial to have an SDR. We\nthen give a non-trivial necessary condition, which implies that some\npolynomials have no SDR, answering a question of Grenet et al.\n  A large part of the paper is then devoted to the case of multilinear\npolynomials. We prove that the existence of an SDR for a multilinear polynomial\nis equivalent to the existence of a factorization of the polynomial in certain\nquotient rings. We develop some algorithms to test the factorizability in these\nrings and use them to find SDRs when they exist. Altogether, this gives us\npolynomial-time algorithms to factorize the polynomials in the quotient rings\nand to build SDRs. We conclude by describing the case of Alternating\nDeterminantal Representations in any characteristic. \n\n"}
{"id": "1210.6287", "contents": "Title: Fast Exact Max-Kernel Search Abstract: The wide applicability of kernels makes the problem of max-kernel search\nubiquitous and more general than the usual similarity search in metric spaces.\nWe focus on solving this problem efficiently. We begin by characterizing the\ninherent hardness of the max-kernel search problem with a novel notion of\ndirectional concentration. Following that, we present a method to use an $O(n\n\\log n)$ algorithm to index any set of objects (points in $\\Real^\\dims$ or\nabstract objects) directly in the Hilbert space without any explicit feature\nrepresentations of the objects in this space. We present the first provably\n$O(\\log n)$ algorithm for exact max-kernel search using this index. Empirical\nresults for a variety of data sets as well as abstract objects demonstrate up\nto 4 orders of magnitude speedup in some cases. Extensions for approximate\nmax-kernel search are also presented. \n\n"}
{"id": "1210.7605", "contents": "Title: List-coloring embedded graphs Abstract: For any fixed surface Sigma of genus g, we give an algorithm to decide\nwhether a graph G of girth at least five embedded in Sigma is colorable from an\nassignment of lists of size three in time O(|V(G)|). Furthermore, we can allow\na subgraph (of any size) with at most s components to be precolored, at the\nexpense of increasing the time complexity of the algorithm to\nO(|V(G)|^{K(g+s)+1}) for some absolute constant K; in both cases, the\nmultiplicative constant hidden in the O-notation depends on g and s. This also\nenables us to find such a coloring when it exists. The idea of the algorithm\ncan be applied to other similar problems, e.g., 5-list-coloring of graphs on\nsurfaces. \n\n"}
{"id": "1211.5350", "contents": "Title: Note on the Greedy Parsing Optimality for Dictionary-Based Text\n  Compression Abstract: Dynamic dictionary-based compression schemes are the most daily used data\ncompression schemes since they appeared in the foundational papers of Ziv and\nLempel in 1977, commonly referred to as LZ77. Their work is the base of\nDeflate, gZip, WinZip, 7Zip and many others compression software. All of those\ncompression schemes use variants of the greedy approach to parse the text into\ndictionary phrases. Greedy parsing optimality was proved by Cohn et al. (1996)\nfor fixed length code and unbounded dictionaries. The optimality of the greedy\nparsing was never proved for bounded size dictionary which actually all of\nthose schemes require. We define the suffix-closed property for dynamic\ndictionaries and we show that any LZ77-based dictionary, including the bounded\nvariants, satisfy this property. Under this condition we prove the optimality\nof the greedy parsing as a variant of the proof by Cohn et al. \n\n"}
{"id": "1211.5414", "contents": "Title: Analysis of a randomized approximation scheme for matrix multiplication Abstract: This note gives a simple analysis of a randomized approximation scheme for\nmatrix multiplication proposed by Sarlos (2006) based on a random rotation\nfollowed by uniform column sampling. The result follows from a matrix version\nof Bernstein's inequality and a tail inequality for quadratic forms in\nsubgaussian random vectors. \n\n"}
{"id": "1211.7138", "contents": "Title: Euclidean Partitions Optimizing Noise Stability Abstract: The Standard Simplex Conjecture of Isaksson and Mossel asks for the partition\n$\\{A_{i}\\}_{i=1}^{k}$ of $\\mathbb{R}^{n}$ into $k\\leq n+1$ pieces of equal\nGaussian measure of optimal noise stability. That is, for $\\rho>0$, we maximize\n$$\n\\sum_{i=1}^{k}\\int_{\\mathbb{R}^{n}}\\int_{\\mathbb{R}^{n}}1_{A_{i}}(x)1_{A_{i}}(x\\rho+y\\sqrt{1-\\rho^{2}})\ne^{-(x_{1}^{2}+\\cdots+x_{n}^{2})/2}e^{-(y_{1}^{2}+\\cdots+y_{n}^{2})/2}dxdy. $$\nIsaksson and Mossel guessed the best partition for this problem and proved some\napplications of their conjecture. For example, the Standard Simplex Conjecture\nimplies the Plurality is Stablest Conjecture. For $k=3,n\\geq2$ and\n$0<\\rho<\\rho_{0}(k,n)$, we prove the Standard Simplex Conjecture. The full\nconjecture has applications to theoretical computer science, and to geometric\nmulti-bubble problems (after Isaksson and Mossel). \n\n"}
{"id": "1212.0752", "contents": "Title: Parameters of Two-Prover-One-Round Game and The Hardness of Connectivity\n  Problems Abstract: Optimizing parameters of Two-Prover-One-Round Game (2P1R) is an important\ntask in PCPs literature as it would imply a smaller PCP with the same or\nstronger soundness. While this is a basic question in PCPs community, the\nconnection between the parameters of PCPs and hardness of approximations is\nsometime obscure to approximation algorithm community. In this paper, we\ninvestigate the connection between the parameters of 2P1R and the hardness of\napproximating the class of so-called connectivity problems, which includes as\nsubclasses the survivable network design and (multi)cut problems. Based on\nrecent development on 2P1R by Chan (ECCC 2011) and several techniques in PCPs\nliterature, we improve hardness results of some connectivity problems that are\nin the form $k^\\sigma$, for some (very) small constant $\\sigma>0$, to hardness\nresults of the form $k^c$ for some explicit constant $c$, where $k$ is a\nconnectivity parameter. In addition, we show how to convert these hardness into\nhardness results of the form $D^{c'}$, where $D$ is the number of demand pairs\n(or the number of terminals).\n  Thus, we give improved hardness results of k^{1/2-\\epsilon} and\nk^{1/10-\\epsilon} for the root $k$-connectivity problem on directed and\nundirected graphs, k^{1/6-\\epsilon} for the vertex-connectivity survivable\nnetwork design problem on undirected graphs, and k^{1/6-\\epsilon} for the\nvertex-connectivity $k$-route cut problem on undirected graphs. \n\n"}
{"id": "1212.2284", "contents": "Title: The Complexity of Planar Boolean #CSP with Complex Weights Abstract: We prove a complexity dichotomy theorem for symmetric complex-weighted\nBoolean #CSP when the constraint graph of the input must be planar. The\nproblems that are #P-hard over general graphs but tractable over planar graphs\nare precisely those with a holographic reduction to matchgates. This\ngeneralizes a theorem of Cai, Lu, and Xia for the case of real weights. We also\nobtain a dichotomy theorem for a symmetric arity 4 signature with complex\nweights in the planar Holant framework, which we use in the proof of our #CSP\ndichotomy. In particular, we reduce the problem of evaluating the Tutte\npolynomial of a planar graph at the point (3,3) to counting the number of\nEulerian orientations over planar 4-regular graphs to show the latter is\n#P-hard. This strengthens a theorem by Huang and Lu to the planar setting. Our\nproof techniques combine new ideas with refinements and extensions of existing\ntechniques. These include planar pairings, the recursive unary construction,\nthe anti-gadget technique, and pinning in the Hadamard basis. \n\n"}
{"id": "1212.3849", "contents": "Title: Every locally characterized affine-invariant property is testable Abstract: Let F = F_p for any fixed prime p >= 2. An affine-invariant property is a\nproperty of functions on F^n that is closed under taking affine transformations\nof the domain. We prove that all affine-invariant property having local\ncharacterizations are testable. In fact, we show a proximity-oblivious test for\nany such property P, meaning that there is a test that, given an input function\nf, makes a constant number of queries to f, always accepts if f satisfies P,\nand rejects with positive probability if the distance between f and P is\nnonzero. More generally, we show that any affine-invariant property that is\nclosed under taking restrictions to subspaces and has bounded complexity is\ntestable.\n  We also prove that any property that can be described as the property of\ndecomposing into a known structure of low-degree polynomials is locally\ncharacterized and is, hence, testable. For example, whether a function is a\nproduct of two degree-d polynomials, whether a function splits into a product\nof d linear polynomials, and whether a function has low rank are all examples\nof degree-structural properties and are therefore locally characterized.\n  Our results depend on a new Gowers inverse theorem by Tao and Ziegler for low\ncharacteristic fields that decomposes any polynomial with large Gowers norm\ninto a function of low-degree non-classical polynomials. We establish a new\nequidistribution result for high rank non-classical polynomials that drives the\nproofs of both the testability results and the local characterization of\ndegree-structural properties. \n\n"}
{"id": "1212.4372", "contents": "Title: Sliding Windows with Limited Storage Abstract: We consider time-space tradeoffs for exactly computing frequency moments and\norder statistics over sliding windows. Given an input of length 2n-1, the task\nis to output the function of each window of length n, giving n outputs in\ntotal. Computations over sliding windows are related to direct sum problems\nexcept that inputs to instances almost completely overlap.\n  We show an average case and randomized time-space tradeoff lower bound of TS\nin Omega(n^2) for multi-way branching programs, and hence standard RAM and\nword-RAM models, to compute the number of distinct elements, F_0, in sliding\nwindows over alphabet [n]. The same lower bound holds for computing the\nlow-order bit of F_0 and computing any frequency moment F_k for k not equal to\n1. We complement this lower bound with a TS in \\tilde O(n^2) deterministic RAM\nalgorithm for exactly computing F_k in sliding windows.\n  We show time-space separations between the complexity of sliding-window\nelement distinctness and that of sliding-window $F_0\\bmod 2$ computation. In\nparticular for alphabet [n] there is a very simple errorless sliding-window\nalgorithm for element distinctness that runs in O(n) time on average and uses\nO(log{n}) space.\n  We show that any algorithm for a single element distinctness instance can be\nextended to an algorithm for the sliding-window version of element distinctness\nwith at most a polylogarithmic increase in the time-space product.\n  Finally, we show that the sliding-window computation of order statistics such\nas the maximum and minimum can be computed with only a logarithmic increase in\ntime, but that a TS in Omega(n^2) lower bound holds for sliding-window\ncomputation of order statistics such as the median, a nearly linear increase in\ntime when space is small. \n\n"}
{"id": "1212.5880", "contents": "Title: Local Thresholding in General Network Graphs Abstract: Local thresholding algorithms were first presented more than a decade ago and\nhave since been applied to a variety of data mining tasks in peer-to-peer\nsystems, wireless sensor networks, and in grid systems. One critical assumption\nmade by those algorithms has always been cycle-free routing. The existence of\neven one cycle may lead all peers to the wrong outcome. Outside the lab,\nunfortunately, cycle freedom is not easy to achieve.\n  This work is the first to lift the requirement of cycle freedom by presenting\na local thresholding algorithm suitable for general network graphs. The\nalgorithm relies on a new repositioning of the problem in weighted vector\narithmetics, on a new stopping rule, whose proof does not require that the\nnetwork be cycle free, and on new methods for balance correction when the\nstopping rule fails.\n  The new stopping and update rules permit calculation of the very same\nfunctions that were calculable using previous algorithms, which do assume cycle\nfreedom. The algorithm is implemented on a standard peer-to-peer simulator and\nis validated for networks of up to 80,000 peers, organized in three different\ntopologies, which are representative of the topology of major current\ndistributed systems: the Internet, structured peer-to-peer systems, and\nwireless sensor networks. \n\n"}
{"id": "1212.6925", "contents": "Title: Superlinear lower bounds for multipass graph processing Abstract: We prove $n^{1+\\Omega(1/p)}/p^{O(1)}$ lower bounds for the space complexity\nof $p$-pass streaming algorithms solving the following problems on $n$-vertex\ngraphs:\n  * testing if an undirected graph has a perfect matching (this implies lower\nbounds for computing a maximum matching or even just the maximum matching\nsize),\n  * testing if two specific vertices are at distance at most $2(p+1)$ in an\nundirected graph,\n  * testing if there is a directed path from $s$ to $t$ for two specific\nvertices $s$ and $t$ in a directed graph.\n  Prior to our result, it was known that these problems require $\\Omega(n^2)$\nspace in one pass, but no $n^{1+\\Omega(1)}$ lower bound was known for any $p\\ge\n2$.\n  These streaming results follow from a communication complexity lower bound\nfor a communication game in which the players hold two graphs on the same set\nof vertices. The task of the players is to find out whether the sets of\nvertices at distance exactly $p+1$ from a specific vertex intersect. The game\nrequires a significant amount of communication only if the players are forced\nto speak in a specific difficult order. This is reminiscent of lower bounds for\ncommunication problems such as indexing and pointer chasing. Among other\nthings, our line of attack requires proving an information cost lower bound for\na decision version of the classic pointer chasing problem and a direct sum type\ntheorem for the disjunction of several instances of this problem. \n\n"}
{"id": "1301.2497", "contents": "Title: Update-Efficient Regenerating Codes with Minimum Per-Node Storage Abstract: Regenerating codes provide an efficient way to recover data at failed nodes\nin distributed storage systems. It has been shown that regenerating codes can\nbe designed to minimize the per-node storage (called MSR) or minimize the\ncommunication overhead for regeneration (called MBR). In this work, we propose\na new encoding scheme for [n,d] error- correcting MSR codes that generalizes\nour earlier work on error-correcting regenerating codes. We show that by\nchoosing a suitable diagonal matrix, any generator matrix of the [n,{\\alpha}]\nReed-Solomon (RS) code can be integrated into the encoding matrix. Hence, MSR\ncodes with the least update complexity can be found. An efficient decoding\nscheme is also proposed that utilizes the [n,{\\alpha}] RS code to perform data\nreconstruction. The proposed decoding scheme has better error correction\ncapability and incurs the least number of node accesses when errors are\npresent. \n\n"}
{"id": "1301.5359", "contents": "Title: Local Graph Coloring and Index Coding Abstract: We present a novel upper bound for the optimal index coding rate. Our bound\nuses a graph theoretic quantity called the local chromatic number. We show how\na good local coloring can be used to create a good index code. The local\ncoloring is used as an alignment guide to assign index coding vectors from a\ngeneral position MDS code. We further show that a natural LP relaxation yields\nan even stronger index code. Our bounds provably outperform the state of the\nart on index coding but at most by a constant factor. \n\n"}
{"id": "1302.0418", "contents": "Title: Arthur-Merlin Streaming Complexity Abstract: We study the power of Arthur-Merlin probabilistic proof systems in the data\nstream model. We show a canonical $\\mathcal{AM}$ streaming algorithm for a wide\nclass of data stream problems. The algorithm offers a tradeoff between the\nlength of the proof and the space complexity that is needed to verify it.\n  As an application, we give an $\\mathcal{AM}$ streaming algorithm for the\n\\emph{Distinct Elements} problem. Given a data stream of length $m$ over\nalphabet of size $n$, the algorithm uses $\\tilde O(s)$ space and a proof of\nsize $\\tilde O(w)$, for every $s,w$ such that $s \\cdot w \\ge n$ (where $\\tilde\nO$ hides a $\\polylog(m,n)$ factor). We also prove a lower bound, showing that\nevery $\\mathcal{MA}$ streaming algorithm for the \\emph{Distinct Elements}\nproblem that uses $s$ bits of space and a proof of size $w$, satisfies $s \\cdot\nw = \\Omega(n)$.\n  As a part of the proof of the lower bound for the \\emph{Distinct Elements}\nproblem, we show a new lower bound of $\\Omega(\\sqrt n)$ on the $\\mathcal{MA}$\ncommunication complexity of the \\emph{Gap Hamming Distance} problem, and prove\nits tightness. \n\n"}
{"id": "1302.1669", "contents": "Title: Possible and Necessary Winner Problem in Social Polls Abstract: Social networks are increasingly being used to conduct polls. We introduce a\nsimple model of such social polling. We suppose agents vote sequentially, but\nthe order in which agents choose to vote is not necessarily fixed. We also\nsuppose that an agent's vote is influenced by the votes of their friends who\nhave already voted. Despite its simplicity, this model provides useful insights\ninto a number of areas including social polling, sequential voting, and\nmanipulation. We prove that the number of candidates and the network structure\naffect the computational complexity of computing which candidate necessarily or\npossibly can win in such a social poll. For social networks with bounded\ntreewidth and a bounded number of candidates, we provide polynomial algorithms\nfor both problems. In other cases, we prove that computing which candidates\nnecessarily or possibly win are computationally intractable. \n\n"}
{"id": "1302.2787", "contents": "Title: Acquaintance Time of a Graph Abstract: We define the following parameter of connected graphs. For a given graph $G$\nwe place one agent in each vertex of $G$. Every pair of agents sharing a common\nedge is declared to be acquainted. In each round we choose some matching of $G$\n(not necessarily a maximal matching), and for each edge in the matching the\nagents on this edge swap places. After the swap, again, every pair of agents\nsharing a common edge become acquainted, and the process continues. We define\nthe \\emph{acquaintance time} of a graph $G$, denoted by $AC(G)$, to be the\nminimal number of rounds required until every two agents are acquainted.\n  We first study the acquaintance time for some natural families of graphs\nincluding the path, expanders, the binary tree, and the complete bipartite\ngraph. We also show that for all positive integers $n$ and $k \\leq n^{1.5}$\nthere exists an $n$-vertex graph $G$ such that $AC(G) =\\Theta(k)$. We also\nprove that for all $n$-vertex connected graphs $G$ we have $AC(G) =\nO\\left(\\frac{n^2}{\\log(n)/\\log\\log(n)}\\right)$, improving the $O(n^2)$ trivial\nupper bound achieved by sequentially letting each agent perform depth-first\nsearch along a spanning tree of $G$.\n  Studying the computational complexity of this problem, we prove that for any\nconstant $t \\geq 1$ the problem of deciding that a given graph $G$ has $AC(G)\n\\leq t$ or $AC(G) \\geq 2t$ is $\\mathcal{NP}$-complete. That is, $AC(G)$ is\n$\\mathcal{NP}$-hard to approximate within multiplicative factor of 2, as well\nas within any additive constant factor.\n  On the algorithmic side, we give a deterministic algorithm that given a graph\n$G$ with $AC(G)=1$ finds a ${\\lceil n/c\\rceil}$-rounds strategy for\nacquaintance in time $n^{c+O(1)}$. We also design a randomized polynomial time\nalgorithm that given a graph $G$ with $AC(G)=1$ finds with high probability an\n$O(\\log(n))$-rounds strategy for acquaintance. \n\n"}
{"id": "1302.3496", "contents": "Title: On Polynomial Kernels for Integer Linear Programs: Covering, Packing and\n  Feasibility Abstract: We study the existence of polynomial kernels for the problem of deciding\nfeasibility of integer linear programs (ILPs), and for finding good solutions\nfor covering and packing ILPs. Our main results are as follows: First, we show\nthat the ILP Feasibility problem admits no polynomial kernelization when\nparameterized by both the number of variables and the number of constraints,\nunless NP \\subseteq coNP/poly. This extends to the restricted cases of bounded\nvariable degree and bounded number of variables per constraint, and to covering\nand packing ILPs. Second, we give a polynomial kernelization for the Cover ILP\nproblem, asking for a solution to Ax >= b with c^Tx <= k, parameterized by k,\nwhen A is row-sparse; this generalizes a known polynomial kernelization for the\nspecial case with 0/1-variables and coefficients (d-Hitting Set). \n\n"}
{"id": "1302.4707", "contents": "Title: On the Complexity of Barrier Resilience for Fat Regions Abstract: In the \\emph {barrier resilience} problem (introduced by Kumar {\\em et al.},\nWireless Networks 2007), we are given a collection of regions of the plane,\nacting as obstacles, and we would like to remove the minimum number of regions\nso that two fixed points can be connected without crossing any region. In this\npaper, we show that the problem is NP-hard when the collection only contains\nfat regions with bounded ply $\\Delta$ (even when they are axis-aligned\nrectangles of aspect ratio $1 : (1 + \\varepsilon)$). We also show that the\nproblem is fixed-parameter tractable (FPT) for unit disks and for\nsimilarly-sized $\\beta$-fat regions with bounded ply $\\Delta$ and $O(1)$\npairwise boundary intersections. Furthermore, we can use our FPT algorithm to\nconstruct an $(1+\\varepsilon)$-approximation algorithm that runs in\n$O(2^{f(\\Delta, \\varepsilon,\\beta)}n^5)$ time, where $f\\in\nO(\\frac{\\Delta^4\\beta^8}{\\varepsilon^4}\\log(\\beta\\Delta/\\varepsilon))$. \n\n"}
{"id": "1302.5843", "contents": "Title: Ising formulations of many NP problems Abstract: We provide Ising formulations for many NP-complete and NP-hard problems,\nincluding all of Karp's 21 NP-complete problems. This collects and extends\nmappings to the Ising model from partitioning, covering and satisfiability. In\neach case, the required number of spins is at most cubic in the size of the\nproblem. This work may be useful in designing adiabatic quantum optimization\nalgorithms. \n\n"}
{"id": "1302.6191", "contents": "Title: Dual Lower Bounds for Approximate Degree and Markov-Bernstein\n  Inequalities Abstract: The $\\epsilon$-approximate degree of a Boolean function $f: \\{-1, 1\\}^n \\to\n\\{-1, 1\\}$ is the minimum degree of a real polynomial that approximates $f$ to\nwithin $\\epsilon$ in the $\\ell_\\infty$ norm. We prove several lower bounds on\nthis important complexity measure by explicitly constructing solutions to the\ndual of an appropriate linear program. Our first result resolves the\n$\\epsilon$-approximate degree of the two-level AND-OR tree for any constant\n$\\epsilon > 0$. We show that this quantity is $\\Theta(\\sqrt{n})$, closing a\nline of incrementally larger lower bounds. The same lower bound was recently\nobtained independently by Sherstov using related techniques. Our second result\ngives an explicit dual polynomial that witnesses a tight lower bound for the\napproximate degree of any symmetric Boolean function, addressing a question of\n\\v{S}palek. Our final contribution is to reprove several Markov-type\ninequalities from approximation theory by constructing explicit dual solutions\nto natural linear programs. These inequalities underly the proofs of many of\nthe best-known approximate degree lower bounds, and have important uses\nthroughout theoretical computer science. \n\n"}
{"id": "1302.6562", "contents": "Title: An Improvement to Levenshtein's Upper Bound on the Cardinality of\n  Deletion Correcting Codes Abstract: We consider deletion correcting codes over a q-ary alphabet. It is well known\nthat any code capable of correcting s deletions can also correct any\ncombination of s total insertions and deletions. To obtain asymptotic upper\nbounds on code size, we apply a packing argument to channels that perform\ndifferent mixtures of insertions and deletions. Even though the set of codes is\nidentical for all of these channels, the bounds that we obtain vary. Prior to\nthis work, only the bounds corresponding to the all insertion case and the all\ndeletion case were known. We recover these as special cases. The bound from the\nall deletion case, due to Levenshtein, has been the best known for more than\nforty five years. Our generalized bound is better than Levenshtein's bound\nwhenever the number of deletions to be corrected is larger than the alphabet\nsize. \n\n"}
{"id": "1302.7262", "contents": "Title: Towards a provably resilient scheme for graph-based watermarking Abstract: Digital watermarks have been considered a promising way to fight software\npiracy. Graph-based watermarking schemes encode authorship/ownership data as\ncontrol-flow graph of dummy code. In 2012, Chroni and Nikolopoulos developed an\ningenious such scheme which was claimed to withstand attacks in the form of a\nsingle edge removal. We extend the work of those authors in various aspects.\nFirst, we give a formal characterization of the class of graphs generated by\ntheir encoding function. Then, we formulate a linear-time algorithm which\nrecovers from ill-intentioned removals of $k \\leq 2$ edges, therefore proving\ntheir claim. Furthermore, we provide a simpler decoding function and an\nalgorithm to restore watermarks with an arbitrary number of missing edges\nwhenever at all possible. By disclosing and improving upon the resilience of\nChroni and Nikolopoulos's watermark, our results reinforce the interest in\nregarding it as a possible solution to numerous applications. \n\n"}
{"id": "1303.0478", "contents": "Title: Monomial Testing and Applications Abstract: In this paper, we devise two algorithms for the problem of testing\n$q$-monomials of degree $k$ in any multivariate polynomial represented by a\ncircuit, regardless of the primality of $q$. One is an $O^*(2^k)$ time\nrandomized algorithm. The other is an $O^*(12.8^k)$ time deterministic\nalgorithm for the same $q$-monomial testing problem but requiring the\npolynomials to be represented by tree-like circuits. Several applications of\n$q$-monomial testing are also given, including a deterministic $O^*(12.8^{mk})$\nupper bound for the $m$-set $k$-packing problem. \n\n"}
{"id": "1303.1209", "contents": "Title: Sample-Optimal Average-Case Sparse Fourier Transform in Two Dimensions Abstract: We present the first sample-optimal sublinear time algorithms for the sparse\nDiscrete Fourier Transform over a two-dimensional sqrt{n} x sqrt{n} grid. Our\nalgorithms are analyzed for /average case/ signals. For signals whose spectrum\nis exactly sparse, our algorithms use O(k) samples and run in O(k log k) time,\nwhere k is the expected sparsity of the signal. For signals whose spectrum is\napproximately sparse, our algorithm uses O(k log n) samples and runs in O(k\nlog^2 n) time; the latter algorithm works for k=Theta(sqrt{n}). The number of\nsamples used by our algorithms matches the known lower bounds for the\nrespective signal models.\n  By a known reduction, our algorithms give similar results for the\none-dimensional sparse Discrete Fourier Transform when n is a power of a small\ncomposite number (e.g., n = 6^t). \n\n"}
{"id": "1303.3921", "contents": "Title: On the Locality of Codeword Symbols in Non-Linear Codes Abstract: Consider a possibly non-linear (n,K,d)_q code. Coordinate i has locality r if\nits value is determined by some r other coordinates. A recent line of work\nobtained an optimal trade-off between information locality of codes and their\nredundancy. Further, for linear codes meeting this trade-off, structure\ntheorems were derived. In this work we give a new proof of the locality /\nredundancy trade-off and generalize structure theorems to non-linear codes. \n\n"}
{"id": "1303.5479", "contents": "Title: Bottom-k and Priority Sampling, Set Similarity and Subset Sums with\n  Minimal Independence Abstract: We consider bottom-k sampling for a set X, picking a sample S_k(X) consisting\nof the k elements that are smallest according to a given hash function h. With\nthis sample we can estimate the relative size f=|Y|/|X| of any subset Y as\n|S_k(X) intersect Y|/k. A standard application is the estimation of the Jaccard\nsimilarity f=|A intersect B|/|A union B| between sets A and B. Given the\nbottom-k samples from A and B, we construct the bottom-k sample of their union\nas S_k(A union B)=S_k(S_k(A) union S_k(B)), and then the similarity is\nestimated as |S_k(A union B) intersect S_k(A) intersect S_k(B)|/k.\n  We show here that even if the hash function is only 2-independent, the\nexpected relative error is O(1/sqrt(fk)). For fk=Omega(1) this is within a\nconstant factor of the expected relative error with truly random hashing.\n  For comparison, consider the classic approach of kxmin-wise where we use k\nhash independent functions h_1,...,h_k, storing the smallest element with each\nhash function. For kxmin-wise there is an at least constant bias with constant\nindependence, and it is not reduced with larger k. Recently Feigenblat et al.\nshowed that bottom-k circumvents the bias if the hash function is 8-independent\nand k is sufficiently large. We get down to 2-independence for any k. Our\nresult is based on a simply union bound, transferring generic concentration\nbounds for the hashing scheme to the bottom-k sample, e.g., getting stronger\nprobability error bounds with higher independence.\n  For weighted sets, we consider priority sampling which adapts efficiently to\nthe concrete input weights, e.g., benefiting strongly from heavy-tailed input.\nThis time, the analysis is much more involved, but again we show that generic\nconcentration bounds can be applied. \n\n"}
{"id": "1303.6437", "contents": "Title: New Inapproximability Bounds for TSP Abstract: In this paper, we study the approximability of the metric Traveling Salesman\nProblem (TSP) and prove new explicit inapproximability bounds for that problem.\nThe best up to now known hardness of approximation bounds were 185/184 for the\nsymmetric case (due to Lampis) and 117/116 for the asymmetric case (due to\nPapadimitriou and Vempala). We construct here two new bounded occurrence CSP\nreductions which improve these bounds to 123/122 and 75/74, respectively. The\nlatter bound is the first improvement in more than a decade for the case of the\nasymmetric TSP. One of our main tools, which may be of independent interest, is\na new construction of a bounded degree wheel amplifier used in the proof of our\nresults. \n\n"}
{"id": "1304.0730", "contents": "Title: Representation, Approximation and Learning of Submodular Functions Using\n  Low-rank Decision Trees Abstract: We study the complexity of approximate representation and learning of\nsubmodular functions over the uniform distribution on the Boolean hypercube\n$\\{0,1\\}^n$. Our main result is the following structural theorem: any\nsubmodular function is $\\epsilon$-close in $\\ell_2$ to a real-valued decision\ntree (DT) of depth $O(1/\\epsilon^2)$. This immediately implies that any\nsubmodular function is $\\epsilon$-close to a function of at most\n$2^{O(1/\\epsilon^2)}$ variables and has a spectral $\\ell_1$ norm of\n$2^{O(1/\\epsilon^2)}$. It also implies the closest previous result that states\nthat submodular functions can be approximated by polynomials of degree\n$O(1/\\epsilon^2)$ (Cheraghchi et al., 2012). Our result is proved by\nconstructing an approximation of a submodular function by a DT of rank\n$4/\\epsilon^2$ and a proof that any rank-$r$ DT can be $\\epsilon$-approximated\nby a DT of depth $\\frac{5}{2}(r+\\log(1/\\epsilon))$.\n  We show that these structural results can be exploited to give an\nattribute-efficient PAC learning algorithm for submodular functions running in\ntime $\\tilde{O}(n^2) \\cdot 2^{O(1/\\epsilon^{4})}$. The best previous algorithm\nfor the problem requires $n^{O(1/\\epsilon^{2})}$ time and examples (Cheraghchi\net al., 2012) but works also in the agnostic setting. In addition, we give\nimproved learning algorithms for a number of related settings.\n  We also prove that our PAC and agnostic learning algorithms are essentially\noptimal via two lower bounds: (1) an information-theoretic lower bound of\n$2^{\\Omega(1/\\epsilon^{2/3})}$ on the complexity of learning monotone\nsubmodular functions in any reasonable model; (2) computational lower bound of\n$n^{\\Omega(1/\\epsilon^{2/3})}$ based on a reduction to learning of sparse\nparities with noise, widely-believed to be intractable. These are the first\nlower bounds for learning of submodular functions over the uniform\ndistribution. \n\n"}
{"id": "1304.1007", "contents": "Title: Linear-in-$\\Delta$ Lower Bounds in the LOCAL Model Abstract: By prior work, there is a distributed algorithm that finds a maximal\nfractional matching (maximal edge packing) in $O(\\Delta)$ rounds, where\n$\\Delta$ is the maximum degree of the graph. We show that this is optimal:\nthere is no distributed algorithm that finds a maximal fractional matching in\n$o(\\Delta)$ rounds.\n  Our work gives the first linear-in-$\\Delta$ lower bound for a natural graph\nproblem in the standard model of distributed computing---prior lower bounds for\na wide range of graph problems have been at best logarithmic in $\\Delta$. \n\n"}
{"id": "1304.2816", "contents": "Title: Asymptotic Behaviour and Ratios of Complexity in Cellular Automata Abstract: We study the asymptotic behaviour of symbolic computing systems, notably\none-dimensional cellular automata (CA), in order to ascertain whether and at\nwhat rate the number of complex versus simple rules dominate the rule space for\nincreasing neighbourhood range and number of symbols (or colours), and how\ndifferent behaviour is distributed in the spaces of different cellular automata\nformalisms. Using two different measures, Shannon's block entropy and\nKolmogorov complexity, the latter approximated by two different methods\n(lossless compressibility and block decomposition), we arrive at the same trend\nof larger complex behavioural fractions. We also advance a notion of asymptotic\nand limit behaviour for individual rules, both over initial conditions and\nruntimes, and we provide a formalisation of Wolfram's classification as a limit\nfunction in terms of Kolmogorov complexity. \n\n"}
{"id": "1304.3812", "contents": "Title: Time-Optimal Interactive Proofs for Circuit Evaluation Abstract: Recently, researchers have been working toward the development of practical\ngeneral-purpose protocols for verifiable computation. These protocols enable a\ncomputationally weak verifier to offload computations to a powerful but\nuntrusted prover, while providing the verifier with a guarantee that the prover\nperformed the computations correctly. Despite substantial progress, existing\nimplementations are not yet practical. The main bottleneck is typically the\nextra effort required by the prover to return an answer with a guarantee of\ncorrectness, compared to returning an answer with no guarantee.\n  We describe a refinement of a powerful interactive proof protocol originally\ndue to Goldwasser, Kalai, and Rothblum. Cormode, Mitzenmacher, and Thaler show\nhow to implement the prover in this protocol in time O(S log S), where S is the\nsize of an arithmetic circuit computing the function of interest. Our\nrefinements apply to circuits whose wiring pattern is sufficiently \"regular\";\nfor these circuits, we bring the runtime of the prover down to O(S). That is,\nour prover can evaluate the circuit with a guarantee of correctness, with only\na constant-factor blowup in work compared to evaluating the circuit with no\nguarantee.\n  We argue that our refinements capture a large class of circuits, and prove\nsome theorems formalizing this. Experimentally, our refinements yield a 200x\nspeedup for the prover over the implementation of Cormode et al., and our\nprover is less than 10x slower than a C++ program that simply evaluates the\ncircuit. Along the way, we describe a special-purpose protocol for matrix\nmultiplication that is of interest in its own right.\n  Our final contribution is a protocol targeted at general data parallel\ncomputation. Compared to prior work, this protocol can more efficiently verify\ncomplicated computations as long as that computation is applied independently\nto many pieces of data. \n\n"}
{"id": "1304.3816", "contents": "Title: Annotations for Sparse Data Streams Abstract: Motivated by cloud computing, a number of recent works have studied annotated\ndata streams and variants thereof. In this setting, a computationally weak\nverifier (cloud user), lacking the resources to store and manipulate his\nmassive input locally, accesses a powerful but untrusted prover (cloud\nservice). The verifier must work within the restrictive data streaming\nparadigm. The prover, who can annotate the data stream as it is read, must not\njust supply the answer but also convince the verifier of its correctness.\nIdeally, both the amount of annotation and the space used by the verifier\nshould be sublinear in the relevant input size parameters.\n  A rich theory of such algorithms -- which we call schemes -- has emerged.\nPrior work has shown how to leverage the prover's power to efficiently solve\nproblems that have no non-trivial standard data stream algorithms. However,\nwhile optimal schemes are now known for several basic problems, such optimality\nholds only for streams whose length is commensurate with the size of the data\nuniverse. In contrast, many real-world datasets are relatively sparse,\nincluding graphs that contain only O(n^2) edges, and IP traffic streams that\ncontain much fewer than the total number of possible IP addresses, 2^128 in\nIPv6.\n  We design the first schemes that allow both the annotation and the space\nusage to be sublinear in the total number of stream updates rather than the\nsize of the data universe. We solve significant problems, including variations\nof INDEX, SET-DISJOINTNESS, and FREQUENCY-MOMENTS, plus several natural\nproblems on graphs. On the other hand, we give a new lower bound that, for the\nfirst time, rules out smooth tradeoffs between annotation and space usage for a\nspecific problem. Our technique brings out new nuances in Merlin-Arthur\ncommunication complexity models, and provides a separation between online\nversions of the MA and AMA models. \n\n"}
{"id": "1304.4321", "contents": "Title: Polar Codes: Speed of polarization and polynomial gap to capacity Abstract: We prove that, for all binary-input symmetric memoryless channels, polar\ncodes enable reliable communication at rates within $\\epsilon > 0$ of the\nShannon capacity with a block length, construction complexity, and decoding\ncomplexity all bounded by a {\\em polynomial} in $1/\\epsilon$. Polar coding\ngives the {\\em first known explicit construction} with rigorous proofs of all\nthese properties; previous constructions were not known to achieve capacity\nwith less than $\\exp(1/\\epsilon)$ decoding complexity except for erasure\nchannels.\n  We establish the capacity-achieving property of polar codes via a direct\nanalysis of the underlying martingale of conditional entropies, without relying\non the martingale convergence theorem. This step gives rough polarization\n(noise levels $\\approx \\epsilon$ for the \"good\" channels), which can then be\nadequately amplified by tracking the decay of the channel Bhattacharyya\nparameters. Our effective bounds imply that polar codes can have block length\n(and encoding/decoding complexity) bounded by a polynomial in $1/\\epsilon$. The\ngenerator matrix of such polar codes can be constructed in polynomial time by\nalgorithmically computing an adequate approximation of the polarization\nprocess. \n\n"}
{"id": "1304.5714", "contents": "Title: On the structure and syntactic complexity of generalized definite\n  languages Abstract: We give a forbidden pattern characterization for the class of generalized\ndefinite languages, show that the corresponding problem is NL-complete and can\nbe solved in quadratic time. We also show that their syntactic complexity\ncoincides with that of the definite languages and give an upper bound of n! for\nthis measure. \n\n"}
{"id": "1304.6023", "contents": "Title: Spaces, Trees and Colors: The Algorithmic Landscape of Document\n  Retrieval on Sequences Abstract: Document retrieval is one of the best established information retrieval\nactivities since the sixties, pervading all search engines. Its aim is to\nobtain, from a collection of text documents, those most relevant to a pattern\nquery. Current technology is mostly oriented to \"natural language\" text\ncollections, where inverted indices are the preferred solution. As successful\nas this paradigm has been, it fails to properly handle some East Asian\nlanguages and other scenarios where the \"natural language\" assumptions do not\nhold. In this survey we cover the recent research in extending the document\nretrieval techniques to a broader class of sequence collections, which has\napplications bioinformatics, data and Web mining, chemoinformatics, software\nengineering, multimedia information retrieval, and many others. We focus on the\nalgorithmic aspects of the techniques, uncovering a rich world of relations\nbetween document retrieval challenges and fundamental problems on trees,\nstrings, range queries, discrete geometry, and others. \n\n"}
{"id": "1304.6333", "contents": "Title: Unifying and generalizing known lower bounds via geometric complexity\n  theory Abstract: We show that most arithmetic circuit lower bounds and relations between lower\nbounds naturally fit into the representation-theoretic framework suggested by\ngeometric complexity theory (GCT), including: the partial derivatives technique\n(Nisan-Wigderson), the results of Razborov and Smolensky on $AC^0[p]$,\nmultilinear formula and circuit size lower bounds (Raz et al.), the degree\nbound (Strassen, Baur-Strassen), the connected components technique (Ben-Or),\ndepth 3 arithmetic circuit lower bounds over finite fields\n(Grigoriev-Karpinski), lower bounds on permanent versus determinant\n(Mignon-Ressayre, Landsberg-Manivel-Ressayre), lower bounds on matrix\nmultiplication (B\\\"{u}rgisser-Ikenmeyer) (these last two were already known to\nfit into GCT), the chasms at depth 3 and 4 (Gupta-Kayal-Kamath-Saptharishi;\nAgrawal-Vinay; Koiran), matrix rigidity (Valiant) and others. That is, the\noriginal proofs, with what is often just a little extra work, already provide\nrepresentation-theoretic obstructions in the sense of GCT for their respective\nlower bounds. This enables us to expose a new viewpoint on GCT, whereby it is a\nnatural unification and broad generalization of known results. It also shows\nthat the framework of GCT is at least as powerful as known methods, and gives\nmany new proofs-of-concept that GCT can indeed provide significant asymptotic\nlower bounds. This new viewpoint also opens up the possibility of fruitful\ntwo-way interactions between previous results and the new methods of GCT; we\nprovide several concrete suggestions of such interactions. For example, the\nrepresentation-theoretic viewpoint of GCT naturally provides new properties to\nconsider in the search for new lower bounds. \n\n"}
{"id": "1304.6800", "contents": "Title: Approximation Hardness of Graphic TSP on Cubic Graphs Abstract: We prove explicit approximation hardness results for the Graphic TSP on cubic\nand subcubic graphs as well as the new inapproximability bounds for the\ncorresponding instances of the (1,2)-TSP. The proof technique uses new modular\nconstructions of simulating gadgets for the restricted cubic and subcubic\ninstances. The modular constructions used in the paper could be also of\nindependent interest. \n\n"}
{"id": "1304.7235", "contents": "Title: Finding Short Paths on Polytopes by the Shadow Vertex Algorithm Abstract: We show that the shadow vertex algorithm can be used to compute a short path\nbetween a given pair of vertices of a polytope P = {x : Ax \\leq b} along the\nedges of P, where A \\in R^{m \\times n} is a real-valued matrix. Both, the\nlength of the path and the running time of the algorithm, are polynomial in m,\nn, and a parameter 1/delta that is a measure for the flatness of the vertices\nof P. For integer matrices A \\in Z^{m \\times n} we show a connection between\ndelta and the largest absolute value Delta of any sub-determinant of A,\nyielding a bound of O(Delta^4 m n^4) for the length of the computed path. This\nbound is expressed in the same parameter Delta as the recent non-constructive\nbound of O(Delta^2 n^4 \\log (n Delta)) by Bonifas et al.\n  For the special case of totally unimodular matrices, the length of the\ncomputed path simplifies to O(m n^4), which significantly improves the\npreviously best known constructive bound of O(m^{16} n^3 \\log^3(mn)) by Dyer\nand Frieze. \n\n"}
{"id": "1304.8135", "contents": "Title: From Hierarchical Partitions to Hierarchical Covers: Optimal\n  Fault-Tolerant Spanners for Doubling Metrics Abstract: In this paper we devise an optimal construction of fault-tolerant spanners\nfor doubling metrics. Specifically, for any $n$-point doubling metric, any\n$\\eps > 0$, and any integer $0 \\le k \\le n-2$, our construction provides a\n$k$-fault-tolerant $(1+\\eps)$-spanner with optimal degree $O(k)$ within optimal\ntime $O(n \\log n + k n)$.\n  We then strengthen this result to provide near-optimal (up to a factor of\n$\\log k$) guarantees on the diameter and weight of our spanners, namely,\ndiameter $O(\\log n)$ and weight $O(k^2 + k \\log n) \\cdot \\omega(MST)$, while\npreserving the optimal guarantees on the degree $O(k)$ and the running time\n$O(n \\log n + k n)$.\n  Our result settles several fundamental open questions in this area,\nculminating a long line of research that started with the STOC'95 paper of Arya\net al.\\ and the STOC'98 paper of Levcopoulos et al.\n  On the way to this result we develop a new technique for constructing\nspanners in doubling metrics. Our spanner construction is based on a novel\n\\emph{hierarchical cover} of the metric, whereas most previous constructions of\nspanners for doubling and Euclidean metrics (such as the net-tree spanner) are\nbased on \\emph{hierarchical partitions} of the metric. We demonstrate the power\nof hierarchical covers in the context of geometric spanners by improving the\nstate-of-the-art results in this area. \n\n"}
{"id": "1305.1021", "contents": "Title: Parameterized Quantum Query Complexity of Graph Collision Abstract: We present three new quantum algorithms in the quantum query model for\n\\textsc{graph-collision} problem: \\begin{itemize} \\item an algorithm based on\ntree decomposition that uses $O\\left(\\sqrt{n}t^{\\sfrac{1}{6}}\\right)$ queries\nwhere $t$ is the treewidth of the graph; \\item an algorithm constructed on a\nspan program that improves a result by Gavinsky and Ito. The algorithm uses\n$O(\\sqrt{n}+\\sqrt{\\alpha^{**}})$ queries, where $\\alpha^{**}(G)$ is a graph\nparameter defined by \\[\\alpha^{**}(G):=\\min_{VC\\text{-- vertex cover\nof}G}{\\max_{\\substack{I\\subseteq VC\\\\I\\text{-- independent set}}}{\\sum_{v\\in\nI}{\\deg{v}}}};\\] \\item an algorithm for a subclass of circulant graphs that\nuses $O(\\sqrt{n})$ queries. \\end{itemize} We also present an example of a\npossibly difficult graph $G$ for which all the known graphs fail to solve graph\ncollision in $O(\\sqrt{n} \\log^c n)$ queries. \n\n"}
{"id": "1305.1535", "contents": "Title: Probabilistic Constructions of Computable Objects and a Computable\n  Version of Lov\\'asz Local Lemma Abstract: A nonconstructive proof can be used to prove the existence of an object with\nsome properties without providing an explicit example of such an object. A\nspecial case is a probabilistic proof where we show that an object with\nrequired properties appears with some positive probability in some random\nprocess. Can we use such arguments to prove the existence of a computable\ninfinite object? Sometimes yes: following [8], we show how the notion of a\nlayerwise computable mapping can be used to prove a computable version of\nLov\\'asz local lemma. (A survey of Moser-Tardos proof is included to make the\npaper self-contained.) \n\n"}
{"id": "1305.6376", "contents": "Title: Fractional Pebbling Game Lower Bounds Abstract: Fractional pebbling is a generalization of black-white pebbling introduced\nrecently. In this reasearch paper we solve an open problem by proving a tight\nlower bound on the pebble weight required to fractionally pebble a balanced\nd-ary tree of height h. This bound has close ties with branching programs and\nthe separation of P from NL. \n\n"}
{"id": "1306.2187", "contents": "Title: Metric Dimension for Gabriel Unit Disk Graphs is NP-Complete Abstract: We show that finding a minimal number of landmark nodes for a unique virtual\naddressing by hop-distances in wireless ad-hoc sensor networks is NP-complete\neven if the networks are unit disk graphs that contain only Gabriel edges. This\nproblem is equivalent to Metric Dimension for Gabriel unit disk graphs. The\nGabriel edges of a unit disc graph induce a planar O(\\sqrt{n}) distance and an\noptimal energy spanner. This is one of the most interesting restrictions of\nMetric Dimension in the context of wireless multi-hop networks. \n\n"}
{"id": "1306.3649", "contents": "Title: Iterative Plan Construction for the Workflow Satisfiability Problem Abstract: The \\emph{Workflow Satisfiability Problem (WSP)} is a problem of practical\ninterest that arises whenever tasks need to be performed by authorized users,\nsubject to constraints defined by business rules. We are required to decide\nwhether there exists a \\emph{plan} -- an assignment of tasks to authorized\nusers -- such that all constraints are satisfied. Several bespoke algorithms\nhave been constructed for solving the WSP, optimised to deal with constraints\n(business rules) of particular types.\n  It is natural to see the WSP as a subclass of the {\\em Constraint\nSatisfaction Problem (CSP)} in which the variables are tasks and the domain is\nthe set of users. What makes the WSP distinctive as a CSP is that we can assume\nthat the number of tasks is very small compared to the number of users. This is\nin sharp contrast with traditional CSP models where the domain is small and the\nnumber of variables is very large. As such, it is appropriate to ask for which\nconstraint languages the WSP is fixed-parameter tractable (FPT), parameterized\nby the number of tasks. We have identified a new FPT constraint language,\nuser-independent constraint, that includes many of the constraints of interest\nin business processing systems. We are also able to prove that the union of FPT\nlanguages remains FPT if they satisfy a simple compatibility condition.\n  In this paper we present our generic algorithm, in which plans are grouped\ninto equivalence classes, each class being associated with a \\emph{pattern}. We\ndemonstrate that our generic algorithm has running time $O^*(2^{k\\log k})$,\nwhere $k$ is the number of tasks, for the language of user-independent\nconstraints. We also show that there is no algorithm of running time\n$O^*(2^{o(k\\log k)})$ for user-independent constraints unless the Exponential\nTime Hypothesis fails. \n\n"}
{"id": "1306.6710", "contents": "Title: The two-handed tile assembly model is not intrinsically universal Abstract: The well-studied Two-Handed Tile Assembly Model (2HAM) is a model of tile\nassembly in which pairs of large assemblies can bind, or self-assemble,\ntogether. In order to bind, two assemblies must have matching glues that can\nsimultaneously touch each other, and stick together with strength that is at\nleast the temperature $\\tau$, where $\\tau$ is some fixed positive integer. We\nask whether the 2HAM is intrinsically universal, in other words we ask: is\nthere a single universal 2HAM tile set $U$ which can be used to simulate any\ninstance of the model? Our main result is a negative answer to this question.\nWe show that for all $\\tau' < \\tau$, each temperature-$\\tau'$ 2HAM tile system\ndoes not simulate at least one temperature-$\\tau$ 2HAM tile system. This\nimpossibility result proves that the 2HAM is not intrinsically universal, in\nstark contrast to the simpler (single-tile addition only) abstract Tile\nAssembly Model which is intrinsically universal (\"The tile assembly model is\nintrinsically universal\", FOCS 2012). However, on the positive side, we prove\nthat, for every fixed temperature $\\tau \\geq 2$, temperature-$\\tau$ 2HAM tile\nsystems are indeed intrinsically universal: in other words, for each $\\tau$\nthere is a single universal 2HAM tile set $U$ that, when appropriately\ninitialized, is capable of simulating the behavior of any temperature-$\\tau$\n2HAM tile system. As a corollary of these results we find an infinite set of\ninfinite hierarchies of 2HAM systems with strictly increasing simulation power\nwithin each hierarchy. Finally, we show that for each $\\tau$, there is a\ntemperature-$\\tau$ 2HAM system that simultaneously simulates all\ntemperature-$\\tau$ 2HAM systems. \n\n"}
{"id": "1307.0556", "contents": "Title: The Complexity of Counting Homomorphisms to Cactus Graphs Modulo 2 Abstract: A homomorphism from a graph G to a graph H is a function from V(G) to V(H)\nthat preserves edges. Many combinatorial structures that arise in mathematics\nand computer science can be represented naturally as graph homomorphisms and as\nweighted sums of graph homomorphisms. In this paper, we study the complexity of\ncounting homomorphisms modulo 2. The complexity of modular counting was\nintroduced by Papadimitriou and Zachos and it has been pioneered by Valiant who\nfamously introduced a problem for which counting modulo 7 is easy but counting\nmodulo 2 is intractable. Modular counting provides a rich setting in which to\nstudy the structure of homomorphism problems. In this case, the structure of\nthe graph H has a big influence on the complexity of the problem. Thus, our\napproach is graph-theoretic. We give a complete solution for the class of\ncactus graphs, which are connected graphs in which every edge belongs to at\nmost one cycle. Cactus graphs arise in many applications such as the modelling\nof wireless sensor networks and the comparison of genomes. We show that, for\nsome cactus graphs H, counting homomorphisms to H modulo 2 can be done in\npolynomial time. For every other fixed cactus graph H, the problem is complete\nfor the complexity class parity-P which is a wide complexity class to which\nevery problem in the polynomial hierarchy can be reduced (using randomised\nreductions). Determining which H lead to tractable problems can be done in\npolynomial time. Our result builds upon the work of Faben and Jerrum, who gave\na dichotomy for the case in which H is a tree. \n\n"}
{"id": "1307.1353", "contents": "Title: One Hierarchy Spawns Another: Graph Deconstructions and the Complexity\n  Classification of Conjunctive Queries Abstract: We study the problem of conjunctive query evaluation relative to a class of\nqueries; this problem is formulated here as the relational homomorphism problem\nrelative to a class of structures A, wherein each instance must be a pair of\nstructures such that the first structure is an element of A. We present a\ncomprehensive complexity classification of these problems, which strongly links\ngraph-theoretic properties of A to the complexity of the corresponding\nhomomorphism problem. In particular, we define a binary relation on graph\nclasses, which is a preorder, and completely describe the resulting hierarchy\ngiven by this relation. This relation is defined in terms of a notion which we\ncall graph deconstruction and which is a variant of the well-known notion of\ntree decomposition. We then use this hierarchy of graph classes to infer a\ncomplexity hierarchy of homomorphism problems which is comprehensive up to a\ncomputationally very weak notion of reduction, namely, a parameterized version\nof quantifier-free first-order reduction. In doing so, we obtain a\nsignificantly refined complexity classification of homomorphism problems, as\nwell as a unifying, modular, and conceptually clean treatment of existing\ncomplexity classifications. We then present and develop the theory of\nEhrenfeucht-Fraisse-style pebble games which solve the homomorphism problems\nwhere the cores of the structures in A have bounded tree depth. Finally, we use\nour framework to classify the complexity of model checking existential\nsentences having bounded quantifier rank. \n\n"}
{"id": "1307.2863", "contents": "Title: Dynamic Data Structure for Tree-Depth Decomposition Abstract: We present a dynamic data structure for representing a graph $G$ with\ntree-depth at most $D$. Tree-depth is an important graph parameter which arose\nin the study of sparse graph classes.\n  The structure allows addition and removal of edges and vertices such that the\nresulting graph still has tree-depth at most $D$, in time bounds depending only\non $D$. A tree-depth decomposition of the graph is maintained explicitly.\n  This makes the data structure useful for dynamization of static algorithms\nfor graphs with bounded tree-depth. As an example application, we give a\ndynamic data structure for MSO-property testing, with time bounds for removal\ndepending only on $D$ and constant-time testing of the property, while the time\nfor the initialization and insertion also depends on the size of the formula\nexpressing the property. \n\n"}
{"id": "1307.3142", "contents": "Title: Perfect Codes in the Discrete Simplex Abstract: We study the problem of existence of (nontrivial) perfect codes in the\ndiscrete $ n $-simplex $ \\Delta_{\\ell}^n := \\left\\{ \\begin{pmatrix} x_0,\n\\ldots, x_n \\end{pmatrix} : x_i \\in \\mathbb{Z}_{+}, \\sum_i x_i = \\ell \\right\\}\n$ under $ \\ell_1 $ metric. The problem is motivated by the so-called multiset\ncodes, which have recently been introduced by the authors as appropriate\nconstructs for error correction in the permutation channels. It is shown that $\ne $-perfect codes in the $ 1 $-simplex $ \\Delta_{\\ell}^1 $ exist for any $ \\ell\n\\geq 2e + 1 $, the $ 2 $-simplex $ \\Delta_{\\ell}^2 $ admits an $ e $-perfect\ncode if and only if $ \\ell = 3e + 1 $, while there are no perfect codes in\nhigher-dimensional simplices. In other words, perfect multiset codes exist only\nover binary and ternary alphabets. \n\n"}
{"id": "1307.5001", "contents": "Title: On Lower Complexity Bounds for Large-Scale Smooth Convex Optimization Abstract: We derive lower bounds on the black-box oracle complexity of large-scale\nsmooth convex minimization problems, with emphasis on minimizing smooth (with\nHolder continuous, with a given exponent and constant, gradient) convex\nfunctions over high-dimensional ||.||_p-balls, 1<=p<=\\infty. Our bounds turn\nout to be tight (up to logarithmic in the design dimension factors), and can be\nviewed as a substantial extension of the existing lower complexity bounds for\nlarge-scale convex minimization covering the nonsmooth case and the 'Euclidean'\nsmooth case (minimization of convex functions with Lipschitz continuous\ngradients over Euclidean balls). As a byproduct of our results, we demonstrate\nthat the classical Conditional Gradient algorithm is near-optimal, in the sense\nof Information-Based Complexity Theory, when minimizing smooth convex functions\nover high-dimensional ||.||_\\infty-balls and their matrix analogies -- spectral\nnorm balls in the spaces of square matrices. \n\n"}
{"id": "1307.5582", "contents": "Title: Random Rates for 0-Extension and Low-Diameter Decompositions Abstract: Consider the problem of partitioning an arbitrary metric space into pieces of\ndiameter at most \\Delta, such every pair of points is separated with relatively\nlow probability. We propose a rate-based algorithm inspired by\nmultiplicatively-weighted Voronoi diagrams, and prove it has optimal\ntrade-offs. This also gives us another logarithmic approximation algorithm for\nthe 0-extension problem. \n\n"}
{"id": "1307.7176", "contents": "Title: Phase retrieval from very few measurements Abstract: In many applications, signals are measured according to a linear process, but\nthe phases of these measurements are often unreliable or not available. To\nreconstruct the signal, one must perform a process known as phase retrieval.\nThis paper focuses on completely determining signals with as few intensity\nmeasurements as possible, and on efficient phase retrieval algorithms from such\nmeasurements. For the case of complex M-dimensional signals, we construct a\nmeasurement ensemble of size 4M-4 which yields injective intensity\nmeasurements; this is conjectured to be the smallest such ensemble. For the\ncase of real signals, we devise a theory of \"almost\" injective intensity\nmeasurements, and we characterize such ensembles. Later, we show that phase\nretrieval from M+1 almost injective intensity measurements is NP-hard,\nindicating that computationally efficient phase retrieval must come at the\nprice of measurement redundancy. \n\n"}
{"id": "1308.0068", "contents": "Title: Communication lower bounds and optimal algorithms for programs that\n  reference arrays -- Part 1 Abstract: The movement of data (communication) between levels of a memory hierarchy, or\nbetween parallel processors on a network, can greatly dominate the cost of\ncomputation, so algorithms that minimize communication are of interest.\nMotivated by this, attainable lower bounds for the amount of communication\nrequired by algorithms were established by several groups for a variety of\nalgorithms, including matrix computations. Prior work of\nBallard-Demmel-Holtz-Schwartz relied on a geometric inequality of Loomis and\nWhitney for this purpose. In this paper the general theory of discrete\nmultilinear Holder-Brascamp-Lieb (HBL) inequalities is used to establish\ncommunication lower bounds for a much wider class of algorithms. In some cases,\nalgorithms are presented which attain these lower bounds.\n  Several contributions are made to the theory of HBL inequalities proper. The\noptimal constant in such an inequality for torsion-free Abelian groups is shown\nto equal one whenever it is finite. Bennett-Carbery-Christ-Tao had\ncharacterized the tuples of exponents for which such an inequality is valid as\nthe convex polyhedron defined by a certain finite list of inequalities. The\nproblem of constructing an algorithm to decide whether a given inequality is on\nthis list, is shown to be equivalent to Hilbert's Tenth Problem over the\nrationals, which remains open. Nonetheless, an algorithm which computes the\npolyhedron itself is constructed. \n\n"}
{"id": "1308.0180", "contents": "Title: Space complexity of list H-colouring: a dichotomy Abstract: The Dichotomy Conjecture for constraint satisfaction problems (CSPs) states\nthat every CSP is in P or is NP-complete (Feder-Vardi, 1993). It has been\nverified for conservative problems (also known as list homomorphism problems)\nby A. Bulatov (2003). We augment this result by showing that for digraph\ntemplates H, every conservative CSP, denoted LHOM(H), is solvable in logspace\nor is hard for NL. More precisely, we introduce a digraph structure we call a\ncircular N, and prove the following dichotomy: if H contains no circular N then\nLHOM(H) admits a logspace algorithm, and otherwise LHOM(H) is hard for NL. Our\nalgorithm operates by reducing the lists in a complex manner based on a novel\ndecomposition of an auxiliary digraph, combined with repeated applications of\nReingold's algorithm for undirected reachability (2005). We also prove an\nalgebraic version of this dichotomy: the digraphs without a circular N are\nprecisely those that admit a finite chain of polymorphisms satisfying the\nHagemann-Mitschke identities. This confirms a conjecture of Larose and Tesson\n(2007) for LHOM(H). Moreover, we show that the presence of a circular N can be\ndecided in time polynomial in the size of H. \n\n"}
{"id": "1308.1068", "contents": "Title: List H-Coloring a Graph by Removing Few Vertices Abstract: In the deletion version of the list homomorphism problem, we are given graphs\nG and H, a list L(v) that is a subset of V(H) for each vertex v of G, and an\ninteger k. The task is to decide whether there exists a subset W of V(G) of\nsize at most k such that there is a homomorphism from G \\ W to H respecting the\nlists. We show that DL-Hom(H), parameterized by k and |H|, is fixed-parameter\ntractable for any (P6, C6)-free bipartite graph H; already for this restricted\nclass of graphs, the problem generalizes Vertex Cover, Odd Cycle Transversal,\nand Vertex Multiway Cut parameterized by the size of the cutset and the number\nof terminals. We conjecture that DL-Hom(H) is fixed-parameter tractable for the\nclass of graphs H for which the list homomorphism problem (without deletions)\nis polynomial-time solvable; by a result of Feder, Hell and Huang (1999), a\ngraph H belongs to this class precisely if it is a bipartite graph whose\ncomplement is a circular arc graph. We show that this conjecture is equivalent\nto the fixed-parameter tractability of a single fairly natural satisfiability\nproblem, Clause Deletion Chain-SAT. \n\n"}
{"id": "1308.2409", "contents": "Title: On the Parameterized Complexity of Reconfiguration Problems Abstract: We present the first results on the parameterized complexity of\nreconfiguration problems, where a reconfiguration version of an optimization\nproblem $Q$ takes as input two feasible solutions $S$ and $T$ and determines if\nthere is a sequence of {\\em reconfiguration steps} that can be applied to\ntransform $S$ into $T$ such that each step results in a feasible solution to\n$Q$. For most of the results in this paper, $S$ and $T$ are subsets of vertices\nof a given graph and a reconfiguration step adds or deletes a vertex. Our study\nis motivated by recent results establishing that for most NP-hard problems, the\nclassical complexity of reconfiguration is PSPACE-complete. We address the\nquestion for several important graph properties under two natural\nparameterizations: $k$, the size of the solutions, and $\\ell$, the length of\nthe sequence of steps. Our first general result is an algorithmic paradigm, the\n{\\em reconfiguration kernel}, used to obtain fixed-parameter algorithms for the\nreconfiguration versions of {\\sc Vertex Cover} and, more generally, {\\sc\nBounded Hitting Set} and {\\sc Feedback Vertex Set}, all parameterized by $k$.\nIn contrast, we show that reconfiguring {\\sc Unbounded Hitting Set} is\n$W[2]$-hard when parameterized by $k+\\ell$. We also demonstrate the\n$W[1]$-hardness of the reconfiguration versions of a large class of\nmaximization problems parameterized by $k+\\ell$, and of their corresponding\ndeletion problems parameterized by $\\ell$; in doing so, we show that there\nexist problems in FPT when parameterized by $k$, but whose reconfiguration\nversions are $W[1]$-hard when parameterized by $k+\\ell$. \n\n"}
{"id": "1308.2599", "contents": "Title: Parameterized Rural Postman Problem Abstract: The Directed Rural Postman Problem (DRPP) can be formulated as follows: given\na strongly connected directed multigraph $D=(V,A)$ with nonnegative integral\nweights on the arcs, a subset $R$ of $A$ and a nonnegative integer $\\ell$,\ndecide whether $D$ has a closed directed walk containing every arc of $R$ and\nof total weight at most $\\ell$. Let $k$ be the number of weakly connected\ncomponents in the the subgraph of $D$ induced by $R$. Sorge et al. (2012) ask\nwhether the DRPP is fixed-parameter tractable (FPT) when parameterized by $k$,\ni.e., whether there is an algorithm of running time $O^*(f(k))$ where $f$ is a\nfunction of $k$ only and the $O^*$ notation suppresses polynomial factors.\nSorge et al. (2012) note that this question is of significant practical\nrelevance and has been open for more than thirty years. Using an algebraic\napproach, we prove that DRPP has a randomized algorithm of running time\n$O^*(2^k)$ when $\\ell$ is bounded by a polynomial in the number of vertices in\n$D$. We also show that the same result holds for the undirected version of\nDRPP, where $D$ is a connected undirected multigraph. \n\n"}
{"id": "1308.2617", "contents": "Title: Independent Set, Induced Matching, and Pricing: Connections and Tight\n  (Subexponential Time) Approximation Hardnesses Abstract: We present a series of almost settled inapproximability results for three\nfundamental problems. The first in our series is the subexponential-time\ninapproximability of the maximum independent set problem, a question studied in\nthe area of parameterized complexity. The second is the hardness of\napproximating the maximum induced matching problem on bounded-degree bipartite\ngraphs. The last in our series is the tight hardness of approximating the\nk-hypergraph pricing problem, a fundamental problem arising from the area of\nalgorithmic game theory. In particular, assuming the Exponential Time\nHypothesis, our two main results are:\n  - For any r larger than some constant, any r-approximation algorithm for the\nmaximum independent set problem must run in at least\n2^{n^{1-\\epsilon}/r^{1+\\epsilon}} time. This nearly matches the upper bound of\n2^{n/r} (Cygan et al., 2008). It also improves some hardness results in the\ndomain of parameterized complexity (e.g., Escoffier et al., 2012 and Chitnis et\nal., 2013)\n  - For any k larger than some constant, there is no polynomial time min\n(k^{1-\\epsilon}, n^{1/2-\\epsilon})-approximation algorithm for the k-hypergraph\npricing problem, where n is the number of vertices in an input graph. This\nalmost matches the upper bound of min (O(k), \\tilde O(\\sqrt{n})) (by Balcan and\nBlum, 2007 and an algorithm in this paper).\n  We note an interesting fact that, in contrast to n^{1/2-\\epsilon} hardness\nfor polynomial-time algorithms, the k-hypergraph pricing problem admits\nn^{\\delta} approximation for any \\delta >0 in quasi-polynomial time. This puts\nthis problem in a rare approximability class in which approximability\nthresholds can be improved significantly by allowing algorithms to run in\nquasi-polynomial time. \n\n"}
{"id": "1308.3613", "contents": "Title: Polynomial kernels collapse the W-hierarchy Abstract: We prove that, for many parameterized problems in the class FPT, the\nexistence of polynomial kernels implies the collapse of the W-hierarchy (i.e.,\nW[P] = FPT). The collapsing results are also extended to assumed exponential\nkernels for problems in the class FPT. In particular, we establish a close\nrelationship between polynomial (and exponential) kernelizability and the\nexistence of sub-exponential time algorithms for a spectrum of circuit\nsatisfiability problems in FPT. To the best of our knowledge, this is the first\nwork that connects hardness for polynomial kernelizability of FPT problems to\nparameterized intractability. Our work also offers some new insights into the\nclass FPT. \n\n"}
{"id": "1308.4469", "contents": "Title: External Memory Algorithms For Path Traversal in Graphs Abstract: This thesis presents a number of results related to path traversal in trees\nand graphs. In particular, we focus on data structures which allow such\ntraversals to be performed efficiently in the external memory setting. In\naddition, for trees and planar graphs the data structures we present are\nsuccinct. Our tree structures permit efficient bottom-up path traversal in\nrooted trees of arbitrary degree and efficient top-down path traversal in\nbinary trees. In the graph setting, we permit efficient traversal of an\narbitrary path in bounded degree planar graphs. Our data structures for both\ntrees and graphs match or slightly improve current best results for external\nmemory path traversal in these settings while at the same time improving space\nbounds due to the succinct nature of our data structures. Employing our path\ntraversal structure for bounded degree planar graphs, we describe a number of\nuseful applications of this technique for triangular meshes in R^2. As an\nextension of the R^2 representation for triangular meshes we also present an\nefficient external memory representation for well-shaped tetrahedral meshes in\nR^3. The external memory representation we present is based on a partitioning\nscheme that matches the current best-known results for well-shaped tetrahedral\nmeshes. We describe applications of path traversal in tetrahedral meshes which\nare made efficient in the external memory setting using our structure. Finally,\nwe present a result on using jump-and-walk point location in well-shaped meshes\nin both R^2 and R^3. We demonstrate that, given an approximate nearest\nneighbour from among the vertices of a mesh, locating the simplex containing\nthe query point involves a constant length walk (path traversal) in the mesh. \n\n"}
{"id": "1308.5741", "contents": "Title: Fixed parameter tractability of crossing minimization of almost-trees Abstract: We investigate exact crossing minimization for graphs that differ from trees\nby a small number of additional edges, for several variants of the crossing\nminimization problem. In particular, we provide fixed parameter tractable\nalgorithms for the 1-page book crossing number, the 2-page book crossing\nnumber, and the minimum number of crossed edges in 1-page and 2-page book\ndrawings. \n\n"}
{"id": "1308.6384", "contents": "Title: Collecting Coupons with Random Initial Stake Abstract: Motivated by a problem in the theory of randomized search heuristics, we give\na very precise analysis for the coupon collector problem where the collector\nstarts with a random set of coupons (chosen uniformly from all sets).\n  We show that the expected number of rounds until we have a coupon of each\ntype is $nH_{n/2} - 1/2 \\pm o(1)$, where $H_{n/2}$ denotes the $(n/2)$th\nharmonic number when $n$ is even, and $H_{n/2}:= (1/2) H_{\\lfloor n/2 \\rfloor}\n+ (1/2) H_{\\lceil n/2 \\rceil}$ when $n$ is odd. Consequently, the coupon\ncollector with random initial stake is by half a round faster than the one\nstarting with exactly $n/2$ coupons (apart from additive $o(1)$ terms).\n  This result implies that classic simple heuristic called \\emph{randomized\nlocal search} needs an expected number of $nH_{n/2} - 1/2 \\pm o(1)$ iterations\nto find the optimum of any monotonic function defined on bit-strings of length\n$n$. \n\n"}
{"id": "1309.0157", "contents": "Title: A complementary construction using mutually unbiased bases Abstract: We present a construction for complementary pairs of arrays that exploits a\nset of mutually-unbiased bases, and enumerate these arrays as well as the\ncorresponding set of complementary sequences obtained from the arrays by\nprojection. We also sketch an algorithm to uniquely generate these sequences.\nThe pairwise squared inner-product of members of the sequence set is shown to\nbe $\\frac{1}{2}$. Moreover, a subset of the set can be viewed as a codebook\nthat asymptotically achieves $\\sqrt{\\frac{3}{2}}$ times the Welch bound. \n\n"}
{"id": "1309.0436", "contents": "Title: A Non-Interactive Quantum Bit Commitment Scheme that Exploits the\n  Computational Hardness of Quantum State Distinction Abstract: We propose an efficient quantum protocol performing quantum bit commitment,\nwhich is a simple cryptographic primitive involved with two parties, called a\ncommitter and a verifier. Our protocol is non-interactive, uses no supplemental\nshared information, and achieves computational concealing and statistical\nbinding under a natural complexity-theoretical assumption. An earlier protocol\nin the literature relies on the existence of an efficient quantum one-way\nfunction. Our protocol, on the contrary, exploits a seemingly weaker assumption\non computational difficulty of distinguishing two specific ensembles of reduced\nquantum states. This assumption is guaranteed by, for example, computational\nhardness of solving the graph automorphism problem efficiently on a quantum\ncomputer. \n\n"}
{"id": "1309.0563", "contents": "Title: Approximate Constraint Satisfaction Requires Large LP Relaxations Abstract: We prove super-polynomial lower bounds on the size of linear programming\nrelaxations for approximation versions of constraint satisfaction problems. We\nshow that for these problems, polynomial-sized linear programs are exactly as\npowerful as programs arising from a constant number of rounds of the\nSherali-Adams hierarchy.\n  In particular, any polynomial-sized linear program for Max Cut has an\nintegrality gap of 1/2 and any such linear program for Max 3-Sat has an\nintegrality gap of 7/8. \n\n"}
{"id": "1309.1453", "contents": "Title: Parallel machine scheduling with step deteriorating jobs and setup times\n  by a hybrid discrete cuckoo search algorithm Abstract: This article considers the parallel machine scheduling problem with\nstep-deteriorating jobs and sequence-dependent setup times. The objective is to\nminimize the total tardiness by determining the allocation and sequence of jobs\non identical parallel machines. In this problem, the processing time of each\njob is a step function dependent upon its starting time. An individual extended\ntime is penalized when the starting time of a job is later than a specific\ndeterioration date. The possibility of deterioration of a job makes the\nparallel machine scheduling problem more challenging than ordinary ones. A\nmixed integer programming model for the optimal solution is derived. Due to its\nNP-hard nature, a hybrid discrete cuckoo search algorithm is proposed to solve\nthis problem. In order to generate a good initial swarm, a modified heuristic\nnamed the MBHG is incorporated into the initialization of population. Several\ndiscrete operators are proposed in the random walk of L\\'{e}vy Flights and the\ncrossover search. Moreover, a local search procedure based on variable\nneighborhood descent is integrated into the algorithm as a hybrid strategy in\norder to improve the quality of elite solutions. Computational experiments are\nexecuted on two sets of randomly generated test instances. The results show\nthat the proposed hybrid algorithm can yield better solutions in comparison\nwith the commercial solver CPLEX with one hour time limit, discrete cuckoo\nsearch algorithm and the existing variable neighborhood search algorithm. \n\n"}
{"id": "1309.1776", "contents": "Title: Algorithms for group isomorphism via group extensions and cohomology Abstract: The isomorphism problem for finite groups of order n (GpI) has long been\nknown to be solvable in $n^{\\log n+O(1)}$ time, but only recently were\npolynomial-time algorithms designed for several interesting group classes.\nInspired by recent progress, we revisit the strategy for GpI via the extension\ntheory of groups.\n  The extension theory describes how a normal subgroup N is related to G/N via\nG, and this naturally leads to a divide-and-conquer strategy that splits GpI\ninto two subproblems: one regarding group actions on other groups, and one\nregarding group cohomology. When the normal subgroup N is abelian, this\nstrategy is well-known. Our first contribution is to extend this strategy to\nhandle the case when N is not necessarily abelian. This allows us to provide a\nunified explanation of all recent polynomial-time algorithms for special group\nclasses.\n  Guided by this strategy, to make further progress on GpI, we consider\ncentral-radical groups, proposed in Babai et al. (SODA 2011): the class of\ngroups such that G mod its center has no abelian normal subgroups. This class\nis a natural extension of the group class considered by Babai et al. (ICALP\n2012), namely those groups with no abelian normal subgroups. Following the\nabove strategy, we solve GpI in $n^{O(\\log \\log n)}$ time for central-radical\ngroups, and in polynomial time for several prominent subclasses of\ncentral-radical groups. We also solve GpI in $n^{O(\\log\\log n)}$ time for\ngroups whose solvable normal subgroups are elementary abelian but not\nnecessarily central. As far as we are aware, this is the first time there have\nbeen worst-case guarantees on a $n^{o(\\log n)}$-time algorithm that tackles\nboth aspects of GpI---actions and cohomology---simultaneously. \n\n"}
{"id": "1309.2987", "contents": "Title: The Average Sensitivity of an Intersection of Half Spaces Abstract: We prove new bounds on the average sensitivity of the indicator function of\nan intersection of $k$ halfspaces. In particular, we prove the optimal bound of\n$O(\\sqrt{n\\log(k)})$. This generalizes a result of Nazarov, who proved the\nanalogous result in the Gaussian case, and improves upon a result of Harsha,\nKlivans and Meka. Furthermore, our result has implications for the runtime\nrequired to learn intersections of halfspaces. \n\n"}
{"id": "1309.4033", "contents": "Title: The complexity of parity graph homomorphism: an initial investigation Abstract: Given a graph G, we investigate the question of determining the parity of the\nnumber of homomorphisms from G to some other fixed graph H. We conjecture that\nthis problem exhibits a complexity dichotomy, such that all parity graph\nhomomorphism problems are either polynomial-time solvable or parityP-complete,\nand provide a conjectured characterisation of the easy cases.\n  We show that the conjecture is true for the restricted case in which the\ngraph H is a tree, and provide some tools that may be useful in further\ninvestigation into the parity graph homomorphism problem, and the problem of\ncounting homomorphisms for other moduli. \n\n"}
{"id": "1309.4266", "contents": "Title: Complexities of relational structures Abstract: The relational complexity, introduced by G. Cherlin, G. Martin, and D.\nSaracino, is a measure of ultrahomogeneity of a relational structure. It\nprovides an information on minimal arity of additional invariant relations\nneeded to turn given structure into an ultrahomogeneous one. The original\nmotivation was group theory. This work focuses more on structures and provides\nan alternative approach. Our study is motivated by related concept of lift\ncomplexity studied by Hubicka and Nesetril. \n\n"}
{"id": "1309.5206", "contents": "Title: New Algorithms for Solving Tropical Linear Systems Abstract: The problem of solving tropical linear systems, a natural problem of tropical\nmathematics, has already proven to be very interesting from the algorithmic\npoint of view: it is known to be in $NP\\cap coNP$ but no polynomial time\nalgorithm is known, although counterexamples for existing pseudopolynomial\nalgorithms are (and have to be) very complex.\n  In this work, we continue the study of algorithms for solving tropical linear\nsystems. First, we present a new reformulation of Grigoriev's algorithm that\nbrings it closer to the algorithm of Akian, Gaubert, and Guterman; this lets us\nformulate a whole family of new algorithms, and we present algorithms from this\nfamily for which no known superpolynomial counterexamples work. Second, we\npresent a family of algorithms for solving overdetermined tropical systems. We\nshow that for weakly overdetermined systems, there are polynomial algorithms in\nthis family. We also present a concrete algorithm from this family that can\nsolve a tropical linear system defined by an $m\\times n$ matrix with maximal\nelement $M$ in time $\\Theta\\left({m \\choose n} \\mathrm{poly}\\left(m, n, \\log\nM\\right)\\right)$, and this time matches the complexity of the best of\npreviously known algorithms for feasibility testing. \n\n"}
{"id": "1309.6078", "contents": "Title: Discordant Compact Logic-Arithmetic Structures in Discrete Optimization\n  Problems Abstract: In sphere of research of discrete optimization algorithms efficiency the\nimportant place occupies a method of polynomial reducibility of some problems\nto others with use of special purpose components. In this paper a novel method\nof compact representation for sets of binary sequences in the form of \"compact\ntriplets structures\" (CTS) and \"compact couples structures\" (CCS) is stated,\nsupposing both logic and arithmetic interpretation of data. It is shown that\nany non-empty CTS in dual interpretation represents some unique Boolean formula\nin 3-CNF and the tabular CTS contains all satisfyig sets of the formula as\nconcatenations of the triplets chosen from the neighbouring tiers. In general,\nany 3-CNF formula is transformed by decomposition to a system of discordant\nCTS's, each being associated with an individual permutation of variables\nconstructed by a polynomial algorithm. As a result the problem of the formula\nsatisfiability is reduced to the following one: ascertain the fact of existence\n(or absence) of a \"joint satisfying set\" (JSS) for all discordant structures,\nbased on the different permutations. Further transformation of each CTS to CCS\nis used; correctness of preservation of the allowed sets is reached by simple\nalgorithmic restrictions on triplets concatenation. Then the procedure of\n\"inverting of the same name columns\" in the various structures is entered for\nthe purpose of reducing the problem of JSS revealing to elementary detection of\nn-tuples of zeros in the CCS system. The formula is synthesized, being on the\nstructure a variation of 2-CNF, associated with the calculation procedure\nrealizing adaptation of the polynomial algorithm of constraints distribution\n(well-known in the optimization theory) to the efficient resolving Boolean\nformula coded by means of discordant compact structures. \n\n"}
{"id": "1310.1649", "contents": "Title: QuickLexSort: An efficient algorithm for lexicographically sorting\n  nested restrictions of a database Abstract: Lexicographical sorting is a fundamental problem with applications to\ncontingency tables, databases, Bayesian networks, and more. A standard method\nto lexicographically sort general data is to iteratively use a stable sort -- a\nsort which preserves existing orders. Here we present a new method of\nlexicographical sorting called QuickLexSort. Whereas a stable sort based\nlexicographical sorting algorithm operates from the least important to most\nimportant features, in contrast, QuickLexSort sorts from the most important to\nleast important features, refining the sort as it goes. QuickLexSort first\nrequires a one-time modest pre-processing step where each feature of the data\nset is sorted independently. When lexicographically sorting a database,\nQuickLexSort (including pre-processing) has comparable running time to using a\nstable sort based approach. For a data base with $m$ rows and $n$ columns, and\na sorting algorithm running in time $O(mlog(m))$, a stable sort based\nlexicographical sort and QuickLexSort will both take time $O(nmlog(m))$.\nHowever in many applications one has the need to lexicographically sort nested\ndata, e.g.\\ all possible sub-matrices up to a certain cardinality of columns.\nIn such cases we show QuickLexSort gives a performance improvement of a log\nfactor of the database length (rows in matrix) over using a standard stable\nsort based approach. E.g.\\ to sort all sub-matrices up to cardinality $k$,\nQuickLexSort has running time $O(mn^k)$ whereas a stable sort based\nlexicographical sort will take time $O(mlog(m)n^k)$. After the pre-processing\nstep that is run only once for the entire matrix, QuickLexSort has a running\ntime linear in the number of nested sub-matrices to sort. We conclude with an\napplication to Bayesian network scoring to detect epistasis using SNP marker\ndata. \n\n"}
{"id": "1310.2841", "contents": "Title: Half-integrality, LP-branching and FPT Algorithms Abstract: A recent trend in parameterized algorithms is the application of polytope\ntools (specifically, LP-branching) to FPT algorithms (e.g., Cygan et al., 2011;\nNarayanaswamy et al., 2012). However, although interesting results have been\nachieved, the methods require the underlying polytope to have very restrictive\nproperties (half-integrality and persistence), which are known only for few\nproblems (essentially Vertex Cover (Nemhauser and Trotter, 1975) and Node\nMultiway Cut (Garg et al., 1994)). Taking a slightly different approach, we\nview half-integrality as a \\emph{discrete} relaxation of a problem, e.g., a\nrelaxation of the search space from $\\{0,1\\}^V$ to $\\{0,1/2,1\\}^V$ such that\nthe new problem admits a polynomial-time exact solution. Using tools from CSP\n(in particular Thapper and \\v{Z}ivn\\'y, 2012) to study the existence of such\nrelaxations, we provide a much broader class of half-integral polytopes with\nthe required properties, unifying and extending previously known cases.\n  In addition to the insight into problems with half-integral relaxations, our\nresults yield a range of new and improved FPT algorithms, including an\n$O^*(|\\Sigma|^{2k})$-time algorithm for node-deletion Unique Label Cover with\nlabel set $\\Sigma$ and an $O^*(4^k)$-time algorithm for Group Feedback Vertex\nSet, including the setting where the group is only given by oracle access. All\nthese significantly improve on previous results. The latter result also implies\nthe first single-exponential time FPT algorithm for Subset Feedback Vertex Set,\nanswering an open question of Cygan et al. (2012).\n  Additionally, we propose a network flow-based approach to solve some cases of\nthe relaxation problem. This gives the first linear-time FPT algorithm to\nedge-deletion Unique Label Cover. \n\n"}
{"id": "1310.6524", "contents": "Title: Some hard families of parameterised counting problems Abstract: We consider parameterised subgraph-counting problems of the following form:\ngiven a graph G, how many k-tuples of its vertices have a given property? A\nnumber of such problems are known to be #W[1]-complete; here we substantially\ngeneralise some of these existing results by proving hardness for two large\nfamilies of such problems. We demonstrate that it is #W[1]-hard to count the\nnumber of k-vertex subgraphs having any property where the number of distinct\nedge-densities of labelled subgraphs that satisfy the property is o(k^2). In\nthe special case that the property in question depends only on the number of\nedges in the subgraph, we give a strengthening of this result which leads to\nour second family of hard problems. \n\n"}
{"id": "1311.1029", "contents": "Title: Problems in number theory from busy beaver competition Abstract: By introducing the busy beaver competition of Turing machines, in 1962, Rado\ndefined noncomputable functions on positive integers. The study of these\nfunctions and variants leads to many mathematical challenges. This article\ntakes up the following one: How can a small Turing machine manage to produce\nvery big numbers? It provides the following answer: mostly by simulating\nCollatz-like functions, that are generalizations of the famous 3x+1 function.\nThese functions, like the 3x+1 function, lead to new unsolved problems in\nnumber theory. \n\n"}
{"id": "1311.4066", "contents": "Title: Polynomial-time Solvable #CSP Problems via Algebraic Models and Pfaffian\n  Circuits Abstract: A Pfaffian circuit is a tensor contraction network where the edges are\nlabeled with changes of bases in such a way that a very specific set of\ncombinatorial properties are satisfied. By modeling the permissible changes of\nbases as systems of polynomial equations, and then solving via computation, we\nare able to identify classes of 0/1 planar #CSP problems solvable in\npolynomial-time via the Pfaffian circuit evaluation theorem (a variant of L.\nValiant's Holant Theorem). We present two different models of 0/1 variables,\none that is possible under a homogeneous change of basis, and one that is\npossible under a heterogeneous change of basis only. We enumerate a series of\n1,2,3, and 4-arity gates/cogates that represent constraints, and define a class\nof constraints that is possible under the assumption of a ``bridge\" between two\nparticular changes of bases. We discuss the issue of planarity of Pfaffian\ncircuits, and demonstrate possible directions in algebraic computation for\ndesigning a Pfaffian tensor contraction network fragment that can simulate a\nswap gate/cogate. We conclude by developing the notion of a decomposable\ngate/cogate, and discuss the computational benefits of this definition. \n\n"}
{"id": "1311.4451", "contents": "Title: #BIS-Hardness for 2-Spin Systems on Bipartite Bounded Degree Graphs in\n  the Tree Nonuniqueness Region Abstract: Counting independent sets on bipartite graphs (#BIS) is considered a\ncanonical counting problem of intermediate approximation complexity. It is\nconjectured that #BIS neither has an FPRAS nor is as hard as #SAT to\napproximate. We study #BIS in the general framework of two-state spin systems\non bipartite graphs. We define two notions, nearly-independent phase-correlated\nspins and unary symmetry breaking. We prove that it is #BIS-hard to approximate\nthe partition function of any 2-spin system on bipartite graphs supporting\nthese two notions. As a consequence, we classify the complexity of\napproximating the partition function of antiferromagnetic 2-spin systems on\nbounded-degree bipartite graphs. \n\n"}
{"id": "1311.5022", "contents": "Title: Extended Formulations for Online Linear Bandit Optimization Abstract: On-line linear optimization on combinatorial action sets (d-dimensional\nactions) with bandit feedback, is known to have complexity in the order of the\ndimension of the problem. The exponential weighted strategy achieves the best\nknown regret bound that is of the order of $d^{2}\\sqrt{n}$ (where $d$ is the\ndimension of the problem, $n$ is the time horizon). However, such strategies\nare provably suboptimal or computationally inefficient. The complexity is\nattributed to the combinatorial structure of the action set and the dearth of\nefficient exploration strategies of the set. Mirror descent with entropic\nregularization function comes close to solving this problem by enforcing a\nmeticulous projection of weights with an inherent boundary condition. Entropic\nregularization in mirror descent is the only known way of achieving a\nlogarithmic dependence on the dimension. Here, we argue otherwise and recover\nthe original intuition of exponential weighting by borrowing a technique from\ndiscrete optimization and approximation algorithms called `extended\nformulation'. Such formulations appeal to the underlying geometry of the set\nwith a guaranteed logarithmic dependence on the dimension underpinned by an\ninformation theoretic entropic analysis. \n\n"}
{"id": "1311.6209", "contents": "Title: Distributed Algorithms for Large-Scale Graphs Abstract: Motivated by the increasing need for fast processing of large-scale graphs,\nwe study a number of fundamental graph problems in a message-passing model for\ndistributed computing, called $k$-machine model, where we have $k$ machines\nthat jointly perform computations on $n$-node graphs. The graph is assumed to\nbe partitioned in a balanced fashion among the $k$ machines, a common\nimplementation in many real-world systems. Communication is point-to-point via\nbandwidth-constrained links, and the goal is to minimize the round complexity,\ni.e., the number of communication rounds required to finish a computation.\n  We present a generic methodology that allows to obtain efficient algorithms\nin the $k$-machine model using distributed algorithms for the classical CONGEST\nmodel of distributed computing. Using this methodology, we obtain algorithms\nfor various fundamental graph problems such as connectivity, minimum spanning\ntrees, shortest paths, maximal independent sets, and finding subgraphs, showing\nthat many of these problems can be solved in $\\tilde{O}(n/k)$ rounds; this\nshows that one can achieve speedup nearly linear in $k$.\n  To complement our upper bounds, we present lower bounds on the round\ncomplexity that quantify the fundamental limitations of solving graph problems\ndistributively. We first show a lower bound of $\\Omega(n/k)$ rounds for\ncomputing a spanning tree of the input graph. This result implies the same\nbound for other fundamental problems such as computing a minimum spanning tree,\nbreadth-first tree, or shortest paths tree. We also show a $\\tilde\n\\Omega(n/k^2)$ lower bound for connectivity, spanning tree verification and\nother related problems. The latter lower bounds follow from the development and\napplication of novel results in a random-partition variant of the classical\ncommunication complexity model. \n\n"}
{"id": "1311.7178", "contents": "Title: Efficient deterministic approximate counting for low-degree polynomial\n  threshold functions Abstract: We give a deterministic algorithm for approximately counting satisfying\nassignments of a degree-$d$ polynomial threshold function (PTF). Given a\ndegree-$d$ input polynomial $p(x_1,\\dots,x_n)$ over $R^n$ and a parameter\n$\\epsilon> 0$, our algorithm approximates $\\Pr_{x \\sim \\{-1,1\\}^n}[p(x) \\geq\n0]$ to within an additive $\\pm \\epsilon$ in time $O_{d,\\epsilon}(1)\\cdot\n\\mathop{poly}(n^d)$. (Any sort of efficient multiplicative approximation is\nimpossible even for randomized algorithms assuming $NP\\not=RP$.) Note that the\nrunning time of our algorithm (as a function of $n^d$, the number of\ncoefficients of a degree-$d$ PTF) is a \\emph{fixed} polynomial. The fastest\nprevious algorithm for this problem (due to Kane), based on constructions of\nunconditional pseudorandom generators for degree-$d$ PTFs, runs in time\n$n^{O_{d,c}(1) \\cdot \\epsilon^{-c}}$ for all $c > 0$.\n  The key novel contributions of this work are: A new multivariate central\nlimit theorem, proved using tools from Malliavin calculus and Stein's Method.\nThis new CLT shows that any collection of Gaussian polynomials with small\neigenvalues must have a joint distribution which is very close to a\nmultidimensional Gaussian distribution. A new decomposition of low-degree\nmultilinear polynomials over Gaussian inputs. Roughly speaking we show that (up\nto some small error) any such polynomial can be decomposed into a bounded\nnumber of multilinear polynomials all of which have extremely small\neigenvalues. We use these new ingredients to give a deterministic algorithm for\na Gaussian-space version of the approximate counting problem, and then employ\nstandard techniques for working with low-degree PTFs (invariance principles and\nregularity lemmas) to reduce the original approximate counting problem over the\nBoolean hypercube to the Gaussian version. \n\n"}
{"id": "1311.7631", "contents": "Title: Absorption Time of the Moran Process Abstract: The Moran process models the spread of mutations in populations on graphs. We\ninvestigate the absorption time of the process, which is the time taken for a\nmutation introduced at a randomly chosen vertex to either spread to the whole\npopulation, or to become extinct. It is known that the expected absorption time\nfor an advantageous mutation is O(n^4) on an n-vertex undirected graph, which\nallows the behaviour of the process on undirected graphs to be analysed using\nthe Markov chain Monte Carlo method. We show that this does not extend to\ndirected graphs by exhibiting an infinite family of directed graphs for which\nthe expected absorption time is exponential in the number of vertices. However,\nfor regular directed graphs, we show that the expected absorption time is\nOmega(n log n) and O(n^2). We exhibit families of graphs matching these bounds\nand give improved bounds for other families of graphs, based on isoperimetric\nnumber. Our results are obtained via stochastic dominations which we\ndemonstrate by establishing a coupling in a related continuous-time model. The\ncoupling also implies several natural domination results regarding the fixation\nprobability of the original (discrete-time) process, resolving a conjecture of\nShakarian, Roos and Johnson. \n\n"}
{"id": "1312.0132", "contents": "Title: Critical Graphs in Index Coding Abstract: In this paper we define critical graphs as minimal graphs that support a\ngiven set of rates for the index coding problem, and study them for both the\none-shot and asymptotic setups. For the case of equal rates, we find the\ncritical graph with minimum number of edges for both one-shot and asymptotic\ncases. For the general case of possibly distinct rates, we show that for\none-shot and asymptotic linear index coding, as well as asymptotic non-linear\nindex coding, each critical graph is a union of disjoint strongly connected\nsubgraphs (USCS). On the other hand, we identify a non-USCS critical graph for\na one-shot non-linear index coding problem. Next, we identify a few graph\nstructures that are critical. We also generalize some of our results to the\ngroupcast problem. In addition, we show that the capacity region of the index\ncoding is additive for union of disjoint graphs. \n\n"}
{"id": "1312.1277", "contents": "Title: Bandits and Experts in Metric Spaces Abstract: In a multi-armed bandit problem, an online algorithm chooses from a set of\nstrategies in a sequence of trials so as to maximize the total payoff of the\nchosen strategies. While the performance of bandit algorithms with a small\nfinite strategy set is quite well understood, bandit problems with large\nstrategy sets are still a topic of very active investigation, motivated by\npractical applications such as online auctions and web advertisement. The goal\nof such research is to identify broad and natural classes of strategy sets and\npayoff functions which enable the design of efficient solutions.\n  In this work we study a very general setting for the multi-armed bandit\nproblem in which the strategies form a metric space, and the payoff function\nsatisfies a Lipschitz condition with respect to the metric. We refer to this\nproblem as the \"Lipschitz MAB problem\". We present a solution for the\nmulti-armed bandit problem in this setting. That is, for every metric space we\ndefine an isometry invariant which bounds from below the performance of\nLipschitz MAB algorithms for this metric space, and we present an algorithm\nwhich comes arbitrarily close to meeting this bound. Furthermore, our technique\ngives even better results for benign payoff functions. We also address the\nfull-feedback (\"best expert\") version of the problem, where after every round\nthe payoffs from all arms are revealed. \n\n"}
{"id": "1312.1986", "contents": "Title: Approximating the Stationary Probability of a Single State in a Markov\n  chain Abstract: In this paper, we present a novel iterative Monte Carlo method for\napproximating the stationary probability of a single state of a positive\nrecurrent Markov chain. We utilize the characterization that the stationary\nprobability of a state $i$ is inversely proportional to the expected return\ntime of a random walk beginning at $i$. Our method obtains an\n$\\epsilon$-multiplicative close estimate with probability greater than $1 -\n\\alpha$ using at most $\\tilde{O}\\left(t_{\\text{mix}} \\ln(1/\\alpha) / \\pi_i\n\\epsilon^2 \\right)$ simulated random walk steps on the Markov chain across all\niterations, where $t_{\\text{mix}}$ is the standard mixing time and $\\pi_i$ is\nthe stationary probability. In addition, the estimate at each iteration is\nguaranteed to be an upper bound with high probability, and is decreasing in\nexpectation with the iteration count, allowing us to monitor the progress of\nthe algorithm and design effective termination criteria. We propose a\ntermination criteria which guarantees a $\\epsilon (1 + 4 \\ln(2)\nt_{\\text{mix}})$ multiplicative error performance for states with stationary\nprobability larger than $\\Delta$, while providing an additive error for states\nwith stationary probability less than $\\Delta \\in (0,1)$. The algorithm along\nwith this termination criteria uses at most\n$\\tilde{O}\\left(\\frac{\\ln(1/\\alpha)}{\\epsilon^2}\n\\min\\left(\\frac{t_{\\text{mix}}}{\\pi_i}, \\frac{1}{\\epsilon\n\\Delta}\\right)\\right)$ simulated random walk steps, which is bounded by a\nconstant with respect to the Markov Chain. We provide a tight analysis of our\nalgorithm based on a locally weighted variant of the mixing time. Our results\nnaturally extend for countably infinite state space Markov chains via Lyapunov\nfunction analysis. \n\n"}
{"id": "1312.2141", "contents": "Title: Dynamic Complexity of Planar 3-connected Graph Isomorphism Abstract: Dynamic Complexity (as introduced by Patnaik and Immerman) tries to express\nhow hard it is to update the solution to a problem when the input is changed\nslightly. It considers the changes required to some stored data structure\n(possibly a massive database) as small quantities of data (or a tuple) are\ninserted or deleted from the database (or a structure over some vocabulary).\nThe main difference from previous notions of dynamic complexity is that instead\nof treating the update quantitatively by finding the the time/space trade-offs,\nit tries to consider the update qualitatively, by finding the complexity class\nin which the update can be expressed (or made). In this setting, DynFO, or\nDynamic First-Order, is one of the smallest and the most natural complexity\nclass (since SQL queries can be expressed in First-Order Logic), and contains\nthose problems whose solutions (or the stored data structure from which the\nsolution can be found) can be updated in First-Order Logic when the data\nstructure undergoes small changes.\n  Etessami considered the problem of isomorphism in the dynamic setting, and\nshowed that Tree Isomorphism can be decided in DynFO. In this work, we show\nthat isomorphism of Planar 3-connected graphs can be decided in DynFO+ (which\nis DynFO with some polynomial precomputation). We maintain a canonical\ndescription of 3-connected Planar graphs by maintaining a database which is\naccessed and modified by First-Order queries when edges are added to or deleted\nfrom the graph. We specifically exploit the ideas of Breadth-First Search and\nCanonical Breadth-First Search to prove the results. We also introduce a novel\nmethod for canonizing a 3-connected planar graph in First-Order Logic from\nCanonical Breadth-First Search Trees. \n\n"}
{"id": "1312.2226", "contents": "Title: On two Algorithmic Problems about Synchronizing Automata Abstract: Under the assumption $\\mathcal{P} \\neq \\mathcal{NP}$, we prove that two\nnatural problems from the theory of synchronizing automata cannot be solved in\npolynomial time. The first problem is to decide whether a given reachable\npartial automaton is synchronizing. The second one is, given an $n$-state\nbinary complete synchronizing automaton, to compute its reset threshold within\nperformance ratio less than $d \\ln{(n)}$ for a specific constant $d>0$. \n\n"}
{"id": "1312.2496", "contents": "Title: On the hardness of classically simulating the one clean qubit model Abstract: Deterministic quantum computation with one quantum bit (DQC1) is a model of\nquantum computing where the input restricted to containing a single qubit in a\npure state and with all other qubits in a completely-mixed state, with only a\nsingle qubit measurement at the end of the computation [E. Knill and R.\nLaflamme, Phys. Rev. Lett. {\\bf81}, 5672 (1998)]. While it is known that DQC1\ncan efficiently solve several problems for which no known classical efficient\nalgorithms exist, the question of whether DQC1 is really more powerful than\nclassical computation remains open. In this paper, we introduce a slightly\nmodified version of DQC1, which we call DQC1$_k$, where $k$ output qubits are\nmeasured, and show that DQC1$_k$ cannot be classically efficiently simulated\nfor any $k\\geq3$ unless the polynomial hierarchy collapses at the third level. \n\n"}
{"id": "1312.2915", "contents": "Title: Hardness of Finding Independent Sets in 2-Colorable Hypergraphs and of\n  Satisfiable CSPs Abstract: This work revisits the PCP Verifiers used in the works of Hastad [Has01],\nGuruswami et al.[GHS02], Holmerin[Hol02] and Guruswami[Gur00] for satisfiable\nMax-E3-SAT and Max-Ek-Set-Splitting, and independent set in 2-colorable\n4-uniform hypergraphs. We provide simpler and more efficient PCP Verifiers to\nprove the following improved hardness results: Assuming that NP\\not\\subseteq\nDTIME(N^{O(loglog N)}),\n  There is no polynomial time algorithm that, given an n-vertex 2-colorable\n4-uniform hypergraph, finds an independent set of n/(log n)^c vertices, for\nsome constant c > 0.\n  There is no polynomial time algorithm that satisfies 7/8 + 1/(log n)^c\nfraction of the clauses of a satisfiable Max-E3-SAT instance of size n, for\nsome constant c > 0.\n  For any fixed k >= 4, there is no polynomial time algorithm that finds a\npartition splitting (1 - 2^{-k+1}) + 1/(log n)^c fraction of the k-sets of a\nsatisfiable Max-Ek-Set-Splitting instance of size n, for some constant c > 0.\n  Our hardness factor for independent set in 2-colorable 4-uniform hypergraphs\nis an exponential improvement over the previous results of Guruswami et\nal.[GHS02] and Holmerin[Hol02]. Similarly, our inapproximability of (log\nn)^{-c} beyond the random assignment threshold for Max-E3-SAT and\nMax-Ek-Set-Splitting is an exponential improvement over the previous bounds\nproved in [Has01], [Hol02] and [Gur00]. The PCP Verifiers used in our results\navoid the use of a variable bias parameter used in previous works, which leads\nto the improved hardness thresholds in addition to simplifying the analysis\nsubstantially. Apart from standard techniques from Fourier Analysis, for the\nfirst mentioned result we use a mixing estimate of Markov Chains based on\nuniform reverse hypercontractivity over general product spaces from the work of\nMossel et al.[MOS13]. \n\n"}
{"id": "1312.4182", "contents": "Title: Adaptive Protocols for Interactive Communication Abstract: How much adversarial noise can protocols for interactive communication\ntolerate? This question was examined by Braverman and Rao (IEEE Trans. Inf.\nTheory, 2014) for the case of \"robust\" protocols, where each party sends\nmessages only in fixed and predetermined rounds. We consider a new class of\nnon-robust protocols for Interactive Communication, which we call adaptive\nprotocols. Such protocols adapt structurally to the noise induced by the\nchannel in the sense that both the order of speaking, and the length of the\nprotocol may vary depending on observed noise.\n  We define models that capture adaptive protocols and study upper and lower\nbounds on the permissible noise rate in these models. When the length of the\nprotocol may adaptively change according to the noise, we demonstrate a\nprotocol that tolerates noise rates up to $1/3$. When the order of speaking may\nadaptively change as well, we demonstrate a protocol that tolerates noise rates\nup to $2/3$. Hence, adaptivity circumvents an impossibility result of $1/4$ on\nthe fraction of tolerable noise (Braverman and Rao, 2014). \n\n"}
{"id": "1312.4628", "contents": "Title: Counting Triangulations and other Crossing-free Structures via Onion\n  Layers Abstract: Let $P$ be a set of $n$ points in the plane. A crossing-free structure on $P$\nis a plane graph with vertex set $P$. Examples of crossing-free structures\ninclude triangulations of $P$, spanning cycles of $P$, also known as\npolygonalizations of $P$, among others. In this paper we develop a general\ntechnique for computing the number of crossing-free structures of an input set\n$P$. We apply the technique to obtain algorithms for computing the number of\ntriangulations, matchings, and spanning cycles of $P$. The running time of our\nalgorithms is upper bounded by $n^{O(k)}$, where $k$ is the number of onion\nlayers of $P$. In particular, for $k = O(1)$ our algorithms run in polynomial\ntime. In addition, we show that our algorithm for counting triangulations is\nnever slower than $O^{*}(3.1414^{n})$, even when $k = \\Theta(n)$. Given that\nthere are several well-studied configurations of points with at least\n$\\Omega(3.464^{n})$ triangulations, and some even with $\\Omega(8^{n})$\ntriangulations, our algorithm asymptotically outperforms any enumeration\nalgorithm for such instances. In fact, it is widely believed that any set of\n$n$ points must have at least $\\Omega(3.464^{n})$ triangulations. If this is\ntrue, then our algorithm is strictly sub-linear in the number of triangulations\ncounted. We also show that our techniques are general enough to solve the\n\"Restricted-Triangulation-Counting-Problem\", which we prove to be $W[2]$-hard\nin the parameter $k$. This implies a \"no free lunch\" result: In order to be\nfixed-parameter tractable, our general algorithm must rely on additional\nproperties that are specific to the considered class of structures. \n\n"}
{"id": "1312.4673", "contents": "Title: Generalized Quantum Arthur-Merlin Games Abstract: This paper investigates the role of interaction and coins in public-coin\nquantum interactive proof systems (also called quantum Arthur-Merlin games).\nWhile prior works focused on classical public coins even in the quantum\nsetting, the present work introduces a generalized version of quantum\nArthur-Merlin games where the public coins can be quantum as well: the verifier\ncan send not only random bits, but also halves of EPR pairs. First, it is\nproved that the class of two-turn quantum Arthur-Merlin games with quantum\npublic coins, denoted qq-QAM in this paper, does not change by adding a\nconstant number of turns of classical interactions prior to the communications\nof the qq-QAM proof systems. This can be viewed as a quantum analogue of the\ncelebrated collapse theorem for AM due to Babai. To prove this collapse\ntheorem, this paper provides a natural complete problem for qq-QAM: deciding\nwhether the output of a given quantum circuit is close to a totally mixed\nstate. This complete problem is on the very line of the previous studies\ninvestigating the hardness of checking the properties related to quantum\ncircuits, and is of independent interest. It is further proved that the class\nqq-QAM_1 of two-turn quantum-public-coin quantum Arthur-Merlin proof systems\nwith perfect completeness gives new bounds for standard well-studied classes of\ntwo-turn interactive proof systems. Finally, the collapse theorem above is\nextended to comprehensively classify the role of interaction and public coins\nin quantum Arthur-Merlin games: it is proved that, for any constant m>1, the\nclass of problems having an m-turn quantum Arthur-Merlin proof system is either\nequal to PSPACE or equal to the class of problems having a two-turn quantum\nArthur-Merlin game of a specific type, which provides a complete set of quantum\nanalogues of Babai's collapse theorem. \n\n"}
{"id": "1312.5520", "contents": "Title: Bar 1-Visibility Graphs and their relation to other Nearly Planar Graphs Abstract: A graph is called a strong (resp. weak) bar 1-visibility graph if its\nvertices can be represented as horizontal segments (bars) in the plane so that\nits edges are all (resp. a subset of) the pairs of vertices whose bars have a\n$\\epsilon$-thick vertical line connecting them that intersects at most one\nother bar.\n  We explore the relation among weak (resp. strong) bar 1-visibility graphs and\nother nearly planar graph classes. In particular, we study their relation to\n1-planar graphs, which have a drawing with at most one crossing per edge;\nquasi-planar graphs, which have a drawing with no three mutually crossing\nedges; the squares of planar 1-flow networks, which are upward digraphs with\nin- or out-degree at most one. Our main results are that 1-planar graphs and\nthe (undirected) squares of planar 1-flow networks are weak bar 1-visibility\ngraphs and that these are quasi-planar graphs. \n\n"}
{"id": "1312.5686", "contents": "Title: Complexity Hierarchies Beyond Elementary Abstract: We introduce a hierarchy of fast-growing complexity classes and show its\nsuitability for completeness statements of many non elementary problems. This\nhierarchy allows the classification of many decision problems with a\nnon-elementary complexity, which occur naturally in logic, combinatorics,\nformal languages, verification, etc., with complexities ranging from simple\ntowers of exponentials to Ackermannian and beyond. \n\n"}
{"id": "1312.5972", "contents": "Title: A Comprehensive Analysis of Polyhedral Lift-and-Project Methods Abstract: We consider lift-and-project methods for combinatorial optimization problems\nand focus mostly on those lift-and-project methods which generate polyhedral\nrelaxations of the convex hull of integer solutions. We introduce many new\nvariants of Sherali--Adams and Bienstock--Zuckerberg operators. These new\noperators fill the spectrum of polyhedral lift-and-project operators in a way\nwhich makes all of them more transparent, easier to relate to each other, and\neasier to analyze. We provide new techniques to analyze the worst-case\nperformances as well as relative strengths of these operators in a unified way.\nIn particular, using the new techniques and a result of Mathieu and Sinclair\nfrom 2009, we prove that the polyhedral Bienstock--Zuckerberg operator requires\nat least $\\sqrt{2n}- \\frac{3}{2}$ iterations to compute the matching polytope\nof the $(2n+1)$-clique. We further prove that the operator requires\napproximately $\\frac{n}{2}$ iterations to reach the stable set polytope of the\n$n$-clique, if we start with the fractional stable set polytope. Lastly, we\nshow that some of the worst-case instances for the positive semidefinite\nLov\\'asz--Schrijver lift-and-project operator are also bad instances for the\nstrongest variants of the Sherali--Adams operator with positive semidefinite\nstrengthenings, and discuss some consequences for integrality gaps of convex\nrelaxations. \n\n"}
{"id": "1312.7468", "contents": "Title: Tree-width and Logspace: Determinants and Counting Euler Tours Abstract: Motivated by the recent result of [EJT10] showing that MSO properties are\nLogspace computable on graphs of bounded tree-width, we consider the complexity\nof computing the determinant of the adjacency matrix of a bounded tree-width\ngraph and prove that it is L-complete. It is important to notice that the\ndeterminant is neither an MSO-property nor counts the number of solutions of an\nMSO-predicate. We extend this technique to count the number of spanning\narborescences and directed Euler tours in bounded tree-width digraphs, and\nfurther to counting the number of spanning trees and the number of Euler tours\nin undirected graphs, all in L. Notice that undirected Euler tours are not\nknown to be MSO-expressible and the corresponding counting problem is in fact\n#P-hard for general graphs. Counting undirected Euler tours in bounded\ntree-width graphs was not known to be polynomial time computable till very\nrecently Chebolu et al [CCM13] gave a polynomial time algorithm for this\nproblem (concurrently and independently of this work). Finally, we also show\nsome linear algebraic extensions of the determinant algorithm to show how to\ncompute the charcteristic polynomial and trace of the powers of a bounded\ntree-width graph in L. \n\n"}
{"id": "1401.0579", "contents": "Title: More Algorithms for Provable Dictionary Learning Abstract: In dictionary learning, also known as sparse coding, the algorithm is given\nsamples of the form $y = Ax$ where $x\\in \\mathbb{R}^m$ is an unknown random\nsparse vector and $A$ is an unknown dictionary matrix in $\\mathbb{R}^{n\\times\nm}$ (usually $m > n$, which is the overcomplete case). The goal is to learn $A$\nand $x$. This problem has been studied in neuroscience, machine learning,\nvisions, and image processing. In practice it is solved by heuristic algorithms\nand provable algorithms seemed hard to find. Recently, provable algorithms were\nfound that work if the unknown feature vector $x$ is $\\sqrt{n}$-sparse or even\nsparser. Spielman et al. \\cite{DBLP:journals/jmlr/SpielmanWW12} did this for\ndictionaries where $m=n$; Arora et al. \\cite{AGM} gave an algorithm for\novercomplete ($m >n$) and incoherent matrices $A$; and Agarwal et al.\n\\cite{DBLP:journals/corr/AgarwalAN13} handled a similar case but with weaker\nguarantees.\n  This raised the problem of designing provable algorithms that allow sparsity\n$\\gg \\sqrt{n}$ in the hidden vector $x$. The current paper designs algorithms\nthat allow sparsity up to $n/poly(\\log n)$. It works for a class of matrices\nwhere features are individually recoverable, a new notion identified in this\npaper that may motivate further work.\n  The algorithm runs in quasipolynomial time because they use limited\nenumeration. \n\n"}
{"id": "1401.1434", "contents": "Title: Computational Aspects of the Hausdorff Distance in Unbounded Dimension Abstract: We study the computational complexity of determining the Hausdorff distance\nof two polytopes given in halfspace- or vertex-presentation in arbitrary\ndimension. Subsequently, a matching problem is investigated where a convex body\nis allowed to be homothetically transformed in order to minimize its Hausdorff\ndistance to another one. For this problem, we characterize optimal solutions,\ndeduce a Helly-type theorem and give polynomial time (approximation) algorithms\nfor polytopes. \n\n"}
{"id": "1401.3807", "contents": "Title: On the Existence of MDS Codes Over Small Fields With Constrained\n  Generator Matrices Abstract: We study the existence over small fields of Maximum Distance Separable (MDS)\ncodes with generator matrices having specified supports (i.e. having specified\nlocations of zero entries). This problem unifies and simplifies the problems\nposed in recent works of Yan and Sprintson (NetCod'13) on weakly secure\ncooperative data exchange, of Halbawi et al. (arxiv'13) on distributed\nReed-Solomon codes for simple multiple access networks, and of Dau et al.\n(ISIT'13) on MDS codes with balanced and sparse generator matrices. We\nconjecture that there exist such $[n,k]_q$ MDS codes as long as $q \\geq n + k -\n1$, if the specified supports of the generator matrices satisfy the so-called\nMDS condition, which can be verified in polynomial time. We propose a\ncombinatorial approach to tackle the conjecture, and prove that the conjecture\nholds for a special case when the sets of zero coordinates of rows of the\ngenerator matrix share with each other (pairwise) at most one common element.\nBased on our numerical result, the conjecture is also verified for all $k \\leq\n7$. Our approach is based on a novel generalization of the well-known Hall's\nmarriage theorem, which allows (overlapping) multiple representatives instead\nof a single representative for each subset. \n\n"}
{"id": "1401.4720", "contents": "Title: Computing low-degree factors of lacunary polynomials: a Newton-Puiseux\n  approach Abstract: We present a new algorithm for the computation of the irreducible factors of\ndegree at most $d$, with multiplicity, of multivariate lacunary polynomials\nover fields of characteristic zero. The algorithm reduces this computation to\nthe computation of irreducible factors of degree at most $d$ of univariate\nlacunary polynomials and to the factorization of low-degree multivariate\npolynomials. The reduction runs in time polynomial in the size of the input\npolynomial and in $d$. As a result, we obtain a new polynomial-time algorithm\nfor the computation of low-degree factors, with multiplicity, of multivariate\nlacunary polynomials over number fields, but our method also gives partial\nresults for other fields, such as the fields of $p$-adic numbers or for\nabsolute or approximate factorization for instance.\n  The core of our reduction uses the Newton polygon of the input polynomial,\nand its validity is based on the Newton-Puiseux expansion of roots of bivariate\npolynomials. In particular, we bound the valuation of $f(X,\\phi)$ where $f$ is\na lacunary polynomial and $\\phi$ a Puiseux series whose vanishing polynomial\nhas low degree. \n\n"}
{"id": "1401.4734", "contents": "Title: Optimal Fractional Repetition Codes based on Graphs and Designs Abstract: Fractional repetition (FR) codes is a family of codes for distributed storage\nsystems that allow for uncoded exact repairs having the minimum repair\nbandwidth. However, in contrast to minimum bandwidth regenerating (MBR) codes,\nwhere a random set of a certain size of available nodes is used for a node\nrepair, the repairs with FR codes are table based. This usually allows to store\nmore data compared to MBR codes. In this work, we consider bounds on the\nfractional repetition capacity, which is the maximum amount of data that can be\nstored using an FR code. Optimal FR codes which attain these bounds are\npresented. The constructions of these FR codes are based on combinatorial\ndesigns and on families of regular and biregular graphs. These constructions of\nFR codes for given parameters raise some interesting questions in graph theory.\nThese questions and some of their solutions are discussed in this paper. In\naddition, based on a connection between FR codes and batch codes, we propose a\nnew family of codes for DSS, namely fractional repetition batch codes, which\nhave the properties of batch codes and FR codes simultaneously. These are the\nfirst codes for DSS which allow for uncoded efficient exact repairs and load\nbalancing which can be performed by several users in parallel. Other concepts\nrelated to FR codes are also discussed. \n\n"}
{"id": "1401.6848", "contents": "Title: AM with Multiple Merlins Abstract: We introduce and study a new model of interactive proofs: AM(k), or\nArthur-Merlin with k non-communicating Merlins. Unlike with the better-known\nMIP, here the assumption is that each Merlin receives an independent random\nchallenge from Arthur. One motivation for this model (which we explore in\ndetail) comes from the close analogies between it and the quantum complexity\nclass QMA(k), but the AM(k) model is also natural in its own right.\n  We illustrate the power of multiple Merlins by giving an AM(2) protocol for\n3SAT, in which the Merlins' challenges and responses consist of only\nn^{1/2+o(1)} bits each. Our protocol has the consequence that, assuming the\nExponential Time Hypothesis (ETH), any algorithm for approximating a dense CSP\nwith a polynomial-size alphabet must take n^{(log n)^{1-o(1)}} time. Algorithms\nnearly matching this lower bound are known, but their running times had never\nbeen previously explained. Brandao and Harrow have also recently used our 3SAT\nprotocol to show quasipolynomial hardness for approximating the values of\ncertain entangled games.\n  In the other direction, we give a simple quasipolynomial-time approximation\nalgorithm for free games, and use it to prove that, assuming the ETH, our 3SAT\nprotocol is essentially optimal. More generally, we show that multiple Merlins\nnever provide more than a polynomial advantage over one: that is, AM(k)=AM for\nall k=poly(n). The key to this result is a subsampling theorem for free games,\nwhich follows from powerful results by Alon et al. and Barak et al. on\nsubsampling dense CSPs, and which says that the value of any free game can be\nclosely approximated by the value of a logarithmic-sized random subgame. \n\n"}
{"id": "1402.0052", "contents": "Title: Performance of the Survey Propagation-guided decimation algorithm for\n  the random NAE-K-SAT problem Abstract: We show that the Survey Propagation-guided decimation algorithm fails to find\nsatisfying assignments on random instances of the \"Not-All-Equal-$K$-SAT\"\nproblem if the number of message passing iterations is bounded by a constant\nindependent of the size of the instance and the clause-to-variable ratio is\nabove $(1+o_K(1)){2^{K-1}\\over K}\\log^2 K$ for sufficiently large $K$. Our\nanalysis in fact applies to a broad class of algorithms described as\n\"sequential local algorithms\". Such algorithms iteratively set variables based\non some local information and then recurse on the reduced instance. Survey\nPropagation-guided as well as Belief Propagation-guided decimation algorithms -\ntwo widely studied message passing based algorithms, fall under this category\nof algorithms provided the number of message passing iterations is bounded by a\nconstant. Another well-known algorithm falling into this category is the Unit\nClause algorithm. Our work constitutes the first rigorous analysis of the\nperformance of the SP-guided decimation algorithm.\n  The approach underlying our paper is based on an intricate geometry of the\nsolution space of random NAE-$K$-SAT problem. We show that above the\n$(1+o_K(1)){2^{K-1}\\over K}\\log^2 K$ threshold, the overlap structure of\n$m$-tuples of satisfying assignments exhibit a certain clustering behavior\nexpressed in the form of constraints on distances between the $m$ assignments,\nfor appropriately chosen $m$. We further show that if a sequential local\nalgorithm succeeds in finding a satisfying assignment with probability bounded\naway from zero, then one can construct an $m$-tuple of solutions violating\nthese constraints, thus leading to a contradiction. Along with (citation), this\nresult is the first work which directly links the clustering property of random\nconstraint satisfaction problems to the computational hardness of finding\nsatisfying assignments. \n\n"}
{"id": "1402.3444", "contents": "Title: Subgraph Enumeration in Massive Graphs Abstract: We consider the problem of enumerating all instances of a given pattern graph\nin a large data graph. Our focus is on determining the input/output (I/O)\ncomplexity of this problem. Let $E$ be the number of edges in the data graph,\n$k=O(1)$ be the number of vertices in the pattern graph, $B$ be the block\nlength, and $M$ be the main memory size. The main results of the paper are two\nalgorithms that enumerate all instances of the pattern graph. The first one is\na deterministic algorithm that exploits a suitable independent set of the\npattern graph of size $1\\leq s \\leq k/2$ and requires\n$O\\left(E^{k-s}/\\left(BM^{k-s-1}\\right)\\right)$ I/Os. The second algorithm is a\nrandomized algorithm that enumerates all instances in\n$O\\left(E^{k/2}/\\left(BM^{k/2-1}\\right)\\right)$ expected I/Os; the same bound\nalso applies with high probability under some assumptions. A lower bound shows\nthat the deterministic algorithm is optimal for some pattern graphs with\n$s=k/2$ (e.g., paths and cycles of even length, meshes of even side), while the\nrandomized algorithm is optimal for a wide class of pattern graphs, called Alon\nclass (e.g., cliques, cycles and every graph with a perfect matching). \n\n"}
{"id": "1402.3449", "contents": "Title: Quantum Pushdown Automata with a Garbage Tape Abstract: Several kinds of quantum pushdown automaton models have been proposed, and\ntheir computational power is investigated intensively. However, for some\nquantum pushdown automaton models, it is not known whether quantum models are\nat least as powerful as classical counterparts or not. This is due to the\nreversibility restriction. In this paper, we introduce a new quantum pushdown\nautomaton model that has a garbage tape. This model can overcome the\nreversibility restriction by exploiting the garbage tape to store popped\nsymbols. We show that the proposed model can simulate any quantum pushdown\nautomaton with a classical stack as well as any probabilistic pushdown\nautomaton. We also show that our model can solve a certain promise problem\nexactly while deterministic pushdown automata cannot. These results imply that\nour model is strictly more powerful than classical counterparts in the setting\nof exact, one-sided error and non-deterministic computation. \n\n"}
{"id": "1402.3973", "contents": "Title: Dimensionality reduction with subgaussian matrices: a unified theory Abstract: We present a theory for Euclidean dimensionality reduction with subgaussian\nmatrices which unifies several restricted isometry property and\nJohnson-Lindenstrauss type results obtained earlier for specific data sets. In\nparticular, we recover and, in several cases, improve results for sets of\nsparse and structured sparse vectors, low-rank matrices and tensors, and smooth\nmanifolds. In addition, we establish a new Johnson-Lindenstrauss embedding for\ndata sets taking the form of an infinite union of subspaces of a Hilbert space. \n\n"}
{"id": "1402.4376", "contents": "Title: On Coloring Resilient Graphs Abstract: We introduce a new notion of resilience for constraint satisfaction problems,\nwith the goal of more precisely determining the boundary between NP-hardness\nand the existence of efficient algorithms for resilient instances. In\nparticular, we study $r$-resiliently $k$-colorable graphs, which are those\n$k$-colorable graphs that remain $k$-colorable even after the addition of any\n$r$ new edges. We prove lower bounds on the NP-hardness of coloring resiliently\ncolorable graphs, and provide an algorithm that colors sufficiently resilient\ngraphs. We also analyze the corresponding notion of resilience for $k$-SAT.\nThis notion of resilience suggests an array of open questions for graph\ncoloring and other combinatorial problems. \n\n"}
{"id": "1402.5857", "contents": "Title: The challenges of unbounded treewidth in parameterised subgraph counting\n  problems Abstract: Parameterised subgraph counting problems are the most thoroughly studied\ntopic in the theory of parameterised counting, and there has been significant\nrecent progress in this area. Many of the existing tractability results for\nparameterised problems which involve finding or counting subgraphs with\nparticular properties rely on bounding the treewidth of these subgraphs in some\nsense; here, we prove a number of hardness results for the situation in which\nthis bounded treewidth condition does not hold, resulting in dichotomies for\nsome special cases of the general subgraph counting problem. The paper also\ngives a thorough survey of known results on this subject and the methods used,\nas well as discussing the relationships both between multicolour and uncoloured\nversions of subgraph counting problems, and between exact counting, approximate\ncounting and the corresponding decision problems. \n\n"}
{"id": "1402.6404", "contents": "Title: On the Algebraic Structure of Linear Trellises Abstract: Trellises are crucial graphical representations of codes. While conventional\ntrellises are well understood, the general theory of (tail-biting) trellises is\nstill under development. Iterative decoding concretely motivates such theory.\nIn this paper we first develop a new algebraic framework for a systematic\nanalysis of linear trellises which enables us to address open foundational\nquestions. In particular, we present a useful and powerful characterization of\nlinear trellis isomorphy. We also obtain a new proof of the Factorization\nTheorem of Koetter/Vardy and point out unnoticed problems for the group case.\n  Next, we apply our work to: describe all the elementary trellis\nfactorizations of linear trellises and consequently to determine all the\nminimal linear trellises for a given code; prove that nonmergeable one-to-one\nlinear trellises are strikingly determined by the edge-label sequences of\ncertain closed paths; prove self-duality theorems for minimal linear trellises;\nanalyze quasi-cyclic linear trellises and consequently extend results on\nreduced linear trellises to nonreduced ones. To achieve this, we also provide\nnew insight into mergeability and path connectivity properties of linear\ntrellises.\n  Our classification results are important for iterative decoding as we show\nthat minimal linear trellises can yield different pseudocodewords even if they\nhave the same graph structure. \n\n"}
{"id": "1402.6485", "contents": "Title: Solving MaxSAT and #SAT on structured CNF formulas Abstract: In this paper we propose a structural parameter of CNF formulas and use it to\nidentify instances of weighted MaxSAT and #SAT that can be solved in polynomial\ntime. Given a CNF formula we say that a set of clauses is precisely satisfiable\nif there is some complete assignment satisfying these clauses only. Let the\nps-value of the formula be the number of precisely satisfiable sets of clauses.\nApplying the notion of branch decompositions to CNF formulas and using ps-value\nas cut function, we define the ps-width of a formula. For a formula given with\na decomposition of polynomial ps-width we show dynamic programming algorithms\nsolving weighted MaxSAT and #SAT in polynomial time. Combining with results of\n'Belmonte and Vatshelle, Graph classes with structured neighborhoods and\nalgorithmic applications, Theor. Comput. Sci. 511: 54-65 (2013)' we get\npolynomial-time algorithms solving weighted MaxSAT and #SAT for some classes of\nstructured CNF formulas. For example, we get $O(m^2(m + n)s)$ algorithms for\nformulas $F$ of $m$ clauses and $n$ variables and size $s$, if $F$ has a linear\nordering of the variables and clauses such that for any variable $x$ occurring\nin clause $C$, if $x$ appears before $C$ then any variable between them also\noccurs in $C$, and if $C$ appears before $x$ then $x$ occurs also in any clause\nbetween them. Note that the class of incidence graphs of such formulas do not\nhave bounded clique-width. \n\n"}
{"id": "1403.0476", "contents": "Title: Algebraic Properties of Valued Constraint Satisfaction Problem Abstract: The paper presents an algebraic framework for optimization problems\nexpressible as Valued Constraint Satisfaction Problems. Our results generalize\nthe algebraic framework for the decision version (CSPs) provided by Bulatov et\nal. [SICOMP 2005]. We introduce the notions of weighted algebras and varieties\nand use the Galois connection due to Cohen et al. [SICOMP 2013] to link VCSP\nlanguages to weighted algebras. We show that the difficulty of VCSP depends\nonly on the weighted variety generated by the associated weighted algebra.\nParalleling the results for CSPs we exhibit a reduction to cores and rigid\ncores which allows us to focus on idempotent weighted varieties. Further, we\npropose an analogue of the Algebraic CSP Dichotomy Conjecture; prove the\nhardness direction and verify that it agrees with known results for VCSPs on\ntwo-element sets [Cohen et al. 2006], finite-valued VCSPs [Thapper and Zivny\n2013] and conservative VCSPs [Kolmogorov and Zivny 2013]. \n\n"}
{"id": "1403.3841", "contents": "Title: Doubles and Negatives are Positive (in Self-Assembly) Abstract: In the abstract Tile Assembly Model (aTAM), the phenomenon of cooperation\noccurs when the attachment of a new tile to a growing assembly requires it to\nbind to more than one tile already in the assembly. Often referred to as\n``temperature-2'' systems, those which employ cooperation are known to be quite\npowerful (i.e. they are computationally universal and can build an enormous\nvariety of shapes and structures). Conversely, aTAM systems which do not\nenforce cooperative behavior, a.k.a. ``temperature-1'' systems, are conjectured\nto be relatively very weak, likely to be unable to perform complex computations\nor algorithmically direct the process of self-assembly. Nonetheless, a variety\nof models based on slight modifications to the aTAM have been developed in\nwhich temperature-1 systems are in fact capable of Turing universal computation\nthrough a restricted notion of cooperation. Despite that power, though, several\nof those models have previously been proven to be unable to perform or simulate\nthe stronger form of cooperation exhibited by temperature-2 aTAM systems.\n  In this paper, we first prove that another model in which temperature-1\nsystems are computationally universal, namely the restricted glue TAM (rgTAM)\nin which tiles are allowed to have edges which exhibit repulsive forces, is\nalso unable to simulate the strongly cooperative behavior of the temperature-2\naTAM. We then show that by combining the properties of two such models, the\nDupled Tile Assembly Model (DTAM) and the rgTAM into the DrgTAM, we derive a\nmodel which is actually more powerful at temperature-1 than the aTAM at\ntemperature-2. Specifically, the DrgTAM, at temperature-1, can simulate any\naTAM system of any temperature, and it also contains systems which cannot be\nsimulated by any system in the aTAM. \n\n"}
{"id": "1403.3881", "contents": "Title: Complexity of Equilibrium in Diffusion Games on Social Networks Abstract: In this paper, we consider the competitive diffusion game, and study the\nexistence of its pure-strategy Nash equilibrium when defined over general\nundirected networks. We first determine the set of pure-strategy Nash\nequilibria for two special but well-known classes of networks, namely the\nlattice and the hypercube. Characterizing the utility of the players in terms\nof graphical distances of their initial seed placements to other nodes in the\nnetwork, we show that in general networks the decision process on the existence\nof pure-strategy Nash equilibrium is an NP-hard problem. Following this, we\nprovide some necessary conditions for a given profile to be a Nash equilibrium.\nFurthermore, we study players' utilities in the competitive diffusion game over\nErdos-Renyi random graphs and show that as the size of the network grows, the\nutilities of the players are highly concentrated around their expectation, and\nare bounded below by some threshold based on the parameters of the network.\nFinally, we obtain a lower bound for the maximum social welfare of the game\nwith two players, and study sub-modularity of the players' utilities. \n\n"}
{"id": "1403.4153", "contents": "Title: A family of polycyclic groups over which the uniform conjugacy problem\n  is NP-complete Abstract: In this paper we study the conjugacy problem in polycyclic groups. Our main\nresult is that we construct polycyclic groups $G_n$ whose conjugacy problem is\nat least as hard as the subset sum problem with $n$ indeterminates. As such,\nthe conjugacy problem over the groups $G_n$ is NP-complete where the parameters\nof the problem are taken in terms of $n$ and the length of the elements given\non input. \n\n"}
{"id": "1403.6758", "contents": "Title: Facility Location in Evolving Metrics Abstract: Understanding the dynamics of evolving social or infrastructure networks is a\nchallenge in applied areas such as epidemiology, viral marketing, or urban\nplanning. During the past decade, data has been collected on such networks but\nhas yet to be fully analyzed. We propose to use information on the dynamics of\nthe data to find stable partitions of the network into groups. For that\npurpose, we introduce a time-dependent, dynamic version of the facility\nlocation problem, that includes a switching cost when a client's assignment\nchanges from one facility to another. This might provide a better\nrepresentation of an evolving network, emphasizing the abrupt change of\nrelationships between subjects rather than the continuous evolution of the\nunderlying network. We show that in realistic examples this model yields indeed\nbetter fitting solutions than optimizing every snapshot independently. We\npresent an $O(\\log nT)$-approximation algorithm and a matching hardness result,\nwhere $n$ is the number of clients and $T$ the number of time steps. We also\ngive an other algorithms with approximation ratio $O(\\log nT)$ for the variant\nwhere one pays at each time step (leasing) for each open facility. \n\n"}
{"id": "1404.0337", "contents": "Title: The Complexity of Bounded Length Graph Recoloring Abstract: We study the following question: Given are two $k$-colorings $\\alpha$ and\n$\\beta$ of a graph $G$ on $n$ vertices, and integer $\\ell$. The question is\nwhether $\\alpha$ can be modified into $\\beta$, by recoloring vertices one at a\ntime, while maintaining a $k$-coloring throughout, and using at most $\\ell$\nsuch recoloring steps. This problem is weakly PSPACE-hard for every constant\n$k\\ge 4$. We show that it is also strongly NP-hard for every constant $k\\ge 4$.\nOn the positive side, we give an $O(f(k,\\ell) n^{O(1)})$ algorithm for the\nproblem, for some computable function $f$. Hence the problem is fixed-parameter\ntractable when parameterized by $k+\\ell$. Finally, we show that the problem is\nW[1]-hard (but in XP) when parameterized only by $\\ell$. \n\n"}
{"id": "1404.0967", "contents": "Title: Binary pattern tile set synthesis is NP-hard Abstract: In the field of algorithmic self-assembly, a long-standing unproven\nconjecture has been that of the NP-hardness of binary pattern tile set\nsynthesis (2-PATS). The $k$-PATS problem is that of designing a tile assembly\nsystem with the smallest number of tile types which will self-assemble an input\npattern of $k$ colors. Of both theoretical and practical significance, $k$-PATS\nhas been studied in a series of papers which have shown $k$-PATS to be NP-hard\nfor $k = 60$, $k = 29$, and then $k = 11$. In this paper, we close the\nfundamental conjecture that 2-PATS is NP-hard, concluding this line of study.\n  While most of our proof relies on standard mathematical proof techniques, one\ncrucial lemma makes use of a computer-assisted proof, which is a relatively\nnovel but increasingly utilized paradigm for deriving proofs for complex\nmathematical problems. This tool is especially powerful for attacking\ncombinatorial problems, as exemplified by the proof of the four color theorem\nby Appel and Haken (simplified later by Robertson, Sanders, Seymour, and\nThomas) or the recent important advance on the Erd\\H{o}s discrepancy problem by\nKonev and Lisitsa using computer programs. We utilize a massively parallel\nalgorithm and thus turn an otherwise intractable portion of our proof into a\nprogram which requires approximately a year of computation time, bringing the\nuse of computer-assisted proofs to a new scale. We fully detail the algorithm\nemployed by our code, and make the code freely available online. \n\n"}
{"id": "1404.2827", "contents": "Title: A Simple Algorithm for Hamiltonicity Abstract: We develop a new algebraic technique that solves the following problem: Given\na black box that contains an arithmetic circuit $f$ over a field of\ncharacteristic $2$ of degree~$d$. Decide whether $f$, expressed as an\nequivalent multivariate polynomial, contains a multilinear monomial of degree\n$d$.\n  This problem was solved by Williams \\cite{W} and Bj\\\"orklund et. al.\n\\cite{BHKK} for a white box (the circuit is given as an input) that contains\narithmetic circuit. We show a simple black box algorithm that solves the\nproblem with the same time complexity.\n  This gives a simple randomized algorithm for the simple $k$-path problem for\ndirected graphs of the same time complexity\\footnote{$O^*(f(k))$ is\n$O(poly(n)\\cdot f(k))$} $O^*(2^k)$ as in \\cite{W} and with reusing the same\nideas from \\cite{BHKK} with the above gives another algorithm (probably not\nsimpler) for undirected graphs of the same time complexity $O^*(1.657^k)$ as in\n\\cite{B10,BHKK}. \n\n"}
{"id": "1404.3577", "contents": "Title: Cache-Oblivious VAT-Algorithms Abstract: The VAT-model (virtual address translation model) extends the EM-model\n(external memory model) and takes the cost of address translation in virtual\nmemories into account. In this model, the cost of a single memory access may be\nlogarithmic in the largest address used. We show that the VAT-cost of\ncache-oblivious algorithms is only by a constant factor larger than their\nEM-cost; this requires a somewhat more stringent tall cache assumption as for\nthe EM-model. \n\n"}
{"id": "1404.3801", "contents": "Title: Shortest reconfiguration paths in the solution space of Boolean formulas Abstract: Given a Boolean formula and a satisfying assignment, a flip is an operation\nthat changes the value of a variable in the assignment so that the resulting\nassignment remains satisfying. We study the problem of computing the shortest\nsequence of flips (if one exists) that transforms a given satisfying assignment\n$s$ to another satisfying assignment $t$ of a Boolean formula. Earlier work\ncharacterized the complexity of finding any (not necessarily the shortest)\nsequence of flips from one satisfying assignment to another using Schaefer's\nframework for classification of Boolean formulas. We build on it to provide a\ntrichotomy for the complexity of finding the shortest sequence of flips and\nshow that it is either in P, NP-complete, or PSPACE-complete.\n  Our result adds to the small set of complexity results known for shortest\nreconfiguration sequence problems by providing an example where the shortest\nsequence can be found in polynomial time even though its length is not equal to\nthe symmetric difference of the values of the variables in $s$ and $t$. This is\nin contrast to all reconfiguration problems studied so far, where polynomial\ntime algorithms for computing the shortest path were known only for cases where\nthe path modified the symmetric difference only. \n\n"}
{"id": "1404.4749", "contents": "Title: Decoding binary node labels from censored edge measurements: Phase\n  transition and efficient recovery Abstract: We consider the problem of clustering a graph $G$ into two communities by\nobserving a subset of the vertex correlations. Specifically, we consider the\ninverse problem with observed variables $Y=B_G x \\oplus Z$, where $B_G$ is the\nincidence matrix of a graph $G$, $x$ is the vector of unknown vertex variables\n(with a uniform prior) and $Z$ is a noise vector with Bernoulli$(\\varepsilon)$\ni.i.d. entries. All variables and operations are Boolean. This model is\nmotivated by coding, synchronization, and community detection problems. In\nparticular, it corresponds to a stochastic block model or a correlation\nclustering problem with two communities and censored edges. Without noise,\nexact recovery (up to global flip) of $x$ is possible if and only the graph $G$\nis connected, with a sharp threshold at the edge probability $\\log(n)/n$ for\nErd\\H{o}s-R\\'enyi random graphs. The first goal of this paper is to determine\nhow the edge probability $p$ needs to scale to allow exact recovery in the\npresence of noise. Defining the degree (oversampling) rate of the graph by\n$\\alpha =np/\\log(n)$, it is shown that exact recovery is possible if and only\nif $\\alpha >2/(1-2\\varepsilon)^2+ o(1/(1-2\\varepsilon)^2)$. In other words,\n$2/(1-2\\varepsilon)^2$ is the information theoretic threshold for exact\nrecovery at low-SNR. In addition, an efficient recovery algorithm based on\nsemidefinite programming is proposed and shown to succeed in the threshold\nregime up to twice the optimal rate. For a deterministic graph $G$, defining\nthe degree rate as $\\alpha=d/\\log(n)$, where $d$ is the minimum degree of the\ngraph, it is shown that the proposed method achieves the rate $\\alpha>\n4((1+\\lambda)/(1-\\lambda)^2)/(1-2\\varepsilon)^2+ o(1/(1-2\\varepsilon)^2)$,\nwhere $1-\\lambda$ is the spectral gap of the graph $G$. \n\n"}
{"id": "1405.0527", "contents": "Title: Parallel computation using active self-assembly Abstract: We study the computational complexity of the recently proposed nubot model of\nmolecular-scale self-assembly. The model generalises asynchronous cellular\nautomata to have non-local movement where large assemblies of molecules can be\npushed and pulled around, analogous to millions of molecular motors in animal\nmuscle effecting the rapid movement of macroscale arms and legs. We show that\nthe nubot model is capable of simulating Boolean circuits of polylogarithmic\ndepth and polynomial size, in only polylogarithmic expected time. In\ncomputational complexity terms, we show that any problem from the complexity\nclass NC is solvable in polylogarithmic expected time and polynomial workspace\nusing nubots.\n  Along the way, we give fast parallel nubot algorithms for a number of\nproblems including line growth, sorting, Boolean matrix multiplication and\nspace-bounded Turing machine simulation, all using a constant number of nubot\nstates (monomer types). Circuit depth is a well-studied notion of parallel\ntime, and our result implies that the nubot model is a highly parallel model of\ncomputation in a formal sense. Asynchronous cellular automata are not capable\nof this parallelism, and our result shows that adding a rigid-body movement\nprimitive to such a model, to get the nubot model, drastically increases\nparallel processing abilities. \n\n"}
{"id": "1405.1464", "contents": "Title: Generalized sphere-packing and sphere-covering bounds on the size of\n  codes for combinatorial channels Abstract: Many of the classic problems of coding theory are highly symmetric, which\nmakes it easy to derive sphere-packing upper bounds and sphere-covering lower\nbounds on the size of codes. We discuss the generalizations of sphere-packing\nand sphere-covering bounds to arbitrary error models. These generalizations\nbecome especially important when the sizes of the error spheres are nonuniform.\nThe best possible sphere-packing and sphere-covering bounds are solutions to\nlinear programs. We derive a series of bounds from approximations to packing\nand covering problems and study the relationships and trade-offs between them.\nWe compare sphere-covering lower bounds with other graph theoretic lower bounds\nsuch as Tur\\'{a}n's theorem. We show how to obtain upper bounds by optimizing\nacross a family of channels that admit the same codes. We present a\ngeneralization of the local degree bound of Kulkarni and Kiyavash and use it to\nimprove the best known upper bounds on the sizes of single deletion correcting\ncodes and single grain error correcting codes. \n\n"}
{"id": "1405.2424", "contents": "Title: Identification, location-domination and metric dimension on interval and\n  permutation graphs. II. Algorithms and complexity Abstract: We consider the problems of finding optimal identifying codes, (open)\nlocating-dominating sets and resolving sets (denoted IDENTIFYING CODE, (OPEN)\nLOCATING-DOMINATING SET and METRIC DIMENSION) of an interval or a permutation\ngraph. In these problems, one asks to distinguish all vertices of a graph by a\nsubset of the vertices, using either the neighbourhood within the solution set\nor the distances to the solution vertices. Using a general reduction for this\nclass of problems, we prove that the decision problems associated to these four\nnotions are NP-complete, even for interval graphs of diameter $2$ and\npermutation graphs of diameter $2$. While IDENTIFYING CODE and (OPEN)\nLOCATING-DOMINATING SET are trivially fixed-parameter-tractable when\nparameterized by solution size, it is known that in the same setting METRIC\nDIMENSION is $W[2]$-hard. We show that for interval graphs, this\nparameterization of METRIC DIMENSION is fixed-parameter-tractable. \n\n"}
{"id": "1405.2447", "contents": "Title: Reconfiguration over tree decompositions Abstract: A vertex-subset graph problem $Q$ defines which subsets of the vertices of an\ninput graph are feasible solutions. The reconfiguration version of a\nvertex-subset problem $Q$ asks whether it is possible to transform one feasible\nsolution for $Q$ into another in at most $\\ell$ steps, where each step is a\nvertex addition or deletion, and each intermediate set is also a feasible\nsolution for $Q$ of size bounded by $k$. Motivated by recent results\nestablishing W[1]-hardness of the reconfiguration versions of most\nvertex-subset problems parameterized by $\\ell$, we investigate the complexity\nof such problems restricted to graphs of bounded treewidth. We show that the\nreconfiguration versions of most vertex-subset problems remain PSPACE-complete\non graphs of treewidth at most $t$ but are fixed-parameter tractable\nparameterized by $\\ell + t$ for all vertex-subset problems definable in monadic\nsecond-order logic (MSOL). To prove the latter result, we introduce a technique\nwhich allows us to circumvent cardinality constraints and define\nreconfiguration problems in MSOL. \n\n"}
{"id": "1405.5754", "contents": "Title: Twenty-Five Comparators is Optimal when Sorting Nine Inputs (and\n  Twenty-Nine for Ten) Abstract: This paper describes a computer-assisted non-existence proof of nine-input\nsorting networks consisting of 24 comparators, hence showing that the\n25-comparator sorting network found by Floyd in 1964 is optimal. As a\ncorollary, we obtain that the 29-comparator network found by Waksman in 1969 is\noptimal when sorting ten inputs.\n  This closes the two smallest open instances of the optimal size sorting\nnetwork problem, which have been open since the results of Floyd and Knuth from\n1966 proving optimality for sorting networks of up to eight inputs.\n  The proof involves a combination of two methodologies: one based on\nexploiting the abundance of symmetries in sorting networks, and the other,\nbased on an encoding of the problem to that of satisfiability of propositional\nlogic. We illustrate that, while each of these can single handed solve smaller\ninstances of the problem, it is their combination which leads to an efficient\nsolution for nine inputs. \n\n"}
{"id": "1405.5869", "contents": "Title: Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search\n  (MIPS) Abstract: We present the first provably sublinear time algorithm for approximate\n\\emph{Maximum Inner Product Search} (MIPS). Our proposal is also the first\nhashing algorithm for searching with (un-normalized) inner product as the\nunderlying similarity measure. Finding hashing schemes for MIPS was considered\nhard. We formally show that the existing Locality Sensitive Hashing (LSH)\nframework is insufficient for solving MIPS, and then we extend the existing LSH\nframework to allow asymmetric hashing schemes. Our proposal is based on an\ninteresting mathematical phenomenon in which inner products, after independent\nasymmetric transformations, can be converted into the problem of approximate\nnear neighbor search. This key observation makes efficient sublinear hashing\nscheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we\nprovide an explicit construction of provably fast hashing scheme for MIPS. The\nproposed construction and the extended LSH framework could be of independent\ntheoretical interest. Our proposed algorithm is simple and easy to implement.\nWe evaluate the method, for retrieving inner products, in the collaborative\nfiltering task of item recommendations on Netflix and Movielens datasets. \n\n"}
{"id": "1405.5975", "contents": "Title: Solving Multi-choice Secretary Problem in Parallel: An Optimal\n  Observation-Selection Protocol Abstract: The classical secretary problem investigates the question of how to hire the\nbest secretary from $n$ candidates who come in a uniformly random order. In\nthis work we investigate a parallel generalizations of this problem introduced\nby Feldman and Tennenholtz [14]. We call it shared $Q$-queue $J$-choice\n$K$-best secretary problem. In this problem, $n$ candidates are evenly\ndistributed into $Q$ queues, and instead of hiring the best one, the employer\nwants to hire $J$ candidates among the best $K$ persons. The $J$ quotas are\nshared by all queues. This problem is a generalized version of $J$-choice\n$K$-best problem which has been extensively studied and it has more practical\nvalue as it characterizes the parallel situation.\n  Although a few of works have been done about this generalization, to the best\nof our knowledge, no optimal deterministic protocol was known with general $Q$\nqueues. In this paper, we provide an optimal deterministic protocol for this\nproblem. The protocol is in the same style of the $1\\over e$-solution for the\nclassical secretary problem, but with multiple phases and adaptive criteria.\nOur protocol is very simple and efficient, and we show that several\ngeneralizations, such as the fractional $J$-choice $K$-best secretary problem\nand exclusive $Q$-queue $J$-choice $K$-best secretary problem, can be solved\noptimally by this protocol with slight modification and the latter one solves\nan open problem of Feldman and Tennenholtz [14].\n  In addition, we provide theoretical analysis for two typical cases, including\nthe 1-queue 1-choice $K$-best problem and the shared 2-queue 2-choice 2-best\nproblem. For the former, we prove a lower bound $1-O(\\frac{\\ln^2K}{K^2})$ of\nthe competitive ratio. For the latter, we show the optimal competitive ratio is\n$\\approx0.372$ while previously the best known result is 0.356 [14]. \n\n"}
{"id": "1405.6791", "contents": "Title: Agnostic Learning of Disjunctions on Symmetric Distributions Abstract: We consider the problem of approximating and learning disjunctions (or\nequivalently, conjunctions) on symmetric distributions over $\\{0,1\\}^n$.\nSymmetric distributions are distributions whose PDF is invariant under any\npermutation of the variables. We give a simple proof that for every symmetric\ndistribution $\\mathcal{D}$, there exists a set of $n^{O(\\log{(1/\\epsilon)})}$\nfunctions $\\mathcal{S}$, such that for every disjunction $c$, there is function\n$p$, expressible as a linear combination of functions in $\\mathcal{S}$, such\nthat $p$ $\\epsilon$-approximates $c$ in $\\ell_1$ distance on $\\mathcal{D}$ or\n$\\mathbf{E}_{x \\sim \\mathcal{D}}[ |c(x)-p(x)|] \\leq \\epsilon$. This directly\ngives an agnostic learning algorithm for disjunctions on symmetric\ndistributions that runs in time $n^{O( \\log{(1/\\epsilon)})}$. The best known\nprevious bound is $n^{O(1/\\epsilon^4)}$ and follows from approximation of the\nmore general class of halfspaces (Wimmer, 2010). We also show that there exists\na symmetric distribution $\\mathcal{D}$, such that the minimum degree of a\npolynomial that $1/3$-approximates the disjunction of all $n$ variables is\n$\\ell_1$ distance on $\\mathcal{D}$ is $\\Omega( \\sqrt{n})$. Therefore the\nlearning result above cannot be achieved via $\\ell_1$-regression with a\npolynomial basis used in most other agnostic learning algorithms.\n  Our technique also gives a simple proof that for any product distribution\n$\\mathcal{D}$ and every disjunction $c$, there exists a polynomial $p$ of\ndegree $O(\\log{(1/\\epsilon)})$ such that $p$ $\\epsilon$-approximates $c$ in\n$\\ell_1$ distance on $\\mathcal{D}$. This was first proved by Blais et al.\n(2008) via a more involved argument. \n\n"}
{"id": "1405.6851", "contents": "Title: Exact Algorithms for 0-1 Integer Programs with Linear Equality\n  Constraints Abstract: In this paper, we show $O(1.415^n)$-time and $O(1.190^n)$-space exact\nalgorithms for 0-1 integer programs where constraints are linear equalities and\ncoefficients are arbitrary real numbers. Our algorithms are quadratically\nfaster than exhaustive search and almost quadratically faster than an algorithm\nfor an inequality version of the problem by Impagliazzo, Lovett, Paturi and\nSchneider (arXiv:1401.5512), which motivated our work. Rather than improving\nthe time and space complexity, we advance to a simple direction as inclusion of\nmany NP-hard problems in terms of exact exponential algorithms. Specifically,\nwe extend our algorithms to linear optimization problems. \n\n"}
{"id": "1405.7192", "contents": "Title: The PeerRank Method for Peer Assessment Abstract: We propose the PeerRank method for peer assessment. This constructs a grade\nfor an agent based on the grades proposed by the agents evaluating the agent.\nSince the grade of an agent is a measure of their ability to grade correctly,\nthe PeerRank method weights grades by the grades of the grading agent. The\nPeerRank method also provides an incentive for agents to grade correctly. As\nthe grades of an agent depend on the grades of the grading agents, and as these\ngrades themselves depend on the grades of other agents, we define the PeerRank\nmethod by a fixed point equation similar to the PageRank method for ranking\nweb-pages. We identify some formal properties of the PeerRank method (for\nexample, it satisfies axioms of unanimity, no dummy, no discrimination and\nsymmetry), discuss some examples, compare with related work and evaluate the\nperformance on some synthetic data. Our results show considerable promise,\nreducing the error in grade predictions by a factor of 2 or more in many cases\nover the natural baseline of averaging peer grades. \n\n"}
{"id": "1405.7375", "contents": "Title: Tensor Network Contractions for #SAT Abstract: The computational cost of counting the number of solutions satisfying a\nBoolean formula, which is a problem instance of #SAT, has proven subtle to\nquantify. Even when finding individual satisfying solutions is computationally\neasy (e.g. 2-SAT, which is in P), determining the number of solutions is\n#P-hard. Recently, computational methods simulating quantum systems experienced\nadvancements due to the development of tensor network algorithms and associated\nquantum physics-inspired techniques. By these methods, we give an algorithm\nusing an axiomatic tensor contraction language for n-variable #SAT instances\nwith complexity $O((g+cd)^{O(1)} 2^c)$ where $c$ is the number of COPY-tensors,\n$g$ is the number of gates, and $d$ is the maximal degree of any COPY-tensor.\nThus, counting problems can be solved efficiently when their tensor network\nexpression has at most $O(\\log c)$ COPY-tensors and polynomial fan-out. This\nframework also admits an intuitive proof of a variant of the Tovey conjecture\n(the r,1-SAT instance of the Dubois-Tovey theorem). This study increases the\ntheory, expressiveness and application of tensor based algorithmic tools and\nprovides an alternative insight on these problems which have a long history in\nstatistical physics and computer science. \n\n"}
{"id": "1405.7910", "contents": "Title: Optimal CUR Matrix Decompositions Abstract: The CUR decomposition of an $m \\times n$ matrix $A$ finds an $m \\times c$\nmatrix $C$ with a subset of $c < n$ columns of $A,$ together with an $r \\times\nn$ matrix $R$ with a subset of $r < m$ rows of $A,$ as well as a $c \\times r$\nlow-rank matrix $U$ such that the matrix $C U R$ approximates the matrix $A,$\nthat is, $ || A - CUR ||_F^2 \\le (1+\\epsilon) || A - A_k||_F^2$, where\n$||.||_F$ denotes the Frobenius norm and $A_k$ is the best $m \\times n$ matrix\nof rank $k$ constructed via the SVD. We present input-sparsity-time and\ndeterministic algorithms for constructing such a CUR decomposition where\n$c=O(k/\\epsilon)$ and $r=O(k/\\epsilon)$ and rank$(U) = k$. Up to constant\nfactors, our algorithms are simultaneously optimal in $c, r,$ and rank$(U)$. \n\n"}
{"id": "1406.1244", "contents": "Title: Distributed Approximation of Minimum Routing Cost Trees Abstract: We study the NP-hard problem of approximating a Minimum Routing Cost Spanning\nTree in the message passing model with limited bandwidth (CONGEST model). In\nthis problem one tries to find a spanning tree of a graph $G$ over $n$ nodes\nthat minimizes the sum of distances between all pairs of nodes. In the\nconsidered model every node can transmit a different (but short) message to\neach of its neighbors in each synchronous round. We provide a randomized\n$(2+\\epsilon)$-approximation with runtime $O(D+\\frac{\\log n}{\\epsilon})$ for\nunweighted graphs. Here, $D$ is the diameter of $G$. This improves over both,\nthe (expected) approximation factor $O(\\log n)$ and the runtime $O(D\\log^2 n)$\nof the best previously known algorithm.\n  Due to stating our results in a very general way, we also derive an (optimal)\nruntime of $O(D)$ when considering $O(\\log n)$-approximations as done by the\nbest previously known algorithm. In addition we derive a deterministic\n$2$-approximation. \n\n"}
{"id": "1406.1717", "contents": "Title: Median Filtering is Equivalent to Sorting Abstract: This work shows that the following problems are equivalent, both in theory\nand in practice:\n  - median filtering: given an $n$-element vector, compute the sliding window\nmedian with window size $k$,\n  - piecewise sorting: given an $n$-element vector, divide it in $n/k$ blocks\nof length $k$ and sort each block.\n  By prior work, median filtering is known to be at least as hard as piecewise\nsorting: with a single median filter operation we can sort $\\Theta(n/k)$ blocks\nof length $\\Theta(k)$. The present work shows that median filtering is also as\neasy as piecewise sorting: we can do median filtering with one piecewise\nsorting operation and linear-time postprocessing. In particular, median\nfiltering can directly benefit from the vast literature on sorting\nalgorithms---for example, adaptive sorting algorithms imply adaptive median\nfiltering algorithms.\n  The reduction is very efficient in practice---for random inputs the\nperformance of the new sorting-based algorithm is on a par with the fastest\nheap-based algorithms, and for benign data distributions it typically\noutperforms prior algorithms.\n  The key technical idea is that we can represent the sliding window with a\npair of sorted doubly-linked lists: we delete items from one list and add items\nto the other list. Deletions are easy; additions can be done efficiently if we\nreverse the time twice: First we construct the full list and delete the items\nin the reverse order. Then we undo each deletion with Knuth's dancing links\ntechnique. \n\n"}
{"id": "1406.5665", "contents": "Title: Constant Factor Approximation for Balanced Cut in the PIE model Abstract: We propose and study a new semi-random semi-adversarial model for Balanced\nCut, a planted model with permutation-invariant random edges (PIE). Our model\nis much more general than planted models considered previously. Consider a set\nof vertices V partitioned into two clusters $L$ and $R$ of equal size. Let $G$\nbe an arbitrary graph on $V$ with no edges between $L$ and $R$. Let\n$E_{random}$ be a set of edges sampled from an arbitrary permutation-invariant\ndistribution (a distribution that is invariant under permutation of vertices in\n$L$ and in $R$). Then we say that $G + E_{random}$ is a graph with\npermutation-invariant random edges.\n  We present an approximation algorithm for the Balanced Cut problem that finds\na balanced cut of cost $O(|E_{random}|) + n \\text{polylog}(n)$ in this model.\nIn the regime when $|E_{random}| = \\Omega(n \\text{polylog}(n))$, this is a\nconstant factor approximation with respect to the cost of the planted cut. \n\n"}
{"id": "1406.5667", "contents": "Title: Correlation Clustering with Noisy Partial Information Abstract: In this paper, we propose and study a semi-random model for the Correlation\nClustering problem on arbitrary graphs G. We give two approximation algorithms\nfor Correlation Clustering instances from this model. The first algorithm finds\na solution of value $(1+ \\delta) optcost + O_{\\delta}(n\\log^3 n)$ with high\nprobability, where $optcost$ is the value of the optimal solution (for every\n$\\delta > 0$). The second algorithm finds the ground truth clustering with an\narbitrarily small classification error $\\eta$ (under some additional\nassumptions on the instance). \n\n"}
{"id": "1406.6625", "contents": "Title: Computational Lower Bounds for Community Detection on Random Graphs Abstract: This paper studies the problem of detecting the presence of a small dense\ncommunity planted in a large Erd\\H{o}s-R\\'enyi random graph $\\mathcal{G}(N,q)$,\nwhere the edge probability within the community exceeds $q$ by a constant\nfactor. Assuming the hardness of the planted clique detection problem, we show\nthat the computational complexity of detecting the community exhibits the\nfollowing phase transition phenomenon: As the graph size $N$ grows and the\ngraph becomes sparser according to $q=N^{-\\alpha}$, there exists a critical\nvalue of $\\alpha = \\frac{2}{3}$, below which there exists a computationally\nintensive procedure that can detect far smaller communities than any\ncomputationally efficient procedure, and above which a linear-time procedure is\nstatistically optimal. The results also lead to the average-case hardness\nresults for recovering the dense community and approximating the densest\n$K$-subgraph. \n\n"}
{"id": "1406.7363", "contents": "Title: On the Synchronization Rate for e-machines Abstract: It is known, that an $\\epsilon$-machine is either exactly or asymptotically\nsynchronizing. In the exact case, the observer can infer the current machine\nstate after observing $L$ generated symbols with probability $1-a^L$ where $0\n\\leq a<1$ is a so-called synchronization rate constant. In the asymptotic case,\nthe probability of the correct prediction the current machine state after\nobserving $L$ generated symbols tends to $1$ exponentially fast as $1-b^L$ for\n$0<b<1$ and the infimum of such $b$ is a so-called prediction rate constant.\n  Hence the synchronization and prediction rate constants serve as natural\nmeasures of synchronization for $\\epsilon$-machines. In the present work we\nshow how to approximate these constants in polynomial time in terms of the\nnumber of machine states. \n\n"}
{"id": "1406.7535", "contents": "Title: Hitting-sets for ROABP and Sum of Set-Multilinear circuits Abstract: We give a $n^{O(\\log n)}$-time ($n$ is the input size) blackbox polynomial\nidentity testing algorithm for unknown-order read-once oblivious algebraic\nbranching programs (ROABP). The best result known for this class was\n$n^{O(\\log^2 n)}$ due to Forbes-Saptharishi-Shpilka (STOC 2014), and that too\nonly for multilinear ROABP. We get rid of their exponential dependence on the\nindividual degree. With this, we match the time-complexity for the unknown\norder ROABP with the known order ROABP (due to Forbes-Shpilka (FOCS 2013)) and\nalso with the depth-$3$ set-multilinear circuits (due to Agrawal-Saha-Saxena\n(STOC 2013)). Our proof is simpler and involves a new technique called basis\nisolation.\n  The depth-$3$ model has recently gained much importance, as it has become a\nstepping-stone to understanding general arithmetic circuits. Its restriction to\nmultilinearity has known exponential lower bounds but no nontrivial blackbox\nidentity tests. In this paper, we take a step towards designing such\nhitting-sets. We give the first subexponential whitebox PIT for the sum of\nconstantly many set-multilinear depth-$3$ circuits. To achieve this, we define\nnotions of distance and base sets. Distance, for a multilinear depth-$3$\ncircuit, measures how far are the partitions from a mere refinement. We design\na hitting-set in time $n^{O(d \\log n)}$ for $d$-distance. Further, we give an\nextension of our result to models where the distance is large but it is small\nwhen restricted to certain base sets (of variables).\n  We also explore a new model of ROABP where the factor-matrices are invertible\n(called invertible-factor ROABP). We design a hitting-set in time\npoly($n^{w^2}$) for width-$w$ invertible-factor ROABP. Further, we could do\nwithout the invertibility restriction when $w=2$. Previously, the best result\nfor width-$2$ ROABP was quasi-polynomial time (Forbes-Saptharishi-Shpilka, STOC\n2014). \n\n"}
{"id": "1407.0085", "contents": "Title: Improved Quantum Algorithm for Triangle Finding via Combinatorial\n  Arguments Abstract: In this paper we present a quantum algorithm solving the triangle finding\nproblem in unweighted graphs with query complexity $\\tilde O(n^{5/4})$, where\n$n$ denotes the number of vertices in the graph. This improves the previous\nupper bound $O(n^{9/7})=O(n^{1.285...})$ recently obtained by Lee, Magniez and\nSantha. Our result shows, for the first time, that in the quantum query\ncomplexity setting unweighted triangle finding is easier than its edge-weighted\nversion, since for finding an edge-weighted triangle Belovs and Rosmanis proved\nthat any quantum algorithm requires $\\Omega(n^{9/7}/\\sqrt{\\log n})$ queries.\nOur result also illustrates some limitations of the non-adaptive learning graph\napproach used to obtain the previous $O(n^{9/7})$ upper bound since, even over\nunweighted graphs, any quantum algorithm for triangle finding obtained using\nthis approach requires $\\Omega(n^{9/7}/\\sqrt{\\log n})$ queries as well. To\nbypass the obstacles characterized by these lower bounds, our quantum algorithm\nuses combinatorial ideas exploiting the graph-theoretic properties of triangle\nfinding, which cannot be used when considering edge-weighted graphs or the\nnon-adaptive learning graph approach. \n\n"}
{"id": "1407.2479", "contents": "Title: Making the Most of Your Samples Abstract: We study the problem of setting a price for a potential buyer with a\nvaluation drawn from an unknown distribution $D$. The seller has \"data\"' about\n$D$ in the form of $m \\ge 1$ i.i.d. samples, and the algorithmic challenge is\nto use these samples to obtain expected revenue as close as possible to what\ncould be achieved with advance knowledge of $D$.\n  Our first set of results quantifies the number of samples $m$ that are\nnecessary and sufficient to obtain a $(1-\\epsilon)$-approximation. For example,\nfor an unknown distribution that satisfies the monotone hazard rate (MHR)\ncondition, we prove that $\\tilde{\\Theta}(\\epsilon^{-3/2})$ samples are\nnecessary and sufficient. Remarkably, this is fewer samples than is necessary\nto accurately estimate the expected revenue obtained by even a single reserve\nprice. We also prove essentially tight sample complexity bounds for regular\ndistributions, bounded-support distributions, and a wide class of irregular\ndistributions. Our lower bound approach borrows tools from differential privacy\nand information theory, and we believe it could find further applications in\nauction theory.\n  Our second set of results considers the single-sample case. For regular\ndistributions, we prove that no pricing strategy is better than\n$\\tfrac{1}{2}$-approximate, and this is optimal by the Bulow-Klemperer theorem.\nFor MHR distributions, we show how to do better: we give a simple pricing\nstrategy that guarantees expected revenue at least $0.589$ times the maximum\npossible. We also prove that no pricing strategy achieves an approximation\nguarantee better than $\\frac{e}{4} \\approx .68$. \n\n"}
{"id": "1407.2929", "contents": "Title: Complexity of counting subgraphs: only the boundedness of the\n  vertex-cover number counts Abstract: For a class $\\mathcal{H}$ of graphs, #Sub$(\\mathcal{H})$ is the counting\nproblem that, given a graph $H\\in \\mathcal{H}$ and an arbitrary graph $G$, asks\nfor the number of subgraphs of $G$ isomorphic to $H$. It is known that if\n$\\mathcal{H}$ has bounded vertex-cover number (equivalently, the size of the\nmaximum matching in $\\mathcal{H}$ is bounded), then #Sub$(\\mathcal{H})$ is\npolynomial-time solvable. We complement this result with a corresponding lower\nbound: if $\\mathcal{H}$ is any recursively enumerable class of graphs with\nunbounded vertex-cover number, then #Sub$(\\mathcal{H})$ is #W[1]-hard\nparameterized by the size of $H$ and hence not polynomial-time solvable and not\neven fixed-parameter tractable, unless FPT = #W[1].\n  As a first step of the proof, we show that counting $k$-matchings in\nbipartite graphs is #W[1]-hard. Recently, Curticapean [ICALP 2013] proved the\n#W[1]-hardness of counting $k$-matchings in general graphs; our result\nstrengthens this statement to bipartite graphs with a considerably simpler\nproof and even shows that, assuming the Exponential Time Hypothesis (ETH),\nthere is no $f(k)n^{o(k/\\log k)}$ time algorithm for counting $k$-matchings in\nbipartite graphs for any computable function $f(k)$. As a consequence, we\nobtain an independent and somewhat simpler proof of the classical result of\nFlum and Grohe [SICOMP 2004] stating that counting paths of length $k$ is\n#W[1]-hard, as well as a similar almost-tight ETH-based lower bound on the\nexponent. \n\n"}
{"id": "1407.4286", "contents": "Title: Constructing small tree grammars and small circuits for formulas Abstract: It is shown that every tree of size $n$ over a fixed set of $\\sigma$\ndifferent ranked symbols can be decomposed (in linear time as well as in\nlogspace) into $O\\big(\\frac{n}{\\log_\\sigma n}\\big) = O\\big(\\frac{n \\log\n\\sigma}{\\log n}\\big)$ many hierarchically defined pieces. Formally, such a\nhierarchical decomposition has the form of a straight-line linear context-free\ntree grammar of size $O\\big(\\frac{n}{\\log_\\sigma n}\\big)$, which can be used as\na compressed representation of the input tree. This generalizes an analogous\nresult for strings. Previous grammar-based tree compressors were not analyzed\nfor the worst-case size of the computed grammar, except for the top dag of\nBille et al., for which only the weaker upper bound of\n$O\\big(\\frac{n}{\\log_\\sigma^{0.19} n}\\big)$ (which was very recently improved\nto $O\\big(\\frac{n \\cdot \\log \\log_\\sigma n}{\\log_\\sigma n}\\big)$ by\nH\\\"ubschle-Schneider and Raman) for unranked and unlabelled trees has been\nderived. The main result is used to show that every arithmetical formula of\nsize $n$, in which only $m \\leq n$ different variables occur, can be\ntransformed (in linear time as well as in logspace) into an arithmetical\ncircuit of size $O\\big(\\frac{n \\cdot \\log m}{\\log n}\\big)$ and depth $O(\\log\nn)$. This refines a classical result of Brent from 1974, according to which an\narithmetical formula of size $n$ can be transformed into a logarithmic depth\ncircuit of size $O(n)$. \n\n"}
{"id": "1407.4293", "contents": "Title: A complexity analysis of Policy Iteration through combinatorial matrices\n  arising from Unique Sink Orientations Abstract: Unique Sink Orientations (USOs) are an appealing abstraction of several major\noptimization problems of applied mathematics such as for instance Linear\nProgramming (LP), Markov Decision Processes (MDPs) or 2-player Turn Based\nStochastic Games (2TBSGs). A polynomial time algorithm to find the sink of a\nUSO would translate into a strongly polynomial time algorithm to solve the\naforementioned problems---a major quest for all three cases. In addition, we\nmay translate MDPs and 2TBSGs into the problem of finding the sink of an\nacyclic USO of a cube, which can be done using the well-known Policy Iteration\nalgorithm (PI). The study of its complexity is the object of this work. Despite\nits exponential worst case complexity, the principle of PI is a powerful source\nof inspiration for other methods.\n  As our first contribution, we disprove Hansen and Zwick's conjecture claiming\nthat the number of steps of PI should follow the Fibonacci sequence in the\nworst case. Our analysis relies on a new combinatorial formulation of the\nproblem---the so-called Order-Regularity formulation (OR). Then, for our second\ncontribution, we (exponentially) improve the $\\Omega(1.4142^n)$ lower bound on\nthe number of steps of PI from Schurr and Szab\\'o in the case of the OR\nformulation and obtain an $\\Omega(1.4269^n)$ bound. \n\n"}
{"id": "1407.6178", "contents": "Title: Computing the $2$-blocks of directed graphs Abstract: Let $G$ be a directed graph. A \\textit{$2$-directed block} in $G$ is a\nmaximal vertex set $C^{2d}\\subseteq V$ with $|C^{2d}|\\geq 2$ such that for each\npair of distinct vertices $x,y \\in C^{2d}$, there exist two vertex-disjoint\npaths from $x$ to $y$ and two vertex-disjoint paths from $y$ to $x$ in $G$. In\ncontrast to the $2$-vertex-connected components of $G$, the subgraphs induced\nby the $2$-directed blocks may consist of few or no edges. In this paper we\npresent two algorithms for computing the $2$-directed blocks of $G$ in\n$O(\\min\\lbrace m,(t_{sap}+t_{sb})n\\rbrace n)$ time, where $t_{sap}$ is the\nnumber of the strong articulation points of $G$ and $t_{sb}$ is the number of\nthe strong bridges of $G$. Furthermore, we study two related concepts: the\n$2$-strong blocks and the $2$-edge blocks of $G$. We give two algorithms for\ncomputing the $2$-strong blocks of $G$ in $O( \\min \\lbrace m,t_{sap} n\\rbrace\nn)$ time and we show that the $2$-edge blocks of $G$ can be computed in $O(\\min\n\\lbrace m, t_{sb} n \\rbrace n)$ time. In this paper we also study some\noptimization problems related to the strong articulation points and the\n$2$-blocks of a directed graph. Given a strongly connected graph $G=(V,E)$,\nfind a minimum cardinality set $E^{*}\\subseteq E$ such that $G^{*}=(V,E^{*})$\nis strongly connected and the strong articulation points of $G$ coincide with\nthe strong articulation points of $G^{*}$. This problem is called minimum\nstrongly connected spanning subgraph with the same strong articulation points.\nWe show that there is a linear time $17/3$ approximation algorithm for this\nNP-hard problem. We also consider the problem of finding a minimum strongly\nconnected spanning subgraph with the same $2$-blocks in a strongly connected\ngraph $G$. We present approximation algorithms for three versions of this\nproblem, depending on the type of $2$-blocks. \n\n"}
{"id": "1407.7061", "contents": "Title: A Parallel Branch and Bound Algorithm for the Maximum Labelled Clique\n  Problem Abstract: The maximum labelled clique problem is a variant of the maximum clique\nproblem where edges in the graph are given labels, and we are not allowed to\nuse more than a certain number of distinct labels in a solution. We introduce a\nnew branch-and-bound algorithm for the problem, and explain how it may be\nparallelised. We evaluate an implementation on a set of benchmark instances,\nand show that it is consistently faster than previously published results,\nsometimes by four or five orders of magnitude. \n\n"}
{"id": "1407.8088", "contents": "Title: The Grow-Shrink strategy for learning Markov network structures\n  constrained by context-specific independences Abstract: Markov networks are models for compactly representing complex probability\ndistributions. They are composed by a structure and a set of numerical weights.\nThe structure qualitatively describes independences in the distribution, which\ncan be exploited to factorize the distribution into a set of compact functions.\nA key application for learning structures from data is to automatically\ndiscover knowledge. In practice, structure learning algorithms focused on\n\"knowledge discovery\" present a limitation: they use a coarse-grained\nrepresentation of the structure. As a result, this representation cannot\ndescribe context-specific independences. Very recently, an algorithm called\nCSPC was designed to overcome this limitation, but it has a high computational\ncomplexity. This work tries to mitigate this downside presenting CSGS, an\nalgorithm that uses the Grow-Shrink strategy for reducing unnecessary\ncomputations. On an empirical evaluation, the structures learned by CSGS\nachieve competitive accuracies and lower computational complexity with respect\nto those obtained by CSPC. \n\n"}
{"id": "1408.1211", "contents": "Title: A Unifying Hierarchy of Valuations with Complements and Substitutes Abstract: We introduce a new hierarchy over monotone set functions, that we refer to as\n$\\mathcal{MPH}$ (Maximum over Positive Hypergraphs). Levels of the hierarchy\ncorrespond to the degree of complementarity in a given function. The highest\nlevel of the hierarchy, $\\mathcal{MPH}$-$m$ (where $m$ is the total number of\nitems) captures all monotone functions. The lowest level, $\\mathcal{MPH}$-$1$,\ncaptures all monotone submodular functions, and more generally, the class of\nfunctions known as $\\mathcal{XOS}$. Every monotone function that has a positive\nhypergraph representation of rank $k$ (in the sense defined by Abraham,\nBabaioff, Dughmi and Roughgarden [EC 2012]) is in $\\mathcal{MPH}$-$k$. Every\nmonotone function that has supermodular degree $k$ (in the sense defined by\nFeige and Izsak [ITCS 2013]) is in $\\mathcal{MPH}$-$(k+1)$. In both cases, the\nconverse direction does not hold, even in an approximate sense. We present\nadditional results that demonstrate the expressiveness power of\n$\\mathcal{MPH}$-$k$.\n  One can obtain good approximation ratios for some natural optimization\nproblems, provided that functions are required to lie in low levels of the\n$\\mathcal{MPH}$ hierarchy. We present two such applications. One shows that the\nmaximum welfare problem can be approximated within a ratio of $k+1$ if all\nplayers hold valuation functions in $\\mathcal{MPH}$-$k$. The other is an upper\nbound of $2k$ on the price of anarchy of simultaneous first price auctions.\n  Being in $\\mathcal{MPH}$-$k$ can be shown to involve two requirements -- one\nis monotonicity and the other is a certain requirement that we refer to as\n$\\mathcal{PLE}$ (Positive Lower Envelope). Removing the monotonicity\nrequirement, one obtains the $\\mathcal{PLE}$ hierarchy over all non-negative\nset functions (whether monotone or not), which can be fertile ground for\nfurther research. \n\n"}
{"id": "1408.1577", "contents": "Title: Towards More Practical Linear Programming-based Techniques for\n  Algorithmic Mechanism Design Abstract: R. Lavy and C. Swamy (FOCS 2005, J. ACM 2011) introduced a general method for\nobtaining truthful-in-expectation mechanisms from linear programming based\napproximation algorithms. Due to the use of the Ellipsoid method, a direct\nimplementation of the method is unlikely to be efficient in practice. We\npropose to use the much simpler and usually faster multiplicative weights\nupdate method instead. The simplification comes at the cost of slightly weaker\napproximation and truthfulness guarantees. \n\n"}
{"id": "1408.3374", "contents": "Title: Robust Adaptive Routing Under Uncertainty Abstract: We consider the problem of finding an optimal history-dependent routing\nstrategy on a directed graph weighted by stochastic arc costs when the\nobjective is to minimize the risk of spending more than a prescribed budget. To\nhelp mitigate the impact of the lack of information on the arc cost probability\ndistributions, we introduce a robust counterpart where the distributions are\nonly known through confidence intervals on some statistics such as the mean,\nthe mean absolute deviation, and any quantile. Leveraging recent results in\ndistributionally robust optimization, we develop a general-purpose algorithm to\ncompute an approximate optimal strategy. To illustrate the benefits of the\nrobust approach, we run numerical experiments with field data from the\nSingapore road network. \n\n"}
{"id": "1408.4263", "contents": "Title: Quantified Conjunctive Queries on Partially Ordered Sets Abstract: We study the computational problem of checking whether a quantified\nconjunctive query (a first-order sentence built using only conjunction as\nBoolean connective) is true in a finite poset (a reflexive, antisymmetric, and\ntransitive directed graph). We prove that the problem is already NP-hard on a\ncertain fixed poset, and investigate structural properties of posets yielding\nfixed-parameter tractability when the problem is parameterized by the query.\nOur main algorithmic result is that model checking quantified conjunctive\nqueries on posets of bounded width is fixed-parameter tractable (the width of a\nposet is the maximum size of a subset of pairwise incomparable elements). We\ncomplement our algorithmic result by complexity results with respect to classes\nof finite posets in a hierarchy of natural poset invariants, establishing its\ntightness in this sense. \n\n"}
{"id": "1408.7019", "contents": "Title: On Index Coding and Graph Homomorphism Abstract: In this work, we study the problem of index coding from graph homomorphism\nperspective. We show that the minimum broadcast rate of an index coding problem\nfor different variations of the problem such as non-linear, scalar, and vector\nindex code, can be upper bounded by the minimum broadcast rate of another index\ncoding problem when there exists a homomorphism from the complement of the side\ninformation graph of the first problem to that of the second problem. As a\nresult, we show that several upper bounds on scalar and vector index code\nproblem are special cases of one of our main theorems.\n  For the linear scalar index coding problem, it has been shown in [1] that the\nbinary linear index of a graph is equal to a graph theoretical parameter called\nminrank of the graph. For undirected graphs, in [2] it is shown that\n$\\mathrm{minrank}(G) = k$ if and only if there exists a homomorphism from\n$\\bar{G}$ to a predefined graph $\\bar{G}_k$. Combining these two results, it\nfollows that for undirected graphs, all the digraphs with linear index of at\nmost k coincide with the graphs $G$ for which there exists a homomorphism from\n$\\bar{G}$ to $\\bar{G}_k$. In this paper, we give a direct proof to this result\nthat works for digraphs as well.\n  We show how to use this classification result to generate lower bounds on\nscalar and vector index. In particular, we provide a lower bound for the scalar\nindex of a digraph in terms of the chromatic number of its complement.\n  Using our framework, we show that by changing the field size, linear index of\na digraph can be at most increased by a factor that is independent from the\nnumber of the nodes. \n\n"}
{"id": "1409.2170", "contents": "Title: The universal homogeneous binary tree Abstract: A partial order is called semilinear iff the upper bounds of each element are\nlinearly ordered and any two elements have a common upper bound. There exists,\nup to isomorphism, a unique countable existentially closed semilinear order,\nwhich we denote by S2. We study the reducts of S2, that is, the relational\nstructures with the same domain as S2 all of whose relations are first-order\ndefinable in S2. Our main result is a classification of the model-complete\ncores of the reducts of S2. From this, we also obtain a classification of\nreducts up to first-order interdefinability, which is equivalent to a\nclassification of all closed permutation groups that contain the automorphism\ngroup of S2. \n\n"}
{"id": "1409.3093", "contents": "Title: Gaussian Noise Sensitivity and BosonSampling Abstract: We study the sensitivity to noise of |permanent(X)|^2 for random real and\ncomplex n x n Gaussian matrices X, and show that asymptotically the correlation\nbetween the noisy and noiseless outcomes tends to zero when the noise level is\n{\\omega}(1)/n. This suggests that, under certain reasonable noise models, the\nprobability distributions produced by noisy BosonSampling are very sensitive to\nnoise. We also show that when the amount of noise is constant the noisy value\nof |permanent(X)|^2 can be approximated efficiently on a classical computer.\nThese results seem to weaken the possibility of demonstrating quantum-speedup\nvia BosonSampling without quantum fault-tolerance. \n\n"}
{"id": "1409.3323", "contents": "Title: The Structure of Promises in Quantum Speedups Abstract: It has long been known that in the usual black-box model, one cannot get\nsuper-polynomial quantum speedups without some promise on the inputs. In this\npaper, we examine certain types of symmetric promises, and show that they also\ncannot give rise to super-polynomial quantum speedups. We conclude that\nexponential quantum speedups only occur given \"structured\" promises on the\ninput.\n  Specifically, we show that there is a polynomial relationship of degree $12$\nbetween $D(f)$ and $Q(f)$ for any function $f$ defined on permutations\n(elements of $\\{0,1,\\dots, M-1\\}^n$ in which each alphabet element occurs\nexactly once). We generalize this result to all functions $f$ defined on orbits\nof the symmetric group action $S_n$ (which acts on an element of $\\{0,1,\\dots,\nM-1\\}^n$ by permuting its entries). We also show that when $M$ is constant, any\nfunction $f$ defined on a \"symmetric set\" - one invariant under $S_n$ -\nsatisfies $R(f)=O(Q(f)^{12(M-1)})$. \n\n"}
{"id": "1409.6015", "contents": "Title: A fast algorithm for computing irreducible triangulations of closed\n  surfaces in $E^d$ Abstract: We give a fast algorithm for computing an irreducible triangulation\n$T^\\prime$ of an oriented, connected, boundaryless, and compact surface $S$ in\n$E^d$ from any given triangulation $T$ of $S$. If the genus $g$ of $S$ is\npositive, then our algorithm takes $O(g^2+gn)$ time to obtain $T^\\prime$, where\n$n$ is the number of triangles of $T$. Otherwise, $T^\\prime$ is obtained in\nlinear time in $n$. While the latter upper bound is optimal, the former upper\nbound improves upon the currently best known upper bound by a $(\\lg n / g)$\nfactor. In both cases, the memory space required by our algorithm is in\n${\\Theta}(n)$. \n\n"}
{"id": "1409.6366", "contents": "Title: A direct proof for Lovett's bound on the communication complexity of low\n  rank matrices Abstract: The log-rank conjecture in communication complexity suggests that the\ndeterministic communication complexity of any Boolean rank-r function is\nbounded by polylog(r). Recently, major progress was made by Lovett who proved\nthat the communication complexity is bounded by O(r^1/2 * log r). Lovett's\nproof is based on known estimates on the discrepancy of low-rank matrices. We\ngive a simple, direct proof based on a hyperplane rounding argument that in our\nopinion sheds more light on the reason why a root factor suffices and what is\nnecessary to improve on this factor. \n\n"}
{"id": "1409.6913", "contents": "Title: Generating a Quadratic Forms from a Given Genus Abstract: Given a non-empty genus in $n$ dimensions with determinant $d$, we give a\nrandomized algorithm that outputs a quadratic form from this genus. The time\ncomplexity of the algorithm is poly$(n,\\log d)$; assuming Generalized Riemann\nHypothesis (GRH). \n\n"}
{"id": "1410.0276", "contents": "Title: Improved Bounds for the Flat Wall Theorem Abstract: The Flat Wall Theorem of Robertson and Seymour states that there is some\nfunction $f$, such that for all integers $w,t>1$, every graph $G$ containing a\nwall of size $f(w,t)$, must contain either (i) a $K_t$-minor; or (ii) a small\nsubset $A\\subset V(G)$ of vertices, and a flat wall of size $w$ in $G\\setminus\nA$. Kawarabayashi, Thomas and Wollan recently showed a self-contained proof of\nthis theorem with the following two sets of parameters: (1)\n$f(w,t)=\\Theta(t^{24}(t^2+w))$ with $|A|=O(t^{24})$, and (2)\n$f(w,t)=w^{2^{\\Theta(t^{24})}}$ with $|A|\\leq t-5$. The latter result gives the\nbest possible bound on $|A|$. In this paper we improve their bounds to\n$f(w,t)=\\Theta(t(t+w))$ with $|A|\\leq t-5$. For the special case where the\nmaximum vertex degree in $G$ is bounded by $D$, we show that, if $G$ contains a\nwall of size $\\Omega(Dt(t+w))$, then either $G$ contains a $K_t$-minor, or\nthere is a flat wall of size $w$ in $G$. This setting naturally arises in\nalgorithms for the Edge-Disjoint Paths problem, with $D\\leq 4$. Like the proof\nof Kawarabayashi et al., our proof is self-contained, except for using a\nwell-known theorem on routing pairs of disjoint paths. We also provide\nefficient algorithms that return either a model of the $K_t$-minor, or a vertex\nset $A$ and a flat wall of size $w$ in $G\\setminus A$.\n  We complement our result for the low-degree scenario by proving an almost\nmatching lower bound: namely, for all integers $w,t>1$, there is a graph $G$,\ncontaining a wall of size $\\Omega(wt)$, such that the maximum vertex degree in\n$G$ is 5, and $G$ contains no flat wall of size $w$, and no $K_t$-minor. \n\n"}
{"id": "1410.0768", "contents": "Title: Space-Efficient Path-Reporting Approximate Distance Oracles Abstract: We consider approximate {\\em path-reporting} distance oracles, distance\nlabeling and labeled routing with extremely low space requirement, for general\nundirected graphs. For distance oracles, we show how to break the n\\log n space\nbound of Thorup and Zwick if approximate {\\em paths} rather than distances need\nto be reported. For approximate distance labeling and labeled routing, we break\nthe previously best known space bound of O(log n) words per vertex. The cost\nfor such space efficiency is an increased stretch. \n\n"}
{"id": "1410.1371", "contents": "Title: Linear Index Coding via Graph Homomorphism Abstract: It is known that the minimum broadcast rate of a linear index code over\n$\\mathbb{F}_q$ is equal to the $minrank_q$ of the underlying digraph. In [3] it\nis proved that for $\\mathbb{F}_2$ and any positive integer $k$,\n$minrank_q(G)\\leq k$ iff there exists a homomorphism from the complement of the\ngraph $G$ to the complement of a particular undirected graph family called\n\"graph family $\\{G_k\\}$\". As observed in [2], by combining these two results\none can relate the linear index coding problem of undirected graphs to the\ngraph homomorphism problem. In [4], a direct connection between linear index\ncoding problem and graph homomorphism problem is introduced. In contrast to the\nformer approach, the direct connection holds for digraphs as well and applies\nto any field size. More precisely, in [4], a graph family $\\{H_k^q\\}$ is\nintroduced and shown that whether or not the scalar linear index of a digraph\n$G$ is less than or equal to $k$ is equivalent to the existence of a graph\nhomomorphism from the complement of $G$ to the complement of $H_k^q$.\n  Here, we first study the structure of the digraphs $H_k^q$. Analogous to the\nresult of [2] about undirected graphs, we prove that $H_k^q$'s are vertex\ntransitive digraphs. Using this, and by applying a lemma of Hell and Nesetril\n[5], we derive a class of necessary conditions for digraphs $G$ to satisfy\n$lind_q(G)\\leq k$. Particularly, we obtain new lower bounds on $lind_q(G)$.\n  Our next result is about the computational complexity of scalar linear index\nof a digraph. It is known that deciding whether the scalar linear index of an\nundirected graph is equal to $k$ or not is NP-complete for $k\\ge 3$ and is\npolynomially decidable for $k=1,2$ [3]. For digraphs, it is shown in [6] that\nfor the binary alphabet, the decision problem for $k=2$ is NP-complete. We use\ngraph homomorphism framework to extend this result to arbitrary alphabet. \n\n"}
{"id": "1410.2595", "contents": "Title: Spatial mixing and the connective constant: Optimal bounds Abstract: We study the problem of deterministic approximate counting of matchings and\nindependent sets in graphs of bounded connective constant. More generally, we\nconsider the problem of evaluating the partition functions of the monomer-dimer\nmodel (which is defined as a weighted sum over all matchings where each\nmatching is given a weight $\\gamma^{|V| - 2 |M|}$ in terms of a fixed parameter\ngamma called the monomer activity) and the hard core model (which is defined as\na weighted sum over all independent sets where an independent set I is given a\nweight $\\lambda^{|I|}$ in terms of a fixed parameter lambda called the vertex\nactivity). The connective constant is a natural measure of the average degree\nof a graph which has been studied extensively in combinatorics and mathematical\nphysics, and can be bounded by a constant even for certain unbounded degree\ngraphs such as those sampled from the sparse Erd\\H{o}s-R\\'enyi model $G(n,\nd/n)$.\n  Our main technical contribution is to prove the best possible rates of decay\nof correlations in the natural probability distributions induced by both the\nhard core model and the monomer-dimer model in graphs with a given bound on the\nconnective constant. These results on decay of correlations are obtained using\na new framework based on the so-called message approach that has been\nextensively used recently to prove such results for bounded degree graphs. We\nthen use these optimal decay of correlations results to obtain FPTASs for the\ntwo problems on graphs of bounded connective constant.\n  Our techniques also allow us to improve upon known bounds for decay of\ncorrelations for the hard core model on various regular lattices, including\nthose obtained by Restrepo, Shin, Vigoda and Tetali (2011) for the special case\nof Z^2 using sophisticated numerically intensive methods tailored to that\nspecial case. \n\n"}
{"id": "1410.3386", "contents": "Title: Testing Poisson Binomial Distributions Abstract: A Poisson Binomial distribution over $n$ variables is the distribution of the\nsum of $n$ independent Bernoullis. We provide a sample near-optimal algorithm\nfor testing whether a distribution $P$ supported on $\\{0,...,n\\}$ to which we\nhave sample access is a Poisson Binomial distribution, or far from all Poisson\nBinomial distributions. The sample complexity of our algorithm is $O(n^{1/4})$\nto which we provide a matching lower bound. We note that our sample complexity\nimproves quadratically upon that of the naive \"learn followed by tolerant-test\"\napproach, while instance optimal identity testing [VV14] is not applicable\nsince we are looking to simultaneously test against a whole family of\ndistributions. \n\n"}
{"id": "1410.3889", "contents": "Title: Cheeger-type approximation for sparsest $st$-cut Abstract: We introduce the $st$-cut version the Sparsest-Cut problem, where the goal is\nto find a cut of minimum sparsity among those separating two distinguished\nvertices $s,t\\in V$. Clearly, this problem is at least as hard as the usual\n(non-$st$) version. Our main result is a polynomial-time algorithm for the\nproduct-demands setting, that produces a cut of sparsity $O(\\sqrt{\\OPT})$,\nwhere $\\OPT$ denotes the optimum, and the total edge capacity and the total\ndemand are assumed (by normalization) to be $1$.\n  Our result generalizes the recent work of Trevisan [arXiv, 2013] for the\nnon-$st$ version of the same problem (Sparsest-Cut with product demands), which\nin turn generalizes the bound achieved by the discrete Cheeger inequality, a\ncornerstone of Spectral Graph Theory that has numerous applications. Indeed,\nCheeger's inequality handles graph conductance, the special case of product\ndemands that are proportional to the vertex (capacitated) degrees. Along the\nway, we obtain an $O(\\log n)$-approximation, where $n=\\card{V}$, for the\ngeneral-demands setting of Sparsest $st$-Cut. \n\n"}
{"id": "1410.5604", "contents": "Title: Z2-double cyclic codes Abstract: A binary linear code $C$ is a $\\mathbb{Z}_2$-double cyclic code if the set of\ncoordinates can be partitioned into two subsets such that any cyclic shift of\nthe coordinates of both subsets leaves invariant the code. These codes can be\nidentified as submodules of the $\\mathbb{Z}_2[x]$-module\n$\\mathbb{Z}_2[x]/(x^r-1)\\times\\mathbb{Z}_2[x]/(x^s-1).$ We determine the\nstructure of $\\mathbb{Z}_2$-double cyclic codes giving the generator\npolynomials of these codes. The related polynomial representation of\n$\\mathbb{Z}_2$-double cyclic codes and its duals, and the relations between the\npolynomial generators of these codes are studied. \n\n"}
{"id": "1410.5778", "contents": "Title: Simple PTAS's for families of graphs excluding a minor Abstract: We show that very simple algorithms based on local search are polynomial-time\napproximation schemes for Maximum Independent Set, Minimum Vertex Cover and\nMinimum Dominating Set, when the input graphs have a fixed forbidden minor. \n\n"}
{"id": "1411.2286", "contents": "Title: On Characterizing the Data Access Complexity of Programs Abstract: Technology trends will cause data movement to account for the majority of\nenergy expenditure and execution time on emerging computers. Therefore,\ncomputational complexity will no longer be a sufficient metric for comparing\nalgorithms, and a fundamental characterization of data access complexity will\nbe increasingly important. The problem of developing lower bounds for data\naccess complexity has been modeled using the formalism of Hong & Kung's\nred/blue pebble game for computational directed acyclic graphs (CDAGs).\nHowever, previously developed approaches to lower bounds analysis for the\nred/blue pebble game are very limited in effectiveness when applied to CDAGs of\nreal programs, with computations comprised of multiple sub-computations with\ndiffering DAG structure. We address this problem by developing an approach for\neffectively composing lower bounds based on graph decomposition. We also\ndevelop a static analysis algorithm to derive the asymptotic data-access lower\nbounds of programs, as a function of the problem size and cache size. \n\n"}
{"id": "1411.3164", "contents": "Title: A linear time algorithm for the orbit problem over cyclic groups Abstract: The orbit problem is at the heart of symmetry reduction methods for model\nchecking concurrent systems. It asks whether two given configurations in a\nconcurrent system (represented as finite strings over some finite alphabet) are\nin the same orbit with respect to a given finite permutation group (represented\nby their generators) acting on this set of configurations by permuting indices.\nIt is known that the problem is in general as hard as the graph isomorphism\nproblem, whose precise complexity (whether it is solvable in polynomial-time)\nis a long-standing open problem. In this paper, we consider the restriction of\nthe orbit problem when the permutation group is cyclic (i.e. generated by a\nsingle permutation), an important restriction of the problem. It is known that\nthis subproblem is solvable in polynomial-time. Our main result is a\nlinear-time algorithm for this subproblem. \n\n"}
{"id": "1411.3517", "contents": "Title: Derandomized Graph Product Results using the Low Degree Long Code Abstract: In this paper, we address the question of whether the recent derandomization\nresults obtained by the use of the low-degree long code can be extended to\nother product settings. We consider two settings: (1) the graph product results\nof Alon, Dinur, Friedgut and Sudakov [GAFA, 2004] and (2) the \"majority is\nstablest\" type of result obtained by Dinur, Mossel and Regev [SICOMP, 2009] and\nDinur and Shinkar [In Proc. APPROX, 2010] while studying the hardness of\napproximate graph coloring.\n  In our first result, we show that there exists a considerably smaller\nsubgraph of $K_3^{\\otimes R}$ which exhibits the following property (shown for\n$K_3^{\\otimes R}$ by Alon et al.): independent sets close in size to the\nmaximum independent set are well approximated by dictators.\n  The \"majority is stablest\" type of result of Dinur et al. and Dinur and\nShinkar shows that if there exist two sets of vertices $A$ and $B$ in\n$K_3^{\\otimes R}$ with very few edges with one endpoint in $A$ and another in\n$B$, then it must be the case that the two sets $A$ and $B$ share a single\ninfluential coordinate. In our second result, we show that a similar \"majority\nis stablest\" statement holds good for a considerably smaller subgraph of\n$K_3^{\\otimes R}$. Furthermore using this result, we give a more efficient\nreduction from Unique Games to the graph coloring problem, leading to improved\nhardness of approximation results for coloring. \n\n"}
{"id": "1411.4942", "contents": "Title: Path Sampling: A Fast and Provable Method for Estimating 4-Vertex\n  Subgraph Counts Abstract: Counting the frequency of small subgraphs is a fundamental technique in\nnetwork analysis across various domains, most notably in bioinformatics and\nsocial networks. The special case of triangle counting has received much\nattention. Getting results for 4-vertex patterns is highly challenging, and\nthere are few practical results known that can scale to massive sizes. Indeed,\neven a highly tuned enumeration code takes more than a day on a graph with\nmillions of edges. Most previous work that runs for truly massive graphs employ\nclusters and massive parallelization.\n  We provide a sampling algorithm that provably and accurately approximates the\nfrequencies of all 4-vertex pattern subgraphs. Our algorithm is based on a\nnovel technique of 3-path sampling and a special pruning scheme to decrease the\nvariance in estimates. We provide theoretical proofs for the accuracy of our\nalgorithm, and give formal bounds for the error and confidence of our\nestimates. We perform a detailed empirical study and show that our algorithm\nprovides estimates within 1% relative error for all subpatterns (over a large\nclass of test graphs), while being orders of magnitude faster than enumeration\nand other sampling based algorithms. Our algorithm takes less than a minute (on\na single commodity machine) to process an Orkut social network with 300 million\nedges. \n\n"}
{"id": "1411.6667", "contents": "Title: Deletion codes in the high-noise and high-rate regimes Abstract: The noise model of deletions poses significant challenges in coding theory,\nwith basic questions like the capacity of the binary deletion channel still\nbeing open. In this paper, we study the harder model of worst-case deletions,\nwith a focus on constructing efficiently decodable codes for the two extreme\nregimes of high-noise and high-rate. Specifically, we construct polynomial-time\ndecodable codes with the following trade-offs (for any eps > 0):\n  (1) Codes that can correct a fraction 1-eps of deletions with rate poly(eps)\nover an alphabet of size poly(1/eps);\n  (2) Binary codes of rate 1-O~(sqrt(eps)) that can correct a fraction eps of\ndeletions; and\n  (3) Binary codes that can be list decoded from a fraction (1/2-eps) of\ndeletions with rate poly(eps)\n  Our work is the first to achieve the qualitative goals of correcting a\ndeletion fraction approaching 1 over bounded alphabets, and correcting a\nconstant fraction of bit deletions with rate aproaching 1. The above results\nbring our understanding of deletion code constructions in these regimes to a\nsimilar level as worst-case errors. \n\n"}
{"id": "1411.7455", "contents": "Title: Dimension Expanders via Rank Condensers Abstract: An emerging theory of \"linear-algebraic pseudorandomness\" aims to understand\nthe linear-algebraic analogs of fundamental Boolean pseudorandom objects where\nthe rank of subspaces plays the role of the size of subsets. In this work, we\nstudy and highlight the interrelationships between several such algebraic\nobjects such as subspace designs, dimension expanders, seeded rank condensers,\ntwo-source rank condensers, and rank-metric codes. In particular, with the\nrecent construction of near-optimal subspace designs by Guruswami and Kopparty\nas a starting point, we construct good (seeded) rank condensers (both lossless\nand lossy versions), which are a small collection of linear maps $\\mathbb{F}^n\n\\to \\mathbb{F}^t$ for $t \\ll n$ such that for every subset of $\\mathbb{F}^n$ of\nsmall rank, its rank is preserved (up to a constant factor in the lossy case)\nby at least one of the maps.\n  We then compose a tensoring operation with our lossy rank condenser to\nconstruct constant-degree dimension expanders over polynomially large fields.\nThat is, we give $O(1)$ explicit linear maps $A_i:\\mathbb{F}^n\\to \\mathbb{F}^n$\nsuch that for any subspace $V \\subseteq \\mathbb{F}^n$ of dimension at most\n$n/2$, $\\dim\\bigl( \\sum_i A_i(V)\\bigr) \\ge (1+\\Omega(1)) \\dim(V)$. Previous\nconstructions of such constant-degree dimension expanders were based on\nKazhdan's property $T$ (for the case when $\\mathbb{F}$ has characteristic zero)\nor monotone expanders (for every field $\\mathbb{F}$); in either case the\nconstruction was harder than that of usual vertex expanders. Our construction,\non the other hand, is simpler.\n  Via an equivalence to linear rank-metric codes, we then construct optimal\nlossless two-source condensers. We then use our seeded rank condensers to\nobtain near-optimal lossy two-source condensers for constant rank sources. \n\n"}
{"id": "1412.0325", "contents": "Title: Matchings with lower quotas: Algorithms and complexity Abstract: We study a natural generalization of the maximum weight many-to-one matching\nproblem. We are given an undirected bipartite graph $G= (A \\cup P, E)$ with\nweights on the edges in $E$, and with lower and upper quotas on the vertices in\n$P$. We seek a maximum weight many-to-one matching satisfying two sets of\nconstraints: vertices in $A$ are incident to at most one matching edge, while\nvertices in $P$ are either unmatched or they are incident to a number of\nmatching edges between their lower and upper quota. This problem, which we call\nmaximum weight many-to-one matching with lower and upper quotas (WMLQ), has\napplications to the assignment of students to projects within university\ncourses, where there are constraints on the minimum and maximum numbers of\nstudents that must be assigned to each project.\n  In this paper, we provide a comprehensive analysis of the complexity of WMLQ\nfrom the viewpoints of classic polynomial time algorithms, fixed-parameter\ntractability, as well as approximability. We draw the line between NP-hard and\npolynomially tractable instances in terms of degree and quota constraints and\nprovide efficient algorithms to solve the tractable ones. We further show that\nthe problem can be solved in polynomial time for instances with bounded\ntreewidth; however, the corresponding runtime is exponential in the treewidth\nwith the maximum upper quota $u_{max}$ as basis, and we prove that this\ndependence is necessary unless FPT = W[1]. The approximability of WMLQ is also\ndiscussed: we present an approximation algorithm for the general case with\nperformance guarantee $u_{\\max}+1$, which is asymptotically best possible\nunless P = NP. Finally, we elaborate on how most of our positive results carry\nover to matchings in arbitrary graphs with lower quotas. \n\n"}
{"id": "1412.0348", "contents": "Title: Edit Distance Cannot Be Computed in Strongly Subquadratic Time (unless\n  SETH is false) Abstract: The edit distance (a.k.a. the Levenshtein distance) between two strings is\ndefined as the minimum number of insertions, deletions or substitutions of\nsymbols needed to transform one string into another. The problem of computing\nthe edit distance between two strings is a classical computational task, with a\nwell-known algorithm based on dynamic programming. Unfortunately, all known\nalgorithms for this problem run in nearly quadratic time.\n  In this paper we provide evidence that the near-quadratic running time bounds\nknown for the problem of computing edit distance might be tight. Specifically,\nwe show that, if the edit distance can be computed in time $O(n^{2-\\delta})$\nfor some constant $\\delta>0$, then the satisfiability of conjunctive normal\nform formulas with $N$ variables and $M$ clauses can be solved in time\n$M^{O(1)} 2^{(1-\\epsilon)N}$ for a constant $\\epsilon>0$. The latter result\nwould violate the Strong Exponential Time Hypothesis, which postulates that\nsuch algorithms do not exist. \n\n"}
{"id": "1412.3334", "contents": "Title: Computational Complexity of Competitive Diffusion on (Un)weighted Graphs Abstract: Consider an undirected graph modeling a social network, where the vertices\nrepresent users, and the edges do connections among them. In the competitive\ndiffusion game, each of a number of players chooses a vertex as a seed to\npropagate his/her opinion, and then it spreads along the edges in the graphs.\nThe objective of every player is to maximize the number of vertices the opinion\ninfects. In this paper, we investigate a computational problem of asking\nwhether a pure Nash equilibrium exists in the competitive diffusion game on\nunweighed and weighted graphs, and present several negative and positive\nresults. We first prove that the problem is W[1]-hard when parameterized by the\nnumber of players even for unweighted graphs. We also show that the problem is\nNP-hard even for series-parallel graphs with positive integer weights, and is\nNP-hard even for forests with arbitrary integer weights. Furthermore, we show\nthat the problem for forest of paths with arbitrary weights is solvable in\npseudo-polynomial time; and it is solvable in quadratic time if a given graph\nis unweighted. We also prove that the problem for chain, cochain, and threshold\ngraphs with arbitrary integer weights is solvable in polynomial time. \n\n"}
{"id": "1412.5075", "contents": "Title: $k$-best enumeration Abstract: We survey $k$-best enumeration problems and the algorithms for solving them,\nincluding in particular the problems of finding the $k$ shortest paths, $k$\nsmallest spanning trees, and $k$ best matchings in weighted graphs. \n\n"}
{"id": "1412.5657", "contents": "Title: Boolean function monotonicity testing requires (almost) $n^{1/2}$\n  non-adaptive queries Abstract: We prove a lower bound of $\\Omega(n^{1/2 - c})$, for all $c>0$, on the query\ncomplexity of (two-sided error) non-adaptive algorithms for testing whether an\n$n$-variable Boolean function is monotone versus constant-far from monotone.\nThis improves a $\\tilde{\\Omega}(n^{1/5})$ lower bound for the same problem that\nwas recently given in [CST14] and is very close to $\\Omega(n^{1/2})$, which we\nconjecture is the optimal lower bound for this model. \n\n"}
{"id": "1412.6787", "contents": "Title: Instruction sequence size complexity of parity Abstract: Each Boolean function can be computed by a single-pass instruction sequence\nthat contains only instructions to set and get the content of Boolean\nregisters, forward jump instructions, and a termination instruction. Auxiliary\nBoolean registers are not necessary for this. In the current paper, we show\nthat, in the case of the parity functions, shorter instruction sequences are\npossible with the use of an auxiliary Boolean register in the presence of\ninstructions to complement the content of auxiliary Boolean registers. This\nresult supports, in a setting where programs are instruction sequences acting\non Boolean registers, a basic intuition behind the storage of auxiliary data,\nnamely the intuition that this makes possible a reduction of the size of a\nprogram. \n\n"}
{"id": "1501.01689", "contents": "Title: Sparse Solutions to Nonnegative Linear Systems and Applications Abstract: We give an efficient algorithm for finding sparse approximate solutions to\nlinear systems of equations with nonnegative coefficients. Unlike most known\nresults for sparse recovery, we do not require {\\em any} assumption on the\nmatrix other than non-negativity. Our algorithm is combinatorial in nature,\ninspired by techniques for the set cover problem, as well as the multiplicative\nweight update method.\n  We then present a natural application to learning mixture models in the PAC\nframework. For learning a mixture of $k$ axis-aligned Gaussians in $d$\ndimensions, we give an algorithm that outputs a mixture of $O(k/\\epsilon^3)$\nGaussians that is $\\epsilon$-close in statistical distance to the true\ndistribution, without any separation assumptions. The time and sample\ncomplexity is roughly $O(kd/\\epsilon^3)^{d}$. This is polynomial when $d$ is\nconstant -- precisely the regime in which known methods fail to identify the\ncomponents efficiently.\n  Given that non-negativity is a natural assumption, we believe that our result\nmay find use in other settings in which we wish to approximately explain data\nusing a small number of a (large) candidate set of components. \n\n"}
{"id": "1501.02388", "contents": "Title: Computational Performance Evaluation of Two Integer Linear Programming\n  Models for the Minimum Common String Partition Problem Abstract: In the minimum common string partition (MCSP) problem two related input\nstrings are given. \"Related\" refers to the property that both strings consist\nof the same set of letters appearing the same number of times in each of the\ntwo strings. The MCSP seeks a minimum cardinality partitioning of one string\ninto non-overlapping substrings that is also a valid partitioning for the\nsecond string. This problem has applications in bioinformatics e.g. in\nanalyzing related DNA or protein sequences. For strings with lengths less than\nabout 1000 letters, a previously published integer linear programming (ILP)\nformulation yields, when solved with a state-of-the-art solver such as CPLEX,\nsatisfactory results. In this work, we propose a new, alternative ILP model\nthat is compared to the former one. While a polyhedral study shows the linear\nprogramming relaxations of the two models to be equally strong, a comprehensive\nexperimental comparison using real-world as well as artificially created\nbenchmark instances indicates substantial computational advantages of the new\nformulation. \n\n"}
{"id": "1501.05177", "contents": "Title: Optimal Fractional Repetition Codes and Fractional Repetition Batch\n  Codes Abstract: Fractional repetition (FR) codes is a family of codes for distributed storage\nsystems (DSS) that allow uncoded exact repairs with minimum repair bandwidth.\nIn this work, we consider a bound on the maximum amount of data that can be\nstored using an FR code. Optimal FR codes which attain this bound are\npresented. The constructions of these FR codes are based on families of regular\ngraphs, such as Tur\\'an graphs and graphs with large girth; and on\ncombinatorial designs, such as transversal designs and generalized polygons. In\naddition, based on a connection between FR codes and batch codes, we propose a\nnew family of codes for DSS, called fractional repetition batch codes, which\nallow uncoded efficient exact repairs and load balancing which can be performed\nby several users in parallel. \n\n"}
{"id": "1501.06095", "contents": "Title: Between Pure and Approximate Differential Privacy Abstract: We show a new lower bound on the sample complexity of $(\\varepsilon,\n\\delta)$-differentially private algorithms that accurately answer statistical\nqueries on high-dimensional databases. The novelty of our bound is that it\ndepends optimally on the parameter $\\delta$, which loosely corresponds to the\nprobability that the algorithm fails to be private, and is the first to\nsmoothly interpolate between approximate differential privacy ($\\delta > 0$)\nand pure differential privacy ($\\delta = 0$).\n  Specifically, we consider a database $D \\in \\{\\pm1\\}^{n \\times d}$ and its\n\\emph{one-way marginals}, which are the $d$ queries of the form \"What fraction\nof individual records have the $i$-th bit set to $+1$?\" We show that in order\nto answer all of these queries to within error $\\pm \\alpha$ (on average) while\nsatisfying $(\\varepsilon, \\delta)$-differential privacy, it is necessary that\n$$ n \\geq \\Omega\\left( \\frac{\\sqrt{d \\log(1/\\delta)}}{\\alpha \\varepsilon}\n\\right), $$ which is optimal up to constant factors. To prove our lower bound,\nwe build on the connection between \\emph{fingerprinting codes} and lower bounds\nin differential privacy (Bun, Ullman, and Vadhan, STOC'14).\n  In addition to our lower bound, we give new purely and approximately\ndifferentially private algorithms for answering arbitrary statistical queries\nthat improve on the sample complexity of the standard Laplace and Gaussian\nmechanisms for achieving worst-case accuracy guarantees by a logarithmic\nfactor. \n\n"}
{"id": "1501.06946", "contents": "Title: New Bounds on Optimal Sorting Networks Abstract: We present new parallel sorting networks for $17$ to $20$ inputs. For $17,\n19,$ and $20$ inputs these new networks are faster (i.e., they require less\ncomputation steps) than the previously known best networks. Therefore, we\nimprove upon the known upper bounds for minimal depth sorting networks on $17,\n19,$ and $20$ channels. Furthermore, we show that our sorting network for $17$\ninputs is optimal in the sense that no sorting network using less layers\nexists. This solves the main open problem of [D. Bundala & J. Za\\'vodn\\'y.\nOptimal sorting networks, Proc. LATA 2014]. \n\n"}
{"id": "1501.07053", "contents": "Title: Quadratic-Time Hardness of LCS and other Sequence Similarity Measures Abstract: Two important similarity measures between sequences are the longest common\nsubsequence (LCS) and the dynamic time warping distance (DTWD). The\ncomputations of these measures for two given sequences are central tasks in a\nvariety of applications. Simple dynamic programming algorithms solve these\ntasks in $O(n^2)$ time, and despite an extensive amount of research, no\nalgorithms with significantly better worst case upper bounds are known.\n  In this paper, we show that an $O(n^{2-\\epsilon})$ time algorithm, for some\n$\\epsilon>0$, for computing the LCS or the DTWD of two sequences of length $n$\nover a constant size alphabet, refutes the popular Strong Exponential Time\nHypothesis (SETH). Moreover, we show that computing the LCS of $k$ strings over\nan alphabet of size $O(k)$ cannot be done in $O(n^{k-\\epsilon})$ time, for any\n$\\epsilon>0$, under SETH. Finally, we also address the time complexity of\napproximating the DTWD of two strings in truly subquadratic time. \n\n"}
{"id": "1501.07814", "contents": "Title: Valued Workflow Satisfiability Problem Abstract: A workflow is a collection of steps that must be executed in some specific\norder to achieve an objective. A computerised workflow management system may\nenforce authorisation policies and constraints, thereby restricting which users\ncan perform particular steps in a workflow. The existence of policies and\nconstraints may mean that a workflow is unsatisfiable, in the sense that it is\nimpossible to find an authorised user for each step in the workflow and satisfy\nall constraints. In this paper, we consider the problem of finding the \"least\nbad\" assignment of users to workflow steps by assigning a weight to each policy\nand constraint violation. To this end, we introduce a framework for associating\ncosts with the violation of workflow policies and constraints and define the\n\\emph{valued workflow satisfiability problem} (Valued WSP), whose solution is\nan assignment of steps to users of minimum cost. We establish the computational\ncomplexity of Valued WSP with user-independent constraints and show that it is\nfixed-parameter tractable. We then describe an algorithm for solving Valued WSP\nwith user-independent constraints and evaluate its performance, comparing it to\nthat of an off-the-shelf mixed integer programming package. \n\n"}
{"id": "1502.00067", "contents": "Title: Quantum interpretations of AWPP and APP Abstract: AWPP is a complexity class introduced by Fenner, Fortnow, Kurtz, and Li,\nwhich is defined using GapP functions. Although it is an important class as the\nbest upperbound of BQP, its definition seems to be somehow artificial, and\ntherefore it would be better if we have some \"physical interpretation\" of AWPP.\nHere we provide a quantum physical interpretation of AWPP: we show that AWPP is\nequal to the class of problems efficiently solved by a quantum computer with\nthe ability of postselecting an event whose probability is close to an FP\nfunction. This result is applied to also obtain a quantum physical\ninterpretation of APP. In addition, we consider \"classical physical analogue\"\nof these results, and show that a restricted version of ${\\rm BPP}_{\\rm path}$\ncontains ${\\rm UP}\\cap{\\rm coUP}$ and is contained in WAPP. \n\n"}
{"id": "1502.01063", "contents": "Title: Quadratic Conditional Lower Bounds for String Problems and Dynamic Time\n  Warping Abstract: Classic similarity measures of strings are longest common subsequence and\nLevenshtein distance (i.e., the classic edit distance). A classic similarity\nmeasure of curves is dynamic time warping. These measures can be computed by\nsimple $O(n^2)$ dynamic programming algorithms, and despite much effort no\nalgorithms with significantly better running time are known.\n  We prove that, even restricted to binary strings or one-dimensional curves,\nrespectively, these measures do not have strongly subquadratic time algorithms,\ni.e., no algorithms with running time $O(n^{2-\\varepsilon})$ for any\n$\\varepsilon > 0$, unless the Strong Exponential Time Hypothesis fails. We\ngeneralize the result to edit distance for arbitrary fixed costs of the four\noperations (deletion in one of the two strings, matching, substitution), by\nidentifying trivial cases that can be solved in constant time, and proving\nquadratic-time hardness on binary strings for all other cost choices. This\nimproves and generalizes the known hardness result for Levenshtein distance\n[Backurs, Indyk STOC'15] by the restriction to binary strings and the\ngeneralization to arbitrary costs, and adds important problems to a recent line\nof research showing conditional lower bounds for a growing number of quadratic\ntime problems.\n  As our main technical contribution, we introduce a framework for proving\nquadratic-time hardness of similarity measures. To apply the framework it\nsuffices to construct a single gadget, which encapsulates all the expressive\npower necessary to emulate a reduction from satisfiability.\n  Finally, we prove quadratic-time hardness for longest palindromic subsequence\nand longest tandem subsequence via reductions from longest common subsequence,\nshowing that conditional lower bounds based on the Strong Exponential Time\nHypothesis also apply to string problems that are not necessarily similarity\nmeasures. \n\n"}
{"id": "1502.01220", "contents": "Title: Unveiling The Tree: A Convex Framework for Sparse Problems Abstract: This paper presents a general framework for generating greedy algorithms for\nsolving convex constraint satisfaction problems for sparse solutions by mapping\nthe satisfaction problem into one of graph traversal on a rooted tree of\nunknown topology. For every pre-walk of the tree an initial set of generally\ndense feasible solutions is processed in such a way that the sparsity of each\nsolution increases with each generation unveiled. The specific computation\nperformed at any particular child node is shown to correspond to an embedding\nof a polytope into the polytope received from that nodes parent. Several issues\nrelated to pre-walk order selection, computational complexity and tractability,\nand the use of heuristic and/or side information is discussed. An example of a\nsingle-path, depth-first algorithm on a tree with randomized vertex reduction\nand a run-time path selection algorithm is presented in the context of sparse\nlowpass filter design. \n\n"}
{"id": "1502.03316", "contents": "Title: The Hardness of Approximation of Euclidean k-means Abstract: The Euclidean $k$-means problem is a classical problem that has been\nextensively studied in the theoretical computer science, machine learning and\nthe computational geometry communities. In this problem, we are given a set of\n$n$ points in Euclidean space $R^d$, and the goal is to choose $k$ centers in\n$R^d$ so that the sum of squared distances of each point to its nearest center\nis minimized. The best approximation algorithms for this problem include a\npolynomial time constant factor approximation for general $k$ and a\n$(1+\\epsilon)$-approximation which runs in time $poly(n) 2^{O(k/\\epsilon)}$. At\nthe other extreme, the only known computational complexity result for this\nproblem is NP-hardness [ADHP'09]. The main difficulty in obtaining hardness\nresults stems from the Euclidean nature of the problem, and the fact that any\npoint in $R^d$ can be a potential center. This gap in understanding left open\nthe intriguing possibility that the problem might admit a PTAS for all $k,d$.\n  In this paper we provide the first hardness of approximation for the\nEuclidean $k$-means problem. Concretely, we show that there exists a constant\n$\\epsilon > 0$ such that it is NP-hard to approximate the $k$-means objective\nto within a factor of $(1+\\epsilon)$. We show this via an efficient reduction\nfrom the vertex cover problem on triangle-free graphs: given a triangle-free\ngraph, the goal is to choose the fewest number of vertices which are incident\non all the edges. Additionally, we give a proof that the current best hardness\nresults for vertex cover can be carried over to triangle-free graphs. To show\nthis we transform $G$, a known hard vertex cover instance, by taking a graph\nproduct with a suitably chosen graph $H$, and showing that the size of the\n(normalized) maximum independent set is almost exactly preserved in the product\ngraph using a spectral analysis, which might be of independent interest. \n\n"}
{"id": "1502.03965", "contents": "Title: Uniform Kernelization Complexity of Hitting Forbidden Minors Abstract: The F-Minor-Free Deletion problem asks, for a fixed set F and an input\nconsisting of a graph G and integer k, whether k vertices can be removed from G\nsuch that the resulting graph does not contain any member of F as a minor. This\npaper analyzes to what extent provably effective and efficient preprocessing is\npossible for F-Minor-Free Deletion. Fomin et al. (FOCS 2012) showed that the\nspecial case Planar F-Deletion (when F contains at least one planar graph) has\na kernel of size f(F) * k^{g(F)} for some functions f and g. The degree g of\nthe polynomial grows very quickly; it is not even known to be computable. Fomin\net al. left open whether Planar F-Deletion has kernels whose size is uniformly\npolynomial, i.e., of the form f(F) * k^c for some universal constant c that\ndoes not depend on F. Our results in this paper are twofold. (1) We prove that\nsome Planar F-Deletion problems do not have uniformly polynomial kernels\n(unless NP is in coNP/poly). In particular, we prove that Treewidth-Eta\nDeletion does not have a kernel with O(k^{eta/4} - eps) vertices for any eps >\n0, unless NP is in coNP/poly. In fact, we even prove the kernelization lower\nbound for the larger parameter vertex cover number. This resolves an open\nproblem of Cygan et al. (IPEC 2011). It is a natural question whether further\nrestrictions on F lead to uniformly polynomial kernels. However, we prove that\neven when F contains a path, the degree of the polynomial must, in general,\ndepend on the set F. (2) A canonical F-Minor-Free Deletion problem when F\ncontains a path is Treedepth-eta Deletion: can k vertices be removed to obtain\na graph of treedepth at most eta? We prove that Treedepth-eta Deletion admits\nuniformly polynomial kernels with O(k^6) vertices for every fixed eta. In order\nto develop the kernelization we prove several new results about the structure\nof optimal treedepth-decompositions. \n\n"}
{"id": "1502.04545", "contents": "Title: Parallel Identity Testing for Skew Circuits with Big Powers and\n  Applications Abstract: Powerful skew arithmetic circuits are introduced. These are skew arithmetic\ncircuits with variables, where input gates can be labelled with powers $x^n$\nfor binary encoded numbers $n$. It is shown that polynomial identity testing\nfor powerful skew arithmetic circuits belongs to $\\mathsf{coRNC}^2$, which\ngeneralizes a corresponding result for (standard) skew circuits. Two\napplications of this result are presented: (i) Equivalence of\nhigher-dimensional straight-line programs can be tested in $\\mathsf{coRNC}^2$;\nthis result is even new in the one-dimensional case, where the straight-line\nprograms produce strings. (ii) The compressed word problem (or circuit\nevaluation problem) for certain wreath products of finitely generated abelian\ngroups belongs to $\\mathsf{coRNC}^2$. \n\n"}
{"id": "1502.05023", "contents": "Title: A New Sampling Technique for Tensors Abstract: In this paper we propose new techniques to sample arbitrary third-order\ntensors, with an objective of speeding up tensor algorithms that have recently\ngained popularity in machine learning. Our main contribution is a new way to\nselect, in a biased random way, only $O(n^{1.5}/\\epsilon^2)$ of the possible\n$n^3$ elements while still achieving each of the three goals: \\\\ {\\em (a)\ntensor sparsification}: for a tensor that has to be formed from arbitrary\nsamples, compute very few elements to get a good spectral approximation, and\nfor arbitrary orthogonal tensors {\\em (b) tensor completion:} recover an\nexactly low-rank tensor from a small number of samples via alternating least\nsquares, or {\\em (c) tensor factorization:} approximating factors of a low-rank\ntensor corrupted by noise. \\\\ Our sampling can be used along with existing\ntensor-based algorithms to speed them up, removing the computational bottleneck\nin these methods. \n\n"}
{"id": "1502.05204", "contents": "Title: Clustered Integer 3SUM via Additive Combinatorics Abstract: We present a collection of new results on problems related to 3SUM,\nincluding:\n  1. The first truly subquadratic algorithm for\n  $\\ \\ \\ \\ \\ $ 1a. computing the (min,+) convolution for monotone increasing\nsequences with integer values bounded by $O(n)$,\n  $\\ \\ \\ \\ \\ $1b. solving 3SUM for monotone sets in 2D with integer coordinates\nbounded by $O(n)$, and\n  $\\ \\ \\ \\ \\ $1c. preprocessing a binary string for histogram indexing (also\ncalled jumbled indexing).\n  The running time is:\n$O(n^{(9+\\sqrt{177})/12}\\,\\textrm{polylog}\\,n)=O(n^{1.859})$ with\nrandomization, or $O(n^{1.864})$ deterministically. This greatly improves the\nprevious $n^2/2^{\\Omega(\\sqrt{\\log n})}$ time bound obtained from Williams'\nrecent result on all-pairs shortest paths [STOC'14], and answers an open\nquestion raised by several researchers studying the histogram indexing problem.\n  2. The first algorithm for histogram indexing for any constant alphabet size\nthat achieves truly subquadratic preprocessing time and truly sublinear query\ntime.\n  3. A truly subquadratic algorithm for integer 3SUM in the case when the given\nset can be partitioned into $n^{1-\\delta}$ clusters each covered by an interval\nof length $n$, for any constant $\\delta>0$.\n  4. An algorithm to preprocess any set of $n$ integers so that subsequently\n3SUM on any given subset can be solved in $O(n^{13/7}\\,\\textrm{polylog}\\,n)$\ntime.\n  All these results are obtained by a surprising new technique, based on the\nBalog--Szemer\\'edi--Gowers Theorem from additive combinatorics. \n\n"}
{"id": "1502.05675", "contents": "Title: NP-Hardness and Inapproximability of Sparse PCA Abstract: We give a reduction from {\\sc clique} to establish that sparse PCA is\nNP-hard. The reduction has a gap which we use to exclude an FPTAS for sparse\nPCA (unless P=NP). Under weaker complexity assumptions, we also exclude\npolynomial constant-factor approximation algorithms. \n\n"}
{"id": "1502.06764", "contents": "Title: A deterministic sublinear-time nonadaptive algorithm for metric\n  $1$-median selection Abstract: We give a deterministic $O(hn^{1+1/h})$-time $(2h)$-approximation nonadaptive\nalgorithm for $1$-median selection in $n$-point metric spaces, where\n$h\\in\\mathbb{Z}^+\\setminus\\{1\\}$ is arbitrary. Our proof generalizes that of\nChang. \n\n"}
{"id": "1503.00260", "contents": "Title: Parameter Compilation Abstract: In resolving instances of a computational problem, if multiple instances of\ninterest share a feature in common, it may be fruitful to compile this feature\ninto a format that allows for more efficient resolution, even if the\ncompilation is relatively expensive. In this article, we introduce a formal\nframework for classifying problems according to their compilability. The basic\nobject in our framework is that of a parameterized problem, which here is a\nlanguage along with a parameterization---a map which provides, for each\ninstance, a so-called parameter on which compilation may be performed. Our\nframework is positioned within the paradigm of parameterized complexity, and\nour notions are relatable to established concepts in the theory of\nparameterized complexity. Indeed, we view our framework as playing a unifying\nrole, integrating together parameterized complexity and compilability theory. \n\n"}
{"id": "1503.04099", "contents": "Title: Algorithms and complexity for Turaev-Viro invariants Abstract: The Turaev-Viro invariants are a powerful family of topological invariants\nfor distinguishing between different 3-manifolds. They are invaluable for\nmathematical software, but current algorithms to compute them require\nexponential time.\n  The invariants are parameterised by an integer $r \\geq 3$. We resolve the\nquestion of complexity for $r=3$ and $r=4$, giving simple proofs that computing\nTuraev-Viro invariants for $r=3$ is polynomial time, but for $r=4$ is \\#P-hard.\nMoreover, we give an explicit fixed-parameter tractable algorithm for arbitrary\n$r$, and show through concrete implementation and experimentation that this\nalgorithm is practical---and indeed preferable---to the prior state of the art\nfor real computation. \n\n"}
{"id": "1503.05879", "contents": "Title: Regular realizability problems and regular languages Abstract: We investigate regular realizability (RR) problems, which are the problems of\nverifying whether intersection of a regular language -- the input of the\nproblem -- and fixed language called filter is non-empty. We consider two kind\nof problems depending on representation of regular language. If a regular\nlanguage on input is represented by a DFA, then we obtain (deterministic)\nregular realizability problem and we show that in this case the complexity of\nregular realizability problem for an arbitrary regular filter is either\nL-complete or NL-complete. We also show that in case of representation regular\nlanguage on input by NFA the problem is always NL-complete. \n\n"}
{"id": "1503.05977", "contents": "Title: Dynamic Data Structures for Document Collections and Graphs Abstract: In the dynamic indexing problem, we must maintain a changing collection of\ntext documents so that we can efficiently support insertions, deletions, and\npattern matching queries. We are especially interested in developing efficient\ndata structures that store and query the documents in compressed form. All\nprevious compressed solutions to this problem rely on answering rank and select\nqueries on a dynamic sequence of symbols. Because of the lower bound in\n[Fredman and Saks, 1989], answering rank queries presents a bottleneck in\ncompressed dynamic indexing. In this paper we show how this lower bound can be\ncircumvented using our new framework. We demonstrate that the gap between\nstatic and dynamic variants of the indexing problem can be almost closed. Our\nmethod is based on a novel framework for adding dynamism to static compressed\ndata structures. Our framework also applies more generally to dynamizing other\nproblems. We show, for example, how our framework can be applied to develop\ncompressed representations of dynamic graphs and binary relations. \n\n"}
{"id": "1503.06394", "contents": "Title: Large-scale Log-determinant Computation through Stochastic Chebyshev\n  Expansions Abstract: Logarithms of determinants of large positive definite matrices appear\nubiquitously in machine learning applications including Gaussian graphical and\nGaussian process models, partition functions of discrete graphical models,\nminimum-volume ellipsoids, metric learning and kernel learning. Log-determinant\ncomputation involves the Cholesky decomposition at the cost cubic in the number\nof variables, i.e., the matrix dimension, which makes it prohibitive for\nlarge-scale applications. We propose a linear-time randomized algorithm to\napproximate log-determinants for very large-scale positive definite and general\nnon-singular matrices using a stochastic trace approximation, called the\nHutchinson method, coupled with Chebyshev polynomial expansions that both rely\non efficient matrix-vector multiplications. We establish rigorous additive and\nmultiplicative approximation error bounds depending on the condition number of\nthe input matrix. In our experiments, the proposed algorithm can provide very\nhigh accuracy solutions at orders of magnitude faster time than the Cholesky\ndecomposition and Schur completion, and enables us to compute log-determinants\nof matrices involving tens of millions of variables. \n\n"}
{"id": "1503.07463", "contents": "Title: Computing the partition function of a polynomial on the Boolean cube Abstract: For a polynomial f: {-1, 1}^n --> C, we define the partition function as the\naverage of e^{lambda f(x)} over all points x in {-1, 1}^n, where lambda in C is\na parameter. We present a quasi-polynomial algorithm, which, given such f,\nlambda and epsilon >0 approximates the partition function within a relative\nerror of epsilon in N^{O(ln n -ln epsilon)} time provided |lambda| < 1/(2 L\nsqrt{deg f}), where L=L(f) is a parameter bounding the Lipschitz constant of f\nfrom above and N is the number of monomials in f. As a corollary, we obtain a\nquasi-polynomial algorithm, which, given such an f with coefficients +1 and -1\nand such that every variable enters not more than 4 monomials, approximates the\nmaximum of f on {-1, 1}^n within a factor of O(sqrt{deg f}/delta), provided the\nmaximum is N delta for some 0< delta <1. If every variable enters not more than\nk monomials for some fixed k > 4, we are able to establish a similar result\nwhen delta > (k-1)/k. \n\n"}
{"id": "1504.00545", "contents": "Title: A Bulk-Parallel Priority Queue in External Memory with STXXL Abstract: We propose the design and an implementation of a bulk-parallel external\nmemory priority queue to take advantage of both shared-memory parallelism and\nhigh external memory transfer speeds to parallel disks. To achieve higher\nperformance by decoupling item insertions and extractions, we offer two\nparallelization interfaces: one using \"bulk\" sequences, the other by defining\n\"limit\" items. In the design, we discuss how to parallelize insertions using\nmultiple heaps, and how to calculate a dynamic prediction sequence to prefetch\nblocks and apply parallel multiway merge for extraction. Our experimental\nresults show that in the selected benchmarks the priority queue reaches 75% of\nthe full parallel I/O bandwidth of rotational disks and and 65% of SSDs, or the\nspeed of sorting in external memory when bounded by computation. \n\n"}
{"id": "1504.01033", "contents": "Title: Watch and Learn: Optimizing from Revealed Preferences Feedback Abstract: A Stackelberg game is played between a leader and a follower. The leader\nfirst chooses an action, then the follower plays his best response. The goal of\nthe leader is to pick the action that will maximize his payoff given the\nfollower's best response. In this paper we present an approach to solving for\nthe leader's optimal strategy in certain Stackelberg games where the follower's\nutility function (and thus the subsequent best response of the follower) is\nunknown.\n  Stackelberg games capture, for example, the following interaction between a\nproducer and a consumer. The producer chooses the prices of the goods he\nproduces, and then a consumer chooses to buy a utility maximizing bundle of\ngoods. The goal of the seller here is to set prices to maximize his\nprofit---his revenue, minus the production cost of the purchased bundle. It is\nquite natural that the seller in this example should not know the buyer's\nutility function. However, he does have access to revealed preference\nfeedback---he can set prices, and then observe the purchased bundle and his own\nprofit. We give algorithms for efficiently solving, in terms of both\ncomputational and query complexity, a broad class of Stackelberg games in which\nthe follower's utility function is unknown, using only \"revealed preference\"\naccess to it. This class includes in particular the profit maximization\nproblem, as well as the optimal tolling problem in nonatomic congestion games,\nwhen the latency functions are unknown. Surprisingly, we are able to solve\nthese problems even though the optimization problems are non-convex in the\nleader's actions. \n\n"}
{"id": "1504.01431", "contents": "Title: If the Current Clique Algorithms are Optimal, so is Valiant's Parser Abstract: The CFG recognition problem is: given a context-free grammar $\\mathcal{G}$\nand a string $w$ of length $n$, decide if $w$ can be obtained from\n$\\mathcal{G}$. This is the most basic parsing question and is a core computer\nscience problem. Valiant's parser from 1975 solves the problem in\n$O(n^{\\omega})$ time, where $\\omega<2.373$ is the matrix multiplication\nexponent. Dozens of parsing algorithms have been proposed over the years, yet\nValiant's upper bound remains unbeaten. The best combinatorial algorithms have\nmildly subcubic $O(n^3/\\log^3{n})$ complexity.\n  Lee (JACM'01) provided evidence that fast matrix multiplication is needed for\nCFG parsing, and that very efficient and practical algorithms might be hard or\neven impossible to obtain. Lee showed that any algorithm for a more general\nparsing problem with running time $O(|\\mathcal{G}|\\cdot n^{3-\\varepsilon})$ can\nbe converted into a surprising subcubic algorithm for Boolean Matrix\nMultiplication. Unfortunately, Lee's hardness result required that the grammar\nsize be $|\\mathcal{G}|=\\Omega(n^6)$. Nothing was known for the more relevant\ncase of constant size grammars.\n  In this work, we prove that any improvement on Valiant's algorithm, even for\nconstant size grammars, either in terms of runtime or by avoiding the\ninefficiencies of fast matrix multiplication, would imply a breakthrough\nalgorithm for the $k$-Clique problem: given a graph on $n$ nodes, decide if\nthere are $k$ that form a clique.\n  Besides classifying the complexity of a fundamental problem, our reduction\nhas led us to similar lower bounds for more modern and well-studied cubic time\nproblems for which faster algorithms are highly desirable in practice: RNA\nFolding, a central problem in computational biology, and Dyck Language Edit\nDistance, answering an open question of Saha (FOCS'14). \n\n"}
{"id": "1504.02044", "contents": "Title: An Algorithmic Proof of the Lovasz Local Lemma via Resampling Oracles Abstract: The Lovasz Local Lemma is a seminal result in probabilistic combinatorics. It\ngives a sufficient condition on a probability space and a collection of events\nfor the existence of an outcome that simultaneously avoids all of those events.\nFinding such an outcome by an efficient algorithm has been an active research\ntopic for decades. Breakthrough work of Moser and Tardos (2009) presented an\nefficient algorithm for a general setting primarily characterized by a product\nstructure on the probability space.\n  In this work we present an efficient algorithm for a much more general\nsetting. Our main assumption is that there exist certain functions, called\nresampling oracles, that can be invoked to address the undesired occurrence of\nthe events. We show that, in all scenarios to which the original Lovasz Local\nLemma applies, there exist resampling oracles, although they are not\nnecessarily efficient. Nevertheless, for essentially all known applications of\nthe Lovasz Local Lemma and its generalizations, we have designed efficient\nresampling oracles. As applications of these techniques, we present new results\nfor packings of Latin transversals, rainbow matchings and rainbow spanning\ntrees. \n\n"}
{"id": "1504.02411", "contents": "Title: Can Almost Everybody be Almost Happy? PCP for PPAD and the\n  Inapproximability of Nash Abstract: We conjecture that PPAD has a PCP-like complete problem, seeking a near\nequilibrium in which all but very few players have very little incentive to\ndeviate. We show that, if one assumes that this problem requires exponential\ntime, several open problems in this area are settled. The most important\nimplication, proved via a \"birthday repetition\" reduction, is that the n^O(log\nn) approximation scheme of [LMM03] for the Nash equilibrium of two-player games\nis essentially optimum. Two other open problems in the area are resolved once\none assumes this conjecture, establishing that certain approximate equilibria\nare PPAD-complete: Finding a relative approximation of two-player Nash\nequilibria (without the well-supported restriction of [Das13]), and an\napproximate competitive equilibrium with equal incomes [Bud11] with small\nclearing error and near-optimal Gini coefficient. \n\n"}
{"id": "1504.03856", "contents": "Title: Sparse multivariate polynomial interpolation in the basis of Schubert\n  polynomials Abstract: Schubert polynomials were discovered by A. Lascoux and M. Sch\\\"utzenberger in\nthe study of cohomology rings of flag manifolds in 1980's. These polynomials\ngeneralize Schur polynomials, and form a linear basis of multivariate\npolynomials. In 2003, Lenart and Sottile introduced skew Schubert polynomials,\nwhich generalize skew Schur polynomials, and expand in the Schubert basis with\nthe generalized Littlewood-Richardson coefficients.\n  In this paper we initiate the study of these two families of polynomials from\nthe perspective of computational complexity theory. We first observe that skew\nSchubert polynomials, and therefore Schubert polynomials, are in $\\CountP$\n(when evaluating on non-negative integral inputs) and $\\VNP$.\n  Our main result is a deterministic algorithm that computes the expansion of a\npolynomial $f$ of degree $d$ in $\\Z[x_1, \\dots, x_n]$ in the basis of Schubert\npolynomials, assuming an oracle computing Schubert polynomials. This algorithm\nruns in time polynomial in $n$, $d$, and the bit size of the expansion. This\ngeneralizes, and derandomizes, the sparse interpolation algorithm of symmetric\npolynomials in the Schur basis by Barvinok and Fomin (Advances in Applied\nMathematics, 18(3):271--285). In fact, our interpolation algorithm is general\nenough to accommodate any linear basis satisfying certain natural properties.\n  Applications of the above results include a new algorithm that computes the\ngeneralized Littlewood-Richardson coefficients. \n\n"}
{"id": "1504.03987", "contents": "Title: Random Laplacian matrices and convex relaxations Abstract: The largest eigenvalue of a matrix is always larger or equal than its largest\ndiagonal entry. We show that for a large class of random Laplacian matrices,\nthis bound is essentially tight: the largest eigenvalue is, up to lower order\nterms, often the size of the largest diagonal entry.\n  Besides being a simple tool to obtain precise estimates on the largest\neigenvalue of a large class of random Laplacian matrices, our main result\nsettles a number of open problems related to the tightness of certain convex\nrelaxation-based algorithms. It easily implies the optimality of the\nsemidefinite relaxation approaches to problems such as $\\mathbb{Z}_2$\nSynchronization and Stochastic Block Model recovery. Interestingly, this result\nreadily implies the connectivity threshold for Erd\\H{o}s-R\\'{e}nyi graphs and\nsuggests that these three phenomena are manifestations of the same underlying\nprinciple. The main tool is a recent estimate on the spectral norm of matrices\nwith independent entries by van Handel and the author. \n\n"}
{"id": "1504.05535", "contents": "Title: Tree compression using string grammars Abstract: We study the compressed representation of a ranked tree by a (string)\nstraight-line program (SLP) for its preorder traversal, and compare it with the\nwell-studied representation by straight-line context free tree grammars (which\nare also known as tree straight-line programs or TSLPs). Although SLPs turn out\nto be exponentially more succinct than TSLPs, we show that many simple tree\nqueries can still be performed efficiently on SLPs, such as computing the\nheight and Horton-Strahler number of a tree, tree navigation, or evaluation of\nBoolean expressions. Other problems on tree traversals turn out to be\nintractable, e.g. pattern matching and evaluation of tree automata. \n\n"}
{"id": "1504.06501", "contents": "Title: Run Generation Revisited: What Goes Up May or May Not Come Down Abstract: In this paper, we revisit the classic problem of run generation. Run\ngeneration is the first phase of external-memory sorting, where the objective\nis to scan through the data, reorder elements using a small buffer of size M ,\nand output runs (contiguously sorted chunks of elements) that are as long as\npossible.\n  We develop algorithms for minimizing the total number of runs (or\nequivalently, maximizing the average run length) when the runs are allowed to\nbe sorted or reverse sorted. We study the problem in the online setting, both\nwith and without resource augmentation, and in the offline setting.\n  (1) We analyze alternating-up-down replacement selection (runs alternate\nbetween sorted and reverse sorted), which was studied by Knuth as far back as\n1963. We show that this simple policy is asymptotically optimal. Specifically,\nwe show that alternating-up-down replacement selection is 2-competitive and no\ndeterministic online algorithm can perform better.\n  (2) We give online algorithms having smaller competitive ratios with resource\naugmentation. Specifically, we exhibit a deterministic algorithm that, when\ngiven a buffer of size 4M , is able to match or beat any optimal algorithm\nhaving a buffer of size M . Furthermore, we present a randomized online\nalgorithm which is 7/4-competitive when given a buffer twice that of the\noptimal.\n  (3) We demonstrate that performance can also be improved with a small amount\nof foresight. We give an algorithm, which is 3/2-competitive, with\nforeknowledge of the next 3M elements of the input stream. For the extreme case\nwhere all future elements are known, we design a PTAS for computing the optimal\nstrategy a run generation algorithm must follow.\n  (4) Finally, we present algorithms tailored for nearly sorted inputs which\nare guaranteed to have optimal solutions with sufficiently long runs. \n\n"}
{"id": "1504.06712", "contents": "Title: Faster Lightweight Lempel-Ziv Parsing Abstract: We present an algorithm that computes the Lempel-Ziv decomposition in\n$O(n(\\log\\sigma + \\log\\log n))$ time and $n\\log\\sigma + \\epsilon n$ bits of\nspace, where $\\epsilon$ is a constant rational parameter, $n$ is the length of\nthe input string, and $\\sigma$ is the alphabet size. The $n\\log\\sigma$ bits in\nthe space bound are for the input string itself which is treated as read-only. \n\n"}
{"id": "1504.07149", "contents": "Title: Trip-Based Public Transit Routing Abstract: We study the problem of computing all Pareto-optimal journeys in a public\ntransit network regarding the two criteria of arrival time and number of\ntransfers taken. We take a novel approach, focusing on trips and transfers\nbetween them, allowing fine-grained modeling. Our experiments on the\nmetropolitan network of London show that the algorithm computes full 24-hour\nprofiles in 70 ms after a preprocessing phase of 30 s, allowing fast queries in\ndynamic scenarios. \n\n"}
{"id": "1504.07830", "contents": "Title: On k-Submodular Relaxation Abstract: $k$-submodular functions, introduced by Huber and Kolmogorov, are functions\ndefined on $\\{0, 1, 2, \\dots, k\\}^n$ satisfying certain submodular-type\ninequalities. $k$-submodular functions typically arise as relaxations of\nNP-hard problems, and the relaxations by $k$-submodular functions play key\nroles in design of efficient, approximation, or fixed-parameter tractable\nalgorithms. Motivated by this, we consider the following problem: Given a\nfunction $f : \\{1, 2, \\dots, k\\}^n \\rightarrow \\mathbb{R} \\cup \\{\\infty\\}$,\ndetermine whether $f$ is extended to a $k$-submodular function $g : \\{0, 1, 2,\n\\dots, k\\}^n \\rightarrow \\mathbb{R} \\cup \\{\\infty\\}$, where $g$ is called a\n$k$-submodular relaxation of $f$.\n  We give a polymorphic characterization of those functions which admit a\n$k$-submodular relaxation, and also give a combinatorial $O((k^n)^2)$-time\nalgorithm to find a $k$-submodular relaxation or establish that a\n$k$-submodular relaxation does not exist. Our algorithm has interesting\nproperties: (1) If the input function is integer valued, then our algorithm\noutputs a half-integral relaxation, and (2) if the input function is binary,\nthen our algorithm outputs the unique optimal relaxation. We present\napplications of our algorithm to valued constraint satisfaction problems. \n\n"}
{"id": "1505.00058", "contents": "Title: Transforming NP to P: An Approach to Solve NP Complete Problems Abstract: NP complete problem is one of the most challenging issues. The question of\nwhether all problems in NP are also in P is generally considered one of the\nmost important open questions in mathematics and theoretical computer science\nas it has far-reaching consequences to other problems in mathematics, computer\nscience, biology, philosophy and cryptography. There are intensive research on\nproving `NP not equal to P' and `NP equals to P'. However, none of the `proved'\nresults is commonly accepted by the research community up to now. In this\npaper, instead of proving either one, we aim to provide new perspective:\ntransforming two typical NP complete problems to exactly solvable P problems in\npolynomial time. This approach helps to solve originally NP complete problems\nwith practical applications. It may shine light on solving other NP complete\nproblems in similar way. \n\n"}
{"id": "1505.00184", "contents": "Title: Instance Optimal Geometric Algorithms Abstract: We prove the existence of an algorithm $A$ for computing 2-d or 3-d convex\nhulls that is optimal for every point set in the following sense: for every\nsequence $\\sigma$ of $n$ points and for every algorithm $A'$ in a certain class\n$\\mathcal{A}$, the running time of $A$ on input $\\sigma$ is at most a constant\nfactor times the maximum running time of $A'$ on the worst possible permutation\nof $\\sigma$ for $A'$. We establish a stronger property: for every sequence\n$\\sigma$ of points and every algorithm $A'$, the running time of $A$ on\n$\\sigma$ is at most a constant factor times the average running time of $A'$\nover all permutations of $\\sigma$. We call algorithms satisfying these\nproperties instance-optimal in the order-oblivious and random-order setting.\nSuch instance-optimal algorithms simultaneously subsume output-sensitive\nalgorithms and distribution-dependent average-case algorithms, and all\nalgorithms that do not take advantage of the order of the input or that assume\nthe input is given in a random order. The class $\\mathcal{A}$ under\nconsideration consists of all algorithms in a decision tree model where the\ntests involve only multilinear functions with a constant number of arguments.\nTo establish an instance-specific lower bound, we deviate from traditional\nBen-Or-style proofs and adopt a new adversary argument. For 2-d convex hulls,\nwe prove that a version of the well known algorithm by Kirkpatrick and Seidel\n(1986) or Chan, Snoeyink, and Yap (1995) already attains this lower bound. For\n3-d convex hulls, we propose a new algorithm. We further obtain\ninstance-optimal results for a few other standard problems in computational\ngeometry. Our framework also reveals connection to distribution-sensitive data\nstructures and yields new results as a byproduct, for example, on on-line\northogonal range searching in 2-d and on-line halfspace range reporting in 2-d\nand 3-d. \n\n"}
{"id": "1505.00244", "contents": "Title: An Improved Private Mechanism for Small Databases Abstract: We study the problem of answering a workload of linear queries $\\mathcal{Q}$,\non a database of size at most $n = o(|\\mathcal{Q}|)$ drawn from a universe\n$\\mathcal{U}$ under the constraint of (approximate) differential privacy.\nNikolov, Talwar, and Zhang~\\cite{NTZ} proposed an efficient mechanism that, for\nany given $\\mathcal{Q}$ and $n$, answers the queries with average error that is\nat most a factor polynomial in $\\log |\\mathcal{Q}|$ and $\\log |\\mathcal{U}|$\nworse than the best possible. Here we improve on this guarantee and give a\nmechanism whose competitiveness ratio is at most polynomial in $\\log n$ and\n$\\log |\\mathcal{U}|$, and has no dependence on $|\\mathcal{Q}|$. Our mechanism\nis based on the projection mechanism of Nikolov, Talwar, and Zhang, but in\nplace of an ad-hoc noise distribution, we use a distribution which is in a\nsense optimal for the projection mechanism, and analyze it using convex duality\nand the restricted invertibility principle. \n\n"}
{"id": "1505.01467", "contents": "Title: Tight Bounds for Linear Sketches of Approximate Matchings Abstract: We resolve the space complexity of linear sketches for approximating the\nmaximum matching problem in dynamic graph streams where the stream may include\nboth edge insertion and deletion. Specifically, we show that for any $\\epsilon\n> 0$, there exists a one-pass streaming algorithm, which only maintains a\nlinear sketch of size $\\tilde{O}(n^{2-3\\epsilon})$ bits and recovers an\n$n^\\epsilon$-approximate maximum matching in dynamic graph streams, where $n$\nis the number of vertices in the graph. In contrast to the extensively studied\ninsertion-only model, to the best of our knowledge, no non-trivial single-pass\nstreaming algorithms were previously known for approximating the maximum\nmatching problem on general dynamic graph streams.\n  Furthermore, we show that our upper bound is essentially tight. Namely, any\nlinear sketch for approximating the maximum matching to within a factor of\n$O(n^\\epsilon)$ has to be of size $n^{2-3\\epsilon -o(1)}$ bits. We establish\nthis lower bound by analyzing the corresponding simultaneous number-in-hand\ncommunication model, with a combinatorial construction based on\nRuzsa-Szemer\\'{e}di graphs. \n\n"}
{"id": "1505.02372", "contents": "Title: On Asymptotic Gate Complexity and Depth of Reversible Circuits With\n  Additional Memory Abstract: The reversible logic can be used in various research areas, e.g. quantum\ncomputation, cryptography and signal processing. In the paper we study\nreversible logic circuits with additional inputs, which consist of NOT, CNOT\nand C\\textsuperscript{2}NOT gates. We consider a set $F(n,q)$ of all\ntransformations $\\mathbb B^n \\to \\mathbb B^n$ that can be realized by\nreversible circuits with $(n+q)$ inputs. An analogue of Lupanov's method for\nthe synthesis of reversible logic circuits with additional inputs is described.\nWe prove upper asymptotic bounds for the Shannon gate complexity function\n$L(n,q)$ and the depth function $D(n,q)$ in case of $q > 0$: $L(n,q_0) \\lesssim\n2^n$ if $q_0 \\sim n 2^{n-o(n)}$ and $D(n,q_1) \\lesssim 3n$ if $q_1 \\sim 2^n$. \n\n"}
{"id": "1505.02829", "contents": "Title: Polynomial enumeration of chordless cycles on cyclically orientable\n  graphs Abstract: In a finite undirected simple graph, a chordless cycle is an induced subgraph\nwhich is a cycle. A graph is called cyclically orientable if it admits an\norientation in which every chordless cycle is cyclically oriented. We propose\nan algorithm to enumerate all chordless cycles of such a graph. Compared to\nother similar algorithms, the proposed algorithm have the advantage of finding\neach chordless cycle only once in time complexity $\\mathcal{O}(n^2)$ in the\ninput size, where $n$ is the number of vertices. \n\n"}
{"id": "1505.02993", "contents": "Title: A Holant Dichotomy: Is the FKT Algorithm Universal? Abstract: We prove a complexity dichotomy for complex-weighted Holant problems with an\narbitrary set of symmetric constraint functions on Boolean variables. This\ndichotomy is specifically to answer the question: Is the FKT algorithm under a\nholographic transformation a \\emph{universal} strategy to obtain\npolynomial-time algorithms for problems over planar graphs that are intractable\nin general? This dichotomy is a culmination of previous ones, including those\nfor Spin Systems, Holant, and #CSP. A recurring theme has been that a\nholographic reduction to FKT is a universal strategy. Surprisingly, for planar\nHolant, we discover new planar tractable problems that are not expressible by a\nholographic reduction to FKT.\n  In previous work, an important tool was a dichotomy for #CSP^d, which denotes\n#CSP where every variable appears a multiple of d times. However its proof\nviolates planarity. We prove a dichotomy for planar #CSP^2. We apply this\nplanar #CSP^2 dichotomy in the proof of the planar Holant dichotomy.\n  As a special case of our new planar tractable problems, counting perfect\nmatchings (#PM) over k-uniform hypergraphs is polynomial-time computable when\nthe incidence graph is planar and k >= 5. The same problem is #P-hard when k=3\nor k=4, which is also a consequence of our dichotomy. When k=2, it becomes #PM\nover planar graphs and is tractable again. More generally, over hypergraphs\nwith specified hyperedge sizes and the same planarity assumption, #PM is\npolynomial-time computable if the greatest common divisor of all hyperedge\nsizes is at least 5. \n\n"}
{"id": "1505.04229", "contents": "Title: Container Relocation Problem: Approximation, Asymptotic, and Incomplete\n  Information Abstract: The Container Relocation Problem (CRP) is concerned with finding a sequence\nof moves of containers that minimizes the number of relocations needed to\nretrieve all containers respecting a given order of retrieval. While the\nproblem is known to be NP-hard, certain algorithms such as the A* search and\nheuristics perform reasonably well on many instances of the problem. In this\npaper, we first focus on the A* search algorithm, and analyze lower and upper\nbounds that are easy to compute and can be used to prune nodes. Our analysis\nsheds light on which bounds result in fast computation within a given\napproximation gap. We present extensive simulation results that improve upon\nour theoretical analysis, and further show that our method finds the optimum\nsolution on most instances of medium-size bays. On \"hard\" instances, our method\nfinds an approximate solution with a small gap and within a time frame that is\nfast for practical applications. We also study the average-case asymptotic\nbehavior of the CRP where the number of columns grows. We calculate the\nexpected number of relocations in the limit, and show that the optimum number\nof relocations converges to a simple and intuitive lower-bound. We further\nstudy the CRP with incomplete information by relaxing the assumption that the\norder of retrieval of all containers are initially known. This assumption is\nparticularly unrealistic in ports without an appointment system. We assume that\nthe retrieval order of a subset of containers is known initially and the\nretrieval order of the remaining containers is observed later at a given\nspecific time. Before this time, we assume a probabilistic distribution on the\nretrieval order of unknown containers. We combine the A* algorithm with\nsampling technique to solve this two-stage stochastic optimization problem. We\nshow that our algorithm is fast and the error due to sampling and pruning is\nreasonably small. \n\n"}
{"id": "1505.04383", "contents": "Title: How to refute a random CSP Abstract: Let $P$ be a $k$-ary predicate over a finite alphabet. Consider a random\nCSP$(P)$ instance $I$ over $n$ variables with $m$ constraints. When $m \\gg n$\nthe instance $I$ will be unsatisfiable with high probability, and we want to\nfind a refutation - i.e., a certificate of unsatisfiability. When $P$ is the\n$3$-ary OR predicate, this is the well studied problem of refuting random\n$3$-SAT formulas, and an efficient algorithm is known only when $m \\gg\nn^{3/2}$. Understanding the density required for refutation of other predicates\nis important in cryptography, proof complexity, and learning theory.\nPreviously, it was known that for a $k$-ary predicate, having $m \\gg n^{\\lceil\nk/2 \\rceil}$ constraints suffices for refutation. We give a criterion for\npredicates that often yields efficient refutation algorithms at much lower\ndensities. Specifically, if $P$ fails to support a $t$-wise uniform\ndistribution, then there is an efficient algorithm that refutes random CSP$(P)$\ninstances $I$ whp when $m \\gg n^{t/2}$. Indeed, our algorithm will \"somewhat\nstrongly\" refute $I$, certifying $\\mathrm{Opt}(I) \\leq 1-\\Omega_k(1)$, if $t =\nk$ then we get the strongest possible refutation, certifying $\\mathrm{Opt}(I)\n\\leq \\mathrm{E}[P] + o(1)$. This last result is new even in the context of\nrandom $k$-SAT. Regarding the optimality of our $m \\gg n^{t/2}$ requirement,\nprior work on SDP hierarchies has given some evidence that efficient refutation\nof random CSP$(P)$ may be impossible when $m \\ll n^{t/2}$. Thus there is an\nindication our algorithm's dependence on $m$ is optimal for every $P$, at least\nin the context of SDP hierarchies. Along these lines, we show that our\nrefutation algorithm can be carried out by the $O(1)$-round SOS SDP hierarchy.\nFinally, as an application of our result, we falsify assumptions used to show\nhardness-of-learning results in recent work of Daniely, Linial, and\nShalev-Shwartz. \n\n"}
{"id": "1505.04969", "contents": "Title: Average-case complexity of a branch-and-bound algorithm for maximum\n  independent set, under the $\\mathcal{G}(n,p)$ random model Abstract: We study average-case complexity of branch-and-bound for maximum independent\nset in random graphs under the $\\mathcal{G}(n,p)$ distribution. In this model\nevery pair $(u,v)$ of vertices belongs to $E$ with probability $p$\nindependently on the existence of any other edge. We make a precise case\nanalysis, providing phase transitions between subexponential and exponential\ncomplexities depending on the probability $p$ of the random model. \n\n"}
{"id": "1505.05628", "contents": "Title: A generalization of Kung's theorem Abstract: We give a generalization of Kung's theorem on critical exponents of linear\ncodes over a finite field, in terms of sums of extended weight polynomials of\nlinear codes. For all i=k+1,...,n, we give an upper bound on the smallest\ninteger m such that there exist m codewords whose union of supports has\ncardinality at least i. \n\n"}
{"id": "1506.02243", "contents": "Title: On approximating tree spanners that are breadth first search trees Abstract: A tree $t$-spanner $T$ of a graph $G$ is a spanning tree of $G$ such that the\ndistance in $T$ between every pair of verices is at most $t$ times the distance\nin $G$ between them. There are efficient algorithms that find a tree $t\\cdot\nO(\\log n)$-spanner of a graph $G$, when $G$ admits a tree $t$-spanner. In this\npaper, the search space is narrowed to $v$-concentrated spanning trees, a\nsimple family that includes all the breadth first search trees starting from\nvertex $v$. In this case, it is not easy to find approximate tree spanners\nwithin factor almost $o(\\log n)$. Specifically, let $m$ and $t$ be integers,\nsuch that $m>0$ and $t\\geq 7$. If there is an efficient algorithm that receives\nas input a graph $G$ and a vertex $v$ and returns a $v$-concentrated tree\n$t\\cdot o((\\log n)^{m/(m+1)})$-spanner of $G$, when $G$ admits a\n$v$-concentrated tree $t$-spanner, then there is an algorithm that decides\n3-SAT in quasi-polynomial time. \n\n"}
{"id": "1506.02936", "contents": "Title: Parity Decision Tree Complexity and 4-Party Communication Complexity of\n  XOR-functions Are Polynomially Equivalent Abstract: In this note, we study the relation between the parity decision tree\ncomplexity of a boolean function $f$, denoted by $\\mathrm{D}_{\\oplus}(f)$, and\nthe $k$-party number-in-hand multiparty communication complexity of the XOR\nfunctions $F(x_1,\\ldots, x_k)= f(x_1\\oplus\\cdots\\oplus x_k)$, denoted by\n$\\mathrm{CC}^{(k)}(F)$. It is known that $\\mathrm{CC}^{(k)}(F)\\leq\nk\\cdot\\mathrm{D}_{\\oplus}(f)$ because the players can simulate the parity\ndecision tree that computes $f$. In this note, we show that\n\\[\\mathrm{D}_{\\oplus}(f)\\leq O\\big(\\mathrm{CC}^{(4)}(F)^5\\big).\\] Our main tool\nis a recent result from additive combinatorics due to Sanders. As\n$\\mathrm{CC}^{(k)}(F)$ is non-decreasing as $k$ grows, the parity decision tree\ncomplexity of $f$ and the communication complexity of the corresponding\n$k$-argument XOR functions are polynomially equivalent whenever $k\\geq 4$.\n  Remark: After the first version of this paper was finished, we discovered\nthat Hatami and Lovett had already discovered the same result a few years ago,\nwithout writing it up. \n\n"}
{"id": "1506.04719", "contents": "Title: Separations in Query Complexity Based on Pointer Functions Abstract: In 1986, Saks and Wigderson conjectured that the largest separation between\ndeterministic and zero-error randomized query complexity for a total boolean\nfunction is given by the function $f$ on $n=2^k$ bits defined by a complete\nbinary tree of NAND gates of depth $k$, which achieves $R_0(f) =\nO(D(f)^{0.7537\\ldots})$. We show this is false by giving an example of a total\nboolean function $f$ on $n$ bits whose deterministic query complexity is\n$\\Omega(n/\\log(n))$ while its zero-error randomized query complexity is $\\tilde\nO(\\sqrt{n})$. We further show that the quantum query complexity of the same\nfunction is $\\tilde O(n^{1/4})$, giving the first example of a total function\nwith a super-quadratic gap between its quantum and deterministic query\ncomplexities.\n  We also construct a total boolean function $g$ on $n$ variables that has\nzero-error randomized query complexity $\\Omega(n/\\log(n))$ and bounded-error\nrandomized query complexity $R(g) = \\tilde O(\\sqrt{n})$. This is the first\nsuper-linear separation between these two complexity measures. The exact\nquantum query complexity of the same function is $Q_E(g) = \\tilde O(\\sqrt{n})$.\n  These two functions show that the relations $D(f) = O(R_1(f)^2)$ and $R_0(f)\n= \\tilde O(R(f)^2)$ are optimal, up to poly-logarithmic factors. Further\nvariations of these functions give additional separations between other query\ncomplexity measures: a cubic separation between $Q$ and $R_0$, a $3/2$-power\nseparation between $Q_E$ and $R$, and a 4th power separation between\napproximate degree and bounded-error randomized query complexity.\n  All of these examples are variants of a function recently introduced by\n\\goos, Pitassi, and Watson which they used to separate the unambiguous\n1-certificate complexity from deterministic query complexity and to resolve the\nfamous Clique versus Independent Set problem in communication complexity. \n\n"}
{"id": "1506.06302", "contents": "Title: Inapproximability of $H$-Transversal/Packing Abstract: Given an undirected graph $G = (V_G, E_G)$ and a fixed \"pattern\" graph $H =\n(V_H, E_H)$ with $k$ vertices, we consider the $H$-Transversal and $H$-Packing\nproblems. The former asks to find the smallest $S \\subseteq V_G$ such that the\nsubgraph induced by $V_G \\setminus S$ does not have $H$ as a subgraph, and the\nlatter asks to find the maximum number of pairwise disjoint $k$-subsets $S_1,\n..., S_m \\subseteq V_G$ such that the subgraph induced by each $S_i$ has $H$ as\na subgraph.\n  We prove that if $H$ is 2-connected, $H$-Transversal and $H$-Packing are\nalmost as hard to approximate as general $k$-Hypergraph Vertex Cover and\n$k$-Set Packing, so it is NP-hard to approximate them within a factor of\n$\\Omega (k)$ and $\\widetilde \\Omega (k)$ respectively. We also show that there\nis a 1-connected $H$ where $H$-Transversal admits an $O(\\log k)$-approximation\nalgorithm, so that the connectivity requirement cannot be relaxed from 2 to 1.\nFor a special case of $H$-Transversal where $H$ is a (family of) cycles, we\nmention the implication of our result to the related Feedback Vertex Set\nproblem, and give a different hardness proof for directed graphs. \n\n"}
{"id": "1506.07076", "contents": "Title: Fully Dynamic Matching in Bipartite Graphs Abstract: Maximum cardinality matching in bipartite graphs is an important and\nwell-studied problem. The fully dynamic version, in which edges are inserted\nand deleted over time has also been the subject of much attention. Existing\nalgorithms for dynamic matching (in general graphs) seem to fall into two\ngroups: there are fast (mostly randomized) algorithms that do not achieve a\nbetter than 2-approximation, and there slow algorithms with $\\O(\\sqrt{m})$\nupdate time that achieve a better-than-2 approximation. Thus the obvious\nquestion is whether we can design an algorithm -- deterministic or randomized\n-- that achieves a tradeoff between these two: a $o(\\sqrt{m})$ approximation\nand a better-than-2 approximation simultaneously. We answer this question in\nthe affirmative for bipartite graphs.\n  Our main result is a fully dynamic algorithm that maintains a $3/2 + \\eps$\napproximation in worst-case update time $O(m^{1/4}\\eps^{/2.5})$. We also give\nstronger results for graphs whose arboricity is at most $\\al$, achieving a $(1+\n\\eps)$ approximation in worst-case time $O(\\al (\\al + \\log n))$ for constant\n$\\eps$. When the arboricity is constant, this bound is $O(\\log n)$ and when the\narboricity is polylogarithmic the update time is also polylogarithmic.\n  The most important technical developement is the use of an intermediate graph\nwe call an edge degree constrained subgraph (EDCS). This graph places\nconstraints on the sum of the degrees of the endpoints of each edge: upper\nbounds for matched edges and lower bounds for unmatched edges. The main\ntechnical content of our paper involves showing both how to maintain an EDCS\ndynamically and that and EDCS always contains a sufficiently large matching. We\nalso make use of graph orientations to help bound the amount of work done\nduring each update. \n\n"}
{"id": "1506.07773", "contents": "Title: Maximum weighted independent sets with a budget Abstract: Given a graph $G$, a non-negative integer $k$, and a weight function that\nmaps each vertex in $G$ to a positive real number, the \\emph{Maximum Weighted\nBudgeted Independent Set (MWBIS) problem} is about finding a maximum weighted\nindependent set in $G$ of cardinality at most $k$. A special case of MWBIS,\nwhen the weight assigned to each vertex is equal to its degree in $G$, is\ncalled the \\emph{Maximum Independent Vertex Coverage (MIVC)} problem. In other\nwords, the MIVC problem is about finding an independent set of cardinality at\nmost $k$ with maximum coverage.\n  Since it is a generalization of the well-known Maximum Weighted Independent\nSet (MWIS) problem, MWBIS too does not have any constant factor polynomial time\napproximation algorithm assuming $P \\neq NP$. In this paper, we study MWBIS in\nthe context of bipartite graphs. We show that, unlike MWIS, the MIVC (and\nthereby the MWBIS) problem in bipartite graphs is NP-hard. Then, we show that\nthe MWBIS problem admits a $\\frac{1}{2}$-factor approximation algorithm in the\nclass of bipartite graphs, which matches the integrality gap of a natural LP\nrelaxation. \n\n"}
{"id": "1506.07810", "contents": "Title: Canonizing Graphs of Bounded Tree Width in Logspace Abstract: Graph canonization is the problem of computing a unique representative, a\ncanon, from the isomorphism class of a given graph. This implies that two\ngraphs are isomorphic exactly if their canons are equal. We show that graphs of\nbounded tree width can be canonized by logarithmic-space (logspace) algorithms.\nThis implies that the isomorphism problem for graphs of bounded tree width can\nbe decided in logspace. In the light of isomorphism for trees being hard for\nthe complexity class logspace, this makes the ubiquitous class of graphs of\nbounded tree width one of the few classes of graphs for which the complexity of\nthe isomorphism problem has been exactly determined. \n\n"}
{"id": "1506.08518", "contents": "Title: Fast Computation of Abelian Runs Abstract: Given a word $w$ and a Parikh vector $\\mathcal{P}$, an abelian run of period\n$\\mathcal{P}$ in $w$ is a maximal occurrence of a substring of $w$ having\nabelian period $\\mathcal{P}$. Our main result is an online algorithm that,\ngiven a word $w$ of length $n$ over an alphabet of cardinality $\\sigma$ and a\nParikh vector $\\mathcal{P}$, returns all the abelian runs of period\n$\\mathcal{P}$ in $w$ in time $O(n)$ and space $O(\\sigma+p)$, where $p$ is the\nnorm of $\\mathcal{P}$, i.e., the sum of its components. We also present an\nonline algorithm that computes all the abelian runs with periods of norm $p$ in\n$w$ in time $O(np)$, for any given norm $p$. Finally, we give an $O(n^2)$-time\noffline randomized algorithm for computing all the abelian runs of $w$. Its\ndeterministic counterpart runs in $O(n^2\\log\\sigma)$ time. \n\n"}
{"id": "1506.09145", "contents": "Title: Track Layouts, Layered Path Decompositions, and Leveled Planarity Abstract: We investigate two types of graph layouts, track layouts and layered path\ndecompositions, and the relations between their associated parameters\ntrack-number and layered pathwidth. We use these two types of layouts to\ncharacterize leveled planar graphs, which are the graphs with planar leveled\ndrawings with no dummy vertices. It follows from the known NP-completeness of\nleveled planarity that track-number and layered pathwidth are also NP-complete,\neven for the smallest constant parameter values that make these parameters\nnontrivial. We prove that the graphs with bounded layered pathwidth include\nouterplanar graphs, Halin graphs, and squaregraphs, but that (despite having\nbounded track-number) series-parallel graphs do not have bounded layered\npathwidth. Finally, we investigate the parameterized complexity of these\nlayouts, showing that past methods used for book layouts do not work to\nparameterize the problem by treewidth or almost-tree number but that the\nproblem is (non-uniformly) fixed-parameter tractable for tree-depth. \n\n"}
{"id": "1506.09158", "contents": "Title: On Fair Size-Based Scheduling Abstract: By executing jobs serially rather than in parallel, size-based scheduling\npolicies can shorten time needed to complete jobs; however, major obstacles to\ntheir applicability are fairness guarantees and the fact that job sizes are\nrarely known exactly a-priori. Here, we introduce the Pri family of size-based\nscheduling policies; Pri simulates any reference scheduler and executes jobs in\nthe order of their simulated completion: we show that these schedulers give\nstrong fairness guarantees, since no job completes later in Pri than in the\nreference policy. In addition, we introduce PSBS, a practical implementation of\nsuch a scheduler: it works online (i.e., without needing knowledge of jobs\nsubmitted in the future), it has an efficient O(log n) implementation and it\nallows setting priorities to jobs. Most importantly, unlike earlier size-based\npolicies, the performance of PSBS degrades gracefully with errors, leading to\nperformances that are close to optimal in a variety of realistic use cases. \n\n"}
{"id": "1507.00056", "contents": "Title: Private Approximations of the 2nd-Moment Matrix Using Existing\n  Techniques in Linear Regression Abstract: We introduce three differentially-private algorithms that approximates the\n2nd-moment matrix of the data. These algorithm, which in contrast to existing\nalgorithms output positive-definite matrices, correspond to existing techniques\nin linear regression literature. Specifically, we discuss the following three\ntechniques. (i) For Ridge Regression, we propose setting the regularization\ncoefficient so that by approximating the solution using Johnson-Lindenstrauss\ntransform we preserve privacy. (ii) We show that adding a small batch of random\nsamples to our data preserves differential privacy. (iii) We show that sampling\nthe 2nd-moment matrix from a Bayesian posterior inverse-Wishart distribution is\ndifferentially private provided the prior is set correctly. We also evaluate\nour techniques experimentally and compare them to the existing \"Analyze Gauss\"\nalgorithm of Dwork et al. \n\n"}
{"id": "1507.00662", "contents": "Title: On the Approximability of Digraph Ordering Abstract: Given an n-vertex digraph D = (V, A) the Max-k-Ordering problem is to compute\na labeling $\\ell : V \\to [k]$ maximizing the number of forward edges, i.e.\nedges (u,v) such that $\\ell$(u) < $\\ell$(v). For different values of k, this\nreduces to Maximum Acyclic Subgraph (k=n), and Max-Dicut (k=2). This work\nstudies the approximability of Max-k-Ordering and its generalizations,\nmotivated by their applications to job scheduling with soft precedence\nconstraints. We give an LP rounding based 2-approximation algorithm for\nMax-k-Ordering for any k={2,..., n}, improving on the known\n2k/(k-1)-approximation obtained via random assignment. The tightness of this\nrounding is shown by proving that for any k={2,..., n} and constant\n$\\varepsilon > 0$, Max-k-Ordering has an LP integrality gap of 2 -\n$\\varepsilon$ for $n^{\\Omega\\left(1/\\log\\log k\\right)}$ rounds of the\nSherali-Adams hierarchy.\n  A further generalization of Max-k-Ordering is the restricted maximum acyclic\nsubgraph problem or RMAS, where each vertex v has a finite set of allowable\nlabels $S_v \\subseteq \\mathbb{Z}^+$. We prove an LP rounding based\n$4\\sqrt{2}/(\\sqrt{2}+1) \\approx 2.344$ approximation for it, improving on the\n$2\\sqrt{2} \\approx 2.828$ approximation recently given by Grandoni et al.\n(Information Processing Letters, Vol. 115(2), Pages 182-185, 2015). In fact,\nour approximation algorithm also works for a general version where the\nobjective counts the edges which go forward by at least a positive offset\nspecific to each edge.\n  The minimization formulation of digraph ordering is DAG edge deletion or\nDED(k), which requires deleting the minimum number of edges from an n-vertex\ndirected acyclic graph (DAG) to remove all paths of length k. We show that\nboth, the LP relaxation and a local ratio approach for DED(k) yield\nk-approximation for any $k\\in [n]$. \n\n"}
{"id": "1507.00843", "contents": "Title: Optimal linear Bernoulli factories for small mean problems Abstract: Suppose a coin with unknown probability $p$ of heads can be flipped as often\nas desired. A Bernoulli factory for a function $f$ is an algorithm that uses\nflips of the coin together with auxiliary randomness to flip a single coin with\nprobability $f(p)$ of heads. Applications include near perfect sampling from\nthe stationary distribution of regenerative processes. When $f$ is analytic,\nthe problem can be reduced to a Bernoulli factory of the form $f(p) = Cp$ for\nconstant $C$. Presented here is a new algorithm where for small values of $Cp$,\nrequires roughly only $C$ coin flips to generate a $Cp$ coin. From information\ntheory considerations, this is also conjectured to be (to first order) the\nminimum number of flips needed by any such algorithm.\n  For $Cp$ large, the new algorithm can also be used to build a new Bernoulli\nfactory that uses only 80\\% of the expected coin flips of the older method, and\napplies to the more general problem of a multivariate Bernoulli factory, where\nthere are $k$ coins, the $k$th coin has unknown probability $p_k$ of heads, and\nthe goal is to simulate a coin flip with probability $C_1 p_1 + \\cdots + C_k\np_k$ of heads. \n\n"}
{"id": "1507.01768", "contents": "Title: The Restricted Isometry Property of Subsampled Fourier Matrices Abstract: A matrix $A \\in \\mathbb{C}^{q \\times N}$ satisfies the restricted isometry\nproperty of order $k$ with constant $\\varepsilon$ if it preserves the $\\ell_2$\nnorm of all $k$-sparse vectors up to a factor of $1\\pm \\varepsilon$. We prove\nthat a matrix $A$ obtained by randomly sampling $q = O(k \\cdot \\log^2 k \\cdot\n\\log N)$ rows from an $N \\times N$ Fourier matrix satisfies the restricted\nisometry property of order $k$ with a fixed $\\varepsilon$ with high\nprobability. This improves on Rudelson and Vershynin (Comm. Pure Appl. Math.,\n2008), its subsequent improvements, and Bourgain (GAFA Seminar Notes, 2014). \n\n"}
{"id": "1507.03046", "contents": "Title: An efficient tree decomposition method for permanents and mixed\n  discriminants Abstract: We present an efficient algorithm to compute permanents, mixed discriminants\nand hyperdeterminants of structured matrices and multidimensional arrays\n(tensors). We describe the sparsity structure of an array in terms of a graph,\nand we assume that its treewidth, denoted as $\\omega$, is small. Our algorithm\nrequires $O(n 2^\\omega)$ arithmetic operations to compute permanents, and\n$O(n^2 + n 3^\\omega)$ for mixed discriminants and hyperdeterminants. We finally\nshow that mixed volume computation continues to be hard under bounded treewidth\nassumptions. \n\n"}
{"id": "1507.03269", "contents": "Title: Tensor principal component analysis via sum-of-squares proofs Abstract: We study a statistical model for the tensor principal component analysis\nproblem introduced by Montanari and Richard: Given a order-$3$ tensor $T$ of\nthe form $T = \\tau \\cdot v_0^{\\otimes 3} + A$, where $\\tau \\geq 0$ is a\nsignal-to-noise ratio, $v_0$ is a unit vector, and $A$ is a random noise\ntensor, the goal is to recover the planted vector $v_0$. For the case that $A$\nhas iid standard Gaussian entries, we give an efficient algorithm to recover\n$v_0$ whenever $\\tau \\geq \\omega(n^{3/4} \\log(n)^{1/4})$, and certify that the\nrecovered vector is close to a maximum likelihood estimator, all with high\nprobability over the random choice of $A$. The previous best algorithms with\nprovable guarantees required $\\tau \\geq \\Omega(n)$.\n  In the regime $\\tau \\leq o(n)$, natural tensor-unfolding-based spectral\nrelaxations for the underlying optimization problem break down (in the sense\nthat their integrality gap is large). To go beyond this barrier, we use convex\nrelaxations based on the sum-of-squares method. Our recovery algorithm proceeds\nby rounding a degree-$4$ sum-of-squares relaxations of the\nmaximum-likelihood-estimation problem for the statistical model. To complement\nour algorithmic results, we show that degree-$4$ sum-of-squares relaxations\nbreak down for $\\tau \\leq O(n^{3/4}/\\log(n)^{1/4})$, which demonstrates that\nimproving our current guarantees (by more than logarithmic factors) would\nrequire new techniques or might even be intractable.\n  Finally, we show how to exploit additional problem structure in order to\nsolve our sum-of-squares relaxations, up to some approximation, very\nefficiently. Our fastest algorithm runs in nearly-linear time using shifted\n(matrix) power iteration and has similar guarantees as above. The analysis of\nthis algorithm also confirms a variant of a conjecture of Montanari and Richard\nabout singular vectors of tensor unfoldings. \n\n"}
{"id": "1507.03885", "contents": "Title: Quantum Lower Bound for Graph Collision Implies Lower Bound for Triangle\n  Detection Abstract: We show that an improvement to the best known quantum lower bound for\nGRAPH-COLLISION problem implies an improvement to the best known lower bound\nfor TRIANGLE problem in the quantum query complexity model. In GRAPH-COLLISION\nwe are given free access to a graph $(V,E)$ and access to a function\n$f:V\\rightarrow \\{0,1\\}$ as a black box. We are asked to determine if there\nexist $(u,v) \\in E$, such that $f(u)=f(v)=1$. In TRIANGLE we have a black box\naccess to an adjacency matrix of a graph and we have to determine if the graph\ncontains a triangle. For both of these problems the known lower bounds are\ntrivial ($\\Omega(\\sqrt{n})$ and $\\Omega(n)$, respectively) and there is no\nknown matching upper bound. \n\n"}
{"id": "1507.04220", "contents": "Title: A numerical analysis of Quicksort: How many cases are bad cases? Abstract: We present numerical results for the probability of bad cases for Quicksort,\ni.e. cases of input data for which the sorting cost considerably exceeds that\nof the average. Dynamic programming was used to compute solutions of the\nrecurrence for the frequency distributions of comparisons. From these\nsolutions, probabilities of numbers of comparisons above certain thresholds\nrelative to the average were extracted. Computations were done for array sizes\nup to n = 500 elements and for several methods to select the partitioning\nelement, from a simple random selection to what we call \"recursive median of\nthree medians.\" We found that the probability strongly depends on the selection\nmethod: for n = 500 and a theshold 25% above the average number of comparisons\nit ranges from 2.2*10^(-3) to 3.0*10^(-23). A version of Quicksort based on the\nrecursive median of medians approach is proposed, for which our data suggest a\nworst case time complexity of O(n^1.37). \n\n"}
{"id": "1507.04461", "contents": "Title: Assignment Problems of Different-Sized Inputs in MapReduce Abstract: A MapReduce algorithm can be described by a mapping schema, which assigns\ninputs to a set of reducers, such that for each required output there exists a\nreducer that receives all the inputs that participate in the computation of\nthis output. Reducers have a capacity, which limits the sets of inputs that\nthey can be assigned. However, individual inputs may vary in terms of size. We\nconsider, for the first time, mapping schemas where input sizes are part of the\nconsiderations and restrictions. One of the significant parameters to optimize\nin any MapReduce job is communication cost between the map and reduce phases.\nThe communication cost can be optimized by minimizing the number of copies of\ninputs sent to the reducers. The communication cost is closely related to the\nnumber of reducers of constrained capacity that are used to accommodate\nappropriately the inputs, so that the requirement of how the inputs must meet\nin a reducer is satisfied. In this work, we consider a family of problems where\nit is required that each input meets with each other input in at least one\nreducer. We also consider a slightly different family of problems in which,\neach input of a list, X, is required to meet each input of another list, Y, in\nat least one reducer. We prove that finding an optimal mapping schema for these\nfamilies of problems is NP-hard, and present a bin-packing-based approximation\nalgorithm for finding a near optimal mapping schema. \n\n"}
{"id": "1507.05086", "contents": "Title: Parallel Correlation Clustering on Big Graphs Abstract: Given a similarity graph between items, correlation clustering (CC) groups\nsimilar items together and dissimilar ones apart. One of the most popular CC\nalgorithms is KwikCluster: an algorithm that serially clusters neighborhoods of\nvertices, and obtains a 3-approximation ratio. Unfortunately, KwikCluster in\npractice requires a large number of clustering rounds, a potential bottleneck\nfor large graphs.\n  We present C4 and ClusterWild!, two algorithms for parallel correlation\nclustering that run in a polylogarithmic number of rounds and achieve nearly\nlinear speedups, provably. C4 uses concurrency control to enforce\nserializability of a parallel clustering process, and guarantees a\n3-approximation ratio. ClusterWild! is a coordination free algorithm that\nabandons consistency for the benefit of better scaling; this leads to a\nprovably small loss in the 3-approximation ratio.\n  We provide extensive experimental results for both algorithms, where we\noutperform the state of the art, both in terms of clustering accuracy and\nrunning time. We show that our algorithms can cluster billion-edge graphs in\nunder 5 seconds on 32 cores, while achieving a 15x speedup. \n\n"}
{"id": "1507.05485", "contents": "Title: A deterministic algorithm to compute approximate roots of polynomial\n  systems in polynomial average time Abstract: We describe a deterministic algorithm that computes an approximate root of n\ncomplex polynomial equations in n unknowns in average polynomial time with\nrespect to the size of the input, in the Blum-Shub-Smale model with square\nroot. It rests upon a derandomization of an algorithm of Beltr\\'an and Pardo\nand gives a deterministic affirmative answer to Smale's 17th problem. The main\nidea is to make use of the randomness contained in the input itself. \n\n"}
{"id": "1507.05605", "contents": "Title: A semidefinite program for unbalanced multisection in the stochastic\n  block model Abstract: We propose a semidefinite programming (SDP) algorithm for community detection\nin the stochastic block model, a popular model for networks with latent\ncommunity structure. We prove that our algorithm achieves exact recovery of the\nlatent communities, up to the information-theoretic limits determined by Abbe\nand Sandon (2015). Our result extends prior SDP approaches by allowing for many\ncommunities of different sizes. By virtue of a semidefinite approach, our\nalgorithms succeed against a semirandom variant of the stochastic block model,\nguaranteeing a form of robustness and generalization. We further explore how\nsemirandom models can lend insight into both the strengths and limitations of\nSDPs in this setting. \n\n"}
{"id": "1507.05854", "contents": "Title: Global Convergence of Non-Convex Gradient Descent for Computing Matrix\n  Squareroot Abstract: While there has been a significant amount of work studying gradient descent\ntechniques for non-convex optimization problems over the last few years, all\nexisting results establish either local convergence with good rates or global\nconvergence with highly suboptimal rates, for many problems of interest. In\nthis paper, we take the first step in getting the best of both worlds --\nestablishing global convergence and obtaining a good rate of convergence for\nthe problem of computing squareroot of a positive definite (PD) matrix, which\nis a widely studied problem in numerical linear algebra with applications in\nmachine learning and statistics among others. Given a PD matrix $M$ and a PD\nstarting point $U_0$, we show that gradient descent with appropriately chosen\nstep-size finds an $\\epsilon$-accurate squareroot of $M$ in $O(\\alpha \\log\n(\\|M-U_0^2\\|_F /\\epsilon))$ iterations, where $\\alpha =\n(\\max\\{\\|U_0\\|_2^2,\\|M\\|_2\\} / \\min \\{\\sigma_{\\min}^2(U_0),\\sigma_{\\min}(M) \\}\n)^{3/2}$. Our result is the first to establish global convergence for this\nproblem and that it is robust to errors in each iteration. A key contribution\nof our work is the general proof technique which we believe should further\nexcite research in understanding deterministic and stochastic variants of\nsimple non-convex gradient descent algorithms with good global convergence\nrates for other problems in machine learning and numerical linear algebra. \n\n"}
{"id": "1507.08158", "contents": "Title: Fast Biclustering by Dual Parameterization Abstract: We study two clustering problems, Starforest Editing, the problem of adding\nand deleting edges to obtain a disjoint union of stars, and the generalization\nBicluster Editing. We show that, in addition to being NP-hard, none of the\nproblems can be solved in subexponential time unless the exponential time\nhypothesis fails.\n  Misra, Panolan, and Saurabh (MFCS 2013) argue that introducing a bound on the\nnumber of connected components in the solution should not make the problem\neasier: In particular, they argue that the subexponential time algorithm for\nediting to a fixed number of clusters (p-Cluster Editing) by Fomin et al. (J.\nComput. Syst. Sci., 80(7) 2014) is an exception rather than the rule. Here, p\nis a secondary parameter, bounding the number of components in the solution.\n  However, upon bounding the number of stars or bicliques in the solution, we\nobtain algorithms which run in time $2^{5 \\sqrt{pk}} + O(n+m)$ for p-Starforest\nEditing and $2^{O(p \\sqrt{k} \\log(pk))} + O(n+m)$ for p-Bicluster Editing. We\nobtain a similar result for the more general case of t-Partite p-Cluster\nEditing. This is subexponential in k for fixed number of clusters, since p is\nthen considered a constant.\n  Our results even out the number of multivariate subexponential time\nalgorithms and give reasons to believe that this area warrants further study. \n\n"}
{"id": "1507.08792", "contents": "Title: A cubic vertex kernel for Diamond-free Edge Deletion and more Abstract: A diamond is a graph obtained by removing an edge from a complete graph on\nfour vertices. A graph is diamond-free if it does not contain an induced\ndiamond. The Diamond-free Edge Deletion problem asks whether there exist at\nmost $k$ edges in the input graph $G$ whose deletion results in a diamond-free\ngraph. For this problem, a polynomial kernel of $O(k^4$) vertices was found by\nFellows et. al. (Discrete Optimization, 2011).\n  In this paper, we give an improved kernel of $O(k^3)$ vertices for\nDiamond-free Edge Deletion. Further, we give an $O(k^2)$ vertex kernel for a\nrelated problem {Diamond,K_t}-free Edge Deletion, where $t\\geq 4$ is any fixed\ninteger. To complement our results, we prove that these problems are\nNP-complete even for $K_4$-free graphs and can be solved neither in\nsubexponential time (i.e., $2^{o(|G|)}$) nor in parameterized subexponential\ntime (i.e., $2^{o(k)}\\cdot |G|^{O(1)}$), unless Exponential Time Hypothesis\nfails. Our reduction implies the hardness and lower bound for a general class\nof problems, where these problems come as a special case. \n\n"}
{"id": "1508.01110", "contents": "Title: Symmetries of matrix multiplication algorithms. I Abstract: In this work the algorithms of fast multiplication of matrices are\nconsidered. To any algorithm there associated a certain group of automorphisms.\nThese automorphism groups are found for some well-known algorithms, including\nalgorithms of Hopcroft, Laderman, and Pan. The automorphism group is isomorphic\nto $S_3\\times Z_2$ and $S_4$ for Hopcroft anf Laderman algorithms,\nrespectively. The studying of symmetry of algorithms may be a fruitful idea for\nfinding fast algorithms, by an analogy with well-known optimization problems\nfor codes, lattices, and graphs.\n  {\\em Keywords}: Strassen algorithm, symmetry, fast matrix multiplication. \n\n"}
{"id": "1508.05282", "contents": "Title: Lower bounds for the parameterized complexity of Minimum Fill-in and\n  other completion problems Abstract: In this work, we focus on several completion problems for subclasses of\nchordal graphs: Minimum Fill-In, Interval Completion, Proper Interval\nCompletion, Threshold Completion, and Trivially Perfect Completion. In these\nproblems, the task is to add at most k edges to a given graph in order to\nobtain a chordal, interval, proper interval, threshold, or trivially perfect\ngraph, respectively. We prove the following lower bounds for all these\nproblems, as well as for the related Chain Completion problem: Assuming the\nExponential Time Hypothesis, none of these problems can be solved in time\n2^O(n^(1/2) / log^c n) or 2^O(k^(1/4) / log^c k) n^O(1), for some integer c.\nAssuming the non-existence of a subexponential-time approximation scheme for\nMin Bisection on d-regular graphs, for some constant d, none of these problems\ncan be solved in time 2^o(n) or 2^o(sqrt(k)) n^O(1).\n  For all the aforementioned completion problems, apart from Proper Interval\nCompletion, FPT algorithms with running time of the form 2^O(sqrt(k) log k)\nn^O(1) are known. Thus, the second result proves that a significant improvement\nof any of these algorithms would lead to a surprising breakthrough in the\ndesign of approximation algorithms for Min Bisection.\n  To prove our results, we use a reduction methodology based on combining the\nclassic approach of starting with a sparse instance of 3-Sat, prepared using\nthe Sparsification Lemma, with the existence of almost linear-size\nProbabilistically Checkable Proofs (PCPs). Apart from our main results, we also\nobtain lower bounds excluding the existence of subexponential algorithms for\nthe Optimum Linear Arrangement problem, as well as improved, yet still not\ntight, lower bounds for Feedback Arc Set in Tournaments. \n\n"}
{"id": "1508.05788", "contents": "Title: Permanent v. determinant: an exponential lower bound assumingsymmetry\n  and a potential path towards Valiant's conjecture Abstract: We initiate a study of determinantal representations with symmetry. We show\nthat Grenet's determinantal representation for the permanent is optimal among\ndeterminantal representations respecting left multiplication by permutation and\ndiagonal matrices (roughly half the symmetry group of the permanent). In\nparticular, if any optimal determinantal representation of the permanent must\nbe polynomially related to one with such symmetry, then Valiant's conjecture on\npermanent v. determinant is true. \n\n"}
{"id": "1508.06019", "contents": "Title: Dense Subset Sum may be the hardest Abstract: The Subset Sum problem asks whether a given set of $n$ positive integers\ncontains a subset of elements that sum up to a given target $t$. It is an\noutstanding open question whether the $O^*(2^{n/2})$-time algorithm for Subset\nSum by Horowitz and Sahni [J. ACM 1974] can be beaten in the worst-case setting\nby a \"truly faster\", $O^*(2^{(0.5-\\delta)n})$-time algorithm, with some\nconstant $\\delta > 0$. Continuing an earlier work [STACS 2015], we study Subset\nSum parameterized by the maximum bin size $\\beta$, defined as the largest\nnumber of subsets of the $n$ input integers that yield the same sum. For every\n$\\epsilon > 0$ we give a truly faster algorithm for instances with $\\beta \\leq\n2^{(0.5-\\epsilon)n}$, as well as instances with $\\beta \\geq 2^{0.661n}$.\nConsequently, we also obtain a characterization in terms of the popular density\nparameter $n/\\log_2 t$: if all instances of density at least $1.003$ admit a\ntruly faster algorithm, then so does every instance. This goes against the\ncurrent intuition that instances of density 1 are the hardest, and therefore is\na step toward answering the open question in the affirmative. Our results stem\nfrom novel combinations of earlier algorithms for Subset Sum and a study of an\nextremal question in additive combinatorics connected to the problem of\nUniquely Decodable Code Pairs in information theory. \n\n"}
{"id": "1508.06420", "contents": "Title: The Stable Fixtures Problem with Payments Abstract: We generalize two well-known game-theoretic models by introducing multiple\npartners matching games, defined by a graph $G=(N,E)$, with an integer vertex\ncapacity function $b$ and an edge weighting $w$. The set $N$ consists of a\nnumber of players that are to form a set $M\\subseteq E$ of 2-player coalitions\n$ij$ with value $w(ij)$, such that each player $i$ is in at most $b(i)$\ncoalitions. A payoff vector is a mapping $p: N \\times N \\rightarrow {\\mathbb\nR}$ with $p(i,j)+p(j,i)=w(ij)$ if $ij\\in M$ and $p(i,j)=p(j,i)=0$ if $ij\\notin\nM$. The pair $(M,p)$ is called a solution. A pair of players $i,j$ with $ij\\in\nE\\setminus M$ blocks a solution $(M,p)$ if $i,j$ can form, possibly only after\nwithdrawing from one of their existing 2-player coalitions, a new 2-player\ncoalition in which they are mutually better off. A solution is stable if it has\nno blocking pairs.\n  We give a polynomial-time algorithm that either finds that a given multiple\npartners matching game has no stable solution, or obtains a stable solution for\nit. We characterize the set of stable solutions of a multiple partners matching\ngame in two different ways and show how this leads to simple proofs for a\nnumber of known results of Sotomayor (1992,1999,2007) for multiple partners\nssignment games and to generalizations of some of these results to multiple\npartners matching games. We also perform a study on the core of the\ncorresponding cooperative game, where coalitions of any size may be formed. In\nparticular we show that the standard relation between the existence of a stable\nsolution and the non-emptiness of the core, which holds in the other models\nwith payments, is no longer valid for our (most general) model. We also prove\nthat the problem of deciding if an allocation belongs to the core jumps from\nbeing polynomial-time solvable for $b\\leq 2$ to NP-complete for $b\\equiv 3$. \n\n"}
{"id": "1508.07338", "contents": "Title: A linear time algorithm for quantum 2-SAT Abstract: The Boolean constraint satisfaction problem 3-SAT is arguably the canonical\nNP-complete problem. In contrast, 2-SAT can not only be decided in polynomial\ntime, but in fact in deterministic linear time. In 2006, Bravyi proposed a\nphysically motivated generalization of k-SAT to the quantum setting, defining\nthe problem \"quantum k-SAT\". He showed that quantum 2-SAT is also solvable in\npolynomial time on a classical computer, in particular in deterministic time\nO(n^4), assuming unit-cost arithmetic over a field extension of the rational\nnumbers, where n is number of variables. In this paper, we present an algorithm\nfor quantum 2-SAT which runs in linear time, i.e. deterministic time O(n+m) for\nn and m the number of variables and clauses, respectively. Our approach\nexploits the transfer matrix techniques of Laumann et al. [QIC, 2010] used in\nthe study of phase transitions for random quantum 2-SAT, and bears similarities\nwith both the linear time 2-SAT algorithms of Even, Itai, and Shamir (based on\nbacktracking) [SICOMP, 1976] and Aspvall, Plass, and Tarjan (based on strongly\nconnected components) [IPL, 1979]. \n\n"}
{"id": "1509.00092", "contents": "Title: Explicit resilient functions matching Ajtai-Linial Abstract: A Boolean function on n variables is q-resilient if for any subset of at most\nq variables, the function is very likely to be determined by a uniformly random\nassignment to the remaining n-q variables; in other words, no coalition of at\nmost q variables has significant influence on the function. Resilient functions\nhave been extensively studied with a variety of applications in cryptography,\ndistributed computing, and pseudorandomness. The best known balanced resilient\nfunction on n variables due to Ajtai and Linial ([AL93]) is Omega(n/(log^2\nn))-resilient. However, the construction of Ajtai and Linial is by the\nprobabilistic method and does not give an efficiently computable function.\n  In this work we give an explicit monotone depth three almost-balanced Boolean\nfunction on n bits that is Omega(n/(log^2 n))-resilient matching the work of\nAjtai and Linial. The best previous explicit construction due to Meka [Meka09]\n(which only gives a logarithmic depth function) and Chattopadhyay and\nZuckermman [CZ15] were only n^{1-c}-resilient for any constant c < 1. Our\nconstruction and analysis are motivated by (and simplifies parts of) the recent\nbreakthrough of [CZ15] giving explicit two-sources extractors for\npolylogarithmic min-entropy; a key ingredient in their result was the\nconstruction of explicit constant-depth resilient functions.\n  An important ingredient in our construction is a new randomness optimal\noblivious sampler which preserves moment generating functions of sums of\nvariables and could be useful elsewhere. \n\n"}
{"id": "1509.02374", "contents": "Title: Quantum walk speedup of backtracking algorithms Abstract: We describe a general method to obtain quantum speedups of classical\nalgorithms which are based on the technique of backtracking, a standard\napproach for solving constraint satisfaction problems (CSPs). Backtracking\nalgorithms explore a tree whose vertices are partial solutions to a CSP in an\nattempt to find a complete solution. Assume there is a classical backtracking\nalgorithm which finds a solution to a CSP on n variables, or outputs that none\nexists, and whose corresponding tree contains T vertices, each vertex\ncorresponding to a test of a partial solution. Then we show that there is a\nbounded-error quantum algorithm which completes the same task using O(sqrt(T)\nn^(3/2) log n) tests. In particular, this quantum algorithm can be used to\nspeed up the DPLL algorithm, which is the basis of many of the most efficient\nSAT solvers used in practice. The quantum algorithm is based on the use of a\nquantum walk algorithm of Belovs to search in the backtracking tree. We also\ndiscuss how, for certain distributions on the inputs, the algorithm can lead to\nan exponential reduction in expected runtime. \n\n"}
{"id": "1509.02897", "contents": "Title: Practical and Optimal LSH for Angular Distance Abstract: We show the existence of a Locality-Sensitive Hashing (LSH) family for the\nangular distance that yields an approximate Near Neighbor Search algorithm with\nthe asymptotically optimal running time exponent. Unlike earlier algorithms\nwith this property (e.g., Spherical LSH [Andoni, Indyk, Nguyen, Razenshteyn\n2014], [Andoni, Razenshteyn 2015]), our algorithm is also practical, improving\nupon the well-studied hyperplane LSH [Charikar, 2002] in practice. We also\nintroduce a multiprobe version of this algorithm, and conduct experimental\nevaluation on real and synthetic data sets.\n  We complement the above positive results with a fine-grained lower bound for\nthe quality of any LSH family for angular distance. Our lower bound implies\nthat the above LSH family exhibits a trade-off between evaluation time and\nquality that is close to optimal for a natural class of LSH functions. \n\n"}
{"id": "1509.05065", "contents": "Title: Estimating operator norms using covering nets Abstract: We present several polynomial- and quasipolynomial-time approximation schemes\nfor a large class of generalized operator norms. Special cases include the\n$2\\rightarrow q$ norm of matrices for $q>2$, the support function of the set of\nseparable quantum states, finding the least noisy output of\nentanglement-breaking quantum channels, and approximating the injective tensor\nnorm for a map between two Banach spaces whose factorization norm through\n$\\ell_1^n$ is bounded.\n  These reproduce and in some cases improve upon the performance of previous\nalgorithms by Brand\\~ao-Christandl-Yard and followup work, which were based on\nthe Sum-of-Squares hierarchy and whose analysis used techniques from quantum\ninformation such as the monogamy principle of entanglement. Our algorithms, by\ncontrast, are based on brute force enumeration over carefully chosen covering\nnets. These have the advantage of using less memory, having much simpler proofs\nand giving new geometric insights into the problem. Net-based algorithms for\nsimilar problems were also presented by Shi-Wu and Barak-Kelner-Steurer, but in\neach case with a run-time that is exponential in the rank of some matrix. We\nachieve polynomial or quasipolynomial runtimes by using the much smaller nets\nthat exist in $\\ell_1$ spaces. This principle has been used in learning theory,\nwhere it is known as Maurey's empirical method. \n\n"}
{"id": "1509.05896", "contents": "Title: On space efficiency of algorithms working on structural decompositions\n  of graphs Abstract: Dynamic programming on path and tree decompositions of graphs is a technique\nthat is ubiquitous in the field of parameterized and exponential-time\nalgorithms. However, one of its drawbacks is that the space usage is\nexponential in the decomposition's width. Following the work of Allender et al.\n[Theory of Computing, '14], we investigate whether this space complexity\nexplosion is unavoidable. Using the idea of reparameterization of Cai and\nJuedes [J. Comput. Syst. Sci., '03], we prove that the question is closely\nrelated to a conjecture that the Longest Common Subsequence problem\nparameterized by the number of input strings does not admit an algorithm that\nsimultaneously uses XP time and FPT space. Moreover, we complete the complexity\nlandscape sketched for pathwidth and treewidth by Allender et al. by\nconsidering the parameter tree-depth. We prove that computations on tree-depth\ndecompositions correspond to a model of non-deterministic machines that work in\npolynomial time and logarithmic space, with access to an auxiliary stack of\nmaximum height equal to the decomposition's depth. Together with the results of\nAllender et al., this describes a hierarchy of complexity classes for\npolynomial-time non-deterministic machines with different restrictions on the\naccess to working space, which mirrors the classic relations between treewidth,\npathwidth, and tree-depth. \n\n"}
{"id": "1509.06257", "contents": "Title: Communication Complexity (for Algorithm Designers) Abstract: This document collects the lecture notes from my course \"Communication\nComplexity (for Algorithm Designers),'' taught at Stanford in the winter\nquarter of 2015. The two primary goals of the course are: 1. Learn several\ncanonical problems that have proved the most useful for proving lower bounds\n(Disjointness, Index, Gap-Hamming, etc.). 2. Learn how to reduce lower bounds\nfor fundamental algorithmic problems to communication complexity lower bounds.\nAlong the way, we'll also: 3. Get exposure to lots of cool computational models\nand some famous results about them --- data streams and linear sketches,\ncompressive sensing, space-query time trade-offs in data structures,\nsublinear-time algorithms, and the extension complexity of linear programs. 4.\nScratch the surface of techniques for proving communication complexity lower\nbounds (fooling sets, corruption bounds, etc.). \n\n"}
{"id": "1510.01891", "contents": "Title: On the Hardest Problem Formulations for the 0/1 Lasserre Hierarchy Abstract: The Lasserre/Sum-of-Squares (SoS) hierarchy is a systematic procedure for\nconstructing a sequence of increasingly tight semidefinite relaxations. It is\nknown that the hierarchy converges to the 0/1 polytope in n levels and captures\nthe convex relaxations used in the best available approximation algorithms for\na wide variety of optimization problems.\n  In this paper we characterize the set of 0/1 integer linear problems and\nunconstrained 0/1 polynomial optimization problems that can still have an\nintegrality gap at level n-1. These problems are the hardest for the Lasserre\nhierarchy in this sense. \n\n"}
{"id": "1510.02197", "contents": "Title: A characterization of linearizable instances of the quadratic minimum\n  spanning tree problem Abstract: We investigate special cases of the quadratic minimum spanning tree problem\n(QMSTP) on a graph $G=(V,E)$ that can be solved as a linear minimum spanning\ntree problem. Characterization of such problems on graphs with special\nproperties are given. This include complete graphs, complete bipartite graphs,\ncactuses among others. Our characterization can be verified in $O(|E|^2)$ time.\nIn the case of complete graphs and when the cost matrix is given in factored\nform, we show that our characterization can be verified in $O(|E|)$ time.\nRelated open problems are also indicated. \n\n"}
{"id": "1510.02873", "contents": "Title: Group testing schemes from codes and designs Abstract: In group testing, simple binary-output tests are designed to identify a small\nnumber $t$ of defective items that are present in a large population of $N$\nitems. Each test takes as input a group of items and produces a binary output\nindicating whether the group is free of the defective items or contains one or\nmore of them. In this paper we study a relaxation of the combinatorial group\ntesting problem. A matrix is called $(t,\\epsilon)$-disjunct if it gives rise to\na nonadaptive group testing scheme with the property of identifying a uniformly\nrandom $t$-set of defective subjects out of a population of size $N$ with false\npositive probability of an item at most $\\epsilon$. We establish a new\nconnection between $(t,\\epsilon)$-disjunct matrices and error correcting codes\nbased on the dual distance of the codes and derive estimates of the parameters\nof codes that give rise to such schemes. Our methods rely on the moments of the\ndistance distribution of codes and inequalities for moments of sums of\nindependent random variables. We also provide a new connection between group\ntesting schemes and combinatorial designs. \n\n"}
{"id": "1510.04622", "contents": "Title: Subtree Isomorphism Revisited Abstract: The Subtree Isomorphism problem asks whether a given tree is contained in\nanother given tree. The problem is of fundamental importance and has been\nstudied since the 1960s. For some variants, e.g., ordered trees, near-linear\ntime algorithms are known, but for the general case truly subquadratic\nalgorithms remain elusive.\n  Our first result is a reduction from the Orthogonal Vectors problem to\nSubtree Isomorphism, showing that a truly subquadratic algorithm for the latter\nrefutes the Strong Exponential Time Hypothesis (SETH).\n  In light of this conditional lower bound, we focus on natural special cases\nfor which no truly subquadratic algorithms are known. We classify these cases\nagainst the quadratic barrier, showing in particular that:\n  -- Even for binary, rooted trees, a truly subquadratic algorithm refutes\nSETH.\n  -- Even for rooted trees of depth $O(\\log\\log{n})$, where $n$ is the total\nnumber of vertices, a truly subquadratic algorithm refutes SETH.\n  -- For every constant $d$, there is a constant $\\epsilon_d>0$ and a\nrandomized, truly subquadratic algorithm for degree-$d$ rooted trees of depth\nat most $(1+ \\epsilon_d) \\log_{d}{n}$. In particular, there is an $O(\\min\\{\n2.85^h ,n^2 \\})$ algorithm for binary trees of depth $h$.\n  Our reductions utilize new \"tree gadgets\" that are likely useful for future\nSETH-based lower bounds for problems on trees. Our upper bounds apply a\nfolklore result from randomized decision tree complexity. \n\n"}
{"id": "1510.04833", "contents": "Title: How the structure of precedence constraints may change the complexity\n  class of scheduling problems Abstract: This survey aims at demonstrating that the structure of precedence\nconstraints plays a tremendous role on the complexity of scheduling problems.\nIndeed many problems can be NP-hard when considering general precedence\nconstraints, while they become polynomially solvable for particular precedence\nconstraints. We also show that there still are many very exciting challenges in\nthis research area. \n\n"}
{"id": "1510.04991", "contents": "Title: Honest signaling in zero-sum games is hard, and lying is even harder Abstract: We prove that, assuming the exponential time hypothesis, finding an\n\\epsilon-approximately optimal symmetric signaling scheme in a two-player\nzero-sum game requires quasi-polynomial time. This is tight by [Cheng et al.,\nFOCS'15] and resolves an open question of [Dughmi, FOCS'14]. We also prove that\nfinding a multiplicative approximation is NP-hard.\n  We also introduce a new model where a dishonest signaler may publicly commit\nto use one scheme, but post signals according to a different scheme. For this\nmodel, we prove that even finding a (1-2^{-n})-approximately optimal scheme is\nNP-hard. \n\n"}
{"id": "1510.05058", "contents": "Title: A Distance Measure for the Analysis of Polar Opinion Dynamics in Social\n  Networks Abstract: Analysis of opinion dynamics in social networks plays an important role in\ntoday's life. For applications such as predicting users' political preference,\nit is particularly important to be able to analyze the dynamics of competing\nopinions. While observing the evolution of polar opinions of a social network's\nusers over time, can we tell when the network \"behaved\" abnormally?\nFurthermore, can we predict how the opinions of the users will change in the\nfuture? Do opinions evolve according to existing network opinion dynamics\nmodels? To answer such questions, it is not sufficient to study individual user\nbehavior, since opinions can spread far beyond users' egonets. We need a method\nto analyze opinion dynamics of all network users simultaneously and capture the\neffect of individuals' behavior on the global evolution pattern of the social\nnetwork.\n  In this work, we introduce Social Network Distance (SND) - a distance measure\nthat quantifies the \"cost\" of evolution of one snapshot of a social network\ninto another snapshot under various models of polar opinion propagation. SND\nhas a rich semantics of a transportation problem, yet, is computable in time\nlinear in the number of users, which makes SND applicable to the analysis of\nlarge-scale online social networks. In our experiments with synthetic and\nreal-world Twitter data, we demonstrate the utility of our distance measure for\nanomalous event detection. It achieves a true positive rate of 0.83, twice as\nhigh as that of alternatives. When employed for opinion prediction in Twitter,\nour method's accuracy is 75.63%, which is 7.5% higher than that of the next\nbest method.\n  Source Code: https://cs.ucsb.edu/~victor/pub/ucsb/dbl/snd/ \n\n"}
{"id": "1510.05137", "contents": "Title: Integrality Gaps and Approximation Algorithms for Dispersers and\n  Bipartite Expanders Abstract: We study the problem of approximating the quality of a disperser. A bipartite\ngraph $G$ on $([N],[M])$ is a $(\\rho N,(1-\\delta)M)$-disperser if for any\nsubset $S\\subseteq [N]$ of size $\\rho N$, the neighbor set $\\Gamma(S)$ contains\nat least $(1-\\delta)M$ distinct vertices. Our main results are strong\nintegrality gaps in the Lasserre hierarchy and an approximation algorithm for\ndispersers.\n  \\begin{enumerate}\n  \\item For any $\\alpha>0$, $\\delta>0$, and a random bipartite graph $G$ with\nleft degree $D=O(\\log N)$, we prove that the Lasserre hierarchy cannot\ndistinguish whether $G$ is an $(N^{\\alpha},(1-\\delta)M)$-disperser or not an\n$(N^{1-\\alpha},\\delta M)$-disperser.\n  \\item For any $\\rho>0$, we prove that there exist infinitely many constants\n$d$ such that the Lasserre hierarchy cannot distinguish whether a random\nbipartite graph $G$ with right degree $d$ is a $(\\rho N,\n(1-(1-\\rho)^d)M)$-disperser or not a $(\\rho N, (1-\\Omega(\\frac{1-\\rho}{\\rho d +\n1-\\rho}))M)$-disperser. We also provide an efficient algorithm to find a subset\nof size exact $\\rho N$ that has an approximation ratio matching the integrality\ngap within an extra loss of\n$\\frac{\\min\\{\\frac{\\rho}{1-\\rho},\\frac{1-\\rho}{\\rho}\\}}{\\log d}$.\n\\end{enumerate}\n  Our method gives an integrality gap in the Lasserre hierarchy for bipartite\nexpanders with left degree~$D$. $G$ on $([N],[M])$ is a $(\\rho N,a)$-expander\nif for any subset $S\\subseteq [N]$ of size $\\rho N$, the neighbor set\n$\\Gamma(S)$ contains at least $a \\cdot \\rho N$ distinct vertices. We prove that\nfor any constant $\\epsilon>0$, there exist constants $\\epsilon'<\\epsilon,\\rho,$\nand $D$ such that the Lasserre hierarchy cannot distinguish whether a bipartite\ngraph on $([N],[M])$ with left degree $D$ is a $(\\rho N,\n(1-\\epsilon')D)$-expander or not a $(\\rho N, (1-\\epsilon)D)$-expander. \n\n"}
{"id": "1510.08154", "contents": "Title: A faster FPT Algorithm and a smaller Kernel for Block Graph Vertex\n  Deletion Abstract: A graph $G$ is called a \\emph{block graph} if each maximal $2$-connected\ncomponent of $G$ is a clique. In this paper we study the Block Graph Vertex\nDeletion from the perspective of fixed parameter tractable (FPT) and\nkernelization algorithm. In particular, an input to Block Graph Vertex Deletion\nconsists of a graph $G$ and a positive integer $k$ and the objective to check\nwhether there exists a subset $S \\subseteq V(G)$ of size at most $k$ such that\nthe graph induced on $V(G)\\setminus S$ is a block graph. In this paper we give\nan FPT algorithm with running time $4^kn^{O(1)}$ and a polynomial kernel of\nsize $O(k^4)$ for Block Graph Vertex Deletion. The running time of our FPT\nalgorithm improves over the previous best algorithm for the problem that ran in\ntime $10^kn^{O(1)}$ and the size of our kernel reduces over the previously\nknown kernel of size $O(k^9)$. Our results are based on a novel connection\nbetween Block Graph Vertex Deletion and the classical {\\sc Feedback Vertex Set}\nproblem in graphs without induced $C_4$ and $K_4-e$. To achieve our results we\nalso obtain an algorithm for {\\sc Weighted Feedback Vertex Set} running in time\n$3.618^kn^{O(1)}$ and improving over the running time of previously known\nalgorithm with running time $5^kn^{O(1)}$. \n\n"}
{"id": "1511.01699", "contents": "Title: Low Rank Approximation of Binary Matrices: Column Subset Selection and\n  Generalizations Abstract: Low rank matrix approximation is an important tool in machine learning. Given\na data matrix, low rank approximation helps to find factors, patterns and\nprovides concise representations for the data. Research on low rank\napproximation usually focus on real matrices. However, in many applications\ndata are binary (categorical) rather than continuous. This leads to the problem\nof low rank approximation of binary matrix. Here we are given a $d \\times n$\nbinary matrix $A$ and a small integer $k$. The goal is to find two binary\nmatrices $U$ and $V$ of sizes $d \\times k$ and $k \\times n$ respectively, so\nthat the Frobenius norm of $A - U V$ is minimized. There are two models of this\nproblem, depending on the definition of the dot product of binary vectors: The\n$\\mathrm{GF}(2)$ model and the Boolean semiring model. Unlike low rank\napproximation of real matrix which can be efficiently solved by Singular Value\nDecomposition, approximation of binary matrix is $NP$-hard even for $k=1$.\n  In this paper, we consider the problem of Column Subset Selection (CSS), in\nwhich one low rank matrix must be formed by $k$ columns of the data matrix. We\ncharacterize the approximation ratio of CSS for binary matrices. For $GF(2)$\nmodel, we show the approximation ratio of CSS is bounded by\n$\\frac{k}{2}+1+\\frac{k}{2(2^k-1)}$ and this bound is asymptotically tight. For\nBoolean model, it turns out that CSS is no longer sufficient to obtain a bound.\nWe then develop a Generalized CSS (GCSS) procedure in which the columns of one\nlow rank matrix are generated from Boolean formulas operating bitwise on\ncolumns of the data matrix. We show the approximation ratio of GCSS is bounded\nby $2^{k-1}+1$, and the exponential dependency on $k$ is inherent. \n\n"}
{"id": "1511.02235", "contents": "Title: NAND-Trees, Average Choice Complexity, and Effective Resistance Abstract: We show that the quantum query complexity of evaluating NAND-tree instances\nwith average choice complexity at most $W$ is $O(W)$, where average choice\ncomplexity is a measure of the difficulty of winning the associated two-player\ngame. This generalizes a superpolynomial speedup over classical query\ncomplexity due to Zhan et al. [Zhan et al., ITCS 2012, 249-265]. We further\nshow that the player with a winning strategy for the two-player game associated\nwith the NAND-tree can win the game with an expected\n$\\widetilde{O}(N^{1/4}\\sqrt{{\\cal C}(x)})$ quantum queries against a random\nopponent, where ${\\cal C }(x)$ is the average choice complexity of the\ninstance. This gives an improvement over the query complexity of the naive\nstrategy, which costs $\\widetilde{O}(\\sqrt{N})$ queries.\n  The results rely on a connection between NAND-tree evaluation and\n$st$-connectivity problems on certain graphs, and span programs for\n$st$-connectivity problems. Our results follow from relating average choice\ncomplexity to the effective resistance of these graphs, which itself\ncorresponds to the span program witness size. \n\n"}
{"id": "1511.02513", "contents": "Title: Algorithmic Stability for Adaptive Data Analysis Abstract: Adaptivity is an important feature of data analysis---the choice of questions\nto ask about a dataset often depends on previous interactions with the same\ndataset. However, statistical validity is typically studied in a nonadaptive\nmodel, where all questions are specified before the dataset is drawn. Recent\nwork by Dwork et al. (STOC, 2015) and Hardt and Ullman (FOCS, 2014) initiated\nthe formal study of this problem, and gave the first upper and lower bounds on\nthe achievable generalization error for adaptive data analysis.\n  Specifically, suppose there is an unknown distribution $\\mathbf{P}$ and a set\nof $n$ independent samples $\\mathbf{x}$ is drawn from $\\mathbf{P}$. We seek an\nalgorithm that, given $\\mathbf{x}$ as input, accurately answers a sequence of\nadaptively chosen queries about the unknown distribution $\\mathbf{P}$. How many\nsamples $n$ must we draw from the distribution, as a function of the type of\nqueries, the number of queries, and the desired level of accuracy?\n  In this work we make two new contributions:\n  (i) We give upper bounds on the number of samples $n$ that are needed to\nanswer statistical queries. The bounds improve and simplify the work of Dwork\net al. (STOC, 2015), and have been applied in subsequent work by those authors\n(Science, 2015, NIPS, 2015).\n  (ii) We prove the first upper bounds on the number of samples required to\nanswer more general families of queries. These include arbitrary\nlow-sensitivity queries and an important class of optimization queries.\n  As in Dwork et al., our algorithms are based on a connection with algorithmic\nstability in the form of differential privacy. We extend their work by giving a\nquantitatively optimal, more general, and simpler proof of their main theorem\nthat stability implies low generalization error. We also study weaker stability\nguarantees such as bounded KL divergence and total variation distance. \n\n"}
{"id": "1511.02599", "contents": "Title: Waste Makes Haste: Bounded Time Protocols for Envy-Free Cake Cutting\n  with Free Disposal Abstract: We consider the classic problem of envy-free division of a heterogeneous good\n(\"cake\") among several agents. It is known that, when the allotted pieces must\nbe connected, the problem cannot be solved by a finite algorithm for 3 or more\nagents. The impossibility result, however, assumes that the entire cake must be\nallocated. In this paper we replace the entire-allocation requirement with a\nweaker \\emph{partial-proportionality} requirement: the piece given to each\nagent must be worth for it at least a certain positive fraction of the entire\ncake value. We prove that this version of the problem is solvable in bounded\ntime even when the pieces must be connected. We present simple, bounded-time\nenvy-free cake-cutting algorithms for: (1) giving each of $n$ agents a\nconnected piece with a positive value; (2) giving each of 3 agents a connected\npiece worth at least 1/3; (3) giving each of 4 agents a connected piece worth\nat least 1/7; (4) giving each of 4 agents a disconnected piece worth at least\n1/4; (5) giving each of $n$ agents a disconnected piece worth at least\n$(1-\\epsilon)/n$ for any positive $\\epsilon$. \n\n"}
{"id": "1511.02899", "contents": "Title: On the Combinatorial Version of the Slepian-Wolf Problem Abstract: We study the following combinatorial version of the Slepian-Wolf coding\nscheme. Two isolated Senders are given binary strings $X$ and $Y$ respectively;\nthe length of each string is equal to $n$, and the Hamming distance between the\nstrings is at most $\\alpha n$. The Senders compress their strings and\ncommunicate the results to the Receiver. Then the Receiver must reconstruct\nboth strings $X$ and $Y$. The aim is to minimise the lengths of the transmitted\nmessages.\n  For an asymmetric variant of this problem (where one of the Senders transmits\nthe input string to the Receiver without compression) with deterministic\nencoding a nontrivial lower bound was found by A.Orlitsky and K.Viswanathany.\nIn our paper we prove a new lower bound for the schemes with syndrome coding,\nwhere at least one of the Senders uses linear encoding of the input string.\n  For the combinatorial Slepian-Wolf problem with randomized encoding the\ntheoretical optimum of communication complexity was recently found by the first\nauthor, though effective protocols with optimal lengths of messages remained\nunknown. We close this gap and present a polynomial time randomized protocol\nthat achieves the optimal communication complexity. \n\n"}
{"id": "1511.03229", "contents": "Title: Learning Communities in the Presence of Errors Abstract: We study the problem of learning communities in the presence of modeling\nerrors and give robust recovery algorithms for the Stochastic Block Model\n(SBM). This model, which is also known as the Planted Partition Model, is\nwidely used for community detection and graph partitioning in various fields,\nincluding machine learning, statistics, and social sciences. Many algorithms\nexist for learning communities in the Stochastic Block Model, but they do not\nwork well in the presence of errors.\n  In this paper, we initiate the study of robust algorithms for partial\nrecovery in SBM with modeling errors or noise. We consider graphs generated\naccording to the Stochastic Block Model and then modified by an adversary. We\nallow two types of adversarial errors, Feige---Kilian or monotone errors, and\nedge outlier errors. Mossel, Neeman and Sly (STOC 2015) posed an open question\nabout whether an almost exact recovery is possible when the adversary is\nallowed to add $o(n)$ edges. Our work answers this question affirmatively even\nin the case of $k>2$ communities.\n  We then show that our algorithms work not only when the instances come from\nSBM, but also work when the instances come from any distribution of graphs that\nis $\\epsilon m$ close to SBM in the Kullback---Leibler divergence. This result\nalso works in the presence of adversarial errors. Finally, we present almost\ntight lower bounds for two communities. \n\n"}
{"id": "1511.03927", "contents": "Title: Find Your Place: Simple Distributed Algorithms for Community Detection Abstract: Given an underlying graph, we consider the following \\emph{dynamics}:\nInitially, each node locally chooses a value in $\\{-1,1\\}$, uniformly at random\nand independently of other nodes. Then, in each consecutive round, every node\nupdates its local value to the average of the values held by its neighbors, at\nthe same time applying an elementary, local clustering rule that only depends\non the current and the previous values held by the node.\n  We prove that the process resulting from this dynamics produces a clustering\nthat exactly or approximately (depending on the graph) reflects the underlying\ncut in logarithmic time, under various graph models that exhibit a sparse\nbalanced cut, including the stochastic block model. We also prove that a\nnatural extension of this dynamics performs community detection on a\nregularized version of the stochastic block model with multiple communities.\n  Rather surprisingly, our results provide rigorous evidence for the ability of\nan extremely simple and natural dynamics to address a computational problem\nthat is non-trivial even in a centralized setting. \n\n"}
{"id": "1511.05546", "contents": "Title: Complexity and Approximability of Parameterized MAX-CSPs Abstract: We study the optimization version of constraint satisfaction problems\n(Max-CSPs) in the framework of parameterized complexity; the goal is to compute\nthe maximum fraction of constraints that can be satisfied simultaneously. In\nstandard CSPs, we want to decide whether this fraction equals one. The\nparameters we investigate are structural measures, such as the treewidth or the\nclique-width of the variable-constraint incidence graph of the CSP instance.\n  We consider Max-CSPs with the constraint types AND, OR, PARITY, and MAJORITY,\nand with various parameters k, and we attempt to fully classify them into the\nfollowing three cases: 1. The exact optimum can be computed in FPT time. 2. It\nis W[1]-hard to compute the exact optimum, but there is a randomized FPT\napproximation scheme (FPTAS), which computes a $(1-\\epsilon)$-approximation in\ntime $f(k,\\epsilon)\\cdot poly(n)$. 3. There is no FPTAS unless FPT=W[1].\n  For the corresponding standard CSPs, we establish FPT vs. W[1]-hardness\nresults. \n\n"}
{"id": "1511.05680", "contents": "Title: Wishart Mechanism for Differentially Private Principal Components\n  Analysis Abstract: We propose a new input perturbation mechanism for publishing a covariance\nmatrix to achieve $(\\epsilon,0)$-differential privacy. Our mechanism uses a\nWishart distribution to generate matrix noise. In particular, We apply this\nmechanism to principal component analysis. Our mechanism is able to keep the\npositive semi-definiteness of the published covariance matrix. Thus, our\napproach gives rise to a general publishing framework for input perturbation of\na symmetric positive semidefinite matrix. Moreover, compared with the classic\nLaplace mechanism, our method has better utility guarantee. To the best of our\nknowledge, Wishart mechanism is the best input perturbation approach for\n$(\\epsilon,0)$-differentially private PCA. We also compare our work with\nprevious exponential mechanism algorithms in the literature and provide near\noptimal bound while having more flexibility and less computational\nintractability. \n\n"}
{"id": "1511.06558", "contents": "Title: Near-Optimal UGC-hardness of Approximating Max k-CSP_R Abstract: In this paper, we prove an almost-optimal hardness for Max $k$-CSP$_R$ based\non Khot's Unique Games Conjecture (UGC). In Max $k$-CSP$_R$, we are given a set\nof predicates each of which depends on exactly $k$ variables. Each variable can\ntake any value from $1, 2, \\dots, R$. The goal is to find an assignment to\nvariables that maximizes the number of satisfied predicates.\n  Assuming the Unique Games Conjecture, we show that it is NP-hard to\napproximate Max $k$-CSP$_R$ to within factor $2^{O(k \\log k)}(\\log\nR)^{k/2}/R^{k - 1}$ for any $k, R$. To the best of our knowledge, this result\nimproves on all the known hardness of approximation results when $3 \\leq k =\no(\\log R/\\log \\log R)$. In this case, the previous best hardness result was\nNP-hardness of approximating within a factor $O(k/R^{k-2})$ by Chan. When $k =\n2$, our result matches the best known UGC-hardness result of Khot, Kindler,\nMossel and O'Donnell.\n  In addition, by extending an algorithm for Max 2-CSP$_R$ by Kindler, Kolla\nand Trevisan, we provide an $\\Omega(\\log R/R^{k - 1})$-approximation algorithm\nfor Max $k$-CSP$_R$. This algorithm implies that our inapproximability result\nis tight up to a factor of $2^{O(k \\log k)}(\\log R)^{k/2 - 1}$. In comparison,\nwhen $3 \\leq k$ is a constant, the previously known gap was $O(R)$, which is\nsignificantly larger than our gap of $O(\\text{polylog } R)$.\n  Finally, we show that we can replace the Unique Games Conjecture assumption\nwith Khot's $d$-to-1 Conjecture and still get asymptotically the same hardness\nof approximation. \n\n"}
{"id": "1511.07529", "contents": "Title: Calculating the Unrooted Subtree Prune-and-Regraft Distance Abstract: The subtree prune-and-regraft (SPR) distance metric is a fundamental way of\ncomparing evolutionary trees. It has wide-ranging applications, such as to\nstudy lateral genetic transfer, viral recombination, and Markov chain Monte\nCarlo phylogenetic inference. Although the rooted version of SPR distance can\nbe computed relatively efficiently between rooted trees using\nfixed-parameter-tractable maximum agreement forest (MAF) algorithms, no MAF\nformulation is known for the unrooted case. Correspondingly, previous\nalgorithms are unable to compute unrooted SPR distances larger than 7.\n  In this paper, we substantially advance understanding of and computational\nalgorithms for the unrooted SPR distance. First we identify four properties of\noptimal SPR paths, each of which suggests that no MAF formulation exists in the\nunrooted case. Then we introduce the replug distance, a new lower bound on the\nunrooted SPR distance that is amenable to MAF methods, and give an efficient\nfixed-parameter algorithm for calculating it. Finally, we develop a\n\"progressive A*\" search algorithm using multiple heuristics, including the TBR\nand replug distances, to exactly compute the unrooted SPR distance. Our\nalgorithm is nearly two orders of magnitude faster than previous methods on\nsmall trees, and allows computation of unrooted SPR distances as large as 14 on\ntrees with 50 leaves. \n\n"}
{"id": "1511.07605", "contents": "Title: On the Computational Complexity of Limit Cycles in Dynamical Systems Abstract: We study the Poincare-Bendixson theorem for two-dimensional continuous\ndynamical systems in compact domains from the point of view of computation,\nseeking algorithms for finding the limit cycle promised by this classical\nresult. We start by considering a discrete analogue of this theorem and show\nthat both finding a point on a limit cycle, and determining if a given point is\non one, are PSPACE-complete.\n  For the continuous version, we show that both problems are uncomputable in\nthe real complexity sense; i.e., their complexity is arbitrarily high.\nSubsequently, we introduce a notion of an \"approximate cycle\" and prove an\n\"approximate\" Poincar\\'e-Bendixson theorem guaranteeing that some orbits come\nvery close to forming a cycle in the absence of approximate fixpoints;\nsurprisingly, it holds for all dimensions. The corresponding computational\nproblem defined in terms of arithmetic circuits is PSPACE-complete. \n\n"}
{"id": "1511.08682", "contents": "Title: Polynomials, Quantum Query Complexity, and Grothendieck's Inequality Abstract: We show an equivalence between 1-query quantum algorithms and representations\nby degree-2 polynomials. Namely, a partial Boolean function $f$ is computable\nby a 1-query quantum algorithm with error bounded by $\\epsilon<1/2$ iff $f$ can\nbe approximated by a degree-2 polynomial with error bounded by $\\epsilon'<1/2$.\nThis result holds for two different notions of approximation by a polynomial:\nthe standard definition of Nisan and Szegedy and the approximation by\nblock-multilinear polynomials recently introduced by Aaronson and Ambainis\n(STOC'2015, arxiv:1411.5729).\n  We also show two results for polynomials of higher degree. First, there is a\ntotal Boolean function which requires $\\tilde{\\Omega}(n)$ quantum queries but\ncan be represented by a block-multilinear polynomial of degree\n$\\tilde{O}(\\sqrt{n})$. Thus, in the general case (for an arbitrary number of\nqueries), block-multilinear polynomials are not equivalent to quantum\nalgorithms.\n  Second, for any constant degree $k$, the two notions of approximation by a\npolynomial (the standard and the block-multilinear) are equivalent. As a\nconsequence, we solve an open problem of Aaronson and Ambainis, showing that\none can estimate the value of any bounded degree-$k$ polynomial $p:\\{0, 1\\}^n\n\\rightarrow [-1, 1]$ with $O(n^{1-\\frac{1}{2k}})$ queries. \n\n"}
{"id": "1511.09229", "contents": "Title: Efficient Deterministic Single Round Document Exchange for Edit Distance Abstract: Suppose that we have two parties that possess each a binary string. Suppose\nthat the length of the first string (document) is $n$ and that the two strings\n(documents) have edit distance (minimal number of deletes, inserts and\nsubstitutions needed to transform one string into the other) at most $k$. The\nproblem we want to solve is to devise an efficient protocol in which the first\nparty sends a single message that allows the second party to guess the first\nparty's string. In this paper we show an efficient deterministic protocol for\nthis problem. The protocol runs in time $O(n\\cdot \\mathtt{polylog}(n))$ and has\nmessage size $O(k^2+k\\log^2n)$ bits. To the best of our knowledge, ours is the\nfirst efficient deterministic protocol for this problem, if efficiency is\nmeasured in both the message size and the running time. As an immediate\napplication of our new protocol, we show a new error correcting code that is\nefficient even for large numbers of (adversarial) edit errors. \n\n"}
{"id": "1512.00775", "contents": "Title: Uncertainty Principle and Sampling of Signals Defined on Graphs Abstract: In many applications, from sensor to social networks, gene regulatory\nnetworks or big data, observations can be represented as a signal defined over\nthe vertices of a graph. Building on the recently introduced Graph Fourier\nTransform, the first contribution of this paper is to provide an uncertainty\nprinciple for signals on graph. As a by-product of this theory, we show how to\nbuild a dictionary of maximally concentrated signals on vertex/frequency\ndomains. Then, we establish a direct relation between uncertainty principle and\nsampling, which forms the basis for a sampling theorem of signals defined on\ngraph. Based on this theory, we show that, besides sampling rate, the samples'\nlocation plays a key role in the performance of signal recovery algorithms.\nHence, we suggest a few alternative sampling strategies and compare them with\nrecently proposed methods. \n\n"}
{"id": "1512.01996", "contents": "Title: A bloated FM-index reducing the number of cache misses during the search Abstract: The FM-index is a well-known compressed full-text index, based on the\nBurrows-Wheeler transform (BWT). During a pattern search, the BWT sequence is\naccessed at \"random\" locations, which is cache-unfriendly. In this paper, we\nare interested in speeding up the FM-index by working on $q$-grams rather than\nindividual characters, at the cost of using more space. The first presented\nvariant is related to an inverted index on $q$-grams, yet the occurrence lists\nin our solution are in the sorted suffix order rather than text order in a\ntraditional inverted index. This variant obtains $O(m/|CL| + \\log n \\log m)$\ncache misses in the worst case, where $n$ and $m$ are the text and pattern\nlengths, respectively, and $|CL|$ is the CPU cache line size, in symbols\n(typically 64 in modern hardware). This index is often several times faster\nthan the fastest known FM-indexes (especially for long patterns), yet the space\nrequirements are enormous, $O(n\\log^2 n)$ bits in theory and about $80n$-$95n$\nbytes in practice. For this reason, we dub our approach FM-bloated. The second\npresented variant requires $O(n\\log n)$ bits of space. \n\n"}
{"id": "1512.03531", "contents": "Title: Constructive noncommutative rank computation is in deterministic\n  polynomial time Abstract: We extend our techniques developed in our earlier paper appeared in\nComputational Complexity, 2017 (preprint: arXiv:1508.00690) to obtain a\ndeterministic polynomial time algorithm for computing the non-commutative rank\ntogether with certificates of linear spaces of matrices over sufficiently large\nbase fields.\n  The key new idea is a reduction procedure that keeps the blow-up parameter\nsmall, and there are two methods to implement this idea: the first one is a\ngreedy argument that removes certain rows and columns, and the second one is an\nefficient algorithmic version of a result of Derksen and Makam. Both methods\nrely crucially on the regularity lemma in our aforementioned paper, and in this\nmanuscript we also improve that lemma by removing a coprime condition there. \n\n"}
{"id": "1512.04633", "contents": "Title: Efficient Algorithms for Personalized PageRank Abstract: We present new, more efficient algorithms for estimating random walk scores\nsuch as Personalized PageRank from a given source node to one or several target\nnodes. These scores are useful for personalized search and recommendations on\nnetworks including social networks, user-item networks, and the web. Past work\nhas proposed using Monte Carlo or using linear algebra to estimate scores from\na single source to every target, making them inefficient for a single pair. Our\ncontribution is a new bidirectional algorithm which combines linear algebra and\nMonte Carlo to achieve significant speed improvements. On a diverse set of six\ngraphs, our algorithm is 70x faster than past state-of-the-art algorithms. We\nalso present theoretical analysis: while past algorithms require $\\Omega(n)$\ntime to estimate a random walk score of typical size $\\frac{1}{n}$ on an\n$n$-node graph to a given constant accuracy, our algorithm requires only\n$O(\\sqrt{m})$ expected time for an average target, where $m$ is the number of\nedges, and is provably accurate.\n  In addition to our core bidirectional estimator for personalized PageRank, we\npresent an alternative algorithm for undirected graphs, a generalization to\narbitrary walk lengths and Markov Chains, an algorithm for personalized search\nranking, and an algorithm for sampling random paths from a given source to a\ngiven set of targets. We expect our bidirectional methods can be extended in\nother ways and will be useful subroutines in other graph analysis problems. \n\n"}
{"id": "1512.05996", "contents": "Title: Advice Complexity of the Online Induced Subgraph Problem Abstract: Several well-studied graph problems aim to select a largest (or smallest)\ninduced subgraph with a given property of the input graph. Examples of such\nproblems include maximum independent set, maximum planar graph, and many\nothers. We consider these problems, where the vertices are presented online.\nWith each vertex, the online algorithm must decide whether to include it into\nthe constructed subgraph, based only on the subgraph induced by the vertices\npresented so far. We study the properties that are common to all these problems\nby investigating the generalized problem: for a hereditary property \\pty, find\nsome maximal induced subgraph having \\pty. We study this problem from the point\nof view of advice complexity. Using a result from Boyar et al. [STACS 2015], we\ngive a tight trade-off relationship stating that for inputs of length n roughly\nn/c bits of advice are both needed and sufficient to obtain a solution with\ncompetitive ratio c, regardless of the choice of \\pty, for any c (possibly a\nfunction of n). Surprisingly, a similar result cannot be obtained for the\nsymmetric problem: for a given cohereditary property \\pty, find a minimum\nsubgraph having \\pty. We show that the advice complexity of this problem varies\nsignificantly with the choice of \\pty.\n  We also consider preemptive online model, where the decision of the algorithm\nis not completely irreversible. In particular, the algorithm may discard some\nvertices previously assigned to the constructed set, but discarded vertices\ncannot be reinserted into the set again. We show that, for the maximum induced\nsubgraph problem, preemption cannot help much, giving a lower bound of\n$\\Omega(n/(c^2\\log c))$ bits of advice needed to obtain competitive ratio $c$,\nwhere $c$ is any increasing function bounded by \\sqrt{n/log n}. We also give a\nlinear lower bound for c close to 1. \n\n"}
{"id": "1512.06427", "contents": "Title: Towards Integrated Glance To Restructuring in Combinatorial Optimization Abstract: The paper focuses on a new class of combinatorial problems which consists in\nrestructuring of solutions (as sets/structures) in combinatorial optimization.\nTwo main features of the restructuring process are examined: (i) a cost of the\nrestructuring, (ii) a closeness to a goal solution. Three types of the\nrestructuring problems are under study: (a) one-stage structuring, (b)\nmulti-stage structuring, and (c) structuring over changed element set.\nOne-criterion and multicriteria problem formulations can be considered. The\nrestructuring problems correspond to redesign (improvement, upgrade) of modular\nsystems or solutions. The restructuring approach is described and illustrated\n(problem statements, solving schemes, examples) for the following combinatorial\noptimization problems: knapsack problem, multiple choice problem, assignment\nproblem, spanning tree problems, clustering problem, multicriteria ranking\n(sorting) problem, morphological clique problem. Numerical examples illustrate\nthe restructuring problems and solving schemes. \n\n"}
{"id": "1512.07892", "contents": "Title: Further extensions of Clifford circuits and their classical simulation\n  complexities Abstract: Extended Clifford circuits straddle the boundary between classical and\nquantum computational power. Whether such circuits are efficiently classically\nsimulable seems to depend delicately on the ingredients of the circuits. While\nsome combinations of ingredients lead to efficiently classically simulable\ncircuits, other combinations, which might just be slightly different, lead to\ncircuits which are likely not. We extend the results of Jozsa and Van den Nest\n[Quant. Info. Comput. 14, 633 (2014)] by studying two further extensions of\nClifford circuits. First, we consider how the classical simulation complexity\nchanges when we allow for more general measurements. Second, we investigate\ndifferent notions of what it means to \"classically simulate\" a quantum circuit.\nThese further extensions give us 24 new combinations of ingredients compared to\nJozsa and Van den Nest, and we give a complete classification of their\nclassical simulation complexities. Our results provide more examples where\nseemingly modest changes to the ingredients of Clifford circuits lead to\n\"large\" changes in the classical simulation complexities of the circuits, and\nalso include new examples of extended Clifford circuits that exhibit \"quantum\nsupremacy\", in the sense that it is not possible to efficiently classically\nsample from the output distributions of such circuits, unless the polynomial\nhierarchy collapses. \n\n"}
{"id": "1601.00479", "contents": "Title: Motivating Time-Inconsistent Agents: A Computational Approach Abstract: In this paper we investigate the computational complexity of motivating\ntime-inconsistent agents to complete long term projects. We resort to an\nelegant graph-theoretic model, introduced by Kleinberg and Oren, which consists\nof a task graph $G$ with $n$ vertices, including a source $s$ and target $t$,\nand an agent that incrementally constructs a path from $s$ to $t$ in order to\ncollect rewards. The twist is that the agent is present-biased and discounts\nfuture costs and rewards by a factor $\\beta\\in [0,1]$. Our design objective is\nto ensure that the agent reaches $t$ i.e.\\ completes the project, for as little\nreward as possible. Such graphs are called motivating. We consider two\nstrategies. First, we place a single reward $r$ at $t$ and try to guide the\nagent by removing edges from $G$. We prove that deciding the existence of such\nmotivating subgraphs is NP-complete if $r$ is fixed. More importantly, we\ngeneralize our reduction to a hardness of approximation result for computing\nthe minimum $r$ that admits a motivating subgraph. In particular, we show that\nno polynomial-time approximation to within a ratio of $\\sqrt{n}/4$ or less is\npossible, unless ${\\rm P}={\\rm NP}$. Furthermore, we develop a\n$(1+\\sqrt{n})$-approximation algorithm and thus settle the approximability of\ncomputing motivating subgraphs. Secondly, we study motivating reward\nconfigurations, where non-negative rewards $r(v)$ may be placed on arbitrary\nvertices $v$ of $G$. The agent only receives the rewards of visited vertices.\nAgain we give an NP-completeness result for deciding the existence of a\nmotivating reward configuration within a fixed budget $b$. This result even\nholds if $b=0$, which in turn implies that no efficient approximation of a\nminimum $b$ within a ration grater or equal to $1$ is possible, unless ${\\rm\nP}={\\rm NP}$. \n\n"}
{"id": "1601.01067", "contents": "Title: A Polynomial-time Algorithm to Compute Generalized Hermite Normal Form\n  of Matrices over Z[x] Abstract: In this paper, a polynomial-time algorithm is given to compute the\ngeneralized Hermite normal form for a matrix F over Z[x], or equivalently, the\nreduced Groebner basis of the Z[x]-module generated by the column vectors of F.\nThe algorithm is also shown to be practically more efficient than existing\nalgorithms. The algorithm is based on three key ingredients. First, an F4 style\nalgorithm to compute the Groebner basis is adopted, where a novel prolongation\nis designed such that the coefficient matrices under consideration have\npolynomial sizes. Second, fast algorithms to compute Hermite normal forms of\nmatrices over Z are used. Third, the complexity of the algorithm are guaranteed\nby a nice estimation for the degree and height bounds of the polynomials in the\ngeneralized Hermite normal form. \n\n"}
{"id": "1601.01744", "contents": "Title: Performance of QAOA on Typical Instances of Constraint Satisfaction\n  Problems with Bounded Degree Abstract: We consider constraint satisfaction problems of bounded degree, with a good\nnotion of \"typicality\", e.g. the negation of the variables in each constraint\nis taken independently at random. Using the quantum approximate optimization\nalgorithm (QAOA), we show that $ \\mu+\\Omega(1/\\sqrt{D}) $ fraction of the\nconstraints can be satisfied for typical instances, with the assignment\nefficiently produced by QAOA. We do so by showing that the averaged fraction of\nconstraints being satisfied is $ \\mu+\\Omega(1/\\sqrt{D}) $, with small variance.\nHere $ \\mu $ is the fraction that would be satisfied by a uniformly random\nassignment, and $ D $ is the number of constraints that each variable can\nappear. CSPs with typicality include Max-$ k $XOR and Max-$ k $SAT. We point\nout how it can be applied to determine the typical ground-state energy of some\nlocal Hamiltonians. We also give a similar result for instances with \"no\noverlapping constraints\", using the quantum algorithm. We sketch how the\nclassical algorithm might achieve some partial result. \n\n"}
{"id": "1601.02867", "contents": "Title: A Sidetrack-Based Algorithm for Finding the k Shortest Simple Paths in a\n  Directed Graph Abstract: We present an algorithm for the k shortest simple path problem on weighted\ndirected graphs (kSSP) that is based on Eppstein's algorithm for a similar\nproblem in which paths are allowed to contain cycles. In contrast to most other\nalgorithms for kSSP, ours is not based on Yen's algorithm and does not solve\nreplacement path problems. Its worst-case running time is on par with\nstate-of-the-art algorithms for kSSP. Using our algorithm, one may find O(m)\nsimple paths with a single shortest path tree computation and O(n + m)\nadditional time per path in well-behaved cases, where n is the number of nodes\nand m is the number of edges. Our computational results show that on random\ngraphs and large road networks, these well-behaved cases are quite common and\nour algorithm is faster than existing algorithms by an order of magnitude.\nFurther, the running time is far better predictable due to very small\ndispersion. \n\n"}
{"id": "1601.04974", "contents": "Title: Nonbinary tree-based phylogenetic networks Abstract: Rooted phylogenetic networks are used to describe evolutionary histories that\ncontain non-treelike evolutionary events such as hybridization and horizontal\ngene transfer. In some cases, such histories can be described by a phylogenetic\nbase-tree with additional linking arcs, which can for example represent gene\ntransfer events. Such phylogenetic networks are called tree-based. Here, we\nconsider two possible generalizations of this concept to nonbinary networks,\nwhich we call tree-based and strictly-tree-based nonbinary phylogenetic\nnetworks. We give simple graph-theoretic characterizations of tree-based and\nstrictly-tree-based nonbinary phylogenetic networks. Moreover, we show for each\nof these two classes that it can be decided in polynomial time whether a given\nnetwork is contained in the class. Our approach also provides a new view on\ntree-based binary phylogenetic networks. Finally, we discuss two examples of\nnonbinary phylogenetic networks in biology and show how our results can be\napplied to them. \n\n"}
{"id": "1601.05047", "contents": "Title: Proving Differential Privacy via Probabilistic Couplings Abstract: In this paper, we develop compositional methods for formally verifying\ndifferential privacy for algorithms whose analysis goes beyond the composition\ntheorem. Our methods are based on the observation that differential privacy has\ndeep connections with a generalization of probabilistic couplings, an\nestablished mathematical tool for reasoning about stochastic processes. Even\nwhen the composition theorem is not helpful, we can often prove privacy by a\ncoupling argument.\n  We demonstrate our methods on two algorithms: the Exponential mechanism and\nthe Above Threshold algorithm, the critical component of the famous Sparse\nVector algorithm. We verify these examples in a relational program logic\napRHL+, which can construct approximate couplings. This logic extends the\nexisting apRHL logic with more general rules for the Laplace mechanism and the\none-sided Laplace mechanism, and new structural rules enabling pointwise\nreasoning about privacy; all the rules are inspired by the connection with\ncoupling. While our paper is presented from a formal verification perspective,\nwe believe that its main insight is of independent interest for the\ndifferential privacy community. \n\n"}
{"id": "1601.05557", "contents": "Title: A New Approach for Testing Properties of Discrete Distributions Abstract: In this work, we give a novel general approach for distribution testing. We\ndescribe two techniques: our first technique gives sample-optimal testers,\nwhile our second technique gives matching sample lower bounds. As a\nconsequence, we resolve the sample complexity of a wide variety of testing\nproblems.\n  Our upper bounds are obtained via a modular reduction-based approach. Our\napproach yields optimal testers for numerous problems by using a standard\n$\\ell_2$-identity tester as a black-box. Using this recipe, we obtain simple\nestimators for a wide range of problems, encompassing most problems previously\nstudied in the TCS literature, namely: (1) identity testing to a fixed\ndistribution, (2) closeness testing between two unknown distributions (with\nequal/unequal sample sizes), (3) independence testing (in any number of\ndimensions), (4) closeness testing for collections of distributions, and (5)\ntesting histograms. For all of these problems, our testers are sample-optimal,\nup to constant factors. With the exception of (1), ours are the {\\em first\nsample-optimal testers for the corresponding problems.} Moreover, our\nestimators are significantly simpler to state and analyze compared to previous\nresults.\n  As an application of our reduction-based technique, we obtain the first {\\em\nnearly instance-optimal} algorithm for testing equivalence between two {\\em\nunknown} distributions. Moreover, our technique naturally generalizes to other\nmetrics beyond the $\\ell_1$-distance.\n  Our lower bounds are obtained via a direct information-theoretic approach:\nGiven a candidate hard instance, our proof proceeds by bounding the mutual\ninformation between appropriate random variables. While this is a classical\nmethod in information theory, prior to our work, it had not been used in\ndistribution property testing. \n\n"}
{"id": "1602.01819", "contents": "Title: A short note on Merlin-Arthur protocols for subset sum Abstract: In the subset sum problem we are given n positive integers along with a\ntarget integer t. A solution is a subset of these integers summing to t. In\nthis short note we show that for a given subset sum instance there is a proof\nof size $O^*(\\sqrt{t})$ of what the number of solutions is that can be\nconstructed in $O^*(t)$ time and can be probabilistically verified in time\n$O^*(\\sqrt{t})$ with at most constant error probability. Here, the $O^*()$\nnotation omits factors polynomial in the input size $n\\log(t)$. \n\n"}
{"id": "1602.02018", "contents": "Title: Compressive Spectral Clustering Abstract: Spectral clustering has become a popular technique due to its high\nperformance in many contexts. It comprises three main steps: create a\nsimilarity graph between N objects to cluster, compute the first k eigenvectors\nof its Laplacian matrix to define a feature vector for each object, and run\nk-means on these features to separate objects into k classes. Each of these\nthree steps becomes computationally intensive for large N and/or k. We propose\nto speed up the last two steps based on recent results in the emerging field of\ngraph signal processing: graph filtering of random signals, and random sampling\nof bandlimited graph signals. We prove that our method, with a gain in\ncomputation time that can reach several orders of magnitude, is in fact an\napproximation of spectral clustering, for which we are able to control the\nerror. We test the performance of our method on artificial and real-world\nnetwork data. \n\n"}
{"id": "1602.02211", "contents": "Title: Fuzzy Maximum Satisfiability Abstract: In this paper, we extend the Maximum Satisfiability (MaxSAT) problem to\n{\\L}ukasiewicz logic. The MaxSAT problem for a set of formulae {\\Phi} is the\nproblem of finding an assignment to the variables in {\\Phi} that satisfies the\nmaximum number of formulae. Three possible solutions (encodings) are proposed\nto the new problem: (1) Disjunctive Linear Relations (DLRs), (2) Mixed Integer\nLinear Programming (MILP) and (3) Weighted Constraint Satisfaction Problem\n(WCSP). Like its Boolean counterpart, the extended fuzzy MaxSAT will have\nnumerous applications in optimization problems that involve vagueness. \n\n"}
{"id": "1602.02841", "contents": "Title: Combinatorial Scoring of Phylogenetic Networks Abstract: Construction of phylogenetic trees and networks for extant species from their\ncharacters represents one of the key problems in phylogenomics. While solution\nto this problem is not always uniquely defined and there exist multiple methods\nfor tree/network construction, it becomes important to measure how well the\nconstructed networks capture the given character relationship across the\nspecies.\n  In the current study, we propose a novel method for measuring the specificity\nof a given phylogenetic network in terms of the total number of distributions\nof character states at the leaves that the network may impose. While for binary\nphylogenetic trees, this number has an exact formula and depends only on the\nnumber of leaves and character states but not on the tree topology, the\nsituation is much more complicated for non-binary trees or networks.\nNevertheless, we develop an algorithm for combinatorial enumeration of such\ndistributions, which is applicable for arbitrary trees and networks under some\nreasonable assumptions. \n\n"}
{"id": "1602.04181", "contents": "Title: Spectral Alignment of Graphs Abstract: Graph alignment refers to the problem of finding a bijective mapping across\nvertices of two graphs such that, if two nodes are connected in the first\ngraph, their images are connected in the second graph. This problem arises in\nmany fields such as computational biology, social sciences, and computer vision\nand is often cast as a quadratic assignment problem (QAP). Most standard graph\nalignment methods consider an optimization that maximizes the number of matches\nbetween the two graphs, ignoring the effect of mismatches. We propose a\ngeneralized graph alignment formulation that considers both matches and\nmismatches in a standard QAP formulation. This modification can have a major\nimpact in aligning graphs with different sizes and heterogenous edge densities.\nMoreover, we propose two methods for solving the generalized graph alignment\nproblem based on spectral decomposition of matrices. We compare the performance\nof proposed methods with some existing graph alignment algorithms including\nNatalie2, GHOST, IsoRank, NetAlign, Klau's approach as well as a semidefinite\nprogramming-based method over various synthetic and real graph models. Our\nproposed method based on simultaneous alignment of multiple eigenvectors leads\nto consistently good performance in different graph models. In particular, in\nthe alignment of regular graph structures which is one of the most difficult\ngraph alignment cases, our proposed method significantly outperforms other\nmethods. \n\n"}
{"id": "1602.04275", "contents": "Title: Equations for secant varieties of Chow varieties Abstract: The Chow variety of polynomials that decompose as a product of linear forms\nhas been studied for more than 100 years. Finding equations in the ideal of\nsecant varieties of Chow varieties would enable one to measure the complexity\nthe permanent to prove Valiant's conjecture $\\mathbf{VP\\neq VNP}$. In this\narticle, I use the method of prolongation to obtain equations for secant\nvarieties of Chow varieties as $GL(V)$-modules. \n\n"}
{"id": "1602.04478", "contents": "Title: Subgraph Counting: Color Coding Beyond Trees Abstract: The problem of counting occurrences of query graphs in a large data graph,\nknown as subgraph counting, is fundamental to several domains such as genomics\nand social network analysis. Many important special cases (e.g. triangle\ncounting) have received significant attention. Color coding is a very general\nand powerful algorithmic technique for subgraph counting. Color coding has been\nshown to be effective in several applications, but scalable implementations are\nonly known for the special case of {\\em tree queries} (i.e. queries of\ntreewidth one).\n  In this paper we present the first efficient distributed implementation for\ncolor coding that goes beyond tree queries: our algorithm applies to any query\ngraph of treewidth $2$. Since tree queries can be solved in time linear in the\nsize of the data graph, our contribution is the first step into the realm of\ncolour coding for queries that require superlinear running time in the worst\ncase. This superlinear complexity leads to significant load balancing problems\non graphs with heavy tailed degree distributions. Our algorithm structures the\ncomputation to work around high degree nodes in the data graph, and achieves\nvery good runtime and scalability on a diverse collection of data and query\ngraph pairs as a result. We also provide theoretical analysis of our\nalgorithmic techniques, showing asymptotic improvements in runtime on random\ngraphs with power law degree distributions, a popular model for real world\ngraphs. \n\n"}
{"id": "1602.04995", "contents": "Title: On the Density of non-Simple 3-Planar Graphs Abstract: A \\emph{$k$-planar graph} is a graph that can be drawn in the plane such that\nevery edge is crossed at most $k$ times. For $k \\leq 4$, Pach and T\\'oth proved\na bound of $(k+3)(n-2)$ on the total number of edges of a $k$-planar graph,\nwhich is tight for $k=1,2$. For $k=3$, the bound of $6n-12$ has been improved\nto $\\frac{11}{2}n-11$ and has been shown to be optimal up to an additive\nconstant for simple graphs. In this paper, we prove that the bound of\n$\\frac{11}{2}n-11$ edges also holds for non-simple $3$-planar graphs that admit\ndrawings in which non-homotopic parallel edges and self-loops are allowed.\nBased on this result, a characterization of \\emph{optimal $3$-planar graphs}\n(that is, $3$-planar graphs with $n$ vertices and exactly $\\frac{11}{2}n-11$\nedges) might be possible, as to the best of our knowledge the densest known\nsimple $3$-planar is not known to be optimal. \n\n"}
{"id": "1602.05897", "contents": "Title: Toward Deeper Understanding of Neural Networks: The Power of\n  Initialization and a Dual View on Expressivity Abstract: We develop a general duality between neural networks and compositional\nkernels, striving towards a better understanding of deep learning. We show that\ninitial representations generated by common random initializations are\nsufficiently rich to express all functions in the dual kernel space. Hence,\nthough the training objective is hard to optimize in the worst case, the\ninitial weights form a good starting point for optimization. Our dual view also\nreveals a pragmatic and aesthetic perspective of neural networks and\nunderscores their expressive power. \n\n"}
{"id": "1602.05899", "contents": "Title: An algorithm for the weighted metric dimension of two-dimensional grids Abstract: A two-dimensional grid consists of vertices of the form (i,j) for 1 \\leq i\n\\leq m and 1 \\leq j \\leq n, for fixed m,n > 1. Two vertices are adjacent if the\n\\ell_1 distance between their vectors is equal to 1. A landmark set is a subset\nof vertices L \\subseteq V, such that for any distinct pair of vertices u,v \\in\nV, there exists a vertex of L whose distances to u and v are not equal. We\ndesign an efficient algorithm for finding a minimum landmark set with respect\nto total cost in a grid graph with non-negative costs defined on the vertices. \n\n"}
{"id": "1602.07570", "contents": "Title: Bayesian Exploration: Incentivizing Exploration in Bayesian Games Abstract: We consider a ubiquitous scenario in the Internet economy when individual\ndecision-makers (henceforth, agents) both produce and consume information as\nthey make strategic choices in an uncertain environment. This creates a\nthree-way tradeoff between exploration (trying out insufficiently explored\nalternatives to help others in the future), exploitation (making optimal\ndecisions given the information discovered by other agents), and incentives of\nthe agents (who are myopically interested in exploitation, while preferring the\nothers to explore). We posit a principal who controls the flow of information\nfrom agents that came before, and strives to coordinate the agents towards a\nsocially optimal balance between exploration and exploitation, not using any\nmonetary transfers. The goal is to design a recommendation policy for the\nprincipal which respects agents' incentives and minimizes a suitable notion of\nregret.\n  We extend prior work in this direction to allow the agents to interact with\none another in a shared environment: at each time step, multiple agents arrive\nto play a Bayesian game, receive recommendations, choose their actions, receive\ntheir payoffs, and then leave the game forever. The agents now face two sources\nof uncertainty: the actions of the other agents and the parameters of the\nuncertain game environment.\n  Our main contribution is to show that the principal can achieve constant\nregret when the utilities are deterministic (where the constant depends on the\nprior distribution, but not on the time horizon), and logarithmic regret when\nthe utilities are stochastic. As a key technical tool, we introduce the concept\nof explorable actions, the actions which some incentive-compatible policy can\nrecommend with non-zero probability. We show how the principal can identify\n(and explore) all explorable actions, and use the revealed information to\nperform optimally. \n\n"}
{"id": "1602.08656", "contents": "Title: Quantum Arthur-Merlin with single-qubit measurements Abstract: We show that the class QAM does not change even if the verifier's ability is\nrestricted to only single-qubit measurements. To show the result, we use the\nidea of the measurement-based quantum computing: the verifier, who can do only\nsingle-qubit measurements, can test the graph state sent from the prover and\nuse it for his measurement-based quantum computing. We also introduce a new\nQMA-complete problem related to the stabilizer test. \n\n"}
{"id": "1603.00254", "contents": "Title: On the hardness of switching to a small number of edges Abstract: Seidel's switching is a graph operation which makes a given vertex adjacent\nto precisely those vertices to which it was non-adjacent before, while keeping\nthe rest of the graph unchanged. Two graphs are called switching-equivalent if\none can be made isomorphic to the other one by a sequence of switches.\n  Jel\\'inkov\\'a et al. [DMTCS 13, no. 2, 2011] presented a proof that it is\nNP-complete to decide if the input graph can be switched to contain at most a\ngiven number of edges. There turns out to be a flaw in their proof. We present\na correct proof.\n  Furthermore, we prove that the problem remains NP-complete even when\nrestricted to graphs whose density is bounded from above by an arbitrary fixed\nconstant. This partially answers a question of Matou\\v{s}ek and Wagner\n[Discrete Comput. Geom. 52, no. 1, 2014]. \n\n"}
{"id": "1603.01107", "contents": "Title: Minimization of B\\\"uchi Automata using Fair Simulation Abstract: We present an algorithm, which reduces the size of B\\\"uchi automata using\nfair simulation. Its time complexity is $\\mathcal{O}(|Q|^4 \\cdot |\\Delta|^2)$,\nthe space complexity is $\\mathcal{O}(|Q| \\cdot |\\Delta|)$.\n  Simulation is a common approach for minimizing $\\omega$-automata such as\nB\\\"uchi automata. Direct simulation, delayed simulation and fair simulation are\ndifferent types of simulation. As we will show, minimization based on direct or\ndelayed simulation is conceptually simple. Whereas the algorithm based on fair\nsimulation is more complex. However, fair simulation allows a stronger\nminimization of the automaton.\n  Further, we illustrate the theory behind the algorithm, cover optimizations\nuseful in practice, give experimental results and compare our technique to\nother minimization strategies. \n\n"}
{"id": "1603.05620", "contents": "Title: A Moment Majorization principle for random matrix ensembles Abstract: We prove a moment majorization principle for matrix-valued functions with\ndomain $\\{-1,1\\}^{m}$, $m\\in\\mathbb{N}$. The principle is an inequality between\nhigher-order moments of a non-commutative multilinear polynomial with different\nrandom matrix ensemble inputs, where each variable has small influence and the\nvariables are instantiated independently.\n  This technical result can be interpreted as a noncommutative generalization\nof one of the two inequalities of the seminal invariance principle of Mossel,\nO'Donnell and Oleszkiewicz. Applications to noncommutative noise stability and\nnoncommutative anticoncentration are given. \n\n"}
{"id": "1603.06505", "contents": "Title: Characterizations of symmetrically partial Boolean functions with exact\n  quantum query complexity Abstract: We give and prove an optimal exact quantum query algorithm with complexity\n$k+1$ for computing the promise problem (i.e., symmetric and partial Boolean\nfunction) $DJ_n^k$ defined as: $DJ_n^k(x)=1$ for $|x|=n/2$, $DJ_n^k(x)=0$ for\n$|x|$ in the set $\\{0, 1,\\ldots, k, n-k, n-k+1,\\ldots,n\\}$, and it is undefined\nfor the rest cases, where $n$ is even, $|x|$ is the Hamming weight of $x$. The\ncase of $k=0$ is the well-known Deutsch-Jozsa problem. We outline all symmetric\n(and partial) Boolean functions with degrees 1 and 2, and prove their exact\nquantum query complexity. Then we prove that any symmetrical (and partial)\nBoolean function $f$ has exact quantum 1-query complexity if and only if $f$\ncan be computed by the Deutsch-Jozsa algorithm. We also discover the optimal\nexact quantum 2-query complexity for distinguishing between inputs of Hamming\nweight $\\{ \\lfloor n/2\\rfloor, \\lceil n/2\\rceil \\}$ and Hamming weight in the\nset $\\{ 0, n\\}$ for all odd $n$. In addition, a method is provided to determine\nthe degree of any symmetrical (and partial) Boolean function. \n\n"}
{"id": "1603.06985", "contents": "Title: A Quantum Version of Sch\\\"oning's Algorithm Applied to Quantum 2-SAT Abstract: We study a quantum algorithm that consists of a simple quantum Markov\nprocess, and we analyze its behavior on restricted versions of Quantum 2-SAT.\nWe prove that the algorithm solves this decision problem with high probability\nfor n qubits, L clauses, and promise gap c in time O(n^2 L^2 c^{-2}). If the\nHamiltonian is additionally polynomially gapped, our algorithm efficiently\nproduces a state that has high overlap with the satisfying subspace. The Markov\nprocess we study is a quantum analogue of Sch\\\"oning's probabilistic algorithm\nfor k-SAT. \n\n"}
{"id": "1603.07796", "contents": "Title: Approximate Personalized PageRank on Dynamic Graphs Abstract: We propose and analyze two algorithms for maintaining approximate\nPersonalized PageRank (PPR) vectors on a dynamic graph, where edges are added\nor deleted. Our algorithms are natural dynamic versions of two known local\nvariations of power iteration. One, Forward Push, propagates probability mass\nforwards along edges from a source node, while the other, Reverse Push,\npropagates local changes backwards along edges from a target. In both\nvariations, we maintain an invariant between two vectors, and when an edge is\nupdated, our algorithm first modifies the vectors to restore the invariant,\nthen performs any needed local push operations to restore accuracy.\n  For Reverse Push, we prove that for an arbitrary directed graph in a random\nedge model, or for an arbitrary undirected graph, given a uniformly random\ntarget node $t$, the cost to maintain a PPR vector to $t$ of additive error\n$\\varepsilon$ as $k$ edges are updated is $O(k + \\bar{d} / \\varepsilon)$, where\n$\\bar{d}$ is the average degree of the graph. This is $O(1)$ work per update,\nplus the cost of computing a reverse vector once on a static graph. For Forward\nPush, we show that on an arbitrary undirected graph, given a uniformly random\nstart node $s$, the cost to maintain a PPR vector from $s$ of degree-normalized\nerror $\\varepsilon$ as $k$ edges are updated is $O(k + 1 / \\varepsilon)$, which\nis again $O(1)$ per update plus the cost of computing a PPR vector once on a\nstatic graph. \n\n"}
{"id": "1604.01067", "contents": "Title: Sparse matrices for weighted sparse recovery Abstract: We derived the first sparse recovery guarantees for weighted $\\ell_1$\nminimization with sparse random matrices and the class of weighted sparse\nsignals, using a weighted versions of the null space property to derive these\nguarantees. These sparse matrices from expender graphs can be applied very fast\nand have other better computational complexities than their dense counterparts.\nIn addition we show that, using such sparse matrices, weighted sparse recovery\nwith weighted $\\ell_1$ minimization leads to sample complexities that are\nlinear in the weighted sparsity of the signal and these sampling rates can be\nsmaller than those of standard sparse recovery. Moreover, these results reduce\nto known results in standard sparse recovery and sparse recovery with prior\ninformation and the results are supported by numerical experiments. \n\n"}
{"id": "1604.02188", "contents": "Title: Simultaneous Nearest Neighbor Search Abstract: Motivated by applications in computer vision and databases, we introduce and\nstudy the Simultaneous Nearest Neighbor Search (SNN) problem. Given a set of\ndata points, the goal of SNN is to design a data structure that, given a\ncollection of queries, finds a collection of close points that are compatible\nwith each other. Formally, we are given $k$ query points $Q=q_1,\\cdots,q_k$,\nand a compatibility graph $G$ with vertices in $Q$, and the goal is to return\ndata points $p_1,\\cdots,p_k$ that minimize (i) the weighted sum of the\ndistances from $q_i$ to $p_i$ and (ii) the weighted sum, over all edges $(i,j)$\nin the compatibility graph $G$, of the distances between $p_i$ and $p_j$. The\nproblem has several applications, where one wants to return a set of consistent\nanswers to multiple related queries. This generalizes well-studied\ncomputational problems, including NN, Aggregate NN and the 0-extension problem.\n  In this paper we propose and analyze the following general two-step method\nfor designing efficient data structures for SNN. In the first step, for each\nquery point $q_i$ we find its (approximate) nearest neighbor point $\\hat{p}_i$;\nthis can be done efficiently using existing approximate nearest neighbor\nstructures. In the second step, we solve an off-line optimization problem over\nsets $q_1,\\cdots,q_k$ and $\\hat{p}_1,\\cdots,\\hat{p}_k$; this can be done\nefficiently given that $k$ is much smaller than $n$. Even though\n$\\hat{p}_1,\\cdots,\\hat{p}_k$ might not constitute the optimal answers to\nqueries $q_1,\\cdots,q_k$, we show that, for the unweighted case, the resulting\nalgorithm is $O(\\log k/\\log \\log k)$-approximation. Also, we show that the\napproximation factor can be in fact reduced to a constant for compatibility\ngraphs frequently occurring in practice.\n  Finally, we show that the \"empirical approximation factor\" provided by the\nabove approach is very close to 1. \n\n"}
{"id": "1604.03482", "contents": "Title: Joint alignment of multiple protein-protein interaction networks via\n  convex optimization Abstract: Motivation: High-throughput experimental techniques have been producing more\nand more protein-protein interaction (PPI) data. PPI network alignment greatly\nbenefits the understanding of evolutionary relationship among species, helps\nidentify conserved sub-networks and provides extra information for functional\nannotations. Although a few methods have been developed for multiple PPI\nnetwork alignment, the alignment quality is still far away from perfect and\nthus, new network alignment methods are needed. Result: In this paper, we\npresent a novel method, denoted as ConvexAlign, for joint alignment of multiple\nPPI networks by convex optimization of a scoring function composed of sequence\nsimilarity, topological score and interaction conservation score. In contrast\nto existing methods that generate multiple alignments in a greedy or\nprogressive manner, our convex method optimizes alignments globally and\nenforces consistency among all pairwise alignments, resulting in much better\nalignment quality. Tested on both synthetic and real data, our experimental\nresults show that ConvexAlign outperforms several popular methods in producing\nfunctionally coherent alignments. ConvexAlign even has a larger advantage over\nthe others in aligning real PPI networks. ConvexAlign also finds a few\nconserved complexes among 5 species which cannot be detected by the other\nmethods. \n\n"}
{"id": "1604.05023", "contents": "Title: Impact of Knowledge on Election Time in Anonymous Networks Abstract: Leader election is one of the basic problems in distributed computing. For\nanonymous networks, the task of leader election is formulated as follows: every\nnode v of the network must output a simple path, which is coded as a sequence\nof port numbers, such that all these paths end at a common node, the leader. In\nthis paper, we study deterministic leader election in arbitrary anonymous\nnetworks. It is well known that leader election is impossible in some networks,\nregardless of the allocated amount of time, even if nodes know the map of the\nnetwork. However, even in networks in which it is possible to elect a leader\nknowing the map, the task may be still impossible without any knowledge,\nregardless of the allocated time. On the other hand, for any network in which\nleader election is possible knowing the map, there is a minimum time, called\nthe election index, in which this can be done. Informally, the election index\nof a network is the minimum depth at which views of all nodes are distinct. Our\naim is to establish tradeoffs between the allocated time $\\tau$ and the amount\nof information that has to be given a priori to the nodes to enable leader\nelection in time $\\tau$ in all networks for which leader election in this time\nis at all possible. Following the framework of algorithms with advice, this\ninformation is provided to all nodes at the start by an oracle knowing the\nentire network. The length of this string (its number of bits) is called the\nsize of advice. For a given time $\\tau$ allocated to leader election, we give\nupper and lower bounds on the minimum size of advice sufficient to perform\nleader election in time $\\tau$. We focus on the two sides of the time spectrum\nand give tight (or almost tight) bounds on the minimum size of advice for these\nextremes. We also show that constant advice is not sufficient for leader\nelection in all graphs, regardless of the allocated time. \n\n"}
{"id": "1604.06605", "contents": "Title: Indexing Variation Graphs Abstract: Variation graphs, which represent genetic variation within a population, are\nreplacing sequences as reference genomes. Path indexes are one of the most\nimportant tools for working with variation graphs. They generalize text indexes\nto graphs, allowing one to find the paths matching the query string. We propose\nusing de Bruijn graphs as path indexes, compressing them by merging redundant\nsubgraphs, and encoding them with the Burrows-Wheeler transform. The resulting\nfast, space-efficient, and versatile index is used in the variation graph\ntoolkit vg. \n\n"}
{"id": "1604.06707", "contents": "Title: Loopless Gray Code Enumeration and the Tower of Bucharest Abstract: We give new algorithms for generating all n-tuples over an alphabet of m\nletters, changing only one letter at a time (Gray codes). These algorithms are\nbased on the connection with variations of the Towers of Hanoi game. Our\nalgorithms are loopless, in the sense that the next change can be determined in\na constant number of steps, and they can be implemented in hardware. We also\ngive another family of loopless algorithms that is based on the idea of working\nahead and saving the work in a buffer. \n\n"}
{"id": "1604.07128", "contents": "Title: On the Grundy number of Cameron graphs Abstract: The Grundy number of a graph is the maximal number of colors attained by a\nfirst-fit coloring of the graph. The class of Cameron graphs is the Seidel\nswitching class of cographs. In this paper we show that the Grundy number is\ncomputable in polynomial time for Cameron graphs. \n\n"}
{"id": "1604.07432", "contents": "Title: Degree and Sensitivity: tails of two distributions Abstract: The sensitivity of a Boolean function f is the maximum over all inputs x, of\nthe number of sensitive coordinates of x. The well-known sensitivity conjecture\nof Nisan (see also Nisan and Szegedy) states that every sensitivity-s Boolean\nfunction can be computed by a polynomial over the reals of degree poly(s). The\nbest known upper bounds on degree, however, are exponential rather than\npolynomial in s.\n  Our main result is an approximate version of the conjecture: every Boolean\nfunction with sensitivity s can be epsilon-approximated (in L_2) by a\npolynomial whose degree is O(s log(1/epsilon)). This is the first improvement\non the folklore bound of s/epsilon. Further, we show that improving the bound\nto O(s^c log(1/epsilon)^d)$ for any d < 1 and any c > 0 will imply the\nsensitivity conjecture. Thus our result is essentially the best one can hope\nfor without proving the conjecture.\n  We postulate a robust analogue of the sensitivity conjecture: if most inputs\nto a Boolean function f have low sensitivity, then most of the Fourier mass of\nf is concentrated on small subsets, and present an approach towards proving\nthis conjecture. \n\n"}
{"id": "1604.08738", "contents": "Title: I/O-Efficient Generation of Massive Graphs Following the LFR Benchmark Abstract: LFR is a popular benchmark graph generator used to evaluate community\ndetection algorithms. We present EM-LFR, the first external memory algorithm\nable to generate massive complex networks following the LFR benchmark. Its most\nexpensive component is the generation of random graphs with prescribed degree\nsequences which can be divided into two steps: the graphs are first\nmaterialized deterministically using the Havel-Hakimi algorithm, and then\nrandomized. Our main contributions are EM-HH and EM-ES, two I/O-efficient\nexternal memory algorithms for these two steps. We also propose EM-CM/ES, an\nalternative sampling scheme using the Configuration Model and rewiring steps to\nobtain a random simple graph. In an experimental evaluation we demonstrate\ntheir performance; our implementation is able to handle graphs with more than\n37 billion edges on a single machine, is competitive with a massive parallel\ndistributed algorithm, and is faster than a state-of-the-art internal memory\nimplementation even on instances fitting in main memory. EM-LFR's\nimplementation is capable of generating large graph instances orders of\nmagnitude faster than the original implementation. We give evidence that both\nimplementations yield graphs with matching properties by applying clustering\nalgorithms to generated instances. Similarly, we analyse the evolution of graph\nproperties as EM-ES is executed on networks obtained with EM-CM/ES and find\nthat the alternative approach can accelerate the sampling process. \n\n"}
{"id": "1605.00462", "contents": "Title: Sharper Upper Bounds for Unbalanced Uniquely Decodable Code Pairs Abstract: Two sets $A, B \\subseteq \\{0, 1\\}^n$ form a Uniquely Decodable Code Pair\n(UDCP) if every pair $a \\in A$, $b \\in B$ yields a distinct sum $a+b$, where\nthe addition is over $\\mathbb{Z}^n$. We show that every UDCP $A, B$, with $|A|\n= 2^{(1-\\epsilon)n}$ and $|B| = 2^{\\beta n}$, satisfies $\\beta \\leq 0.4228\n+\\sqrt{\\epsilon}$. For sufficiently small $\\epsilon$, this bound significantly\nimproves previous bounds by Urbanke and Li~[Information Theory Workshop '98]\nand Ordentlich and Shayevitz~[2014, arXiv:1412.8415], which upper bound $\\beta$\nby $0.4921$ and $0.4798$, respectively, as $\\epsilon$ approaches $0$. \n\n"}
{"id": "1605.01436", "contents": "Title: Sampling Requirements for Stable Autoregressive Estimation Abstract: We consider the problem of estimating the parameters of a linear univariate\nautoregressive model with sub-Gaussian innovations from a limited sequence of\nconsecutive observations. Assuming that the parameters are compressible, we\nanalyze the performance of the $\\ell_1$-regularized least squares as well as a\ngreedy estimator of the parameters and characterize the sampling trade-offs\nrequired for stable recovery in the non-asymptotic regime. In particular, we\nshow that for a fixed sparsity level, stable recovery of AR parameters is\npossible when the number of samples scale sub-linearly with the AR order. Our\nresults improve over existing sampling complexity requirements in AR estimation\nusing the LASSO, when the sparsity level scales faster than the square root of\nthe model order. We further derive sufficient conditions on the sparsity level\nthat guarantee the minimax optimality of the $\\ell_1$-regularized least squares\nestimate. Applying these techniques to simulated data as well as real-world\ndatasets from crude oil prices and traffic speed data confirm our predicted\ntheoretical performance gains in terms of estimation accuracy and model\nselection. \n\n"}
{"id": "1605.01488", "contents": "Title: Fully dynamic data structure for LCE queries in compressed space Abstract: A Longest Common Extension (LCE) query on a text $T$ of length $N$ asks for\nthe length of the longest common prefix of suffixes starting at given two\npositions. We show that the signature encoding $\\mathcal{G}$ of size $w =\nO(\\min(z \\log N \\log^* M, N))$ [Mehlhorn et al., Algorithmica 17(2):183-198,\n1997] of $T$, which can be seen as a compressed representation of $T$, has a\ncapability to support LCE queries in $O(\\log N + \\log \\ell \\log^* M)$ time,\nwhere $\\ell$ is the answer to the query, $z$ is the size of the Lempel-Ziv77\n(LZ77) factorization of $T$, and $M \\geq 4N$ is an integer that can be handled\nin constant time under word RAM model. In compressed space, this is the fastest\ndeterministic LCE data structure in many cases. Moreover, $\\mathcal{G}$ can be\nenhanced to support efficient update operations: After processing $\\mathcal{G}$\nin $O(w f_{\\mathcal{A}})$ time, we can insert/delete any (sub)string of length\n$y$ into/from an arbitrary position of $T$ in $O((y+ \\log N\\log^* M)\nf_{\\mathcal{A}})$ time, where $f_{\\mathcal{A}} = O(\\min \\{ \\frac{\\log\\log M\n\\log\\log w}{\\log\\log\\log M}, \\sqrt{\\frac{\\log w}{\\log\\log w}} \\})$. This yields\nthe first fully dynamic LCE data structure. We also present efficient\nconstruction algorithms from various types of inputs: We can construct\n$\\mathcal{G}$ in $O(N f_{\\mathcal{A}})$ time from uncompressed string $T$; in\n$O(n \\log\\log n \\log N \\log^* M)$ time from grammar-compressed string $T$\nrepresented by a straight-line program of size $n$; and in $O(z f_{\\mathcal{A}}\n\\log N \\log^* M)$ time from LZ77-compressed string $T$ with $z$ factors. On top\nof the above contributions, we show several applications of our data structures\nwhich improve previous best known results on grammar-compressed string\nprocessing. \n\n"}
{"id": "1605.03019", "contents": "Title: Tight Sum-of-Squares lower bounds for binary polynomial optimization\n  problems Abstract: We give two results concerning the power of the Sum-of-Squares(SoS)/Lasserre\nhierarchy. For binary polynomial optimization problems of degree $2d$ and an\nodd number of variables $n$, we prove that $\\frac{n+2d-1}{2}$ levels of the\nSoS/Lasserre hierarchy are necessary to provide the exact optimal value. This\nmatches the recent upper bound result by Sakaue, Takeda, Kim and Ito.\n  Additionally, we study a conjecture by Laurent, who considered the linear\nrepresentation of a set with no integral points. She showed that the\nSherali-Adams hierarchy requires $n$ levels to detect the empty integer hull,\nand conjectured that the SoS/Lasserre rank for the same problem is $n-1$. We\ndisprove this conjecture and derive lower and upper bounds for the rank. \n\n"}
{"id": "1605.03203", "contents": "Title: Approximating Min-Cost Chain-Constrained Spanning Trees: A Reduction\n  from Weighted to Unweighted Problems Abstract: We study the {\\em min-cost chain-constrained spanning-tree} (abbreviated\n\\mcst) problem: find a min-cost spanning tree in a graph subject to degree\nconstraints on a nested family of node sets. We devise the {\\em first} polytime\nalgorithm that finds a spanning tree that (i) violates the degree constraints\nby at most a constant factor {\\em and} (ii) whose cost is within a constant\nfactor of the optimum. Previously, only an algorithm for {\\em unweighted} \\cst\nwas known \\cite{olver}, which satisfied (i) but did not yield any cost bounds.\nThis also yields the first result that obtains an $O(1)$-factor for {\\em both}\nthe cost approximation and violation of degree constraints for any\nspanning-tree problem with general degree bounds on node sets, where an edge\nparticipates in a super-constant number of degree constraints.\n  A notable feature of our algorithm is that we {\\em reduce} \\mcst to\nunweighted \\cst (and then utilize \\cite{olver}) via a novel application of {\\em\nLagrangian duality} to simplify the {\\em cost structure} of the underlying\nproblem and obtain a decomposition into certain uniform-cost subproblems.\n  We show that this Lagrangian-relaxation based idea is in fact applicable more\ngenerally and, for any cost-minimization problem with packing side-constraints,\nyields a reduction from the weighted to the unweighted problem. We believe that\nthis reduction is of independent interest. As another application of our\ntechnique, we consider the {\\em $k$-budgeted matroid basis} problem, where we\nbuild upon a recent rounding algorithm of \\cite{BansalN16} to obtain an\nimproved $n^{O(k^{1.5}/\\epsilon)}$-time algorithm that returns a solution that\nsatisfies (any) one of the budget constraints exactly and incurs a\n$(1+\\epsilon)$-violation of the other budget constraints. \n\n"}
{"id": "1605.03243", "contents": "Title: On \"Exponential Lower Bounds for Polytopes in Combinatorial\n  Optimization\" by Fiorini et al. (2015): A Refutation For Models With Disjoint\n  Sets of Descriptive Variables Abstract: We provide a numerical refutation of the developments of Fiorini et al.\n(2015)* for models with disjoint sets of descriptive variables. We also provide\nan insight into the meaning of the existence of a one-to-one linear map between\nsolutions of such models.\n  *: Fiorini, S., S. Massar, S. Pokutta, H.R. Tiwary, and R. de Wolf (2015).\nExponential Lower Bounds for Polytopes in Combinatorial Optimization. Journal\nof the ACM 62:2, Article No. 17. \n\n"}
{"id": "1605.03266", "contents": "Title: A Quantum Approach to the Unique Sink Orientation Problem Abstract: We consider quantum algorithms for the unique sink orientation problem on\ncubes. This problem is widely considered to be of intermediate computational\ncomplexity. This is because there no known polynomial algorithm (classical or\nquantum) from the problem and yet it arrises as part of a series of problems\nfor which it being intractable would imply complexity theoretic collapses. We\ngive a reduction which proves that if one can efficiently evaluate the kth\npower of the unique sink orientation outmap, then there exists a polynomial\ntime quantum algorithm for the unique sink orientation problem on cubes. \n\n"}
{"id": "1605.03785", "contents": "Title: Variable-length Non-overlapping Codes Abstract: We define a variable-length code having the property that no (non-empty)\nprefix of each its codeword is a suffix of any other one, and vice versa. This\nkind of code can be seen as an extension of two well-known codes in literature,\ncalled respectively fix-free code and non-overlapping code. In this paper, some\nconstructive algorithms for such codes are presented as well as numerical\nresults about their cardinality. \n\n"}
{"id": "1605.07061", "contents": "Title: On Restricted Nonnegative Matrix Factorization Abstract: Nonnegative matrix factorization (NMF) is the problem of decomposing a given\nnonnegative $n \\times m$ matrix $M$ into a product of a nonnegative $n \\times\nd$ matrix $W$ and a nonnegative $d \\times m$ matrix $H$. Restricted NMF\nrequires in addition that the column spaces of $M$ and $W$ coincide. Finding\nthe minimal inner dimension $d$ is known to be NP-hard, both for NMF and\nrestricted NMF. We show that restricted NMF is closely related to a question\nabout the nature of minimal probabilistic automata, posed by Paz in his seminal\n1971 textbook. We use this connection to answer Paz's question negatively, thus\nfalsifying a positive answer claimed in 1974. Furthermore, we investigate\nwhether a rational matrix $M$ always has a restricted NMF of minimal inner\ndimension whose factors $W$ and $H$ are also rational. We show that this holds\nfor matrices $M$ of rank at most $3$ and we exhibit a rank-$4$ matrix for which\n$W$ and $H$ require irrational entries. \n\n"}
{"id": "1605.07272", "contents": "Title: Matrix Completion has No Spurious Local Minimum Abstract: Matrix completion is a basic machine learning problem that has wide\napplications, especially in collaborative filtering and recommender systems.\nSimple non-convex optimization algorithms are popular and effective in\npractice. Despite recent progress in proving various non-convex algorithms\nconverge from a good initial point, it remains unclear why random or arbitrary\ninitialization suffices in practice. We prove that the commonly used non-convex\nobjective function for \\textit{positive semidefinite} matrix completion has no\nspurious local minima --- all local minima must also be global. Therefore, many\npopular optimization algorithms such as (stochastic) gradient descent can\nprovably solve positive semidefinite matrix completion with \\textit{arbitrary}\ninitialization in polynomial time. The result can be generalized to the setting\nwhen the observed entries contain noise. We believe that our main proof\nstrategy can be useful for understanding geometric properties of other\nstatistical problems involving partial or noisy observations. \n\n"}
{"id": "1605.07583", "contents": "Title: Recursive Sampling for the Nystr\\\"om Method Abstract: We give the first algorithm for kernel Nystr\\\"om approximation that runs in\n*linear time in the number of training points* and is provably accurate for all\nkernel matrices, without dependence on regularity or incoherence conditions.\nThe algorithm projects the kernel onto a set of $s$ landmark points sampled by\ntheir *ridge leverage scores*, requiring just $O(ns)$ kernel evaluations and\n$O(ns^2)$ additional runtime. While leverage score sampling has long been known\nto give strong theoretical guarantees for Nystr\\\"om approximation, by employing\na fast recursive sampling scheme, our algorithm is the first to make the\napproach scalable. Empirically we show that it finds more accurate, lower rank\nkernel approximations in less time than popular techniques such as uniformly\nsampled Nystr\\\"om approximation and the random Fourier features method. \n\n"}
{"id": "1605.09223", "contents": "Title: On large subsets of $F_q^n$ with no three-term arithmetic progression Abstract: In this note, we show that the method of Croot, Lev, and Pach can be used to\nbound the size of a subset of $F_q^n$ with no three terms in arithmetic\nprogression by $c^n$ with $c < q$. For $q=3$, the problem of finding the\nlargest subset with no three terms in arithmetic progression is called the `cap\nproblem'. Previously the best known upper bound for the cap problem, due to\nBateman and Katz, was $O(3^n / n^{1+\\epsilon})$. \n\n"}
{"id": "1605.09721", "contents": "Title: CYCLADES: Conflict-free Asynchronous Machine Learning Abstract: We present CYCLADES, a general framework for parallelizing stochastic\noptimization algorithms in a shared memory setting. CYCLADES is asynchronous\nduring shared model updates, and requires no memory locking mechanisms, similar\nto HOGWILD!-type algorithms. Unlike HOGWILD!, CYCLADES introduces no conflicts\nduring the parallel execution, and offers a black-box analysis for provable\nspeedups across a large family of algorithms. Due to its inherent conflict-free\nnature and cache locality, our multi-core implementation of CYCLADES\nconsistently outperforms HOGWILD!-type algorithms on sufficiently sparse\ndatasets, leading to up to 40% speedup gains compared to the HOGWILD!\nimplementation of SGD, and up to 5x gains over asynchronous implementations of\nvariance reduction algorithms. \n\n"}
{"id": "1606.00898", "contents": "Title: Factoring Polynomials over Finite Fields using Drinfeld Modules with\n  Complex Multiplication Abstract: We present novel algorithms to factor polynomials over a finite field $\\F_q$\nof odd characteristic using rank $2$ Drinfeld modules with complex\nmultiplication. The main idea is to compute a lift of the Hasse invariant\n(modulo the polynomial $f(x) \\in \\F_q[x]$ to be factored) with respect to a\nDrinfeld module $\\phi$ with complex multiplication. Factors of $f(x)$ supported\non prime ideals with supersingular reduction at $\\phi$ have vanishing Hasse\ninvariant and can be separated from the rest. A Drinfeld module analogue of\nDeligne's congruence plays a key role in computing the Hasse invariant lift. We\npresent two algorithms based on this idea. The first algorithm chooses Drinfeld\nmodules with complex multiplication at random and has a quadratic expected run\ntime. The second is a deterministic algorithm with $O(\\sqrt{p})$ run time\ndependence on the characteristic $p$ of $\\F_q$. \n\n"}
{"id": "1606.01844", "contents": "Title: Walking on the Edge and Cosystolic Expansion Abstract: Random walks on regular bounded degree expander graphs have numerous\napplications. A key property of these walks is that they converge rapidly to\nthe uniform distribution on the vertices. The recent study of expansion of high\ndimensional simplicial complexes, which are the high dimensional analogues of\ngraphs, calls for the natural generalization of random walks to higher\ndimensions. In particular, a high order random walk on a $2$-dimensional\nsimplicial complex moves at random between neighboring edges of the complex,\nwhere two edges are considered neighbors if they share a common triangle. We\nshow that if a regular $2$-dimensional simplicial complex is a cosystolic\nexpander and the underlying graph of the complex has a spectral gap larger than\n$1/2$, then the random walk on the edges of the complex converges rapidly to\nthe uniform distribution on the edges. \n\n"}
{"id": "1606.02786", "contents": "Title: Maximum Selection and Sorting with Adversarial Comparators and an\n  Application to Density Estimation Abstract: We study maximum selection and sorting of $n$ numbers using pairwise\ncomparators that output the larger of their two inputs if the inputs are more\nthan a given threshold apart, and output an adversarially-chosen input\notherwise. We consider two adversarial models. A non-adaptive adversary that\ndecides on the outcomes in advance based solely on the inputs, and an adaptive\nadversary that can decide on the outcome of each query depending on previous\nqueries and outcomes.\n  Against the non-adaptive adversary, we derive a maximum-selection algorithm\nthat uses at most $2n$ comparisons in expectation, and a sorting algorithm that\nuses at most $2n \\ln n$ comparisons in expectation. These numbers are within\nsmall constant factors from the best possible. Against the adaptive adversary,\nwe propose a maximum-selection algorithm that uses $\\Theta(n\\log\n(1/{\\epsilon}))$ comparisons to output a correct answer with probability at\nleast $1-\\epsilon$. The existence of this algorithm affirmatively resolves an\nopen problem of Ajtai, Feldman, Hassadim, and Nelson.\n  Our study was motivated by a density-estimation problem where, given samples\nfrom an unknown underlying distribution, we would like to find a distribution\nin a known class of $n$ candidate distributions that is close to underlying\ndistribution in $\\ell_1$ distance. Scheffe's algorithm outputs a distribution\nat an $\\ell_1$ distance at most 9 times the minimum and runs in time\n$\\Theta(n^2\\log n)$. Using maximum selection, we propose an algorithm with the\nsame approximation guarantee but run time of $\\Theta(n\\log n)$. \n\n"}
{"id": "1606.03427", "contents": "Title: Better partitions of protein graphs for subsystem quantum chemistry Abstract: Determining the interaction strength between proteins and small molecules is\nkey to analyzing their biological function. Quantum-mechanical calculations\nsuch as \\emph{Density Functional Theory} (DFT) give accurate and theoretically\nwell-founded results. With common implementations the running time of DFT\ncalculations increases quadratically with molecule size. Thus, numerous\nsubsystem-based approaches have been developed to accelerate quantum-chemical\ncalculations. These approaches partition the protein into different fragments,\nwhich are treated separately. Interactions between different fragments are\napproximated and introduce inaccuracies in the calculated interaction energies.\n  To minimize these inaccuracies, we represent the amino acids and their\ninteractions as a weighted graph in order to apply graph partitioning. None of\nthe existing graph partitioning work can be directly used, though, due to the\nunique constraints in partitioning such protein graphs. We therefore present\nand evaluate several algorithms, partially building upon established concepts,\nbut adapted to handle the new constraints. For the special case of partitioning\na protein along the main chain, we also present an efficient dynamic\nprogramming algorithm that yields provably optimal results. In the general\nscenario our algorithms usually improve the previous approach significantly and\ntake at most a few seconds. \n\n"}
{"id": "1606.04592", "contents": "Title: Algebraic Problems Equivalent to Beating Exponent 3/2 for Polynomial\n  Factorization over Finite Fields Abstract: The fastest known algorithm for factoring univariate polynomials over finite\nfields is the Kedlaya-Umans (fast modular composition) implementation of the\nKaltofen-Shoup algorithm. It is randomized and takes $\\widetilde{O}(n^{3/2}\\log\nq + n \\log^2 q)$ time to factor polynomials of degree $n$ over the finite field\n$\\mathbb{F}_q$ with $q$ elements. A significant open problem is if the $3/2$\nexponent can be improved. We study a collection of algebraic problems and\nestablish a web of reductions between them. A consequence is that an algorithm\nfor any one of these problems with exponent better than $3/2$ would yield an\nalgorithm for polynomial factorization with exponent better than $3/2$. \n\n"}
{"id": "1606.04696", "contents": "Title: Geodesic Walks in Polytopes Abstract: We introduce the geodesic walk for sampling Riemannian manifolds and apply it\nto the problem of generating uniform random points from polytopes in R^n\nspecified by m inequalities. The walk is a discrete-time simulation of a\nstochastic differential equation (SDE) on the Riemannian manifold equipped with\nthe metric induced by the Hessian of a convex function; each step is the\nsolution of an ordinary differential equation (ODE). The resulting sampling\nalgorithm for polytopes mixes in O*(mn^{3/4}) steps. This is the first walk\nthat breaks the quadratic barrier for mixing in high dimension, improving on\nthe previous best bound of O*(mn) by Kannan and Narayanan for the Dikin walk.\nWe also show that each step of the geodesic walk (solving an ODE) can be\nimplemented efficiently, thus improving the time complexity for sampling\npolytopes. Our analysis of the geodesic walk for general Hessian manifolds does\nnot assume positive curvature and might be of independent interest. \n\n"}
{"id": "1606.05979", "contents": "Title: Strategic Bidding for Producers in Nodal Electricity Markets: A Convex\n  Relaxation Approach Abstract: Strategic bidding problems in electricity markets are widely studied in power\nsystems, often by formulating complex bi-level optimization problems that are\nhard to solve. The state-of-the-art approach to solve such problems is to\nreformulate them as mixed-integer linear programs (MILPs). However, the\ncomputational time of such MILP reformulations grows dramatically, once the\nnetwork size increases, scheduling horizon increases, or randomness is taken\ninto consideration. In this paper, we take a fundamentally different approach\nand propose effective and customized convex programming tools to solve the\nstrategic bidding problem for producers in nodal electricity markets. Our\napproach is inspired by the Schmudgen's Positivstellensatz Theorem in\nsemi-algebraic geometry; but then we go through several steps based upon both\nconvex optimization and mixed-integer programming that results in obtaining\nclose to optimal bidding solutions, as evidenced by several numerical case\nstudies, besides having a huge advantage on reducing computation time. While\nthe computation time of the state-of-the-art MILP approach grows exponentially\nwhen we increase the scheduling horizon or the number of random scenarios, the\ncomputation time of our approach increases rather linearly. \n\n"}
{"id": "1606.06204", "contents": "Title: Parallel Priority-Flood Depression Filling For Trillion Cell Digital\n  Elevation Models On Desktops Or Clusters Abstract: Algorithms for extracting hydrologic features and properties from digital\nelevation models (DEMs) are challenged by large datasets, which often cannot\nfit within a computer's RAM. Depression filling is an important preconditioning\nstep to many of these algorithms. Here, I present a new, linearly-scaling\nalgorithm which parallelizes the Priority-Flood depression-filling algorithm by\nsubdividing a DEM into tiles. Using a single-producer, multi-consumer design,\nthe new algorithm works equally well on one core, multiple cores, or multiple\nmachines and can take advantage of large memories or cope with small ones.\nUnlike previous algorithms, the new algorithm guarantees a fixed number of\nmemory access and communication events per subdivision of the DEM. In\ncomparison testing, this results in the new algorithm running generally faster\nwhile using fewer resources than previous algorithms. For moderately sized\ntiles, the algorithm exhibits ~60% strong and weak scaling efficiencies up to\n48 cores, and linear time scaling across datasets ranging over three orders of\nmagnitude. The largest dataset on which I run the algorithm has 2 trillion\n(2*10^12) cells. With 48 cores, processing required 4.8 hours wall-time (9.3\ncompute-days). This test is three orders of magnitude larger than any\npreviously performed in the literature. Complete, well-commented source code\nand correctness tests are available for download from a repository. \n\n"}
{"id": "1606.07425", "contents": "Title: Generalized Preconditioning and Network Flow Problems Abstract: We consider approximation algorithms for the problem of finding $x$ of\nminimal norm $\\|x\\|$ satisfying a linear system $\\mathbf{A} x = \\mathbf{b}$,\nwhere the norm $\\|\\cdot \\|$ is arbitrary and generally non-Euclidean. We show a\nsimple general technique for composing solvers, converting iterative solvers\nwith residual error $\\|\\mathbf{A} x - \\mathbf{b}\\| \\leq t^{-\\Omega(1)}$ into\nsolvers with residual error $\\exp(-\\Omega(t))$, at the cost of an increase in\n$\\|x\\|$, by recursively invoking the solver on the residual problem\n$\\tilde{\\mathbf{b}} = \\mathbf{b} - \\mathbf{A} x$. Convergence of the composed\nsolvers depends strongly on a generalization of the classical condition number\nto general norms, reducing the task of designing algorithms for many such\nproblems to that of designing a \\emph{generalized preconditioner} for\n$\\mathbf{A}$. The new ideas significantly generalize those introduced by the\nauthor's earlier work on maximum flow, making them more widely applicable.\n  As an application of the new technique, we present a nearly-linear time\napproximation algorithm for uncapacitated minimum-cost flow on undirected\ngraphs. Given an undirected graph with $m$ edges labelled with costs, and $n$\nvertices labelled with demands, the algorithm takes\n$\\epsilon^{-2}m^{1+o(1)}$-time and outputs a flow routing the demands with\ntotal cost at most $(1+\\epsilon)$ times larger than minimal, along with a dual\nsolution proving near-optimality. The generalized preconditioner is obtained by\nembedding the cost metric into $\\ell_1$, and then considering a simple\nhierarchical routing scheme in $\\ell_1$ where demands initially supported on a\ndense lattice are pulled from a sparser lattice by randomly rounding unaligned\ncoordinates to their aligned neighbors. Analysis of the generalized condition\nnumber for the preconditioner follows that of the classical multigrid algorithm\nfor lattice Laplacian systems. \n\n"}
{"id": "1606.08275", "contents": "Title: Near-Optimal Computation of Runs over General Alphabet via Non-Crossing\n  LCE Queries Abstract: Longest common extension queries (LCE queries) and runs are ubiquitous in\nalgorithmic stringology. Linear-time algorithms computing runs and\npreprocessing for constant-time LCE queries have been known for over a decade.\nHowever, these algorithms assume a linearly-sortable integer alphabet. A recent\nbreakthrough paper by Bannai et.\\ al.\\ (SODA 2015) showed a link between the\ntwo notions: all the runs in a string can be computed via a linear number of\nLCE queries. The first to consider these problems over a general ordered\nalphabet was Kosolobov (\\emph{Inf.\\ Process.\\ Lett.}, 2016), who presented an\n$O(n (\\log n)^{2/3})$-time algorithm for answering $O(n)$ LCE queries. This\nresult was improved by Gawrychowski et.\\ al.\\ (accepted to CPM 2016) to $O(n\n\\log \\log n)$ time. In this work we note a special \\emph{non-crossing} property\nof LCE queries asked in the runs computation. We show that any $n$ such\nnon-crossing queries can be answered on-line in $O(n \\alpha(n))$ time, which\nyields an $O(n \\alpha(n))$-time algorithm for computing runs. \n\n"}
{"id": "1606.08645", "contents": "Title: Biconnectivity, $st$-numbering and other applications of DFS using\n  $O(n)$ bits Abstract: We consider space efficient implementations of some classical applications of\nDFS including the problem of testing biconnectivity and $2$-edge connectivity,\nfinding cut vertices and cut edges, computing chain decomposition and\n$st$-numbering of a given undirected graph $G$ on $n$ vertices and $m$ edges.\nClassical algorithms for them typically use DFS and some $\\Omega (\\lg n)$\nbits\\footnote{We use $\\lg$ to denote logarithm to the base $2$.} of information\nat each vertex. Building on a recent $O(n)$-bits implementation of DFS due to\nElmasry et al. (STACS 2015) we provide $O(n)$-bit implementations for all these\napplications of DFS. Our algorithms take $O(m \\lg^c n \\lg\\lg n)$ time for some\nsmall constant $c$ (where $c \\leq 2$). Central to our implementation is a\nsuccinct representation of the DFS tree and a space efficient partitioning of\nthe DFS tree into connected subtrees, which maybe of independent interest for\ndesigning other space efficient graph algorithms. \n\n"}
{"id": "1606.08719", "contents": "Title: Using Sequence Ensembles for Seeding Alignments of MinION Sequencing\n  Data Abstract: Oxford Nanopore MinION sequencer is currently the smallest sequencing device\navailable. While being able to produce very long reads (reads of up to 100~kbp\nwere reported), it is prone to high sequencing error rates of up to 30%. Since\nmost of these errors are insertions or deletions, it is very difficult to adapt\npopular seed-based algorithms designed for aligning data sets with much lower\nerror rates.\n  Base calling of MinION reads is typically done using hidden Markov models. In\nthis paper, we propose to represent each sequencing read by an ensemble of\nsequences sampled from such a probabilistic model. This approach can improve\nthe sensitivity and false positive rate of seeding an alignment compared to\nusing a single representative base call sequence for each read. \n\n"}
{"id": "1606.08817", "contents": "Title: Better Unrelated Machine Scheduling for Weighted Completion Time via\n  Random Offsets from Non-Uniform Distributions Abstract: In this paper we consider the classic scheduling problem of minimizing total\nweighted completion time on unrelated machines when jobs have release times,\ni.e, $R | r_{ij} | \\sum_j w_j C_j$ using the three-field notation. For this\nproblem, a 2-approximation is known based on a novel convex programming (J. ACM\n2001 by Skutella). It has been a long standing open problem if one can improve\nupon this 2-approximation (Open Problem 8 in J. of Sched. 1999 by Schuurman and\nWoeginger). We answer this question in the affirmative by giving a\n1.8786-approximation. We achieve this via a surprisingly simple linear\nprogramming, but a novel rounding algorithm and analysis. A key ingredient of\nour algorithm is the use of random offsets sampled from non-uniform\ndistributions.\n  We also consider the preemptive version of the problem, i.e, $R | r_{ij},pmtn\n| \\sum_j w_j C_j$. We again use the idea of sampling offsets from non-uniform\ndistributions to give the first better than 2-approximation for this problem.\nThis improvement also requires use of a configuration LP with variables for\neach job's complete schedules along with more careful analysis. For both\nnon-preemptive and preemptive versions, we break the approximation barrier of 2\nfor the first time. \n\n"}
{"id": "1606.09449", "contents": "Title: Clique-Width and Directed Width Measures for Answer-Set Programming Abstract: Disjunctive Answer Set Programming (ASP) is a powerful declarative\nprogramming paradigm whose main decision problems are located on the second\nlevel of the polynomial hierarchy. Identifying tractable fragments and\ndeveloping efficient algorithms for such fragments are thus important\nobjectives in order to complement the sophisticated ASP systems available to\ndate. Hard problems can become tractable if some problem parameter is bounded\nby a fixed constant; such problems are then called fixed-parameter tractable\n(FPT). While several FPT results for ASP exist, parameters that relate to\ndirected or signed graphs representing the program at hand have been neglected\nso far. In this paper, we first give some negative observations showing that\ndirected width measures on the dependency graph of a program do not lead to FPT\nresults. We then consider the graph parameter of signed clique-width and\npresent a novel dynamic programming algorithm that is FPT w.r.t. this\nparameter. Clique-width is more general than the well-known treewidth, and, to\nthe best of our knowledge, ours is the first FPT algorithm for bounded\nclique-width for reasoning problems beyond SAT. \n\n"}
{"id": "1607.01162", "contents": "Title: Unit Interval Vertex Deletion: Fewer Vertices are Relevant Abstract: The unit interval vertex deletion problem asks for a set of at most $k$\nvertices whose deletion from an $n$-vertex graph makes it a unit interval\ngraph. We develop an $O(k^4)$-vertex kernel for the problem, significantly\nimproving the $O(k^{53})$-vertex kernel of Fomin, Saurabh, and Villanger\n[ESA'12; SIAM J. Discrete Math 27(2013)]. We introduce a novel way of\norganizing cliques of a unit interval graph. Our constructive proof for the\ncorrectness of our algorithm, using interval models, greatly simplifies the\ndestructive proofs, based on forbidden induced subgraphs, for similar problems\nin literature. \n\n"}
{"id": "1607.01229", "contents": "Title: Improved Lower Bounds for Online Hypercube and Rectangle Packing Abstract: Packing a given sequence of items into as few bins as possible in an online\nfashion is a widely studied problem. We improve lower bounds for packing boxes\ninto bins in two or more dimensions, both for general algorithms for squares\nand rectangles (in two dimensions) and for an important subclass, so-called\nHarmonic-type algorithms for hypercubes (in two or more dimensions). Lastly, we\nshow that two adaptions of ideas from a one-dimensional packing algorithm to\nsquare packing do not help to break the barrier of 2. \n\n"}
{"id": "1607.02922", "contents": "Title: Characterization and recognition of proper tagged probe interval graphs Abstract: Interval graphs were used in the study of genomics by the famous molecular\nbiologist Benzer. Later on probe interval graphs were introduced by Zhang as a\ngeneralization of interval graphs for the study of cosmid contig mapping of\nDNA.\n  A tagged probe interval graph (briefly, TPIG) is motivated by similar\napplications to genomics, where the set of vertices is partitioned into two\nsets, namely, probes and nonprobes and there is an interval on the real line\ncorresponding to each vertex. The graph has an edge between two probe vertices\nif their corresponding intervals intersect, has an edge between a probe vertex\nand a nonprobe vertex if the interval corresponding to a nonprobe vertex\ncontains at least one end point of the interval corresponding to a probe vertex\nand the set of non-probe vertices is an independent set. This class of graphs\nhave been defined nearly two decades ago, but till today there is no known\nrecognition algorithm for it.\n  In this paper, we consider a natural subclass of TPIG, namely, the class of\nproper tagged probe interval graphs (in short PTPIG). We present\ncharacterization and a linear time recognition algorithm for PTPIG. To obtain\nthis characterization theorem we introduce a new concept called canonical\nsequence for proper interval graphs, which, we belief, has an independent\ninterest in the study of proper interval graphs. Also to obtain the recognition\nalgorithm for PTPIG, we introduce and solve a variation of consecutive $1$'s\nproblem, namely, oriented consecutive $1$'s problem and some variations of\nPQ-tree algorithm. We also discuss the interrelations between the classes of\nPTPIG and TPIG with probe interval graphs and probe proper interval graphs. \n\n"}
{"id": "1607.02951", "contents": "Title: Design Patterns in Beeping Algorithms: Examples, Emulation, and Analysis Abstract: We consider networks of processes which interact with beeps. In the basic\nmodel defined by Cornejo and Kuhn (2010), processes can choose in each round\neither to beep or to listen. Those who beep are unable to detect simultaneous\nbeeps. Those who listen can only distinguish between silence and the presence\nof at least one beep. We refer to this model as $BL$ (beep or listen). Stronger\nmodels exist where the nodes can detect collision while they are beeping\n($B_{cd}L$), listening ($BL_{cd}$), or both ($B_{cd}L_{cd}$). Beeping models\nare weak in essence and even simple tasks are difficult or unfeasible within.\n  We present a set of generic building blocks (design patterns) which seem to\noccur frequently in the design of beeping algorithms. They include multi-slot\nphases: the fact of dividing the main loop into a number of specialised slots;\nexclusive beeps: having a single node beep at a time in a neighbourhood (within\none or two hops); adaptive probability: increasing or decreasing the\nprobability of beeping to produce more exclusive beeps; internal (resp.\nperipheral) collision detection: for detecting collision while beeping (resp.\nlistening). Based on these patterns, we provide algorithms for a number of\nbasic problems, including colouring, 2-hop colouring, degree computation, 2-hop\nMIS, and collision detection (in $BL$). The patterns make it possible to\nformulate these algorithms in a rather concise and elegant way. Their analyses\nare more technical; one of them improves significantly upon that of the best\nknown MIS algorithm by Jeavons et al. (2016). Finally, inspired by a technique\nfrom Afek et al. (2013), our last contribution is to show that any Las Vegas\nalgorithm relying on collision detection can be transposed into a Monte Carlo\nalgorithm without collision detection at the cost of a logarithmic slowdown,\nwhich we prove is optimal. \n\n"}
{"id": "1607.02986", "contents": "Title: A Birthday Repetition Theorem and Complexity of Approximating Dense CSPs Abstract: A $(k \\times l)$-birthday repetition $\\mathcal{G}^{k \\times l}$ of a\ntwo-prover game $\\mathcal{G}$ is a game in which the two provers are sent\nrandom sets of questions from $\\mathcal{G}$ of sizes $k$ and $l$ respectively.\nThese two sets are sampled independently uniformly among all sets of questions\nof those particular sizes. We prove the following birthday repetition theorem:\nwhen $\\mathcal{G}$ satisfies some mild conditions, $val(\\mathcal{G}^{k \\times\nl})$ decreases exponentially in $\\Omega(kl/n)$ where $n$ is the total number of\nquestions. Our result positively resolves an open question posted by Aaronson,\nImpagliazzo and Moshkovitz (CCC 2014).\n  As an application of our birthday repetition theorem, we obtain new\nfine-grained hardness of approximation results for dense CSPs. Specifically, we\nestablish a tight trade-off between running time and approximation ratio for\ndense CSPs by showing conditional lower bounds, integrality gaps and\napproximation algorithms. In particular, for any sufficiently large $i$ and for\nevery $k \\geq 2$, we show the following results:\n  - We exhibit an $O(q^{1/i})$-approximation algorithm for dense Max $k$-CSPs\nwith alphabet size $q$ via $O_k(i)$-level of Sherali-Adams relaxation.\n  - Through our birthday repetition theorem, we obtain an integrality gap of\n$q^{1/i}$ for $\\tilde\\Omega_k(i)$-level Lasserre relaxation for fully-dense Max\n$k$-CSP.\n  - Assuming that there is a constant $\\epsilon > 0$ such that Max 3SAT cannot\nbe approximated to within $(1-\\epsilon)$ of the optimal in sub-exponential\ntime, our birthday repetition theorem implies that any algorithm that\napproximates fully-dense Max $k$-CSP to within a $q^{1/i}$ factor takes\n$(nq)^{\\tilde \\Omega_k(i)}$ time, almost tightly matching the algorithmic\nresult based on Sherali-Adams relaxation. \n\n"}
{"id": "1607.04229", "contents": "Title: Improving Viterbi is Hard: Better Runtimes Imply Faster Clique\n  Algorithms Abstract: The classic algorithm of Viterbi computes the most likely path in a Hidden\nMarkov Model (HMM) that results in a given sequence of observations. It runs in\ntime $O(Tn^2)$ given a sequence of $T$ observations from a HMM with $n$ states.\nDespite significant interest in the problem and prolonged effort by different\ncommunities, no known algorithm achieves more than a polylogarithmic speedup.\n  In this paper, we explain this difficulty by providing matching conditional\nlower bounds. We show that the Viterbi algorithm runtime is optimal up to\nsubpolynomial factors even when the number of distinct observations is small.\nOur lower bounds are based on assumptions that the best known algorithms for\nthe All-Pairs Shortest Paths problem (APSP) and for the Max-Weight $k$-Clique\nproblem in edge-weighted graphs are essentially tight.\n  Finally, using a recent algorithm by Green Larsen and Williams for online\nBoolean matrix-vector multiplication, we get a $2^{\\Omega(\\sqrt {\\log n})}$\nspeedup for the Viterbi algorithm when there are few distinct transition\nprobabilities in the HMM. \n\n"}
{"id": "1607.04346", "contents": "Title: Space-Efficient Construction of Compressed Indexes in Deterministic\n  Linear Time Abstract: We show that the compressed suffix array and the compressed suffix tree of a\nstring $T$ can be built in $O(n)$ deterministic time using $O(n\\log\\sigma)$\nbits of space, where $n$ is the string length and $\\sigma$ is the alphabet\nsize. Previously described deterministic algorithms either run in time that\ndepends on the alphabet size or need $\\omega(n\\log \\sigma)$ bits of working\nspace. Our result has immediate applications to other problems, such as\nyielding the first linear-time LZ77 and LZ78 parsing algorithms that use $O(n\n\\log\\sigma)$ bits. \n\n"}
{"id": "1607.04787", "contents": "Title: Robust algorithms with polynomial loss for near-unanimity CSPs Abstract: An instance of the Constraint Satisfaction Problem (CSP) is given by a family\nof constraints on overlapping sets of variables, and the goal is to assign\nvalues from a fixed domain to the variables so that all constraints are\nsatisfied. In the optimization version, the goal is to maximize the number of\nsatisfied constraints. An approximation algorithm for CSP is called robust if\nit outputs an assignment satisfying a $(1-g(\\varepsilon))$-fraction of\nconstraints on any $(1-\\varepsilon)$-satisfiable instance, where the loss\nfunction $g$ is such that $g(\\varepsilon)\\rightarrow 0$ as\n$\\varepsilon\\rightarrow 0$.\n  We study how the robust approximability of CSPs depends on the set of\nconstraint relations allowed in instances, the so-called constraint language.\nAll constraint languages admitting a robust polynomial-time algorithm (with\nsome $g$) have been characterised by Barto and Kozik, with the general bound on\nthe loss $g$ being doubly exponential, specifically\n$g(\\varepsilon)=O((\\log\\log(1/\\varepsilon))/\\log(1/\\varepsilon))$. It is\nnatural to ask when a better loss can be achieved: in particular, polynomial\nloss $g(\\varepsilon)=O(\\varepsilon^{1/k})$ for some constant $k$. In this\npaper, we consider CSPs with a constraint language having a near-unanimity\npolymorphism. We give two randomized robust algorithms with polynomial loss for\nsuch CSPs: one works for any near-unanimity polymorphism and the parameter $k$\nin the loss depends on the size of the domain and the arity of the relations in\n$\\Gamma$, while the other works for a special ternary near-unanimity operation\ncalled dual discriminator with $k=2$ for any domain size. In the latter case,\nthe CSP is a common generalisation of Unique Games with a fixed domain and\n2-SAT. In the former case, we use the algebraic approach to the CSP. Both cases\nuse the standard semidefinite programming relaxation for CSP. \n\n"}
{"id": "1607.05133", "contents": "Title: Improved Hardness for Cut, Interdiction, and Firefighter Problems Abstract: We study variants of the classic $s$-$t$ cut problem and prove the following\nimproved hardness results assuming the Unique Games Conjecture (UGC).\n  - For any constant $k \\geq 2$ and $\\epsilon > 0$, we show that Directed\nMulticut with $k$ source-sink pairs is hard to approximate within a factor $k -\n\\epsilon$. This matches the trivial $k$-approximation algorithm. By a simple\nreduction, our result for $k = 2$ implies that Directed Multiway Cut with two\nterminals (also known as $s$-$t$ Bicut) is hard to approximate within a factor\n$2 - \\epsilon$, matching the trivial $2$-approximation algorithm. Previously,\nthe best hardness factor for these problems (for constant $k$) was $1.5 -\n\\epsilon$ under the UGC.\n  - For Length-Bounded Cut and Shortest Path Interdiction, we show that both\nproblems are hard to approximate within any constant factor, even if we allow\nbicriteria approximation. If we want to cut vertices or the graph is directed,\nour hardness factor for Length-Bounded Cut matches the best approximation ratio\nup to a constant. Previously, the best hardness factor was $1.1377$ for\nLength-Bounded Cut and $2$ for Shortest Path Interdiction.\n  - Assuming a variant of the UGC (implied by another variant of Bansal and\nKhot), we prove that it is hard to approximate Resource Minimization Fire\nContainment within any constant factor. Previously, the best hardness factor\nwas $2$.\n  Our results are based on a general method of converting an integrality gap\ninstance to a length-control dictatorship test for variants of the $s$-$t$ cut\nproblem, which may be useful for other problems. \n\n"}
{"id": "1607.06017", "contents": "Title: Doubly Accelerated Methods for Faster CCA and Generalized\n  Eigendecomposition Abstract: We study $k$-GenEV, the problem of finding the top $k$ generalized\neigenvectors, and $k$-CCA, the problem of finding the top $k$ vectors in\ncanonical-correlation analysis. We propose algorithms $\\mathtt{LazyEV}$ and\n$\\mathtt{LazyCCA}$ to solve the two problems with running times linearly\ndependent on the input size and on $k$.\n  Furthermore, our algorithms are DOUBLY-ACCELERATED: our running times depend\nonly on the square root of the matrix condition number, and on the square root\nof the eigengap. This is the first such result for both $k$-GenEV or $k$-CCA.\nWe also provide the first gap-free results, which provide running times that\ndepend on $1/\\sqrt{\\varepsilon}$ rather than the eigengap. \n\n"}
{"id": "1607.06711", "contents": "Title: Algorithmic and optimization aspects of Brascamp-Lieb inequalities, via\n  Operator Scaling Abstract: The celebrated Brascamp-Lieb (BL) inequalities (and their extensions) are an\nimportant mathematical tool, unifying and generalizing numerous inequalities in\nanalysis, convex geometry and information theory. While their structural theory\nis very well understood, far less is known about computing their main\nparameters.\n  We give polynomial time algorithms to compute feasibility of BL-datum, the\noptimal BL-constant and a weak separation oracle for the BL-polytope. The same\nresult holds for the so-called Reverse BL inequalities of Barthe. The best\nknown algorithms for any of these tasks required at least exponential time.\n  The algorithms are obtained by a simple efficient reduction of a given\nBL-datum to an instance of the Operator Scaling problem defined by Gurvits, for\nwhich the present authors have provided a polynomial time algorithm. This\nreduction implies algorithmic versions of many of the known structural results,\nand in some cases provide proofs that are different or simpler than existing\nones.\n  Of particular interest is the fact that the operator scaling algorithm is\ncontinuous in its input. Thus as a simple corollary of our reduction we obtain\nexplicit bounds on the magnitude and continuity of the BL-constant in terms of\nthe BL-data. To the best of our knowledge no such bounds were known, as past\narguments relied on compactness. The continuity of BL-constants is important\nfor developing non-linear BL inequalities that have recently found so many\napplications. \n\n"}
{"id": "1607.06757", "contents": "Title: Hereditary Graph Classes: When the Complexities of Colouring and Clique\n  Cover Coincide Abstract: A graph is $(H_1,H_2)$-free for a pair of graphs $H_1,H_2$ if it contains no\ninduced subgraph isomorphic to $H_1$ or $H_2$. In 2001, Kr\\'al',\nKratochv\\'{\\i}l, Tuza, and Woeginger initiated a study into the complexity of\nColouring for $(H_1,H_2)$-free graphs. Since then, others have tried to\ncomplete their study, but many cases remain open. We focus on those\n$(H_1,H_2)$-free graphs where $H_2$ is $\\overline{H_1}$, the complement of\n$H_1$. As these classes are closed under complementation, the computational\ncomplexities of Colouring and Clique Cover coincide. By combining new and known\nresults, we are able to classify the complexity of Colouring and Clique Cover\nfor $(H,\\overline{H})$-free graphs for all cases except when $H=sP_1+ P_3$ for\n$s\\geq 3$ or $H=sP_1+P_4$ for $s\\geq 2$. We also classify the complexity of\nColouring on graph classes characterized by forbidding a finite number of\nself-complementary induced subgraphs, and we initiate a study of $k$-Colouring\nfor $(P_r,\\overline{P_r})$-free graphs. \n\n"}
{"id": "1607.06865", "contents": "Title: Connectivity Oracles for Graphs Subject to Vertex Failures Abstract: We introduce new data structures for answering connectivity queries in graphs\nsubject to batched vertex failures. A deterministic structure processes a batch\nof $d\\leq d_{\\star}$ failed vertices in $\\tilde{O}(d^3)$ time and thereafter\nanswers connectivity queries in $O(d)$ time. It occupies space $O(d_{\\star}\nm\\log n)$. We develop a randomized Monte Carlo version of our data structure\nwith update time $\\tilde{O}(d^2)$, query time $O(d)$, and space $\\tilde{O}(m)$\nfor any failure bound $d\\le n$. This is the first connectivity oracle for\ngeneral graphs that can efficiently deal with an unbounded number of vertex\nfailures.\n  We also develop a more efficient Monte Carlo edge-failure connectivity\noracle. Using space $O(n\\log^2 n)$, $d$ edge failures are processed in $O(d\\log\nd\\log\\log n)$ time and thereafter, connectivity queries are answered in\n$O(\\log\\log n)$ time, which are correct w.h.p.\n  Our data structures are based on a new decomposition theorem for an\nundirected graph $G=(V,E)$, which is of independent interest. It states that\nfor any terminal set $U\\subseteq V$ we can remove a set $B$ of $|U|/(s-2)$\nvertices such that the remaining graph contains a Steiner forest for $U-B$ with\nmaximum degree $s$. \n\n"}
{"id": "1607.07130", "contents": "Title: A No-Go Theorem for Derandomized Parallel Repetition: Beyond\n  Feige-Kilian Abstract: In this work we show a barrier towards proving a randomness-efficient\nparallel repetition, a promising avenue for achieving many tight\ninapproximability results. Feige and Kilian (STOC'95) proved an impossibility\nresult for randomness-efficient parallel repetition for two prover games with\nsmall degree, i.e., when each prover has only few possibilities for the\nquestion of the other prover. In recent years, there have been indications that\nrandomness-efficient parallel repetition (also called derandomized parallel\nrepetition) might be possible for games with large degree, circumventing the\nimpossibility result of Feige and Kilian. In particular, Dinur and Meir\n(CCC'11) construct games with large degree whose repetition can be derandomized\nusing a theorem of Impagliazzo, Kabanets and Wigderson (SICOMP'12). However,\nobtaining derandomized parallel repetition theorems that would yield optimal\ninapproximability results has remained elusive.\n  This paper presents an explanation for the current impasse in progress, by\nproving a limitation on derandomized parallel repetition. We formalize two\nproperties which we call \"fortification-friendliness\" and \"yields robust\nembeddings.\" We show that any proof of derandomized parallel repetition\nachieving almost-linear blow-up cannot both (a) be fortification-friendly and\n(b) yield robust embeddings. Unlike Feige and Kilian, we do not require the\nsmall degree assumption.\n  Given that virtually all existing proofs of parallel repetition, including\nthe derandomized parallel repetition result of Dinur and Meir, share these two\nproperties, our no-go theorem highlights a major barrier to achieving\nalmost-linear derandomized parallel repetition. \n\n"}
{"id": "1607.07200", "contents": "Title: Approximating Multicut and the Demand Graph Abstract: In the minimum Multicut problem, the input is an edge-weighted supply graph\n$G=(V,E)$ and a simple demand graph $H=(V,F)$. Either $G$ and $H$ are directed\n(DMulC) or both are undirected (UMulC). The goal is to remove a minimum weight\nset of edges in $G$ such that there is no path from $s$ to $t$ in the remaining\ngraph for any $(s,t) \\in F$. UMulC admits an $O(\\log k)$-approximation where\n$k$ is the vertex cover size of $H$ while the best known approximation for\nDMulC is $\\min\\{k, \\tilde{O}(n^{11/23})\\}$. These approximations are obtained\nby proving corresponding results on the multicommodity flow-cut gap. In\ncontrast to these results some special cases of Multicut, such as the\nwell-studied Multiway Cut problem, admit a constant factor approximation in\nboth undirected and directed graphs. Motivated by both concrete instances from\napplications and abstract considerations, we consider the role that the\nstructure of the demand graph $H$ plays in determining the approximability of\nMulticut.\n  In undirected graphs our main result is a $2$-approximation in $n^{O(t)}$\ntime when the demand graph $H$ excludes an induced matching of size $t$. This\ngives a constant factor approximation for a specific demand graph that\nmotivated this work.\n  In contrast to undirected graphs, we prove that in directed graphs such\napproximation algorithms can not exist. Assuming the Unique Games Conjecture\n(UGC), for a large class of fixed demand graphs DMulC cannot be approximated to\na factor better than worst-case flow-cut gap. As a consequence we prove that\nfor any fixed $k$, assuming UGC, DMulC with $k$ demand pairs is hard to\napproximate to within a factor better than $k$. On the positive side, we prove\nan approximation of $k$ when the demand graph excludes certain graphs as an\ninduced subgraph. This generalizes the Multiway Cut result to a much larger\nclass of demand graphs. \n\n"}
{"id": "1607.07306", "contents": "Title: The Costs and Benefits of Sharing: Sequential Individual Rationality and\n  Sequential Fairness Abstract: In designing dynamic shared service systems that incentivize customers to opt\nfor shared rather than exclusive service, the traditional notion of individual\nrationality may be insufficient, as a customer's estimated utility could\nfluctuate arbitrarily during their time in the shared system, as long as their\nrealized utility at service completion is not worse than that for exclusive\nservice. In this work, within a model that explicitly considers the\n\"inconvenience costs\" incurred by customers due to sharing, we introduce the\nnotion of sequential individual rationality (SIR) that requires that the\ndisutility of existing customers is nonincreasing as the system state changes\ndue to new customer arrivals. Next, under SIR, we observe that cost sharing can\nalso be viewed as benefit sharing, which inspires a natural definition of\nsequential fairness (SF) - the total incremental benefit due to a new customer\nis shared among existing customers in proportion to the incremental\ninconvenience suffered.\n  We demonstrate the effectiveness of these notions by applying them to a\nridesharing system, where unexpected detours to pick up subsequent passengers\ninconvenience the existing passengers. Imposing SIR and SF reveals interesting\nand surprising results, including: (a) natural limits on the incremental\ndetours permissible, (b) exact characterization of \"SIR-feasible\" routes, which\nboast sublinear upper and lower bounds on the fractional detours, (c) exact\ncharacterization of sequentially fair cost sharing schemes, which includes a\nstrong requirement that passengers must compensate each other for the detour\ninconveniences that they cause, and (d) new algorithmic problems related to and\nmotivated by SIR. \n\n"}
{"id": "1607.07647", "contents": "Title: A Scalable Algorithm for Tracking an Unknown Number of Targets Using\n  Multiple Sensors Abstract: We propose a method for tracking an unknown number of targets based on\nmeasurements provided by multiple sensors. Our method achieves low\ncomputational complexity and excellent scalability by running belief\npropagation on a suitably devised factor graph. A redundant formulation of data\nassociation uncertainty and the use of \"augmented target states\" including\nbinary target indicators make it possible to exploit statistical independencies\nfor a drastic reduction of complexity. An increase in the number of targets,\nsensors, or measurements leads to additional variable nodes in the factor graph\nbut not to higher dimensions of the messages. As a consequence, the complexity\nof our method scales only quadratically in the number of targets, linearly in\nthe number of sensors, and linearly in the number of measurements per sensors.\nThe performance of the method compares well with that of previously proposed\nmethods, including methods with a less favorable scaling behavior. In\nparticular, our method can outperform multisensor versions of the probability\nhypothesis density (PHD) filter, the cardinalized PHD filter, and the\nmulti-Bernoulli filter. \n\n"}
{"id": "1607.07673", "contents": "Title: A Selectable Sloppy Heap Abstract: We study the selection problem, namely that of computing the $i$th order\nstatistic of $n$ given elements. Here we offer a data structure called\n\\emph{selectable sloppy heap} handling a dynamic version in which upon request:\n(i)~a new element is inserted or (ii)~an element of a prescribed quantile group\nis deleted from the data structure. Each operation is executed in (ideal!)\nconstant time---and is thus independent of $n$ (the number of elements stored\nin the data structure)---provided that the number of quantile groups is fixed.\nThis is the first result of this kind accommodating both insertion and deletion\nin constant time. As such, our data structure outperforms the soft heap data\nstructure of Chazelle (which only offers constant amortized complexity for a\nfixed error rate $0<\\varepsilon \\leq 1/2$) in applications such as dynamic\npercentile maintenance. The design demonstrates how slowing down a certain\ncomputation can speed up the data structure. \n\n"}
{"id": "1607.08176", "contents": "Title: Suffix arrays with a twist Abstract: The suffix array is a classic full-text index, combining effectiveness with\nsimplicity. We discuss three approaches aiming to improve its efficiency even\nmore: changes to the navigation, data layout and adding extra data. In short,\nwe show that $(i)$ how we search for the right interval boundary impacts\nsignificantly the overall search speed, $(ii)$ a B-tree data layout easily wins\nover the standard one, $(iii)$ the well-known idea of a lookup table for the\nprefixes of the suffixes can be refined with using compression, $(iv)$ caching\nprefixes of the suffixes in a helper array can pose a(nother) practical\nspace-time tradeoff. \n\n"}
{"id": "1607.08192", "contents": "Title: Counting matchings with k unmatched vertices in planar graphs Abstract: We consider the problem of counting matchings in planar graphs. While perfect\nmatchings in planar graphs can be counted by a classical polynomial-time\nalgorithm, the problem of counting all matchings (possibly containing unmatched\nvertices, also known as defects) is known to be #P-complete on planar graphs.\nTo interpolate between the hard case of counting matchings and the easy case of\ncounting perfect matchings, we study the parameterized problem of counting\nmatchings with exactly k unmatched vertices in a planar graph G, on input G and\nk. This setting has a natural interpretation in statistical physics, and it is\na special case of counting perfect matchings in k-apex graphs (graphs that can\nbe turned planar by removing at most k vertices).\n  Starting from a recent #W[1]-hardness proof for counting perfect matchings on\nk-apex graphs, we obtain that counting matchings with k unmatched vertices in\nplanar graphs is #W[1]-hard. In contrast, given a plane graph G with s\ndistinguished faces, there is an $O(2^s \\cdot n^3)$ time algorithm for counting\nthose matchings with k unmatched vertices such that all unmatched vertices lie\non the distinguished faces. This implies an $f(k,s)\\cdot n^{O(1)}$ time\nalgorithm for counting perfect matchings in k-apex graphs whose apex\nneighborhood is covered by s faces. \n\n"}
{"id": "1608.00117", "contents": "Title: Improved stochastic trace estimation using mutually unbiased bases Abstract: We examine the problem of estimating the trace of a matrix $A$ when given\naccess to an oracle which computes $x^\\dagger A x$ for an input vector $x$. We\nmake use of the basis vectors from a set of mutually unbiased bases, widely\nstudied in the field of quantum information processing, in the selection of\nprobing vectors $x$. This approach offers a new state of the art single shot\nsampling variance while requiring only $O(\\log(n))$ random bits to generate\neach vector. This significantly improves on traditional methods such as\nHutchinson's and Gaussian estimators in terms of the number of random bits\nrequired and worst case sample variance. \n\n"}
{"id": "1608.00529", "contents": "Title: Hardness of Permutation Pattern Matching Abstract: Permutation Pattern Matching (or PPM) is a decision problem whose input is a\npair of permutations $\\pi$ and $\\tau$, represented as sequences of integers,\nand the task is to determine whether $\\tau$ contains a subsequence\norder-isomorphic to $\\pi$. Bose, Buss and Lubiw proved that PPM is NP-complete\non general inputs.\n  We show that PPM is NP-complete even when $\\pi$ has no decreasing subsequence\nof length 3 and $\\tau$ has no decreasing subsequence of length 4. This provides\nthe first known example of PPM being hard when one or both of $\\pi$ and\n$\\sigma$ are restricted to a proper hereditary class of permutations.\n  This hardness result is tight in the sense that PPM is known to be polynomial\nwhen both $\\pi$ and $\\tau$ avoid a decreasing subsequence of length 3, as well\nas when $\\pi$ avoids a decreasing subsequence of length 2. The result is also\ntight in another sense: we will show that for any hereditary proper subclass C\nof the class of permutations avoiding a decreasing sequence of length 3, there\nis a polynomial algorithm solving PPM instances where $\\pi$ is from C and\n$\\tau$ is arbitrary.\n  We also obtain analogous hardness and tractability results for the class of\nso-called skew-merged patterns.\n  From these results, we deduce a complexity dichotomy for the PPM problem\nrestricted to $\\pi$ belonging to $Av(\\rho)$, where $Av(\\rho)$ denotes the class\nof permutations avoiding a permutation $\\rho$. Specifically, we show that the\nproblem is polynomial when $\\rho$ is in the set {1, 12, 21, 132, 213, 231,\n312}, and it is NP-complete for any other $\\rho$. \n\n"}
{"id": "1608.01689", "contents": "Title: Derandomizing Local Distributed Algorithms under Bandwidth Restrictions Abstract: This paper addresses the cornerstone family of \\emph{local problems} in\ndistributed computing, and investigates the curious gap between randomized and\ndeterministic solutions under bandwidth restrictions.\n  Our main contribution is in providing tools for derandomizing solutions to\nlocal problems, when the $n$ nodes can only send $O(\\log n)$-bit messages in\neach round of communication. We combine bounded independence, which we show to\nbe sufficient for some algorithms, with the method of conditional expectations\nand with additional machinery, to obtain the following results.\n  Our techniques give a deterministic maximal independent set (MIS) algorithm\nin the CONGEST model, where the communication graph is identical to the input\ngraph, in $O(D\\log^2 n)$ rounds, where $D$ is the diameter of the graph. The\nbest known running time in terms of $n$ alone is $2^{O(\\sqrt{\\log n})}$, which\nis super-polylogarithmic, and requires large messages. For the CONGEST model,\nthe only known previous solution is a coloring-based $O(\\Delta + \\log^*\nn)$-round algorithm, where $\\Delta$ is the maximal degree in the graph.\n  On the way to obtaining the above, we show that in the \\emph{Congested\nClique} model, which allows all-to-all communication, there is a deterministic\nMIS algorithm that runs in $O(\\log \\Delta \\log n)$ rounds.%, where $\\Delta$ is\nthe maximum degree. When $\\Delta=O(n^{1/3})$, the bound improves to $O(\\log\n\\Delta)$ and holds also for $(\\Delta+1)$-coloring.\n  In addition, we deterministically construct a $(2k-1)$-spanner with\n$O(kn^{1+1/k}\\log n)$ edges in $O(k \\log n)$ rounds. For comparison, in the\nmore stringent CONGEST model, the best deterministic algorithm for constructing\na $(2k-1)$-spanner with $O(kn^{1+1/k})$ edges runs in $O(n^{1-1/k})$ rounds. \n\n"}
{"id": "1608.03165", "contents": "Title: Linear Programming based Converses for Finite Blocklength Lossy Joint\n  Source-Channel Coding Abstract: A linear programming (LP) based framework is presented for obtaining\nconverses for finite blocklength lossy joint source-channel coding problems.\nThe framework applies for any loss criterion, generalizes certain previously\nknown converses, and also extends to multi-terminal settings. The finite\nblocklength problem is posed equivalently as a nonconvex optimization problem\nand using a lift-and-project-like method, a close but tractable LP relaxation\nof this problem is derived. Lower bounds on the original problem are obtained\nby the construction of feasible points for the dual of the LP relaxation. A\nparticular application of this approach leads to new converses which recover\nand improve on the converses of Kostina and Verdu for finite blocklength lossy\njoint source-channel coding and lossy source coding. For finite blocklength\nchannel coding, the LP relaxation recovers the converse of Polyanskiy, Poor and\nVerdu and leads to a new improvement on the converse of Wolfowitz, showing\nthereby that our LP relaxation is asymptotically tight with increasing\nblocklengths for channel coding, lossless source coding and joint\nsource-channel coding with the excess distortion probability as the loss\ncriterion. Using a duality based argument, a new converse is derived for finite\nblocklength joint source-channel coding for a class of source-channel pairs.\nEmploying this converse, the LP relaxation is also shown to be tight for all\nblocklengths for the minimization of the expected average symbol-wise Hamming\ndistortion of a $q$-ary uniform source over a $q$-ary symmetric memoryless\nchannel for any $q \\in N$. The optimization formulation and the\nlift-and-project method are extended to networked settings and demonstrated by\nobtaining an improvement on a converse of Zhou et al. for the successive\nrefinement problem for successively refinable source-distortion measure\ntriplets. \n\n"}
{"id": "1608.03643", "contents": "Title: Chi-squared Amplification: Identifying Hidden Hubs Abstract: We consider the following general hidden hubs model: an $n \\times n$ random\nmatrix $A$ with a subset $S$ of $k$ special rows (hubs): entries in rows\noutside $S$ are generated from the probability distribution $p_0 \\sim\nN(0,\\sigma_0^2)$; for each row in $S$, some $k$ of its entries are generated\nfrom $p_1 \\sim N(0,\\sigma_1^2)$, $\\sigma_1>\\sigma_0$, and the rest of the\nentries from $p_0$. The problem is to identify the high-degree hubs\nefficiently. This model includes and significantly generalizes the planted\nGaussian Submatrix Model, where the special entries are all in a $k \\times k$\nsubmatrix. There are two well-known barriers: if $k\\geq c\\sqrt{n\\ln n}$, just\nthe row sums are sufficient to find $S$ in the general model. For the submatrix\nproblem, this can be improved by a $\\sqrt{\\ln n}$ factor to $k \\ge c\\sqrt{n}$\nby spectral methods or combinatorial methods. In the variant with $p_0=\\pm 1$\n(with probability $1/2$ each) and $p_1\\equiv 1$, neither barrier has been\nbroken.\n  We give a polynomial-time algorithm to identify all the hidden hubs with high\nprobability for $k \\ge n^{0.5-\\delta}$ for some $\\delta >0$, when\n$\\sigma_1^2>2\\sigma_0^2$. The algorithm extends to the setting where planted\nentries might have different variances each at least as large as $\\sigma_1^2$.\nWe also show a nearly matching lower bound: for $\\sigma_1^2 \\le 2\\sigma_0^2$,\nthere is no polynomial-time Statistical Query algorithm for distinguishing\nbetween a matrix whose entries are all from $N(0,\\sigma_0^2)$ and a matrix with\n$k=n^{0.5-\\delta}$ hidden hubs for any $\\delta >0$. The lower bound as well as\nthe algorithm are related to whether the chi-squared distance of the two\ndistributions diverges. At the critical value $\\sigma_1^2=2\\sigma_0^2$, we show\nthat the general hidden hubs problem can be solved for $k\\geq c\\sqrt n(\\ln\nn)^{1/4}$, improving on the naive row sum-based method. \n\n"}
{"id": "1608.04112", "contents": "Title: Optimal Polynomial-Time Estimators: A Bayesian Notion of Approximation\n  Algorithm Abstract: We introduce a new concept of approximation applicable to decision problems\nand functions, inspired by Bayesian probability. From the perspective of a\nBayesian reasoner with limited computational resources, the answer to a problem\nthat cannot be solved exactly is uncertain and therefore should be described by\na random variable. It thus should make sense to talk about the expected value\nof this random variable, an idea we formalize in the language of average-case\ncomplexity theory by introducing the concept of \"optimal polynomial-time\nestimators.\" We prove some existence theorems and completeness results, and\nshow that optimal polynomial-time estimators exhibit many parallels with\n\"classical\" probability theory. \n\n"}
{"id": "1608.04773", "contents": "Title: Faster Principal Component Regression and Stable Matrix Chebyshev\n  Approximation Abstract: We solve principal component regression (PCR), up to a multiplicative\naccuracy $1+\\gamma$, by reducing the problem to $\\tilde{O}(\\gamma^{-1})$\nblack-box calls of ridge regression. Therefore, our algorithm does not require\nany explicit construction of the top principal components, and is suitable for\nlarge-scale PCR instances. In contrast, previous result requires\n$\\tilde{O}(\\gamma^{-2})$ such black-box calls.\n  We obtain this result by developing a general stable recurrence formula for\nmatrix Chebyshev polynomials, and a degree-optimal polynomial approximation to\nthe matrix sign function. Our techniques may be of independent interests,\nespecially when designing iterative methods. \n\n"}
{"id": "1608.04829", "contents": "Title: Quantum Merlin-Arthur with noisy channel Abstract: What happens if in QMA the quantum channel between Merlin and Arthur is\nnoisy? It is not difficult to show that such a modification does not change the\ncomputational power as long as the noise is not too strong so that errors are\ncorrectable with high probability, since if Merlin encodes the witness state in\na quantum error-correction code and sends it to Arthur, Arthur can correct the\nerror caused by the noisy channel. If we further assume that Arthur can do only\nsingle-qubit measurements, however, the problem becomes nontrivial, since in\nthis case Arthur cannot do the universal quantum computation by himself. In\nthis paper, we show that such a restricted complexity class is still equivalent\nto QMA. To show it, we use measurement-based quantum computing: honest Merlin\nsends the graph state to Arthur, and Arthur does fault-tolerant\nmeasurement-based quantum computing on the noisy graph state with only\nsingle-qubit measurements. By measuring stabilizer operators, Arthur also\nchecks the correctness of the graph state. Although this idea itself was\nalready used in several previous papers, these results cannot be directly used\nto the present case, since the test that checks the graph state used in these\npapers is so strict that even honest Merlin is rejected with high probability\nif the channel is noisy. We therefore introduce a more relaxed test that can\naccept not only the ideal graph state but also noisy graph states that are\nerror-correctable. \n\n"}
{"id": "1608.06980", "contents": "Title: A $\\widetilde{O}(n)$ Non-Adaptive Tester for Unateness Abstract: Khot and Shinkar (RANDOM, 2016) recently describe an adaptive, $O(n\n\\log(n)/\\varepsilon)$-query tester for unateness of Boolean functions\n$f:\\{0,1\\}^n \\to \\{0,1\\}$. In this note we describe a simple non-adaptive, $O(n\n\\log(n/\\varepsilon)/\\varepsilon)$ -query tester for unateness for functions\nover the hypercube with any ordered range. \n\n"}
{"id": "1608.07020", "contents": "Title: Power of Uninitialized Qubits in Shallow Quantum Circuits Abstract: We study the computational power of shallow quantum circuits with $O(\\log n)$\ninitialized and $n^{O(1)}$ uninitialized ancillary qubits, where $n$ is the\ninput length and the initial state of the uninitialized ancillary qubits is\narbitrary. First, we show that such a circuit can compute any symmetric\nfunction on $n$ bits that is classically computable in polynomial time. Then,\nwe regard such a circuit as an oracle and show that a polynomial-time classical\nalgorithm with the oracle can estimate the elements of any unitary matrix\ncorresponding to a constant-depth quantum circuit on $n$ qubits. Since it seems\nunlikely that these tasks can be done with only $O(\\log n)$ initialized\nancillary qubits, our results give evidences that adding uninitialized\nancillary qubits increases the computational power of shallow quantum circuits\nwith only $O(\\log n)$ initialized ancillary qubits. Lastly, to understand the\nlimitations of uninitialized ancillary qubits, we focus on\nnear-logarithmic-depth quantum circuits with them and show the impossibility of\ncomputing the parity function on $n$ bits. \n\n"}
{"id": "1609.00090", "contents": "Title: Attribute Truss Community Search Abstract: Recently, community search over graphs has attracted significant attention\nand many algorithms have been developed for finding dense subgraphs from large\ngraphs that contain given query nodes. In applications such as analysis of\nprotein protein interaction (PPI) networks, citation graphs, and collaboration\nnetworks, nodes tend to have attributes. Unfortunately, previously developed\ncommunity search algorithms ignore these attributes and result in communities\nwith poor cohesion w.r.t. their node attributes. In this paper, we study the\nproblem of attribute-driven community search, that is, given an undirected\ngraph $G$ where nodes are associated with attributes, and an input query $Q$\nconsisting of nodes $V_q$ and attributes $W_q$, find the communities containing\n$V_q$, in which most community members are densely inter-connected and have\nsimilar attributes.\n  We formulate our problem of finding attributed truss communities (ATC), as\nfinding all connected and close k-truss subgraphs containing $V_q$, that are\nlocally maximal and have the largest attribute relevance score among such\nsubgraphs. We design a novel attribute relevance score function and establish\nits desirable properties. The problem is shown to be NP-hard. However, we\ndevelop an efficient greedy algorithmic framework, which finds a maximal\n$k$-truss containing $V_q$, and then iteratively removes the nodes with the\nleast popular attributes and shrinks the graph so as to satisfy community\nconstraints. We also build an elegant index to maintain the known $k$-truss\nstructure and attribute information, and propose efficient query processing\nalgorithms. Extensive experiments on large real-world networks with\nground-truth communities shows the efficiency and effectiveness of our proposed\nmethods. \n\n"}
{"id": "1609.00750", "contents": "Title: Predicting Signed Edges with $O(n^{1+o(1)} \\log{n})$ Queries Abstract: Social networks and interactions in social media involve both positive and\nnegative relationships. Signed graphs capture both types of relationships:\npositive edges correspond to pairs of \"friends\", and negative edges to pairs of\n\"foes\". The {\\em edge sign prediction problem}, which aims to predict whether\nan interaction between a pair of nodes will be positive or negative, is an\nimportant graph mining task for which many heuristics have recently been\nproposed \\cite{leskovec2010predicting,leskovec2010signed}.\n  Motivated by social balance theory, we model the edge sign prediction problem\nas a noisy correlation clustering problem with two clusters. We are allowed to\nquery each pair of nodes whether they belong to the same cluster or not, but\nthe answer to the query is corrupted with some probability $0<q<\\frac{1}{2}$.\nLet $c=\\frac{1}{2}-q$ be the gap. We provide an algorithm that recovers the\nclustering with high probability in the presence of noise for any constant gap\n$c$ with $O(n^{1+\\tfrac{1}{\\log\\log{n}}}\\log{n})$ queries. Our algorithm uses\nsimple breadth first search as its main algorithmic primitive. Finally, we\nprovide a novel generalization to $k \\geq 3$ clusters and prove that our\ntechniques can recover the clustering if the gap is constant in this\ngeneralized setting. \n\n"}
{"id": "1609.02305", "contents": "Title: Survey of Consistent Software-Defined Network Updates Abstract: Computer networks have become a critical infrastructure. In fact, networks\nshould not only meet strict requirements in terms of correctness, availability,\nand performance, but they should also be very flexible and support fast\nupdates, e.g., due to policy changes, increasing traffic, or failures. This\npaper presents a structured survey of mechanism and protocols to update\ncomputer networks in a fast and consistent manner. In particular, we identify\nand discuss the different desirable consistency properties that should be\nprovided throughout a network update, the algorithmic techniques which are\nneeded to meet these consistency properties, and the implications on the speed\nand costs at which updates can be performed. We also explain the relationship\nbetween consistent network update problems and classic algorithmic optimization\nones. While our survey is mainly motivated by the advent of Software-Defined\nNetworks (SDNs) and their primary need for correct and efficient update\ntechniques, the fundamental underlying problems are not new, and we provide a\nhistorical perspective of the subject as well. \n\n"}
{"id": "1609.02901", "contents": "Title: Rapid Mixing of Geodesic Walks on Manifolds with Positive Curvature Abstract: We introduce a Markov chain for sampling from the uniform distribution on a\nRiemannian manifold $\\mathcal{M}$, which we call the $\\textit{geodesic walk}$.\nWe prove that the mixing time of this walk on any manifold with positive\nsectional curvature $C_{x}(u,v)$ bounded both above and below by $0 <\n\\mathfrak{m}_{2} \\leq C_{x}(u,v) \\leq \\mathfrak{M}_2 < \\infty$ is\n$\\mathcal{O}^*\\left(\\frac{\\mathfrak{M}_2}{\\mathfrak{m}_2}\\right)$. In\nparticular, this bound on the mixing time does not depend explicitly on the\ndimension of the manifold. In the special case that $\\mathcal{M}$ is the\nboundary of a convex body, we give an explicit and computationally tractable\nalgorithm for approximating the exact geodesic walk. As a consequence, we\nobtain an algorithm for sampling uniformly from the surface of a convex body\nthat has running time bounded solely in terms of the curvature of the body. \n\n"}
{"id": "1609.04347", "contents": "Title: A Linear Time Parameterized Algorithm for Directed Feedback Vertex Set Abstract: In the Directed Feedback Vertex Set (DFVS) problem, the input is a directed\ngraph $D$ on $n$ vertices and $m$ edges, and an integer $k$. The objective is\nto determine whether there exists a set of at most $k$ vertices intersecting\nevery directed cycle of $D$. Whether or not DFVS admits a fixed parameter\ntractable (FPT) algorithm was considered the most important open problem in\nparameterized complexity until Chen, Liu, Lu, O'Sullivan and Razgon [JACM 2008]\nanswered the question in the affirmative. They gave an algorithm for the\nproblem with running time $O(k!4^kk^4nm)$. Since then, no faster algorithm for\nthe problem has been found. In this paper, we give an algorithm for DFVS with\nrunning time $O(k!4^kk^5(n+m))$. Our algorithm is the first algorithm for DFVS\nwith linear dependence on input size. Furthermore, the asymptotic dependence of\nthe running time of our algorithm on the parameter $k$ matches up to a factor\n$k$ the algorithm of Chen, Liu, Lu, O'Sullivan and Razgon.\n  On the way to designing our algorithm for DFVS, we give a general methodology\nto shave off a factor of $n$ from iterative-compression based algorithms for a\nfew other well-studied covering problems in parameterized complexity. We\ndemonstrate the applicability of this technique by speeding up by a factor of\n$n$, the current best FPT algorithms for Multicut [STOC 2011, SICOMP 2014] and\nDirected Subset Feedback Vertex Set [ICALP 2012, TALG 2014]. \n\n"}
{"id": "1609.05537", "contents": "Title: Quantum Speed-ups for Semidefinite Programming Abstract: We give a quantum algorithm for solving semidefinite programs (SDPs). It has\nworst-case running time $n^{\\frac{1}{2}} m^{\\frac{1}{2}} s^2\n\\text{poly}(\\log(n), \\log(m), R, r, 1/\\delta)$, with $n$ and $s$ the dimension\nand row-sparsity of the input matrices, respectively, $m$ the number of\nconstraints, $\\delta$ the accuracy of the solution, and $R, r$ a upper bounds\non the size of the optimal primal and dual solutions. This gives a square-root\nunconditional speed-up over any classical method for solving SDPs both in $n$\nand $m$. We prove the algorithm cannot be substantially improved (in terms of\n$n$ and $m$) giving a $\\Omega(n^{\\frac{1}{2}}+m^{\\frac{1}{2}})$ quantum lower\nbound for solving semidefinite programs with constant $s, R, r$ and $\\delta$.\n  The quantum algorithm is constructed by a combination of quantum Gibbs\nsampling and the multiplicative weight method. In particular it is based on a\nclassical algorithm of Arora and Kale for approximately solving SDPs. We\npresent a modification of their algorithm to eliminate the need for solving an\ninner linear program which may be of independent interest. \n\n"}
{"id": "1609.05715", "contents": "Title: SpectroMeter: Amortized Sublinear Spectral Approximation of Distance on\n  Graphs Abstract: We present a method to approximate pairwise distance on a graph, having an\namortized sub-linear complexity in its size. The proposed method follows the so\ncalled heat method due to Crane et al. The only additional input are the values\nof the eigenfunctions of the graph Laplacian at a subset of the vertices. Using\nthese values we estimate a random walk from the source points, and normalize\nthe result into a unit gradient function. The eigenfunctions are then used to\nsynthesize distance values abiding by these constraints at desired locations.\nWe show that this method works in practice on different types of inputs ranging\nfrom triangular meshes to general graphs. We also demonstrate that the\nresulting approximate distance is accurate enough to be used as the input to a\nrecent method for intrinsic shape correspondence computation. \n\n"}
{"id": "1609.06736", "contents": "Title: Improving and extending the testing of distributions for\n  shape-restricted properties Abstract: Distribution testing deals with what information can be deduced about an\nunknown distribution over $\\{1,\\ldots,n\\}$, where the algorithm is only allowed\nto obtain a relatively small number of independent samples from the\ndistribution. In the extended conditional sampling model, the algorithm is also\nallowed to obtain samples from the restriction of the original distribution on\nsubsets of $\\{1,\\ldots,n\\}$.\n  In 2015, Canonne, Diakonikolas, Gouleakis and Rubinfeld unified several\nprevious results, and showed that for any property of distributions satisfying\na \"decomposability\" criterion, there exists an algorithm (in the basic model)\nthat can distinguish with high probability distributions satisfying the\nproperty from distributions that are far from it in the variation distance.\n  We present here a more efficient yet simpler algorithm for the basic model,\nas well as very efficient algorithms for the conditional model, which until now\nwas not investigated under the umbrella of decomposable properties.\nAdditionally, we provide an algorithm for the conditional model that handles a\nmuch larger class of properties.\n  Our core mechanism is a way of efficiently producing an interval-partition of\n$\\{1,\\ldots,n\\}$ that satisfies a \"fine-grain\" quality. We show that with such\na partition at hand we can directly move forward with testing individual\nintervals, instead of first searching for the \"correct\" partition of\n$\\{1,\\ldots,n\\}$. \n\n"}
{"id": "1609.07239", "contents": "Title: A Topological Algorithm for Determining How Road Networks Evolve Over\n  Time Abstract: We provide an efficient algorithm for determining how a road network has\nevolved over time, given two snapshot instances from different dates. To allow\nfor such determinations across different databases and even against hand drawn\nmaps, we take a strictly topological approach in this paper, so that we compare\nroad networks based strictly on graph-theoretic properties. Given two road\nnetworks of same region from two different dates, our approach allows one to\nmatch road network portions that remain intact and also point out added or\nremoved portions. We analyze our algorithm both theoretically, showing that it\nruns in polynomial time for non-degenerate road networks even though a related\nproblem is NP-complete, and experimentally, using dated road networks from the\nTIGER/Line archive of the U.S. Census Bureau. \n\n"}
{"id": "1609.08253", "contents": "Title: On the Group and Color Isomorphism Problems Abstract: In this paper, we prove results on the relationship between the complexity of\nthe group and color isomorphism problems. The difficulty of color isomorphism\nproblems is known to be closely linked to the the composition factors of the\npermutation group involved. Previous works are primarily concerned with\napplying color isomorphism to bou nded degree graph isomorphism, and have\ntherefore focused on the alternating composit ion factors, since those are the\nbottleneck in the case of graph isomorphism.\n  We consider the color isomorphism problem with composition factors restricted\nto those other than the alternating group, show that group isomorphism reduces\nin n^(O(log log n)) time to this problem, and, conversely, that a special case\nof this color isomorphism problem reduces to a slight generalization of group\nisomorphism. We then sharpen our results by identifying the projective special\nlinear group as the main obstacle to faster algorithms for group isomorphism\nand prove that the aforementioned reduc tion from group isomorphism to color\nisomorphism in fact produces only cyclic and projective special linear factors.\nOur results demonstrate that, just as the alternatin g group was a barrier to\nfaster algorithms for graph isomorphism for three decades, the projective\nspecial linear group is an obstacle to faster algorithms for group isomorphism. \n\n"}
{"id": "1609.08403", "contents": "Title: Tight Hardness Results for Distance and Centrality Problems in Constant\n  Degree Graphs Abstract: Finding important nodes in a graph and measuring their importance is a\nfundamental problem in the analysis of social networks, transportation\nnetworks, biological systems, etc. Among popular such metrics are graph\ncentrality, betweenness centrality (BC), and reach centrality (RC). These\nmeasures are also very related to classic notions like diameter and radius.\nRoditty and Vassilevska Williams~[STOC'13] showed that no algorithm can compute\na (3/2-\\delta)-approximation of the diameter in sparse and unweighted graphs\nfaster that n^{2-o(1)} time unless the widely believed strong exponential time\nhypothesis (SETH) is false. Abboud et al.~[SODA'15] and [SODA'16] further\nanalyzed these problems under the recent line of research on hardness in P.\nThey showed that in sparse and unweighted graphs (weighted for BC) none of\nthese problems can be solved faster than n^{2-o(1)} unless some popular\nconjecture is false. Furthermore they ruled out a (2-\\delta)-approximation for\nRC, a (3/2-\\delta)-approximation for Radius and a (5/3-\\delta)-approximation\nfor computing all eccentricities of a graph for any \\delta > 0. We extend these\nresults to the case of unweighted graphs with constant maximum degree. Through\nnew graph constructions we are able to obtain the same approximation and time\nbounds as for sparse graphs even in unweighted bounded-degree graphs. We show\nthat no (3/2-\\delta) approximation of Radius or Diameter,\n(2-\\delta)-approximation of RC, (5/3-\\delta)-approximation of all\neccentricities or exact algorithm for BC exists in time n^{2-o(1)} for such\ngraphs and any \\delta > 0. This strengthens the result for BC of Abboud et\nal.~[SODA'16] by showing a hardness result for unweighted graphs, and follows\nin the footsteps of Abboud et al.~[SODA'16] and Abboud and Dahlgaard~[FOCS'16]\nin showing conditional lower bounds for restricted but realistic graph classes. \n\n"}
{"id": "1609.09864", "contents": "Title: Technical Report: Graph-Structured Sparse Optimization for Connected\n  Subgraph Detection Abstract: Structured sparse optimization is an important and challenging problem for\nanalyzing high-dimensional data in a variety of applications such as\nbioinformatics, medical imaging, social networks, and astronomy. Although a\nnumber of structured sparsity models have been explored, such as trees, groups,\nclusters, and paths, connected subgraphs have been rarely explored in the\ncurrent literature. One of the main technical challenges is that there is no\nstructured sparsity-inducing norm that can directly model the space of\nconnected subgraphs, and there is no exact implementation of a projection\noracle for connected subgraphs due to its NP-hardness. In this paper, we\nexplore efficient approximate projection oracles for connected subgraphs, and\npropose two new efficient algorithms, namely, Graph-IHT and Graph-GHTP, to\noptimize a generic nonlinear objective function subject to connectivity\nconstraint on the support of the variables. Our proposed algorithms enjoy\nstrong guarantees analogous to several current methods for sparsity-constrained\noptimization, such as Projected Gradient Descent (PGD), Approximate Model\nIterative Hard Thresholding (AM-IHT), and Gradient Hard Thresholding Pursuit\n(GHTP) with respect to convergence rate and approximation accuracy. We apply\nour proposed algorithms to optimize several well-known graph scan statistics in\nseveral applications of connected subgraph detection as a case study, and the\nexperimental results demonstrate that our proposed algorithms outperform\nstate-of-the-art methods. \n\n"}
{"id": "1610.00505", "contents": "Title: On the Weighted Quartet Consensus problem Abstract: In phylogenetics, the consensus problem consists in summarizing a set of\nphylogenetic trees that all classify the same set of species into a single\ntree. Several definitions of consensus exist in the literature; in this paper\nwe focus on the Weighted Quartet Consensus problem, a problem with unknown\ncomplexity status so far. Here we prove that the Weighted Quartet Consensus\nproblem is NP-hard and we give a 1/2-factor approximation for this problem.\nDuring the process, we propose a derandomization procedure of a previously\nknown randomized 1/3-factor approximation. We also investigate the\nfixed-parameter tractability of this problem. \n\n"}
{"id": "1610.00581", "contents": "Title: Time and Space Efficient Quantum Algorithms for Detecting Cycles and\n  Testing Bipartiteness Abstract: We study space and time efficient quantum algorithms for two graph problems\n-- deciding whether an $n$-vertex graph is a forest, and whether it is\nbipartite. Via a reduction to the s-t connectivity problem, we describe quantum\nalgorithms for deciding both properties in $\\tilde{O}(n^{3/2})$ time and using\n$O(\\log n)$ classical and quantum bits of storage in the adjacency matrix\nmodel. We then present quantum algorithms for deciding the two properties in\nthe adjacency array model, which run in time $\\tilde{O}(n\\sqrt{d_m})$ and also\nrequire $O(\\log n)$ space, where $d_m$ is the maximum degree of any vertex in\nthe input graph. \n\n"}
{"id": "1610.01185", "contents": "Title: Recursion-Theoretic Ranking and Compression Abstract: For which sets A does there exist a mapping, computed by a total or partial\nrecursive function, such that the mapping, when its domain is restricted to A,\nis a 1-to-1, onto mapping to $\\Sigma^*$? And for which sets A does there exist\nsuch a mapping that respects the lexicographical ordering within A? Both cases\nare types of perfect, minimal hash functions. The complexity-theoretic versions\nof these notions are known as compression functions and ranking functions. The\npresent paper defines and studies the recursion-theoretic versions of\ncompression and ranking functions, and in particular studies the question of\nwhich sets have, or lack, such functions. Thus, this is a case where, in\ncontrast to the usual direction of notion transferal, notions from complexity\ntheory are inspiring notions, and an investigation, in computability theory.\n  We show that the rankable and compressible sets broadly populate the\n1-truth-table degrees, and we prove that every nonempty coRE cylinder is\nrecursively compressible. \n\n"}
{"id": "1610.01920", "contents": "Title: Optimal Separation in Exact Query Complexities for Simon's Problem Abstract: Simon's problem is one of the most important problems demonstrating the power\nof quantum computers, which achieves a large separation between quantum and\nclassical query complexities. However, Simon's discussion on his problem was\nlimited to bounded-error setting, which means his algorithm can not always get\nthe correct answer. Exact quantum algorithms for Simon's problem have also been\nproposed, which deterministically solve the problem with O(n) queries. Also the\nquantum lower bound \\Omega(n) for Simon's problem is known. Although these\nalgorithms are either complicated or specialized, their results give an O(n)\nversus \\Omega(\\sqrt{2^{n}}) separation in exact query complexities for Simon's\nproblem (\\Omega(\\sqrt{2^{n}}) is the lower bound for classical probabilistic\nalgorithms), but it has not been proved whether this separation is optimal. In\nthis paper, we propose another exact quantum algorithm for solving Simon's\nproblem with O(n) queries, which is simple, concrete and does not rely on\nspecial query oracles. Our algorithm combines Simon's algorithm with the\nquantum amplitude amplification technique to ensure its determinism. In\nparticular, we show that Simon's problem can be solved by a classical\ndeterministic algorithm with O(\\sqrt{2^{n}}) queries (as we are aware, there\nwere no classical deterministic algorithms for solving Simon's problem with\nO(\\sqrt{2^{n}}) queries). Combining some previous results, we obtain the\noptimal separation in exact query complexities for Simon's problem: \\Theta({n})\nversus \\Theta({\\sqrt{2^{n}}}). \n\n"}
{"id": "1610.03574", "contents": "Title: Robust self-testing of many-qubit states Abstract: We introduce a simple two-player test which certifies that the players apply\ntensor products of Pauli $\\sigma_X$ and $\\sigma_Z$ observables on the tensor\nproduct of $n$ EPR pairs. The test has constant robustness: any strategy\nachieving success probability within an additive $\\varepsilon$ of the optimal\nmust be $\\mathrm{poly}(\\varepsilon)$-close, in the appropriate distance\nmeasure, to the honest $n$-qubit strategy. The test involves $2n$-bit questions\nand $2$-bit answers. The key technical ingredient is a quantum version of the\nclassical linearity test of Blum, Luby, and Rubinfeld.\n  As applications of our result we give (i) the first robust self-test for $n$\nEPR pairs; (ii) a quantum multiprover interactive proof system for the local\nHamiltonian problem with a constant number of provers and classical questions\nand answers, and a constant completeness-soundness gap independent of system\nsize; (iii) a robust protocol for delegated quantum computation. \n\n"}
{"id": "1610.03582", "contents": "Title: QCMA hardness of ground space connectivity for commuting Hamiltonians Abstract: In this work we consider the ground space connectivity problem for commuting\nlocal Hamiltonians. The ground space connectivity problem asks whether it is\npossible to go from one (efficiently preparable) state to another by applying a\npolynomial length sequence of 2-qubit unitaries while remaining at all times in\na state with low energy for a given Hamiltonian $H$. It was shown in [Gharibian\nand Sikora, ICALP'15] that this problem is QCMA-complete for general local\nHamiltonians, where QCMA is defined as QMA with a classical witness and BQP\nverifier. Here we show that the commuting version of the problem is also\nQCMA-complete. This provides one of the first examples where commuting local\nHamiltonians exhibit complexity theoretic hardness equivalent to general local\nHamiltonians. \n\n"}
{"id": "1610.04712", "contents": "Title: A Near-Linear Pseudopolynomial Time Algorithm for Subset Sum Abstract: Given a set $Z$ of $n$ positive integers and a target value $t$, the Subset\nSum problem asks whether any subset of $Z$ sums to $t$. A textbook\npseudopolynomial time algorithm by Bellman from 1957 solves Subset Sum in time\n$O(nt)$. This has been improved to $O(n \\max Z)$ by Pisinger [J. Algorithms'99]\nand recently to $\\tilde O(\\sqrt{n} t)$ by Koiliaris and Xu [SODA'17].\n  Here we present a simple randomized algorithm running in time $\\tilde\nO(n+t)$. This improves upon a classic algorithm and is likely to be\nnear-optimal, since it matches conditional lower bounds from Set Cover and\nk-Clique.\n  We then use our new algorithm and additional tricks to improve the best known\npolynomial space solution from time $\\tilde O(n^3 t)$ and space $\\tilde O(n^2)$\nto time $\\tilde O(nt)$ and space $\\tilde O(n \\log t)$, assuming the Extended\nRiemann Hypothesis. Unconditionally, we obtain time $\\tilde O(n\nt^{1+\\varepsilon})$ and space $\\tilde O(n t^\\varepsilon)$ for any constant\n$\\varepsilon > 0$. \n\n"}
{"id": "1610.08349", "contents": "Title: Multiplayer parallel repetition for expander games Abstract: We investigate the value of parallel repetition of one-round games with any\nnumber of players $k\\ge 2$. It has been an open question whether an analogue of\nRaz's Parallel Repetition Theorem holds for games with more than two players,\ni.e., whether the value of the repeated game decays exponentially with the\nnumber of repetitions. Verbitsky has shown, via a reduction to the density\nHales-Jewett theorem, that the value of the repeated game must approach zero,\nas the number of repetitions increases. However, the rate of decay obtained in\nthis way is extremely slow, and it is an open question whether the true rate is\nexponential as is the case for all two-player games.\n  Exponential decay bounds are known for several special cases of multi-player\ngames, e.g., free games and anchored games. In this work, we identify a certain\nexpansion property of the base game and show all games with this property\nsatisfy an exponential decay parallel repetition bound. Free games and anchored\ngames satisfy this expansion property, and thus our parallel repetition theorem\nreproduces all earlier exponential-decay bounds for multiplayer games. More\ngenerally, our parallel repetition bound applies to all multiplayer games that\nare connected in a certain sense.\n  We also describe a very simple game, called the GHZ game, that does not\nsatisfy this connectivity property, and for which we do not know an exponential\ndecay bound. We suspect that progress on bounding the value of this the\nparallel repetition of the GHZ game will lead to further progress on the\ngeneral question. \n\n"}
{"id": "1610.08809", "contents": "Title: Aligning coding sequences with frameshift extension penalties Abstract: Frameshift translation is an important phenomenon that contributes to the\nappearance of novel Coding DNA Sequences (CDS) and functions in gene evolution,\nby allowing alternative amino acid translations of genes coding regions.\nFrameshift translations can be identified by aligning two CDS, from a same gene\nor from homologous genes, while accounting for their codon structure. Two main\nclasses of algorithms have been proposed to solve the problem of aligning CDS,\neither by amino acid sequence alignment back-translation, or by simultaneously\naccounting for the nucleotide and amino acid levels. The former does not allow\nto account for frameshift translations and up to now, the latter exclusively\naccounts for frameshift translation initiation, not accounting for the length\nof the translation disruption caused by a frameshift.\n  Here, we introduce a new scoring scheme with an algorithm for the pairwise\nalignment of CDS accounting for frameshift translation initiation and length,\nwhile simultaneously accounting for nucleotide and amino acid sequences. We\ncompare the method to other CDS alignment methods based on an application to\nthe comparison of pairs of CDS from homologous \\emph{human}, \\emph{mouse} and\n\\emph{cow} genes of ten mammalian gene families from the Ensembl-Compara\ndatabase. The results show that our method is particularly robust to parameter\nchanges as compared to existing methods. It also appears to be a good\ncompromise, performing well both in the presence and absence of frameshift\ntranslations between the CDS. An implementation of the method is available at\nhttps://github.com/UdeS-CoBIUS/FsePSA. \n\n"}
{"id": "1610.09660", "contents": "Title: Canonical Functions: a proof via topological dynamics Abstract: Canonical functions are a powerful concept with numerous applications in the\nstudy of groups, monoids, and clones on countable structures with Ramsey-type\nproperties. In this short note, we present a proof of the existence of\ncanonical functions in certain sets using topological dynamics, providing a\nshorter alternative to the original combinatorial argument. We moreover present\nequivalent algebraic characterisations of canonicity. \n\n"}
{"id": "1611.00665", "contents": "Title: Combinatorial Prophet Inequalities Abstract: We introduce a novel framework of Prophet Inequalities for combinatorial\nvaluation functions. For a (non-monotone) submodular objective function over an\narbitrary matroid feasibility constraint, we give an $O(1)$-competitive\nalgorithm. For a monotone subadditive objective function over an arbitrary\ndownward-closed feasibility constraint, we give an $O(\\log n \\log^2\nr)$-competitive algorithm (where $r$ is the cardinality of the largest feasible\nsubset).\n  Inspired by the proof of our subadditive prophet inequality, we also obtain\nan $O(\\log n \\cdot \\log^2 r)$-competitive algorithm for the Secretary Problem\nwith a monotone subadditive objective function subject to an arbitrary\ndownward-closed feasibility constraint. Even for the special case of a\ncardinality feasibility constraint, our algorithm circumvents an\n$\\Omega(\\sqrt{n})$ lower bound by Bateni, Hajiaghayi, and Zadimoghaddam\n\\cite{BHZ13-submodular-secretary_original} in a restricted query model.\n  En route to our submodular prophet inequality, we prove a technical result of\nindependent interest: we show a variant of the Correlation Gap Lemma for\nnon-monotone submodular functions. \n\n"}
{"id": "1611.00755", "contents": "Title: Almost-Linear-Time Algorithms for Markov Chains and New Spectral\n  Primitives for Directed Graphs Abstract: In this paper we introduce a notion of spectral approximation for directed\ngraphs. While there are many potential ways one might define approximation for\ndirected graphs, most of them are too strong to allow sparse approximations in\ngeneral. In contrast, we prove that for our notion of approximation, such\nsparsifiers do exist, and we show how to compute them in almost linear time.\n  Using this notion of approximation, we provide a general framework for\nsolving asymmetric linear systems that is broadly inspired by the work of\n[Peng-Spielman, STOC`14]. Applying this framework in conjunction with our\nsparsification algorithm, we obtain an almost linear time algorithm for solving\ndirected Laplacian systems associated with Eulerian Graphs. Using this solver\nin the recent framework of [Cohen-Kelner-Peebles-Peng-Sidford-Vladu, FOCS`16],\nwe obtain almost linear time algorithms for solving a directed Laplacian linear\nsystem, computing the stationary distribution of a Markov chain, computing\nexpected commute times in a directed graph, and more.\n  For each of these problems, our algorithms improves the previous best running\ntimes of $O((nm^{3/4} + n^{2/3} m) \\log^{O(1)} (n \\kappa \\epsilon^{-1}))$ to\n$O((m + n2^{O(\\sqrt{\\log{n}\\log\\log{n}})}) \\log^{O(1)} (n \\kappa\n\\epsilon^{-1}))$ where $n$ is the number of vertices in the graph, $m$ is the\nnumber of edges, $\\kappa$ is a natural condition number associated with the\nproblem, and $\\epsilon$ is the desired accuracy. We hope these results open the\ndoor for further studies into directed spectral graph theory, and will serve as\na stepping stone for designing a new generation of fast algorithms for directed\ngraphs. \n\n"}
{"id": "1611.03473", "contents": "Title: Statistical Query Lower Bounds for Robust Estimation of High-dimensional\n  Gaussians and Gaussian Mixtures Abstract: We describe a general technique that yields the first {\\em Statistical Query\nlower bounds} for a range of fundamental high-dimensional learning problems\ninvolving Gaussian distributions. Our main results are for the problems of (1)\nlearning Gaussian mixture models (GMMs), and (2) robust (agnostic) learning of\na single unknown Gaussian distribution. For each of these problems, we show a\n{\\em super-polynomial gap} between the (information-theoretic) sample\ncomplexity and the computational complexity of {\\em any} Statistical Query\nalgorithm for the problem. Our SQ lower bound for Problem (1) is qualitatively\nmatched by known learning algorithms for GMMs. Our lower bound for Problem (2)\nimplies that the accuracy of the robust learning algorithm\nin~\\cite{DiakonikolasKKLMS16} is essentially best possible among all\npolynomial-time SQ algorithms.\n  Our SQ lower bounds are attained via a unified moment-matching technique that\nis useful in other contexts and may be of broader interest. Our technique\nyields nearly-tight lower bounds for a number of related unsupervised\nestimation problems. Specifically, for the problems of (3) robust covariance\nestimation in spectral norm, and (4) robust sparse mean estimation, we\nestablish a quadratic {\\em statistical--computational tradeoff} for SQ\nalgorithms, matching known upper bounds. Finally, our technique can be used to\nobtain tight sample complexity lower bounds for high-dimensional {\\em testing}\nproblems. Specifically, for the classical problem of robustly {\\em testing} an\nunknown mean (known covariance) Gaussian, our technique implies an\ninformation-theoretic sample lower bound that scales {\\em linearly} in the\ndimension. Our sample lower bound matches the sample complexity of the\ncorresponding robust {\\em learning} problem and separates the sample complexity\nof robust testing from standard (non-robust) testing. \n\n"}
{"id": "1611.06650", "contents": "Title: Trading information complexity for error Abstract: We consider the standard two-party communication model. The central problem\nstudied in this article is how much one can save in information complexity by\nallowing an error of $\\epsilon$.\n  For arbitrary functions, we obtain lower bounds and upper bounds indicating a\ngain that is of order $\\Omega(h(\\epsilon))$ and $O(h(\\sqrt{\\epsilon}))$. Here\n$h$ denotes the binary entropy function. We analyze the case of the two-bit AND\nfunction in detail to show that for this function the gain is\n$\\Theta(h(\\epsilon))$. This answers a question of [M. Braverman, A. Garg, D.\nPankratov, and O. Weinstein, From information to exact communication (extended\nabstract), STOC'13].\n  We obtain sharp bounds for the set disjointness function of order $n$. For\nthe case of the distributional error, we introduce a new protocol that achieves\na gain of $\\Theta(\\sqrt{h(\\epsilon)})$ provided that $n$ is sufficiently large.\nWe apply these results to answer another of question of Braverman et al.\nregarding the randomized communication complexity of the set disjointness\nfunction.\n  Answering a question of [Mark Braverman, Interactive information complexity,\nSTOC'12], we apply our analysis of the set disjointness function to establish a\ngap between the two different notions of the prior-free information cost. This\nimplies that amortized randomized communication complexity is not necessarily\nequal to the amortized distributional communication complexity with respect to\nthe hardest distribution. \n\n"}
{"id": "1611.08326", "contents": "Title: Detecting communities is hard, and counting them is even harder Abstract: We consider the algorithmic problem of community detection in networks. Given\nan undirected friendship graph $G=\\left(V,E\\right)$, a subset $S\\subseteq V$ is\nan $\\left(\\alpha,\\beta\\right)$-community if:\n  * Every member of the community is friends with an $\\alpha$-fraction of the\ncommunity;\n  * Every non-member is friends with at most a $\\beta$-fraction of the\ncommunity.\n  Arora et al [AGSS12] gave a quasi-polynomial time algorithm for enumerating\nall the $\\left(\\alpha,\\beta\\right)$-communities for any constants\n$\\alpha>\\beta$.\n  Here, we prove that, assuming the Exponential Time Hypothesis (ETH),\nquasi-polynomial time is in fact necessary - and even for a much weaker\napproximation desideratum. Namely, distinguishing between:\n  * $G$ contains an $\\left(1,o\\left(1\\right)\\right)$-community; and\n  * $G$ does not contain an\n$\\left(\\beta+o\\left(1\\right),\\beta\\right)$-community for any\n$\\beta\\in\\left[0,1\\right]$.\n  We also prove that counting the number of\n$\\left(1,o\\left(1\\right)\\right)$-communities requires quasi-polynomial time\nassuming the weaker #ETH. \n\n"}
{"id": "1611.08946", "contents": "Title: Exponential Separation of Quantum Communication and Classical\n  Information Abstract: We exhibit a Boolean function for which the quantum communication complexity\nis exponentially larger than the classical information complexity. An\nexponential separation in the other direction was already known from the work\nof Kerenidis et. al. [SICOMP 44, pp. 1550-1572], hence our work implies that\nthese two complexity measures are incomparable. As classical information\ncomplexity is an upper bound on quantum information complexity, which in turn\nis equal to amortized quantum communication complexity, our work implies that a\ntight direct sum result for distributional quantum communication complexity\ncannot hold. The function we use to present such a separation is the Symmetric\nk-ary Pointer Jumping function introduced by Rao and Sinha [ECCC TR15-057],\nwhose classical communication complexity is exponentially larger than its\nclassical information complexity. In this paper, we show that the quantum\ncommunication complexity of this function is polynomially equivalent to its\nclassical communication complexity. The high-level idea behind our proof is\narguably the simplest so far for such an exponential separation between\ninformation and communication, driven by a sequence of round-elimination\narguments, allowing us to simplify further the approach of Rao and Sinha.\n  As another application of the techniques that we develop, we give a simple\nproof for an optimal trade-off between Alice's and Bob's communication while\ncomputing the related Greater-Than function on n bits: say Bob communicates at\nmost b bits, then Alice must send n/exp(O(b)) bits to Bob. This holds even when\nallowing pre-shared entanglement. We also present a classical protocol\nachieving this bound. \n\n"}
{"id": "1612.00547", "contents": "Title: Gradient Descent Finds the Cubic-Regularized Non-Convex Newton Step Abstract: We consider the minimization of non-convex quadratic forms regularized by a\ncubic term, which exhibit multiple saddle points and poor local minima.\nNonetheless, we prove that, under mild assumptions, gradient descent\napproximates the $\\textit{global minimum}$ to within $\\varepsilon$ accuracy in\n$O(\\varepsilon^{-1}\\log(1/\\varepsilon))$ steps for large $\\varepsilon$ and\n$O(\\log(1/\\varepsilon))$ steps for small $\\varepsilon$ (compared to a condition\nnumber we define), with at most logarithmic dependence on the problem\ndimension. When we use gradient descent to approximate the cubic-regularized\nNewton step, our result implies a rate of convergence to second-order\nstationary points of general smooth non-convex functions. \n\n"}
{"id": "1612.01527", "contents": "Title: Matrix multiplication algorithms from group orbits Abstract: We show how to construct highly symmetric algorithms for matrix\nmultiplication. In particular, we consider algorithms which decompose the\nmatrix multiplication tensor into a sum of rank-1 tensors, where the\ndecomposition itself consists of orbits under some finite group action. We show\nhow to use the representation theory of the corresponding group to derive\nsimple constraints on the decomposition, which we solve by hand for n=2,3,4,5,\nrecovering Strassen's algorithm (in a particularly symmetric form) and new\nalgorithms for larger n. While these new algorithms do not improve the known\nupper bounds on tensor rank or the matrix multiplication exponent, they are\nbeautiful in their own right, and we point out modifications of this idea that\ncould plausibly lead to further improvements. Our constructions also suggest\nfurther patterns that could be mined for new algorithms, including a\ntantalizing connection with lattices. In particular, using lattices we give the\nmost transparent proof to date of Strassen's algorithm; the same proof works\nfor all n, to yield a decomposition with $n^3 - n + 1$ terms. \n\n"}
{"id": "1612.01817", "contents": "Title: Pseudodeterministic Constructions in Subexponential Time Abstract: We study pseudodeterministic constructions, i.e., randomized algorithms which\noutput the same solution on most computation paths. We establish\nunconditionally that there is an infinite sequence $\\{p_n\\}_{n \\in \\mathbb{N}}$\nof increasing primes and a randomized algorithm $A$ running in expected\nsub-exponential time such that for each $n$, on input $1^{|p_n|}$, $A$ outputs\n$p_n$ with probability $1$. In other words, our result provides a\npseudodeterministic construction of primes in sub-exponential time which works\ninfinitely often.\n  This result follows from a much more general theorem about\npseudodeterministic constructions. A property $Q \\subseteq \\{0,1\\}^{*}$ is\n$\\gamma$-dense if for large enough $n$, $|Q \\cap \\{0,1\\}^n| \\geq \\gamma 2^n$.\nWe show that for each $c > 0$ at least one of the following holds: (1) There is\na pseudodeterministic polynomial time construction of a family $\\{H_n\\}$ of\nsets, $H_n \\subseteq \\{0,1\\}^n$, such that for each $(1/n^c)$-dense property $Q\n\\in \\mathsf{DTIME}(n^c)$ and every large enough $n$, $H_n \\cap Q \\neq\n\\emptyset$; or (2) There is a deterministic sub-exponential time construction\nof a family $\\{H'_n\\}$ of sets, $H'_n \\subseteq \\{0,1\\}^n$, such that for each\n$(1/n^c)$-dense property $Q \\in \\mathsf{DTIME}(n^c)$ and for infinitely many\nvalues of $n$, $H'_n \\cap Q \\neq \\emptyset$.\n  We provide further algorithmic applications that might be of independent\ninterest. Perhaps intriguingly, while our main results are unconditional, they\nhave a non-constructive element, arising from a sequence of applications of the\nhardness versus randomness paradigm. \n\n"}
{"id": "1612.02503", "contents": "Title: What do Shannon-type Inequalities, Submodular Width, and Disjunctive\n  Datalog have to do with one another? Abstract: Recent works on bounding the output size of a conjunctive query with\nfunctional dependencies and degree constraints have shown a deep connection\nbetween fundamental questions in information theory and database theory. We\nprove analogous output bounds for disjunctive datalog rules, and answer several\nopen questions regarding the tightness and looseness of these bounds along the\nway. Our bounds are intimately related to Shannon-type information\ninequalities. We devise the notion of a \"proof sequence\" of a specific class of\nShannon-type information inequalities called \"Shannon flow inequalities\". We\nthen show how such a proof sequence can be interpreted as symbolic instructions\nguiding an algorithm called \"PANDA\", which answers disjunctive datalog rules\nwithin the time that the size bound predicted. We show that PANDA can be used\nas a black-box to devise algorithms matching precisely the fractional hypertree\nwidth and the submodular width runtimes for aggregate and conjunctive queries\nwith functional dependencies and degree constraints.\n  Our results improve upon known results in three ways. First, our bounds and\nalgorithms are for the much more general class of disjunctive datalog rules, of\nwhich conjunctive queries are a special case. Second, the runtime of PANDA\nmatches precisely the submodular width bound, while the previous algorithm by\nMarx has a runtime that is polynomial in this bound. Third, our bounds and\nalgorithms work for queries with input cardinality bounds, functional\ndependencies, and degree constraints.\n  Overall, our results show a deep connection between three seemingly unrelated\nlines of research; and, our results on proof sequences for Shannon flow\ninequalities might be of independent interest. \n\n"}
{"id": "1612.05460", "contents": "Title: A Dual Ascent Framework for Lagrangean Decomposition of Combinatorial\n  Problems Abstract: We propose a general dual ascent framework for Lagrangean decomposition of\ncombinatorial problems. Although methods of this type have shown their\nefficiency for a number of problems, so far there was no general algorithm\napplicable to multiple problem types. In his work, we propose such a general\nalgorithm. It depends on several parameters, which can be used to optimize its\nperformance in each particular setting. We demonstrate efficacy of our method\non graph matching and multicut problems, where it outperforms state-of-the-art\nsolvers including those based on subgradient optimization and off-the-shelf\nlinear programming solvers. \n\n"}
{"id": "1612.05531", "contents": "Title: A general purpose algorithm for counting simple cycles and simple paths\n  of any length Abstract: We describe a general purpose algorithm for counting simple cycles and simple\npaths of any length $\\ell$ on a (weighted di)graph on $N$ vertices and $M$\nedges, achieving a time complexity of\n$O\\left(N+M+\\big(\\ell^\\omega+\\ell\\Delta\\big) |S_\\ell|\\right)$. In this\nexpression, $|S_\\ell|$ is the number of (weakly) connected induced subgraphs of\n$G$ on at most $\\ell$ vertices, $\\Delta$ is the maximum degree of any vertex\nand $\\omega$ is the exponent of matrix multiplication. We compare the algorithm\ncomplexity both theoretically and experimentally with most of the existing\nalgorithms for the same task. These comparisons show that the algorithm\ndescribed here is the best general purpose algorithm for the class of graphs\nwhere $(\\ell^{\\omega-1}\\Delta^{-1}+1) |S_\\ell|\\leq |\\text{Cycle}_\\ell|$, with\n$|\\text{Cycle}_\\ell|$ the total number of simple cycles of length at most\n$\\ell$, including backtracks and self-loops. On Erd\\H{o}s-R\\'enyi random\ngraphs, we find empirically that this happens when the edge probability is\nlarger than circa $4/N$. In addition, we show that some real-world networks\nalso belong to this class. Finally, the algorithm permits the enumeration of\nsimple cycles and simple paths on networks where vertices are labeled from an\nalphabet on $n$ letters with a time complexity of\n$O\\left(N+M+\\big(n^\\ell\\ell^\\omega+\\ell\\Delta\\big) |S_\\ell|\\right)$. A Matlab\nimplementation of the algorithm proposed here is available for download. \n\n"}
{"id": "1612.07162", "contents": "Title: Supercritical Space-Width Trade-offs for Resolution Abstract: We show that there are CNF formulas which can be refuted in resolution in\nboth small space and small width, but for which any small-width proof must have\nspace exceeding by far the linear worst-case upper bound. This significantly\nstrengthens the space-width trade-offs in [Ben-Sasson '09]}, and provides one\nmore example of trade-offs in the \"supercritical\" regime above worst case\nrecently identified by [Razborov '16]. We obtain our results by using\nRazborov's new hardness condensation technique and combining it with the space\nlower bounds in [Ben-Sasson and Nordstrom '08]. \n\n"}
{"id": "1612.07276", "contents": "Title: Splitting $B_2$-VPG graphs into outer-string and co-comparability graphs Abstract: In this paper, we show that any $B_2$-VPG graph (i.e., an intersection graph\nof orthogonal curves with at most 2 bends) can be decomposed into $O(\\log n)$\nouterstring graphs or $O(\\log^3 n)$ permutation graphs. This leads to better\napproximation algorithms for hereditary graph problems, such as independent\nset, clique and clique cover, on $B_2$-VPG graphs. \n\n"}
{"id": "1612.09083", "contents": "Title: A Constant Optimization of the Binary Indexed Tree Query Operation Abstract: There are several data structures which can calculate the prefix sums of an\narray efficiently, while handling point updates on the array, such as Segment\nTrees and Binary Indexed Trees (BIT). Both these data structures can handle the\nthese two operations (query and update) in $O(\\log{n})$ time. In this paper, we\npresent a data structure similar to the BIT, but with an even smaller constant.\nTo do this, we use Zeckendorf's Theorem, a property of the Fibonacci sequence\nof numbers. The new data structure achieves the same complexity of\n$O(\\log{n})$, but requires about $\\log_{\\phi^{2}} n$ computations for the Query\nOperation as opposed to the $\\log_{2} n$ computations required for a BIT Query\nOperation in the worst case. \n\n"}
{"id": "1701.01485", "contents": "Title: Non interactive simulation of correlated distributions is decidable Abstract: A basic problem in information theory is the following: Let $\\mathbf{P} =\n(\\mathbf{X}, \\mathbf{Y})$ be an arbitrary distribution where the marginals\n$\\mathbf{X}$ and $\\mathbf{Y}$ are (potentially) correlated. Let Alice and Bob\nbe two players where Alice gets samples $\\{x_i\\}_{i \\ge 1}$ and Bob gets\nsamples $\\{y_i\\}_{i \\ge 1}$ and for all $i$, $(x_i, y_i) \\sim \\mathbf{P}$. What\njoint distributions $\\mathbf{Q}$ can be simulated by Alice and Bob without any\ninteraction?\n  Classical works in information theory by G{\\'a}cs-K{\\\"o}rner and Wyner answer\nthis question when at least one of $\\mathbf{P}$ or $\\mathbf{Q}$ is the\ndistribution on $\\{0,1\\} \\times \\{0,1\\}$ where each marginal is unbiased and\nidentical. However, other than this special case, the answer to this question\nis understood in very few cases. Recently, Ghazi, Kamath and Sudan showed that\nthis problem is decidable for $\\mathbf{Q}$ supported on $\\{0,1\\} \\times\n\\{0,1\\}$. We extend their result to $\\mathbf{Q}$ supported on any finite\nalphabet.\n  We rely on recent results in Gaussian geometry (by the authors) as well as a\nnew \\emph{smoothing argument} inspired by the method of \\emph{boosting} from\nlearning theory and potential function arguments from complexity theory and\nadditive combinatorics. \n\n"}
{"id": "1701.02302", "contents": "Title: A Homological Theory of Functions Abstract: In computational complexity, a complexity class is given by a set of problems\nor functions, and a basic challenge is to show separations of complexity\nclasses $A \\not= B$ especially when $A$ is known to be a subset of $B$. In this\npaper we introduce a homological theory of functions that can be used to\nestablish complexity separations, while also providing other interesting\nconsequences. We propose to associate a topological space $S_A$ to each class\nof functions $A$, such that, to separate complexity classes $A \\subseteq B'$,\nit suffices to observe a change in \"the number of holes\", i.e. homology, in\n$S_A$ as a subclass $B$ of $B'$ is added to $A$. In other words, if the\nhomologies of $S_A$ and $S_{A \\cup B}$ are different, then $A \\not= B'$. We\ndevelop the underlying theory of functions based on combinatorial and\nhomological commutative algebra and Stanley-Reisner theory, and recover Minsky\nand Papert's 1969 result that parity cannot be computed by nonmaximal degree\npolynomial threshold functions. In the process, we derive a \"maximal principle\"\nfor polynomial threshold functions that is used to extend this result further\nto arbitrary symmetric functions. A surprising coincidence is demonstrated,\nwhere the maximal dimension of \"holes\" in $S_A$ upper bounds the VC dimension\nof $A$, with equality for common computational cases such as the class of\npolynomial threshold functions or the class of linear functionals in $\\mathbb\nF_2$, or common algebraic cases such as when the Stanley-Reisner ring of $S_A$\nis Cohen-Macaulay. As another interesting application of our theory, we prove a\nresult that a priori has nothing to do with complexity separation: it\ncharacterizes when a vector subspace intersects the positive cone, in terms of\nhomological conditions. By analogy to Farkas' result doing the same with\n*linear conditions*, we call our theorem the Homological Farkas Lemma. \n\n"}
{"id": "1701.02628", "contents": "Title: Greed is Good: Optimistic Algorithms for Bipartite-Graph Partial\n  Coloring on Multicore Architectures Abstract: In parallel computing, a valid graph coloring yields a lock-free processing\nof the colored tasks, data points, etc., without expensive synchronization\nmechanisms. However, coloring is not free and the overhead can be significant.\nIn particular, for the bipartite-graph partial coloring (BGPC) and distance-2\ngraph coloring (D2GC) problems, which have various use-cases within the\nscientific computing and numerical optimization domains, the coloring overhead\ncan be in the order of minutes with a single thread for many real-life graphs.\n  In this work, we propose parallel algorithms for bipartite-graph partial\ncoloring on shared-memory architectures. Compared to the existing shared-memory\nBGPC algorithms, the proposed ones employ greedier and more optimistic\ntechniques that yield a better parallel coloring performance. In particular, on\n16 cores, the proposed algorithms perform more than 4x faster than their\ncounterparts in the ColPack library which is, to the best of our knowledge, the\nonly publicly-available coloring library for multicore architectures. In\naddition to BGPC, the proposed techniques are employed to devise parallel\ndistance-2 graph coloring algorithms and similar performance improvements have\nbeen observed. Finally, we propose two costless balancing heuristics for BGPC\nthat can reduce the skewness and imbalance on the cardinality of color sets\n(almost) for free. The heuristics can also be used for the D2GC problem and in\ngeneral, they will probably yield a better color-based parallelization\nperformance especially on many-core architectures. \n\n"}
{"id": "1701.02836", "contents": "Title: Computing Abelian regularities on RLE strings Abstract: Two strings x and y are said to be Abelian equivalent if x is a permutation\nof y, or vice versa. If a string z satisfies z = xy with x and y being Abelian\nequivalent, then z is said to be an Abelian square. If a string w can be\nfactorized into a sequence v_1,...,v_s of strings such that v_1 ,..., v_{s-1}\nare all Abelian equivalent and vs is a substring of a permutation of v_1, then\nw is said to have a regular Abelian period (p,t) where p = |v_1| and t = |v_s|.\nIf a substring w_1[i..i+l-1] of a string w_1 and a substring w_2[j..j+l-1] of\nanother string w_2 are Abelian equivalent, then the substrings are said to be a\ncommon Abelian factor of w_1 and w_2 and if the length l is the maximum of such\nthen the substrings are said to be a longest common Abelian factor of w_1 and\nw_2. We propose efficient algorithms which compute these Abelian regularities\nusing the run length encoding (RLE) of strings. For a given string w of length\nn whose RLE is of size m, we propose algorithms which compute all Abelian\nsquares occurring in w in O(mn) time, and all regular Abelian periods of w in\nO(mn) time. For two given strings w_1 and w_2 of total length n and of total\nRLE size m, we propose an algorithm which computes all longest common Abelian\nfactors in O(m^2n) time. \n\n"}
{"id": "1701.03990", "contents": "Title: Quantum algorithm for multivariate polynomial interpolation Abstract: How many quantum queries are required to determine the coefficients of a\ndegree-$d$ polynomial in $n$ variables? We present and analyze quantum\nalgorithms for this multivariate polynomial interpolation problem over the\nfields $\\mathbb{F}_q$, $\\mathbb{R}$, and $\\mathbb{C}$. We show that\n$k_{\\mathbb{C}}$ and $2k_{\\mathbb{C}}$ queries suffice to achieve probability\n$1$ for $\\mathbb{C}$ and $\\mathbb{R}$, respectively, where\n$k_{\\mathbb{C}}=\\smash{\\lceil\\frac{1}{n+1}{n+d\\choose d}\\rceil}$ except for\n$d=2$ and four other special cases. For $\\mathbb{F}_q$, we show that\n$\\smash{\\lceil\\frac{d}{n+d}{n+d\\choose d}\\rceil}$ queries suffice to achieve\nprobability approaching $1$ for large field order $q$. The classical query\ncomplexity of this problem is $\\smash{n+d\\choose d}$, so our result provides a\nspeedup by a factor of $n+1$, $\\frac{n+1}{2}$, and $\\frac{n+d}{d}$ for\n$\\mathbb{C}$, $\\mathbb{R}$, and $\\mathbb{F}_q$, respectively. Thus we find a\nmuch larger gap between classical and quantum algorithms than the univariate\ncase, where the speedup is by a factor of $2$. For the case of $\\mathbb{F}_q$,\nwe conjecture that $2k_{\\mathbb{C}}$ queries also suffice to achieve\nprobability approaching $1$ for large field order $q$, although we leave this\nas an open problem. \n\n"}
{"id": "1701.06064", "contents": "Title: On Recoverable and Two-Stage Robust Selection Problems with Budgeted\n  Uncertainty Abstract: In this paper the problem of selecting $p$ out of $n$ available items is\ndiscussed, such that their total cost is minimized. We assume that costs are\nnot known exactly, but stem from a set of possible outcomes.\n  Robust recoverable and two-stage models of this selection problem are\nanalyzed. In the two-stage problem, up to $p$ items is chosen in the first\nstage, and the solution is completed once the scenario becomes revealed in the\nsecond stage. In the recoverable problem, a set of $p$ items is selected in the\nfirst stage, and can be modified by exchanging up to $k$ items in the second\nstage, after a scenario reveals.\n  We assume that uncertain costs are modeled through bounded uncertainty sets,\ni.e., the interval uncertainty sets with an additional linear (budget)\nconstraint, in their discrete and continuous variants. Polynomial algorithms\nfor recoverable and two-stage selection problems with continuous bounded\nuncertainty, and compact mixed integer formulations in the case of discrete\nbounded uncertainty are constructed. \n\n"}
{"id": "1701.06386", "contents": "Title: A Structured View on Weighted Counting with Relations to Counting,\n  Quantum Computation and Applications Abstract: Weighted counting problems are a natural generalization of counting problems\nwhere a weight is associated with every computational path of polynomial-time\nnon-deterministic Turing machines and the goal is to compute the sum of the\nweights of all paths (instead of just computing the number of accepting paths).\nUseful closure properties and plenty of applications make weighted counting\nproblems interesting. The general definition of these problems captures even\nundecidable problems, but it turns out that obtaining an exponentially small\nadditive approximation is just as hard as solving conventional counting\nproblems. In many cases such an approximation is sufficient and working with\nweighted counting problems tends to be very convenient. We present a structured\nview on weighted counting by defining classes that depend on the range of the\nfunction that assigns weights to paths and by showing the relationships between\nthese different classes. These classes constitute generalizations of the usual\ncounting problems. Weighted counting allows us to easily cast a number of\nfamous results of computational complexity in its terms, especially regarding\ncounting and quantum computation. Moreover, these classes are flexible enough\nand capture the complexity of various problems in fields such as probabilistic\ngraphical models and stochastic combinatorial optimization. Using the weighted\ncounting terminology and our results, we are able to simplify and answer some\nopen questions in those fields. \n\n"}
{"id": "1701.08022", "contents": "Title: KMC 3: counting and manipulating k-mer statistics Abstract: Summary: Counting all k-mers in a given dataset is a standard procedure in\nmany bioinformatics applications. We introduce KMC3, a significant improvement\nof the former KMC2 algorithm together with KMC tools for manipulating k-mer\ndatabases. Usefulness of the tools is shown on a few real problems.\nAvailability: Program is freely available at\nhttp://sun.aei.polsl.pl/REFRESH/kmc. Contact: sebastian.deorowicz@polsl.pl \n\n"}
{"id": "1702.00353", "contents": "Title: The non-cooperative tile assembly model is not intrinsically universal\n  or capable of bounded Turing machine simulation Abstract: The field of algorithmic self-assembly is concerned with the computational\nand expressive power of nanoscale self-assembling molecular systems. In the\nwell-studied cooperative, or temperature 2, abstract tile assembly model it is\nknown that there is a tile set to simulate any Turing machine and an\nintrinsically universal tile set that simulates the shapes and dynamics of any\ninstance of the model, up to spatial rescaling. It has been an open question as\nto whether the seemingly simpler noncooperative, or temperature 1, model is\ncapable of such behaviour. Here we show that this is not the case, by showing\nthat there is no tile set in the noncooperative model that is intrinsically\nuniversal, nor one capable of time-bounded Turing machine simulation within a\nbounded region of the plane.\n  Although the noncooperative model intuitively seems to lack the complexity\nand power of the cooperative model it was not obvious how to prove this. One\nreason is that there have been few tools to analyse the structure of\ncomplicated paths in the plane. This paper provides a number of such tools. A\nsecond reason is that almost every obvious and small generalisation to the\nmodel (e.g. allowing error, 3D, non-square tiles, signals/wires on tiles, tiles\nthat repel each other, parallel synchronous growth) endows it with great\ncomputational, and sometimes simulation, power. Our main results show that all\nof these generalisations provably increase computational and/or simulation\npower. Our results hold for both deterministic and nondeterministic\nnoncooperative systems. Our first main result stands in stark contrast with the\nfact that for both the cooperative tile assembly model, and for 3D\nnoncooperative tile assembly, there are respective intrinsically universal\ntilesets. Our second main result gives a new technique (reduction to\nsimulation) for proving negative results about computation in tile assembly. \n\n"}
{"id": "1702.01703", "contents": "Title: Variant tolerant read mapping using min-hashing Abstract: DNA read mapping is a ubiquitous task in bioinformatics, and many tools have\nbeen developed to solve the read mapping problem. However, there are two trends\nthat are changing the landscape of readmapping: First, new sequencing\ntechnologies provide very long reads with high error rates (up to 15%). Second,\nmany genetic variants in the population are known, so the reference genome is\nnot considered as a single string over ACGT, but as a complex object containing\nthese variants. Most existing read mappers do not handle these new\ncircumstances appropriately.\n  We introduce a new read mapper prototype called VATRAM that considers\nvariants. It is based on Min-Hashing of q-gram sets of reference genome\nwindows. Min-Hashing is one form of locality sensitive hashing. The variants\nare directly inserted into VATRAMs index which leads to a fast mapping process.\nOur results show that VATRAM achieves better precision and recall than\nstate-of-the-art read mappers like BWA under certain cirumstances. VATRAM is\nopen source and can be accessed at\nhttps://bitbucket.org/Quedenfeld/vatram-src/. \n\n"}
{"id": "1702.02133", "contents": "Title: A New Graph Parameter To Measure Linearity Abstract: Consider a sequence of LexBFS vertex orderings {\\sigma}1, {\\sigma}2, . . .\nwhere each ordering {\\sigma}i is used to break ties for {\\sigma}i+1. Since the\ntotal number of vertex orderings of a finite graph is finite, this sequence\nmust end in a cycle of vertex orderings. The possible length of this cycle is\nthe main subject of this work. Intuitively, we prove for graphs with a known\nnotion of linearity (e.g., interval graphs with their interval representation\non the real line), this cycle cannot be too big, no matter which vertex\nordering we start with. More precisely, it was conjectured in [9] that for\ncocomparability graphs, the size of this cycle is always 2, independent of the\nstarting order. Furthermore [27] asked whether for arbitrary graphs, the size\nof such a cycle is always bounded by the asteroidal number of the graph. In\nthis work, while we answer this latter question negatively, we provide support\nfor the conjecture on cocomparability graphs by proving it for the subclass of\ndomino-free cocomparability graphs. This subclass contains cographs, proper\ninterval, interval, and cobipartite graphs. We also provide simpler independent\nproofs for each of these cases which lead to stronger results on this\nsubclasses. \n\n"}
{"id": "1702.02685", "contents": "Title: Combinatorial Alphabet-Dependent Bounds for Locally Recoverable Codes Abstract: Locally recoverable (LRC) codes have recently been a focus point of research\nin coding theory due to their theoretical appeal and applications in\ndistributed storage systems. In an LRC code, any erased symbol of a codeword\ncan be recovered by accessing only a small number of other symbols. For LRC\ncodes over a small alphabet (such as binary), the optimal rate-distance\ntrade-off is unknown. We present several new combinatorial bounds on LRC codes\nincluding the locality-aware sphere packing and Plotkin bounds. We also develop\nan approach to linear programming (LP) bounds on LRC codes. The resulting LP\nbound gives better estimates in examples than the other upper bounds known in\nthe literature. Further, we provide the tightest known upper bound on the rate\nof linear LRC codes with a given relative distance, an improvement over the\nprevious best known bounds. \n\n"}
{"id": "1702.03605", "contents": "Title: Nearly Instance Optimal Sample Complexity Bounds for Top-k Arm Selection Abstract: In the Best-$k$-Arm problem, we are given $n$ stochastic bandit arms, each\nassociated with an unknown reward distribution. We are required to identify the\n$k$ arms with the largest means by taking as few samples as possible. In this\npaper, we make progress towards a complete characterization of the\ninstance-wise sample complexity bounds for the Best-$k$-Arm problem. On the\nlower bound side, we obtain a novel complexity term to measure the sample\ncomplexity that every Best-$k$-Arm instance requires. This is derived by an\ninteresting and nontrivial reduction from the Best-$1$-Arm problem. We also\nprovide an elimination-based algorithm that matches the instance-wise lower\nbound within doubly-logarithmic factors. The sample complexity of our algorithm\nstrictly dominates the state-of-the-art for Best-$k$-Arm (module constant\nfactors). \n\n"}
{"id": "1702.04536", "contents": "Title: A $(2+\\epsilon)$-Approximation for Maximum Weight Matching in the\n  Semi-Streaming Model Abstract: We present a simple deterministic single-pass $(2+\\epsilon)$-approximation\nalgorithm for the maximum weight matching problem in the semi-streaming model.\nThis improves upon the currently best known approximation ratio of\n$(4+\\epsilon)$.\n  Our algorithm uses $O(n\\log^2 n)$ bits of space for constant values of\n$\\epsilon$. It relies on a variation of the local-ratio theorem, which may be\nof use for other algorithms in the semi-streaming model as well. \n\n"}
{"id": "1702.05456", "contents": "Title: LCL problems on grids Abstract: LCLs or locally checkable labelling problems (e.g. maximal independent set,\nmaximal matching, and vertex colouring) in the LOCAL model of computation are\nvery well-understood in cycles (toroidal 1-dimensional grids): every problem\nhas a complexity of $O(1)$, $\\Theta(\\log^* n)$, or $\\Theta(n)$, and the design\nof optimal algorithms can be fully automated.\n  This work develops the complexity theory of LCL problems for toroidal\n2-dimensional grids. The complexity classes are the same as in the\n1-dimensional case: $O(1)$, $\\Theta(\\log^* n)$, and $\\Theta(n)$. However, given\nan LCL problem it is undecidable whether its complexity is $\\Theta(\\log^* n)$\nor $\\Theta(n)$ in 2-dimensional grids.\n  Nevertheless, if we correctly guess that the complexity of a problem is\n$\\Theta(\\log^* n)$, we can completely automate the design of optimal\nalgorithms. For any problem we can find an algorithm that is of a normal form\n$A' \\circ S_k$, where $A'$ is a finite function, $S_k$ is an algorithm for\nfinding a maximal independent set in $k$th power of the grid, and $k$ is a\nconstant.\n  Finally, partially with the help of automated design tools, we classify the\ncomplexity of several concrete LCL problems related to colourings and\norientations. \n\n"}
{"id": "1702.05888", "contents": "Title: Memory Efficient Max Flow for Multi-label Submodular MRFs Abstract: Multi-label submodular Markov Random Fields (MRFs) have been shown to be\nsolvable using max-flow based on an encoding of the labels proposed by\nIshikawa, in which each variable $X_i$ is represented by $\\ell$ nodes (where\n$\\ell$ is the number of labels) arranged in a column. However, this method in\ngeneral requires $2\\,\\ell^2$ edges for each pair of neighbouring variables.\nThis makes it inapplicable to realistic problems with many variables and\nlabels, due to excessive memory requirement. In this paper, we introduce a\nvariant of the max-flow algorithm that requires much less storage.\nConsequently, our algorithm makes it possible to optimally solve multi-label\nsubmodular problems involving large numbers of variables and labels on a\nstandard computer. \n\n"}
{"id": "1702.05951", "contents": "Title: Refined Vertex Sparsifiers of Planar Graphs Abstract: We study the following version of cut sparsification. Given a large\nedge-weighted network $G$ with $k$ terminal vertices, compress it into a\nsmaller network $H$ with the same terminals, such that every minimum terminal\ncut in $H$ approximates the corresponding one in $G$, up to a factor $q\\geq 1$\nthat is called the quality. (The case $q=1$ is known also as a mimicking\nnetwork). We provide new insights about the structure of minimum terminal cuts,\nleading to new results for cut sparsifiers of planar graphs. Our first\ncontribution identifies a subset of the minimum terminal cuts, which we call\nelementary, that generates all the others. Consequently, $H$ is a cut\nsparsifier if and only if it preserves all the elementary terminal cuts (up to\nthis factor $q$). This structural characterization lead to improved bounds on\nthe size of $H$. For example, it improve the bound of mimicking-network size\nfor planar graphs into a near-optimal one. Our second and main contribution is\nto refine the known bounds in terms of $\\gamma=\\gamma(G)$, which is defined as\nthe minimum number of faces that are incident to all the terminals in a planar\ngraph $G$. We prove that the number of elementary terminal cuts is\n$O((2k/\\gamma)^{2\\gamma})$ (compared to $O(2^k)$ terminal cuts), and\nfurthermore obtain a mimicking-network of size $O(\\gamma 2^{2\\gamma} k^4)$,\nwhich is near-optimal as a function of $\\gamma$. In the analysis we break the\nelementary terminal cuts into fragments, and count them carefully. Our third\ncontribution is a duality between cut sparsification and distance\nsparsification for certain planar graphs, when the sparsifier $H$ is required\nto be a minor of $G$. This duality connects problems that were previously\nstudied separately, implying new results, new proofs of known results, and\nequivalences between open gaps. \n\n"}
{"id": "1702.06503", "contents": "Title: When can Graph Hyperbolicity be computed in Linear Time? Abstract: Hyperbolicity measures, in terms of (distance) metrics, how close a given\ngraph is to being a tree. Due to its relevance in modeling real-world networks,\nhyperbolicity has seen intensive research over the last years. Unfortunately,\nthe best known algorithms for computing the hyperbolicity number of a graph\n(the smaller, the more tree-like) have running time $O(n^4)$, where $n$ is the\nnumber of graph vertices. Exploiting the framework of parameterized complexity\nanalysis, we explore possibilities for \"linear-time FPT\" algorithms to compute\nhyperbolicity. For instance, we show that hyperbolicity can be computed in time\n$O(2^{O(k)} + n +m)$ ($m$ being the number of graph edges) while at the same\ntime, unless the SETH fails, there is no $2^{o(k)}n^2$-time algorithm. \n\n"}
{"id": "1702.06723", "contents": "Title: Compact linear programs for 2SAT Abstract: For each integer $n$ we present an explicit formulation of a compact linear\nprogram, with $O(n^3)$ variables and constraints, which determines the\nsatisfiability of any 2SAT formula with $n$ boolean variables by a single\nlinear optimization. This contrasts with the fact that the natural polytope for\nthis problem, formed from the convex hull of all satisfiable formulas and their\nsatisfying assignments, has superpolynomial extension complexity. Our\nformulation is based on multicommodity flows. We also discuss connections of\nthese results to the stable matching problem. \n\n"}
{"id": "1702.07815", "contents": "Title: Subquadratic Algorithms for the Diameter and the Sum of Pairwise\n  Distances in Planar Graphs Abstract: We show how to compute for $n$-vertex planar graphs in $O(n^{11/6}{\\rm\npolylog}(n))$ expected time the diameter and the sum of the pairwise distances.\nThe algorithms work for directed graphs with real weights and no negative\ncycles. In $O(n^{15/8}{\\rm polylog}(n))$ expected time we can also compute the\nnumber of pairs of vertices at distance smaller than a given threshold. These\nare the first algorithms for these problems using time $O(n^c)$ for some\nconstant $c<2$, even when restricted to undirected, unweighted planar graphs. \n\n"}
{"id": "1702.07961", "contents": "Title: An Efficient Multiway Mergesort for GPU Architectures Abstract: Sorting is a primitive operation that is a building block for countless\nalgorithms. As such, it is important to design sorting algorithms that approach\npeak performance on a range of hardware architectures. Graphics Processing\nUnits (GPUs) are particularly attractive architectures as they provides massive\nparallelism and computing power. However, the intricacies of their compute and\nmemory hierarchies make designing GPU-efficient algorithms challenging. In this\nwork we present GPU Multiway Mergesort (MMS), a new GPU-efficient multiway\nmergesort algorithm. MMS employs a new partitioning technique that exposes the\nparallelism needed by modern GPU architectures. To the best of our knowledge,\nMMS is the first sorting algorithm for the GPU that is asymptotically optimal\nin terms of global memory accesses and that is completely free of shared memory\nbank conflicts.\n  We realize an initial implementation of MMS, evaluate its performance on\nthree modern GPU architectures, and compare it to competitive implementations\navailable in state-of-the-art GPU libraries. Despite these implementations\nbeing highly optimized, MMS compares favorably, achieving performance\nimprovements for most random inputs. Furthermore, unlike MMS, state-of-the-art\nalgorithms are susceptible to bank conflicts. We find that for certain inputs\nthat cause these algorithms to incur large numbers of bank conflicts, MMS can\nachieve up to a 37.6% speedup over its fastest competitor. Overall, even though\nits current implementation is not fully optimized, due to its efficient use of\nthe memory hierarchy, MMS outperforms the fastest comparison-based sorting\nimplementations available to date. \n\n"}
{"id": "1702.08255", "contents": "Title: Learning with Errors is easy with quantum samples Abstract: Learning with Errors is one of the fundamental problems in computational\nlearning theory and has in the last years become the cornerstone of\npost-quantum cryptography. In this work, we study the quantum sample complexity\nof Learning with Errors and show that there exists an efficient quantum\nlearning algorithm (with polynomial sample and time complexity) for the\nLearning with Errors problem where the error distribution is the one used in\ncryptography. While our quantum learning algorithm does not break the LWE-based\nencryption schemes proposed in the cryptography literature, it does have some\ninteresting implications for cryptography: first, when building an LWE-based\nscheme, one needs to be careful about the access to the public-key generation\nalgorithm that is given to the adversary; second, our algorithm shows a\npossible way for attacking LWE-based encryption by using classical samples to\napproximate the quantum sample state, since then using our quantum learning\nalgorithm would solve LWE. \n\n"}
{"id": "1703.00941", "contents": "Title: On the Fine-grained Complexity of One-Dimensional Dynamic Programming Abstract: In this paper, we investigate the complexity of one-dimensional dynamic\nprogramming, or more specifically, of the Least-Weight Subsequence (LWS)\nproblem: Given a sequence of $n$ data items together with weights for every\npair of the items, the task is to determine a subsequence $S$ minimizing the\ntotal weight of the pairs adjacent in $S$. A large number of natural problems\ncan be formulated as LWS problems, yielding obvious $O(n^2)$-time solutions.\n  In many interesting instances, the $O(n^2)$-many weights can be succinctly\nrepresented. Yet except for near-linear time algorithms for some specific\nspecial cases, little is known about when an LWS instantiation admits a\nsubquadratic-time algorithm and when it does not. In particular, no lower\nbounds for LWS instantiations have been known before. In an attempt to remedy\nthis situation, we provide a general approach to study the fine-grained\ncomplexity of succinct instantiations of the LWS problem. In particular, given\nan LWS instantiation we identify a highly parallel core problem that is\nsubquadratically equivalent. This provides either an explanation for the\napparent hardness of the problem or an avenue to find improved algorithms as\nthe case may be.\n  More specifically, we prove subquadratic equivalences between the following\npairs (an LWS instantiation and the corresponding core problem) of problems: a\nlow-rank version of LWS and minimum inner product, finding the longest chain of\nnested boxes and vector domination, and a coin change problem which is closely\nrelated to the knapsack problem and (min,+)-convolution. Using these\nequivalences and known SETH-hardness results for some of the core problems, we\ndeduce tight conditional lower bounds for the corresponding LWS instantiations.\nWe also establish the (min,+)-convolution-hardness of the knapsack problem. \n\n"}
{"id": "1703.01686", "contents": "Title: Parameterized complexity of finding a spanning tree with minimum reload\n  cost diameter Abstract: We study the minimum diameter spanning tree problem under the reload cost\nmodel (DIAMETER-TREE for short) introduced by Wirth and Steffan (2001). In this\nproblem, given an undirected edge-colored graph $G$, reload costs on a path\narise at a node where the path uses consecutive edges of different colors. The\nobjective is to find a spanning tree of $G$ of minimum diameter with respect to\nthe reload costs. We initiate a systematic study of the parameterized\ncomplexity of the DIAMETER-TREE problem by considering the following\nparameters: the cost of a solution, and the treewidth and the maximum degree\n$\\Delta$ of the input graph. We prove that DIAMETER-TREE is para-NP-hard for\nany combination of two of these three parameters, and that it is FPT\nparameterized by the three of them. We also prove that the problem can be\nsolved in polynomial time on cactus graphs. This result is somehow surprising\nsince we prove DIAMETER-TREE to be NP-hard on graphs of treewidth two, which is\nbest possible as the problem can be trivially solved on forests. When the\nreload costs satisfy the triangle inequality, Wirth and Steffan (2001) proved\nthat the problem can be solved in polynomial time on graphs with $\\Delta = 3$,\nand Galbiati (2008) proved that it is NP-hard if $\\Delta = 4$. Our results\nshow, in particular, that without the requirement of the triangle inequality,\nthe problem is NP-hard if $\\Delta = 3$, which is also best possible. Finally,\nin the case where the reload costs are polynomially bounded by the size of the\ninput graph, we prove that DIAMETER-TREE is in XP and W[1]-hard parameterized\nby the treewidth plus $\\Delta$. \n\n"}
{"id": "1703.04143", "contents": "Title: Bernoulli Factories and Black-Box Reductions in Mechanism Design Abstract: We provide a polynomial time reduction from Bayesian incentive compatible\nmechanism design to Bayesian algorithm design for welfare maximization\nproblems. Unlike prior results, our reduction achieves exact incentive\ncompatibility for problems with multi-dimensional and continuous type spaces.\nThe key technical barrier preventing exact incentive compatibility in prior\nblack-box reductions is that repairing violations of incentive constraints\nrequires understanding the distribution of the mechanism's output, which is\ntypically #P-hard to compute. Reductions that instead estimate the output\ndistribution by sampling inevitably suffer from sampling error, which typically\nprecludes exact incentive compatibility.\n  We overcome this barrier by employing and generalizing the computational\nmodel in the literature on $\\textit{Bernoulli factories}$. In a Bernoulli\nfactory problem, one is given a function mapping the bias of an \"input coin\" to\nthat of an \"output coin\", and the challenge is to efficiently simulate the\noutput coin given only sample access to the input coin. This is the key\ningredient in designing an incentive compatible mechanism for bipartite\nmatching, which can be used to make the approximately incentive compatible\nreduction of Hartline et al. (2015) exactly incentive compatible. \n\n"}
{"id": "1703.05784", "contents": "Title: A Nearly Optimal Lower Bound on the Approximate Degree of AC$^0$ Abstract: The approximate degree of a Boolean function $f \\colon \\{-1, 1\\}^n\n\\rightarrow \\{-1, 1\\}$ is the least degree of a real polynomial that\napproximates $f$ pointwise to error at most $1/3$. We introduce a generic\nmethod for increasing the approximate degree of a given function, while\npreserving its computability by constant-depth circuits.\n  Specifically, we show how to transform any Boolean function $f$ with\napproximate degree $d$ into a function $F$ on $O(n \\cdot\n\\operatorname{polylog}(n))$ variables with approximate degree at least $D =\n\\Omega(n^{1/3} \\cdot d^{2/3})$. In particular, if $d= n^{1-\\Omega(1)}$, then\n$D$ is polynomially larger than $d$. Moreover, if $f$ is computed by a\npolynomial-size Boolean circuit of constant depth, then so is $F$.\n  By recursively applying our transformation, for any constant $\\delta > 0$ we\nexhibit an AC$^0$ function of approximate degree $\\Omega(n^{1-\\delta})$. This\nimproves over the best previous lower bound of $\\Omega(n^{2/3})$ due to\nAaronson and Shi (J. ACM 2004), and nearly matches the trivial upper bound of\n$n$ that holds for any function. Our lower bounds also apply to\n(quasipolynomial-size) DNFs of polylogarithmic width.\n  We describe several applications of these results. We give:\n  * For any constant $\\delta > 0$, an $\\Omega(n^{1-\\delta})$ lower bound on the\nquantum communication complexity of a function in AC$^0$.\n  * A Boolean function $f$ with approximate degree at least $C(f)^{2-o(1)}$,\nwhere $C(f)$ is the certificate complexity of $f$. This separation is optimal\nup to the $o(1)$ term in the exponent.\n  * Improved secret sharing schemes with reconstruction procedures in AC$^0$. \n\n"}
{"id": "1703.06065", "contents": "Title: Block CUR: Decomposing Matrices using Groups of Columns Abstract: A common problem in large-scale data analysis is to approximate a matrix\nusing a combination of specifically sampled rows and columns, known as CUR\ndecomposition. Unfortunately, in many real-world environments, the ability to\nsample specific individual rows or columns of the matrix is limited by either\nsystem constraints or cost. In this paper, we consider matrix approximation by\nsampling predefined \\emph{blocks} of columns (or rows) from the matrix. We\npresent an algorithm for sampling useful column blocks and provide novel\nguarantees for the quality of the approximation. This algorithm has application\nin problems as diverse as biometric data analysis to distributed computing. We\ndemonstrate the effectiveness of the proposed algorithms for computing the\nBlock CUR decomposition of large matrices in a distributed setting with\nmultiple nodes in a compute cluster, where such blocks correspond to columns\n(or rows) of the matrix stored on the same node, which can be retrieved with\nmuch less overhead than retrieving individual columns stored across different\nnodes. In the biometric setting, the rows correspond to different users and\ncolumns correspond to users' biometric reaction to external stimuli, {\\em\ne.g.,}~watching video content, at a particular time instant. There is\nsignificant cost in acquiring each user's reaction to lengthy content so we\nsample a few important scenes to approximate the biometric response. An\nindividual time sample in this use case cannot be queried in isolation due to\nthe lack of context that caused that biometric reaction. Instead, collections\nof time segments ({\\em i.e.,} blocks) must be presented to the user. The\npractical application of these algorithms is shown via experimental results\nusing real-world user biometric data from a content testing environment. \n\n"}
{"id": "1703.07734", "contents": "Title: On the Probe Complexity of Local Computation Algorithms Abstract: The Local Computation Algorithms (LCA) model is a computational model aimed\nat problem instances with huge inputs and output. For graph problems, the input\ngraph is accessed using probes: strong probes (SP) specify a vertex $v$ and\nreceive as a reply a list of $v$'s neighbors, and weak probes (WP) specify a\nvertex $v$ and a port number $i$ and receive as a reply $v$'s $i^{th}$\nneighbor. Given a local query (e.g., \"is a certain vertex in the vertex cover\nof the input graph?\"), an LCA should compute the corresponding local output\n(e.g., \"yes\" or \"no\") while making only a small number of probes, with the\nrequirement that all local outputs form a single global solution (e.g., a legal\nvertex cover). We study the probe complexity of LCAs that are required to work\non graphs that may have arbitrarily large degrees. In particular, such LCAs are\nexpected to probe the graph a number of times that is significantly smaller\nthan the maximum, average, or even minimum degree.\n  For weak probes, we focus on the weak coloring problem. Among our results we\nshow a separation between weak 3-coloring and weak 2-coloring for deterministic\nLCAs: $\\log^* n + O(1)$ weak probes suffice for weak 3-coloring, but\n$\\Omega\\left(\\frac{\\log n}{\\log\\log n}\\right)$ weak probes are required for\nweak 2-coloring.\n  For strong probes, we consider randomized LCAs for vertex cover and\nmaximal/maximum matching. Our negative results include showing that there are\ngraphs for which finding a \\emph{maximal} matching requires $\\Omega(\\sqrt{n})$\nstrong probes. On the positive side, we design a randomized LCA that finds a\n$(1-\\epsilon)$ approximation to \\emph{maximum} matching in regular graphs, and\nuses $\\frac{1}{\\epsilon }^{O\\left( \\frac{1}{\\epsilon ^2}\\right)}$ probes,\nindependently of the number of vertices and of their degrees. \n\n"}
{"id": "1703.08139", "contents": "Title: Optimal lower bounds for universal relation, samplers, and finding\n  duplicates Abstract: In the communication problem $\\mathbf{UR}$ (universal relation) [KRW95],\nAlice and Bob respectively receive $x$ and $y$ in $\\{0,1\\}^n$ with the promise\nthat $x\\neq y$. The last player to receive a message must output an index $i$\nsuch that $x_i\\neq y_i$. We prove that the randomized one-way communication\ncomplexity of this problem in the public coin model is exactly $\\Theta(\\min\\{n,\n\\log(1/\\delta)\\log^2(\\frac{n}{\\log(1/\\delta)})\\})$ bits for failure probability\n$\\delta$. Our lower bound holds even if promised $\\mathop{support}(y)\\subset\n\\mathop{support}(x)$. As a corollary, we obtain optimal lower bounds for\n$\\ell_p$-sampling in strict turnstile streams for $0\\le p < 2$, as well as for\nthe problem of finding duplicates in a stream. Our lower bounds do not need to\nuse large weights, and hold even if it is promised that $x\\in\\{0,1\\}^n$ at all\npoints in the stream.\n  Our lower bound demonstrates that any algorithm $\\mathcal{A}$ solving\nsampling problems in turnstile streams in low memory can be used to encode\nsubsets of $[n]$ of certain sizes into a number of bits below the information\ntheoretic minimum. Our encoder makes adaptive queries to $\\mathcal{A}$\nthroughout its execution, but done carefully so as to not violate correctness.\nThis is accomplished by injecting random noise into the encoder's interactions\nwith $\\mathcal{A}$, which is loosely motivated by techniques in differential\nprivacy. Our correctness analysis involves understanding the ability of\n$\\mathcal{A}$ to correctly answer adaptive queries which have positive but\nbounded mutual information with $\\mathcal{A}$'s internal randomness, and may be\nof independent interest in the newly emerging area of adaptive data analysis\nwith a theoretical computer science lens. \n\n"}
{"id": "1703.08433", "contents": "Title: Metric random matchings with applications Abstract: Let $(\\{1,2,\\ldots,n\\},d)$ be a metric space. We analyze the expected value\nand the variance of $\\sum_{i=1}^{\\lfloor\nn/2\\rfloor}\\,d({\\boldsymbol{\\pi}}(2i-1),{\\boldsymbol{\\pi}}(2i))$ for a\nuniformly random permutation ${\\boldsymbol{\\pi}}$ of $\\{1,2,\\ldots,n\\}$,\nleading to the following results: (I) Consider the problem of finding a point\nin $\\{1,2,\\ldots,n\\}$ with the minimum sum of distances to all points. We show\nthat this problem has a randomized algorithm that (1) always outputs a\n$(2+\\epsilon)$-approximate solution in expected $O(n/\\epsilon^2)$ time and that\n(2) inherits Indyk's~\\cite{Ind99, Ind00} algorithm to output a\n$(1+\\epsilon)$-approximate solution in $O(n/\\epsilon^2)$ time with probability\n$\\Omega(1)$, where $\\epsilon\\in(0,1)$. (II) The average distance in\n$(\\{1,2,\\ldots,n\\},d)$ can be approximated in $O(n/\\epsilon)$ time to within a\nmultiplicative factor in $[\\,1/2-\\epsilon,1\\,]$ with probability\n$1/2+\\Omega(1)$, where $\\epsilon>0$. (III) Assume $d$ to be a graph metric.\nThen the average distance in $(\\{1,2,\\ldots,n\\},d)$ can be approximated in\n$O(n)$ time to within a multiplicative factor in $[\\,1-\\epsilon,1+\\epsilon\\,]$\nwith probability $1/2+\\Omega(1)$, where $\\epsilon=\\omega(1/n^{1/4})$. \n\n"}
{"id": "1703.08940", "contents": "Title: Tree Edit Distance Cannot be Computed in Strongly Subcubic Time (unless\n  APSP can) Abstract: The edit distance between two rooted ordered trees with $n$ nodes labeled\nfrom an alphabet~$\\Sigma$ is the minimum cost of transforming one tree into the\nother by a sequence of elementary operations consisting of deleting and\nrelabeling existing nodes, as well as inserting new nodes. Tree edit distance\nis a well known generalization of string edit distance. The fastest known\nalgorithm for tree edit distance runs in cubic $O(n^3)$ time and is based on a\nsimilar dynamic programming solution as string edit distance. In this paper we\nshow that a truly subcubic $O(n^{3-\\varepsilon})$ time algorithm for tree edit\ndistance is unlikely: For $|\\Sigma| = \\Omega(n)$, a truly subcubic algorithm\nfor tree edit distance implies a truly subcubic algorithm for the all pairs\nshortest paths problem. For $|\\Sigma| = O(1)$, a truly subcubic algorithm for\ntree edit distance implies an $O(n^{k-\\varepsilon})$ algorithm for finding a\nmaximum weight $k$-clique.\n  Thus, while in terms of upper bounds string edit distance and tree edit\ndistance are highly related, in terms of lower bounds string edit distance\nexhibits the hardness of the strong exponential time hypothesis [Backurs, Indyk\nSTOC'15] whereas tree edit distance exhibits the hardness of all pairs shortest\npaths. Our result provides a matching conditional lower bound for one of the\nlast remaining classic dynamic programming problems. \n\n"}
{"id": "1703.10205", "contents": "Title: A Sharp Tail Bound for the Expander Random Sampler Abstract: Consider an expander graph in which a $\\mu$ fraction of the vertices are\nmarked. A random walk starts at a uniform vertex and at each step continues to\na random neighbor. Gillman showed in 1993 that the number of marked vertices\nseen in a random walk of length $n$ is concentrated around its expectation,\n$\\Phi := \\mu n$, independent of the size of the graph. Here we provide a new\nand sharp tail bound, improving on the existing bounds whenever $\\mu$ is not\ntoo large. \n\n"}
{"id": "1704.00238", "contents": "Title: Clustering in Hilbert space of a quantum optimization problem Abstract: The solution space of many classical optimization problems breaks up into\nclusters which are extensively distant from one another in the Hamming metric.\nHere, we show that an analogous quantum clustering phenomenon takes place in\nthe ground state subspace of a certain quantum optimization problem. This\ninvolves extending the notion of clustering to Hilbert space, where the\nclassical Hamming distance is not immediately useful. Quantum clusters\ncorrespond to macroscopically distinct subspaces of the full quantum ground\nstate space which grow with the system size. We explicitly demonstrate that\nsuch clusters arise in the solution space of random quantum satisfiability\n(3-QSAT) at its satisfiability transition. We estimate both the number of these\nclusters and their internal entropy. The former are given by the number of\nhardcore dimer coverings of the core of the interaction graph, while the latter\nis related to the underconstrained degrees of freedom not touched by the\ndimers. We additionally provide new numerical evidence suggesting that the\n3-QSAT satisfiability transition may coincide with the product satisfiability\ntransition, which would imply the absence of an intermediate entangled\nsatisfiable phase. \n\n"}
{"id": "1704.00386", "contents": "Title: Local Algorithms for Hierarchical Dense Subgraph Discovery Abstract: Finding the dense regions of a graph and relations among them is a\nfundamental problem in network analysis. Core and truss decompositions reveal\ndense subgraphs with hierarchical relations. The incremental nature of\nalgorithms for computing these decompositions and the need for global\ninformation at each step of the algorithm hinders scalable parallelization and\napproximations since the densest regions are not revealed until the end. In a\nprevious work, Lu et al. proposed to iteratively compute the $h$-indices of\nneighbor vertex degrees to obtain the core numbers and prove that the\nconvergence is obtained after a finite number of iterations. This work\ngeneralizes the iterative $h$-index computation for truss decomposition as well\nas nucleus decomposition which leverages higher-order structures to generalize\ncore and truss decompositions. In addition, we prove convergence bounds on the\nnumber of iterations. We present a framework of local algorithms to obtain the\ncore, truss, and nucleus decompositions. Our algorithms are local, parallel,\noffer high scalability, and enable approximations to explore time and quality\ntrade-offs. Our shared-memory implementation verifies the efficiency,\nscalability, and effectiveness of our local algorithms on real-world networks. \n\n"}
{"id": "1704.00633", "contents": "Title: Optimal lower bounds for universal relation, and for samplers and\n  finding duplicates in streams Abstract: In the communication problem $\\mathbf{UR}$ (universal relation) [KRW95],\nAlice and Bob respectively receive $x, y \\in\\{0,1\\}^n$ with the promise that\n$x\\neq y$. The last player to receive a message must output an index $i$ such\nthat $x_i\\neq y_i$. We prove that the randomized one-way communication\ncomplexity of this problem in the public coin model is exactly\n$\\Theta(\\min\\{n,\\log(1/\\delta)\\log^2(\\frac n{\\log(1/\\delta)})\\})$ for failure\nprobability $\\delta$. Our lower bound holds even if promised\n$\\mathop{support}(y)\\subset \\mathop{support}(x)$. As a corollary, we obtain\noptimal lower bounds for $\\ell_p$-sampling in strict turnstile streams for\n$0\\le p < 2$, as well as for the problem of finding duplicates in a stream. Our\nlower bounds do not need to use large weights, and hold even if promised\n$x\\in\\{0,1\\}^n$ at all points in the stream.\n  We give two different proofs of our main result. The first proof demonstrates\nthat any algorithm $\\mathcal A$ solving sampling problems in turnstile streams\nin low memory can be used to encode subsets of $[n]$ of certain sizes into a\nnumber of bits below the information theoretic minimum. Our encoder makes\nadaptive queries to $\\mathcal A$ throughout its execution, but done carefully\nso as to not violate correctness. This is accomplished by injecting random\nnoise into the encoder's interactions with $\\mathcal A$, which is loosely\nmotivated by techniques in differential privacy. Our second proof is via a\nnovel randomized reduction from Augmented Indexing [MNSW98] which needs to\ninteract with $\\mathcal A$ adaptively. To handle the adaptivity we identify\ncertain likely interaction patterns and union bound over them to guarantee\ncorrect interaction on all of them. To guarantee correctness, it is important\nthat the interaction hides some of its randomness from $\\mathcal A$ in the\nreduction. \n\n"}
{"id": "1704.00765", "contents": "Title: Quantum Algorithms for Graph Connectivity and Formula Evaluation Abstract: We give a new upper bound on the quantum query complexity of deciding\n$st$-connectivity on certain classes of planar graphs, and show the bound is\nsometimes exponentially better than previous results. We then show Boolean\nformula evaluation reduces to deciding connectivity on just such a class of\ngraphs. Applying the algorithm for $st$-connectivity to Boolean formula\nevaluation problems, we match the $O(\\sqrt{N})$ bound on the quantum query\ncomplexity of evaluating formulas on $N$ variables, give a quadratic speed-up\nover the classical query complexity of a certain class of promise Boolean\nformulas, and show this approach can yield superpolynomial quantum/classical\nseparations. These results indicate that this $st$-connectivity-based approach\nmay be the \"right\" way of looking at quantum algorithms for formula evaluation. \n\n"}
{"id": "1704.01101", "contents": "Title: On Resource-bounded versions of the van Lambalgen theorem Abstract: The van Lambalgen theorem is a surprising result in algorithmic information\ntheory concerning the symmetry of relative randomness. It establishes that for\nany pair of infinite sequences $A$ and $B$, $B$ is Martin-L\\\"of random and $A$\nis Martin-L\\\"of random relative to $B$ if and only if the interleaved sequence\n$A \\uplus B$ is Martin-L\\\"of random. This implies that $A$ is relative random\nto $B$ if and only if $B$ is random relative to $A$ \\cite{vanLambalgen},\n\\cite{Nies09}, \\cite{HirschfeldtBook}. This paper studies the validity of this\nphenomenon for different notions of time-bounded relative randomness.\n  We prove the classical van Lambalgen theorem using martingales and Kolmogorov\ncompressibility. We establish the failure of relative randomness in these\nsettings, for both time-bounded martingales and time-bounded Kolmogorov\ncomplexity. We adapt our classical proofs when applicable to the time-bounded\nsetting, and construct counterexamples when they fail. The mode of failure of\nthe theorem may depend on the notion of time-bounded randomness. \n\n"}
{"id": "1704.01937", "contents": "Title: Promise Constraint Satisfaction: Algebraic Structure and a Symmetric\n  Boolean Dichotomy Abstract: A classic result due to Schaefer (1978) classifies all constraint\nsatisfaction problems (CSPs) over the Boolean domain as being either in\n$\\mathsf{P}$ or $\\mathsf{NP}$-hard. This paper considers a promise-problem\nvariant of CSPs called PCSPs. A PCSP over a finite set of pairs of constraints\n$\\Gamma$ consists of a pair $(\\Psi_P, \\Psi_Q)$ of CSPs with the same set of\nvariables such that for every $(P, Q) \\in \\Gamma$, $P(x_{i_1}, ..., x_{i_k})$\nis a clause of $\\Psi_P$ if and only if $Q(x_{i_1}, ..., x_{i_k})$ is a clause\nof $\\Psi_Q$. The promise problem $\\operatorname{PCSP}(\\Gamma)$ is to\ndistinguish, given $(\\Psi_P, \\Psi_Q)$, between the cases $\\Psi_P$ is\nsatisfiable and $\\Psi_Q$ is unsatisfiable. Many natural problems including\napproximate graph and hypergraph coloring can be placed in this framework.\n  This paper is motivated by the pursuit of understanding the computational\ncomplexity of Boolean promise CSPs. As our main result, we show that\n$\\operatorname{PCSP}(\\Gamma)$ exhibits a dichotomy (it is either polynomial\ntime solvable or $\\mathsf{NP}$-hard) when the relations in $\\Gamma$ are\nsymmetric and allow for negations of variables. We achieve our dichotomy\ntheorem by extending the weak polymorphism framework of Austrin, Guruswami, and\nH\\aa stad [FOCS '14] which itself is a generalization of the algebraic approach\nto study CSPs. In both the algorithm and hardness portions of our proof, we\nincorporate new ideas and techniques not utilized in the CSP case.\n  Furthermore, we show that the computational complexity of any promise CSP\n(over arbitrary finite domains) is captured entirely by its weak polymorphisms,\na feature known as Galois correspondence, as well as give necessary and\nsufficient conditions for the structure of this set of weak polymorphisms. Such\ninsights call us to question the existence of a general dichotomy for Boolean\nPCSPs. \n\n"}
{"id": "1704.02939", "contents": "Title: Minor-matching hypertree width Abstract: In this paper we present a new width measure for a tree decomposition,\nminor-matching hypertree width, $\\mu\\text{-}tw$, for graphs and hypergraphs,\nsuch that bounding the width guarantees that set of maximal independent sets\nhas a polynomially-sized restriction to each decomposition bag. The relaxed\nconditions of the decomposition allow a much wider class of graphs and\nhypergraphs of bounded width compared to other tree decompositions. We show\nthat, for fixed $k$, there are $2^{(1 - \\frac1k + o(1)){n \\choose 2}}$\n$n$-vertex graphs of minor-matching hypertree width at most $k$. A number of\nproblems including Maximum Independence Set, $k$-Colouring, and Homomorphism of\nuniform hypergraphs permit polynomial-time solutions for hypergraphs with\nbounded minor-matching hypertree width and bounded rank. We show that for any\ngiven $k$ and any graph $G$, it is possible to construct a decomposition of\nminor-matching hypertree width at most $O(k^3)$, or to prove that\n$\\mu\\text{-}tw(G) > k$ in time $n^{O(k^3)}$. This is done by presenting a\ngeneral algorithm for approximating the hypertree width of well-behaved\nmeasures, and reducing $\\mu\\text{-}tw$ to such measure. The result relating the\nrestriction of the maximal independent sets to a set $S$ with the set of\ninduced matchings intersecting $S$ in graphs, and minor matchings intersecting\n$S$ in hypergraphs, might be of independent interest. \n\n"}
{"id": "1704.02958", "contents": "Title: On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel\n  Methods and Neural Networks Abstract: Empirical risk minimization (ERM) is ubiquitous in machine learning and\nunderlies most supervised learning methods. While there has been a large body\nof work on algorithms for various ERM problems, the exact computational\ncomplexity of ERM is still not understood. We address this issue for multiple\npopular ERM problems including kernel SVMs, kernel ridge regression, and\ntraining the final layer of a neural network. In particular, we give\nconditional hardness results for these problems based on complexity-theoretic\nassumptions such as the Strong Exponential Time Hypothesis. Under these\nassumptions, we show that there are no algorithms that solve the aforementioned\nERM problems to high accuracy in sub-quadratic time. We also give similar\nhardness results for computing the gradient of the empirical loss, which is the\nmain computational burden in many non-convex learning tasks. \n\n"}
{"id": "1704.03640", "contents": "Title: Hardness of classically sampling one clean qubit model with constant\n  total variation distance error Abstract: The one clean qubit model (or the DQC1 model) is a restricted model of\nquantum computing where only a single input qubit is pure and all other input\nqubits are maximally mixed. In spite of the severe restriction, the model can\nsolve several problems (such as calculating Jones polynomials) whose classical\nefficient solutions are not known. Furthermore, it was shown that if the output\nprobability distribution of the one clean qubit model can be classically\nefficiently sampled with a constant multiplicative error, then the polynomial\nhierarchy collapses to the second level. Is it possible to improve the\nmultiplicative error hardness result to a constant total variation distance\nerror one like other sub-universal quantum computing models such as the IQP\nmodel, the Boson Sampling model, and the Fourier Sampling model? In this paper,\nwe show that it is indeed possible if we accept a modified version of the\naverage case hardness conjecture. Interestingly, the anti-concentration lemma\ncan be easily shown by using the special property of the one clean qubit model\nthat each output probability is so small that no concentration occurs. \n\n"}
{"id": "1704.03664", "contents": "Title: Approximating Optimization Problems using EAs on Scale-Free Networks Abstract: It has been observed that many complex real-world networks have certain\nproperties, such as a high clustering coefficient, a low diameter, and a\npower-law degree distribution. A network with a power-law degree distribution\nis known as scale-free network. In order to study these networks, various\nrandom graph models have been proposed, e.g. Preferential Attachment, Chung-Lu,\nor Hyperbolic.\n  We look at the interplay between the power-law degree distribution and the\nrun time of optimization techniques for well known combinatorial problems. We\nobserve that on scale-free networks, simple evolutionary algorithms (EAs)\nquickly reach a constant-factor approximation ratio on common covering problems\n  We prove that the single-objective (1+1)EA reaches a constant-factor\napproximation ratio on the Minimum Dominating Set problem, the Minimum Vertex\nCover problem, the Minimum Connected Dominating Set problem, and the Maximum\nIndependent Set problem in expected polynomial number of calls to the fitness\nfunction.\n  Furthermore, we prove that the multi-objective GSEMO algorithm reaches a\nbetter approximation ratio than the (1+1)EA on those problems, within\npolynomial fitness evaluations. \n\n"}
{"id": "1704.05254", "contents": "Title: Grammar-Based Graph Compression Abstract: We present a new graph compressor that works by recursively detecting\nrepeated substructures and representing them through grammar rules. We show\nthat for a large number of graphs the compressor obtains smaller\nrepresentations than other approaches. Specific queries such as reachability\nbetween two nodes or regular path queries can be evaluated in linear time (or\nquadratic times, respectively), over the grammar, thus allowing speed-ups\nproportional to the compression ratio. \n\n"}
{"id": "1704.05303", "contents": "Title: The Robot Routing Problem for Collecting Aggregate Stochastic Rewards Abstract: We propose a new model for formalizing reward collection problems on graphs\nwith dynamically generated rewards which may appear and disappear based on a\nstochastic model. The *robot routing problem* is modeled as a graph whose nodes\nare stochastic processes generating potential rewards over discrete time. The\nrewards are generated according to the stochastic process, but at each step, an\nexisting reward disappears with a given probability. The edges in the graph\nencode the (unit-distance) paths between the rewards' locations. On visiting a\nnode, the robot collects the accumulated reward at the node at that time, but\ntraveling between the nodes takes time. The optimization question asks to\ncompute an optimal (or epsilon-optimal) path that maximizes the expected\ncollected rewards.\n  We consider the finite and infinite-horizon robot routing problems. For\nfinite-horizon, the goal is to maximize the total expected reward, while for\ninfinite horizon we consider limit-average objectives. We study the\ncomputational and strategy complexity of these problems, establish NP-lower\nbounds and show that optimal strategies require memory in general. We also\nprovide an algorithm for computing epsilon-optimal infinite paths for arbitrary\nepsilon > 0. \n\n"}
{"id": "1704.06297", "contents": "Title: A Time Hierarchy Theorem for the LOCAL Model Abstract: The celebrated Time Hierarchy Theorem for Turing machines states, informally,\nthat more problems can be solved given more time. The extent to which a time\nhierarchy-type theorem holds in the distributed LOCAL model has been open for\nmany years. It is consistent with previous results that all natural problems in\nthe LOCAL model can be classified according to a small constant number of\ncomplexities, such as $O(1),O(\\log^* n), O(\\log n), 2^{O(\\sqrt{\\log n})}$, etc.\n  In this paper we establish the first time hierarchy theorem for the LOCAL\nmodel and prove that several gaps exist in the LOCAL time hierarchy.\n  1. We define an infinite set of simple coloring problems called Hierarchical\n$2\\frac{1}{2}$-Coloring}. A correctly colored graph can be confirmed by simply\nchecking the neighborhood of each vertex, so this problem fits into the class\nof locally checkable labeling (LCL) problems. However, the complexity of the\n$k$-level Hierarchical $2\\frac{1}{2}$-Coloring problem is $\\Theta(n^{1/k})$,\nfor $k\\in\\mathbb{Z}^+$. The upper and lower bounds hold for both general graphs\nand trees, and for both randomized and deterministic algorithms.\n  2. Consider any LCL problem on bounded degree trees. We prove an\nautomatic-speedup theorem that states that any randomized $n^{o(1)}$-time\nalgorithm solving the LCL can be transformed into a deterministic $O(\\log\nn)$-time algorithm. Together with a previous result, this establishes that on\ntrees, there are no natural deterministic complexities in the ranges\n$\\omega(\\log^* n)$---$o(\\log n)$ or $\\omega(\\log n)$---$n^{o(1)}$.\n  3. We expose a gap in the randomized time hierarchy on general graphs. Any\nrandomized algorithm that solves an LCL problem in sublogarithmic time can be\nsped up to run in $O(T_{LLL})$ time, which is the complexity of the distributed\nLovasz local lemma problem, currently known to be $\\Omega(\\log\\log n)$ and\n$O(\\log n)$. \n\n"}
{"id": "1704.06774", "contents": "Title: Quantum algorithm for tree size estimation, with applications to\n  backtracking and 2-player games Abstract: We study quantum algorithms on search trees of unknown structure, in a model\nwhere the tree can be discovered by local exploration. That is, we are given\nthe root of the tree and access to a black box which, given a vertex $v$,\noutputs the children of $v$.\n  We construct a quantum algorithm which, given such access to a search tree of\ndepth at most $n$, estimates the size of the tree $T$ within a factor of $1\\pm\n\\delta$ in $\\tilde{O}(\\sqrt{nT})$ steps. More generally, the same algorithm can\nbe used to estimate size of directed acyclic graphs (DAGs) in a similar model.\n  We then show two applications of this result: a) We show how to transform a\nclassical backtracking search algorithm which examines $T$ nodes of a search\ntree into an $\\tilde{O}(\\sqrt{T}n^{3/2})$ time quantum algorithm, improving\nover an earlier quantum backtracking algorithm of Montanaro (arXiv:1509.02374).\nb) We give a quantum algorithm for evaluating AND-OR formulas in a model where\nthe formula can be discovered by local exploration (modeling position trees in\n2-player games). We show that, in this setting, formulas of size $T$ and depth\n$T^{o(1)}$ can be evaluated in quantum time $O(T^{1/2+o(1)})$. Thus, the\nquantum speedup is essentially the same as in the case when the formula is\nknown in advance. \n\n"}
{"id": "1704.06899", "contents": "Title: An exact algorithm exhibiting RS-RSB/easy-hard correspondence for the\n  maximum independent set problem Abstract: A recently proposed exact algorithm for the maximum independent set problem\nis analyzed. The typical running time is improved exponentially in some\nparameter regions compared to simple binary search. The algorithm also\novercomes the core transition point, where the conventional leaf removal\nalgorithm fails, and works up to the replica symmetry breaking (RSB) transition\npoint. This suggests that a leaf removal core itself is not enough for typical\nhardness in the random maximum independent set problem, providing further\nevidence for RSB being the obstacle for algorithms in general. \n\n"}
{"id": "1704.06980", "contents": "Title: A Match in Time Saves Nine: Deterministic Online Matching With Delays Abstract: We consider the problem of online Min-cost Perfect Matching with Delays\n(MPMD) introduced by Emek et al. (STOC 2016). In this problem, an even number\nof requests appear in a metric space at different times and the goal of an\nonline algorithm is to match them in pairs. In contrast to traditional online\nmatching problems, in MPMD all requests appear online and an algorithm can\nmatch any pair of requests, but such decision may be delayed (e.g., to find a\nbetter match). The cost is the sum of matching distances and the introduced\ndelays.\n  We present the first deterministic online algorithm for this problem. Its\ncompetitive ratio is $O(m^{\\log_2 5.5})$ $ = O(m^{2.46})$, where $2 m$ is the\nnumber of requests. This is polynomial in the number of metric space points if\nall requests are given at different points. In particular, the bound does not\ndepend on other parameters of the metric, such as its aspect ratio. Unlike\nprevious (randomized) solutions for the MPMD problem, our algorithm does not\nneed to know the metric space in advance. \n\n"}
{"id": "1704.07284", "contents": "Title: Hitting minors on bounded treewidth graphs. I. General upper bounds Abstract: For a finite collection of graphs ${\\cal F}$, the ${\\cal F}$-M-DELETION\nproblem consists in, given a graph $G$ and an integer $k$, deciding whether\nthere exists $S \\subseteq V(G)$ with $|S| \\leq k$ such that $G \\setminus S$\ndoes not contain any of the graphs in ${\\cal F}$ as a minor. We are interested\nin the parameterized complexity of ${\\cal F}$-M-DELETION when the parameter is\nthe treewidth of $G$, denoted by $tw$. Our objective is to determine, for a\nfixed ${\\cal F}$, the smallest function $f_{{\\cal F}}$ such that {${\\cal\nF}$-M-DELETION can be solved in time $f_{{\\cal F}}(tw) \\cdot n^{O(1)}$ on\n$n$-vertex graphs. We prove that $f_{{\\cal F}}(tw) = 2^{2^{O(tw \\cdot\\log\ntw)}}$ for every collection ${\\cal F}$, that $f_{{\\cal F}}(tw) = 2^{O(tw\n\\cdot\\log tw)}$ if ${\\cal F}$ contains a planar graph, and that $f_{{\\cal\nF}}(tw) = 2^{O(tw)}$ if in addition the input graph $G$ is planar or embedded\nin a surface. We also consider the version of the problem where the graphs in\n${\\cal F}$ are forbidden as topological minors, called ${\\cal F}$-TM-DELETION.\nWe prove similar results for this problem, except that in the last two\nalgorithms, instead of requiring ${\\cal F}$ to contain a planar graph, we need\nit to contain a subcubic planar graph. This is the first of a series of\narticles on this topic. \n\n"}
{"id": "1704.08235", "contents": "Title: Decremental Data Structures for Connectivity and Dominators in Directed\n  Graphs Abstract: We introduce a new dynamic data structure for maintaining the strongly\nconnected components (SCCs) of a directed graph (digraph) under edge deletions,\nso as to answer a rich repertoire of connectivity queries. Our main technical\ncontribution is a decremental data structure that supports sensitivity queries\nof the form \"are $ u $ and $ v $ strongly connected in the graph $ G \\setminus\nw $?\", for any triple of vertices $ u, v, w $, while $ G $ undergoes deletions\nof edges. Our data structure processes a sequence of edge deletions in a\ndigraph with $n$ vertices in $O(m n \\log{n})$ total time and $O(n^2 \\log{n})$\nspace, where $m$ is the number of edges before any deletion, and answers the\nabove queries in constant time. We can leverage our data structure to obtain\ndecremental data structures for many more types of queries within the same time\nand space complexity. For instance for edge-related queries, such as testing\nwhether two query vertices $u$ and $v$ are strongly connected in $G \\setminus\ne$, for some query edge $e$.\n  As another important application of our decremental data structure, we\nprovide the first nontrivial algorithm for maintaining the dominator tree of a\nflow graph under edge deletions. We present an algorithm that processes a\nsequence of edge deletions in a flow graph in $O(m n \\log{n})$ total time and\n$O(n^2 \\log{n})$ space. For reducible flow graphs we provide an $O(mn)$-time\nand $O(m + n)$-space algorithm. We give a conditional lower bound that provides\nevidence that these running times may be tight up to subpolynomial factors. \n\n"}
{"id": "1705.00985", "contents": "Title: Determinant-Preserving Sparsification of SDDM Matrices with Applications\n  to Counting and Sampling Spanning Trees Abstract: We show variants of spectral sparsification routines can preserve the total\nspanning tree counts of graphs, which by Kirchhoff's matrix-tree theorem, is\nequivalent to determinant of a graph Laplacian minor, or equivalently, of any\nSDDM matrix. Our analyses utilizes this combinatorial connection to bridge\nbetween statistical leverage scores / effective resistances and the analysis of\nrandom graphs by [Janson, Combinatorics, Probability and Computing `94]. This\nleads to a routine that in quadratic time, sparsifies a graph down to about\n$n^{1.5}$ edges in ways that preserve both the determinant and the distribution\nof spanning trees (provided the sparsified graph is viewed as a random object).\nExtending this algorithm to work with Schur complements and approximate\nCholeksy factorizations leads to algorithms for counting and sampling spanning\ntrees which are nearly optimal for dense graphs.\n  We give an algorithm that computes a $(1 \\pm \\delta)$ approximation to the\ndeterminant of any SDDM matrix with constant probability in about $n^2\n\\delta^{-2}$ time. This is the first routine for graphs that outperforms\ngeneral-purpose routines for computing determinants of arbitrary matrices. We\nalso give an algorithm that generates in about $n^2 \\delta^{-2}$ time a\nspanning tree of a weighted undirected graph from a distribution with total\nvariation distance of $\\delta$ from the $w$-uniform distribution . \n\n"}
{"id": "1705.01843", "contents": "Title: Quantum SDP-Solvers: Better upper and lower bounds Abstract: Brand\\~ao and Svore very recently gave quantum algorithms for approximately\nsolving semidefinite programs, which in some regimes are faster than the\nbest-possible classical algorithms in terms of the dimension $n$ of the problem\nand the number $m$ of constraints, but worse in terms of various other\nparameters. In this paper we improve their algorithms in several ways, getting\nbetter dependence on those other parameters. To this end we develop new\ntechniques for quantum algorithms, for instance a general way to efficiently\nimplement smooth functions of sparse Hamiltonians, and a generalized\nminimum-finding procedure.\n  We also show limits on this approach to quantum SDP-solvers, for instance for\ncombinatorial optimizations problems that have a lot of symmetry. Finally, we\nprove some general lower bounds showing that in the worst case, the complexity\nof every quantum LP-solver (and hence also SDP-solver) has to scale linearly\nwith $mn$ when $m\\approx n$, which is the same as classical. \n\n"}
{"id": "1705.01963", "contents": "Title: Polynomial time decodable codes for the binary deletion channel Abstract: In the random deletion channel, each bit is deleted independently with\nprobability $p$. For the random deletion channel, the existence of codes of\nrate $(1-p)/9$, and thus bounded away from $0$ for any $p < 1$, has been known.\nWe give an explicit construction with polynomial time encoding and deletion\ncorrection algorithms with rate $c_0 (1-p)$ for an absolute constant $c_0 > 0$. \n\n"}
{"id": "1705.02944", "contents": "Title: Hardness Results for Structured Linear Systems Abstract: We show that if the nearly-linear time solvers for Laplacian matrices and\ntheir generalizations can be extended to solve just slightly larger families of\nlinear systems, then they can be used to quickly solve all systems of linear\nequations over the reals. This result can be viewed either positively or\nnegatively: either we will develop nearly-linear time algorithms for solving\nall systems of linear equations over the reals, or progress on the families we\ncan solve in nearly-linear time will soon halt. \n\n"}
{"id": "1705.03150", "contents": "Title: Binary de Bruijn Sequences via Zech's Logarithms Abstract: The focus of this work is to show how to combine Zech's logarithms and each\nof the cycle joining and cross-join pairing methods to construct binary de\nBruijn sequences of any order. A basic implementation is supplied as a\nproof-of-concept.\n  The cycles, in the cycle joining method, are typically generated by a linear\nfeedback shift register. We prove a crucial characterization that determining\nZech's logarithms is equivalent to identifying conjugate pairs shared by any\ntwo distinct cycles. This speeds up the task of building a connected adjacency\nsubgraph that contains all vertices of the complete adjacency graph. Distinct\nspanning trees in either graph correspond to cyclically inequivalent de Bruijn\nsequences. As the cycles are being joined, guided by the conjugate pairs, we\ntrack the changes in the feedback function. Certificates of star or almost-star\nspanning trees conveniently handle large order cases.\n  The characterization of conjugate pairs via Zech's logarithms, as positional\nmarkings, is then adapted to identify cross-join pairs. A modified $m$-sequence\nis initially used, for ease of generation. The process can be repeated on each\nof the resulting de Bruijn sequences. We show how to integrate an analytic\ntool, attributed to Fryers, in the process.\n  Most prior constructions in the literature measure the complexity of the\ncorresponding bit-by-bit algorithms. Our approach is different. We aim first to\nbuild a connected adjacency subgraph that is certified to contain all of the\ncycles as vertices. The ingredients are computed just once and concisely\nstored. Simple strategies are offered to keep the complexities low as the order\ngrows. \n\n"}
{"id": "1705.03283", "contents": "Title: An exponential lower bound for Individualization-Refinement algorithms\n  for Graph Isomorphism Abstract: The individualization-refinement paradigm provides a strong toolbox for\ntesting isomorphism of two graphs and indeed, the currently fastest\nimplementations of isomorphism solvers all follow this approach. While these\nsolvers are fast in practice, from a theoretical point of view, no general\nlower bounds concerning the worst case complexity of these tools are known. In\nfact, it is an open question whether individualization-refinement algorithms\ncan achieve upper bounds on the running time similar to the more theoretical\ntechniques based on a group theoretic approach.\n  In this work we give a negative answer to this question and construct a\nfamily of graphs on which algorithms based on the individualization-refinement\nparadigm require exponential time. Contrary to a previous construction of\nMiyazaki, that only applies to a specific implementation within the\nindividualization-refinement framework, our construction is immune to changing\nthe cell selector, or adding various heuristic invariants to the algorithm.\nFurthermore, our graphs also provide exponential lower bounds in the case when\nthe $k$-dimensional Weisfeiler-Leman algorithm is used to replace the standard\ncolor refinement operator and the arguments even work when the entire\nautomorphism group of the inputs is initially provided to the algorithm. \n\n"}
{"id": "1705.03934", "contents": "Title: Autoscaling Bloom Filter: Controlling Trade-off Between True and False\n  Positives Abstract: A Bloom filter is a simple data structure supporting membership queries on a\nset. The standard Bloom filter does not support the delete operation,\ntherefore, many applications use a counting Bloom filter to enable deletion.\nThis paper proposes a generalization of the counting Bloom filter approach,\ncalled \"autoscaling Bloom filters\", which allows adjustment of its capacity\nwith probabilistic bounds on false positives and true positives. In essence,\nthe autoscaling Bloom filter is a binarized counting Bloom filter with an\nadjustable binarization threshold. We present the mathematical analysis of the\nperformance as well as give a procedure for minimization of the false positive\nrate. \n\n"}
{"id": "1705.04840", "contents": "Title: Sublogarithmic Distributed Algorithms for Lov\\'asz Local lemma, and the\n  Complexity Hierarchy Abstract: Locally Checkable Labeling (LCL) problems include essentially all the classic\nproblems of $\\mathsf{LOCAL}$ distributed algorithms. In a recent enlightening\nrevelation, Chang and Pettie [arXiv 1704.06297] showed that any LCL (on bounded\ndegree graphs) that has an $o(\\log n)$-round randomized algorithm can be solved\nin $T_{LLL}(n)$ rounds, which is the randomized complexity of solving (a\nrelaxed variant of) the Lov\\'asz Local Lemma (LLL) on bounded degree $n$-node\ngraphs. Currently, the best known upper bound on $T_{LLL}(n)$ is $O(\\log n)$,\nby Chung, Pettie, and Su [PODC'14], while the best known lower bound is\n$\\Omega(\\log\\log n)$, by Brandt et al. [STOC'16]. Chang and Pettie conjectured\nthat there should be an $O(\\log\\log n)$-round algorithm.\n  Making the first step of progress towards this conjecture, and providing a\nsignificant improvement on the algorithm of Chung et al. [PODC'14], we prove\nthat $T_{LLL}(n)= 2^{O(\\sqrt{\\log\\log n})}$. Thus, any $o(\\log n)$-round\nrandomized distributed algorithm for any LCL problem on bounded degree graphs\ncan be automatically sped up to run in $2^{O(\\sqrt{\\log\\log n})}$ rounds.\n  Using this improvement and a number of other ideas, we also improve the\ncomplexity of a number of graph coloring problems (in arbitrary degree graphs)\nfrom the $O(\\log n)$-round results of Chung, Pettie and Su [PODC'14] to\n$2^{O(\\sqrt{\\log\\log n})}$. These problems include defective coloring, frugal\ncoloring, and list vertex-coloring. \n\n"}
{"id": "1705.08282", "contents": "Title: Algorithms and hardness results for happy coloring problems Abstract: In a vertex-colored graph, an edge is happy if its endpoints have the same\ncolor. Similarly, a vertex is happy if all its incident edges are happy.\nMotivated by the computation of homophily in social networks, we consider the\nalgorithmic aspects of the following Maximum Happy Edges (k-MHE) problem: given\na partially k-colored graph G, find an extended full k-coloring of G maximizing\nthe number of happy edges. When we want to maximize the number of happy\nvertices, the problem is known as Maximum Happy Vertices (k-MHV). We further\nstudy the complexity of the problems and their weighted variants. For instance,\nwe prove that for every k >= 3, both problems are NP-complete for bipartite\ngraphs and k-MHV remains hard for split graphs. In terms of exact algorithms,\nwe show both problems can be solved in time O*(2^n), and give an even faster\nO*(1.89^n)-time algorithm when k = 3. From a parameterized perspective, we give\na linear vertex kernel for Weighted k-MHE, where edges are weighted and the\ngoal is to obtain happy edges of at least a specified total weight. Finally, we\nprove both problems are solvable in polynomial-time when the graph has bounded\ntreewidth or bounded neighborhood diversity. \n\n"}
{"id": "1705.09652", "contents": "Title: The border support rank of two-by-two matrix multiplication is seven Abstract: We show that the border support rank of the tensor corresponding to\ntwo-by-two matrix multiplication is seven over the complex numbers. We do this\nby constructing two polynomials that vanish on all complex tensors with format\nfour-by-four-by-four and border rank at most six, but that do not vanish\nsimultaneously on any tensor with the same support as the two-by-two matrix\nmultiplication tensor. This extends the work of Hauenstein, Ikenmeyer, and\nLandsberg. We also give two proofs that the support rank of the two-by-two\nmatrix multiplication tensor is seven over any field: one proof using a result\nof De Groote saying that the decomposition of this tensor is unique up to\nsandwiching, and another proof via the substitution method. These results\nanswer a question asked by Cohn and Umans. Studying the border support rank of\nthe matrix multiplication tensor is relevant for the design of matrix\nmultiplication algorithms, because upper bounds on the border support rank of\nthe matrix multiplication tensor lead to upper bounds on the computational\ncomplexity of matrix multiplication, via a construction of Cohn and Umans.\nMoreover, support rank has applications in quantum communication complexity. \n\n"}
{"id": "1706.00078", "contents": "Title: Low-Rank Matrix Approximation in the Infinity Norm Abstract: The low-rank matrix approximation problem with respect to the entry-wise\n$\\ell_{\\infty}$-norm is the following: given a matrix $M$ and a factorization\nrank $r$, find a matrix $X$ whose rank is at most $r$ and that minimizes\n$\\max_{i,j} |M_{ij} - X_{ij}|$. In this paper, we prove that the decision\nvariant of this problem for $r=1$ is NP-complete using a reduction from the\nproblem `not all equal 3SAT'. We also analyze several cases when the problem\ncan be solved in polynomial time, and propose a simple practical heuristic\nalgorithm which we apply on the problem of the recovery of a quantized low-rank\nmatrix. \n\n"}
{"id": "1706.03451", "contents": "Title: Constraint Satisfaction Problem Dichotomy for Finite Templates: a Proof\n  Via Consistency Checks Abstract: One of the central problems in the study of parametrized constraint\nsatisfaction problems is the Dichotomy Conjecture by T. Feder and M. Vardi\nstating that the constraint satisfaction problem (CSP) over a fixed, finite\nconstraint language is either solvable in polynomial time or\n\\textsc{NP}-complete. The conjecture was verified in certain special cases\n(domains with a relatively small number of elements, constraint languages\ncontaining all unary relations, etc.) In this article, we present a proof of\nthe Dichotomy Conjecture via local consistency and AF- consistency checks. In\nfact, we show that, for every Taylor domain, which is\n$(2\\lceil\\frac{K}{2}\\rceil,3\\lceil\\frac{K}{2}\\rceil)$-consistent, where $K$ is\nthe largest arity of a relation in the constraint language, we can define\npolynomially many proper subinstances such that, the original instance of the\nCSP is solvable if, and only if, the problem has a solution in one of those\nsubinstances.. Finally, a solution is constructed using the combination of SLAC\n(Singleton Linear Arc Consistency), introduced by M. Kozik, and AF-consistency. \n\n"}
{"id": "1706.03887", "contents": "Title: Clustering High Dimensional Dynamic Data Streams Abstract: We present data streaming algorithms for the $k$-median problem in\nhigh-dimensional dynamic geometric data streams, i.e. streams allowing both\ninsertions and deletions of points from a discrete Euclidean space $\\{1, 2,\n\\ldots \\Delta\\}^d$. Our algorithms use $k \\epsilon^{-2} poly(d \\log \\Delta)$\nspace/time and maintain with high probability a small weighted set of points (a\ncoreset) such that for every set of $k$ centers the cost of the coreset\n$(1+\\epsilon)$-approximates the cost of the streamed point set. We also provide\nalgorithms that guarantee only positive weights in the coreset with additional\nlogarithmic factors in the space and time complexities. We can use this\npositively-weighted coreset to compute a $(1+\\epsilon)$-approximation for the\n$k$-median problem by any efficient offline $k$-median algorithm. All previous\nalgorithms for computing a $(1+\\epsilon)$-approximation for the $k$-median\nproblem over dynamic data streams required space and time exponential in $d$.\nOur algorithms can be generalized to metric spaces of bounded doubling\ndimension. \n\n"}
{"id": "1706.04225", "contents": "Title: Inner Rank and Lower Bounds for Matrix Multiplication Abstract: We develop a notion of {\\em inner rank} as a tool for obtaining lower bounds\non the rank of matrix multiplication tensors. We use it to give a short proof\nthat the border rank (and therefore rank) of the tensor associated with\n$n\\times n$ matrix multiplication over an arbitrary field is at least\n$2n^2-n+1$. While inner rank does not provide improvements to currently known\nlower bounds, we argue that this notion merits further study. \n\n"}
{"id": "1706.04540", "contents": "Title: On Error Detection in Asymmetric Channels Abstract: We study the error detection problem in $ q $-ary asymmetric channels wherein\nevery input symbol $ x_i $ is mapped to an output symbol $ y_i $ satisfying $\ny_i \\geq x_i $. A general setting is assumed where the noise vectors are\n(potentially) restricted in: 1) the amplitude, $ y_i - x_i \\leq a $, 2) the\nHamming weight, $ \\sum_{i=1}^n 1_{\\{y_i \\neq x_i\\}} \\leq h $, and 3) the total\nweight, $ \\sum_{i=1}^n (y_i - x_i) \\leq t $. Optimal codes detecting these\ntypes of errors are described for certain sets of parameters $ a, h, t $, both\nin the standard and in the cyclic ($ \\operatorname{mod}\\, q $) version of the\nproblem. It is also demonstrated that these codes are optimal in the large\nalphabet limit for every $ a, h, t $ and every block-length $ n $. \n\n"}
{"id": "1706.05847", "contents": "Title: Conditional Lower Bounds for Space/Time Tradeoffs Abstract: In recent years much effort has been concentrated towards achieving\npolynomial time lower bounds on algorithms for solving various well-known\nproblems. A useful technique for showing such lower bounds is to prove them\nconditionally based on well-studied hardness assumptions such as 3SUM, APSP,\nSETH, etc. This line of research helps to obtain a better understanding of the\ncomplexity inside P.\n  A related question asks to prove conditional space lower bounds on data\nstructures that are constructed to solve certain algorithmic tasks after an\ninitial preprocessing stage. This question received little attention in\nprevious research even though it has potential strong impact.\n  In this paper we address this question and show that surprisingly many of the\nwell-studied hard problems that are known to have conditional polynomial time\nlower bounds are also hard when concerning space. This hardness is shown as a\ntradeoff between the space consumed by the data structure and the time needed\nto answer queries. The tradeoff may be either smooth or admit one or more\nsingularity points.\n  We reveal interesting connections between different space hardness\nconjectures and present matching upper bounds. We also apply these hardness\nconjectures to both static and dynamic problems and prove their conditional\nspace hardness.\n  We believe that this novel framework of polynomial space conjectures can play\nan important role in expressing polynomial space lower bounds of many important\nalgorithmic problems. Moreover, it seems that it can also help in achieving a\nbetter understanding of the hardness of their corresponding problems in terms\nof time. \n\n"}
{"id": "1706.07290", "contents": "Title: New Cardinality Estimation Methods for HyperLogLog Sketches Abstract: This work presents new cardinality estimation methods for data sets recorded\nby HyperLogLog sketches. A simple derivation of the original estimator was\nfound, that also gives insight how to correct its deficiencies. The result is\nan improved estimator that is unbiased over the full cardinality range, is easy\ncomputable, and does not rely on empirically determined data as previous\napproaches. Based on the maximum likelihood principle a second unbiased\nestimation method is presented which can also be extended to estimate\ncardinalities of union, intersection, or relative complements of two sets that\nare both represented as HyperLogLog sketches. Experimental results show that\nthis approach is more precise than the conventional technique using the\ninclusion-exclusion principle. \n\n"}
{"id": "1706.08786", "contents": "Title: The Complexity of Counting Surjective Homomorphisms and Compactions Abstract: A homomorphism from a graph G to a graph H is a function from the vertices of\nG to the vertices of H that preserves edges. A homomorphism is surjective if it\nuses all of the vertices of H and it is a compaction if it uses all of the\nvertices of H and all of the non-loop edges of H. Hell and Nesetril gave a\ncomplete characterisation of the complexity of deciding whether there is a\nhomomorphism from an input graph G to a fixed graph H. A complete\ncharacterisation is not known for surjective homomorphisms or for compactions,\nthough there are many interesting results. Dyer and Greenhill gave a complete\ncharacterisation of the complexity of counting homomorphisms from an input\ngraph G to a fixed graph H. In this paper, we give a complete characterisation\nof the complexity of counting surjective homomorphisms from an input graph G to\na fixed graph H and we also give a complete characterisation of the complexity\nof counting compactions from an input graph G to a fixed graph H. In an\naddendum we use our characterisations to point out a dichotomy for the\ncomplexity of the respective approximate counting problems (in the connected\ncase). \n\n"}
{"id": "1706.09043", "contents": "Title: Critical Vertices and Edges in $H$-free Graphs Abstract: A vertex or edge in a graph is critical if its deletion reduces the chromatic\nnumber of the graph by 1. We consider the problems of deciding whether a graph\nhas a critical vertex or edge, respectively. We give a complexity dichotomy for\nboth problems restricted to $H$-free graphs, that is, graphs with no induced\nsubgraph isomorphic to $H$. Moreover, we show that an edge is critical if and\nonly if its contraction reduces the chromatic number by 1. Hence, we also\nobtain a complexity dichotomy for the problem of deciding if a graph has an\nedge whose contraction reduces the chromatic number by 1. \n\n"}
{"id": "1706.09066", "contents": "Title: On the complexity of finding internally vertex-disjoint long directed\n  paths Abstract: For two positive integers $k$ and $\\ell$, a $(k \\times \\ell)$-spindle is the\nunion of $k$ pairwise internally vertex-disjoint directed paths with $\\ell$\narcs between two vertices $u$ and $v$. We are interested in the (parameterized)\ncomplexity of several problems consisting in deciding whether a given digraph\ncontains a subdivision of a spindle, which generalize both the Maximum Flow and\nLongest Path problems. We obtain the following complexity dichotomy: for a\nfixed $\\ell \\geq 1$, finding the largest $k$ such that an input digraph $G$\ncontains a subdivision of a $(k \\times \\ell)$-spindle is polynomial-time\nsolvable if $\\ell \\leq 3$, and NP-hard otherwise. We place special emphasis on\nfinding spindles with exactly two paths and present FPT algorithms that are\nasymptotically optimal under the ETH. These algorithms are based on the\ntechnique of representative families in matroids, and use also color-coding as\na subroutine. Finally, we study the case where the input graph is acyclic, and\npresent several algorithmic and hardness results. \n\n"}
{"id": "1706.09362", "contents": "Title: Sample-based high-dimensional convexity testing Abstract: In the problem of high-dimensional convexity testing, there is an unknown set\n$S \\subseteq \\mathbb{R}^n$ which is promised to be either convex or\n$\\varepsilon$-far from every convex body with respect to the standard\nmultivariate normal distribution $\\mathcal{N}(0, 1)^n$. The job of a testing\nalgorithm is then to distinguish between these two cases while making as few\ninspections of the set $S$ as possible.\n  In this work we consider sample-based testing algorithms, in which the\ntesting algorithm only has access to labeled samples\n$(\\boldsymbol{x},S(\\boldsymbol{x}))$ where each $\\boldsymbol{x}$ is\nindependently drawn from $\\mathcal{N}(0, 1)^n$. We give nearly matching sample\ncomplexity upper and lower bounds for both one-sided and two-sided convexity\ntesting algorithms in this framework. For constant $\\varepsilon$, our results\nshow that the sample complexity of one-sided convexity testing is\n$2^{\\tilde{\\Theta}(n)}$ samples, while for two-sided convexity testing it is\n$2^{\\tilde{\\Theta}(\\sqrt{n})}$. \n\n"}
{"id": "1706.09370", "contents": "Title: DynASP2.5: Dynamic Programming on Tree Decompositions in Action Abstract: A vibrant theoretical research area are efficient exact parameterized\nalgorithms. Very recent solving competitions such as the PACE challenge show\nthat there is also increasing practical interest in the parameterized\nalgorithms community. An important research question is whether dedicated\nparameterized exact algorithms exhibit certain practical relevance and one can\neven beat well-established problem solvers. We consider the logic-based\ndeclarative modeling language and problem solving framework Answer Set\nProgramming (ASP). State-of-the-art ASP solvers rely considerably on Sat-based\nalgorithms. An ASP solver (DynASP2), which is based on a classical dynamic\nprogramming on tree decompositions, has been published very recently.\nUnfortunately, DynASP2 can outperform modern ASP solvers on programs of small\ntreewidth only if the question of interest is to count the number of solutions.\nIn this paper, we describe underlying concepts of our new implementation\n(DynASP2.5) that shows competitive behavior to state-of-the-art ASP solvers\neven for finding just one solution when solving problems as the Steiner tree\nproblem that have been modeled in ASP on graphs with low treewidth. Our\nimplementation is based on a novel approach that we call multi-pass dynamic\nprogramming (M-DPSINC). \n\n"}
{"id": "1706.09884", "contents": "Title: On the Limitations of First-Order Approximation in GAN Dynamics Abstract: While Generative Adversarial Networks (GANs) have demonstrated promising\nperformance on multiple vision tasks, their learning dynamics are not yet well\nunderstood, both in theory and in practice. To address this issue, we study GAN\ndynamics in a simple yet rich parametric model that exhibits several of the\ncommon problematic convergence behaviors such as vanishing gradients, mode\ncollapse, and diverging or oscillatory behavior. In spite of the non-convex\nnature of our model, we are able to perform a rigorous theoretical analysis of\nits convergence behavior. Our analysis reveals an interesting dichotomy: a GAN\nwith an optimal discriminator provably converges, while first order\napproximations of the discriminator steps lead to unstable GAN dynamics and\nmode collapse. Our result suggests that using first order discriminator steps\n(the de-facto standard in most existing GAN setups) might be one of the factors\nthat makes GAN training challenging in practice. \n\n"}
{"id": "1706.10110", "contents": "Title: On Using Toeplitz and Circulant Matrices for Johnson-Lindenstrauss\n  Transforms Abstract: The Johnson-Lindenstrauss lemma is one of the corner stone results in\ndimensionality reduction. It says that given $N$, for any set of $N$ vectors $X\n\\subset \\mathbb{R}^n$, there exists a mapping $f : X \\to \\mathbb{R}^m$ such\nthat $f(X)$ preserves all pairwise distances between vectors in $X$ to within\n$(1 \\pm \\varepsilon)$ if $m = O(\\varepsilon^{-2} \\lg N)$. Much effort has gone\ninto developing fast embedding algorithms, with the Fast Johnson-Lindenstrauss\ntransform of Ailon and Chazelle being one of the most well-known techniques.\nThe current fastest algorithm that yields the optimal $m =\nO(\\varepsilon^{-2}\\lg N)$ dimensions has an embedding time of $O(n \\lg n +\n\\varepsilon^{-2} \\lg^3 N)$. An exciting approach towards improving this, due to\nHinrichs and Vyb\\'iral, is to use a random $m \\times n$ Toeplitz matrix for the\nembedding. Using Fast Fourier Transform, the embedding of a vector can then be\ncomputed in $O(n \\lg m)$ time. The big question is of course whether $m =\nO(\\varepsilon^{-2} \\lg N)$ dimensions suffice for this technique. If so, this\nwould end a decades long quest to obtain faster and faster\nJohnson-Lindenstrauss transforms. The current best analysis of the embedding of\nHinrichs and Vyb\\'iral shows that $m = O(\\varepsilon^{-2}\\lg^2 N)$ dimensions\nsuffices. The main result of this paper, is a proof that this analysis\nunfortunately cannot be tightened any further, i.e., there exists a set of $N$\nvectors requiring $m = \\Omega(\\varepsilon^{-2} \\lg^2 N)$ for the Toeplitz\napproach to work. \n\n"}
{"id": "1707.00349", "contents": "Title: A new algorithm for fast generalized DFTs Abstract: We give an new arithmetic algorithm to compute the generalized Discrete\nFourier Transform (DFT) over finite groups $G$. The new algorithm uses\n$O(|G|^{\\omega/2 + o(1)})$ operations to compute the generalized DFT over\nfinite groups of Lie type, including the linear, orthogonal, and symplectic\nfamilies and their variants, as well as all finite simple groups of Lie type.\nHere $\\omega$ is the exponent of matrix multiplication, so the exponent\n$\\omega/2$ is optimal if $\\omega = 2$. Previously, \"exponent one\" algorithms\nwere known for supersolvable groups and the symmetric and alternating groups.\nNo exponent one algorithms were known (even under the assumption $\\omega = 2$)\nfor families of linear groups of fixed dimension, and indeed the previous\nbest-known algorithm for $SL_2(F_q)$ had exponent $4/3$ despite being the focus\nof significant effort. We unconditionally achieve exponent at most $1.19$ for\nthis group, and exponent one if $\\omega = 2$. Our algorithm also yields an\nimproved exponent for computing the generalized DFT over general finite groups\n$G$, which beats the longstanding previous best upper bound, for any $\\omega$.\nIn particular, assuming $\\omega = 2$, we achieve exponent $\\sqrt{2}$, while the\nprevious best was $3/2$. \n\n"}
{"id": "1707.01182", "contents": "Title: Biased Predecessor Search Abstract: We consider the problem of performing predecessor searches in a bounded\nuniverse while achieving query times that depend on the distribution of\nqueries. We obtain several data structures with various properties: in\nparticular, we give data structures that achieve expected query times\nlogarithmic in the entropy of the distribution of queries but with space\nbounded in terms of universe size, as well as data structures that use only\nlinear space but with query times that are higher (but still sublinear)\nfunctions of the entropy. For these structures, the distribution is assumed to\nbe known. We also consider individual query times on universe elements with\ngeneral weights, as well as the case when the distribution is not known in\nadvance. \n\n"}
{"id": "1707.01743", "contents": "Title: Fast Compressed Self-Indexes with Deterministic Linear-Time Construction Abstract: We introduce a compressed suffix array representation that, on a text $T$ of\nlength $n$ over an alphabet of size $\\sigma$, can be built in $O(n)$\ndeterministic time, within $O(n\\log\\sigma)$ bits of working space, and counts\nthe number of occurrences of any pattern $P$ in $T$ in time $O(|P| + \\log\\log_w\n\\sigma)$ on a RAM machine of $w=\\Omega(\\log n)$-bit words. This new index\noutperforms all the other compressed indexes that can be built in linear\ndeterministic time, and some others. The only faster indexes can be built in\nlinear time only in expectation, or require $\\Theta(n\\log n)$ bits. We also\nshow that, by using $O(n\\log\\sigma)$ bits, we can build in linear time an index\nthat counts in time $O(|P|/\\log_\\sigma n + \\log n(\\log\\log n)^2)$, which is\nRAM-optimal for $w=\\Theta(\\log n)$ and sufficiently long patterns. \n\n"}
{"id": "1707.02000", "contents": "Title: Shared-memory Graph Truss Decomposition Abstract: We present PKT, a new shared-memory parallel algorithm and OpenMP\nimplementation for the truss decomposition of large sparse graphs. A k-truss is\na dense subgraph definition that can be considered a relaxation of a clique.\nTruss decomposition refers to a partitioning of all the edges in the graph\nbased on their k-truss membership. The truss decomposition of a graph has many\napplications. We show that our new approach PKT consistently outperforms other\ntruss decomposition approaches for a collection of large sparse graphs and on a\n24-core shared-memory server. PKT is based on a recently proposed algorithm for\nk-core decomposition. \n\n"}
{"id": "1707.02190", "contents": "Title: A subexponential parameterized algorithm for Directed Subset Traveling\n  Salesman Problem on planar graphs Abstract: There are numerous examples of the so-called ``square root phenomenon'' in\nthe field of parameterized algorithms: many of the most fundamental graph\nproblems, parameterized by some natural parameter $k$, become significantly\nsimpler when restricted to planar graphs and in particular the best possible\nrunning time is exponential in $O(\\sqrt{k})$ instead of $O(k)$ (modulo standard\ncomplexity assumptions). We consider a classic optimization problem Subset\nTraveling Salesman, where we are asked to visit all the terminals $T$ by a\nminimum-weight closed walk. We investigate the parameterized complexity of this\nproblem in planar graphs, where the number $k=|T|$ of terminals is regarded as\nthe parameter. We show that Subset TSP can be solved in time $2^{O(\\sqrt{k}\\log\nk)}\\cdot n^{O(1)}$ even on edge-weighted directed planar graphs. This improves\nupon the algorithm of Klein and Marx [SODA 2014] with the same running time\nthat worked only on undirected planar graphs with polynomially large integer\nweights. \n\n"}
{"id": "1707.02260", "contents": "Title: Fair Personalization Abstract: Personalization is pervasive in the online space as, when combined with\nlearning, it leads to higher efficiency and revenue by allowing the most\nrelevant content to be served to each user. However, recent studies suggest\nthat such personalization can propagate societal or systemic biases, which has\nled to calls for regulatory mechanisms and algorithms to combat inequality.\nHere we propose a rigorous algorithmic framework that allows for the\npossibility to control biased or discriminatory personalization with respect to\nsensitive attributes of users without losing all of the benefits of\npersonalization. \n\n"}
{"id": "1707.02670", "contents": "Title: Accelerated Stochastic Power Iteration Abstract: Principal component analysis (PCA) is one of the most powerful tools in\nmachine learning. The simplest method for PCA, the power iteration, requires\n$\\mathcal O(1/\\Delta)$ full-data passes to recover the principal component of a\nmatrix with eigen-gap $\\Delta$. Lanczos, a significantly more complex method,\nachieves an accelerated rate of $\\mathcal O(1/\\sqrt{\\Delta})$ passes. Modern\napplications, however, motivate methods that only ingest a subset of available\ndata, known as the stochastic setting. In the online stochastic setting, simple\nalgorithms like Oja's iteration achieve the optimal sample complexity $\\mathcal\nO(\\sigma^2/\\Delta^2)$. Unfortunately, they are fully sequential, and also\nrequire $\\mathcal O(\\sigma^2/\\Delta^2)$ iterations, far from the $\\mathcal\nO(1/\\sqrt{\\Delta})$ rate of Lanczos. We propose a simple variant of the power\niteration with an added momentum term, that achieves both the optimal sample\nand iteration complexity. In the full-pass setting, standard analysis shows\nthat momentum achieves the accelerated rate, $\\mathcal O(1/\\sqrt{\\Delta})$. We\ndemonstrate empirically that naively applying momentum to a stochastic method,\ndoes not result in acceleration. We perform a novel, tight variance analysis\nthat reveals the \"breaking-point variance\" beyond which this acceleration does\nnot occur. By combining this insight with modern variance reduction techniques,\nwe construct stochastic PCA algorithms, for the online and offline setting,\nthat achieve an accelerated iteration complexity $\\mathcal O(1/\\sqrt{\\Delta})$.\nDue to the embarassingly parallel nature of our methods, this acceleration\ntranslates directly to wall-clock time if deployed in a parallel environment.\nOur approach is very general, and applies to many non-convex optimization\nproblems that can now be accelerated using the same technique. \n\n"}
{"id": "1707.04316", "contents": "Title: How hard is it to satisfy (almost) all roommates? Abstract: The classic Stable Roommates problem (which is the non-bipartite\ngeneralization of the well-known Stable Marriage problem) asks whether there is\na stable matching for a given set of agents, i.e. a partitioning of the agents\ninto disjoint pairs such that no two agents induce a blocking pair. Herein,\neach agent has a preference list denoting who it prefers to have as a partner,\nand two agents are blocking if they prefer to be with each other rather than\nwith their assigned partners. Since stable matchings may not be unique, we\nstudy an NP-hard optimization variant of Stable Roommates, called Egal Stable\nRoommates, which seeks to find a stable matching with a minimum egalitarian\ncost {\\gamma}, i.e. the sum of the dissatisfaction of the agents is minimum.\nThe dissatisfaction of an agent is the number of agents that this agent prefers\nover its partner if it is matched; otherwise it is the length of its preference\nlist. We also study almost stable matchings, called Min-Block-Pair Stable\nRoommates, which seeks to find a matching with a minimum number {\\beta} of\nblocking pairs. Our main result is that Egal Stable Roommates parameterized by\n{\\gamma} is fixed-parameter tractable, while Min-Block-Pair Stable Roommates\nparameterized by {\\beta} is W[1]-hard, even if the length of each preference\nlist is at most five. \n\n"}
{"id": "1707.04615", "contents": "Title: On the Complexity of Learning Neural Networks Abstract: The stunning empirical successes of neural networks currently lack rigorous\ntheoretical explanation. What form would such an explanation take, in the face\nof existing complexity-theoretic lower bounds? A first step might be to show\nthat data generated by neural networks with a single hidden layer, smooth\nactivation functions and benign input distributions can be learned efficiently.\nWe demonstrate here a comprehensive lower bound ruling out this possibility:\nfor a wide class of activation functions (including all currently used), and\ninputs drawn from any logconcave distribution, there is a family of\none-hidden-layer functions whose output is a sum gate, that are hard to learn\nin a precise sense: any statistical query algorithm (which includes all known\nvariants of stochastic gradient descent with any loss function) needs an\nexponential number of queries even using tolerance inversely proportional to\nthe input dimensionality. Moreover, this hard family of functions is realizable\nwith a small (sublinear in dimension) number of activation units in the single\nhidden layer. The lower bound is also robust to small perturbations of the true\nweights. Systematic experiments illustrate a phase transition in the training\nerror as predicted by the analysis. \n\n"}
{"id": "1707.04858", "contents": "Title: On Approximating the Number of $k$-cliques in Sublinear Time Abstract: We study the problem of approximating the number of $k$-cliques in a graph\nwhen given query access to the graph.\n  We consider the standard query model for general graphs via (1) degree\nqueries, (2) neighbor queries and (3) pair queries. Let $n$ denote the number\nof vertices in the graph, $m$ the number of edges, and $C_k$ the number of\n$k$-cliques. We design an algorithm that outputs a\n$(1+\\varepsilon)$-approximation (with high probability) for $C_k$, whose\nexpected query complexity and running time are\n$O\\left(\\frac{n}{C_k^{1/k}}+\\frac{m^{k/2}}{C_k}\\right)\\poly(\\log\nn,1/\\varepsilon,k)$.\n  Hence, the complexity of the algorithm is sublinear in the size of the graph\nfor $C_k = \\omega(m^{k/2-1})$. Furthermore, we prove a lower bound showing that\nthe query complexity of our algorithm is essentially optimal (up to the\ndependence on $\\log n$, $1/\\varepsilon$ and $k$).\n  The previous results in this vein are by Feige (SICOMP 06) and by Goldreich\nand Ron (RSA 08) for edge counting ($k=2$) and by Eden et al. (FOCS 2015) for\ntriangle counting ($k=3$). Our result matches the complexities of these\nresults.\n  The previous result by Eden et al. hinges on a certain amortization technique\nthat works only for triangle counting, and does not generalize for larger\ncliques. We obtain a general algorithm that works for any $k\\geq 3$ by\ndesigning a procedure that samples each $k$-clique incident to a given set $S$\nof vertices with approximately equal probability. The primary difficulty is in\nfinding cliques incident to purely high-degree vertices, since random sampling\nwithin neighbors has a low success probability. This is achieved by an\nalgorithm that samples uniform random high degree vertices and a careful\ntradeoff between estimating cliques incident purely to high-degree vertices and\nthose that include a low-degree vertex. \n\n"}
{"id": "1707.05527", "contents": "Title: Nested Convex Bodies are Chaseable Abstract: In the Convex Body Chasing problem, we are given an initial point $v_0$ in\n$R^d$ and an online sequence of $n$ convex bodies $F_1, ..., F_n$. When we\nreceive $F_i$, we are required to move inside $F_i$. Our goal is to minimize\nthe total distance travelled. This fundamental online problem was first studied\nby Friedman and Linial (DCG 1993). They proved an $\\Omega(\\sqrt{d})$ lower\nbound on the competitive ratio, and conjectured that a competitive ratio\ndepending only on d is possible. However, despite much interest in the problem,\nthe conjecture remains wide open.\n  We consider the setting in which the convex bodies are nested: $F_1 \\supset\n... \\supset F_n$. The nested setting is closely related to extending the online\nLP framework of Buchbinder and Naor (ESA 2005) to arbitrary linear constraints.\nMoreover, this setting retains much of the difficulty of the general setting\nand captures an essential obstacle in resolving Friedman and Linial's\nconjecture. In this work, we give the first $f(d)$-competitive algorithm for\nchasing nested convex bodies in $R^d$. \n\n"}
{"id": "1707.06664", "contents": "Title: Estimation of Sparsity via Simple Measurements Abstract: We consider several related problems of estimating the 'sparsity' or number\nof nonzero elements $d$ in a length $n$ vector $\\mathbf{x}$ by observing only\n$\\mathbf{b} = M \\odot \\mathbf{x}$, where $M$ is a predesigned test matrix\nindependent of $\\mathbf{x}$, and the operation $\\odot$ varies between problems.\nWe aim to provide a $\\Delta$-approximation of sparsity for some constant\n$\\Delta$ with a minimal number of measurements (rows of $M$). This framework\ngeneralizes multiple problems, such as estimation of sparsity in group testing\nand compressed sensing. We use techniques from coding theory as well as\nprobabilistic methods to show that $O(D \\log D \\log n)$ rows are sufficient\nwhen the operation $\\odot$ is logical OR (i.e., group testing), and nearly this\nmany are necessary, where $D$ is a known upper bound on $d$. When instead the\noperation $\\odot$ is multiplication over $\\mathbb{R}$ or a finite field\n$\\mathbb{F}_q$, we show that respectively $\\Theta(D)$ and $\\Theta(D \\log_q\n\\frac{n}{D})$ measurements are necessary and sufficient. \n\n"}
{"id": "1707.08730", "contents": "Title: A Quantum Approach to Subset-Sum and Similar Problems Abstract: In this paper, we study the subset-sum problem by using a quantum heuristic\napproach similar to the verification circuit of quantum Arthur-Merlin games.\nUnder described certain assumptions, we show that the exact solution of the\nsubset sum problem my be obtained in polynomial time and the exponential\nspeed-up over the classical algorithms may be possible. We give a numerical\nexample and discuss the complexity of the approach and its further application\nto the knapsack problem. \n\n"}
{"id": "1707.09863", "contents": "Title: Dimensionality reduction of SDPs through sketching Abstract: We show how to sketch semidefinite programs (SDPs) using positive maps in\norder to reduce their dimension. More precisely, we use\nJohnson\\hyp{}Lindenstrauss transforms to produce a smaller SDP whose solution\npreserves feasibility or approximates the value of the original problem with\nhigh probability. These techniques allow to improve both complexity and storage\nspace requirements. They apply to problems in which the Schatten 1-norm of the\nmatrices specifying the SDP and also of a solution to the problem is constant\nin the problem size. Furthermore, we provide some results which clarify the\nlimitations of positive, linear sketches in this setting. \n\n"}
{"id": "1708.00240", "contents": "Title: An Efficient Algorithm for Mixed Domination on Generalized\n  Series-Parallel Graphs Abstract: A mixed dominating set $S$ of a graph $G=(V,E)$ is a subset $ S \\subseteq V\n\\cup E$ such that each element $v\\in (V \\cup E) \\setminus S$ is adjacent or\nincident to at least one element in $S$. The mixed domination number\n$\\gamma_m(G)$ of a graph $G$ is the minimum cardinality among all mixed\ndominating sets in $G$. The problem of finding $\\gamma_{m}(G)$ is know to be\nNP-complete. In this paper, we present an explicit polynomial-time algorithm to\nconstruct a mixed dominating set of size $\\gamma_{m}(G)$ by a parse tree when\n$G$ is a generalized series-parallel graph. \n\n"}
{"id": "1708.03708", "contents": "Title: Eigenvalue Decay Implies Polynomial-Time Learnability for Neural\n  Networks Abstract: We consider the problem of learning function classes computed by neural\nnetworks with various activations (e.g. ReLU or Sigmoid), a task believed to be\ncomputationally intractable in the worst-case. A major open problem is to\nunderstand the minimal assumptions under which these classes admit provably\nefficient algorithms. In this work we show that a natural distributional\nassumption corresponding to {\\em eigenvalue decay} of the Gram matrix yields\npolynomial-time algorithms in the non-realizable setting for expressive classes\nof networks (e.g. feed-forward networks of ReLUs). We make no assumptions on\nthe structure of the network or the labels. Given sufficiently-strong\npolynomial eigenvalue decay, we obtain {\\em fully}-polynomial time algorithms\nin {\\em all} the relevant parameters with respect to square-loss. Milder decay\nassumptions also lead to improved algorithms. This is the first purely\ndistributional assumption that leads to polynomial-time algorithms for networks\nof ReLUs, even with one hidden layer. Further, unlike prior distributional\nassumptions (e.g., the marginal distribution is Gaussian), eigenvalue decay has\nbeen observed in practice on common data sets. \n\n"}
{"id": "1708.05976", "contents": "Title: On the construction of small subsets containing special elements in a\n  finite field Abstract: In this note we construct a series of small subsets containing a non-d-th\npower element in a finite field by applying certain bounds on incomplete\ncharacter sums.\n  Precisely, let $h=\\lfloor q^{\\delta}\\rfloor>1$ and $d\\mid q^h-1$. Let $r$ be\na prime divisor of $q-1$ such that the largest prime power part of $q-1$ has\nthe form $r^s$. Then there is a constant $0<\\epsilon<1$ such that for a ratio\nat least $ {q^{-\\epsilon h}}$ of $\\alpha\\in \\mathbb{F}_{q^{h}}\n\\backslash\\mathbb{F}_{q}$, the set $S=\\{ \\alpha-x^t, x\\in\\mathbb{F}_{q}\\}$ of\ncardinality $1+\\frac {q-1} {M(h)}$ contains a non-d-th power in\n$\\mathbb{F}_{q^{\\lfloor q^\\delta\\rfloor}}$, where $t$ is the largest power of\n$r$ such that $t<\\sqrt{q}/h$ and $M(h)$ is defined as $$M(h)=\\max_{r \\mid\n(q-1)} r^{\\min\\{v_r(q-1), \\lfloor\\log_r{q}/2-\\log_r h\\rfloor\\}}.$$ Here $r$\nruns thourgh prime divisors and $v_r(x)$ is the $r$-adic oder of $x$. For odd\n$q$, the choice of $\\delta=\\frac 12-d, d=o(1)>0$ shows that there exists an\nexplicit subset of cardinality $q^{1-d}=O(\\log^{2+\\epsilon'}(q^h))$ containing\na non-quadratic element in the field $\\mathbb{F}_{q^h}$. On the other hand, the\nchoice of $h=2$ shows that for any odd prime power $q$, there is an explicit\nsubset of cardinality $1+\\frac {q-1}{M(2)}$ containing a non-quadratic element\nin $\\mathbb{F}_{q^2}$. This improves a $q-1$ construction by Coulter and Kosick\n\\cite{CK} since $\\lfloor \\log_2{(q-1)}\\rfloor\\leq M(2) < \\sqrt{q}$.\n  In addition, we obtain a similar construction for small sets containing a\nprimitive element. The construction works well provided $\\phi(q^h-1)$ is very\nsmall, where $\\phi$ is the Euler's totient function. \n\n"}
{"id": "1708.06730", "contents": "Title: Upward Partitioned Book Embeddings Abstract: We analyze a directed variation of the book embedding problem when the page\npartition is prespecified and the nodes on the spine must be in topological\norder (upward book embedding). Given a directed acyclic graph and a partition\nof its edges into $k$ pages, can we linearly order the vertices such that the\ndrawing is upward (a topological sort) and each page avoids crossings? We prove\nthat the problem is NP-complete for $k\\ge 3$, and for $k\\ge 4$ even in the\nspecial case when each page is a matching. By contrast, the problem can be\nsolved in linear time for $k=2$ pages when pages are restricted to matchings.\nThe problem comes from Jack Edmonds (1997), motivated as a generalization of\nthe map folding problem from computational origami. \n\n"}
{"id": "1708.06783", "contents": "Title: Recovering Nonuniform Planted Partitions via Iterated Projection Abstract: In the planted partition problem, the $n$ vertices of a random graph are\npartitioned into $k$ \"clusters,\" and edges between vertices in the same cluster\nand different clusters are included with constant probability $p$ and $q$,\nrespectively (where $0 \\le q < p \\le 1$). We give an efficient spectral\nalgorithm that recovers the clusters with high probability, provided that the\nsizes of any two clusters are either very close or separated by $\\geq\n\\Omega(\\sqrt n)$. We also discuss a generalization of planted partition in\nwhich the algorithm's input is not a random graph, but a random real symmetric\nmatrix with independent above-diagonal entries.\n  Our algorithm is an adaptation of a previous algorithm for the uniform case,\ni.e., when all clusters are size $n / k \\geq \\Omega(\\sqrt n)$. The original\nalgorithm recovers the clusters one by one via iterated projection: it\nconstructs the orthogonal projection operator onto the dominant $k$-dimensional\neigenspace of the random graph's adjacency matrix, uses it to recover one of\nthe clusters, then deletes it and recurses on the remaining vertices. We show\nherein that a similar algorithm works in the nonuniform case. \n\n"}
{"id": "1708.07829", "contents": "Title: Algorithms for Big Data: Graphs and PageRank Abstract: This work consists of a study of a set of techniques and strategies related\nwith algorithm's design, whose purpose is the resolution of problems on massive\ndata sets, in an efficient way. This field is known as Algorithms for Big Data.\nIn particular, this work has studied the Streaming Algorithms, which represents\nthe basis of the data structures of sublinear order $o(n)$ in space, known as\nSketches. In addition, it has deepened in the study of problems applied to\nGraphs on the Semi-Streaming model. Next, the PageRank algorithm was analyzed\nas a concrete case study. Finally, the development of a library for the\nresolution of graph problems, implemented on the top of the intensive\nmathematical computation platform known as TensorFlow has been started. \n\n"}
{"id": "1708.09107", "contents": "Title: Planar L-Drawings of Directed Graphs Abstract: We study planar drawings of directed graphs in the L-drawing standard. We\nprovide necessary conditions for the existence of these drawings and show that\ntesting for the existence of a planar L-drawing is an NP-complete problem.\nMotivated by this result, we focus on upward-planar L-drawings. We show that\ndirected st-graphs admitting an upward- (resp. upward-rightward-) planar\nL-drawing are exactly those admitting a bitonic (resp. monotonically\nincreasing) st-ordering. We give a linear-time algorithm that computes a\nbitonic (resp. monotonically increasing) st-ordering of a planar st-graph or\nreports that there exists none. \n\n"}
{"id": "1708.09398", "contents": "Title: Designing Strassen's algorithm Abstract: In 1969, Strassen shocked the world by showing that two n x n matrices could\nbe multiplied in time asymptotically less than $O(n^3)$. While the recursive\nconstruction in his algorithm is very clear, the key gain was made by showing\nthat 2 x 2 matrix multiplication could be performed with only 7 multiplications\ninstead of 8. The latter construction was arrived at by a process of\nelimination and appears to come out of thin air. Here, we give the simplest and\nmost transparent proof of Strassen's algorithm that we are aware of, using only\na simple unitary 2-design and a few easy lines of calculation. Moreover, using\nbasic facts from the representation theory of finite groups, we use 2-designs\ncoming from group orbits to generalize our construction to all n (although the\nresulting algorithms aren't optimal for n at least 3). \n\n"}
{"id": "1709.00869", "contents": "Title: Estimating graph parameters with random walks Abstract: An algorithm observes the trajectories of random walks over an unknown graph\n$G$, starting from the same vertex $x$, as well as the degrees along the\ntrajectories. For all finite connected graphs, one can estimate the number of\nedges $m$ up to a bounded factor in\n$O\\left(t_{\\mathrm{rel}}^{3/4}\\sqrt{m/d}\\right)$ steps, where\n$t_{\\mathrm{rel}}$ is the relaxation time of the lazy random walk on $G$ and\n$d$ is the minimum degree in $G$. Alternatively, $m$ can be estimated in\n$O\\left(t_{\\mathrm{unif}} +t_{\\mathrm{rel}}^{5/6}\\sqrt{n}\\right)$, where $n$ is\nthe number of vertices and $t_{\\mathrm{unif}}$ is the uniform mixing time on\n$G$. The number of vertices $n$ can then be estimated up to a bounded factor in\nan additional $O\\left(t_{\\mathrm{unif}}\\frac{m}{n}\\right)$ steps. Our\nalgorithms are based on counting the number of intersections of random walk\npaths $X,Y$, i.e. the number of pairs $(t,s)$ such that $X_t=Y_s$. This\nimproves on previous estimates which only consider collisions (i.e., times $t$\nwith $X_t=Y_t$). We also show that the complexity of our algorithms is optimal,\neven when restricting to graphs with a prescribed relaxation time. Finally, we\nshow that, given either $m$ or the mixing time of $G$, we can compute the\n\"other parameter\" with a self-stopping algorithm. \n\n"}
{"id": "1709.01670", "contents": "Title: Parameterized complexity of machine scheduling: 15 open problems Abstract: Machine scheduling problems are a long-time key domain of algorithms and\ncomplexity research. A novel approach to machine scheduling problems are\nfixed-parameter algorithms. To stimulate this thriving research direction, we\npropose 15 open questions in this area whose resolution we expect to lead to\nthe discovery of new approaches and techniques both in scheduling and\nparameterized complexity theory. \n\n"}
{"id": "1709.02674", "contents": "Title: Uniform generation of random graphs with power-law degree sequences Abstract: We give a linear-time algorithm that approximately uniformly generates a\nrandom simple graph with a power-law degree sequence whose exponent is at least\n2.8811. While sampling graphs with power-law degree sequence of exponent at\nleast 3 is fairly easy, and many samplers work efficiently in this case, the\nproblem becomes dramatically more difficult when the exponent drops below 3;\nours is the first provably practicable sampler for this case. We also show that\nwith an appropriate rejection scheme, our algorithm can be tuned into an exact\nuniform sampler. The running time of the exact sampler is O(n^{2.107}) with\nhigh probability, and O(n^{4.081}) in expectation. \n\n"}
{"id": "1709.02837", "contents": "Title: Obstructions to a small hyperbolicity in Helly graphs Abstract: It is known that for every graph $G$ there exists the smallest Helly graph\n$\\cal H(G)$ into which $G$ isometrically embeds ($\\cal H(G)$ is called the\ninjective hull of $G$) such that the hyperbolicity of $\\cal H(G)$ is equal to\nthe hyperbolicity of $G$. Motivated by this, we investigate structural\nproperties of Helly graphs that govern their hyperbolicity and identify three\nisometric subgraphs of the King-grid as structural obstructions to a small\nhyperbolicity in Helly graphs. \n\n"}
{"id": "1709.03294", "contents": "Title: Root Separation for Trinomials Abstract: We give a separation bound for the complex roots of a trinomial $f \\in\n\\mathbb{Z}[X]$. The logarithm of the inverse of our separation bound is\npolynomial in the size of the sparse encoding of $f$; in particular, it is\npolynomial in $\\log (\\deg f)$. It is known that no such bound is possible for\n4-nomials (polynomials with 4 monomials). For trinomials, the classical results\n(which are based on the degree of $f$ rather than the number of monomials) give\nseparation bounds that are exponentially worse.As an algorithmic application,\nwe show that the number of real roots of a trinomial $f$ can be computed in\ntime polynomial in the size of the sparse encoding of~$f$. The same problem is\nopen for 4-nomials. \n\n"}
{"id": "1709.06113", "contents": "Title: Crossing Patterns in Nonplanar Road Networks Abstract: We define the crossing graph of a given embedded graph (such as a road\nnetwork) to be a graph with a vertex for each edge of the embedding, with two\ncrossing graph vertices adjacent when the corresponding two edges of the\nembedding cross each other. In this paper, we study the sparsity properties of\ncrossing graphs of real-world road networks. We show that, in large road\nnetworks (the Urban Road Network Dataset), the crossing graphs have connected\ncomponents that are primarily trees, and that the remaining non-tree components\nare typically sparse (technically, that they have bounded degeneracy). We prove\ntheoretically that when an embedded graph has a sparse crossing graph, it has\nother desirable properties that lead to fast algorithms for shortest paths and\nother algorithms important in geographic information systems. Notably, these\ngraphs have polynomial expansion, meaning that they and all their subgraphs\nhave small separators. \n\n"}
{"id": "1709.07822", "contents": "Title: Planar Graph Perfect Matching is in NC Abstract: Is perfect matching in NC? That is, is there a deterministic fast parallel\nalgorithm for it? This has been an outstanding open question in theoretical\ncomputer science for over three decades, ever since the discovery of RNC\nmatching algorithms. Within this question, the case of planar graphs has\nremained an enigma: On the one hand, counting the number of perfect matchings\nis far harder than finding one (the former is #P-complete and the latter is in\nP), and on the other, for planar graphs, counting has long been known to be in\nNC whereas finding one has resisted a solution.\n  In this paper, we give an NC algorithm for finding a perfect matching in a\nplanar graph. Our algorithm uses the above-stated fact about counting matchings\nin a crucial way. Our main new idea is an NC algorithm for finding a face of\nthe perfect matching polytope at which $\\Omega(n)$ new conditions, involving\nconstraints of the polytope, are simultaneously satisfied. Several other ideas\nare also needed, such as finding a point in the interior of the minimum weight\nface of this polytope and finding a balanced tight odd set in NC. \n\n"}
{"id": "1709.09307", "contents": "Title: On the construction of converging hierarchies for polynomial\n  optimization based on certificates of global positivity Abstract: In recent years, techniques based on convex optimization and real algebra\nthat produce converging hierarchies of lower bounds for polynomial minimization\nproblems have gained much popularity. At their heart, these hierarchies rely\ncrucially on Positivstellens\\\"atze from the late 20th century (e.g., due to\nStengle, Putinar, or Schm\\\"udgen) that certify positivity of a polynomial on an\narbitrary closed basic semialgebraic set. In this paper, we show that such\nhierarchies could in fact be designed from much more limited\nPositivstellens\\\"atze dating back to the early 20th century that only certify\npositivity of a polynomial globally. More precisely, we show that any inner\napproximation to the cone of positive homogeneous polynomials that is\narbitrarily tight can be turned into a converging hierarchy of lower bounds for\ngeneral polynomial minimization problems with compact feasible sets. This in\nparticular leads to a semidefinite programming-based hierarchy that relies\nsolely on Artin's solution to Hilbert's 17th problem. We also use a classical\nresult of Poly\\'a on global positivity of even forms to construct an\n\"optimization-free\" converging hierarchy for general polynomial minimization\nproblems. This hierarchy only requires polynomial multiplication and checking\nnonnegativity of coefficients of certain fixed polynomials. As a corollary, we\nobtain new linear programming and second-order cone programming-based\nhierarchies for polynomial minimization problems that rely on the recently\nintroduced concepts of dsos and sdsos polynomials. We remark that the scope of\nthis paper is theoretical at this stage as our hierarchies-though they involve\nat most two sum of squares constraints or only basic arithmetic at each\nlevel-require the use of bisection and increase the number of variables (resp.\ndegree) of the problem by the number of inequality constraints plus three\n(resp. by a factor of two). \n\n"}
{"id": "1709.09778", "contents": "Title: Sampling Without Compromising Accuracy in Adaptive Data Analysis Abstract: In this work, we study how to use sampling to speed up mechanisms for\nanswering adaptive queries into datasets without reducing the accuracy of those\nmechanisms. This is important to do when both the datasets and the number of\nqueries asked are very large. In particular, we describe a mechanism that\nprovides a polynomial speed-up per query over previous mechanisms, without\nneeding to increase the total amount of data required to maintain the same\ngeneralization error as before. We prove that this speed-up holds for arbitrary\nstatistical queries. We also provide an even faster method for achieving\nstatistically-meaningful responses wherein the mechanism is only allowed to see\na constant number of samples from the data per query. Finally, we show that our\ngeneral results yield a simple, fast, and unified approach for adaptively\noptimizing convex and strongly convex functions over a dataset. \n\n"}
{"id": "1709.10258", "contents": "Title: An improved algorithm for recognizing matroids Abstract: Let $M$ be a matroid defined on a finite set $E$ and $L\\subset E$. $L$ is\nlocked in $M$ if $M|L$ and $M^*|(E\\backslash L)$ are 2-connected, and\n$min\\{r(L), r^*(E\\backslash L)\\} \\geq 2$. Locked subsets characterize\nnontrivial facets of the bases polytope. In this paper, we give a new axiom\nsystem for matroids based on locked subsets. We deduce an algorithm for\nrecognizing matroids improving the running time complexity of the best known\ntill today. This algorithm induces a polynomial time algorithm for recognizing\nuniform matroids. This latter problem is intractable if we use an independence\noracle. \n\n"}
{"id": "1709.10307", "contents": "Title: Non-approximability and Polylogarithmic Approximations of the\n  Single-Sink Unsplittable and Confluent Dynamic Flow Problems Abstract: Dynamic Flows were introduced by Ford and Fulkerson in 1958 to model flows\nover time. They define edge capacities to be the total amount of flow that can\nenter an edge {\\em in one time unit}. Each edge also has a length, representing\nthe time needed to traverse it. Dynamic Flows have been used to model many\nproblems including traffic congestion, hop-routing of packets and evacuation\nprotocols in buildings. While the basic problem of moving the maximal amount of\nsupplies from sources to sinks is polynomial time solvable, natural minor\nmodifications can make it NP-hard.\n  One such modification is that flows be confluent, i.e., all flows leaving a\nvertex must leave along the same edge. This corresponds to natural conditions\nin, e.g., evacuation planning and hop routing.\n  We investigate the single-sink Confluent Quickest Flow problem. The input is\na graph with edge capacities and lengths, sources with supplies and a sink. The\nproblem is to find a confluent flow minimizing the time required to send\nsupplies to the sink. Our main results include: a) Logarithmic\nNon-Approximability: Directed Confluent Quickest Flows cannot be approximated\nin polynomial time with an $O(\\log n)$ approximation factor, unless $P=NP$.\n  b) Polylogarithmic Bicriteria Approximations: Polynomial time $(O(\\log^8 n),\nO(\\log^2 \\kappa))$ bicritera approximation algorithms for the Confluent\nQuickest Flow problem where $\\kappa$ is the number of sinks, in both directed\nand undirected graphs.\n  Corresponding results are also developed for the Confluent Maximum Flow over\ntimeproblem. The techniques developed also improve recent approximation\nalgorithms for static confluent flows. \n\n"}
{"id": "1710.00904", "contents": "Title: Online and Distributed Robust Regressions under Adversarial Data\n  Corruption Abstract: In today's era of big data, robust least-squares regression becomes a more\nchallenging problem when considering the adversarial corruption along with\nexplosive growth of datasets. Traditional robust methods can handle the noise\nbut suffer from several challenges when applied in huge dataset including 1)\ncomputational infeasibility of handling an entire dataset at once, 2) existence\nof heterogeneously distributed corruption, and 3) difficulty in corruption\nestimation when data cannot be entirely loaded. This paper proposes online and\ndistributed robust regression approaches, both of which can concurrently\naddress all the above challenges. Specifically, the distributed algorithm\noptimizes the regression coefficients of each data block via heuristic hard\nthresholding and combines all the estimates in a distributed robust\nconsolidation. Furthermore, an online version of the distributed algorithm is\nproposed to incrementally update the existing estimates with new incoming data.\nWe also prove that our algorithms benefit from strong robustness guarantees in\nterms of regression coefficient recovery with a constant upper bound on the\nerror of state-of-the-art batch methods. Extensive experiments on synthetic and\nreal datasets demonstrate that our approaches are superior to those of existing\nmethods in effectiveness, with competitive efficiency. \n\n"}
{"id": "1710.01969", "contents": "Title: Simultaneous Multiparty Communication Complexity of Composed Functions Abstract: In the Number On the Forehead (NOF) multiparty communication model, $k$\nplayers want to evaluate a function $F : X_1 \\times\\cdots\\times X_k\\rightarrow\nY$ on some input $(x_1,\\dots,x_k)$ by broadcasting bits according to a\npredetermined protocol. The input is distributed in such a way that each player\n$i$ sees all of it except $x_i$. In the simultaneous setting, the players\ncannot speak to each other but instead send information to a referee. The\nreferee does not know the players' input, and cannot give any information back.\nAt the end, the referee must be able to recover $F(x_1,\\dots,x_k)$ from what\nshe obtained.\n  A central open question, called the $\\log n$ barrier, is to find a function\nwhich is hard to compute for $polylog(n)$ or more players (where the $x_i$'s\nhave size $poly(n)$) in the simultaneous NOF model. This has important\napplications in circuit complexity, as it could help to separate $ACC^0$ from\nother complexity classes. One of the candidates belongs to the family of\ncomposed functions. The input to these functions is represented by a $k\\times\n(t\\cdot n)$ boolean matrix $M$, whose row $i$ is the input $x_i$ and $t$ is a\nblock-width parameter. A symmetric composed function acting on $M$ is specified\nby two symmetric $n$- and $kt$-variate functions $f$ and $g$, that output\n$f\\circ g(M)=f(g(B_1),\\dots,g(B_n))$ where $B_j$ is the $j$-th block of width\n$t$ of $M$. As the majority function $MAJ$ is conjectured to be outside of\n$ACC^0$, Babai et. al. suggested to study $MAJ\\circ MAJ_t$, with $t$ large\nenough.\n  So far, it was only known that $t=1$ is not enough for $MAJ\\circ MAJ_t$ to\nbreak the $\\log n$ barrier in the simultaneous deterministic NOF model. In this\npaper, we extend this result to any constant block-width $t>1$, by giving a\nprotocol of cost $2^{O(2^t)}\\log^{2^{t+1}}(n)$ for any symmetric composed\nfunction when there are $2^{\\Omega(2^t)}\\log n$ players. \n\n"}
{"id": "1710.02587", "contents": "Title: The Paulsen Problem, Continuous Operator Scaling, and Smoothed Analysis Abstract: The Paulsen problem is a basic open problem in operator theory: Given vectors\n$u_1, \\ldots, u_n \\in \\mathbb R^d$ that are $\\epsilon$-nearly satisfying the\nParseval's condition and the equal norm condition, is it close to a set of\nvectors $v_1, \\ldots, v_n \\in \\mathbb R^d$ that exactly satisfy the Parseval's\ncondition and the equal norm condition? Given $u_1, \\ldots, u_n$, the squared\ndistance (to the set of exact solutions) is defined as $\\inf_{v} \\sum_{i=1}^n\n\\| u_i - v_i \\|_2^2$ where the infimum is over the set of exact solutions.\nPrevious results show that the squared distance of any $\\epsilon$-nearly\nsolution is at most $O({\\rm{poly}}(d,n,\\epsilon))$ and there are\n$\\epsilon$-nearly solutions with squared distance at least $\\Omega(d\\epsilon)$.\nThe fundamental open question is whether the squared distance can be\nindependent of the number of vectors $n$.\n  We answer this question affirmatively by proving that the squared distance of\nany $\\epsilon$-nearly solution is $O(d^{13/2} \\epsilon)$. Our approach is based\non a continuous version of the operator scaling algorithm and consists of two\nparts. First, we define a dynamical system based on operator scaling and use it\nto prove that the squared distance of any $\\epsilon$-nearly solution is $O(d^2\nn \\epsilon)$. Then, we show that by randomly perturbing the input vectors, the\ndynamical system will converge faster and the squared distance of an\n$\\epsilon$-nearly solution is $O(d^{5/2} \\epsilon)$ when $n$ is large enough\nand $\\epsilon$ is small enough. To analyze the convergence of the dynamical\nsystem, we develop some new techniques in lower bounding the operator capacity,\na concept introduced by Gurvits to analyze the operator scaling algorithm. \n\n"}
{"id": "1710.03091", "contents": "Title: On Variants of Network Flow Stability Abstract: We consider a general stable flow problem in a directed and capacitated\nnetwork, where each vertex has a strict preference list over the incoming and\noutgoing edges. A flow is stable if no group of vertices forming a path can\nmutually benefit by rerouting the flow. Motivated by applications in supply\nchain networks, we generalize the traditional Kirchhoff's law, requiring the\noutflow is equal to the inflow at every nonterminal node, to a monotone\npiecewise linear relationship between the inflows and the outflows. We show the\nexistence of a stable flow using Scarf's Lemma, and provide a polynomial time\nalgorithm to find such a stable flow. We further show that finding a minimum\ncost generalized stable network is NP-hard, while the problem is polynomial\ntime solvable for the traditional stable flow satisfying Kirchhoff's law. \n\n"}
{"id": "1710.06025", "contents": "Title: Quantum query complexity of entropy estimation Abstract: Estimation of Shannon and R\\'enyi entropies of unknown discrete distributions\nis a fundamental problem in statistical property testing and an active research\ntopic in both theoretical computer science and information theory. Tight bounds\non the number of samples to estimate these entropies have been established in\nthe classical setting, while little is known about their quantum counterparts.\nIn this paper, we give the first quantum algorithms for estimating\n$\\alpha$-R\\'enyi entropies (Shannon entropy being 1-Renyi entropy). In\nparticular, we demonstrate a quadratic quantum speedup for Shannon entropy\nestimation and a generic quantum speedup for $\\alpha$-R\\'enyi entropy\nestimation for all $\\alpha\\geq 0$, including a tight bound for the\ncollision-entropy (2-R\\'enyi entropy). We also provide quantum upper bounds for\nextreme cases such as the Hartley entropy (i.e., the logarithm of the support\nsize of a distribution, corresponding to $\\alpha=0$) and the min-entropy case\n(i.e., $\\alpha=+\\infty$), as well as the Kullback-Leibler divergence between\ntwo distributions. Moreover, we complement our results with quantum lower\nbounds on $\\alpha$-R\\'enyi entropy estimation for all $\\alpha\\geq 0$. \n\n"}
{"id": "1710.06261", "contents": "Title: Convergence Rate of Riemannian Hamiltonian Monte Carlo and Faster\n  Polytope Volume Computation Abstract: We give the first rigorous proof of the convergence of Riemannian Hamiltonian\nMonte Carlo, a general (and practical) method for sampling Gibbs distributions.\nOur analysis shows that the rate of convergence is bounded in terms of natural\nsmoothness parameters of an associated Riemannian manifold. We then apply the\nmethod with the manifold defined by the log barrier function to the problems of\n(1) uniformly sampling a polytope and (2) computing its volume, the latter by\nextending Gaussian cooling to the manifold setting. In both cases, the total\nnumber of steps needed is O^{*}(mn^{\\frac{2}{3}}), improving the state of the\nart. A key ingredient of our analysis is a proof of an analog of the KLS\nconjecture for Gibbs distributions over manifolds. \n\n"}
{"id": "1710.07774", "contents": "Title: A Unified PTAS for Prize Collecting TSP and Steiner Tree Problem in\n  Doubling Metrics Abstract: We present a unified polynomial-time approximation scheme (PTAS) for the\nprize collecting traveling salesman problem (PCTSP) and the prize collecting\nSteiner tree problem (PCSTP) in doubling metrics. Given a metric space and a\npenalty function on a subset of points known as terminals, a solution is a\nsubgraph on points in the metric space, whose cost is the weight of its edges\nplus the penalty due to terminals not covered by the subgraph. Under our\nunified framework, the solution subgraph needs to be Eulerian for PCTSP, while\nit needs to be connected for PCSTP. Before our work, even a QPTAS for the\nproblems in doubling metrics is not known.\n  Our unified PTAS is based on the previous dynamic programming frameworks\nproposed in [Talwar STOC 2004] and [Bartal, Gottlieb, Krauthgamer STOC 2012].\nHowever, since it is unknown which part of the optimal cost is due to edge\nlengths and which part is due to penalties of uncovered terminals, we need to\ndevelop new techniques to apply previous divide-and-conquer strategies and\nsparse instance decompositions. \n\n"}
{"id": "1710.08436", "contents": "Title: HyperMinHash: MinHash in LogLog space Abstract: In this extended abstract, we describe and analyze a lossy compression of\nMinHash from buckets of size $O(\\log n)$ to buckets of size $O(\\log\\log n)$ by\nencoding using floating-point notation. This new compressed sketch, which we\ncall HyperMinHash, as we build off a HyperLogLog scaffold, can be used as a\ndrop-in replacement of MinHash. Unlike comparable Jaccard index fingerprinting\nalgorithms in sub-logarithmic space (such as b-bit MinHash), HyperMinHash\nretains MinHash's features of streaming updates, unions, and cardinality\nestimation. For a multiplicative approximation error $1+ \\epsilon$ on a Jaccard\nindex $ t $, given a random oracle, HyperMinHash needs $O\\left(\\epsilon^{-2}\n\\left( \\log\\log n + \\log \\frac{1}{ t \\epsilon} \\right)\\right)$ space.\nHyperMinHash allows estimating Jaccard indices of 0.01 for set cardinalities on\nthe order of $10^{19}$ with relative error of around 10\\% using 64KiB of\nmemory; MinHash can only estimate Jaccard indices for cardinalities of\n$10^{10}$ with the same memory consumption. \n\n"}
{"id": "1710.09595", "contents": "Title: Quantum versus Classical Online Streaming Algorithms with Logarithmic\n  Size of Memory Abstract: We consider online algorithms with respect to the competitive ratio. Here, we\ninvestigate quantum and classical one-way automata with non-constant size of\nmemory (streaming algorithms) as a model for online algorithms. We construct\nproblems that can be solved by quantum online streaming algorithms better than\nby classical ones in a case of logarithmic or sublogarithmic size of memory. \n\n"}
{"id": "1710.09780", "contents": "Title: Interactions of Computational Complexity Theory and Mathematics Abstract: $ $[This paper is a (self contained) chapter in a new book, Mathematics and\nComputation, whose draft is available on my homepage at\nhttps://www.math.ias.edu/avi/book ].\n  We survey some concrete interaction areas between computational complexity\ntheory and different fields of mathematics. We hope to demonstrate here that\nhardly any area of modern mathematics is untouched by the computational\nconnection (which in some cases is completely natural and in others may seem\nquite surprising). In my view, the breadth, depth, beauty and novelty of these\nconnections is inspiring, and speaks to a great potential of future\ninteractions (which indeed, are quickly expanding). We aim for variety. We give\nshort, simple descriptions (without proofs or much technical detail) of ideas,\nmotivations, results and connections; this will hopefully entice the reader to\ndig deeper. Each vignette focuses only on a single topic within a large\nmathematical filed. We cover the following:\n  $\\bullet$ Number Theory: Primality testing\n  $\\bullet$ Combinatorial Geometry: Point-line incidences\n  $\\bullet$ Operator Theory: The Kadison-Singer problem\n  $\\bullet$ Metric Geometry: Distortion of embeddings\n  $\\bullet$ Group Theory: Generation and random generation\n  $\\bullet$ Statistical Physics: Monte-Carlo Markov chains\n  $\\bullet$ Analysis and Probability: Noise stability\n  $\\bullet$ Lattice Theory: Short vectors\n  $\\bullet$ Invariant Theory: Actions on matrix tuples \n\n"}
{"id": "1710.10026", "contents": "Title: A note on faithful coupling of Markov chains Abstract: One often needs to turn a coupling $(X_i, Y_i)_{i\\geq 0}$ of a Markov chain\ninto a sticky coupling where once $X_T = Y_T$ at some $T$, then from then on,\nat each subsequent time step $T'\\geq T$, we shall have $X_{T'} = Y_{T'}$.\nHowever, not all of what are considered couplings in literature, even Markovian\ncouplings, can be turned into sticky couplings, as proved by Rosenthal through\na counter example. Rosenthal then proposed a strengthening of the Markovian\ncoupling notion, termed as faithful coupling, from which a sticky coupling can\nindeed be obtained. We identify the reason why a sticky coupling could not be\nobtained in the counter example of Rosenthal, which motivates us to define a\ntype of coupling which can obviously be turned into a sticky coupling. We show\nthen that the new type of coupling that we define, and the faithful coupling as\ndefined by Rosenthal, are actually identical. Our note may be seen as a\ndemonstration of the naturalness of the notion of faithful coupling. \n\n"}
{"id": "1710.10322", "contents": "Title: Maximally Recoverable LRCs: A field size lower bound and constructions\n  for few heavy parities Abstract: The explosion in the volumes of data being stored online has resulted in\ndistributed storage systems transitioning to erasure coding based schemes.\nLocal Reconstruction Codes (LRCs) have emerged as the codes of choice for these\napplications. These codes can correct a small number of erasures by accessing\nonly a small number of remaining coordinates. An $(n,r,h,a,q)$-LRC is a linear\ncode over $\\mathbb{F}_q$ of length $n$, whose codeword symbols are partitioned\ninto $g=n/r$ local groups each of size $r$. It has $h$ global parity checks and\neach local group has $a$ local parity checks. Such an LRC is Maximally\nRecoverable (MR), if it corrects all erasure patterns which are\ninformation-theoretically correctable under the stipulated structure of local\nand global parity checks.\n  We show the first non-trivial lower bounds on the field size required for MR\nLRCs. When $a,h$ are constant and the number of local groups $g \\ge h$, while\n$r$ may grow with $n$, our lower bound simplifies to $q\\ge\n\\Omega_{a,h}\\left(n\\cdot r^{\\min\\{a,h-2\\}}\\right).$ No superlinear (in $n$)\nlower bounds were known prior to this work for any setting of parameters.\n  MR LRCs deployed in practice have a small number of global parities,\ntypically $h=2,3$. We complement our lower bounds by giving constructions with\nsmall field size for $h\\le 3$. For $h=2$, we give a linear field size\nconstruction. We also show a surprising application of elliptic curves and\narithmetic progression free sets in the construction of MR LRCs. \n\n"}
{"id": "1710.10655", "contents": "Title: If it ain't broke, don't fix it: Sparse metric repair Abstract: Many modern data-intensive computational problems either require, or benefit\nfrom distance or similarity data that adhere to a metric. The algorithms run\nfaster or have better performance guarantees. Unfortunately, in real\napplications, the data are messy and values are noisy. The distances between\nthe data points are far from satisfying a metric. Indeed, there are a number of\ndifferent algorithms for finding the closest set of distances to the given ones\nthat also satisfy a metric (sometimes with the extra condition of being\nEuclidean). These algorithms can have unintended consequences, they can change\na large number of the original data points, and alter many other features of\nthe data.\n  The goal of sparse metric repair is to make as few changes as possible to the\noriginal data set or underlying distances so as to ensure the resulting\ndistances satisfy the properties of a metric. In other words, we seek to\nminimize the sparsity (or the $\\ell_0$ \"norm\") of the changes we make to the\ndistances subject to the new distances satisfying a metric. We give three\ndifferent combinatorial algorithms to repair a metric sparsely. In one setting\nthe algorithm is guaranteed to return the sparsest solution and in the other\nsettings, the algorithms repair the metric. Without prior information, the\nalgorithms run in time proportional to the cube of the number of input data\npoints and, with prior information we can reduce the running time considerably. \n\n"}
{"id": "1710.11278", "contents": "Title: Approximating Continuous Functions by ReLU Nets of Minimal Width Abstract: This article concerns the expressive power of depth in deep feed-forward\nneural nets with ReLU activations. Specifically, we answer the following\nquestion: for a fixed $d_{in}\\geq 1,$ what is the minimal width $w$ so that\nneural nets with ReLU activations, input dimension $d_{in}$, hidden layer\nwidths at most $w,$ and arbitrary depth can approximate any continuous,\nreal-valued function of $d_{in}$ variables arbitrarily well? It turns out that\nthis minimal width is exactly equal to $d_{in}+1.$ That is, if all the hidden\nlayer widths are bounded by $d_{in}$, then even in the infinite depth limit,\nReLU nets can only express a very limited class of functions, and, on the other\nhand, any continuous function on the $d_{in}$-dimensional unit cube can be\napproximated to arbitrary precision by ReLU nets in which all hidden layers\nhave width exactly $d_{in}+1.$ Our construction in fact shows that any\ncontinuous function $f:[0,1]^{d_{in}}\\to\\mathbb R^{d_{out}}$ can be\napproximated by a net of width $d_{in}+d_{out}$. We obtain quantitative depth\nestimates for such an approximation in terms of the modulus of continuity of\n$f$. \n\n"}
{"id": "1710.11513", "contents": "Title: Designing RNA Secondary Structures is Hard Abstract: An RNA sequence is a word over an alphabet on four elements $\\{A,C,G,U\\}$\ncalled bases. RNA sequences fold into secondary structures where some bases\nmatch one another while others remain unpaired. Pseudoknot-free secondary\nstructures can be represented as well-parenthesized expressions with additional\ndots, where pairs of matching parentheses symbolize paired bases and dots,\nunpaired bases. The two fundamental problems in RNA algorithmic are to predict\nhow sequences fold within some model of energy and to design sequences of bases\nwhich will fold into targeted secondary structures. Predicting how a given RNA\nsequence folds into a pseudoknot-free secondary structure is known to be\nsolvable in cubic time since the eighties and in truly subcubic time by a\nrecent result of Bringmann et al. (FOCS 2016). As a stark contrast, it is\nunknown whether or not designing a given RNA secondary structure is a tractable\ntask; this has been raised as a challenging open question by Anne Condon (ICALP\n2003). Because of its crucial importance in a number of fields such as\npharmaceutical research and biochemistry, there are dozens of heuristics and\nsoftware libraries dedicated to RNA secondary structure design. It is therefore\nrather surprising that the computational complexity of this central problem in\nbioinformatics has been unsettled for decades.\n  In this paper we show that, in the simplest model of energy which is the\nWatson-Crick model the design of secondary structures is NP-complete if one\nadds natural constraints of the form: index $i$ of the sequence has to be\nlabeled by base $b$. This negative result suggests that the same lower bound\nholds for more realistic models of energy. It is noteworthy that the additional\nconstraints are by no means artificial: they are provided by all the RNA design\npieces of software and they do correspond to the actual practice. \n\n"}
{"id": "1711.00565", "contents": "Title: Typically-Correct Derandomization for Small Time and Space Abstract: Suppose a language $L$ can be decided by a bounded-error randomized algorithm\nthat runs in space $S$ and time $n \\cdot \\text{poly}(S)$. We give a randomized\nalgorithm for $L$ that still runs in space $O(S)$ and time $n \\cdot\n\\text{poly}(S)$ that uses only $O(S)$ random bits; our algorithm has a low\nfailure probability on all but a negligible fraction of inputs of each length.\nAn immediate corollary is a deterministic algorithm for $L$ that runs in space\n$O(S)$ and succeeds on all but a negligible fraction of inputs of each length.\nWe also give several other complexity-theoretic applications of our technique. \n\n"}
{"id": "1711.01623", "contents": "Title: Fooling Views: A New Lower Bound Technique for Distributed Computations\n  under Congestion Abstract: We introduce a novel lower bound technique for distributed graph algorithms\nunder bandwidth limitations.\n  We define the notion of \\emph{fooling views} and exemplify its strength by\nproving two new lower bounds for triangle membership in the CONGEST(B) model:\n  (i) Any $1$-round algorithm requires $B\\geq c\\Delta \\log n$ for a constant\n$c>0$.\n  (ii) If $B=1$, even in constant-degree graphs any algorithm must take\n$\\Omega(\\log^* n)$ rounds.\n  The implication of the former is the first proven separation between the\nLOCAL and the CONGEST models for deterministic triangle membership.\n  The latter result is the first non-trivial lower bound on the number of\nrounds required, even for \\emph{triangle detection}, under limited bandwidth.\n  All previous known techniques are provably incapable of giving these bounds.\n  We hope that our approach may pave the way for proving lower bounds for\nadditional problems in various settings of distributed computing for which\nprevious techniques do not suffice. \n\n"}
{"id": "1711.02194", "contents": "Title: On Derandomizing Local Distributed Algorithms Abstract: The gap between the known randomized and deterministic local distributed\nalgorithms underlies arguably the most fundamental and central open question in\ndistributed graph algorithms. In this paper, we develop a generic and clean\nrecipe for derandomizing LOCAL algorithms. We also exhibit how this simple\nrecipe leads to significant improvements on a number of problem. Two main\nresults are:\n  - An improved distributed hypergraph maximal matching algorithm, improving on\nFischer, Ghaffari, and Kuhn [FOCS'17], and giving improved algorithms for\nedge-coloring, maximum matching approximation, and low out-degree edge\norientation. The first gives an improved algorithm for Open Problem 11.4 of the\nbook of Barenboim and Elkin, and the last gives the first positive resolution\nof their Open Problem 11.10.\n  - An improved distributed algorithm for the Lov\\'{a}sz Local Lemma, which\ngets closer to a conjecture of Chang and Pettie [FOCS'17], and moreover leads\nto improved distributed algorithms for problems such as defective coloring and\n$k$-SAT. \n\n"}
{"id": "1711.05157", "contents": "Title: A note on the complexity of Feedback Vertex Set parameterized by\n  mim-width Abstract: We complement the recent algorithmic result that Feedback Vertex Set is\nXP-time solvable parameterized by the mim-width of a given branch decomposition\nof the input graph [3] by showing that the problem is W[1]-hard in this\nparameterization. The hardness holds even for linear mim-width, as well as for\nH-graphs, where the parameter is the number of edges in H. To obtain this\nresult, we adapt a reduction due to Fomin, Golovach and Raymond [2], following\nthe same line of reasoning but adding a new gadget. \n\n"}
{"id": "1711.06625", "contents": "Title: Dynamic Matching: Reducing Integral Algorithms to Approximately-Maximal\n  Fractional Algorithms Abstract: We present a simple randomized reduction from fully-dynamic integral matching\nalgorithms to fully-dynamic \"approximately-maximal\" fractional matching\nalgorithms. Applying this reduction to the recent fractional matching algorithm\nof Bhattacharya, Henzinger, and Nanongkai (SODA 2017), we obtain a novel result\nfor the integral problem. Specifically, our main result is a randomized\nfully-dynamic $(2+\\epsilon)$-approximate integral matching algorithm with small\npolylog worst-case update time. For the $(2+\\epsilon)$-approximation regime\nonly a \\emph{fractional} fully-dynamic $(2+\\epsilon)$-matching algorithm with\nworst-case polylog update time was previously known, due to Bhattacharya et\nal.~(SODA 2017). Our algorithm is the first algorithm that maintains\napproximate matchings with worst-case update time better than polynomial, for\nany constant approximation ratio. As a consequence, we also obtain the first\nconstant-approximate worst-case polylogarithmic update time maximum weight\nmatching algorithm. \n\n"}
{"id": "1711.07285", "contents": "Title: Quantum Query Algorithms are Completely Bounded Forms Abstract: We prove a characterization of $t$-query quantum algorithms in terms of the\nunit ball of a space of degree-$2t$ polynomials. Based on this, we obtain a\nrefined notion of approximate polynomial degree that equals the quantum query\ncomplexity, answering a question of Aaronson et al. (CCC'16). Our proof is\nbased on a fundamental result of Christensen and Sinclair (J. Funct. Anal.,\n1987) that generalizes the well-known Stinespring representation for quantum\nchannels to multilinear forms. Using our characterization, we show that many\npolynomials of degree four are far from those coming from two-query quantum\nalgorithms. We also give a simple and short proof of one of the results of\nAaronson et al. showing an equivalence between one-query quantum algorithms and\nbounded quadratic polynomials.\n  Revision note: A mistake was found in the proof of the second result on\ndegree-4 polynomials far from 2-query quantum algorithms. An explanation of the\nissue, a corrected proof and stronger examples are presented in work of\nEscudero Guti\\'errez and the second author. \n\n"}
{"id": "1711.08039", "contents": "Title: Alternating minimization, scaling algorithms, and the null-cone problem\n  from invariant theory Abstract: Alternating minimization heuristics seek to solve a (difficult) global\noptimization task through iteratively solving a sequence of (much easier) local\noptimization tasks on different parts (or blocks) of the input parameters.\nWhile popular and widely applicable, very few examples of this heuristic are\nrigorously shown to converge to optimality, and even fewer to do so\nefficiently.\n  In this paper we present a general framework which is amenable to rigorous\nanalysis, and expose its applicability. Its main feature is that the local\noptimization domains are each a group of invertible matrices, together\nnaturally acting on tensors, and the optimization problem is minimizing the\nnorm of an input tensor under this joint action. The solution of this\noptimization problem captures a basic problem in Invariant Theory, called the\nnull-cone problem.\n  This algebraic framework turns out to encompass natural computational\nproblems in combinatorial optimization, algebra, analysis, quantum information\ntheory, and geometric complexity theory. It includes and extends to high\ndimensions the recent advances on (2-dimensional) operator scaling.\n  Our main result is a fully polynomial time approximation scheme for this\ngeneral problem, which may be viewed as a multi-dimensional scaling algorithm.\nThis directly leads to progress on some of the problems in the areas above, and\na unified view of others. We explain how faster convergence of an algorithm for\nthe same problem will allow resolving central open problems.\n  Our main techniques come from Invariant Theory, and include its rich\nnon-commutative duality theory, and new bounds on the bitsizes of coefficients\nof invariant polynomials. They enrich the algorithmic toolbox of this very\ncomputational field of mathematics, and are directly related to some challenges\nin geometric complexity theory (GCT). \n\n"}
{"id": "1711.08082", "contents": "Title: Parameter Estimation in Gaussian Mixture Models with Malicious Noise,\n  without Balanced Mixing Coefficients Abstract: We consider the problem of estimating means of two Gaussians in a 2-Gaussian\nmixture, which is not balanced and is corrupted by noise of an arbitrary\ndistribution. We present a robust algorithm to estimate the parameters,\ntogether with upper bounds on the numbers of samples required for the estimate\nto be correct, where the bounds are parametrised by the dimension, ratio of the\nmixing coefficients, a measure of the separation of the two Gaussians, related\nto Mahalanobis distance, and a condition number of the covariance matrix. In\ntheory, this is the first sample-complexity result for imbalanced mixtures\ncorrupted by adversarial noise. In practice, our algorithm outperforms the\nvanilla Expectation-Maximisation (EM) algorithm in terms of estimation error. \n\n"}
{"id": "1711.10145", "contents": "Title: Lower Bounds for Approximating the Matching Polytope Abstract: We prove that any extended formulation that approximates the matching\npolytope on $n$-vertex graphs up to a factor of $(1+\\varepsilon)$ for any\n$\\frac2n \\le \\varepsilon \\le 1$ must have at least\n$\\binom{n}{{\\alpha}/{\\varepsilon}}$ defining inequalities where $0<\\alpha<1$ is\nan absolute constant. This is tight as exhibited by the $(1+\\varepsilon)$\napproximating linear program obtained by dropping the odd set constraints of\nsize larger than $({1+\\varepsilon})/{\\varepsilon}$ from the description of the\nmatching polytope. Previously, a tight lower bound of $2^{\\Omega(n)}$ was only\nknown for $\\varepsilon = O\\left(\\frac{1}{n}\\right)$ [Rothvoss, STOC '14; Braun\nand Pokutta, IEEE Trans. Information Theory '15] whereas for $\\frac2n \\le\n\\varepsilon \\le 1$, the best lower bound was\n$2^{\\Omega\\left({1}/{\\varepsilon}\\right)}$ [Rothvoss, STOC '14]. The key new\ningredient in our proof is a close connection to the non-negative rank of a\nlopsided version of the unique disjointness matrix. \n\n"}
{"id": "1711.10530", "contents": "Title: Parametrised second-order complexity theory with applications to the\n  study of interval computation Abstract: We extend the framework for complexity of operators in analysis devised by\nKawamura and Cook (2012) to allow for the treatment of a wider class of\nrepresentations. The main novelty is to endow represented spaces of interest\nwith an additional function on names, called a parameter, which measures the\ncomplexity of a given name. This parameter generalises the size function which\nis usually used in second-order complexity theory and therefore also central to\nthe framework of Kawamura and Cook. The complexity of an algorithm is measured\nin terms of its running time as a second-order function in the parameter, as\nwell as in terms of how much it increases the complexity of a given name, as\nmeasured by the parameters on the input and output side.\n  As an application we develop a rigorous computational complexity theory for\ninterval computation. In the framework of Kawamura and Cook the representation\nof real numbers based on nested interval enclosures does not yield a reasonable\ncomplexity theory. In our new framework this representation is polytime\nequivalent to the usual Cauchy representation based on dyadic rational\napproximation. By contrast, the representation of continuous real functions\nbased on interval enclosures is strictly smaller in the polytime reducibility\nlattice than the usual representation, which encodes a modulus of continuity.\nFurthermore, the function space representation based on interval enclosures is\noptimal in the sense that it contains the minimal amount of information amongst\nthose representations which render evaluation polytime computable. \n\n"}
{"id": "1711.10605", "contents": "Title: Merlin-Arthur with efficient quantum Merlin and quantum supremacy for\n  the second level of the Fourier hierarchy Abstract: We introduce a simple sub-universal quantum computing model, which we call\nthe Hadamard-classical circuit with one-qubit (HC1Q) model. It consists of a\nclassical reversible circuit sandwiched by two layers of Hadamard gates, and\ntherefore it is in the second level of the Fourier hierarchy. We show that\noutput probability distributions of the HC1Q model cannot be classically\nefficiently sampled within a multiplicative error unless the polynomial-time\nhierarchy collapses to the second level. The proof technique is different from\nthose used for previous sub-universal models, such as IQP, Boson Sampling, and\nDQC1, and therefore the technique itself might be useful for finding other\nsub-universal models that are hard to classically simulate. We also study the\nclassical verification of quantum computing in the second level of the Fourier\nhierarchy. To this end, we define a promise problem, which we call the\nprobability distribution distinguishability with maximum norm (PDD-Max). It is\na promise problem to decide whether output probability distributions of two\nquantum circuits are far apart or close. We show that PDD-Max is BQP-complete,\nbut if the two circuits are restricted to some types in the second level of the\nFourier hierarchy, such as the HC1Q model or the IQP model, PDD-Max has a\nMerlin-Arthur system with quantum polynomial-time Merlin and classical\nprobabilistic polynomial-time Arthur. \n\n"}
{"id": "1712.02447", "contents": "Title: On Colouring $(2P_2,H)$-Free and $(P_5,H)$-Free Graphs Abstract: The Colouring problem asks whether the vertices of a graph can be coloured\nwith at most $k$ colours for a given integer $k$ in such a way that no two\nadjacent vertices receive the same colour. A graph is $(H_1,H_2)$-free if it\nhas no induced subgraph isomorphic to $H_1$ or $H_2$. A connected graph $H_1$\nis almost classified if Colouring on $(H_1,H_2)$-free graphs is known to be\npolynomial-time solvable or NP-complete for all but finitely many connected\ngraphs $H_2$. We show that every connected graph $H_1$ apart from the claw\n$K_{1,3}$ and the $5$-vertex path $P_5$ is almost classified. We also prove a\nnumber of new hardness results for Colouring on $(2P_2,H)$-free graphs. This\nenables us to list all graphs $H$ for which the complexity of Colouring is open\non $(2P_2,H)$-free graphs and all graphs $H$ for which the complexity of\nColouring is open on $(P_5,H)$-free graphs. In fact we show that these two\nlists coincide. Moreover, we show that the complexities of Colouring for\n$(2P_2,H)$-free graphs and for $(P_5,H)$-free graphs are the same for all known\ncases. \n\n"}
{"id": "1712.03714", "contents": "Title: Efficient enumeration of solutions produced by closure operations Abstract: In this paper we address the problem of generating all elements obtained by\nthe saturation of an initial set by some operations. More precisely, we prove\nthat we can generate the closure of a boolean relation (a set of boolean\nvectors) by polymorphisms with a polynomial delay. Therefore we can compute\nwith polynomial delay the closure of a family of sets by any set of \"set\noperations\": union, intersection, symmetric difference, subsets, supersets\n$\\dots$). To do so, we study the $Membership_{\\mathcal{F}}$ problem: for a set\nof operations $\\mathcal{F}$, decide whether an element belongs to the closure\nby $\\mathcal{F}$ of a family of elements. In the boolean case, we prove that\n$Membership_{\\mathcal{F}}$ is in P for any set of boolean operations\n$\\mathcal{F}$. When the input vectors are over a domain larger than two\nelements, we prove that the generic enumeration method fails, since\n$Membership_{\\mathcal{F}}$ is NP-hard for some $\\mathcal{F}$. We also study the\nproblem of generating minimal or maximal elements of closures and prove that\nsome of them are related to well known enumeration problems such as the\nenumeration of the circuits of a matroid or the enumeration of maximal\nindependent sets of a hypergraph. This article improves on previous works of\nthe same authors. \n\n"}
{"id": "1712.04281", "contents": "Title: Computational Complexity of the Interleaving Distance Abstract: The interleaving distance is arguably the most prominent distance measure in\ntopological data analysis. In this paper, we provide bounds on the\ncomputational complexity of determining the interleaving distance in several\nsettings. We show that the interleaving distance is NP-hard to compute for\npersistence modules valued in the category of vector spaces. In the specific\nsetting of multidimensional persistent homology we show that the problem is at\nleast as hard as a matrix invertibility problem. Furthermore, this allows us to\nconclude that the interleaving distance of interval decomposable modules\ndepends on the characteristic of the field. Persistence modules valued in the\ncategory of sets are also studied. As a corollary, we obtain that the\nisomorphism problem for Reeb graphs is graph isomorphism complete. \n\n"}
{"id": "1712.04581", "contents": "Title: Potential-Function Proofs for First-Order Methods Abstract: This note discusses proofs for convergence of first-order methods based on\nsimple potential-function arguments. We cover methods like gradient descent\n(for both smooth and non-smooth settings), mirror descent, and some accelerated\nvariants. \n\n"}
{"id": "1712.06309", "contents": "Title: FPT-algorithms for some problems related to integer programming Abstract: In this paper, we present FPT-algorithms for special cases of the shortest\nlattice vector, integer linear programming, and simplex width computation\nproblems, when matrices included in the problems' formulations are near square.\nThe parameter is the maximum absolute value of rank minors of the corresponding\nmatrices. Additionally, we present FPT-algorithms with respect to the same\nparameter for the problems, when the matrices have no singular rank\nsub-matrices. \n\n"}
{"id": "1712.06690", "contents": "Title: An Experimental Evaluation of a Bounded Expansion Algorithmic Pipeline Abstract: Previous work has suggested that the structural restrictions of graphs from\nclasses of bounded expansion--locally dense pockets in a globally sparse\ngraph--naturally coincide with common properties of real-world networks such as\nclustering and heavy-tailed degree distributions. As such, fixed-parameter\ntractable algorithms for bounded expansion classes may offer a promising\nframework for network analysis where other approaches have struggled to scale.\nHowever, there has been little work done in implementing and evaluating the\nperformance of these structure-based algorithms. To this end we introduce\nCONCUSS, a proof-of-concept implementation of a generic algorithmic pipeline\nfor classes of bounded expansion. In particular, we focus on using CONCUSS for\nsubgraph isomorphism counting (also called motif or graphlet counting), which\nhas been used extensively as a tool for analyzing biological and social\nnetworks. Through a broad set of experiments we first evaluate the interactions\nbetween implementation/engineering choices at multiple stages of the pipeline\nand their effects on overall run time. From there, we establish viability of\nthe bounded expansion framework by demonstrating that in some scenarios CONCUSS\nachieves run times competitive with a popular algorithm for subgraph\nisomorphism counting that does not exploit graph structure. Finally, we\nempirically identify two particular ways in which future theoretical advances\ncould alleviate bottlenecks in the algorithmic pipeline. \n\n"}
{"id": "1712.06865", "contents": "Title: Approximate Correlation Clustering Using Same-Cluster Queries Abstract: Ashtiani et al. (NIPS 2016) introduced a semi-supervised framework for\nclustering (SSAC) where a learner is allowed to make same-cluster queries. More\nspecifically, in their model, there is a query oracle that answers queries of\nthe form given any two vertices, do they belong to the same optimal cluster?.\nAshtiani et al. showed the usefulness of such a query framework by giving a\npolynomial time algorithm for the k-means clustering problem where the input\ndataset satisfies some separation condition. Ailon et al. extended the above\nwork to the approximation setting by giving an efficient (1+\\eps)-approximation\nalgorithm for k-means for any small \\eps > 0 and any dataset within the SSAC\nframework. In this work, we extend this line of study to the correlation\nclustering problem. Correlation clustering is a graph clustering problem where\npairwise similarity (or dissimilarity) information is given for every pair of\nvertices and the objective is to partition the vertices into clusters that\nminimise the disagreement (or maximises agreement) with the pairwise\ninformation given as input. These problems are popularly known as MinDisAgree\nand MaxAgree problems, and MinDisAgree[k] and MaxAgree[k] are versions of these\nproblems where the number of optimal clusters is at most k. There exist\nPolynomial Time Approximation Schemes (PTAS) for MinDisAgree[k] and MaxAgree[k]\nwhere the approximation guarantee is (1+\\eps) for any small \\eps and the\nrunning time is polynomial in the input parameters but exponential in k and\n1/\\eps. We obtain an (1+\\eps)-approximation algorithm for any small \\eps with\nrunning time that is polynomial in the input parameters and also in k and\n1/\\eps. We also give non-trivial upper and lower bounds on the number of\nsame-cluster queries, the lower bound being based on the Exponential Time\nHypothesis (ETH). \n\n"}
{"id": "1712.07041", "contents": "Title: The cavity approach for Steiner trees packing problems Abstract: The Belief Propagation approximation, or cavity method, has been recently\napplied to several combinatorial optimization problems in its zero-temperature\nimplementation, the max-sum algorithm. In particular, recent developments to\nsolve the edge-disjoint paths problem and the prize-collecting Steiner tree\nproblem on graphs have shown remarkable results for several classes of graphs\nand for benchmark instances. Here we propose a generalization of these\ntechniques for two variants of the Steiner trees packing problem where multiple\n\"interacting\" trees have to be sought within a given graph. Depending on the\ninteraction among trees we distinguish the vertex-disjoint Steiner trees\nproblem, where trees cannot share nodes, from the edge-disjoint Steiner trees\nproblem, where edges cannot be shared by trees but nodes can be members of\nmultiple trees. Several practical problems of huge interest in network design\ncan be mapped into these two variants, for instance, the physical design of\nVery Large Scale Integration (VLSI) chips. The formalism described here relies\non two components edge-variables that allows us to formulate a massage-passing\nalgorithm for the V-DStP and two algorithms for the E-DStP differing in the\nscaling of the computational time with respect to some relevant parameters. We\nwill show that one of the two formalisms used for the edge-disjoint variant\nallow us to map the max-sum update equations into a weighted maximum matching\nproblem over proper bipartite graphs. We developed a heuristic procedure based\non the max-sum equations that shows excellent performance in synthetic networks\n(in particular outperforming standard multi-step greedy procedures by large\nmargins) and on large benchmark instances of VLSI for which the optimal\nsolution is known, on which the algorithm found the optimum in two cases and\nthe gap to optimality was never larger than 4 %. \n\n"}
{"id": "1712.07476", "contents": "Title: The graph tessellation cover number: extremal bounds, efficient\n  algorithms and hardness Abstract: A tessellation of a graph is a partition of its vertices into vertex disjoint\ncliques. A tessellation cover of a graph is a set of tessellations that covers\nall of its edges. The $t$-tessellability problem aims to decide whether there\nis a tessellation cover of the graph with $t$ tessellations. This problem is\nmotivated by its applications to quantum walk models, in especial, the\nevolution operator of the staggered model is obtained from a graph tessellation\ncover. We establish upper bounds on the tessellation cover number given by the\nminimum between the chromatic index of the graph and the chromatic number of\nits clique graph and we show graph classes for which these bounds are tight. We\nprove $\\mathcal{NP}$-completeness for $t$-tessellability if the instance is\nrestricted to planar graphs, chordal (2,1)-graphs, (1,2)-graphs, diamond-free\ngraphs with diameter five, or for any fixed $t$ at least 3. On the other hand,\nwe improve the complexity for 2-tessellability to a linear-time algorithm. \n\n"}
{"id": "1712.08880", "contents": "Title: Lectures on Randomized Numerical Linear Algebra Abstract: This chapter is based on lectures on Randomized Numerical Linear Algebra from\nthe 2016 Park City Mathematics Institute summer school on The Mathematics of\nData. \n\n"}
{"id": "1712.09630", "contents": "Title: Tensor network complexity of multilinear maps Abstract: We study tensor networks as a model of arithmetic computation for evaluating\nmultilinear maps. These capture any algorithm based on low border rank tensor\ndecompositions, such as $O(n^{\\omega+\\epsilon})$ time matrix multiplication,\nand in addition many other algorithms such as $O(n \\log n)$ time discrete\nFourier transform and $O^*(2^n)$ time for computing the permanent of a matrix.\nHowever tensor networks sometimes yield faster algorithms than those that\nfollow from low-rank decompositions. For instance the fastest known\n$O(n^{(\\omega +\\epsilon)t})$ time algorithms for counting $3t$-cliques can be\nimplemented with tensor networks, even though the underlying tensor has border\nrank $n^{3t}$ for all $t \\ge 2$. For counting homomorphisms of a general\npattern graph $P$ into a host graph on $n$ vertices we obtain an upper bound of\n$O(n^{(\\omega+\\epsilon)\\operatorname{bw}(P)/2})$ where $\\operatorname{bw}(P)$\nis the branchwidth of $P$. This essentially matches the bound for counting\ncliques, and yields small improvements over previous algorithms for many\nchoices of $P$.\n  While powerful, the model still has limitations, and we are able to show a\nnumber of unconditional lower bounds for various multilinear maps, including:\n  (a) an $\\Omega(n^{\\operatorname{bw}(P)})$ time lower bound for counting\nhomomorphisms from $P$ to an $n$-vertex graph, matching the upper bound if\n$\\omega = 2$. In particular for $P$ a $v$-clique this yields an\n$\\Omega(n^{\\lceil 2v/3 \\rceil})$ time lower bound for counting $v$-cliques, and\nfor $P$ a $k$-uniform $v$-hyperclique we obtain an $\\Omega(n^v)$ time lower\nbound for $k \\ge 3$, ruling out tensor networks as an approach to obtaining\nnon-trivial algorithms for hyperclique counting and the Max-$3$-CSP problem.\n  (b) an $\\Omega(2^{0.918n})$ time lower bound for the permanent of an $n\n\\times n$ matrix. \n\n"}
{"id": "1712.09948", "contents": "Title: Minimizing Polarization and Disagreement in Social Networks Abstract: The rise of social media and online social networks has been a disruptive\nforce in society. Opinions are increasingly shaped by interactions on online\nsocial media, and social phenomena including disagreement and polarization are\nnow tightly woven into everyday life. In this work we initiate the study of the\nfollowing question: given $n$ agents, each with its own initial opinion that\nreflects its core value on a topic, and an opinion dynamics model, what is the\nstructure of a social network that minimizes {\\em polarization} and {\\em\ndisagreement} simultaneously?\n  This question is central to recommender systems: should a recommender system\nprefer a link suggestion between two online users with similar mindsets in\norder to keep disagreement low, or between two users with different opinions in\norder to expose each to the other's viewpoint of the world, and decrease\noverall levels of polarization? Our contributions include a mathematical\nformalization of this question as an optimization problem and an exact,\ntime-efficient algorithm. We also prove that there always exists a network with\n$O(n/\\epsilon^2)$ edges that is a $(1+\\epsilon)$ approximation to the optimum.\nFor a fixed graph, we additionally show how to optimize our objective function\nover the agents' innate opinions in polynomial time.\n  We perform an empirical study of our proposed methods on synthetic and\nreal-world data that verify their value as mining tools to better understand\nthe trade-off between of disagreement and polarization. We find that there is a\nlot of space to reduce both polarization and disagreement in real-world\nnetworks; for instance, on a Reddit network where users exchange comments on\npolitics, our methods achieve a $\\sim 60\\,000$-fold reduction in polarization\nand disagreement. \n\n"}
{"id": "1801.04497", "contents": "Title: Near-optimal approximation algorithm for simultaneous Max-Cut Abstract: In the simultaneous Max-Cut problem, we are given $k$ weighted graphs on the\nsame set of $n$ vertices, and the goal is to find a cut of the vertex set so\nthat the minimum, over the $k$ graphs, of the cut value is as large as\npossible. Previous work [BKS15] gave a polynomial time algorithm which achieved\nan approximation factor of $1/2 - o(1)$ for this problem (and an approximation\nfactor of $1/2 + \\epsilon_k$ in the unweighted case, where $\\epsilon_k\n\\rightarrow 0$ as $k \\rightarrow \\infty$).\n  In this work, we give a polynomial time approximation algorithm for\nsimultaneous Max-Cut with an approximation factor of $0.8780$ (for all constant\n$k$). The natural SDP formulation for simultaneous Max-Cut was shown to have an\nintegrality gap of $1/2+\\epsilon_k$ in [BKS15]. In achieving the better\napproximation guarantee, we use a stronger Sum-of-Squares hierarchy SDP\nrelaxation and a rounding algorithm based on Raghavendra-Tan [RT12], in\naddition to techniques from [BKS15]. \n\n"}
{"id": "1801.07150", "contents": "Title: A Novel Weighted Distance Measure for Multi-Attributed Graph Abstract: Due to exponential growth of complex data, graph structure has become\nincreasingly important to model various entities and their interactions, with\nmany interesting applications including, bioinformatics, social network\nanalysis, etc. Depending on the complexity of the data, the underlying graph\nmodel can be a simple directed/undirected and/or weighted/un-weighted graph to\na complex graph (aka multi-attributed graph) where vertices and edges are\nlabelled with multi-dimensional vectors. In this paper, we present a novel\nweighted distance measure based on weighted Euclidean norm which is defined as\na function of both vertex and edge attributes, and it can be used for various\ngraph analysis tasks including classification and cluster analysis. The\nproposed distance measure has flexibility to increase/decrease the weightage of\nedge labels while calculating the distance between vertex-pairs. We have also\nproposed a MAGDist algorithm, which reads multi-attributed graph stored in CSV\nfiles containing the list of vertex vectors and edge vectors, and calculates\nthe distance between each vertex-pair using the proposed weighted distance\nmeasure. Finally, we have proposed a multi-attributed similarity graph\ngeneration algorithm, MAGSim, which reads the output of MAGDist algorithm and\ngenerates a similarity graph that can be analysed using classification and\nclustering algorithms. The significance and accuracy of the proposed distance\nmeasure and algorithms is evaluated on Iris and Twitter data sets, and it is\nfound that the similarity graph generated by our proposed method yields better\nclustering results than the existing similarity graph generation methods. \n\n"}
{"id": "1801.07301", "contents": "Title: Secure $k$-ish Nearest Neighbors Classifier Abstract: In machine learning, classifiers are used to predict a class of a given query\nbased on an existing (classified) database. Given a database S of n\nd-dimensional points and a d-dimensional query q, the k-nearest neighbors (kNN)\nclassifier assigns q with the majority class of its k nearest neighbors in S.\n  In the secure version of kNN, S and q are owned by two different parties that\ndo not want to share their data. Unfortunately, all known solutions for secure\nkNN either require a large communication complexity between the parties, or are\nvery inefficient to run.\n  In this work we present a classifier based on kNN, that can be implemented\nefficiently with homomorphic encryption (HE). The efficiency of our classifier\ncomes from a relaxation we make on kNN, where we allow it to consider kappa\nnearest neighbors for kappa ~ k with some probability. We therefore call our\nclassifier k-ish Nearest Neighbors (k-ish NN).\n  The success probability of our solution depends on the distribution of the\ndistances from q to S and increase as its statistical distance to Gaussian\ndecrease.\n  To implement our classifier we introduce the concept of double-blinded\ncoin-toss. In a doubly-blinded coin-toss the success probability as well as the\noutput of the toss are encrypted. We use this coin-toss to efficiently\napproximate the average and variance of the distances from q to S. We believe\nthese two techniques may be of independent interest.\n  When implemented with HE, the k-ish NN has a circuit depth that is\nindependent of n, therefore making it scalable. We also implemented our\nclassifier in an open source library based on HELib and tested it on a breast\ntumor database. The accuracy of our classifier (F_1 score) were 98\\% and\nclassification took less than 3 hours compared to (estimated) weeks in current\nHE implementations. \n\n"}
{"id": "1801.07342", "contents": "Title: Perfect simulation of the Hard Disks Model by Partial Rejection Sampling Abstract: We present a perfect simulation of the hard disks model via the partial\nrejection sampling method. Provided the density of disks is not too high, the\nmethod produces exact samples in $O(\\log n)$ rounds, and total time $O(n)$,\nwhere $n$ is the expected number of disks. The method extends easily to the\nhard spheres model in $d>2$ dimensions. In order to apply the partial rejection\nmethod to this continuous setting, we provide an alternative perspective of its\ncorrectness and run-time analysis that is valid for general state spaces. \n\n"}
{"id": "1801.08693", "contents": "Title: Improved Finite Blocklength Converses for Slepian-Wolf Coding via Linear\n  Programming Abstract: A new finite blocklength converse for the Slepian- Wolf coding problem is\npresented which significantly improves on the best known converse for this\nproblem, due to Miyake and Kanaya [2]. To obtain this converse, an extension of\nthe linear programming (LP) based framework for finite blocklength point-\nto-point coding problems from [3] is employed. However, a direct application of\nthis framework demands a complicated analysis for the Slepian-Wolf problem. An\nanalytically simpler approach is presented wherein LP-based finite blocklength\nconverses for this problem are synthesized from point-to-point lossless source\ncoding problems with perfect side-information at the decoder. New finite\nblocklength metaconverses for these point-to-point problems are derived by\nemploying the LP-based framework, and the new converse for Slepian-Wolf coding\nis obtained by an appropriate combination of these converses. \n\n"}
{"id": "1801.09159", "contents": "Title: Faster Approximate(d) Text-to-Pattern L1 Distance Abstract: The problem of finding \\emph{distance} between \\emph{pattern} of length $m$\nand \\emph{text} of length $n$ is a typical way of generalizing pattern matching\nto incorporate dissimilarity score. For both Hamming and $L_1$ distances only a\nsuper linear upper bound $\\widetilde{O}(n\\sqrt{m})$ are known, which prompts\nthe question of relaxing the problem: either by asking for $(1 \\pm\n\\varepsilon)$ approximate distance (every distance is reported up to a\nmultiplicative factor), or $k$-approximated distance (distances exceeding $k$\nare reported as $\\infty$). We focus on $L_1$ distance, for which we show new\nalgorithms achieving complexities respectively $\\widetilde{O}(\\varepsilon^{-1}\nn)$ and $\\widetilde{O}((m+k\\sqrt{m}) \\cdot n/m)$. This is a significant\nimprovement upon previous algorithms with runtime\n$\\widetilde{O}(\\varepsilon^{-2} n)$ of Lipsky and Porat [Algorithmica 2011] and\n$\\widetilde{O}(n\\sqrt{k})$ of Amir, Lipsky, Porat and Umanski [CPM 2005]. \n\n"}
{"id": "1801.09798", "contents": "Title: Earthmover Resilience and Testing in Ordered Structures Abstract: One of the main challenges in property testing is to characterize those\nproperties that are testable with a constant number of queries. For unordered\nstructures such as graphs and hypergraphs this task has been mostly settled.\nHowever, for ordered structures such as strings, images, and ordered graphs,\nthe characterization problem seems very difficult in general.\n  In this paper, we identify a wide class of properties of ordered structures -\nthe earthmover resilient (ER) properties - and show that the \"good behavior\" of\nsuch properties allows us to obtain general testability results that are\nsimilar to (and more general than) those of unordered graphs. A property P is\nER if, roughly speaking, slight changes in the order of the elements in an\nobject satisfying P cannot make this object far from P. The class of ER\nproperties includes, e.g., all unordered graph properties, many natural visual\nproperties of images, such as convexity, and all hereditary properties of\nordered graphs and images.\n  A special case of our results implies, building on a recent result of Alon\nand the authors, that the distance of a given image or ordered graph from any\nhereditary property can be estimated (with good probability) up to a constant\nadditive error, using a constant number of queries. \n\n"}
{"id": "1802.00884", "contents": "Title: A Model for Learned Bloom Filters and Related Structures Abstract: Recent work has suggested enhancing Bloom filters by using a pre-filter,\nbased on applying machine learning to model the data set the Bloom filter is\nmeant to represent. Here we model such learned Bloom filters, clarifying what\nguarantees can and cannot be associated with such a structure. \n\n"}
{"id": "1802.00963", "contents": "Title: Coding Theory: the unit-derived methodology Abstract: The unit-derived method in coding theory is shown to be a unique optimal\nscheme for constructing and analysing codes. In many cases efficient and\npractical decoding methods are produced. Codes with efficient decoding\nalgorithms at maximal distances possible are derived from unit schemes. In\nparticular unit-derived codes from Vandermonde or Fourier matrices are\nparticularly commendable giving rise to mds codes of varying rates with\npractical and efficient decoding algorithms. For a given rate and given error\ncorrection capability, explicit codes with efficient error correcting\nalgorithms are designed to these specifications. An explicit constructive proof\nwith an efficient decoding algorithm is given for Shannon's theorem. For a\ngiven finite field, codes are constructed which are `optimal' for this field. \n\n"}
{"id": "1802.02050", "contents": "Title: Optimal Data Reduction for Graph Coloring Using Low-Degree Polynomials Abstract: The theory of kernelization can be used to rigorously analyze data reduction\nfor graph coloring problems. Here, the aim is to reduce a q-Coloring input to\nan equivalent but smaller input whose size is provably bounded in terms of\nstructural properties, such as the size of a minimum vertex cover. In this\npaper we settle two open problems about data reduction for q-Coloring.\n  First, we obtain a kernel of bitsize $O(k^{q-1}\\log{k})$ for q-Coloring\nparameterized by Vertex Cover, for any q >= 3. This size bound is optimal up to\n$k^{o(1)}$ factors assuming NP is not a subset of coNP/poly, and improves on\nthe previous-best kernel of size $O(k^q)$. We generalize this result for\ndeciding q-colorability of a graph G, to deciding the existence of a\nhomomorphism from G to an arbitrary fixed graph H. Furthermore, we can replace\nthe parameter vertex cover by the less restrictive parameter twin-cover. We\nprove that H-Coloring parameterized by Twin-Cover has a kernel of size\n$O(k^{\\Delta(H)}\\log k)$.\n  Our second result shows that 3-Coloring does not admit non-trivial\nsparsification: assuming NP is not a subset of coNP/poly, the parameterization\nby the number of vertices n admits no (generalized) kernel of size $O(n^{2-e})$\nfor any e > 0. Previously, such a lower bound was only known for coloring with\nq >= 4 colors. \n\n"}
{"id": "1802.02325", "contents": "Title: On The Hardness of Approximate and Exact (Bichromatic) Maximum Inner\n  Product Abstract: In this paper we study the (Bichromatic) Maximum Inner Product Problem\n(Max-IP), in which we are given sets $A$ and $B$ of vectors, and the goal is to\nfind $a \\in A$ and $b \\in B$ maximizing inner product $a \\cdot b$. Max-IP is\nvery basic and serves as the base problem in the recent breakthrough of [Abboud\net al., FOCS 2017] on hardness of approximation for polynomial-time problems.\nIt is also used (implicitly) in the argument for hardness of exact\n$\\ell_2$-Furthest Pair (and other important problems in computational geometry)\nin poly-log-log dimensions in [Williams, SODA 2018]. We have three main results\nregarding this problem.\n  First, we study the best multiplicative approximation ratio for Boolean\nMax-IP in sub-quadratic time. We show that, for Max-IP with two sets of $n$\nvectors from $\\{0,1\\}^{d}$, there is an $n^{2 - \\Omega(1)}$ time $\\left( d/\\log\nn \\right)^{\\Omega(1)}$-multiplicative-approximating algorithm, and we show this\nis conditionally optimal, as such a $\\left(d/\\log\nn\\right)^{o(1)}$-approximating algorithm would refute SETH.\n  Second, we achieve a similar characterization for the best additive\napproximation error to Boolean Max-IP. We show that, for Max-IP with two sets\nof $n$ vectors from $\\{0,1\\}^{d}$, there is an $n^{2 - \\Omega(1)}$ time\n$\\Omega(d)$-additive-approximating algorithm, and this is conditionally\noptimal, as such an $o(d)$-approximating algorithm would refute SETH\n[Rubinstein, STOC 2018].\n  Last, we revisit the hardness of solving Max-IP exactly for vectors with\ninteger entries. We show that, under SETH, for Max-IP with sets of $n$ vectors\nfrom $\\mathbb{Z}^{d}$ for some $d = 2^{O(\\log^{*} n)}$, every exact algorithm\nrequires $n^{2 - o(1)}$ time. With the reduction from [Williams, SODA 2018], it\nfollows that $\\ell_2$-Furthest Pair and Bichromatic $\\ell_2$-Closest Pair in\n$2^{O(\\log^{*} n)}$ dimensions require $n^{2 - o(1)}$ time. \n\n"}
{"id": "1802.04367", "contents": "Title: Computational Optimal Transport: Complexity by Accelerated Gradient\n  Descent Is Better Than by Sinkhorn's Algorithm Abstract: We analyze two algorithms for approximating the general optimal transport\n(OT) distance between two discrete distributions of size $n$, up to accuracy\n$\\varepsilon$. For the first algorithm, which is based on the celebrated\nSinkhorn's algorithm, we prove the complexity bound\n$\\widetilde{O}\\left({n^2/\\varepsilon^2}\\right)$ arithmetic operations. For the\nsecond one, which is based on our novel Adaptive Primal-Dual Accelerated\nGradient Descent (APDAGD) algorithm, we prove the complexity bound\n$\\widetilde{O}\\left(\\min\\left\\{n^{9/4}/\\varepsilon, n^{2}/\\varepsilon^2\n\\right\\}\\right)$ arithmetic operations. Both bounds have better dependence on\n$\\varepsilon$ than the state-of-the-art result given by\n$\\widetilde{O}\\left({n^2/\\varepsilon^3}\\right)$. Our second algorithm not only\nhas better dependence on $\\varepsilon$ in the complexity bound, but also is not\nspecific to entropic regularization and can solve the OT problem with different\nregularizers. \n\n"}
{"id": "1802.04705", "contents": "Title: Hadamard Response: Estimating Distributions Privately, Efficiently, and\n  with Little Communication Abstract: We study the problem of estimating $k$-ary distributions under\n$\\varepsilon$-local differential privacy. $n$ samples are distributed across\nusers who send privatized versions of their sample to a central server. All\npreviously known sample optimal algorithms require linear (in $k$)\ncommunication from each user in the high privacy regime $(\\varepsilon=O(1))$,\nand run in time that grows as $n\\cdot k$, which can be prohibitive for large\ndomain size $k$.\n  We propose Hadamard Response (HR}, a local privatization scheme that requires\nno shared randomness and is symmetric with respect to the users. Our scheme has\norder optimal sample complexity for all $\\varepsilon$, a communication of at\nmost $\\log k+2$ bits per user, and nearly linear running time of $\\tilde{O}(n +\nk)$.\n  Our encoding and decoding are based on Hadamard matrices, and are simple to\nimplement. The statistical performance relies on the coding theoretic aspects\nof Hadamard matrices, ie, the large Hamming distance between the rows. An\nefficient implementation of the algorithm using the Fast Walsh-Hadamard\ntransform gives the computational gains.\n  We compare our approach with Randomized Response (RR), RAPPOR, and\nsubset-selection mechanisms (SS), both theoretically, and experimentally. For\n$k=10000$, our algorithm runs about 100x faster than SS, and RAPPOR. \n\n"}
{"id": "1802.04859", "contents": "Title: Distribution-free Junta Testing Abstract: We study the problem of testing whether an unknown $n$-variable Boolean\nfunction is a $k$-junta in the distribution-free property testing model, where\nthe distance between functions is measured with respect to an arbitrary and\nunknown probability distribution over $\\{0,1\\}^n$. Our first main result is\nthat distribution-free $k$-junta testing can be performed, with one-sided\nerror, by an adaptive algorithm that uses $\\tilde{O}(k^2)/\\epsilon$ queries\n(independent of $n$). Complementing this, our second main result is a lower\nbound showing that any non-adaptive distribution-free $k$-junta testing\nalgorithm must make $\\Omega(2^{k/3})$ queries even to test to accuracy\n$\\epsilon=1/3$. These bounds establish that while the optimal query complexity\nof non-adaptive $k$-junta testing is $2^{\\Theta(k)}$, for adaptive testing it\nis $\\text{poly}(k)$, and thus show that adaptivity provides an exponential\nimprovement in the distribution-free query complexity of testing juntas. \n\n"}
{"id": "1802.05905", "contents": "Title: Assigning times to minimise reachability in temporal graphs Abstract: Temporal graphs (in which edges are active at specified times) are of\nparticular relevance for spreading processes on graphs, e.g.~the spread of\ndisease or dissemination of information. Motivated by real-world applications,\nmodification of static graphs to control this spread has proven a rich topic\nfor previous research. Here, we introduce a new type of modification for\ntemporal graphs: the number of active times for each edge is fixed, but we can\nchange the relative order in which (sets of) edges are active. We investigate\nthe problem of determining an ordering of edges that minimises the maximum\nnumber of vertices reachable from any single starting vertex;\nepidemiologically, this corresponds to the worst-case number of vertices\ninfected in a single disease outbreak. We study two versions of this problem,\nboth of which we show to be $\\NP$-hard, and identify cases in which the problem\ncan be solved or approximated efficiently. \n\n"}
{"id": "1802.05906", "contents": "Title: Refining the $r$-index Abstract: Gagie, Navarro and Prezza's $r$-index (SODA, 2018) promises to speed up DNA\nalignment and variation calling by allowing us to index entire genomic\ndatabases, provided certain obstacles can be overcome. In this paper we first\nstrengthen and simplify Policriti and Prezza's Toehold Lemma (DCC '16;\nAlgorithmica, 2017), which inspired the $r$-index and plays an important role\nin its implementation. We then show how to update the $r$-index efficiently\nafter adding a new genome to the database, which is likely to be vital in\npractice. As a by-product of this result, we obtain an online version of\nPolicriti and Prezza's algorithm for constructing the LZ77 parse from a\nrun-length compressed Burrows-Wheeler Transform. Our experiments demonstrate\nthe practicality of all three of these results. Finally, we show how to augment\nthe $r$-index such that, given a new genome and fast random access to the\ndatabase, we can quickly compute the matching statistics and maximal exact\nmatches of the new genome with respect to the database. \n\n"}
{"id": "1802.06953", "contents": "Title: Distributed Symmetry Breaking in Sampling (Optimal Distributed Randomly\n  Coloring with Fewer Colors) Abstract: We examine the problem of almost-uniform sampling proper $q$-colorings of a\ngraph whose maximum degree is $\\Delta$. A famous result, discovered\nindependently by Jerrum(1995) and Salas and Sokal(1997), is that, assuming $q >\n(2+\\delta) \\Delta$, the Glauber dynamics (a.k.a. single-site dynamics) for this\nproblem has mixing time $O(n \\log n)$, where $n$ is the number of vertices, and\nthus provides a nearly linear time sampling algorithm for this problem. A\nnatural question is the extent to which this algorithm can be parallelized.\nPrevious work Feng, Sun and Yin [PODC'17] has shown that a $O(\\Delta \\log n)$\ntime parallelized algorithm is possible, and that $\\Omega(\\log n)$ time is\nnecessary.\n  We give a distributed sampling algorithm, which we call the Lazy Local\nMetropolis Algorithm, that achieves an optimal parallelization of this classic\nalgorithm. It improves its predecessor, the Local Metropolis algorithm of Feng,\nSun and Yin [PODC'17], by introducing a step of distributed symmetry breaking\nthat helps the mixing of the distributed sampling algorithm.\n  For sampling almost-uniform proper $q$-colorings of graphs $G$ on $n$\nvertices, we show that the Lazy Local Metropolis algorithm achieves an optimal\n$O(\\log n)$ mixing time if either of the following conditions is true for an\narbitrary constant $\\delta>0$:\n  $\\bullet$ $q\\ge(2+\\delta)\\Delta$, on general graphs with maximum degree\n$\\Delta$;\n  $\\bullet$ $q \\geq (\\alpha^* + \\delta)\\Delta$, where $\\alpha^* \\approx 1.763$\nsatisfies $\\alpha^* = \\mathrm{e}^{1/\\alpha^*}$, on graphs with sufficiently\nlarge maximum degree $\\Delta\\ge \\Delta_0(\\delta)$ and girth at least $9$. \n\n"}
{"id": "1802.06992", "contents": "Title: Sublinear Algorithms for MAXCUT and Correlation Clustering Abstract: We study sublinear algorithms for two fundamental graph problems, MAXCUT and\ncorrelation clustering. Our focus is on constructing core-sets as well as\ndeveloping streaming algorithms for these problems. Constant space algorithms\nare known for dense graphs for these problems, while $\\Omega(n)$ lower bounds\nexist (in the streaming setting) for sparse graphs.\n  Our goal in this paper is to bridge the gap between these extremes. Our first\nresult is to construct core-sets of size $\\tilde{O}(n^{1-\\delta})$ for both the\nproblems, on graphs with average degree $n^{\\delta}$ (for any $\\delta >0$).\nThis turns out to be optimal, under the exponential time hypothesis (ETH). Our\ncore-set analysis is based on studying random-induced sub-problems of\noptimization problems. To the best of our knowledge, all the known results in\nour parameter range rely crucially on near-regularity assumptions. We avoid\nthese by using a biased sampling approach, which we analyze using recent\nresults on concentration of quadratic functions. We then show that our\nconstruction yields a 2-pass streaming $(1+\\epsilon)$-approximation for both\nproblems; the algorithm uses $\\tilde{O}(n^{1-\\delta})$ space, for graphs of\naverage degree $n^\\delta$. \n\n"}
{"id": "1802.07098", "contents": "Title: Do Less, Get More: Streaming Submodular Maximization with Subsampling Abstract: In this paper, we develop the first one-pass streaming algorithm for\nsubmodular maximization that does not evaluate the entire stream even once. By\ncarefully subsampling each element of data stream, our algorithm enjoys the\ntightest approximation guarantees in various settings while having the smallest\nmemory footprint and requiring the lowest number of function evaluations. More\nspecifically, for a monotone submodular function and a $p$-matchoid constraint,\nour randomized algorithm achieves a $4p$ approximation ratio (in expectation)\nwith $O(k)$ memory and $O(km/p)$ queries per element ($k$ is the size of the\nlargest feasible solution and $m$ is the number of matroids used to define the\nconstraint). For the non-monotone case, our approximation ratio increases only\nslightly to $4p+2-o(1)$. To the best or our knowledge, our algorithm is the\nfirst that combines the benefits of streaming and subsampling in a novel way in\norder to truly scale submodular maximization to massive machine learning\nproblems. To showcase its practicality, we empirically evaluated the\nperformance of our algorithm on a video summarization application and observed\nthat it outperforms the state-of-the-art algorithm by up to fifty fold, while\nmaintaining practically the same utility. \n\n"}
{"id": "1802.07209", "contents": "Title: Distributed Symmetry-Breaking Algorithms for Congested Cliques Abstract: The {Congested Clique} is a distributed-computing model for single-hop\nnetworks with restricted bandwidth that has been very intensively studied\nrecently. It models a network by an $n$-vertex graph in which any pair of\nvertices can communicate one with another by transmitting $O(\\log n )$ bits in\neach round. Various problems have been studied in this setting, but for some of\nthem the best-known results are those for general networks. In this paper we\ndevise significantly improved algorithms for various symmetry-breaking\nproblems, such as forests-decompositions, vertex-colorings, and maximal\nindependent set.\n  We analyze the running time of our algorithms as a function of the arboricity\n$a$ of a clique subgraph that is given as input. Our algorithms are especially\nefficient in Trees, planar graphs, graphs with constant genus, and many other\ngraphs that have bounded arboricity, but unbounded size. We obtain\n$O(a)$-forest-decomposition algorithm with $O(\\log a)$ time that improves the\npreviously-known $O(\\log n)$ time, $O(a^{2 + \\epsilon})$-coloring in $O(\\log^*\nn)$ time that improves upon an $O(\\log n)$-time algorithm, $O(a)$-coloring in\n$O(a^{\\epsilon})$-time that improves upon several previous algorithms, and a\nmaximal independent set algorithm with $O(\\sqrt a)$ time that improves at least\nquadratically upon the state-of-the-art for small and moderate values of $a$.\n  Those results are achieved using several techniques. First, we produce a\nforest decomposition with a helpful structure called {$H$-partition} within\n$O(\\log a)$ rounds. In general graphs this structure requires $\\Theta(\\log n)$\ntime, but in Congested Cliques we are able to compute it faster. We employ this\nstructure in conjunction with partitioning techniques that allow us to solve\nvarious symmetry-breaking problems efficiently. \n\n"}
{"id": "1802.07375", "contents": "Title: Periodicity in Data Streams with Wildcards Abstract: We investigate the problem of detecting periodic trends within a string $S$\nof length $n$, arriving in the streaming model, containing at most $k$ wildcard\ncharacters, where $k=o(n)$. A wildcard character is a special character that\ncan be assigned any other character. We say $S$ has wildcard-period $p$ if\nthere exists an assignment to each of the wildcard characters so that in the\nresulting stream the length $n-p$ prefix equals the length $n-p$ suffix. We\npresent a two-pass streaming algorithm that computes wildcard-periods of $S$\nusing $\\mathcal{O}(k^3\\,\\mathsf{polylog}\\,n)$ bits of space, while we also show\nthat this problem cannot be solved in sublinear space in one pass. We then give\na one-pass randomized streaming algorithm that computes all wildcard-periods\n$p$ of $S$ with $p<\\frac{n}{2}$ and no wildcard characters appearing in the\nlast $p$ symbols of $S$, using $\\mathcal{O}(k^3\\mathsf{polylog}\\, n)$ space. \n\n"}
{"id": "1802.07382", "contents": "Title: Generic Coreset for Scalable Learning of Monotonic Kernels: Logistic\n  Regression, Sigmoid and more Abstract: Coreset (or core-set) is a small weighted \\emph{subset} $Q$ of an input set\n$P$ with respect to a given \\emph{monotonic} function\n$f:\\mathbb{R}\\to\\mathbb{R}$ that \\emph{provably} approximates its fitting loss\n$\\sum_{p\\in P}f(p\\cdot x)$ to \\emph{any} given $x\\in\\mathbb{R}^d$. Using $Q$ we\ncan obtain approximation of $x^*$ that minimizes this loss, by running\n\\emph{existing} optimization algorithms on $Q$. In this work we provide: (i) A\nlower bound which proves that there are sets with no coresets smaller than\n$n=|P|$ for general monotonic loss functions. (ii) A proof that, under a\nnatural assumption that holds e.g. for logistic regression and the sigmoid\nactivation functions, a small coreset exists for \\emph{any} input $P$. (iii) A\ngeneric coreset construction algorithm that computes such a small coreset $Q$\nin $O(nd+n\\log n)$ time, and (iv) Experimental results which demonstrate that\nour coresets are effective and are much smaller in practice than predicted in\ntheory. \n\n"}
{"id": "1802.07632", "contents": "Title: Spanning Tree Congestion and Computation of Generalized\n  Gy\\H{o}ri-Lov\\'{a}sz Partition Abstract: We study a natural problem in graph sparsification, the Spanning Tree\nCongestion (\\STC) problem. Informally, the \\STC problem seeks a spanning tree\nwith no tree-edge \\emph{routing} too many of the original edges. The root of\nthis problem dates back to at least 30 years ago, motivated by applications in\nnetwork design, parallel computing and circuit design. Variants of the problem\nhave also seen algorithmic applications as a preprocessing step of several\nimportant graph algorithms.\n  For any general connected graph with $n$ vertices and $m$ edges, we show that\nits STC is at most $\\mathcal{O}(\\sqrt{mn})$, which is asymptotically optimal\nsince we also demonstrate graphs with STC at least $\\Omega(\\sqrt{mn})$. We\npresent a polynomial-time algorithm which computes a spanning tree with\ncongestion $\\mathcal{O}(\\sqrt{mn}\\cdot \\log n)$. We also present another\nalgorithm for computing a spanning tree with congestion\n$\\mathcal{O}(\\sqrt{mn})$; this algorithm runs in sub-exponential time when $m =\n\\omega(n \\log^2 n)$.\n  For achieving the above results, an important intermediate theorem is\n\\emph{generalized Gy\\H{o}ri-Lov\\'{a}sz theorem}, for which Chen et al. gave a\nnon-constructive proof. We give the first elementary and constructive proof by\nproviding a local search algorithm with running time $\\mathcal{O}^*\\left( 4^n\n\\right)$, which is a key ingredient of the above-mentioned sub-exponential time\nalgorithm. We discuss a few consequences of the theorem concerning graph\npartitioning, which might be of independent interest.\n  We also show that for any graph which satisfies certain \\emph{expanding\nproperties}, its STC is at most $\\mathcal{O}(n)$, and a corresponding spanning\ntree can be computed in polynomial time. We then use this to show that a random\ngraph has STC $\\Theta(n)$ with high probability. \n\n"}
{"id": "1802.07795", "contents": "Title: Communication Complexity of One-Shot Remote State Preparation Abstract: Quantum teleportation uses prior shared entanglement and classical\ncommunication to send an unknown quantum state from one party to another.\nRemote state preparation (RSP) is a similar distributed task in which the\nsender knows the entire classical description of the state to be sent. (This\nmay also be viewed as the task of non-oblivious compression of a single sample\nfrom an ensemble of quantum states.) We study the communication complexity of\napproximate remote state preparation, in which the goal is to prepare an\napproximation of the desired quantum state. Jain [Quant. Inf. & Comp., 2006]\nshowed that the worst-case communication complexity of approximate RSP can be\nbounded from above in terms of the maximum possible information in an encoding.\nHe also showed that this quantity is a lower bound for communication complexity\nof (exact) remote state preparation. In this work, we tightly characterize the\nworst-case and average-case communication complexity of remote state\npreparation in terms of non-asymptotic information-theoretic quantities. We\nalso show that the average-case communication complexity of RSP can be much\nsmaller than the worst-case one. In the process, we show that n bits cannot be\ncommunicated with less than n transmitted bits in LOCC protocols. This\nstrengthens a result due to Nayak and Salzman [J. ACM, 2006] and may be of\nindependent interest. \n\n"}
{"id": "1802.08563", "contents": "Title: The Parameterized Hardness of the k-Center Problem in Transportation\n  Networks Abstract: In this paper we study the hardness of the $k$-Center problem on inputs that\nmodel transportation networks. For the problem, a graph $G=(V,E)$ with edge\nlengths and an integer $k$ are given and a center set $C\\subseteq V$ needs to\nbe chosen such that $|C|\\leq k$. The aim is to minimize the maximum distance of\nany vertex in the graph to the closest center. This problem arises in many\napplications of logistics, and thus it is natural to consider inputs that model\ntransportation networks. Such inputs are often assumed to be planar graphs, low\ndoubling metrics, or bounded highway dimension graphs. For each of these\nmodels, parameterized approximation algorithms have been shown to exist. We\ncomplement these results by proving that the $k$-Center problem is W[1]-hard on\nplanar graphs of constant doubling dimension, where the parameter is the\ncombination of the number of centers $k$, the highway dimension $h$, and the\npathwidth $p$. Moreover, under the Exponential Time Hypothesis there is no\n$f(k,p,h)\\cdot n^{o(p+\\sqrt{k+h})}$ time algorithm for any computable function\n$f$. Thus it is unlikely that the optimum solution to $k$-Center can be found\nefficiently, even when assuming that the input graph abides to all of the above\nmodels for transportation networks at once!\n  Additionally we give a simple parameterized $(1+\\varepsilon)$-approximation\nalgorithm for inputs of doubling dimension $d$ with runtime\n$(k^k/\\varepsilon^{O(kd)})\\cdot n^{O(1)}$. This generalizes a previous result,\nwhich considered inputs in $D$-dimensional $L_q$ metrics. \n\n"}
{"id": "1802.08637", "contents": "Title: Network Models for Multiobjective Discrete Optimization Abstract: This paper provides a novel framework for solving multiobjective discrete\noptimization problems with an arbitrary number of objectives. Our framework\nformulates these problems as network models, in that enumerating the Pareto\nfrontier amounts to solving a multicriteria shortest path problem in an\nauxiliary network. We design techniques for exploiting the network model in\norder to accelerate the identification of the Pareto frontier, most notably a\nnumber of operations to simplify the network by removing nodes and arcs while\npreserving the set of nondominated solutions. We show that the proposed\nframework yields orders-of-magnitude performance improvements over existing\nstate-of-the-art algorithms on five problem classes containing both linear and\nnonlinear objective functions. \n\n"}
{"id": "1802.08676", "contents": "Title: A Quantum-Search-Aided Dynamic Programming Framework for Pareto Optimal\n  Routing in Wireless Multihop Networks Abstract: Wireless Multihop Networks (WMHNs) have to strike a trade-off among diverse\nand often conflicting Quality-of-Service (QoS) requirements. The resultant\nsolutions may be included by the Pareto Front under the concept of Pareto\nOptimality. However, the problem of finding all the Pareto-optimal routes in\nWMHNs is classified as NP-hard, since the number of legitimate routes increases\nexponentially, as the nodes proliferate. Quantum Computing offers an attractive\nframework of rendering the Pareto-optimal routing problem tractable. In this\ncontext, a pair of quantum-assisted algorithms have been proposed, namely the\nNon-Dominated Quantum Optimization (NDQO) and the Non-Dominated Quantum\nIterative Optimization (NDQIO). However, their complexity is proportional to\n$\\sqrt{N}$, where $N$ corresponds to the total number of legitimate routes,\nthus still failing to find the solutions in \"polynomial time\". As a remedy, we\ndevise a dynamic programming framework and propose the so-called Evolutionary\nQuantum Pareto Optimization (EQPO) algorithm. We analytically characterize the\ncomplexity imposed by the EQPO algorithm and demonstrate that it succeeds in\nsolving the Pareto-optimal routing problem in polynomial time. Finally, we\ndemonstrate by simulations that the EQPO algorithm achieves a complexity\nreduction, which is at least an order of magnitude, when compared to its\npredecessors, albeit at the cost of a modest heuristic accuracy reduction. \n\n"}
{"id": "1802.09007", "contents": "Title: Evaluating and Tuning n-fold Integer Programming Abstract: In recent years, algorithmic breakthroughs in stringology, computational\nsocial choice, scheduling, etc., were achieved by applying the theory of\nso-called $n$-fold integer programming. An $n$-fold integer program (IP) has a\nhighly uniform block structured constraint matrix. Hemmecke, Onn, and Romanchuk\n[Math. Programming, 2013] showed an algorithm with runtime $a^{O(rst + r^2s)}\nn^3$, where $a$ is the largest coefficient, $r,s$, and $t$ are dimensions of\nblocks of the constraint matrix and $n$ is the total dimension of the IP; thus,\nan algorithm efficient if the blocks are of small size and with small\ncoefficients. The algorithm works by iteratively improving a feasible solution\nwith augmenting steps, and $n$-fold IPs have the special property that\naugmenting steps are guaranteed to exist in a not-too-large neighborhood.\n  We have implemented the algorithm and learned the following along the way.\nThe original algorithm is practically unusable, but we discover a series of\nimprovements which make its evaluation possible. Crucially, we observe that a\ncertain constant in the algorithm can be treated as a tuning parameter, which\nyields an efficient heuristic (essentially searching in a\nsmaller-than-guaranteed neighborhood). Furthermore, the algorithm uses an\noverly expensive strategy to find a \"best\" step, while finding only an\n\"approximatelly best\" step is much cheaper, yet sufficient for quick\nconvergence. Using this insight, we improve the asymptotic dependence on $n$\nfrom $n^3$ to $n^2 \\log n$.\n  We show that decreasing the tuning parameter initially leads to an increased\nnumber of iterations needed for convergence and eventually to getting stuck in\nlocal optima, as expected. However, surprisingly small values of the parameter\nalready exhibit good behavior. Second, our new strategy for finding\n\"approximatelly best\" steps wildly outperforms the original construction. \n\n"}
{"id": "1802.09121", "contents": "Title: Limits on representing Boolean functions by linear combinations of\n  simple functions: thresholds, ReLUs, and low-degree polynomials Abstract: We consider the problem of representing Boolean functions exactly by \"sparse\"\nlinear combinations (over $\\mathbb{R}$) of functions from some \"simple\" class\n${\\cal C}$. In particular, given ${\\cal C}$ we are interested in finding\nlow-complexity functions lacking sparse representations. When ${\\cal C}$ is the\nset of PARITY functions or the set of conjunctions, this sort of problem has a\nwell-understood answer, the problem becomes interesting when ${\\cal C}$ is\n\"overcomplete\" and the set of functions is not linearly independent. We focus\non the cases where ${\\cal C}$ is the set of linear threshold functions, the set\nof rectified linear units (ReLUs), and the set of low-degree polynomials over a\nfinite field, all of which are well-studied in different contexts.\n  We provide generic tools for proving lower bounds on representations of this\nkind. Applying these, we give several new lower bounds for \"semi-explicit\"\nBoolean functions. For example, we show there are functions in nondeterministic\nquasi-polynomial time that require super-polynomial size:\n  $\\bullet$ Depth-two neural networks with sign activation function, a special\ncase of depth-two threshold circuit lower bounds.\n  $\\bullet$ Depth-two neural networks with ReLU activation function.\n  $\\bullet$ $\\mathbb{R}$-linear combinations of $O(1)$-degree\n$\\mathbb{F}_p$-polynomials, for every prime $p$ (related to problems regarding\nHigher-Order \"Uncertainty Principles\"). We also obtain a function in $E^{NP}$\nrequiring $2^{\\Omega(n)}$ linear combinations.\n  $\\bullet$ $\\mathbb{R}$-linear combinations of $ACC \\circ THR$ circuits of\npolynomial size (further generalizing the recent lower bounds of Murray and the\nauthor).\n  (The above is a shortened abstract. For the full abstract, see the paper.) \n\n"}
{"id": "1802.09610", "contents": "Title: Aggregative Coarsening for Multilevel Hypergraph Partitioning Abstract: Algorithms for many hypergraph problems, including partitioning, utilize\nmultilevel frameworks to achieve a good trade-off between the performance and\nthe quality of results. In this paper we introduce two novel aggregative\ncoarsening schemes and incorporate them within state-of-the-art hypergraph\npartitioner Zoltan. Our coarsening schemes are inspired by the algebraic\nmultigrid and stable matching approaches. We demonstrate the effectiveness of\nthe developed schemes as a part of multilevel hypergraph partitioning framework\non a wide range of problems. \n\n"}
{"id": "1802.10048", "contents": "Title: Parameterized Complexity of Diameter Abstract: Diameter -- the task of computing the length of a longest shortest path -- is\na fundamental graph problem. Assuming the Strong Exponential Time Hypothesis,\nthere is no $O(n^{1.99})$-time algorithm even in sparse graphs [Roditty and\nWilliams, 2013]. To circumvent this lower bound we aim for algorithms with\nrunning time $f(k)(n+m)$ where $k$ is a parameter and $f$ is a function as\nsmall as possible. We investigate which parameters allow for such running\ntimes. To this end, we systematically explore a hierarchy of structural graph\nparameters. \n\n"}
{"id": "1803.00796", "contents": "Title: Fine-Grained Complexity of Analyzing Compressed Data: Quantifying\n  Improvements over Decompress-And-Solve Abstract: Can we analyze data without decompressing it? As our data keeps growing,\nunderstanding the time complexity of problems on compressed inputs, rather than\nin convenient uncompressed forms, becomes more and more relevant. Suppose we\nare given a compression of size $n$ of data that originally has size $N$, and\nwe want to solve a problem with time complexity $T(\\cdot)$. The naive strategy\nof \"decompress-and-solve\" gives time $T(N)$, whereas \"the gold standard\" is\ntime $T(n)$: to analyze the compression as efficiently as if the original data\nwas small.\n  We restrict our attention to data in the form of a string (text, files,\ngenomes, etc.) and study the most ubiquitous tasks. While the challenge might\nseem to depend heavily on the specific compression scheme, most methods of\npractical relevance (Lempel-Ziv-family, dictionary methods, and others) can be\nunified under the elegant notion of Grammar Compressions. A vast literature,\nacross many disciplines, established this as an influential notion for\nAlgorithm design.\n  We introduce a framework for proving (conditional) lower bounds in this\nfield, allowing us to assess whether decompress-and-solve can be improved, and\nby how much. Our main results are:\n  - The $O(nN\\sqrt{\\log{N/n}})$ bound for LCS and the $O(\\min\\{N \\log N, nM\\})$\nbound for Pattern Matching with Wildcards are optimal up to $N^{o(1)}$ factors,\nunder the Strong Exponential Time Hypothesis. (Here, $M$ denotes the\nuncompressed length of the compressed pattern.)\n  - Decompress-and-solve is essentially optimal for Context-Free Grammar\nParsing and RNA Folding, under the $k$-Clique conjecture.\n  - We give an algorithm showing that decompress-and-solve is not optimal for\nDisjointness. \n\n"}
{"id": "1803.00904", "contents": "Title: Hardness of Approximate Nearest Neighbor Search Abstract: We prove conditional near-quadratic running time lower bounds for approximate\nBichromatic Closest Pair with Euclidean, Manhattan, Hamming, or edit distance.\nSpecifically, unless the Strong Exponential Time Hypothesis (SETH) is false,\nfor every $\\delta>0$ there exists a constant $\\epsilon>0$ such that computing a\n$(1+\\epsilon)$-approximation to the Bichromatic Closest Pair requires\n$n^{2-\\delta}$ time. In particular, this implies a near-linear query time for\nApproximate Nearest Neighbor search with polynomial preprocessing time.\n  Our reduction uses the Distributed PCP framework of [ARW'17], but obtains\nimproved efficiency using Algebraic Geometry (AG) codes. Efficient PCPs from AG\ncodes have been constructed in other settings before [BKKMS'16, BCGRS'17], but\nour construction is the first to yield new hardness results. \n\n"}
{"id": "1803.02270", "contents": "Title: Revisiting Frequency Moment Estimation in Random Order Streams Abstract: We revisit one of the classic problems in the data stream literature, namely,\nthat of estimating the frequency moments $F_p$ for $0 < p < 2$ of an underlying\n$n$-dimensional vector presented as a sequence of additive updates in a stream.\nIt is well-known that using $p$-stable distributions one can approximate any of\nthese moments up to a multiplicative $(1+\\epsilon)$-factor using\n$O(\\epsilon^{-2} \\log n)$ bits of space, and this space bound is optimal up to\na constant factor in the turnstile streaming model. We show that surprisingly,\nif one instead considers the popular random-order model of insertion-only\nstreams, in which the updates to the underlying vector arrive in a random\norder, then one can beat this space bound and achieve $\\tilde{O}(\\epsilon^{-2}\n+ \\log n)$ bits of space, where the $\\tilde{O}$ hides poly$(\\log(1/\\epsilon) +\n\\log \\log n)$ factors. If $\\epsilon^{-2} \\approx \\log n$, this represents a\nroughly quadratic improvement in the space achievable in turnstile streams. Our\nalgorithm is in fact deterministic, and we show our space bound is optimal up\nto poly$(\\log(1/\\epsilon) + \\log \\log n)$ factors for deterministic algorithms\nin the random order model. We also obtain a similar improvement in space for $p\n= 2$ whenever $F_2 \\gtrsim \\log n\\cdot F_1$. \n\n"}
{"id": "1803.03239", "contents": "Title: Fairness Through Computationally-Bounded Awareness Abstract: We study the problem of fair classification within the versatile framework of\nDwork et al. [ITCS '12], which assumes the existence of a metric that measures\nsimilarity between pairs of individuals. Unlike earlier work, we do not assume\nthat the entire metric is known to the learning algorithm; instead, the learner\ncan query this arbitrary metric a bounded number of times. We propose a new\nnotion of fairness called metric multifairness and show how to achieve this\nnotion in our setting. Metric multifairness is parameterized by a similarity\nmetric $d$ on pairs of individuals to classify and a rich collection ${\\cal C}$\nof (possibly overlapping) \"comparison sets\" over pairs of individuals. At a\nhigh level, metric multifairness guarantees that similar subpopulations are\ntreated similarly, as long as these subpopulations are identified within the\nclass ${\\cal C}$. \n\n"}
{"id": "1803.03242", "contents": "Title: Probably Approximately Metric-Fair Learning Abstract: The seminal work of Dwork {\\em et al.} [ITCS 2012] introduced a metric-based\nnotion of individual fairness. Given a task-specific similarity metric, their\nnotion required that every pair of similar individuals should be treated\nsimilarly. In the context of machine learning, however, individual fairness\ndoes not generalize from a training set to the underlying population. We show\nthat this can lead to computational intractability even for simple\nfair-learning tasks.\n  With this motivation in mind, we introduce and study a relaxed notion of {\\em\napproximate metric-fairness}: for a random pair of individuals sampled from the\npopulation, with all but a small probability of error, if they are similar then\nthey should be treated similarly. We formalize the goal of achieving\napproximate metric-fairness simultaneously with best-possible accuracy as\nProbably Approximately Correct and Fair (PACF) Learning. We show that\napproximate metric-fairness {\\em does} generalize, and leverage these\ngeneralization guarantees to construct polynomial-time PACF learning algorithms\nfor the classes of linear and logistic predictors. \n\n"}
{"id": "1803.03839", "contents": "Title: Efficient Enumeration of Bipartite Subgraphs in Graphs Abstract: Subgraph enumeration problems ask to output all subgraphs of an input graph\nthat belongs to the specified graph class or satisfy the given constraint.\nThese problems have been widely studied in theoretical computer science. As\nfar, many efficient enumeration algorithms for the fundamental substructures\nsuch as spanning trees, cycles, and paths, have been developed. This paper\naddresses the enumeration problem of bipartite subgraphs. Even though bipartite\ngraphs are quite fundamental and have numerous applications in both theory and\napplication, its enumeration algorithms have not been intensively studied, to\nthe best of our knowledge. We propose the first non-trivial algorithms for\nenumerating all bipartite subgraphs in a given graph. As the main results, we\ndevelop two efficient algorithms: the one enumerates all bipartite induced\nsubgraphs of a graph with degeneracy $k$ in $O(k)$ time per solution. The other\nenumerates all bipartite subgraphs in $O(1)$ time per solution. \n\n"}
{"id": "1803.03922", "contents": "Title: Scalable Breadth-First Search on a GPU Cluster Abstract: On a GPU cluster, the ratio of high computing power to communication\nbandwidth makes scaling breadth-first search (BFS) on a scale-free graph\nextremely challenging. By separating high and low out-degree vertices, we\npresent an implementation with scalable computation and a model for scalable\ncommunication for BFS and direction-optimized BFS. Our communication model uses\nglobal reduction for high-degree vertices, and point-to-point transmission for\nlow-degree vertices. Leveraging the characteristics of degree separation, we\nreduce the graph size to one third of the conventional edge list\nrepresentation. With several other optimizations, we observe linear weak\nscaling as we increase the number of GPUs, and achieve 259.8 GTEPS on a\nscale-33 Graph500 RMAT graph with 124 GPUs on the latest CORAL early access\nsystem. \n\n"}
{"id": "1803.04282", "contents": "Title: Linear-Time In-Place DFS and BFS on the Word RAM Abstract: We present an in-place depth first search (DFS) and an in-place breadth first\nsearch (BFS) that runs on a word RAM in linear time such that, if the adjacency\narrays of the input graph are given in a sorted order, the input is restored\nafter running the algorithm. To obtain our results we use properties of the\nrepresentation used to store the given graph and show several linear-time\nin-place graph transformations from one representation into another. \n\n"}
{"id": "1803.04292", "contents": "Title: Geodabs: Trajectory Indexing Meets Fingerprinting at Scale Abstract: Finding trajectories and discovering motifs that are similar in large\ndatasets is a central problem for a wide range of applications. Solutions\naddressing this problem usually rely on spatial indexing and on the computation\nof a similarity measure in polynomial time. Although effective in the context\nof sparse trajectory datasets, this approach is too expensive in the context of\ndense datasets, where many trajectories potentially match with a given query.\nIn this paper, we apply fingerprinting, a copy-detection mechanism used in the\ncontext of textual data, to trajectories. To this end, we fingerprint\ntrajectories with geodabs, a construction based on geohash aimed at trajectory\nfingerprinting. We demonstrate that by relying on the properties of a space\nfilling curve geodabs can be used to build sharded inverted indexes. We show\nhow normalization affects precision and recall, two key measures in information\nretrieval. We then demonstrate that the probabilistic nature of fingerprinting\nhas a marginal effect on the quality of the results. Finally, we evaluate our\nmethod in terms of performances and show that, in contrast with existing\nmethods, it is not affected by the density of the trajectory dataset and that\nit can be efficiently distributed. \n\n"}
{"id": "1803.04744", "contents": "Title: On Integer Programming, Discrepancy, and Convolution Abstract: Integer programs with m constraints are solvable in pseudo-polynomial time in\n$\\Delta$, the largest coefficient in a constraint, when m is a fixed constant.\nWe give a new algorithm with a running time of $O(\\sqrt{m}\\Delta)^{2m} +\nO(nm)$, which improves on the state-of-the-art. Moreover, we show that\nimproving on our algorithm for any $m$ is equivalent to improving over the\nquadratic time algorithm for $(\\min,~+)$-convolution. This is a strong evidence\nthat our algorithm's running time is the best possible. We also present a\nspecialized algorithm with running time $O(\\sqrt{m} \\Delta)^{(1 + o(1))m} +\nO(nm)$ for testing feasibility of an integer program and also give a tight\nlower bound, which is based on the SETH in this case. \n\n"}
{"id": "1803.05718", "contents": "Title: Approximating Max-Cut under Graph-MSO Constraints Abstract: We consider the max-cut and max-$k$-cut problems under graph-based\nconstraints. Our approach can handle any constraint specified using monadic\nsecond-order (MSO) logic on graphs of constant treewidth. We give a\n$\\frac{1}{2}$-approximation algorithm for this class of problems. \n\n"}
{"id": "1803.05825", "contents": "Title: A Generalized Matching Reconfiguration Problem Abstract: The goal in {\\em reconfiguration problems} is to compute a {\\em gradual\ntransformation} between two feasible solutions of a problem such that all\nintermediate solutions are also feasible. In the {\\em Matching Reconfiguration\nProblem} (MRP), proposed in a pioneering work by Ito et al.\\ from 2008, we are\ngiven a graph $G$ and two matchings $M$ and $M'$, and we are asked whether\nthere is a sequence of matchings in $G$ starting with $M$ and ending at $M'$,\neach resulting from the previous one by either adding or deleting a single edge\nin $G$, without ever going through a matching of size $< \\min\\{|M|,|M'|\\}-1$.\nIto et al.\\ gave a polynomial time algorithm for the problem.\n  In this paper we introduce a natural generalization of the MRP that depends\non an integer parameter $\\Delta \\ge 1$: here we are allowed to make $\\Delta$\nchanges to the current solution rather than 1 at each step of the\n{transformation procedure}. There is always a valid sequence of matchings\ntransforming $M$ to $M'$ if $\\Delta$ is sufficiently large, and naturally we\nwould like to minimize $\\Delta$. We first devise an optimal transformation\nprocedure for unweighted matching with $\\Delta = 3$, and then extend it to\nweighted matchings to achieve asymptotically optimal guarantees. The running\ntime of these procedures is linear.\n  We further demonstrate the applicability of this generalized problem to\ndynamic graph matchings. In this area, the number of changes to the maintained\nmatching per update step (the \\emph{recourse bound}) is an important quality\nmeasure. Nevertheless, the \\emph{worst-case} recourse bounds of almost all\nknown dynamic matching algorithms are prohibitively large, much larger than the\ncorresponding update times. We fill in this gap via a surprisingly simple\nblack-box reduction: Any dynamic algorithm for maintaining [...] \n\n"}
{"id": "1803.05948", "contents": "Title: Average Cost of QuickXsort with Pivot Sampling Abstract: QuickXsort is a strategy to combine Quicksort with another sorting method X,\nso that the result has essentially the same comparison cost as X in isolation,\nbut sorts in place even when X requires a linear-size buffer. We solve the\nrecurrence for QuickXsort precisely up to the linear term including the\noptimization to choose pivots from a sample of k elements. This allows to\nimmediately obtain overall average costs using only the average costs of\nsorting method X (as if run in isolation). We thereby extend and greatly\nsimplify the analysis of QuickHeapsort and QuickMergesort with practically\nefficient pivot selection, and give the first tight upper bounds including the\nlinear term for such methods. \n\n"}
{"id": "1803.06521", "contents": "Title: Beyond the Low-Degree Algorithm: Mixtures of Subcubes and Their\n  Applications Abstract: We introduce the problem of learning mixtures of $k$ subcubes over\n$\\{0,1\\}^n$, which contains many classic learning theory problems as a special\ncase (and is itself a special case of others). We give a surprising $n^{O(\\log\nk)}$-time learning algorithm based on higher-order multilinear moments. It is\nnot possible to learn the parameters because the same distribution can be\nrepresented by quite different models. Instead, we develop a framework for\nreasoning about how multilinear moments can pinpoint essential features of the\nmixture, like the number of components.\n  We also give applications of our algorithm to learning decision trees with\nstochastic transitions (which also capture interesting scenarios where the\ntransitions are deterministic but there are latent variables). Using our\nalgorithm for learning mixtures of subcubes, we can approximate the Bayes\noptimal classifier within additive error $\\epsilon$ on $k$-leaf decision trees\nwith at most $s$ stochastic transitions on any root-to-leaf path in $n^{O(s +\n\\log k)}\\cdot\\text{poly}(1/\\epsilon)$ time. In this stochastic setting, the\nclassic Occam algorithms for learning decision trees with zero stochastic\ntransitions break down, while the low-degree algorithm of Linial et al.\ninherently has a quasipolynomial dependence on $1/\\epsilon$.\n  In contrast, as we will show, mixtures of $k$ subcubes are uniquely\ndetermined by their degree $2 \\log k$ moments and hence provide a useful\nabstraction for simultaneously achieving the polynomial dependence on\n$1/\\epsilon$ of the classic Occam algorithms for decision trees and the\nflexibility of the low-degree algorithm in being able to accommodate stochastic\ntransitions. Using our multilinear moment techniques, we also give the first\nimproved upper and lower bounds since the work of Feldman et al. for the\nrelated but harder problem of learning mixtures of binary product\ndistributions. \n\n"}
{"id": "1803.06878", "contents": "Title: Parameterized Complexity of Fair Vertex Evaluation Problems Abstract: A prototypical graph problem is centered around a graph-theoretic property\nfor a set of vertices and a solution to it is a set of vertices for which the\ndesired property holds. The task is to decide whether, in the given graph,\nthere exists a solution of a certain quality, where we use size as a quality\nmeasure. In this work, we are changing the measure to the fair measure\n[Lin&Sahni: Fair edge deletion problems. IEEE Trans. Comput. 89]. The measure\nis k if the number of solution neighbors does not exceed k for any vertex in\nthe graph. One possible way to study graph problems is by defining the property\nin a certain logic. For a given objective an evaluation problem is to find a\nset (of vertices) that simultaneously minimizes the assumed measure and\nsatisfies an appropriate formula.\n  In the presented paper we show that there is an FPT algorithm for the MSO\nFair Vertex Evaluation problem for formulas with one free variable\nparameterized by the twin cover number of the input graph. Here, the free\nvariable corresponds to the solution sought. One may define an extended variant\nof MSO Fair Vertex Evaluation for formulas with l free variables; here we\nmeasure a maximum number of neighbors in each of the l sets. However, such\nvariant is W[1]-hard for parameter l even on graphs with twin cover one.\nFurthermore, we study the Fair Vertex Cover (Fair VC) problem. Fair VC is among\nthe simplest problems with respect to the demanded property (i.e., the rest\nforms an edgeless graph). On the negative side, Fair VC is W[1]-hard when\nparameterized by both treedepth and feedback vertex set of the input graph. On\nthe positive side, we provide an FPT algorithm for the parameter modular width. \n\n"}
{"id": "1803.08621", "contents": "Title: Parallel Range, Segment and Rectangle Queries with Augmented Maps Abstract: The range, segment and rectangle query problems are fundamental problems in\ncomputational geometry, and have extensive applications in many domains.\nDespite the significant theoretical work on these problems, efficient\nimplementations can be complicated. We know of very few practical\nimplementations of the algorithms in parallel, and most implementations do not\nhave tight theoretical bounds. We focus on simple and efficient parallel\nalgorithms and implementations for these queries, which have tight worst-case\nbound in theory and good parallel performance in practice. We propose to use a\nsimple framework (the augmented map) to model the problem. Based on the\naugmented map interface, we develop both multi-level tree structures and\nsweepline algorithms supporting range, segment and rectangle queries in two\ndimensions. For the sweepline algorithms, we propose a parallel paradigm and\nshow corresponding cost bounds. All of our data structures are work-efficient\nto build in theory and achieve a low parallel depth. The query time is almost\nlinear to the output size.\n  We have implemented all the data structures described in the paper using a\nparallel augmented map library. Based on the library each data structure only\nrequires about 100 lines of C++ code. We test their performance on large data\nsets (up to $10^8$ elements) and a machine with 72-cores (144 hyperthreads).\nThe parallel construction achieves 32-68x speedup. Speedup numbers on queries\nare up to 126-fold. Our sequential implementation outperforms the CGAL library\nby at least 2x in both construction and queries. Our sequential implementation\ncan be slightly slower than the R-tree in the Boost library in some cases\n(0.6-2.5x), but has significantly better query performance (1.6-1400x) than\nBoost. \n\n"}
{"id": "1803.09370", "contents": "Title: Popular Matching in Roommates Setting is NP-hard Abstract: An input to the Popular Matching problem, in the roommates setting, consists\nof a graph $G$ and each vertex ranks its neighbors in strict order, known as\nits preference. In the Popular Matching problem the objective is to test\nwhether there exists a matching $M^\\star$ such that there is no matching $M$\nwhere more people are happier with $M$ than with $M^\\star$. In this paper we\nsettle the computational complexity of the Popular Matching problem in the\nroommates setting by showing that the problem is NP-complete. Thus, we resolve\nan open question that has been repeatedly, explicitly asked over the last\ndecade. \n\n"}
{"id": "1803.10184", "contents": "Title: A New Optimal Algorithm for Computing the Visibility Area of a simple\n  Polygon from a Viewpoint Abstract: Given a simple polygon $ \\mathcal {P} $ of $ n $ vertices in the Plane. We\nstudy the problem of computing the visibility area from a given viewpoint $ q $\ninside $ \\mathcal {P} $ where only sub-linear variables are allowed for working\nspace. Without any memory-constrained, this problem was previously solved in $\nO(n) $-time and $ O(n) $-variables space. In a newer research, the visibility\narea of a point be computed in $ O(n) $-time, using $ O(\\sqrt{n}) $ variables\nfor working space. In this paper, we present an optimal-time algorithm, using $\nO(c/\\log n) $ variables space for computing visibility area, where $ c<n $ is\nthe number of critical vertices. We keep the algorithm in the linear-time and\nreduce space as much as possible. \n\n"}
{"id": "1803.10366", "contents": "Title: Smoothed Online Convex Optimization in High Dimensions via Online\n  Balanced Descent Abstract: We study Smoothed Online Convex Optimization, a version of online convex\noptimization where the learner incurs a penalty for changing her actions\nbetween rounds. Given a $\\Omega(\\sqrt{d})$ lower bound on the competitive ratio\nof any online algorithm, where $d$ is the dimension of the action space, we ask\nunder what conditions this bound can be beaten. We introduce a novel\nalgorithmic framework for this problem, Online Balanced Descent (OBD), which\nworks by iteratively projecting the previous point onto a carefully chosen\nlevel set of the current cost function so as to balance the switching costs and\nhitting costs. We demonstrate the generality of the OBD framework by showing\nhow, with different choices of \"balance,\" OBD can improve upon state-of-the-art\nperformance guarantees for both competitive ratio and regret, in particular,\nOBD is the first algorithm to achieve a dimension-free competitive ratio, $3 +\nO(1/\\alpha)$, for locally polyhedral costs, where $\\alpha$ measures the\n\"steepness\" of the costs. We also prove bounds on the dynamic regret of OBD\nwhen the balance is performed in the dual space that are dimension-free and\nimply that OBD has sublinear static regret. \n\n"}
{"id": "1803.11132", "contents": "Title: Notes on computational-to-statistical gaps: predictions using\n  statistical physics Abstract: In these notes we describe heuristics to predict computational-to-statistical\ngaps in certain statistical problems. These are regimes in which the underlying\nstatistical problem is information-theoretically possible although no efficient\nalgorithm exists, rendering the problem essentially unsolvable for large\ninstances. The methods we describe here are based on mature, albeit\nnon-rigorous, tools from statistical physics.\n  These notes are based on a lecture series given by the authors at the Courant\nInstitute of Mathematical Sciences in New York City, on May 16th, 2017. \n\n"}
{"id": "1804.01366", "contents": "Title: Losing Treewidth by Separating Subsets Abstract: We study the problem of deleting the smallest set $S$ of vertices (resp.\nedges) from a given graph $G$ such that the induced subgraph (resp. subgraph)\n$G \\setminus S$ belongs to some class $\\mathcal{H}$. We consider the case where\ngraphs in $\\mathcal{H}$ have treewidth bounded by $t$, and give a general\nframework to obtain approximation algorithms for both vertex and edge-deletion\nsettings from approximation algorithms for certain natural graph partitioning\nproblems called $k$-Subset Vertex Separator and $k$-Subset Edge Separator,\nrespectively.\n  For the vertex deletion setting, our framework combined with the current best\nresult for $k$-Subset Vertex Separator, yields a significant improvement in the\napproximation ratios for basic problems such as $k$-Treewidth Vertex Deletion\nand Planar-$F$ Vertex Deletion. Our algorithms are simpler than previous works\nand give the first uniform approximation algorithms under the natural\nparameterization.\n  For the edge deletion setting, we give improved approximation algorithms for\n$k$-Subset Edge Separator combining ideas from LP relaxations and important\nseparators. We present their applications in bounded-degree graphs, and also\ngive an APX-hardness result for the edge deletion problems. \n\n"}
{"id": "1804.02273", "contents": "Title: BFS Enumeration for Breaking Symmetries in Graphs Abstract: There are numerous NP-hard combinatorial problems which involve searching for\nan undirected graph satisfying a certain property. One way to solve such\nproblems is to translate a problem into an instance of the boolean\nsatisfiability (SAT) or constraint satisfaction (CSP) problem. Such reduction\nusually can give rise to numerous isomorphic representations of the same graph.\nOne way to reduce the search space and speed up the search under these\nconditions is to introduce symmetrybreaking predicates. In this paper we\nintroduce three novel and practically effective symmetry-breaking predicates\nfor an undirected connected graph search based on breadth-first search (BFS)\nenumeration and compare with existing symmetry-breaking methods on several\ngraph problems. \n\n"}
{"id": "1804.02785", "contents": "Title: Maximizing the Number of Spanning Trees in a Connected Graph Abstract: We study the problem of maximizing the number of spanning trees in a\nconnected graph by adding at most $k$ edges from a given candidate edge set. We\ngive both algorithmic and hardness results for this problem:\n  - We give a greedy algorithm that, using submodularity, obtains an\napproximation ratio of $(1 - 1/e - \\epsilon)$ in the exponent of the number of\nspanning trees for any $\\epsilon > 0$ in time $\\tilde{O}(m \\epsilon^{-1} + (n +\nq) \\epsilon^{-3})$, where $m$ and $q$ is the number of edges in the original\ngraph and the candidate edge set, respectively. Our running time is optimal\nwith respect to the input size up to logarithmic factors, and substantially\nimproves upon the $O(n^3)$ running time of the previous proposed greedy\nalgorithm with approximation ratio $(1 - 1/e)$ in the exponent. Notably, the\nindependence of our running time of $k$ is novel, comparing to conventional\ntop-$k$ selections on graphs that usually run in $\\Omega(mk)$ time. A key\ningredient of our greedy algorithm is a routine for maintaining effective\nresistances under edge additions in an online-offline hybrid setting.\n  - We show the exponential inapproximability of this problem by proving that\nthere exists a constant $c > 0$ such that it is NP-hard to approximate the\noptimum number of spanning trees in the exponent within $(1 - c)$. This\ninapproximability result follows from a reduction from the minimum path cover\nin undirected graphs, whose hardness again follows from the constant\ninapproximability of the Traveling Salesman Problem (TSP) with distances 1 and\n2. Thus, the approximation ratio of our algorithm is also optimal up to a\nconstant factor in the exponent. To our knowledge, this is the first hardness\nof approximation result for maximizing the number of spanning trees in a graph,\nor equivalently, by Kirchhoff's matrix-tree theorem, maximizing the determinant\nof an SDDM matrix. \n\n"}
{"id": "1804.04038", "contents": "Title: Fully Dynamic Effective Resistances Abstract: In this paper we consider the \\emph{fully-dynamic} All-Pairs Effective\nResistance problem, where the goal is to maintain effective resistances on a\ngraph $G$ among any pair of query vertices under an intermixed sequence of edge\ninsertions and deletions in $G$. The effective resistance between a pair of\nvertices is a physics-motivated quantity that encapsulates both the congestion\nand the dilation of a flow. It is directly related to random walks, and it has\nbeen instrumental in the recent works for designing fast algorithms for\ncombinatorial optimization problems, graph sparsification, and network science.\n  We give a data-structure that maintains $(1+\\epsilon)$-approximations to\nall-pair effective resistances of a fully-dynamic unweighted, undirected\nmulti-graph $G$ with $\\tilde{O}(m^{4/5}\\epsilon^{-4})$ expected amortized\nupdate and query time, against an oblivious adversary. Key to our result is the\nmaintenance of a dynamic \\emph{Schur complement}~(also known as vertex\nresistance sparsifier) onto a set of terminal vertices of our choice.\n  This maintenance is obtained (1) by interpreting the Schur complement as a\nsum of random walks and (2) by randomly picking the vertex subset into which\nthe sparsifier is constructed. We can then show that each update in the graph\naffects a small number of such walks, which in turn leads to our sub-linear\nupdate time. We believe that this local representation of vertex sparsifiers\nmay be of independent interest. \n\n"}
{"id": "1804.04263", "contents": "Title: Dualities in Tree Representations Abstract: A characterization of the tree $T^*$ such that\n$\\mathrm{BP}(T^*)=\\overleftrightarrow{\\mathrm{DFUDS}(T)}$, the reversal of\n$\\mathrm{DFUDS}(T)$ is given. An immediate consequence is a rigorous\ncharacterization of the tree $\\hat{T}$ such that\n$\\mathrm{BP}(\\hat{T})=\\mathrm{DFUDS}(T)$. In summary, $\\mathrm{BP}$ and\n$\\mathrm{DFUDS}$ are unified within an encompassing framework, which might have\nthe potential to imply future simplifications with regard to queries in\n$\\mathrm{BP}$ and/or $\\mathrm{DFUDS}$. Immediate benefits displayed here are to\nidentify so far unnoted commonalities in most recent work on the Range Minimum\nQuery problem, and to provide improvements for the Minimum Length Interval\nQuery problem. \n\n"}
{"id": "1804.05097", "contents": "Title: Design and Implementation of Dynamic Memory Management in a Reversible\n  Object-Oriented Programming Language Abstract: The reversible object-oriented programming language (ROOPL) was presented in\nlate 2016 and proved that object-oriented programming paradigms works in the\nreversible setting. The language featured simple statically scoped objects\nwhich made non-trivial programs tedious, if not impossible to write using the\nlimited tools provided. We introduce an extension to ROOPL in form the new\nlanguage ROOPL++, featuring dynamic memory management and fixed-sized arrays\nfor increased language expressiveness. The language is a superset of ROOPL and\nhas formally been defined by its language semantics, type system and\ncomputational universality. Considerations for reversible memory manager\nlayouts are discussed and ultimately lead to the selection of the Buddy Memory\nlayout. Translations of the extensions added in ROOPL++ to the reversible\nassembly language PISA are presented to provide garbage-free computations. The\ndynamic memory management extension successfully increases the expressiveness\nof ROOPL and as a result, shows that non-trivial reversible data structures,\nsuch as binary trees and doubly-linked lists, are feasible and do not\ncontradict the reversible computing paradigm. \n\n"}
{"id": "1804.05436", "contents": "Title: Hidden Hamiltonian Cycle Recovery via Linear Programming Abstract: We introduce the problem of hidden Hamiltonian cycle recovery, where there is\nan unknown Hamiltonian cycle in an $n$-vertex complete graph that needs to be\ninferred from noisy edge measurements. The measurements are independent and\ndistributed according to $\\calP_n$ for edges in the cycle and $\\calQ_n$\notherwise. This formulation is motivated by a problem in genome assembly, where\nthe goal is to order a set of contigs (genome subsequences) according to their\npositions on the genome using long-range linking measurements between the\ncontigs. Computing the maximum likelihood estimate in this model reduces to a\nTraveling Salesman Problem (TSP). Despite the NP-hardness of TSP, we show that\na simple linear programming (LP) relaxation, namely the fractional $2$-factor\n(F2F) LP, recovers the hidden Hamiltonian cycle with high probability as $n \\to\n\\infty$ provided that $\\alpha_n - \\log n \\to \\infty$, where $\\alpha_n\n\\triangleq -2 \\log \\int \\sqrt{d P_n d Q_n}$ is the R\\'enyi divergence of order\n$\\frac{1}{2}$. This condition is information-theoretically optimal in the sense\nthat, under mild distributional assumptions, $\\alpha_n \\geq (1+o(1)) \\log n$ is\nnecessary for any algorithm to succeed regardless of the computational cost.\n  Departing from the usual proof techniques based on dual witness construction,\nthe analysis relies on the combinatorial characterization (in particular, the\nhalf-integrality) of the extreme points of the F2F polytope. Represented as\nbicolored multi-graphs, these extreme points are further decomposed into\nsimpler \"blossom-type\" structures for the large deviation analysis and counting\narguments. Evaluation of the algorithm on real data shows improvements over\nexisting approaches. \n\n"}
{"id": "1804.06355", "contents": "Title: An Exponential Speedup in Parallel Running Time for Submodular\n  Maximization without Loss in Approximation Abstract: In this paper we study the adaptivity of submodular maximization. Adaptivity\nquantifies the number of sequential rounds that an algorithm makes when\nfunction evaluations can be executed in parallel. Adaptivity is a fundamental\nconcept that is heavily studied across a variety of areas in computer science,\nlargely due to the need for parallelizing computation. For the canonical\nproblem of maximizing a monotone submodular function under a cardinality\nconstraint, it is well known that a simple greedy algorithm achieves a $1-1/e$\napproximation and that this approximation is optimal for polynomial-time\nalgorithms. Somewhat surprisingly, despite extensive efforts on submodular\noptimization for large-scale datasets, until very recently there was no known\nalgorithm that achieves a constant factor approximation for this problem whose\nadaptivity is sublinear in the size of the ground set $n$.\n  Recent work by Balkanski and Singer describes an algorithm that obtains an\napproximation arbitrarily close to $1/3$ in $\\mathcal{O}(\\log n)$ adaptive\nrounds and shows that no algorithm can obtain a constant factor approximation\nin $\\tilde{o}(\\log n)$ adaptive rounds. This approach achieves an exponential\nspeedup in adaptivity (and parallel running time) at the expense of\napproximation quality.\n  In this paper we describe a novel approach that yields an algorithm whose\napproximation is arbitrarily close to the optimal $1-1/e$ guarantee in\n$\\mathcal{O}(\\log n)$ adaptive rounds. This algorithm therefore achieves an\nexponential speedup in parallel running time for submodular maximization at the\nexpense of an arbitrarily small loss in approximation quality. This guarantee\nis optimal in both approximation and adaptivity, up to lower order terms. \n\n"}
{"id": "1804.08731", "contents": "Title: Longest Common Substring Made Fully Dynamic Abstract: In the longest common substring (LCS) problem, we are given two strings $S$\nand $T$, each of length at most $n$, and we are asked to find a longest string\noccurring as a fragment of both $S$ and $T$. This is a classical and\nwell-studied problem in computer science with a known $\\mathcal{O}(n)$-time\nsolution. In the fully dynamic version of the problem, edit operations are\nallowed in either of the two strings, and we are asked to report an LCS after\neach such operation. We present the first solution to this problem that\nrequires sublinear time per edit operation. In particular, we show how to\nreturn an LCS in $\\tilde{\\mathcal{O}}(n^{2/3})$ time (or\n$\\tilde{\\mathcal{O}}(\\sqrt{n})$ time if edits are allowed in only one of the\ntwo strings) after each operation using $\\tilde{\\mathcal{O}}(n)$ space.\n  This line of research was recently initiated by the authors [SPIRE 2017] in a\nsomewhat restricted dynamic variant. An $\\tilde{\\mathcal{O}}(n)$-sized data\nstructure that returns an LCS of the two strings after a single edit operation\n(that is reverted afterwards) in $\\tilde{\\mathcal{O}}(1)$ time was presented.\nAt CPM 2018, three papers studied analogously restricted dynamic variants of\nproblems on strings. We show that our techniques can be used to obtain fully\ndynamic algorithms for several classical problems on strings, namely, computing\nthe longest repeat, the longest palindrome and the longest Lyndon substring of\na string. The only previously known sublinear-time dynamic algorithms for\nproblems on strings were obtained for maintaining a dynamic collection of\nstrings for comparison queries and for pattern matching with the most recent\nadvances made by Gawrychowski et al. [SODA 2018] and by Clifford et al. [STACS\n2018]. \n\n"}
{"id": "1804.08885", "contents": "Title: Polynomial Kernels for Hitting Forbidden Minors under Structural\n  Parameterizations Abstract: We investigate polynomial-time preprocessing for the problem of hitting\nforbidden minors in a graph, using the framework of kernelization. For a fixed\nfinite set of connected graphs F, the F-Deletion problem is the following:\ngiven a graph G and integer k, is it possible to delete k vertices from G to\nensure the resulting graph does not contain any graph from F as a minor?\nEarlier work by Fomin, Lokshtanov, Misra, and Saurabh [FOCS'12] showed that\nwhen F contains a planar graph, an instance (G,k) can be reduced in polynomial\ntime to an equivalent one of size $k^{O(1)}$. In this work we focus on\nstructural measures of the complexity of an instance, with the aim of giving\nnontrivial preprocessing guarantees for instances whose solutions are large.\nMotivated by several impossibility results, we parameterize the F-Deletion\nproblem by the size of a vertex modulator whose removal results in a graph of\nconstant treedepth $\\eta$.\n  We prove that for each set F of connected graphs and constant $\\eta$, the\nF-Deletion problem parameterized by the size of a treedepth-$\\eta$ modulator\nhas a polynomial kernel. Our kernelization is fully explicit and does not\ndepend on protrusion reduction or well-quasi-ordering, which are sources of\nalgorithmic non-constructivity in earlier works on F-Deletion. Our main\ntechnical contribution is to analyze how models of a forbidden minor in a graph\nG with modulator X, interact with the various connected components of G-X. By\nbounding the number of different types of behavior that can occur by a\npolynomial in |X|, we obtain a polynomial kernel using a recursive\npreprocessing strategy. Our results extend earlier work for specific instances\nof F-Deletion such as Vertex Cover and Feedback Vertex Set. It also generalizes\nearlier preprocessing results for F-Deletion parameterized by a vertex cover,\nwhich is a treedepth-one modulator. \n\n"}
{"id": "1804.08978", "contents": "Title: Tighter Connections Between Formula-SAT and Shaving Logs Abstract: A noticeable fraction of Algorithms papers in the last few decades improve\nthe running time of well-known algorithms for fundamental problems by\nlogarithmic factors. For example, the $O(n^2)$ dynamic programming solution to\nthe Longest Common Subsequence problem (LCS) was improved to $O(n^2/\\log^2 n)$\nin several ways and using a variety of ingenious tricks. This line of research,\nalso known as \"the art of shaving log factors\", lacks a tool for proving\nnegative results. Specifically, how can we show that it is unlikely that LCS\ncan be solved in time $O(n^2/\\log^3 n)$?\n  Perhaps the only approach for such results was suggested in a recent paper of\nAbboud, Hansen, Vassilevska W. and Williams (STOC'16). The authors blame the\nhardness of shaving logs on the hardness of solving satisfiability on Boolean\nformulas (Formula-SAT) faster than exhaustive search. They show that an\n$O(n^2/\\log^{1000} n)$ algorithm for LCS would imply a major advance in circuit\nlower bounds. Whether this approach can lead to tighter barriers was unclear.\n  In this paper, we push this approach to its limit and, in particular, prove\nthat a well-known barrier from complexity theory stands in the way for shaving\nfive additional log factors for fundamental combinatorial problems. For LCS,\nregular expression pattern matching, as well as the Fr\\'echet distance problem\nfrom Computational Geometry, we show that an $O(n^2/\\log^{7+\\varepsilon} n)$\nruntime would imply new Formula-SAT algorithms.\n  Our main result is a reduction from SAT on formulas of size $s$ over $n$\nvariables to LCS on sequences of length $N=2^{n/2} \\cdot s^{1+o(1)}$. Our\nreduction is essentially as efficient as possible, and it greatly improves the\npreviously known reduction for LCS with $N=2^{n/2} \\cdot s^c$, for some $c \\geq\n100$. \n\n"}
{"id": "1804.09448", "contents": "Title: Extensor-Coding Abstract: We devise an algorithm that approximately computes the number of paths of\nlength $k$ in a given directed graph with $n$ vertices up to a multiplicative\nerror of $1 \\pm \\varepsilon$. Our algorithm runs in time $\\varepsilon^{-2}\n4^k(n+m) \\operatorname{poly}(k)$. The algorithm is based on associating with\neach vertex an element in the exterior (or, Grassmann) algebra, called an\nextensor, and then performing computations in this algebra. This connection to\nexterior algebra generalizes a number of previous approaches for the longest\npath problem and is of independent conceptual interest. Using this approach, we\nalso obtain a deterministic $2^{k}\\cdot\\operatorname{poly}(n)$ time algorithm\nto find a $k$-path in a given directed graph that is promised to have few of\nthem. Our results and techniques generalize to the subgraph isomorphism problem\nwhen the subgraphs we are looking for have bounded pathwidth. Finally, we also\nobtain a randomized algorithm to detect $k$-multilinear terms in a multivariate\npolynomial given as a general algebraic circuit. To the best of our knowledge,\nthis was previously only known for algebraic circuits not involving negative\nconstants. \n\n"}
{"id": "1804.09745", "contents": "Title: On the Structure of Unique Shortest Paths in Graphs Abstract: This paper develops a structural theory of unique shortest paths in\nreal-weighted graphs. Our main goal is to characterize exactly which sets of\nnode sequences, which we call path systems, can be realized as unique shortest\npaths in a graph with arbitrary real edge weights. We say that such a path\nsystem is strongly metrizable.\n  An easy fact implicit in the literature is that a strongly metrizable path\nsystem must be consistent, meaning that no two of its paths may intersect,\nsplit apart, and then intersect again. Our main result characterizes strong\nmetrizability via some new forbidden intersection patterns along these lines.\nIn other words, we describe a family of forbidden patterns beyond consistency,\nand we prove that a path system is strongly metrizable if and only if it is\nconsistent and it avoids all of the patterns in this family. We offer separate\n(but closely related) characterizations in this way for the settings of\ndirected, undirected, and directed acyclic graphs.\n  Our characterizations are based on a new connection between shortest paths\nand topology; in particular, our new forbidden patterns are in natural\ncorrespondence with two-colored topological $2$-manifolds, which we visualize\nas polyhedra. We believe that this connection may be of independent interest,\nand we further show that it implies some additional structural corollaries that\nseem to suggest new and possibly deep-rooted connections between these areas. \n\n"}
{"id": "1804.09787", "contents": "Title: Interleaved group products Abstract: Let $G$ be the special linear group $\\mathrm{SL}(2,q)$. We show that if\n$(a_1,\\ldots,a_t)$ and $(b_1,\\ldots,b_t)$ are sampled uniformly from large\nsubsets $A$ and $B$ of $G^t$ then their interleaved product $a_1 b_1 a_2 b_2\n\\cdots a_t b_t$ is nearly uniform over $G$. This extends a result of the first\nauthor, which corresponds to the independent case where $A$ and $B$ are product\nsets. We obtain a number of other results. For example, we show that if $X$ is\na probability distribution on $G^m$ such that any two coordinates are uniform\nin $G^2$, then a pointwise product of $s$ independent copies of $X$ is nearly\nuniform in $G^m$, where $s$ depends on $m$ only. Extensions to other groups are\nalso discussed.\n  We obtain closely related results in communication complexity, which is the\nsetting where some of these questions were first asked by Miles and Viola. For\nexample, suppose party $A_i$ of $k$ parties $A_1,\\dots,A_k$ receives on its\nforehead a $t$-tuple $(a_{i1},\\dots,a_{it})$ of elements from $G$. The parties\nare promised that the interleaved product $a_{11}\\dots a_{k1}a_{12}\\dots\na_{k2}\\dots a_{1t}\\dots a_{kt}$ is equal either to the identity $e$ or to some\nother fixed element $g\\in G$, and their goal is to determine which of the two\nthe product is equal to. We show that for all fixed $k$ and all sufficiently\nlarge $t$ the communication is $\\Omega(t \\log |G|)$, which is tight. Even for\n$k=2$ the previous best lower bound was $\\Omega(t)$. As an application, we\nestablish the security of the leakage-resilient circuits studied by Miles and\nViola in the \"only computation leaks\" model. \n\n"}
{"id": "1804.10673", "contents": "Title: Buffered Count-Min Sketch on SSD: Theory and Experiments Abstract: Frequency estimation data structures such as the count-min sketch (CMS) have\nfound numerous applications in databases, networking, computational biology and\nother domains. Many applications that use the count-min sketch process massive\nand rapidly evolving datasets. For data-intensive applications that aim to keep\nthe overestimate error low, the count-min sketch may become too large to store\nin available RAM and may have to migrate to external storage (e.g., SSD.) Due\nto the random-read/write nature of hash operations of the count-min sketch,\nsimply placing it on SSD stifles the performance of time-critical applications,\nrequiring about 4-6 random reads/writes to SSD per estimate (lookup) and update\n(insert) operation.\n  In this paper, we expand on the preliminary idea of the Buffered Count-Min\nSketch (BCMS) [15], an SSD variant of the count-min sketch, that used hash\nlocalization to scale efficiently out of RAM while keeping the total error\nbounded. We describe the design and implementation of the buffered count-min\nsketch, and empirically show that our implementation achieves 3.7x-4.7x the\nspeedup on update (insert) and 4.3x speedup on estimate (lookup) operations.\n  Our design also offers an asymptotic improvement in the external-memory model\n[1] over the original data structure: r random I/Os are reduced to 1 I/O for\nthe estimate operation. For a data structure that uses k blocks on SSD, was the\nword/counter size, r as the number of rows, M as the number of bits in the main\nmemory, our data structure uses kwr/M amortized I/Os for updates, or, if kwr/M\n>1, 1 I/O in the worst case. In typical scenarios, kwr/M is much smaller than\n1. This is in contrast to O(r) I/Os incurred for each update in the original\ndata structure. \n\n"}
{"id": "1804.10827", "contents": "Title: On Euclidean $k$-Means Clustering with $\\alpha$-Center Proximity Abstract: $k$-means clustering is NP-hard in the worst case but previous work has shown\nefficient algorithms assuming the optimal $k$-means clusters are \\emph{stable}\nunder additive or multiplicative perturbation of data. This has two caveats.\nFirst, we do not know how to efficiently verify this property of optimal\nsolutions that are NP-hard to compute in the first place. Second, the stability\nassumptions required for polynomial time $k$-means algorithms are often\nunreasonable when compared to the ground-truth clusters in real-world data. A\nconsequence of multiplicative perturbation resilience is \\emph{center\nproximity}, that is, every point is closer to the center of its own cluster\nthan the center of any other cluster, by some multiplicative factor $\\alpha >\n1$.\n  We study the problem of minimizing the Euclidean $k$-means objective only\nover clusterings that satisfy $\\alpha$-center proximity. We give a simple\nalgorithm to find the optimal $\\alpha$-center-proximal $k$-means clustering in\nrunning time exponential in $k$ and $1/(\\alpha - 1)$ but linear in the number\nof points and the dimension. We define an analogous $\\alpha$-center proximity\ncondition for outliers, and give similar algorithmic guarantees for $k$-means\nwith outliers and $\\alpha$-center proximity. On the hardness side we show that\nfor any $\\alpha' > 1$, there exists an $\\alpha \\leq \\alpha'$, $(\\alpha >1)$,\nand an $\\varepsilon_0 > 0$ such that minimizing the $k$-means objective over\nclusterings that satisfy $\\alpha$-center proximity is NP-hard to approximate\nwithin a multiplicative $(1+\\varepsilon_0)$ factor. \n\n"}
{"id": "1804.11086", "contents": "Title: A Subquadratic Algorithm for 3XOR Abstract: Given a set $X$ of $n$ binary words of equal length $w$, the 3XOR problem\nasks for three elements $a, b, c \\in X$ such that $a \\oplus b=c$, where $\n\\oplus$ denotes the bitwise XOR operation. The problem can be easily solved on\na word RAM with word length $w$ in time $O(n^2 \\log{n})$. Using Han's fast\ninteger sorting algorithm (2002/2004) this can be reduced to $O(n^2\n\\log{\\log{n}})$. With randomization or a sophisticated deterministic dictionary\nconstruction, creating a hash table for $X$ with constant lookup time leads to\nan algorithm with (expected) running time $O(n^2)$. At present, seemingly no\nfaster algorithms are known. We present a surprisingly simple deterministic,\nquadratic time algorithm for 3XOR. Its core is a version of the Patricia trie\nfor $X$, which makes it possible to traverse the set $a \\oplus X$ in ascending\norder for arbitrary $a\\in \\{0, 1\\}^{w}$ in linear time.\n  Furthermore, we describe a randomized algorithm for 3XOR with expected\nrunning time $O(n^2\\cdot\\min\\{\\log^3{w}/w, (\\log\\log{n})^2/\\log^2 n\\})$. The\nalgorithm transfers techniques to our setting that were used by Baran, Demaine,\nand P\\u{a}tra\\c{s}cu (2005/2008) for solving the related int3SUM problem (the\nsame problem with integer addition in place of binary XOR) in expected time\n$o(n^2)$. As suggested by Jafargholi and Viola (2016), linear hash functions\nare employed. The latter authors also showed that assuming 3XOR needs expected\nrunning time $n^{2-o(1)}$ one can prove conditional lower bounds for triangle\nenumeration just as with 3SUM. We demonstrate that 3XOR can be reduced to other\nproblems as well, treating the examples offline SetDisjointness and offline\nSetIntersection, which were studied for 3SUM by Kopelowitz, Pettie, and Porat\n(2016). \n\n"}
{"id": "1805.00862", "contents": "Title: Spectral clustering algorithms for the detection of clusters in\n  block-cyclic and block-acyclic graphs Abstract: We propose two spectral algorithms for partitioning nodes in directed graphs\nrespectively with a cyclic and an acyclic pattern of connection between groups\nof nodes. Our methods are based on the computation of extremal eigenvalues of\nthe transition matrix associated to the directed graph. The two algorithms\noutperform state-of-the art methods for directed graph clustering on synthetic\ndatasets, including methods based on blockmodels, bibliometric symmetrization\nand random walks. Our algorithms have the same space complexity as classical\nspectral clustering algorithms for undirected graphs and their time complexity\nis also linear in the number of edges in the graph. One of our methods is\napplied to a trophic network based on predator-prey relationships. It\nsuccessfully extracts common categories of preys and predators encountered in\nfood chains. The same method is also applied to highlight the hierarchical\nstructure of a worldwide network of Autonomous Systems depicting business\nagreements between Internet Service Providers. \n\n"}
{"id": "1805.01074", "contents": "Title: Lower Bounds for Tolerant Junta and Unateness Testing via Rejection\n  Sampling of Graphs Abstract: We introduce a new model for testing graph properties which we call the\n\\emph{rejection sampling model}. We show that testing bipartiteness of\n$n$-nodes graphs using rejection sampling queries requires complexity\n$\\widetilde{\\Omega}(n^2)$. Via reductions from the rejection sampling model, we\ngive three new lower bounds for tolerant testing of Boolean functions of the\nform $f\\colon\\{0,1\\}^n\\to \\{0,1\\}$:\n  $\\bullet$Tolerant $k$-junta testing with \\emph{non-adaptive} queries requires\n$\\widetilde{\\Omega}(k^2)$ queries.\n  $\\bullet$Tolerant unateness testing requires $\\widetilde{\\Omega}(n)$ queries.\n  $\\bullet$Tolerant unateness testing with \\emph{non-adaptive} queries requires\n$\\widetilde{\\Omega}(n^{3/2})$ queries.\n  Given the $\\widetilde{O}(k^{3/2})$-query non-adaptive junta tester of Blais\n\\cite{B08}, we conclude that non-adaptive tolerant junta testing requires more\nqueries than non-tolerant junta testing. In addition, given the\n$\\widetilde{O}(n^{3/4})$-query unateness tester of Chen, Waingarten, and Xie\n\\cite{CWX17b} and the $\\widetilde{O}(n)$-query non-adaptive unateness tester of\nBaleshzar, Chakrabarty, Pallavoor, Raskhodnikova, and Seshadhri \\cite{BCPRS17},\nwe conclude that tolerant unateness testing requires more queries than\nnon-tolerant unateness testing, in both adaptive and non-adaptive settings.\nThese lower bounds provide the first separation between tolerant and\nnon-tolerant testing for a natural property of Boolean functions. \n\n"}
{"id": "1805.02526", "contents": "Title: The Online Best Reply Algorithm for Resource Allocation Problems Abstract: We study the performance of a best reply algorithm for online resource\nallocation problems with a diseconomy of scale. In an online resource\nallocation problem, we are given a set of resources and a set of requests that\narrive in an online manner. Each request consists of a set of feasible\nallocations and an allocation is a set of resources. The total cost of an\nallocation vector is given by the sum of the resources' costs, where each\nresource's cost depends on the total load on the resource under the allocation\nvector. We analyze the natural online procedure where each request is allocated\ngreedily to a feasible set of resources that minimizes the individual cost of\nthat particular request. In the literature, this algorithm is also known as a\none-round walk-in congestion games starting from the empty state. For\nunweighted resource allocation problems with polynomial cost functions with\nmaximum degree $d$, upper bounds on the competitive ratio of this greedy\nalgorithm were known only for the special cases $d\\in\\{1, 2, 3\\}$. In this\npaper, we show a general upper bound on the competitive ratio of $d(d /\nW(\\frac{1.2d-1}{d+1}))^{d+1}$ for the unweighted case where $W$ denotes the\nLambert-W function on $\\mathbb{R}_{\\geq 0}$. For the weighted case, we show\nthat the competitive ratio of the greedy algorithm is bounded from above by\n$(d/W(\\frac{d}{d+1}))^{d+1}$. \n\n"}
{"id": "1805.02974", "contents": "Title: Massively Parallel Algorithms for Finding Well-Connected Components in\n  Sparse Graphs Abstract: A fundamental question that shrouds the emergence of massively parallel\ncomputing (MPC) platforms is how can the additional power of the MPC paradigm\nbe leveraged to achieve faster algorithms compared to classical parallel models\nsuch as PRAM?\n  Previous research has identified the sparse graph connectivity problem as a\nmajor obstacle to such improvement: While classical logarithmic-round PRAM\nalgorithms for finding connected components in any $n$-vertex graph have been\nknown for more than three decades, no $o(\\log{n})$-round MPC algorithms are\nknown for this task with truly sublinear in $n$ memory per machine. This\nproblem arises when processing massive yet sparse graphs with $O(n)$ edges, for\nwhich the interesting setting of parameters is $n^{1-\\Omega(1)}$ memory per\nmachine. It is conjectured that achieving an $o(\\log{n})$-round algorithm for\nconnectivity on general sparse graphs with $n^{1-\\Omega(1)}$ per-machine memory\nmay not be possible, and this conjecture also forms the basis for multiple\nconditional hardness results on the round complexity of other problems in the\nMPC model.\n  We take an opportunistic approach towards the sparse graph connectivity\nproblem, by designing an algorithm with improved performance guarantees in\nterms of the connectivity structure of the input graph. Formally, we design an\nalgorithm that finds all connected components with spectral gap at least\n$\\lambda$ in a graph in $O(\\log\\log{n} + \\log{(1/\\lambda)})$ MPC rounds and\n$n^{\\Omega(1)}$ memory per machine. As such, this algorithm achieves an\nexponential round reduction on sparse \"well-connected\" components (i.e.,\n$\\lambda \\geq 1/\\text{polylog}{(n)}$) using only $n^{\\Omega(1)}$ memory per\nmachine and $\\widetilde{O}(n)$ total memory, and still operates in $o(\\log n)$\nrounds even when $\\lambda = 1/n^{o(1)}$. \n\n"}
{"id": "1805.03192", "contents": "Title: The Computational Complexity of Finding Hamiltonian Cycles in Grid\n  Graphs of Semiregular Tessellations Abstract: Finding Hamitonian Cycles in square grid graphs is a well studied and\nimportant questions. More recent work has extended these results to triangular\nand hexagonal grids, as well as further restricted versions. In this paper, we\nexamine a class of more complex grids, as well as looking at the problem with\nrestricted types of paths. We investigate the hardness of Hamiltonian cycle\nproblem in grid graphs of semiregular tessellations. We give NP-hardness\nreductions for finding Hamiltonian paths in grid graphs based on all eight of\nthe semiregular tessilations. Next, we investigate variations on the problem of\nfinding Hamiltonian Paths in grid graphs when the path is forced to turn at\nevery vertex. We give a polynomial time algorithm for deciding if a square grid\ngraph admits a Hamiltonian cycle which turns at every vertex. We then show\ndeciding if cubic grid graphs, even if the height is restricted to $2$, admit a\nHamiltonian cycle is NP-complete. \n\n"}
{"id": "1805.03253", "contents": "Title: Efficient Shortest Paths in Scale-Free Networks with Underlying\n  Hyperbolic Geometry Abstract: A common way to accelerate shortest path algorithms on graphs is the use of a\nbidirectional search, which simultaneously explores the graph from the start\nand the destination. It has been observed recently that this strategy performs\nparticularly well on scale-free real-world networks. Such networks typically\nhave a heterogeneous degree distribution (e.g., a power-law distribution) and\nhigh clustering (i.e., vertices with a common neighbor are likely to be\nconnected themselves). These two properties can be obtained by assuming an\nunderlying hyperbolic geometry.\n  To explain the observed behavior of the bidirectional search, we analyze its\nrunning time on hyperbolic random graphs and prove that it is $\\mathcal {\\tilde\nO}(n^{2 - 1/\\alpha} + n^{1/(2\\alpha)} + \\delta_{\\max})$ with high probability,\nwhere $\\alpha \\in (0.5, 1)$ controls the power-law exponent of the degree\ndistribution, and $\\delta_{\\max}$ is the maximum degree. This bound is\nsublinear, improving the obvious worst-case linear bound. Although our analysis\ndepends on the underlying geometry, the algorithm itself is oblivious to it. \n\n"}
{"id": "1805.04764", "contents": "Title: New Distributed Algorithms in Almost Mixing Time via Transformations\n  from Parallel Algorithms Abstract: We show that many classical optimization problems --- such as\n$(1\\pm\\epsilon)$-approximate maximum flow, shortest path, and transshipment ---\ncan be computed in $\\newcommand{\\tmix}{{\\tau_{\\text{mix}}}}\\tmix(G)\\cdot\nn^{o(1)}$ rounds of distributed message passing, where $\\tmix(G)$ is the mixing\ntime of the network graph $G$. This extends the result of Ghaffari et al.\\\n[PODC'17], whose main result is a distributed MST algorithm in $\\tmix(G)\\cdot\n2^{O(\\sqrt{\\log n \\log\\log n})}$ rounds in the CONGEST model, to a much wider\nclass of optimization problems. For many practical networks of interest, e.g.,\npeer-to-peer or overlay network structures, the mixing time $\\tmix(G)$ is\nsmall, e.g., polylogarithmic. On these networks, our algorithms bypass the\n$\\tilde\\Omega(\\sqrt n+D)$ lower bound of Das Sarma et al.\\ [STOC'11], which\napplies for worst-case graphs and applies to all of the above optimization\nproblems. For all of the problems except MST, this is the first distributed\nalgorithm which takes $o(\\sqrt n)$ rounds on a (nontrivial) restricted class of\nnetwork graphs.\n  Towards deriving these improved distributed algorithms, our main contribution\nis a general transformation that simulates any work-efficient PRAM algorithm\nrunning in $T$ parallel rounds via a distributed algorithm running in $T\\cdot\n\\tmix(G)\\cdot 2^{O(\\sqrt{\\log n})}$ rounds. Work- and time-efficient parallel\nalgorithms for all of the aforementioned problems follow by combining the work\nof Sherman [FOCS'13, SODA'17] and Peng and Spielman [STOC'14]. Thus, simulating\nthese parallel algorithms using our transformation framework produces the\ndesired distributed algorithms.\n  The core technical component of our transformation is the algorithmic problem\nof solving \\emph{multi-commodity routing}---that is, roughly, routing $n$\npackets each from a given source to a given destination---in random graphs. For\nthis problem, we obtain a... \n\n"}
{"id": "1805.05208", "contents": "Title: Theoretically Efficient Parallel Graph Algorithms Can Be Fast and\n  Scalable Abstract: There has been significant recent interest in parallel graph processing due\nto the need to quickly analyze the large graphs available today. Many graph\ncodes have been designed for distributed memory or external memory. However,\ntoday even the largest publicly-available real-world graph (the Hyperlink Web\ngraph with over 3.5 billion vertices and 128 billion edges) can fit in the\nmemory of a single commodity multicore server. Nevertheless, most experimental\nwork in the literature report results on much smaller graphs, and the ones for\nthe Hyperlink graph use distributed or external memory. Therefore, it is\nnatural to ask whether we can efficiently solve a broad class of graph problems\non this graph in memory.\n  This paper shows that theoretically-efficient parallel graph algorithms can\nscale to the largest publicly-available graphs using a single machine with a\nterabyte of RAM, processing them in minutes. We give implementations of\ntheoretically-efficient parallel algorithms for 20 important graph problems. We\nalso present the optimizations and techniques that we used in our\nimplementations, which were crucial in enabling us to process these large\ngraphs quickly. We show that the running times of our implementations\noutperform existing state-of-the-art implementations on the largest real-world\ngraphs. For many of the problems that we consider, this is the first time they\nhave been solved on graphs at this scale. We have made the implementations\ndeveloped in this work publicly-available as the Graph-Based Benchmark Suite\n(GBBS). \n\n"}
{"id": "1805.05305", "contents": "Title: Transforming graph states using single-qubit operations Abstract: Stabilizer states form an important class of states in quantum information,\nand are of central importance in quantum error correction. Here, we provide an\nalgorithm for deciding whether one stabilizer (target) state can be obtained\nfrom another stabilizer (source) state by single-qubit Clifford operations\n(LC), single-qubit Pauli measurements (LPM), and classical communication (CC)\nbetween sites holding the individual qubits. What's more, we provide a recipe\nto obtain the sequence of LC+LPM+CC operations which prepare the desired target\nstate from the source state, and show how these operations can be applied in\nparallel to reach the target state in constant time. Our algorithm has\napplications in quantum networks, quantum computing, and can also serve as a\ndesign tool - for example, to find transformations between quantum error\ncorrecting codes. We provide a software implementation of our algorithm that\nmakes this tool easier to apply.\n  A key insight leading to our algorithm is to show that the problem is\nequivalent to one in graph theory, which is to decide whether some graph G' is\na vertex-minor of another graph G. Here we show that the vertex-minor problem\ncan be solved in time O(|G|^3) where |G| is the size of the graph G, whenever\nthe rank-width of G and the size of G' are bounded. Our algorithm is based on\ntechniques by Courcelle for solving fixed parameter tractable problems, where\nhere the relevant fixed parameter is the rank width. The second half of this\npaper serves as an accessible but far from exhausting introduction to these\nconcepts, that could be useful for many other problems in quantum information. \n\n"}
{"id": "1805.05306", "contents": "Title: How to transform graph states using single-qubit operations:\n  computational complexity and algorithms Abstract: Graph states are ubiquitous in quantum information with diverse applications\nranging from quantum network protocols to measurement based quantum computing.\nHere we consider the question whether one graph (source) state can be\ntransformed into another graph (target) state, using a specific set of quantum\noperations (LC+LPM+CC): single-qubit Clifford operations (LC), single-qubit\nPauli measurements (LPM) and classical communication (CC) between sites holding\nthe individual qubits. We first show that deciding whether a graph state |G>\ncan be transformed into another graph state |G'> using LC+LPM+CC is\nNP-Complete, even if |G'> is restricted to be the GHZ-state. However, we also\nprovide efficient algorithms for two situations of practical interest:\n  1. |G> has Schmidt-rank width one and |G'> is a GHZ-state. The Schmidt-rank\nwidth is an entanglement measure of quantum states, meaning this algorithm is\nefficient if the original state has little entanglement. Our algorithm has\nruntime O(|V(G')||V(G)|^3), and is also efficient in practice even on small\ninstances as further showcased by a freely available software implementation.\n  2. |G> is in a certain class of states with unbounded Schmidt-rank width, and\n|G'> is a GHZ-state of a constant size. Here the runtime is O(poly(|V(G)|)),\nshowing that more efficient algorithms can in principle be found even for\nstates holding a large amount of entanglement, as long as the output state has\nconstant size.\n  Our results make use of the insight that deciding whether a graph state |G>\ncan be transformed to another graph state |G'> is equivalent to a known\ndecision problem in graph theory, namely the problem of deciding whether a\ngraph G' is a vertex-minor of a graph G. Many of the technical tools developed\nto obtain our results may be of independent interest. \n\n"}
{"id": "1805.05404", "contents": "Title: Congested Clique Algorithms for Graph Spanners Abstract: Graph spanners are sparse subgraphs that faithfully preserve the distances in\nthe original graph up to small stretch. Spanner have been studied extensively\nas they have a wide range of applications ranging from distance oracles,\nlabeling schemes and routing to solving linear systems and spectral\nsparsification. A $k$-spanner maintains pairwise distances up to multiplicative\nfactor of $k$. It is a folklore that for every $n$-vertex graph $G$, one can\nconstruct a $(2k-1)$ spanner with $O(n^{1+1/k})$ edges. In a distributed\nsetting, such spanners can be constructed in the standard CONGEST model using\n$O(k^2)$ rounds, when randomization is allowed.\n  In this work, we consider spanner constructions in the congested clique\nmodel, and show: (1) A randomized construction of a $(2k-1)$-spanner with\n$\\widetilde{O}(n^{1+1/k})$ edges in $O(\\log k)$ rounds. The previous best\nalgorithm runs in $O(k)$ rounds. (2) A deterministic construction of a\n$(2k-1)$-spanner with $\\widetilde{O}(n^{1+1/k})$ edges in $O(\\log k +(\\log\\log\nn)^3)$ rounds. The previous best algorithm runs in $O(k\\log n)$ rounds. This\nimprovement is achieved by a new derandomization theorem for hitting sets which\nmight be of independent interest. (3) A deterministic construction of a\n$O(k)$-spanner with $O(k \\cdot n^{1+1/k})$ edges in $O(\\log k)$ rounds. \n\n"}
{"id": "1805.05729", "contents": "Title: A Survey on Analog Models of Computation Abstract: We present a survey on analog models of computations. Analog can be\nunderstood both as computing by analogy, or as working on the continuum. We\nconsider both approaches, often intertwined, with a point of view mostly\noriented by computation theory. \n\n"}
{"id": "1805.06151", "contents": "Title: Improved Worst-Case Deterministic Parallel Dynamic Minimum Spanning\n  Forest Abstract: This paper gives a new deterministic algorithm for the dynamic Minimum\nSpanning Forest (MSF) problem in the EREW PRAM model, where the goal is to\nmaintain a MSF of a weighted graph with $n$ vertices and $m$ edges while\nsupporting edge insertions and deletions. We show that one can solve the\ndynamic MSF problem using $O(\\sqrt n)$ processors and $O(\\log n)$ worst-case\nupdate time, for a total of $O(\\sqrt n \\log n)$ work. This improves on the work\nof Ferragina [IPPS 1995] which costs $O(\\log n)$ worst-case update time and\n$O(n^{2/3} \\log{\\frac{m}{n}})$ work. \n\n"}
{"id": "1805.08953", "contents": "Title: Maximal and maximum transitive relation contained in a given binary\n  relation Abstract: We study the problem of finding a \\textit{maximal} transitive relation\ncontained in a given binary relation. Given a binary relation of size $m$\ndefined on a set of size $n$, we present a polynomial time algorithm that finds\na maximal transitive sub-relation in time $O(n^2 + nm)$.\n  We also study the problem of finding a \\textit{maximum} transitive relation\ncontained in a binary relation. This is the problem of computing a maximum\ntransitive subgraph in a given digraph. For the class of directed graphs with\nthe underlying graph being triangle-free, we present a $0.874$-approximation\nalgorithm. This is achieved via a simple connection to the problem of maximum\ndirected cut. Further, we give an upper bound for the size of any maximum\ntransitive relation to be $m/4 + cm^{4/5}$, where $c > 0$ and $m$ is the number\nof edges in the digraph. \n\n"}
{"id": "1805.10042", "contents": "Title: Algorithms for Anti-Powers in Strings Abstract: A string $S[1,n]$ is a power (or tandem repeat) of order $k$ and period $n/k$\nif it can decomposed into $k$ consecutive equal-length blocks of letters.\nPowers and periods are fundamental to string processing, and algorithms for\ntheir efficient computation have wide application and are heavily studied.\nRecently, Fici et al. (Proc. ICALP 2016) defined an {\\em anti-power} of order\n$k$ to be a string composed of $k$ pairwise-distinct blocks of the same length\n($n/k$, called {\\em anti-period}). Anti-powers are a natural converse to\npowers, and are objects of combinatorial interest in their own right. In this\npaper we initiate the algorithmic study of anti-powers. Given a string $S$, we\ndescribe an optimal algorithm for locating all substrings of $S$ that are\nanti-powers of a specified order. The optimality of the algorithm follows form\na combinatorial lemma that provides a lower bound on the number of distinct\nanti-powers of a given order: we prove that a string of length $n$ can contain\n$\\Theta(n^2/k)$ distinct anti-powers of order $k$. \n\n"}
{"id": "1806.02329", "contents": "Title: Mitigating Bias in Adaptive Data Gathering via Differential Privacy Abstract: Data that is gathered adaptively --- via bandit algorithms, for example ---\nexhibits bias. This is true both when gathering simple numeric valued data ---\nthe empirical means kept track of by stochastic bandit algorithms are biased\ndownwards --- and when gathering more complicated data --- running hypothesis\ntests on complex data gathered via contextual bandit algorithms leads to false\ndiscovery. In this paper, we show that this problem is mitigated if the data\ncollection procedure is differentially private. This lets us both bound the\nbias of simple numeric valued quantities (like the empirical means of\nstochastic bandit algorithms), and correct the p-values of hypothesis tests run\non the adaptively gathered data. Moreover, there exist differentially private\nbandit algorithms with near optimal regret bounds: we apply existing theorems\nin the simple stochastic case, and give a new analysis for linear contextual\nbandits. We complement our theoretical results with experiments validating our\ntheory. \n\n"}
{"id": "1806.02771", "contents": "Title: Structural Rounding: Approximation Algorithms for Graphs Near an\n  Algorithmically Tractable Class Abstract: We develop a new framework for generalizing approximation algorithms from the\nstructural graph algorithm literature so that they apply to graphs somewhat\nclose to that class (a scenario we expect is common when working with\nreal-world networks) while still guaranteeing approximation ratios. The idea is\nto $\\textit{edit}$ a given graph via vertex- or edge-deletions to put the graph\ninto an algorithmically tractable class, apply known approximation algorithms\nfor that class, and then $\\textit{lift}$ the solution to apply to the original\ngraph. We give a general characterization of when an optimization problem is\namenable to this approach, and show that it includes many well-studied graph\nproblems, such as Independent Set, Vertex Cover, Feedback Vertex Set, Minimum\nMaximal Matching, Chromatic Number, ($\\ell$-)Dominating Set, Edge\n($\\ell$-)Dominating Set, and Connected Dominating Set.\n  To enable this framework, we develop new editing algorithms that find the\napproximately-fewest edits required to bring a given graph into one of several\nimportant graph classes (in some cases, also approximating the target parameter\nof the family). For bounded degeneracy, we obtain a bicriteria\n$(4,4)$-approximation which also extends to a smoother bicriteria trade-off.\nFor bounded treewidth, we obtain a bicriteria $(O(\\log^{1.5} n), O(\\sqrt{\\log\nw}))$-approximation, and for bounded pathwidth, we obtain a bicriteria\n$(O(\\log^{1.5} n), O(\\sqrt{\\log w} \\cdot \\log n))$-approximation. For treedepth\n$2$ (also related to bounded expansion), we obtain a $4$-approximation. We also\nprove complementary hardness-of-approximation results assuming $\\mathrm{P} \\neq\n\\mathrm{NP}$: in particular, these problems are all log-factor inapproximable,\nexcept the last which is not approximable below some constant factor ($2$\nassuming UGC). \n\n"}
{"id": "1806.02894", "contents": "Title: Optimal Design of Process Flexibility for General Production Systems Abstract: Process flexibility is widely adopted as an effective strategy for responding\nto uncertain demand. Many algorithms for constructing sparse flexibility\ndesigns with good theoretical guarantees have been developed for balanced and\nsymmetrical production systems. These systems assume that the number of plants\nequals the number of products, that supplies have the same capacity, and that\ndemands are independently and identically distributed. In this paper, we relax\nthese assumptions and consider a general class of production systems. We\nconstruct a simple flexibility design to fulfill $(1-\\epsilon)$-fraction of\nexpected demand with high probability (w.h.p.) where the average degree is\n$O(\\ln(1/\\epsilon))$. To motivate our construction, we first consider a natural\nweighted probabilistic construction from Chou et al. (2011) where the degree of\neach node is proportional to its expected capacity. However, this strategy is\nshown to be sub-optimal. To obtain an optimal construction, we develop a simple\nyet effective thresholding scheme. The analysis of our approach extends the\nclassical analysis of expander graphs by overcoming several technical\ndifficulties. Our approach may prove useful in other applications that require\nexpansion properties of graphs with non-uniform degree sequences. \n\n"}
{"id": "1806.04310", "contents": "Title: MISSION: Ultra Large-Scale Feature Selection using Count-Sketches Abstract: Feature selection is an important challenge in machine learning. It plays a\ncrucial role in the explainability of machine-driven decisions that are rapidly\npermeating throughout modern society. Unfortunately, the explosion in the size\nand dimensionality of real-world datasets poses a severe challenge to standard\nfeature selection algorithms. Today, it is not uncommon for datasets to have\nbillions of dimensions. At such scale, even storing the feature vector is\nimpossible, causing most existing feature selection methods to fail.\nWorkarounds like feature hashing, a standard approach to large-scale machine\nlearning, helps with the computational feasibility, but at the cost of losing\nthe interpretability of features. In this paper, we present MISSION, a novel\nframework for ultra large-scale feature selection that performs stochastic\ngradient descent while maintaining an efficient representation of the features\nin memory using a Count-Sketch data structure. MISSION retains the simplicity\nof feature hashing without sacrificing the interpretability of the features\nwhile using only O(log^2(p)) working memory. We demonstrate that MISSION\naccurately and efficiently performs feature selection on real-world,\nlarge-scale datasets with billions of dimensions. \n\n"}
{"id": "1806.04484", "contents": "Title: A Fourier-Analytic Approach for the Discrepancy of Random Set Systems Abstract: One of the prominent open problems in combinatorics is the discrepancy of set\nsystems where each element lies in at most $t$ sets. The Beck-Fiala conjecture\nsuggests that the right bound is $O(\\sqrt{t})$, but for three decades the only\nknown bound not depending on the size of the set system has been $O(t)$.\nArguably we currently lack techniques for breaking that barrier.\n  In this paper we introduce discrepancy bounds based on Fourier analysis. We\ndemonstrate our method on random set systems. Suppose one has $n$ elements and\n$m$ sets containing each element independently with probability $p$. We prove\nthat in the regime of $n \\geq \\Theta(m^2\\log(m))$, the discrepancy is at most\n$1$ with high probability. Previously, a result of Ezra and Lovett gave a bound\nof $O(1)$ under the stricter assumption that $n \\gg m^t$. \n\n"}
{"id": "1806.05797", "contents": "Title: Polyhedra Circuits and Their Applications Abstract: We introduce polyhedra circuits. Each polyhedra circuit characterizes a\ngeometric region in $\\mathbb{R}^d$. They can be applied to represent a rich\nclass of geometric objects, which include all polyhedra and the union of a\nfinite number of polyhedra. They can be used to approximate a large class of\n$d$-dimensional manifolds in $\\mathbb{R}^d$. Barvinok developed polynomial time\nalgorithms to compute the volume of a rational polyhedra, and to count the\nnumber of lattice points in a rational polyhedra in a fixed dimensional space\n$\\mathbb{R}^d$ with a fix $d$. Define $T_V(d,\\, n)$ be the polynomial time in\n$n$ to compute the volume of one rational polyhedra, $T_L(d,\\, n)$ be the\npolynomial time in $n$ to count the number of lattice points in one rational\npolyhedra with $d$ be a fixed dimensional number, $T_I(d,\\, n)$ be the\npolynomial time in $n$ to solve integer linear programming time with $d$ be the\nfixed dimensional number, where $n$ is the total number of linear inequalities\nfrom input polyhedra. We develop algorithms to count the number of lattice\npoints in the geometric region determined by a polyhedra circuit in\n$O\\left(nd\\cdot r_d(n)\\cdot T_V(d,\\, n)\\right)$ time and to compute the volume\nof the geometric region determined by a polyhedra circuit in $O\\left(n\\cdot\nr_d(n)\\cdot T_I(d,\\, n)+r_d(n)T_L(d,\\, n)\\right)$ time, where $n$ is the number\nof input linear inequalities, $d$ is number of variables and $r_d(n)$ be the\nmaximal number of regions that $n$ linear inequalities with $d$ variables\npartition $\\mathbb{R}^d$. \n\n"}
{"id": "1806.05878", "contents": "Title: Landscape Boolean Functions Abstract: In this paper we define a class of Boolean and generalized Boolean functions\ndefined on $\\mathbb{F}_2^n$ with values in $\\mathbb{Z}_q$ (mostly, we consider\n$q=2^k$), which we call landscape functions (whose class containing generalized\nbent, semibent, and plateaued) and find their complete characterization in\nterms of their components. In particular, we show that the previously published\ncharacterizations of generalized bent and plateaued Boolean functions are in\nfact particular cases of this more general setting. Furthermore, we provide an\ninductive construction of landscape functions, having any number of nonzero\nWalsh-Hadamard coefficients. We also completely characterize generalized\nplateaued functions in terms of the second derivatives and fourth moments. \n\n"}
{"id": "1806.06173", "contents": "Title: On the Complexity of Detecting Convexity over a Box Abstract: It has recently been shown that the problem of testing global convexity of\npolynomials of degree four is {strongly} NP-hard, answering an open question of\nN.Z. Shor. This result is minimal in the degree of the polynomial when global\nconvexity is of concern. In a number of applications however, one is interested\nin testing convexity only over a compact region, most commonly a box (i.e.,\nhyper-rectangle). In this paper, we show that this problem is also strongly\nNP-hard, in fact for polynomials of degree as low as three. This result is\nminimal in the degree of the polynomial and in some sense justifies why\nconvexity detection in nonlinear optimization solvers is limited to quadratic\nfunctions or functions with special structure. As a byproduct, our proof shows\nthat the problem of testing whether all matrices in an interval family are\npositive semidefinite is strongly NP-hard. This problem, which was previously\nshown to be (weakly) NP-hard by Nemirovski, is of independent interest in the\ntheory of robust control. \n\n"}
{"id": "1806.06704", "contents": "Title: Formulations for designing robust networks. An application to wind power\n  collection Abstract: We are interested in the design of survivable capacitated rooted Steiner\nnetworks. Given a graph G = (V, E), capacity and cost functions on E, a root r,\na subset T of V of terminals and an integer k, we search for a minimum cost\nsubset E $\\subset$ E, covering T and r, such that the network induced by E is\nk-survivable: after the removal of any k edges, there still exists a feasible\nflow from r to T. We also consider the possibility of protecting a given number\nof edges. We propose three different formulations: a cut-set, a flow and a\nbi-level formulation where the second-level is a min-max problem (with an\nattacker and a defender). We propose algorithms for each problem formulation\nand compare their efficiency. \n\n"}
{"id": "1806.06996", "contents": "Title: Optimization over Nonnegative and Convex Polynomials With and Without\n  Semidefinite Programming Abstract: The problem of optimizing over the cone of nonnegative polynomials is a\nfundamental problem in computational mathematics, with applications to\npolynomial optimization, control, machine learning, game theory, and\ncombinatorics, among others. A number of breakthrough papers in the early 2000s\nshowed that this problem, long thought to be out of reach, could be tackled by\nusing sum of squares programming. This technique however has proved to be\nexpensive for large-scale problems, as it involves solving large semidefinite\nprograms (SDPs).\n  In the first part of this thesis, we present two methods for approximately\nsolving large-scale sum of squares programs that dispense altogether with\nsemidefinite programming and only involve solving a sequence of linear or\nsecond order cone programs generated in an adaptive fashion. We then focus on\nthe problem of finding tight lower bounds on polynomial optimization problems\n(POPs), a fundamental task in this area that is most commonly handled through\nthe use of SDP-based sum of squares hierarchies (e.g., due to Lasserre and\nParrilo). In contrast to previous approaches, we provide the first theoretical\nframework for constructing converging hierarchies of lower bounds on POPs whose\ncomputation simply requires the ability to multiply certain fixed polynomials\ntogether and to check nonnegativity of the coefficients of their product.\n  In the second part of this thesis, we focus on the theory and applications of\nthe problem of optimizing over convex polynomials, a subcase of the problem of\noptimizing over nonnegative polynomials. (See manuscript for the rest of the\nabstract.) \n\n"}
{"id": "1806.07508", "contents": "Title: Reducibility and Computational Lower Bounds for Problems with Planted\n  Sparse Structure Abstract: The prototypical high-dimensional statistics problem entails finding a\nstructured signal in noise. Many of these problems exhibit an intriguing\nphenomenon: the amount of data needed by all known computationally efficient\nalgorithms far exceeds what is needed for inefficient algorithms that search\nover all possible structures. A line of work initiated by Berthet and Rigollet\nin 2013 has aimed to explain these statistical-computational gaps by reducing\nfrom conjecturally hard average-case problems in computer science. However, the\ndelicate nature of average-case reductions has limited the applicability of\nthis approach. In this work we introduce several new techniques to give a web\nof average-case reductions showing strong computational lower bounds based on\nthe planted clique conjecture using natural problems as intermediates. These\ninclude tight lower bounds for Planted Independent Set, Planted Dense Subgraph,\nSparse Spiked Wigner, Sparse PCA, a subgraph variant of the Stochastic Block\nModel and a biased variant of Sparse PCA. We also give algorithms matching our\nlower bounds and identify the information-theoretic limits of the models we\nconsider. \n\n"}
{"id": "1806.10586", "contents": "Title: Approximability of Discriminators Implies Diversity in GANs Abstract: While Generative Adversarial Networks (GANs) have empirically produced\nimpressive results on learning complex real-world distributions, recent works\nhave shown that they suffer from lack of diversity or mode collapse. The\ntheoretical work of Arora et al. suggests a dilemma about GANs' statistical\nproperties: powerful discriminators cause overfitting, whereas weak\ndiscriminators cannot detect mode collapse.\n  By contrast, we show in this paper that GANs can in principle learn\ndistributions in Wasserstein distance (or KL-divergence in many cases) with\npolynomial sample complexity, if the discriminator class has strong\ndistinguishing power against the particular generator class (instead of against\nall possible generators). For various generator classes such as mixture of\nGaussians, exponential families, and invertible and injective neural networks\ngenerators, we design corresponding discriminators (which are often neural nets\nof specific architectures) such that the Integral Probability Metric (IPM)\ninduced by the discriminators can provably approximate the Wasserstein distance\nand/or KL-divergence. This implies that if the training is successful, then the\nlearned distribution is close to the true distribution in Wasserstein distance\nor KL divergence, and thus cannot drop modes. Our preliminary experiments show\nthat on synthetic datasets the test IPM is well correlated with KL divergence\nor the Wasserstein distance, indicating that the lack of diversity in GANs may\nbe caused by the sub-optimality in optimization instead of statistical\ninefficiency. \n\n"}
{"id": "1806.11542", "contents": "Title: High Dimensional Discrete Integration over the Hypergrid Abstract: Recently Ermon et al. (2013) pioneered a way to practically compute\napproximations to large scale counting or discrete integration problems by\nusing random hashes. The hashes are used to reduce the counting problem into\nmany separate discrete optimization problems. The optimization problems then\ncan be solved by an NP-oracle such as commercial SAT solvers or integer linear\nprogramming (ILP) solvers. In particular, Ermon et al. showed that if the\ndomain of integration is $\\{0,1\\}^n$ then it is possible to obtain a solution\nwithin a factor of $16$ of the optimal (a 16-approximation) by this technique.\n  In many crucial counting tasks, such as computation of partition function of\nferromagnetic Potts model, the domain of integration is naturally $\\{0,1,\\dots,\nq-1\\}^n, q>2$, the hypergrid. The straightforward extension of Ermon et al.'s\nmethod allows a $q^2$-approximation for this problem. For large values of $q$,\nthis is undesirable. In this paper, we show an improved technique to obtain an\napproximation factor of $4+O(1/q^2)$ to this problem. We are able to achieve\nthis by using an idea of optimization over multiple bins of the hash functions,\nthat can be easily implemented by inequality constraints, or even in\nunconstrained way. Also the burden on the NP-oracle is not increased by our\nmethod (an ILP solver can still be used). We provide experimental simulation\nresults to support the theoretical guarantees of our algorithms. \n\n"}
{"id": "1807.01247", "contents": "Title: Ortho-polygon Visibility Representations of 3-connected 1-plane Graphs Abstract: An ortho-polygon visibility representation $\\Gamma$ of a $1$-plane graph $G$\n(OPVR of $G$) is an embedding preserving drawing that maps each vertex of $G$\nto a distinct orthogonal polygon and each edge of $G$ to a vertical or\nhorizontal visibility between its end-vertices. The representation $\\Gamma$ has\nvertex complexity $k$ if every polygon of $\\Gamma$ has at most $k$ reflex\ncorners. It is known that $3$-connected $1$-plane graphs admit an OPVR with\nvertex complexity at most twelve, while vertex complexity at least two may be\nrequired in some cases. In this paper, we reduce this gap by showing that\nvertex complexity five is always sufficient, while vertex complexity four may\nbe required in some cases. These results are based on the study of the\ncombinatorial properties of the B-, T-, and W-configurations in $3$-connected\n$1$-plane graphs. An implication of the upper bound is the existence of a\n$\\tilde{O}(n^\\frac{10}{7})$-time drawing algorithm that computes an OPVR of an\n$n$-vertex $3$-connected $1$-plane graph on an integer grid of size $O(n)\n\\times O(n)$ and with vertex complexity at most five. \n\n"}
{"id": "1807.02571", "contents": "Title: Leveraging Well-Conditioned Bases: Streaming \\& Distributed Summaries in\n  Minkowski $p$-Norms Abstract: Work on approximate linear algebra has led to efficient distributed and\nstreaming algorithms for problems such as approximate matrix multiplication,\nlow rank approximation, and regression, primarily for the Euclidean norm\n$\\ell_2$. We study other $\\ell_p$ norms, which are more robust for $p < 2$, and\ncan be used to find outliers for $p > 2$. Unlike previous algorithms for such\nnorms, we give algorithms that are (1) deterministic, (2) work simultaneously\nfor every $p \\geq 1$, including $p = \\infty$, and (3) can be implemented in\nboth distributed and streaming environments. We apply our results to\n$\\ell_p$-regression, entrywise $\\ell_1$-low rank approximation, and approximate\nmatrix multiplication. \n\n"}
{"id": "1807.03721", "contents": "Title: On the complexity of the (approximate) nearest colored node problem Abstract: Given a graph $G=(V,E)$ where each vertex is assigned a color from the set\n$C=\\{c_1, c_2, .., c_\\sigma\\}$. In the (approximate) nearest colored node\nproblem, we want to query, given $v \\in V$ and $c \\in C$, for the (approximate)\ndistance $\\widehat{\\mathbf{dist}}(v, c)$ from $v$ to the nearest node of color\n$c$. For any integer $1 \\leq k \\leq \\log n$, we present a Color Distance Oracle\n(also often referred to as Vertex-label Distance Oracle) of stretch $4k-5$\nusing space $O(kn\\sigma^{1/k})$ and query time $O(\\log{k})$. This improves the\nquery time from $O(k)$ to $O(\\log{k})$ over the best known Color Distance\nOracle by Chechik \\cite{DBLP:journals/corr/abs-1109-3114}. We then prove a\nlower bound in the cell probe model showing that our query time is optimal in\nregard to space up to constant factors. We also investigate dynamic settings of\nthe problem and find new upper and lower bounds. \n\n"}
{"id": "1807.05241", "contents": "Title: Approximation Algorithms for Clustering via Weighted Impurity Measures Abstract: An impurity measures $I:{R}^k \\to {R}^+$ maps a $k$-dimensional vector ${\\bf\nv}$ to a non-negative value $I({\\bf v})$ so that the more homogeneous ${\\bf\nv}$, the larger its impurity. We study clustering based on impurity measures:\ngiven a collection $V$ of $n$ many $k$-dimensional vectors and an impurity\nmeasure $I$, the goal is to find a partition ${\\cal P}$ of $V$ into $L$ groups\n$V_1,\\ldots,V_L$ that minimizes the total impurities of the groups in ${\\cal\nP}$, i.e., $I({\\cal P})= \\sum_{m=1}^{L} I(\\sum_{{\\bf v} \\in V_m}{\\bf v}).$\n  Impurity minimization is widely used as quality assessment measure in\nprobability distribution clustering and in categorical clustering where it is\nnot possible to rely on geometric properties of the data set. However, in\ncontrast to the case of metric based clustering, the current knowledge of\nimpurity measure based clustering in terms of approximation and\ninapproximability results is very limited.\n  Our research contributes to fill this gap. We first present a simple linear\ntime algorithm that simultaneously achieves $3$-approximation for the Gini\nimpurity measure and $O(\\log(\\sum_{{\\bf v} \\in V} \\| {\\bf v}\n\\|_1))$-approximation for the Entropy impurity measure. Then, for the Entropy\nimpurity measure---where we also show that finding the optimal clustering is\nstrongly NP-hard---we are able to design a polynomial time\n$O(\\log^2(\\min\\{k,L\\}))$-approximation algorithm. Our algorithm relies on a\nnontrivial characterization of a class of clusterings that necessarily includes\na partition achieving $O(\\log^2(\\min\\{k,L\\}))$--approximation of the impurity\nof the optimal partition. Remarkably, this is the first polynomial time\nalgorithm with approximation guarantee independent of the number of\npoints/vector and not relying on any restriction on the components of the\nvectors for producing clusterings with minimum entropy. \n\n"}
{"id": "1807.05443", "contents": "Title: Exact Algorithms and Lower Bounds for Stable Instances of Euclidean\n  k-Means Abstract: We investigate the complexity of solving stable or perturbation-resilient\ninstances of $k$-Means and $k$-Median clustering in fixed dimension Euclidean\nmetrics (more generally doubling metrics). The notion of stable (perturbation\nresilient) instances was introduced by Bilu and Linial [2010] and Awasthi et\nal. [2012]. In our context we say a $k$-Means instance is $\\alpha$-stable if\nthere is a unique OPT which remains optimum if distances are (non-uniformly)\nstretched by a factor of at most $\\alpha$. Stable clustering instances have\nbeen studied to explain why heuristics such as Lloyd's algorithm perform well\nin practice. In this work we show that for any fixed $\\epsilon>0$,\n$(1+\\epsilon)$-stable instances of $k$-Means in doubling metrics can be solved\nin polynomial time. More precisely we show a natural multiswap local search\nalgorithm finds OPT for $(1+\\epsilon)$-stable instances of $k$-Means and\n$k$-Median in a polynomial number of iterations. We complement this result by\nshowing that under a new PCP theorem, this is essentially tight: that when the\ndimension d is part of the input, there is a fixed $\\epsilon_0>0$ s.t. there is\nnot even a PTAS for $(1+\\epsilon_0)$-stable $k$-Means in $R^d$ unless NP=RP. To\ndo this, we consider a robust property of CSPs; call an instance stable if\nthere is a unique optimum solution $x^*$ and for any other solution $x'$, the\nnumber of unsatisfied clauses is proportional to the Hamming distance between\n$x^*$ and $x'$. Dinur et al. have already shown stable QSAT is hard to\napproximate for some constant Q, our hypothesis is simply that stable QSAT with\nbounded variable occurrence is also hard. Given this hypothesis we consider\n\"stability-preserving\" reductions to prove our hardness for stable k-Means.\nSuch reductions seem to be more fragile than standard L-reductions and may be\nof further use to demonstrate other stable optimization problems are hard. \n\n"}
{"id": "1807.06168", "contents": "Title: Anaconda: A Non-Adaptive Conditional Sampling Algorithm for Distribution\n  Testing Abstract: We investigate distribution testing with access to non-adaptive conditional\nsamples. In the conditional sampling model, the algorithm is given the\nfollowing access to a distribution: it submits a query set $S$ to an oracle,\nwhich returns a sample from the distribution conditioned on being from $S$. In\nthe non-adaptive setting, all query sets must be specified in advance of\nviewing the outcomes.\n  Our main result is the first polylogarithmic-query algorithm for equivalence\ntesting, deciding whether two unknown distributions are equal to or far from\neach other. This is an exponential improvement over the previous best upper\nbound, and demonstrates that the complexity of the problem in this model is\nintermediate to the the complexity of the problem in the standard sampling\nmodel and the adaptive conditional sampling model. We also significantly\nimprove the sample complexity for the easier problems of uniformity and\nidentity testing. For the former, our algorithm requires only $\\tilde O(\\log\nn)$ queries, matching the information-theoretic lower bound up to a $O(\\log\n\\log n)$-factor.\n  Our algorithm works by reducing the problem from $\\ell_1$-testing to\n$\\ell_\\infty$-testing, which enjoys a much cheaper sample complexity.\nNecessitated by the limited power of the non-adaptive model, our algorithm is\nvery simple to state. However, there are significant challenges in the\nanalysis, due to the complex structure of how two arbitrary distributions may\ndiffer. \n\n"}
{"id": "1807.06456", "contents": "Title: Quantum Chebyshev's Inequality and Applications Abstract: In this paper we provide new quantum algorithms with polynomial speed-up for\na range of problems for which no such results were known, or we improve\nprevious algorithms. First, we consider the approximation of the frequency\nmoments $F_k$ of order $k \\geq 3$ in the multi-pass streaming model with\nupdates (turnstile model). We design a $P$-pass quantum streaming algorithm\nwith memory $M$ satisfying a tradeoff of $P^2 M = \\tilde{O}(n^{1-2/k})$,\nwhereas the best classical algorithm requires $P M = \\Theta(n^{1-2/k})$. Then,\nwe study the problem of estimating the number $m$ of edges and the number $t$\nof triangles given query access to an $n$-vertex graph. We describe optimal\nquantum algorithms that perform $\\tilde{O}(\\sqrt{n}/m^{1/4})$ and\n$\\tilde{O}(\\sqrt{n}/t^{1/6} + m^{3/4}/\\sqrt{t})$ queries respectively. This is\na quadratic speed-up compared to the classical complexity of these problems.\n  For this purpose we develop a new quantum paradigm that we call Quantum\nChebyshev's inequality. Namely we demonstrate that, in a certain model of\nquantum sampling, one can approximate with relative error the mean of any\nrandom variable with a number of quantum samples that is linear in the ratio of\nthe square root of the variance to the mean. Classically the dependency is\nquadratic. Our algorithm subsumes a previous result of Montanaro [Mon15]. This\nnew paradigm is based on a refinement of the Amplitude Estimation algorithm of\nBrassard et al. [BHMT02] and of previous quantum algorithms for the mean\nestimation problem. We show that this speed-up is optimal, and we identify\nanother common model of quantum sampling where it cannot be obtained. For our\napplications, we also adapt the variable-time amplitude amplification technique\nof Ambainis [Amb10] into a variable-time amplitude estimation algorithm. \n\n"}
{"id": "1807.06921", "contents": "Title: Time-Bounded Influence Diffusion with Incentives Abstract: A widely studied model of influence diffusion in social networks represents\nthe network as a graph $G=(V,E)$ with an influence threshold $t(v)$ for each\nnode. Initially the members of an initial set $S\\subseteq V$ are influenced.\nDuring each subsequent round, the set of influenced nodes is augmented by\nincluding every node $v$ that has at least $t(v)$ previously influenced\nneighbours. The general problem is to find a small initial set that influences\nthe whole network. In this paper we extend this model by using\n\\emph{incentives} to reduce the thresholds of some nodes. The goal is to\nminimize the total of the incentives required to ensure that the process\ncompletes within a given number of rounds. The problem is hard to approximate\nin general networks. We present polynomial-time algorithms for paths, trees,\nand complete networks. \n\n"}
{"id": "1807.07193", "contents": "Title: Linear Programming Approximations for Index Coding Abstract: Index coding, a source coding problem over broadcast channels, has been a\nsubject of both theoretical and practical interest since its introduction (by\nBirk and Kol, 1998). In short, the problem can be defined as follows: there is\nan input $\\textbf{x} \\triangleq (\\textbf{x}_1, \\dots, \\textbf{x}_n)$, a set of\n$n$ clients who each desire a single symbol $\\textbf{x}_i$ of the input, and a\nbroadcaster whose goal is to send as few messages as possible to all clients so\nthat each one can recover its desired symbol. Additionally, each client has\nsome predetermined \"side information,\" corresponding to certain symbols of the\ninput $\\textbf{x}$, which we represent as the \"side information graph\"\n$\\mathcal{G}$. The graph $\\mathcal{G}$ has a vertex $v_i$ for each client and a\ndirected edge $(v_i, v_j)$ indicating that client $i$ knows the $j$th symbol of\nthe input. Given a fixed side information graph $\\mathcal{G}$, we are\ninterested in determining or approximating the \"broadcast rate\" of index coding\non the graph, i.e. the fewest number of messages the broadcaster can transmit\nso that every client gets their desired information.\n  Using index coding schemes based on linear programs (LPs), we take a\ntwo-pronged approach to approximating the broadcast rate. First, extending\nearlier work on planar graphs, we focus on approximating the broadcast rate for\nspecial graph families such as graphs with small chromatic number and disk\ngraphs. In certain cases, we are able to show that simple LP-based schemes give\nconstant-factor approximations of the broadcast rate, which seem extremely\ndifficult to obtain in the general case. Second, we provide several LP-based\nschemes for the general case which are not constant-factor approximations, but\nwhich strictly improve on the prior best-known schemes. \n\n"}
{"id": "1807.07527", "contents": "Title: Optimal Las Vegas Approximate Near Neighbors in $\\ell_p$ Abstract: We show that approximate near neighbor search in high dimensions can be\nsolved in a Las Vegas fashion (i.e., without false negatives) for $\\ell_p$\n($1\\le p\\le 2$) while matching the performance of optimal locality-sensitive\nhashing. Specifically, we construct a data-independent Las Vegas data structure\nwith query time $O(dn^{\\rho})$ and space usage $O(dn^{1+\\rho})$ for $(r, c\nr)$-approximate near neighbors in $\\mathbb{R}^{d}$ under the $\\ell_p$ norm,\nwhere $\\rho = 1/c^p + o(1)$. Furthermore, we give a Las Vegas\nlocality-sensitive filter construction for the unit sphere that can be used\nwith the data-dependent data structure of Andoni et al. (SODA 2017) to achieve\noptimal space-time tradeoffs in the data-dependent setting. For the symmetric\ncase, this gives us a data-dependent Las Vegas data structure with query time\n$O(dn^{\\rho})$ and space usage $O(dn^{1+\\rho})$ for $(r, c r)$-approximate near\nneighbors in $\\mathbb{R}^{d}$ under the $\\ell_p$ norm, where $\\rho = 1/(2c^p -\n1) + o(1)$.\n  Our data-independent construction improves on the recent Las Vegas data\nstructure of Ahle (FOCS 2017) for $\\ell_p$ when $1 < p\\le 2$. Our\ndata-dependent construction does even better for $\\ell_p$ for all $p\\in [1, 2]$\nand is the first Las Vegas approximate near neighbors data structure to make\nuse of data-dependent approaches. We also answer open questions of Indyk (SODA\n2000), Pagh (SODA 2016), and Ahle by showing that for approximate near\nneighbors, Las Vegas data structures can match state-of-the-art Monte Carlo\ndata structures in performance for both the data-independent and data-dependent\nsettings and across space-time tradeoffs. \n\n"}
{"id": "1807.08678", "contents": "Title: Submodular Function Maximization in Parallel via the Multilinear\n  Relaxation Abstract: Balkanski and Singer [5] recently initiated the study of adaptivity (or\nparallelism) for constrained submodular function maximization, and studied the\nsetting of a cardinality constraint. Very recent improvements for this problem\nby Balkanski, Rubinstein, and Singer [6] and Ene and Nguyen [21] resulted in a\nnear-optimal $(1-1/e-\\epsilon)$-approximation in $O(\\log n/\\epsilon^2)$ rounds\nof adaptivity. Partly motivated by the goal of extending these results to more\ngeneral constraints, we describe parallel algorithms for approximately\nmaximizing the multilinear relaxation of a monotone submodular function subject\nto packing constraints. Formally our problem is to maximize $F(x)$ over $x \\in\n[0,1]^{n}$ subject to $Ax \\le 1$ where $F$ is the multilinear relaxation of a\nmonotone submodular function. Our algorithm achieves a near-optimal\n$(1-1/e-\\epsilon)$-approximation in $O(\\log^2 m \\log n/\\epsilon^4)$ rounds\nwhere $n$ is the cardinality of the ground set and $m$ is the number of packing\nconstraints. For many constraints of interest, the resulting fractional\nsolution can be rounded via known randomized rounding schemes that are\noblivious to the specific submodular function. We thus derive randomized\nalgorithms with poly-logarithmic adaptivity for a number of constraints\nincluding partition and laminar matroids, matchings, knapsack constraints, and\ntheir intersections. \n\n"}
{"id": "1807.08949", "contents": "Title: A Note on Clustering Aggregation for Binary Clusterings Abstract: We consider the clustering aggregation problem in which we are given a set of\nclusterings and want to find an aggregated clustering which minimizes the sum\nof mismatches to the input clusterings. In the binary case (each clustering is\na bipartition) this problem was known to be NP-hard under Turing reductions. We\nstrengthen this result by providing a polynomial-time many-one reduction. Our\nresult also implies that no $2^{o(n)}\\cdot |I'|^{O(1)}$-time algorithm exists\nthat solves any given clustering instance $I'$ with $n$ elements, unless the\n\\ETH{} fails. On the positive side, we show that the problem is fixed-parameter\ntractable with respect to the number of input clusterings and we give an\ninteger linear programming formulation. \n\n"}
{"id": "1807.09358", "contents": "Title: An Approximation Algorithm for Risk-averse Submodular Optimization Abstract: We study the problem of incorporating risk while making combinatorial\ndecisions under uncertainty. We formulate a discrete submodular maximization\nproblem for selecting a set using Conditional-Value-at-Risk (CVaR), a risk\nmetric commonly used in financial analysis. While CVaR has recently been used\nin optimization of linear cost functions in robotics, we take the first stages\ntowards extending this to discrete submodular optimization and provide several\npositive results. Specifically, we propose the Sequential Greedy Algorithm that\nprovides an approximation guarantee on finding the maxima of the CVaR cost\nfunction under a matroidal constraint. The approximation guarantee shows that\nthe solution produced by our algorithm is within a constant factor of the\noptimal and an additive term that depends on the optimal. Our analysis uses the\ncurvature of the submodular set function, and proves that the algorithm runs in\npolynomial time. This formulates a number of combinatorial optimization\nproblems that appear in robotics. We use two such problems, vehicle assignment\nunder uncertainty for mobility-on-demand and sensor selection with failures for\nenvironmental monitoring, as case studies to demonstrate the efficacy of our\nformulation. \n\n"}
{"id": "1807.09386", "contents": "Title: On the Randomized Complexity of Minimizing a Convex Quadratic Function Abstract: Minimizing a convex, quadratic objective of the form\n$f_{\\mathbf{A},\\mathbf{b}}(x) := \\frac{1}{2}x^\\top \\mathbf{A} x - \\langle\n\\mathbf{b}, x \\rangle$ for $\\mathbf{A} \\succ 0 $ is a fundamental problem in\nmachine learning and optimization. In this work, we prove gradient-query\ncomplexity lower bounds for minimizing convex quadratic functions which apply\nto both deterministic and \\emph{randomized} algorithms. Specifically, for\n$\\kappa > 1$, we exhibit a distribution over $(\\mathbf{A},\\mathbf{b})$ with\ncondition number $\\mathrm{cond}(\\mathbf{A}) \\le \\kappa$, such that any\n\\emph{randomized} algorithm requires $\\Omega(\\sqrt{\\kappa})$ gradient queries\nto find a solution $\\hat x$ for which $\\|\\hat x - \\mathbf x_\\star\\| \\le\n\\epsilon_0\\|\\mathbf{x}_{\\star}\\|$, where $\\mathbf x_{\\star} =\n\\mathbf{A}^{-1}\\mathbf{b}$ is the optimal solution, and $\\epsilon_0$ a small\nconstant. Setting $\\kappa =1/\\epsilon$, this lower bound implies the minimax\nrate of $T = \\Omega(\\lambda_1(\\mathbf{A})\\|\\mathbf\nx_\\star\\|^2/\\sqrt{\\epsilon})$ queries required to minimize an arbitrary convex\nquadratic function up to error $f(\\hat{x}) - f(\\mathbf x_\\star) \\le \\epsilon$.\n  Our lower bound holds for a distribution derived from classical ensembles in\nrandom matrix theory, and relies on a careful reduction from adaptively\nestimating a planted vector $\\mathbf u$ in a deformed Wigner model. A key step\nin deriving sharp lower bounds is demonstrating that the optimization error\n$\\mathbf x_\\star - \\hat x$ cannot align too closely with $\\mathbf{u}$. To this\nend, we prove an upper bound on the cosine between $\\mathbf x_\\star - \\hat x$\nand $\\mathbf u$ in terms of the MMSE of estimating the plant $\\mathbf u$ in a\ndeformed Wigner model. We then bound the MMSE by carefully modifying a result\ndue to Lelarge and Miolane 2016, which rigorously establishes a general\nreplica-symmetric formula for planted matrix models. \n\n"}
{"id": "1807.11135", "contents": "Title: A Hybrid Quantum-Classical Paradigm to Mitigate Embedding Costs in\n  Quantum Annealing---Abridged Version Abstract: Quantum annealing has shown significant potential as an approach to near-term\nquantum computing. Despite promising progress towards obtaining a quantum\nspeedup, quantum annealers are limited by the need to embed problem instances\nwithin the (often highly restricted) connectivity graph of the annealer. This\nembedding can be costly to perform and may destroy any computational speedup.\nHere we present a hybrid quantum-classical paradigm to help mitigate this\nlimitation, and show how a raw speedup that is negated by the embedding time\ncan nonetheless be exploited in certain circumstances. We illustrate this\napproach with initial results on a proof-of-concept implementation of an\nalgorithm for the dynamically weighted maximum independent set problem. \n\n"}
{"id": "1807.11609", "contents": "Title: A Proof of Entropy Minimization for Outputs in Deletion Channels via\n  Hidden Word Statistics Abstract: From the output produced by a memoryless deletion channel from a uniformly\nrandom input of known length $n$, one obtains a posterior distribution on the\nchannel input. The difference between the Shannon entropy of this distribution\nand that of the uniform prior measures the amount of information about the\nchannel input which is conveyed by the output of length $m$, and it is natural\nto ask for which outputs this is extremized. This question was posed in a\nprevious work, where it was conjectured on the basis of experimental data that\nthe entropy of the posterior is minimized and maximized by the constant strings\n$\\texttt{000}\\ldots$ and $\\texttt{111}\\ldots$ and the alternating strings\n$\\texttt{0101}\\ldots$ and $\\texttt{1010}\\ldots$ respectively. In the present\nwork we confirm the minimization conjecture in the asymptotic limit using\nresults from hidden word statistics. We show how the analytic-combinatorial\nmethods of Flajolet, Szpankowski and Vall\\'ee for dealing with the hidden\npattern matching problem can be applied to resolve the case of fixed output\nlength and $n\\rightarrow\\infty$, by obtaining estimates for the entropy in\nterms of the moments of the posterior distribution and establishing its\nminimization via a measure of autocorrelation. \n\n"}
{"id": "1807.11702", "contents": "Title: Efficient Computation of Sequence Mappability Abstract: In the $(k,m)$-mappability problem, for a given sequence $T$ of length $n$,\nthe goal is to compute a table whose $i$th entry is the number of indices $j\n\\ne i$ such that the length-$m$ substrings of $T$ starting at positions $i$ and\n$j$ have at most $k$ mismatches. Previous works on this problem focused on\nheuristics computing a rough approximation of the result or on the case of\n$k=1$. We present several efficient algorithms for the general case of the\nproblem. Our main result is an algorithm that, for $k=\\mathcal{O}(1)$, works in\n$\\mathcal{O}(n)$ space and, with high probability, in $\\mathcal{O}(n \\cdot\n\\min\\{m^k,\\log^k n\\})$ time. Our algorithm requires a careful adaptation of the\n$k$-errata trees of Cole et al. [STOC 2004] to avoid multiple counting of pairs\nof substrings. Our technique can also be applied to solve the all-pairs Hamming\ndistance problem introduced by Crochemore et al. [WABI 2017]. We further\ndevelop $\\mathcal{O}(n^2)$-time algorithms to compute all $(k,m)$-mappability\ntables for a fixed $m$ and all $k\\in \\{0,\\ldots,m\\}$ or a fixed $k$ and all\n$m\\in\\{k,\\ldots,n\\}$. Finally, we show that, for $k,m = \\Theta(\\log n)$, the\n$(k,m)$-mappability problem cannot be solved in strongly subquadratic time\nunless the Strong Exponential Time Hypothesis fails.\n  This is an improved and extended version of a paper that was presented at\nSPIRE 2018. \n\n"}
{"id": "1808.01949", "contents": "Title: OptStream: Releasing Time Series Privately Abstract: Many applications of machine learning and optimization operate on data\nstreams. While these datasets are fundamental to fuel decision-making\nalgorithms, often they contain sensitive information about individuals and\ntheir usage poses significant privacy risks. Motivated by an application in\nenergy systems, this paper presents OPTSTREAM, a novel algorithm for releasing\ndifferentially private data streams under the w-event model of privacy.\nOPTSTREAM is a 4-step procedure consisting of sampling, perturbation,\nreconstruction, and post-processing modules. First, the sampling module selects\na small set of points to access in each period of interest. Then, the\nperturbation module adds noise to the sampled data points to guarantee privacy.\nNext, the reconstruction module reassembles non-sampled data points from the\nperturbed sample points. Finally, the post-processing module uses convex\noptimization over the private output of the previous modules, as well as the\nprivate answers of additional queries on the data stream, to improve accuracy\nby redistributing the added noise. OPTSTREAM is evaluated on a test case\ninvolving the release of a real data stream from the largest European\ntransmission operator. Experimental results show that OPTSTREAM may not only\nimprove the accuracy of state-of-the-art methods by at least one order of\nmagnitude but also supports accurate load forecasting on the private data. \n\n"}
{"id": "1808.02359", "contents": "Title: On the Computational Complexity of Length- and Neighborhood-Constrained\n  Path Problems Abstract: Finding paths in graphs is a fundamental graph-theoretic task. In this work,\nwe we are concerned with finding a path with some constraints on its length and\nthe number of vertices neighboring the path, that is, being outside of and\nincident with the path. Herein, we consider short and long paths on the one\nside, and small and large neighborhoods on the other side---yielding four\ndecision problems. We show that all four problems are NP-complete, even in\nplanar graphs with small maximum degree. Moreover, we study all four variants\nwhen parameterized by a bound $k$ on the length of the path, by a bound $\\ell$\non the size of neighborhood, and by $k + \\ell$. \n\n"}
{"id": "1808.03494", "contents": "Title: On the Complexity of Solving Subtraction Games Abstract: We study algorithms for solving Subtraction games, which sometimes are\nreferred to as one-heap Nim games. We describe a quantum algorithm which is\napplicable to any game on DAG, and show that its query compexity for solving an\narbitrary Subtraction game of $n$ stones is $O(n^{3/2}\\log n)$. The best known\ndeterministic algorithms for solving such games are based on the dynamic\nprogramming approach. We show that this approach is asymptotically optimal and\nthat classical query complexity for solving a Subtraction game is generally\n$\\Theta(n^2)$. This paper perhaps is the first explicit \"quantum\" contribution\nto algorithmic game theory. \n\n"}
{"id": "1808.03526", "contents": "Title: Maximum Weight Online Matching with Deadlines Abstract: We study the problem of matching agents who arrive at a marketplace over time\nand leave after d time periods. Agents can only be matched while they are\npresent in the marketplace. Each pair of agents can yield a different match\nvalue, and the planner's goal is to maximize the total value over a finite time\nhorizon. First we study the case in which vertices arrive in an adversarial\norder. We provide a randomized 0.25-competitive algorithm building on a result\nby Feldman et al. (2009) and Lehman et al. (2006). We extend the model to the\ncase in which departure times are drawn independently from a distribution with\nnon-decreasing hazard rate, for which we establish a 1/8-competitive algorithm.\n  When the arrival order is chosen uniformly at random, we show that a batching\nalgorithm, which computes a maximum-weighted matching every (d+1) periods, is\n0.279-competitive. \n\n"}
{"id": "1808.04539", "contents": "Title: Constructions of maximally recoverable local reconstruction codes via\n  function fields Abstract: Local Reconstruction Codes (LRCs) allow for recovery from a small number of\nerasures in a local manner based on just a few other codeword symbols. A\nmaximally recoverable (MR) LRC offers the best possible blend of such local and\nglobal fault tolerance, guaranteeing recovery from all erasure patterns which\nare information-theoretically correctable given the presence of local recovery\ngroups. In an $(n,r,h,a)$-LRC, the $n$ codeword symbols are partitioned into\n$r$ disjoint groups each of which include $a$ local parity checks capable of\nlocally correcting $a$ erasures.\n  MR LRCs have received much attention recently, with many explicit\nconstructions covering different regimes of parameters. Unfortunately, all\nknown constructions require a large field size that exponential in $h$ or $a$,\nand it is of interest to obtain MR LRCs of minimal possible field size. In this\nwork, we develop an approach based on function fields to construct MR LRCs. Our\nmethod recovers, and in most parameter regimes improves, the field size of\nprevious approaches. For instance, for the case of small $r \\ll \\epsilon \\log\nn$ and large $h \\ge \\Omega(n^{1-\\epsilon})$, we improve the field size from\nroughly $n^h$ to $n^{\\epsilon h}$. For the case of $a=1$ (one local parity\ncheck), we improve the field size quadratically from $r^{h(h+1)}$ to $r^{h\n\\lfloor (h+1)/2 \\rfloor}$ for some range of $r$. The improvements are modest,\nbut more importantly are obtained in a unified manner via a promising new idea. \n\n"}
{"id": "1808.04558", "contents": "Title: Explicit construction of optimal locally recoverable codes of distance 5\n  and 6 via binary constant weight codes Abstract: It was shown in \\cite{GXY18} that the length $n$ of a $q$-ary linear locally\nrecoverable code with distance $d\\ge 5$ is upper bounded by $O(dq^3)$. Thus, it\nis a challenging problem to construct $q$-ary locally recoverable codes with\ndistance $d\\ge 5$ and length approaching the upper bound. The paper\n\\cite{GXY18} also gave an algorithmic construction of $q$-ary locally\nrecoverable codes with locality $r$ and length $n=\\Omega_r(q^2)$ for $d=5$ and\n$6$, where $\\Omega_r$ means that the implicit constant depends on locality $r$.\nIn the present paper, we present an explicit construction of $q$-ary locally\nrecoverable codes of distance $d= 5$ and $6$ via binary constant weight codes.\nIt turns out that (i) our construction is simpler and more explicit; and (ii)\nlengths of our codes are larger than those given in \\cite{GXY18}. \n\n"}
{"id": "1808.05879", "contents": "Title: Cardinality Estimators do not Preserve Privacy Abstract: Cardinality estimators like HyperLogLog are sketching algorithms that\nestimate the number of distinct elements in a large multiset. Their use in\nprivacy-sensitive contexts raises the question of whether they leak private\ninformation. In particular, can they provide any privacy guarantees while\npreserving their strong aggregation properties? We formulate an abstract notion\nof cardinality estimators, that captures this aggregation requirement: one can\nmerge sketches without losing precision. We propose an attacker model and a\ncorresponding privacy definition, strictly weaker than differential privacy: we\nassume that the attacker has no prior knowledge of the data. We then show that\nif a cardinality estimator satisfies this definition, then it cannot have a\nreasonable level of accuracy. We prove similar results for weaker versions of\nour definition, and analyze the privacy of existing algorithms, showing that\ntheir average privacy loss is significant, even for multisets with large\ncardinalities. We conclude that strong aggregation requirements are\nincompatible with any reasonable definition of privacy, and that cardinality\nestimators should be considered as sensitive as raw data. We also propose risk\nmitigation strategies for their real-world applications. \n\n"}
{"id": "1808.10328", "contents": "Title: Asymptotically Optimal Codes Correcting Fixed-Length Duplication Errors\n  in DNA Storage Systems Abstract: A (tandem) duplication of length $ k $ is an insertion of an exact copy of a\nsubstring of length $ k $ next to its original position. This and related types\nof impairments are of relevance in modeling communication in the presence of\nsynchronization errors, as well as in several information storage applications.\nWe demonstrate that Levenshtein's construction of binary codes correcting\ninsertions of zeros is, with minor modifications, applicable also to channels\nwith arbitrary alphabets and with duplication errors of arbitrary (but fixed)\nlength $ k $. Furthermore, we derive bounds on the cardinality of optimal $ q\n$-ary codes correcting up to $ t $ duplications of length $ k $, and establish\nthe following corollaries in the asymptotic regime of growing block-length: 1.)\nthe presented family of codes is optimal for every $ q, t, k $, in the sense of\nthe asymptotic scaling of code redundancy; 2.) the upper bound, when\nspecialized to $ q = 2 $, $ k = 1 $, improves upon Levenshtein's bound for\nevery $ t \\geq 3 $; 3.) the bounds coincide for $ t = 1 $, thus yielding the\nexact asymptotic behavior of the size of optimal single-duplication-correcting\ncodes. \n\n"}
{"id": "1808.10530", "contents": "Title: Hashing-Based-Estimators for Kernel Density in High Dimensions Abstract: Given a set of points $P\\subset \\mathbb{R}^{d}$ and a kernel $k$, the Kernel\nDensity Estimate at a point $x\\in\\mathbb{R}^{d}$ is defined as\n$\\mathrm{KDE}_{P}(x)=\\frac{1}{|P|}\\sum_{y\\in P} k(x,y)$. We study the problem\nof designing a data structure that given a data set $P$ and a kernel function,\nreturns *approximations to the kernel density* of a query point in *sublinear\ntime*. We introduce a class of unbiased estimators for kernel density\nimplemented through locality-sensitive hashing, and give general theorems\nbounding the variance of such estimators. These estimators give rise to\nefficient data structures for estimating the kernel density in high dimensions\nfor a variety of commonly used kernels. Our work is the first to provide\ndata-structures with theoretical guarantees that improve upon simple random\nsampling in high dimensions. \n\n"}
{"id": "1808.10826", "contents": "Title: Upward Planar Morphs Abstract: We prove that, given two topologically-equivalent upward planar straight-line\ndrawings of an $n$-vertex directed graph $G$, there always exists a morph\nbetween them such that all the intermediate drawings of the morph are upward\nplanar and straight-line. Such a morph consists of $O(1)$ morphing steps if $G$\nis a reduced planar $st$-graph, $O(n)$ morphing steps if $G$ is a planar\n$st$-graph, $O(n)$ morphing steps if $G$ is a reduced upward planar graph, and\n$O(n^2)$ morphing steps if $G$ is a general upward planar graph. Further, we\nshow that $\\Omega(n)$ morphing steps might be necessary for an upward planar\nmorph between two topologically-equivalent upward planar straight-line drawings\nof an $n$-vertex path. \n\n"}
{"id": "1809.00394", "contents": "Title: Mining Frequent Patterns in Evolving Graphs Abstract: Given a labeled graph, the frequent-subgraph mining (FSM) problem asks to\nfind all the $k$-vertex subgraphs that appear with frequency greater than a\ngiven threshold. FSM has numerous applications ranging from biology to network\nscience, as it provides a compact summary of the characteristics of the graph.\nHowever, the task is challenging, even more so for evolving graphs due to the\nstreaming nature of the input and the exponential time complexity of the\nproblem.\n  In this paper, we initiate the study of the approximate FSM problem in both\nincremental and fully-dynamic streaming settings, where arbitrary edges can be\nadded or removed from the graph. For each streaming setting, we propose\nalgorithms that can extract a high-quality approximation of the frequent\n$k$-vertex subgraphs for a given threshold, at any given time instance, with\nhigh probability. In contrast to the existing state-of-the-art solutions that\nrequire iterating over the entire set of subgraphs for any update, our\nalgorithms operate by maintaining a uniform sample of $k$-vertex subgraphs with\noptimized neighborhood-exploration procedures local to the updates. We provide\ntheoretical analysis of the proposed algorithms and empirically demonstrate\nthat the proposed algorithms generate high-quality results compared to\nbaselines. \n\n"}
{"id": "1809.02350", "contents": "Title: FRESH: Fr\\'echet Similarity with Hashing Abstract: This paper studies the $r$-range search problem for curves under the\ncontinuous Fr\\'echet distance: given a dataset $S$ of $n$ polygonal curves and\na threshold $r>0$, construct a data structure that, for any query curve $q$,\nefficiently returns all entries in $S$ with distance at most $r$ from $q$. We\npropose FRESH, an approximate and randomized approach for $r$-range search,\nthat leverages on a locality sensitive hashing scheme for detecting candidate\nnear neighbors of the query curve, and on a subsequent pruning step based on a\ncascade of curve simplifications. We experimentally compare \\fresh to exact and\ndeterministic solutions, and we show that high performance can be reached by\nsuitably relaxing precision and recall. \n\n"}
{"id": "1809.03589", "contents": "Title: Unconstraining graph-constrained group testing Abstract: In network tomography, one goal is to identify a small set of failed links in\na network, by sending a few packets through the network and seeing which reach\ntheir destination. This problem can be seen as a variant of combinatorial group\ntesting, which has been studied before under the moniker \"graph-constrained\ngroup testing.\"\n  The main contribution of this work is to show that for most graphs, the\n\"constraints\" imposed by the underlying network topology are no constraint at\nall. That is, the number of tests required to identify the failed links in\n\"graph-constrained\" group testing is near-optimal even for the corresponding\ngroup testing problem with no graph constraints.\n  Our approach is based on a simple randomized construction of tests, to\nanalyze our construction, we prove new results about the size of giant\ncomponents in randomly sparsified graphs. Finally, we provide empirical results\nwhich suggest that our connected-subgraph tests perform better not just in\ntheory but also in practice, and in particular perform better on a real-world\nnetwork topology. \n\n"}
{"id": "1809.04091", "contents": "Title: Quantum Algorithms for Structured Prediction Abstract: We introduce two quantum algorithms for solving structured prediction\nproblems. We first show that a stochastic gradient descent that uses the\nquantum minimum finding algorithm and takes its probabilistic failure into\naccount solves the structured prediction problem with a runtime that scales\nwith the square root of the size of the label space, and in $\\widetilde\nO\\left(1/\\epsilon\\right)$ with respect to the precision, $\\epsilon$, of the\nsolution. Motivated by robust inference techniques in machine learning, we then\nintroduce another quantum algorithm that solves a smooth approximation of the\nstructured prediction problem with a similar quantum speedup in the size of the\nlabel space and a similar scaling in the precision parameter. In doing so, we\nanalyze a variant of stochastic gradient descent for convex optimization in the\npresence of an additive error in the calculation of the gradients, and show\nthat its convergence rate does not deteriorate if the additive errors are of\nthe order $O(\\sqrt\\epsilon)$. This algorithm uses quantum Gibbs sampling at\ntemperature $\\Omega (\\epsilon)$ as a subroutine. Based on these theoretical\nobservations, we propose a method for using quantum Gibbs samplers to combine\nfeedforward neural networks with probabilistic graphical models for quantum\nmachine learning. Our numerical results using Monte Carlo simulations on an\nimage tagging task demonstrate the benefit of the approach. \n\n"}
{"id": "1809.06171", "contents": "Title: Best-case and Worst-case Sparsifiability of Boolean CSPs Abstract: We continue the investigation of polynomial-time sparsification for\nNP-complete Boolean Constraint Satisfaction Problems (CSPs). The goal in\nsparsification is to reduce the number of constraints in a problem instance\nwithout changing the answer, such that a bound on the number of resulting\nconstraints can be given in terms of the number of variables n. We investigate\nhow the worst-case sparsification size depends on the types of constraints\nallowed in the problem formulation (the constraint language). Two algorithmic\nresults are presented. The first result essentially shows that for any arity k,\nthe only constraint type for which no nontrivial sparsification is possible has\nexactly one falsifying assignment, and corresponds to logical OR (up to\nnegations). Our second result concerns linear sparsification, that is, a\nreduction to an equivalent instance with O(n) constraints. Using linear algebra\nover rings of integers modulo prime powers, we give an elegant necessary and\nsufficient condition for a constraint type to be captured by a degree-1\npolynomial over such a ring, which yields linear sparsifications. The\ncombination of these algorithmic results allows us to prove two\ncharacterizations that capture the optimal sparsification sizes for a range of\nBoolean CSPs. For NP-complete Boolean CSPs whose constraints are symmetric (the\nsatisfaction depends only on the number of 1 values in the assignment, not on\ntheir positions), we give a complete characterization of which constraint\nlanguages allow for a linear sparsification. For Boolean CSPs in which every\nconstraint has arity at most three, we characterize the optimal size of\nsparsifications in terms of the largest OR that can be expressed by the\nconstraint language. \n\n"}
{"id": "1809.06523", "contents": "Title: Negative type diversities, a multi-dimensional analogue of negative type\n  metrics Abstract: Diversities are a generalization of metric spaces in which a non-negative\nvalue is assigned to all finite subsets of a set, rather than just to pairs of\npoints. Here we provide an analogue of the theory of negative type metrics for\ndiversities. We introduce negative type diversities, and show that, as in the\nmetric space case, they are a generalization of $L_1$-embeddable diversities.\nWe provide a number of characterizations of negative type diversities,\nincluding a geometric characterisation. Much of the recent interest in negative\ntype metrics stems from the connections between metric embeddings and\napproximation algorithms. We extend some of this work into the diversity\nsetting, showing that lower bounds for embeddings of negative type metrics into\n$L_1$ can be extended to diversities by using recently established extremal\nresults on hypergraphs. \n\n"}
{"id": "1809.08822", "contents": "Title: Almost optimal algorithms for diameter-optimally augmenting trees Abstract: We consider the problem of augmenting an $n$-vertex tree with one shortcut in\norder to minimize the diameter of the resulting graph. The tree is embedded in\nan unknown space and we have access to an oracle that, when queried on a pair\nof vertices $u$ and $v$, reports the weight of the shortcut $(u,v)$ in constant\ntime. Previously, the problem was solved in $O(n^2 \\log^3 n)$ time for general\nweights [Oh and Ahn, ISAAC 2016], in $O(n^2 \\log n)$ time for trees embedded in\na metric space [Gro{\\ss}e et al., {\\tt arXiv:1607.05547}], and in $O(n \\log n)$\ntime for paths embedded in a metric space [Wang, WADS 2017]. Furthermore, a\n$(1+\\varepsilon)$-approximation algorithm running in $O(n+1/\\varepsilon^{3})$\nhas been designed for paths embedded in $\\mathbb{R}^d$, for constant values of\n$d$ [Gro{\\ss}e et al., ICALP 2015].\n  The contribution of this paper is twofold: we address the problem for trees\n(not only paths) and we also improve upon all known results. More precisely, we\ndesign a {\\em time-optimal} $O(n^2)$ time algorithm for general weights.\nMoreover, for trees embedded in a metric space, we design (i) an exact $O(n\n\\log n)$ time algorithm and (ii) a $(1+\\varepsilon)$-approximation algorithm\nthat runs in $O\\big(n+ \\varepsilon^{-1}\\log \\varepsilon^{-1}\\big)$ time. \n\n"}
{"id": "1809.09819", "contents": "Title: Improved bounds on Fourier entropy and Min-entropy Abstract: Given a Boolean function $f:\\{-1,1\\}^n\\to \\{-1,1\\}$, the Fourier distribution\nassigns probability $\\widehat{f}(S)^2$ to $S\\subseteq [n]$. The Fourier\nEntropy-Influence (FEI) conjecture of Friedgut and Kalai asks if there exist a\nuniversal constant C>0 such that $H(\\hat{f}^2)\\leq C Inf(f)$, where\n$H(\\hat{f}^2)$ is the Shannon entropy of the Fourier distribution of $f$ and\n$Inf(f)$ is the total influence of $f$.\n  1) We consider the weaker Fourier Min-entropy-Influence (FMEI) conjecture.\nThis asks if $H_{\\infty}(\\hat{f}^2)\\leq C Inf(f)$, where\n$H_{\\infty}(\\hat{f}^2)$ is the min-entropy of the Fourier distribution. We show\n$H_{\\infty}(\\hat{f}^2)\\leq 2C_{\\min}^\\oplus(f)$, where $C_{\\min}^\\oplus(f)$ is\nthe minimum parity certificate complexity of $f$. We also show that for every\n$\\epsilon\\geq 0$, we have $H_{\\infty}(\\hat{f}^2)\\leq 2\\log\n(\\|\\hat{f}\\|_{1,\\epsilon}/(1-\\epsilon))$, where $\\|\\hat{f}\\|_{1,\\epsilon}$ is\nthe approximate spectral norm of $f$. As a corollary, we verify the FMEI\nconjecture for the class of read-$k$ $DNF$s (for constant $k$).\n  2) We show that $H(\\hat{f}^2)\\leq 2 aUC^\\oplus(f)$, where $aUC^\\oplus(f)$ is\nthe average unambiguous parity certificate complexity of $f$. This improves\nupon Chakraborty et al. An important consequence of the FEI conjecture is the\nlong-standing Mansour's conjecture. We show that a weaker version of FEI\nalready implies Mansour's conjecture: is $H(\\hat{f}^2)\\leq C\n\\min\\{C^0(f),C^1(f)\\}$?, where $C^0(f), C^1(f)$ are the 0- and 1-certificate\ncomplexities of $f$, respectively.\n  3) We study what FEI implies about the structure of polynomials that\n1/3-approximate a Boolean function. We pose a conjecture (which is implied by\nFEI): no \"flat\" degree-$d$ polynomial of sparsity $2^{\\omega(d)}$ can\n1/3-approximate a Boolean function. We prove this conjecture unconditionally\nfor a particular class of polynomials. \n\n"}
{"id": "1809.10325", "contents": "Title: Being Corrupt Requires Being Clever, But Detecting Corruption Doesn't Abstract: We consider a variation of the problem of corruption detection on networks\nposed by Alon, Mossel, and Pemantle '15. In this model, each vertex of a graph\ncan be either truthful or corrupt. Each vertex reports about the types\n(truthful or corrupt) of all its neighbors to a central agency, where truthful\nnodes report the true types they see and corrupt nodes report adversarially.\nThe central agency aggregates these reports and attempts to find a single\ntruthful node. Inspired by real auditing networks, we pose our problem for\narbitrary graphs and consider corruption through a computational lens. We\nidentify a key combinatorial parameter of the graph $m(G)$, which is the\nminimal number of corrupted agents needed to prevent the central agency from\nidentifying a single truthful node. We give an efficient (in fact, linear time)\nalgorithm for the central agency to identify a truthful node that is successful\nwhenever the number of corrupt nodes is less than $m(G)/2$. On the other hand,\nwe prove that for any constant $\\alpha > 1$, it is NP-hard to find a subset of\nnodes $S$ in $G$ such that corrupting $S$ prevents the central agency from\nfinding one truthful node and $|S| \\leq \\alpha m(G)$, assuming the Small Set\nExpansion Hypothesis (Raghavendra and Steurer, STOC '10). We conclude that\nbeing corrupt requires being clever, while detecting corruption does not.\n  Our main technical insight is a relation between the minimum number of\ncorrupt nodes required to hide all truthful nodes and a certain notion of\nvertex separability for the underlying graph. Additionally, this insight lets\nus design an efficient algorithm for a corrupt party to decide which graphs\nrequire the fewest corrupted nodes, up to a multiplicative factor of $O(\\log\nn)$. \n\n"}
{"id": "1809.10469", "contents": "Title: Probabilistic Analysis of Edge Elimination for Euclidean TSP Abstract: One way to speed up the calculation of optimal TSP tours in practice is\neliminating edges that are certainly not in the optimal tour as a preprocessing\nstep. In order to do so several edge elimination approaches have been proposed\nin the past. In this work we investigate two of them in the scenario where the\ninput consists of $n$ independently distributed random points in the\n2-dimensional unit square with bounded density function from above and below by\narbitrary positive constants. We show that after the edge elimination procedure\nof Hougardy and Schroeder the expected number of remaining edges is\n$\\Theta(n)$, while after that the non-recursive part of Jonker and Volgenant\nthe expected number of remaining edges is $\\Theta(n^2)$. \n\n"}
{"id": "1810.01136", "contents": "Title: A deterministic polynomial kernel for Odd Cycle Transversal and Vertex\n  Multiway Cut in planar graphs Abstract: We show that Odd Cycle Transversal and Vertex Multiway Cut admit\ndeterministic polynomial kernels when restricted to planar graphs and\nparameterized by the solution size. This answers a question of Saurabh. On the\nway to these results, we provide an efficient sparsification routine in the\nflavor of the sparsification routine used for the Steiner Tree problem in\nplanar graphs (FOCS 2014). It differs from the previous work because it\npreserves the existence of low-cost subgraphs that are not necessarily Steiner\ntrees in the original plane graph, but structures that turn into (supergraphs\nof) Steiner trees after adding all edges between pairs of vertices that lie on\na common face. We also show connections between Vertex Multiway Cut and the\nVertex Planarization problem, where the existence of a polynomial kernel\nremains an important open problem. \n\n"}
{"id": "1810.02183", "contents": "Title: Revealing Network Structure, Confidentially: Improved Rates for\n  Node-Private Graphon Estimation Abstract: Motivated by growing concerns over ensuring privacy on social networks, we\ndevelop new algorithms and impossibility results for fitting complex\nstatistical models to network data subject to rigorous privacy guarantees. We\nconsider the so-called node-differentially private algorithms, which compute\ninformation about a graph or network while provably revealing almost no\ninformation about the presence or absence of a particular node in the graph.\n  We provide new algorithms for node-differentially private estimation for a\npopular and expressive family of network models: stochastic block models and\ntheir generalization, graphons. Our algorithms improve on prior work, reducing\ntheir error quadratically and matching, in many regimes, the optimal nonprivate\nalgorithm. We also show that for the simplest random graph models ($G(n,p)$ and\n$G(n,m)$), node-private algorithms can be qualitatively more accurate than for\nmore complex models---converging at a rate of $\\frac{1}{\\epsilon^2 n^{3}}$\ninstead of $\\frac{1}{\\epsilon^2 n^2}$. This result uses a new extension lemma\nfor differentially private algorithms that we hope will be broadly useful. \n\n"}
{"id": "1810.02304", "contents": "Title: Polynomial-time Recognition of 4-Steiner Powers Abstract: The $k^{th}$-power of a given graph $G=(V,E)$ is obtained from $G$ by adding\nan edge between every two distinct vertices at a distance at most $k$ in $G$.\nWe call $G$ a $k$-Steiner power if it is an induced subgraph of the\n$k^{th}$-power of some tree. Our main contribution is a polynomial-time\nrecognition algorithm of $4$-Steiner powers, thereby extending the\ndecade-year-old results of (Lin, Kearney and Jiang, ISAAC'00) for $k=1,2$ and\n(Chang and Ko, WG'07) for $k=3$.\n  A graph $G$ is termed $k$-leaf power if there is some tree $T$ such that: all\nvertices in $V(G)$ are leaf-nodes of $T$, and $G$ is an induced subgraph of the\n$k^{th}$-power of $T$. As a byproduct of our main result, we give the first\nknown polynomial-time recognition algorithm for $6$-leaf powers. \n\n"}
{"id": "1810.03980", "contents": "Title: Explicit optimal-length locally repairable codes of distance 5 Abstract: Locally repairable codes (LRCs) have received significant recent attention as\na method of designing data storage systems robust to server failure. Optimal\nLRCs offer the ideal trade-off between minimum distance and locality, a measure\nof the cost of repairing a single codeword symbol. For optimal LRCs with\nminimum distance greater than or equal to 5, block length is bounded by a\npolynomial function of alphabet size. In this paper, we give explicit\nconstructions of optimal-length (in terms of alphabet size), optimal LRCs with\nminimum distance equal to 5. \n\n"}
{"id": "1810.04109", "contents": "Title: Performance Guarantees of Distributed Algorithms for QoS in Wireless Ad\n  Hoc Networks Abstract: Consider a wireless network where each communication link has a minimum\nbandwidth quality-of-service requirement. Certain pairs of wireless links\ninterfere with each other due to being in the same vicinity, and this\ninterference is modeled by a conflict graph. Given the conflict graph and link\nbandwidth requirements, the objective is to determine, using only localized\ninformation, whether the demands of all the links can be satisfied. At one\nextreme, each node knows the demands of only its neighbors; at the other\nextreme, there exists an optimal, centralized scheduler that has global\ninformation. The present work interpolates between these two extremes by\nquantifying the tradeoff between the degree of decentralization and the\nperformance of the distributed algorithm. This open problem is resolved for the\nprimary interference model, and the following general result is obtained: if\neach node knows the demands of all links in a ball of radius $d$ centered at\nthe node, then there is a distributed algorithm whose performance is away from\nthat of an optimal, centralized algorithm by a factor of at most\n$(2d+3)/(2d+2)$. The tradeoff between performance and complexity of the\ndistributed algorithm is also analyzed. It is shown that for line networks\nunder the protocol interference model, the row constraints are a factor of at\nmost $3$ away from optimal. Both bounds are best possible. \n\n"}
{"id": "1810.04254", "contents": "Title: Extreme Classification in Log Memory Abstract: We present Merged-Averaged Classifiers via Hashing (MACH) for\nK-classification with ultra-large values of K. Compared to traditional\none-vs-all classifiers that require O(Kd) memory and inference cost, MACH only\nneed O(d log K) (d is dimensionality )memory while only requiring O(K log K + d\nlog K) operation for inference. MACH is a generic K-classification algorithm,\nwith provably theoretical guarantees, which requires O(log K) memory without\nany assumption on the relationship between classes. MACH uses universal hashing\nto reduce classification with a large number of classes to few independent\nclassification tasks with small (constant) number of classes. We provide\ntheoretical quantification of discriminability-memory tradeoff. With MACH we\ncan train ODP dataset with 100,000 classes and 400,000 features on a single\nTitan X GPU, with the classification accuracy of 19.28%, which is the\nbest-reported accuracy on this dataset. Before this work, the best performing\nbaseline is a one-vs-all classifier that requires 40 billion parameters (160 GB\nmodel size) and achieves 9% accuracy. In contrast, MACH can achieve 9% accuracy\nwith 480x reduction in the model size (of mere 0.3GB). With MACH, we also\ndemonstrate complete training of fine-grained imagenet dataset (compressed size\n104GB), with 21,000 classes, on a single GPU. To the best of our knowledge,\nthis is the first work to demonstrate complete training of these extreme-class\ndatasets on a single Titan X. \n\n"}
{"id": "1810.07896", "contents": "Title: Solving Linear Programs in the Current Matrix Multiplication Time Abstract: This paper shows how to solve linear programs of the form $\\min_{Ax=b,x\\geq0}\nc^\\top x$ with $n$ variables in time\n$$O^*((n^{\\omega}+n^{2.5-\\alpha/2}+n^{2+1/6}) \\log(n/\\delta))$$ where $\\omega$\nis the exponent of matrix multiplication, $\\alpha$ is the dual exponent of\nmatrix multiplication, and $\\delta$ is the relative accuracy. For the current\nvalue of $\\omega\\sim2.37$ and $\\alpha\\sim0.31$, our algorithm takes\n$O^*(n^{\\omega} \\log(n/\\delta))$ time. When $\\omega = 2$, our algorithm takes\n$O^*(n^{2+1/6} \\log(n/\\delta))$ time.\n  Our algorithm utilizes several new concepts that we believe may be of\nindependent interest:\n  $\\bullet$ We define a stochastic central path method.\n  $\\bullet$ We show how to maintain a projection matrix\n$\\sqrt{W}A^{\\top}(AWA^{\\top})^{-1}A\\sqrt{W}$ in sub-quadratic time under\n$\\ell_{2}$ multiplicative changes in the diagonal matrix $W$. \n\n"}
{"id": "1810.08054", "contents": "Title: Locally Private Mean Estimation: Z-test and Tight Confidence Intervals Abstract: This work provides tight upper- and lower-bounds for the problem of mean\nestimation under $\\epsilon$-differential privacy in the local model, when the\ninput is composed of $n$ i.i.d. drawn samples from a normal distribution with\nvariance $\\sigma$. Our algorithms result in a $(1-\\beta)$-confidence interval\nfor the underlying distribution's mean $\\mu$ of length $\\tilde O\\left(\n\\frac{\\sigma \\sqrt{\\log(\\frac 1 \\beta)}}{\\epsilon\\sqrt n} \\right)$. In\naddition, our algorithms leverage binary search using local differential\nprivacy for quantile estimation, a result which may be of separate interest.\nMoreover, we prove a matching lower-bound (up to poly-log factors), showing\nthat any one-shot (each individual is presented with a single query) local\ndifferentially private algorithm must return an interval of length\n$\\Omega\\left( \\frac{\\sigma\\sqrt{\\log(1/\\beta)}}{\\epsilon\\sqrt{n}}\\right)$. \n\n"}
{"id": "1810.08109", "contents": "Title: Best-of-two-worlds analysis of online search Abstract: In search problems, a mobile searcher seeks to locate a target that hides in\nsome unknown position of the environment. Such problems are typically\nconsidered to be of an on-line nature, in that the input is unknown to the\nsearcher, and the performance of a search strategy is usually analyzed by means\nof the standard framework of the competitive ratio, which compares the cost\nincurred by the searcher to an optimal strategy that knows the location of the\ntarget. However, one can argue that even for simple search problems,\ncompetitive analysis fails to distinguish between strategies which,\nintuitively, should have different performance in practice.\n  Motivated by the above, in this work we introduce and study measures\nsupplementary to competitive analysis in the context of search problems. In\nparticular, we focus on the well-known problem of linear search, informally\nknown as the cow-path problem, for which there is an infinite number of\nstrategies that achieve an optimal competitive ratio equal to 9. We propose a\nmeasure that reflects the rate at which the line is being explored by the\nsearcher, and which can be seen as an extension of the bijective ratio over an\nuncountable set of requests. Using this measure we show that a natural strategy\nthat explores the line aggressively is optimal among all 9-competitive\nstrategies. This provides, in particular, a strict separation from the\ncompetitively optimal doubling strategy, which is much more conservative in\nterms of exploration. We also provide evidence that this aggressiveness is\nrequisite for optimality, by showing that any optimal strategy must mimic the\naggressive strategy in its first few explorations. \n\n"}
{"id": "1810.08345", "contents": "Title: A Matrix Chernoff Bound for Strongly Rayleigh Distributions and Spectral\n  Sparsifiers from a few Random Spanning Trees Abstract: Strongly Rayleigh distributions are a class of negatively dependent\ndistributions of binary-valued random variables [Borcea, Branden, Liggett JAMS\n09]. Recently, these distributions have played a crucial role in the analysis\nof algorithms for fundamental graph problems, e.g. Traveling Salesman Problem\n[Gharan, Saberi, Singh FOCS 11]. We prove a new matrix Chernoff bound for\nStrongly Rayleigh distributions.\n  As an immediate application, we show that adding together the Laplacians of\n$\\epsilon^{-2} \\log^2 n$ random spanning trees gives an $(1\\pm \\epsilon)$\nspectral sparsifiers of graph Laplacians with high probability. Thus, we\npositively answer an open question posed in [Baston, Spielman, Srivastava, Teng\nJACM 13]. Our number of spanning trees for spectral sparsifier matches the\nnumber of spanning trees required to obtain a cut sparsifier in [Fung,\nHariharan, Harvey, Panigraphi STOC 11]. The previous best result was by naively\napplying a classical matrix Chernoff bound which requires $\\epsilon^{-2} n \\log\nn$ spanning trees. For the tree averaging procedure to agree with the original\ngraph Laplacian in expectation, each edge of the tree should be reweighted by\nthe inverse of the edge leverage score in the original graph. We also show that\nwhen using this reweighting of the edges, the Laplacian of single random tree\nis bounded above in the PSD order by the original graph Laplacian times a\nfactor $\\log n$ with high probability, i.e. $L_T \\preceq O(\\log n) L_G$.\n  We show a lower bound that almost matches our last result, namely that in\nsome graphs, with high probability, the random spanning tree is $\\it{not}$\nbounded above in the spectral order by $\\frac{\\log n}{\\log\\log n}$ times the\noriginal graph Laplacian. We also show a lower bound that in $\\epsilon^{-2}\n\\log n$ spanning trees are necessary to get a $(1\\pm \\epsilon)$ spectral\nsparsifier. \n\n"}
{"id": "1810.09027", "contents": "Title: Massively Parallel Approximate Distance Sketches Abstract: Data structures that allow efficient distance estimation (distance oracles,\ndistance sketches, etc.) have been extensively studied, and are particularly\nwell studied in centralized models and classical distributed models such as\nCONGEST. We initiate their study in newer (and arguably more realistic) models\nof distributed computation: the Congested Clique model and the Massively\nParallel Computation (MPC) model. We provide efficient constructions in both of\nthese models, but our core results are for MPC. In MPC we give two main\nresults: an algorithm that constructs stretch/space optimal distance sketches\nbut takes a (small) polynomial number of rounds, and an algorithm that\nconstructs distance sketches with worse stretch but that only takes\npolylogarithmic rounds.\n  Along the way, we show that other useful combinatorial structures can also be\ncomputed in MPC. In particular, one key component we use to construct distance\nsketches are an MPC construction of the hopsets of Elkin and Neiman (2016).\nThis result has additional applications such as the first polylogarithmic time\nalgorithm for constant approximate single-source shortest paths for weighted\ngraphs in the low memory MPC setting. \n\n"}
{"id": "1810.11262", "contents": "Title: Some comments on the structure of the best known networks sorting 16\n  elements Abstract: We propose an explanation of the structure of the best known sorting networks\nfor 16 elements with respect to the complexity and to the depth due to Green\nand van Voorhis. \n\n"}
{"id": "1810.11421", "contents": "Title: Mining Maximal Induced Bicliques using Odd Cycle Transversals Abstract: Many common graph data mining tasks take the form of identifying dense\nsubgraphs (e.g. clustering, clique-finding, etc). In biological applications,\nthe natural model for these dense substructures is often a complete bipartite\ngraph (biclique), and the problem requires enumerating all maximal bicliques\n(instead of just identifying the largest or densest). The best known algorithm\nin general graphs is due to Dias et al., and runs in time O(M |V|^4 ), where M\nis the number of maximal induced bicliques (MIBs) in the graph. When the graph\nbeing searched is itself bipartite, Zhang et al. give a faster algorithm where\nthe time per MIB depends on the number of edges in the graph. In this work, we\npresent a new algorithm for enumerating MIBs in general graphs, whose run time\ndepends on how \"close to bipartite\" the input is. Specifically, the runtime is\nparameterized by the size k of an odd cycle transversal (OCT), a vertex set\nwhose deletion results in a bipartite graph. Our algorithm runs in time O(M\n|V||E|k^2 3^(k/3) ), which is an improvement on Dias et al. whenever k <=\n3log_3(|V|). We implement our algorithm alongside a variant of Dias et al.'s in\nopen-source C++ code, and experimentally verify that the OCT-based approach is\nfaster in practice on graphs with a wide variety of sizes, densities, and OCT\ndecompositions. \n\n"}
{"id": "1810.12030", "contents": "Title: Simon's problem for linear functions Abstract: Simon's problem asks the following: determine if a function $f: \\{0,1\\}^n\n\\rightarrow \\{0,1\\}^n$ is one-to-one or if there exists a unique $s \\in\n\\{0,1\\}^n$ such that $f(x) = f(x \\oplus s)$ for all $x \\in \\{0,1\\}^n$, given\nthe promise that exactly one of the two holds. A classical algorithm that can\nsolve this problem for every $f$ requires $2^{\\Omega(n)}$ queries to $f$. Simon\nshowed that there is a quantum algorithm that can solve this promise problem\nfor every $f$ using only $\\mathcal O(n)$ quantum queries to $f$. A matching\nlower bound on the number of quantum queries was given by Koiran et al., even\nfor functions $f: {\\mathbb{F}_p^n} \\to {\\mathbb{F}_p^n}$. We give a short proof\nthat $\\mathcal O(n)$ quantum queries is optimal even when we are additionally\npromised that $f$ is linear. This is somewhat surprising because for linear\nfunctions there even exists a classical $n$-query algorithm. \n\n"}
{"id": "1810.12272", "contents": "Title: Adversarial Risk and Robustness: General Definitions and Implications\n  for the Uniform Distribution Abstract: We study adversarial perturbations when the instances are uniformly\ndistributed over $\\{0,1\\}^n$. We study both \"inherent\" bounds that apply to any\nproblem and any classifier for such a problem as well as bounds that apply to\nspecific problems and specific hypothesis classes.\n  As the current literature contains multiple definitions of adversarial risk\nand robustness, we start by giving a taxonomy for these definitions based on\ntheir goals, we identify one of them as the one guaranteeing misclassification\nby pushing the instances to the error region. We then study some classic\nalgorithms for learning monotone conjunctions and compare their adversarial\nrisk and robustness under different definitions by attacking the hypotheses\nusing instances drawn from the uniform distribution. We observe that sometimes\nthese definitions lead to significantly different bounds. Thus, this study\nadvocates for the use of the error-region definition, even though other\ndefinitions, in other contexts, may coincide with the error-region definition.\n  Using the error-region definition of adversarial perturbations, we then study\ninherent bounds on risk and robustness of any classifier for any classification\nproblem whose instances are uniformly distributed over $\\{0,1\\}^n$. Using the\nisoperimetric inequality for the Boolean hypercube, we show that for initial\nerror $0.01$, there always exists an adversarial perturbation that changes\n$O(\\sqrt{n})$ bits of the instances to increase the risk to $0.5$, making\nclassifier's decisions meaningless. Furthermore, by also using the central\nlimit theorem we show that when $n\\to \\infty$, at most $c \\cdot \\sqrt{n}$ bits\nof perturbations, for a universal constant $c< 1.17$, suffice for increasing\nthe risk to $0.5$, and the same $c \\cdot \\sqrt{n} $ bits of perturbations on\naverage suffice to increase the risk to $1$, hence bounding the robustness by\n$c \\cdot \\sqrt{n}$. \n\n"}
{"id": "1811.00148", "contents": "Title: Recovery Guarantees for Quadratic Tensors with Sparse Observations Abstract: We consider the tensor completion problem of predicting the missing entries\nof a tensor. The commonly used CP model has a triple product form, but an\nalternate family of quadratic models, which are the sum of pairwise products\ninstead of a triple product, have emerged from applications such as\nrecommendation systems. Non-convex methods are the method of choice for\nlearning quadratic models, and this work examines their sample complexity and\nerror guarantee. Our main result is that with the number of samples being only\nlinear in the dimension, all local minima of the mean squared error objective\nare global minima and recover the original tensor. We substantiate our\ntheoretical results with experiments on synthetic and real-world data, showing\nthat quadratic models have better performance than CP models where there are a\nlimited amount of observations available. \n\n"}
{"id": "1811.00816", "contents": "Title: Planar Graphs of Bounded Degree have Constant Queue Number Abstract: A \\emph{queue layout} of a graph consists of a \\emph{linear order} of its\nvertices and a partition of its edges into \\emph{queues}, so that no two\nindependent edges of the same queue are nested. The \\emph{queue number} of a\ngraph is the minimum number of queues required by any of its queue layouts. A\nlong-standing conjecture by Heath, Leighton and Rosenberg states that the queue\nnumber of planar graphs is bounded. This conjecture has been partially settled\nin the positive for several subfamilies of planar graphs (most of which have\nbounded treewidth). In this paper, we make a further important step towards\nsettling this conjecture. We prove that planar graphs of bounded degree (which\nmay have unbounded treewidth) have bounded queue number.\n  A notable implication of this result is that every planar graph of bounded\ndegree admits a three-dimensional straight-line grid drawing in linear volume.\nFurther implications are that every planar graph of bounded degree has bounded\ntrack number, and that every $k$-planar graph (i.e., every graph that can be\ndrawn in the plane with at most $k$ crossings per edge) of bounded degree has\nbounded queue number. \n\n"}
{"id": "1811.00944", "contents": "Title: Spectral Methods from Tensor Networks Abstract: A tensor network is a diagram that specifies a way to \"multiply\" a collection\nof tensors together to produce another tensor (or matrix). Many existing\nalgorithms for tensor problems (such as tensor decomposition and tensor PCA),\nalthough they are not presented this way, can be viewed as spectral methods on\nmatrices built from simple tensor networks. In this work we leverage the full\npower of this abstraction to design new algorithms for certain continuous\ntensor decomposition problems.\n  An important and challenging family of tensor problems comes from orbit\nrecovery, a class of inference problems involving group actions (inspired by\napplications such as cryo-electron microscopy). Orbit recovery problems over\nfinite groups can often be solved via standard tensor methods. However, for\ninfinite groups, no general algorithms are known. We give a new spectral\nalgorithm based on tensor networks for one such problem: continuous\nmulti-reference alignment over the infinite group SO(2). Our algorithm extends\nto the more general heterogeneous case. \n\n"}
{"id": "1811.01442", "contents": "Title: Towards a Zero-One Law for Column Subset Selection Abstract: There are a number of approximation algorithms for NP-hard versions of low\nrank approximation, such as finding a rank-$k$ matrix $B$ minimizing the sum of\nabsolute values of differences to a given $n$-by-$n$ matrix $A$,\n$\\min_{\\textrm{rank-}k~B}\\|A-B\\|_1$, or more generally finding a rank-$k$\nmatrix $B$ which minimizes the sum of $p$-th powers of absolute values of\ndifferences, $\\min_{\\textrm{rank-}k~B}\\|A-B\\|_p^p$. Many of these algorithms\nare linear time columns subset selection algorithms, returning a subset of\n$\\mathrm{poly}(k \\log n)$ columns whose cost is no more than a\n$\\mathrm{poly}(k)$ factor larger than the cost of the best rank-$k$ matrix. The\nabove error measures are special cases of the following general entrywise low\nrank approximation problem: given an arbitrary function $g:\\mathbb{R}\n\\rightarrow \\mathbb{R}_{\\geq 0}$, find a rank-$k$ matrix $B$ which minimizes\n$\\|A-B\\|_g = \\sum_{i,j}g(A_{i,j}-B_{i,j})$. A natural question is which\nfunctions $g$ admit efficient approximation algorithms? Indeed, this is a\ncentral question of recent work studying generalized low rank models. In this\nwork we give approximation algorithms for $\\textit{every}$ function $g$ which\nis approximately monotone and satisfies an approximate triangle inequality, and\nwe show both of these conditions are necessary. Further, our algorithm is\nefficient if the function $g$ admits an efficient approximate regression\nalgorithm. Our approximation algorithms handle functions which are not even\nscale-invariant, such as the Huber loss function, which we show have very\ndifferent structural properties than $\\ell_p$-norms, e.g., one can show the\nlack of scale-invariance causes any column subset selection algorithm to\nprovably require a $\\sqrt{\\log n}$ factor larger number of columns than\n$\\ell_p$-norms; nevertheless we design the first efficient column subset\nselection algorithms for such error measures. \n\n"}
{"id": "1811.01740", "contents": "Title: On the complexity of cache analysis for different replacement policies Abstract: Modern processors use cache memory: a memory access that \"hits\" the cache\nreturns early, while a \"miss\" takes more time. Given a memory access in a\nprogram, cache analysis consists in deciding whether this access is always a\nhit, always a miss, or is a hit or a miss depending on execution. Such an\nanalysis is of high importance for bounding the worst-case execution time of\nsafety-critical real-time programs.There exist multiple possible policies for\nevicting old data from the cache when new data are brought in, and different\npolicies, though apparently similar in goals and performance, may be very\ndifferent from the analysis point of view. In this paper, we explore these\ndifferences from a complexity-theoretical point of view. Specifically, we show\nthat, among the common replacement policies, LRU (Least Recently Used) is the\nonly one whose analysis is NP-complete, whereas the analysis problems for the\nother policies are PSPACE-complete. \n\n"}
{"id": "1811.03195", "contents": "Title: Performance of Johnson-Lindenstrauss Transform for k-Means and k-Medians\n  Clustering Abstract: Consider an instance of Euclidean $k$-means or $k$-medians clustering. We\nshow that the cost of the optimal solution is preserved up to a factor of\n$(1+\\varepsilon)$ under a projection onto a random $O(\\log(k / \\varepsilon) /\n\\varepsilon^2)$-dimensional subspace. Further, the cost of every clustering is\npreserved within $(1+\\varepsilon)$. More generally, our result applies to any\ndimension reduction map satisfying a mild sub-Gaussian-tail condition. Our\nbound on the dimension is nearly optimal. Additionally, our result applies to\nEuclidean $k$-clustering with the distances raised to the $p$-th power for any\nconstant $p$.\n  For $k$-means, our result resolves an open problem posed by Cohen, Elder,\nMusco, Musco, and Persu (STOC 2015); for $k$-medians, it answers a question\nraised by Kannan. \n\n"}
{"id": "1811.03491", "contents": "Title: Degree-$d$ Chow Parameters Robustly Determine Degree-$d$ PTFs (and\n  Algorithmic Applications) Abstract: The degree-$d$ Chow parameters of a Boolean function $f: \\{-1,1\\}^n \\to\n\\mathbb{R}$ are its degree at most $d$ Fourier coefficients. It is well-known\nthat degree-$d$ Chow parameters uniquely characterize degree-$d$ polynomial\nthreshold functions (PTFs) within the space of all bounded functions. In this\npaper, we prove a robust version of this theorem: For $f$ any Boolean\ndegree-$d$ PTF and $g$ any bounded function, if the degree-$d$ Chow parameters\nof $f$ are close to the degree-$d$ Chow parameters of $g$ in $\\ell_2$-norm,\nthen $f$ is close to $g$ in $\\ell_1$-distance. Notably, our bound relating the\ntwo distances is completely independent of the dimension $n$. That is, we show\nthat Boolean degree-$d$ PTFs are {\\em robustly identifiable} from their\ndegree-$d$ Chow parameters. Results of this form had been shown for the $d=1$\ncase~\\cite{OS11:chow, DeDFS14}, but no non-trivial bound was previously known\nfor $d >1$.\n  Our robust identifiability result gives the following algorithmic\napplications: First, we show that Boolean degree-$d$ PTFs can be efficiently\napproximately reconstructed from approximations to their degree-$d$ Chow\nparameters. This immediately implies that degree-$d$ PTFs are efficiently\nlearnable in the uniform distribution $d$-RFA\nmodel~\\cite{BenDavidDichterman:98}. As a byproduct of our approach, we also\nobtain the first low integer-weight approximations of degree-$d$ PTFs, for\n$d>1$. As our second application, our robust identifiability result gives the\nfirst efficient algorithm, with dimension-independent error guarantees, for\nmalicious learning of Boolean degree-$d$ PTFs under the uniform distribution. \n\n"}
{"id": "1811.06026", "contents": "Title: Incentivizing Exploration with Selective Data Disclosure Abstract: We propose and design recommendation systems that incentivize efficient\nexploration. Agents arrive sequentially, choose actions and receive rewards,\ndrawn from fixed but unknown action-specific distributions. The recommendation\nsystem presents each agent with actions and rewards from a subsequence of past\nagents, chosen ex ante. Thus, the agents engage in sequential social learning,\nmoderated by these subsequences. We asymptotically attain optimal regret rate\nfor exploration, using a flexible frequentist behavioral model and mitigating\nrationality and commitment assumptions inherent in prior work. We suggest three\ncomponents of effective recommendation systems: independent focus groups, group\naggregators, and interlaced information structures. \n\n"}
{"id": "1811.06336", "contents": "Title: State Complexity Characterizations of Parameterized Degree-Bounded Graph\n  Connectivity, Sub-Linear Space Computation, and the Linear Space Hypothesis Abstract: The linear space hypothesis is a practical working hypothesis, which\noriginally states the insolvability of a restricted 2CNF Boolean formula\nsatisfiability problem parameterized by the number of Boolean variables. From\nthis hypothesis, it naturally follows that the degree-3 directed graph\nconnectivity problem (3DSTCON) parameterized by the number of vertices in a\ngiven graph cannot belong to PsubLIN, composed of all parameterized decision\nproblems computable by polynomial-time, sub-linear-space deterministic Turing\nmachines. This hypothesis immediately implies L$\\neq$NL and it was used as a\nsolid foundation to obtain new lower bounds on the computational complexity of\nvarious NL search and NL optimization problems. The state complexity of\ntransformation refers to the cost of converting one type of finite automata to\nanother type, where the cost is measured in terms of the increase of the number\nof inner states of the converted automata from that of the original automata.\nWe relate the linear space hypothesis to the state complexity of transforming\nrestricted 2-way nondeterministic finite automata to computationally equivalent\n2-way alternating finite automata having narrow computation graphs. For this\npurpose, we present state complexity characterizations of 3DSTCON and PsubLIN.\nWe further characterize a nonuniform version of the linear space hypothesis in\nterms of the state complexity of transformation. \n\n"}
{"id": "1811.07515", "contents": "Title: Classical Algorithms from Quantum and Arthur-Merlin Communication\n  Protocols Abstract: The polynomial method from circuit complexity has been applied to several\nfundamental problems and obtains the state-of-the-art running times. As\nobserved in [Alman and Williams, STOC 2017], almost all applications of the\npolynomial method in algorithm design ultimately rely on certain low-rank\ndecompositions of the computation matrices corresponding to key subroutines.\nThey suggest that making use of low-rank decompositions directly could lead to\nmore powerful algorithms, as the polynomial method is just one way to derive\nsuch a decomposition. Inspired by their observation, in this paper, we study\nanother way of systematically constructing low-rank decompositions of matrices\nwhich could be used by algorithms. It is known that various types of\ncommunication protocols lead to certain low-rank decompositions (e.g.,\n$\\mathsf{P}$ protocols/rank, $\\mathsf{BQP}$ protocols/approximate rank). These\nare usually interpreted as approaches for proving communication lower bounds,\nwhile in this work we explore the other direction.\n  We have the two generic algorithmic applications of communication protocols.\nThe first connection is that a fast $\\mathsf{BQP}$ communication protocol for a\nfunction $f$ implies a fast deterministic additive approximate counting\nalgorithm for a related pair counting problem. The second connection is that a\nfast $\\mathsf{AM}^{\\mathsf{cc}}$ protocol for a function $f$ implies a\nfaster-than-bruteforce algorithm for $f\\textsf{-Satisfying-Pair}$.\n  We also apply our second connection to shed some light on long-standing open\nproblems in communication complexity. We show that if the Longest Common\nSubsequence problem admits an efficient $\\mathsf{AM}^{\\mathsf{cc}}$ protocol,\nthen polynomial-size Formula-$\\textsf{SAT}$ admits a $2^{n - n^{1-\\delta}}$\ntime algorithm for any constant $\\delta > 0$. \n\n"}
{"id": "1811.08506", "contents": "Title: Tight Approximation Ratio for Minimum Maximal Matching Abstract: We study a combinatorial problem called Minimum Maximal Matching, where we\nare asked to find in a general graph the smallest that can not be extended. We\nshow that this problem is hard to approximate with a constant smaller than 2,\nassuming the Unique Games Conjecture.\n  As a corollary we show, that Minimum Maximal Matching in bipartite graphs is\nhard to approximate with constant smaller than $\\frac{4}{3}$, with the same\nassumption. With a stronger variant of the Unique Games Conjecture --- that is\nSmall Set Expansion Hypothesis --- we are able to improve the hardness result\nup to the factor of $\\frac{3}{2}$. \n\n"}
{"id": "1811.08811", "contents": "Title: $k$-Cut: A Simple Approximately-Uniform Method for Sampling Ballots in\n  Post-Election Audits Abstract: We present an approximate sampling framework and discuss how risk-limiting\naudits can compensate for these approximations, while maintaining their\n\"risk-limiting\" properties. Our framework is general and can compensate for\ncounting mistakes made during audits.\n  Moreover, we present and analyze a simple approximate sampling\nmethod,\"$k$-cut\", for picking a ballot randomly from a stack, without counting.\nOur method involves doing $k$ \"cuts\", each involving moving a random portion of\nballots from the top to the bottom of the stack, and then picking the ballot on\ntop. Unlike conventional methods of picking a ballot at random, $k$-cut does\nnot require identification numbers on the ballots or counting many ballots per\ndraw. We analyze how close the distribution of chosen ballots is to the uniform\ndistribution, and design different mitigation procedures. We show that $k=6$\ncuts is enough for an risk-limiting election audit, based on empirical data,\nwhich would provide a significant increase in efficiency. \n\n"}
{"id": "1811.09136", "contents": "Title: REPT: A Streaming Algorithm of Approximating Global and Local Triangle\n  Counts in Parallel Abstract: Recently, considerable efforts have been devoted to approximately computing\nthe global and local (i.e., incident to each node) triangle counts of a large\ngraph stream represented as a sequence of edges. Existing approximate triangle\ncounting algorithms rely on sampling techniques to reduce the computational\ncost. However, their estimation errors are significantly determined by the\ncovariance between sampled triangles. Moreover, little attention has been paid\nto developing parallel one-pass streaming algorithms that can be used to fast\nand approximately count triangles on a multi-core machine or a cluster of\nmachines. To solve these problems, we develop a novel parallel method REPT to\nsignificantly reduce the covariance (even completely eliminate the covariance\nfor some cases) between sampled triangles. We theoretically prove that REPT is\nmore accurate than parallelizing existing triangle count estimation algorithms\nin a direct manner. In addition, we also conduct extensive experiments on a\nvariety of real-world graphs, and the results demonstrate that our method REPT\nis several times more accurate than state-of-the-art methods. \n\n"}
{"id": "1811.10834", "contents": "Title: A Schur Complement Cheeger Inequality Abstract: Cheeger's inequality shows that any undirected graph $G$ with minimum nonzero\nnormalized Laplacian eigenvalue $\\lambda_G$ has a cut with conductance at most\n$O(\\sqrt{\\lambda_G})$. Qualitatively, Cheeger's inequality says that if the\nrelaxation time of a graph is high, there is a cut that certifies this.\nHowever, there is a gap in this relationship, as cuts can have conductance as\nlow as $\\Theta(\\lambda_G)$.\n  To better approximate the relaxation time of a graph, we consider a more\ngeneral object. Instead of bounding the mixing time with cuts, we bound it with\ncuts in graphs obtained by Schur complementing out vertices from the graph $G$.\nCombinatorially, these Schur complements describe random walks in $G$\nrestricted to a subset of its vertices. As a result, all Schur complement cuts\nhave conductance at least $\\Omega(\\lambda_G)$. We show that unlike with cuts,\nthis inequality is tight up to a constant factor. Specifically, there is a\nSchur complement cut with conductance at most $O(\\lambda_G)$. \n\n"}
{"id": "1811.11148", "contents": "Title: The Structure of Optimal Private Tests for Simple Hypotheses Abstract: Hypothesis testing plays a central role in statistical inference, and is used\nin many settings where privacy concerns are paramount. This work answers a\nbasic question about privately testing simple hypotheses: given two\ndistributions $P$ and $Q$, and a privacy level $\\varepsilon$, how many i.i.d.\nsamples are needed to distinguish $P$ from $Q$ subject to\n$\\varepsilon$-differential privacy, and what sort of tests have optimal sample\ncomplexity? Specifically, we characterize this sample complexity up to constant\nfactors in terms of the structure of $P$ and $Q$ and the privacy level\n$\\varepsilon$, and show that this sample complexity is achieved by a certain\nrandomized and clamped variant of the log-likelihood ratio test. Our result is\nan analogue of the classical Neyman-Pearson lemma in the setting of private\nhypothesis testing. We also give an application of our result to the private\nchange-point detection. Our characterization applies more generally to\nhypothesis tests satisfying essentially any notion of algorithmic stability,\nwhich is known to imply strong generalization bounds in adaptive data analysis,\nand thus our results have applications even when privacy is not a primary\nconcern. \n\n"}
{"id": "1811.12547", "contents": "Title: The inverse Voronoi problem in graphs Abstract: We introduce the inverse Voronoi diagram problem in graphs: given a graph $G$\nwith positive edge-lengths and a collection $\\mathbb{U}$ of subsets of vertices\nof $V(G)$, decide whether $\\mathbb{U}$ is a Voronoi diagram in $G$ with respect\nto the shortest-path metric. We show that the problem is NP-hard, even for\nplanar graphs where all the edges have unit length. We also study the\nparameterized complexity of the problem and show that the problem is W[1]-hard\nwhen parameterized by the number of Voronoi cells or by the pathwidth of the\ngraph. For trees we show that the problem can be solved in $O(N+n \\log^2 n)$\ntime, where $n$ is the number of vertices in the tree and $N=n+\\sum_{U\\in\n\\mathbb{U}}|U|$ is the size of the description of the input. We also provide a\nlower bound of $\\Omega(n \\log n)$ time for trees with $n$ vertices. \n\n"}
{"id": "1812.01482", "contents": "Title: Target Set Selection parameterized by vertex cover and more Abstract: Given a simple, undirected graph $G$ with a threshold function $\\tau:V(G)\n\\rightarrow \\mathbb{N}$, the \\textsc{Target Set Selection} (TSS) problem is\nabout choosing a minimum cardinality set, say $S \\subseteq V(G)$, such that\nstarting a diffusion process with $S$ as its seed set will eventually result in\nactivating all the nodes in $G$. For any non-negative integer $i$, we say a set\n$T\\subseteq V(G)$ is a \"degree-$i$ modulator\" of $G$ if the degree of any\nvertex in the graph $G-T$ is at most $i$. Degree-$0$ modulators of a graph are\nprecisely its vertex covers. Consider a graph $G$ on $n$ vertices and $m$\nedges. We have the following results on the TSS problem:\n  -> It was shown by Nichterlein et al. [Social Network Analysis and Mining,\n2013] that it is possible to compute an optimal-sized target set in\n$O(2^{(2^{t}+1)t}\\cdot m)$ time, where $t$ denotes the cardinality of a minimum\ndegree-$0$ modulator of $G$. We improve this result by designing an algorithm\nrunning in time $2^{O(t\\log t)}n^{O(1)}$.\n  -> We design a $2^{2^{O(t)}}n^{O(1)}$ time algorithm to compute an optimal\ntarget set for $G$, where $t$ is the size of a minimum degree-$1$ modulator of\n$G$. \n\n"}
{"id": "1812.01789", "contents": "Title: Hard combinatorial problems and minor embeddings on lattice graphs Abstract: Today, hardware constraints are an important limitation on quantum adiabatic\noptimization algorithms. Firstly, computational problems must be formulated as\nquadratic unconstrained binary optimization (QUBO) in the presence of noisy\ncoupling constants. Secondly, the interaction graph of the QUBO must have an\neffective minor embedding into a two-dimensional nonplanar lattice graph. We\ndescribe new strategies for constructing QUBOs for NP-complete/hard\ncombinatorial problems that address both of these challenges. Our results\ninclude asymptotically improved embeddings for number partitioning, filling\nknapsacks, graph coloring, and finding Hamiltonian cycles. These embeddings can\nbe also be found with reduced computational effort. Our new embedding for\nnumber partitioning may be more effective on next-generation hardware. \n\n"}
{"id": "1812.05821", "contents": "Title: Partial Function Extension with Applications to Learning and Property\n  Testing Abstract: In partial function extension, we are given a partial function consisting of\n$n$ points from a domain and a function value at each point. Our objective is\nto determine if this partial function can be extended to a function defined on\nthe domain, that additionally satisfies a given property, such as convexity.\nThis basic problem underlies research questions in many areas, such as\nlearning, property testing, and game theory. We formally study the problem of\nextending partial functions to satisfy fundamental properties in combinatorial\noptimization, focusing on upper and lower bounds for extension and applications\nto learning and property testing.\n  (1) For subadditive functions, we show the extension problem is\ncoNP-complete, and we give tight bounds on the approximability. We also give an\nimproved lower bound for learning subadditive functions, and give the first\nnontrivial testers for subadditive and XOS functions.\n  (2) For submodular functions, we show that if a partial function can be\nextended to a submodular function on the lattice closure (the minimal set that\ncontains the partial function and is closed under union and intersection) of\nthe partial function, it can be extended to a submodular function on the entire\ndomain. We obtain algorithms for determining extendibility in a number of\ncases, including if $n$ is a constant, or the points are nearly the same size.\nThe complexity of extendibility is in general unresolved.\n  (3) Lastly, for convex functions in $\\mathbb{R}^m$, we show an interesting\njuxtaposition: while we can determine the existence of an extension\nefficiently, computing the value of a widely-studied convex extension at a\ngiven point is strongly NP-hard. \n\n"}
{"id": "1812.06177", "contents": "Title: Simple Concurrent Labeling Algorithms for Connected Components Abstract: We study a class of simple algorithms for concurrently computing the\nconnected components of an $n$-vertex, $m$-edge graph. Our algorithms are easy\nto implement in either the COMBINING CRCW PRAM or the MPC computing model. For\ntwo related algorithms in this class, we obtain $\\Theta(\\lg n)$ step and\n$\\Theta(m \\lg n)$ work bounds. For two others, we obtain $O(\\lg^2 n)$ step and\n$O(m \\lg^2 n)$ work bounds, which are tight for one of them. All our algorithms\nare simpler than related algorithms in the literature. We also point out some\ngaps and errors in the analysis of previous algorithms. Our results show that\neven a basic problem like connected components still has secrets to reveal. \n\n"}
{"id": "1812.08664", "contents": "Title: Near-Linear Time Approximation Schemes for Clustering in Doubling\n  Metrics Abstract: We consider the classic Facility Location, $k$-Median, and $k$-Means problems\nin metric spaces of doubling dimension $d$. We give nearly linear-time\napproximation schemes for each problem. The complexity of our algorithms is\n$2^{(\\log(1/\\eps)/\\eps)^{O(d^2)}} n \\log^4 n + 2^{O(d)} n \\log^9 n$, making a\nsignificant improvement over the state-of-the-art algorithms which run in time\n$n^{(d/\\eps)^{O(d)}}$.\n  Moreover, we show how to extend the techniques used to get the first\nefficient approximation schemes for the problems of prize-collecting\n$k$-Medians and $k$-Means, and efficient bicriteria approximation schemes for\n$k$-Medians with outliers, $k$-Means with outliers and $k$-Center. \n\n"}
{"id": "1812.08731", "contents": "Title: Limits on the Universal Method for Matrix Multiplication Abstract: In this work, we prove limitations on the known methods for designing matrix\nmultiplication algorithms. Alman and Vassilevska Williams recently defined the\nUniversal Method, which substantially generalizes all the known approaches\nincluding Strassen's Laser Method and Cohn and Umans' Group Theoretic Method.\nWe prove concrete lower bounds on the algorithms one can design by applying the\nUniversal Method to many different tensors. Our proofs use new tools for upper\nbounding the asymptotic slice rank of a wide range of tensors. Our main result\nis that the Universal method applied to any Coppersmith-Winograd tensor $CW_q$\ncannot yield a bound on $\\omega$, the exponent of matrix multiplication, better\nthan $2.16805$. By comparison, it was previously only known that the weaker\n`Galactic Method' applied to $CW_q$ could not achieve an exponent of $2$.\n  We also study the Laser Method (which is, in principle, a highly special case\nof the Universal Method) and prove that it is \"complete\" for matrix\nmultiplication algorithms: when it applies to a tensor $T$, it achieves $\\omega\n= 2$ if and only if it is possible for the Universal method applied to $T$ to\nachieve $\\omega = 2$. Hence, the Laser Method, which was originally used as an\nalgorithmic tool, can also be seen as a lower bounding tool. For example, in\ntheir landmark paper, Coppersmith and Winograd achieved a bound of $\\omega \\leq\n2.376$, by applying the Laser Method to $CW_q$. By our result, the fact that\nthey did not achieve $\\omega=2$ implies a lower bound on the Universal Method\napplied to $CW_q$. Indeed, if it were possible for the Universal Method applied\nto $CW_q$ to achieve $\\omega=2$, then Coppersmith and Winograd's application of\nthe Laser Method would have achieved $\\omega=2$. \n\n"}
{"id": "1812.09094", "contents": "Title: A Simple Algorithm for Computing the Document Array Abstract: We present a simple algorithm for computing the document array given a string\ncollection and its suffix array as input. Our algorithm runs in linear time\nusing constant additional space for strings from constant alphabets. \n\n"}
{"id": "1812.10854", "contents": "Title: Fair Coresets and Streaming Algorithms for Fair k-Means Clustering Abstract: We study fair clustering problems as proposed by Chierichetti et al. (NIPS\n2017). Here, points have a sensitive attribute and all clusters in the solution\nare required to be balanced with respect to it (to counteract any form of\ndata-inherent bias). Previous algorithms for fair clustering do not scale well.\n  We show how to model and compute so-called coresets for fair clustering\nproblems, which can be used to significantly reduce the input data size. We\nprove that the coresets are composable and show how to compute them in a\nstreaming setting. Furthermore, we propose a variant of Lloyd's algorithm that\ncomputes fair clusterings and extend it to a fair k-means++ clustering\nalgorithm. We implement these algorithms and provide empirical evidence that\nthe combination of our approximation algorithms and the coreset construction\nyields a scalable algorithm for fair k-means clustering. \n\n"}
{"id": "1901.01861", "contents": "Title: On the Parameterized Complexity of $k$-Edge Colouring Abstract: For every fixed integer $k \\geq 1$, we prove that $k$-Edge Colouring is\nfixed-parameter-tractable when parameterized by the number of vertices of\nmaximum degree. \n\n"}
{"id": "1901.02536", "contents": "Title: Fast generalized DFTs for all finite groups Abstract: For any finite group $G$, we give an arithmetic algorithm to compute\ngeneralized Discrete Fourier Transforms (DFTs) with respect to $G$, using\n$O(|G|^{\\omega/2 + \\epsilon})$ operations, for any $\\epsilon > 0$. Here,\n$\\omega$ is the exponent of matrix multiplication. \n\n"}
{"id": "1901.02871", "contents": "Title: The Lingering of Gradients: Theory and Applications Abstract: Classically, the time complexity of a first-order method is estimated by its\nnumber of gradient computations. In this paper, we study a more refined\ncomplexity by taking into account the `lingering' of gradients: once a gradient\nis computed at $x_k$, the additional time to compute gradients at\n$x_{k+1},x_{k+2},\\dots$ may be reduced.\n  We show how this improves the running time of several first-order methods.\nFor instance, if the `additional time' scales linearly with respect to the\ntraveled distance, then the `convergence rate' of gradient descent can be\nimproved from $1/T$ to $\\exp(-T^{1/3})$. On the application side, we solve a\nhypothetical revenue management problem on the Yahoo! Front Page Today Module\nwith 4.6m users to $10^{-6}$ error using only 6 passes of the dataset; and\nsolve a real-life support vector machine problem to an accuracy that is two\norders of magnitude better comparing to the state-of-the-art algorithm. \n\n"}
{"id": "1901.03364", "contents": "Title: On the Descriptive Complexity of Color Coding Abstract: Color coding is an algorithmic technique used in parameterized complexity\ntheory to detect \"small\" structures inside graphs. The idea is to derandomize\nalgorithms that first randomly color a graph and then search for an\neasily-detectable, small color pattern. We transfer color coding to the world\nof descriptive complexity theory by characterizing -- purely in terms of the\nsyntactic structure of describing formulas -- when the powerful second-order\nquantifiers representing a random coloring can be replaced by equivalent,\nsimple first-order formulas. Building on this result, we identify syntactic\nproperties of first-order quantifiers that can be eliminated from formulas\ndescribing parameterized problems. The result applies to many packing and\nembedding problems, but also to the long path problem. Together with a new\nresult on the parameterized complexity of formula families involving only a\nfixed number of variables, we get that many problems lie in FPT just because of\nthe way they are commonly described using logical formulas. \n\n"}
{"id": "1901.05620", "contents": "Title: The Pareto Record Frontier Abstract: For iid $d$-dimensional observations $X^{(1)}, X^{(2)}, \\ldots$ with\nindependent Exponential$(1)$ coordinates, consider the boundary (relative to\nthe closed positive orthant), or \"frontier\", $F_n$ of the closed Pareto\nrecord-setting (RS) region \\[ \\mbox{RS}_n := \\{0 \\leq x \\in {\\mathbb R}^d: x\n\\not\\prec X^{(i)}\\ \\mbox{for all $1 \\leq i \\leq n$}\\} \\] at time $n$, where $0\n\\leq x$ means that $0 \\leq x_j$ for $1 \\leq j \\leq d$ and $x \\prec y$ means\nthat $x_j < y_j$ for $1 \\leq j \\leq d$. With $x_+ := \\sum_{j = 1}^d x_j$, let\n\\[ F_n^- := \\min\\{x_+: x \\in F_n\\} \\quad \\mbox{and} \\quad F_n^+ := \\max\\{x_+: x\n\\in F_n\\}, \\] and define the width of $F_n$ as \\[ W_n := F_n^+ - F_n^-. \\] We\ndescribe typical and almost sure behavior of the processes $F^+$, $F^-$, and\n$W$. In particular, we show that $F^+_n \\sim \\ln n \\sim F^-_n$ almost surely\nand that $W_n / \\ln \\ln n$ converges in probability to $d - 1$; and for $d \\geq\n2$ we show that, almost surely, the set of limit points of the sequence $W_n /\n\\ln \\ln n$ is the interval $[d - 1, d]$.\n  We also obtain modifications of our results that are important in connection\nwith efficient simulation of Pareto records. Let $T_m$ denote the time that the\n$m$th record is set. We show that $F^+_{T_m} \\sim (d! m)^{1/d} \\sim F^-_{T_m}$\nalmost surely and that $W_{T_m} / \\ln m$ converges in probability to $1 -\nd^{-1}$; and for $d \\geq 2$ we show that, almost surely, the sequence $W_{T_m}\n/ \\ln m$ has $\\liminf$ equal to $1 - d^{-1}$ and $\\limsup$ equal to $1$. \n\n"}
{"id": "1901.06482", "contents": "Title: On Efficient Optimal Transport: An Analysis of Greedy and Accelerated\n  Mirror Descent Algorithms Abstract: We provide theoretical analyses for two algorithms that solve the regularized\noptimal transport (OT) problem between two discrete probability measures with\nat most $n$ atoms. We show that a greedy variant of the classical Sinkhorn\nalgorithm, known as the \\emph{Greenkhorn algorithm}, can be improved to\n$\\widetilde{\\mathcal{O}}(n^2\\varepsilon^{-2})$, improving on the best known\ncomplexity bound of $\\widetilde{\\mathcal{O}}(n^2\\varepsilon^{-3})$. Notably,\nthis matches the best known complexity bound for the Sinkhorn algorithm and\nhelps explain why the Greenkhorn algorithm can outperform the Sinkhorn\nalgorithm in practice. Our proof technique, which is based on a primal-dual\nformulation and a novel upper bound for the dual solution, also leads to a new\nclass of algorithms that we refer to as \\emph{adaptive primal-dual accelerated\nmirror descent} (APDAMD) algorithms. We prove that the complexity of these\nalgorithms is $\\widetilde{\\mathcal{O}}(n^2\\sqrt{\\delta}\\varepsilon^{-1})$,\nwhere $\\delta > 0$ refers to the inverse of the strong convexity module of\nBregman divergence with respect to $\\|\\cdot\\|_\\infty$. This implies that the\nAPDAMD algorithm is faster than the Sinkhorn and Greenkhorn algorithms in terms\nof $\\varepsilon$. Experimental results on synthetic and real datasets\ndemonstrate the favorable performance of the Greenkhorn and APDAMD algorithms\nin practice. \n\n"}
{"id": "1901.08575", "contents": "Title: Deterministic 2-Dimensional Temperature-1 Tile Assembly Systems Cannot\n  Compute Abstract: We consider non cooperative binding in so called `temperature 1', in\ndeterministic (here called {\\it confluent}) tile self-assembly systems (1-TAS)\nand prove the standing conjecture that such systems do not have universal\ncomputational power. We call a TAS whose maximal assemblies contain at least\none ultimately periodic assembly path {\\it para-periodic}. We observe that a\nconfluent 1-TAS has at most one maximal producible assembly, $\\alpha_{max}$,\nthat can be considered a union of path assemblies, and we show that such a\nsystem is always para-periodic. This result is obtained through a superposition\nand a combination of two paths that produce a new path with desired properties,\na technique that we call \\emph{co-grow} of two paths. Moreover we provide a\ncharacterization of an $\\alpha_{max}$ of a confluent 1-TAS as one of two\npossible cases, so called, a grid or a disjoint union of combs. To a given\n$\\alpha_{max}$ we can associate a finite labeled graph, called \\emph{quipu},\nsuch that the union of all labels of paths in the quipu equals $\\alpha_{max}$,\ntherefore giving a finite description for $\\alpha_{max}$. This finite\ndescription implies that $\\alpha_{max}$ is a union of semi-affine subsets of\n$\\mathbb{Z}^2$ and since such a finite description can be algorithmicly\ngenerated from any 1-TAS, 1-TAS cannot have universal computational power. \n\n"}
{"id": "1901.09423", "contents": "Title: Subspace arrangements, graph rigidity and derandomization through\n  submodular optimization Abstract: This paper presents a deterministic, strongly polynomial time algorithm for\ncomputing the matrix rank for a class of symbolic matrices (whose entries are\npolynomials over a field). This class was introduced, in a different language,\nby Lov\\'asz [Lov] in his study of flats in matroids, and proved a duality\ntheorem putting this problem in $NP \\cap coNP$. As such, our result is another\ndemonstration where ``good characterization'' in the sense of Edmonds leads to\nan efficient algorithm. In a different paper Lov\\'asz [Lov79] proved that all\nsuch symbolic rank problems have efficient probabilistic algorithms, namely are\nin $BPP$. As such, our algorithm may be interpreted as a derandomization\nresult, in the long sequence special cases of the PIT (Polynomial Identity\nTesting) problem. Finally, Lov\\'asz and Yemini [LoYe] showed how the same\nproblem generalizes the graph rigidity problem in two dimensions. As such, our\nalgorithm may be seen as a generalization of the well-known deterministic\nalgorithm for the latter problem.\n  There are two somewhat unusual technical features in this paper. The first is\nthe translation of Lov\\'asz' flats problem into a symbolic rank one. The second\nis the use of submodular optimization for derandomization. We hope that the\ntools developed for both will be useful for related problems, in particular for\nbetter understanding of graph rigidity in higher dimensions. \n\n"}
{"id": "1901.09527", "contents": "Title: Envy-free Matchings in Bipartite Graphs and their Applications to Fair\n  Division Abstract: A matching in a bipartite graph with parts X and Y is called envy-free if no\nunmatched vertex in X is a adjacent to a matched vertex in Y. Every perfect\nmatching is envy-free, but envy-free matchings exist even when perfect\nmatchings do not. We prove that every bipartite graph has a unique partition\nsuch that all envy-free matchings are contained in one of the partition sets.\nUsing this structural theorem, we provide a polynomial-time algorithm for\nfinding an envy-free matching of maximum cardinality. For edge-weighted\nbipartite graphs, we provide a polynomial-time algorithm for finding a\nmaximum-cardinality envy-free matching of minimum total weight. We show how\nenvy-free matchings can be used in various fair division problems with either\ncontinuous resources (\"cakes\") or discrete ones. In particular, we propose a\nsymmetric algorithm for proportional cake-cutting, an algorithm for\n1-out-of-(2n-2) maximin-share allocation of discrete goods, and an algorithm\nfor 1-out-of-floor(2n/3) maximin-share allocation of discrete bads among n\nagents. \n\n"}
{"id": "1901.09687", "contents": "Title: Rates of adaptive group testing in the linear regime Abstract: We consider adaptive group testing in the linear regime, where the number of\ndefective items scales linearly with the number of items. We analyse an\nalgorithm based on generalized binary splitting. Provided fewer than half the\nitems are defective, we achieve rates of over 0.9 bits per test for\ncombinatorial zero-error testing, and over 0.95 bits per test for probabilistic\nsmall-error testing. \n\n"}
{"id": "1901.10045", "contents": "Title: Online Algorithms for Constructing Linear-size Suffix Trie Abstract: The suffix trees are fundamental data structures for various kinds of string\nprocessing. The suffix tree of a string $T$ of length $n$ has $O(n)$ nodes and\nedges, and the string label of each edge is encoded by a pair of positions in\n$T$. Thus, even after the tree is built, the input text $T$ needs to be kept\nstored and random access to $T$ is still needed. The linear-size suffix tries\n(LSTs), proposed by Crochemore et al. [Linear-size suffix tries, TCS\n638:171-178, 2016], are a `stand-alone' alternative to the suffix trees.\nNamely, the LST of a string $T$ of length $n$ occupies $O(n)$ total space, and\nsupports pattern matching and other tasks in the same efficiency as the suffix\ntree without the need to store the input text $T$. Crochemore et al. proposed\nan offline algorithm which transforms the suffix tree of $T$ into the LST of\n$T$ in $O(n \\log \\sigma)$ time and $O(n)$ space, where $\\sigma$ is the alphabet\nsize. In this paper, we present two types of online algorithms which `directly'\nconstruct the LST, from right to left, and from left to right, without\nconstructing the suffix tree as an intermediate structure. Both algorithms\nconstruct the LST incrementally when a new symbol is read, and do not access to\nthe previously read symbols. The right-to-left construction algorithm works in\n$O(n \\log \\sigma)$ time and $O(n)$ space and the left-to-right construction\nalgorithm works in $O(n (\\log \\sigma + \\log n / \\log \\log n))$ time and $O(n)$\nspace. The main feature of our algorithms is that the input text does not need\nto be stored. \n\n"}
{"id": "1901.10084", "contents": "Title: A Parallel Projection Method for Metric Constrained Optimization Abstract: Many clustering applications in machine learning and data mining rely on\nsolving metric-constrained optimization problems. These problems are\ncharacterized by $O(n^3)$ constraints that enforce triangle inequalities on\ndistance variables associated with $n$ objects in a large dataset. Despite its\nusefulness, metric-constrained optimization is challenging in practice due to\nthe cubic number of constraints and the high-memory requirements of standard\noptimization software. Recent work has shown that iterative projection methods\nare able to solve metric-constrained optimization problems on a much larger\nscale than was previously possible, thanks to their comparatively low memory\nrequirement. However, the major limitation of projection methods is their slow\nconvergence rate. In this paper we present a parallel projection method for\nmetric-constrained optimization which allows us to speed up the convergence\nrate in practice. The key to our approach is a new parallel execution schedule\nthat allows us to perform projections at multiple metric constraints\nsimultaneously without any conflicts or locking of variables. We illustrate the\neffectiveness of this execution schedule by implementing and testing a parallel\nprojection method for solving the metric-constrained linear programming\nrelaxation of correlation clustering. We show numerous experimental results on\nproblems involving up to 2.9 trillion constraints. \n\n"}
{"id": "cond-mat/0301271", "contents": "Title: Solving satisfiability problems by fluctuations: The dynamics of\n  stochastic local search algorithms Abstract: Stochastic local search algorithms are frequently used to numerically solve\nhard combinatorial optimization or decision problems. We give numerical and\napproximate analytical descriptions of the dynamics of such algorithms applied\nto random satisfiability problems. We find two different dynamical regimes,\ndepending on the number of constraints per variable: For low constraintness,\nthe problems are solved efficiently, i.e. in linear time. For higher\nconstraintness, the solution times become exponential. We observe that the\ndynamical behavior is characterized by a fast equilibration and fluctuations\naround this equilibrium. If the algorithm runs long enough, an exponentially\nrare fluctuation towards a solution appears. \n\n"}
{"id": "cond-mat/0301272", "contents": "Title: Relaxation and Metastability in the RandomWalkSAT search procedure Abstract: An analysis of the average properties of a local search resolution procedure\nfor the satisfaction of random Boolean constraints is presented. Depending on\nthe ratio alpha of constraints per variable, resolution takes a time T_res\ngrowing linearly (T_res \\sim tau(alpha) N, alpha < alpha_d) or exponentially\n(T_res \\sim exp(N zeta(alpha)), alpha > alpha_d) with the size N of the\ninstance. The relaxation time tau(alpha) in the linear phase is calculated\nthrough a systematic expansion scheme based on a quantum formulation of the\nevolution operator. For alpha > alpha_d, the system is trapped in some\nmetastable state, and resolution occurs from escape from this state through\ncrossing of a large barrier. An annealed calculation of the height zeta(alpha)\nof this barrier is proposed. The polynomial/exponentiel cross-over alpha_d is\nnot related to the onset of clustering among solutions. \n\n"}
{"id": "cond-mat/0504185", "contents": "Title: Disaster Management in Scale-Free Networks: Recovery from and Protection\n  Against Intentional Attacks Abstract: Susceptibility of scale free Power Law (PL) networks to attacks has been\ntraditionally studied in the context of what may be termed as {\\em\ninstantaneous attacks}, where a randomly selected set of nodes and edges are\ndeleted while the network is kept {\\em static}. In this paper, we shift the\nfocus to the study of {\\em progressive} and instantaneous attacks on {\\em\nreactive} grown and random PL networks, which can respond to attacks and take\nremedial steps. In the process, we present several techniques that managed\nnetworks can adopt to minimize the damages during attacks, and also to\nefficiently recover from the aftermath of successful attacks. For example, we\npresent (i) compensatory dynamics that minimize the damages inflicted by\ntargeted progressive attacks, such as linear-preferential deletions of nodes in\ngrown PL networks; the resulting dynamic naturally leads to the emergence of\nnetworks with PL degree distributions with exponential cutoffs; (ii)\ndistributed healing algorithms that can scale the maximum degree of nodes in a\nPL network using only local decisions, and (iii) efficient means of creating\ngiant connected components in a PL network that has been fragmented by attacks\non a large number of high-degree nodes. Such targeted attacks are considered to\nbe a major vulnerability of PL networks; however, our results show that the\nintroduction of only a small number of random edges, through a {\\em reverse\npercolation} process, can restore connectivity, which in turn allows\nrestoration of other topological properties of the original network. Thus, the\nscale-free nature of the networks can itself be effectively utilized for\nprotection and recovery purposes. \n\n"}
{"id": "cond-mat/0607290", "contents": "Title: A rigorous proof of the cavity method for counting matchings Abstract: In this paper we rigorously prove the validity of the cavity method for the\nproblem of counting the number of matchings in graphs with large girth. Cavity\nmethod is an important heuristic developed by statistical physicists that has\nlead to the development of faster distributed algorithms for problems in\nvarious combinatorial optimization problems. The validity of the approach has\nbeen supported mostly by numerical simulations. In this paper we prove the\nvalidity of cavity method for the problem of counting matchings using rigorous\ntechniques. We hope that these rigorous approaches will finally help us\nestablish the validity of the cavity method in general. \n\n"}
{"id": "cs/0102018", "contents": "Title: An effective Procedure for Speeding up Algorithms Abstract: The provably asymptotically fastest algorithm within a factor of 5 for\nformally described problems will be constructed. The main idea is to enumerate\nall programs provably equivalent to the original problem by enumerating all\nproofs. The algorithm could be interpreted as a generalization and improvement\nof Levin search, which is, within a multiplicative constant, the fastest\nalgorithm for inverting functions. Blum's speed-up theorem is avoided by taking\ninto account only programs for which a correctness proof exists. Furthermore,\nit is shown that the fastest program that computes a certain function is also\none of the shortest programs provably computing this function. To quantify this\nstatement, the definition of Kolmogorov complexity is extended, and two new\nnatural measures for the complexity of a function are defined. \n\n"}
{"id": "cs/0107015", "contents": "Title: From Neel to NPC: Colouring Small Worlds Abstract: In this note, we present results for the colouring problem on small world\ngraphs created by rewiring square, triangular, and two kinds of cubic (with\ncoordination numbers 5 and 6) lattices. As the rewiring parameter p tends to 1,\nwe find the expected crossover to the behaviour of random graphs with\ncorresponding connectivity. However, for the cubic lattices there is a region\nnear p=0 for which the graphs are colourable. This could in principle be used\nas an additional heuristic for solving real world colouring or scheduling\nproblems. Small worlds with connectivity 5 and p ~ 0.1 provide an interesting\nensemble of graphs whose colourability is hard to determine. For square\nlattices, we get good data collapse plotting the fraction of colourable graphs\nagainst the rescaled parameter parameter $p N^{-\\nu}$ with $\\nu = 1.35$. No\nsuch collapse can be obtained for the data from lattices with coordination\nnumber 5 or 6. \n\n"}
{"id": "cs/0201007", "contents": "Title: Algorithm for generating orthogonal matrices with rational elements Abstract: Special orthogonal matrices with rational elements form the group SO(n,Q),\nwhere Q is the field of rational numbers. A theorem describing the structure of\nan arbitrary matrix from this group is proved. This theorem yields an algorithm\nfor generating such matrices by means of random number routines. \n\n"}
{"id": "cs/0203029", "contents": "Title: Forbidden Information Abstract: Goedel Incompleteness Theorem leaves open a way around it, vaguely perceived\nfor a long time but not clearly identified. (Thus, Goedel believed informal\narguments can answer any math question.) Closing this loophole does not seem\nobvious and involves Kolmogorov complexity. (This is unrelated to, well studied\nbefore, complexity quantifications of the usual Goedel effects.) I consider\nextensions U of the universal partial recursive predicate (or, say, Peano\nArithmetic). I prove that any U either leaves an n-bit input (statement)\nunresolved or contains nearly all information about the n-bit prefix of any\nr.e. real r (which is n bits for some r). I argue that creating significant\ninformation about a SPECIFIC math sequence is impossible regardless of the\nmethods used. Similar problems and answers apply to other unsolvability results\nfor tasks allowing multiple solutions, e.g. non-recursive tilings. \n\n"}
{"id": "cs/0207097", "contents": "Title: Optimal Ordered Problem Solver Abstract: We present a novel, general, optimally fast, incremental way of searching for\na universal algorithm that solves each task in a sequence of tasks. The Optimal\nOrdered Problem Solver (OOPS) continually organizes and exploits previously\nfound solutions to earlier tasks, efficiently searching not only the space of\ndomain-specific algorithms, but also the space of search algorithms.\nEssentially we extend the principles of optimal nonincremental universal search\nto build an incremental universal learner that is able to improve itself\nthrough experience. In illustrative experiments, our self-improver becomes the\nfirst general system that learns to solve all n disk Towers of Hanoi tasks\n(solution size 2^n-1) for n up to 30, profiting from previously solved, simpler\ntasks involving samples of a simple context free language. \n\n"}
{"id": "cs/0301015", "contents": "Title: Some remarks on the survey decimation algorithm for K-satisfiability Abstract: In this note we study the convergence of the survey decimation algorithm. An\nanalytic formula for the reduction of the complexity during the decimation is\nderived. The limit of the converge of the algorithm are estimated in the random\ncase: interesting phenomena appear near the boundary of convergence. \n\n"}
{"id": "cs/0302022", "contents": "Title: Fault-tolerant routing in peer-to-peer systems Abstract: We consider the problem of designing an overlay network and routing mechanism\nthat permits finding resources efficiently in a peer-to-peer system. We argue\nthat many existing approaches to this problem can be modeled as the\nconstruction of a random graph embedded in a metric space whose points\nrepresent resource identifiers, where the probability of a connection between\ntwo nodes depends only on the distance between them in the metric space. We\nstudy the performance of a peer-to-peer system where nodes are embedded at grid\npoints in a simple metric space: a one-dimensional real line. We prove upper\nand lower bounds on the message complexity of locating particular resources in\nsuch a system, under a variety of assumptions about failures of either nodes or\nthe connections between them. Our lower bounds in particular show that the use\nof inverse power-law distributions in routing, as suggested by Kleinberg\n(1999), is close to optimal. We also give efficient heuristics to dynamically\nmaintain such a system as new nodes arrive and old nodes depart. Finally, we\ngive experimental results that suggest promising directions for future work. \n\n"}
{"id": "cs/0304018", "contents": "Title: Quasiconvex Analysis of Backtracking Algorithms Abstract: We consider a class of multivariate recurrences frequently arising in the\nworst case analysis of Davis-Putnam-style exponential time backtracking\nalgorithms for NP-hard problems. We describe a technique for proving asymptotic\nupper bounds on these recurrences, by using a suitable weight function to\nreduce the problem to that of solving univariate linear recurrences; show how\nto use quasiconvex programming to determine the weight function yielding the\nsmallest upper bound; and prove that the resulting upper bounds are within a\npolynomial factor of the true asymptotics of the recurrence. We develop and\nimplement a multiple-gradient descent algorithm for the resulting quasiconvex\nprograms, using a real-number arithmetic package for guaranteed accuracy of the\ncomputed worst case time bounds. \n\n"}
{"id": "cs/0308010", "contents": "Title: On the probabilistic approach to the random satisfiability problem Abstract: In this note I will review some of the recent results that have been obtained\nin the probabilistic approach to the random satisfiability problem. At the\npresent moment the results are only heuristic. In the case of the random\n3-satisfiability problem a phase transition from the satisfiable to the\nunsatisfiable phase is found at $\\alpha=4.267$. There are other values of\n$\\alpha$ that separates different regimes and they will be described in\ndetails. In this context the properties of the survey decimation algorithm will\nalso be discussed. \n\n"}
{"id": "cs/0309033", "contents": "Title: Lower bounds for predecessor searching in the cell probe model Abstract: We consider a fundamental problem in data structures, static predecessor\nsearching: Given a subset S of size n from the universe [m], store S so that\nqueries of the form \"What is the predecessor of x in S?\" can be answered\nefficiently. We study this problem in the cell probe model introduced by Yao.\nRecently, Beame and Fich obtained optimal bounds on the number of probes needed\nby any deterministic query scheme if the associated storage scheme uses only\nn^{O(1)} cells of word size (\\log m)^{O(1)} bits. We give a new lower bound\nproof for this problem that matches the bounds of Beame and Fich. Our lower\nbound proof has the following advantages: it works for randomised query schemes\ntoo, while Beame and Fich's proof works for deterministic query schemes only.\nIt also extends to `quantum address-only' query schemes that we define in this\npaper, and is simpler than Beame and Fich's proof. We prove our lower bound\nusing the round elimination approach of Miltersen, Nisan, Safra and Wigderson.\nUsing tools from information theory, we prove a strong round elimination lemma\nfor communication complexity that enables us to obtain a tight lower bound for\nthe predecessor problem. Our strong round elimination lemma also extends to\nquantum communication complexity. We also use our round elimination lemma to\nobtain a rounds versus communication tradeoff for the `greater-than' problem,\nimproving on the tradeoff in Miltersen et al. We believe that our round\nelimination lemma is of independent interest and should have other\napplications. \n\n"}
{"id": "cs/0508037", "contents": "Title: The Phase Transition in Exact Cover Abstract: We study EC3, a variant of Exact Cover which is equivalent to Positive 1-in-3\nSAT. Random instances of EC3 were recently used as benchmarks for simulations\nof an adiabatic quantum algorithm. Empirical results suggest that EC3 has a\nphase transition from satisfiability to unsatisfiability when the number of\nclauses per variable r exceeds some threshold r* ~= 0.62 +- 0.01. Using the\nmethod of differential equations, we show that if r <= 0.546 w.h.p. a random\ninstance of EC3 is satisfiable. Combined with previous results this limits the\nlocation of the threshold, if it exists, to the range 0.546 < r* < 0.644. \n\n"}
{"id": "cs/0510043", "contents": "Title: On Minimal Pseudo-Codewords of Tanner Graphs from Projective Planes Abstract: We would like to better understand the fundamental cone of Tanner graphs\nderived from finite projective planes. Towards this goal, we discuss bounds on\nthe AWGNC and BSC pseudo-weight of minimal pseudo-codewords of such Tanner\ngraphs, on one hand, and study the structure of minimal pseudo-codewords, on\nthe other. \n\n"}
{"id": "cs/0510049", "contents": "Title: Bounds on the Pseudo-Weight of Minimal Pseudo-Codewords of Projective\n  Geometry Codes Abstract: In this paper we focus our attention on a family of finite geometry codes,\ncalled type-I projective geometry low-density parity-check (PG-LDPC) codes,\nthat are constructed based on the projective planes PG{2,q). In particular, we\nstudy their minimal codewords and pseudo-codewords, as it is known that these\nvectors characterize completely the code performance under maximum-likelihood\ndecoding and linear programming decoding, respectively. The main results of\nthis paper consist of upper and lower bounds on the pseudo-weight of the\nminimal pseudo-codewords of type-I PG-LDPC codes. \n\n"}
{"id": "cs/0601048", "contents": "Title: Permutation Polynomial Interleavers: An Algebraic-Geometric Perspective Abstract: An interleaver is a critical component for the channel coding\n  performance of turbo codes. Algebraic constructions are\n  important because they admit analytical designs and\n  simple, practical hardware implementation. The spread factor of an\n  interleaver is a common measure for turbo coding\n  applications. Maximum-spread interleavers are interleavers whose\n  spread factors achieve the upper bound. An infinite sequence of\n  quadratic permutation polynomials over integer rings that generate\n  maximum-spread interleavers is presented. New properties of\n  permutation polynomial interleavers are investigated from an\n  algebraic-geometric perspective resulting in a new non-linearity metric\n  for interleavers. A new interleaver metric that is a function of both\n  the non-linearity metric and the spread factor is proposed.\n  It is numerically demonstrated that the spread factor has a\n  diminishing importance with the block length. A table of good\n  interleavers for a variety of interleaver lengths according to the\n  new metric is listed. Extensive computer simulation results with impressive\n  frame error rates confirm the efficacy of the new metric. Further,\n  when tail-biting constituent codes are used, the resulting turbo\n  codes are quasi-cyclic. \n\n"}
{"id": "cs/0601066", "contents": "Title: On the Existence of Universally Decodable Matrices Abstract: Universally decodable matrices (UDMs) can be used for coding purposes when\ntransmitting over slow fading channels. These matrices are parameterized by\npositive integers $L$ and $N$ and a prime power $q$. The main result of this\npaper is that the simple condition $L \\leq q+1$ is both necessary and\nsufficient for $(L,N,q)$-UDMs to exist. The existence proof is constructive and\nyields a coding scheme that is equivalent to a class of codes that was proposed\nby Rosenbloom and Tsfasman. Our work resolves an open problem posed recently in\nthe literature. \n\n"}
{"id": "cs/0601075", "contents": "Title: On Universally Decodable Matrices for Space-Time Coding Abstract: The notion of universally decodable matrices (UDMs) was recently introduced\nby Tavildar and Viswanath while studying slow fading channels. It turns out\nthat the problem of constructing UDMs is tightly connected to the problem of\nconstructing maximum distance separable (MDS) codes. In this paper, we first\nstudy the properties of UDMs in general and then we discuss an explicit\nconstruction of a class of UDMs, a construction which can be seen as an\nextension of Reed-Solomon codes. In fact, we show that this extension is, in a\nsense to be made more precise later on, unique. Moreover, the structure of this\nclass of UDMs allows us to answer some open conjectures by Tavildar, Viswanath,\nand Doshi in the positive, and it also allows us to formulate an efficient\ndecoding algorithm for this class of UDMs. It turns out that our construction\nyields a coding scheme that is essentially equivalent to a class of codes that\nwas proposed by Rosenbloom and Tsfasman. Moreover, we point out connections to\nso-called repeated-root cyclic codes. \n\n"}
{"id": "cs/0602089", "contents": "Title: Pseudo-Codeword Analysis of Tanner Graphs from Projective and Euclidean\n  Planes Abstract: In order to understand the performance of a code under maximum-likelihood\n(ML) decoding, one studies the codewords, in particular the minimal codewords,\nand their Hamming weights. In the context of linear programming (LP) decoding,\none's attention needs to be shifted to the pseudo-codewords, in particular to\nthe minimal pseudo-codewords, and their pseudo-weights. In this paper we\ninvestigate some families of codes that have good properties under LP decoding,\nnamely certain families of low-density parity-check (LDPC) codes that are\nderived from projective and Euclidean planes: we study the structure of their\nminimal pseudo-codewords and give lower bounds on their pseudo-weight. \n\n"}
{"id": "cs/0603043", "contents": "Title: Time-Space Trade-Offs for Predecessor Search Abstract: We develop a new technique for proving cell-probe lower bounds for static\ndata structures. Previous lower bounds used a reduction to communication games,\nwhich was known not to be tight by counting arguments. We give the first lower\nbound for an explicit problem which breaks this communication complexity\nbarrier. In addition, our bounds give the first separation between polynomial\nand near linear space. Such a separation is inherently impossible by\ncommunication complexity.\n  Using our lower bound technique and new upper bound constructions, we obtain\ntight bounds for searching predecessors among a static set of integers. Given a\nset Y of n integers of l bits each, the goal is to efficiently find\npredecessor(x) = max{y in Y | y <= x}, by representing Y on a RAM using space\nS.\n  In external memory, it follows that the optimal strategy is to use either\nstandard B-trees, or a RAM algorithm ignoring the larger block size. In the\nimportant case of l = c*lg n, for c>1 (i.e. polynomial universes), and near\nlinear space (such as S = n*poly(lg n)), the optimal search time is Theta(lg\nl). Thus, our lower bound implies the surprising conclusion that van Emde Boas'\nclassic data structure from [FOCS'75] is optimal in this case. Note that for\nspace n^{1+eps}, a running time of O(lg l / lglg l) was given by Beame and Fich\n[STOC'99]. \n\n"}
{"id": "cs/0603084", "contents": "Title: Random 3CNF formulas elude the Lovasz theta function Abstract: Let $\\phi$ be a 3CNF formula with n variables and m clauses. A simple\nnonconstructive argument shows that when m is sufficiently large compared to n,\nmost 3CNF formulas are not satisfiable. It is an open question whether there is\nan efficient refutation algorithm that for most such formulas proves that they\nare not satisfiable. A possible approach to refute a formula $\\phi$ is: first,\ntranslate it into a graph $G_{\\phi}$ using a generic reduction from 3-SAT to\nmax-IS, then bound the maximum independent set of $G_{\\phi}$ using the Lovasz\n$\\vartheta$ function. If the $\\vartheta$ function returns a value $< m$, this\nis a certificate for the unsatisfiability of $\\phi$. We show that for random\nformulas with $m < n^{3/2 -o(1)}$ clauses, the above approach fails, i.e. the\n$\\vartheta$ function is likely to return a value of m. \n\n"}
{"id": "cs/0609156", "contents": "Title: Entangled Graphs Abstract: In this paper we prove a separability criterion for mixed states in $\\mathbb\nC^p\\otimes\\mathbb C^q$. We also show that the density matrix of a graph with\nonly one entangled edge is entangled. \n\n"}
{"id": "cs/0609159", "contents": "Title: Duality for Several Families of Evaluation Codes Abstract: We consider generalizations of Reed-Muller codes, toric codes, and codes from\ncertain plane curves, such as those defined by norm and trace functions on\nfinite fields. In each case we are interested in codes defined by evaluating\narbitrary subsets of monomials, and in identifying when the dual codes are also\nobtained by evaluating monomials. We then move to the context of order domain\ntheory, in which the subsets of monomials can be chosen to optimize decoding\nperformance using the Berlekamp-Massey-Sakata algorithm with majority voting.\nWe show that for the codes under consideration these subsets are well-behaved\nand the dual codes are also defined by monomials. \n\n"}
{"id": "cs/0610042", "contents": "Title: A Polynomial Time Algorithm for The Traveling Salesman Problem Abstract: The ATSP polytope can be expressed by asymmetric polynomial size linear\nprogram. \n\n"}
{"id": "cs/0702014", "contents": "Title: Probabilistic Analysis of Linear Programming Decoding Abstract: We initiate the probabilistic analysis of linear programming (LP) decoding of\nlow-density parity-check (LDPC) codes. Specifically, we show that for a random\nLDPC code ensemble, the linear programming decoder of Feldman et al. succeeds\nin correcting a constant fraction of errors with high probability. The fraction\nof correctable errors guaranteed by our analysis surpasses previous\nnon-asymptotic results for LDPC codes, and in particular exceeds the best\nprevious finite-length result on LP decoding by a factor greater than ten. This\nimprovement stems in part from our analysis of probabilistic bit-flipping\nchannels, as opposed to adversarial channels. At the core of our analysis is a\nnovel combinatorial characterization of LP decoding success, based on the\nnotion of a generalized matching. An interesting by-product of our analysis is\nto establish the existence of ``probabilistic expansion'' in random bipartite\ngraphs, in which one requires only that almost every (as opposed to every) set\nof a certain size expands, for sets much larger than in the classical\nworst-case setting. \n\n"}
{"id": "cs/0702124", "contents": "Title: A Sequential Algorithm for Generating Random Graphs Abstract: We present a nearly-linear time algorithm for counting and randomly\ngenerating simple graphs with a given degree sequence in a certain range. For\ndegree sequence $(d_i)_{i=1}^n$ with maximum degree $d_{\\max}=O(m^{1/4-\\tau})$,\nour algorithm generates almost uniform random graphs with that degree sequence\nin time $O(m\\,d_{\\max})$ where $m=\\f{1}{2}\\sum_id_i$ is the number of edges in\nthe graph and $\\tau$ is any positive constant. The fastest known algorithm for\nuniform generation of these graphs McKay Wormald (1990) has a running time of\n$O(m^2d_{\\max}^2)$. Our method also gives an independent proof of McKay's\nestimate McKay (1985) for the number of such graphs.\n  We also use sequential importance sampling to derive fully Polynomial-time\nRandomized Approximation Schemes (FPRAS) for counting and uniformly generating\nrandom graphs for the same range of $d_{\\max}=O(m^{1/4-\\tau})$.\n  Moreover, we show that for $d = O(n^{1/2-\\tau})$, our algorithm can generate\nan asymptotically uniform $d$-regular graph. Our results improve the previous\nbound of $d = O(n^{1/3-\\tau})$ due to Kim and Vu (2004) for regular graphs. \n\n"}
{"id": "cs/0703110", "contents": "Title: Geometric Complexity Theory IV: nonstandard quantum group for the\n  Kronecker problem Abstract: The Kronecker coefficient g_{\\lambda \\mu \\nu} is the multiplicity of the\nGL(V)\\times GL(W)-irreducible V_\\lambda \\otimes W_\\mu in the restriction of the\nGL(X)-irreducible X_\\nu via the natural map GL(V)\\times GL(W) \\to GL(V \\otimes\nW), where V, W are \\mathbb{C}-vector spaces and X = V \\otimes W. A fundamental\nopen problem in algebraic combinatorics is to find a positive combinatorial\nformula for these coefficients.\n  We construct two quantum objects for this problem, which we call the\nnonstandard quantum group and nonstandard Hecke algebra. We show that the\nnonstandard quantum group has a compact real form and its representations are\ncompletely reducible, that the nonstandard Hecke algebra is semisimple, and\nthat they satisfy an analog of quantum Schur-Weyl duality.\n  Using these nonstandard objects as a guide, we follow the approach of Adsul,\nSohoni, and Subrahmanyam to construct, in the case dim(V) = dim(W) =2, a\nrepresentation \\check{X}_\\nu of the nonstandard quantum group that specializes\nto Res_{GL(V) \\times GL(W)} X_\\nu at q=1. We then define a global crystal basis\n+HNSTC(\\nu) of \\check{X}_\\nu that solves the two-row Kronecker problem: the\nnumber of highest weight elements of +HNSTC(\\nu) of weight (\\lambda,\\mu) is the\nKronecker coefficient g_{\\lambda \\mu \\nu}. We go on to develop the beginnings\nof a graphical calculus for this basis, along the lines of the U_q(\\sl_2)\ngraphical calculus, and use this to organize the crystal components of\n+HNSTC(\\nu) into eight families. This yields a fairly simple, explicit and\npositive formula for two-row Kronecker coefficients, generalizing a formula of\nBrown, van Willigenburg, and Zabrocki. As a byproduct of the approach, we also\nobtain a rule for the decomposition of Res_{GL_2 \\times GL_2 \\rtimes \\S_2}\nX_\\nu into irreducibles. \n\n"}
{"id": "cs/0703145", "contents": "Title: The Simultaneous Triple Product Property and Group-theoretic Results for\n  the Exponent of Matrix Multiplication Abstract: We describe certain special consequences of certain elementary methods from\ngroup theory for studying the algebraic complexity of matrix multiplication, as\ndeveloped by H. Cohn, C. Umans et. al. in 2003 and 2005. The measure of\ncomplexity here is the exponent of matrix multiplication, a real parameter\nbetween 2 and 3, which has been conjectured to be 2. More specifically, a\nfinite group may simultaneously \"realize\" several independent matrix\nmultiplications via its regular algebra if it has a family of triples of\n\"index\" subsets which satisfy the so-called simultaneous triple product\nproperty (STPP), in which case the complexity of these several multiplications\ndoes not exceed the rank (complexity) of the algebra. This leads to bounds for\nthe exponent in terms of the size of the group and the sizes of its STPP\ntriples, as well as the dimensions of its distinct irreducible representations.\nWreath products of Abelian with symmetric groups appear especially important,\nin this regard, and we give an example of such a group which shows that the\nexponent is less than 2.84, and could be possibly be as small as 2.02 depending\non the number of simultaneous matrix multiplications it realizes. \n\n"}
{"id": "cs/9904019", "contents": "Title: Bounds for Small-Error and Zero-Error Quantum Algorithms Abstract: We present a number of results related to quantum algorithms with small error\nprobability and quantum algorithms that are zero-error. First, we give a tight\nanalysis of the trade-offs between the number of queries of quantum search\nalgorithms, their error probability, the size of the search space, and the\nnumber of solutions in this space. Using this, we deduce new lower and upper\nbounds for quantum versions of amplification problems. Next, we establish\nnearly optimal quantum-classical separations for the query complexity of\nmonotone functions in the zero-error model (where our quantum zero-error model\nis defined so as to be robust when the quantum gates are noisy). Also, we\npresent a communication complexity problem related to a total function for\nwhich there is a quantum-classical communication complexity gap in the\nzero-error model. Finally, we prove separations for monotone graph properties\nin the zero-error and other error models which imply that the evasiveness\nconjecture for such properties does not hold for quantum computers. \n\n"}
{"id": "math/0210052", "contents": "Title: Criteria for Balance in Abelian Gain Graphs, with Applications to\n  Piecewise-Linear Geometry Abstract: A gain graph is a triple (G,h,H), where G is a connected graph with an\narbitrary, but fixed, orientation of edges, H is a group, and h is a\nhomomorphism from the free group on the edges of G to H. A gain graph is called\nbalanced if the h-image of each closed walk on G is the identity.\n  Consider a gain graph with abelian gain group having no odd torsion. If there\nis a basis of the graph's binary cycle space each of whose members can be\nlifted to a closed walk whose gain is the identity, then the gain graph is\nbalanced, provided that the graph is finite or the group has no nontrivial\ninfinitely 2-divisible elements. We apply this theorem to deduce a result on\nthe projective geometry of piecewise-linear realizations of cell-decompositions\nof manifolds. \n\n"}
{"id": "math/0304100", "contents": "Title: A Direct Ultrametric Approach to Additive Complexity and the Shub-Smale\n  Tau Conjecture Abstract: The Shub-Smale Tau Conjecture is a hypothesis relating the number of integral\nroots of a polynomial f in one variable and the Straight-Line Program (SLP)\ncomplexity of f. A consequence of the truth of this conjecture is that, for the\nBlum-Shub-Smale model over the complex numbers, P differs from NP. We prove two\nweak versions of the Tau Conjecture and in so doing show that the Tau\nConjecture follows from an even more plausible hypothesis.\n  Our results follow from a new p-adic analogue of earlier work relating real\nalgebraic geometry to additive complexity. For instance, we can show that a\nnonzero univariate polynomial of additive complexity s can have no more than\n15+s^3(s+1)(7.5)^s s! =O(e^{s\\log s}) roots in the 2-adic rational numbers Q_2,\nthus dramatically improving an earlier result of the author. This immediately\nimplies the same bound on the number of ordinary rational roots, whereas the\nbest previous upper bound via earlier techniques from real algebraic geometry\nwas a quantity in Omega((22.6)^{s^2}).\n  This paper presents another step in the author's program of establishing an\nalgorithmic arithmetic version of fewnomial theory. \n\n"}
{"id": "math/0311047", "contents": "Title: Assessing security of some group based cryptosystems Abstract: One of the possible generalizations of the discrete logarithm problem to\narbitrary groups is the so-called conjugacy search problem (sometimes\nerroneously called just the conjugacy problem): given two elements a, b of a\ngroup G and the information that a^x=b for some x \\in G, find at least one\nparticular element x like that. Here a^x stands for xax^{-1}. The computational\ndifficulty of this problem in some particular groups has been used in several\ngroup based cryptosystems. Recently, a few preprints have been in circulation\nthat suggested various \"neighbourhood search\" type heuristic attacks on the\nconjugacy search problem. The goal of the present survey is to stress a\n(probably well known) fact that these heuristic attacks alone are not a threat\nto the security of a cryptosystem, and, more importantly, to suggest a more\ncredible approach to assessing security of group based cryptosystems. Such an\napproach should be necessarily based on the concept of the average case\ncomplexity (or expected running time) of an algorithm.\n  These arguments support the following conclusion: although it is generally\nfeasible to base the security of a cryptosystem on the difficulty of the\nconjugacy search problem, the group G itself (the \"platform\") has to be chosen\nvery carefully. In particular, experimental as well as theoretical evidence\ncollected so far makes it appear likely that braid groups are not a good choice\nfor the platform. We also reflect on possible replacements. \n\n"}
{"id": "math/0501388", "contents": "Title: Efficiently Detecting Torsion Points and Subtori Abstract: Suppose X is the complex zero set of a finite collection of polynomials in\nZ[x_1,...,x_n]. We show that deciding whether X contains a point all of whose\ncoordinates are d_th roots of unity can be done within NP^NP (relative to the\nsparse encoding), under a plausible assumption on primes in arithmetic\nprogression. In particular, our hypothesis can still hold even under certain\nfailures of the Generalized Riemann Hypothesis, such as the presence of\nSiegel-Landau zeroes. Furthermore, we give a similar (but UNconditional)\ncomplexity upper bound for n=1. Finally, letting T be any algebraic subgroup of\n(C^*)^n we show that deciding whether X contains T is coNP-complete (relative\nto an even more efficient encoding),unconditionally. We thus obtain new\nnon-trivial families of multivariate polynomial systems where deciding the\nexistence of complex roots can be done unconditionally in the polynomial\nhierarchy -- a family of complexity classes lying between PSPACE and P,\nintimately connected with the P=?NP Problem. We also discuss a connection to\nLaurent's solution of Chabauty's Conjecture from arithmetic geometry. \n\n"}
{"id": "quant-ph/0002066", "contents": "Title: Quantum lower bounds by quantum arguments Abstract: We propose a new method for proving lower bounds on quantum query algorithms.\nInstead of a classical adversary that runs the algorithm with one input and\nthen modifies the input, we use a quantum adversary that runs the algorithm\nwith a superposition of inputs. If the algorithm works correctly, its state\nbecomes entangled with the superposition over inputs. We bound the number of\nqueries needed to achieve a sufficient entanglement and this implies a lower\nbound on the number of queries for the computation.\n  Using this method, we prove two new $\\Omega(\\sqrt{N})$ lower bounds on\ncomputing AND of ORs and inverting a permutation and also provide more uniform\nproofs for several known lower bounds which have been previously proven via\nvariety of different techniques. \n\n"}
{"id": "quant-ph/0110018", "contents": "Title: Algorithmic Information Theoretic Issues in Quantum Mechanics Abstract: taking aside the review part, a finite-cardinality's set of new ideas\nconcerning algorithmic information issues in Quantum Mechanics is introduced\nand analyzed \n\n"}
{"id": "quant-ph/0307017", "contents": "Title: Quaternionic Computing Abstract: We introduce a model of computation based on quaternions, which is inspired\non the quantum computing model. Pure states are vectors of a suitable linear\nspace over the quaternions. Other aspects of the theory are the same as in\nquantum computing: superposition and linearity of the state space, unitarity of\nthe transformations, and projective measurements. However, one notable\nexception is the fact that quaternionic circuits do not have a uniquely defined\nbehaviour, unless a total ordering of evaluation of the gates is defined. Given\nsuch an ordering a unique unitary operator can be associated with the\nquaternionic circuit and a proper semantics of computation can be associated\nwith it.\n  The main result of this paper consists in showing that this model is no more\npowerful than quantum computing, as long as such an ordering of gates can be\ndefined. More concretely we show, that for all quaternionic computation using n\nquaterbits, the behaviour of the circuit for each possible gate ordering can be\nsimulated with n+1 qubits, and this with little or no overhead in circuit size.\nThe proof of this result is inspired of a new simplified and improved proof of\nthe equivalence of a similar model based on real amplitudes to quantum\ncomputing, which states that any quantum computation using n qubits can be\nsimulated with n+1 rebits, and in this with no circuit size overhead.\n  Beyond this potential computational equivalence, however, we propose this\nmodel as a simpler framework in which to discuss the possibility of a\nquaternionic quantum mechanics or information theory. In particular, it already\nallows us to illustrate that the introduction of quaternions might violate some\nof the ``natural'' properties that we have come to expect from physical models. \n\n"}
{"id": "quant-ph/0402095", "contents": "Title: Limitations of Quantum Advice and One-Way Communication Abstract: Although a quantum state requires exponentially many classical bits to\ndescribe, the laws of quantum mechanics impose severe restrictions on how that\nstate can be accessed. This paper shows in three settings that quantum messages\nhave only limited advantages over classical ones.\n  First, we show that $\\mathsf{BQP/qpoly}\\subseteq\\mathsf{PP/poly}$, where\n$\\mathsf{BQP/qpoly}$ is the class of problems solvable in quantum polynomial\ntime, given a polynomial-size \"quantum advice state\" that depends only on the\ninput length. This resolves a question of Buhrman, and means that we should not\nhope for an unrelativized separation between quantum and classical advice.\nUnderlying our complexity result is a general new relation between\ndeterministic and quantum one-way communication complexities, which applies to\npartial as well as total functions.\n  Second, we construct an oracle relative to which $\\mathsf{NP}\\not \\subset\n\\mathsf{BQP/qpoly}$. To do so, we use the polynomial method to give the first\ncorrect proof of a direct product theorem for quantum search. This theorem has\nother applications; for example, it can be used to fix a result of Klauck about\nquantum time-space tradeoffs for sorting.\n  Third, we introduce a new trace distance method for proving lower bounds on\nquantum one-way communication complexity. Using this method, we obtain optimal\nquantum lower bounds for two problems of Ambainis, for which no nontrivial\nlower bounds were previously known even for classical randomized protocols.\n  A preliminary version of this paper appeared in the 2004 Conference on\nComputational Complexity (CCC). \n\n"}
{"id": "quant-ph/0504012", "contents": "Title: Quantum search algorithms Abstract: We review some of quantum algorithms for search problems: Grover's search\nalgorithm, its generalization to amplitude amplification, the applications of\namplitude amplification to various problems and the recent quantum algorithms\nbased on quantum walks. \n\n"}
{"id": "quant-ph/0506200", "contents": "Title: Solving Satisfiability Problems by the Ground-State Quantum Computer Abstract: A quantum algorithm is proposed to solve the Satisfiability problems by the\nground-state quantum computer. The scale of the energy gap of the ground-state\nquantum computer is analyzed for the 3-bit Exact Cover problem. The time cost\nof this algorithm on the general SAT problems is discussed. \n\n"}
{"id": "quant-ph/0511200", "contents": "Title: A New Quantum Lower Bound Method, with Applications to Direct Product\n  Theorems and Time-Space Tradeoffs Abstract: We give a new version of the adversary method for proving lower bounds on\nquantum query algorithms. The new method is based on analyzing the eigenspace\nstructure of the problem at hand. We use it to prove a new and optimal strong\ndirect product theorem for 2-sided error quantum algorithms computing k\nindependent instances of a symmetric Boolean function: if the algorithm uses\nsignificantly less than k times the number of queries needed for one instance\nof the function, then its success probability is exponentially small in k. We\nalso use the polynomial method to prove a direct product theorem for 1-sided\nerror algorithms for k threshold functions with a stronger bound on the success\nprobability. Finally, we present a quantum algorithm for evaluating solutions\nto systems of linear inequalities, and use our direct product theorems to show\nthat the time-space tradeoff of this algorithm is close to optimal. \n\n"}
{"id": "quant-ph/0604056", "contents": "Title: Quantum Versus Classical Proofs and Advice Abstract: This paper studies whether quantum proofs are more powerful than classical\nproofs, or in complexity terms, whether QMA=QCMA. We prove three results about\nthis question. First, we give a \"quantum oracle separation\" between QMA and\nQCMA. More concretely, we show that any quantum algorithm needs\n$\\Omega(\\sqrt{2^n/(m+1)})$ queries to find an $n$-qubit \"marked state\"\n$\\lvert\\psi\\rangle$, even if given an $m$-bit classical description of\n$\\lvert\\psi\\rangle$ together with a quantum black box that recognizes\n$\\lvert\\psi\\rangle$. Second, we give an explicit QCMA protocol that nearly\nachieves this lower bound. Third, we show that, in the one previously-known\ncase where quantum proofs seemed to provide an exponential advantage, classical\nproofs are basically just as powerful. In particular, Watrous gave a QMA\nprotocol for verifying non-membership in finite groups. Under plausible\ngroup-theoretic assumptions, we give a QCMA protocol for the same problem. Even\nwith no assumptions, our protocol makes only polynomially many queries to the\ngroup oracle. We end with some conjectures about quantum versus classical\noracles, and about the possibility of a classical oracle separation between QMA\nand QCMA. \n\n"}
{"id": "quant-ph/0608026", "contents": "Title: Search via Quantum Walk Abstract: We propose a new method for designing quantum search algorithms for finding a\n\"marked\" element in the state space of a classical Markov chain. The algorithm\nis based on a quantum walk \\'a la Szegedy (2004) that is defined in terms of\nthe Markov chain. The main new idea is to apply quantum phase estimation to the\nquantum walk in order to implement an approximate reflection operator. This\noperator is then used in an amplitude amplification scheme. As a result we\nconsiderably expand the scope of the previous approaches of Ambainis (2004) and\nSzegedy (2004). Our algorithm combines the benefits of these approaches in\nterms of being able to find marked elements, incurring the smaller cost of the\ntwo, and being applicable to a larger class of Markov chains. In addition, it\nis conceptually simple and avoids some technical difficulties in the previous\nanalyses of several algorithms based on quantum walk. \n\n"}
{"id": "quant-ph/9904079", "contents": "Title: Average-Case Quantum Query Complexity Abstract: We compare classical and quantum query complexities of total Boolean\nfunctions. It is known that for worst-case complexity, the gap between quantum\nand classical can be at most polynomial. We show that for average-case\ncomplexity under the uniform distribution, quantum algorithms can be\nexponentially faster than classical algorithms. Under non-uniform distributions\nthe gap can even be super-exponential. We also prove some general bounds for\naverage-case complexity and show that the average-case quantum complexity of\nMAJORITY under the uniform distribution is nearly quadratically better than the\nclassical complexity. \n\n"}
