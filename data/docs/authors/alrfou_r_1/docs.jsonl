{"id": "0704.2902", "contents": "Title: Recommending Related Papers Based on Digital Library Access Records Abstract: An important goal for digital libraries is to enable researchers to more\neasily explore related work. While citation data is often used as an indicator\nof relatedness, in this paper we demonstrate that digital access records (e.g.\nhttp-server logs) can be used as indicators as well. In particular, we show\nthat measures based on co-access provide better coverage than co-citation, that\nthey are available much sooner, and that they are more accurate for recent\npapers. \n\n"}
{"id": "0707.0895", "contents": "Title: Segmentation and Context of Literary and Musical Sequences Abstract: We test a segmentation algorithm, based on the calculation of the\nJensen-Shannon divergence between probability distributions, to two symbolic\nsequences of literary and musical origin. The first sequence represents the\nsuccessive appearance of characters in a theatrical play, and the second\nrepresents the succession of tones from the twelve-tone scale in a keyboard\nsonata. The algorithm divides the sequences into segments of maximal\ncompositional divergence between them. For the play, these segments are related\nto changes in the frequency of appearance of different characters and in the\ngeographical setting of the action. For the sonata, the segments correspond to\ntonal domains and reveal in detail the characteristic tonal progression of such\nkind of musical composition. \n\n"}
{"id": "0711.2270", "contents": "Title: Can a Computer Laugh ? Abstract: A computer model of \"a sense of humour\" suggested previously\n[arXiv:0711.2058,0711.2061], relating the humorous effect with a specific\nmalfunction in information processing, is given in somewhat different\nexposition. Psychological aspects of humour are elaborated more thoroughly. The\nmechanism of laughter is formulated on the more general level. Detailed\ndiscussion is presented for the higher levels of information processing, which\nare responsible for a perception of complex samples of humour. Development of a\nsense of humour in the process of evolution is discussed. \n\n"}
{"id": "0903.0034", "contents": "Title: Measuring Independence of Datasets Abstract: A data stream model represents setting where approximating pairwise, or\n$k$-wise, independence with sublinear memory is of considerable importance. In\nthe streaming model the joint distribution is given by a stream of $k$-tuples,\nwith the goal of testing correlations among the components measured over the\nentire stream. In the streaming model, Indyk and McGregor (SODA 08) recently\ngave exciting new results for measuring pairwise independence. The Indyk and\nMcGregor methods provide $\\log{n}$-approximation under statistical distance\nbetween the joint and product distributions in the streaming model. Indyk and\nMcGregor leave, as their main open question, the problem of improving their\n$\\log n$-approximation for the statistical distance metric.\n  In this paper we solve the main open problem posed by of Indyk and McGregor\nfor the statistical distance for pairwise independence and extend this result\nto any constant $k$. In particular, we present an algorithm that computes an\n$(\\epsilon, \\delta)$-approximation of the statistical distance between the\njoint and product distributions defined by a stream of $k$-tuples. Our\nalgorithm requires $O(({1\\over \\epsilon}\\log({nm\\over \\delta}))^{(30+k)^k})$\nmemory and a single pass over the data stream. \n\n"}
{"id": "1001.2186", "contents": "Title: Building reputation systems for better ranking Abstract: How to rank web pages, scientists and online resources has recently attracted\nincreasing attention from both physicists and computer scientists. In this\npaper, we study the ranking problem of rating systems where users vote objects\nby discrete ratings. We propose an algorithm that can simultaneously evaluate\nthe user reputation and object quality in an iterative refinement way.\nAccording to both the artificially generated data and the real data from\nMovieLens and Amazon, our algorithm can considerably enhance the ranking\naccuracy. This work highlights the significance of reputation systems in the\nInternet era and points out a way to evaluate and compare the performances of\ndifferent reputation systems. \n\n"}
{"id": "1003.3661", "contents": "Title: An HTTP-Based Versioning Mechanism for Linked Data Abstract: Dereferencing a URI returns a representation of the current state of the\nresource identified by that URI. But, on the Web representations of prior\nstates of a resource are also available, for example, as resource versions in\nContent Management Systems or archival resources in Web Archives such as the\nInternet Archive. This paper introduces a resource versioning mechanism that is\nfully based on HTTP and uses datetime as a global version indicator. The\napproach allows \"follow your nose\" style navigation both from the current\ntime-generic resource to associated time-specific version resources as well as\namong version resources. The proposed versioning mechanism is congruent with\nthe Architecture of the World Wide Web, and is based on the Memento framework\nthat extends HTTP with transparent content negotiation in the datetime\ndimension. The paper shows how the versioning approach applies to Linked Data,\nand by means of a demonstrator built for DBpedia, it also illustrates how it\ncan be used to conduct a time-series analysis across versions of Linked Data\ndescriptions. \n\n"}
{"id": "1003.4146", "contents": "Title: A Mathematical Approach to the Study of the United States Code Abstract: The United States Code (Code) is a document containing over 22 million words\nthat represents a large and important source of Federal statutory law. Scholars\nand policy advocates often discuss the direction and magnitude of changes in\nvarious aspects of the Code. However, few have mathematically formalized the\nnotions behind these discussions or directly measured the resulting\nrepresentations. This paper addresses the current state of the literature in\ntwo ways. First, we formalize a representation of the United States Code as the\nunion of a hierarchical network and a citation network over vertices containing\nthe language of the Code. This representation reflects the fact that the Code\nis a hierarchically organized document containing language and explicit\ncitations between provisions. Second, we use this formalization to measure\naspects of the Code as codified in October 2008, November 2009, and March 2010.\nThese measurements allow for a characterization of the actual changes in the\nCode over time. Our findings indicate that in the recent past, the Code has\ngrown in its amount of structure, interdependence, and language. \n\n"}
{"id": "1102.1027", "contents": "Title: Collective Classification of Textual Documents by Guided\n  Self-Organization in T-Cell Cross-Regulation Dynamics Abstract: We present and study an agent-based model of T-Cell cross-regulation in the\nadaptive immune system, which we apply to binary classification. Our method\nexpands an existing analytical model of T-cell cross-regulation (Carneiro et\nal. in Immunol Rev 216(1):48-68, 2007) that was used to study the\nself-organizing dynamics of a single population of T-Cells in interaction with\nan idealized antigen presenting cell capable of presenting a single antigen.\nWith agent-based modeling we are able to study the self-organizing dynamics of\nmultiple populations of distinct T-cells which interact via antigen presenting\ncells that present hundreds of distinct antigens. Moreover, we show that such\nself-organizing dynamics can be guided to produce an effective binary\nclassification of antigens, which is competitive with existing machine learning\nmethods when applied to biomedical text classification. More specifically, here\nwe test our model on a dataset of publicly available full-text biomedical\narticles provided by the BioCreative challenge (Krallinger in The biocreative\nii. 5 challenge overview, p 19, 2009). We study the robustness of our model's\nparameter configurations, and show that it leads to encouraging results\ncomparable to state-of-the-art classifiers. Our results help us understand both\nT-cell cross-regulation as a general principle of guided self-organization, as\nwell as its applicability to document classification. Therefore, we show that\nour bio-inspired algorithm is a promising novel method for biomedical article\nclassification and for binary document classification in general. \n\n"}
{"id": "1103.0890", "contents": "Title: Efficient Multi-Template Learning for Structured Prediction Abstract: Conditional random field (CRF) and Structural Support Vector Machine\n(Structural SVM) are two state-of-the-art methods for structured prediction\nwhich captures the interdependencies among output variables. The success of\nthese methods is attributed to the fact that their discriminative models are\nable to account for overlapping features on the whole input observations. These\nfeatures are usually generated by applying a given set of templates on labeled\ndata, but improper templates may lead to degraded performance. To alleviate\nthis issue, in this paper, we propose a novel multiple template learning\nparadigm to learn structured prediction and the importance of each template\nsimultaneously, so that hundreds of arbitrary templates could be added into the\nlearning model without caution. This paradigm can be formulated as a special\nmultiple kernel learning problem with exponential number of constraints. Then\nwe introduce an efficient cutting plane algorithm to solve this problem in the\nprimal, and its convergence is presented. We also evaluate the proposed\nlearning paradigm on two widely-studied structured prediction tasks,\n\\emph{i.e.} sequence labeling and dependency parsing. Extensive experimental\nresults show that the proposed method outperforms CRFs and Structural SVMs due\nto exploiting the importance of each template. Our complexity analysis and\nempirical results also show that our proposed method is more efficient than\nOnlineMKL on very sparse and high-dimensional data. We further extend this\nparadigm for structured prediction using generalized $p$-block norm\nregularization with $p>1$, and experiments show competitive performances when\n$p \\in [1,2)$. \n\n"}
{"id": "1106.2229", "contents": "Title: Fast, Linear Time Hierarchical Clustering using the Baire Metric Abstract: The Baire metric induces an ultrametric on a dataset and is of linear\ncomputational complexity, contrasted with the standard quadratic time\nagglomerative hierarchical clustering algorithm. In this work we evaluate\nempirically this new approach to hierarchical clustering. We compare\nhierarchical clustering based on the Baire metric with (i) agglomerative\nhierarchical clustering, in terms of algorithm properties; (ii) generalized\nultrametrics, in terms of definition; and (iii) fast clustering through k-means\npartititioning, in terms of quality of results. For the latter, we carry out an\nin depth astronomical study. We apply the Baire distance to spectrometric and\nphotometric redshifts from the Sloan Digital Sky Survey using, in this work,\nabout half a million astronomical objects. We want to know how well the (more\ncostly to determine) spectrometric redshifts can predict the (more easily\nobtained) photometric redshifts, i.e. we seek to regress the spectrometric on\nthe photometric redshifts, and we use clusterwise regression for this. \n\n"}
{"id": "1112.2187", "contents": "Title: Chinese Restaurant Game - Part II: Applications to Wireless Networking,\n  Cloud Computing, and Online Social Networking Abstract: In Part I of this two-part paper [1], we proposed a new game, called Chinese\nrestaurant game, to analyze the social learning problem with negative network\nexternality. The best responses of agents in the Chinese restaurant game with\nimperfect signals are constructed through a recursive method, and the influence\nof both learning and network externality on the utilities of agents is studied.\nIn Part II of this two-part paper, we illustrate three applications of Chinese\nrestaurant game in wireless networking, cloud computing, and online social\nnetworking. For each application, we formulate the corresponding problem as a\nChinese restaurant game and analyze how agents learn and make strategic\ndecisions in the problem. The proposed method is compared with four\ncommon-sense methods in terms of agents' utilities and the overall system\nperformance through simulations. We find that the proposed Chinese restaurant\ngame theoretic approach indeed helps agents make better decisions and improves\nthe overall system performance. Furthermore, agents with different decision\norders have different advantages in terms of their utilities, which also\nverifies the conclusions drawn in Part I of this two-part paper. \n\n"}
{"id": "1112.2188", "contents": "Title: Chinese Restaurant Game - Part I: Theory of Learning with Negative\n  Network Externality Abstract: In a social network, agents are intelligent and have the capability to make\ndecisions to maximize their utilities. They can either make wise decisions by\ntaking advantages of other agents' experiences through learning, or make\ndecisions earlier to avoid competitions from huge crowds. Both these two\neffects, social learning and negative network externality, play important roles\nin the decision process of an agent. While there are existing works on either\nsocial learning or negative network externality, a general study on considering\nboth these two contradictory effects is still limited. We find that the Chinese\nrestaurant process, a popular random process, provides a well-defined structure\nto model the decision process of an agent under these two effects. By\nintroducing the strategic behavior into the non-strategic Chinese restaurant\nprocess, in Part I of this two-part paper, we propose a new game, called\nChinese Restaurant Game, to formulate the social learning problem with negative\nnetwork externality. Through analyzing the proposed Chinese restaurant game, we\nderive the optimal strategy of each agent and provide a recursive method to\nachieve the optimal strategy. How social learning and negative network\nexternality influence each other under various settings is also studied through\nsimulations. \n\n"}
{"id": "1202.2903", "contents": "Title: Scaling Laws in Human Language Abstract: Zipf's law on word frequency is observed in English, French, Spanish,\nItalian, and so on, yet it does not hold for Chinese, Japanese or Korean\ncharacters. A model for writing process is proposed to explain the above\ndifference, which takes into account the effects of finite vocabulary size.\nExperiments, simulations and analytical solution agree well with each other.\nThe results show that the frequency distribution follows a power law with\nexponent being equal to 1, at which the corresponding Zipf's exponent diverges.\nActually, the distribution obeys exponential form in the Zipf's plot. Deviating\nfrom the Heaps' law, the number of distinct words grows with the text length in\nthree stages: It grows linearly in the beginning, then turns to a logarithmical\nform, and eventually saturates. This work refines previous understanding about\nZipf's law and Heaps' law in language systems. \n\n"}
{"id": "1204.0170", "contents": "Title: A New Approach to Speeding Up Topic Modeling Abstract: Latent Dirichlet allocation (LDA) is a widely-used probabilistic topic\nmodeling paradigm, and recently finds many applications in computer vision and\ncomputational biology. In this paper, we propose a fast and accurate batch\nalgorithm, active belief propagation (ABP), for training LDA. Usually batch LDA\nalgorithms require repeated scanning of the entire corpus and searching the\ncomplete topic space. To process massive corpora having a large number of\ntopics, the training iteration of batch LDA algorithms is often inefficient and\ntime-consuming. To accelerate the training speed, ABP actively scans the subset\nof corpus and searches the subset of topic space for topic modeling, therefore\nsaves enormous training time in each iteration. To ensure accuracy, ABP selects\nonly those documents and topics that contribute to the largest residuals within\nthe residual belief propagation (RBP) framework. On four real-world corpora,\nABP performs around $10$ to $100$ times faster than state-of-the-art batch LDA\nalgorithms with a comparable topic modeling accuracy. \n\n"}
{"id": "1205.2056", "contents": "Title: Dynamic Behavioral Mixed-Membership Model for Large Evolving Networks Abstract: The majority of real-world networks are dynamic and extremely large (e.g.,\nInternet Traffic, Twitter, Facebook, ...). To understand the structural\nbehavior of nodes in these large dynamic networks, it may be necessary to model\nthe dynamics of behavioral roles representing the main connectivity patterns\nover time. In this paper, we propose a dynamic behavioral mixed-membership\nmodel (DBMM) that captures the roles of nodes in the graph and how they evolve\nover time. Unlike other node-centric models, our model is scalable for\nanalyzing large dynamic networks. In addition, DBMM is flexible,\nparameter-free, has no functional form or parameterization, and is\ninterpretable (identifies explainable patterns). The performance results\nindicate our approach can be applied to very large networks while the\nexperimental results show that our model uncovers interesting patterns\nunderlying the dynamics of these networks. \n\n"}
{"id": "1205.3193", "contents": "Title: A Comparative Study of Collaborative Filtering Algorithms Abstract: Collaborative filtering is a rapidly advancing research area. Every year\nseveral new techniques are proposed and yet it is not clear which of the\ntechniques work best and under what conditions. In this paper we conduct a\nstudy comparing several collaborative filtering techniques -- both classic and\nrecent state-of-the-art -- in a variety of experimental contexts. Specifically,\nwe report conclusions controlling for number of items, number of users,\nsparsity level, performance criteria, and computational complexity. Our\nconclusions identify what algorithms work well and in what conditions, and\ncontribute to both industrial deployment collaborative filtering algorithms and\nto the research community. \n\n"}
{"id": "1207.2340", "contents": "Title: Pseudo-likelihood methods for community detection in large sparse\n  networks Abstract: Many algorithms have been proposed for fitting network models with\ncommunities, but most of them do not scale well to large networks, and often\nfail on sparse networks. Here we propose a new fast pseudo-likelihood method\nfor fitting the stochastic block model for networks, as well as a variant that\nallows for an arbitrary degree distribution by conditioning on degrees. We show\nthat the algorithms perform well under a range of settings, including on very\nsparse networks, and illustrate on the example of a network of political blogs.\nWe also propose spectral clustering with perturbations, a method of independent\ninterest, which works well on sparse networks where regular spectral clustering\nfails, and use it to provide an initial value for pseudo-likelihood. We prove\nthat pseudo-likelihood provides consistent estimates of the communities under a\nmild condition on the starting value, for the case of a block model with two\ncommunities. \n\n"}
{"id": "1208.0787", "contents": "Title: A Random Walk Based Model Incorporating Social Information for\n  Recommendations Abstract: Collaborative filtering (CF) is one of the most popular approaches to build a\nrecommendation system. In this paper, we propose a hybrid collaborative\nfiltering model based on a Makovian random walk to address the data sparsity\nand cold start problems in recommendation systems. More precisely, we construct\na directed graph whose nodes consist of items and users, together with item\ncontent, user profile and social network information. We incorporate user's\nratings into edge settings in the graph model. The model provides personalized\nrecommendations and predictions to individuals and groups. The proposed\nalgorithms are evaluated on MovieLens and Epinions datasets. Experimental\nresults show that the proposed methods perform well compared with other\ngraph-based methods, especially in the cold start case. \n\n"}
{"id": "1209.2910", "contents": "Title: Community Detection in the Labelled Stochastic Block Model Abstract: We consider the problem of community detection from observed interactions\nbetween individuals, in the context where multiple types of interaction are\npossible. We use labelled stochastic block models to represent the observed\ndata, where labels correspond to interaction types. Focusing on a two-community\nscenario, we conjecture a threshold for the problem of reconstructing the\nhidden communities in a way that is correlated with the true partition. To\nsubstantiate the conjecture, we prove that the given threshold correctly\nidentifies a transition on the behaviour of belief propagation from insensitive\nto sensitive. We further prove that the same threshold corresponds to the\ntransition in a related inference problem on a tree model from infeasible to\nfeasible. Finally, numerical results using belief propagation for community\ndetection give further support to the conjecture. \n\n"}
{"id": "1212.6110", "contents": "Title: Hyperplane Arrangements and Locality-Sensitive Hashing with Lift Abstract: Locality-sensitive hashing converts high-dimensional feature vectors, such as\nimage and speech, into bit arrays and allows high-speed similarity calculation\nwith the Hamming distance. There is a hashing scheme that maps feature vectors\nto bit arrays depending on the signs of the inner products between feature\nvectors and the normal vectors of hyperplanes placed in the feature space. This\nhashing can be seen as a discretization of the feature space by hyperplanes. If\nlabels for data are given, one can determine the hyperplanes by using learning\nalgorithms. However, many proposed learning methods do not consider the\nhyperplanes' offsets. Not doing so decreases the number of partitioned regions,\nand the correlation between Hamming distances and Euclidean distances becomes\nsmall. In this paper, we propose a lift map that converts learning algorithms\nwithout the offsets to the ones that take into account the offsets. With this\nmethod, the learning methods without the offsets give the discretizations of\nspaces as if it takes into account the offsets. For the proposed method, we\ninput several high-dimensional feature data sets and studied the relationship\nbetween the statistical characteristics of data, the number of hyperplanes, and\nthe effect of the proposed method. \n\n"}
{"id": "1301.3627", "contents": "Title: Two SVDs produce more focal deep learning representations Abstract: A key characteristic of work on deep learning and neural networks in general\nis that it relies on representations of the input that support generalization,\nrobust inference, domain adaptation and other desirable functionalities. Much\nrecent progress in the field has focused on efficient and effective methods for\ncomputing representations. In this paper, we propose an alternative method that\nis more efficient than prior work and produces representations that have a\nproperty we call focality -- a property we hypothesize to be important for\nneural network representations. The method consists of a simple application of\ntwo consecutive SVDs and is inspired by Anandkumar (2012). \n\n"}
{"id": "1302.0420", "contents": "Title: Benchmarking some Portuguese S&T system research units: 2nd Edition Abstract: The increasing use of productivity and impact metrics for evaluation and\ncomparison, not only of individual researchers but also of institutions,\nuniversities and even countries, has prompted the development of bibliometrics.\nCurrently, metrics are becoming widely accepted as an easy and balanced way to\nassist the peer review and evaluation of scientists and/or research units,\nprovided they have adequate precision and recall.\n  This paper presents a benchmarking study of a selected list of representative\nPortuguese research units, based on a fairly complete set of parameters:\nbibliometric parameters, number of competitive projects and number of PhDs\nproduced. The study aimed at collecting productivity and impact data from the\nselected research units in comparable conditions i.e., using objective metrics\nbased on public information, retrievable on-line and/or from official sources\nand thus verifiable and repeatable. The study has thus focused on the activity\nof the 2003-06 period, where such data was available from the latest official\nevaluation.\n  The main advantage of our study was the application of automatic tools,\nachieving relevant results at a reduced cost. Moreover, the results over the\nselected units suggest that this kind of analyses will be very useful to\nbenchmark scientific productivity and impact, and assist peer review. \n\n"}
{"id": "1302.2671", "contents": "Title: Latent Self-Exciting Point Process Model for Spatial-Temporal Networks Abstract: We propose a latent self-exciting point process model that describes\ngeographically distributed interactions between pairs of entities. In contrast\nto most existing approaches that assume fully observable interactions, here we\nconsider a scenario where certain interaction events lack information about\nparticipants. Instead, this information needs to be inferred from the available\nobservations. We develop an efficient approximate algorithm based on\nvariational expectation-maximization to infer unknown participants in an event\ngiven the location and the time of the event. We validate the model on\nsynthetic as well as real-world data, and obtain very promising results on the\nidentity-inference task. We also use our model to predict the timing and\nparticipants of future events, and demonstrate that it compares favorably with\nbaseline approaches. \n\n"}
{"id": "1303.0350", "contents": "Title: Structure-semantics interplay in complex networks and its effects on the\n  predictability of similarity in texts Abstract: There are different ways to define similarity for grouping similar texts into\nclusters, as the concept of similarity may depend on the purpose of the task.\nFor instance, in topic extraction similar texts mean those within the same\nsemantic field, whereas in author recognition stylistic features should be\nconsidered. In this study, we introduce ways to classify texts employing\nconcepts of complex networks, which may be able to capture syntactic, semantic\nand even pragmatic features. The interplay between the various metrics of the\ncomplex networks is analyzed with three applications, namely identification of\nmachine translation (MT) systems, evaluation of quality of machine translated\ntexts and authorship recognition. We shall show that topological features of\nthe networks representing texts can enhance the ability to identify MT systems\nin particular cases. For evaluating the quality of MT texts, on the other hand,\nhigh correlation was obtained with methods capable of capturing the semantics.\nThis was expected because the golden standards used are themselves based on\nword co-occurrence. Notwithstanding, the Katz similarity, which involves\nsemantic and structure in the comparison of texts, achieved the highest\ncorrelation with the NIST measurement, indicating that in some cases the\ncombination of both approaches can improve the ability to quantify quality in\nMT. In authorship recognition, again the topological features were relevant in\nsome contexts, though for the books and authors analyzed good results were\nobtained with semantic features as well. Because hybrid approaches encompassing\nsemantic and topological features have not been extensively used, we believe\nthat the methodology proposed here may be useful to enhance text classification\nconsiderably, as it combines well-established strategies. \n\n"}
{"id": "1303.2826", "contents": "Title: Probabilistic Topic and Syntax Modeling with Part-of-Speech LDA Abstract: This article presents a probabilistic generative model for text based on\nsemantic topics and syntactic classes called Part-of-Speech LDA (POSLDA).\nPOSLDA simultaneously uncovers short-range syntactic patterns (syntax) and\nlong-range semantic patterns (topics) that exist in document collections. This\nresults in word distributions that are specific to both topics (sports,\neducation, ...) and parts-of-speech (nouns, verbs, ...). For example,\nmultinomial distributions over words are uncovered that can be understood as\n\"nouns about weather\" or \"verbs about law\". We describe the model and an\napproximate inference algorithm and then demonstrate the quality of the learned\ntopics both qualitatively and quantitatively. Then, we discuss an NLP\napplication where the output of POSLDA can lead to strong improvements in\nquality: unsupervised part-of-speech tagging. We describe algorithms for this\ntask that make use of POSLDA-learned distributions that result in improved\nperformance beyond the state of the art. \n\n"}
{"id": "1303.5613", "contents": "Title: Network Detection Theory and Performance Abstract: Network detection is an important capability in many areas of applied\nresearch in which data can be represented as a graph of entities and\nrelationships. Oftentimes the object of interest is a relatively small subgraph\nin an enormous, potentially uninteresting background. This aspect characterizes\nnetwork detection as a \"big data\" problem. Graph partitioning and network\ndiscovery have been major research areas over the last ten years, driven by\ninterest in internet search, cyber security, social networks, and criminal or\nterrorist activities. The specific problem of network discovery is addressed as\na special case of graph partitioning in which membership in a small subgraph of\ninterest must be determined. Algebraic graph theory is used as the basis to\nanalyze and compare different network detection methods. A new Bayesian network\ndetection framework is introduced that partitions the graph based on prior\ninformation and direct observations. The new approach, called space-time threat\npropagation, is proved to maximize the probability of detection and is\ntherefore optimum in the Neyman-Pearson sense. This optimality criterion is\ncompared to spectral community detection approaches which divide the global\ngraph into subsets or communities with optimal connectivity properties. We also\nexplore a new generative stochastic model for covert networks and analyze using\nreceiver operating characteristics the detection performance of both classes of\noptimal detection techniques. \n\n"}
{"id": "1304.5974", "contents": "Title: Dynamic stochastic blockmodels: Statistical models for time-evolving\n  networks Abstract: Significant efforts have gone into the development of statistical models for\nanalyzing data in the form of networks, such as social networks. Most existing\nwork has focused on modeling static networks, which represent either a single\ntime snapshot or an aggregate view over time. There has been recent interest in\nstatistical modeling of dynamic networks, which are observed at multiple points\nin time and offer a richer representation of many complex phenomena. In this\npaper, we propose a state-space model for dynamic networks that extends the\nwell-known stochastic blockmodel for static networks to the dynamic setting. We\nthen propose a procedure to fit the model using a modification of the extended\nKalman filter augmented with a local search. We apply the procedure to analyze\na dynamic social network of email communication. \n\n"}
{"id": "1304.7942", "contents": "Title: ManTIME: Temporal expression identification and normalization in the\n  TempEval-3 challenge Abstract: This paper describes a temporal expression identification and normalization\nsystem, ManTIME, developed for the TempEval-3 challenge. The identification\nphase combines the use of conditional random fields along with a\npost-processing identification pipeline, whereas the normalization phase is\ncarried out using NorMA, an open-source rule-based temporal normalizer. We\ninvestigate the performance variation with respect to different feature types.\nSpecifically, we show that the use of WordNet-based features in the\nidentification task negatively affects the overall performance, and that there\nis no statistically significant difference in using gazetteers, shallow parsing\nand propositional noun phrases labels on top of the morphological features. On\nthe test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in the\nidentification phase. Normalization accuracies are 0.84 (type attribute) and\n0.77 (value attribute). Surprisingly, the use of the silver data (alone or in\naddition to the gold annotated ones) does not improve the performance. \n\n"}
{"id": "1306.6111", "contents": "Title: Understanding the Predictive Power of Computational Mechanics and Echo\n  State Networks in Social Media Abstract: There is a large amount of interest in understanding users of social media in\norder to predict their behavior in this space. Despite this interest, user\npredictability in social media is not well-understood. To examine this\nquestion, we consider a network of fifteen thousand users on Twitter over a\nseven week period. We apply two contrasting modeling paradigms: computational\nmechanics and echo state networks. Both methods attempt to model the behavior\nof users on the basis of their past behavior. We demonstrate that the behavior\nof users on Twitter can be well-modeled as processes with self-feedback. We\nfind that the two modeling approaches perform very similarly for most users,\nbut that they differ in performance on a small subset of the users. By\nexploring the properties of these performance-differentiated users, we\nhighlight the challenges faced in applying predictive models to dynamic social\ndata. \n\n"}
{"id": "1306.6259", "contents": "Title: Highlighting Entanglement of Cultures via Ranking of Multilingual\n  Wikipedia Articles Abstract: How different cultures evaluate a person? Is an important person in one\nculture is also important in the other culture? We address these questions via\nranking of multilingual Wikipedia articles. With three ranking algorithms based\non network structure of Wikipedia, we assign ranking to all articles in 9\nmultilingual editions of Wikipedia and investigate general ranking structure of\nPageRank, CheiRank and 2DRank. In particular, we focus on articles related to\npersons, identify top 30 persons for each rank among different editions and\nanalyze distinctions of their distributions over activity fields such as\npolitics, art, science, religion, sport for each edition. We find that local\nheroes are dominant but also global heroes exist and create an effective\nnetwork representing entanglement of cultures. The Google matrix analysis of\nnetwork of cultures shows signs of the Zipf law distribution. This approach\nallows to examine diversity and shared characteristics of knowledge\norganization between cultures. The developed computational, data driven\napproach highlights cultural interconnections in a new perspective. \n\n"}
{"id": "1307.6235", "contents": "Title: Graphical law beneath each written natural language Abstract: We study twenty four written natural languages. We draw in the log scale,\nnumber of words starting with a letter vs rank of the letter, both normalised.\nWe find that all the graphs are of the similar type. The graphs are\ntantalisingly closer to the curves of reduced magnetisation vs reduced\ntemperature for magnetic materials. We make a weak conjecture that a curve of\nmagnetisation underlies a written natural language. \n\n"}
{"id": "1307.7973", "contents": "Title: Connecting Language and Knowledge Bases with Embedding Models for\n  Relation Extraction Abstract: This paper proposes a novel approach for relation extraction from free text\nwhich is trained to jointly use information from the text and from existing\nknowledge. Our model is based on two scoring functions that operate by learning\nlow-dimensional embeddings of words and of entities and relationships from a\nknowledge base. We empirically show on New York Times articles aligned with\nFreebase relations that our approach is able to efficiently use the extra\ninformation provided by a large subset of Freebase data (4M entities, 23k\nrelationships) to improve over existing methods that rely on text features\nalone. \n\n"}
{"id": "1308.2853", "contents": "Title: When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor\n  Tucker Decompositions with Structured Sparsity Abstract: Overcomplete latent representations have been very popular for unsupervised\nfeature learning in recent years. In this paper, we specify which overcomplete\nmodels can be identified given observable moments of a certain order. We\nconsider probabilistic admixture or topic models in the overcomplete regime,\nwhere the number of latent topics can greatly exceed the size of the observed\nword vocabulary. While general overcomplete topic models are not identifiable,\nwe establish generic identifiability under a constraint, referred to as topic\npersistence. Our sufficient conditions for identifiability involve a novel set\nof \"higher order\" expansion conditions on the topic-word matrix or the\npopulation structure of the model. This set of higher-order expansion\nconditions allow for overcomplete models, and require the existence of a\nperfect matching from latent topics to higher order observed words. We\nestablish that random structured topic models are identifiable w.h.p. in the\novercomplete regime. Our identifiability results allows for general\n(non-degenerate) distributions for modeling the topic proportions, and thus, we\ncan handle arbitrarily correlated topics in our framework. Our identifiability\nresults imply uniqueness of a class of tensor decompositions with structured\nsparsity which is contained in the class of Tucker decompositions, but is more\ngeneral than the Candecomp/Parafac (CP) decomposition. \n\n"}
{"id": "1309.1501", "contents": "Title: Improvements to deep convolutional neural networks for LVCSR Abstract: Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural\nNetworks (DNN), as they are able to better reduce spectral variation in the\ninput signal. This has also been confirmed experimentally, with CNNs showing\nimprovements in word error rate (WER) between 4-12% relative compared to DNNs\nacross a variety of LVCSR tasks. In this paper, we describe different methods\nto further improve CNN performance. First, we conduct a deep analysis comparing\nlimited weight sharing and full weight sharing with state-of-the-art features.\nSecond, we apply various pooling strategies that have shown improvements in\ncomputer vision to an LVCSR speech task. Third, we introduce a method to\neffectively incorporate speaker adaptation, namely fMLLR, into log-mel\nfeatures. Fourth, we introduce an effective strategy to use dropout during\nHessian-free sequence training. We find that with these improvements,\nparticularly with fMLLR and dropout, we are able to achieve an additional 2-3%\nrelative improvement in WER on a 50-hour Broadcast News task over our previous\nbest CNN baseline. On a larger 400-hour BN task, we find an additional 4-5%\nrelative improvement over our previous best CNN baseline. \n\n"}
{"id": "1309.4058", "contents": "Title: Why SOV might be initially preferred and then lost or recovered? A\n  theoretical framework Abstract: Little is known about why SOV order is initially preferred and then discarded\nor recovered. Here we present a framework for understanding these and many\nrelated word order phenomena: the diversity of dominant orders, the existence\nof free words orders, the need of alternative word orders and word order\nreversions and cycles in evolution. Under that framework, word order is\nregarded as a multiconstraint satisfaction problem in which at least two\nconstraints are in conflict: online memory minimization and maximum\npredictability. \n\n"}
{"id": "1309.6707", "contents": "Title: Distributed Online Learning in Social Recommender Systems Abstract: In this paper, we consider decentralized sequential decision making in\ndistributed online recommender systems, where items are recommended to users\nbased on their search query as well as their specific background including\nhistory of bought items, gender and age, all of which comprise the context\ninformation of the user. In contrast to centralized recommender systems, in\nwhich there is a single centralized seller who has access to the complete\ninventory of items as well as the complete record of sales and user\ninformation, in decentralized recommender systems each seller/learner only has\naccess to the inventory of items and user information for its own products and\nnot the products and user information of other sellers, but can get commission\nif it sells an item of another seller. Therefore the sellers must distributedly\nfind out for an incoming user which items to recommend (from the set of own\nitems or items of another seller), in order to maximize the revenue from own\nsales and commissions. We formulate this problem as a cooperative contextual\nbandit problem, analytically bound the performance of the sellers compared to\nthe best recommendation strategy given the complete realization of user\narrivals and the inventory of items, as well as the context-dependent purchase\nprobabilities of each item, and verify our results via numerical examples on a\ndistributed data set adapted based on Amazon data. We evaluate the dependence\nof the performance of a seller on the inventory of items the seller has, the\nnumber of connections it has with the other sellers, and the commissions which\nthe seller gets by selling items of other sellers to its users. \n\n"}
{"id": "1310.2408", "contents": "Title: Improved Bayesian Logistic Supervised Topic Models with Data\n  Augmentation Abstract: Supervised topic models with a logistic likelihood have two issues that\npotentially limit their practical use: 1) response variables are usually\nover-weighted by document word counts; and 2) existing variational inference\nmethods make strict mean-field assumptions. We address these issues by: 1)\nintroducing a regularization constant to better balance the two parts based on\nan optimization formulation of Bayesian inference; and 2) developing a simple\nGibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and\ncollapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm\nhas analytical forms of each conditional distribution without making any\nrestricting assumptions and can be easily parallelized. Empirical results\ndemonstrate significant improvements on prediction performance and time\nefficiency. \n\n"}
{"id": "1310.2409", "contents": "Title: Discriminative Relational Topic Models Abstract: Many scientific and engineering fields involve analyzing network data. For\ndocument networks, relational topic models (RTMs) provide a probabilistic\ngenerative process to describe both the link structure and document contents,\nand they have shown promise on predicting network structures and discovering\nlatent topic representations. However, existing RTMs have limitations in both\nthe restricted model expressiveness and incapability of dealing with imbalanced\nnetwork data. To expand the scope and improve the inference accuracy of RTMs,\nthis paper presents three extensions: 1) unlike the common link likelihood with\na diagonal weight matrix that allows the-same-topic interactions only, we\ngeneralize it to use a full weight matrix that captures all pairwise topic\ninteractions and is applicable to asymmetric networks; 2) instead of doing\nstandard Bayesian inference, we perform regularized Bayesian inference\n(RegBayes) with a regularization parameter to deal with the imbalanced link\nstructure issue in common real networks and improve the discriminative ability\nof learned latent representations; and 3) instead of doing variational\napproximation with strict mean-field assumptions, we present collapsed Gibbs\nsampling algorithms for the generalized relational topic models by exploring\ndata augmentation without making restricting assumptions. Under the generic\nRegBayes framework, we carefully investigate two popular discriminative loss\nfunctions, namely, the logistic log-loss and the max-margin hinge loss.\nExperimental results on several real network datasets demonstrate the\nsignificance of these extensions on improving the prediction performance, and\nthe time efficiency can be dramatically improved with a simple fast\napproximation method. \n\n"}
{"id": "1311.1704", "contents": "Title: Scalable Recommendation with Poisson Factorization Abstract: We develop a Bayesian Poisson matrix factorization model for forming\nrecommendations from sparse user behavior data. These data are large user/item\nmatrices where each user has provided feedback on only a small subset of items,\neither explicitly (e.g., through star ratings) or implicitly (e.g., through\nviews or purchases). In contrast to traditional matrix factorization\napproaches, Poisson factorization implicitly models each user's limited\nattention to consume items. Moreover, because of the mathematical form of the\nPoisson likelihood, the model needs only to explicitly consider the observed\nentries in the matrix, leading to both scalable computation and good predictive\nperformance. We develop a variational inference algorithm for approximate\nposterior inference that scales up to massive data sets. This is an efficient\nalgorithm that iterates over the observed entries and adjusts an approximate\nposterior over the user/item representations. We apply our method to large\nreal-world user data containing users rating movies, users listening to songs,\nand users reading scientific papers. In all these settings, Bayesian Poisson\nfactorization outperforms state-of-the-art matrix factorization methods. \n\n"}
{"id": "1311.3064", "contents": "Title: Ranking users, papers and authors in online scientific communities Abstract: The ever-increasing quantity and complexity of scientific production have\nmade it difficult for researchers to keep track of advances in their own\nfields. This, together with growing popularity of online scientific\ncommunities, calls for the development of effective information filtering\ntools. We propose here a method to simultaneously compute reputation of users\nand quality of scientific artifacts in an online scientific community.\nEvaluation on artificially-generated data and real data from the Econophysics\nForum is used to determine the method's best-performing variants. We show that\nwhen the method is extended by considering author credit, its performance\nimproves on multiple levels. In particular, top papers have higher citation\ncount and top authors have higher $h$-index than top papers and top authors\nchosen by other algorithms. \n\n"}
{"id": "1311.5552", "contents": "Title: Bayesian Discovery of Threat Networks Abstract: A novel unified Bayesian framework for network detection is developed, under\nwhich a detection algorithm is derived based on random walks on graphs. The\nalgorithm detects threat networks using partial observations of their activity,\nand is proved to be optimum in the Neyman-Pearson sense. The algorithm is\ndefined by a graph, at least one observation, and a diffusion model for threat.\nA link to well-known spectral detection methods is provided, and the\nequivalence of the random walk and harmonic solutions to the Bayesian\nformulation is proven. A general diffusion model is introduced that utilizes\nspatio-temporal relationships between vertices, and is used for a specific\nspace-time formulation that leads to significant performance improvements on\ncoordinated covert networks. This performance is demonstrated using a new\nhybrid mixed-membership blockmodel introduced to simulate random covert\nnetworks with realistic properties. \n\n"}
{"id": "1312.0493", "contents": "Title: Bidirectional Recursive Neural Networks for Token-Level Labeling with\n  Structure Abstract: Recently, deep architectures, such as recurrent and recursive neural networks\nhave been successfully applied to various natural language processing tasks.\nInspired by bidirectional recurrent neural networks which use representations\nthat summarize the past and future around an instance, we propose a novel\narchitecture that aims to capture the structural information around an input,\nand use it to label instances. We apply our method to the task of opinion\nexpression extraction, where we employ the binary parse tree of a sentence as\nthe structure, and word vector representations as the initial representation of\na single token. We conduct preliminary experiments to investigate its\nperformance and compare it to the sequential approach. \n\n"}
{"id": "1403.1252", "contents": "Title: Inducing Language Networks from Continuous Space Word Representations Abstract: Recent advancements in unsupervised feature learning have developed powerful\nlatent representations of words. However, it is still not clear what makes one\nrepresentation better than another and how we can learn the ideal\nrepresentation. Understanding the structure of latent spaces attained is key to\nany future advancement in unsupervised learning. In this work, we introduce a\nnew view of continuous space word representations as language networks. We\nexplore two techniques to create language networks from learned features by\ninducing them for two popular word representation methods and examining the\nproperties of their resulting networks. We find that the induced networks\ndiffer from other methods of creating language networks, and that they contain\nmeaningful community structure. \n\n"}
{"id": "1403.3460", "contents": "Title: Scalable and Robust Construction of Topical Hierarchies Abstract: Automated generation of high-quality topical hierarchies for a text\ncollection is a dream problem in knowledge engineering with many valuable\napplications. In this paper a scalable and robust algorithm is proposed for\nconstructing a hierarchy of topics from a text collection. We divide and\nconquer the problem using a top-down recursive framework, based on a tensor\northogonal decomposition technique. We solve a critical challenge to perform\nscalable inference for our newly designed hierarchical topic model. Experiments\nwith various real-world datasets illustrate its ability to generate robust,\nhigh-quality hierarchies efficiently. Our method reduces the time of\nconstruction by several orders of magnitude, and its robust feature renders it\npossible for users to interactively revise the hierarchy. \n\n"}
{"id": "1403.6652", "contents": "Title: DeepWalk: Online Learning of Social Representations Abstract: We present DeepWalk, a novel approach for learning latent representations of\nvertices in a network. These latent representations encode social relations in\na continuous vector space, which is easily exploited by statistical models.\nDeepWalk generalizes recent advancements in language modeling and unsupervised\nfeature learning (or deep learning) from sequences of words to graphs. DeepWalk\nuses local information obtained from truncated random walks to learn latent\nrepresentations by treating walks as the equivalent of sentences. We\ndemonstrate DeepWalk's latent representations on several multi-label network\nclassification tasks for social networks such as BlogCatalog, Flickr, and\nYouTube. Our results show that DeepWalk outperforms challenging baselines which\nare allowed a global view of the network, especially in the presence of missing\ninformation. DeepWalk's representations can provide $F_1$ scores up to 10%\nhigher than competing methods when labeled data is sparse. In some experiments,\nDeepWalk's representations are able to outperform all baseline methods while\nusing 60% less training data. DeepWalk is also scalable. It is an online\nlearning algorithm which builds useful incremental results, and is trivially\nparallelizable. These qualities make it suitable for a broad class of real\nworld applications such as network classification, and anomaly detection. \n\n"}
{"id": "1406.1078", "contents": "Title: Learning Phrase Representations using RNN Encoder-Decoder for\n  Statistical Machine Translation Abstract: In this paper, we propose a novel neural network model called RNN\nEncoder-Decoder that consists of two recurrent neural networks (RNN). One RNN\nencodes a sequence of symbols into a fixed-length vector representation, and\nthe other decodes the representation into another sequence of symbols. The\nencoder and decoder of the proposed model are jointly trained to maximize the\nconditional probability of a target sequence given a source sequence. The\nperformance of a statistical machine translation system is empirically found to\nimprove by using the conditional probabilities of phrase pairs computed by the\nRNN Encoder-Decoder as an additional feature in the existing log-linear model.\nQualitatively, we show that the proposed model learns a semantically and\nsyntactically meaningful representation of linguistic phrases. \n\n"}
{"id": "1406.2022", "contents": "Title: Two-dimensional Sentiment Analysis of text Abstract: Sentiment Analysis aims to get the underlying viewpoint of the text, which\ncould be anything that holds a subjective opinion, such as an online review,\nMovie rating, Comments on Blog posts etc. This paper presents a novel approach\nthat classify text in two-dimensional Emotional space, based on the sentiments\nof the author. The approach uses existing lexical resources to extract feature\nset, which is trained using Supervised Learning techniques. \n\n"}
{"id": "1406.6312", "contents": "Title: Scalable Topical Phrase Mining from Text Corpora Abstract: While most topic modeling algorithms model text corpora with unigrams, human\ninterpretation often relies on inherent grouping of terms into phrases. As\nsuch, we consider the problem of discovering topical phrases of mixed lengths.\nExisting work either performs post processing to the inference results of\nunigram-based topic models, or utilizes complex n-gram-discovery topic models.\nThese methods generally produce low-quality topical phrases or suffer from poor\nscalability on even moderately-sized datasets. We propose a different approach\nthat is both computationally efficient and effective. Our solution combines a\nnovel phrase mining framework to segment a document into single and multi-word\nphrases, and a new topic model that operates on the induced document partition.\nOur approach discovers high quality topical phrases with negligible extra cost\nto the bag-of-words topic model in a variety of datasets including research\npublication titles, abstracts, reviews, and news articles. \n\n"}
{"id": "1407.0623", "contents": "Title: A Data-Driven Approach for Tag Refinement and Localization in Web Videos Abstract: Tagging of visual content is becoming more and more widespread as web-based\nservices and social networks have popularized tagging functionalities among\ntheir users. These user-generated tags are used to ease browsing and\nexploration of media collections, e.g. using tag clouds, or to retrieve\nmultimedia content. However, not all media are equally tagged by users. Using\nthe current systems is easy to tag a single photo, and even tagging a part of a\nphoto, like a face, has become common in sites like Flickr and Facebook. On the\nother hand, tagging a video sequence is more complicated and time consuming, so\nthat users just tag the overall content of a video. In this paper we present a\nmethod for automatic video annotation that increases the number of tags\noriginally provided by users, and localizes them temporally, associating tags\nto keyframes. Our approach exploits collective knowledge embedded in\nuser-generated tags and web sources, and visual similarity of keyframes and\nimages uploaded to social sites like YouTube and Flickr, as well as web sources\nlike Google and Bing. Given a keyframe, our method is able to select on the fly\nfrom these visual sources the training exemplars that should be the most\nrelevant for this test sample, and proceeds to transfer labels across similar\nimages. Compared to existing video tagging approaches that require training\nclassifiers for each tag, our system has few parameters, is easy to implement\nand can deal with an open vocabulary scenario. We demonstrate the approach on\ntag refinement and localization on DUT-WEBV, a large dataset of web videos, and\nshow state-of-the-art results. \n\n"}
{"id": "1407.1687", "contents": "Title: KNET: A General Framework for Learning Word Embedding using\n  Morphological Knowledge Abstract: Neural network techniques are widely applied to obtain high-quality\ndistributed representations of words, i.e., word embeddings, to address text\nmining, information retrieval, and natural language processing tasks. Recently,\nefficient methods have been proposed to learn word embeddings from context that\ncaptures both semantic and syntactic relationships between words. However, it\nis challenging to handle unseen words or rare words with insufficient context.\nIn this paper, inspired by the study on word recognition process in cognitive\npsychology, we propose to take advantage of seemingly less obvious but\nessentially important morphological knowledge to address these challenges. In\nparticular, we introduce a novel neural network architecture called KNET that\nleverages both contextual information and morphological word similarity built\nbased on morphological knowledge to learn word embeddings. Meanwhile, the\nlearning architecture is also able to refine the pre-defined morphological\nknowledge and obtain more accurate word similarity. Experiments on an\nanalogical reasoning task and a word similarity task both demonstrate that the\nproposed KNET framework can greatly enhance the effectiveness of word\nembeddings. \n\n"}
{"id": "1408.2927", "contents": "Title: Hashing for Similarity Search: A Survey Abstract: Similarity search (nearest neighbor search) is a problem of pursuing the data\nitems whose distances to a query item are the smallest from a large database.\nVarious methods have been developed to address this problem, and recently a lot\nof efforts have been devoted to approximate search. In this paper, we present a\nsurvey on one of the main solutions, hashing, which has been widely studied\nsince the pioneering work locality sensitive hashing. We divide the hashing\nalgorithms two main categories: locality sensitive hashing, which designs hash\nfunctions without exploring the data distribution and learning to hash, which\nlearns hash functions according the data distribution, and review them from\nvarious aspects, including hash function design and distance measure and search\nscheme in the hash coding space. \n\n"}
{"id": "1409.1259", "contents": "Title: On the Properties of Neural Machine Translation: Encoder-Decoder\n  Approaches Abstract: Neural machine translation is a relatively new approach to statistical\nmachine translation based purely on neural networks. The neural machine\ntranslation models often consist of an encoder and a decoder. The encoder\nextracts a fixed-length representation from a variable-length input sentence,\nand the decoder generates a correct translation from this representation. In\nthis paper, we focus on analyzing the properties of the neural machine\ntranslation using two models; RNN Encoder--Decoder and a newly proposed gated\nrecursive convolutional neural network. We show that the neural machine\ntranslation performs relatively well on short sentences without unknown words,\nbut its performance degrades rapidly as the length of the sentence and the\nnumber of unknown words increase. Furthermore, we find that the proposed gated\nrecursive convolutional network learns a grammatical structure of a sentence\nautomatically. \n\n"}
{"id": "1409.1461", "contents": "Title: On the Accuracy of Hyper-local Geotagging of Social Media Content Abstract: Social media users share billions of items per year, only a small fraction of\nwhich is geotagged. We present a data- driven approach for identifying\nnon-geotagged content items that can be associated with a hyper-local\ngeographic area by modeling the location distributions of hyper-local n-grams\nthat appear in the text. We explore the trade-off between accuracy, precision\nand coverage of this method. Further, we explore differences across content\nreceived from multiple platforms and devices, and show, for example, that\ncontent shared via different sources and applications produces significantly\ndifferent geographic distributions, and that it is best to model and predict\nlocation for items according to their source. Our findings show the potential\nand the bounds of a data-driven approach to geotag short social media texts,\nand offer implications for all applications that use data-driven approaches to\nlocate content. \n\n"}
{"id": "1409.2195", "contents": "Title: Analyzing the Language of Food on Social Media Abstract: We investigate the predictive power behind the language of food on social\nmedia. We collect a corpus of over three million food-related posts from\nTwitter and demonstrate that many latent population characteristics can be\ndirectly predicted from this data: overweight rate, diabetes rate, political\nleaning, and home geographical location of authors. For all tasks, our\nlanguage-based models significantly outperform the majority-class baselines.\nPerformance is further improved with more complex natural language processing,\nsuch as topic modeling. We analyze which textual features have most predictive\npower for these datasets, providing insight into the connections between the\nlanguage of food, geographic locale, and community characteristics. Lastly, we\ndesign and implement an online system for real-time query and visualization of\nthe dataset. Visualization tools, such as geo-referenced heatmaps,\nsemantics-preserving wordclouds and temporal histograms, allow us to discover\nmore complex, global patterns mirrored in the language of food. \n\n"}
{"id": "1409.2944", "contents": "Title: Collaborative Deep Learning for Recommender Systems Abstract: Collaborative filtering (CF) is a successful approach commonly used by many\nrecommender systems. Conventional CF-based methods use the ratings given to\nitems by users as the sole source of information for learning to make\nrecommendation. However, the ratings are often very sparse in many\napplications, causing CF-based methods to degrade significantly in their\nrecommendation performance. To address this sparsity problem, auxiliary\ninformation such as item content information may be utilized. Collaborative\ntopic regression (CTR) is an appealing recent method taking this approach which\ntightly couples the two components that learn from two different sources of\ninformation. Nevertheless, the latent representation learned by CTR may not be\nvery effective when the auxiliary information is very sparse. To address this\nproblem, we generalize recent advances in deep learning from i.i.d. input to\nnon-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian\nmodel called collaborative deep learning (CDL), which jointly performs deep\nrepresentation learning for the content information and collaborative filtering\nfor the ratings (feedback) matrix. Extensive experiments on three real-world\ndatasets from different domains show that CDL can significantly advance the\nstate of the art. \n\n"}
{"id": "1409.2993", "contents": "Title: \"Look Ma, No Hands!\" A Parameter-Free Topic Model Abstract: It has always been a burden to the users of statistical topic models to\npredetermine the right number of topics, which is a key parameter of most topic\nmodels. Conventionally, automatic selection of this parameter is done through\neither statistical model selection (e.g., cross-validation, AIC, or BIC) or\nBayesian nonparametric models (e.g., hierarchical Dirichlet process). These\nmethods either rely on repeated runs of the inference algorithm to search\nthrough a large range of parameter values which does not suit the mining of big\ndata, or replace this parameter with alternative parameters that are less\nintuitive and still hard to be determined. In this paper, we explore to\n\"eliminate\" this parameter from a new perspective. We first present a\nnonparametric treatment of the PLSA model named nonparametric probabilistic\nlatent semantic analysis (nPLSA). The inference procedure of nPLSA allows for\nthe exploration and comparison of different numbers of topics within a single\nexecution, yet remains as simple as that of PLSA. This is achieved by\nsubstituting the parameter of the number of topics with an alternative\nparameter that is the minimal goodness of fit of a document. We show that the\nnew parameter can be further eliminated by two parameter-free treatments:\neither by monitoring the diversity among the discovered topics or by a weak\nsupervision from users in the form of an exemplar topic. The parameter-free\ntopic model finds the appropriate number of topics when the diversity among the\ndiscovered topics is maximized, or when the granularity of the discovered\ntopics matches the exemplar topic. Experiments on both synthetic and real data\nprove that the parameter-free topic model extracts topics with a comparable\nquality comparing to classical topic models with \"manual transmission\". The\nquality of the topics outperforms those extracted through classical Bayesian\nnonparametric models. \n\n"}
{"id": "1410.2455", "contents": "Title: BilBOWA: Fast Bilingual Distributed Representations without Word\n  Alignments Abstract: We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple\nand computationally-efficient model for learning bilingual distributed\nrepresentations of words which can scale to large monolingual datasets and does\nnot require word-aligned parallel training data. Instead it trains directly on\nmonolingual data and extracts a bilingual signal from a smaller set of raw-text\nsentence-aligned data. This is achieved using a novel sampled bag-of-words\ncross-lingual objective, which is used to regularize two noise-contrastive\nlanguage models for efficient cross-lingual feature learning. We show that\nbilingual embeddings learned using the proposed model outperform\nstate-of-the-art methods on a cross-lingual document classification task as\nwell as a lexical translation task on WMT11 data. \n\n"}
{"id": "1410.3916", "contents": "Title: Memory Networks Abstract: We describe a new class of learning models called memory networks. Memory\nnetworks reason with inference components combined with a long-term memory\ncomponent; they learn how to use these jointly. The long-term memory can be\nread and written to, with the goal of using it for prediction. We investigate\nthese models in the context of question answering (QA) where the long-term\nmemory effectively acts as a (dynamic) knowledge base, and the output is a\ntextual response. We evaluate them on a large-scale QA task, and a smaller, but\nmore complex, toy task generated from a simulated world. In the latter, we show\nthe reasoning power of such models by chaining multiple supporting sentences to\nanswer questions that require understanding the intension of verbs. \n\n"}
{"id": "1410.4355", "contents": "Title: Multi-Level Anomaly Detection on Time-Varying Graph Data Abstract: This work presents a novel modeling and analysis framework for graph\nsequences which addresses the challenge of detecting and contextualizing\nanomalies in labelled, streaming graph data. We introduce a generalization of\nthe BTER model of Seshadhri et al. by adding flexibility to community\nstructure, and use this model to perform multi-scale graph anomaly detection.\nSpecifically, probability models describing coarse subgraphs are built by\naggregating probabilities at finer levels, and these closely related\nhierarchical models simultaneously detect deviations from expectation. This\ntechnique provides insight into a graph's structure and internal context that\nmay shed light on a detected event. Additionally, this multi-scale analysis\nfacilitates intuitive visualizations by allowing users to narrow focus from an\nanomalous graph to particular subgraphs or nodes causing the anomaly.\n  For evaluation, two hierarchical anomaly detectors are tested against a\nbaseline Gaussian method on a series of sampled graphs. We demonstrate that our\ngraph statistics-based approach outperforms both a distribution-based detector\nand the baseline in a labeled setting with community structure, and it\naccurately detects anomalies in synthetic and real-world datasets at the node,\nsubgraph, and graph levels. To illustrate the accessibility of information made\npossible via this technique, the anomaly detector and an associated interactive\nvisualization tool are tested on NCAA football data, where teams and\nconferences that moved within the league are identified with perfect recall,\nand precision greater than 0.786. \n\n"}
{"id": "1410.5401", "contents": "Title: Neural Turing Machines Abstract: We extend the capabilities of neural networks by coupling them to external\nmemory resources, which they can interact with by attentional processes. The\ncombined system is analogous to a Turing Machine or Von Neumann architecture\nbut is differentiable end-to-end, allowing it to be efficiently trained with\ngradient descent. Preliminary results demonstrate that Neural Turing Machines\ncan infer simple algorithms such as copying, sorting, and associative recall\nfrom input and output examples. \n\n"}
{"id": "1410.5485", "contents": "Title: A stronger null hypothesis for crossing dependencies Abstract: The syntactic structure of a sentence can be modeled as a tree where vertices\nare words and edges indicate syntactic dependencies between words. It is\nwell-known that those edges normally do not cross when drawn over the sentence.\nHere a new null hypothesis for the number of edge crossings of a sentence is\npresented. That null hypothesis takes into account the length of the pair of\nedges that may cross and predicts the relative number of crossings in random\ntrees with a small error, suggesting that a ban of crossings or a principle of\nminimization of crossings are not needed in general to explain the origins of\nnon-crossing dependencies. Our work paves the way for more powerful null\nhypotheses to investigate the origins of non-crossing dependencies in nature. \n\n"}
{"id": "1411.2539", "contents": "Title: Unifying Visual-Semantic Embeddings with Multimodal Neural Language\n  Models Abstract: Inspired by recent advances in multimodal learning and machine translation,\nwe introduce an encoder-decoder pipeline that learns (a): a multimodal joint\nembedding space with images and text and (b): a novel language model for\ndecoding distributed representations from our space. Our pipeline effectively\nunifies joint image-text embedding models with multimodal neural language\nmodels. We introduce the structure-content neural language model that\ndisentangles the structure of a sentence to its content, conditioned on\nrepresentations produced by the encoder. The encoder allows one to rank images\nand sentences while the decoder can generate novel descriptions from scratch.\nUsing LSTM to encode sentences, we match the state-of-the-art performance on\nFlickr8K and Flickr30K without using object detections. We also set new best\nresults when using the 19-layer Oxford convolutional network. Furthermore we\nshow that with linear encoders, the learned embedding space captures multimodal\nregularities in terms of vector space arithmetic e.g. *image of a blue car* -\n\"blue\" + \"red\" is near images of red cars. Sample captions generated for 800\nimages are made available for comparison. \n\n"}
{"id": "1411.2645", "contents": "Title: Non-crossing dependencies: least effort, not grammar Abstract: The use of null hypotheses (in a statistical sense) is common in hard\nsciences but not in theoretical linguistics. Here the null hypothesis that the\nlow frequency of syntactic dependency crossings is expected by an arbitrary\nordering of words is rejected. It is shown that this would require star\ndependency structures, which are both unrealistic and too restrictive. The\nhypothesis of the limited resources of the human brain is revisited. Stronger\nnull hypotheses taking into account actual dependency lengths for the\nlikelihood of crossings are presented. Those hypotheses suggests that crossings\nare likely to reduce when dependencies are shortened. A hypothesis based on\npressure to reduce dependency lengths is more parsimonious than a principle of\nminimization of crossings or a grammatical ban that is totally dissociated from\nthe general and non-linguistic principle of economy. \n\n"}
{"id": "1411.3315", "contents": "Title: Statistically Significant Detection of Linguistic Change Abstract: We propose a new computational approach for tracking and detecting\nstatistically significant linguistic shifts in the meaning and usage of words.\nSuch linguistic shifts are especially prevalent on the Internet, where the\nrapid exchange of ideas can quickly change a word's meaning. Our meta-analysis\napproach constructs property time series of word usage, and then uses\nstatistically sound change point detection algorithms to identify significant\nlinguistic shifts.\n  We consider and analyze three approaches of increasing complexity to generate\nsuch linguistic property time series, the culmination of which uses\ndistributional characteristics inferred from word co-occurrences. Using\nrecently proposed deep neural language models, we first train vector\nrepresentations of words for each time period. Second, we warp the vector\nspaces into one unified coordinate system. Finally, we construct a\ndistance-based distributional time series for each word to track it's\nlinguistic displacement over time.\n  We demonstrate that our approach is scalable by tracking linguistic change\nacross years of micro-blogging using Twitter, a decade of product reviews using\na corpus of movie reviews from Amazon, and a century of written books using the\nGoogle Book-ngrams. Our analysis reveals interesting patterns of language usage\nchange commensurate with each medium. \n\n"}
{"id": "1411.3827", "contents": "Title: Autonomization of Monoidal Categories Abstract: We show that contrary to common belief in the DisCoCat community, a monoidal\ncategory is all that is needed to define a categorical compositional model of\nnatural language. This relies on a construction which freely adds adjoints to a\nmonoidal category. In the case of distributional semantics, this broadens the\nrange of available models, to include non-linear maps and cartesian products\nfor instance. We illustrate the applications of this principle to various\ndistributional models of meaning. \n\n"}
{"id": "1411.5654", "contents": "Title: Learning a Recurrent Visual Representation for Image Caption Generation Abstract: In this paper we explore the bi-directional mapping between images and their\nsentence-based descriptions. We propose learning this mapping using a recurrent\nneural network. Unlike previous approaches that map both sentences and images\nto a common embedding, we enable the generation of novel sentences given an\nimage. Using the same model, we can also reconstruct the visual features\nassociated with an image given its visual description. We use a novel recurrent\nvisual memory that automatically learns to remember long-term visual concepts\nto aid in both sentence generation and visual feature reconstruction. We\nevaluate our approach on several tasks. These include sentence generation,\nsentence retrieval and image retrieval. State-of-the-art results are shown for\nthe task of generating novel image descriptions. When compared to human\ngenerated captions, our automatically generated captions are preferred by\nhumans over $19.8\\%$ of the time. Results are better than or comparable to\nstate-of-the-art results on the image and sentence retrieval tasks for methods\nusing similar visual features. \n\n"}
{"id": "1411.5988", "contents": "Title: Clustering evolving data using kernel-based methods Abstract: In this thesis, we propose several modelling strategies to tackle evolving\ndata in different contexts. In the framework of static clustering, we start by\nintroducing a soft kernel spectral clustering (SKSC) algorithm, which can\nbetter deal with overlapping clusters with respect to kernel spectral\nclustering (KSC) and provides more interpretable outcomes. Afterwards, a whole\nstrategy based upon KSC for community detection of static networks is proposed,\nwhere the extraction of a high quality training sub-graph, the choice of the\nkernel function, the model selection and the applicability to large-scale data\nare key aspects. This paves the way for the development of a novel clustering\nalgorithm for the analysis of evolving networks called kernel spectral\nclustering with memory effect (MKSC), where the temporal smoothness between\nclustering results in successive time steps is incorporated at the level of the\nprimal optimization problem, by properly modifying the KSC formulation. Later\non, an application of KSC to fault detection of an industrial machine is\npresented. Here, a smart pre-processing of the data by means of a proper\nwindowing operation is necessary to catch the ongoing degradation process\naffecting the machine. In this way, in a genuinely unsupervised manner, it is\npossible to raise an early warning when necessary, in an online fashion.\nFinally, we propose a new algorithm called incremental kernel spectral\nclustering (IKSC) for online learning of non-stationary data. This ambitious\nchallenge is faced by taking advantage of the out-of-sample property of kernel\nspectral clustering (KSC) to adapt the initial model, in order to tackle\nmerging, splitting or drifting of clusters across time. Real-world applications\nconsidered in this thesis include image segmentation, time-series clustering,\ncommunity detection of static and evolving networks. \n\n"}
{"id": "1412.0007", "contents": "Title: Paradigm shifts. Part I. Collagen. Confirming and complementing the work\n  of Henry Small Abstract: The paradigm shift in collagen research during the early 1970s marked by the\ndiscovery of the collagen precursor molecule procollagen was traced using\nco-citation analysis and title word frequency determination, confirming\nprevious work performed by Henry Small. \n\n"}
{"id": "1412.1058", "contents": "Title: Effective Use of Word Order for Text Categorization with Convolutional\n  Neural Networks Abstract: Convolutional neural network (CNN) is a neural network that can make use of\nthe internal structure of data such as the 2D structure of image data. This\npaper studies CNN on text categorization to exploit the 1D structure (namely,\nword order) of text data for accurate prediction. Instead of using\nlow-dimensional word vectors as input as is often done, we directly apply CNN\nto high-dimensional text data, which leads to directly learning embedding of\nsmall text regions for use in classification. In addition to a straightforward\nadaptation of CNN from image to text, a simple but new variation which employs\nbag-of-word conversion in the convolution layer is proposed. An extension to\ncombine multiple convolution layers is also explored for higher accuracy. The\nexperiments demonstrate the effectiveness of our approach in comparison with\nstate-of-the-art methods. \n\n"}
{"id": "1412.2812", "contents": "Title: Unsupervised Induction of Semantic Roles within a Reconstruction-Error\n  Minimization Framework Abstract: We introduce a new approach to unsupervised estimation of feature-rich\nsemantic role labeling models. Our model consists of two components: (1) an\nencoding component: a semantic role labeling model which predicts roles given a\nrich set of syntactic and lexical features; (2) a reconstruction component: a\ntensor factorization model which relies on roles to predict argument fillers.\nWhen the components are estimated jointly to minimize errors in argument\nreconstruction, the induced roles largely correspond to roles defined in\nannotated resources. Our method performs on par with most accurate role\ninduction methods on English and German, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguages. \n\n"}
{"id": "1412.4986", "contents": "Title: A Scalable Asynchronous Distributed Algorithm for Topic Modeling Abstract: Learning meaningful topic models with massive document collections which\ncontain millions of documents and billions of tokens is challenging because of\ntwo reasons: First, one needs to deal with a large number of topics (typically\nin the order of thousands). Second, one needs a scalable and efficient way of\ndistributing the computation across multiple machines. In this paper we present\na novel algorithm F+Nomad LDA which simultaneously tackles both these problems.\nIn order to handle large number of topics we use an appropriately modified\nFenwick tree. This data structure allows us to sample from a multinomial\ndistribution over $T$ items in $O(\\log T)$ time. Moreover, when topic counts\nchange the data structure can be updated in $O(\\log T)$ time. In order to\ndistribute the computation across multiple processor we present a novel\nasynchronous framework inspired by the Nomad algorithm of\n\\cite{YunYuHsietal13}. We show that F+Nomad LDA significantly outperform\nstate-of-the-art on massive problems which involve millions of documents,\nbillions of words, and thousands of topics. \n\n"}
{"id": "1412.5335", "contents": "Title: Ensemble of Generative and Discriminative Techniques for Sentiment\n  Analysis of Movie Reviews Abstract: Sentiment analysis is a common task in natural language processing that aims\nto detect polarity of a text document (typically a consumer review). In the\nsimplest settings, we discriminate only between positive and negative\nsentiment, turning the task into a standard binary classification problem. We\ncompare several ma- chine learning approaches to this problem, and combine them\nto achieve the best possible results. We show how to use for this task the\nstandard generative lan- guage models, which are slightly complementary to the\nstate of the art techniques. We achieve strong results on a well-known dataset\nof IMDB movie reviews. Our results are easily reproducible, as we publish also\nthe code needed to repeat the experiments. This should simplify further advance\nof the state of the art, as other researchers can combine their techniques with\nours with little effort. \n\n"}
{"id": "1412.6577", "contents": "Title: Modeling Compositionality with Multiplicative Recurrent Neural Networks Abstract: We present the multiplicative recurrent neural network as a general model for\ncompositional meaning in language, and evaluate it on the task of fine-grained\nsentiment analysis. We establish a connection to the previously investigated\nmatrix-space models for compositionality, and show they are special cases of\nthe multiplicative recurrent net. Our experiments show that these models\nperform comparably or better than Elman-type additive recurrent neural networks\nand outperform matrix-space models on a standard fine-grained sentiment\nanalysis corpus. Furthermore, they yield comparable results to structural deep\nmodels on the recently published Stanford Sentiment Treebank without the need\nfor generating parse trees. \n\n"}
{"id": "1412.6623", "contents": "Title: Word Representations via Gaussian Embedding Abstract: Current work in lexical distributed representations maps each word to a point\nvector in low-dimensional space. Mapping instead to a density provides many\ninteresting advantages, including better capturing uncertainty about a\nrepresentation and its relationships, expressing asymmetries more naturally\nthan dot product or cosine similarity, and enabling more expressive\nparameterization of decision boundaries. This paper advocates for density-based\ndistributed embeddings and presents a method for learning representations in\nthe space of Gaussian distributions. We compare performance on various word\nembedding benchmarks, investigate the ability of these embeddings to model\nentailment and other asymmetric relationships, and explore novel properties of\nthe representation. \n\n"}
{"id": "1412.6815", "contents": "Title: Extraction of Salient Sentences from Labelled Documents Abstract: We present a hierarchical convolutional document model with an architecture\ndesigned to support introspection of the document structure. Using this model,\nwe show how to use visualisation techniques from the computer vision literature\nto identify and extract topic-relevant sentences.\n  We also introduce a new scalable evaluation technique for automatic sentence\nextraction systems that avoids the need for time consuming human annotation of\nvalidation data. \n\n"}
{"id": "1412.6980", "contents": "Title: Adam: A Method for Stochastic Optimization Abstract: We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm. \n\n"}
{"id": "1412.8439", "contents": "Title: Spy vs. Spy: Rumor Source Obfuscation Abstract: Anonymous messaging platforms, such as Secret and Whisper, have emerged as\nimportant social media for sharing one's thoughts without the fear of being\njudged by friends, family, or the public. Further, such anonymous platforms are\ncrucial in nations with authoritarian governments; the right to free expression\nand sometimes the personal safety of the author of the message depend on\nanonymity. Whether for fear of judgment or personal endangerment, it is crucial\nto keep anonymous the identity of the user who initially posted a sensitive\nmessage. In this paper, we consider an adversary who observes a snapshot of the\nspread of a message at a certain time. Recent advances in rumor source\ndetection shows that the existing messaging protocols are vulnerable against\nsuch an adversary. We introduce a novel messaging protocol, which we call\nadaptive diffusion, and show that it spreads the messages fast and achieves a\nperfect obfuscation of the source when the underlying contact network is an\ninfinite regular tree: all users with the message are nearly equally likely to\nhave been the origin of the message. Experiments on a sampled Facebook network\nshow that it effectively hides the location of the source even when the graph\nis finite, irregular and has cycles. We further consider a stronger adversarial\nmodel where a subset of colluding users track the reception of messages. We\nshow that the adaptive diffusion provides a strong protection of the anonymity\nof the source even under this scenario. \n\n"}
{"id": "1502.02367", "contents": "Title: Gated Feedback Recurrent Neural Networks Abstract: In this work, we propose a novel recurrent neural network (RNN) architecture.\nThe proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of\nstacking multiple recurrent layers by allowing and controlling signals flowing\nfrom upper recurrent layers to lower layers using a global gating unit for each\npair of layers. The recurrent signals exchanged between layers are gated\nadaptively based on the previous hidden states and the current input. We\nevaluated the proposed GF-RNN with different types of recurrent units, such as\ntanh, long short-term memory and gated recurrent units, on the tasks of\ncharacter-level language modeling and Python program evaluation. Our empirical\nevaluation of different RNN units, revealed that in both tasks, the GF-RNN\noutperforms the conventional approaches to build deep stacked RNNs. We suggest\nthat the improvement arises because the GF-RNN can adaptively assign different\nlayers to different timescales and layer-to-layer interactions (including the\ntop-down ones which are not usually present in a stacked RNN) by learning to\ngate these interactions. \n\n"}
{"id": "1502.03167", "contents": "Title: Batch Normalization: Accelerating Deep Network Training by Reducing\n  Internal Covariate Shift Abstract: Training Deep Neural Networks is complicated by the fact that the\ndistribution of each layer's inputs changes during training, as the parameters\nof the previous layers change. This slows down the training by requiring lower\nlearning rates and careful parameter initialization, and makes it notoriously\nhard to train models with saturating nonlinearities. We refer to this\nphenomenon as internal covariate shift, and address the problem by normalizing\nlayer inputs. Our method draws its strength from making normalization a part of\nthe model architecture and performing the normalization for each training\nmini-batch. Batch Normalization allows us to use much higher learning rates and\nbe less careful about initialization. It also acts as a regularizer, in some\ncases eliminating the need for Dropout. Applied to a state-of-the-art image\nclassification model, Batch Normalization achieves the same accuracy with 14\ntimes fewer training steps, and beats the original model by a significant\nmargin. Using an ensemble of batch-normalized networks, we improve upon the\nbest published result on ImageNet classification: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the accuracy of human raters. \n\n"}
{"id": "1502.03630", "contents": "Title: Ordering-sensitive and Semantic-aware Topic Modeling Abstract: Topic modeling of textual corpora is an important and challenging problem. In\nmost previous work, the \"bag-of-words\" assumption is usually made which ignores\nthe ordering of words. This assumption simplifies the computation, but it\nunrealistically loses the ordering information and the semantic of words in the\ncontext. In this paper, we present a Gaussian Mixture Neural Topic Model\n(GMNTM) which incorporates both the ordering of words and the semantic meaning\nof sentences into topic modeling. Specifically, we represent each topic as a\ncluster of multi-dimensional vectors and embed the corpus into a collection of\nvectors generated by the Gaussian mixture model. Each word is affected not only\nby its topic, but also by the embedding vector of its surrounding words and the\ncontext. The Gaussian mixture components and the topic of documents, sentences\nand words can be learnt jointly. Extensive experiments show that our model can\nlearn better topics and more accurate word distributions for each topic.\nQuantitatively, comparing to state-of-the-art topic modeling approaches, GMNTM\nobtains significantly better performance in terms of perplexity, retrieval\naccuracy and classification accuracy. \n\n"}
{"id": "1502.04623", "contents": "Title: DRAW: A Recurrent Neural Network For Image Generation Abstract: This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural\nnetwork architecture for image generation. DRAW networks combine a novel\nspatial attention mechanism that mimics the foveation of the human eye, with a\nsequential variational auto-encoding framework that allows for the iterative\nconstruction of complex images. The system substantially improves on the state\nof the art for generative models on MNIST, and, when trained on the Street View\nHouse Numbers dataset, it generates images that cannot be distinguished from\nreal data with the naked eye. \n\n"}
{"id": "1502.06922", "contents": "Title: Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis\n  and Application to Information Retrieval Abstract: This paper develops a model that addresses sentence embedding, a hot topic in\ncurrent natural language processing research, using recurrent neural networks\nwith Long Short-Term Memory (LSTM) cells. Due to its ability to capture long\nterm memory, the LSTM-RNN accumulates increasingly richer information as it\ngoes through the sentence, and when it reaches the last word, the hidden layer\nof the network provides a semantic representation of the whole sentence. In\nthis paper, the LSTM-RNN is trained in a weakly supervised manner on user\nclick-through data logged by a commercial web search engine. Visualization and\nanalysis are performed to understand how the embedding process works. The model\nis found to automatically attenuate the unimportant words and detects the\nsalient keywords in the sentence. Furthermore, these detected keywords are\nfound to automatically activate different cells of the LSTM-RNN, where words\nbelonging to a similar topic activate the same cell. As a semantic\nrepresentation of the sentence, the embedding vector can be used in many\ndifferent applications. These automatic keyword detection and topic allocation\nabilities enabled by the LSTM-RNN allow the network to perform document\nretrieval, a difficult language processing task, where the similarity between\nthe query and documents can be measured by the distance between their\ncorresponding sentence embedding vectors computed by the LSTM-RNN. On a web\nsearch task, the LSTM-RNN embedding is shown to significantly outperform\nseveral existing state of the art methods. We emphasize that the proposed model\ngenerates sentence embedding vectors that are specially useful for web document\nretrieval tasks. A comparison with a well known general sentence embedding\nmethod, the Paragraph Vector, is performed. The results show that the proposed\nmethod in this paper significantly outperforms it for web document retrieval\ntask. \n\n"}
{"id": "1502.07058", "contents": "Title: Evaluation of Deep Convolutional Nets for Document Image Classification\n  and Retrieval Abstract: This paper presents a new state-of-the-art for document image classification\nand retrieval, using features learned by deep convolutional neural networks\n(CNNs). In object and scene analysis, deep neural nets are capable of learning\na hierarchical chain of abstraction from pixel inputs to concise and\ndescriptive representations. The current work explores this capacity in the\nrealm of document analysis, and confirms that this representation strategy is\nsuperior to a variety of popular hand-crafted alternatives. Experiments also\nshow that (i) features extracted from CNNs are robust to compression, (ii) CNNs\ntrained on non-document images transfer well to document analysis tasks, and\n(iii) enforcing region-specific feature-learning is unnecessary given\nsufficient training data. This work also makes available a new labelled subset\nof the IIT-CDIP collection, containing 400,000 document images across 16\ncategories, useful for training new CNNs for document analysis. \n\n"}
{"id": "1502.08030", "contents": "Title: Author Name Disambiguation by Using Deep Neural Network Abstract: Author name ambiguity decreases the quality and reliability of information\nretrieved from digital libraries. Existing methods have tried to solve this\nproblem by predefining a feature set based on expert's knowledge for a specific\ndataset. In this paper, we propose a new approach which uses deep neural\nnetwork to learn features automatically from data. Additionally, we propose the\ngeneral system architecture for author name disambiguation on any dataset. In\nthis research, we evaluate the proposed method on a dataset containing\nVietnamese author names. The results show that this method significantly\noutperforms other methods that use predefined feature set. The proposed method\nachieves 99.31% in terms of accuracy. Prediction error rate decreases from\n1.83% to 0.69%, i.e., it decreases by 1.14%, or 62.3% relatively compared with\nother methods that use predefined feature set (Table 3). \n\n"}
{"id": "1503.00024", "contents": "Title: Influence Maximization with Bandits Abstract: We consider the problem of \\emph{influence maximization}, the problem of\nmaximizing the number of people that become aware of a product by finding the\n`best' set of `seed' users to expose the product to. Most prior work on this\ntopic assumes that we know the probability of each user influencing each other\nuser, or we have data that lets us estimate these influences. However, this\ninformation is typically not initially available or is difficult to obtain. To\navoid this assumption, we adopt a combinatorial multi-armed bandit paradigm\nthat estimates the influence probabilities as we sequentially try different\nseed sets. We establish bounds on the performance of this procedure under the\nexisting edge-level feedback as well as a novel and more realistic node-level\nfeedback. Beyond our theoretical results, we describe a practical\nimplementation and experimentally demonstrate its efficiency and effectiveness\non four real datasets. \n\n"}
{"id": "1503.02364", "contents": "Title: Neural Responding Machine for Short-Text Conversation Abstract: We propose Neural Responding Machine (NRM), a neural network-based response\ngenerator for Short-Text Conversation. NRM takes the general encoder-decoder\nframework: it formalizes the generation of response as a decoding process based\non the latent representation of the input text, while both encoding and\ndecoding are realized with recurrent neural networks (RNN). The NRM is trained\nwith a large amount of one-round conversation data collected from a\nmicroblogging service. Empirical study shows that NRM can generate\ngrammatically correct and content-wise appropriate responses to over 75% of the\ninput text, outperforming state-of-the-arts in the same setting, including\nretrieval-based and SMT-based models. \n\n"}
{"id": "1503.08542", "contents": "Title: Nonparametric Relational Topic Models through Dependent Gamma Processes Abstract: Traditional Relational Topic Models provide a way to discover the hidden\ntopics from a document network. Many theoretical and practical tasks, such as\ndimensional reduction, document clustering, link prediction, benefit from this\nrevealed knowledge. However, existing relational topic models are based on an\nassumption that the number of hidden topics is known in advance, and this is\nimpractical in many real-world applications. Therefore, in order to relax this\nassumption, we propose a nonparametric relational topic model in this paper.\nInstead of using fixed-dimensional probability distributions in its generative\nmodel, we use stochastic processes. Specifically, a gamma process is assigned\nto each document, which represents the topic interest of this document.\nAlthough this method provides an elegant solution, it brings additional\nchallenges when mathematically modeling the inherent network structure of\ntypical document network, i.e., two spatially closer documents tend to have\nmore similar topics. Furthermore, we require that the topics are shared by all\nthe documents. In order to resolve these challenges, we use a subsampling\nstrategy to assign each document a different gamma process from the global\ngamma process, and the subsampling probabilities of documents are assigned with\na Markov Random Field constraint that inherits the document network structure.\nThrough the designed posterior inference algorithm, we can discover the hidden\ntopics and its number simultaneously. Experimental results on both synthetic\nand real-world network datasets demonstrate the capabilities of learning the\nhidden topics and, more importantly, the number of topics. \n\n"}
{"id": "1504.01482", "contents": "Title: Deep Recurrent Neural Networks for Acoustic Modelling Abstract: We present a novel deep Recurrent Neural Network (RNN) model for acoustic\nmodelling in Automatic Speech Recognition (ASR). We term our contribution as a\nTC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with\nTime Convolution (TC), followed by a Bidirectional Long Short-Term Memory\n(BLSTM), and a final DNN. The first DNN acts as a feature processor to our\nmodel, the BLSTM then generates a context from the sequence acoustic signal,\nand the final DNN takes the context and models the posterior probabilities of\nthe acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ)\neval92 task or more than 8% relative improvement over the baseline DNN models. \n\n"}
{"id": "1504.01483", "contents": "Title: Transferring Knowledge from a RNN to a DNN Abstract: Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art\nresults in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent\nNeural Network (RNN) models have been shown to outperform DNNs counterparts.\nHowever, state-of-the-art DNN and RNN models tend to be impractical to deploy\non embedded systems with limited computational capacity. Traditionally, the\napproach for embedded platforms is to either train a small DNN directly, or to\ntrain a small DNN that learns the output distribution of a large DNN. In this\npaper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We\nuse the RNN model to generate soft alignments and minimize the Kullback-Leibler\ndivergence against the small DNN. The small DNN trained on the soft RNN\nalignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 task\ncompared to a baseline 4.54 WER or more than 13% relative improvement. \n\n"}
{"id": "1504.06667", "contents": "Title: Handling oversampling in dynamic networks using link prediction Abstract: Oversampling is a common characteristic of data representing dynamic\nnetworks. It introduces noise into representations of dynamic networks, but\nthere has been little work so far to compensate for it. Oversampling can affect\nthe quality of many important algorithmic problems on dynamic networks,\nincluding link prediction. Link prediction seeks to predict edges that will be\nadded to the network given previous snapshots. We show that not only does\noversampling affect the quality of link prediction, but that we can use link\nprediction to recover from the effects of oversampling. We also introduce a\nnovel generative model of noise in dynamic networks that represents\noversampling. We demonstrate the results of our approach on both synthetic and\nreal-world data. \n\n"}
{"id": "1505.04094", "contents": "Title: Evaluating Link Prediction Methods Abstract: Link prediction is a popular research area with important applications in a\nvariety of disciplines, including biology, social science, security, and\nmedicine. The fundamental requirement of link prediction is the accurate and\neffective prediction of new links in networks. While there are many different\nmethods proposed for link prediction, we argue that the practical performance\npotential of these methods is often unknown because of challenges in the\nevaluation of link prediction, which impact the reliability and reproducibility\nof results. We describe these challenges, provide theoretical proofs and\nempirical examples demonstrating how current methods lead to questionable\nconclusions, show how the fallacy of these conclusions is illuminated by\nmethods we propose, and develop recommendations for consistent, standard, and\napplicable evaluation metrics. We also recommend the use of precision-recall\nthreshold curves and associated areas in lieu of receiver operating\ncharacteristic curves due to complications that arise from extreme imbalance in\nthe link prediction classification problem. \n\n"}
{"id": "1505.04342", "contents": "Title: Sifting Robotic from Organic Text: A Natural Language Approach for\n  Detecting Automation on Twitter Abstract: Twitter, a popular social media outlet, has evolved into a vast source of\nlinguistic data, rich with opinion, sentiment, and discussion. Due to the\nincreasing popularity of Twitter, its perceived potential for exerting social\ninfluence has led to the rise of a diverse community of automatons, commonly\nreferred to as bots. These inorganic and semi-organic Twitter entities can\nrange from the benevolent (e.g., weather-update bots, help-wanted-alert bots)\nto the malevolent (e.g., spamming messages, advertisements, or radical\nopinions). Existing detection algorithms typically leverage meta-data (time\nbetween tweets, number of followers, etc.) to identify robotic accounts. Here,\nwe present a powerful classification scheme that exclusively uses the natural\nlanguage text from organic users to provide a criterion for identifying\naccounts posting automated messages. Since the classifier operates on text\nalone, it is flexible and may be applied to any textual data beyond the\nTwitter-sphere. \n\n"}
{"id": "1505.07909", "contents": "Title: Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered\n  Word Embedding Abstract: Intelligence Quotient (IQ) Test is a set of standardized questions designed\nto evaluate human intelligence. Verbal comprehension questions appear very\nfrequently in IQ tests, which measure human's verbal ability including the\nunderstanding of the words with multiple senses, the synonyms and antonyms, and\nthe analogies among words. In this work, we explore whether such tests can be\nsolved automatically by artificial intelligence technologies, especially the\ndeep learning technologies that are recently developed and successfully applied\nin a number of fields. However, we found that the task was quite challenging,\nand simply applying existing technologies (e.g., word embedding) could not\nachieve a good performance, mainly due to the multiple senses of words and the\ncomplex relations among words. To tackle these challenges, we propose a novel\nframework consisting of three components. First, we build a classifier to\nrecognize the specific type of a verbal question (e.g., analogy,\nclassification, synonym, or antonym). Second, we obtain distributed\nrepresentations of words and relations by leveraging a novel word embedding\nmethod that considers the multi-sense nature of words and the relational\nknowledge among words (or their senses) contained in dictionaries. Third, for\neach type of questions, we propose a specific solver based on the obtained\ndistributed word representations and relation representations. Experimental\nresults have shown that the proposed framework can not only outperform existing\nmethods for solving verbal comprehension questions but also exceed the average\nperformance of the Amazon Mechanical Turk workers involved in the study. The\nresults indicate that with appropriate uses of the deep learning technologies\nwe might be a further step closer to the human intelligence. \n\n"}
{"id": "1506.00333", "contents": "Title: Learning to Answer Questions From Image Using Convolutional Neural\n  Network Abstract: In this paper, we propose to employ the convolutional neural network (CNN)\nfor the image question answering (QA). Our proposed CNN provides an end-to-end\nframework with convolutional architectures for learning not only the image and\nquestion representations, but also their inter-modal interactions to produce\nthe answer. More specifically, our model consists of three CNNs: one image CNN\nto encode the image content, one sentence CNN to compose the words of the\nquestion, and one multimodal convolution layer to learn their joint\nrepresentation for the classification in the space of candidate answer words.\nWe demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA\ndatasets, which are two benchmark datasets for the image QA, with the\nperformances significantly outperforming the state-of-the-art. \n\n"}
{"id": "1506.01070", "contents": "Title: Do Multi-Sense Embeddings Improve Natural Language Understanding? Abstract: Learning a distinct representation for each sense of an ambiguous word could\nlead to more powerful and fine-grained models of vector-space representations.\nYet while `multi-sense' methods have been proposed and tested on artificial\nword-similarity tasks, we don't know if they improve real natural language\nunderstanding tasks. In this paper we introduce a multi-sense embedding model\nbased on Chinese Restaurant Processes that achieves state of the art\nperformance on matching human word similarity judgments, and propose a\npipelined architecture for incorporating multi-sense embeddings into language\nunderstanding.\n  We then test the performance of our model on part-of-speech tagging, named\nentity recognition, sentiment analysis, semantic relation identification and\nsemantic relatedness, controlling for embedding dimensionality. We find that\nmulti-sense embeddings do improve performance on some tasks (part-of-speech\ntagging, semantic relation identification, semantic relatedness) but not on\nothers (named entity recognition, various forms of sentiment analysis). We\ndiscuss how these differences may be caused by the different role of word sense\ninformation in each of the tasks. The results highlight the importance of\ntesting embedding models in real applications. \n\n"}
{"id": "1506.01094", "contents": "Title: Traversing Knowledge Graphs in Vector Space Abstract: Path queries on a knowledge graph can be used to answer compositional\nquestions such as \"What languages are spoken by people living in Lisbon?\".\nHowever, knowledge graphs often have missing facts (edges) which disrupts path\nqueries. Recent models for knowledge base completion impute missing facts by\nembedding knowledge graphs in vector spaces. We show that these models can be\nrecursively applied to answer path queries, but that they suffer from cascading\nerrors. This motivates a new \"compositional\" training objective, which\ndramatically improves all models' ability to answer path queries, in some cases\nmore than doubling accuracy. On a standard knowledge base completion task, we\nalso demonstrate that compositional training acts as a novel form of structural\nregularization, reliably improving performance across all base models (reducing\nerrors by up to 43%) and achieving new state-of-the-art results. \n\n"}
{"id": "1506.02075", "contents": "Title: Large-scale Simple Question Answering with Memory Networks Abstract: Training large-scale question answering systems is complicated because\ntraining sources usually cover a small portion of the range of possible\nquestions. This paper studies the impact of multitask and transfer learning for\nsimple question answering; a setting for which the reasoning required to answer\nis quite easy, as long as one can retrieve the correct evidence given a\nquestion, which can be difficult in large-scale conditions. To this end, we\nintroduce a new dataset of 100k questions that we use in conjunction with\nexisting benchmarks. We conduct our study within the framework of Memory\nNetworks (Weston et al., 2015) because this perspective allows us to eventually\nscale up to more complex reasoning, and show that Memory Networks can be\nsuccessfully trained to achieve excellent performance. \n\n"}
{"id": "1506.04488", "contents": "Title: Distilling Word Embeddings: An Encoding Approach Abstract: Distilling knowledge from a well-trained cumbersome network to a small one\nhas recently become a new research topic, as lightweight neural networks with\nhigh performance are particularly in need in various resource-restricted\nsystems. This paper addresses the problem of distilling word embeddings for NLP\ntasks. We propose an encoding approach to distill task-specific knowledge from\na set of high-dimensional embeddings, which can reduce model complexity by a\nlarge margin as well as retain high accuracy, showing a good compromise between\nefficiency and performance. Experiments in two tasks reveal the phenomenon that\ndistilling knowledge from cumbersome embeddings is better than directly\ntraining neural networks with small embeddings. \n\n"}
{"id": "1506.05163", "contents": "Title: Deep Convolutional Networks on Graph-Structured Data Abstract: Deep Learning's recent successes have mostly relied on Convolutional\nNetworks, which exploit fundamental statistical properties of images, sounds\nand video data: the local stationarity and multi-scale compositional structure,\nthat allows expressing long range interactions in terms of shorter, localized\ninteractions. However, there exist other important examples, such as text\ndocuments or bioinformatic data, that may lack some or all of these strong\nstatistical regularities.\n  In this paper we consider the general question of how to construct deep\narchitectures with small learning complexity on general non-Euclidean domains,\nwhich are typically unknown and need to be estimated from the data. In\nparticular, we develop an extension of Spectral Networks which incorporates a\nGraph Estimation procedure, that we test on large-scale classification\nproblems, matching or improving over Dropout Networks with far less parameters\nto estimate. \n\n"}
{"id": "1506.05865", "contents": "Title: LCSTS: A Large Scale Chinese Short Text Summarization Dataset Abstract: Automatic text summarization is widely regarded as the highly difficult\nproblem, partially because of the lack of large text summarization data set.\nDue to the great challenge of constructing the large scale summaries for full\ntext, in this paper, we introduce a large corpus of Chinese short text\nsummarization dataset constructed from the Chinese microblogging website Sina\nWeibo, which is released to the public\n{http://icrc.hitsz.edu.cn/Article/show/139.html}. This corpus consists of over\n2 million real Chinese short texts with short summaries given by the author of\neach text. We also manually tagged the relevance of 10,666 short summaries with\ntheir corresponding short texts. Based on the corpus, we introduce recurrent\nneural network for the summary generation and achieve promising results, which\nnot only shows the usefulness of the proposed corpus for short text\nsummarization research, but also provides a baseline for further research on\nthis topic. \n\n"}
{"id": "1506.05869", "contents": "Title: A Neural Conversational Model Abstract: Conversational modeling is an important task in natural language\nunderstanding and machine intelligence. Although previous approaches exist,\nthey are often restricted to specific domains (e.g., booking an airline ticket)\nand require hand-crafted rules. In this paper, we present a simple approach for\nthis task which uses the recently proposed sequence to sequence framework. Our\nmodel converses by predicting the next sentence given the previous sentence or\nsentences in a conversation. The strength of our model is that it can be\ntrained end-to-end and thus requires much fewer hand-crafted rules. We find\nthat this straightforward model can generate simple conversations given a large\nconversational training dataset. Our preliminary results suggest that, despite\noptimizing the wrong objective function, the model is able to converse well. It\nis able extract knowledge from both a domain specific dataset, and from a\nlarge, noisy, and general domain dataset of movie subtitles. On a\ndomain-specific IT helpdesk dataset, the model can find a solution to a\ntechnical problem via conversations. On a noisy open-domain movie transcript\ndataset, the model can perform simple forms of common sense reasoning. As\nexpected, we also find that the lack of consistency is a common failure mode of\nour model. \n\n"}
{"id": "1506.06021", "contents": "Title: Measuring Emotional Contagion in Social Media Abstract: Social media are used as main discussion channels by millions of individuals\nevery day. The content individuals produce in daily social-media-based\nmicro-communications, and the emotions therein expressed, may impact the\nemotional states of others. A recent experiment performed on Facebook\nhypothesized that emotions spread online, even in absence of non-verbal cues\ntypical of in-person interactions, and that individuals are more likely to\nadopt positive or negative emotions if these are over-expressed in their social\nnetwork. Experiments of this type, however, raise ethical concerns, as they\nrequire massive-scale content manipulation with unknown consequences for the\nindividuals therein involved. Here, we study the dynamics of emotional\ncontagion using Twitter. Rather than manipulating content, we devise a null\nmodel that discounts some confounding factors (including the effect of\nemotional contagion). We measure the emotional valence of content the users are\nexposed to before posting their own tweets. We determine that on average a\nnegative post follows an over-exposure to 4.34% more negative content than\nbaseline, while positive posts occur after an average over-exposure to 4.50%\nmore positive contents. We highlight the presence of a linear relationship\nbetween the average emotional valence of the stimuli users are exposed to, and\nthat of the responses they produce. We also identify two different classes of\nindividuals: highly and scarcely susceptible to emotional contagion. Highly\nsusceptible users are significantly less inclined to adopt negative emotions\nthan the scarcely susceptible ones, but equally likely to adopt positive\nemotions. In general, the likelihood of adopting positive emotions is much\ngreater than that of negative emotions. \n\n"}
{"id": "1506.06072", "contents": "Title: Quantifying the Effect of Sentiment on Information Diffusion in Social\n  Media Abstract: Social media have become the main vehicle of information production and\nconsumption online. Millions of users every day log on their Facebook or\nTwitter accounts to get updates and news, read about their topics of interest,\nand become exposed to new opportunities and interactions. Although recent\nstudies suggest that the contents users produce will affect the emotions of\ntheir readers, we still lack a rigorous understanding of the role and effects\nof contents sentiment on the dynamics of information diffusion. This work aims\nat quantifying the effect of sentiment on information diffusion, to understand:\n(i) whether positive conversations spread faster and/or broader than negative\nones (or vice-versa); (ii) what kind of emotions are more typical of popular\nconversations on social media; and, (iii) what type of sentiment is expressed\nin conversations characterized by different temporal dynamics. Our findings\nshow that, at the level of contents, negative messages spread faster than\npositive ones, but positive ones reach larger audiences, suggesting that people\nare more inclined to share and favorite positive contents, the so-called\npositive bias. As for the entire conversations, we highlight how different\ntemporal dynamics exhibit different sentiment patterns: for example, positive\nsentiment builds up for highly-anticipated events, while unexpected events are\nmainly characterized by negative sentiment. Our contribution is a milestone to\nunderstand how the emotions expressed in short texts affect their spreading in\nonline social ecosystems, and may help to craft effective policies and\nstrategies for content generation and diffusion. \n\n"}
{"id": "1506.06490", "contents": "Title: Answer Sequence Learning with Neural Networks for Answer Selection in\n  Community Question Answering Abstract: In this paper, the answer selection problem in community question answering\n(CQA) is regarded as an answer sequence labeling task, and a novel approach is\nproposed based on the recurrent architecture for this problem. Our approach\napplies convolution neural networks (CNNs) to learning the joint representation\nof question-answer pair firstly, and then uses the joint representation as\ninput of the long short-term memory (LSTM) to learn the answer sequence of a\nquestion for labeling the matching quality of each answer. Experiments\nconducted on the SemEval 2015 CQA dataset shows the effectiveness of our\napproach. \n\n"}
{"id": "1506.07285", "contents": "Title: Ask Me Anything: Dynamic Memory Networks for Natural Language Processing Abstract: Most tasks in natural language processing can be cast into question answering\n(QA) problems over language input. We introduce the dynamic memory network\n(DMN), a neural network architecture which processes input sequences and\nquestions, forms episodic memories, and generates relevant answers. Questions\ntrigger an iterative attention process which allows the model to condition its\nattention on the inputs and the result of previous iterations. These results\nare then reasoned over in a hierarchical recurrent sequence model to generate\nanswers. The DMN can be trained end-to-end and obtains state-of-the-art results\non several types of tasks and datasets: question answering (Facebook's bAbI\ndataset), text classification for sentiment analysis (Stanford Sentiment\nTreebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The\ntraining for these different tasks relies exclusively on trained word vector\nrepresentations and input-question-answer triplets. \n\n"}
{"id": "1506.08909", "contents": "Title: The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured\n  Multi-Turn Dialogue Systems Abstract: This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost\n1 million multi-turn dialogues, with a total of over 7 million utterances and\n100 million words. This provides a unique resource for research into building\ndialogue managers based on neural language models that can make use of large\namounts of unlabeled data. The dataset has both the multi-turn property of\nconversations in the Dialog State Tracking Challenge datasets, and the\nunstructured nature of interactions from microblog services such as Twitter. We\nalso describe two neural learning architectures suitable for analyzing this\ndataset, and provide benchmark performance on the task of selecting the best\nnext response. \n\n"}
{"id": "1507.00955", "contents": "Title: Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and\n  Their Combination Abstract: This paper covers the two approaches for sentiment analysis: i) lexicon based\nmethod; ii) machine learning method. We describe several techniques to\nimplement these approaches and discuss how they can be adopted for sentiment\nclassification of Twitter messages. We present a comparative study of different\nlexicon combinations and show that enhancing sentiment lexicons with emoticons,\nabbreviations and social-media slang expressions increases the accuracy of\nlexicon-based classification for Twitter. We discuss the importance of feature\ngeneration and feature selection processes for machine learning sentiment\nclassification. To quantify the performance of the main sentiment analysis\nmethods over Twitter we run these algorithms on a benchmark Twitter dataset\nfrom the SemEval-2013 competition, task 2-B. The results show that machine\nlearning method based on SVM and Naive Bayes classifiers outperforms the\nlexicon method. We present a new ensemble method that uses a lexicon based\nsentiment score as input feature for the machine learning approach. The\ncombined method proved to produce more precise classifications. We also show\nthat employing a cost-sensitive classifier for highly unbalanced datasets\nyields an improvement of sentiment classification performance up to 7%. \n\n"}
{"id": "1507.01526", "contents": "Title: Grid Long Short-Term Memory Abstract: This paper introduces Grid Long Short-Term Memory, a network of LSTM cells\narranged in a multidimensional grid that can be applied to vectors, sequences\nor higher dimensional data such as images. The network differs from existing\ndeep LSTM architectures in that the cells are connected between network layers\nas well as along the spatiotemporal dimensions of the data. The network\nprovides a unified way of using LSTM for both deep and sequential computation.\nWe apply the model to algorithmic tasks such as 15-digit integer addition and\nsequence memorization, where it is able to significantly outperform the\nstandard LSTM. We then give results for two empirical tasks. We find that 2D\nGrid LSTM achieves 1.47 bits per character on the Wikipedia character\nprediction benchmark, which is state-of-the-art among neural approaches. In\naddition, we use the Grid LSTM to define a novel two-dimensional translation\nmodel, the Reencoder, and show that it outperforms a phrase-based reference\nsystem on a Chinese-to-English translation task. \n\n"}
{"id": "1507.02293", "contents": "Title: COEVOLVE: A Joint Point Process Model for Information Diffusion and\n  Network Co-evolution Abstract: Information diffusion in online social networks is affected by the underlying\nnetwork topology, but it also has the power to change it. Online users are\nconstantly creating new links when exposed to new information sources, and in\nturn these links are alternating the way information spreads. However, these\ntwo highly intertwined stochastic processes, information diffusion and network\nevolution, have been predominantly studied separately, ignoring their\nco-evolutionary dynamics.\n  We propose a temporal point process model, COEVOLVE, for such joint dynamics,\nallowing the intensity of one process to be modulated by that of the other.\nThis model allows us to efficiently simulate interleaved diffusion and network\nevents, and generate traces obeying common diffusion and network patterns\nobserved in real-world networks. Furthermore, we also develop a convex\noptimization framework to learn the parameters of the model from historical\ndiffusion and network evolution traces. We experimented with both synthetic\ndata and data gathered from Twitter, and show that our model provides a good\nfit to the data as well as more accurate predictions than alternatives. \n\n"}
{"id": "1507.04808", "contents": "Title: Building End-To-End Dialogue Systems Using Generative Hierarchical\n  Neural Network Models Abstract: We investigate the task of building open domain, conversational dialogue\nsystems based on large dialogue corpora using generative models. Generative\nmodels produce system responses that are autonomously generated word-by-word,\nopening up the possibility for realistic, flexible interactions. In support of\nthis goal, we extend the recently proposed hierarchical recurrent\nencoder-decoder neural network to the dialogue domain, and demonstrate that\nthis model is competitive with state-of-the-art neural language models and\nback-off n-gram models. We investigate the limitations of this and similar\napproaches, and show how its performance can be improved by bootstrapping the\nlearning from a larger question-answer pair corpus and from pretrained word\nembeddings. \n\n"}
{"id": "1507.04808", "contents": "Title: Building End-To-End Dialogue Systems Using Generative Hierarchical\n  Neural Network Models Abstract: We investigate the task of building open domain, conversational dialogue\nsystems based on large dialogue corpora using generative models. Generative\nmodels produce system responses that are autonomously generated word-by-word,\nopening up the possibility for realistic, flexible interactions. In support of\nthis goal, we extend the recently proposed hierarchical recurrent\nencoder-decoder neural network to the dialogue domain, and demonstrate that\nthis model is competitive with state-of-the-art neural language models and\nback-off n-gram models. We investigate the limitations of this and similar\napproaches, and show how its performance can be improved by bootstrapping the\nlearning from a larger question-answer pair corpus and from pretrained word\nembeddings. \n\n"}
{"id": "1507.05910", "contents": "Title: Clustering is Efficient for Approximate Maximum Inner Product Search Abstract: Efficient Maximum Inner Product Search (MIPS) is an important task that has a\nwide applicability in recommendation systems and classification with a large\nnumber of classes. Solutions based on locality-sensitive hashing (LSH) as well\nas tree-based solutions have been investigated in the recent literature, to\nperform approximate MIPS in sublinear time. In this paper, we compare these to\nanother extremely simple approach for solving approximate MIPS, based on\nvariants of the k-means clustering algorithm. Specifically, we propose to train\na spherical k-means, after having reduced the MIPS problem to a Maximum Cosine\nSimilarity Search (MCSS). Experiments on two standard recommendation system\nbenchmarks as well as on large vocabulary word embeddings, show that this\nsimple approach yields much higher speedups, for the same retrieval precision,\nthan current state-of-the-art hashing-based and tree-based methods. This simple\nmethod also yields more robust retrievals when the query is corrupted by noise. \n\n"}
{"id": "1507.08396", "contents": "Title: Tag-Weighted Topic Model For Large-scale Semi-Structured Documents Abstract: To date, there have been massive Semi-Structured Documents (SSDs) during the\nevolution of the Internet. These SSDs contain both unstructured features (e.g.,\nplain text) and metadata (e.g., tags). Most previous works focused on modeling\nthe unstructured text, and recently, some other methods have been proposed to\nmodel the unstructured text with specific tags. To build a general model for\nSSDs remains an important problem in terms of both model fitness and\nefficiency. We propose a novel method to model the SSDs by a so-called\nTag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the\ntags and words information, not only to learn the document-topic and topic-word\ndistributions, but also to infer the tag-topic distributions for text mining\ntasks. We present an efficient variational inference method with an EM\nalgorithm for estimating the model parameters. Meanwhile, we propose three\nlarge-scale solutions for our model under the MapReduce distributed computing\nplatform for modeling large-scale SSDs. The experimental results show the\neffectiveness, efficiency and the robustness by comparing our model with the\nstate-of-the-art methods in document modeling, tags prediction and text\nclassification. We also show the performance of the three distributed solutions\nin terms of time and accuracy on document modeling. \n\n"}
{"id": "1508.00189", "contents": "Title: Class Vectors: Embedding representation of Document Classes Abstract: Distributed representations of words and paragraphs as semantic embeddings in\nhigh dimensional data are used across a number of Natural Language\nUnderstanding tasks such as retrieval, translation, and classification. In this\nwork, we propose \"Class Vectors\" - a framework for learning a vector per class\nin the same embedding space as the word and paragraph embeddings. Similarity\nbetween these class vectors and word vectors are used as features to classify a\ndocument to a class. In experiment on several sentiment analysis tasks such as\nYelp reviews and Amazon electronic product reviews, class vectors have shown\nbetter or comparable results in classification while learning very meaningful\nclass embeddings. \n\n"}
{"id": "1508.01011", "contents": "Title: Learning from LDA using Deep Neural Networks Abstract: Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian\nmodel for topic inference. In spite of its great success, inferring the latent\ntopic distribution with LDA is time-consuming. Motivated by the transfer\nlearning approach proposed by~\\newcite{hinton2015distilling}, we present a\nnovel method that uses LDA to supervise the training of a deep neural network\n(DNN), so that the DNN can approximate the costly LDA inference with less\ncomputation. Our experiments on a document classification task show that a\nsimple DNN can learn the LDA behavior pretty well, while the inference is\nspeeded up tens or hundreds of times. \n\n"}
{"id": "1508.03386", "contents": "Title: Learning from Real Users: Rating Dialogue Success with Neural Networks\n  for Reinforcement Learning in Spoken Dialogue Systems Abstract: To train a statistical spoken dialogue system (SDS) it is essential that an\naccurate method for measuring task success is available. To date training has\nrelied on presenting a task to either simulated or paid users and inferring the\ndialogue's success by observing whether this presented task was achieved or\nnot. Our aim however is to be able to learn from real users acting under their\nown volition, in which case it is non-trivial to rate the success as any prior\nknowledge of the task is simply unavailable. User feedback may be utilised but\nhas been found to be inconsistent. Hence, here we present two neural network\nmodels that evaluate a sequence of turn-level features to rate the success of a\ndialogue. Importantly these models make no use of any prior knowledge of the\nuser's task. The models are trained on dialogues generated by a simulated user\nand the best model is then used to train a policy on-line which is shown to\nperform at least as well as a baseline system using prior knowledge of the\nuser's task. We note that the models should also be of interest for evaluating\nSDS and for monitoring a dialogue in rule-based SDS. \n\n"}
{"id": "1508.03391", "contents": "Title: Reward Shaping with Recurrent Neural Networks for Speeding up On-Line\n  Policy Learning in Spoken Dialogue Systems Abstract: Statistical spoken dialogue systems have the attractive property of being\nable to be optimised from data via interactions with real users. However in the\nreinforcement learning paradigm the dialogue manager (agent) often requires\nsignificant time to explore the state-action space to learn to behave in a\ndesirable manner. This is a critical issue when the system is trained on-line\nwith real users where learning costs are expensive. Reward shaping is one\npromising technique for addressing these concerns. Here we examine three\nrecurrent neural network (RNN) approaches for providing reward shaping\ninformation in addition to the primary (task-orientated) environmental\nfeedback. These RNNs are trained on returns from dialogues generated by a\nsimulated user and attempt to diffuse the overall evaluation of the dialogue\nback down to the turn level to guide the agent towards good behaviour faster.\nIn both simulated and real user scenarios these RNNs are shown to increase\npolicy learning speed. Importantly, they do not require prior knowledge of the\nuser's goal. \n\n"}
{"id": "1508.04395", "contents": "Title: End-to-End Attention-based Large Vocabulary Speech Recognition Abstract: Many of the current state-of-the-art Large Vocabulary Continuous Speech\nRecognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov\nModels (HMMs). Most of these systems contain separate components that deal with\nthe acoustic modelling, language modelling and sequence decoding. We\ninvestigate a more direct approach in which the HMM is replaced with a\nRecurrent Neural Network (RNN) that performs sequence prediction directly at\nthe character level. Alignment between the input features and the desired\ncharacter sequence is learned automatically by an attention mechanism built\ninto the RNN. For each predicted character, the attention mechanism scans the\ninput sequence and chooses relevant frames. We propose two methods to speed up\nthis operation: limiting the scan to a subset of most promising frames and\npooling over time the information contained in neighboring frames, thereby\nreducing source sequence length. Integrating an n-gram language model into the\ndecoding process yields recognition accuracies similar to other HMM-free\nRNN-based approaches. \n\n"}
{"id": "1508.05565", "contents": "Title: Necessary and Sufficient Conditions and a Provably Efficient Algorithm\n  for Separable Topic Discovery Abstract: We develop necessary and sufficient conditions and a novel provably\nconsistent and efficient algorithm for discovering topics (latent factors) from\nobservations (documents) that are realized from a probabilistic mixture of\nshared latent factors that have certain properties. Our focus is on the class\nof topic models in which each shared latent factor contains a novel word that\nis unique to that factor, a property that has come to be known as separability.\nOur algorithm is based on the key insight that the novel words correspond to\nthe extreme points of the convex hull formed by the row-vectors of a suitably\nnormalized word co-occurrence matrix. We leverage this geometric insight to\nestablish polynomial computation and sample complexity bounds based on a few\nisotropic random projections of the rows of the normalized word co-occurrence\nmatrix. Our proposed random-projections-based algorithm is naturally amenable\nto an efficient distributed implementation and is attractive for modern\nweb-scale distributed data mining applications. \n\n"}
{"id": "1508.07744", "contents": "Title: Ethnicity sensitive author disambiguation using semi-supervised learning Abstract: Author name disambiguation in bibliographic databases is the problem of\ngrouping together scientific publications written by the same person,\naccounting for potential homonyms and/or synonyms. Among solutions to this\nproblem, digital libraries are increasingly offering tools for authors to\nmanually curate their publications and claim those that are theirs. Indirectly,\nthese tools allow for the inexpensive collection of large annotated training\ndata, which can be further leveraged to build a complementary automated\ndisambiguation system capable of inferring patterns for identifying\npublications written by the same person. Building on more than 1 million\npublicly released crowdsourced annotations, we propose an automated author\ndisambiguation solution exploiting this data (i) to learn an accurate\nclassifier for identifying coreferring authors and (ii) to guide the clustering\nof scientific publications by distinct authors in a semi-supervised way. To the\nbest of our knowledge, our analysis is the first to be carried out on data of\nthis size and coverage. With respect to the state of the art, we validate the\ngeneral pipeline used in most existing solutions, and improve by: (i) proposing\nphonetic-based blocking strategies, thereby increasing recall; and (ii) adding\nstrong ethnicity-sensitive features for learning a linkage function, thereby\ntailoring disambiguation to non-Western author names whenever necessary. \n\n"}
{"id": "1509.01626", "contents": "Title: Character-level Convolutional Networks for Text Classification Abstract: This article offers an empirical exploration on the use of character-level\nconvolutional networks (ConvNets) for text classification. We constructed\nseveral large-scale datasets to show that character-level convolutional\nnetworks could achieve state-of-the-art or competitive results. Comparisons are\noffered against traditional models such as bag of words, n-grams and their\nTFIDF variants, and deep learning models such as word-based ConvNets and\nrecurrent neural networks. \n\n"}
{"id": "1509.02301", "contents": "Title: Probabilistic Bag-Of-Hyperlinks Model for Entity Linking Abstract: Many fundamental problems in natural language processing rely on determining\nwhat entities appear in a given text. Commonly referenced as entity linking,\nthis step is a fundamental component of many NLP tasks such as text\nunderstanding, automatic summarization, semantic search or machine translation.\nName ambiguity, word polysemy, context dependencies and a heavy-tailed\ndistribution of entities contribute to the complexity of this problem.\n  We here propose a probabilistic approach that makes use of an effective\ngraphical model to perform collective entity disambiguation. Input mentions\n(i.e.,~linkable token spans) are disambiguated jointly across an entire\ndocument by combining a document-level prior of entity co-occurrences with\nlocal information captured from mentions and their surrounding context. The\nmodel is based on simple sufficient statistics extracted from data, thus\nrelying on few parameters to be learned.\n  Our method does not require extensive feature engineering, nor an expensive\ntraining procedure. We use loopy belief propagation to perform approximate\ninference. The low complexity of our model makes this step sufficiently fast\nfor real-time usage. We demonstrate the accuracy of our approach on a wide\nrange of benchmark datasets, showing that it matches, and in many cases\noutperforms, existing state-of-the-art methods. \n\n"}
{"id": "1509.06664", "contents": "Title: Reasoning about Entailment with Neural Attention Abstract: While most approaches to automatically recognizing entailment relations have\nused classifiers employing hand engineered features derived from complex\nnatural language processing pipelines, in practice their performance has been\nonly slightly better than bag-of-word pair classifiers using only lexical\nsimilarity. The only attempt so far to build an end-to-end differentiable\nneural network for entailment failed to outperform such a simple similarity\nclassifier. In this paper, we propose a neural model that reads two sentences\nto determine entailment using long short-term memory units. We extend this\nmodel with a word-by-word neural attention mechanism that encourages reasoning\nover entailments of pairs of words and phrases. Furthermore, we present a\nqualitative analysis of attention weights produced by this model, demonstrating\nsuch reasoning capabilities. On a large entailment dataset this model\noutperforms the previous best neural model and a classifier with engineered\nfeatures by a substantial margin. It is the first generic end-to-end\ndifferentiable system that achieves state-of-the-art accuracy on a textual\nentailment dataset. \n\n"}
{"id": "1509.07344", "contents": "Title: Opinion mining from twitter data using evolutionary multinomial mixture\n  models Abstract: Image of an entity can be defined as a structured and dynamic representation\nwhich can be extracted from the opinions of a group of users or population.\nAutomatic extraction of such an image has certain importance in political\nscience and sociology related studies, e.g., when an extended inquiry from\nlarge-scale data is required. We study the images of two politically\nsignificant entities of France. These images are constructed by analyzing the\nopinions collected from a well known social media called Twitter. Our goal is\nto build a system which can be used to automatically extract the image of\nentities over time.\n  In this paper, we propose a novel evolutionary clustering method based on the\nparametric link among Multinomial mixture models. First we propose the\nformulation of a generalized model that establishes parametric links among the\nMultinomial distributions. Afterward, we follow a model-based clustering\napproach to explore different parametric sub-models and select the best model.\nFor the experiments, first we use synthetic temporal data. Next, we apply the\nmethod to analyze the annotated social media data. Results show that the\nproposed method is better than the state-of-the-art based on the common\nevaluation metrics. Additionally, our method can provide interpretation about\nthe temporal evolution of the clusters. \n\n"}
{"id": "1509.08990", "contents": "Title: Learning without Recall: A Case for Log-Linear Learning Abstract: We analyze a model of learning and belief formation in networks in which\nagents follow Bayes rule yet they do not recall their history of past\nobservations and cannot reason about how other agents' beliefs are formed. They\ndo so by making rational inferences about their observations which include a\nsequence of independent and identically distributed private signals as well as\nthe beliefs of their neighboring agents at each time. Fully rational agents\nwould successively apply Bayes rule to the entire history of observations. This\nleads to forebodingly complex inferences due to lack of knowledge about the\nglobal network structure that causes those observations. To address these\ncomplexities, we consider a Learning without Recall model, which in addition to\nproviding a tractable framework for analyzing the behavior of rational agents\nin social networks, can also provide a behavioral foundation for the variety of\nnon-Bayesian update rules in the literature. We present the implications of\nvarious choices for time-varying priors of such agents and how this choice\naffects learning and its rate. \n\n"}
{"id": "1510.03753", "contents": "Title: Improved Deep Learning Baselines for Ubuntu Corpus Dialogs Abstract: This paper presents results of our experiments for the next utterance ranking\non the Ubuntu Dialog Corpus -- the largest publicly available multi-turn dialog\ncorpus. First, we use an in-house implementation of previously reported models\nto do an independent evaluation using the same data. Second, we evaluate the\nperformances of various LSTMs, Bi-LSTMs and CNNs on the dataset. Third, we\ncreate an ensemble by averaging predictions of multiple models. The ensemble\nfurther improves the performance and it achieves a state-of-the-art result for\nthe next utterance ranking on this dataset. Finally, we discuss our future\nplans using this corpus. \n\n"}
{"id": "1510.05198", "contents": "Title: Learning multi-faceted representations of individuals from heterogeneous\n  evidence using neural networks Abstract: Inferring latent attributes of people online is an important social computing\ntask, but requires integrating the many heterogeneous sources of information\navailable on the web. We propose learning individual representations of people\nusing neural nets to integrate rich linguistic and network evidence gathered\nfrom social media. The algorithm is able to combine diverse cues, such as the\ntext a person writes, their attributes (e.g. gender, employer, education,\nlocation) and social relations to other people. We show that by integrating\nboth textual and network evidence, these representations offer improved\nperformance at four important tasks in social media inference on Twitter:\npredicting (1) gender, (2) occupation, (3) location, and (4) friendships for\nusers. Our approach scales to large datasets and the learned representations\ncan be used as general features in and have the potential to benefit a large\nnumber of downstream tasks including link prediction, community detection, or\nprobabilistic reasoning over social networks. \n\n"}
{"id": "1510.06786", "contents": "Title: Freshman or Fresher? Quantifying the Geographic Variation of Internet\n  Language Abstract: We present a new computational technique to detect and analyze statistically\nsignificant geographic variation in language. Our meta-analysis approach\ncaptures statistical properties of word usage across geographical regions and\nuses statistical methods to identify significant changes specific to regions.\nWhile previous approaches have primarily focused on lexical variation between\nregions, our method identifies words that demonstrate semantic and syntactic\nvariation as well.\n  We extend recently developed techniques for neural language models to learn\nword representations which capture differing semantics across geographical\nregions. In order to quantify this variation and ensure robust detection of\ntrue regional differences, we formulate a null model to determine whether\nobserved changes are statistically significant. Our method is the first such\napproach to explicitly account for random variation due to chance while\ndetecting regional variation in word meaning.\n  To validate our model, we study and analyze two different massive online data\nsets: millions of tweets from Twitter spanning not only four different\ncountries but also fifty states, as well as millions of phrases contained in\nthe Google Book Ngrams. Our analysis reveals interesting facets of language\nchange at multiple scales of geographic resolution -- from neighboring states\nto distant continents.\n  Finally, using our model, we propose a measure of semantic distance between\nlanguages. Our analysis of British and American English over a period of 100\nyears reveals that semantic variation between these dialects is shrinking. \n\n"}
{"id": "1510.08480", "contents": "Title: Emoticons vs. Emojis on Twitter: A Causal Inference Approach Abstract: Online writing lacks the non-verbal cues present in face-to-face\ncommunication, which provide additional contextual information about the\nutterance, such as the speaker's intention or affective state. To fill this\nvoid, a number of orthographic features, such as emoticons, expressive\nlengthening, and non-standard punctuation, have become popular in social media\nservices including Twitter and Instagram. Recently, emojis have been introduced\nto social media, and are increasingly popular. This raises the question of\nwhether these predefined pictographic characters will come to replace earlier\northographic methods of paralinguistic communication. In this abstract, we\nattempt to shed light on this question, using a matching approach from causal\ninference to test whether the adoption of emojis causes individual users to\nemploy fewer emoticons in their text on Twitter. \n\n"}
{"id": "1510.08565", "contents": "Title: Attention with Intention for a Neural Network Conversation Model Abstract: In a conversation or a dialogue process, attention and intention play\nintrinsic roles. This paper proposes a neural network based approach that\nmodels the attention and intention processes. It essentially consists of three\nrecurrent networks. The encoder network is a word-level model representing\nsource side sentences. The intention network is a recurrent network that models\nthe dynamics of the intention process. The decoder network is a recurrent\nnetwork produces responses to the input from the source side. It is a language\nmodel that is dependent on the intention and has an attention mechanism to\nattend to particular source side words, when predicting a symbol in the\nresponse. The model is trained end-to-end without labeling data. Experiments\nshow that this model generates natural responses to user inputs. \n\n"}
{"id": "1511.01158", "contents": "Title: Distributed Deep Learning for Question Answering Abstract: This paper is an empirical study of the distributed deep learning for\nquestion answering subtasks: answer selection and question classification.\nComparison studies of SGD, MSGD, ADADELTA, ADAGRAD, ADAM/ADAMAX, RMSPROP,\nDOWNPOUR and EASGD/EAMSGD algorithms have been presented. Experimental results\nshow that the distributed framework based on the message passing interface can\naccelerate the convergence speed at a sublinear scale. This paper demonstrates\nthe importance of distributed training. For example, with 48 workers, a 24x\nspeedup is achievable for the answer selection task and running time is\ndecreased from 138.2 hours to 5.81 hours, which will increase the productivity\nsignificantly. \n\n"}
{"id": "1511.01432", "contents": "Title: Semi-supervised Sequence Learning Abstract: We present two approaches that use unlabeled data to improve sequence\nlearning with recurrent networks. The first approach is to predict what comes\nnext in a sequence, which is a conventional language model in natural language\nprocessing. The second approach is to use a sequence autoencoder, which reads\nthe input sequence into a vector and predicts the input sequence again. These\ntwo algorithms can be used as a \"pretraining\" step for a later supervised\nsequence learning algorithm. In other words, the parameters obtained from the\nunsupervised step can be used as a starting point for other supervised training\nmodels. In our experiments, we find that long short term memory recurrent\nnetworks after being pretrained with the two approaches are more stable and\ngeneralize better. With pretraining, we are able to train long short term\nmemory recurrent networks up to a few hundred timesteps, thereby achieving\nstrong performance in many text classification tasks, such as IMDB, DBpedia and\n20 Newsgroups. \n\n"}
{"id": "1511.03292", "contents": "Title: From Images to Sentences through Scene Description Graphs using\n  Commonsense Reasoning and Knowledge Abstract: In this paper we propose the construction of linguistic descriptions of\nimages. This is achieved through the extraction of scene description graphs\n(SDGs) from visual scenes using an automatically constructed knowledge base.\nSDGs are constructed using both vision and reasoning. Specifically, commonsense\nreasoning is applied on (a) detections obtained from existing perception\nmethods on given images, (b) a \"commonsense\" knowledge base constructed using\nnatural language processing of image annotations and (c) lexical ontological\nknowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based\nevaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most\ncases, sentences auto-constructed from SDGs obtained by our method give a more\nrelevant and thorough description of an image than a recent state-of-the-art\nimage caption based approach. Our Image-Sentence Alignment Evaluation results\nare also comparable to that of the recent state-of-the art approaches. \n\n"}
{"id": "1511.03683", "contents": "Title: Generative Concatenative Nets Jointly Learn to Write and Classify\n  Reviews Abstract: A recommender system's basic task is to estimate how users will respond to\nunseen items. This is typically modeled in terms of how a user might rate a\nproduct, but here we aim to extend such approaches to model how a user would\nwrite about the product. To do so, we design a character-level Recurrent Neural\nNetwork (RNN) that generates personalized product reviews. The network\nconvincingly learns styles and opinions of nearly 1000 distinct authors, using\na large corpus of reviews from BeerAdvocate.com. It also tailors reviews to\ndescribe specific items, categories, and star ratings. Using a simple input\nreplication strategy, the Generative Concatenative Network (GCN) preserves the\nsignal of static auxiliary inputs across wide sequence intervals. Without any\nadditional training, the generative model can classify reviews, identifying the\nauthor of the review, the product category, and the sentiment (rating), with\nremarkable accuracy. Our evaluation shows the GCN captures complex dynamics in\ntext, such as the effect of negation, misspellings, slang, and large\nvocabularies gracefully absent any machinery explicitly dedicated to the\npurpose. \n\n"}
{"id": "1511.04164", "contents": "Title: Natural Language Object Retrieval Abstract: In this paper, we address the task of natural language object retrieval, to\nlocalize a target object within a given image based on a natural language query\nof the object. Natural language object retrieval differs from text-based image\nretrieval task as it involves spatial information about objects within the\nscene and global scene context. To address this issue, we propose a novel\nSpatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate\nboxes for object retrieval, integrating spatial configurations and global\nscene-level contextual information into the network. Our model processes query\ntext, local image descriptors, spatial configurations and global context\nfeatures through a recurrent network, outputs the probability of the query text\nconditioned on each candidate box as a score for the box, and can transfer\nvisual-linguistic knowledge from image captioning domain to our task.\nExperimental results demonstrate that our method effectively utilizes both\nlocal and global information, outperforming previous baseline methods\nsignificantly on different datasets and scenarios, and can exploit large scale\nvision and language datasets for knowledge transfer. \n\n"}
{"id": "1511.04868", "contents": "Title: A Neural Transducer Abstract: Sequence-to-sequence models have achieved impressive results on various\ntasks. However, they are unsuitable for tasks that require incremental\npredictions to be made as more data arrives or tasks that have long input\nsequences and output sequences. This is because they generate an output\nsequence conditioned on an entire input sequence. In this paper, we present a\nNeural Transducer that can make incremental predictions as more input arrives,\nwithout redoing the entire computation. Unlike sequence-to-sequence models, the\nNeural Transducer computes the next-step distribution conditioned on the\npartially observed input sequence and the partially generated sequence. At each\ntime step, the transducer can decide to emit zero to many output symbols. The\ndata can be processed using an encoder and presented as input to the\ntransducer. The discrete decision to emit a symbol at every time step makes it\ndifficult to learn with conventional backpropagation. It is however possible to\ntrain the transducer by using a dynamic programming algorithm to generate\ntarget discrete decisions. Our experiments show that the Neural Transducer\nworks well in settings where it is required to produce output predictions as\ndata come in. We also find that the Neural Transducer performs well for long\nsequences even when attention mechanisms are not used. \n\n"}
{"id": "1511.05118", "contents": "Title: Random sampling of bandlimited signals on graphs Abstract: We study the problem of sampling k-bandlimited signals on graphs. We propose\ntwo sampling strategies that consist in selecting a small subset of nodes at\nrandom. The first strategy is non-adaptive, i.e., independent of the graph\nstructure, and its performance depends on a parameter called the graph\ncoherence. On the contrary, the second strategy is adaptive but yields optimal\nresults. Indeed, no more than O(k log(k)) measurements are sufficient to ensure\nan accurate and stable recovery of all k-bandlimited signals. This second\nstrategy is based on a careful choice of the sampling distribution, which can\nbe estimated quickly. Then, we propose a computationally efficient decoder to\nreconstruct k-bandlimited signals from their samples. We prove that it yields\naccurate reconstructions and that it is also stable to noise. Finally, we\nconduct several experiments to test these techniques. \n\n"}
{"id": "1511.05284", "contents": "Title: Deep Compositional Captioning: Describing Novel Object Categories\n  without Paired Training Data Abstract: While recent deep neural network models have achieved promising results on\nthe image captioning task, they rely largely on the availability of corpora\nwith paired image and sentence captions to describe objects in context. In this\nwork, we propose the Deep Compositional Captioner (DCC) to address the task of\ngenerating descriptions of novel objects which are not present in paired\nimage-sentence datasets. Our method achieves this by leveraging large object\nrecognition datasets and external text corpora and by transferring knowledge\nbetween semantically similar concepts. Current deep caption models can only\ndescribe objects contained in paired image-sentence corpora, despite the fact\nthat they are pre-trained with large object recognition datasets, namely\nImageNet. In contrast, our model can compose sentences that describe novel\nobjects and their interactions with other objects. We demonstrate our model's\nability to describe novel concepts by empirically evaluating its performance on\nMSCOCO and show qualitative results on ImageNet images of objects for which no\npaired image-caption data exist. Further, we extend our approach to generate\ndescriptions of objects in video clips. Our results show that DCC has distinct\nadvantages over existing image and video captioning approaches for generating\ndescriptions of new objects in context. \n\n"}
{"id": "1511.05641", "contents": "Title: Net2Net: Accelerating Learning via Knowledge Transfer Abstract: We introduce techniques for rapidly transferring the information stored in\none neural net into another neural net. The main purpose is to accelerate the\ntraining of a significantly larger neural net. During real-world workflows, one\noften trains very many different neural networks during the experimentation and\ndesign process. This is a wasteful process in which each new model is trained\nfrom scratch. Our Net2Net technique accelerates the experimentation process by\ninstantaneously transferring the knowledge from a previous network to each new\ndeeper or wider network. Our techniques are based on the concept of\nfunction-preserving transformations between neural network specifications. This\ndiffers from previous approaches to pre-training that altered the function\nrepresented by a neural net when adding layers to it. Using our knowledge\ntransfer mechanism to add depth to Inception modules, we demonstrate a new\nstate of the art accuracy rating on the ImageNet dataset. \n\n"}
{"id": "1511.05756", "contents": "Title: Image Question Answering using Convolutional Neural Network with Dynamic\n  Parameter Prediction Abstract: We tackle image question answering (ImageQA) problem by learning a\nconvolutional neural network (CNN) with a dynamic parameter layer whose weights\nare determined adaptively based on questions. For the adaptive parameter\nprediction, we employ a separate parameter prediction network, which consists\nof gated recurrent unit (GRU) taking a question as its input and a\nfully-connected layer generating a set of candidate weights as its output.\nHowever, it is challenging to construct a parameter prediction network for a\nlarge number of parameters in the fully-connected dynamic parameter layer of\nthe CNN. We reduce the complexity of this problem by incorporating a hashing\ntechnique, where the candidate weights given by the parameter prediction\nnetwork are selected using a predefined hash function to determine individual\nweights in the dynamic parameter layer. The proposed network---joint network\nwith the CNN for ImageQA and the parameter prediction network---is trained\nend-to-end through back-propagation, where its weights are initialized using a\npre-trained CNN and GRU. The proposed algorithm illustrates the\nstate-of-the-art performance on all available public ImageQA benchmarks. \n\n"}
{"id": "1511.06033", "contents": "Title: EigenRec: Generalizing PureSVD for Effective and Efficient Top-N\n  Recommendations Abstract: We introduce EigenRec; a versatile and efficient Latent-Factor framework for\nTop-N Recommendations that includes the well-known PureSVD algorithm as a\nspecial case. EigenRec builds a low dimensional model of an inter-item\nproximity matrix that combines a similarity component, with a scaling operator,\ndesigned to control the influence of the prior item popularity on the final\nmodel. Seeing PureSVD within our framework provides intuition about its inner\nworkings, exposes its inherent limitations, and also, paves the path towards\npainlessly improving its recommendation performance. A comprehensive set of\nexperiments on the MovieLens and the Yahoo datasets based on widely applied\nperformance metrics, indicate that EigenRec outperforms several\nstate-of-the-art algorithms, in terms of Standard and Long-Tail recommendation\naccuracy, exhibiting low susceptibility to sparsity, even in its most extreme\nmanifestations -- the Cold-Start problems. At the same time EigenRec has an\nattractive computational profile and it can apply readily in large-scale\nrecommendation settings. \n\n"}
{"id": "1511.06219", "contents": "Title: Knowledge Base Population using Semantic Label Propagation Abstract: A crucial aspect of a knowledge base population system that extracts new\nfacts from text corpora, is the generation of training data for its relation\nextractors. In this paper, we present a method that maximizes the effectiveness\nof newly trained relation extractors at a minimal annotation cost. Manual\nlabeling can be significantly reduced by Distant Supervision, which is a method\nto construct training data automatically by aligning a large text corpus with\nan existing knowledge base of known facts. For example, all sentences\nmentioning both 'Barack Obama' and 'US' may serve as positive training\ninstances for the relation born_in(subject,object). However, distant\nsupervision typically results in a highly noisy training set: many training\nsentences do not really express the intended relation. We propose to combine\ndistant supervision with minimal manual supervision in a technique called\nfeature labeling, to eliminate noise from the large and noisy initial training\nset, resulting in a significant increase of precision. We further improve on\nthis approach by introducing the Semantic Label Propagation method, which uses\nthe similarity between low-dimensional representations of candidate training\ninstances, to extend the training set in order to increase recall while\nmaintaining high precision. Our proposed strategy for generating training data\nis studied and evaluated on an established test collection designed for\nknowledge base population tasks. The experimental results show that the\nSemantic Label Propagation strategy leads to substantial performance gains when\ncompared to existing approaches, while requiring an almost negligible manual\nannotation effort. \n\n"}
{"id": "1511.06303", "contents": "Title: Alternative structures for character-level RNNs Abstract: Recurrent neural networks are convenient and efficient models for language\nmodeling. However, when applied on the level of characters instead of words,\nthey suffer from several problems. In order to successfully model long-term\ndependencies, the hidden representation needs to be large. This in turn implies\nhigher computational costs, which can become prohibitive in practice. We\npropose two alternative structural modifications to the classical RNN model.\nThe first one consists on conditioning the character level representation on\nthe previous word representation. The other one uses the character history to\ncondition the output probability. We evaluate the performance of the two\nproposed modifications on challenging, multi-lingual real world data. \n\n"}
{"id": "1511.06361", "contents": "Title: Order-Embeddings of Images and Language Abstract: Hypernymy, textual entailment, and image captioning can be seen as special\ncases of a single visual-semantic hierarchy over words, sentences, and images.\nIn this paper we advocate for explicitly modeling the partial order structure\nof this hierarchy. Towards this goal, we introduce a general method for\nlearning ordered representations, and show how it can be applied to a variety\nof tasks involving images and language. We show that the resulting\nrepresentations improve performance over current approaches for hypernym\nprediction and image-caption retrieval. \n\n"}
{"id": "1511.06396", "contents": "Title: Multilingual Relation Extraction using Compositional Universal Schema Abstract: Universal schema builds a knowledge base (KB) of entities and relations by\njointly embedding all relation types from input KBs as well as textual patterns\nexpressing relations from raw text. In most previous applications of universal\nschema, each textual pattern is represented as a single embedding, preventing\ngeneralization to unseen patterns. Recent work employs a neural network to\ncapture patterns' compositional semantics, providing generalization to all\npossible input text. In response, this paper introduces significant further\nimprovements to the coverage and flexibility of universal schema relation\nextraction: predictions for entities unseen in training and multilingual\ntransfer learning to domains with no annotation. We evaluate our model through\nextensive experiments on the English and Spanish TAC KBP benchmark,\noutperforming the top system from TAC 2013 slot-filling using no handwritten\npatterns or additional annotation. We also consider a multilingual setting in\nwhich English training data entities overlap with the seed KB, but Spanish text\ndoes not. Despite having no annotation for Spanish data, we train an accurate\npredictor, with additional improvements obtained by tying word embeddings\nacross languages. Furthermore, we find that multilingual training improves\nEnglish relation extraction accuracy. Our approach is thus suited to\nbroad-coverage automated knowledge base construction in a variety of languages\nand domains. \n\n"}
{"id": "1511.06397", "contents": "Title: Compressing Word Embeddings Abstract: Recent methods for learning vector space representations of words have\nsucceeded in capturing fine-grained semantic and syntactic regularities using\nvector arithmetic. However, these vector space representations (created through\nlarge-scale text analysis) are typically stored verbatim, since their internal\nstructure is opaque. Using word-analogy tests to monitor the level of detail\nstored in compressed re-representations of the same vector space, the\ntrade-offs between the reduction in memory usage and expressiveness are\ninvestigated. A simple scheme is outlined that can reduce the memory footprint\nof a state-of-the-art embedding by a factor of 10, with only minimal impact on\nperformance. Then, using the same `bit budget', a binary (approximate)\nfactorisation of the same space is also explored, with the aim of creating an\nequivalent representation with better interpretability. \n\n"}
{"id": "1511.06407", "contents": "Title: Recurrent Models for Auditory Attention in Multi-Microphone Distance\n  Speech Recognition Abstract: Integration of multiple microphone data is one of the key ways to achieve\nrobust speech recognition in noisy environments or when the speaker is located\nat some distance from the input device. Signal processing techniques such as\nbeamforming are widely used to extract a speech signal of interest from\nbackground noise. These techniques, however, are highly dependent on prior\nspatial information about the microphones and the environment in which the\nsystem is being used. In this work, we present a neural attention network that\ndirectly combines multi-channel audio to generate phonetic states without\nrequiring any prior knowledge of the microphone layout or any explicit signal\npreprocessing for speech enhancement. We embed an attention mechanism within a\nRecurrent Neural Network (RNN) based acoustic model to automatically tune its\nattention to a more reliable input source. Unlike traditional multi-channel\npreprocessing, our system can be optimized towards the desired output in one\nstep. Although attention-based models have recently achieved impressive results\non sequence-to-sequence learning, no attention mechanisms have previously been\napplied to learn potentially asynchronous and non-stationary multiple inputs.\nWe evaluate our neural attention model on the CHiME-3 challenge task, and show\nthat the model achieves comparable performance to beamforming using a purely\ndata-driven method. \n\n"}
{"id": "1511.06434", "contents": "Title: Unsupervised Representation Learning with Deep Convolutional Generative\n  Adversarial Networks Abstract: In recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and\nunsupervised learning. We introduce a class of CNNs called deep convolutional\ngenerative adversarial networks (DCGANs), that have certain architectural\nconstraints, and demonstrate that they are a strong candidate for unsupervised\nlearning. Training on various image datasets, we show convincing evidence that\nour deep convolutional adversarial pair learns a hierarchy of representations\nfrom object parts to scenes in both the generator and discriminator.\nAdditionally, we use the learned features for novel tasks - demonstrating their\napplicability as general image representations. \n\n"}
{"id": "1511.06464", "contents": "Title: Unitary Evolution Recurrent Neural Networks Abstract: Recurrent neural networks (RNNs) are notoriously difficult to train. When the\neigenvalues of the hidden to hidden weight matrix deviate from absolute value\n1, optimization becomes difficult due to the well studied issue of vanishing\nand exploding gradients, especially when trying to learn long-term\ndependencies. To circumvent this problem, we propose a new architecture that\nlearns a unitary weight matrix, with eigenvalues of absolute value exactly 1.\nThe challenge we address is that of parametrizing unitary matrices in a way\nthat does not require expensive computations (such as eigendecomposition) after\neach weight update. We construct an expressive unitary weight matrix by\ncomposing several structured matrices that act as building blocks with\nparameters to be learned. Optimization with this parameterization becomes\nfeasible only when considering hidden states in the complex domain. We\ndemonstrate the potential of this architecture by achieving state of the art\nresults in several hard tasks involving very long-term dependencies. \n\n"}
{"id": "1511.06909", "contents": "Title: BlackOut: Speeding up Recurrent Neural Network Language Models With Very\n  Large Vocabularies Abstract: We propose BlackOut, an approximation algorithm to efficiently train massive\nrecurrent neural network language models (RNNLMs) with million word\nvocabularies. BlackOut is motivated by using a discriminative loss, and we\ndescribe a new sampling strategy which significantly reduces computation while\nimproving stability, sample efficiency, and rate of convergence. One way to\nunderstand BlackOut is to view it as an extension of the DropOut strategy to\nthe output layer, wherein we use a discriminative training loss and a weighted\nsampling scheme. We also establish close connections between BlackOut,\nimportance sampling, and noise contrastive estimation (NCE). Our experiments,\non the recently released one billion word language modeling benchmark,\ndemonstrate scalability and accuracy of BlackOut; we outperform the\nstate-of-the art, and achieve the lowest perplexity scores on this dataset.\nMoreover, unlike other established methods which typically require GPUs or CPU\nclusters, we show that a carefully implemented version of BlackOut requires\nonly 1-10 days on a single machine to train a RNNLM with a million word\nvocabulary and billions of parameters on one billion words. Although we\ndescribe BlackOut in the context of RNNLM training, it can be used to any\nnetworks with large softmax output layers. \n\n"}
{"id": "1511.08762", "contents": "Title: Informative Data Projections: A Framework and Two Examples Abstract: Methods for Projection Pursuit aim to facilitate the visual exploration of\nhigh-dimensional data by identifying interesting low-dimensional projections. A\nmajor challenge is the design of a suitable quality metric of projections,\ncommonly referred to as the projection index, to be maximized by the Projection\nPursuit algorithm. In this paper, we introduce a new information-theoretic\nstrategy for tackling this problem, based on quantifying the amount of\ninformation the projection conveys to a user given their prior beliefs about\nthe data. The resulting projection index is a subjective quantity, explicitly\ndependent on the intended user. As a useful illustration, we developed this\nidea for two particular kinds of prior beliefs. The first kind leads to PCA\n(Principal Component Analysis), shining new light on when PCA is (not)\nappropriate. The second kind leads to a novel projection index, the\nmaximization of which can be regarded as a robust variant of PCA. We show how\nthis projection index, though non-convex, can be effectively maximized using a\nmodified power method as well as using a semidefinite programming relaxation.\nThe usefulness of this new projection index is demonstrated in comparative\nempirical experiments against PCA and a popular Projection Pursuit method. \n\n"}
{"id": "1511.09249", "contents": "Title: On Learning to Think: Algorithmic Information Theory for Novel\n  Combinations of Reinforcement Learning Controllers and Recurrent Neural World\n  Models Abstract: This paper addresses the general problem of reinforcement learning (RL) in\npartially observable environments. In 2013, our large RL recurrent neural\nnetworks (RNNs) learned from scratch to drive simulated cars from\nhigh-dimensional video input. However, real brains are more powerful in many\nways. In particular, they learn a predictive model of their initially unknown\nenvironment, and somehow use it for abstract (e.g., hierarchical) planning and\nreasoning. Guided by algorithmic information theory, we describe RNN-based AIs\n(RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending\nsequences of tasks, some of them provided by the user, others invented by the\nRNNAI itself in a curious, playful fashion, to improve its RNN-based world\nmodel. Unlike our previous model-building RNN-based RL machines dating back to\n1990, the RNNAI learns to actively query its model for abstract reasoning and\nplanning and decision making, essentially \"learning to think.\" The basic ideas\nof this report can be applied to many other cases where one RNN-like system\nexploits the algorithmic information content of another. They are taken from a\ngrant proposal submitted in Fall 2014, and also explain concepts such as\n\"mirror neurons.\" Experimental results will be described in separate papers. \n\n"}
{"id": "1512.02433", "contents": "Title: Minimum Risk Training for Neural Machine Translation Abstract: We propose minimum risk training for end-to-end neural machine translation.\nUnlike conventional maximum likelihood estimation, minimum risk training is\ncapable of optimizing model parameters directly with respect to arbitrary\nevaluation metrics, which are not necessarily differentiable. Experiments show\nthat our approach achieves significant improvements over maximum likelihood\nestimation on a state-of-the-art neural machine translation system across\nvarious languages pairs. Transparent to architectures, our approach can be\napplied to more neural networks and potentially benefit more NLP tasks. \n\n"}
{"id": "1512.03385", "contents": "Title: Deep Residual Learning for Image Recognition Abstract: Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation. \n\n"}
{"id": "1512.04036", "contents": "Title: Tracking Idea Flows between Social Groups Abstract: In many applications, ideas that are described by a set of words often flow\nbetween different groups. To facilitate users in analyzing the flow, we present\na method to model the flow behaviors that aims at identifying the lead-lag\nrelationships between word clusters of different user groups. In particular, an\nimproved Bayesian conditional cointegration based on dynamic time warping is\nemployed to learn links between words in different groups. A tensor-based\ntechnique is developed to cluster these linked words into different clusters\n(ideas) and track the flow of ideas. The main feature of the tensor\nrepresentation is that we introduce two additional dimensions to represent both\ntime and lead-lag relationships. Experiments on both synthetic and real\ndatasets show that our method is more effective than methods based on\ntraditional clustering techniques and achieves better accuracy. A case study\nwas conducted to demonstrate the usefulness of our method in helping users\nunderstand the flow of ideas between different user groups on social media \n\n"}
{"id": "1512.04633", "contents": "Title: Efficient Algorithms for Personalized PageRank Abstract: We present new, more efficient algorithms for estimating random walk scores\nsuch as Personalized PageRank from a given source node to one or several target\nnodes. These scores are useful for personalized search and recommendations on\nnetworks including social networks, user-item networks, and the web. Past work\nhas proposed using Monte Carlo or using linear algebra to estimate scores from\na single source to every target, making them inefficient for a single pair. Our\ncontribution is a new bidirectional algorithm which combines linear algebra and\nMonte Carlo to achieve significant speed improvements. On a diverse set of six\ngraphs, our algorithm is 70x faster than past state-of-the-art algorithms. We\nalso present theoretical analysis: while past algorithms require $\\Omega(n)$\ntime to estimate a random walk score of typical size $\\frac{1}{n}$ on an\n$n$-node graph to a given constant accuracy, our algorithm requires only\n$O(\\sqrt{m})$ expected time for an average target, where $m$ is the number of\nedges, and is provably accurate.\n  In addition to our core bidirectional estimator for personalized PageRank, we\npresent an alternative algorithm for undirected graphs, a generalization to\narbitrary walk lengths and Markov Chains, an algorithm for personalized search\nranking, and an algorithm for sampling random paths from a given source to a\ngiven set of targets. We expect our bidirectional methods can be extended in\nother ways and will be useful subroutines in other graph analysis problems. \n\n"}
{"id": "1512.05742", "contents": "Title: A Survey of Available Corpora for Building Data-Driven Dialogue Systems Abstract: During the past decade, several areas of speech and language understanding\nhave witnessed substantial breakthroughs from the use of data-driven models. In\nthe area of dialogue systems, the trend is less obvious, and most practical\nsystems are still built through significant engineering and expert knowledge.\nNevertheless, several recent results suggest that data-driven approaches are\nfeasible and quite promising. To facilitate research in this area, we have\ncarried out a wide survey of publicly available datasets suitable for\ndata-driven learning of dialogue systems. We discuss important characteristics\nof these datasets, how they can be used to learn diverse dialogue strategies,\nand their other potential uses. We also examine methods for transfer learning\nbetween datasets and the use of external knowledge. Finally, we discuss\nappropriate choice of evaluation metrics for the learning objective. \n\n"}
{"id": "1512.06785", "contents": "Title: Beyond Classification: Latent User Interests Profiling from Visual\n  Contents Analysis Abstract: User preference profiling is an important task in modern online social\nnetworks (OSN). With the proliferation of image-centric social platforms, such\nas Pinterest, visual contents have become one of the most informative data\nstreams for understanding user preferences. Traditional approaches usually\ntreat visual content analysis as a general classification problem where one or\nmore labels are assigned to each image. Although such an approach simplifies\nthe process of image analysis, it misses the rich context and visual cues that\nplay an important role in people's perception of images. In this paper, we\nexplore the possibilities of learning a user's latent visual preferences\ndirectly from image contents. We propose a distance metric learning method\nbased on Deep Convolutional Neural Networks (CNN) to directly extract\nsimilarity information from visual contents and use the derived distance metric\nto mine individual users' fine-grained visual preferences. Through our\npreliminary experiments using data from 5,790 Pinterest users, we show that\neven for the images within the same category, each user possesses distinct and\nindividually-identifiable visual preferences that are consistent over their\nlifetime. Our results underscore the untapped potential of finer-grained visual\npreference profiling in understanding users' preferences. \n\n"}
{"id": "1601.01356", "contents": "Title: From Word Embeddings to Item Recommendation Abstract: Social network platforms can use the data produced by their users to serve\nthem better. One of the services these platforms provide is recommendation\nservice. Recommendation systems can predict the future preferences of users\nusing their past preferences. In the recommendation systems literature there\nare various techniques, such as neighborhood based methods, machine-learning\nbased methods and matrix-factorization based methods. In this work, a set of\nwell known methods from natural language processing domain, namely Word2Vec, is\napplied to recommendation systems domain. Unlike previous works that use\nWord2Vec for recommendation, this work uses non-textual features, the\ncheck-ins, and it recommends venues to visit/check-in to the target users. For\nthe experiments, a Foursquare check-in dataset is used. The results show that\nuse of continuous vector space representations of items modeled by techniques\nof Word2Vec is promising for making recommendations. \n\n"}
{"id": "1601.05775", "contents": "Title: Local Network Community Detection with Continuous Optimization of\n  Conductance and Weighted Kernel K-Means Abstract: Local network community detection is the task of finding a single community\nof nodes concentrated around few given seed nodes in a localized way.\nConductance is a popular objective function used in many algorithms for local\ncommunity detection. This paper studies a continuous relaxation of conductance.\nWe show that continuous optimization of this objective still leads to discrete\ncommunities. We investigate the relation of conductance with weighted kernel\nk-means for a single community, which leads to the introduction of a new\nobjective function, $\\sigma$-conductance. Conductance is obtained by setting\n$\\sigma$ to $0$. Two algorithms, EMc and PGDc, are proposed to locally optimize\n$\\sigma$-conductance and automatically tune the parameter $\\sigma$. They are\nbased on expectation maximization and projected gradient descent, respectively.\nWe prove locality and give performance guarantees for EMc and PGDc for a class\nof dense and well separated communities centered around the seeds. Experiments\nare conducted on networks with ground-truth communities, comparing to\nstate-of-the-art graph diffusion algorithms for conductance optimization. On\nlarge graphs, results indicate that EMc and PGDc stay localized and produce\ncommunities most similar to the ground, while graph diffusion algorithms\ngenerate large communities of lower quality. \n\n"}
{"id": "1601.06551", "contents": "Title: Robust Influence Maximization Abstract: In this paper, we address the important issue of uncertainty in the edge\ninfluence probability estimates for the well studied influence maximization\nproblem --- the task of finding $k$ seed nodes in a social network to maximize\nthe influence spread. We propose the problem of robust influence maximization,\nwhich maximizes the worst-case ratio between the influence spread of the chosen\nseed set and the optimal seed set, given the uncertainty of the parameter\ninput. We design an algorithm that solves this problem with a\nsolution-dependent bound. We further study uniform sampling and adaptive\nsampling methods to effectively reduce the uncertainty on parameters and\nimprove the robustness of the influence maximization task. Our empirical\nresults show that parameter uncertainty may greatly affect influence\nmaximization performance and prior studies that learned influence probabilities\ncould lead to poor performance in robust influence maximization due to\nrelatively large uncertainty in parameter estimates, and information cascade\nbased adaptive sampling method may be an effective way to improve the\nrobustness of influence maximization. \n\n"}
{"id": "1601.06581", "contents": "Title: Character-Level Incremental Speech Recognition with Recurrent Neural\n  Networks Abstract: In real-time speech recognition applications, the latency is an important\nissue. We have developed a character-level incremental speech recognition (ISR)\nsystem that responds quickly even during the speech, where the hypotheses are\ngradually improved while the speaking proceeds. The algorithm employs a\nspeech-to-character unidirectional recurrent neural network (RNN), which is\nend-to-end trained with connectionist temporal classification (CTC), and an\nRNN-based character-level language model (LM). The output values of the\nCTC-trained RNN are character-level probabilities, which are processed by beam\nsearch decoding. The RNN LM augments the decoding by providing long-term\ndependency information. We propose tree-based online beam search with\nadditional depth-pruning, which enables the system to process infinitely long\ninput speech with low latency. This system not only responds quickly on speech\nbut also can dictate out-of-vocabulary (OOV) words according to pronunciation.\nThe proposed model achieves the word error rate (WER) of 8.90% on the Wall\nStreet Journal (WSJ) Nov'92 20K evaluation set when trained on the WSJ SI-284\ntraining set. \n\n"}
{"id": "1602.00426", "contents": "Title: An Iterative Deep Learning Framework for Unsupervised Discovery of\n  Speech Features and Linguistic Units with Applications on Spoken Term\n  Detection Abstract: In this work we aim to discover high quality speech features and linguistic\nunits directly from unlabeled speech data in a zero resource scenario. The\nresults are evaluated using the metrics and corpora proposed in the Zero\nResource Speech Challenge organized at Interspeech 2015. A Multi-layered\nAcoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets\nof acoustic tokens from the given corpus. Each acoustic token set is specified\nby a set of hyperparameters that describe the model configuration. These sets\nof acoustic tokens carry different characteristics fof the given corpus and the\nlanguage behind, thus can be mutually reinforced. The multiple sets of token\nlabels are then used as the targets of a Multi-target Deep Neural Network\n(MDNN) trained on low-level acoustic features. Bottleneck features extracted\nfrom the MDNN are then used as the feedback input to the MAT and the MDNN\nitself in the next iteration. We call this iterative deep learning framework\nthe Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), which\ngenerates both high quality speech features for the Track 1 of the Challenge\nand acoustic tokens for the Track 2 of the Challenge. In addition, we performed\nextra experiments on the same corpora on the application of query-by-example\nspoken term detection. The experimental results showed the iterative deep\nlearning framework of MAT-DNN improved the detection performance due to better\nunderlying speech features and acoustic tokens. \n\n"}
{"id": "1602.02068", "contents": "Title: From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label\n  Classification Abstract: We propose sparsemax, a new activation function similar to the traditional\nsoftmax, but able to output sparse probabilities. After deriving its\nproperties, we show how its Jacobian can be efficiently computed, enabling its\nuse in a network trained with backpropagation. Then, we propose a new smooth\nand convex loss function which is the sparsemax analogue of the logistic loss.\nWe reveal an unexpected connection between this new loss and the Huber\nclassification loss. We obtain promising empirical results in multi-label\nclassification problems and in attention-based neural networks for natural\nlanguage inference. For the latter, we achieve a similar performance as the\ntraditional softmax, but with a selective, more compact, attention focus. \n\n"}
{"id": "1602.02215", "contents": "Title: Swivel: Improving Embeddings by Noticing What's Missing Abstract: We present Submatrix-wise Vector Embedding Learner (Swivel), a method for\ngenerating low-dimensional feature embeddings from a feature co-occurrence\nmatrix. Swivel performs approximate factorization of the point-wise mutual\ninformation matrix via stochastic gradient descent. It uses a piecewise loss\nwith special handling for unobserved co-occurrences, and thus makes use of all\nthe information in the matrix. While this requires computation proportional to\nthe size of the entire matrix, we make use of vectorized multiplication to\nprocess thousands of rows and columns at once to compute millions of predicted\nvalues. Furthermore, we partition the matrix into shards in order to\nparallelize the computation across many nodes. This approach results in more\naccurate embeddings than can be achieved with methods that consider only\nobserved co-occurrences, and can scale to much larger corpora than can be\nhandled with sampling methods. \n\n"}
{"id": "1602.02255", "contents": "Title: Deep Cross-Modal Hashing Abstract: Due to its low storage cost and fast query speed, cross-modal hashing (CMH)\nhas been widely used for similarity search in multimedia retrieval\napplications. However, almost all existing CMH methods are based on\nhand-crafted features which might not be optimally compatible with the\nhash-code learning procedure. As a result, existing CMH methods with\nhandcrafted features may not achieve satisfactory performance. In this paper,\nwe propose a novel cross-modal hashing method, called deep crossmodal hashing\n(DCMH), by integrating feature learning and hash-code learning into the same\nframework. DCMH is an end-to-end learning framework with deep neural networks,\none for each modality, to perform feature learning from scratch. Experiments on\ntwo real datasets with text-image modalities show that DCMH can outperform\nother baselines to achieve the state-of-the-art performance in cross-modal\nretrieval applications. \n\n"}
{"id": "1602.02373", "contents": "Title: Supervised and Semi-Supervised Text Categorization using LSTM for Region\n  Embeddings Abstract: One-hot CNN (convolutional neural network) has been shown to be effective for\ntext categorization (Johnson & Zhang, 2015). We view it as a special case of a\ngeneral framework which jointly trains a linear model with a non-linear feature\ngenerator consisting of `text region embedding + pooling'. Under this\nframework, we explore a more sophisticated region embedding method using Long\nShort-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly\nlarge) sizes, whereas the region size needs to be fixed in a CNN. We seek\neffective and efficient use of LSTM for this purpose in the supervised and\nsemi-supervised settings. The best results were obtained by combining region\nembeddings in the form of LSTM and convolution layers trained on unlabeled\ndata. The results indicate that on this task, embeddings of text regions, which\ncan convey complex concepts, are more useful than embeddings of single words in\nisolation. We report performances exceeding the previous best results on four\nbenchmark datasets. \n\n"}
{"id": "1602.02410", "contents": "Title: Exploring the Limits of Language Modeling Abstract: In this work we explore recent advances in Recurrent Neural Networks for\nlarge scale Language Modeling, a task central to language understanding. We\nextend current models to deal with two key challenges present in this task:\ncorpora and vocabulary sizes, and complex, long term structure of language. We\nperform an exhaustive study on techniques such as character Convolutional\nNeural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.\nOur best single model significantly improves state-of-the-art perplexity from\n51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),\nwhile an ensemble of models sets a new record by improving perplexity from 41.0\ndown to 23.7. We also release these models for the NLP and ML community to\nstudy and improve upon. \n\n"}
{"id": "1602.02850", "contents": "Title: Toward Optimal Feature Selection in Naive Bayes for Text Categorization Abstract: Automated feature selection is important for text categorization to reduce\nthe feature size and to speed up the learning process of classifiers. In this\npaper, we present a novel and efficient feature selection framework based on\nthe Information Theory, which aims to rank the features with their\ndiscriminative capacity for classification. We first revisit two information\nmeasures: Kullback-Leibler divergence and Jeffreys divergence for binary\nhypothesis testing, and analyze their asymptotic properties relating to type I\nand type II errors of a Bayesian classifier. We then introduce a new divergence\nmeasure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure\nmulti-distribution divergence for multi-class classification. Based on the\nJMH-divergence, we develop two efficient feature selection methods, termed\nmaximum discrimination ($MD$) and $MD-\\chi^2$ methods, for text categorization.\nThe promising results of extensive experiments demonstrate the effectiveness of\nthe proposed approaches. \n\n"}
{"id": "1602.03001", "contents": "Title: A Convolutional Attention Network for Extreme Summarization of Source\n  Code Abstract: Attention mechanisms in neural networks have proved useful for problems in\nwhich the input and output do not have fixed dimension. Often there exist\nfeatures that are locally translation invariant and would be valuable for\ndirecting the model's attention, but previous attentional architectures are not\nconstructed to learn such features specifically. We introduce an attentional\nneural network that employs convolution on the input tokens to detect local\ntime-invariant and long-range topical attention features in a context-dependent\nway. We apply this architecture to the problem of extreme summarization of\nsource code snippets into short, descriptive function name-like summaries.\nUsing those features, the model sequentially generates a summary by\nmarginalizing over two attention mechanisms: one that predicts the next summary\ntoken based on the attention weights of the input tokens and another that is\nable to copy a code token as-is directly into the summary. We demonstrate our\nconvolutional attention neural network's performance on 10 popular Java\nprojects showing that it achieves better performance compared to previous\nattentional mechanisms. \n\n"}
{"id": "1602.03483", "contents": "Title: Learning Distributed Representations of Sentences from Unlabelled Data Abstract: Unsupervised methods for learning distributed representations of words are\nubiquitous in today's NLP research, but far less is known about the best ways\nto learn distributed phrase or sentence representations from unlabelled data.\nThis paper is a systematic comparison of models that learn such\nrepresentations. We find that the optimal approach depends critically on the\nintended application. Deeper, more complex models are preferable for\nrepresentations to be used in supervised systems, but shallow log-linear models\nwork best for building representation spaces that can be decoded with simple\nspatial distance metrics. We also propose two new unsupervised\nrepresentation-learning objectives designed to optimise the trade-off between\ntraining time, domain portability and performance. \n\n"}
{"id": "1602.04983", "contents": "Title: Contextual Media Retrieval Using Natural Language Queries Abstract: The widespread integration of cameras in hand-held and head-worn devices as\nwell as the ability to share content online enables a large and diverse visual\ncapture of the world that millions of users build up collectively every day. We\nenvision these images as well as associated meta information, such as GPS\ncoordinates and timestamps, to form a collective visual memory that can be\nqueried while automatically taking the ever-changing context of mobile users\ninto account. As a first step towards this vision, in this work we present\nXplore-M-Ego: a novel media retrieval system that allows users to query a\ndynamic database of images and videos using spatio-temporal natural language\nqueries. We evaluate our system using a new dataset of real user queries as\nwell as through a usability study. One key finding is that there is a\nconsiderable amount of inter-user variability, for example in the resolution of\nspatial relations in natural language utterances. We show that our retrieval\nsystem can cope with this variability using personalisation through an online\nlearning-based retrieval formulation. \n\n"}
{"id": "1602.05157", "contents": "Title: A Ranking Algorithm for Re-finding Abstract: Re-finding files from a personal computer is a frequent demand to users. When\nencountered a difficult re-finding task, people may not recall the attributes\nused by conventional re-finding methods, such as a file's path, file name,\nkeywords etc., the re-finding would fail.\n  We proposed a method to support difficult re-finding tasks. By asking the\nuser a list of questions about the target, such as a document's pages, author\nnumbers, accumulated reading time, last reading location etc. Then use the\nuser's answers to filter out the target.\n  After the user answered a list of questions about the target file, we\nevaluate the user's familiar degree about the target file based on the answers.\nWe devise a ranking algorithm which sorts the candidates by comparing the\nuser's familiarity degree about the target and the candidates.\n  We also propose a method to generate re-finding tasks artificially based on\nthe user's own document corpus. \n\n"}
{"id": "1602.05307", "contents": "Title: Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label\n  Embedding Abstract: Current systems of fine-grained entity typing use distant supervision in\nconjunction with existing knowledge bases to assign categories (type labels) to\nentity mentions. However, the type labels so obtained from knowledge bases are\noften noisy (i.e., incorrect for the entity mention's local context). We define\na new task, Label Noise Reduction in Entity Typing (LNR), to be the automatic\nidentification of correct type labels (type-paths) for training examples, given\nthe set of candidate type labels obtained by distant supervision with a given\ntype hierarchy. The unknown type labels for individual entity mentions and the\nsemantic similarity between entity types pose unique challenges for solving the\nLNR task. We propose a general framework, called PLE, to jointly embed entity\nmentions, text features and entity types into the same low-dimensional space\nwhere, in that space, objects whose types are semantically close have similar\nrepresentations. Then we estimate the type-path for each training example in a\ntop-down manner using the learned embeddings. We formulate a global objective\nfor learning the embeddings from text corpora and knowledge bases, which adopts\na novel margin-based loss that is robust to noisy labels and faithfully models\ntype correlation derived from knowledge bases. Our experiments on three public\ntyping datasets demonstrate the effectiveness and robustness of PLE, with an\naverage of 25% improvement in accuracy compared to next best method. \n\n"}
{"id": "1602.05307", "contents": "Title: Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label\n  Embedding Abstract: Current systems of fine-grained entity typing use distant supervision in\nconjunction with existing knowledge bases to assign categories (type labels) to\nentity mentions. However, the type labels so obtained from knowledge bases are\noften noisy (i.e., incorrect for the entity mention's local context). We define\na new task, Label Noise Reduction in Entity Typing (LNR), to be the automatic\nidentification of correct type labels (type-paths) for training examples, given\nthe set of candidate type labels obtained by distant supervision with a given\ntype hierarchy. The unknown type labels for individual entity mentions and the\nsemantic similarity between entity types pose unique challenges for solving the\nLNR task. We propose a general framework, called PLE, to jointly embed entity\nmentions, text features and entity types into the same low-dimensional space\nwhere, in that space, objects whose types are semantically close have similar\nrepresentations. Then we estimate the type-path for each training example in a\ntop-down manner using the learned embeddings. We formulate a global objective\nfor learning the embeddings from text corpora and knowledge bases, which adopts\na novel margin-based loss that is robust to noisy labels and faithfully models\ntype correlation derived from knowledge bases. Our experiments on three public\ntyping datasets demonstrate the effectiveness and robustness of PLE, with an\naverage of 25% improvement in accuracy compared to next best method. \n\n"}
{"id": "1602.07019", "contents": "Title: Sentence Similarity Learning by Lexical Decomposition and Composition Abstract: Most conventional sentence similarity methods only focus on similar parts of\ntwo input sentences, and simply ignore the dissimilar parts, which usually give\nus some clues and semantic meanings about the sentences. In this work, we\npropose a model to take into account both the similarities and dissimilarities\nby decomposing and composing lexical semantics over sentences. The model\nrepresents each word as a vector, and calculates a semantic matching vector for\neach word based on all words in the other sentence. Then, each word vector is\ndecomposed into a similar component and a dissimilar component based on the\nsemantic matching vector. After this, a two-channel CNN model is employed to\ncapture features by composing the similar and dissimilar components. Finally, a\nsimilarity score is estimated over the composed feature vectors. Experimental\nresults show that our model gets the state-of-the-art performance on the answer\nsentence selection task, and achieves a comparable result on the paraphrase\nidentification task. \n\n"}
{"id": "1603.00106", "contents": "Title: Characterizing Diseases from Unstructured Text: A Vocabulary Driven\n  Word2vec Approach Abstract: Traditional disease surveillance can be augmented with a wide variety of\nreal-time sources such as, news and social media. However, these sources are in\ngeneral unstructured and, construction of surveillance tools such as\ntaxonomical correlations and trace mapping involves considerable human\nsupervision. In this paper, we motivate a disease vocabulary driven word2vec\nmodel (Dis2Vec) to model diseases and constituent attributes as word embeddings\nfrom the HealthMap news corpus. We use these word embeddings to automatically\ncreate disease taxonomies and evaluate our model against corresponding human\nannotated taxonomies. We compare our model accuracies against several\nstate-of-the art word2vec methods. Our results demonstrate that Dis2Vec\noutperforms traditional distributed vector representations in its ability to\nfaithfully capture taxonomical attributes across different class of diseases\nsuch as endemic, emerging and rare. \n\n"}
{"id": "1603.01232", "contents": "Title: Multi-domain Neural Network Language Generation for Spoken Dialogue\n  Systems Abstract: Moving from limited-domain natural language generation (NLG) to open domain\nis difficult because the number of semantic input combinations grows\nexponentially with the number of domains. Therefore, it is important to\nleverage existing resources and exploit similarities between domains to\nfacilitate domain adaptation. In this paper, we propose a procedure to train\nmulti-domain, Recurrent Neural Network-based (RNN) language generators via\nmultiple adaptation steps. In this procedure, a model is first trained on\ncounterfeited data synthesised from an out-of-domain dataset, and then fine\ntuned on a small set of in-domain utterances with a discriminative objective\nfunction. Corpus-based evaluation results show that the proposed procedure can\nachieve competitive performance in terms of BLEU score and slot error rate\nwhile significantly reducing the data needed to train generators in new, unseen\ndomains. In subjective testing, human judges confirm that the procedure greatly\nimproves generator performance when only a small amount of data is available in\nthe domain. \n\n"}
{"id": "1603.01354", "contents": "Title: End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF Abstract: State-of-the-art sequence labeling systems traditionally require large\namounts of task-specific knowledge in the form of hand-crafted features and\ndata pre-processing. In this paper, we introduce a novel neutral network\narchitecture that benefits from both word- and character-level representations\nautomatically, by using combination of bidirectional LSTM, CNN and CRF. Our\nsystem is truly end-to-end, requiring no feature engineering or data\npre-processing, thus making it applicable to a wide range of sequence labeling\ntasks. We evaluate our system on two data sets for two sequence labeling tasks\n--- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003\ncorpus for named entity recognition (NER). We obtain state-of-the-art\nperformance on both the two data --- 97.55\\% accuracy for POS tagging and\n91.21\\% F1 for NER. \n\n"}
{"id": "1603.01547", "contents": "Title: Text Understanding with the Attention Sum Reader Network Abstract: Several large cloze-style context-question-answer datasets have been\nintroduced recently: the CNN and Daily Mail news data and the Children's Book\nTest. Thanks to the size of these datasets, the associated text comprehension\ntask is well suited for deep-learning techniques that currently seem to\noutperform all alternative approaches. We present a new, simple model that uses\nattention to directly pick the answer from the context as opposed to computing\nthe answer using a blended representation of words in the document as is usual\nin similar models. This makes the model particularly suitable for\nquestion-answering problems where the answer is a single word from the\ndocument. Ensemble of our models sets new state of the art on all evaluated\ndatasets. \n\n"}
{"id": "1603.04595", "contents": "Title: Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval Abstract: The goal of this work is the computation of very compact binary hashes for\nimage instance retrieval. Our approach has two novel contributions. The first\none is Nested Invariance Pooling (NIP), a method inspired from i-theory, a\nmathematical theory for computing group invariant transformations with\nfeed-forward neural networks. NIP is able to produce compact and\nwell-performing descriptors with visual representations extracted from\nconvolutional neural networks. We specifically incorporate scale, translation\nand rotation invariances but the scheme can be extended to any arbitrary sets\nof transformations. We also show that using moments of increasing order\nthroughout nesting is important. The NIP descriptors are then hashed to the\ntarget code size (32-256 bits) with a Restricted Boltzmann Machine with a novel\nbatch-level regularization scheme specifically designed for the purpose of\nhashing (RBMH). A thorough empirical evaluation with state-of-the-art shows\nthat the results obtained both with the NIP descriptors and the NIP+RBMH hashes\nare consistently outstanding across a wide range of datasets. \n\n"}
{"id": "1603.05118", "contents": "Title: Recurrent Dropout without Memory Loss Abstract: This paper presents a novel approach to recurrent neural network (RNN)\nregularization. Differently from the widely adopted dropout method, which is\napplied to \\textit{forward} connections of feed-forward architectures or RNNs,\nwe propose to drop neurons directly in \\textit{recurrent} connections in a way\nthat does not cause loss of long-term memory. Our approach is as easy to\nimplement and apply as the regular feed-forward dropout and we demonstrate its\neffectiveness for Long Short-Term Memory network, the most popular type of RNN\ncells. Our experiments on NLP benchmarks show consistent improvements even when\ncombined with conventional feed-forward dropout. \n\n"}
{"id": "1603.06038", "contents": "Title: Tensor Methods and Recommender Systems Abstract: A substantial progress in development of new and efficient tensor\nfactorization techniques has led to an extensive research of their\napplicability in recommender systems field. Tensor-based recommender models\npush the boundaries of traditional collaborative filtering techniques by taking\ninto account a multifaceted nature of real environments, which allows to\nproduce more accurate, situational (e.g. context-aware, criteria-driven)\nrecommendations. Despite the promising results, tensor-based methods are poorly\ncovered in existing recommender systems surveys. This survey aims to complement\nprevious works and provide a comprehensive overview on the subject. To the best\nof our knowledge, this is the first attempt to consolidate studies from various\napplication domains in an easily readable, digestible format, which helps to\nget a notion of the current state of the field. We also provide a high level\ndiscussion of the future perspectives and directions for further improvement of\ntensor-based recommendation systems. \n\n"}
{"id": "1603.06059", "contents": "Title: Generating Natural Questions About an Image Abstract: There has been an explosion of work in the vision & language community during\nthe past few years from image captioning to video transcription, and answering\nquestions about images. These tasks have focused on literal descriptions of the\nimage. To move beyond the literal, we choose to explore how questions about an\nimage are often directed at commonsense inference and the abstract events\nevoked by objects in the image. In this paper, we introduce the novel task of\nVisual Question Generation (VQG), where the system is tasked with asking a\nnatural and engaging question when shown an image. We provide three datasets\nwhich cover a variety of images from object-centric to event-centric, with\nconsiderably more abstract training data than provided to state-of-the-art\ncaptioning systems thus far. We train and test several generative and retrieval\nmodels to tackle the task of VQG. Evaluation results show that while such\nmodels ask reasonable questions for a variety of images, there is still a wide\ngap with human performance which motivates further work on connecting images\nwith commonsense knowledge and pragmatics. Our proposed task offers a new\nchallenge to the community which we hope furthers interest in exploring deeper\nconnections between vision & language. \n\n"}
{"id": "1603.06270", "contents": "Title: Multi-Task Cross-Lingual Sequence Tagging from Scratch Abstract: We present a deep hierarchical recurrent neural network for sequence tagging.\nGiven a sequence of words, our model employs deep gated recurrent units on both\ncharacter and word levels to encode morphology and context information, and\napplies a conditional random field layer to predict the tags. Our model is task\nindependent, language independent, and feature engineering free. We further\nextend our model to multi-task and cross-lingual joint training by sharing the\narchitecture and parameters. Our model achieves state-of-the-art results in\nmultiple languages on several benchmark tasks including POS tagging, chunking,\nand NER. We also demonstrate that multi-task and cross-lingual joint training\ncan improve the performance in various cases. \n\n"}
{"id": "1603.06679", "contents": "Title: Recursive Neural Conditional Random Fields for Aspect-based Sentiment\n  Analysis Abstract: In aspect-based sentiment analysis, extracting aspect terms along with the\nopinions being expressed from user-generated content is one of the most\nimportant subtasks. Previous studies have shown that exploiting connections\nbetween aspect and opinion terms is promising for this task. In this paper, we\npropose a novel joint model that integrates recursive neural networks and\nconditional random fields into a unified framework for explicit aspect and\nopinion terms co-extraction. The proposed model learns high-level\ndiscriminative features and double propagate information between aspect and\nopinion terms, simultaneously. Moreover, it is flexible to incorporate\nhand-crafted features into the proposed model to further boost its information\nextraction performance. Experimental results on the SemEval Challenge 2014\ndataset show the superiority of our proposed model over several baseline\nmethods as well as the winning systems of the challenge. \n\n"}
{"id": "1603.07771", "contents": "Title: Neural Text Generation from Structured Data with Application to the\n  Biography Domain Abstract: This paper introduces a neural model for concept-to-text generation that\nscales to large, rich domains. We experiment with a new dataset of biographies\nfrom Wikipedia that is an order of magnitude larger than existing resources\nwith over 700k samples. The dataset is also vastly more diverse with a 400k\nvocabulary, compared to a few hundred words for Weathergov or Robocup. Our\nmodel builds upon recent work on conditional neural language model for text\ngeneration. To deal with the large vocabulary, we extend these models to mix a\nfixed vocabulary with copy actions that transfer sample-specific words from the\ninput database to the generated output sentence. Our neural model significantly\nout-performs a classical Kneser-Ney language model adapted to this task by\nnearly 15 BLEU. \n\n"}
{"id": "1603.08023", "contents": "Title: How NOT To Evaluate Your Dialogue System: An Empirical Study of\n  Unsupervised Evaluation Metrics for Dialogue Response Generation Abstract: We investigate evaluation metrics for dialogue response generation systems\nwhere supervised labels, such as task completion, are not available. Recent\nworks in response generation have adopted metrics from machine translation to\ncompare a model's generated response to a single target response. We show that\nthese metrics correlate very weakly with human judgements in the non-technical\nTwitter domain, and not at all in the technical Ubuntu domain. We provide\nquantitative and qualitative results highlighting specific weaknesses in\nexisting metrics, and provide recommendations for future development of better\nautomatic evaluation metrics for dialogue systems. \n\n"}
{"id": "1603.08474", "contents": "Title: Deep Embedding for Spatial Role Labeling Abstract: This paper introduces the visually informed embedding of word (VIEW), a\ncontinuous vector representation for a word extracted from a deep neural model\ntrained using the Microsoft COCO data set to forecast the spatial arrangements\nbetween visual objects, given a textual description. The model is composed of a\ndeep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory\n(LSTM) network, the latter being preceded by an embedding layer. The VIEW is\napplied to transferring multimodal background knowledge to Spatial Role\nLabeling (SpRL) algorithms, which recognize spatial relations between objects\nmentioned in the text. This work also contributes with a new method to select\ncomplementary features and a fine-tuning method for MLP that improves the $F1$\nmeasure in classifying the words into spatial roles. The VIEW is evaluated with\nthe Task 3 of SemEval-2013 benchmark data set, SpaceEval. \n\n"}
{"id": "1603.08861", "contents": "Title: Revisiting Semi-Supervised Learning with Graph Embeddings Abstract: We present a semi-supervised learning framework based on graph embeddings.\nGiven a graph between instances, we train an embedding for each instance to\njointly predict the class label and the neighborhood context in the graph. We\ndevelop both transductive and inductive variants of our method. In the\ntransductive variant of our method, the class labels are determined by both the\nlearned embeddings and input feature vectors, while in the inductive variant,\nthe embeddings are defined as a parametric function of the feature vectors, so\npredictions can be made on instances not seen during training. On a large and\ndiverse set of benchmark tasks, including text classification, distantly\nsupervised entity extraction, and entity classification, we show improved\nperformance over many of the existing models. \n\n"}
{"id": "1603.09025", "contents": "Title: Recurrent Batch Normalization Abstract: We propose a reparameterization of LSTM that brings the benefits of batch\nnormalization to recurrent neural networks. Whereas previous works only apply\nbatch normalization to the input-to-hidden transformation of RNNs, we\ndemonstrate that it is both possible and beneficial to batch-normalize the\nhidden-to-hidden transition, thereby reducing internal covariate shift between\ntime steps. We evaluate our proposal on various sequential problems such as\nsequence classification, language modeling and question answering. Our\nempirical results show that our batch-normalized LSTM consistently leads to\nfaster convergence and improved generalization. \n\n"}
{"id": "1603.09128", "contents": "Title: Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders Abstract: We present an approach to learning multi-sense word embeddings relying both\non monolingual and bilingual information. Our model consists of an encoder,\nwhich uses monolingual and bilingual context (i.e. a parallel sentence) to\nchoose a sense for a given word, and a decoder which predicts context words\nbased on the chosen sense. The two components are estimated jointly. We observe\nthat the word representations induced from bilingual data outperform the\nmonolingual counterparts across a range of evaluation tasks, even though\ncrosslingual information is not available at test time. \n\n"}
{"id": "1603.09188", "contents": "Title: Unsupervised Visual Sense Disambiguation for Verbs using Multimodal\n  Embeddings Abstract: We introduce a new task, visual sense disambiguation for verbs: given an\nimage and a verb, assign the correct sense of the verb, i.e., the one that\ndescribes the action depicted in the image. Just as textual word sense\ndisambiguation is useful for a wide range of NLP tasks, visual sense\ndisambiguation can be useful for multimodal tasks such as image retrieval,\nimage description, and text illustration. We introduce VerSe, a new dataset\nthat augments existing multimodal datasets (COCO and TUHOI) with sense labels.\nWe propose an unsupervised algorithm based on Lesk which performs visual sense\ndisambiguation using textual, visual, or multimodal embeddings. We find that\ntextual embeddings perform well when gold-standard textual annotations (object\nlabels and image descriptions) are available, while multimodal embeddings\nperform well on unannotated images. We also verify our findings by using the\ntextual and multimodal embeddings as features in a supervised setting and\nanalyse the performance of visual sense disambiguation task. VerSe is made\npublicly available and can be downloaded at:\nhttps://github.com/spandanagella/verse. \n\n"}
{"id": "1603.09381", "contents": "Title: Clinical Information Extraction via Convolutional Neural Network Abstract: We report an implementation of a clinical information extraction tool that\nleverages deep neural network to annotate event spans and their attributes from\nraw clinical notes and pathology reports. Our approach uses context words and\ntheir part-of-speech tags and shape information as features. Then we hire\ntemporal (1D) convolutional neural network to learn hidden feature\nrepresentations. Finally, we use Multilayer Perceptron (MLP) to predict event\nspans. The empirical evaluation demonstrates that our approach significantly\noutperforms baselines. \n\n"}
{"id": "1603.09643", "contents": "Title: Multi-task Recurrent Model for Speech and Speaker Recognition Abstract: Although highly correlated, speech and speaker recognition have been regarded\nas two independent tasks and studied by two communities. This is certainly not\nthe way that people behave: we decipher both speech content and speaker traits\nat the same time. This paper presents a unified model to perform speech and\nspeaker recognition simultaneously and altogether. The model is based on a\nunified neural network where the output of one task is fed to the input of the\nother, leading to a multi-task recurrent network. Experiments show that the\njoint model outperforms the task-specific models on both the two tasks. \n\n"}
{"id": "1604.00077", "contents": "Title: Neural Attention Models for Sequence Classification: Analysis and\n  Application to Key Term Extraction and Dialogue Act Detection Abstract: Recurrent neural network architectures combining with attention mechanism, or\nneural attention model, have shown promising performance recently for the tasks\nincluding speech recognition, image caption generation, visual question\nanswering and machine translation. In this paper, neural attention model is\napplied on two sequence classification tasks, dialogue act detection and key\nterm extraction. In the sequence labeling tasks, the model input is a sequence,\nand the output is the label of the input sequence. The major difficulty of\nsequence labeling is that when the input sequence is long, it can include many\nnoisy or irrelevant part. If the information in the whole sequence is treated\nequally, the noisy or irrelevant part may degrade the classification\nperformance. The attention mechanism is helpful for sequence classification\ntask because it is capable of highlighting important part among the entire\nsequence for the classification task. The experimental results show that with\nthe attention mechanism, discernible improvements were achieved in the sequence\nlabeling task considered here. The roles of the attention mechanism in the\ntasks are further analyzed and visualized in this paper. \n\n"}
{"id": "1604.00734", "contents": "Title: Capturing Semantic Similarity for Entity Linking with Convolutional\n  Neural Networks Abstract: A key challenge in entity linking is making effective use of contextual\ninformation to disambiguate mentions that might refer to different entities in\ndifferent contexts. We present a model that uses convolutional neural networks\nto capture semantic correspondence between a mention's context and a proposed\ntarget entity. These convolutional networks operate at multiple granularities\nto exploit various kinds of topic information, and their rich parameterization\ngives them the capacity to learn which n-grams characterize different topics.\nWe combine these networks with a sparse linear model to achieve\nstate-of-the-art performance on multiple entity linking datasets, outperforming\nthe prior systems of Durrett and Klein (2014) and Nguyen et al. (2014). \n\n"}
{"id": "1604.00790", "contents": "Title: Image Captioning with Deep Bidirectional LSTMs Abstract: This work presents an end-to-end trainable deep bidirectional LSTM\n(Long-Short Term Memory) model for image captioning. Our model builds on a deep\nconvolutional neural network (CNN) and two separate LSTM networks. It is\ncapable of learning long term visual-language interactions by making use of\nhistory and future context information at high level semantic space. Two novel\ndeep bidirectional variant models, in which we increase the depth of\nnonlinearity transition in different way, are proposed to learn hierarchical\nvisual-language embeddings. Data augmentation techniques such as multi-crop,\nmulti-scale and vertical mirror are proposed to prevent overfitting in training\ndeep models. We visualize the evolution of bidirectional LSTM internal states\nover time and qualitatively analyze how our models \"translate\" image to\nsentence. Our proposed models are evaluated on caption generation and\nimage-sentence retrieval tasks with three benchmark datasets: Flickr8K,\nFlickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models\nachieve highly competitive performance to the state-of-the-art results on\ncaption generation even without integrating additional mechanism (e.g. object\ndetection, attention model etc.) and significantly outperform recent methods on\nretrieval task. \n\n"}
{"id": "1604.00933", "contents": "Title: Entity Type Recognition using an Ensemble of Distributional Semantic\n  Models to Enhance Query Understanding Abstract: We present an ensemble approach for categorizing search query entities in the\nrecruitment domain. Understanding the types of entities expressed in a search\nquery (Company, Skill, Job Title, etc.) enables more intelligent information\nretrieval based upon those entities compared to a traditional keyword-based\nsearch. Because search queries are typically very short, leveraging a\ntraditional bag-of-words model to identify entity types would be inappropriate\ndue to the lack of contextual information. Our approach instead combines clues\nfrom different sources of varying complexity in order to collect real-world\nknowledge about query entities. We employ distributional semantic\nrepresentations of query entities through two models: 1) contextual vectors\ngenerated from encyclopedic corpora like Wikipedia, and 2) high dimensional\nword embedding vectors generated from millions of job postings using word2vec.\nAdditionally, our approach utilizes both entity linguistic properties obtained\nfrom WordNet and ontological properties extracted from DBpedia. We evaluate our\napproach on a data set created at CareerBuilder; the largest job board in the\nUS. The data set contains entities extracted from millions of job\nseekers/recruiters search queries, job postings, and resume documents. After\nconstructing the distributional vectors of search entities, we use supervised\nmachine learning to infer search entity types. Empirical results show that our\napproach outperforms the state-of-the-art word2vec distributional semantics\nmodel trained on Wikipedia. Moreover, we achieve micro-averaged F 1 score of\n97% using the proposed distributional representations ensemble. \n\n"}
{"id": "1604.01131", "contents": "Title: Discovering items with potential popularity on social media Abstract: Predicting the future popularity of online content is highly important in\nmany applications. Preferential attachment phenomena is encountered in scale\nfree networks.Under it's influece popular items get more popular thereby\nresulting in long tailed distribution problem. Consequently, new items which\ncan be popular (potential ones), are suppressed by the already popular items.\nThis paper proposes a novel model which is able to identify potential items. It\nidentifies the potentially popular items by considering the number of links or\nratings it has recieved in recent past along with it's popularity decay. For\nobtaining an effecient model we consider only temporal features of the content,\navoiding the cost of extracting other features. We have found that people\nfollow recent behaviours of their peers. In presence of fit or quality items\nalready popular items lose it's popularity. Prediction accuracy is measured on\nthree industrial datasets namely Movielens, Netflix and Facebook wall post.\nExperimental results show that compare to state-of-the-art model our model have\nbetter prediction accuracy. \n\n"}
{"id": "1604.01692", "contents": "Title: An Ensemble Method to Produce High-Quality Word Embeddings (2016) Abstract: A currently successful approach to computational semantics is to represent\nwords as embeddings in a machine-learned vector space. We present an ensemble\nmethod that combines embeddings produced by GloVe (Pennington et al., 2014) and\nword2vec (Mikolov et al., 2013) with structured knowledge from the semantic\nnetworks ConceptNet (Speer and Havasi, 2012) and PPDB (Ganitkevitch et al.,\n2013), merging their information into a common representation with a large,\nmultilingual vocabulary. The embeddings it produces achieve state-of-the-art\nperformance on many word-similarity evaluations. Its score of $\\rho = .596$ on\nan evaluation of rare words (Luong et al., 2013) is 16% higher than the\nprevious best known system. \n\n"}
{"id": "1604.02038", "contents": "Title: Sentence Level Recurrent Topic Model: Letting Topics Speak for\n  Themselves Abstract: We propose Sentence Level Recurrent Topic Model (SLRTM), a new topic model\nthat assumes the generation of each word within a sentence to depend on both\nthe topic of the sentence and the whole history of its preceding words in the\nsentence. Different from conventional topic models that largely ignore the\nsequential order of words or their topic coherence, SLRTM gives full\ncharacterization to them by using a Recurrent Neural Networks (RNN) based\nframework. Experimental results have shown that SLRTM outperforms several\nstrong baselines on various tasks. Furthermore, SLRTM can automatically\ngenerate sentences given a topic (i.e., topics to sentences), which is a key\ntechnology for real world applications such as personalized short text\nconversation. \n\n"}
{"id": "1604.04558", "contents": "Title: Accessing accurate documents by mining auxiliary document information Abstract: Earlier techniques of text mining included algorithms like k-means, Naive\nBayes, SVM which classify and cluster the text document for mining relevant\ninformation about the documents. The need for improving the mining techniques\nhas us searching for techniques using the available algorithms. This paper\nproposes one technique which uses the auxiliary information that is present\ninside the text documents to improve the mining. This auxiliary information can\nbe a description to the content. This information can be either useful or\ncompletely useless for mining. The user should assess the worth of the\nauxiliary information before considering this technique for text mining. In\nthis paper, a combination of classical clustering algorithms is used to mine\nthe datasets. The algorithm runs in two stages which carry out mining at\ndifferent levels of abstraction. The clustered documents would then be\nclassified based on the necessary groups. The proposed technique is aimed at\nimproved results of document clustering. \n\n"}
{"id": "1604.04562", "contents": "Title: A Network-based End-to-End Trainable Task-oriented Dialogue System Abstract: Teaching machines to accomplish tasks by conversing naturally with humans is\nchallenging. Currently, developing task-oriented dialogue systems requires\ncreating multiple components and typically this involves either a large amount\nof handcrafting, or acquiring costly labelled datasets to solve a statistical\nlearning problem for each component. In this work we introduce a neural\nnetwork-based text-in, text-out end-to-end trainable goal-oriented dialogue\nsystem along with a new way of collecting dialogue data based on a novel\npipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue\nsystems easily and without making too many assumptions about the task at hand.\nThe results show that the model can converse with human subjects naturally\nwhilst helping them to accomplish tasks in a restaurant search domain. \n\n"}
{"id": "1604.05073", "contents": "Title: Speed-Constrained Tuning for Statistical Machine Translation Using\n  Bayesian Optimization Abstract: We address the problem of automatically finding the parameters of a\nstatistical machine translation system that maximize BLEU scores while ensuring\nthat decoding speed exceeds a minimum value. We propose the use of Bayesian\nOptimization to efficiently tune the speed-related decoding parameters by\neasily incorporating speed as a noisy constraint function. The obtained\nparameter values are guaranteed to satisfy the speed constraint with an\nassociated confidence margin. Across three language pairs and two speed\nconstraint values, we report overall optimization time reduction compared to\ngrid and random search. We also show that Bayesian Optimization can decouple\nspeed and BLEU measurements, resulting in a further reduction of overall\noptimization time as speed is measured over a small subset of sentences. \n\n"}
{"id": "1604.06635", "contents": "Title: Bridging LSTM Architecture and the Neural Dynamics during Reading Abstract: Recently, the long short-term memory neural network (LSTM) has attracted wide\ninterest due to its success in many tasks. LSTM architecture consists of a\nmemory cell and three gates, which looks similar to the neuronal networks in\nthe brain. However, there still lacks the evidence of the cognitive\nplausibility of LSTM architecture as well as its working mechanism. In this\npaper, we study the cognitive plausibility of LSTM by aligning its internal\narchitecture with the brain activity observed via fMRI when the subjects read a\nstory. Experiment results show that the artificial memory vector in LSTM can\naccurately predict the observed sequential brain activities, indicating the\ncorrelation between LSTM architecture and the cognitive process of story\nreading. \n\n"}
{"id": "1604.08120", "contents": "Title: Extracting Temporal and Causal Relations between Events Abstract: Structured information resulting from temporal information processing is\ncrucial for a variety of natural language processing tasks, for instance to\ngenerate timeline summarization of events from news documents, or to answer\ntemporal/causal-related questions about some events. In this thesis we present\na framework for an integrated temporal and causal relation extraction system.\nWe first develop a robust extraction component for each type of relations, i.e.\ntemporal order and causality. We then combine the two extraction components\ninto an integrated relation extraction system, CATENA---CAusal and Temporal\nrelation Extraction from NAtural language texts---, by utilizing the\npresumption about event precedence in causality, that causing events must\nhappened BEFORE resulting events. Several resources and techniques to improve\nour relation extraction systems are also discussed, including word embeddings\nand training data expansion. Finally, we report our adaptation efforts of\ntemporal information processing for languages other than English, namely\nItalian and Indonesian. \n\n"}
{"id": "1605.00090", "contents": "Title: Response Selection with Topic Clues for Retrieval-based Chatbots Abstract: We consider incorporating topic information into message-response matching to\nboost responses with rich content in retrieval-based chatbots. To this end, we\npropose a topic-aware convolutional neural tensor network (TACNTN). In TACNTN,\nmatching between a message and a response is not only conducted between a\nmessage vector and a response vector generated by convolutional neural\nnetworks, but also leverages extra topic information encoded in two topic\nvectors. The two topic vectors are linear combinations of topic words of the\nmessage and the response respectively, where the topic words are obtained from\na pre-trained LDA model and their weights are determined by themselves as well\nas the message vector and the response vector. The message vector, the response\nvector, and the two topic vectors are fed to neural tensors to calculate a\nmatching score. Empirical study on a public data set and a human annotated data\nset shows that TACNTN can significantly outperform state-of-the-art methods for\nmessage-response matching. \n\n"}
{"id": "1605.00223", "contents": "Title: Text-mining the NeuroSynth corpus using Deep Boltzmann Machines Abstract: Large-scale automated meta-analysis of neuroimaging data has recently\nestablished itself as an important tool in advancing our understanding of human\nbrain function. This research has been pioneered by NeuroSynth, a database\ncollecting both brain activation coordinates and associated text across a large\ncohort of neuroimaging research papers. One of the fundamental aspects of such\nmeta-analysis is text-mining. To date, word counts and more sophisticated\nmethods such as Latent Dirichlet Allocation have been proposed. In this work we\npresent an unsupervised study of the NeuroSynth text corpus using Deep\nBoltzmann Machines (DBMs). The use of DBMs yields several advantages over the\naforementioned methods, principal among which is the fact that it yields both\nword and document embeddings in a high-dimensional vector space. Such\nembeddings serve to facilitate the use of traditional machine learning\ntechniques on the text corpus. The proposed DBM model is shown to learn\nembeddings with a clear semantic structure. \n\n"}
{"id": "1605.00635", "contents": "Title: The Capacity of Robust Private Information Retrieval with Colluding\n  Databases Abstract: Private information retrieval (PIR) is the problem of retrieving as\nefficiently as possible, one out of $K$ messages from $N$ non-communicating\nreplicated databases (each holds all $K$ messages) while keeping the identity\nof the desired message index a secret from each individual database. The\ninformation theoretic capacity of PIR (equivalently, the reciprocal of minimum\ndownload cost) is the maximum number of bits of desired information that can be\nprivately retrieved per bit of downloaded information. $T$-private PIR is a\ngeneralization of PIR to include the requirement that even if any $T$ of the\n$N$ databases collude, the identity of the retrieved message remains completely\nunknown to them. Robust PIR is another generalization that refers to the\nscenario where we have $M \\geq N$ databases, out of which any $M - N$ may fail\nto respond. For $K$ messages and $M\\geq N$ databases out of which at least some\n$N$ must respond, we show that the capacity of $T$-private and Robust PIR is\n$\\left(1+T/N+T^2/N^2+\\cdots+T^{K-1}/N^{K-1}\\right)^{-1}$. The result includes\nas special cases the capacity of PIR without robustness ($M=N$) or $T$-privacy\nconstraints ($T=1$). \n\n"}
{"id": "1605.00659", "contents": "Title: Predicting online extremism, content adopters, and interaction\n  reciprocity Abstract: We present a machine learning framework that leverages a mixture of metadata,\nnetwork, and temporal features to detect extremist users, and predict content\nadopters and interaction reciprocity in social media. We exploit a unique\ndataset containing millions of tweets generated by more than 25 thousand users\nwho have been manually identified, reported, and suspended by Twitter due to\ntheir involvement with extremist campaigns. We also leverage millions of tweets\ngenerated by a random sample of 25 thousand regular users who were exposed to,\nor consumed, extremist content. We carry out three forecasting tasks, (i) to\ndetect extremist users, (ii) to estimate whether regular users will adopt\nextremist content, and finally (iii) to predict whether users will reciprocate\ncontacts initiated by extremists. All forecasting tasks are set up in two\nscenarios: a post hoc (time independent) prediction task on aggregated data,\nand a simulated real-time prediction task. The performance of our framework is\nextremely promising, yielding in the different forecasting scenarios up to 93%\nAUC for extremist user detection, up to 80% AUC for content adoption\nprediction, and finally up to 72% AUC for interaction reciprocity forecasting.\nWe conclude by providing a thorough feature analysis that helps determine which\nare the emerging signals that provide predictive power in different scenarios. \n\n"}
{"id": "1605.02134", "contents": "Title: Neural Recovery Machine for Chinese Dropped Pronoun Abstract: Dropped pronouns (DPs) are ubiquitous in pro-drop languages like Chinese,\nJapanese etc. Previous work mainly focused on painstakingly exploring the\nempirical features for DPs recovery. In this paper, we propose a neural\nrecovery machine (NRM) to model and recover DPs in Chinese, so that to avoid\nthe non-trivial feature engineering process. The experimental results show that\nthe proposed NRM significantly outperforms the state-of-the-art approaches on\nboth two heterogeneous datasets. Further experiment results of Chinese zero\npronoun (ZP) resolution show that the performance of ZP resolution can also be\nimproved by recovering the ZPs to DPs. \n\n"}
{"id": "1605.02688", "contents": "Title: Theano: A Python framework for fast computation of mathematical\n  expressions Abstract: Theano is a Python library that allows to define, optimize, and evaluate\nmathematical expressions involving multi-dimensional arrays efficiently. Since\nits introduction, it has been one of the most used CPU and GPU mathematical\ncompilers - especially in the machine learning community - and has shown steady\nperformance improvements. Theano is being actively and continuously developed\nsince 2008, multiple frameworks have been built on top of it and it has been\nused to produce many state-of-the-art machine learning models.\n  The present article is structured as follows. Section I provides an overview\nof the Theano software and its community. Section II presents the principal\nfeatures of Theano and how to use them, and compares them with other similar\nprojects. Section III focuses on recently-introduced functionalities and\nimprovements. Section IV compares the performance of Theano against Torch7 and\nTensorFlow on several machine learning models. Section V discusses current\nlimitations of Theano and potential ways of improving it. \n\n"}
{"id": "1605.03481", "contents": "Title: Tweet2Vec: Character-Based Distributed Representations for Social Media Abstract: Text from social media provides a set of challenges that can cause\ntraditional NLP approaches to fail. Informal language, spelling errors,\nabbreviations, and special characters are all commonplace in these posts,\nleading to a prohibitively large vocabulary size for word-level approaches. We\npropose a character composition model, tweet2vec, which finds vector-space\nrepresentations of whole tweets by learning complex, non-local dependencies in\ncharacter sequences. The proposed model outperforms a word-level baseline at\npredicting user-annotated hashtags associated with the posts, doing\nsignificantly better when the input contains many out-of-vocabulary words or\nunusual character sequences. Our tweet2vec encoder is publicly available. \n\n"}
{"id": "1605.03852", "contents": "Title: Learning the Curriculum with Bayesian Optimization for Task-Specific\n  Word Representation Learning Abstract: We use Bayesian optimization to learn curricula for word representation\nlearning, optimizing performance on downstream tasks that depend on the learned\nrepresentations as features. The curricula are modeled by a linear ranking\nfunction which is the scalar product of a learned weight vector and an\nengineered feature vector that characterizes the different aspects of the\ncomplexity of each instance in the training corpus. We show that learning the\ncurriculum improves performance on a variety of downstream tasks over random\norders and in comparison to the natural corpus order. \n\n"}
{"id": "1605.04764", "contents": "Title: Geometry Aware Mappings for High Dimensional Sparse Factors Abstract: While matrix factorisation models are ubiquitous in large scale\nrecommendation and search, real time application of such models requires inner\nproduct computations over an intractably large set of item factors. In this\nmanuscript we present a novel framework that uses the inverted index\nrepresentation to exploit structural properties of sparse vectors to\nsignificantly reduce the run time computational cost of factorisation models.\nWe develop techniques that use geometry aware permutation maps on a tessellated\nunit sphere to obtain high dimensional sparse embeddings for latent factors\nwith sparsity patterns related to angular closeness of the original latent\nfactors. We also design several efficient and deterministic realisations within\nthis framework and demonstrate with experiments that our techniques lead to\nfaster run time operation with minimal loss of accuracy. \n\n"}
{"id": "1605.05101", "contents": "Title: Recurrent Neural Network for Text Classification with Multi-Task\n  Learning Abstract: Neural network based methods have obtained great progress on a variety of\nnatural language processing tasks. However, in most previous works, the models\nare learned based on single-task supervised objectives, which often suffer from\ninsufficient training data. In this paper, we use the multi-task learning\nframework to jointly learn across multiple related tasks. Based on recurrent\nneural network, we propose three different mechanisms of sharing information to\nmodel text with task-specific and shared layers. The entire network is trained\njointly on all these tasks. Experiments on four benchmark text classification\ntasks show that our proposed models can improve the performance of a task with\nthe help of other related tasks. \n\n"}
{"id": "1605.05362", "contents": "Title: Yelp Dataset Challenge: Review Rating Prediction Abstract: Review websites, such as TripAdvisor and Yelp, allow users to post online\nreviews for various businesses, products and services, and have been recently\nshown to have a significant influence on consumer shopping behaviour. An online\nreview typically consists of free-form text and a star rating out of 5. The\nproblem of predicting a user's star rating for a product, given the user's text\nreview for that product, is called Review Rating Prediction and has lately\nbecome a popular, albeit hard, problem in machine learning. In this paper, we\ntreat Review Rating Prediction as a multi-class classification problem, and\nbuild sixteen different prediction models by combining four feature extraction\nmethods, (i) unigrams, (ii) bigrams, (iii) trigrams and (iv) Latent Semantic\nIndexing, with four machine learning algorithms, (i) logistic regression, (ii)\nNaive Bayes classification, (iii) perceptrons, and (iv) linear Support Vector\nClassification. We analyse the performance of each of these sixteen models to\ncome up with the best model for predicting the ratings from reviews. We use the\ndataset provided by Yelp for training and testing the models. \n\n"}
{"id": "1605.06650", "contents": "Title: Latent Tree Models for Hierarchical Topic Detection Abstract: We present a novel method for hierarchical topic detection where topics are\nobtained by clustering documents in multiple ways. Specifically, we model\ndocument collections using a class of graphical models called hierarchical\nlatent tree models (HLTMs). The variables at the bottom level of an HLTM are\nobserved binary variables that represent the presence/absence of words in a\ndocument. The variables at other levels are binary latent variables, with those\nat the lowest latent level representing word co-occurrence patterns and those\nat higher levels representing co-occurrence of patterns at the level below.\nEach latent variable gives a soft partition of the documents, and document\nclusters in the partitions are interpreted as topics. Latent variables at high\nlevels of the hierarchy capture long-range word co-occurrence patterns and\nhence give thematically more general topics, while those at low levels of the\nhierarchy capture short-range word co-occurrence patterns and give thematically\nmore specific topics. Unlike LDA-based topic models, HLTMs do not refer to a\ndocument generation process and use word variables instead of token variables.\nThey use a tree structure to model the relationships between topics and words,\nwhich is conducive to the discovery of meaningful topics and topic hierarchies. \n\n"}
{"id": "1605.06855", "contents": "Title: Smart broadcasting: Do you want to be seen? Abstract: Many users in online social networks are constantly trying to gain attention\nfrom their followers by broadcasting posts to them. These broadcasters are\nlikely to gain greater attention if their posts can remain visible for a longer\nperiod of time among their followers' most recent feeds. Then when to post? In\nthis paper, we study the problem of smart broadcasting using the framework of\ntemporal point processes, where we model users feeds and posts as discrete\nevents occurring in continuous time. Based on such continuous-time model, then\nchoosing a broadcasting strategy for a user becomes a problem of designing the\nconditional intensity of her posting events. We derive a novel formula which\nlinks this conditional intensity with the visibility of the user in her\nfollowers' feeds. Furthermore, by exploiting this formula, we develop an\nefficient convex optimization framework for the when-to-post problem. Our\nmethod can find broadcasting strategies that reach a desired visibility level\nwith provable guarantees. We experimented with data gathered from Twitter, and\nshow that our framework can consistently make broadcasters' post more visible\nthan alternatives. \n\n"}
{"id": "1605.07422", "contents": "Title: Computing Web-scale Topic Models using an Asynchronous Parameter Server Abstract: Topic models such as Latent Dirichlet Allocation (LDA) have been widely used\nin information retrieval for tasks ranging from smoothing and feedback methods\nto tools for exploratory search and discovery. However, classical methods for\ninferring topic models do not scale up to the massive size of today's publicly\navailable Web-scale data sets. The state-of-the-art approaches rely on custom\nstrategies, implementations and hardware to facilitate their asynchronous,\ncommunication-intensive workloads.\n  We present APS-LDA, which integrates state-of-the-art topic modeling with\ncluster computing frameworks such as Spark using a novel asynchronous parameter\nserver. Advantages of this integration include convenient usage of existing\ndata processing pipelines and eliminating the need for disk writes as data can\nbe kept in memory from start to finish. Our goal is not to outperform highly\ncustomized implementations, but to propose a general high-performance topic\nmodeling framework that can easily be used in today's data processing\npipelines. We compare APS-LDA to the existing Spark LDA implementations and\nshow that our system can, on a 480-core cluster, process up to 135 times more\ndata and 10 times more topics without sacrificing model quality. \n\n"}
{"id": "1605.07722", "contents": "Title: Yum-me: A Personalized Nutrient-based Meal Recommender System Abstract: Nutrient-based meal recommendations have the potential to help individuals\nprevent or manage conditions such as diabetes and obesity. However, learning\npeople's food preferences and making recommendations that simultaneously appeal\nto their palate and satisfy nutritional expectations are challenging. Existing\napproaches either only learn high-level preferences or require a prolonged\nlearning period. We propose Yum-me, a personalized nutrient-based meal\nrecommender system designed to meet individuals' nutritional expectations,\ndietary restrictions, and fine-grained food preferences. Yum-me enables a\nsimple and accurate food preference profiling procedure via a visual quiz-based\nuser interface, and projects the learned profile into the domain of\nnutritionally appropriate food options to find ones that will appeal to the\nuser. We present the design and implementation of Yum-me, and further describe\nand evaluate two innovative contributions. The first contriution is an open\nsource state-of-the-art food image analysis model, named FoodDist. We\ndemonstrate FoodDist's superior performance through careful benchmarking and\ndiscuss its applicability across a wide array of dietary applications. The\nsecond contribution is a novel online learning framework that learns food\npreference from item-wise and pairwise image comparisons. We evaluate the\nframework in a field study of 227 anonymous users and demonstrate that it\noutperforms other baselines by a significant margin. We further conducted an\nend-to-end validation of the feasibility and effectiveness of Yum-me through a\n60-person user study, in which Yum-me improves the recommendation acceptance\nrate by 42.63%. \n\n"}
{"id": "1605.07912", "contents": "Title: Review Networks for Caption Generation Abstract: We propose a novel extension of the encoder-decoder framework, called a\nreview network. The review network is generic and can enhance any existing\nencoder- decoder model: in this paper, we consider RNN decoders with both CNN\nand RNN encoders. The review network performs a number of review steps with\nattention mechanism on the encoder hidden states, and outputs a thought vector\nafter each review step; the thought vectors are used as the input of the\nattention mechanism in the decoder. We show that conventional encoder-decoders\nare a special case of our framework. Empirically, we show that our framework\nimproves over state-of- the-art encoder-decoder systems on the tasks of image\ncaptioning and source code captioning. \n\n"}
{"id": "1605.09128", "contents": "Title: Control of Memory, Active Perception, and Action in Minecraft Abstract: In this paper, we introduce a new set of reinforcement learning (RL) tasks in\nMinecraft (a flexible 3D world). We then use these tasks to systematically\ncompare and contrast existing deep reinforcement learning (DRL) architectures\nwith our new memory-based DRL architectures. These tasks are designed to\nemphasize, in a controllable manner, issues that pose challenges for RL methods\nincluding partial observability (due to first-person visual observations),\ndelayed rewards, high-dimensional visual observations, and the need to use\nactive perception in a correct manner so as to perform well in the tasks. While\nthese tasks are conceptually simple to describe, by virtue of having all of\nthese challenges simultaneously they are difficult for current DRL\narchitectures. Additionally, we evaluate the generalization performance of the\narchitectures on environments not used during training. The experimental\nresults show that our new architectures generalize to unseen environments\nbetter than existing DRL architectures. \n\n"}
{"id": "1606.00253", "contents": "Title: On a Topic Model for Sentences Abstract: Probabilistic topic models are generative models that describe the content of\ndocuments by discovering the latent topics underlying them. However, the\nstructure of the textual input, and for instance the grouping of words in\ncoherent text spans such as sentences, contains much information which is\ngenerally lost with these models. In this paper, we propose sentenceLDA, an\nextension of LDA whose goal is to overcome this limitation by incorporating the\nstructure of the text in the generative and inference processes. We illustrate\nthe advantages of sentenceLDA by comparing it with LDA using both intrinsic\n(perplexity) and extrinsic (text classification) evaluation tasks on different\ntext collections. \n\n"}
{"id": "1606.00372", "contents": "Title: Conversational Contextual Cues: The Case of Personalization and History\n  for Response Ranking Abstract: We investigate the task of modeling open-domain, multi-turn, unstructured,\nmulti-participant, conversational dialogue. We specifically study the effect of\nincorporating different elements of the conversation. Unlike previous efforts,\nwhich focused on modeling messages and responses, we extend the modeling to\nlong context and participant's history. Our system does not rely on handwritten\nrules or engineered features; instead, we train deep neural networks on a large\nconversational dataset. In particular, we exploit the structure of Reddit\ncomments and posts to extract 2.1 billion messages and 133 million\nconversations. We evaluate our models on the task of predicting the next\nresponse in a conversation, and we find that modeling both context and\nparticipants improves prediction accuracy. \n\n"}
{"id": "1606.00372", "contents": "Title: Conversational Contextual Cues: The Case of Personalization and History\n  for Response Ranking Abstract: We investigate the task of modeling open-domain, multi-turn, unstructured,\nmulti-participant, conversational dialogue. We specifically study the effect of\nincorporating different elements of the conversation. Unlike previous efforts,\nwhich focused on modeling messages and responses, we extend the modeling to\nlong context and participant's history. Our system does not rely on handwritten\nrules or engineered features; instead, we train deep neural networks on a large\nconversational dataset. In particular, we exploit the structure of Reddit\ncomments and posts to extract 2.1 billion messages and 133 million\nconversations. We evaluate our models on the task of predicting the next\nresponse in a conversation, and we find that modeling both context and\nparticipants improves prediction accuracy. \n\n"}
{"id": "1606.00577", "contents": "Title: Source-LDA: Enhancing probabilistic topic models using prior knowledge\n  sources Abstract: A popular approach to topic modeling involves extracting co-occurring n-grams\nof a corpus into semantic themes. The set of n-grams in a theme represents an\nunderlying topic, but most topic modeling approaches are not able to label\nthese sets of words with a single n-gram. Such labels are useful for topic\nidentification in summarization systems. This paper introduces a novel approach\nto labeling a group of n-grams comprising an individual topic. The approach\ntaken is to complement the existing topic distributions over words with a known\ndistribution based on a predefined set of topics. This is done by integrating\nexisting labeled knowledge sources representing known potential topics into the\nprobabilistic topic model. These knowledge sources are translated into a\ndistribution and used to set the hyperparameters of the Dirichlet generated\ndistribution over words. In the inference these modified distributions guide\nthe convergence of the latent topics to conform with the complementary\ndistributions. This approach ensures that the topic inference process is\nconsistent with existing knowledge. The label assignment from the complementary\nknowledge sources are then transferred to the latent topics of the corpus. The\nresults show both accurate label assignment to topics as well as improved topic\ngeneration than those obtained using various labeling approaches based off\nLatent Dirichlet allocation (LDA). \n\n"}
{"id": "1606.00589", "contents": "Title: Single-Model Encoder-Decoder with Explicit Morphological Representation\n  for Reinflection Abstract: Morphological reinflection is the task of generating a target form given a\nsource form, a source tag and a target tag. We propose a new way of modeling\nthis task with neural encoder-decoder models. Our approach reduces the amount\nof required training data for this architecture and achieves state-of-the-art\nresults, making encoder-decoder models applicable to morphological reinflection\neven for low-resource languages. We further present a new automatic correction\nmethod for the outputs based on edit trees. \n\n"}
{"id": "1606.01269", "contents": "Title: End-to-end LSTM-based dialog control optimized with supervised and\n  reinforcement learning Abstract: This paper presents a model for end-to-end learning of task-oriented dialog\nsystems. The main component of the model is a recurrent neural network (an\nLSTM), which maps from raw dialog history directly to a distribution over\nsystem actions. The LSTM automatically infers a representation of dialog\nhistory, which relieves the system developer of much of the manual feature\nengineering of dialog state. In addition, the developer can provide software\nthat expresses business rules and provides access to programmatic APIs,\nenabling the LSTM to take actions in the real world on behalf of the user. The\nLSTM can be optimized using supervised learning (SL), where a domain expert\nprovides example dialogs which the LSTM should imitate; or using reinforcement\nlearning (RL), where the system improves by interacting directly with end\nusers. Experiments show that SL and RL are complementary: SL alone can derive a\nreasonable initial policy from a small number of training dialogs; and starting\nRL optimization with a policy trained with SL substantially accelerates the\nlearning rate of RL. \n\n"}
{"id": "1606.01305", "contents": "Title: Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations Abstract: We propose zoneout, a novel method for regularizing RNNs. At each timestep,\nzoneout stochastically forces some hidden units to maintain their previous\nvalues. Like dropout, zoneout uses random noise to train a pseudo-ensemble,\nimproving generalization. But by preserving instead of dropping hidden units,\ngradient information and state information are more readily propagated through\ntime, as in feedforward stochastic depth networks. We perform an empirical\ninvestigation of various RNN regularizers, and find that zoneout gives\nsignificant performance improvements across tasks. We achieve competitive\nresults with relatively simple models in character- and word-level language\nmodelling on the Penn Treebank and Text8 datasets, and combining with recurrent\nbatch normalization yields state-of-the-art results on permuted sequential\nMNIST. \n\n"}
{"id": "1606.01305", "contents": "Title: Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations Abstract: We propose zoneout, a novel method for regularizing RNNs. At each timestep,\nzoneout stochastically forces some hidden units to maintain their previous\nvalues. Like dropout, zoneout uses random noise to train a pseudo-ensemble,\nimproving generalization. But by preserving instead of dropping hidden units,\ngradient information and state information are more readily propagated through\ntime, as in feedforward stochastic depth networks. We perform an empirical\ninvestigation of various RNN regularizers, and find that zoneout gives\nsignificant performance improvements across tasks. We achieve competitive\nresults with relatively simple models in character- and word-level language\nmodelling on the Penn Treebank and Text8 datasets, and combining with recurrent\nbatch normalization yields state-of-the-art results on permuted sequential\nMNIST. \n\n"}
{"id": "1606.01781", "contents": "Title: Very Deep Convolutional Networks for Text Classification Abstract: The dominant approach for many NLP tasks are recurrent neural networks, in\nparticular LSTMs, and convolutional neural networks. However, these\narchitectures are rather shallow in comparison to the deep convolutional\nnetworks which have pushed the state-of-the-art in computer vision. We present\na new architecture (VDCNN) for text processing which operates directly at the\ncharacter level and uses only small convolutions and pooling operations. We are\nable to show that the performance of this model increases with depth: using up\nto 29 convolutional layers, we report improvements over the state-of-the-art on\nseveral public text classification tasks. To the best of our knowledge, this is\nthe first time that very deep convolutional nets have been applied to text\nprocessing. \n\n"}
{"id": "1606.02270", "contents": "Title: Natural Language Comprehension with the EpiReader Abstract: We present the EpiReader, a novel model for machine comprehension of text.\nMachine comprehension of unstructured, real-world text is a major research goal\nfor natural language processing. Current tests of machine comprehension pose\nquestions whose answers can be inferred from some supporting text, and evaluate\na model's response to the questions. The EpiReader is an end-to-end neural\nmodel comprising two components: the first component proposes a small set of\ncandidate answers after comparing a question to its supporting text, and the\nsecond component formulates hypotheses using the proposed candidates and the\nquestion, then reranks the hypotheses based on their estimated concordance with\nthe supporting text. We present experiments demonstrating that the EpiReader\nsets a new state-of-the-art on the CNN and Children's Book Test machine\ncomprehension benchmarks, outperforming previous neural models by a significant\nmargin. \n\n"}
{"id": "1606.03002", "contents": "Title: MuFuRU: The Multi-Function Recurrent Unit Abstract: Recurrent neural networks such as the GRU and LSTM found wide adoption in\nnatural language processing and achieve state-of-the-art results for many\ntasks. These models are characterized by a memory state that can be written to\nand read from by applying gated composition operations to the current input and\nthe previous state. However, they only cover a small subset of potentially\nuseful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that\nallow for arbitrary differentiable functions as composition operations.\nFurthermore, MuFuRUs allow for an input- and state-dependent choice of these\ncomposition operations that is learned. Our experiments demonstrate that the\nadditional functionality helps in different sequence modeling tasks, including\nthe evaluation of propositional logic formulae, language modeling and sentiment\nanalysis. \n\n"}
{"id": "1606.03044", "contents": "Title: The \"Horse'' Inside: Seeking Causes Behind the Behaviours of Music\n  Content Analysis Systems Abstract: Building systems that possess the sensitivity and intelligence to identify\nand describe high-level attributes in music audio signals continues to be an\nelusive goal, but one that surely has broad and deep implications for a wide\nvariety of applications. Hundreds of papers have so far been published toward\nthis goal, and great progress appears to have been made. Some systems produce\nremarkable accuracies at recognising high-level semantic concepts, such as\nmusic style, genre and mood. However, it might be that these numbers do not\nmean what they seem. In this paper, we take a state-of-the-art music content\nanalysis system and investigate what causes it to achieve exceptionally high\nperformance in a benchmark music audio dataset. We dissect the system to\nunderstand its operation, determine its sensitivities and limitations, and\npredict the kinds of knowledge it could and could not possess about music. We\nperform a series of experiments to illuminate what the system has actually\nlearned to do, and to what extent it is performing the intended music listening\ntask. Our results demonstrate how the initial manifestation of music\nintelligence in this state-of-the-art can be deceptive. Our work provides\nconstructive directions toward developing music content analysis systems that\ncan address the music information and creation needs of real-world users. \n\n"}
{"id": "1606.03498", "contents": "Title: Improved Techniques for Training GANs Abstract: We present a variety of new architectural features and training procedures\nthat we apply to the generative adversarial networks (GANs) framework. We focus\non two applications of GANs: semi-supervised learning, and the generation of\nimages that humans find visually realistic. Unlike most work on generative\nmodels, our primary goal is not to train a model that assigns high likelihood\nto test data, nor do we require the model to be able to learn well without\nusing any labels. Using our new techniques, we achieve state-of-the-art results\nin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated\nimages are of high quality as confirmed by a visual Turing test: our model\ngenerates MNIST samples that humans cannot distinguish from real data, and\nCIFAR-10 samples that yield a human error rate of 21.3%. We also present\nImageNet samples with unprecedented resolution and show that our methods enable\nthe model to learn recognizable features of ImageNet classes. \n\n"}
{"id": "1606.03568", "contents": "Title: Word Sense Disambiguation using a Bidirectional LSTM Abstract: In this paper we present a clean, yet effective, model for word sense\ndisambiguation. Our approach leverage a bidirectional long short-term memory\nnetwork which is shared between all words. This enables the model to share\nstatistical strength and to scale well with vocabulary size. The model is\ntrained end-to-end, directly from the raw text to sense labels, and makes\neffective use of word order. We evaluate our approach on two standard datasets,\nusing identical hyperparameter settings, which are in turn tuned on a third set\nof held out data. We employ no external resources (e.g. knowledge graphs,\npart-of-speech tagging, etc), language specific features, or hand crafted\nrules, but still achieve statistically equivalent results to the best\nstate-of-the-art systems, that employ no such limitations. \n\n"}
{"id": "1606.04052", "contents": "Title: Dialog state tracking, a machine reading approach using Memory Network Abstract: In an end-to-end dialog system, the aim of dialog state tracking is to\naccurately estimate a compact representation of the current dialog status from\na sequence of noisy observations produced by the speech recognition and the\nnatural language understanding modules. This paper introduces a novel method of\ndialog state tracking based on the general paradigm of machine reading and\nproposes to solve it using an End-to-End Memory Network, MemN2N, a\nmemory-enhanced neural network architecture. We evaluate the proposed approach\non the second Dialog State Tracking Challenge (DSTC-2) dataset. The corpus has\nbeen converted for the occasion in order to frame the hidden state variable\ninference as a question-answering task based on a sequence of utterances\nextracted from a dialog. We show that the proposed tracker gives encouraging\nresults. Then, we propose to extend the DSTC-2 dataset with specific reasoning\ncapabilities requirement like counting, list maintenance, yes-no question\nanswering and indefinite knowledge management. Finally, we present encouraging\nresults using our proposed MemN2N based tracking model. \n\n"}
{"id": "1606.04335", "contents": "Title: LLFR: A Lanczos-Based Latent Factor Recommender for Big Data Scenarios Abstract: The purpose if this master's thesis is to study and develop a new algorithmic\nframework for Collaborative Filtering to produce recommendations in the top-N\nrecommendation problem. Thus, we propose Lanczos Latent Factor Recommender\n(LLFR); a novel \"big data friendly\" collaborative filtering algorithm for top-N\nrecommendation. Using a computationally efficient Lanczos-based procedure, LLFR\nbuilds a low dimensional item similarity model, that can be readily exploited\nto produce personalized ranking vectors over the item space. A number of\nexperiments on real datasets indicate that LLFR outperforms other\nstate-of-the-art top-N recommendation methods from a computational as well as a\nqualitative perspective. Our experimental results also show that its relative\nperformance gains, compared to competing methods, increase as the data get\nsparser, as in the Cold Start Problem. More specifically, this is true both\nwhen the sparsity is generalized - as in the New Community Problem, a very\ncommon problem faced by real recommender systems in their beginning stages,\nwhen there is not sufficient number of ratings for the collaborative filtering\nalgorithms to uncover similarities between items or users - and in the very\ninteresting case where the sparsity is localized in a small fraction of the\ndataset - as in the New Users Problem, where new users are introduced to the\nsystem, they have not rated many items and thus, the CF algorithm can not make\nreliable personalized recommendations yet. \n\n"}
{"id": "1606.04351", "contents": "Title: TwiSE at SemEval-2016 Task 4: Twitter Sentiment Classification Abstract: This paper describes the participation of the team \"TwiSE\" in the SemEval\n2016 challenge. Specifically, we participated in Task 4, namely \"Sentiment\nAnalysis in Twitter\" for which we implemented sentiment classification systems\nfor subtasks A, B, C and D. Our approach consists of two steps. In the first\nstep, we generate and validate diverse feature sets for twitter sentiment\nevaluation, inspired by the work of participants of previous editions of such\nchallenges. In the second step, we focus on the optimization of the evaluation\nmeasures of the different subtasks. To this end, we examine different learning\nstrategies by validating them on the data provided by the task organisers. For\nour final submissions we used an ensemble learning approach (stacked\ngeneralization) for Subtask A and single linear models for the rest of the\nsubtasks. In the official leaderboard we were ranked 9/35, 8/19, 1/11 and 2/14\nfor subtasks A, B, C and D respectively.\\footnote{We make the code available\nfor research purposes at\n\\url{https://github.com/balikasg/SemEval2016-Twitter\\_Sentiment\\_Evaluation}.} \n\n"}
{"id": "1606.04460", "contents": "Title: Model-Free Episodic Control Abstract: State of the art deep reinforcement learning algorithms take many millions of\ninteractions to attain human-level performance. Humans, on the other hand, can\nvery quickly exploit highly rewarding nuances of an environment upon first\ndiscovery. In the brain, such rapid learning is thought to depend on the\nhippocampus and its capacity for episodic memory. Here we investigate whether a\nsimple model of hippocampal episodic control can learn to solve difficult\nsequential decision-making tasks. We demonstrate that it not only attains a\nhighly rewarding strategy significantly faster than state-of-the-art deep\nreinforcement learning algorithms, but also achieves a higher overall reward on\nsome of the more challenging domains. \n\n"}
{"id": "1606.04671", "contents": "Title: Progressive Neural Networks Abstract: Learning to solve complex sequences of tasks--while both leveraging transfer\nand avoiding catastrophic forgetting--remains a key obstacle to achieving\nhuman-level intelligence. The progressive networks approach represents a step\nforward in this direction: they are immune to forgetting and can leverage prior\nknowledge via lateral connections to previously learned features. We evaluate\nthis architecture extensively on a wide variety of reinforcement learning tasks\n(Atari and 3D maze games), and show that it outperforms common baselines based\non pretraining and finetuning. Using a novel sensitivity measure, we\ndemonstrate that transfer occurs at both low-level sensory and high-level\ncontrol layers of the learned policy. \n\n"}
{"id": "1606.05029", "contents": "Title: No Need to Pay Attention: Simple Recurrent Neural Networks Work! (for\n  Answering \"Simple\" Questions) Abstract: First-order factoid question answering assumes that the question can be\nanswered by a single fact in a knowledge base (KB). While this does not seem\nlike a challenging task, many recent attempts that apply either complex\nlinguistic reasoning or deep neural networks achieve 65%-76% accuracy on\nbenchmark sets. Our approach formulates the task as two machine learning\nproblems: detecting the entities in the question, and classifying the question\nas one of the relation types in the KB. We train a recurrent neural network to\nsolve each problem. On the SimpleQuestions dataset, our approach yields\nsubstantial improvements over previously published results --- even neural\nnetworks based on much more complex architectures. The simplicity of our\napproach also has practical advantages, such as efficiency and modularity, that\nare valuable especially in an industry setting. In fact, we present a\npreliminary analysis of the performance of our model on real queries from\nComcast's X1 entertainment platform with millions of users every day. \n\n"}
{"id": "1606.05464", "contents": "Title: Stance Detection with Bidirectional Conditional Encoding Abstract: Stance detection is the task of classifying the attitude expressed in a text\ntowards a target such as Hillary Clinton to be \"positive\", negative\" or\n\"neutral\". Previous work has assumed that either the target is mentioned in the\ntext or that training data for every target is given. This paper considers the\nmore challenging version of this task, where targets are not always mentioned\nand no training data is available for the test targets. We experiment with\nconditional LSTM encoding, which builds a representation of the tweet that is\ndependent on the target, and demonstrate that it outperforms encoding the tweet\nand the target independently. Performance is improved further when the\nconditional model is augmented with bidirectional encoding. We evaluate our\napproach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving\nperformance second best only to a system trained on semi-automatically labelled\ntweets for the test target. When such weak supervision is added, our approach\nachieves state-of-the-art results. \n\n"}
{"id": "1606.05491", "contents": "Title: Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax\n  Trees and Strings Abstract: We present a natural language generator based on the sequence-to-sequence\napproach that can be trained to produce natural language strings as well as\ndeep syntax dependency trees from input dialogue acts, and we use it to\ndirectly compare two-step generation with separate sentence planning and\nsurface realization stages to a joint, one-step approach. We were able to train\nboth setups successfully using very little training data. The joint setup\noffers better performance, surpassing state-of-the-art with regards to\nn-gram-based scores while providing more relevant outputs. \n\n"}
{"id": "1606.06142", "contents": "Title: Comparing the hierarchy of keywords in on-line news portals Abstract: The tagging of on-line content with informative keywords is a widespread\nphenomenon from scientific article repositories through blogs to on-line news\nportals. In most of the cases, the tags on a given item are free words chosen\nby the authors independently. Therefore, relations among keywords in a\ncollection of news items is unknown. However, in most cases the topics and\nconcepts described by these keywords are forming a latent hierarchy, with the\nmore general topics and categories at the top, and more specialised ones at the\nbottom. Here we apply a recent, cooccurrence-based tag hierarchy extraction\nmethod to sets of keywords obtained from four different on-line news portals.\nThe resulting hierarchies show substantial differences not just in the topics\nrendered as important (being at the top of the hierarchy) or of less interest\n(categorised low in the hierarchy), but also in the underlying network\nstructure. This reveals discrepancies between the plausible keyword association\nframeworks in the studied news portals. \n\n"}
{"id": "1606.06164", "contents": "Title: Pragmatic factors in image description: the case of negations Abstract: We provide a qualitative analysis of the descriptions containing negations\n(no, not, n't, nobody, etc) in the Flickr30K corpus, and a categorization of\nnegation uses. Based on this analysis, we provide a set of requirements that an\nimage description system should have in order to generate negation sentences.\nAs a pilot experiment, we used our categorization to manually annotate\nsentences containing negations in the Flickr30K corpus, with an agreement score\nof K=0.67. With this paper, we hope to open up a broader discussion of\nsubjective language in image descriptions. \n\n"}
{"id": "1606.06812", "contents": "Title: Link Prediction via Matrix Completion Abstract: Inspired by practical importance of social networks, economic networks,\nbiological networks and so on, studies on large and complex networks have\nattracted a surge of attentions in the recent years. Link prediction is a\nfundamental issue to understand the mechanisms by which new links are added to\nthe networks. We introduce the method of robust principal component analysis\n(robust PCA) into link prediction, and estimate the missing entries of the\nadjacency matrix. On one hand, our algorithm is based on the sparsity and low\nrank property of the matrix, on the other hand, it also performs very well when\nthe network is dense. This is because a relatively dense real network is also\nsparse in comparison to the complete graph. According to extensive experiments\non real networks from disparate fields, when the target network is connected\nand sufficiently dense, whatever it is weighted or unweighted, our method is\ndemonstrated to be very effective and with prediction accuracy being\nconsiderably improved comparing with many state-of-the-art algorithms. \n\n"}
{"id": "1606.07188", "contents": "Title: Selective Term Proximity Scoring Via BP-ANN Abstract: When two terms occur together in a document, the probability of a close\nrelationship between them and the document itself is greater if they are in\nnearby positions. However, ranking functions including term proximity (TP)\nrequire larger indexes than traditional document-level indexing, which slows\ndown query processing. Previous studies also show that this technique is not\neffective for all types of queries. Here we propose a document ranking model\nwhich decides for which queries it would be beneficial to use a proximity-based\nranking, based on a collection of features of the query. We use a machine\nlearning approach in determining whether utilizing TP will be beneficial.\nExperiments show that the proposed model returns improved rankings while also\nreducing the overhead incurred as a result of using TP statistics. \n\n"}
{"id": "1606.07298", "contents": "Title: Explaining Predictions of Non-Linear Classifiers in NLP Abstract: Layer-wise relevance propagation (LRP) is a recently proposed technique for\nexplaining predictions of complex non-linear classifiers in terms of input\nvariables. In this paper, we apply LRP for the first time to natural language\nprocessing (NLP). More precisely, we use it to explain the predictions of a\nconvolutional neural network (CNN) trained on a topic categorization task. Our\nanalysis highlights which words are relevant for a specific prediction of the\nCNN. We compare our technique to standard sensitivity analysis, both\nqualitatively and quantitatively, using a \"word deleting\" perturbation\nexperiment, a PCA analysis, and various visualizations. All experiments\nvalidate the suitability of LRP for explaining the CNN predictions, which is\nalso in line with results reported in recent image classification studies. \n\n"}
{"id": "1606.07493", "contents": "Title: Sort Story: Sorting Jumbled Images and Captions into Stories Abstract: Temporal common sense has applications in AI tasks such as QA, multi-document\nsummarization, and human-AI communication. We propose the task of sequencing --\ngiven a jumbled set of aligned image-caption pairs that belong to a story, the\ntask is to sort them such that the output sequence forms a coherent story. We\npresent multiple approaches, via unary (position) and pairwise (order)\npredictions, and their ensemble-based combinations, achieving strong results on\nthis task. We use both text-based and image-based features, which depict\ncomplementary improvements. Using qualitative examples, we demonstrate that our\nmodels have learnt interesting aspects of temporal common sense. \n\n"}
{"id": "1606.07496", "contents": "Title: Is a Picture Worth Ten Thousand Words in a Review Dataset? Abstract: While textual reviews have become prominent in many recommendation-based\nsystems, automated frameworks to provide relevant visual cues against text\nreviews where pictures are not available is a new form of task confronted by\ndata mining and machine learning researchers. Suggestions of pictures that are\nrelevant to the content of a review could significantly benefit the users by\nincreasing the effectiveness of a review. We propose a deep learning-based\nframework to automatically: (1) tag the images available in a review dataset,\n(2) generate a caption for each image that does not have one, and (3) enhance\neach review by recommending relevant images that might not be uploaded by the\ncorresponding reviewer. We evaluate the proposed framework using the Yelp\nChallenge Dataset. While a subset of the images in this particular dataset are\ncorrectly captioned, the majority of the pictures do not have any associated\ntext. Moreover, there is no mapping between reviews and images. Each image has\na corresponding business-tag where the picture was taken, though. The overall\ndata setting and unavailability of crucial pieces required for a mapping make\nthe problem of recommending images for reviews a major challenge. Qualitative\nand quantitative evaluations indicate that our proposed framework provides high\nquality enhancements through automatic captioning, tagging, and recommendation\nfor mapping reviews and images. \n\n"}
{"id": "1606.07953", "contents": "Title: Bidirectional Recurrent Neural Networks for Medical Event Detection in\n  Electronic Health Records Abstract: Sequence labeling for extraction of medical events and their attributes from\nunstructured text in Electronic Health Record (EHR) notes is a key step towards\nsemantic understanding of EHRs. It has important applications in health\ninformatics including pharmacovigilance and drug surveillance. The state of the\nart supervised machine learning models in this domain are based on Conditional\nRandom Fields (CRFs) with features calculated from fixed context windows. In\nthis application, we explored various recurrent neural network frameworks and\nshow that they significantly outperformed the CRF models. \n\n"}
{"id": "1606.08154", "contents": "Title: A Neural Network Approach to Joint Modeling Social Networks and Mobile\n  Trajectories Abstract: The accelerated growth of mobile trajectories in location-based services\nbrings valuable data resources to understand users' moving behaviors. Apart\nfrom recording the trajectory data, another major characteristic of these\nlocation-based services is that they also allow the users to connect whomever\nthey like. A combination of social networking and location-based services is\ncalled as location-based social networks (LBSN). As shown in previous works,\nlocations that are frequently visited by socially-related persons tend to be\ncorrelated, which indicates the close association between social connections\nand trajectory behaviors of users in LBSNs. In order to better analyze and mine\nLBSN data, we present a novel neural network model which can joint model both\nsocial networks and mobile trajectories. In specific, our model consists of two\ncomponents: the construction of social networks and the generation of mobile\ntrajectories. We first adopt a network embedding method for the construction of\nsocial networks: a networking representation can be derived for a user. The key\nof our model lies in the component of generating mobile trajectories. We have\nconsidered four factors that influence the generation process of mobile\ntrajectories, namely user visit preference, influence of friends, short-term\nsequential contexts and long-term sequential contexts. To characterize the last\ntwo contexts, we employ the RNN and GRU models to capture the sequential\nrelatedness in mobile trajectories at different levels, i.e., short term or\nlong term. Finally, the two components are tied by sharing the user network\nrepresentations. Experimental results on two important applications demonstrate\nthe effectiveness of our model. Especially, the improvement over baselines is\nmore significant when either network structure or trajectory data is sparse. \n\n"}
{"id": "1606.08689", "contents": "Title: Hierarchical Neural Language Models for Joint Representation of\n  Streaming Documents and their Content Abstract: We consider the problem of learning distributed representations for documents\nin data streams. The documents are represented as low-dimensional vectors and\nare jointly learned with distributed vector representations of word tokens\nusing a hierarchical framework with two embedded neural language models. In\nparticular, we exploit the context of documents in streams and use one of the\nlanguage models to model the document sequences, and the other to model word\nsequences within them. The models learn continuous vector representations for\nboth word tokens and documents such that semantically similar documents and\nwords are close in a common vector space. We discuss extensions to our model,\nwhich can be applied to personalized recommendation and social relationship\nmining by adding further user layers to the hierarchy, thus learning\nuser-specific vectors to represent individual preferences. We validated the\nlearned representations on a public movie rating data set from MovieLens, as\nwell as on a large-scale Yahoo News data comprising three months of user\nactivity logs collected on Yahoo servers. The results indicate that the\nproposed model can learn useful representations of both documents and word\ntokens, outperforming the current state-of-the-art by a large margin. \n\n"}
{"id": "1606.09239", "contents": "Title: Learning Concept Taxonomies from Multi-modal Data Abstract: We study the problem of automatically building hypernym taxonomies from\ntextual and visual data. Previous works in taxonomy induction generally ignore\nthe increasingly prominent visual data, which encode important perceptual\nsemantics. Instead, we propose a probabilistic model for taxonomy induction by\njointly leveraging text and images. To avoid hand-crafted feature engineering,\nwe design end-to-end features based on distributed representations of images\nand words. The model is discriminatively trained given a small set of existing\nontologies and is capable of building full taxonomies from scratch for a\ncollection of unseen conceptual label items with associated images. We evaluate\nour model and features on the WordNet hierarchies, where our system outperforms\nprevious approaches by a large gap. \n\n"}
{"id": "1607.00198", "contents": "Title: Sharing Network Parameters for Crosslingual Named Entity Recognition Abstract: Most state of the art approaches for Named Entity Recognition rely on hand\ncrafted features and annotated corpora. Recently Neural network based models\nhave been proposed which do not require handcrafted features but still require\nannotated corpora. However, such annotated corpora may not be available for\nmany languages. In this paper, we propose a neural network based model which\nallows sharing the decoder as well as word and character level parameters\nbetween two languages thereby allowing a resource fortunate language to aid a\nresource deprived language. Specifically, we focus on the case when limited\nannotated corpora is available in one language ($L_1$) and abundant annotated\ncorpora is available in another language ($L_2$). Sharing the network\narchitecture and parameters between $L_1$ and $L_2$ leads to improved\nperformance in $L_1$. Further, our approach does not require any hand crafted\nfeatures but instead directly learns meaningful feature representations from\nthe training data itself. We experiment with 4 language pairs and show that\nindeed in a resource constrained setup (lesser annotated corpora), a model\njointly trained with data from another language performs better than a model\ntrained only on the limited corpora in one language. \n\n"}
{"id": "1607.01097", "contents": "Title: AdaNet: Adaptive Structural Learning of Artificial Neural Networks Abstract: We present new algorithms for adaptively learning artificial neural networks.\nOur algorithms (AdaNet) adaptively learn both the structure of the network and\nits weights. They are based on a solid theoretical analysis, including\ndata-dependent generalization guarantees that we prove and discuss in detail.\nWe report the results of large-scale experiments with one of our algorithms on\nseveral binary classification tasks extracted from the CIFAR-10 dataset. The\nresults demonstrate that our algorithm can automatically learn network\nstructures with very competitive performance accuracies when compared with\nthose achieved for neural networks found by standard approaches. \n\n"}
{"id": "1607.01133", "contents": "Title: Learning when to trust distant supervision: An application to\n  low-resource POS tagging using cross-lingual projection Abstract: Cross lingual projection of linguistic annotation suffers from many sources\nof bias and noise, leading to unreliable annotations that cannot be used\ndirectly. In this paper, we introduce a novel approach to sequence tagging that\nlearns to correct the errors from cross-lingual projection using an explicit\ndebiasing layer. This is framed as joint learning over two corpora, one tagged\nwith gold standard and the other with projected tags. We evaluated with only\n1,000 tokens tagged with gold standard tags, along with more plentiful parallel\ndata. Our system equals or exceeds the state-of-the-art on eight simulated\nlow-resource settings, as well as two real low-resource languages, Malagasy and\nKinyarwanda. \n\n"}
{"id": "1607.01432", "contents": "Title: Global Neural CCG Parsing with Optimality Guarantees Abstract: We introduce the first global recursive neural parsing model with optimality\nguarantees during decoding. To support global features, we give up dynamic\nprograms and instead search directly in the space of all possible subtrees.\nAlthough this space is exponentially large in the sentence length, we show it\nis possible to learn an efficient A* parser. We augment existing parsing\nmodels, which have informative bounds on the outside score, with a global model\nthat has loose bounds but only needs to model non-local phenomena. The global\nmodel is trained with a new objective that encourages the parser to explore a\ntiny fraction of the search space. The approach is applied to CCG parsing,\nimproving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal\nparse for 99.9% of held-out sentences, exploring on average only 190 subtrees. \n\n"}
{"id": "1607.02250", "contents": "Title: Consensus Attention-based Neural Networks for Chinese Reading\n  Comprehension Abstract: Reading comprehension has embraced a booming in recent NLP research. Several\ninstitutes have released the Cloze-style reading comprehension data, and these\nhave greatly accelerated the research of machine comprehension. In this work,\nwe firstly present Chinese reading comprehension datasets, which consist of\nPeople Daily news dataset and Children's Fairy Tale (CFT) dataset. Also, we\npropose a consensus attention-based neural network architecture to tackle the\nCloze-style reading comprehension problem, which aims to induce a consensus\nattention over every words in the query. Experimental results show that the\nproposed neural network significantly outperforms the state-of-the-art\nbaselines in several public datasets. Furthermore, we setup a baseline for\nChinese reading comprehension task, and hopefully this would speed up the\nprocess for future research. \n\n"}
{"id": "1607.02467", "contents": "Title: Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior\n  Knowledge Abstract: We introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural\nNetworks that replaces the softmax output layer by a log-linear output layer,\nof which the softmax is a special case. This conceptually simple move has two\nmain advantages. First, it allows the learner to combat training data sparsity\nby allowing it to model words (or more generally, output symbols) as complex\ncombinations of attributes without requiring that each combination is directly\nobserved in the training data (as the softmax does). Second, it permits the\ninclusion of flexible prior knowledge in the form of a priori specified modular\nfeatures, where the neural network component learns to dynamically control the\nweights of a log-linear distribution exploiting these features.\n  We conduct experiments in the domain of language modelling of French, that\nexploit morphological prior knowledge and show an important decrease in\nperplexity relative to a baseline RNN.\n  We provide other motivating iillustrations, and finally argue that the\nlog-linear and the neural-network components contribute complementary strengths\nto the LL-RNN: the LL aspect allows the model to incorporate rich prior\nknowledge, while the NN aspect, according to the \"representation learning\"\nparadigm, allows the model to discover novel combination of characteristics. \n\n"}
{"id": "1607.03474", "contents": "Title: Recurrent Highway Networks Abstract: Many sequential processing tasks require complex nonlinear transition\nfunctions from one step to the next. However, recurrent neural networks with\n'deep' transition functions remain difficult to train, even when using Long\nShort-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of\nrecurrent networks based on Gersgorin's circle theorem that illuminates several\nmodeling and optimization issues and improves our understanding of the LSTM\ncell. Based on this analysis we propose Recurrent Highway Networks, which\nextend the LSTM architecture to allow step-to-step transition depths larger\nthan one. Several language modeling experiments demonstrate that the proposed\narchitecture results in powerful and efficient models. On the Penn Treebank\ncorpus, solely increasing the transition depth from 1 to 10 improves word-level\nperplexity from 90.6 to 65.4 using the same number of parameters. On the larger\nWikipedia datasets for character prediction (text8 and enwik8), RHNs outperform\nall previous results and achieve an entropy of 1.27 bits per character. \n\n"}
{"id": "1607.03474", "contents": "Title: Recurrent Highway Networks Abstract: Many sequential processing tasks require complex nonlinear transition\nfunctions from one step to the next. However, recurrent neural networks with\n'deep' transition functions remain difficult to train, even when using Long\nShort-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of\nrecurrent networks based on Gersgorin's circle theorem that illuminates several\nmodeling and optimization issues and improves our understanding of the LSTM\ncell. Based on this analysis we propose Recurrent Highway Networks, which\nextend the LSTM architecture to allow step-to-step transition depths larger\nthan one. Several language modeling experiments demonstrate that the proposed\narchitecture results in powerful and efficient models. On the Penn Treebank\ncorpus, solely increasing the transition depth from 1 to 10 improves word-level\nperplexity from 90.6 to 65.4 using the same number of parameters. On the larger\nWikipedia datasets for character prediction (text8 and enwik8), RHNs outperform\nall previous results and achieve an entropy of 1.27 bits per character. \n\n"}
{"id": "1607.04110", "contents": "Title: Using Recurrent Neural Network for Learning Expressive Ontologies Abstract: Recently, Neural Networks have been proven extremely effective in many\nnatural language processing tasks such as sentiment analysis, question\nanswering, or machine translation. Aiming to exploit such advantages in the\nOntology Learning process, in this technical report we present a detailed\ndescription of a Recurrent Neural Network based system to be used to pursue\nsuch goal. \n\n"}
{"id": "1607.04576", "contents": "Title: Neural Discourse Modeling of Conversations Abstract: Deep neural networks have shown recent promise in many language-related tasks\nsuch as the modeling of conversations. We extend RNN-based sequence to sequence\nmodels to capture the long range discourse across many turns of conversation.\nWe perform a sensitivity analysis on how much additional context affects\nperformance, and provide quantitative and qualitative evidence that these\nmodels are able to capture discourse relationships across multiple utterances.\nOur results quantifies how adding an additional RNN layer for modeling\ndiscourse improves the quality of output utterances and providing more of the\nprevious conversation as input also improves performance. By searching the\ngenerated outputs for specific discourse markers we show how neural discourse\nmodels can exhibit increased coherence and cohesion in conversations. \n\n"}
{"id": "1607.04606", "contents": "Title: Enriching Word Vectors with Subword Information Abstract: Continuous word representations, trained on large unlabeled corpora are\nuseful for many natural language processing tasks. Popular models that learn\nsuch representations ignore the morphology of words, by assigning a distinct\nvector to each word. This is a limitation, especially for languages with large\nvocabularies and many rare words. In this paper, we propose a new approach\nbased on the skipgram model, where each word is represented as a bag of\ncharacter $n$-grams. A vector representation is associated to each character\n$n$-gram; words being represented as the sum of these representations. Our\nmethod is fast, allowing to train models on large corpora quickly and allows us\nto compute word representations for words that did not appear in the training\ndata. We evaluate our word representations on nine different languages, both on\nword similarity and analogy tasks. By comparing to recently proposed\nmorphological word representations, we show that our vectors achieve\nstate-of-the-art performance on these tasks. \n\n"}
{"id": "1607.04683", "contents": "Title: On the efficient representation and execution of deep acoustic models Abstract: In this paper we present a simple and computationally efficient quantization\nscheme that enables us to reduce the resolution of the parameters of a neural\nnetwork from 32-bit floating point values to 8-bit integer values. The proposed\nquantization scheme leads to significant memory savings and enables the use of\noptimized hardware instructions for integer arithmetic, thus significantly\nreducing the cost of inference. Finally, we propose a \"quantization aware\"\ntraining process that applies the proposed scheme during network training and\nfind that it allows us to recover most of the loss in accuracy introduced by\nquantization. We validate the proposed techniques by applying them to a long\nshort-term memory-based acoustic model on an open-ended large vocabulary speech\nrecognition task. \n\n"}
{"id": "1607.05809", "contents": "Title: Neural Contextual Conversation Learning with Labeled Question-Answering\n  Pairs Abstract: Neural conversational models tend to produce generic or safe responses in\ndifferent contexts, e.g., reply \\textit{\"Of course\"} to narrative statements or\n\\textit{\"I don't know\"} to questions. In this paper, we propose an end-to-end\napproach to avoid such problem in neural generative models. Additional memory\nmechanisms have been introduced to standard sequence-to-sequence (seq2seq)\nmodels, so that context can be considered while generating sentences. Three\nseq2seq models, which memorize a fix-sized contextual vector from hidden input,\nhidden input/output and a gated contextual attention structure respectively,\nhave been trained and tested on a dataset of labeled question-answering pairs\nin Chinese. The model with contextual attention outperforms others including\nthe state-of-the-art seq2seq models on perplexity test. The novel contextual\nmodel generates diverse and robust responses, and is able to carry out\nconversations on a wide range of topics appropriately. \n\n"}
{"id": "1608.01281", "contents": "Title: Learning Online Alignments with Continuous Rewards Policy Gradient Abstract: Sequence-to-sequence models with soft attention had significant success in\nmachine translation, speech recognition, and question answering. Though capable\nand easy to use, they require that the entirety of the input sequence is\navailable at the beginning of inference, an assumption that is not valid for\ninstantaneous translation and speech recognition. To address this problem, we\npresent a new method for solving sequence-to-sequence problems using hard\nonline alignments instead of soft offline alignments. The online alignments\nmodel is able to start producing outputs without the need to first process the\nentire input sequence. A highly accurate online sequence-to-sequence model is\nuseful because it can be used to build an accurate voice-based instantaneous\ntranslator. Our model uses hard binary stochastic decisions to select the\ntimesteps at which outputs will be produced. The model is trained to produce\nthese stochastic decisions using a standard policy gradient method. In our\nexperiments, we show that this model achieves encouraging performance on TIMIT\nand Wall Street Journal (WSJ) speech recognition datasets. \n\n"}
{"id": "1608.01402", "contents": "Title: Interacting Conceptual Spaces Abstract: We propose applying the categorical compositional scheme of [6] to conceptual\nspace models of cognition. In order to do this we introduce the category of\nconvex relations as a new setting for categorical compositional semantics,\nemphasizing the convex structure important to conceptual space applications. We\nshow how conceptual spaces for composite types such as adjectives and verbs can\nbe constructed. We illustrate this new model on detailed examples. \n\n"}
{"id": "1608.01404", "contents": "Title: Quantifier Scope in Categorical Compositional Distributional Semantics Abstract: In previous work with J. Hedges, we formalised a generalised quantifiers\ntheory of natural language in categorical compositional distributional\nsemantics with the help of bialgebras. In this paper, we show how quantifier\nscope ambiguity can be represented in that setting and how this representation\ncan be generalised to branching quantifiers. \n\n"}
{"id": "1608.02071", "contents": "Title: Transferring Knowledge from Text to Predict Disease Onset Abstract: In many domains such as medicine, training data is in short supply. In such\ncases, external knowledge is often helpful in building predictive models. We\npropose a novel method to incorporate publicly available domain expertise to\nbuild accurate models. Specifically, we use word2vec models trained on a\ndomain-specific corpus to estimate the relevance of each feature's text\ndescription to the prediction problem. We use these relevance estimates to\nrescale the features, causing more important features to experience weaker\nregularization.\n  We apply our method to predict the onset of five chronic diseases in the next\nfive years in two genders and two age groups. Our rescaling approach improves\nthe accuracy of the model, particularly when there are few positive examples.\nFurthermore, our method selects 60% fewer features, easing interpretation by\nphysicians. Our method is applicable to other domains where feature and outcome\ndescriptions are available. \n\n"}
{"id": "1608.02717", "contents": "Title: Mean Box Pooling: A Rich Image Representation and Output Embedding for\n  the Visual Madlibs Task Abstract: We present Mean Box Pooling, a novel visual representation that pools over\nCNN representations of a large number, highly overlapping object proposals. We\nshow that such representation together with nCCA, a successful multimodal\nembedding technique, achieves state-of-the-art performance on the Visual\nMadlibs task. Moreover, inspired by the nCCA's objective function, we extend\nclassical CNN+LSTM approach to train the network by directly maximizing the\nsimilarity between the internal representation of the deep learning\narchitecture and candidate answers. Again, such approach achieves a significant\nimprovement over the prior work that also uses CNN+LSTM approach on Visual\nMadlibs. \n\n"}
{"id": "1608.04147", "contents": "Title: Numerically Grounded Language Models for Semantic Error Correction Abstract: Semantic error detection and correction is an important task for applications\nsuch as fact checking, speech-to-text or grammatical error correction. Current\napproaches generally focus on relatively shallow semantics and do not account\nfor numeric quantities. Our approach uses language models grounded in numbers\nwithin the text. Such groundings are easily achieved for recurrent neural\nlanguage model architectures, which can be further conditioned on incomplete\nbackground knowledge bases. Our evaluation on clinical reports shows that\nnumerical grounding improves perplexity by 33% and F1 for semantic error\ncorrection by 5 points when compared to ungrounded approaches. Conditioning on\na knowledge base yields further improvements. \n\n"}
{"id": "1608.05813", "contents": "Title: phi-LSTM: A Phrase-based Hierarchical LSTM Model for Image Captioning Abstract: A picture is worth a thousand words. Not until recently, however, we noticed\nsome success stories in understanding of visual scenes: a model that is able to\ndetect/name objects, describe their attributes, and recognize their\nrelationships/interactions. In this paper, we propose a phrase-based\nhierarchical Long Short-Term Memory (phi-LSTM) model to generate image\ndescription. The proposed model encodes sentence as a sequence of combination\nof phrases and words, instead of a sequence of words alone as in those\nconventional solutions. The two levels of this model are dedicated to i) learn\nto generate image relevant noun phrases, and ii) produce appropriate image\ndescription from the phrases and other words in the corpus. Adopting a\nconvolutional neural network to learn image features and the LSTM to learn the\nword sequence in a sentence, the proposed model has shown better or competitive\nresults in comparison to the state-of-the-art models on Flickr8k and Flickr30k\ndatasets. \n\n"}
{"id": "1608.06027", "contents": "Title: Surprisal-Driven Feedback in Recurrent Networks Abstract: Recurrent neural nets are widely used for predicting temporal data. Their\ninherent deep feedforward structure allows learning complex sequential\npatterns. It is believed that top-down feedback might be an important missing\ningredient which in theory could help disambiguate similar patterns depending\non broader context. In this paper we introduce surprisal-driven recurrent\nnetworks, which take into account past error information when making new\npredictions. This is achieved by continuously monitoring the discrepancy\nbetween most recent predictions and the actual observations. Furthermore, we\nshow that it outperforms other stochastic and fully deterministic approaches on\nenwik8 character level prediction task achieving 1.37 BPC on the test portion\nof the text. \n\n"}
{"id": "1608.06298", "contents": "Title: Infusing Collaborative Recommenders with Distributed Representations Abstract: Recommender systems assist users in navigating complex information spaces and\nfocus their attention on the content most relevant to their needs. Often these\nsystems rely on user activity or descriptions of the content. Social annotation\nsystems, in which users collaboratively assign tags to items, provide another\nmeans to capture information about users and items. Each of these data sources\nprovides unique benefits, capturing different relationships.\n  In this paper, we propose leveraging multiple sources of data: ratings data\nas users report their affinity toward an item, tagging data as users assign\nannotations to items, and item data collected from an online database. Taken\ntogether, these datasets provide the opportunity to learn rich distributed\nrepresentations by exploiting recent advances in neural network architectures.\nWe first produce representations that subjectively capture interesting\nrelationships among the data. We then empirically evaluate the utility of the\nrepresentations to predict a user's rating on an item and show that it\noutperforms more traditional representations. Finally, we demonstrate that\ntraditional representations can be combined with representations trained\nthrough a neural network to achieve even better results. \n\n"}
{"id": "1608.06651", "contents": "Title: Unsupervised, Efficient and Semantic Expertise Retrieval Abstract: We introduce an unsupervised discriminative model for the task of retrieving\nexperts in online document collections. We exclusively employ textual evidence\nand avoid explicit feature engineering by learning distributed word\nrepresentations in an unsupervised way. We compare our model to\nstate-of-the-art unsupervised statistical vector space and probabilistic\ngenerative approaches. Our proposed log-linear model achieves the retrieval\nperformance levels of state-of-the-art document-centric methods with the low\ninference cost of so-called profile-centric approaches. It yields a\nstatistically significant improved ranking over vector space and generative\nmodels in most cases, matching the performance of supervised methods on various\nbenchmarks. That is, by using solely text we can do as well as methods that\nwork with external evidence and/or relevance feedback. A contrastive analysis\nof rankings produced by discriminative and generative approaches shows that\nthey have complementary strengths due to the ability of the unsupervised\ndiscriminative model to perform semantic matching. \n\n"}
{"id": "1608.07639", "contents": "Title: Learning to generalize to new compositions in image understanding Abstract: Recurrent neural networks have recently been used for learning to describe\nimages using natural language. However, it has been observed that these models\ngeneralize poorly to scenes that were not observed during training, possibly\ndepending too strongly on the statistics of the text in the training data. Here\nwe propose to describe images using short structured representations, aiming to\ncapture the crux of a description. These structured representations allow us to\ntease-out and evaluate separately two types of generalization: standard\ngeneralization to new images with similar scenes, and generalization to new\ncombinations of known entities. We compare two learning approaches on the\nMS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show,\nAttend and Tell), and a simple structured prediction model on top of a deep\nnetwork. We find that the structured model generalizes to new compositions\nsubstantially better than the LSTM, ~7 times the accuracy of predicting\nstructured representations. By providing a concrete method to quantify\ngeneralization for unseen combinations, we argue that structured\nrepresentations and compositional splits are a useful benchmark for image\ncaptioning, and advocate compositional models that capture linguistic and\nvisual structure. \n\n"}
{"id": "1608.08716", "contents": "Title: Measuring Machine Intelligence Through Visual Question Answering Abstract: As machines have become more intelligent, there has been a renewed interest\nin methods for measuring their intelligence. A common approach is to propose\ntasks for which a human excels, but one which machines find difficult. However,\nan ideal task should also be easy to evaluate and not be easily gameable. We\nbegin with a case study exploring the recently popular task of image captioning\nand its limitations as a task for measuring machine intelligence. An\nalternative and more promising task is Visual Question Answering that tests a\nmachine's ability to reason about language and vision. We describe a dataset\nunprecedented in size created for the task that contains over 760,000 human\ngenerated questions about images. Using around 10 million human generated\nanswers, machines may be easily evaluated. \n\n"}
{"id": "1609.00777", "contents": "Title: Towards End-to-End Reinforcement Learning of Dialogue Agents for\n  Information Access Abstract: This paper proposes KB-InfoBot -- a multi-turn dialogue agent which helps\nusers search Knowledge Bases (KBs) without composing complicated queries. Such\ngoal-oriented dialogue agents typically need to interact with an external\ndatabase to access real-world knowledge. Previous systems achieved this by\nissuing a symbolic query to the KB to retrieve entries based on their\nattributes. However, such symbolic operations break the differentiability of\nthe system and prevent end-to-end training of neural dialogue agents. In this\npaper, we address this limitation by replacing symbolic queries with an induced\n\"soft\" posterior distribution over the KB that indicates which entities the\nuser is interested in. Integrating the soft retrieval process with a\nreinforcement learner leads to higher task success rate and reward in both\nsimulations and against real users. We also present a fully neural end-to-end\nagent, trained entirely from user feedback, and discuss its application towards\npersonalized dialogue agents. The source code is available at\nhttps://github.com/MiuLab/KB-InfoBot. \n\n"}
{"id": "1609.01704", "contents": "Title: Hierarchical Multiscale Recurrent Neural Networks Abstract: Learning both hierarchical and temporal representation has been among the\nlong-standing challenges of recurrent neural networks. Multiscale recurrent\nneural networks have been considered as a promising approach to resolve this\nissue, yet there has been a lack of empirical evidence showing that this type\nof models can actually capture the temporal dependencies by discovering the\nlatent hierarchical structure of the sequence. In this paper, we propose a\nnovel multiscale approach, called the hierarchical multiscale recurrent neural\nnetworks, which can capture the latent hierarchical structure in the sequence\nby encoding the temporal dependencies with different timescales using a novel\nupdate mechanism. We show some evidence that our proposed multiscale\narchitecture can discover underlying hierarchical structure in the sequences\nwithout using explicit boundary information. We evaluate our proposed model on\ncharacter-level language modelling and handwriting sequence modelling. \n\n"}
{"id": "1609.02907", "contents": "Title: Semi-Supervised Classification with Graph Convolutional Networks Abstract: We present a scalable approach for semi-supervised learning on\ngraph-structured data that is based on an efficient variant of convolutional\nneural networks which operate directly on graphs. We motivate the choice of our\nconvolutional architecture via a localized first-order approximation of\nspectral graph convolutions. Our model scales linearly in the number of graph\nedges and learns hidden layer representations that encode both local graph\nstructure and features of nodes. In a number of experiments on citation\nnetworks and on a knowledge graph dataset we demonstrate that our approach\noutperforms related methods by a significant margin. \n\n"}
{"id": "1609.03441", "contents": "Title: Read, Tag, and Parse All at Once, or Fully-neural Dependency Parsing Abstract: We present a dependency parser implemented as a single deep neural network\nthat reads orthographic representations of words and directly generates\ndependencies and their labels. Unlike typical approaches to parsing, the model\ndoesn't require part-of-speech (POS) tagging of the sentences. With proper\nregularization and additional supervision achieved with multitask learning we\nreach state-of-the-art performance on Slavic languages from the Universal\nDependencies treebank: with no linguistic features other than characters, our\nparser is as accurate as a transition- based system trained on perfect POS\ntags. \n\n"}
{"id": "1609.03777", "contents": "Title: Character-Level Language Modeling with Hierarchical Recurrent Neural\n  Networks Abstract: Recurrent neural network (RNN) based character-level language models (CLMs)\nare extremely useful for modeling out-of-vocabulary words by nature. However,\ntheir performance is generally much worse than the word-level language models\n(WLMs), since CLMs need to consider longer history of tokens to properly\npredict the next one. We address this problem by proposing hierarchical RNN\narchitectures, which consist of multiple modules with different timescales.\nDespite the multi-timescale structures, the input and output layers operate\nwith the character-level clock, which allows the existing RNN CLM training\napproaches to be directly applicable without any modifications. Our CLM models\nshow better perplexity than Kneser-Ney (KN) 5-gram WLMs on the One Billion Word\nBenchmark with only 2% of parameters. Also, we present real-time\ncharacter-level end-to-end speech recognition examples on the Wall Street\nJournal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the\nproposed models results in better recognition accuracies even though the number\nof parameters are reduced to 30%. \n\n"}
{"id": "1609.05796", "contents": "Title: Enabling Dark Energy Science with Deep Generative Models of Galaxy\n  Images Abstract: Understanding the nature of dark energy, the mysterious force driving the\naccelerated expansion of the Universe, is a major challenge of modern\ncosmology. The next generation of cosmological surveys, specifically designed\nto address this issue, rely on accurate measurements of the apparent shapes of\ndistant galaxies. However, shape measurement methods suffer from various\nunavoidable biases and therefore will rely on a precise calibration to meet the\naccuracy requirements of the science analysis. This calibration process remains\nan open challenge as it requires large sets of high quality galaxy images. To\nthis end, we study the application of deep conditional generative models in\ngenerating realistic galaxy images. In particular we consider variations on\nconditional variational autoencoder and introduce a new adversarial objective\nfor training of conditional generative networks. Our results suggest a reliable\nalternative to the acquisition of expensive high quality observations for\ngenerating the calibration data needed by the next generation of cosmological\nsurveys. \n\n"}
{"id": "1609.06038", "contents": "Title: Enhanced LSTM for Natural Language Inference Abstract: Reasoning and inference are central to human and artificial intelligence.\nModeling inference in human language is very challenging. With the availability\nof large annotated data (Bowman et al., 2015), it has recently become feasible\nto train neural network based inference models, which have shown to be very\neffective. In this paper, we present a new state-of-the-art result, achieving\nthe accuracy of 88.6% on the Stanford Natural Language Inference Dataset.\nUnlike the previous top models that use very complicated network architectures,\nwe first demonstrate that carefully designing sequential inference models based\non chain LSTMs can outperform all previous models. Based on this, we further\nshow that by explicitly considering recursive architectures in both local\ninference modeling and inference composition, we achieve additional\nimprovement. Particularly, incorporating syntactic parsing information\ncontributes to our best result---it further improves the performance even when\nadded to the already very strong model. \n\n"}
{"id": "1609.06127", "contents": "Title: A framework for mining process models from emails logs Abstract: Due to its wide use in personal, but most importantly, professional contexts,\nemail represents a valuable source of information that can be harvested for\nunderstanding, reengineering and repurposing undocumented business processes of\ncompanies and institutions. Towards this aim, a few researchers investigated\nthe problem of extracting process oriented information from email logs in order\nto take benefit of the many available process mining techniques and tools. In\nthis paper we go further in this direction, by proposing a new method for\nmining process models from email logs that leverage unsupervised machine\nlearning techniques with little human involvement. Moreover, our method allows\nto semi-automatically label emails with activity names, that can be used for\nactivity recognition in new incoming emails. A use case demonstrates the\nusefulness of the proposed solution using a modest in size, yet real-world,\ndataset containing emails that belong to two different process models. \n\n"}
{"id": "1609.06457", "contents": "Title: AMOS: An Automated Model Order Selection Algorithm for Spectral Graph\n  Clustering Abstract: One of the longstanding problems in spectral graph clustering (SGC) is the\nso-called model order selection problem: automated selection of the correct\nnumber of clusters. This is equivalent to the problem of finding the number of\nconnected components or communities in an undirected graph. In this paper, we\npropose AMOS, an automated model order selection algorithm for SGC. Based on a\nrecent analysis of clustering reliability for SGC under the random\ninterconnection model, AMOS works by incrementally increasing the number of\nclusters, estimating the quality of identified clusters, and providing a series\nof clustering reliability tests. Consequently, AMOS outputs clusters of minimal\nmodel order with statistical clustering reliability guarantees. Comparing to\nthree other automated graph clustering methods on real-world datasets, AMOS\nshows superior performance in terms of multiple external and internal\nclustering metrics. \n\n"}
{"id": "1609.06578", "contents": "Title: Twitter Opinion Topic Model: Extracting Product Opinions from Tweets by\n  Leveraging Hashtags and Sentiment Lexicon Abstract: Aspect-based opinion mining is widely applied to review data to aggregate or\nsummarize opinions of a product, and the current state-of-the-art is achieved\nwith Latent Dirichlet Allocation (LDA)-based model. Although social media data\nlike tweets are laden with opinions, their \"dirty\" nature (as natural language)\nhas discouraged researchers from applying LDA-based opinion model for product\nreview mining. Tweets are often informal, unstructured and lacking labeled data\nsuch as categories and ratings, making it challenging for product opinion\nmining. In this paper, we propose an LDA-based opinion model named Twitter\nOpinion Topic Model (TOTM) for opinion mining and sentiment analysis. TOTM\nleverages hashtags, mentions, emoticons and strong sentiment words that are\npresent in tweets in its discovery process. It improves opinion prediction by\nmodeling the target-opinion interaction directly, thus discovering target\nspecific opinion words, neglected in existing approaches. Moreover, we propose\na new formulation of incorporating sentiment prior information into a topic\nmodel, by utilizing an existing public sentiment lexicon. This is novel in that\nit learns and updates with the data. We conduct experiments on 9 million tweets\non electronic products, and demonstrate the improved performance of TOTM in\nboth quantitative evaluations and qualitative analysis. We show that\naspect-based opinion analysis on massive volume of tweets provides useful\nopinions on products. \n\n"}
{"id": "1609.06686", "contents": "Title: Character-level and Multi-channel Convolutional Neural Networks for\n  Large-scale Authorship Attribution Abstract: Convolutional neural networks (CNNs) have demonstrated superior capability\nfor extracting information from raw signals in computer vision. Recently,\ncharacter-level and multi-channel CNNs have exhibited excellent performance for\nsentence classification tasks. We apply CNNs to large-scale authorship\nattribution, which aims to determine an unknown text's author among many\ncandidate authors, motivated by their ability to process character-level\nsignals and to differentiate between a large number of classes, while making\nfast predictions in comparison to state-of-the-art approaches. We extensively\nevaluate CNN-based approaches that leverage word and character channels and\ncompare them against state-of-the-art methods for a large range of author\nnumbers, shedding new light on traditional approaches. We show that\ncharacter-level CNNs outperform the state-of-the-art on four out of five\ndatasets in different domains. Additionally, we present the first application\nof authorship attribution to reddit. \n\n"}
{"id": "1609.07035", "contents": "Title: Abstractive Meeting Summarization UsingDependency Graph Fusion Abstract: Automatic summarization techniques on meeting conversations developed so far\nhave been primarily extractive, resulting in poor summaries. To improve this,\nwe propose an approach to generate abstractive summaries by fusing important\ncontent from several utterances. Any meeting is generally comprised of several\ndiscussion topic segments. For each topic segment within a meeting\nconversation, we aim to generate a one sentence summary from the most important\nutterances using an integer linear programming-based sentence fusion approach.\nExperimental results show that our method can generate more informative\nsummaries than the baselines. \n\n"}
{"id": "1609.07959", "contents": "Title: Multiplicative LSTM for sequence modelling Abstract: We introduce multiplicative LSTM (mLSTM), a recurrent neural network\narchitecture for sequence modelling that combines the long short-term memory\n(LSTM) and multiplicative recurrent neural network architectures. mLSTM is\ncharacterised by its ability to have different recurrent transition functions\nfor each possible input, which we argue makes it more expressive for\nautoregressive density estimation. We demonstrate empirically that mLSTM\noutperforms standard LSTM and its deep variants for a range of character level\nlanguage modelling tasks. In this version of the paper, we regularise mLSTM to\nachieve 1.27 bits/char on text8 and 1.24 bits/char on Hutter Prize. We also\napply a purely byte-level mLSTM on the WikiText-2 dataset to achieve a\ncharacter level entropy of 1.26 bits/char, corresponding to a word level\nperplexity of 88.8, which is comparable to word level LSTMs regularised in\nsimilar ways on the same task. \n\n"}
{"id": "1609.08084", "contents": "Title: Toward Socially-Infused Information Extraction: Embedding Authors,\n  Mentions, and Entities Abstract: Entity linking is the task of identifying mentions of entities in text, and\nlinking them to entries in a knowledge base. This task is especially difficult\nin microblogs, as there is little additional text to provide disambiguating\ncontext; rather, authors rely on an implicit common ground of shared knowledge\nwith their readers. In this paper, we attempt to capture some of this implicit\ncontext by exploiting the social network structure in microblogs. We build on\nthe theory of homophily, which implies that socially linked individuals share\ninterests, and are therefore likely to mention the same sorts of entities. We\nimplement this idea by encoding authors, mentions, and entities in a continuous\nvector space, which is constructed so that socially-connected authors have\nsimilar vector representations. These vectors are incorporated into a neural\nstructured prediction model, which captures structural constraints that are\ninherent in the entity linking task. Together, these design decisions yield F1\nimprovements of 1%-5% on benchmark datasets, as compared to the previous\nstate-of-the-art. \n\n"}
{"id": "1609.08144", "contents": "Title: Google's Neural Machine Translation System: Bridging the Gap between\n  Human and Machine Translation Abstract: Neural Machine Translation (NMT) is an end-to-end learning approach for\nautomated translation, with the potential to overcome many of the weaknesses of\nconventional phrase-based translation systems. Unfortunately, NMT systems are\nknown to be computationally expensive both in training and in translation\ninference. Also, most NMT systems have difficulty with rare words. These issues\nhave hindered NMT's use in practical deployments and services, where both\naccuracy and speed are essential. In this work, we present GNMT, Google's\nNeural Machine Translation system, which attempts to address many of these\nissues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder\nlayers using attention and residual connections. To improve parallelism and\ntherefore decrease training time, our attention mechanism connects the bottom\nlayer of the decoder to the top layer of the encoder. To accelerate the final\ntranslation speed, we employ low-precision arithmetic during inference\ncomputations. To improve handling of rare words, we divide words into a limited\nset of common sub-word units (\"wordpieces\") for both input and output. This\nmethod provides a good balance between the flexibility of \"character\"-delimited\nmodels and the efficiency of \"word\"-delimited models, naturally handles\ntranslation of rare words, and ultimately improves the overall accuracy of the\nsystem. Our beam search technique employs a length-normalization procedure and\nuses a coverage penalty, which encourages generation of an output sentence that\nis most likely to cover all the words in the source sentence. On the WMT'14\nEnglish-to-French and English-to-German benchmarks, GNMT achieves competitive\nresults to state-of-the-art. Using a human side-by-side evaluation on a set of\nisolated simple sentences, it reduces translation errors by an average of 60%\ncompared to Google's phrase-based production system. \n\n"}
{"id": "1609.08409", "contents": "Title: Modelling Radiological Language with Bidirectional Long Short-Term\n  Memory Networks Abstract: Motivated by the need to automate medical information extraction from\nfree-text radiological reports, we present a bi-directional long short-term\nmemory (BiLSTM) neural network architecture for modelling radiological\nlanguage. The model has been used to address two NLP tasks: medical\nnamed-entity recognition (NER) and negation detection. We investigate whether\nlearning several types of word embeddings improves BiLSTM's performance on\nthose tasks. Using a large dataset of chest x-ray reports, we compare the\nproposed model to a baseline dictionary-based NER system and a negation\ndetection system that leverages the hand-crafted rules of the NegEx algorithm\nand the grammatical relations obtained from the Stanford Dependency Parser.\nCompared to these more traditional rule-based systems, we argue that BiLSTM\noffers a strong alternative for both our tasks. \n\n"}
{"id": "1609.08496", "contents": "Title: Topic Modeling over Short Texts by Incorporating Word Embeddings Abstract: Inferring topics from the overwhelming amount of short texts becomes a\ncritical but challenging task for many content analysis tasks, such as content\ncharactering, user interest profiling, and emerging topic detecting. Existing\nmethods such as probabilistic latent semantic analysis (PLSA) and latent\nDirichlet allocation (LDA) cannot solve this prob- lem very well since only\nvery limited word co-occurrence information is available in short texts. This\npaper studies how to incorporate the external word correlation knowledge into\nshort texts to improve the coherence of topic modeling. Based on recent results\nin word embeddings that learn se- mantically representations for words from a\nlarge corpus, we introduce a novel method, Embedding-based Topic Model (ETM),\nto learn latent topics from short texts. ETM not only solves the problem of\nvery limited word co-occurrence information by aggregating short texts into\nlong pseudo- texts, but also utilizes a Markov Random Field regularized model\nthat gives correlated words a better chance to be put into the same topic. The\nexperiments on real-world datasets validate the effectiveness of our model\ncomparing with the state-of-the-art models. \n\n"}
{"id": "1609.08789", "contents": "Title: Memory Visualization for Gated Recurrent Neural Networks in Speech\n  Recognition Abstract: Recurrent neural networks (RNNs) have shown clear superiority in sequence\nmodeling, particularly the ones with gated units, such as long short-term\nmemory (LSTM) and gated recurrent unit (GRU). However, the dynamic properties\nbehind the remarkable performance remain unclear in many applications, e.g.,\nautomatic speech recognition (ASR). This paper employs visualization techniques\nto study the behavior of LSTM and GRU when performing speech recognition tasks.\nOur experiments show some interesting patterns in the gated memory, and some of\nthem have inspired simple yet effective modifications on the network structure.\nWe report two of such modifications: (1) lazy cell update in LSTM, and (2)\nshortcut connections for residual learning. Both modifications lead to more\ncomprehensible and powerful networks. \n\n"}
{"id": "1609.08843", "contents": "Title: Hierarchical Memory Networks for Answer Selection on Unknown Words Abstract: Recently, end-to-end memory networks have shown promising results on Question\nAnswering task, which encode the past facts into an explicit memory and perform\nreasoning ability by making multiple computational steps on the memory.\nHowever, memory networks conduct the reasoning on sentence-level memory to\noutput coarse semantic vectors and do not further take any attention mechanism\nto focus on words, which may lead to the model lose some detail information,\nespecially when the answers are rare or unknown words. In this paper, we\npropose a novel Hierarchical Memory Networks, dubbed HMN. First, we encode the\npast facts into sentence-level memory and word-level memory respectively. Then,\n(k)-max pooling is exploited following reasoning module on the sentence-level\nmemory to sample the (k) most relevant sentences to a question and feed these\nsentences into attention mechanism on the word-level memory to focus the words\nin the selected sentences. Finally, the prediction is jointly learned over the\noutputs of the sentence-level reasoning module and the word-level attention\nmechanism. The experimental results demonstrate that our approach successfully\nconducts answer selection on unknown words and achieves a better performance\nthan memory networks. \n\n"}
{"id": "1609.09106", "contents": "Title: HyperNetworks Abstract: This work explores hypernetworks: an approach of using a one network, also\nknown as a hypernetwork, to generate the weights for another network.\nHypernetworks provide an abstraction that is similar to what is found in\nnature: the relationship between a genotype - the hypernetwork - and a\nphenotype - the main network. Though they are also reminiscent of HyperNEAT in\nevolution, our hypernetworks are trained end-to-end with backpropagation and\nthus are usually faster. The focus of this work is to make hypernetworks useful\nfor deep convolutional networks and long recurrent networks, where\nhypernetworks can be viewed as relaxed form of weight-sharing across layers.\nOur main result is that hypernetworks can generate non-shared weights for LSTM\nand achieve near state-of-the-art results on a variety of sequence modelling\ntasks including character-level language modelling, handwriting generation and\nneural machine translation, challenging the weight-sharing paradigm for\nrecurrent networks. Our results also show that hypernetworks applied to\nconvolutional networks still achieve respectable results for image recognition\ntasks compared to state-of-the-art baseline models while requiring fewer\nlearnable parameters. \n\n"}
{"id": "1609.09189", "contents": "Title: Learning Sentence Representation with Guidance of Human Attention Abstract: Recently, much progress has been made in learning general-purpose sentence\nrepresentations that can be used across domains. However, most of the existing\nmodels typically treat each word in a sentence equally. In contrast, extensive\nstudies have proven that human read sentences efficiently by making a sequence\nof fixation and saccades. This motivates us to improve sentence representations\nby assigning different weights to the vectors of the component words, which can\nbe treated as an attention mechanism on single sentences. To that end, we\npropose two novel attention models, in which the attention weights are derived\nusing significant predictors of human reading time, i.e., Surprisal, POS tags\nand CCG supertags. The extensive experiments demonstrate that the proposed\nmethods significantly improve upon the state-of-the-art sentence representation\nmodels. \n\n"}
{"id": "1609.09315", "contents": "Title: Semantic Parsing with Semi-Supervised Sequential Autoencoders Abstract: We present a novel semi-supervised approach for sequence transduction and\napply it to semantic parsing. The unsupervised component is based on a\ngenerative model in which latent sentences generate the unpaired logical forms.\nWe apply this method to a number of semantic parsing tasks focusing on domains\nwith limited access to labelled training data and extend those datasets with\nsynthetically generated logical forms. \n\n"}
{"id": "1610.00479", "contents": "Title: Nonsymbolic Text Representation Abstract: We introduce the first generic text representation model that is completely\nnonsymbolic, i.e., it does not require the availability of a segmentation or\ntokenization method that attempts to identify words or other symbolic units in\ntext. This applies to training the parameters of the model on a training corpus\nas well as to applying it when computing the representation of a new text. We\nshow that our model performs better than prior work on an information\nextraction and a text denoising task. \n\n"}
{"id": "1610.01096", "contents": "Title: FLOCK: Combating Astroturfing on Livestreaming Platforms Abstract: Livestreaming platforms have become increasingly popular in recent years as a\nmeans of sharing and advertising creative content. Popular content streamers\nwho attract large viewership to their live broadcasts can earn a living by\nmeans of ad revenue, donations and channel subscriptions. Unfortunately, this\nincentivized popularity has simultaneously resulted in incentive for fraudsters\nto provide services to astroturf, or artificially inflate viewership metrics by\nproviding fake \"live\" views to customers. Our work provides a number of major\ncontributions: (a) formulation: we are the first to introduce and characterize\nthe viewbot fraud problem in livestreaming platforms, (b) methodology: we\npropose FLOCK, a principled and unsupervised method which efficiently and\neffectively identifies botted broadcasts and their constituent botted views,\nand (c) practicality: our approach achieves over 98% precision in identifying\nbotted broadcasts and over 90% precision/recall against sizable synthetically\ngenerated viewbot attacks on a real-world livestreaming workload of over 16\nmillion views and 92 thousand broadcasts. FLOCK successfully operates on larger\ndatasets in practice and is regularly used at a large, undisclosed\nlivestreaming corporation. \n\n"}
{"id": "1610.02906", "contents": "Title: A General Framework for Content-enhanced Network Representation Learning Abstract: This paper investigates the problem of network embedding, which aims at\nlearning low-dimensional vector representation of nodes in networks. Most\nexisting network embedding methods rely solely on the network structure, i.e.,\nthe linkage relationships between nodes, but ignore the rich content\ninformation associated with it, which is common in real world networks and\nbeneficial to describing the characteristics of a node. In this paper, we\npropose content-enhanced network embedding (CENE), which is capable of jointly\nleveraging the network structure and the content information. Our approach\nintegrates text modeling and structure modeling in a general framework by\ntreating the content information as a special kind of node. Experiments on\nseveral real world net- works with application to node classification show that\nour models outperform all existing network embedding methods, demonstrating the\nmerits of content information and joint learning. \n\n"}
{"id": "1610.03167", "contents": "Title: An Empirical Exploration of Skip Connections for Sequential Tagging Abstract: In this paper, we empirically explore the effects of various kinds of skip\nconnections in stacked bidirectional LSTMs for sequential tagging. We\ninvestigate three kinds of skip connections connecting to LSTM cells: (a) skip\nconnections to the gates, (b) skip connections to the internal states and (c)\nskip connections to the cell outputs. We present comprehensive experiments\nshowing that skip connections to cell outputs outperform the remaining two.\nFurthermore, we observe that using gated identity functions as skip mappings\nworks pretty well. Based on this novel skip connections, we successfully train\ndeep stacked bidirectional LSTM models and obtain state-of-the-art results on\nCCG supertagging and comparable results on POS tagging. \n\n"}
{"id": "1610.03585", "contents": "Title: A Paradigm for Situated and Goal-Driven Language Learning Abstract: A distinguishing property of human intelligence is the ability to flexibly\nuse language in order to communicate complex ideas with other humans in a\nvariety of contexts. Research in natural language dialogue should focus on\ndesigning communicative agents which can integrate themselves into these\ncontexts and productively collaborate with humans. In this abstract, we propose\na general situated language learning paradigm which is designed to bring about\nrobust language agents able to cooperate productively with humans. \n\n"}
{"id": "1610.04658", "contents": "Title: Simultaneous Learning of Trees and Representations for Extreme\n  Classification and Density Estimation Abstract: We consider multi-class classification where the predictor has a hierarchical\nstructure that allows for a very large number of labels both at train and test\ntime. The predictive power of such models can heavily depend on the structure\nof the tree, and although past work showed how to learn the tree structure, it\nexpected that the feature vectors remained static. We provide a novel algorithm\nto simultaneously perform representation learning for the input data and\nlearning of the hierarchi- cal predictor. Our approach optimizes an objec- tive\nfunction which favors balanced and easily- separable multi-way node partitions.\nWe theoret- ically analyze this objective, showing that it gives rise to a\nboosting style property and a bound on classification error. We next show how\nto extend the algorithm to conditional density estimation. We empirically\nvalidate both variants of the al- gorithm on text classification and language\nmod- eling, respectively, and show that they compare favorably to common\nbaselines in terms of accu- racy and running time. \n\n"}
{"id": "1610.05688", "contents": "Title: Low-rank and Sparse Soft Targets to Learn Better DNN Acoustic Models Abstract: Conventional deep neural networks (DNN) for speech acoustic modeling rely on\nGaussian mixture models (GMM) and hidden Markov model (HMM) to obtain binary\nclass labels as the targets for DNN training. Subword classes in speech\nrecognition systems correspond to context-dependent tied states or senones. The\npresent work addresses some limitations of GMM-HMM senone alignments for DNN\ntraining. We hypothesize that the senone probabilities obtained from a DNN\ntrained with binary labels can provide more accurate targets to learn better\nacoustic models. However, DNN outputs bear inaccuracies which are exhibited as\nhigh dimensional unstructured noise, whereas the informative components are\nstructured and low-dimensional. We exploit principle component analysis (PCA)\nand sparse coding to characterize the senone subspaces. Enhanced probabilities\nobtained from low-rank and sparse reconstructions are used as soft-targets for\nDNN acoustic modeling, that also enables training with untranscribed data.\nExperiments conducted on AMI corpus shows 4.6% relative reduction in word error\nrate. \n\n"}
{"id": "1610.06210", "contents": "Title: A Theme-Rewriting Approach for Generating Algebra Word Problems Abstract: Texts present coherent stories that have a particular theme or overall\nsetting, for example science fiction or western. In this paper, we present a\ntext generation method called {\\it rewriting} that edits existing\nhuman-authored narratives to change their theme without changing the underlying\nstory. We apply the approach to math word problems, where it might help\nstudents stay more engaged by quickly transforming all of their homework\nassignments to the theme of their favorite movie without changing the math\nconcepts that are being taught. Our rewriting method uses a two-stage decoding\nprocess, which proposes new words from the target theme and scores the\nresulting stories according to a number of factors defining aspects of\nsyntactic, semantic, and thematic coherence. Experiments demonstrate that the\nfinal stories typically represent the new theme well while still testing the\noriginal math concepts, outperforming a number of baselines. We also release a\nnew dataset of human-authored rewrites of math word problems in several themes. \n\n"}
{"id": "1610.07328", "contents": "Title: SSH (Sketch, Shingle, & Hash) for Indexing Massive-Scale Time Series Abstract: Similarity search on time series is a frequent operation in large-scale\ndata-driven applications. Sophisticated similarity measures are standard for\ntime series matching, as they are usually misaligned. Dynamic Time Warping or\nDTW is the most widely used similarity measure for time series because it\ncombines alignment and matching at the same time. However, the alignment makes\nDTW slow. To speed up the expensive similarity search with DTW, branch and\nbound based pruning strategies are adopted. However, branch and bound based\npruning are only useful for very short queries (low dimensional time series),\nand the bounds are quite weak for longer queries. Due to the loose bounds\nbranch and bound pruning strategy boils down to a brute-force search.\n  To circumvent this issue, we design SSH (Sketch, Shingle, & Hashing), an\nefficient and approximate hashing scheme which is much faster than the\nstate-of-the-art branch and bound searching technique: the UCR suite. SSH uses\na novel combination of sketching, shingling and hashing techniques to produce\n(probabilistic) indexes which align (near perfectly) with DTW similarity\nmeasure. The generated indexes are then used to create hash buckets for\nsub-linear search. Our results show that SSH is very effective for longer time\nsequence and prunes around 95% candidates, leading to the massive speedup in\nsearch with DTW. Empirical results on two large-scale benchmark time series\ndata show that our proposed method can be around 20 times faster than the\nstate-of-the-art package (UCR suite) without any significant loss in accuracy. \n\n"}
{"id": "1610.07710", "contents": "Title: EmojiNet: Building a Machine Readable Sense Inventory for Emoji Abstract: Emoji are a contemporary and extremely popular way to enhance electronic\ncommunication. Without rigid semantics attached to them, emoji symbols take on\ndifferent meanings based on the context of a message. Thus, like the word sense\ndisambiguation task in natural language processing, machines also need to\ndisambiguate the meaning or sense of an emoji. In a first step toward achieving\nthis goal, this paper presents EmojiNet, the first machine readable sense\ninventory for emoji. EmojiNet is a resource enabling systems to link emoji with\ntheir context-specific meaning. It is automatically constructed by integrating\nmultiple emoji resources with BabelNet, which is the most comprehensive\nmultilingual sense inventory available to date. The paper discusses its\nconstruction, evaluates the automatic resource creation process, and presents a\nuse case where EmojiNet disambiguates emoji usage in tweets. EmojiNet is\navailable online for use at http://emojinet.knoesis.org. \n\n"}
{"id": "1610.08613", "contents": "Title: Can Active Memory Replace Attention? Abstract: Several mechanisms to focus attention of a neural network on selected parts\nof its input or memory have been used successfully in deep learning models in\nrecent years. Attention has improved image classification, image captioning,\nspeech recognition, generative models, and learning algorithmic tasks, but it\nhad probably the largest impact on neural machine translation.\n  Recently, similar improvements have been obtained using alternative\nmechanisms that do not focus on a single part of a memory but operate on all of\nit in parallel, in a uniform way. Such mechanism, which we call active memory,\nimproved over attention in algorithmic tasks, image processing, and in\ngenerative modelling.\n  So far, however, active memory has not improved over attention for most\nnatural language processing tasks, in particular for machine translation. We\nanalyze this shortcoming in this paper and propose an extended model of active\nmemory that matches existing attention models on neural machine translation and\ngeneralizes better to longer sentences. We investigate this model and explain\nwhy previous active memory models did not succeed. Finally, we discuss when\nactive memory brings most benefits and where attention can be a better choice. \n\n"}
{"id": "1610.09769", "contents": "Title: Meta-Path Guided Embedding for Similarity Search in Large-Scale\n  Heterogeneous Information Networks Abstract: Most real-world data can be modeled as heterogeneous information networks\n(HINs) consisting of vertices of multiple types and their relationships. Search\nfor similar vertices of the same type in large HINs, such as bibliographic\nnetworks and business-review networks, is a fundamental problem with broad\napplications. Although similarity search in HINs has been studied previously,\nmost existing approaches neither explore rich semantic information embedded in\nthe network structures nor take user's preference as a guidance.\n  In this paper, we re-examine similarity search in HINs and propose a novel\nembedding-based framework. It models vertices as low-dimensional vectors to\nexplore network structure-embedded similarity. To accommodate user preferences\nat defining similarity semantics, our proposed framework, ESim, accepts\nuser-defined meta-paths as guidance to learn vertex vectors in a user-preferred\nembedding space. Moreover, an efficient and parallel sampling-based\noptimization algorithm has been developed to learn embeddings in large-scale\nHINs. Extensive experiments on real-world large-scale HINs demonstrate a\nsignificant improvement on the effectiveness of ESim over several\nstate-of-the-art algorithms as well as its scalability. \n\n"}
{"id": "1610.10099", "contents": "Title: Neural Machine Translation in Linear Time Abstract: We present a novel neural network for processing sequences. The ByteNet is a\none-dimensional convolutional neural network that is composed of two parts, one\nto encode the source sequence and the other to decode the target sequence. The\ntwo network parts are connected by stacking the decoder on top of the encoder\nand preserving the temporal resolution of the sequences. To address the\ndiffering lengths of the source and the target, we introduce an efficient\nmechanism by which the decoder is dynamically unfolded over the representation\nof the encoder. The ByteNet uses dilation in the convolutional layers to\nincrease its receptive field. The resulting network has two core properties: it\nruns in time that is linear in the length of the sequences and it sidesteps the\nneed for excessive memorization. The ByteNet decoder attains state-of-the-art\nperformance on character-level language modelling and outperforms the previous\nbest results obtained with recurrent networks. The ByteNet also achieves\nstate-of-the-art performance on character-to-character machine translation on\nthe English-to-German WMT translation task, surpassing comparable neural\ntranslation models that are based on recurrent networks with attentional\npooling and run in quadratic time. We find that the latent alignment structure\ncontained in the representations reflects the expected alignment between the\ntokens. \n\n"}
{"id": "1611.00350", "contents": "Title: Adversarial Influence Maximization Abstract: We consider the problem of influence maximization in fixed networks for\ncontagion models in an adversarial setting. The goal is to select an optimal\nset of nodes to seed the influence process, such that the number of influenced\nnodes at the conclusion of the campaign is as large as possible. We formulate\nthe problem as a repeated game between a player and adversary, where the\nadversary specifies the edges along which the contagion may spread, and the\nplayer chooses sets of nodes to influence in an online fashion. We establish\nupper and lower bounds on the minimax pseudo-regret in both undirected and\ndirected networks. \n\n"}
{"id": "1611.01259", "contents": "Title: Generalized Topic Modeling Abstract: Recently there has been significant activity in developing algorithms with\nprovable guarantees for topic modeling. In standard topic models, a topic (such\nas sports, business, or politics) is viewed as a probability distribution $\\vec\na_i$ over words, and a document is generated by first selecting a mixture $\\vec\nw$ over topics, and then generating words i.i.d. from the associated mixture\n$A{\\vec w}$. Given a large collection of such documents, the goal is to recover\nthe topic vectors and then to correctly classify new documents according to\ntheir topic mixture.\n  In this work we consider a broad generalization of this framework in which\nwords are no longer assumed to be drawn i.i.d. and instead a topic is a complex\ndistribution over sequences of paragraphs. Since one could not hope to even\nrepresent such a distribution in general (even if paragraphs are given using\nsome natural feature representation), we aim instead to directly learn a\ndocument classifier. That is, we aim to learn a predictor that given a new\ndocument, accurately predicts its topic mixture, without learning the\ndistributions explicitly. We present several natural conditions under which one\ncan do this efficiently and discuss issues such as noise tolerance and sample\ncomplexity in this model. More generally, our model can be viewed as a\ngeneralization of the multi-view or co-training setting in machine learning. \n\n"}
{"id": "1611.01368", "contents": "Title: Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies Abstract: The success of long short-term memory (LSTM) neural networks in language\nprocessing is typically attributed to their ability to capture long-distance\nstatistical regularities. Linguistic regularities are often sensitive to\nsyntactic structure; can such dependencies be captured by LSTMs, which do not\nhave explicit structural representations? We begin addressing this question\nusing number agreement in English subject-verb dependencies. We probe the\narchitecture's grammatical competence both using training objectives with an\nexplicit grammatical target (number prediction, grammaticality judgments) and\nusing language models. In the strongly supervised settings, the LSTM achieved\nvery high overall accuracy (less than 1% errors), but errors increased when\nsequential and structural information conflicted. The frequency of such errors\nrose sharply in the language-modeling setting. We conclude that LSTMs can\ncapture a non-trivial amount of grammatical structure given targeted\nsupervision, but stronger architectures may be required to further reduce\nerrors; furthermore, the language modeling signal is insufficient for capturing\nsyntax-sensitive dependencies, and should be supplemented with more direct\nsupervision if such dependencies need to be captured. \n\n"}
{"id": "1611.01462", "contents": "Title: Tying Word Vectors and Word Classifiers: A Loss Framework for Language\n  Modeling Abstract: Recurrent neural networks have been very successful at predicting sequences\nof words in tasks such as language modeling. However, all such models are based\non the conventional classification framework, where the model is trained\nagainst one-hot targets, and each word is represented both as an input and as\nan output in isolation. This causes inefficiencies in learning both in terms of\nutilizing all of the information and in terms of the number of parameters\nneeded to train. We introduce a novel theoretical framework that facilitates\nbetter learning in language modeling, and show that our framework leads to\ntying together the input embedding and the output projection matrices, greatly\nreducing the number of trainable variables. Our framework leads to state of the\nart performance on the Penn Treebank with a variety of network models. \n\n"}
{"id": "1611.01578", "contents": "Title: Neural Architecture Search with Reinforcement Learning Abstract: Neural networks are powerful and flexible models that work well for many\ndifficult learning tasks in image, speech and natural language understanding.\nDespite their success, neural networks are still hard to design. In this paper,\nwe use a recurrent network to generate the model descriptions of neural\nnetworks and train this RNN with reinforcement learning to maximize the\nexpected accuracy of the generated architectures on a validation set. On the\nCIFAR-10 dataset, our method, starting from scratch, can design a novel network\narchitecture that rivals the best human-invented architecture in terms of test\nset accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is\n0.09 percent better and 1.05x faster than the previous state-of-the-art model\nthat used a similar architectural scheme. On the Penn Treebank dataset, our\nmodel can compose a novel recurrent cell that outperforms the widely-used LSTM\ncell, and other state-of-the-art baselines. Our cell achieves a test set\nperplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than\nthe previous state-of-the-art model. The cell can also be transferred to the\ncharacter language modeling task on PTB and achieves a state-of-the-art\nperplexity of 1.214. \n\n"}
{"id": "1611.01587", "contents": "Title: A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks Abstract: Transfer and multi-task learning have traditionally focused on either a\nsingle source-target pair or very few, similar tasks. Ideally, the linguistic\nlevels of morphology, syntax and semantics would benefit each other by being\ntrained in a single model. We introduce a joint many-task model together with a\nstrategy for successively growing its depth to solve increasingly complex\ntasks. Higher layers include shortcut connections to lower-level task\npredictions to reflect linguistic hierarchies. We use a simple regularization\nterm to allow for optimizing all model weights to improve one task's loss\nwithout exhibiting catastrophic interference of the other tasks. Our single\nend-to-end model obtains state-of-the-art or competitive results on five\ndifferent tasks from tagging, parsing, relatedness, and entailment tasks. \n\n"}
{"id": "1611.01599", "contents": "Title: LipNet: End-to-End Sentence-level Lipreading Abstract: Lipreading is the task of decoding text from the movement of a speaker's\nmouth. Traditional approaches separated the problem into two stages: designing\nor learning visual features, and prediction. More recent deep lipreading\napproaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman,\n2016a). However, existing work on models trained end-to-end perform only word\nclassification, rather than sentence-level sequence prediction. Studies have\nshown that human lipreading performance increases for longer words (Easton &\nBasala, 1982), indicating the importance of features capturing temporal context\nin an ambiguous communication channel. Motivated by this observation, we\npresent LipNet, a model that maps a variable-length sequence of video frames to\ntext, making use of spatiotemporal convolutions, a recurrent network, and the\nconnectionist temporal classification loss, trained entirely end-to-end. To the\nbest of our knowledge, LipNet is the first end-to-end sentence-level lipreading\nmodel that simultaneously learns spatiotemporal visual features and a sequence\nmodel. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level,\noverlapped speaker split task, outperforming experienced human lipreaders and\nthe previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016). \n\n"}
{"id": "1611.01603", "contents": "Title: Bidirectional Attention Flow for Machine Comprehension Abstract: Machine comprehension (MC), answering a query about a given context\nparagraph, requires modeling complex interactions between the context and the\nquery. Recently, attention mechanisms have been successfully extended to MC.\nTypically these methods use attention to focus on a small portion of the\ncontext and summarize it with a fixed-size vector, couple attentions\ntemporally, and/or often form a uni-directional attention. In this paper we\nintroduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage\nhierarchical process that represents the context at different levels of\ngranularity and uses bi-directional attention flow mechanism to obtain a\nquery-aware context representation without early summarization. Our\nexperimental evaluations show that our model achieves the state-of-the-art\nresults in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze\ntest. \n\n"}
{"id": "1611.01714", "contents": "Title: Beyond Fine Tuning: A Modular Approach to Learning on Small Data Abstract: In this paper we present a technique to train neural network models on small\namounts of data. Current methods for training neural networks on small amounts\nof rich data typically rely on strategies such as fine-tuning a pre-trained\nneural network or the use of domain-specific hand-engineered features. Here we\ntake the approach of treating network layers, or entire networks, as modules\nand combine pre-trained modules with untrained modules, to learn the shift in\ndistributions between data sets. The central impact of using a modular approach\ncomes from adding new representations to a network, as opposed to replacing\nrepresentations via fine-tuning. Using this technique, we are able surpass\nresults using standard fine-tuning transfer learning approaches, and we are\nalso able to significantly increase performance over such approaches when using\nsmaller amounts of data. \n\n"}
{"id": "1611.04326", "contents": "Title: `Who would have thought of that!': A Hierarchical Topic Model for\n  Extraction of Sarcasm-prevalent Topics and Sarcasm Detection Abstract: Topic Models have been reported to be beneficial for aspect-based sentiment\nanalysis. This paper reports a simple topic model for sarcasm detection, a\nfirst, to the best of our knowledge. Designed on the basis of the intuition\nthat sarcastic tweets are likely to have a mixture of words of both sentiments\nas against tweets with literal sentiment (either positive or negative), our\nhierarchical topic model discovers sarcasm-prevalent topics and topic-level\nsentiment. Using a dataset of tweets labeled using hashtags, the model\nestimates topic-level, and sentiment-level distributions. Our evaluation shows\nthat topics such as `work', `gun laws', `weather' are sarcasm-prevalent topics.\nOur model is also able to discover the mixture of sentiment-bearing words that\nexist in a text of a given sentiment-related label. Finally, we apply our model\nto predict sarcasm in tweets. We outperform two prior work based on statistical\nclassifiers with specific features, by around 25\\%. \n\n"}
{"id": "1611.04361", "contents": "Title: Attending to Characters in Neural Sequence Labeling Models Abstract: Sequence labeling architectures use word embeddings for capturing similarity,\nbut suffer when handling previously unseen or rare words. We investigate\ncharacter-level extensions to such models and propose a novel architecture for\ncombining alternative word representations. By using an attention mechanism,\nthe model is able to dynamically decide how much information to use from a\nword- or character-level component. We evaluated different architectures on a\nrange of sequence labeling datasets, and character-level extensions were found\nto improve performance on every benchmark. In addition, the proposed\nattention-based architecture delivered the best results even with a smaller\nnumber of trainable parameters. \n\n"}
{"id": "1611.04741", "contents": "Title: A Neural Architecture Mimicking Humans End-to-End for Natural Language\n  Inference Abstract: In this work we use the recent advances in representation learning to propose\na neural architecture for the problem of natural language inference. Our\napproach is aligned to mimic how a human does the natural language inference\nprocess given two statements. The model uses variants of Long Short Term Memory\n(LSTM), attention mechanism and composable neural networks, to carry out the\ntask. Each part of our model can be mapped to a clear functionality humans do\nfor carrying out the overall task of natural language inference. The model is\nend-to-end differentiable enabling training by stochastic gradient descent. On\nStanford Natural Language Inference(SNLI) dataset, the proposed model achieves\nbetter accuracy numbers than all published models in literature. \n\n"}
{"id": "1611.05373", "contents": "Title: DeepCas: an End-to-end Predictor of Information Cascades Abstract: Information cascades, effectively facilitated by most social network\nplatforms, are recognized as a major factor in almost every social success and\ndisaster in these networks. Can cascades be predicted? While many believe that\nthey are inherently unpredictable, recent work has shown that some key\nproperties of information cascades, such as size, growth, and shape, can be\npredicted by a machine learning algorithm that combines many features. These\npredictors all depend on a bag of hand-crafting features to represent the\ncascade network and the global network structure. Such features, always\ncarefully and sometimes mysteriously designed, are not easy to extend or to\ngeneralize to a different platform or domain.\n  Inspired by the recent successes of deep learning in multiple data mining\ntasks, we investigate whether an end-to-end deep learning approach could\neffectively predict the future size of cascades. Such a method automatically\nlearns the representation of individual cascade graphs in the context of the\nglobal network structure, without hand-crafted features and heuristics. We find\nthat node embeddings fall short of predictive power, and it is critical to\nlearn the representation of a cascade graph as a whole. We present algorithms\nthat learn the representation of cascade graphs in an end-to-end manner, which\nsignificantly improve the performance of cascade prediction over strong\nbaselines that include feature based methods, node embedding methods, and graph\nkernel methods. Our results also provide interesting implications for cascade\nprediction in general. \n\n"}
{"id": "1611.05546", "contents": "Title: Zero-Shot Visual Question Answering Abstract: Part of the appeal of Visual Question Answering (VQA) is its promise to\nanswer new questions about previously unseen images. Most current methods\ndemand training questions that illustrate every possible concept, and will\ntherefore never achieve this capability, since the volume of required training\ndata would be prohibitive. Answering general questions about images requires\nmethods capable of Zero-Shot VQA, that is, methods able to answer questions\nbeyond the scope of the training questions. We propose a new evaluation\nprotocol for VQA methods which measures their ability to perform Zero-Shot VQA,\nand in doing so highlights significant practical deficiencies of current\napproaches, some of which are masked by the biases in current datasets. We\npropose and evaluate several strategies for achieving Zero-Shot VQA, including\nmethods based on pretrained word embeddings, object classifiers with semantic\nembeddings, and test-time retrieval of example images. Our extensive\nexperiments are intended to serve as baselines for Zero-Shot VQA, and they also\nachieve state-of-the-art performance in the standard VQA evaluation setting. \n\n"}
{"id": "1611.08321", "contents": "Title: Training and Evaluating Multimodal Word Embeddings with Large-scale Web\n  Annotated Images Abstract: In this paper, we focus on training and evaluating effective word embeddings\nwith both text and visual information. More specifically, we introduce a\nlarge-scale dataset with 300 million sentences describing over 40 million\nimages crawled and downloaded from publicly available Pins (i.e. an image with\nsentence descriptions uploaded by users) on Pinterest. This dataset is more\nthan 200 times larger than MS COCO, the standard large-scale image dataset with\nsentence descriptions. In addition, we construct an evaluation dataset to\ndirectly assess the effectiveness of word embeddings in terms of finding\nsemantically similar or related words and phrases. The word/phrase pairs in\nthis evaluation dataset are collected from the click data with millions of\nusers in an image search system, thus contain rich semantic relationships.\nBased on these datasets, we propose and compare several Recurrent Neural\nNetworks (RNNs) based multimodal (text and image) models. Experiments show that\nour model benefits from incorporating the visual information into the word\nembeddings, and a weight sharing strategy is crucial for learning such\nmultimodal embeddings. The project page is:\nhttp://www.stat.ucla.edu/~junhua.mao/multimodal_embedding.html \n\n"}
{"id": "1611.09084", "contents": "Title: Hierarchical Hyperlink Prediction for the WWW Abstract: The hyperlink prediction task, that of proposing new links between webpages,\ncan be used to improve search engines, expand the visibility of web pages, and\nincrease the connectivity and navigability of the web. Hyperlink prediction is\ntypically performed on webgraphs composed by thousands or millions of vertices,\nwhere on average each webpage contains less than fifty links. Algorithms\nprocessing graphs so large and sparse require to be both scalable and precise,\na challenging combination. Similarity-based algorithms are among the most\nscalable solutions within the link prediction field, due to their parallel\nnature and computational simplicity. These algorithms independently explore the\nnearby topological features of every missing link from the graph in order to\ndetermine its likelihood. Unfortunately, the precision of similarity-based\nalgorithms is limited, which has prevented their broad application so far. In\nthis work we explore the performance of similarity-based algorithms for the\nparticular problem of hyperlink prediction on large webgraphs, and propose a\nnovel method which assumes the existence of hierarchical properties. We\nevaluate this new approach on several webgraphs and compare its performance\nwith that of the current best similarity-based algorithms. Its remarkable\nperformance leads us to argue on the applicability of the proposal, identifying\nseveral use cases of hyperlink prediction. We also describes the approach we\ntook for the computation of large-scale graphs from the perspective of\nhigh-performance computing, providing details on the implementation and\nparallelization of code. \n\n"}
{"id": "1612.00837", "contents": "Title: Making the V in VQA Matter: Elevating the Role of Image Understanding in\n  Visual Question Answering Abstract: Problems at the intersection of vision and language are of significant\nimportance both as challenging research questions and for the rich set of\napplications they enable. However, inherent structure in our world and bias in\nour language tend to be a simpler signal for learning than visual modalities,\nresulting in models that ignore visual information, leading to an inflated\nsense of their capability.\n  We propose to counter these language priors for the task of Visual Question\nAnswering (VQA) and make vision (the V in VQA) matter! Specifically, we balance\nthe popular VQA dataset by collecting complementary images such that every\nquestion in our balanced dataset is associated with not just a single image,\nbut rather a pair of similar images that result in two different answers to the\nquestion. Our dataset is by construction more balanced than the original VQA\ndataset and has approximately twice the number of image-question pairs. Our\ncomplete balanced dataset is publicly available at www.visualqa.org as part of\nthe 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA\nv2.0).\n  We further benchmark a number of state-of-art VQA models on our balanced\ndataset. All models perform significantly worse on our balanced dataset,\nsuggesting that these models have indeed learned to exploit language priors.\nThis finding provides the first concrete empirical evidence for what seems to\nbe a qualitative sense among practitioners.\n  Finally, our data collection protocol for identifying complementary images\nenables us to develop a novel interpretable model, which in addition to\nproviding an answer to the given (image, question) pair, also provides a\ncounter-example based explanation. Specifically, it identifies an image that is\nsimilar to the original image, but it believes has a different answer to the\nsame question. This can help in building trust for machines among their users. \n\n"}
{"id": "1612.01428", "contents": "Title: Extracting Implicit Social Relation for Social Recommendation Techniques\n  in User Rating Prediction Abstract: Recommendation plays an increasingly important role in our daily lives.\nRecommender systems automatically suggest items to users that might be\ninteresting for them. Recent studies illustrate that incorporating social trust\nin Matrix Factorization methods demonstrably improves accuracy of rating\nprediction. Such approaches mainly use the trust scores explicitly expressed by\nusers. However, it is often challenging to have users provide explicit trust\nscores of each other. There exist quite a few works, which propose Trust\nMetrics to compute and predict trust scores between users based on their\ninteractions. In this paper, first we present how social relation can be\nextracted from users' ratings to items by describing Hellinger distance between\nusers in recommender systems. Then, we propose to incorporate the predicted\ntrust scores into social matrix factorization models. By analyzing social\nrelation extraction from three well-known real-world datasets, which both:\ntrust and recommendation data available, we conclude that using the implicit\nsocial relation in social recommendation techniques has almost the same\nperformance compared to the actual trust scores explicitly expressed by users.\nHence, we build our method, called Hell-TrustSVD, on top of the\nstate-of-the-art social recommendation technique to incorporate both the\nextracted implicit social relations and ratings given by users on the\nprediction of items for an active user. To the best of our knowledge, this is\nthe first work to extend TrustSVD with extracted social trust information. The\nexperimental results support the idea of employing implicit trust into matrix\nfactorization whenever explicit trust is not available, can perform much better\nthan the state-of-the-art approaches in user rating prediction. \n\n"}
{"id": "1612.02482", "contents": "Title: Improving the Performance of Neural Machine Translation Involving\n  Morphologically Rich Languages Abstract: The advent of the attention mechanism in neural machine translation models\nhas improved the performance of machine translation systems by enabling\nselective lookup into the source sentence. In this paper, the efficiencies of\ntranslation using bidirectional encoder attention decoder models were studied\nwith respect to translation involving morphologically rich languages. The\nEnglish - Tamil language pair was selected for this analysis. First, the use of\nWord2Vec embedding for both the English and Tamil words improved the\ntranslation results by 0.73 BLEU points over the baseline RNNSearch model with\n4.84 BLEU score. The use of morphological segmentation before word\nvectorization to split the morphologically rich Tamil words into their\nrespective morphemes before the translation, caused a reduction in the target\nvocabulary size by a factor of 8. Also, this model (RNNMorph) improved the\nperformance of neural machine translation by 7.05 BLEU points over the\nRNNSearch model used over the same corpus. Since the BLEU evaluation of the\nRNNMorph model might be unreliable due to an increase in the number of matching\ntokens per sentence, the performances of the translations were also compared by\nmeans of human evaluation metrics of adequacy, fluency and relative ranking.\nFurther, the use of morphological segmentation also improved the efficacy of\nthe attention mechanism. \n\n"}
{"id": "1612.04118", "contents": "Title: Information Extraction with Character-level Neural Networks and Free\n  Noisy Supervision Abstract: We present an architecture for information extraction from text that augments\nan existing parser with a character-level neural network. The network is\ntrained using a measure of consistency of extracted data with existing\ndatabases as a form of noisy supervision. Our architecture combines the ability\nof constraint-based information extraction systems to easily incorporate domain\nknowledge and constraints with the ability of deep neural networks to leverage\nlarge amounts of data to learn complex features. Boosting the existing parser's\nprecision, the system led to large improvements over a mature and highly tuned\nconstraint-based production information extraction system used at Bloomberg for\nfinancial language text. \n\n"}
{"id": "1612.04174", "contents": "Title: Models of retrieval in sentence comprehension: A computational\n  evaluation using Bayesian hierarchical modeling Abstract: Research on interference has provided evidence that the formation of\ndependencies between non-adjacent words relies on a cue-based retrieval\nmechanism. Two different models can account for one of the main predictions of\ninterference, i.e., a slowdown at a retrieval site, when several items share a\nfeature associated with a retrieval cue: Lewis and Vasishth's (2005)\nactivation-based model and McElree's (2000) direct access model. Even though\nthese two models have been used almost interchangeably, they are based on\ndifferent assumptions and predict differences in the relationship between\nreading times and response accuracy. The activation-based model follows the\nassumptions of ACT-R, and its retrieval process behaves as a lognormal race\nbetween accumulators of evidence with a single variance. Under this model,\naccuracy of the retrieval is determined by the winner of the race and retrieval\ntime by its rate of accumulation. In contrast, the direct access model assumes\na model of memory where only the probability of retrieval varies between items;\nin this model, differences in latencies are a by-product of the possibility and\nrepairing incorrect retrievals. We implemented both models in a Bayesian\nhierarchical framework in order to evaluate them and compare them. We show that\nsome aspects of the data are better fit under the direct access model than\nunder the activation-based model. We suggest that this finding does not rule\nout the possibility that retrieval may be behaving as a race model with\nassumptions that follow less closely the ones from the ACT-R framework. We show\nthat by introducing a modification of the activation model, i.e, by assuming\nthat the accumulation of evidence for retrieval of incorrect items is not only\nslower but noisier (i.e., different variances for the correct and incorrect\nitems), the model can provide a fit as good as the one of the direct access\nmodel. \n\n"}
{"id": "1612.04426", "contents": "Title: Improving Neural Language Models with a Continuous Cache Abstract: We propose an extension to neural network language models to adapt their\nprediction to the recent history. Our model is a simplified version of memory\naugmented networks, which stores past hidden activations as memory and accesses\nthem through a dot product with the current hidden activation. This mechanism\nis very efficient and scales to very large memory sizes. We also draw a link\nbetween the use of external memory in neural network and cache models used with\ncount based language models. We demonstrate on several language model datasets\nthat our approach performs significantly better than recent memory augmented\nnetworks. \n\n"}
{"id": "1612.05001", "contents": "Title: Graph-based semi-supervised learning for relational networks Abstract: We address the problem of semi-supervised learning in relational networks,\nnetworks in which nodes are entities and links are the relationships or\ninteractions between them. Typically this problem is confounded with the\nproblem of graph-based semi-supervised learning (GSSL), because both problems\nrepresent the data as a graph and predict the missing class labels of nodes.\nHowever, not all graphs are created equally. In GSSL a graph is constructed,\noften from independent data, based on similarity. As such, edges tend to\nconnect instances with the same class label. Relational networks, however, can\nbe more heterogeneous and edges do not always indicate similarity. For\ninstance, instead of links being more likely to connect nodes with the same\nclass label, they may occur more frequently between nodes with different class\nlabels (link-heterogeneity). Or nodes with the same class label do not\nnecessarily have the same type of connectivity across the whole network\n(class-heterogeneity), e.g. in a network of sexual interactions we may observe\nlinks between opposite genders in some parts of the graph and links between the\nsame genders in others. Performing classification in networks with different\ntypes of heterogeneity is a hard problem that is made harder still when we do\nnot know a-priori the type or level of heterogeneity. Here we present two\nscalable approaches for graph-based semi-supervised learning for the more\ngeneral case of relational networks. We demonstrate these approaches on\nsynthetic and real-world networks that display different link patterns within\nand between classes. Compared to state-of-the-art approaches, ours give better\nclassification performance without prior knowledge of how classes interact. In\nparticular, our two-step label propagation algorithm gives consistently good\naccuracy and runs on networks of over 1.6 million nodes and 30 million edges in\naround 12 seconds. \n\n"}
{"id": "1612.06778", "contents": "Title: SCDV : Sparse Composite Document Vectors using soft clustering over\n  distributional representations Abstract: We present a feature vector formation technique for documents - Sparse\nComposite Document Vector (SCDV) - which overcomes several shortcomings of the\ncurrent distributional paragraph vector representations that are widely used\nfor text representation. In SCDV, word embedding's are clustered to capture\nmultiple semantic contexts in which words occur. They are then chained together\nto form document topic-vectors that can express complex, multi-topic documents.\nThrough extensive experiments on multi-class and multi-label classification\ntasks, we outperform the previous state-of-the-art method, NTSG (Liu et al.,\n2015a). We also show that SCDV embedding's perform well on heterogeneous tasks\nlike Topic Coherence, context-sensitive Learning and Information Retrieval.\nMoreover, we achieve significant reduction in training and prediction times\ncompared to other representation methods. SCDV achieves best of both worlds -\nbetter performance with lower time and space complexity. \n\n"}
{"id": "1612.06890", "contents": "Title: CLEVR: A Diagnostic Dataset for Compositional Language and Elementary\n  Visual Reasoning Abstract: When building artificial intelligence systems that can reason and answer\nquestions about visual data, we need diagnostic tests to analyze our progress\nand discover shortcomings. Existing benchmarks for visual question answering\ncan help, but have strong biases that models can exploit to correctly answer\nquestions without reasoning. They also conflate multiple sources of error,\nmaking it hard to pinpoint model weaknesses. We present a diagnostic dataset\nthat tests a range of visual reasoning abilities. It contains minimal biases\nand has detailed annotations describing the kind of reasoning each question\nrequires. We use this dataset to analyze a variety of modern visual reasoning\nsystems, providing novel insights into their abilities and limitations. \n\n"}
{"id": "1612.07411", "contents": "Title: A Context-aware Attention Network for Interactive Question Answering Abstract: Neural network based sequence-to-sequence models in an encoder-decoder\nframework have been successfully applied to solve Question Answering (QA)\nproblems, predicting answers from statements and questions. However, almost all\nprevious models have failed to consider detailed context information and\nunknown states under which systems do not have enough information to answer\ngiven questions. These scenarios with incomplete or ambiguous information are\nvery common in the setting of Interactive Question Answering (IQA). To address\nthis challenge, we develop a novel model, employing context-dependent\nword-level attention for more accurate statement representations and\nquestion-guided sentence-level attention for better context modeling. We also\ngenerate unique IQA datasets to test our model, which will be made publicly\navailable. Employing these attention mechanisms, our model accurately\nunderstands when it can output an answer or when it requires generating a\nsupplementary question for additional input depending on different contexts.\nWhen available, user's feedback is encoded and directly applied to update\nsentence-level attention to infer an answer. Extensive experiments on QA and\nIQA datasets quantitatively demonstrate the effectiveness of our model with\nsignificant improvement over state-of-the-art conventional QA models. \n\n"}
{"id": "1612.07843", "contents": "Title: \"What is Relevant in a Text Document?\": An Interpretable Machine\n  Learning Approach Abstract: Text documents can be described by a number of abstract concepts such as\nsemantic category, writing style, or sentiment. Machine learning (ML) models\nhave been trained to automatically map documents to these abstract concepts,\nallowing to annotate very large text collections, more than could be processed\nby a human in a lifetime. Besides predicting the text's category very\naccurately, it is also highly desirable to understand how and why the\ncategorization process takes place. In this paper, we demonstrate that such\nunderstanding can be achieved by tracing the classification decision back to\nindividual words using layer-wise relevance propagation (LRP), a recently\ndeveloped technique for explaining predictions of complex non-linear\nclassifiers. We train two word-based ML models, a convolutional neural network\n(CNN) and a bag-of-words SVM classifier, on a topic categorization task and\nadapt the LRP method to decompose the predictions of these models onto words.\nResulting scores indicate how much individual words contribute to the overall\nclassification decision. This enables one to distill relevant information from\ntext documents without an explicit semantic information extraction step. We\nfurther use the word-wise relevance scores for generating novel vector-based\ndocument representations which capture semantic information. Based on these\ndocument vectors, we introduce a measure of model explanatory power and show\nthat, although the SVM and CNN models perform similarly in terms of\nclassification accuracy, the latter exhibits a higher level of explainability\nwhich makes it more comprehensible for humans and potentially more useful for\nother applications. \n\n"}
{"id": "1612.07940", "contents": "Title: Supervised Opinion Aspect Extraction by Exploiting Past Extraction\n  Results Abstract: One of the key tasks of sentiment analysis of product reviews is to extract\nproduct aspects or features that users have expressed opinions on. In this\nwork, we focus on using supervised sequence labeling as the base approach to\nperforming the task. Although several extraction methods using sequence\nlabeling methods such as Conditional Random Fields (CRF) and Hidden Markov\nModels (HMM) have been proposed, we show that this supervised approach can be\nsignificantly improved by exploiting the idea of concept sharing across\nmultiple domains. For example, \"screen\" is an aspect in iPhone, but not only\niPhone has a screen, many electronic devices have screens too. When \"screen\"\nappears in a review of a new domain (or product), it is likely to be an aspect\ntoo. Knowing this information enables us to do much better extraction in the\nnew domain. This paper proposes a novel extraction method exploiting this idea\nin the context of supervised sequence labeling. Experimental results show that\nit produces markedly better results than without using the past information. \n\n"}
{"id": "1612.08083", "contents": "Title: Language Modeling with Gated Convolutional Networks Abstract: The pre-dominant approach to language modeling to date is based on recurrent\nneural networks. Their success on this task is often linked to their ability to\ncapture unbounded context. In this paper we develop a finite context approach\nthrough stacked convolutions, which can be more efficient since they allow\nparallelization over sequential tokens. We propose a novel simplified gating\nmechanism that outperforms Oord et al (2016) and investigate the impact of key\narchitectural decisions. The proposed approach achieves state-of-the-art on the\nWikiText-103 benchmark, even though it features long-term dependencies, as well\nas competitive results on the Google Billion Words benchmark. Our model reduces\nthe latency to score a sentence by an order of magnitude compared to a\nrecurrent baseline. To our knowledge, this is the first time a non-recurrent\napproach is competitive with strong recurrent models on these large scale\nlanguage tasks. \n\n"}
{"id": "1612.08102", "contents": "Title: On Spectral Analysis of Directed Signed Graphs Abstract: It has been shown that the adjacency eigenspace of a network contains key\ninformation of its underlying structure. However, there has been no study on\nspectral analysis of the adjacency matrices of directed signed graphs. In this\npaper, we derive theoretical approximations of spectral projections from such\ndirected signed networks using matrix perturbation theory. We use the derived\ntheoretical results to study the influences of negative intra cluster and inter\ncluster directed edges on node spectral projections. We then develop a spectral\nclustering based graph partition algorithm, SC-DSG, and conduct evaluations on\nboth synthetic and real datasets. Both theoretical analysis and empirical\nevaluation demonstrate the effectiveness of the proposed algorithm. \n\n"}
{"id": "1612.08220", "contents": "Title: Understanding Neural Networks through Representation Erasure Abstract: While neural networks have been successfully applied to many natural language\nprocessing tasks, they come at the cost of interpretability. In this paper, we\npropose a general methodology to analyze and interpret decisions from a neural\nmodel by observing the effects on the model of erasing various parts of the\nrepresentation, such as input word-vector dimensions, intermediate hidden\nunits, or input words. We present several approaches to analyzing the effects\nof such erasure, from computing the relative difference in evaluation metrics,\nto using reinforcement learning to erase the minimum set of input words in\norder to flip a neural model's decision. In a comprehensive analysis of\nmultiple NLP tasks, including linguistic feature classification, sentence-level\nsentiment analysis, and document level sentiment aspect prediction, we show\nthat the proposed methodology not only offers clear explanations about neural\nmodel decisions, but also provides a way to conduct error analysis on neural\nmodels. \n\n"}
{"id": "1701.00160", "contents": "Title: NIPS 2016 Tutorial: Generative Adversarial Networks Abstract: This report summarizes the tutorial presented by the author at NIPS 2016 on\ngenerative adversarial networks (GANs). The tutorial describes: (1) Why\ngenerative modeling is a topic worth studying, (2) how generative models work,\nand how GANs compare to other generative models, (3) the details of how GANs\nwork, (4) research frontiers in GANs, and (5) state-of-the-art image models\nthat combine GANs with other methods. Finally, the tutorial contains three\nexercises for readers to complete, and the solutions to these exercises. \n\n"}
{"id": "1701.00991", "contents": "Title: World Literature According to Wikipedia: Introduction to a DBpedia-Based\n  Framework Abstract: Among the manifold takes on world literature, it is our goal to contribute to\nthe discussion from a digital point of view by analyzing the representation of\nworld literature in Wikipedia with its millions of articles in hundreds of\nlanguages. As a preliminary, we introduce and compare three different\napproaches to identify writers on Wikipedia using data from DBpedia, a\ncommunity project with the goal of extracting and providing structured\ninformation from Wikipedia. Equipped with our basic set of writers, we analyze\nhow they are represented throughout the 15 biggest Wikipedia language versions.\nWe combine intrinsic measures (mostly examining the connectedness of articles)\nwith extrinsic ones (analyzing how often articles are frequented by readers)\nand develop methods to evaluate our results. The better part of our findings\nseems to convey a rather conservative, old-fashioned version of world\nliterature, but a version derived from reproducible facts revealing an implicit\nliterary canon based on the editing and reading behavior of millions of people.\nWhile still having to solve some known issues, the introduced methods will help\nus build an observatory of world literature to further investigate its\nrepresentativeness and biases. \n\n"}
{"id": "1701.01325", "contents": "Title: Outlier Detection for Text Data : An Extended Version Abstract: The problem of outlier detection is extremely challenging in many domains\nsuch as text, in which the attribute values are typically non-negative, and\nmost values are zero. In such cases, it often becomes difficult to separate the\noutliers from the natural variations in the patterns in the underlying data. In\nthis paper, we present a matrix factorization method, which is naturally able\nto distinguish the anomalies with the use of low rank approximations of the\nunderlying data. Our iterative algorithm TONMF is based on block coordinate\ndescent (BCD) framework. We define blocks over the term-document matrix such\nthat the function becomes solvable. Given most recently updated values of other\nmatrix blocks, we always update one block at a time to its optimal. Our\napproach has significant advantages over traditional methods for text outlier\ndetection. Finally, we present experimental results illustrating the\neffectiveness of our method over competing methods. \n\n"}
{"id": "1701.02481", "contents": "Title: Implicitly Incorporating Morphological Information into Word Embedding Abstract: In this paper, we propose three novel models to enhance word embedding by\nimplicitly using morphological information. Experiments on word similarity and\nsyntactic analogy show that the implicit models are superior to traditional\nexplicit ones. Our models outperform all state-of-the-art baselines and\nsignificantly improve the performance on both tasks. Moreover, our performance\non the smallest corpus is similar to the performance of CBOW on the corpus\nwhich is five times the size of ours. Parameter analysis indicates that the\nimplicit models can supplement semantic information during the word embedding\ntraining process. \n\n"}
{"id": "1701.02810", "contents": "Title: OpenNMT: Open-Source Toolkit for Neural Machine Translation Abstract: We describe an open-source toolkit for neural machine translation (NMT). The\ntoolkit prioritizes efficiency, modularity, and extensibility with the goal of\nsupporting NMT research into model architectures, feature representations, and\nsource modalities, while maintaining competitive performance and reasonable\ntraining requirements. The toolkit consists of modeling and translation\nsupport, as well as detailed pedagogical documentation about the underlying\ntechniques. \n\n"}
{"id": "1701.04313", "contents": "Title: End-to-End ASR-free Keyword Search from Speech Abstract: End-to-end (E2E) systems have achieved competitive results compared to\nconventional hybrid hidden Markov model (HMM)-deep neural network based\nautomatic speech recognition (ASR) systems. Such E2E systems are attractive due\nto the lack of dependence on alignments between input acoustic and output\ngrapheme or HMM state sequence during training. This paper explores the design\nof an ASR-free end-to-end system for text query-based keyword search (KWS) from\nspeech trained with minimal supervision. Our E2E KWS system consists of three\nsub-systems. The first sub-system is a recurrent neural network (RNN)-based\nacoustic auto-encoder trained to reconstruct the audio through a\nfinite-dimensional representation. The second sub-system is a character-level\nRNN language model using embeddings learned from a convolutional neural\nnetwork. Since the acoustic and text query embeddings occupy different\nrepresentation spaces, they are input to a third feed-forward neural network\nthat predicts whether the query occurs in the acoustic utterance or not. This\nE2E ASR-free KWS system performs respectably despite lacking a conventional ASR\nsystem and trains much faster. \n\n"}
{"id": "1701.05804", "contents": "Title: Disentangling group and link persistence in Dynamic Stochastic Block\n  models Abstract: We study the inference of a model of dynamic networks in which both\ncommunities and links keep memory of previous network states. By considering\nmaximum likelihood inference from single snapshot observations of the network,\nwe show that link persistence makes the inference of communities harder,\ndecreasing the detectability threshold, while community persistence tends to\nmake it easier. We analytically show that communities inferred from single\nnetwork snapshot can share a maximum overlap with the underlying communities of\na specific previous instant in time. This leads to time-lagged inference: the\nidentification of past communities rather than present ones. Finally we compute\nthe time lag and propose a corrected algorithm, the Lagged Snapshot Dynamic\n(LSD) algorithm, for community detection in dynamic networks. We analytically\nand numerically characterize the detectability transitions of such algorithm as\na function of the memory parameters of the model and we make a comparison with\na full dynamic inference. \n\n"}
{"id": "1701.05927", "contents": "Title: Learning Particle Physics by Example: Location-Aware Generative\n  Adversarial Networks for Physics Synthesis Abstract: We provide a bridge between generative modeling in the Machine Learning\ncommunity and simulated physical processes in High Energy Particle Physics by\napplying a novel Generative Adversarial Network (GAN) architecture to the\nproduction of jet images -- 2D representations of energy depositions from\nparticles interacting with a calorimeter. We propose a simple architecture, the\nLocation-Aware Generative Adversarial Network, that learns to produce realistic\nradiation patterns from simulated high energy particle collisions. The pixel\nintensities of GAN-generated images faithfully span over many orders of\nmagnitude and exhibit the desired low-dimensional physical properties (i.e.,\njet mass, n-subjettiness, etc.). We shed light on limitations, and provide a\nnovel empirical validation of image quality and validity of GAN-produced\nsimulations of the natural world. This work provides a base for further\nexplorations of GANs for use in faster simulation in High Energy Particle\nPhysics. \n\n"}
{"id": "1701.06538", "contents": "Title: Outrageously Large Neural Networks: The Sparsely-Gated\n  Mixture-of-Experts Layer Abstract: The capacity of a neural network to absorb information is limited by its\nnumber of parameters. Conditional computation, where parts of the network are\nactive on a per-example basis, has been proposed in theory as a way of\ndramatically increasing model capacity without a proportional increase in\ncomputation. In practice, however, there are significant algorithmic and\nperformance challenges. In this work, we address these challenges and finally\nrealize the promise of conditional computation, achieving greater than 1000x\nimprovements in model capacity with only minor losses in computational\nefficiency on modern GPU clusters. We introduce a Sparsely-Gated\nMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward\nsub-networks. A trainable gating network determines a sparse combination of\nthese experts to use for each example. We apply the MoE to the tasks of\nlanguage modeling and machine translation, where model capacity is critical for\nabsorbing the vast quantities of knowledge available in the training corpora.\nWe present model architectures in which a MoE with up to 137 billion parameters\nis applied convolutionally between stacked LSTM layers. On large language\nmodeling and machine translation benchmarks, these models achieve significantly\nbetter results than state-of-the-art at lower computational cost. \n\n"}
{"id": "1702.00887", "contents": "Title: Structured Attention Networks Abstract: Attention networks have proven to be an effective approach for embedding\ncategorical inference within a deep neural network. However, for many tasks we\nmay want to model richer structural dependencies without abandoning end-to-end\ntraining. In this work, we experiment with incorporating richer structural\ndistributions, encoded using graphical models, within deep networks. We show\nthat these structured attention networks are simple extensions of the basic\nattention procedure, and that they allow for extending attention beyond the\nstandard soft-selection approach, such as attending to partial segmentations or\nto subtrees. We experiment with two different classes of structured attention\nnetworks: a linear-chain conditional random field and a graph-based parsing\nmodel, and describe how these models can be practically implemented as neural\nnetwork layers. Experiments show that this approach is effective for\nincorporating structural biases, and structured attention networks outperform\nbaseline attention models on a variety of synthetic and real tasks: tree\ntransduction, neural machine translation, question answering, and natural\nlanguage inference. We further find that models trained in this way learn\ninteresting unsupervised hidden representations that generalize simple\nattention. \n\n"}
{"id": "1702.01739", "contents": "Title: Multi-Message Private Information Retrieval: Capacity Results and\n  Near-Optimal Schemes Abstract: We consider the problem of multi-message private information retrieval (MPIR)\nfrom $N$ non-communicating replicated databases. In MPIR, the user is\ninterested in retrieving $P$ messages out of $M$ stored messages without\nleaking the identity of the retrieved messages. The information-theoretic sum\ncapacity of MPIR $C_s^P$ is the maximum number of desired message symbols that\ncan be retrieved privately per downloaded symbol. For the case $P \\geq\n\\frac{M}{2}$, we determine the exact sum capacity of MPIR as\n$C_s^P=\\frac{1}{1+\\frac{M-P}{PN}}$. The achievable scheme in this case is based\non downloading MDS-coded mixtures of all messages. For $P \\leq \\frac{M}{2}$, we\ndevelop lower and upper bounds for all $M,P,N$. These bounds match if the total\nnumber of messages $M$ is an integer multiple of the number of desired messages\n$P$, i.e., $\\frac{M}{P} \\in \\mathbb{N}$. In this case,\n$C_s^P=\\frac{1-\\frac{1}{N}}{1-(\\frac{1}{N})^{M/P}}$. The achievable scheme in\nthis case generalizes the single-message capacity achieving scheme to have\nunbalanced number of stages per round of download. For all the remaining cases,\nthe difference between the lower and upper bound is at most $0.0082$, which\noccurs for $M=5$, $P=2$, $N=2$. Our results indicate that joint retrieval of\ndesired messages is more efficient than successive use of single-message\nretrieval schemes. \n\n"}
{"id": "1702.01948", "contents": "Title: Continuous-Time User Modeling in the Presence of Badges: A Probabilistic\n  Approach Abstract: User modeling plays an important role in delivering customized web services\nto the users and improving their engagement. However, most user models in the\nliterature do not explicitly consider the temporal behavior of users. More\nrecently, continuous-time user modeling has gained considerable attention and\nmany user behavior models have been proposed based on temporal point processes.\nHowever, typical point process based models often considered the impact of peer\ninfluence and content on the user participation and neglected other factors.\nGamification elements, are among those factors that are neglected, while they\nhave a strong impact on user participation in online services. In this paper,\nwe propose interdependent multi-dimensional temporal point processes that\ncapture the impact of badges on user participation besides the peer influence\nand content factors. We extend the proposed processes to model user actions\nover the community based question and answering websites, and propose an\ninference algorithm based on Variational-EM that can efficiently learn the\nmodel parameters. Extensive experiments on both synthetic and real data\ngathered from Stack Overflow show that our inference algorithm learns the\nparameters efficiently and the proposed method can better predict the user\nbehavior compared to the alternatives. \n\n"}
{"id": "1702.01978", "contents": "Title: Volatility Prediction using Financial Disclosures Sentiments with Word\n  Embedding-based IR Models Abstract: Volatility prediction--an essential concept in financial markets--has\nrecently been addressed using sentiment analysis methods. We investigate the\nsentiment of annual disclosures of companies in stock markets to forecast\nvolatility. We specifically explore the use of recent Information Retrieval\n(IR) term weighting models that are effectively extended by related terms using\nword embeddings. In parallel to textual information, factual market data have\nbeen widely used as the mainstream approach to forecast market risk. We\ntherefore study different fusion methods to combine text and market data\nresources. Our word embedding-based approach significantly outperforms\nstate-of-the-art methods. In addition, we investigate the characteristics of\nthe reports of the companies in different financial sectors. \n\n"}
{"id": "1702.02261", "contents": "Title: Social media mining for identification and exploration of health-related\n  information from pregnant women Abstract: Widespread use of social media has led to the generation of substantial\namounts of information about individuals, including health-related information.\nSocial media provides the opportunity to study health-related information about\nselected population groups who may be of interest for a particular study. In\nthis paper, we explore the possibility of utilizing social media to perform\ntargeted data collection and analysis from a particular population group --\npregnant women. We hypothesize that we can use social media to identify cohorts\nof pregnant women and follow them over time to analyze crucial health-related\ninformation. To identify potentially pregnant women, we employ simple\nrule-based searches that attempt to detect pregnancy announcements with\nmoderate precision. To further filter out false positives and noise, we employ\na supervised classifier using a small number of hand-annotated data. We then\ncollect their posts over time to create longitudinal health timelines and\nattempt to divide the timelines into different pregnancy trimesters. Finally,\nwe assess the usefulness of the timelines by performing a preliminary analysis\nto estimate drug intake patterns of our cohort at different trimesters. Our\nrule-based cohort identification technique collected 53,820 users over thirty\nmonths from Twitter. Our pregnancy announcement classification technique\nachieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user\ntimelines. Analysis of the timelines revealed that pertinent health-related\ninformation, such as drug-intake and adverse reactions can be mined from the\ndata. Our approach to using user timelines in this fashion has produced very\nencouraging results and can be employed for other important tasks where\ncohorts, for which health-related information may not be available from other\nsources, are required to be followed over time to derive population-based\nestimates. \n\n"}
{"id": "1702.02429", "contents": "Title: Trainable Greedy Decoding for Neural Machine Translation Abstract: Recent research in neural machine translation has largely focused on two\naspects; neural network architectures and end-to-end learning algorithms. The\nproblem of decoding, however, has received relatively little attention from the\nresearch community. In this paper, we solely focus on the problem of decoding\ngiven a trained neural machine translation model. Instead of trying to build a\nnew decoding algorithm for any specific decoding objective, we propose the idea\nof trainable decoding algorithm in which we train a decoding algorithm to find\na translation that maximizes an arbitrary decoding objective. More\nspecifically, we design an actor that observes and manipulates the hidden state\nof the neural machine translation decoder and propose to train it using a\nvariant of deterministic policy gradient. We extensively evaluate the proposed\nalgorithm using four language pairs and two decoding objectives and show that\nwe can indeed train a trainable greedy decoder that generates a better\ntranslation (in terms of a target decoding objective) with minimal\ncomputational overhead. \n\n"}
{"id": "1702.02535", "contents": "Title: Exploiting Domain Knowledge via Grouped Weight Sharing with Application\n  to Text Categorization Abstract: A fundamental advantage of neural models for NLP is their ability to learn\nrepresentations from scratch. However, in practice this often means ignoring\nexisting external linguistic resources, e.g., WordNet or domain specific\nontologies such as the Unified Medical Language System (UMLS). We propose a\ngeneral, novel method for exploiting such resources via weight sharing. Prior\nwork on weight sharing in neural networks has considered it largely as a means\nof model compression. In contrast, we treat weight sharing as a flexible\nmechanism for incorporating prior knowledge into neural models. We show that\nthis approach consistently yields improved performance on classification tasks\ncompared to baseline strategies that do not exploit weight sharing. \n\n"}
{"id": "1702.02640", "contents": "Title: Character-level Deep Conflation for Business Data Analytics Abstract: Connecting different text attributes associated with the same entity\n(conflation) is important in business data analytics since it could help merge\ntwo different tables in a database to provide a more comprehensive profile of\nan entity. However, the conflation task is challenging because two text strings\nthat describe the same entity could be quite different from each other for\nreasons such as misspelling. It is therefore critical to develop a conflation\nmodel that is able to truly understand the semantic meaning of the strings and\nmatch them at the semantic level. To this end, we develop a character-level\ndeep conflation model that encodes the input text strings from character level\ninto finite dimension feature vectors, which are then used to compute the\ncosine similarity between the text strings. The model is trained in an\nend-to-end manner using back propagation and stochastic gradient descent to\nmaximize the likelihood of the correct association. Specifically, we propose\ntwo variants of the deep conflation model, based on long-short-term memory\n(LSTM) recurrent neural network (RNN) and convolutional neural network (CNN),\nrespectively. Both models perform well on a real-world business analytics\ndataset and significantly outperform the baseline bag-of-character (BoC) model. \n\n"}
{"id": "1702.04521", "contents": "Title: Frustratingly Short Attention Spans in Neural Language Modeling Abstract: Neural language models predict the next token using a latent representation\nof the immediate token history. Recently, various methods for augmenting neural\nlanguage models with an attention mechanism over a differentiable memory have\nbeen proposed. For predicting the next token, these models query information\nfrom a memory of the recent history which can facilitate learning mid- and\nlong-range dependencies. However, conventional attention mechanisms used in\nmemory-augmented neural language models produce a single output vector per time\nstep. This vector is used both for predicting the next token as well as for the\nkey and value of a differentiable memory of a token history. In this paper, we\npropose a neural language model with a key-value attention mechanism that\noutputs separate representations for the key and value of a differentiable\nmemory, as well as for encoding the next-word distribution. This model\noutperforms existing memory-augmented neural language models on two corpora.\nYet, we found that our method mainly utilizes a memory of the five most recent\noutput representations. This led to the unexpected main finding that a much\nsimpler model based only on the concatenation of recent output representations\nfrom previous time steps is on par with more sophisticated memory-augmented\nneural language models. \n\n"}
{"id": "1702.04615", "contents": "Title: Automated Identification of Drug-Drug Interactions in Pediatric\n  Congestive Heart Failure Patients Abstract: Congestive Heart Failure, or CHF, is a serious medical condition that can\nresult in fluid buildup in the body as a result of a weak heart. When the heart\ncan't pump enough blood to efficiently deliver nutrients and oxygen to the\nbody, kidney function may be impaired, resulting in fluid retention. CHF\npatients require a broad drug regimen to maintain the delicate system balance,\nparticularly between their heart and kidneys. These drugs include ACE\ninhibitors and Beta Blockers to control blood pressure, anticoagulants to\nprevent blood clots, and diuretics to reduce fluid overload. Many of these\ndrugs may interact, and potential effects of these interactions must be weighed\nagainst their benefits. For this project, we consider a set of 44 drugs\nidentified as specifically relevant for treating CHF by pediatric cardiologists\nat Lucile Packard Children's Hospital. This list was generated as part of our\ncurrent work at the LPCH Heart Center. The goal of this project is to identify\nand evaluate potentially harmful drug-drug interactions (DDIs) within pediatric\npatients with Congestive Heart Failure. This identification will be done\nautonomously, so that it may continuously update by evaluating newly published\nliterature. \n\n"}
{"id": "1702.05398", "contents": "Title: Experiment Segmentation in Scientific Discourse as Clause-level\n  Structured Prediction using Recurrent Neural Networks Abstract: We propose a deep learning model for identifying structure within experiment\nnarratives in scientific literature. We take a sequence labeling approach to\nthis problem, and label clauses within experiment narratives to identify the\ndifferent parts of the experiment. Our dataset consists of paragraphs taken\nfrom open access PubMed papers labeled with rhetorical information as a result\nof our pilot annotation. Our model is a Recurrent Neural Network (RNN) with\nLong Short-Term Memory (LSTM) cells that labels clauses. The clause\nrepresentations are computed by combining word representations using a novel\nattention mechanism that involves a separate RNN. We compare this model against\nLSTMs where the input layer has simple or no attention and a feature rich CRF\nmodel. Furthermore, we describe how our work could be useful for information\nextraction from scientific literature. \n\n"}
{"id": "1702.05764", "contents": "Title: Fast, Warped Graph Embedding: Unifying Framework and One-Click Algorithm Abstract: What is the best way to describe a user in a social network with just a few\nnumbers? Mathematically, this is equivalent to assigning a vector\nrepresentation to each node in a graph, a process called graph embedding. We\npropose a novel framework, GEM-D that unifies most of the past algorithms such\nas LapEigs, DeepWalk and node2vec. GEM-D achieves its goal by decomposing any\ngraph embedding algorithm into three building blocks: node proximity function,\nwarping function and loss function. Based on thorough analysis of GEM-D, we\npropose a novel algorithm, called UltimateWalk, which outperforms the\nmost-recently proposed state-of-the-art DeepWalk and node2vec. The\ncontributions of this work are: (1) The proposed framework, GEM-D unifies the\npast graph embedding algorithms and provides a general recipe of how to design\na graph embedding; (2) the nonlinearlity in the warping function contributes\nsignificantly to the quality of embedding and the exponential function is\nempirically optimal; (3) the proposed algorithm, UltimateWalk is one-click (no\nuser-defined parameters), scalable and has a closed-form solution. \n\n"}
{"id": "1702.06378", "contents": "Title: Multitask Learning with CTC and Segmental CRF for Speech Recognition Abstract: Segmental conditional random fields (SCRFs) and connectionist temporal\nclassification (CTC) are two sequence labeling methods used for end-to-end\ntraining of speech recognition models. Both models define a transcription\nprobability by marginalizing decisions about latent segmentation alternatives\nto derive a sequence probability: the former uses a globally normalized joint\nmodel of segment labels and durations, and the latter classifies each frame as\neither an output symbol or a \"continuation\" of the previous label. In this\npaper, we train a recognition model by optimizing an interpolation between the\nSCRF and CTC losses, where the same recurrent neural network (RNN) encoder is\nused for feature extraction for both outputs. We find that this multitask\nobjective improves recognition accuracy when decoding with either the SCRF or\nCTC models. Additionally, we show that CTC can also be used to pretrain the RNN\nencoder, which improves the convergence rate when learning the joint model. \n\n"}
{"id": "1702.06921", "contents": "Title: Distributed Representation of Subgraphs Abstract: Network embeddings have become very popular in learning effective feature\nrepresentations of networks. Motivated by the recent successes of embeddings in\nnatural language processing, researchers have tried to find network embeddings\nin order to exploit machine learning algorithms for mining tasks like node\nclassification and edge prediction. However, most of the work focuses on\nfinding distributed representations of nodes, which are inherently ill-suited\nto tasks such as community detection which are intuitively dependent on\nsubgraphs.\n  Here, we propose sub2vec, an unsupervised scalable algorithm to learn feature\nrepresentations of arbitrary subgraphs. We provide means to characterize\nsimilarties between subgraphs and provide theoretical analysis of sub2vec and\ndemonstrate that it preserves the so-called local proximity. We also highlight\nthe usability of sub2vec by leveraging it for network mining tasks, like\ncommunity detection. We show that sub2vec gets significant gains over\nstate-of-the-art methods and node-embedding methods. In particular, sub2vec\noffers an approach to generate a richer vocabulary of features of subgraphs to\nsupport representation and reasoning. \n\n"}
{"id": "1702.07752", "contents": "Title: A supervised approach to time scale detection in dynamic networks Abstract: For any stream of time-stamped edges that form a dynamic network, an\nimportant choice is the aggregation granularity that an analyst uses to bin the\ndata. Picking such a windowing of the data is often done by hand, or left up to\nthe technology that is collecting the data. However, the choice can make a big\ndifference in the properties of the dynamic network. This is the time scale\ndetection problem. In previous work, this problem is often solved with a\nheuristic as an unsupervised task. As an unsupervised problem, it is difficult\nto measure how well a given algorithm performs. In addition, we show that the\nquality of the windowing is dependent on which task an analyst wants to perform\non the network after windowing. Therefore the time scale detection problem\nshould not be handled independently from the rest of the analysis of the\nnetwork.\n  We introduce a framework that tackles both of these issues: By measuring the\nperformance of the time scale detection algorithm based on how well a given\ntask is accomplished on the resulting network, we are for the first time able\nto directly compare different time scale detection algorithms to each other.\nUsing this framework, we introduce time scale detection algorithms that take a\nsupervised approach: they leverage ground truth on training data to find a good\nwindowing of the test data. We compare the supervised approach to previous\napproaches and several baselines on real data. \n\n"}
{"id": "1703.00034", "contents": "Title: Weighted Random Walk Sampling for Multi-Relational Recommendation Abstract: In the information overloaded web, personalized recommender systems are\nessential tools to help users find most relevant information. The most\nheavily-used recommendation frameworks assume user interactions that are\ncharacterized by a single relation. However, for many tasks, such as\nrecommendation in social networks, user-item interactions must be modeled as a\ncomplex network of multiple relations, not only a single relation. Recently\nresearch on multi-relational factorization and hybrid recommender models has\nshown that using extended meta-paths to capture additional information about\nboth users and items in the network can enhance the accuracy of recommendations\nin such networks. Most of this work is focused on unweighted heterogeneous\nnetworks, and to apply these techniques, weighted relations must be simplified\ninto binary ones. However, information associated with weighted edges, such as\nuser ratings, which may be crucial for recommendation, are lost in such\nbinarization. In this paper, we explore a random walk sampling method in which\nthe frequency of edge sampling is a function of edge weight, and apply this\ngenerate extended meta-paths in weighted heterogeneous networks. With this\nsampling technique, we demonstrate improved performance on multiple data sets\nboth in terms of recommendation accuracy and model generation efficiency. \n\n"}
{"id": "1703.01442", "contents": "Title: Recurrent Poisson Factorization for Temporal Recommendation Abstract: Poisson factorization is a probabilistic model of users and items for\nrecommendation systems, where the so-called implicit consumer data is modeled\nby a factorized Poisson distribution. There are many variants of Poisson\nfactorization methods who show state-of-the-art performance on real-world\nrecommendation tasks. However, most of them do not explicitly take into account\nthe temporal behavior and the recurrent activities of users which is essential\nto recommend the right item to the right user at the right time. In this paper,\nwe introduce Recurrent Poisson Factorization (RPF) framework that generalizes\nthe classical PF methods by utilizing a Poisson process for modeling the\nimplicit feedback. RPF treats time as a natural constituent of the model and\nbrings to the table a rich family of time-sensitive factorization models. To\nelaborate, we instantiate several variants of RPF who are capable of handling\ndynamic user preferences and item specification (DRPF), modeling the\nsocial-aspect of product adoption (SRPF), and capturing the consumption\nheterogeneity among users and items (HRPF). We also develop a variational\nalgorithm for approximate posterior inference that scales up to massive data\nsets. Furthermore, we demonstrate RPF's superior performance over many\nstate-of-the-art methods on synthetic dataset, and large scale real-world\ndatasets on music streaming logs, and user-item interactions in M-Commerce\nplatforms. \n\n"}
{"id": "1703.02504", "contents": "Title: Leveraging Large Amounts of Weakly Supervised Data for Multi-Language\n  Sentiment Classification Abstract: This paper presents a novel approach for multi-lingual sentiment\nclassification in short texts. This is a challenging task as the amount of\ntraining data in languages other than English is very limited. Previously\nproposed multi-lingual approaches typically require to establish a\ncorrespondence to English for which powerful classifiers are already available.\nIn contrast, our method does not require such supervision. We leverage large\namounts of weakly-supervised data in various languages to train a multi-layer\nconvolutional network and demonstrate the importance of using pre-training of\nsuch networks. We thoroughly evaluate our approach on various multi-lingual\ndatasets, including the recent SemEval-2016 sentiment prediction benchmark\n(Task 4), where we achieved state-of-the-art performance. We also compare the\nperformance of our model trained individually for each language to a variant\ntrained for all languages at once. We show that the latter model reaches\nslightly worse - but still acceptable - performance when compared to the single\nlanguage model, while benefiting from better generalization properties across\nlanguages. \n\n"}
{"id": "1703.02860", "contents": "Title: Spice up Your Chat: The Intentions and Sentiment Effects of Using Emoji Abstract: Emojis, as a new way of conveying nonverbal cues, are widely adopted in\ncomputer-mediated communications. In this paper, first from a message sender\nperspective, we focus on people's motives in using four types of emojis --\npositive, neutral, negative, and non-facial. We compare the willingness levels\nof using these emoji types for seven typical intentions that people usually\napply nonverbal cues for in communication. The results of extensive statistical\nhypothesis tests not only report the popularities of the intentions, but also\nuncover the subtle differences between emoji types in terms of intended uses.\nSecond, from a perspective of message recipients, we further study the\nsentiment effects of emojis, as well as their duplications, on verbal messages.\nDifferent from previous studies in emoji sentiment, we study the sentiments of\nemojis and their contexts as a whole. The experiment results indicate that the\npowers of conveying sentiment are different between four emoji types, and the\nsentiment effects of emojis vary in the contexts of different valences. \n\n"}
{"id": "1703.04837", "contents": "Title: SNE: Signed Network Embedding Abstract: Several network embedding models have been developed for unsigned networks.\nHowever, these models based on skip-gram cannot be applied to signed networks\nbecause they can only deal with one type of link. In this paper, we present our\nsigned network embedding model called SNE. Our SNE adopts the log-bilinear\nmodel, uses node representations of all nodes along a given path, and further\nincorporates two signed-type vectors to capture the positive or negative\nrelationship of each edge along the path. We conduct two experiments, node\nclassification and link prediction, on both directed and undirected signed\nnetworks and compare with four baselines including a matrix factorization\nmethod and three state-of-the-art unsigned network embedding models. The\nexperimental results demonstrate the effectiveness of our signed network\nembedding. \n\n"}
{"id": "1703.04943", "contents": "Title: Matched bipartite block model with covariates Abstract: Community detection or clustering is a fundamental task in the analysis of\nnetwork data. Many real networks have a bipartite structure which makes\ncommunity detection challenging. In this paper, we consider a model which\nallows for matched communities in the bipartite setting, in addition to node\ncovariates with information about the matching. We derive a simple fast\nalgorithm for fitting the model based on variational inference ideas and show\nits effectiveness on both simulated and real data. A variation of the model to\nallow for degree-correction is also considered, in addition to a novel approach\nto fitting such degree-corrected models. \n\n"}
{"id": "1703.06180", "contents": "Title: Effective Evaluation using Logged Bandit Feedback from Multiple Loggers Abstract: Accurately evaluating new policies (e.g. ad-placement models, ranking\nfunctions, recommendation functions) is one of the key prerequisites for\nimproving interactive systems. While the conventional approach to evaluation\nrelies on online A/B tests, recent work has shown that counterfactual\nestimators can provide an inexpensive and fast alternative, since they can be\napplied offline using log data that was collected from a different policy\nfielded in the past. In this paper, we address the question of how to estimate\nthe performance of a new target policy when we have log data from multiple\nhistoric policies. This question is of great relevance in practice, since\npolicies get updated frequently in most online systems. We show that naively\ncombining data from multiple logging policies can be highly suboptimal. In\nparticular, we find that the standard Inverse Propensity Score (IPS) estimator\nsuffers especially when logging and target policies diverge -- to a point where\nthrowing away data improves the variance of the estimator. We therefore propose\ntwo alternative estimators which we characterize theoretically and compare\nexperimentally. We find that the new estimators can provide substantially\nimproved estimation accuracy. \n\n"}
{"id": "1703.06630", "contents": "Title: Automatic Text Summarization Approaches to Speed up Topic Model Learning\n  Process Abstract: The number of documents available into Internet moves each day up. For this\nreason, processing this amount of information effectively and expressibly\nbecomes a major concern for companies and scientists. Methods that represent a\ntextual document by a topic representation are widely used in Information\nRetrieval (IR) to process big data such as Wikipedia articles. One of the main\ndifficulty in using topic model on huge data collection is related to the\nmaterial resources (CPU time and memory) required for model estimate. To deal\nwith this issue, we propose to build topic spaces from summarized documents. In\nthis paper, we present a study of topic space representation in the context of\nbig data. The topic space representation behavior is analyzed on different\nlanguages. Experiments show that topic spaces estimated from text summaries are\nas relevant as those estimated from the complete documents. The real advantage\nof such an approach is the processing time gain: we showed that the processing\ntime can be drastically reduced using summarized documents (more than 60\\% in\ngeneral). This study finally points out the differences between thematic\nrepresentations of documents depending on the targeted languages such as\nEnglish or latin languages. \n\n"}
{"id": "1703.08084", "contents": "Title: Multimodal Compact Bilinear Pooling for Multimodal Neural Machine\n  Translation Abstract: In state-of-the-art Neural Machine Translation, an attention mechanism is\nused during decoding to enhance the translation. At every step, the decoder\nuses this mechanism to focus on different parts of the source sentence to\ngather the most useful information before outputting its target word. Recently,\nthe effectiveness of the attention mechanism has also been explored for\nmultimodal tasks, where it becomes possible to focus both on sentence parts and\nimage regions. Approaches to pool two modalities usually include element-wise\nproduct, sum or concatenation. In this paper, we evaluate the more advanced\nMultimodal Compact Bilinear pooling method, which takes the outer product of\ntwo vectors to combine the attention features for the two modalities. This has\nbeen previously investigated for visual question answering. We try out this\napproach for multimodal image caption translation and show improvements\ncompared to basic combination methods. \n\n"}
{"id": "1703.08088", "contents": "Title: Rapid-Rate: A Framework for Semi-supervised Real-time Sentiment Trend\n  Detection in Unstructured Big Data Abstract: Commercial establishments like restaurants, service centres and retailers\nhave several sources of customer feedback about products and services, most of\nwhich need not be as structured as rated reviews provided by services like\nYelp, or Amazon, in terms of sentiment conveyed. For instance, Amazon provides\na fine-grained score on a numeric scale for product reviews. Some sources,\nhowever, like social media (Twitter, Facebook), mailing lists (Google Groups)\nand forums (Quora) contain text data that is much more voluminous, but\nunstructured and unlabelled. It might be in the best interests of a business\nestablishment to assess the general sentiment towards their brand on these\nplatforms as well. This text could be pipelined into a system with a built-in\nprediction model, with the objective of generating real-time graphs on opinion\nand sentiment trends. Although such tasks like the one described about have\nbeen explored with respect to document classification problems in the past, the\nimplementation described in this paper, by virtue of learning a continuous\nfunction rather than a discrete one, offers a lot more depth of insight as\ncompared to document classification approaches. This study aims to explore the\nvalidity of such a continuous function predicting model to quantify sentiment\nabout an entity, without the additional overhead of manual labelling, and\ncomputational preprocessing & feature extraction. This research project also\naims to design and implement a re-usable document regression pipeline as a\nframework, Rapid-Rate, that can be used to predict document scores in\nreal-time. \n\n"}
{"id": "1703.08098", "contents": "Title: A survey of embedding models of entities and relationships for knowledge\n  graph completion Abstract: Knowledge graphs (KGs) of real-world facts about entities and their\nrelationships are useful resources for a variety of natural language processing\ntasks. However, because knowledge graphs are typically incomplete, it is useful\nto perform knowledge graph completion or link prediction, i.e. predict whether\na relationship not in the knowledge graph is likely to be true. This paper\nserves as a comprehensive survey of embedding models of entities and\nrelationships for knowledge graph completion, summarizing up-to-date\nexperimental results on standard benchmark datasets and pointing out potential\nfuture research directions. \n\n"}
{"id": "1703.09831", "contents": "Title: A Deep Compositional Framework for Human-like Language Acquisition in\n  Virtual Environment Abstract: We tackle a task where an agent learns to navigate in a 2D maze-like\nenvironment called XWORLD. In each session, the agent perceives a sequence of\nraw-pixel frames, a natural language command issued by a teacher, and a set of\nrewards. The agent learns the teacher's language from scratch in a grounded and\ncompositional manner, such that after training it is able to correctly execute\nzero-shot commands: 1) the combination of words in the command never appeared\nbefore, and/or 2) the command contains new object concepts that are learned\nfrom another task but never learned from navigation. Our deep framework for the\nagent is trained end to end: it learns simultaneously the visual\nrepresentations of the environment, the syntax and semantics of the language,\nand the action module that outputs actions. The zero-shot learning capability\nof our framework results from its compositionality and modularity with\nparameter tying. We visualize the intermediate outputs of the framework,\ndemonstrating that the agent truly understands how to solve the problem. We\nbelieve that our results provide some preliminary insights on how to train an\nagent with similar abilities in a 3D environment. \n\n"}
{"id": "1704.00051", "contents": "Title: Reading Wikipedia to Answer Open-Domain Questions Abstract: This paper proposes to tackle open- domain question answering using Wikipedia\nas the unique knowledge source: the answer to any factoid question is a text\nspan in a Wikipedia article. This task of machine reading at scale combines the\nchallenges of document retrieval (finding the relevant articles) with that of\nmachine comprehension of text (identifying the answer spans from those\narticles). Our approach combines a search component based on bigram hashing and\nTF-IDF matching with a multi-layer recurrent neural network model trained to\ndetect answers in Wikipedia paragraphs. Our experiments on multiple existing QA\ndatasets indicate that (1) both modules are highly competitive with respect to\nexisting counterparts and (2) multitask learning using distant supervision on\ntheir combination is an effective complete system on this challenging task. \n\n"}
{"id": "1704.00552", "contents": "Title: A Transition-Based Directed Acyclic Graph Parser for UCCA Abstract: We present the first parser for UCCA, a cross-linguistically applicable\nframework for semantic representation, which builds on extensive typological\nwork and supports rapid annotation. UCCA poses a challenge for existing parsing\ntechniques, as it exhibits reentrancy (resulting in DAG structures),\ndiscontinuous structures and non-terminal nodes corresponding to complex\nsemantic units. To our knowledge, the conjunction of these formal properties is\nnot supported by any existing parser. Our transition-based parser, which uses a\nnovel transition set and features based on bidirectional LSTMs, has value not\njust for UCCA parsing: its ability to handle more general graph structures can\ninform the development of parsers for other semantic DAG structures, and in\nlanguages that frequently use discontinuous structures. \n\n"}
{"id": "1704.00784", "contents": "Title: Online and Linear-Time Attention by Enforcing Monotonic Alignments Abstract: Recurrent neural network models with an attention mechanism have proven to be\nextremely effective on a wide variety of sequence-to-sequence problems.\nHowever, the fact that soft attention mechanisms perform a pass over the entire\ninput sequence when producing each element in the output sequence precludes\ntheir use in online settings and results in a quadratic time complexity. Based\non the insight that the alignment between input and output sequence elements is\nmonotonic in many problems of interest, we propose an end-to-end differentiable\nmethod for learning monotonic alignments which, at test time, enables computing\nattention online and in linear time. We validate our approach on sentence\nsummarization, machine translation, and online speech recognition problems and\nachieve results competitive with existing sequence-to-sequence models. \n\n"}
{"id": "1704.00898", "contents": "Title: Interpretation of Semantic Tweet Representations Abstract: Research in analysis of microblogging platforms is experiencing a renewed\nsurge with a large number of works applying representation learning models for\napplications like sentiment analysis, semantic textual similarity computation,\nhashtag prediction, etc. Although the performance of the representation\nlearning models has been better than the traditional baselines for such tasks,\nlittle is known about the elementary properties of a tweet encoded within these\nrepresentations, or why particular representations work better for certain\ntasks. Our work presented here constitutes the first step in opening the\nblack-box of vector embeddings for tweets. Traditional feature engineering\nmethods for high-level applications have exploited various elementary\nproperties of tweets. We believe that a tweet representation is effective for\nan application because it meticulously encodes the application-specific\nelementary properties of tweets. To understand the elementary properties\nencoded in a tweet representation, we evaluate the representations on the\naccuracy to which they can model each of those properties such as tweet length,\npresence of particular words, hashtags, mentions, capitalization, etc. Our\nsystematic extensive study of nine supervised and four unsupervised tweet\nrepresentations against most popular eight textual and five social elementary\nproperties reveal that Bi-directional LSTMs (BLSTMs) and Skip-Thought Vectors\n(STV) best encode the textual and social properties of tweets respectively.\nFastText is the best model for low resource settings, providing very little\ndegradation with reduction in embedding size. Finally, we draw interesting\ninsights by correlating the model performance obtained for elementary property\nprediction tasks with the highlevel downstream applications. \n\n"}
{"id": "1704.01444", "contents": "Title: Learning to Generate Reviews and Discovering Sentiment Abstract: We explore the properties of byte-level recurrent language models. When given\nsufficient amounts of capacity, training data, and compute time, the\nrepresentations learned by these models include disentangled features\ncorresponding to high-level concepts. Specifically, we find a single unit which\nperforms sentiment analysis. These representations, learned in an unsupervised\nmanner, achieve state of the art on the binary subset of the Stanford Sentiment\nTreebank. They are also very data efficient. When using only a handful of\nlabeled examples, our approach matches the performance of strong baselines\ntrained on full datasets. We also demonstrate the sentiment unit has a direct\ninfluence on the generative process of the model. Simply fixing its value to be\npositive or negative generates samples with the corresponding positive or\nnegative sentiment. \n\n"}
{"id": "1704.01444", "contents": "Title: Learning to Generate Reviews and Discovering Sentiment Abstract: We explore the properties of byte-level recurrent language models. When given\nsufficient amounts of capacity, training data, and compute time, the\nrepresentations learned by these models include disentangled features\ncorresponding to high-level concepts. Specifically, we find a single unit which\nperforms sentiment analysis. These representations, learned in an unsupervised\nmanner, achieve state of the art on the binary subset of the Stanford Sentiment\nTreebank. They are also very data efficient. When using only a handful of\nlabeled examples, our approach matches the performance of strong baselines\ntrained on full datasets. We also demonstrate the sentiment unit has a direct\ninfluence on the generative process of the model. Simply fixing its value to be\npositive or negative generates samples with the corresponding positive or\nnegative sentiment. \n\n"}
{"id": "1704.01523", "contents": "Title: MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional\n  Neural Networks Abstract: Over 50 million scholarly articles have been published: they constitute a\nunique repository of knowledge. In particular, one may infer from them\nrelations between scientific concepts, such as synonyms and hyponyms.\nArtificial neural networks have been recently explored for relation extraction.\nIn this work, we continue this line of work and present a system based on a\nconvolutional neural network to extract relations. Our model ranked first in\nthe SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific\narticles (subtask C). \n\n"}
{"id": "1704.03225", "contents": "Title: Reconstruction of three-dimensional porous media using generative\n  adversarial neural networks Abstract: To evaluate the variability of multi-phase flow properties of porous media at\nthe pore scale, it is necessary to acquire a number of representative samples\nof the void-solid structure. While modern x-ray computer tomography has made it\npossible to extract three-dimensional images of the pore space, assessment of\nthe variability in the inherent material properties is often experimentally not\nfeasible. We present a novel method to reconstruct the solid-void structure of\nporous media by applying a generative neural network that allows an implicit\ndescription of the probability distribution represented by three-dimensional\nimage datasets. We show, by using an adversarial learning approach for neural\nnetworks, that this method of unsupervised learning is able to generate\nrepresentative samples of porous media that honor their statistics. We\nsuccessfully compare measures of pore morphology, such as the Euler\ncharacteristic, two-point statistics and directional single-phase permeability\nof synthetic realizations with the calculated properties of a bead pack, Berea\nsandstone, and Ketton limestone. Results show that GANs can be used to\nreconstruct high-resolution three-dimensional images of porous media at\ndifferent scales that are representative of the morphology of the images used\nto train the neural network. The fully convolutional nature of the trained\nneural network allows the generation of large samples while maintaining\ncomputational efficiency. Compared to classical stochastic methods of image\nreconstruction, the implicit representation of the learned data distribution\ncan be stored and reused to generate multiple realizations of the pore\nstructure very rapidly. \n\n"}
{"id": "1704.03560", "contents": "Title: ConceptNet at SemEval-2017 Task 2: Extending Word Embeddings with\n  Multilingual Relational Knowledge Abstract: This paper describes Luminoso's participation in SemEval 2017 Task 2,\n\"Multilingual and Cross-lingual Semantic Word Similarity\", with a system based\non ConceptNet. ConceptNet is an open, multilingual knowledge graph that focuses\non general knowledge that relates the meanings of words and phrases. Our\nsubmission to SemEval was an update of previous work that builds high-quality,\nmultilingual word embeddings from a combination of ConceptNet and\ndistributional semantics. Our system took first place in both subtasks. It\nranked first in 4 out of 5 of the separate languages, and also ranked first in\nall 10 of the cross-lingual language pairs. \n\n"}
{"id": "1704.05091", "contents": "Title: FEUP at SemEval-2017 Task 5: Predicting Sentiment Polarity and Intensity\n  with Financial Word Embeddings Abstract: This paper presents the approach developed at the Faculty of Engineering of\nUniversity of Porto, to participate in SemEval 2017, Task 5: Fine-grained\nSentiment Analysis on Financial Microblogs and News. The task consisted in\npredicting a real continuous variable from -1.0 to +1.0 representing the\npolarity and intensity of sentiment concerning companies/stocks mentioned in\nshort texts. We modeled the task as a regression analysis problem and combined\ntraditional techniques such as pre-processing short texts, bag-of-words\nrepresentations and lexical-based features with enhanced financial specific\nbag-of-embeddings. We used an external collection of tweets and news headlines\nmentioning companies/stocks from S\\&P 500 to create financial word embeddings\nwhich are able to capture domain-specific syntactic and semantic similarities.\nThe resulting approach obtained a cosine similarity score of 0.69 in sub-task\n5.1 - Microblogs and 0.68 in sub-task 5.2 - News Headlines. \n\n"}
{"id": "1704.05119", "contents": "Title: Exploring Sparsity in Recurrent Neural Networks Abstract: Recurrent Neural Networks (RNN) are widely used to solve a variety of\nproblems and as the quantity of data and the amount of available compute have\nincreased, so have model sizes. The number of parameters in recent\nstate-of-the-art networks makes them hard to deploy, especially on mobile\nphones and embedded devices. The challenge is due to both the size of the model\nand the time it takes to evaluate it. In order to deploy these RNNs\nefficiently, we propose a technique to reduce the parameters of a network by\npruning weights during the initial training of the network. At the end of\ntraining, the parameters of the network are sparse while accuracy is still\nclose to the original dense neural network. The network size is reduced by 8x\nand the time required to train the model remains constant. Additionally, we can\nprune a larger dense network to achieve better than baseline performance while\nstill reducing the total number of parameters significantly. Pruning RNNs\nreduces the size of the model and can also help achieve significant inference\ntime speed-up using sparse matrix multiply. Benchmarks show that using our\ntechnique model size can be reduced by 90% and speed-up is around 2x to 7x. \n\n"}
{"id": "1704.05982", "contents": "Title: Retrospective Higher-Order Markov Processes for User Trails Abstract: Users form information trails as they browse the web, checkin with a\ngeolocation, rate items, or consume media. A common problem is to predict what\na user might do next for the purposes of guidance, recommendation, or\nprefetching. First-order and higher-order Markov chains have been widely used\nmethods to study such sequences of data. First-order Markov chains are easy to\nestimate, but lack accuracy when history matters. Higher-order Markov chains,\nin contrast, have too many parameters and suffer from overfitting the training\ndata. Fitting these parameters with regularization and smoothing only offers\nmild improvements. In this paper we propose the retrospective higher-order\nMarkov process (RHOMP) as a low-parameter model for such sequences. This model\nis a special case of a higher-order Markov chain where the transitions depend\nretrospectively on a single history state instead of an arbitrary combination\nof history states. There are two immediate computational advantages: the number\nof parameters is linear in the order of the Markov chain and the model can be\nfit to large state spaces. Furthermore, by providing a specific structure to\nthe higher-order chain, RHOMPs improve the model accuracy by efficiently\nutilizing history states without risks of overfitting the data. We demonstrate\nhow to estimate a RHOMP from data and we demonstrate the effectiveness of our\nmethod on various real application datasets spanning geolocation data, review\nsequences, and business locations. The RHOMP model uniformly outperforms\nhigher-order Markov chains, Kneser-Ney regularization, and tensor\nfactorizations in terms of prediction accuracy. \n\n"}
{"id": "1704.06104", "contents": "Title: Neural End-to-End Learning for Computational Argumentation Mining Abstract: We investigate neural techniques for end-to-end computational argumentation\nmining (AM). We frame AM both as a token-based dependency parsing and as a\ntoken-based sequence tagging problem, including a multi-task learning setup.\nContrary to models that operate on the argument component level, we find that\nframing AM as dependency parsing leads to subpar performance results. In\ncontrast, less complex (local) tagging models based on BiLSTMs perform robustly\nacross classification scenarios, being able to catch long-range dependencies\ninherent to the AM problem. Moreover, we find that jointly learning 'natural'\nsubtasks, in a multi-task learning setup, improves performance. \n\n"}
{"id": "1704.06217", "contents": "Title: Reinforcement Learning with External Knowledge and Two-Stage Q-functions\n  for Predicting Popular Reddit Threads Abstract: This paper addresses the problem of predicting popularity of comments in an\nonline discussion forum using reinforcement learning, particularly addressing\ntwo challenges that arise from having natural language state and action spaces.\nFirst, the state representation, which characterizes the history of comments\ntracked in a discussion at a particular point, is augmented to incorporate the\nglobal context represented by discussions on world events available in an\nexternal knowledge source. Second, a two-stage Q-learning framework is\nintroduced, making it feasible to search the combinatorial action space while\nalso accounting for redundancy among sub-actions. We experiment with five\nReddit communities, showing that the two methods improve over previous reported\nresults on this task. \n\n"}
{"id": "1704.06687", "contents": "Title: Scatteract: Automated extraction of data from scatter plots Abstract: Charts are an excellent way to convey patterns and trends in data, but they\ndo not facilitate further modeling of the data or close inspection of\nindividual data points. We present a fully automated system for extracting the\nnumerical values of data points from images of scatter plots. We use deep\nlearning techniques to identify the key components of the chart, and optical\ncharacter recognition together with robust regression to map from pixels to the\ncoordinate system of the chart. We focus on scatter plots with linear scales,\nwhich already have several interesting challenges. Previous work has done fully\nautomatic extraction for other types of charts, but to our knowledge this is\nthe first approach that is fully automatic for scatter plots. Our method\nperforms well, achieving successful data extraction on 89% of the plots in our\ntest set. \n\n"}
{"id": "1704.06933", "contents": "Title: Adversarial Neural Machine Translation Abstract: In this paper, we study a new learning paradigm for Neural Machine\nTranslation (NMT). Instead of maximizing the likelihood of the human\ntranslation as in previous works, we minimize the distinction between human\ntranslation and the translation given by an NMT model. To achieve this goal,\ninspired by the recent success of generative adversarial networks (GANs), we\nemploy an adversarial training architecture and name it as Adversarial-NMT. In\nAdversarial-NMT, the training of the NMT model is assisted by an adversary,\nwhich is an elaborately designed Convolutional Neural Network (CNN). The goal\nof the adversary is to differentiate the translation result generated by the\nNMT model from that by human. The goal of the NMT model is to produce high\nquality translations so as to cheat the adversary. A policy gradient method is\nleveraged to co-train the NMT model and the adversary. Experimental results on\nEnglish$\\rightarrow$French and German$\\rightarrow$English translation tasks\nshow that Adversarial-NMT can achieve significantly better translation quality\nthan several strong baselines. \n\n"}
{"id": "1704.06956", "contents": "Title: Naturalizing a Programming Language via Interactive Learning Abstract: Our goal is to create a convenient natural language interface for performing\nwell-specified but complex actions such as analyzing data, manipulating text,\nand querying databases. However, existing natural language interfaces for such\ntasks are quite primitive compared to the power one wields with a programming\nlanguage. To bridge this gap, we start with a core programming language and\nallow users to \"naturalize\" the core language incrementally by defining\nalternative, more natural syntax and increasingly complex concepts in terms of\ncompositions of simpler ones. In a voxel world, we show that a community of\nusers can simultaneously teach a common system a diverse language and use it to\nbuild hundreds of complex voxel structures. Over the course of three days,\nthese users went from using only the core language to using the naturalized\nlanguage in 85.9\\% of the last 10K utterances. \n\n"}
{"id": "1704.06970", "contents": "Title: Differentiable Scheduled Sampling for Credit Assignment Abstract: We demonstrate that a continuous relaxation of the argmax operation can be\nused to create a differentiable approximation to greedy decoding for\nsequence-to-sequence (seq2seq) models. By incorporating this approximation into\nthe scheduled sampling training procedure (Bengio et al., 2015)--a well-known\ntechnique for correcting exposure bias--we introduce a new training objective\nthat is continuous and differentiable everywhere and that can provide\ninformative gradients near points where previous decoding decisions change\ntheir value. In addition, by using a related approximation, we demonstrate a\nsimilar approach to sampled-based training. Finally, we show that our approach\noutperforms cross-entropy training and scheduled sampling procedures in two\nsequence prediction tasks: named entity recognition and machine translation. \n\n"}
{"id": "1704.07050", "contents": "Title: Using Global Constraints and Reranking to Improve Cognates Detection Abstract: Global constraints and reranking have not been used in cognates detection\nresearch to date. We propose methods for using global constraints by performing\nrescoring of the score matrices produced by state of the art cognates detection\nsystems. Using global constraints to perform rescoring is complementary to\nstate of the art methods for performing cognates detection and results in\nsignificant performance improvements beyond current state of the art\nperformance on publicly available datasets with different language pairs and\nvarious conditions such as different levels of baseline state of the art\nperformance and different data size conditions, including with more realistic\nlarge data size conditions than have been evaluated with in the past. \n\n"}
{"id": "1704.07130", "contents": "Title: Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge\n  Graph Embeddings Abstract: We study a symmetric collaborative dialogue setting in which two agents, each\nwith private knowledge, must strategically communicate to achieve a common\ngoal. The open-ended dialogue state in this setting poses new challenges for\nexisting dialogue systems. We collected a dataset of 11K human-human dialogues,\nwhich exhibits interesting lexical, semantic, and strategic elements. To model\nboth structured knowledge and unstructured language, we propose a neural model\nwith dynamic knowledge graph embeddings that evolve as the dialogue progresses.\nAutomatic and human evaluations show that our model is both more effective at\nachieving the goal and more human-like than baseline neural and rule-based\nmodels. \n\n"}
{"id": "1704.07138", "contents": "Title: Lexically Constrained Decoding for Sequence Generation Using Grid Beam\n  Search Abstract: We present Grid Beam Search (GBS), an algorithm which extends beam search to\nallow the inclusion of pre-specified lexical constraints. The algorithm can be\nused with any model that generates a sequence $ \\mathbf{\\hat{y}} =\n\\{y_{0}\\ldots y_{T}\\} $, by maximizing $ p(\\mathbf{y} | \\mathbf{x}) =\n\\prod\\limits_{t}p(y_{t} | \\mathbf{x}; \\{y_{0} \\ldots y_{t-1}\\}) $. Lexical\nconstraints take the form of phrases or words that must be present in the\noutput sequence. This is a very general way to incorporate additional knowledge\ninto a model's output without requiring any modification of the model\nparameters or training data. We demonstrate the feasibility and flexibility of\nLexically Constrained Decoding by conducting experiments on Neural\nInteractive-Predictive Translation, as well as Domain Adaptation for Neural\nMachine Translation. Experiments show that GBS can provide large improvements\nin translation quality in interactive scenarios, and that, even without any\nuser input, GBS can be used to achieve significant gains in performance in\ndomain adaptation scenarios. \n\n"}
{"id": "1704.07156", "contents": "Title: Semi-supervised Multitask Learning for Sequence Labeling Abstract: We propose a sequence labeling framework with a secondary training objective,\nlearning to predict surrounding words for every word in the dataset. This\nlanguage modeling objective incentivises the system to learn general-purpose\npatterns of semantic and syntactic composition, which are also useful for\nimproving accuracy on different sequence labeling tasks. The architecture was\nevaluated on a range of datasets, covering the tasks of error detection in\nlearner texts, named entity recognition, chunking and POS-tagging. The novel\nlanguage modeling objective provided consistent performance improvements on\nevery benchmark, without requiring any additional annotated or unannotated\ndata. \n\n"}
{"id": "1704.07287", "contents": "Title: Parsing Speech: A Neural Approach to Integrating Lexical and\n  Acoustic-Prosodic Information Abstract: In conversational speech, the acoustic signal provides cues that help\nlisteners disambiguate difficult parses. For automatically parsing spoken\nutterances, we introduce a model that integrates transcribed text and\nacoustic-prosodic features using a convolutional neural network over energy and\npitch trajectories coupled with an attention-based recurrent neural network\nthat accepts text and prosodic features. We find that different types of\nacoustic-prosodic features are individually helpful, and together give\nstatistically significant improvements in parse and disfluency detection F1\nscores over a strong text-only baseline. For this study with known sentence\nboundaries, error analyses show that the main benefit of acoustic-prosodic\nfeatures is in sentences with disfluencies, attachment decisions are most\nimproved, and transcription errors obscure gains from prosody. \n\n"}
{"id": "1704.07734", "contents": "Title: DeepAM: Migrate APIs with Multi-modal Sequence to Sequence Learning Abstract: Computer programs written in one language are often required to be ported to\nother languages to support multiple devices and environments. When programs use\nlanguage specific APIs (Application Programming Interfaces), it is very\nchallenging to migrate these APIs to the corresponding APIs written in other\nlanguages. Existing approaches mine API mappings from projects that have\ncorresponding versions in two languages. They rely on the sparse availability\nof bilingual projects, thus producing a limited number of API mappings. In this\npaper, we propose an intelligent system called DeepAM for automatically mining\nAPI mappings from a large-scale code corpus without bilingual projects. The key\ncomponent of DeepAM is based on the multimodal sequence to sequence learning\narchitecture that aims to learn joint semantic representations of bilingual API\nsequences from big source code data. Experimental results indicate that DeepAM\nsignificantly increases the accuracy of API mappings as well as the number of\nAPI mappings, when compared with the state-of-the-art approaches. \n\n"}
{"id": "1704.08424", "contents": "Title: Multimodal Word Distributions Abstract: Word embeddings provide point representations of words containing useful\nsemantic information. We introduce multimodal word distributions formed from\nGaussian mixtures, for multiple word meanings, entailment, and rich uncertainty\ninformation. To learn these distributions, we propose an energy-based\nmax-margin objective. We show that the resulting approach captures uniquely\nexpressive semantic information, and outperforms alternatives, such as word2vec\nskip-grams, and Gaussian embeddings, on benchmark datasets such as word\nsimilarity and entailment. \n\n"}
{"id": "1704.08531", "contents": "Title: A Survey of Neural Network Techniques for Feature Extraction from Text Abstract: This paper aims to catalyze the discussions about text feature extraction\ntechniques using neural network architectures. The research questions discussed\nin the paper focus on the state-of-the-art neural network techniques that have\nproven to be useful tools for language processing, language generation, text\nclassification and other computational linguistics tasks. \n\n"}
{"id": "1704.08795", "contents": "Title: Mapping Instructions and Visual Observations to Actions with\n  Reinforcement Learning Abstract: We propose to directly map raw visual observations and text input to actions\nfor instruction execution. While existing approaches assume access to\nstructured environment representations or use a pipeline of separately trained\nmodels, we learn a single model to jointly reason about linguistic and visual\ninput. We use reinforcement learning in a contextual bandit setting to train a\nneural network agent. To guide the agent's exploration, we use reward shaping\nwith different forms of supervision. Our approach does not require intermediate\nrepresentations, planning procedures, or training different models. We evaluate\nin a simulated environment, and show significant improvements over supervised\nlearning and common reinforcement learning variants. \n\n"}
{"id": "1705.00403", "contents": "Title: Dependency Parsing with Dilated Iterated Graph CNNs Abstract: Dependency parses are an effective way to inject linguistic knowledge into\nmany downstream tasks, and many practitioners wish to efficiently parse\nsentences at scale. Recent advances in GPU hardware have enabled neural\nnetworks to achieve significant gains over the previous best models, these\nmodels still fail to leverage GPUs' capability for massive parallelism due to\ntheir requirement of sequential processing of the sentence. In response, we\npropose Dilated Iterated Graph Convolutional Neural Networks (DIG-CNNs) for\ngraph-based dependency parsing, a graph convolutional architecture that allows\nfor efficient end-to-end GPU parsing. In experiments on the English Penn\nTreeBank benchmark, we show that DIG-CNNs perform on par with some of the best\nneural network parsers. \n\n"}
{"id": "1705.00557", "contents": "Title: Discourse-Based Objectives for Fast Unsupervised Sentence Representation\n  Learning Abstract: This work presents a novel objective function for the unsupervised training\nof neural network sentence encoders. It exploits signals from paragraph-level\ndiscourse coherence to train these models to understand text. Our objective is\npurely discriminative, allowing us to train models many times faster than was\npossible under prior methods, and it yields models which perform well in\nextrinsic evaluations. \n\n"}
{"id": "1705.00581", "contents": "Title: Query-adaptive Video Summarization via Quality-aware Relevance\n  Estimation Abstract: Although the problem of automatic video summarization has recently received a\nlot of attention, the problem of creating a video summary that also highlights\nelements relevant to a search query has been less studied. We address this\nproblem by posing query-relevant summarization as a video frame subset\nselection problem, which lets us optimise for summaries which are\nsimultaneously diverse, representative of the entire video, and relevant to a\ntext query. We quantify relevance by measuring the distance between frames and\nqueries in a common textual-visual semantic embedding space induced by a neural\nnetwork. In addition, we extend the model to capture query-independent\nproperties, such as frame quality. We compare our method against previous state\nof the art on textual-visual embeddings for thumbnail selection and show that\nour model outperforms them on relevance prediction. Furthermore, we introduce a\nnew dataset, annotated with diversity and query-specific relevance labels. On\nthis dataset, we train and test our complete model for video summarization and\nshow that it outperforms standard baselines such as Maximal Marginal Relevance. \n\n"}
{"id": "1705.01020", "contents": "Title: Modeling Source Syntax for Neural Machine Translation Abstract: Even though a linguistics-free sequence to sequence model in neural machine\ntranslation (NMT) has certain capability of implicitly learning syntactic\ninformation of source sentences, this paper shows that source syntax can be\nexplicitly incorporated into NMT effectively to provide further improvements.\nSpecifically, we linearize parse trees of source sentences to obtain structural\nlabel sequences. On the basis, we propose three different sorts of encoders to\nincorporate source syntax into NMT: 1) Parallel RNN encoder that learns word\nand label annotation vectors parallelly; 2) Hierarchical RNN encoder that\nlearns word and label annotation vectors in a two-level hierarchy; and 3) Mixed\nRNN encoder that stitchingly learns word and label annotation vectors over\nsequences where words and labels are mixed. Experimentation on\nChinese-to-English translation demonstrates that all the three proposed\nsyntactic encoders are able to improve translation accuracy. It is interesting\nto note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best\nperformance with an significant improvement of 1.4 BLEU points. Moreover, an\nin-depth analysis from several perspectives is provided to reveal how source\nsyntax benefits NMT. \n\n"}
{"id": "1705.02314", "contents": "Title: Building Morphological Chains for Agglutinative Languages Abstract: In this paper, we build morphological chains for agglutinative languages by\nusing a log-linear model for the morphological segmentation task. The model is\nbased on the unsupervised morphological segmentation system called\nMorphoChains. We extend MorphoChains log linear model by expanding the\ncandidate space recursively to cover more split points for agglutinative\nlanguages such as Turkish, whereas in the original model candidates are\ngenerated by considering only binary segmentation of each word. The results\nshow that we improve the state-of-art Turkish scores by 12% having a F-measure\nof 72% and we improve the English scores by 3% having a F-measure of 74%.\nEventually, the system outperforms both MorphoChains and other well-known\nunsupervised morphological segmentation systems. The results indicate that\ncandidate generation plays an important role in such an unsupervised log-linear\nmodel that is learned using contrastive estimation with negative samples. \n\n"}
{"id": "1705.02355", "contents": "Title: Accelerating Science with Generative Adversarial Networks: An\n  Application to 3D Particle Showers in Multi-Layer Calorimeters Abstract: Physicists at the Large Hadron Collider (LHC) rely on detailed simulations of\nparticle collisions to build expectations of what experimental data may look\nlike under different theory modeling assumptions. Petabytes of simulated data\nare needed to develop analysis techniques, though they are expensive to\ngenerate using existing algorithms and computing resources. The modeling of\ndetectors and the precise description of particle cascades as they interact\nwith the material in the calorimeter are the most computationally demanding\nsteps in the simulation pipeline. We therefore introduce a deep neural\nnetwork-based generative model to enable high-fidelity, fast, electromagnetic\ncalorimeter simulation. There are still challenges for achieving precision\nacross the entire phase space, but our current solution can reproduce a variety\nof particle shower properties while achieving speed-up factors of up to\n100,000$\\times$. This opens the door to a new era of fast simulation that could\nsave significant computing time and disk space, while extending the reach of\nphysics searches and precision measurements at the LHC and beyond. \n\n"}
{"id": "1705.02801", "contents": "Title: Graph Embedding Techniques, Applications, and Performance: A Survey Abstract: Graphs, such as social networks, word co-occurrence networks, and\ncommunication networks, occur naturally in various real-world applications.\nAnalyzing them yields insight into the structure of society, language, and\ndifferent patterns of communication. Many approaches have been proposed to\nperform the analysis. Recently, methods which use the representation of graph\nnodes in vector space have gained traction from the research community. In this\nsurvey, we provide a comprehensive and structured analysis of various graph\nembedding techniques proposed in the literature. We first introduce the\nembedding task and its challenges such as scalability, choice of\ndimensionality, and features to be preserved, and their possible solutions. We\nthen present three categories of approaches based on factorization methods,\nrandom walks, and deep learning, with examples of representative algorithms in\neach category and analysis of their performance on various tasks. We evaluate\nthese state-of-the-art methods on a few common datasets and compare their\nperformance against one another. Our analysis concludes by suggesting some\npotential applications and future directions. We finally present the\nopen-source Python library we developed, named GEM (Graph Embedding Methods,\navailable at https://github.com/palash1992/GEM), which provides all presented\nalgorithms within a unified interface to foster and facilitate research on the\ntopic. \n\n"}
{"id": "1705.03633", "contents": "Title: Inferring and Executing Programs for Visual Reasoning Abstract: Existing methods for visual reasoning attempt to directly map inputs to\noutputs using black-box architectures without explicitly modeling the\nunderlying reasoning processes. As a result, these black-box models often learn\nto exploit biases in the data rather than learning to perform visual reasoning.\nInspired by module networks, this paper proposes a model for visual reasoning\nthat consists of a program generator that constructs an explicit representation\nof the reasoning process to be performed, and an execution engine that executes\nthe resulting program to produce an answer. Both the program generator and the\nexecution engine are implemented by neural networks, and are trained using a\ncombination of backpropagation and REINFORCE. Using the CLEVR benchmark for\nvisual reasoning, we show that our model significantly outperforms strong\nbaselines and generalizes better in a variety of settings. \n\n"}
{"id": "1705.03645", "contents": "Title: A Survey of Deep Learning Methods for Relation Extraction Abstract: Relation Extraction is an important sub-task of Information Extraction which\nhas the potential of employing deep learning (DL) models with the creation of\nlarge datasets using distant supervision. In this review, we compare the\ncontributions and pitfalls of the various DL models that have been used for the\ntask, to help guide the path ahead. \n\n"}
{"id": "1705.03865", "contents": "Title: Survey of Visual Question Answering: Datasets and Techniques Abstract: Visual question answering (or VQA) is a new and exciting problem that\ncombines natural language processing and computer vision techniques. We present\na survey of the various datasets and models that have been used to tackle this\ntask. The first part of the survey details the various datasets for VQA and\ncompares them along some common factors. The second part of this survey details\nthe different approaches for VQA, classified into four types: non-deep learning\nmodels, deep learning models without attention, deep learning models with\nattention, and other models which do not fit into the first three. Finally, we\ncompare the performances of these approaches and provide some directions for\nfuture work. \n\n"}
{"id": "1705.03919", "contents": "Title: A Minimal Span-Based Neural Constituency Parser Abstract: In this work, we present a minimal neural model for constituency parsing\nbased on independent scoring of labels and spans. We show that this model is\nnot only compatible with classical dynamic programming techniques, but also\nadmits a novel greedy top-down inference algorithm based on recursive\npartitioning of the input. We demonstrate empirically that both prediction\nschemes are competitive with recent work, and when combined with basic\nextensions to the scoring model are capable of achieving state-of-the-art\nsingle-model performance on the Penn Treebank (91.79 F1) and strong performance\non the French Treebank (82.23 F1). \n\n"}
{"id": "1705.05494", "contents": "Title: Data clustering with edge domination in complex networks Abstract: This paper presents a model for a dynamical system where particles dominate\nedges in a complex network. The proposed dynamical system is then extended to\nan application on the problem of community detection and data clustering. In\nthe case of the data clustering problem, 6 different techniques were simulated\non 10 different datasets in order to compare with the proposed technique. The\nresults show that the proposed algorithm performs well when prior knowledge of\nthe number of clusters is known to the algorithm. \n\n"}
{"id": "1705.06504", "contents": "Title: TableQA: Question Answering on Tabular Data Abstract: Tabular data is difficult to analyze and to search through, yielding for new\ntools and interfaces that would allow even non tech-savvy users to gain\ninsights from open datasets without resorting to specialized data analysis\ntools or even without having to fully understand the dataset structure. The\ngoal of our demonstration is to showcase answering natural language questions\nfrom tabular data, and to discuss related system configuration and model\ntraining aspects. Our prototype is publicly available and open-sourced (see\nhttps://svakulenko.ai.wu.ac.at/tableqa). \n\n"}
{"id": "1705.06824", "contents": "Title: Learning Convolutional Text Representations for Visual Question\n  Answering Abstract: Visual question answering is a recently proposed artificial intelligence task\nthat requires a deep understanding of both images and texts. In deep learning,\nimages are typically modeled through convolutional neural networks, and texts\nare typically modeled through recurrent neural networks. While the requirement\nfor modeling images is similar to traditional computer vision tasks, such as\nobject recognition and image classification, visual question answering raises a\ndifferent need for textual representation as compared to other natural language\nprocessing tasks. In this work, we perform a detailed analysis on natural\nlanguage questions in visual question answering. Based on the analysis, we\npropose to rely on convolutional neural networks for learning textual\nrepresentations. By exploring the various properties of convolutional neural\nnetworks specialized for text data, such as width and depth, we present our\n\"CNN Inception + Gate\" model. We show that our model improves question\nrepresentations and thus the overall accuracy of visual question answering\nmodels. We also show that the text representation requirement in visual\nquestion answering is more complicated and comprehensive than that in\nconventional natural language processing tasks, making it a better task to\nevaluate textual representation methods. Shallow models like fastText, which\ncan obtain comparable results with deep learning models in tasks like text\nclassification, are not suitable in visual question answering. \n\n"}
{"id": "1705.07008", "contents": "Title: A Lightweight Regression Method to Infer Psycholinguistic Properties for\n  Brazilian Portuguese Abstract: Psycholinguistic properties of words have been used in various approaches to\nNatural Language Processing tasks, such as text simplification and readability\nassessment. Most of these properties are subjective, involving costly and\ntime-consuming surveys to be gathered. Recent approaches use the limited\ndatasets of psycholinguistic properties to extend them automatically to large\nlexicons. However, some of the resources used by such approaches are not\navailable to most languages. This study presents a method to infer\npsycholinguistic properties for Brazilian Portuguese (BP) using regressors\nbuilt with a light set of features usually available for less resourced\nlanguages: word length, frequency lists, lexical databases composed of school\ndictionaries and word embedding models. The correlations between the properties\ninferred are close to those obtained by related works. The resulting resource\ncontains 26,874 words in BP annotated with concreteness, age of acquisition,\nimageability and subjective frequency. \n\n"}
{"id": "1705.07563", "contents": "Title: Learning to Rank Using Localized Geometric Mean Metrics Abstract: Many learning-to-rank (LtR) algorithms focus on query-independent model, in\nwhich query and document do not lie in the same feature space, and the rankers\nrely on the feature ensemble about query-document pair instead of the\nsimilarity between query instance and documents. However, existing algorithms\ndo not consider local structures in query-document feature space, and are\nfragile to irrelevant noise features. In this paper, we propose a novel\nRiemannian metric learning algorithm to capture the local structures and\ndevelop a robust LtR algorithm. First, we design a concept called \\textit{ideal\ncandidate document} to introduce metric learning algorithm to query-independent\nmodel. Previous metric learning algorithms aiming to find an optimal metric\nspace are only suitable for query-dependent model, in which query instance and\ndocuments belong to the same feature space and the similarity is directly\ncomputed from the metric space. Then we extend the new and extremely fast\nglobal Geometric Mean Metric Learning (GMML) algorithm to develop a localized\nGMML, namely L-GMML. Based on the combination of local learned metrics, we\nemploy the popular Normalized Discounted Cumulative Gain~(NDCG) scorer and\nWeighted Approximate Rank Pairwise (WARP) loss to optimize the \\textit{ideal\ncandidate document} for each query candidate set. Finally, we can quickly\nevaluate all candidates via the similarity between the \\textit{ideal candidate\ndocument} and other candidates. By leveraging the ability of metric learning\nalgorithms to describe the complex structural information, our approach gives\nus a principled and efficient way to perform LtR tasks. The experiments on\nreal-world datasets demonstrate that our proposed L-GMML algorithm outperforms\nthe state-of-the-art metric learning to rank methods and the stylish\nquery-independent LtR algorithms regarding accuracy and computational\nefficiency. \n\n"}
{"id": "1705.07860", "contents": "Title: On-the-fly Operation Batching in Dynamic Computation Graphs Abstract: Dynamic neural network toolkits such as PyTorch, DyNet, and Chainer offer\nmore flexibility for implementing models that cope with data of varying\ndimensions and structure, relative to toolkits that operate on statically\ndeclared computations (e.g., TensorFlow, CNTK, and Theano). However, existing\ntoolkits - both static and dynamic - require that the developer organize the\ncomputations into the batches necessary for exploiting high-performance\nalgorithms and hardware. This batching task is generally difficult, but it\nbecomes a major hurdle as architectures become complex. In this paper, we\npresent an algorithm, and its implementation in the DyNet toolkit, for\nautomatically batching operations. Developers simply write minibatch\ncomputations as aggregations of single instance computations, and the batching\nalgorithm seamlessly executes them, on the fly, using computationally efficient\nbatched operations. On a variety of tasks, we obtain throughput similar to that\nobtained with manual batches, as well as comparable speedups over\nsingle-instance learning on architectures that are impractical to batch\nmanually. \n\n"}
{"id": "1705.08209", "contents": "Title: Unbiasing Truncated Backpropagation Through Time Abstract: Truncated Backpropagation Through Time (truncated BPTT) is a widespread\nmethod for learning recurrent computational graphs. Truncated BPTT keeps the\ncomputational benefits of Backpropagation Through Time (BPTT) while relieving\nthe need for a complete backtrack through the whole data sequence at every\nstep. However, truncation favors short-term dependencies: the gradient estimate\nof truncated BPTT is biased, so that it does not benefit from the convergence\nguarantees from stochastic gradient theory. We introduce Anticipated Reweighted\nTruncated Backpropagation (ARTBP), an algorithm that keeps the computational\nbenefits of truncated BPTT, while providing unbiasedness. ARTBP works by using\nvariable truncation lengths together with carefully chosen compensation factors\nin the backpropagation equation. We check the viability of ARTBP on two tasks.\nFirst, a simple synthetic task where careful balancing of temporal dependencies\nat different scales is needed: truncated BPTT displays unreliable performance,\nand in worst case scenarios, divergence, while ARTBP converges reliably.\nSecond, on Penn Treebank character-level language modelling, ARTBP slightly\noutperforms truncated BPTT. \n\n"}
{"id": "1705.08947", "contents": "Title: Deep Voice 2: Multi-Speaker Neural Text-to-Speech Abstract: We introduce a technique for augmenting neural text-to-speech (TTS) with\nlowdimensional trainable speaker embeddings to generate different voices from a\nsingle model. As a starting point, we show improvements over the two\nstate-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and\nTacotron. We introduce Deep Voice 2, which is based on a similar pipeline with\nDeep Voice 1, but constructed with higher performance building blocks and\ndemonstrates a significant audio quality improvement over Deep Voice 1. We\nimprove Tacotron by introducing a post-processing neural vocoder, and\ndemonstrate a significant audio quality improvement. We then demonstrate our\ntechnique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron\non two multi-speaker TTS datasets. We show that a single neural TTS system can\nlearn hundreds of unique voices from less than half an hour of data per\nspeaker, while achieving high audio quality synthesis and preserving the\nspeaker identities almost perfectly. \n\n"}
{"id": "1705.09655", "contents": "Title: Style Transfer from Non-Parallel Text by Cross-Alignment Abstract: This paper focuses on style transfer on the basis of non-parallel text. This\nis an instance of a broad family of problems including machine translation,\ndecipherment, and sentiment modification. The key challenge is to separate the\ncontent from other aspects such as style. We assume a shared latent content\ndistribution across different text corpora, and propose a method that leverages\nrefined alignment of latent representations to perform style transfer. The\ntransferred sentences from one style should match example sentences from the\nother style as a population. We demonstrate the effectiveness of this\ncross-alignment method on three tasks: sentiment modification, decipherment of\nword substitution ciphers, and recovery of word order. \n\n"}
{"id": "1705.09975", "contents": "Title: A Deep Multi-View Learning Framework for City Event Extraction from\n  Twitter Data Streams Abstract: Cities have been a thriving place for citizens over the centuries due to\ntheir complex infrastructure. The emergence of the Cyber-Physical-Social\nSystems (CPSS) and context-aware technologies boost a growing interest in\nanalysing, extracting and eventually understanding city events which\nsubsequently can be utilised to leverage the citizen observations of their\ncities. In this paper, we investigate the feasibility of using Twitter textual\nstreams for extracting city events. We propose a hierarchical multi-view deep\nlearning approach to contextualise citizen observations of various city systems\nand services. Our goal has been to build a flexible architecture that can learn\nrepresentations useful for tasks, thus avoiding excessive task-specific feature\nengineering. We apply our approach on a real-world dataset consisting of event\nreports and tweets of over four months from San Francisco Bay Area dataset and\nadditional datasets collected from London. The results of our evaluations show\nthat our proposed solution outperforms the existing models and can be used for\nextracting city related events with an averaged accuracy of 81% over all\nclasses. To further evaluate the impact of our Twitter event extraction model,\nwe have used two sources of authorised reports through collecting road traffic\ndisruptions data from Transport for London API, and parsing the Time Out London\nwebsite for sociocultural events. The analysis showed that 49.5% of the Twitter\ntraffic comments are reported approximately five hours prior to the authorities\nofficial records. Moreover, we discovered that amongst the scheduled\nsociocultural event topics; tweets reporting transportation, cultural and\nsocial events are 31.75% more likely to influence the distribution of the\nTwitter comments than sport, weather and crime topics. \n\n"}
{"id": "1705.10209", "contents": "Title: On Multilingual Training of Neural Dependency Parsers Abstract: We show that a recently proposed neural dependency parser can be improved by\njoint training on multiple languages from the same family. The parser is\nimplemented as a deep neural network whose only input is orthographic\nrepresentations of words. In order to successfully parse, the network has to\ndiscover how linguistically relevant concepts can be inferred from word\nspellings. We analyze the representations of characters and words that are\nlearned by the network to establish which properties of languages were\naccounted for. In particular we show that the parser has approximately learned\nto associate Latin characters with their Cyrillic counterparts and that it can\ngroup Polish and Russian words that have a similar grammatical function.\nFinally, we evaluate the parser on selected languages from the Universal\nDependencies dataset and show that it is competitive with other recently\nproposed state-of-the art methods, while having a simple structure. \n\n"}
{"id": "1705.10874", "contents": "Title: Deep Learning for Environmentally Robust Speech Recognition: An Overview\n  of Recent Developments Abstract: Eliminating the negative effect of non-stationary environmental noise is a\nlong-standing research topic for automatic speech recognition that stills\nremains an important challenge. Data-driven supervised approaches, including\nones based on deep neural networks, have recently emerged as potential\nalternatives to traditional unsupervised approaches and with sufficient\ntraining, can alleviate the shortcomings of the unsupervised methods in various\nreal-life acoustic environments. In this light, we review recently developed,\nrepresentative deep learning approaches for tackling non-stationary additive\nand convolutional degradation of speech with the aim of providing guidelines\nfor those involved in the development of environmentally robust speech\nrecognition systems. We separately discuss single- and multi-channel techniques\ndeveloped for the front-end and back-end of speech recognition systems, as well\nas joint front-end and back-end training frameworks. \n\n"}
{"id": "1706.00290", "contents": "Title: Transfer Learning for Speech Recognition on a Budget Abstract: End-to-end training of automated speech recognition (ASR) systems requires\nmassive data and compute resources. We explore transfer learning based on model\nadaptation as an approach for training ASR models under constrained GPU memory,\nthroughput and training data. We conduct several systematic experiments\nadapting a Wav2Letter convolutional neural network originally trained for\nEnglish ASR to the German language. We show that this technique allows faster\ntraining on consumer-grade resources while requiring less training data in\norder to achieve the same accuracy, thereby lowering the cost of training ASR\nmodels in other languages. Model introspection revealed that small adaptations\nto the network's weights were sufficient for good performance, especially for\ninner layers. \n\n"}
{"id": "1706.00468", "contents": "Title: Function Assistant: A Tool for NL Querying of APIs Abstract: In this paper, we describe Function Assistant, a lightweight Python-based\ntoolkit for querying and exploring source code repositories using natural\nlanguage. The toolkit is designed to help end-users of a target API quickly\nfind information about functions through high-level natural language queries\nand descriptions. For a given text query and background API, the tool finds\ncandidate functions by performing a translation from the text to known\nrepresentations in the API using the semantic parsing approach of Richardson\nand Kuhn (2017). Translations are automatically learned from example text-code\npairs in example APIs. The toolkit includes features for building translation\npipelines and query engines for arbitrary source code projects. To explore this\nlast feature, we perform new experiments on 27 well-known Python projects\nhosted on Github. \n\n"}
{"id": "1706.00612", "contents": "Title: Attentive Convolutional Neural Network based Speech Emotion Recognition:\n  A Study on the Impact of Input Features, Signal Length, and Acted Speech Abstract: Speech emotion recognition is an important and challenging task in the realm\nof human-computer interaction. Prior work proposed a variety of models and\nfeature sets for training a system. In this work, we conduct extensive\nexperiments using an attentive convolutional neural network with multi-view\nlearning objective function. We compare system performance using different\nlengths of the input signal, different types of acoustic features and different\ntypes of emotion speech (improvised/scripted). Our experimental results on the\nInteractive Emotional Motion Capture (IEMOCAP) database reveal that the\nrecognition performance strongly depends on the type of speech data independent\nof the choice of input features. Furthermore, we achieved state-of-the-art\nresults on the improvised speech data of IEMOCAP. \n\n"}
{"id": "1706.00884", "contents": "Title: Task-specific Word Identification from Short Texts Using a Convolutional\n  Neural Network Abstract: Task-specific word identification aims to choose the task-related words that\nbest describe a short text. Existing approaches require well-defined seed words\nor lexical dictionaries (e.g., WordNet), which are often unavailable for many\napplications such as social discrimination detection and fake review detection.\nHowever, we often have a set of labeled short texts where each short text has a\ntask-related class label, e.g., discriminatory or non-discriminatory, specified\nby users or learned by classification algorithms. In this paper, we focus on\nidentifying task-specific words and phrases from short texts by exploiting\ntheir class labels rather than using seed words or lexical dictionaries. We\nconsider the task-specific word and phrase identification as feature learning.\nWe train a convolutional neural network over a set of labeled texts and use\nscore vectors to localize the task-specific words and phrases. Experimental\nresults on sentiment word identification show that our approach significantly\noutperforms existing methods. We further conduct two case studies to show the\neffectiveness of our approach. One case study on a crawled tweets dataset\ndemonstrates that our approach can successfully capture the\ndiscrimination-related words/phrases. The other case study on fake review\ndetection shows that our approach can identify the fake-review words/phrases. \n\n"}
{"id": "1706.00957", "contents": "Title: Semantic Vector Encoding and Similarity Search Using Fulltext Search\n  Engines Abstract: Vector representations and vector space modeling (VSM) play a central role in\nmodern machine learning. We propose a novel approach to `vector similarity\nsearching' over dense semantic representations of words and documents that can\nbe deployed on top of traditional inverted-index-based fulltext engines, taking\nadvantage of their robustness, stability, scalability and ubiquity.\n  We show that this approach allows the indexing and querying of dense vectors\nin text domains. This opens up exciting avenues for major efficiency gains,\nalong with simpler deployment, scaling and monitoring.\n  The end result is a fast and scalable vector database with a tunable\ntrade-off between vector search performance and quality, backed by a standard\nfulltext engine such as Elasticsearch.\n  We empirically demonstrate its querying performance and quality by applying\nthis solution to the task of semantic searching over a dense vector\nrepresentation of the entire English Wikipedia. \n\n"}
{"id": "1706.01556", "contents": "Title: Deep learning for extracting protein-protein interactions from\n  biomedical literature Abstract: State-of-the-art methods for protein-protein interaction (PPI) extraction are\nprimarily feature-based or kernel-based by leveraging lexical and syntactic\ninformation. But how to incorporate such knowledge in the recent deep learning\nmethods remains an open question. In this paper, we propose a multichannel\ndependency-based convolutional neural network model (McDepCNN). It applies one\nchannel to the embedding vector of each word in the sentence, and another\nchannel to the embedding vector of the head of the corresponding word.\nTherefore, the model can use richer information obtained from different\nchannels. Experiments on two public benchmarking datasets, AIMed and BioInfer,\ndemonstrate that McDepCNN compares favorably to the state-of-the-art\nrich-feature and single-kernel based methods. In addition, McDepCNN achieves\n24.4% relative improvement in F1-score over the state-of-the-art methods on\ncross-corpus evaluation and 12% improvement in F1-score over kernel-based\nmethods on \"difficult\" instances. These results suggest that McDepCNN\ngeneralizes more easily over different corpora, and is capable of capturing\nlong distance features in the sentences. \n\n"}
{"id": "1706.02222", "contents": "Title: Gated Recurrent Neural Tensor Network Abstract: Recurrent Neural Networks (RNNs), which are a powerful scheme for modeling\ntemporal and sequential data need to capture long-term dependencies on datasets\nand represent them in hidden layers with a powerful model to capture more\ninformation from inputs. For modeling long-term dependencies in a dataset, the\ngating mechanism concept can help RNNs remember and forget previous\ninformation. Representing the hidden layers of an RNN with more expressive\noperations (i.e., tensor products) helps it learn a more complex relationship\nbetween the current input and the previous hidden layer information. These\nideas can generally improve RNN performances. In this paper, we proposed a\nnovel RNN architecture that combine the concepts of gating mechanism and the\ntensor product into a single model. By combining these two concepts into a\nsingle RNN, our proposed models learn long-term dependencies by modeling with\ngating units and obtain more expressive and direct interaction between input\nand hidden layers using a tensor product on 3-dimensional array (tensor) weight\nparameters. We use Long Short Term Memory (LSTM) RNN and Gated Recurrent Unit\n(GRU) RNN and combine them with a tensor product inside their formulations. Our\nproposed RNNs, which are called a Long-Short Term Memory Recurrent Neural\nTensor Network (LSTMRNTN) and Gated Recurrent Unit Recurrent Neural Tensor\nNetwork (GRURNTN), are made by combining the LSTM and GRU RNN models with the\ntensor product. We conducted experiments with our proposed models on word-level\nand character-level language modeling tasks and revealed that our proposed\nmodels significantly improved their performance compared to our baseline\nmodels. \n\n"}
{"id": "1706.02427", "contents": "Title: Content-Based Table Retrieval for Web Queries Abstract: Understanding the connections between unstructured text and semi-structured\ntable is an important yet neglected problem in natural language processing. In\nthis work, we focus on content-based table retrieval. Given a query, the task\nis to find the most relevant table from a collection of tables. Further\nprogress towards improving this area requires powerful models of semantic\nmatching and richer training and evaluation resources. To remedy this, we\npresent a ranking based approach, and implement both carefully designed\nfeatures and neural network architectures to measure the relevance between a\nquery and the content of a table. Furthermore, we release an open-domain\ndataset that includes 21,113 web queries for 273,816 tables. We conduct\ncomprehensive experiments on both real world and synthetic datasets. Results\nverify the effectiveness of our approach and present the challenges for this\ntask. \n\n"}
{"id": "1706.03146", "contents": "Title: Rethinking Skip-thought: A Neighborhood based Approach Abstract: We study the skip-thought model with neighborhood information as weak\nsupervision. More specifically, we propose a skip-thought neighbor model to\nconsider the adjacent sentences as a neighborhood. We train our skip-thought\nneighbor model on a large corpus with continuous sentences, and then evaluate\nthe trained model on 7 tasks, which include semantic relatedness, paraphrase\ndetection, and classification benchmarks. Both quantitative comparison and\nqualitative investigation are conducted. We empirically show that, our\nskip-thought neighbor model performs as well as the skip-thought model on\nevaluation tasks. In addition, we found that, incorporating an autoencoder path\nin our model didn't aid our model to perform better, while it hurts the\nperformance of the skip-thought model. \n\n"}
{"id": "1706.03196", "contents": "Title: Online Learning for Neural Machine Translation Post-editing Abstract: Neural machine translation has meant a revolution of the field. Nevertheless,\npost-editing the outputs of the system is mandatory for tasks requiring high\ntranslation quality. Post-editing offers a unique opportunity for improving\nneural machine translation systems, using online learning techniques and\ntreating the post-edited translations as new, fresh training data. We review\nclassical learning methods and propose a new optimization algorithm. We\nthoroughly compare online learning algorithms in a post-editing scenario.\nResults show significant improvements in translation quality and effort\nreduction. \n\n"}
{"id": "1706.03441", "contents": "Title: Dialog Structure Through the Lens of Gender, Gender Environment, and\n  Power Abstract: Understanding how the social context of an interaction affects our dialog\nbehavior is of great interest to social scientists who study human behavior, as\nwell as to computer scientists who build automatic methods to infer those\nsocial contexts. In this paper, we study the interaction of power, gender, and\ndialog behavior in organizational interactions. In order to perform this study,\nwe first construct the Gender Identified Enron Corpus of emails, in which we\nsemi-automatically assign the gender of around 23,000 individuals who authored\naround 97,000 email messages in the Enron corpus. This corpus, which is made\nfreely available, is orders of magnitude larger than previously existing gender\nidentified corpora in the email domain. Next, we use this corpus to perform a\nlarge-scale data-oriented study of the interplay of gender and manifestations\nof power. We argue that, in addition to one's own gender, the \"gender\nenvironment\" of an interaction, i.e., the gender makeup of one's interlocutors,\nalso affects the way power is manifested in dialog. We focus especially on\nmanifestations of power in the dialog structure --- both, in a shallow sense\nthat disregards the textual content of messages (e.g., how often do the\nparticipants contribute, how often do they get replies etc.), as well as the\nstructure that is expressed within the textual content (e.g., who issues\nrequests and how are they made, whose requests get responses etc.). We find\nthat both gender and gender environment affect the ways power is manifested in\ndialog, resulting in patterns that reveal the underlying factors. Finally, we\nshow the utility of gender information in the problem of automatically\npredicting the direction of power between pairs of participants in email\ninteractions. \n\n"}
{"id": "1706.03583", "contents": "Title: Streaming Non-monotone Submodular Maximization: Personalized Video\n  Summarization on the Fly Abstract: The need for real time analysis of rapidly producing data streams (e.g.,\nvideo and image streams) motivated the design of streaming algorithms that can\nefficiently extract and summarize useful information from massive data \"on the\nfly\". Such problems can often be reduced to maximizing a submodular set\nfunction subject to various constraints. While efficient streaming methods have\nbeen recently developed for monotone submodular maximization, in a wide range\nof applications, such as video summarization, the underlying utility function\nis non-monotone, and there are often various constraints imposed on the\noptimization problem to consider privacy or personalization. We develop the\nfirst efficient single pass streaming algorithm, Streaming Local Search, that\nfor any streaming monotone submodular maximization algorithm with approximation\nguarantee $\\alpha$ under a collection of independence systems ${\\cal I}$,\nprovides a constant $1/\\big(1+2/\\sqrt{\\alpha}+1/\\alpha\n+2d(1+\\sqrt{\\alpha})\\big)$ approximation guarantee for maximizing a\nnon-monotone submodular function under the intersection of ${\\cal I}$ and $d$\nknapsack constraints. Our experiments show that for video summarization, our\nmethod runs more than 1700 times faster than previous work, while maintaining\npractically the same performance. \n\n"}
{"id": "1706.03610", "contents": "Title: Neural Domain Adaptation for Biomedical Question Answering Abstract: Factoid question answering (QA) has recently benefited from the development\nof deep learning (DL) systems. Neural network models outperform traditional\napproaches in domains where large datasets exist, such as SQuAD (ca. 100,000\nquestions) for Wikipedia articles. However, these systems have not yet been\napplied to QA in more specific domains, such as biomedicine, because datasets\nare generally too small to train a DL system from scratch. For example, the\nBioASQ dataset for biomedical QA comprises less then 900 factoid (single\nanswer) and list (multiple answers) QA instances. In this work, we adapt a\nneural QA system trained on a large open-domain dataset (SQuAD, source) to a\nbiomedical dataset (BioASQ, target) by employing various transfer learning\ntechniques. Our network architecture is based on a state-of-the-art QA system,\nextended with biomedical word embeddings and a novel mechanism to answer list\nquestions. In contrast to existing biomedical QA systems, our system does not\nrely on domain-specific ontologies, parsers or entity taggers, which are\nexpensive to create. Despite this fact, our systems achieve state-of-the-art\nresults on factoid questions and competitive results on list questions. \n\n"}
{"id": "1706.03757", "contents": "Title: Semantic Entity Retrieval Toolkit Abstract: Unsupervised learning of low-dimensional, semantic representations of words\nand entities has recently gained attention. In this paper we describe the\nSemantic Entity Retrieval Toolkit (SERT) that provides implementations of our\npreviously published entity representation models. The toolkit provides a\nunified interface to different representation learning algorithms, fine-grained\nparsing configuration and can be used transparently with GPUs. In addition,\nusers can easily modify existing models or implement their own models in the\nframework. After model training, SERT can be used to rank entities according to\na textual query and extract the learned entity/word representation for use in\ndownstream algorithms, such as clustering or recommendation. \n\n"}
{"id": "1706.03993", "contents": "Title: Getting deep recommenders fit: Bloom embeddings for sparse binary\n  input/output networks Abstract: Recommendation algorithms that incorporate techniques from deep learning are\nbecoming increasingly popular. Due to the structure of the data coming from\nrecommendation domains (i.e., one-hot-encoded vectors of item preferences),\nthese algorithms tend to have large input and output dimensionalities that\ndominate their overall size. This makes them difficult to train, due to the\nlimited memory of graphical processing units, and difficult to deploy on mobile\ndevices with limited hardware. To address these difficulties, we propose Bloom\nembeddings, a compression technique that can be applied to the input and output\nof neural network models dealing with sparse high-dimensional binary-coded\ninstances. Bloom embeddings are computationally efficient, and do not seriously\ncompromise the accuracy of the model up to 1/5 compression ratios. In some\ncases, they even improve over the original accuracy, with relative increases up\nto 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4\nalternative methods, obtaining favorable results. We also discuss a number of\nfurther advantages of Bloom embeddings, such as 'on-the-fly' constant-time\noperation, zero or marginal space requirements, training time speedups, or the\nfact that they do not require any change to the core model architecture or\ntraining configuration. \n\n"}
{"id": "1706.04223", "contents": "Title: Adversarially Regularized Autoencoders Abstract: Deep latent variable models, trained using variational autoencoders or\ngenerative adversarial networks, are now a key technique for representation\nlearning of continuous structures. However, applying similar methods to\ndiscrete structures, such as text sequences or discretized images, has proven\nto be more challenging. In this work, we propose a flexible method for training\ndeep latent variable models of discrete structures. Our approach is based on\nthe recently-proposed Wasserstein autoencoder (WAE) which formalizes the\nadversarial autoencoder (AAE) as an optimal transport problem. We first extend\nthis framework to model discrete sequences, and then further explore different\nlearned priors targeting a controllable representation. This adversarially\nregularized autoencoder (ARAE) allows us to generate natural textual outputs as\nwell as perform manipulations in the latent space to induce change in the\noutput space. Finally we show that the latent representation can be trained to\nperform unaligned textual style transfer, giving improvements both in\nautomatic/human evaluation compared to existing methods. \n\n"}
{"id": "1706.05087", "contents": "Title: Plan, Attend, Generate: Character-level Neural Machine Translation with\n  Planning in the Decoder Abstract: We investigate the integration of a planning mechanism into an\nencoder-decoder architecture with an explicit alignment for character-level\nmachine translation. We develop a model that plans ahead when it computes\nalignments between the source and target sequences, constructing a matrix of\nproposed future alignments and a commitment vector that governs whether to\nfollow or recompute the plan. This mechanism is inspired by the strategic\nattentive reader and writer (STRAW) model. Our proposed model is end-to-end\ntrainable with fully differentiable operations. We show that it outperforms a\nstrong baseline on three character-level decoder neural machine translation on\nWMT'15 corpus. Our analysis demonstrates that our model can compute\nqualitatively intuitive alignments and achieves superior performance with fewer\nparameters. \n\n"}
{"id": "1706.06314", "contents": "Title: Mining Significant Microblogs for Misinformation Identification: An\n  Attention-based Approach Abstract: With the rapid growth of social media, massive misinformation is also\nspreading widely on social media, such as microblog, and bring negative effects\nto human life. Nowadays, automatic misinformation identification has drawn\nattention from academic and industrial communities. For an event on social\nmedia usually consists of multiple microblogs, current methods are mainly based\non global statistical features. However, information on social media is full of\nnoisy and outliers, which should be alleviated. Moreover, most of microblogs\nabout an event have little contribution to the identification of\nmisinformation, where useful information can be easily overwhelmed by useless\ninformation. Thus, it is important to mine significant microblogs for a\nreliable misinformation identification method. In this paper, we propose an\nAttention-based approach for Identification of Misinformation (AIM). Based on\nthe attention mechanism, AIM can select microblogs with largest attention\nvalues for misinformation identification. The attention mechanism in AIM\ncontains two parts: content attention and dynamic attention. Content attention\nis calculated based textual features of each microblog. Dynamic attention is\nrelated to the time interval between the posting time of a microblog and the\nbeginning of the event. To evaluate AIM, we conduct a series of experiments on\nthe Weibo dataset and the Twitter dataset, and the experimental results show\nthat the proposed AIM model outperforms the state-of-the-art methods. \n\n"}
{"id": "1706.06714", "contents": "Title: Neural-based Natural Language Generation in Dialogue using RNN\n  Encoder-Decoder with Semantic Aggregation Abstract: Natural language generation (NLG) is an important component in spoken\ndialogue systems. This paper presents a model called Encoder-Aggregator-Decoder\nwhich is an extension of an Recurrent Neural Network based Encoder-Decoder\narchitecture. The proposed Semantic Aggregator consists of two components: an\nAligner and a Refiner. The Aligner is a conventional attention calculated over\nthe encoded input information, while the Refiner is another attention or gating\nmechanism stacked over the attentive Aligner in order to further select and\naggregate the semantic elements. The proposed model can be jointly trained both\nsentence planning and surface realization to produce natural language\nutterances. The model was extensively assessed on four different NLG domains,\nin which the experimental results showed that the proposed generator\nconsistently outperforms the previous methods on all the NLG domains. \n\n"}
{"id": "1706.07276", "contents": "Title: Jointly Learning Word Embeddings and Latent Topics Abstract: Word embedding models such as Skip-gram learn a vector-space representation\nfor each word, based on the local word collocation patterns that are observed\nin a text corpus. Latent topic models, on the other hand, take a more global\nview, looking at the word distributions across the corpus to assign a topic to\neach word occurrence. These two paradigms are complementary in how they\nrepresent the meaning of word occurrences. While some previous works have\nalready looked at using word embeddings for improving the quality of latent\ntopics, and conversely, at using latent topics for improving word embeddings,\nsuch \"two-step\" methods cannot capture the mutual interaction between the two\nparadigms. In this paper, we propose STE, a framework which can learn word\nembeddings and latent topics in a unified manner. STE naturally obtains\ntopic-specific word embeddings, and thus addresses the issue of polysemy. At\nthe same time, it also learns the term distributions of the topics, and the\ntopic distributions of the documents. Our experimental results demonstrate that\nthe STE model can indeed generate useful topic-specific word embeddings and\ncoherent latent topics in an effective and efficient way. \n\n"}
{"id": "1706.07440", "contents": "Title: End-to-end Conversation Modeling Track in DSTC6 Abstract: End-to-end training of neural networks is a promising approach to automatic\nconstruction of dialog systems using a human-to-human dialog corpus. Recently,\nVinyals et al. tested neural conversation models using OpenSubtitles. Lowe et\nal. released the Ubuntu Dialogue Corpus for researching unstructured multi-turn\ndialogue systems. Furthermore, the approach has been extended to accomplish\ntask oriented dialogs to provide information properly with natural\nconversation. For example, Ghazvininejad et al. proposed a knowledge grounded\nneural conversation model [3], where the research is aiming at combining\nconversational dialogs with task-oriented knowledge using unstructured data\nsuch as Twitter data for conversation and Foursquare data for external\nknowledge.However, the task is still limited to a restaurant information\nservice, and has not yet been tested with a wide variety of dialog tasks. In\naddition, it is still unclear how to create intelligent dialog systems that can\nrespond like a human agent.\n  In consideration of these problems, we proposed a challenge track to the 6th\ndialog system technology challenges (DSTC6) using human-to-human dialog data to\nmimic human dialog behaviors. The focus of the challenge track is to train\nend-to-end conversation models from human-to-human conversation and accomplish\nend-to-end dialog tasks in various situations assuming a customer service, in\nwhich a system plays a role of human agent and generates natural and\ninformative sentences in response to user's questions or comments given dialog\ncontext. \n\n"}
{"id": "1706.07684", "contents": "Title: Contextual Sequence Modeling for Recommendation with Recurrent Neural\n  Networks Abstract: Recommendations can greatly benefit from good representations of the user\nstate at recommendation time. Recent approaches that leverage Recurrent Neural\nNetworks (RNNs) for session-based recommendations have shown that Deep Learning\nmodels can provide useful user representations for recommendation. However,\ncurrent RNN modeling approaches summarize the user state by only taking into\naccount the sequence of items that the user has interacted with in the past,\nwithout taking into account other essential types of context information such\nas the associated types of user-item interactions, the time gaps between events\nand the time of day for each interaction. To address this, we propose a new\nclass of Contextual Recurrent Neural Networks for Recommendation (CRNNs) that\ncan take into account the contextual information both in the input and output\nlayers and modifying the behavior of the RNN by combining the context embedding\nwith the item embedding and more explicitly, in the model dynamics, by\nparametrizing the hidden unit transitions as a function of context information.\nWe compare our CRNNs approach with RNNs and non-sequential baselines and show\ngood improvements on the next event prediction task. \n\n"}
{"id": "1706.07845", "contents": "Title: HARP: Hierarchical Representation Learning for Networks Abstract: We present HARP, a novel method for learning low dimensional embeddings of a\ngraph's nodes which preserves higher-order structural features. Our proposed\nmethod achieves this by compressing the input graph prior to embedding it,\neffectively avoiding troublesome embedding configurations (i.e. local minima)\nwhich can pose problems to non-convex optimization. HARP works by finding a\nsmaller graph which approximates the global structure of its input. This\nsimplified graph is used to learn a set of initial representations, which serve\nas good initializations for learning representations in the original, detailed\ngraph. We inductively extend this idea, by decomposing a graph in a series of\nlevels, and then embed the hierarchy of graphs from the coarsest one to the\noriginal graph. HARP is a general meta-strategy to improve all of the\nstate-of-the-art neural algorithms for embedding graphs, including DeepWalk,\nLINE, and Node2vec. Indeed, we demonstrate that applying HARP's hierarchical\nparadigm yields improved implementations for all three of these methods, as\nevaluated on both classification tasks on real-world graphs such as DBLP,\nBlogCatalog, CiteSeer, and Arxiv, where we achieve a performance gain over the\noriginal implementations by up to 14% Macro F1. \n\n"}
{"id": "1706.09335", "contents": "Title: Generating Appealing Brand Names Abstract: Providing appealing brand names to newly launched products, newly formed\ncompanies or for renaming existing companies is highly important as it can play\na crucial role in deciding its success or failure. In this work, we propose a\ncomputational method to generate appealing brand names based on the description\nof such entities. We use quantitative scores for readability, pronounceability,\nmemorability and uniqueness of the generated names to rank order them. A set of\ndiverse appealing names is recommended to the user for the brand naming task.\nExperimental results show that the names generated by our approach are more\nappealing than names which prior approaches and recruited humans could come up. \n\n"}
{"id": "1706.09739", "contents": "Title: A Deep Multimodal Approach for Cold-start Music Recommendation Abstract: An increasing amount of digital music is being published daily. Music\nstreaming services often ingest all available music, but this poses a\nchallenge: how to recommend new artists for which prior knowledge is scarce? In\nthis work we aim to address this so-called cold-start problem by combining text\nand audio information with user feedback data using deep network architectures.\nOur method is divided into three steps. First, artist embeddings are learned\nfrom biographies by combining semantics, text features, and aggregated usage\ndata. Second, track embeddings are learned from the audio signal and available\nfeedback data. Finally, artist and track embeddings are combined in a\nmultimodal network. Results suggest that both splitting the recommendation\nproblem between feature levels (i.e., artist metadata and audio track), and\nmerging feature embeddings in a multimodal approach improve the accuracy of the\nrecommendations. \n\n"}
{"id": "1706.09789", "contents": "Title: Two-Stage Synthesis Networks for Transfer Learning in Machine\n  Comprehension Abstract: We develop a technique for transfer learning in machine comprehension (MC)\nusing a novel two-stage synthesis network (SynNet). Given a high-performing MC\nmodel in one domain, our technique aims to answer questions about documents in\nanother domain, where we use no labeled data of question-answer pairs. Using\nthe proposed SynNet with a pretrained model from the SQuAD dataset on the\nchallenging NewsQA dataset, we achieve an F1 measure of 44.3% with a single\nmodel and 46.6% with an ensemble, approaching performance of in-domain models\n(F1 measure of 50.0%) and outperforming the out-of-domain baseline of 7.6%,\nwithout use of provided annotations. \n\n"}
{"id": "1706.10067", "contents": "Title: semantify.it, a Platform for Creation, Publication and Distribution of\n  Semantic Annotations Abstract: The application of semantic technologies to content on the web is, in many\nregards, important and urgent. Search engines, chatbots, intelligent personal\nassistants and other technologies increasingly rely on content published as\nsemantic structured data. Yet, the process of creating this kind of data is\nstill complicated and widely unknown. The semantify.it platform implements an\napproach to solve three of the most challenging question regarding the\npublication of structured semantic data, namely: a) what vocabulary to use, b)\nhow to create annotation files and c) how to publish or integrate annotations\nwithin a website without programming. This paper presents the idea and the\ndevelopment of the semantify.it platform. It demonstrates that the creation\nprocess of semantically annotated data does not have to be hard, shows use\ncases and pilot users of the created software and presents where the\ndevelopment of this platform or alike projects lead to in the future. \n\n"}
{"id": "1707.00166", "contents": "Title: Heterogeneous Supervision for Relation Extraction: A Representation\n  Learning Approach Abstract: Relation extraction is a fundamental task in information extraction. Most\nexisting methods have heavy reliance on annotations labeled by human experts,\nwhich are costly and time-consuming. To overcome this drawback, we propose a\nnovel framework, REHession, to conduct relation extractor learning using\nannotations from heterogeneous information source, e.g., knowledge base and\ndomain heuristics. These annotations, referred as heterogeneous supervision,\noften conflict with each other, which brings a new challenge to the original\nrelation extraction task: how to infer the true label from noisy labels for a\ngiven instance. Identifying context information as the backbone of both\nrelation extraction and true label discovery, we adopt embedding techniques to\nlearn the distributed representations of context, which bridges all components\nwith mutual enhancement in an iterative fashion. Extensive experimental results\ndemonstrate the superiority of REHession over the state-of-the-art. \n\n"}
{"id": "1707.00206", "contents": "Title: Efficient Correlated Topic Modeling with Topic Embedding Abstract: Correlated topic modeling has been limited to small model and problem sizes\ndue to their high computational cost and poor scaling. In this paper, we\npropose a new model which learns compact topic embeddings and captures topic\ncorrelations through the closeness between the topic vectors. Our method\nenables efficient inference in the low-dimensional embedding space, reducing\nprevious cubic or quadratic time complexity to linear w.r.t the topic size. We\nfurther speedup variational inference with a fast sampler to exploit sparsity\nof topic occurrence. Extensive experiments show that our approach is capable of\nhandling model and data scales which are several orders of magnitude larger\nthan existing correlation results, without sacrificing modeling quality by\nproviding competitive or superior performance in document classification and\nretrieval. \n\n"}
{"id": "1707.00722", "contents": "Title: Improving LSTM-CTC based ASR performance in domains with limited\n  training data Abstract: This paper addresses the observed performance gap between automatic speech\nrecognition (ASR) systems based on Long Short Term Memory (LSTM) neural\nnetworks trained with the connectionist temporal classification (CTC) loss\nfunction and systems based on hybrid Deep Neural Networks (DNNs) trained with\nthe cross entropy (CE) loss function on domains with limited data. We step\nthrough a number of experiments that show incremental improvements on a\nbaseline EESEN toolkit based LSTM-CTC ASR system trained on the Librispeech\n100hr (train-clean-100) corpus. Our results show that with effective\ncombination of data augmentation and regularization, a LSTM-CTC based system\ncan exceed the performance of a strong Kaldi based baseline trained on the same\ndata. \n\n"}
{"id": "1707.00972", "contents": "Title: Automatic estimation of harmonic tension by distributed representation\n  of chords Abstract: The buildup and release of a sense of tension is one of the most essential\naspects of the process of listening to music. A veridical computational model\nof perceived musical tension would be an important ingredient for many music\ninformatics applications. The present paper presents a new approach to\nmodelling harmonic tension based on a distributed representation of chords. The\nstarting hypothesis is that harmonic tension as perceived by human listeners is\nrelated, among other things, to the expectedness of harmonic units (chords) in\ntheir local harmonic context. We train a word2vec-type neural network to learn\na vector space that captures contextual similarity and expectedness, and define\na quantitative measure of harmonic tension on top of this. To assess the\nveridicality of the model, we compare its outputs on a number of well-defined\nchord classes and cadential contexts to results from pertinent empirical\nstudies in music psychology. Statistical analysis shows that the model's\npredictions conform very well with empirical evidence obtained from human\nlisteners. \n\n"}
{"id": "1707.01830", "contents": "Title: Single-Queue Decoding for Neural Machine Translation Abstract: Neural machine translation models rely on the beam search algorithm for\ndecoding. In practice, we found that the quality of hypotheses in the search\nspace is negatively affected owing to the fixed beam size. To mitigate this\nproblem, we store all hypotheses in a single priority queue and use a universal\nscore function for hypothesis selection. The proposed algorithm is more\nflexible as the discarded hypotheses can be revisited in a later step. We\nfurther design a penalty function to punish the hypotheses that tend to produce\na final translation that is much longer or shorter than expected. Despite its\nsimplicity, we show that the proposed decoding algorithm is able to select\nhypotheses with better qualities and improve the translation performance. \n\n"}
{"id": "1707.01961", "contents": "Title: Long-Term Memory Networks for Question Answering Abstract: Question answering is an important and difficult task in the natural language\nprocessing domain, because many basic natural language processing tasks can be\ncast into a question answering task. Several deep neural network architectures\nhave been developed recently, which employ memory and inference components to\nmemorize and reason over text information, and generate answers to questions.\nHowever, a major drawback of many such models is that they are capable of only\ngenerating single-word answers. In addition, they require large amount of\ntraining data to generate accurate answers. In this paper, we introduce the\nLong-Term Memory Network (LTMN), which incorporates both an external memory\nmodule and a Long Short-Term Memory (LSTM) module to comprehend the input data\nand generate multi-word answers. The LTMN model can be trained end-to-end using\nback-propagation and requires minimal supervision. We test our model on two\nsynthetic data sets (based on Facebook's bAbI data set) and the real-world\nStanford question answering data set, and show that it can achieve\nstate-of-the-art performance. \n\n"}
{"id": "1707.03764", "contents": "Title: N-GrAM: New Groningen Author-profiling Model Abstract: We describe our participation in the PAN 2017 shared task on Author\nProfiling, identifying authors' gender and language variety for English,\nSpanish, Arabic and Portuguese. We describe both the final, submitted system,\nand a series of negative results. Our aim was to create a single model for both\ngender and language, and for all language varieties. Our best-performing system\n(on cross-validated results) is a linear support vector machine (SVM) with word\nunigrams and character 3- to 5-grams as features. A set of additional features,\nincluding POS tags, additional datasets, geographic entities, and Twitter\nhandles, hurt, rather than improve, performance. Results from cross-validation\nindicated high performance overall and results on the test set confirmed them,\nat 0.86 averaged accuracy, with performance on sub-tasks ranging from 0.68 to\n0.98. \n\n"}
{"id": "1707.03904", "contents": "Title: Quasar: Datasets for Question Answering by Search and Reading Abstract: We present two new large-scale datasets aimed at evaluating systems designed\nto comprehend a natural language query and extract its answer from a large\ncorpus of text. The Quasar-S dataset consists of 37000 cloze-style\n(fill-in-the-gap) queries constructed from definitions of software entity tags\non the popular website Stack Overflow. The posts and comments on the website\nserve as the background corpus for answering the cloze questions. The Quasar-T\ndataset consists of 43000 open-domain trivia questions and their answers\nobtained from various internet sources. ClueWeb09 serves as the background\ncorpus for extracting these answers. We pose these datasets as a challenge for\ntwo related subtasks of factoid Question Answering: (1) searching for relevant\npieces of text that include the correct answer to a query, and (2) reading the\nretrieved text to answer the query. We also describe a retrieval system for\nextracting relevant sentences and documents from the corpus given a query, and\ninclude these in the release for researchers wishing to only focus on (2). We\nevaluate several baselines on both datasets, ranging from simple heuristics to\npowerful neural models, and show that these lag behind human performance by\n16.4% and 32.1% for Quasar-S and -T respectively. The datasets are available at\nhttps://github.com/bdhingra/quasar . \n\n"}
{"id": "1707.04108", "contents": "Title: Do Convolutional Networks need to be Deep for Text Classification ? Abstract: We study in this work the importance of depth in convolutional models for\ntext classification, either when character or word inputs are considered. We\nshow on 5 standard text classification and sentiment analysis tasks that deep\nmodels indeed give better performances than shallow networks when the text\ninput is represented as a sequence of characters. However, a simple\nshallow-and-wide network outperforms deep models such as DenseNet with word\ninputs. Our shallow word model further establishes new state-of-the-art\nperformances on two datasets: Yelp Binary (95.9\\%) and Yelp Full (64.9\\%). \n\n"}
{"id": "1707.04678", "contents": "Title: Lyrics-Based Music Genre Classification Using a Hierarchical Attention\n  Network Abstract: Music genre classification, especially using lyrics alone, remains a\nchallenging topic in Music Information Retrieval. In this study we apply\nrecurrent neural network models to classify a large dataset of intact song\nlyrics. As lyrics exhibit a hierarchical layer structure - in which words\ncombine to form lines, lines form segments, and segments form a complete song -\nwe adapt a hierarchical attention network (HAN) to exploit these layers and in\naddition learn the importance of the words, lines, and segments. We test the\nmodel over a 117-genre dataset and a reduced 20-genre dataset. Experimental\nresults show that the HAN outperforms both non-neural models and simpler neural\nmodels, whilst also classifying over a higher number of genres than previous\nresearch. Through the learning process we can also visualise which words or\nlines in a song the model believes are important to classifying the genre. As a\nresult the HAN provides insights, from a computational perspective, into\nlyrical structure and language features that differentiate musical genres. \n\n"}
{"id": "1707.05114", "contents": "Title: Towards Bidirectional Hierarchical Representations for Attention-Based\n  Neural Machine Translation Abstract: This paper proposes a hierarchical attentional neural translation model which\nfocuses on enhancing source-side hierarchical representations by covering both\nlocal and global semantic information using a bidirectional tree-based encoder.\nTo maximize the predictive likelihood of target words, a weighted variant of an\nattention mechanism is used to balance the attentive information between\nlexical and phrase vectors. Using a tree-based rare word encoding, the proposed\nmodel is extended to sub-word level to alleviate the out-of-vocabulary (OOV)\nproblem. Empirical results reveal that the proposed model significantly\noutperforms sequence-to-sequence attention-based and tree-based neural\ntranslation models in English-Chinese translation tasks. \n\n"}
{"id": "1707.05116", "contents": "Title: To Normalize, or Not to Normalize: The Impact of Normalization on\n  Part-of-Speech Tagging Abstract: Does normalization help Part-of-Speech (POS) tagging accuracy on noisy,\nnon-canonical data? To the best of our knowledge, little is known on the actual\nimpact of normalization in a real-world scenario, where gold error detection is\nnot available. We investigate the effect of automatic normalization on POS\ntagging of tweets. We also compare normalization to strategies that leverage\nlarge amounts of unlabeled data kept in its raw form. Our results show that\nnormalization helps, but does not add consistently beyond just word embedding\nlayer initialization. The latter approach yields a tagging model that is\ncompetitive with a Twitter state-of-the-art tagger. \n\n"}
{"id": "1707.05853", "contents": "Title: Encoding Word Confusion Networks with Recurrent Neural Networks for\n  Dialog State Tracking Abstract: This paper presents our novel method to encode word confusion networks, which\ncan represent a rich hypothesis space of automatic speech recognition systems,\nvia recurrent neural networks. We demonstrate the utility of our approach for\nthe task of dialog state tracking in spoken dialog systems that relies on\nautomatic speech recognition output. Encoding confusion networks outperforms\nencoding the best hypothesis of the automatic speech recognition in a neural\nsystem for dialog state tracking on the well-known second Dialog State Tracking\nChallenge dataset. \n\n"}
{"id": "1707.06372", "contents": "Title: Learning to Rank Question Answer Pairs with Holographic Dual LSTM\n  Architecture Abstract: We describe a new deep learning architecture for learning to rank question\nanswer pairs. Our approach extends the long short-term memory (LSTM) network\nwith holographic composition to model the relationship between question and\nanswer representations. As opposed to the neural tensor layer that has been\nadopted recently, the holographic composition provides the benefits of scalable\nand rich representational learning approach without incurring huge parameter\ncosts. Overall, we present Holographic Dual LSTM (HD-LSTM), a unified\narchitecture for both deep sentence modeling and semantic matching.\nEssentially, our model is trained end-to-end whereby the parameters of the LSTM\nare optimized in a way that best explains the correlation between question and\nanswer representations. In addition, our proposed deep learning architecture\nrequires no extensive feature engineering. Via extensive experiments, we show\nthat HD-LSTM outperforms many other neural architectures on two popular\nbenchmark QA datasets. Empirical studies confirm the effectiveness of\nholographic composition over the neural tensor layer. \n\n"}
{"id": "1707.07191", "contents": "Title: MoodSwipe: A Soft Keyboard that Suggests Messages Based on\n  User-Specified Emotions Abstract: We present MoodSwipe, a soft keyboard that suggests text messages given the\nuser-specified emotions utilizing the real dialog data. The aim of MoodSwipe is\nto create a convenient user interface to enjoy the technology of emotion\nclassification and text suggestion, and at the same time to collect labeled\ndata automatically for developing more advanced technologies. While users\nselect the MoodSwipe keyboard, they can type as usual but sense the emotion\nconveyed by their text and receive suggestions for their message as a benefit.\nIn MoodSwipe, the detected emotions serve as the medium for suggested texts,\nwhere viewing the latter is the incentive to correcting the former. We conduct\nseveral experiments to show the superiority of the emotion classification\nmodels trained on the dialog data, and further to verify good emotion cues are\nimportant context for text suggestion. \n\n"}
{"id": "1707.07240", "contents": "Title: Language modeling with Neural trans-dimensional random fields Abstract: Trans-dimensional random field language models (TRF LMs) have recently been\nintroduced, where sentences are modeled as a collection of random fields. The\nTRF approach has been shown to have the advantages of being computationally\nmore efficient in inference than LSTM LMs with close performance and being able\nto flexibly integrating rich features. In this paper we propose neural TRFs,\nbeyond of the previous discrete TRFs that only use linear potentials with\ndiscrete features. The idea is to use nonlinear potentials with continuous\nfeatures, implemented by neural networks (NNs), in the TRF framework. Neural\nTRFs combine the advantages of both NNs and TRFs. The benefits of word\nembedding, nonlinear feature learning and larger context modeling are inherited\nfrom the use of NNs. At the same time, the strength of efficient inference by\navoiding expensive softmax is preserved. A number of technical contributions,\nincluding employing deep convolutional neural networks (CNNs) to define the\npotentials and incorporating the joint stochastic approximation (JSA) strategy\nin the training algorithm, are developed in this work, which enable us to\nsuccessfully train neural TRF LMs. Various LMs are evaluated in terms of speech\nrecognition WERs by rescoring the 1000-best lists of WSJ'92 test data. The\nresults show that neural TRF LMs not only improve over discrete TRF LMs, but\nalso perform slightly better than LSTM LMs with only one fifth of parameters\nand 16x faster inference efficiency. \n\n"}
{"id": "1707.07469", "contents": "Title: Character-level Intra Attention Network for Natural Language Inference Abstract: Natural language inference (NLI) is a central problem in language\nunderstanding. End-to-end artificial neural networks have reached\nstate-of-the-art performance in NLI field recently.\n  In this paper, we propose Character-level Intra Attention Network (CIAN) for\nthe NLI task. In our model, we use the character-level convolutional network to\nreplace the standard word embedding layer, and we use the intra attention to\ncapture the intra-sentence semantics. The proposed CIAN model provides improved\nresults based on a newly published MNLI corpus. \n\n"}
{"id": "1707.08052", "contents": "Title: Challenges in Data-to-Document Generation Abstract: Recent neural models have shown significant progress on the problem of\ngenerating short descriptive texts conditioned on a small number of database\nrecords. In this work, we suggest a slightly more difficult data-to-text\ngeneration task, and investigate how effective current approaches are on this\ntask. In particular, we introduce a new, large-scale corpus of data records\npaired with descriptive documents, propose a series of extractive evaluation\nmethods for analyzing performance, and obtain baseline results using current\nneural generation methods. Experiments show that these models produce fluent\ntext, but fail to convincingly approximate human-generated documents. Moreover,\neven templated baselines exceed the performance of these neural models on some\nmetrics, though copy- and reconstruction-based extensions lead to noticeable\nimprovements. \n\n"}
{"id": "1707.08309", "contents": "Title: Probabilistic Graphical Models for Credibility Analysis in Evolving\n  Online Communities Abstract: One of the major hurdles preventing the full exploitation of information from\nonline communities is the widespread concern regarding the quality and\ncredibility of user-contributed content. Prior works in this domain operate on\na static snapshot of the community, making strong assumptions about the\nstructure of the data (e.g., relational tables), or consider only shallow\nfeatures for text classification.\n  To address the above limitations, we propose probabilistic graphical models\nthat can leverage the joint interplay between multiple factors in online\ncommunities --- like user interactions, community dynamics, and textual content\n--- to automatically assess the credibility of user-contributed online content,\nand the expertise of users and their evolution with user-interpretable\nexplanation. To this end, we devise new models based on Conditional Random\nFields for different settings like incorporating partial expert knowledge for\nsemi-supervised learning, and handling discrete labels as well as numeric\nratings for fine-grained analysis. This enables applications such as extracting\nreliable side-effects of drugs from user-contributed posts in healthforums, and\nidentifying credible content in news communities.\n  Online communities are dynamic, as users join and leave, adapt to evolving\ntrends, and mature over time. To capture this dynamics, we propose generative\nmodels based on Hidden Markov Model, Latent Dirichlet Allocation, and Brownian\nMotion to trace the continuous evolution of user expertise and their language\nmodel over time. This allows us to identify expert users and credible content\njointly over time, improving state-of-the-art recommender systems by explicitly\nconsidering the maturity of users. This also enables applications such as\nidentifying helpful product reviews, and detecting fake and anomalous reviews\nwith limited information. \n\n"}
{"id": "1707.08588", "contents": "Title: Self-organized Hierarchical Softmax Abstract: We propose a new self-organizing hierarchical softmax formulation for\nneural-network-based language models over large vocabularies. Instead of using\na predefined hierarchical structure, our approach is capable of learning word\nclusters with clear syntactical and semantic meaning during the language model\ntraining process. We provide experiments on standard benchmarks for language\nmodeling and sentence compression tasks. We find that this approach is as fast\nas other efficient softmax approximations, while achieving comparable or even\nbetter performance relative to similar full softmax models. \n\n"}
{"id": "1708.00111", "contents": "Title: A Continuous Relaxation of Beam Search for End-to-end Training of Neural\n  Sequence Models Abstract: Beam search is a desirable choice of test-time decoding algorithm for neural\nsequence models because it potentially avoids search errors made by simpler\ngreedy methods. However, typical cross entropy training procedures for these\nmodels do not directly consider the behaviour of the final decoding method. As\na result, for cross-entropy trained models, beam decoding can sometimes yield\nreduced test performance when compared with greedy decoding. In order to train\nmodels that can more effectively make use of beam search, we propose a new\ntraining procedure that focuses on the final loss metric (e.g. Hamming loss)\nevaluated on the output of beam search. While well-defined, this \"direct loss\"\nobjective is itself discontinuous and thus difficult to optimize. Hence, in our\napproach, we form a sub-differentiable surrogate objective by introducing a\nnovel continuous approximation of the beam search decoding procedure. In\nexperiments, we show that optimizing this new training objective yields\nsubstantially better results on two sequence tasks (Named Entity Recognition\nand CCG Supertagging) when compared with both cross entropy trained greedy\ndecoding and cross entropy trained beam decoding baselines. \n\n"}
{"id": "1708.01759", "contents": "Title: Referenceless Quality Estimation for Natural Language Generation Abstract: Traditional automatic evaluation measures for natural language generation\n(NLG) use costly human-authored references to estimate the quality of a system\noutput. In this paper, we propose a referenceless quality estimation (QE)\napproach based on recurrent neural networks, which predicts a quality score for\na NLG system output by comparing it to the source meaning representation only.\nOur method outperforms traditional metrics and a constant baseline in most\nrespects; we also show that synthetic data helps to increase correlation\nresults by 21% compared to the base system. Our results are comparable to\nresults obtained in similar QE tasks despite the more challenging setting. \n\n"}
{"id": "1708.02099", "contents": "Title: Multimodal Classification for Analysing Social Media Abstract: Classification of social media data is an important approach in understanding\nuser behavior on the Web. Although information on social media can be of\ndifferent modalities such as texts, images, audio or videos, traditional\napproaches in classification usually leverage only one prominent modality.\nTechniques that are able to leverage multiple modalities are often complex and\nsusceptible to the absence of some modalities. In this paper, we present simple\nmodels that combine information from different modalities to classify social\nmedia content and are able to handle the above problems with existing\ntechniques. Our models combine information from different modalities using a\npooling layer and an auxiliary learning task is used to learn a common feature\nspace. We demonstrate the performance of our models and their robustness to the\nmissing of some modalities in the emotion classification domain. Our\napproaches, although being simple, can not only achieve significantly higher\naccuracies than traditional fusion approaches but also have comparable results\nwhen only one modality is available. \n\n"}
{"id": "1708.02182", "contents": "Title: Regularizing and Optimizing LSTM Language Models Abstract: Recurrent neural networks (RNNs), such as long short-term memory networks\n(LSTMs), serve as a fundamental building block for many sequence learning\ntasks, including machine translation, language modeling, and question\nanswering. In this paper, we consider the specific problem of word-level\nlanguage modeling and investigate strategies for regularizing and optimizing\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined using a non-monotonic condition as\nopposed to being tuned by the user. Using these and other regularization\nstrategies, we achieve state-of-the-art word level perplexities on two data\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\neffectiveness of a neural cache in conjunction with our proposed model, we\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\n52.0 on WikiText-2. \n\n"}
{"id": "1708.02182", "contents": "Title: Regularizing and Optimizing LSTM Language Models Abstract: Recurrent neural networks (RNNs), such as long short-term memory networks\n(LSTMs), serve as a fundamental building block for many sequence learning\ntasks, including machine translation, language modeling, and question\nanswering. In this paper, we consider the specific problem of word-level\nlanguage modeling and investigate strategies for regularizing and optimizing\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined using a non-monotonic condition as\nopposed to being tuned by the user. Using these and other regularization\nstrategies, we achieve state-of-the-art word level perplexities on two data\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\neffectiveness of a neural cache in conjunction with our proposed model, we\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\n52.0 on WikiText-2. \n\n"}
{"id": "1708.03181", "contents": "Title: Utilizing Embeddings for Ad-hoc Retrieval by Document-to-document\n  Similarity Abstract: Latent semantic representations of words or paragraphs, namely the\nembeddings, have been widely applied to information retrieval (IR). One of the\ncommon approaches of utilizing embeddings for IR is to estimate the\ndocument-to-query (D2Q) similarity in their embeddings. As words with similar\nsyntactic usage are usually very close to each other in the embeddings space,\nalthough they are not semantically similar, the D2Q similarity approach may\nsuffer from the problem of \"multiple degrees of similarity\". To this end, this\npaper proposes a novel approach that estimates a semantic relevance score (SEM)\nbased on document-to-document (D2D) similarity of embeddings. As Word or\nPara2Vec generates embeddings by the context of words/paragraphs, the D2D\nsimilarity approach turns the task of document ranking into the estimation of\nsimilarity between content within different documents. Experimental results on\nstandard TREC test collections show that our proposed approach outperforms\nstrong baselines. \n\n"}
{"id": "1708.03940", "contents": "Title: Leveraging Sparse and Dense Feature Combinations for Sentiment\n  Classification Abstract: Neural networks are one of the most popular approaches for many natural\nlanguage processing tasks such as sentiment analysis. They often outperform\ntraditional machine learning models and achieve the state-of-art results on\nmost tasks. However, many existing deep learning models are complex, difficult\nto train and provide a limited improvement over simpler methods. We propose a\nsimple, robust and powerful model for sentiment classification. This model\noutperforms many deep learning models and achieves comparable results to other\ndeep learning models with complex architectures on sentiment analysis datasets.\nWe publish the code online. \n\n"}
{"id": "1708.04439", "contents": "Title: Extractive Summarization using Deep Learning Abstract: This paper proposes a text summarization approach for factual reports using a\ndeep learning model. This approach consists of three phases: feature\nextraction, feature enhancement, and summary generation, which work together to\nassimilate core information and generate a coherent, understandable summary. We\nare exploring various features to improve the set of sentences selected for the\nsummary, and are using a Restricted Boltzmann Machine to enhance and abstract\nthose features to improve resultant accuracy without losing any important\ninformation. The sentences are scored based on those enhanced features and an\nextractive summary is constructed. Experimentation carried out on several\narticles demonstrates the effectiveness of the proposed approach. Source code\navailable at: https://github.com/vagisha-nidhi/TextSummarizer \n\n"}
{"id": "1708.04681", "contents": "Title: Identifying Harm Events in Clinical Care through Medical Narratives Abstract: Preventable medical errors are estimated to be among the leading causes of\ninjury and death in the United States. To prevent such errors, healthcare\nsystems have implemented patient safety and incident reporting systems. These\nsystems enable clinicians to report unsafe conditions and cases where patients\nhave been harmed due to errors in medical care. These reports are narratives in\nnatural language and while they provide detailed information about the\nsituation, it is non-trivial to perform large scale analysis for identifying\ncommon causes of errors and harm to the patients. In this work, we present a\nmethod based on attentive convolutional and recurrent networks for identifying\nharm events in patient care and categorize the harm based on its severity\nlevel. We demonstrate that our methods can significantly improve the\nperformance over existing methods in identifying harm in clinical care. \n\n"}
{"id": "1708.04729", "contents": "Title: Deconvolutional Paragraph Representation Learning Abstract: Learning latent representations from long text sequences is an important\nfirst step in many natural language processing applications. Recurrent Neural\nNetworks (RNNs) have become a cornerstone for this challenging task. However,\nthe quality of sentences during RNN-based decoding (reconstruction) decreases\nwith the length of the text. We propose a sequence-to-sequence, purely\nconvolutional and deconvolutional autoencoding framework that is free of the\nabove issue, while also being computationally efficient. The proposed method is\nsimple, easy to implement and can be leveraged as a building block for many\napplications. We show empirically that compared to RNNs, our framework is\nbetter at reconstructing and correcting long paragraphs. Quantitative\nevaluation on semi-supervised text classification and summarization tasks\ndemonstrate the potential for better utilization of long unlabeled text data. \n\n"}
{"id": "1708.04776", "contents": "Title: Modality-specific Cross-modal Similarity Measurement with Recurrent\n  Attention Network Abstract: Nowadays, cross-modal retrieval plays an indispensable role to flexibly find\ninformation across different modalities of data. Effectively measuring the\nsimilarity between different modalities of data is the key of cross-modal\nretrieval. Different modalities such as image and text have imbalanced and\ncomplementary relationships, which contain unequal amount of information when\ndescribing the same semantics. For example, images often contain more details\nthat cannot be demonstrated by textual descriptions and vice versa. Existing\nworks based on Deep Neural Network (DNN) mostly construct one common space for\ndifferent modalities to find the latent alignments between them, which lose\ntheir exclusive modality-specific characteristics. Different from the existing\nworks, we propose modality-specific cross-modal similarity measurement (MCSM)\napproach by constructing independent semantic space for each modality, which\nadopts end-to-end framework to directly generate modality-specific cross-modal\nsimilarity without explicit common representation. For each semantic space,\nmodality-specific characteristics within one modality are fully exploited by\nrecurrent attention network, while the data of another modality is projected\ninto this space with attention based joint embedding to utilize the learned\nattention weights for guiding the fine-grained cross-modal correlation\nlearning, which can capture the imbalanced and complementary relationships\nbetween different modalities. Finally, the complementarity between the semantic\nspaces for different modalities is explored by adaptive fusion of the\nmodality-specific cross-modal similarities to perform cross-modal retrieval.\nExperiments on the widely-used Wikipedia and Pascal Sentence datasets as well\nas our constructed large-scale XMediaNet dataset verify the effectiveness of\nour proposed approach, outperforming 9 state-of-the-art methods. \n\n"}
{"id": "1708.05449", "contents": "Title: An Annotated Corpus of Relational Strategies in Customer Service Abstract: We create and release the first publicly available commercial customer\nservice corpus with annotated relational segments. Human-computer data from\nthree live customer service Intelligent Virtual Agents (IVAs) in the domains of\ntravel and telecommunications were collected, and reviewers marked all text\nthat was deemed unnecessary to the determination of user intention. After\nmerging the selections of multiple reviewers to create highlighted texts, a\nsecond round of annotation was done to determine the classes of language\npresent in the highlighted sections such as the presence of Greetings,\nBackstory, Justification, Gratitude, Rants, or Emotions. This resulting corpus\nis a valuable resource for improving the quality and relational abilities of\nIVAs. As well as discussing the corpus itself, we compare the usage of such\nlanguage in human-human interactions on TripAdvisor forums. We show that\nremoval of this language from task-based inputs has a positive effect on IVA\nunderstanding by both an increase in confidence and improvement in responses,\ndemonstrating the need for automated methods of its discovery. \n\n"}
{"id": "1708.05482", "contents": "Title: A Question Answering Approach to Emotion Cause Extraction Abstract: Emotion cause extraction aims to identify the reasons behind a certain\nemotion expressed in text. It is a much more difficult task compared to emotion\nclassification. Inspired by recent advances in using deep memory networks for\nquestion answering (QA), we propose a new approach which considers emotion\ncause identification as a reading comprehension task in QA. Inspired by\nconvolutional neural networks, we propose a new mechanism to store relevant\ncontext in different memory slots to model context information. Our proposed\napproach can extract both word level sequence features and lexical features.\nPerformance evaluation shows that our method achieves the state-of-the-art\nperformance on a recently released emotion cause dataset, outperforming a\nnumber of competitive baselines by at least 3.01% in F-measure. \n\n"}
{"id": "1708.05592", "contents": "Title: Future Word Contexts in Neural Network Language Models Abstract: Recently, bidirectional recurrent network language models (bi-RNNLMs) have\nbeen shown to outperform standard, unidirectional, recurrent neural network\nlanguage models (uni-RNNLMs) on a range of speech recognition tasks. This\nindicates that future word context information beyond the word history can be\nuseful. However, bi-RNNLMs pose a number of challenges as they make use of the\ncomplete previous and future word context information. This impacts both\ntraining efficiency and their use within a lattice rescoring framework. In this\npaper these issues are addressed by proposing a novel neural network structure,\nsucceeding word RNNLMs (su-RNNLMs). Instead of using a recurrent unit to\ncapture the complete future word contexts, a feedforward unit is used to model\na finite number of succeeding, future, words. This model can be trained much\nmore efficiently than bi-RNNLMs and can also be used for lattice rescoring.\nExperimental results on a meeting transcription task (AMI) show the proposed\nmodel consistently outperformed uni-RNNLMs and yield only a slight degradation\ncompared to bi-RNNLMs in N-best rescoring. Additionally, performance\nimprovements can be obtained using lattice rescoring and subsequent confusion\nnetwork decoding. \n\n"}
{"id": "1708.05729", "contents": "Title: Neural machine translation for low-resource languages Abstract: Neural machine translation (NMT) approaches have improved the state of the\nart in many machine translation settings over the last couple of years, but\nthey require large amounts of training data to produce sensible output. We\ndemonstrate that NMT can be used for low-resource languages as well, by\nintroducing more local dependencies and using word alignments to learn sentence\nreordering during translation. In addition to our novel model, we also present\nan empirical evaluation of low-resource phrase-based statistical machine\ntranslation (SMT) and NMT to investigate the lower limits of the respective\ntechnologies. We find that while SMT remains the best option for low-resource\nsettings, our method can produce acceptable translations with only 70000 tokens\nof training data, a level where the baseline NMT system fails completely. \n\n"}
{"id": "1708.07104", "contents": "Title: Automatic Detection of Fake News Abstract: The proliferation of misleading information in everyday access media outlets\nsuch as social media feeds, news blogs, and online newspapers have made it\nchallenging to identify trustworthy news sources, thus increasing the need for\ncomputational tools able to provide insights into the reliability of online\ncontent. In this paper, we focus on the automatic identification of fake\ncontent in online news. Our contribution is twofold. First, we introduce two\nnovel datasets for the task of fake news detection, covering seven different\nnews domains. We describe the collection, annotation, and validation process in\ndetail and present several exploratory analysis on the identification of\nlinguistic differences in fake and legitimate news content. Second, we conduct\na set of learning experiments to build accurate fake news detectors. In\naddition, we provide comparative analyses of the automatic and manual\nidentification of fake news. \n\n"}
{"id": "1708.08123", "contents": "Title: Impact of Feature Selection on Micro-Text Classification Abstract: Social media datasets, especially Twitter tweets, are popular in the field of\ntext classification. Tweets are a valuable source of micro-text (sometimes\nreferred to as \"micro-blogs\"), and have been studied in domains such as\nsentiment analysis, recommendation systems, spam detection, clustering, among\nothers. Tweets often include keywords referred to as \"Hashtags\" that can be\nused as labels for the tweet. Using tweets encompassing 50 labels, we studied\nthe impact of word versus character-level feature selection and extraction on\ndifferent learners to solve a multi-class classification task. We show that\nfeature extraction of simple character-level groups performs better than simple\nword groups and pre-processing methods like normalizing using Porter's Stemming\nand Part-of-Speech (\"POS\")-Lemmatization. \n\n"}
{"id": "1708.09025", "contents": "Title: Unsupervised Terminological Ontology Learning based on Hierarchical\n  Topic Modeling Abstract: In this paper, we present hierarchical relationbased latent Dirichlet\nallocation (hrLDA), a data-driven hierarchical topic model for extracting\nterminological ontologies from a large number of heterogeneous documents. In\ncontrast to traditional topic models, hrLDA relies on noun phrases instead of\nunigrams, considers syntax and document structures, and enriches topic\nhierarchies with topic relations. Through a series of experiments, we\ndemonstrate the superiority of hrLDA over existing topic models, especially for\nbuilding hierarchies. Furthermore, we illustrate the robustness of hrLDA in the\nsettings of noisy data sets, which are likely to occur in many practical\nscenarios. Our ontology evaluation results show that ontologies extracted from\nhrLDA are very competitive with the ontologies created by domain experts. \n\n"}
{"id": "1709.00149", "contents": "Title: Learning what to read: Focused machine reading Abstract: Recent efforts in bioinformatics have achieved tremendous progress in the\nmachine reading of biomedical literature, and the assembly of the extracted\nbiochemical interactions into large-scale models such as protein signaling\npathways. However, batch machine reading of literature at today's scale (PubMed\nalone indexes over 1 million papers per year) is unfeasible due to both cost\nand processing overhead. In this work, we introduce a focused reading approach\nto guide the machine reading of biomedical literature towards what literature\nshould be read to answer a biomedical query as efficiently as possible. We\nintroduce a family of algorithms for focused reading, including an intuitive,\nstrong baseline, and a second approach which uses a reinforcement learning (RL)\nframework that learns when to explore (widen the search) or exploit (narrow\nit). We demonstrate that the RL approach is capable of answering more queries\nthan the baseline, while being more efficient, i.e., reading fewer documents. \n\n"}
{"id": "1709.00354", "contents": "Title: Query-by-example Spoken Term Detection using Attention-based Multi-hop\n  Networks Abstract: Retrieving spoken content with spoken queries, or query-by- example spoken\nterm detection (STD), is attractive because it makes possible the matching of\nsignals directly on the acoustic level without transcribing them into text.\nHere, we propose an end-to-end query-by-example STD model based on an\nattention-based multi-hop network, whose input is a spoken query and an audio\nsegment containing several utterances; the output states whether the audio\nsegment includes the query. The model can be trained in either a supervised\nscenario using labeled data, or in an unsupervised fashion. In the supervised\nscenario, we find that the attention mechanism and multiple hops improve\nperformance, and that the attention weights indicate the time span of the\ndetected terms. In the unsupervised setting, the model mimics the behavior of\nthe existing query-by-example STD system, yielding performance comparable to\nthe existing system but with a lower search time complexity. \n\n"}
{"id": "1709.00387", "contents": "Title: MIT-QCRI Arabic Dialect Identification System for the 2017 Multi-Genre\n  Broadcast Challenge Abstract: In order to successfully annotate the Arabic speech con- tent found in\nopen-domain media broadcasts, it is essential to be able to process a diverse\nset of Arabic dialects. For the 2017 Multi-Genre Broadcast challenge (MGB-3)\nthere were two possible tasks: Arabic speech recognition, and Arabic Dialect\nIdentification (ADI). In this paper, we describe our efforts to create an ADI\nsystem for the MGB-3 challenge, with the goal of distinguishing amongst four\nmajor Arabic dialects, as well as Modern Standard Arabic. Our research fo-\ncused on dialect variability and domain mismatches between the training and\ntest domain. In order to achieve a robust ADI system, we explored both Siamese\nneural network models to learn similarity and dissimilarities among Arabic\ndialects, as well as i-vector post-processing to adapt domain mismatches. Both\nAcoustic and linguistic features were used for the final MGB-3 submissions,\nwith the best primary system achieving 75% accuracy on the official 10hr test\nset. \n\n"}
{"id": "1709.00770", "contents": "Title: Understanding the Logical and Semantic Structure of Large Documents Abstract: Current language understanding approaches focus on small documents, such as\nnewswire articles, blog posts, product reviews and discussion forum entries.\nUnderstanding and extracting information from large documents like legal\nbriefs, proposals, technical manuals and research articles is still a\nchallenging task. We describe a framework that can analyze a large document and\nhelp people to know where a particular information is in that document. We aim\nto automatically identify and classify semantic sections of documents and\nassign consistent and human-understandable labels to similar sections across\ndocuments. A key contribution of our research is modeling the logical and\nsemantic structure of an electronic document. We apply machine learning\ntechniques, including deep learning, in our prototype system. We also make\navailable a dataset of information about a collection of scholarly articles\nfrom the arXiv eprints collection that includes a wide range of metadata for\neach article, including a table of contents, section labels, section\nsummarizations and more. We hope that this dataset will be a useful resource\nfor the machine learning and NLP communities in information retrieval,\ncontent-based question answering and language modeling. \n\n"}
{"id": "1709.01888", "contents": "Title: Language Modeling by Clustering with Word Embeddings for Text\n  Readability Assessment Abstract: We present a clustering-based language model using word embeddings for text\nreadability prediction. Presumably, an Euclidean semantic space hypothesis\nholds true for word embeddings whose training is done by observing word\nco-occurrences. We argue that clustering with word embeddings in the metric\nspace should yield feature representations in a higher semantic space\nappropriate for text regression. Also, by representing features in terms of\nhistograms, our approach can naturally address documents of varying lengths. An\nempirical evaluation using the Common Core Standards corpus reveals that the\nfeatures formed on our clustering-based language model significantly improve\nthe previously known results for the same corpus in readability prediction. We\nalso evaluate the task of sentence matching based on semantic relatedness using\nthe Wiki-SimpleWiki corpus and find that our features lead to superior matching\nperformance. \n\n"}
{"id": "1709.02448", "contents": "Title: Network Vector: Distributed Representations of Networks with Global\n  Context Abstract: We propose a neural embedding algorithm called Network Vector, which learns\ndistributed representations of nodes and the entire networks simultaneously. By\nembedding networks in a low-dimensional space, the algorithm allows us to\ncompare networks in terms of structural similarity and to solve outstanding\npredictive problems. Unlike alternative approaches that focus on node level\nfeatures, we learn a continuous global vector that captures each node's global\ncontext by maximizing the predictive likelihood of random walk paths in the\nnetwork. Our algorithm is scalable to real world graphs with many nodes. We\nevaluate our algorithm on datasets from diverse domains, and compare it with\nstate-of-the-art techniques in node classification, role discovery and concept\nanalogy tasks. The empirical results show the effectiveness and the efficiency\nof our algorithm. \n\n"}
{"id": "1709.03545", "contents": "Title: Learning Graph Topological Features via GAN Abstract: Inspired by the generation power of generative adversarial networks (GANs) in\nimage domains, we introduce a novel hierarchical architecture for learning\ncharacteristic topological features from a single arbitrary input graph via\nGANs. The hierarchical architecture consisting of multiple GANs preserves both\nlocal and global topological features and automatically partitions the input\ngraph into representative stages for feature learning. The stages facilitate\nreconstruction and can be used as indicators of the importance of the\nassociated topological structures. Experiments show that our method produces\nsubgraphs retaining a wide range of topological features, even in early\nreconstruction stages (unlike a single GAN, which cannot easily identify such\nfeatures, let alone reconstruct the original graph). This paper is firstline\nresearch on combining the use of GANs and graph topological analysis. \n\n"}
{"id": "1709.04402", "contents": "Title: On Early-stage Debunking Rumors on Twitter: Leveraging the Wisdom of\n  Weak Learners Abstract: Recently a lot of progress has been made in rumor modeling and rumor\ndetection for micro-blogging streams. However, existing automated methods do\nnot perform very well for early rumor detection, which is crucial in many\nsettings, e.g., in crisis situations. One reason for this is that aggregated\nrumor features such as propagation features, which work well on the long run,\nare - due to their accumulating characteristic - not very helpful in the early\nphase of a rumor. In this work, we present an approach for early rumor\ndetection, which leverages Convolutional Neural Networks for learning the\nhidden representations of individual rumor-related tweets to gain insights on\nthe credibility of each tweets. We then aggregate the predictions from the very\nbeginning of a rumor to obtain the overall event credits (so-called wisdom),\nand finally combine it with a time series based rumor classification model. Our\nextensive experiments show a clearly improved classification performance within\nthe critical very first hours of a rumor. For a better understanding, we also\nconduct an extensive feature evaluation that emphasized on the early stage and\nshows that the low-level credibility has best predictability at all phases of\nthe rumor lifetime. \n\n"}
{"id": "1709.04969", "contents": "Title: Cross-Platform Emoji Interpretation: Analysis, a Solution, and\n  Applications Abstract: Most social media platforms are largely based on text, and users often write\nposts to describe where they are, what they are seeing, and how they are\nfeeling. Because written text lacks the emotional cues of spoken and\nface-to-face dialogue, ambiguities are common in written language. This problem\nis exacerbated in the short, informal nature of many social media posts. To\nbypass this issue, a suite of special characters called \"emojis,\" which are\nsmall pictograms, are embedded within the text. Many emojis are small\ndepictions of facial expressions designed to help disambiguate the emotional\nmeaning of the text. However, a new ambiguity arises in the way that emojis are\nrendered. Every platform (Windows, Mac, and Android, to name a few) renders\nemojis according to their own style. In fact, it has been shown that some\nemojis can be rendered so differently that they look \"happy\" on some platforms,\nand \"sad\" on others. In this work, we use real-world data to verify the\nexistence of this problem. We verify that the usage of the same emoji can be\nsignificantly different across platforms, with some emojis exhibiting different\nsentiment polarities on different platforms. We propose a solution to identify\nthe intended emoji based on the platform-specific nature of the emoji used by\nthe author of a social media post. We apply our solution to sentiment analysis,\na task that can benefit from the emoji calibration technique we use in this\nwork. We conduct experiments to evaluate the effectiveness of the mapping in\nthis task. \n\n"}
{"id": "1709.05584", "contents": "Title: Representation Learning on Graphs: Methods and Applications Abstract: Machine learning on graphs is an important and ubiquitous task with\napplications ranging from drug design to friendship recommendation in social\nnetworks. The primary challenge in this domain is finding a way to represent,\nor encode, graph structure so that it can be easily exploited by machine\nlearning models. Traditionally, machine learning approaches relied on\nuser-defined heuristics to extract features encoding structural information\nabout a graph (e.g., degree statistics or kernel functions). However, recent\nyears have seen a surge in approaches that automatically learn to encode graph\nstructure into low-dimensional embeddings, using techniques based on deep\nlearning and nonlinear dimensionality reduction. Here we provide a conceptual\nreview of key advancements in this area of representation learning on graphs,\nincluding matrix factorization-based methods, random-walk based algorithms, and\ngraph neural networks. We review methods to embed individual nodes as well as\napproaches to embed entire (sub)graphs. In doing so, we develop a unified\nframework to describe these recent approaches, and we highlight a number of\nimportant applications and directions for future work. \n\n"}
{"id": "1709.05749", "contents": "Title: Anticipating Information Needs Based on Check-in Activity Abstract: In this work we address the development of a smart personal assistant that is\ncapable of anticipating a user's information needs based on a novel type of\ncontext: the person's activity inferred from her check-in records on a\nlocation-based social network. Our main contribution is a method that\ntranslates a check-in activity into an information need, which is in turn\naddressed with an appropriate information card. This task is challenging\nbecause of the large number of possible activities and related information\nneeds, which need to be addressed in a mobile dashboard that is limited in\nsize. Our approach considers each possible activity that might follow after the\nlast (and already finished) activity, and selects the top information cards\nsuch that they maximize the likelihood of satisfying the user's information\nneeds for all possible future scenarios. The proposed models also incorporate\nknowledge about the temporal dynamics of information needs. Using a combination\nof historical check-in data and manual assessments collected via crowdsourcing,\nwe show experimentally the effectiveness of our approach. \n\n"}
{"id": "1709.05778", "contents": "Title: Word Vector Enrichment of Low Frequency Words in the Bag-of-Words Model\n  for Short Text Multi-class Classification Problems Abstract: The bag-of-words model is a standard representation of text for many linear\nclassifier learners. In many problem domains, linear classifiers are preferred\nover more complex models due to their efficiency, robustness and\ninterpretability, and the bag-of-words text representation can capture\nsufficient information for linear classifiers to make highly accurate\npredictions. However in settings where there is a large vocabulary, large\nvariance in the frequency of terms in the training corpus, many classes and\nvery short text (e.g., single sentences or document titles) the bag-of-words\nrepresentation becomes extremely sparse, and this can reduce the accuracy of\nclassifiers. A particular issue in such settings is that short texts tend to\ncontain infrequently occurring or rare terms which lack class-conditional\nevidence. In this work we introduce a method for enriching the bag-of-words\nmodel by complementing such rare term information with related terms from both\ngeneral and domain-specific Word Vector models. By reducing sparseness in the\nbag-of-words models, our enrichment approach achieves improved classification\nover several baseline classifiers in a variety of text classification problems.\nOur approach is also efficient because it requires no change to the linear\nclassifier before or during training, since bag-of-words enrichment applies\nonly to text being classified. \n\n"}
{"id": "1709.06636", "contents": "Title: An Attention-based Collaboration Framework for Multi-View Network\n  Representation Learning Abstract: Learning distributed node representations in networks has been attracting\nincreasing attention recently due to its effectiveness in a variety of\napplications. Existing approaches usually study networks with a single type of\nproximity between nodes, which defines a single view of a network. However, in\nreality there usually exists multiple types of proximities between nodes,\nyielding networks with multiple views. This paper studies learning node\nrepresentations for networks with multiple views, which aims to infer robust\nnode representations across different views. We propose a multi-view\nrepresentation learning approach, which promotes the collaboration of different\nviews and lets them vote for the robust representations. During the voting\nprocess, an attention mechanism is introduced, which enables each node to focus\non the most informative views. Experimental results on real-world networks show\nthat the proposed approach outperforms existing state-of-the-art approaches for\nnetwork representation learning with a single view and other competitive\napproaches with multiple views. \n\n"}
{"id": "1709.07109", "contents": "Title: Deconvolutional Latent-Variable Model for Text Sequence Matching Abstract: A latent-variable model is introduced for text matching, inferring sentence\nrepresentations by jointly optimizing generative and discriminative objectives.\nTo alleviate typical optimization challenges in latent-variable models for\ntext, we employ deconvolutional networks as the sequence decoder (generator),\nproviding learned latent codes with more semantic information and better\ngeneralization. Our model, trained in an unsupervised manner, yields stronger\nempirical predictive performance than a decoder based on Long Short-Term Memory\n(LSTM), with less parameters and considerably faster training. Further, we\napply it to text sequence-matching problems. The proposed model significantly\noutperforms several strong sentence-encoding baselines, especially in the\nsemi-supervised setting. \n\n"}
{"id": "1709.07432", "contents": "Title: Dynamic Evaluation of Neural Sequence Models Abstract: We present methodology for using dynamic evaluation to improve neural\nsequence models. Models are adapted to recent history via a gradient descent\nbased mechanism, causing them to assign higher probabilities to re-occurring\nsequential patterns. Dynamic evaluation outperforms existing adaptation\napproaches in our comparisons. Dynamic evaluation improves the state-of-the-art\nword-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1\nand 44.3 respectively, and the state-of-the-art character-level cross-entropies\non the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char\nrespectively. \n\n"}
{"id": "1709.07902", "contents": "Title: Unsupervised Learning of Disentangled and Interpretable Representations\n  from Sequential Data Abstract: We present a factorized hierarchical variational autoencoder, which learns\ndisentangled and interpretable representations from sequential data without\nsupervision. Specifically, we exploit the multi-scale nature of information in\nsequential data by formulating it explicitly within a factorized hierarchical\ngraphical model that imposes sequence-dependent priors and sequence-independent\npriors to different sets of latent variables. The model is evaluated on two\nspeech corpora to demonstrate, qualitatively, its ability to transform speakers\nor linguistic content by manipulating different sets of latent variables; and\nquantitatively, its ability to outperform an i-vector baseline for speaker\nverification and reduce the word error rate by as much as 35% in mismatched\ntrain/test scenarios for automatic speech recognition tasks. \n\n"}
{"id": "1709.09118", "contents": "Title: Tensor Product Generation Networks for Deep NLP Modeling Abstract: We present a new approach to the design of deep networks for natural language\nprocessing (NLP), based on the general technique of Tensor Product\nRepresentations (TPRs) for encoding and processing symbol structures in\ndistributed neural networks. A network architecture --- the Tensor Product\nGeneration Network (TPGN) --- is proposed which is capable in principle of\ncarrying out TPR computation, but which uses unconstrained deep learning to\ndesign its internal representations. Instantiated in a model for image-caption\ngeneration, TPGN outperforms LSTM baselines when evaluated on the COCO dataset.\nThe TPR-capable structure enables interpretation of internal representations\nand operations, which prove to contain considerable grammatical content. Our\ncaption-generation model can be interpreted as generating sequences of\ngrammatical categories and retrieving words by their categories from a plan\nencoded as a distributed representation. \n\n"}
{"id": "1710.00519", "contents": "Title: Attentive Convolution: Equipping CNNs with RNN-style Attention\n  Mechanisms Abstract: In NLP, convolutional neural networks (CNNs) have benefited less than\nrecurrent neural networks (RNNs) from attention mechanisms. We hypothesize that\nthis is because the attention in CNNs has been mainly implemented as attentive\npooling (i.e., it is applied to pooling) rather than as attentive convolution\n(i.e., it is integrated into convolution). Convolution is the differentiator of\nCNNs in that it can powerfully model the higher-level representation of a word\nby taking into account its local fixed-size context in the input text t^x. In\nthis work, we propose an attentive convolution network, ATTCONV. It extends the\ncontext scope of the convolution operation, deriving higher-level features for\na word not only from local context, but also information extracted from\nnonlocal context by the attention mechanism commonly used in RNNs. This\nnonlocal context can come (i) from parts of the input text t^x that are distant\nor (ii) from extra (i.e., external) contexts t^y. Experiments on sentence\nmodeling with zero-context (sentiment analysis), single-context (textual\nentailment) and multiple-context (claim verification) demonstrate the\neffectiveness of ATTCONV in sentence representation learning with the\nincorporation of context. In particular, attentive convolution outperforms\nattentive pooling and is a strong competitor to popular attentive RNNs. \n\n"}
{"id": "1710.02365", "contents": "Title: Czech Text Document Corpus v 2.0 Abstract: This paper introduces \"Czech Text Document Corpus v 2.0\", a collection of\ntext documents for automatic document classification in Czech language. It is\ncomposed of the text documents provided by the Czech News Agency and is freely\navailable for research purposes at http://ctdc.kiv.zcu.cz/. This corpus was\ncreated in order to facilitate a straightforward comparison of the document\nclassification approaches on Czech data. It is particularly dedicated to\nevaluation of multi-label document classification approaches, because one\ndocument is usually labelled with more than one label. Besides the information\nabout the document classes, the corpus is also annotated at the morphological\nlayer. This paper further shows the results of selected state-of-the-art\nmethods on this corpus to offer the possibility of an easy comparison with\nthese approaches. \n\n"}
{"id": "1710.02569", "contents": "Title: Low-resource bilingual lexicon extraction using graph based word\n  embeddings Abstract: In this work we focus on the task of automatically extracting bilingual\nlexicon for the language pair Spanish-Nahuatl. This is a low-resource setting\nwhere only a small amount of parallel corpus is available. Most of the\ndownstream methods do not work well under low-resources conditions. This is\nspecially true for the approaches that use vectorial representations like\nWord2Vec. Our proposal is to construct bilingual word vectors from a graph.\nThis graph is generated using translation pairs obtained from an unsupervised\nword alignment method.\n  We show that, in a low-resource setting, these type of vectors are successful\nin representing words in a bilingual semantic space. Moreover, when a linear\ntransformation is applied to translate words from one language to another, our\ngraph based representations considerably outperform the popular setting that\nuses Word2Vec. \n\n"}
{"id": "1710.02971", "contents": "Title: Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE,\n  and node2vec Abstract: Since the invention of word2vec, the skip-gram model has significantly\nadvanced the research of network embedding, such as the recent emergence of the\nDeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of\nthe aforementioned models with negative sampling can be unified into the matrix\nfactorization framework with closed forms. Our analysis and proofs reveal that:\n(1) DeepWalk empirically produces a low-rank transformation of a network's\nnormalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk\nwhen the size of vertices' context is set to one; (3) As an extension of LINE,\nPTE can be viewed as the joint factorization of multiple networks' Laplacians;\n(4) node2vec is factorizing a matrix related to the stationary distribution and\ntransition probability tensor of a 2nd-order random walk. We further provide\nthe theoretical connections between skip-gram based network embedding\nalgorithms and the theory of graph Laplacian. Finally, we present the NetMF\nmethod as well as its approximation algorithm for computing network embedding.\nOur method offers significant improvements over DeepWalk and LINE for\nconventional network mining tasks. This work lays the theoretical foundation\nfor skip-gram based network embedding methods, leading to a better\nunderstanding of latent network representation learning. \n\n"}
{"id": "1710.02971", "contents": "Title: Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE,\n  and node2vec Abstract: Since the invention of word2vec, the skip-gram model has significantly\nadvanced the research of network embedding, such as the recent emergence of the\nDeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of\nthe aforementioned models with negative sampling can be unified into the matrix\nfactorization framework with closed forms. Our analysis and proofs reveal that:\n(1) DeepWalk empirically produces a low-rank transformation of a network's\nnormalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk\nwhen the size of vertices' context is set to one; (3) As an extension of LINE,\nPTE can be viewed as the joint factorization of multiple networks' Laplacians;\n(4) node2vec is factorizing a matrix related to the stationary distribution and\ntransition probability tensor of a 2nd-order random walk. We further provide\nthe theoretical connections between skip-gram based network embedding\nalgorithms and the theory of graph Laplacian. Finally, we present the NetMF\nmethod as well as its approximation algorithm for computing network embedding.\nOur method offers significant improvements over DeepWalk and LINE for\nconventional network mining tasks. This work lays the theoretical foundation\nfor skip-gram based network embedding methods, leading to a better\nunderstanding of latent network representation learning. \n\n"}
{"id": "1710.04203", "contents": "Title: Crowdsourcing for Beyond Polarity Sentiment Analysis A Pure Emotion\n  Lexicon Abstract: Sentiment analysis aims to uncover emotions conveyed through information. In\nits simplest form, it is performed on a polarity basis, where the goal is to\nclassify information with positive or negative emotion. Recent research has\nexplored more nuanced ways to capture emotions that go beyond polarity. For\nthese methods to work, they require a critical resource: a lexicon that is\nappropriate for the task at hand, in terms of the range of emotions it captures\ndiversity. In the past, sentiment analysis lexicons have been created by\nexperts, such as linguists and behavioural scientists, with strict rules.\nLexicon evaluation was also performed by experts or gold standards. In our\npaper, we propose a crowdsourcing method for lexicon acquisition, which is\nscalable, cost-effective, and doesn't require experts or gold standards. We\nalso compare crowd and expert evaluations of the lexicon, to assess the overall\nlexicon quality, and the evaluation capabilities of the crowd. \n\n"}
{"id": "1710.05298", "contents": "Title: Text2Action: Generative Adversarial Synthesis from Language to Action Abstract: In this paper, we propose a generative model which learns the relationship\nbetween language and human action in order to generate a human action sequence\ngiven a sentence describing human behavior. The proposed generative model is a\ngenerative adversarial network (GAN), which is based on the sequence to\nsequence (SEQ2SEQ) model. Using the proposed generative network, we can\nsynthesize various actions for a robot or a virtual agent using a text encoder\nrecurrent neural network (RNN) and an action decoder RNN. The proposed\ngenerative network is trained from 29,770 pairs of actions and sentence\nannotations extracted from MSR-Video-to-Text (MSR-VTT), a large-scale video\ndataset. We demonstrate that the network can generate human-like actions which\ncan be transferred to a Baxter robot, such that the robot performs an action\nbased on a provided sentence. Results show that the proposed generative network\ncorrectly models the relationship between language and action and can generate\na diverse set of actions from the same sentence. \n\n"}
{"id": "1710.06390", "contents": "Title: Fishing for Clickbaits in Social Images and Texts with\n  Linguistically-Infused Neural Network Models Abstract: This paper presents the results and conclusions of our participation in the\nClickbait Challenge 2017 on automatic clickbait detection in social media. We\nfirst describe linguistically-infused neural network models and identify\ninformative representations to predict the level of clickbaiting present in\nTwitter posts. Our models allow to answer the question not only whether a post\nis a clickbait or not, but to what extent it is a clickbait post e.g., not at\nall, slightly, considerably, or heavily clickbaity using a score ranging from 0\nto 1. We evaluate the predictive power of models trained on varied text and\nimage representations extracted from tweets. Our best performing model that\nrelies on the tweet text and linguistic markers of biased language extracted\nfrom the tweet and the corresponding page yields mean squared error (MSE) of\n0.04, mean absolute error (MAE) of 0.16 and R2 of 0.43 on the held-out test\ndata. For the binary classification setup (clickbait vs. non-clickbait), our\nmodel achieved F1 score of 0.69. We have not found that image representations\ncombined with text yield significant performance improvement yet. Nevertheless,\nthis work is the first to present preliminary analysis of objects extracted\nusing Google Tensorflow object detection API from images in clickbait vs.\nnon-clickbait Twitter posts. Finally, we outline several steps to improve model\nperformance as a part of the future work. \n\n"}
{"id": "1710.06520", "contents": "Title: LASAGNE: Locality And Structure Aware Graph Node Embedding Abstract: In this work we propose Lasagne, a methodology to learn locality and\nstructure aware graph node embeddings in an unsupervised way. In particular, we\nshow that the performance of existing random-walk based approaches depends\nstrongly on the structural properties of the graph, e.g., the size of the\ngraph, whether the graph has a flat or upward-sloping Network Community Profile\n(NCP), whether the graph is expander-like, whether the classes of interest are\nmore k-core-like or more peripheral, etc. For larger graphs with flat NCPs that\nare strongly expander-like, existing methods lead to random walks that expand\nrapidly, touching many dissimilar nodes, thereby leading to lower-quality\nvector representations that are less useful for downstream tasks. Rather than\nrelying on global random walks or neighbors within fixed hop distances, Lasagne\nexploits strongly local Approximate Personalized PageRank stationary\ndistributions to more precisely engineer local information into node\nembeddings. This leads, in particular, to more meaningful and more useful\nvector representations of nodes in poorly-structured graphs. We show that\nLasagne leads to significant improvement in downstream multi-label\nclassification for larger graphs with flat NCPs, that it is comparable for\nsmaller graphs with upward-sloping NCPs, and that is comparable to existing\nmethods for link prediction tasks. \n\n"}
{"id": "1710.09137", "contents": "Title: Linking Tweets with Monolingual and Cross-Lingual News using Transformed\n  Word Embeddings Abstract: Social media platforms have grown into an important medium to spread\ninformation about an event published by the traditional media, such as news\narticles. Grouping such diverse sources of information that discuss the same\ntopic in varied perspectives provide new insights. But the gap in word usage\nbetween informal social media content such as tweets and diligently written\ncontent (e.g. news articles) make such assembling difficult. In this paper, we\npropose a transformation framework to bridge the word usage gap between tweets\nand online news articles across languages by leveraging their word embeddings.\nUsing our framework, word embeddings extracted from tweets and news articles\nare aligned closer to each other across languages, thus facilitating the\nidentification of similarity between news articles and tweets. Experimental\nresults show a notable improvement over baselines for monolingual tweets and\nnews articles comparison, while new findings are reported for cross-lingual\ncomparison. \n\n"}
{"id": "1710.09599", "contents": "Title: Watch Your Step: Learning Node Embeddings via Graph Attention Abstract: Graph embedding methods represent nodes in a continuous vector space,\npreserving information from the graph (e.g. by sampling random walks). There\nare many hyper-parameters to these methods (such as random walk length) which\nhave to be manually tuned for every graph. In this paper, we replace random\nwalk hyper-parameters with trainable parameters that we automatically learn via\nbackpropagation. In particular, we learn a novel attention model on the power\nseries of the transition matrix, which guides the random walk to optimize an\nupstream objective. Unlike previous approaches to attention models, the method\nthat we propose utilizes attention parameters exclusively on the data (e.g. on\nthe random walk), and not used by the model for inference. We experiment on\nlink prediction tasks, as we aim to produce embeddings that best-preserve the\ngraph structure, generalizing to unseen information. We improve\nstate-of-the-art on a comprehensive suite of real world datasets including\nsocial, collaboration, and biological networks. Adding attention to random\nwalks can reduce the error by 20% to 45% on datasets we attempted. Further, our\nlearned attention parameters are different for every graph, and our\nautomatically-found values agree with the optimal choice of hyper-parameter if\nwe manually tune existing methods. \n\n"}
{"id": "1710.09805", "contents": "Title: Improving Negative Sampling for Word Representation using Self-embedded\n  Features Abstract: Although the word-popularity based negative sampler has shown superb\nperformance in the skip-gram model, the theoretical motivation behind\noversampling popular (non-observed) words as negative samples is still not well\nunderstood. In this paper, we start from an investigation of the gradient\nvanishing issue in the skipgram model without a proper negative sampler. By\nperforming an insightful analysis from the stochastic gradient descent (SGD)\nlearning perspective, we demonstrate that, both theoretically and intuitively,\nnegative samples with larger inner product scores are more informative than\nthose with lower scores for the SGD learner in terms of both convergence rate\nand accuracy. Understanding this, we propose an alternative sampling algorithm\nthat dynamically selects informative negative samples during each SGD update.\nMore importantly, the proposed sampler accounts for multi-dimensional\nself-embedded features during the sampling process, which essentially makes it\nmore effective than the original popularity-based (one-dimensional) sampler.\nEmpirical experiments further verify our observations, and show that our\nfine-grained samplers gain significant improvement over the existing ones\nwithout increasing computational complexity. \n\n"}
{"id": "1710.10361", "contents": "Title: Deep Residual Learning for Small-Footprint Keyword Spotting Abstract: We explore the application of deep residual learning and dilated convolutions\nto the keyword spotting task, using the recently-released Google Speech\nCommands Dataset as our benchmark. Our best residual network (ResNet)\nimplementation significantly outperforms Google's previous convolutional neural\nnetworks in terms of accuracy. By varying model depth and width, we can achieve\ncompact models that also outperform previous small-footprint variants. To our\nknowledge, we are the first to examine these approaches for keyword spotting,\nand our results establish an open-source state-of-the-art reference to support\nthe development of future speech-based interfaces. \n\n"}
{"id": "1710.10380", "contents": "Title: Speeding up Context-based Sentence Representation Learning with\n  Non-autoregressive Convolutional Decoding Abstract: Context plays an important role in human language understanding, thus it may\nalso be useful for machines learning vector representations of language. In\nthis paper, we explore an asymmetric encoder-decoder structure for unsupervised\ncontext-based sentence representation learning. We carefully designed\nexperiments to show that neither an autoregressive decoder nor an RNN decoder\nis required. After that, we designed a model which still keeps an RNN as the\nencoder, while using a non-autoregressive convolutional decoder. We further\ncombine a suite of effective designs to significantly improve model efficiency\nwhile also achieving better performance. Our model is trained on two different\nlarge unlabelled corpora, and in both cases the transferability is evaluated on\na set of downstream NLP tasks. We empirically show that our model is simple and\nfast while producing rich sentence representations that excel in downstream\ntasks. \n\n"}
{"id": "1710.10453", "contents": "Title: Inducing Regular Grammars Using Recurrent Neural Networks Abstract: Grammar induction is the task of learning a grammar from a set of examples.\nRecently, neural networks have been shown to be powerful learning machines that\ncan identify patterns in streams of data. In this work we investigate their\neffectiveness in inducing a regular grammar from data, without any assumptions\nabout the grammar. We train a recurrent neural network to distinguish between\nstrings that are in or outside a regular language, and utilize an algorithm for\nextracting the learned finite-state automaton. We apply this method to several\nregular languages and find unexpected results regarding the connections between\nthe network's states that may be regarded as evidence for generalization. \n\n"}
{"id": "1710.10574", "contents": "Title: Personalized word representations Carrying Personalized Semantics\n  Learned from Social Network Posts Abstract: Distributed word representations have been shown to be very useful in various\nnatural language processing (NLP) application tasks. These word vectors learned\nfrom huge corpora very often carry both semantic and syntactic information of\nwords. However, it is well known that each individual user has his own language\npatterns because of different factors such as interested topics, friend groups,\nsocial activities, wording habits, etc., which may imply some kind of\npersonalized semantics. With such personalized semantics, the same word may\nimply slightly differently for different users. For example, the word\n\"Cappuccino\" may imply \"Leisure\", \"Joy\", \"Excellent\" for a user enjoying\ncoffee, by only a kind of drink for someone else. Such personalized semantics\nof course cannot be carried by the standard universal word vectors trained with\nhuge corpora produced by many people. In this paper, we propose a framework to\ntrain different personalized word vectors for different users based on the very\nsuccessful continuous skip-gram model using the social network data posted by\nmany individual users. In this framework, universal background word vectors are\nfirst learned from the background corpora, and then adapted by the personalized\ncorpus for each individual user to learn the personalized word vectors. We use\ntwo application tasks to evaluate the quality of the personalized word vectors\nobtained in this way, the user prediction task and the sentence completion\ntask. These personalized word vectors were shown to carry some personalized\nsemantics and offer improved performance on these two evaluation tasks. \n\n"}
{"id": "1711.00331", "contents": "Title: Semantic Structure and Interpretability of Word Embeddings Abstract: Dense word embeddings, which encode semantic meanings of words to low\ndimensional vector spaces have become very popular in natural language\nprocessing (NLP) research due to their state-of-the-art performances in many\nNLP tasks. Word embeddings are substantially successful in capturing semantic\nrelations among words, so a meaningful semantic structure must be present in\nthe respective vector spaces. However, in many cases, this semantic structure\nis broadly and heterogeneously distributed across the embedding dimensions,\nwhich makes interpretation a big challenge. In this study, we propose a\nstatistical method to uncover the latent semantic structure in the dense word\nembeddings. To perform our analysis we introduce a new dataset (SEMCAT) that\ncontains more than 6500 words semantically grouped under 110 categories. We\nfurther propose a method to quantify the interpretability of the word\nembeddings; the proposed method is a practical alternative to the classical\nword intrusion test that requires human intervention. \n\n"}
{"id": "1711.00726", "contents": "Title: A Comprehensive Low and High-level Feature Analysis for Early Rumor\n  Detection on Twitter Abstract: Recent work have done a good job in modeling rumors and detecting them over\nmicroblog streams. However, the performance of their automatic approaches are\nnot relatively high when looking early in the diffusion. A first intuition is\nthat, at early stage, most of the aggregated rumor features (e.g., propagation\nfeatures) are not mature and distinctive enough. The objective of rumor\ndebunking in microblogs, however, are to detect these misinformation as early\nas possible. In this work, we leverage neural models in learning the hidden\nrepresentations of individual rumor-related tweets at the very beginning of a\nrumor. Our extensive experiments show that the resulting signal improves our\nclassification performance over time, significantly within the first 10 hours.\nTo deepen the understanding of these low and high-level features in\ncontributing to the model performance over time, we conduct an extensive study\non a wide range of high impact rumor features for the 48 hours range. The end\nmodel that engages these features are shown to be competitive, reaches over 90%\naccuracy and out-performs strong baselines in our carefully cured dataset. \n\n"}
{"id": "1711.01048", "contents": "Title: Dual Language Models for Code Switched Speech Recognition Abstract: In this work, we present a simple and elegant approach to language modeling\nfor bilingual code-switched text. Since code-switching is a blend of two or\nmore different languages, a standard bilingual language model can be improved\nupon by using structures of the monolingual language models. We propose a novel\ntechnique called dual language models, which involves building two\ncomplementary monolingual language models and combining them using a\nprobabilistic model for switching between the two. We evaluate the efficacy of\nour approach using a conversational Mandarin-English speech corpus. We prove\nthe robustness of our model by showing significant improvements in perplexity\nmeasures over the standard bilingual language model without the use of any\nexternal information. Similar consistent improvements are also reflected in\nautomatic speech recognition error rates. \n\n"}
{"id": "1711.01567", "contents": "Title: Robust Speech Recognition Using Generative Adversarial Networks Abstract: This paper describes a general, scalable, end-to-end framework that uses the\ngenerative adversarial network (GAN) objective to enable robust speech\nrecognition. Encoders trained with the proposed approach enjoy improved\ninvariance by learning to map noisy audio to the same embedding space as that\nof clean audio. Unlike previous methods, the new framework does not rely on\ndomain expertise or simplifying assumptions as are often needed in signal\nprocessing, and directly encourages robustness in a data-driven way. We show\nthe new approach improves simulated far-field speech recognition of vanilla\nsequence-to-sequence models without specialized front-ends or preprocessing. \n\n"}
{"id": "1711.02013", "contents": "Title: Neural Language Modeling by Jointly Learning Syntax and Lexicon Abstract: We propose a neural language model capable of unsupervised syntactic\nstructure induction. The model leverages the structure information to form\nbetter semantic representations and better language modeling. Standard\nrecurrent neural networks are limited by their structure and fail to\nefficiently use syntactic information. On the other hand, tree-structured\nrecursive networks usually require additional structural supervision at the\ncost of human expert annotation. In this paper, We propose a novel neural\nlanguage model, called the Parsing-Reading-Predict Networks (PRPN), that can\nsimultaneously induce the syntactic structure from unannotated sentences and\nleverage the inferred structure to learn a better language model. In our model,\nthe gradient can be directly back-propagated from the language model loss into\nthe neural parsing network. Experiments show that the proposed model can\ndiscover the underlying syntactic structure and achieve state-of-the-art\nperformance on word/character-level language model tasks. \n\n"}
{"id": "1711.02132", "contents": "Title: Weighted Transformer Network for Machine Translation Abstract: State-of-the-art results on neural machine translation often use attentional\nsequence-to-sequence models with some form of convolution or recursion. Vaswani\net al. (2017) propose a new architecture that avoids recurrence and convolution\ncompletely. Instead, it uses only self-attention and feed-forward layers. While\nthe proposed architecture achieves state-of-the-art results on several machine\ntranslation tasks, it requires a large number of parameters and training\niterations to converge. We propose Weighted Transformer, a Transformer with\nmodified attention layers, that not only outperforms the baseline network in\nBLEU score but also converges 15-40% faster. Specifically, we replace the\nmulti-head attention by multiple self-attention branches that the model learns\nto combine during the training process. Our model improves the state-of-the-art\nperformance by 0.5 BLEU points on the WMT 2014 English-to-German translation\ntask and by 0.4 on the English-to-French translation task. \n\n"}
{"id": "1711.02231", "contents": "Title: Visually-Aware Fashion Recommendation and Design with Generative Image\n  Models Abstract: Building effective recommender systems for domains like fashion is\nchallenging due to the high level of subjectivity and the semantic complexity\nof the features involved (i.e., fashion styles). Recent work has shown that\napproaches to `visual' recommendation (e.g.~clothing, art, etc.) can be made\nmore accurate by incorporating visual signals directly into the recommendation\nobjective, using `off-the-shelf' feature representations derived from deep\nnetworks. Here, we seek to extend this contribution by showing that\nrecommendation performance can be significantly improved by learning `fashion\naware' image representations directly, i.e., by training the image\nrepresentation (from the pixel level) and the recommender system jointly; this\ncontribution is related to recent work using Siamese CNNs, though we are able\nto show improvements over state-of-the-art recommendation techniques such as\nBPR and variants that make use of pre-trained visual features. Furthermore, we\nshow that our model can be used \\emph{generatively}, i.e., given a user and a\nproduct category, we can generate new images (i.e., clothing items) that are\nmost consistent with their personal taste. This represents a first step towards\nbuilding systems that go beyond recommending existing items from a product\ncorpus, but which can be used to suggest styles and aid the design of new\nproducts. \n\n"}
{"id": "1711.02326", "contents": "Title: Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent\n  Networks Abstract: A major drawback of backpropagation through time (BPTT) is the difficulty of\nlearning long-term dependencies, coming from having to propagate credit\ninformation backwards through every single step of the forward computation.\nThis makes BPTT both computationally impractical and biologically implausible.\nFor this reason, full backpropagation through time is rarely used on long\nsequences, and truncated backpropagation through time is used as a heuristic.\nHowever, this usually leads to biased estimates of the gradient in which longer\nterm dependencies are ignored. Addressing this issue, we propose an alternative\nalgorithm, Sparse Attentive Backtracking, which might also be related to\nprinciples used by brains to learn long-term dependencies. Sparse Attentive\nBacktracking learns an attention mechanism over the hidden states of the past\nand selectively backpropagates through paths with high attention weights. This\nallows the model to learn long term dependencies while only backtracking for a\nsmall number of time steps, not just from the recent past but also from\nattended relevant past states. \n\n"}
{"id": "1711.02487", "contents": "Title: Deep density networks and uncertainty in recommender systems Abstract: Building robust online content recommendation systems requires learning\ncomplex interactions between user preferences and content features. The field\nhas evolved rapidly in recent years from traditional multi-arm bandit and\ncollaborative filtering techniques, with new methods employing Deep Learning\nmodels to capture non-linearities. Despite progress, the dynamic nature of\nonline recommendations still poses great challenges, such as finding the\ndelicate balance between exploration and exploitation. In this paper we show\nhow uncertainty estimations can be incorporated by employing them in an\noptimistic exploitation/exploration strategy for more efficient exploration of\nnew recommendations. We provide a novel hybrid deep neural network model, Deep\nDensity Networks (DDN), which integrates content-based deep learning models\nwith a collaborative scheme that is able to robustly model and estimate\nuncertainty. Finally, we present online and offline results after incorporating\nDNN into a real world content recommendation system that serves billions of\nrecommendations per day, and show the benefit of using DDN in practice. \n\n"}
{"id": "1711.02604", "contents": "Title: Unbounded cache model for online language modeling with open vocabulary Abstract: Recently, continuous cache models were proposed as extensions to recurrent\nneural network language models, to adapt their predictions to local changes in\nthe data distribution. These models only capture the local context, of up to a\nfew thousands tokens. In this paper, we propose an extension of continuous\ncache models, which can scale to larger contexts. In particular, we use a large\nscale non-parametric memory component that stores all the hidden activations\nseen in the past. We leverage recent advances in approximate nearest neighbor\nsearch and quantization algorithms to store millions of representations while\nsearching them efficiently. We conduct extensive experiments showing that our\napproach significantly improves the perplexity of pre-trained language models\non new distributions, and can scale efficiently to much larger contexts than\npreviously proposed local cache models. \n\n"}
{"id": "1711.02760", "contents": "Title: Food Recommender Systems: Important Contributions, Challenges and Future\n  Research Directions Abstract: The recommendation of food items is important for many reasons. Attaining\ncooking inspiration via digital sources is becoming evermore popular; as are\nsystems, which recommend other types of food, such as meals in restaurants or\nproducts in supermarkets. Researchers have been studying these kinds of systems\nfor many years, suggesting not only that can they be a means to help people\nfind food they might want to eat, but also help them nourish themselves more\nhealthily. This paper provides a summary of the state-of-the-art of so-called\nfood recommender systems, highlighting both seminal and most recent approaches\nto the problem, as well as important specializations, such as food\nrecommendation systems for groups of users or systems which promote healthy\neating. We moreover discuss the diverse challenges involved in designing recsys\nfor food, summarise the lessons learned from past research and outline what we\nbelieve to be important future directions and open questions for the field. In\nproviding these contributions we hope to provide a useful resource for\nresearchers and practitioners alike. \n\n"}
{"id": "1711.04019", "contents": "Title: A Batch Learning Framework for Scalable Personalized Ranking Abstract: In designing personalized ranking algorithms, it is desirable to encourage a\nhigh precision at the top of the ranked list. Existing methods either seek a\nsmooth convex surrogate for a non-smooth ranking metric or directly modify\nupdating procedures to encourage top accuracy. In this work we point out that\nthese methods do not scale well to a large-scale setting, and this is partly\ndue to the inaccurate pointwise or pairwise rank estimation. We propose a new\nframework for personalized ranking. It uses batch-based rank estimators and\nsmooth rank-sensitive loss functions. This new batch learning framework leads\nto more stable and accurate rank approximations compared to previous work.\nMoreover, it enables explicit use of parallel computation to speed up training.\nWe conduct empirical evaluation on three item recommendation tasks. Our method\nshows consistent accuracy improvements over state-of-the-art methods.\nAdditionally, we observe time efficiency advantages when data scale increases. \n\n"}
{"id": "1711.04044", "contents": "Title: Kernelized Hashcode Representations for Relation Extraction Abstract: Kernel methods have produced state-of-the-art results for a number of NLP\ntasks such as relation extraction, but suffer from poor scalability due to the\nhigh cost of computing kernel similarities between natural language structures.\nA recently proposed technique, kernelized locality-sensitive hashing (KLSH),\ncan significantly reduce the computational cost, but is only applicable to\nclassifiers operating on kNN graphs. Here we propose to use random subspaces of\nKLSH codes for efficiently constructing an explicit representation of NLP\nstructures suitable for general classification methods. Further, we propose an\napproach for optimizing the KLSH model for classification problems by\nmaximizing an approximation of mutual information between the KLSH codes\n(feature vectors) and the class labels. We evaluate the proposed approach on\nbiomedical relation extraction datasets, and observe significant and robust\nimprovements in accuracy w.r.t. state-of-the-art classifiers, along with\ndrastic (orders-of-magnitude) speedup compared to conventional kernel methods. \n\n"}
{"id": "1711.04094", "contents": "Title: Enhancing Network Embedding with Auxiliary Information: An Explicit\n  Matrix Factorization Perspective Abstract: Recent advances in the field of network embedding have shown the\nlow-dimensional network representation is playing a critical role in network\nanalysis. However, most of the existing principles of network embedding do not\nincorporate auxiliary information such as content and labels of nodes flexibly.\nIn this paper, we take a matrix factorization perspective of network embedding,\nand incorporate structure, content and label information of the network\nsimultaneously. For structure, we validate that the matrix we construct\npreserves high-order proximities of the network. Label information can be\nfurther integrated into the matrix via the process of random walk sampling to\nenhance the quality of embedding in an unsupervised manner, i.e., without\nleveraging downstream classifiers. In addition, we generalize the Skip-Gram\nNegative Sampling model to integrate the content of the network in a matrix\nfactorization framework. As a consequence, network embedding can be learned in\na unified framework integrating network structure and node content as well as\nlabel information simultaneously. We demonstrate the efficacy of the proposed\nmodel with the tasks of semi-supervised node classification and link prediction\non a variety of real-world benchmark network datasets. \n\n"}
{"id": "1711.04150", "contents": "Title: STWalk: Learning Trajectory Representations in Temporal Graphs Abstract: Analyzing the temporal behavior of nodes in time-varying graphs is useful for\nmany applications such as targeted advertising, community evolution and outlier\ndetection. In this paper, we present a novel approach, STWalk, for learning\ntrajectory representations of nodes in temporal graphs. The proposed framework\nmakes use of structural properties of graphs at current and previous time-steps\nto learn effective node trajectory representations. STWalk performs random\nwalks on a graph at a given time step (called space-walk) as well as on graphs\nfrom past time-steps (called time-walk) to capture the spatio-temporal behavior\nof nodes. We propose two variants of STWalk to learn trajectory\nrepresentations. In one algorithm, we perform space-walk and time-walk as part\nof a single step. In the other variant, we perform space-walk and time-walk\nseparately and combine the learned representations to get the final trajectory\nembedding. Extensive experiments on three real-world temporal graph datasets\nvalidate the effectiveness of the learned representations when compared to\nthree baseline methods. We also show the goodness of the learned trajectory\nembeddings for change point detection, as well as demonstrate that arithmetic\noperations on these trajectory representations yield interesting and\ninterpretable results. \n\n"}
{"id": "1711.04168", "contents": "Title: Unsupervised Document Embedding With CNNs Abstract: We propose a new model for unsupervised document embedding. Leading existing\napproaches either require complex inference or use recurrent neural networks\n(RNN) that are difficult to parallelize. We take a different route and develop\na convolutional neural network (CNN) embedding model. Our CNN architecture is\nfully parallelizable resulting in over 10x speedup in inference time over RNN\nmodels. Parallelizable architecture enables to train deeper models where each\nsuccessive layer has increasingly larger receptive field and models longer\nrange semantic structure within the document. We additionally propose a fully\nunsupervised learning algorithm to train this model based on stochastic forward\nprediction. Empirical results on two public benchmarks show that our approach\nproduces comparable to state-of-the-art accuracy at a fraction of computational\ncost. \n\n"}
{"id": "1711.04903", "contents": "Title: Robust Multilingual Part-of-Speech Tagging via Adversarial Training Abstract: Adversarial training (AT) is a powerful regularization method for neural\nnetworks, aiming to achieve robustness to input perturbations. Yet, the\nspecific effects of the robustness obtained from AT are still unclear in the\ncontext of natural language processing. In this paper, we propose and analyze a\nneural POS tagging model that exploits AT. In our experiments on the Penn\nTreebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages),\nwe find that AT not only improves the overall tagging accuracy, but also 1)\nprevents over-fitting well in low resource languages and 2) boosts tagging\naccuracy for rare / unseen words. We also demonstrate that 3) the improved\ntagging performance by AT contributes to the downstream task of dependency\nparsing, and that 4) AT helps the model to learn cleaner word representations.\n5) The proposed AT model is generally effective in different sequence labeling\ntasks. These positive results motivate further use of AT for natural language\ntasks. \n\n"}
{"id": "1711.04981", "contents": "Title: SkipFlow: Incorporating Neural Coherence Features for End-to-End\n  Automatic Text Scoring Abstract: Deep learning has demonstrated tremendous potential for Automatic Text\nScoring (ATS) tasks. In this paper, we describe a new neural architecture that\nenhances vanilla neural network models with auxiliary neural coherence\nfeatures. Our new method proposes a new \\textsc{SkipFlow} mechanism that models\nrelationships between snapshots of the hidden representations of a long\nshort-term memory (LSTM) network as it reads. Subsequently, the semantic\nrelationships between multiple snapshots are used as auxiliary features for\nprediction. This has two main benefits. Firstly, essays are typically long\nsequences and therefore the memorization capability of the LSTM network may be\ninsufficient. Implicit access to multiple snapshots can alleviate this problem\nby acting as a protection against vanishing gradients. The parameters of the\n\\textsc{SkipFlow} mechanism also acts as an auxiliary memory. Secondly,\nmodeling relationships between multiple positions allows our model to learn\nfeatures that represent and approximate textual coherence. In our model, we\ncall this \\textit{neural coherence} features. Overall, we present a unified\ndeep learning architecture that generates neural coherence features as it reads\nin an end-to-end fashion. Our approach demonstrates state-of-the-art\nperformance on the benchmark ASAP dataset, outperforming not only feature\nengineering baselines but also other deep learning models. \n\n"}
{"id": "1711.05350", "contents": "Title: A Deep Learning Approach for Expert Identification in Question Answering\n  Communities Abstract: In this paper, we describe an effective convolutional neural network\nframework for identifying the expert in question answering community. This\napproach uses the convolutional neural network and combines user feature\nrepresentations with question feature representations to compute scores that\nthe user who gets the highest score is the expert on this question. Unlike\nprior work, this method does not measure expert based on measure answer content\nquality to identify the expert but only require question sentence and user\nembedding feature to identify the expert. Remarkably, Our model can be applied\nto different languages and different domains. The proposed framework is trained\non two datasets, The first dataset is Stack Overflow and the second one is\nZhihu. The Top-1 accuracy results of our experiments show that our framework\noutperforms the best baseline framework for expert identification. \n\n"}
{"id": "1711.05448", "contents": "Title: Lattice Rescoring Strategies for Long Short Term Memory Language Models\n  in Speech Recognition Abstract: Recurrent neural network (RNN) language models (LMs) and Long Short Term\nMemory (LSTM) LMs, a variant of RNN LMs, have been shown to outperform\ntraditional N-gram LMs on speech recognition tasks. However, these models are\ncomputationally more expensive than N-gram LMs for decoding, and thus,\nchallenging to integrate into speech recognizers. Recent research has proposed\nthe use of lattice-rescoring algorithms using RNNLMs and LSTMLMs as an\nefficient strategy to integrate these models into a speech recognition system.\nIn this paper, we evaluate existing lattice rescoring algorithms along with new\nvariants on a YouTube speech recognition task. Lattice rescoring using LSTMLMs\nreduces the word error rate (WER) for this task by 8\\% relative to the WER\nobtained using an N-gram LM. \n\n"}
{"id": "1711.05468", "contents": "Title: Tracking Typological Traits of Uralic Languages in Distributed Language\n  Representations Abstract: Although linguistic typology has a long history, computational approaches\nhave only recently gained popularity. The use of distributed representations in\ncomputational linguistics has also become increasingly popular. A recent\ndevelopment is to learn distributed representations of language, such that\ntypologically similar languages are spatially close to one another. Although\nempirical successes have been shown for such language representations, they\nhave not been subjected to much typological probing. In this paper, we first\nlook at whether this type of language representations are empirically useful\nfor model transfer between Uralic languages in deep neural networks. We then\ninvestigate which typological features are encoded in these representations by\nattempting to predict features in the World Atlas of Language Structures, at\nvarious stages of fine-tuning of the representations. We focus on Uralic\nlanguages, and find that some typological traits can be automatically inferred\nwith accuracies well above a strong baseline. \n\n"}
{"id": "1711.05678", "contents": "Title: Unsupervised Morphological Expansion of Small Datasets for Improving\n  Word Embeddings Abstract: We present a language independent, unsupervised method for building word\nembeddings using morphological expansion of text. Our model handles the problem\nof data sparsity and yields improved word embeddings by relying on training\nword embeddings on artificially generated sentences. We evaluate our method\nusing small sized training sets on eleven test sets for the word similarity\ntask across seven languages. Further, for English, we evaluated the impacts of\nour approach using a large training set on three standard test sets. Our method\nimproved results across all languages. \n\n"}
{"id": "1711.06061", "contents": "Title: An Encoder-Decoder Framework Translating Natural Language to Database\n  Queries Abstract: Machine translation is going through a radical revolution, driven by the\nexplosive development of deep learning techniques using Convolutional Neural\nNetwork (CNN) and Recurrent Neural Network (RNN). In this paper, we consider a\nspecial case in machine translation problems, targeting to convert natural\nlanguage into Structured Query Language (SQL) for data retrieval over\nrelational database. Although generic CNN and RNN learn the grammar structure\nof SQL when trained with sufficient samples, the accuracy and training\nefficiency of the model could be dramatically improved, when the translation\nmodel is deeply integrated with the grammar rules of SQL. We present a new\nencoder-decoder framework, with a suite of new approaches, including new\nsemantic features fed into the encoder, grammar-aware states injected into the\nmemory of decoder, as well as recursive state management for sub-queries. These\ntechniques help the neural network better focus on understanding semantics of\noperations in natural language and save the efforts on SQL grammar learning.\nThe empirical evaluation on real world database and queries show that our\napproach outperform state-of-the-art solution by a significant margin. \n\n"}
{"id": "1711.06632", "contents": "Title: ATRank: An Attention-Based User Behavior Modeling Framework for\n  Recommendation Abstract: A user can be represented as what he/she does along the history. A common way\nto deal with the user modeling problem is to manually extract all kinds of\naggregated features over the heterogeneous behaviors, which may fail to fully\nrepresent the data itself due to limited human instinct. Recent works usually\nuse RNN-based methods to give an overall embedding of a behavior sequence,\nwhich then could be exploited by the downstream applications. However, this can\nonly preserve very limited information, or aggregated memories of a person.\nWhen a downstream application requires to facilitate the modeled user features,\nit may lose the integrity of the specific highly correlated behavior of the\nuser, and introduce noises derived from unrelated behaviors. This paper\nproposes an attention based user behavior modeling framework called ATRank,\nwhich we mainly use for recommendation tasks. Heterogeneous user behaviors are\nconsidered in our model that we project all types of behaviors into multiple\nlatent semantic spaces, where influence can be made among the behaviors via\nself-attention. Downstream applications then can use the user behavior vectors\nvia vanilla attention. Experiments show that ATRank can achieve better\nperformance and faster training process. We further explore ATRank to use one\nunified model to predict different types of user behaviors at the same time,\nshowing a comparable performance with the highly optimized individual models. \n\n"}
{"id": "1711.07128", "contents": "Title: Hello Edge: Keyword Spotting on Microcontrollers Abstract: Keyword spotting (KWS) is a critical component for enabling speech based user\ninteractions on smart devices. It requires real-time response and high accuracy\nfor good user experience. Recently, neural networks have become an attractive\nchoice for KWS architecture because of their superior accuracy compared to\ntraditional speech processing algorithms. Due to its always-on nature, KWS\napplication has highly constrained power budget and typically runs on tiny\nmicrocontrollers with limited memory and compute capability. The design of\nneural network architecture for KWS must consider these constraints. In this\nwork, we perform neural network architecture evaluation and exploration for\nrunning KWS on resource-constrained microcontrollers. We train various neural\nnetwork architectures for keyword spotting published in literature to compare\ntheir accuracy and memory/compute requirements. We show that it is possible to\noptimize these neural network architectures to fit within the memory and\ncompute constraints of microcontrollers without sacrificing accuracy. We\nfurther explore the depthwise separable convolutional neural network (DS-CNN)\nand compare it against other neural network architectures. DS-CNN achieves an\naccuracy of 95.4%, which is ~10% higher than the DNN model with similar number\nof parameters. \n\n"}
{"id": "1711.07274", "contents": "Title: Speech recognition for medical conversations Abstract: In this work we explored building automatic speech recognition models for\ntranscribing doctor patient conversation. We collected a large scale dataset of\nclinical conversations ($14,000$ hr), designed the task to represent the real\nword scenario, and explored several alignment approaches to iteratively improve\ndata quality. We explored both CTC and LAS systems for building speech\nrecognition models. The LAS was more resilient to noisy data and CTC required\nmore data clean up. A detailed analysis is provided for understanding the\nperformance for clinical tasks. Our analysis showed the speech recognition\nmodels performed well on important medical utterances, while errors occurred in\ncausal conversations. Overall we believe the resulting models can provide\nreasonable quality in practice. \n\n"}
{"id": "1711.07798", "contents": "Title: Visual and Textual Sentiment Analysis Using Deep Fusion Convolutional\n  Neural Networks Abstract: Sentiment analysis is attracting more and more attentions and has become a\nvery hot research topic due to its potential applications in personalized\nrecommendation, opinion mining, etc. Most of the existing methods are based on\neither textual or visual data and can not achieve satisfactory results, as it\nis very hard to extract sufficient information from only one single modality\ndata. Inspired by the observation that there exists strong semantic correlation\nbetween visual and textual data in social medias, we propose an end-to-end deep\nfusion convolutional neural network to jointly learn textual and visual\nsentiment representations from training examples. The two modality information\nare fused together in a pooling layer and fed into fully-connected layers to\npredict the sentiment polarity. We evaluate the proposed approach on two widely\nused data sets. Results show that our method achieves promising result compared\nwith the state-of-the-art methods which clearly demonstrate its competency. \n\n"}
{"id": "1711.08870", "contents": "Title: Continuous Semantic Topic Embedding Model Using Variational Autoencoder Abstract: This paper proposes the continuous semantic topic embedding model (CSTEM)\nwhich finds latent topic variables in documents using continuous semantic\ndistance function between the topics and the words by means of the variational\nautoencoder(VAE). The semantic distance could be represented by any symmetric\nbell-shaped geometric distance function on the Euclidean space, for which the\nMahalanobis distance is used in this paper. In order for the semantic distance\nto perform more properly, we newly introduce an additional model parameter for\neach word to take out the global factor from this distance indicating how\nlikely it occurs regardless of its topic. It certainly improves the problem\nthat the Gaussian distribution which is used in previous topic model with\ncontinuous word embedding could not explain the semantic relation correctly and\nhelps to obtain the higher topic coherence. Through the experiments with the\ndataset of 20 Newsgroup, NIPS papers and CNN/Dailymail corpus, the performance\nof the recent state-of-the-art models is accomplished by our model as well as\ngenerating topic embedding vectors which makes possible to observe where the\ntopic vectors are embedded with the word vectors in the real Euclidean space\nand how the topics are related each other semantically. \n\n"}
{"id": "1711.09476", "contents": "Title: Machine Translation using Semantic Web Technologies: A Survey Abstract: A large number of machine translation approaches have recently been developed\nto facilitate the fluid migration of content across languages. However, the\nliterature suggests that many obstacles must still be dealt with to achieve\nbetter automatic translations. One of these obstacles is lexical and syntactic\nambiguity. A promising way of overcoming this problem is using Semantic Web\ntechnologies. This article presents the results of a systematic review of\nmachine translation approaches that rely on Semantic Web technologies for\ntranslating texts. Overall, our survey suggests that while Semantic Web\ntechnologies can enhance the quality of machine translation outputs for various\nproblems, the combination of both is still in its infancy. \n\n"}
{"id": "1711.09645", "contents": "Title: Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis Abstract: We consider the task of fine-grained sentiment analysis from the perspective\nof multiple instance learning (MIL). Our neural model is trained on document\nsentiment labels, and learns to predict the sentiment of text segments, i.e.\nsentences or elementary discourse units (EDUs), without segment-level\nsupervision. We introduce an attention-based polarity scoring method for\nidentifying positive and negative text snippets and a new dataset which we call\nSPOT (as shorthand for Segment-level POlariTy annotations) for evaluating\nMIL-style sentiment models like ours. Experimental results demonstrate superior\nperformance against multiple baselines, whereas a judgement elicitation study\nshows that EDU-level opinion extraction produces more informative summaries\nthan sentence-based alternatives. \n\n"}
{"id": "1711.10046", "contents": "Title: Recurrent Generative Adversarial Networks for Proximal Learning and\n  Automated Compressive Image Recovery Abstract: Recovering images from undersampled linear measurements typically leads to an\nill-posed linear inverse problem, that asks for proper statistical priors.\nBuilding effective priors is however challenged by the low train and test\noverhead dictated by real-time tasks; and the need for retrieving visually\n\"plausible\" and physically \"feasible\" images with minimal hallucination. To\ncope with these challenges, we design a cascaded network architecture that\nunrolls the proximal gradient iterations by permeating benefits from generative\nresidual networks (ResNet) to modeling the proximal operator. A mixture of\npixel-wise and perceptual costs is then deployed to train proximals. The\noverall architecture resembles back-and-forth projection onto the intersection\nof feasible and plausible images. Extensive computational experiments are\nexamined for a global task of reconstructing MR images of pediatric patients,\nand a more local task of superresolving CelebA faces, that are insightful to\ndesign efficient architectures. Our observations indicate that for MRI\nreconstruction, a recurrent ResNet with a single residual block effectively\nlearns the proximal. This simple architecture appears to significantly\noutperform the alternative deep ResNet architecture by 2dB SNR, and the\nconventional compressed-sensing MRI by 4dB SNR with 100x faster inference. For\nimage superresolution, our preliminary results indicate that modeling the\ndenoising proximal demands deep ResNets. \n\n"}
{"id": "1711.10093", "contents": "Title: Surfacing contextual hate speech words within social media Abstract: Social media platforms have recently seen an increase in the occurrence of\nhate speech discourse which has led to calls for improved detection methods.\nMost of these rely on annotated data, keywords, and a classification technique.\nWhile this approach provides good coverage, it can fall short when dealing with\nnew terms produced by online extremist communities which act as original\nsources of words which have alternate hate speech meanings. These code words\n(which can be both created and adopted words) are designed to evade automatic\ndetection and often have benign meanings in regular discourse. As an example,\n\"skypes\", \"googles\", and \"yahoos\" are all instances of words which have an\nalternate meaning that can be used for hate speech. This overlap introduces\nadditional challenges when relying on keywords for both the collection of data\nthat is specific to hate speech, and downstream classification. In this work,\nwe develop a community detection approach for finding extremist hate speech\ncommunities and collecting data from their members. We also develop a word\nembedding model that learns the alternate hate speech meaning of words and\ndemonstrate the candidacy of our code words with several annotation\nexperiments, designed to determine if it is possible to recognize a word as\nbeing used for hate speech without knowing its alternate meaning. We report an\ninter-annotator agreement rate of K=0.871, and K=0.676 for data drawn from our\nextremist community and the keyword approach respectively, supporting our claim\nthat hate speech detection is a contextual task and does not depend on a fixed\nlist of keywords. Our goal is to advance the domain by providing a high quality\nhate speech dataset in addition to learned code words that can be fed into\nexisting classification approaches, thus improving the accuracy of automated\ndetection. \n\n"}
{"id": "1711.10967", "contents": "Title: The Block Point Process Model for Continuous-Time Event-Based Dynamic\n  Networks Abstract: We consider the problem of analyzing timestamped relational events between a\nset of entities, such as messages between users of an on-line social network.\nSuch data are often analyzed using static or discrete-time network models,\nwhich discard a significant amount of information by aggregating events over\ntime to form network snapshots. In this paper, we introduce a block point\nprocess model (BPPM) for continuous-time event-based dynamic networks. The BPPM\nis inspired by the well-known stochastic block model (SBM) for static networks.\nWe show that networks generated by the BPPM follow an SBM in the limit of a\ngrowing number of nodes. We use this property to develop principled and\nefficient local search and variational inference procedures initialized by\nregularized spectral clustering. We fit BPPMs with exponential Hawkes processes\nto analyze several real network data sets, including a Facebook wall post\nnetwork with over 3,500 nodes and 130,000 events. \n\n"}
{"id": "1712.00377", "contents": "Title: Don't Just Assume; Look and Answer: Overcoming Priors for Visual\n  Question Answering Abstract: A number of studies have found that today's Visual Question Answering (VQA)\nmodels are heavily driven by superficial correlations in the training data and\nlack sufficient image grounding. To encourage development of models geared\ntowards the latter, we propose a new setting for VQA where for every question\ntype, train and test sets have different prior distributions of answers.\nSpecifically, we present new splits of the VQA v1 and VQA v2 datasets, which we\ncall Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2\nrespectively). First, we evaluate several existing VQA models under this new\nsetting and show that their performance degrades significantly compared to the\noriginal VQA setting. Second, we propose a novel Grounded Visual Question\nAnswering model (GVQA) that contains inductive biases and restrictions in the\narchitecture specifically designed to prevent the model from 'cheating' by\nprimarily relying on priors in the training data. Specifically, GVQA explicitly\ndisentangles the recognition of visual concepts present in the image from the\nidentification of plausible answer space for a given question, enabling the\nmodel to more robustly generalize across different distributions of answers.\nGVQA is built off an existing VQA model -- Stacked Attention Networks (SAN).\nOur experiments demonstrate that GVQA significantly outperforms SAN on both\nVQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more\npowerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in\nseveral cases. GVQA offers strengths complementary to SAN when trained and\nevaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more\ntransparent and interpretable than existing VQA models. \n\n"}
{"id": "1712.01238", "contents": "Title: Learning by Asking Questions Abstract: We introduce an interactive learning framework for the development and\ntesting of intelligent visual systems, called learning-by-asking (LBA). We\nexplore LBA in context of the Visual Question Answering (VQA) task. LBA differs\nfrom standard VQA training in that most questions are not observed during\ntraining time, and the learner must ask questions it wants answers to. Thus,\nLBA more closely mimics natural learning and has the potential to be more\ndata-efficient than the traditional VQA setting. We present a model that\nperforms LBA on the CLEVR dataset, and show that it automatically discovers an\neasy-to-hard curriculum when learning interactively from an oracle. Our LBA\ngenerated data consistently matches or outperforms the CLEVR train data and is\nmore sample efficient. We also show that our model asks questions that\ngeneralize to state-of-the-art VQA models and to novel test time distributions. \n\n"}
{"id": "1712.01329", "contents": "Title: Examining Cooperation in Visual Dialog Models Abstract: In this work we propose a blackbox intervention method for visual dialog\nmodels, with the aim of assessing the contribution of individual linguistic or\nvisual components. Concretely, we conduct structured or randomized\ninterventions that aim to impair an individual component of the model, and\nobserve changes in task performance. We reproduce a state-of-the-art visual\ndialog model and demonstrate that our methodology yields surprising insights,\nnamely that both dialog and image information have minimal contributions to\ntask performance. The intervention method presented here can be applied as a\nsanity check for the strength and robustness of each component in visual dialog\nsystems. \n\n"}
{"id": "1712.01821", "contents": "Title: Neural Machine Translation by Generating Multiple Linguistic Factors Abstract: Factored neural machine translation (FNMT) is founded on the idea of using\nthe morphological and grammatical decomposition of the words (factors) at the\noutput side of the neural network. This architecture addresses two well-known\nproblems occurring in MT, namely the size of target language vocabulary and the\nnumber of unknown tokens produced in the translation. FNMT system is designed\nto manage larger vocabulary and reduce the training time (for systems with\nequivalent target language vocabulary size). Moreover, we can produce\ngrammatically correct words that are not part of the vocabulary. FNMT model is\nevaluated on IWSLT'15 English to French task and compared to the baseline\nword-based and BPE-based NMT systems. Promising qualitative and quantitative\nresults (in terms of BLEU and METEOR) are reported. \n\n"}
{"id": "1712.02084", "contents": "Title: Sequence Covering for Efficient Host-Based Intrusion Detection Abstract: This paper introduces a new similarity measure, the covering similarity, that\nwe formally define for evaluating the similarity between a symbolic sequence\nand a set of symbolic sequences. A pair-wise similarity can also be directly\nderived from the covering similarity to compare two symbolic sequences. An\nefficient implementation to compute the covering similarity is proposed that\nuses a suffix tree data-structure, but other implementations, based on suffix\narray for instance, are possible and possibly necessary for handling large\nscale problems. We have used this similarity to isolate attack sequences from\nnormal sequences in the scope of Host-based Intrusion Detection. We have\nassessed the covering similarity on two well-known benchmarks in the field. In\nview of the results reported on these two datasets for the state of the art\nmethods, and according to the comparative study we have carried out based on\nthree challenging similarity measures commonly used for string processing or in\nbioinformatics, we show that the covering similarity is particularly relevant\nto address the detection of anomalies in sequences of system calls \n\n"}
{"id": "1712.02342", "contents": "Title: A Context-Aware User-Item Representation Learning for Item\n  Recommendation Abstract: Both reviews and user-item interactions (i.e., rating scores) have been\nwidely adopted for user rating prediction. However, these existing techniques\nmainly extract the latent representations for users and items in an independent\nand static manner. That is, a single static feature vector is derived to encode\nher preference without considering the particular characteristics of each\ncandidate item. We argue that this static encoding scheme is difficult to fully\ncapture the users' preference. In this paper, we propose a novel context-aware\nuser-item representation learning model for rating prediction, named CARL.\nNamely, CARL derives a joint representation for a given user-item pair based on\ntheir individual latent features and latent feature interactions. Then, CARL\nadopts Factorization Machines to further model higher-order feature\ninteractions on the basis of the user-item pair for rating prediction.\nSpecifically, two separate learning components are devised in CARL to exploit\nreview data and interaction data respectively: review-based feature learning\nand interaction-based feature learning. In review-based learning component,\nwith convolution operations and attention mechanism, the relevant features for\na user-item pair are extracted by jointly considering their corresponding\nreviews. However, these features are only review-driven and may not be\ncomprehensive. Hence, interaction-based learning component further extracts\ncomplementary features from interaction data alone, also on the basis of\nuser-item pairs. The final rating score is then derived with a dynamic linear\nfusion mechanism. Experiments on five real-world datasets show that CARL\nachieves significantly better rating prediction accuracy than existing\nstate-of-the-art alternatives. Also, with attention mechanism, we show that the\nrelevant information in reviews can be highlighted to interpret the rating\nprediction. \n\n"}
{"id": "1712.03158", "contents": "Title: Graph-based time-space trade-offs for approximate near neighbors Abstract: We take a first step towards a rigorous asymptotic analysis of graph-based\napproaches for finding (approximate) nearest neighbors in high-dimensional\nspaces, by analyzing the complexity of (randomized) greedy walks on the\napproximate near neighbor graph. For random data sets of size $n = 2^{o(d)}$ on\nthe $d$-dimensional Euclidean unit sphere, using near neighbor graphs we can\nprovably solve the approximate nearest neighbor problem with approximation\nfactor $c > 1$ in query time $n^{\\rho_q + o(1)}$ and space $n^{1 + \\rho_s +\no(1)}$, for arbitrary $\\rho_q, \\rho_s \\geq 0$ satisfying \\begin{align} (2c^2 -\n1) \\rho_q + 2 c^2 (c^2 - 1) \\sqrt{\\rho_s (1 - \\rho_s)} \\geq c^4. \\end{align}\nGraph-based near neighbor searching is especially competitive with hash-based\nmethods for small $c$ and near-linear memory, and in this regime the asymptotic\nscaling of a greedy graph-based search matches the recent optimal hash-based\ntrade-offs of Andoni-Laarhoven-Razenshteyn-Waingarten [SODA'17]. We further\nstudy how the trade-offs scale when the data set is of size $n =\n2^{\\Theta(d)}$, and analyze asymptotic complexities when applying these results\nto lattice sieving. \n\n"}
{"id": "1712.03463", "contents": "Title: Learning Interpretable Spatial Operations in a Rich 3D Blocks World Abstract: In this paper, we study the problem of mapping natural language instructions\nto complex spatial actions in a 3D blocks world. We first introduce a new\ndataset that pairs complex 3D spatial operations to rich natural language\ndescriptions that require complex spatial and pragmatic interpretations such as\n\"mirroring\", \"twisting\", and \"balancing\". This dataset, built on the simulation\nenvironment of Bisk, Yuret, and Marcu (2016), attains language that is\nsignificantly richer and more complex, while also doubling the size of the\noriginal dataset in the 2D environment with 100 new world configurations and\n250,000 tokens. In addition, we propose a new neural architecture that achieves\ncompetitive results while automatically discovering an inventory of\ninterpretable spatial operations (Figure 5) \n\n"}
{"id": "1712.03897", "contents": "Title: Learning Modality-Invariant Representations for Speech and Images Abstract: In this paper, we explore the unsupervised learning of a semantic embedding\nspace for co-occurring sensory inputs. Specifically, we focus on the task of\nlearning a semantic vector space for both spoken and handwritten digits using\nthe TIDIGITs and MNIST datasets. Current techniques encode image and\naudio/textual inputs directly to semantic embeddings. In contrast, our\ntechnique maps an input to the mean and log variance vectors of a diagonal\nGaussian from which sample semantic embeddings are drawn. In addition to\nencouraging semantic similarity between co-occurring inputs,our loss function\nincludes a regularization term borrowed from variational autoencoders (VAEs)\nwhich drives the posterior distributions over embeddings to be unit Gaussian.\nWe can use this regularization term to filter out modality information while\npreserving semantic information. We speculate this technique may be more\nbroadly applicable to other areas of cross-modality/domain information\nretrieval and transfer learning. \n\n"}
{"id": "1712.04116", "contents": "Title: A Novel Document Generation Process for Topic Detection based on\n  Hierarchical Latent Tree Models Abstract: We propose a novel document generation process based on hierarchical latent\ntree models (HLTMs) learned from data. An HLTM has a layer of observed word\nvariables at the bottom and multiple layers of latent variables on top. For\neach document, we first sample values for the latent variables layer by layer\nvia logic sampling, then draw relative frequencies for the words conditioned on\nthe values of the latent variables, and finally generate words for the document\nusing the relative word frequencies. The motivation for the work is to take\nword counts into consideration with HLTMs. In comparison with LDA-based\nhierarchical document generation processes, the new process achieves\ndrastically better model fit with much fewer parameters. It also yields more\nmeaningful topics and topic hierarchies. It is the new state-of-the-art for the\nhierarchical topic detection. \n\n"}
{"id": "1712.05382", "contents": "Title: Monotonic Chunkwise Attention Abstract: Sequence-to-sequence models with soft attention have been successfully\napplied to a wide variety of problems, but their decoding process incurs a\nquadratic time and space cost and is inapplicable to real-time sequence\ntransduction. To address these issues, we propose Monotonic Chunkwise Attention\n(MoChA), which adaptively splits the input sequence into small chunks over\nwhich soft attention is computed. We show that models utilizing MoChA can be\ntrained efficiently with standard backpropagation while allowing online and\nlinear-time decoding at test time. When applied to online speech recognition,\nwe obtain state-of-the-art results and match the performance of a model using\nan offline soft attention mechanism. In document summarization experiments\nwhere we do not expect monotonic alignments, we show significantly improved\nperformance compared to a baseline monotonic attention-based model. \n\n"}
{"id": "1712.05403", "contents": "Title: Learning to Attend via Word-Aspect Associative Fusion for Aspect-based\n  Sentiment Analysis Abstract: Aspect-based sentiment analysis (ABSA) tries to predict the polarity of a\ngiven document with respect to a given aspect entity. While neural network\narchitectures have been successful in predicting the overall polarity of\nsentences, aspect-specific sentiment analysis still remains as an open problem.\nIn this paper, we propose a novel method for integrating aspect information\ninto the neural model. More specifically, we incorporate aspect information\ninto the neural model by modeling word-aspect relationships. Our novel model,\n\\textit{Aspect Fusion LSTM} (AF-LSTM) learns to attend based on associative\nrelationships between sentence words and aspect which allows our model to\nadaptively focus on the correct words given an aspect term. This ameliorates\nthe flaws of other state-of-the-art models that utilize naive concatenations to\nmodel word-aspect similarity. Instead, our model adopts circular convolution\nand circular correlation to model the similarity between aspect and words and\nelegantly incorporates this within a differentiable neural attention framework.\nFinally, our model is end-to-end differentiable and highly related to\nconvolution-correlation (holographic like) memories. Our proposed neural model\nachieves state-of-the-art performance on benchmark datasets, outperforming\nATAE-LSTM by $4\\%-5\\%$ on average across multiple datasets. \n\n"}
{"id": "1712.05690", "contents": "Title: Sockeye: A Toolkit for Neural Machine Translation Abstract: We describe Sockeye (version 1.12), an open-source sequence-to-sequence\ntoolkit for Neural Machine Translation (NMT). Sockeye is a production-ready\nframework for training and applying models as well as an experimental platform\nfor researchers. Written in Python and built on MXNet, the toolkit offers\nscalable training and inference for the three most prominent encoder-decoder\narchitectures: attentional recurrent neural networks, self-attentional\ntransformers, and fully convolutional networks. Sockeye also supports a wide\nrange of optimizers, normalization and regularization techniques, and inference\nimprovements from current NMT literature. Users can easily run standard\ntraining recipes, explore different model settings, and incorporate new ideas.\nIn this paper, we highlight Sockeye's features and benchmark it against other\nNMT toolkits on two language arcs from the 2017 Conference on Machine\nTranslation (WMT): English-German and Latvian-English. We report competitive\nBLEU scores across all three architectures, including an overall best score for\nSockeye's transformer implementation. To facilitate further comparison, we\nrelease all system outputs and training scripts used in our experiments. The\nSockeye toolkit is free software released under the Apache 2.0 license. \n\n"}
{"id": "1712.06289", "contents": "Title: A Chinese Dataset with Negative Full Forms for General Abbreviation\n  Prediction Abstract: Abbreviation is a common phenomenon across languages, especially in Chinese.\nIn most cases, if an expression can be abbreviated, its abbreviation is used\nmore often than its fully expanded forms, since people tend to convey\ninformation in a most concise way. For various language processing tasks,\nabbreviation is an obstacle to improving the performance, as the textual form\nof an abbreviation does not express useful information, unless it's expanded to\nthe full form. Abbreviation prediction means associating the fully expanded\nforms with their abbreviations. However, due to the deficiency in the\nabbreviation corpora, such a task is limited in current studies, especially\nconsidering general abbreviation prediction should also include those full form\nexpressions that do not have valid abbreviations, namely the negative full\nforms (NFFs). Corpora incorporating negative full forms for general\nabbreviation prediction are few in number. In order to promote the research in\nthis area, we build a dataset for general Chinese abbreviation prediction,\nwhich needs a few preprocessing steps, and evaluate several different models on\nthe built dataset. The dataset is available at\nhttps://github.com/lancopku/Chinese-abbreviation-dataset \n\n"}
{"id": "1712.07229", "contents": "Title: Attentive Memory Networks: Efficient Machine Reading for Conversational\n  Search Abstract: Recent advances in conversational systems have changed the search paradigm.\nTraditionally, a user poses a query to a search engine that returns an answer\nbased on its index, possibly leveraging external knowledge bases and\nconditioning the response on earlier interactions in the search session. In a\nnatural conversation, there is an additional source of information to take into\naccount: utterances produced earlier in a conversation can also be referred to\nand a conversational IR system has to keep track of information conveyed by the\nuser during the conversation, even if it is implicit.\n  We argue that the process of building a representation of the conversation\ncan be framed as a machine reading task, where an automated system is presented\nwith a number of statements about which it should answer questions. The\nquestions should be answered solely by referring to the statements provided,\nwithout consulting external knowledge. The time is right for the information\nretrieval community to embrace this task, both as a stand-alone task and\nintegrated in a broader conversational search setting.\n  In this paper, we focus on machine reading as a stand-alone task and present\nthe Attentive Memory Network (AMN), an end-to-end trainable machine reading\nalgorithm. Its key contribution is in efficiency, achieved by having an\nhierarchical input encoder, iterating over the input only once. Speed is an\nimportant requirement in the setting of conversational search, as gaps between\nconversational turns have a detrimental effect on naturalness. On 20 datasets\ncommonly used for evaluating machine reading algorithms we show that the AMN\nachieves performance comparable to the state-of-the-art models, while using\nconsiderably fewer computations. \n\n"}
{"id": "1712.07316", "contents": "Title: A Flexible Approach to Automated RNN Architecture Generation Abstract: The process of designing neural architectures requires expert knowledge and\nextensive trial and error. While automated architecture search may simplify\nthese requirements, the recurrent neural network (RNN) architectures generated\nby existing methods are limited in both flexibility and components. We propose\na domain-specific language (DSL) for use in automated architecture search which\ncan produce novel RNNs of arbitrary depth and width. The DSL is flexible enough\nto define standard architectures such as the Gated Recurrent Unit and Long\nShort Term Memory and allows the introduction of non-standard RNN components\nsuch as trigonometric curves and layer normalization. Using two different\ncandidate generation techniques, random search with a ranking function and\nreinforcement learning, we explore the novel architectures produced by the RNN\nDSL for language modeling and machine translation domains. The resulting\narchitectures do not follow human intuition yet perform well on their targeted\ntasks, suggesting the space of usable RNN architectures is far larger than\npreviously assumed. \n\n"}
{"id": "1712.07558", "contents": "Title: An Ensemble Model with Ranking for Social Dialogue Abstract: Open-domain social dialogue is one of the long-standing goals of Artificial\nIntelligence. This year, the Amazon Alexa Prize challenge was announced for the\nfirst time, where real customers get to rate systems developed by leading\nuniversities worldwide. The aim of the challenge is to converse \"coherently and\nengagingly with humans on popular topics for 20 minutes\". We describe our Alexa\nPrize system (called 'Alana') consisting of an ensemble of bots, combining\nrule-based and machine learning systems, and using a contextual ranking\nmechanism to choose a system response. The ranker was trained on real user\nfeedback received during the competition, where we address the problem of how\nto train on the noisy and sparse feedback obtained during the competition. \n\n"}
{"id": "1712.09687", "contents": "Title: Combining Representation Learning with Logic for Language Processing Abstract: The current state-of-the-art in many natural language processing and\nautomated knowledge base completion tasks is held by representation learning\nmethods which learn distributed vector representations of symbols via\ngradient-based optimization. They require little or no hand-crafted features,\nthus avoiding the need for most preprocessing steps and task-specific\nassumptions. However, in many cases representation learning requires a large\namount of annotated training data to generalize well to unseen data. Such\nlabeled training data is provided by human annotators who often use formal\nlogic as the language for specifying annotations. This thesis investigates\ndifferent combinations of representation learning methods with logic for\nreducing the need for annotated training data, and for improving\ngeneralization. \n\n"}
{"id": "1712.09783", "contents": "Title: Topic Compositional Neural Language Model Abstract: We propose a Topic Compositional Neural Language Model (TCNLM), a novel\nmethod designed to simultaneously capture both the global semantic meaning and\nthe local word ordering structure in a document. The TCNLM learns the global\nsemantic coherence of a document via a neural topic model, and the probability\nof each learned latent topic is further used to build a Mixture-of-Experts\n(MoE) language model, where each expert (corresponding to one topic) is a\nrecurrent neural network (RNN) that accounts for learning the local structure\nof a word sequence. In order to train the MoE model efficiently, a matrix\nfactorization method is applied, by extending each weight matrix of the RNN to\nbe an ensemble of topic-dependent weight matrices. The degree to which each\nmember of the ensemble is used is tied to the document-dependent probability of\nthe corresponding topics. Experimental results on several corpora show that the\nproposed approach outperforms both a pure RNN-based model and other\ntopic-guided language models. Further, our model yields sensible topics, and\nalso has the capacity to generate meaningful sentences conditioned on given\ntopics. \n\n"}
{"id": "1801.00132", "contents": "Title: Community Detection in Partially Observable Social Networks Abstract: The discovery of community structures in social networks has gained\nsignificant attention since it is a fundamental problem in understanding the\nnetworks' topology and functions. However, most social network data are\ncollected from partially observable networks with both missing nodes and edges.\nIn this paper, we address a new problem of detecting overlapping community\nstructures in the context of such an incomplete network, where communities in\nthe network are allowed to overlap since nodes belong to multiple communities\nat once. To solve this problem, we introduce KroMFac, a new framework that\nconducts community detection via regularized nonnegative matrix factorization\n(NMF) based on the Kronecker graph model. Specifically, from an inferred\nKronecker generative parameter matrix, we first estimate the missing part of\nthe network. As our major contribution to the proposed framework, to improve\ncommunity detection accuracy, we then characterize and select influential nodes\n(which tend to have high degrees) by ranking, and add them to the existing\ngraph. Finally, we uncover the community structures by solving the regularized\nNMF-aided optimization problem in terms of maximizing the likelihood of the\nunderlying graph. Furthermore, adopting normalized mutual information (NMI), we\nempirically show superiority of our KroMFac approach over two baseline schemes\nby using both synthetic and real-world networks. \n\n"}
{"id": "1801.02832", "contents": "Title: Biomedical Question Answering via Weighted Neural Network Passage\n  Retrieval Abstract: The amount of publicly available biomedical literature has been growing\nrapidly in recent years, yet question answering systems still struggle to\nexploit the full potential of this source of data. In a preliminary processing\nstep, many question answering systems rely on retrieval models for identifying\nrelevant documents and passages. This paper proposes a weighted cosine distance\nretrieval scheme based on neural network word embeddings. Our experiments are\nbased on publicly available data and tasks from the BioASQ biomedical question\nanswering challenge and demonstrate significant performance gains over a wide\nrange of state-of-the-art models. \n\n"}
{"id": "1801.03911", "contents": "Title: Stochastic Learning of Nonstationary Kernels for Natural Language\n  Modeling Abstract: Natural language processing often involves computations with semantic or\nsyntactic graphs to facilitate sophisticated reasoning based on structural\nrelationships. While convolution kernels provide a powerful tool for comparing\ngraph structure based on node (word) level relationships, they are difficult to\ncustomize and can be computationally expensive. We propose a generalization of\nconvolution kernels, with a nonstationary model, for better expressibility of\nnatural languages in supervised settings. For a scalable learning of the\nparameters introduced with our model, we propose a novel algorithm that\nleverages stochastic sampling on k-nearest neighbor graphs, along with\napproximations based on locality-sensitive hashing. We demonstrate the\nadvantages of our approach on a challenging real-world (structured inference)\nproblem of automatically extracting biological models from the text of\nscientific papers. \n\n"}
{"id": "1801.05420", "contents": "Title: A Comparative Study of Rule Extraction for Recurrent Neural Networks Abstract: Understanding recurrent networks through rule extraction has a long history.\nThis has taken on new interests due to the need for interpreting or verifying\nneural networks. One basic form for representing stateful rules is\ndeterministic finite automata (DFA). Previous research shows that extracting\nDFAs from trained second-order recurrent networks is not only possible but also\nrelatively stable. Recently, several new types of recurrent networks with more\ncomplicated architectures have been introduced. These handle challenging\nlearning tasks usually involving sequential data. However, it remains an open\nproblem whether DFAs can be adequately extracted from these models.\nSpecifically, it is not clear how DFA extraction will be affected when applied\nto different recurrent networks trained on data sets with different levels of\ncomplexity. Here, we investigate DFA extraction on several widely adopted\nrecurrent networks that are trained to learn a set of seven regular Tomita\ngrammars. We first formally analyze the complexity of Tomita grammars and\ncategorize these grammars according to that complexity. Then we empirically\nevaluate different recurrent networks for their performance of DFA extraction\non all Tomita grammars. Our experiments show that for most recurrent networks,\ntheir extraction performance decreases as the complexity of the underlying\ngrammar increases. On grammars of lower complexity, most recurrent networks\nobtain desirable extraction performance. As for grammars with the highest level\nof complexity, while several complicated models fail with only certain\nrecurrent networks having satisfactory extraction performance. \n\n"}
{"id": "1801.05852", "contents": "Title: Network Representation Learning: A Survey Abstract: With the widespread use of information technologies, information networks are\nbecoming increasingly popular to capture complex relationships across various\ndisciplines, such as social networks, citation networks, telecommunication\nnetworks, and biological networks. Analyzing these networks sheds light on\ndifferent aspects of social life such as the structure of societies,\ninformation diffusion, and communication patterns. In reality, however, the\nlarge scale of information networks often makes network analytic tasks\ncomputationally expensive or intractable. Network representation learning has\nbeen recently proposed as a new learning paradigm to embed network vertices\ninto a low-dimensional vector space, by preserving network topology structure,\nvertex content, and other side information. This facilitates the original\nnetwork to be easily handled in the new vector space for further analysis. In\nthis survey, we perform a comprehensive review of the current literature on\nnetwork representation learning in the data mining and machine learning field.\nWe propose new taxonomies to categorize and summarize the state-of-the-art\nnetwork representation learning techniques according to the underlying learning\nmechanisms, the network information intended to preserve, as well as the\nalgorithmic designs and methodologies. We summarize evaluation protocols used\nfor validating network representation learning including published benchmark\ndatasets, evaluation methods, and open source algorithms. We also perform\nempirical studies to compare the performance of representative algorithms on\ncommon datasets, and analyze their computational complexity. Finally, we\nsuggest promising research directions to facilitate future study. \n\n"}
{"id": "1801.06172", "contents": "Title: Contextual and Position-Aware Factorization Machines for Sentiment\n  Classification Abstract: While existing machine learning models have achieved great success for\nsentiment classification, they typically do not explicitly capture\nsentiment-oriented word interaction, which can lead to poor results for\nfine-grained analysis at the snippet level (a phrase or sentence).\nFactorization Machine provides a possible approach to learning element-wise\ninteraction for recommender systems, but they are not directly applicable to\nour task due to the inability to model contexts and word sequences. In this\nwork, we develop two Position-aware Factorization Machines which consider word\ninteraction, context and position information. Such information is jointly\nencoded in a set of sentiment-oriented word interaction vectors. Compared to\ntraditional word embeddings, SWI vectors explicitly capture sentiment-oriented\nword interaction and simplify the parameter learning. Experimental results show\nthat while they have comparable performance with state-of-the-art methods for\ndocument-level classification, they benefit the snippet/sentence-level\nsentiment analysis. \n\n"}
{"id": "1801.06287", "contents": "Title: What Does a TextCNN Learn? Abstract: TextCNN, the convolutional neural network for text, is a useful deep learning\nalgorithm for sentence classification tasks such as sentiment analysis and\nquestion classification. However, neural networks have long been known as black\nboxes because interpreting them is a challenging task. Researchers have\ndeveloped several tools to understand a CNN for image classification by deep\nvisualization, but research about deep TextCNNs is still insufficient. In this\npaper, we are trying to understand what a TextCNN learns on two classical NLP\ndatasets. Our work focuses on functions of different convolutional kernels and\ncorrelations between convolutional kernels. \n\n"}
{"id": "1801.06510", "contents": "Title: Image Provenance Analysis at Scale Abstract: Prior art has shown it is possible to estimate, through image processing and\ncomputer vision techniques, the types and parameters of transformations that\nhave been applied to the content of individual images to obtain new images.\nGiven a large corpus of images and a query image, an interesting further step\nis to retrieve the set of original images whose content is present in the query\nimage, as well as the detailed sequences of transformations that yield the\nquery image given the original images. This is a problem that recently has\nreceived the name of image provenance analysis. In these times of public media\nmanipulation ( e.g., fake news and meme sharing), obtaining the history of\nimage transformations is relevant for fact checking and authorship\nverification, among many other applications. This article presents an\nend-to-end processing pipeline for image provenance analysis, which works at\nreal-world scale. It employs a cutting-edge image filtering solution that is\ncustom-tailored for the problem at hand, as well as novel techniques for\nobtaining the provenance graph that expresses how the images, as nodes, are\nancestrally connected. A comprehensive set of experiments for each stage of the\npipeline is provided, comparing the proposed solution with state-of-the-art\nresults, employing previously published datasets. In addition, this work\nintroduces a new dataset of real-world provenance cases from the social media\nsite Reddit, along with baseline results. \n\n"}
{"id": "1801.07311", "contents": "Title: Early Detection of Social Media Hoaxes at Scale Abstract: The unmoderated nature of social media enables the diffusion of hoaxes, which\nin turn jeopardises the credibility of information gathered from social media\nplatforms. Existing research on automated detection of hoaxes has the\nlimitation of using relatively small datasets, owing to the difficulty of\ngetting labelled data. This in turn has limited research exploring early\ndetection of hoaxes as well as exploring other factors such as the effect of\nthe size of the training data or the use of sliding windows. To mitigate this\nproblem, we introduce a semi-automated method that leverages the Wikidata\nknowledge base to build large-scale datasets for veracity classification,\nfocusing on celebrity death reports. This enables us to create a dataset with\n4,007 reports including over 13 million tweets, 15% of which are fake.\nExperiments using class-specific representations of word embeddings show that\nwe can achieve F1 scores nearing 72% within 10 minutes of the first tweet being\nposted when we expand the size of the training data following our\nsemi-automated means. Our dataset represents a realistic scenario with a real\ndistribution of true, commemorative and false stories, which we release for\nfurther use as a benchmark in future research. \n\n"}
{"id": "1801.10253", "contents": "Title: The New Modality: Emoji Challenges in Prediction, Anticipation, and\n  Retrieval Abstract: Over the past decade, emoji have emerged as a new and widespread form of\ndigital communication, spanning diverse social networks and spoken languages.\nWe propose to treat these ideograms as a new modality in their own right,\ndistinct in their semantic structure from both the text in which they are often\nembedded as well as the images which they resemble. As a new modality, emoji\npresent rich novel possibilities for representation and interaction. In this\npaper, we explore the challenges that arise naturally from considering the\nemoji modality through the lens of multimedia research. Specifically, the ways\nin which emoji can be related to other common modalities such as text and\nimages. To do so, we first present a large scale dataset of real-world emoji\nusage collected from Twitter. This dataset contains examples of both text-emoji\nand image-emoji relationships. We present baseline results on the challenge of\npredicting emoji from both text and images, using state-of-the-art neural\nnetworks. Further, we offer a first consideration into the problem of how to\naccount for new, unseen emoji - a relevant issue as the emoji vocabulary\ncontinues to expand on a yearly basis. Finally, we present results for\nmultimedia retrieval using emoji as queries. \n\n"}
{"id": "1801.10296", "contents": "Title: Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention\n  for Sequence Modeling Abstract: Many natural language processing tasks solely rely on sparse dependencies\nbetween a few tokens in a sentence. Soft attention mechanisms show promising\nperformance in modeling local/global dependencies by soft probabilities between\nevery two tokens, but they are not effective and efficient when applied to long\nsentences. By contrast, hard attention mechanisms directly select a subset of\ntokens but are difficult and inefficient to train due to their combinatorial\nnature. In this paper, we integrate both soft and hard attention into one\ncontext fusion model, \"reinforced self-attention (ReSA)\", for the mutual\nbenefit of each other. In ReSA, a hard attention trims a sequence for a soft\nself-attention to process, while the soft attention feeds reward signals back\nto facilitate the training of the hard one. For this purpose, we develop a\nnovel hard attention called \"reinforced sequence sampling (RSS)\", selecting\ntokens in parallel and trained via policy gradient. Using two RSS modules, ReSA\nefficiently extracts the sparse dependencies between each pair of selected\ntokens. We finally propose an RNN/CNN-free sentence-encoding model, \"reinforced\nself-attention network (ReSAN)\", solely based on ReSA. It achieves\nstate-of-the-art performance on both Stanford Natural Language Inference (SNLI)\nand Sentences Involving Compositional Knowledge (SICK) datasets. \n\n"}
{"id": "1801.10308", "contents": "Title: Nested LSTMs Abstract: We propose Nested LSTMs (NLSTM), a novel RNN architecture with multiple\nlevels of memory. Nested LSTMs add depth to LSTMs via nesting as opposed to\nstacking. The value of a memory cell in an NLSTM is computed by an LSTM cell,\nwhich has its own inner memory cell. Specifically, instead of computing the\nvalue of the (outer) memory cell as $c^{outer}_t = f_t \\odot c_{t-1} + i_t\n\\odot g_t$, NLSTM memory cells use the concatenation $(f_t \\odot c_{t-1}, i_t\n\\odot g_t)$ as input to an inner LSTM (or NLSTM) memory cell, and set\n$c^{outer}_t$ = $h^{inner}_t$. Nested LSTMs outperform both stacked and\nsingle-layer LSTMs with similar numbers of parameters in our experiments on\nvarious character-level language modeling tasks, and the inner memories of an\nLSTM learn longer term dependencies compared with the higher-level units of a\nstacked LSTM. \n\n"}
{"id": "1802.01766", "contents": "Title: Question-Answer Selection in User to User Marketplace Conversations Abstract: Sellers in user to user marketplaces can be inundated with questions from\npotential buyers. Answers are often already available in the product\ndescription. We collected a dataset of around 590K such questions and answers\nfrom conversations in an online marketplace. We propose a question answering\nsystem that selects a sentence from the product description using a\nneural-network ranking model. We explore multiple encoding strategies, with\nrecurrent neural networks and feed-forward attention layers yielding good\nresults. This paper presents a demo to interactively pose buyer questions and\nvisualize the ranking scores of product description sentences from live online\nlistings. \n\n"}
{"id": "1802.01886", "contents": "Title: Texygen: A Benchmarking Platform for Text Generation Models Abstract: We introduce Texygen, a benchmarking platform to support research on\nopen-domain text generation models. Texygen has not only implemented a majority\nof text generation models, but also covered a set of metrics that evaluate the\ndiversity, the quality and the consistency of the generated texts. The Texygen\nplatform could help standardize the research on text generation and facilitate\nthe sharing of fine-tuned open-source implementations among researchers for\ntheir work. As a consequence, this would help in improving the reproductivity\nand reliability of future research work in text generation. \n\n"}
{"id": "1802.03268", "contents": "Title: Efficient Neural Architecture Search via Parameter Sharing Abstract: We propose Efficient Neural Architecture Search (ENAS), a fast and\ninexpensive approach for automatic model design. In ENAS, a controller learns\nto discover neural network architectures by searching for an optimal subgraph\nwithin a large computational graph. The controller is trained with policy\ngradient to select a subgraph that maximizes the expected reward on the\nvalidation set. Meanwhile the model corresponding to the selected subgraph is\ntrained to minimize a canonical cross entropy loss. Thanks to parameter sharing\nbetween child models, ENAS is fast: it delivers strong empirical performances\nusing much fewer GPU-hours than all existing automatic model design approaches,\nand notably, 1000x less expensive than standard Neural Architecture Search. On\nthe Penn Treebank dataset, ENAS discovers a novel architecture that achieves a\ntest perplexity of 55.8, establishing a new state-of-the-art among all methods\nwithout post-training processing. On the CIFAR-10 dataset, ENAS designs novel\narchitectures that achieve a test error of 2.89%, which is on par with NASNet\n(Zoph et al., 2018), whose test error is 2.65%. \n\n"}
{"id": "1802.03725", "contents": "Title: A Generative Model for Dynamic Networks with Applications Abstract: Networks observed in real world like social networks, collaboration networks\netc., exhibit temporal dynamics, i.e. nodes and edges appear and/or disappear\nover time. In this paper, we propose a generative, latent space based,\nstatistical model for such networks (called dynamic networks). We consider the\ncase where the number of nodes is fixed, but the presence of edges can vary\nover time. Our model allows the number of communities in the network to be\ndifferent at different time steps. We use a neural network based methodology to\nperform approximate inference in the proposed model and its simplified version.\nExperiments done on synthetic and real world networks for the task of community\ndetection and link prediction demonstrate the utility and effectiveness of our\nmodel as compared to other similar existing approaches. \n\n"}
{"id": "1802.03753", "contents": "Title: Sample Efficient Deep Reinforcement Learning for Dialogue Systems with\n  Large Action Spaces Abstract: In spoken dialogue systems, we aim to deploy artificial intelligence to build\nautomated dialogue agents that can converse with humans. A part of this effort\nis the policy optimisation task, which attempts to find a policy describing how\nto respond to humans, in the form of a function taking the current state of the\ndialogue and returning the response of the system. In this paper, we\ninvestigate deep reinforcement learning approaches to solve this problem.\nParticular attention is given to actor-critic methods, off-policy reinforcement\nlearning with experience replay, and various methods aimed at reducing the bias\nand variance of estimators. When combined, these methods result in the\npreviously proposed ACER algorithm that gave competitive results in gaming\nenvironments. These environments however are fully observable and have a\nrelatively small action set so in this paper we examine the application of ACER\nto dialogue policy optimisation. We show that this method beats the current\nstate-of-the-art in deep learning approaches for spoken dialogue systems. This\nnot only leads to a more sample efficient algorithm that can train faster, but\nalso allows us to apply the algorithm in more difficult environments than\nbefore. We thus experiment with learning in a very large action space, which\nhas two orders of magnitude more actions than previously considered. We find\nthat ACER trains significantly faster than the current state-of-the-art. \n\n"}
{"id": "1802.05415", "contents": "Title: Teaching Machines to Code: Neural Markup Generation with Visual\n  Attention Abstract: We present a neural transducer model with visual attention that learns to\ngenerate LaTeX markup of a real-world math formula given its image. Applying\nsequence modeling and transduction techniques that have been very successful\nacross modalities such as natural language, image, handwriting, speech and\naudio; we construct an image-to-markup model that learns to produce\nsyntactically and semantically correct LaTeX markup code over 150 words long\nand achieves a BLEU score of 89%; improving upon the previous state-of-art for\nthe Im2Latex problem. We also demonstrate with heat-map visualization how\nattention helps in interpreting the model and can pinpoint (detect and\nlocalize) symbols on the image accurately despite having been trained without\nany bounding box data. \n\n"}
{"id": "1802.06159", "contents": "Title: Ad Hoc Table Retrieval using Semantic Similarity Abstract: We introduce and address the problem of ad hoc table retrieval: answering a\nkeyword query with a ranked list of tables. This task is not only interesting\non its own account, but is also being used as a core component in many other\ntable-based information access scenarios, such as table completion or table\nmining. The main novel contribution of this work is a method for performing\nsemantic matching between queries and tables. Specifically, we (i) represent\nqueries and tables in multiple semantic spaces (both discrete sparse and\ncontinuous dense vector representations) and (ii) introduce various similarity\nmeasures for matching those semantic representations. We consider all possible\ncombinations of semantic representations and similarity measures and use these\nas features in a supervised learning model. Using a purpose-built test\ncollection based on Wikipedia tables, we demonstrate significant and\nsubstantial improvements over a state-of-the-art baseline. \n\n"}
{"id": "1802.06398", "contents": "Title: HybridSVD: When Collaborative Information is Not Enough Abstract: We propose a new hybrid algorithm that allows incorporating both user and\nitem side information within the standard collaborative filtering technique.\nOne of its key features is that it naturally extends a simple PureSVD approach\nand inherits its unique advantages, such as highly efficient Lanczos-based\noptimization procedure, simplified hyper-parameter tuning and a quick\nfolding-in computation for generating recommendations instantly even in highly\ndynamic online environments. The algorithm utilizes a generalized formulation\nof the singular value decomposition, which adds flexibility to the solution and\nallows imposing the desired structure on its latent space. Conveniently, the\nresulting model also admits an efficient and straightforward solution for the\ncold start scenario. We evaluate our approach on a diverse set of datasets and\nshow its superiority over similar classes of hybrid models. \n\n"}
{"id": "1802.06428", "contents": "Title: Improving Mild Cognitive Impairment Prediction via Reinforcement\n  Learning and Dialogue Simulation Abstract: Mild cognitive impairment (MCI) is a prodromal phase in the progression from\nnormal aging to dementia, especially Alzheimers disease. Even though there is\nmild cognitive decline in MCI patients, they have normal overall cognition and\nthus is challenging to distinguish from normal aging. Using transcribed data\nobtained from recorded conversational interactions between participants and\ntrained interviewers, and applying supervised learning models to these data, a\nrecent clinical trial has shown a promising result in differentiating MCI from\nnormal aging. However, the substantial amount of interactions with medical\nstaff can still incur significant medical care expenses in practice. In this\npaper, we propose a novel reinforcement learning (RL) framework to train an\nefficient dialogue agent on existing transcripts from clinical trials.\nSpecifically, the agent is trained to sketch disease-specific lexical\nprobability distribution, and thus to converse in a way that maximizes the\ndiagnosis accuracy and minimizes the number of conversation turns. We evaluate\nthe performance of the proposed reinforcement learning framework on the MCI\ndiagnosis from a real clinical trial. The results show that while using only a\nfew turns of conversation, our framework can significantly outperform\nstate-of-the-art supervised learning approaches. \n\n"}
{"id": "1802.07244", "contents": "Title: Steering Social Activity: A Stochastic Optimal Control Point Of View Abstract: User engagement in online social networking depends critically on the level\nof social activity in the corresponding platform--the number of online actions,\nsuch as posts, shares or replies, taken by their users. Can we design\ndata-driven algorithms to increase social activity? At a user level, such\nalgorithms may increase activity by helping users decide when to take an action\nto be more likely to be noticed by their peers. At a network level, they may\nincrease activity by incentivizing a few influential users to take more\nactions, which in turn will trigger additional actions by other users. In this\npaper, we model social activity using the framework of marked temporal point\nprocesses, derive an alternate representation of these processes using\nstochastic differential equations (SDEs) with jumps and, exploiting this\nalternate representation, develop two efficient online algorithms with provable\nguarantees to steer social activity both at a user and at a network level. In\ndoing so, we establish a previously unexplored connection between optimal\ncontrol of jump SDEs and doubly stochastic marked temporal point processes,\nwhich is of independent interest. Finally, we experiment both with synthetic\nand real data gathered from Twitter and show that our algorithms consistently\nsteer social activity more effectively than the state of the art. \n\n"}
{"id": "1802.07860", "contents": "Title: Neural Predictive Coding using Convolutional Neural Networks towards\n  Unsupervised Learning of Speaker Characteristics Abstract: Learning speaker-specific features is vital in many applications like speaker\nrecognition, diarization and speech recognition. This paper provides a novel\napproach, we term Neural Predictive Coding (NPC), to learn speaker-specific\ncharacteristics in a completely unsupervised manner from large amounts of\nunlabeled training data that even contain many non-speech events and\nmulti-speaker audio streams. The NPC framework exploits the proposed short-term\nactive-speaker stationarity hypothesis which assumes two temporally-close short\nspeech segments belong to the same speaker, and thus a common representation\nthat can encode the commonalities of both the segments, should capture the\nvocal characteristics of that speaker. We train a convolutional deep siamese\nnetwork to produce \"speaker embeddings\" by learning to separate `same' vs\n`different' speaker pairs which are generated from an unlabeled data of audio\nstreams. Two sets of experiments are done in different scenarios to evaluate\nthe strength of NPC embeddings and compare with state-of-the-art in-domain\nsupervised methods. First, two speaker identification experiments with\ndifferent context lengths are performed in a scenario with comparatively\nlimited within-speaker channel variability. NPC embeddings are found to perform\nthe best at short duration experiment, and they provide complementary\ninformation to i-vectors for full utterance experiments. Second, a large scale\nspeaker verification task having a wide range of within-speaker channel\nvariability is adopted as an upper-bound experiment where comparisons are drawn\nwith in-domain supervised methods. \n\n"}
{"id": "1802.08452", "contents": "Title: Sequence-Aware Recommender Systems Abstract: Recommender systems are one of the most successful applications of data\nmining and machine learning technology in practice. Academic research in the\nfield is historically often based on the matrix completion problem formulation,\nwhere for each user-item-pair only one interaction (e.g., a rating) is\nconsidered. In many application domains, however, multiple user-item\ninteractions of different types can be recorded over time. And, a number of\nrecent works have shown that this information can be used to build richer\nindividual user models and to discover additional behavioral patterns that can\nbe leveraged in the recommendation process. In this work we review existing\nworks that consider information from such sequentially-ordered user- item\ninteraction logs in the recommendation process. Based on this review, we\npropose a categorization of the corresponding recommendation tasks and goals,\nsummarize existing algorithmic solutions, discuss methodological approaches\nwhen benchmarking what we call sequence-aware recommender systems, and outline\nopen challenges in the area. \n\n"}
{"id": "1802.08949", "contents": "Title: OhioState at SemEval-2018 Task 7: Exploiting Data Augmentation for\n  Relation Classification in Scientific Papers using Piecewise Convolutional\n  Neural Networks Abstract: We describe our system for SemEval-2018 Shared Task on Semantic Relation\nExtraction and Classification in Scientific Papers where we focus on the\nClassification task. Our simple piecewise convolution neural encoder performs\ndecently in an end to end manner. A simple inter-task data augmentation\nsignifi- cantly boosts the performance of the model. Our best-performing\nsystems stood 8th out of 20 teams on the classification task on noisy data and\n12th out of 28 teams on the classification task on clean data. \n\n"}
{"id": "1802.09059", "contents": "Title: One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation\n  of Text Data Abstract: Due to recent technical and scientific advances, we have a wealth of\ninformation hidden in unstructured text data such as offline/online narratives,\nresearch articles, and clinical reports. To mine these data properly,\nattributable to their innate ambiguity, a Word Sense Disambiguation (WSD)\nalgorithm can avoid numbers of difficulties in Natural Language Processing\n(NLP) pipeline. However, considering a large number of ambiguous words in one\nlanguage or technical domain, we may encounter limiting constraints for proper\ndeployment of existing WSD models. This paper attempts to address the problem\nof one-classifier-per-one-word WSD algorithms by proposing a single\nBidirectional Long Short-Term Memory (BLSTM) network which by considering\nsenses and context sequences works on all ambiguous words collectively.\nEvaluated on SensEval-3 benchmark, we show the result of our model is\ncomparable with top-performing WSD algorithms. We also discuss how applying\nadditional modifications alleviates the model fault and the need for more\ntraining data. \n\n"}
{"id": "1802.09944", "contents": "Title: The Development of Darwin's Origin of Species Abstract: From 1837, when he returned to England aboard the $\\textit{HMS Beagle}$, to\n1860, just after publication of $\\textit{The Origin of Species}$, Charles\nDarwin kept detailed notes of each book he read or wanted to read. His notes\nand manuscripts provide information about decades of individual scientific\npractice. Previously, we trained topic models on the full texts of each\nreading, and applied information-theoretic measures to detect that changes in\nhis reading patterns coincided with the boundaries of his three major\nintellectual projects in the period 1837-1860. In this new work we apply the\nreading model to five additional documents, four of them by Darwin: the first\nedition of $\\textit{The Origin of Species}$, two private essays stating\nintermediate forms of his theory in 1842 and 1844, a third essay of disputed\ndating, and Alfred Russel Wallace's essay, which Darwin received in 1858. We\naddress three historical inquiries, previously treated qualitatively: 1) the\nmythology of \"Darwin's Delay,\" that despite completing an extensive draft in\n1844, Darwin waited until 1859 to publish $\\textit{The Origin of Species}$ due\nto external pressures; 2) the relationship between Darwin and Wallace's\ncontemporaneous theories, especially in light of their joint presentation; and\n3) dating of the \"Outline and Draft\" which was rediscovered in 1975 and\npostulated first as an 1839 draft preceding the Sketch of 1842, then as an\ninterstitial draft between the 1842 and 1844 essays. \n\n"}
{"id": "1802.10411", "contents": "Title: Distance entropy cartography characterises centrality in complex\n  networks Abstract: We introduce distance entropy as a measure of homogeneity in the distribution\nof path lengths between a given node and its neighbours in a complex network.\nDistance entropy defines a new centrality measure whose properties are\ninvestigated for a variety of synthetic network models. By coupling distance\nentropy information with closeness centrality, we introduce a network\ncartography which allows one to reduce the degeneracy of ranking based on\ncloseness alone. We apply this methodology to the empirical multiplex lexical\nnetwork encoding the linguistic relationships known to English speaking\ntoddlers. We show that the distance entropy cartography better predicts how\nchildren learn words compared to closeness centrality. Our results highlight\nthe importance of distance entropy for gaining insights from distance patterns\nin complex networks. \n\n"}
{"id": "1803.01090", "contents": "Title: On Modular Training of Neural Acoustics-to-Word Model for LVCSR Abstract: End-to-end (E2E) automatic speech recognition (ASR) systems directly map\nacoustics to words using a unified model. Previous works mostly focus on E2E\ntraining a single model which integrates acoustic and language model into a\nwhole. Although E2E training benefits from sequence modeling and simplified\ndecoding pipelines, large amount of transcribed acoustic data is usually\nrequired, and traditional acoustic and language modelling techniques cannot be\nutilized. In this paper, a novel modular training framework of E2E ASR is\nproposed to separately train neural acoustic and language models during\ntraining stage, while still performing end-to-end inference in decoding stage.\nHere, an acoustics-to-phoneme model (A2P) and a phoneme-to-word model (P2W) are\ntrained using acoustic data and text data respectively. A phone synchronous\ndecoding (PSD) module is inserted between A2P and P2W to reduce sequence\nlengths without precision loss. Finally, modules are integrated into an\nacousticsto-word model (A2W) and jointly optimized using acoustic data to\nretain the advantage of sequence modeling. Experiments on a 300- hour\nSwitchboard task show significant improvement over the direct A2W model. The\nefficiency in both training and decoding also benefits from the proposed\nmethod. \n\n"}
{"id": "1803.01848", "contents": "Title: AspEm: Embedding Learning by Aspects in Heterogeneous Information\n  Networks Abstract: Heterogeneous information networks (HINs) are ubiquitous in real-world\napplications. Due to the heterogeneity in HINs, the typed edges may not fully\nalign with each other. In order to capture the semantic subtlety, we propose\nthe concept of aspects with each aspect being a unit representing one\nunderlying semantic facet. Meanwhile, network embedding has emerged as a\npowerful method for learning network representation, where the learned\nembedding can be used as features in various downstream applications.\nTherefore, we are motivated to propose a novel embedding learning\nframework---AspEm---to preserve the semantic information in HINs based on\nmultiple aspects. Instead of preserving information of the network in one\nsemantic space, AspEm encapsulates information regarding each aspect\nindividually. In order to select aspects for embedding purpose, we further\ndevise a solution for AspEm based on dataset-wide statistics. To corroborate\nthe efficacy of AspEm, we conducted experiments on two real-words datasets with\ntwo types of applications---classification and link prediction. Experiment\nresults demonstrate that AspEm can outperform baseline network embedding\nlearning methods by considering multiple aspects, where the aspects can be\nselected from the given HIN in an unsupervised manner. \n\n"}
{"id": "1803.02551", "contents": "Title: Extracting Domain Invariant Features by Unsupervised Learning for Robust\n  Automatic Speech Recognition Abstract: The performance of automatic speech recognition (ASR) systems can be\nsignificantly compromised by previously unseen conditions, which is typically\ndue to a mismatch between training and testing distributions. In this paper, we\naddress robustness by studying domain invariant features, such that domain\ninformation becomes transparent to ASR systems, resolving the mismatch problem.\nSpecifically, we investigate a recent model, called the Factorized Hierarchical\nVariational Autoencoder (FHVAE). FHVAEs learn to factorize sequence-level and\nsegment-level attributes into different latent variables without supervision.\nWe argue that the set of latent variables that contain segment-level\ninformation is our desired domain invariant feature for ASR. Experiments are\nconducted on Aurora-4 and CHiME-4, which demonstrate 41% and 27% absolute word\nerror rate reductions respectively on mismatched domains. \n\n"}
{"id": "1803.03018", "contents": "Title: Cross-domain Recommendation via Deep Domain Adaptation Abstract: The behavior of users in certain services could be a clue that can be used to\ninfer their preferences and may be used to make recommendations for other\nservices they have never used. However, the cross-domain relationships between\nitems and user consumption patterns are not simple, especially when there are\nfew or no common users and items across domains. To address this problem, we\npropose a content-based cross-domain recommendation method for cold-start users\nthat does not require user- and item- overlap. We formulate recommendation as\nextreme multi-class classification where labels (items) corresponding to the\nusers are predicted. With this formulation, the problem is reduced to a domain\nadaptation setting, in which a classifier trained in the source domain is\nadapted to the target domain. For this, we construct a neural network that\ncombines an architecture for domain adaptation, Domain Separation Network, with\na denoising autoencoder for item representation. We assess the performance of\nour approach in experiments on a pair of data sets collected from movie and\nnews services of Yahoo! JAPAN and show that our approach outperforms several\nbaseline methods including a cross-domain collaborative filtering method. \n\n"}
{"id": "1803.03185", "contents": "Title: Drug Recommendation toward Safe Polypharmacy Abstract: Adverse drug reactions (ADRs) induced from high-order drug-drug interactions\n(DDIs) due to polypharmacy represent a significant public health problem. In\nthis paper, we formally formulate the to-avoid and safe (with respect to ADRs)\ndrug recommendation problems when multiple drugs have been taken\nsimultaneously. We develop a joint model with a recommendation component and an\nADR label prediction component to recommend for a prescription a set of\nto-avoid drugs that will induce ADRs if taken together with the prescription.\nWe also develop real drug-drug interaction datasets and corresponding\nevaluation protocols. Our experimental results on real datasets demonstrate the\nstrong performance of the joint model compared to other baseline methods. \n\n"}
{"id": "1803.03376", "contents": "Title: Learning Approximate Inference Networks for Structured Prediction Abstract: Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use\nneural network architectures to define energy functions that can capture\narbitrary dependencies among parts of structured outputs. Prior work used\ngradient descent for inference, relaxing the structured output to a set of\ncontinuous variables and then optimizing the energy with respect to them. We\nreplace this use of gradient descent with a neural network trained to\napproximate structured argmax inference. This \"inference network\" outputs\ncontinuous values that we treat as the output structure. We develop\nlarge-margin training criteria for joint training of the structured energy\nfunction and inference network. On multi-label classification we report\nspeed-ups of 10-60x compared to (Belanger et al, 2017) while also improving\naccuracy. For sequence labeling with simple structured energies, our approach\nperforms comparably to exact inference while being much faster at test time. We\nthen demonstrate improved accuracy by augmenting the energy with a \"label\nlanguage model\" that scores entire output label sequences, showing it can\nimprove handling of long-distance dependencies in part-of-speech tagging.\nFinally, we show how inference networks can replace dynamic programming for\ntest-time inference in conditional random fields, suggestive for their general\nuse for fast inference in structured settings. \n\n"}
{"id": "1803.03665", "contents": "Title: Syntax-Aware Language Modeling with Recurrent Neural Networks Abstract: Neural language models (LMs) are typically trained using only lexical\nfeatures, such as surface forms of words. In this paper, we argue this deprives\nthe LM of crucial syntactic signals that can be detected at high confidence\nusing existing parsers. We present a simple but highly effective approach for\ntraining neural LMs using both lexical and syntactic information, and a novel\napproach for applying such LMs to unparsed text using sequential Monte Carlo\nsampling. In experiments on a range of corpora and corpus sizes, we show our\napproach consistently outperforms standard lexical LMs in character-level\nlanguage modeling; on the other hand, for word-level models the models are on a\npar with standard language models. These results indicate potential for\nexpanding LMs beyond lexical surface features to higher-level NLP features for\ncharacter-level models. \n\n"}
{"id": "1803.04715", "contents": "Title: Hierarchical Learning of Cross-Language Mappings through Distributed\n  Vector Representations for Code Abstract: Translating a program written in one programming language to another can be\nuseful for software development tasks that need functionality implementations\nin different languages. Although past studies have considered this problem,\nthey may be either specific to the language grammars, or specific to certain\nkinds of code elements (e.g., tokens, phrases, API uses). This paper proposes a\nnew approach to automatically learn cross-language representations for various\nkinds of structural code elements that may be used for program translation. Our\nkey idea is two folded: First, we normalize and enrich code token streams with\nadditional structural and semantic information, and train cross-language vector\nrepresentations for the tokens (a.k.a. shared embeddings based on word2vec, a\nneural-network-based technique for producing word embeddings; Second,\nhierarchically from bottom up, we construct shared embeddings for code elements\nof higher levels of granularity (e.g., expressions, statements, methods) from\nthe embeddings for their constituents, and then build mappings among code\nelements across languages based on similarities among embeddings.\n  Our preliminary evaluations on about 40,000 Java and C# source files from 9\nsoftware projects show that our approach can automatically learn shared\nembeddings for various code elements in different languages and identify their\ncross-language mappings with reasonable Mean Average Precision scores. When\ncompared with an existing tool for mapping library API methods, our approach\nidentifies many more mappings accurately. The mapping results and code can be\naccessed at\nhttps://github.com/bdqnghi/hierarchical-programming-language-mapping. We\nbelieve that our idea for learning cross-language vector representations with\ncode structural information can be a useful step towards automated program\ntranslation. \n\n"}
{"id": "1803.04742", "contents": "Title: VERSE: Versatile Graph Embeddings from Similarity Measures Abstract: Embedding a web-scale information network into a low-dimensional vector space\nfacilitates tasks such as link prediction, classification, and visualization.\nPast research has addressed the problem of extracting such embeddings by\nadopting methods from words to graphs, without defining a clearly\ncomprehensible graph-related objective. Yet, as we show, the objectives used in\npast works implicitly utilize similarity measures among graph nodes.\n  In this paper, we carry the similarity orientation of previous works to its\nlogical conclusion; we propose VERtex Similarity Embeddings (VERSE), a simple,\nversatile, and memory-efficient method that derives graph embeddings explicitly\ncalibrated to preserve the distributions of a selected vertex-to-vertex\nsimilarity measure. VERSE learns such embeddings by training a single-layer\nneural network. While its default, scalable version does so via sampling\nsimilarity information, we also develop a variant using the full information\nper vertex. Our experimental study on standard benchmarks and real-world\ndatasets demonstrates that VERSE, instantiated with diverse similarity\nmeasures, outperforms state-of-the-art methods in terms of precision and recall\nin major data mining tasks and supersedes them in time and space efficiency,\nwhile the scalable sampling-based variant achieves equally good results as the\nnon-scalable full variant. \n\n"}
{"id": "1803.04831", "contents": "Title: Independently Recurrent Neural Network (IndRNN): Building A Longer and\n  Deeper RNN Abstract: Recurrent neural networks (RNNs) have been widely used for processing\nsequential data. However, RNNs are commonly difficult to train due to the\nwell-known gradient vanishing and exploding problems and hard to learn\nlong-term patterns. Long short-term memory (LSTM) and gated recurrent unit\n(GRU) were developed to address these problems, but the use of hyperbolic\ntangent and the sigmoid action functions results in gradient decay over layers.\nConsequently, construction of an efficiently trainable deep network is\nchallenging. In addition, all the neurons in an RNN layer are entangled\ntogether and their behaviour is hard to interpret. To address these problems, a\nnew type of RNN, referred to as independently recurrent neural network\n(IndRNN), is proposed in this paper, where neurons in the same layer are\nindependent of each other and they are connected across layers. We have shown\nthat an IndRNN can be easily regulated to prevent the gradient exploding and\nvanishing problems while allowing the network to learn long-term dependencies.\nMoreover, an IndRNN can work with non-saturated activation functions such as\nrelu (rectified linear unit) and be still trained robustly. Multiple IndRNNs\ncan be stacked to construct a network that is deeper than the existing RNNs.\nExperimental results have shown that the proposed IndRNN is able to process\nvery long sequences (over 5000 time steps), can be used to construct very deep\nnetworks (21 layers used in the experiment) and still be trained robustly.\nBetter performances have been achieved on various tasks by using IndRNNs\ncompared with the traditional RNN and LSTM. The code is available at\nhttps://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne. \n\n"}
{"id": "1803.04884", "contents": "Title: IDEL: In-Database Entity Linking with Neural Embeddings Abstract: We present a novel architecture, In-Database Entity Linking (IDEL), in which\nwe integrate the analytics-optimized RDBMS MonetDB with neural text mining\nabilities. Our system design abstracts core tasks of most neural entity linking\nsystems for MonetDB. To the best of our knowledge, this is the first defacto\nimplemented system integrating entity-linking in a database. We leverage the\nability of MonetDB to support in-database-analytics with user defined functions\n(UDFs) implemented in Python. These functions call machine learning libraries\nfor neural text mining, such as TensorFlow. The system achieves zero cost for\ndata shipping and transformation by utilizing MonetDB's ability to embed Python\nprocesses in the database kernel and exchange data in NumPy arrays. IDEL\nrepresents text and relational data in a joint vector space with neural\nembeddings and can compensate errors with ambiguous entity representations. For\ndetecting matching entities, we propose a novel similarity function based on\njoint neural embeddings which are learned via minimizing pairwise contrastive\nranking loss. This function utilizes a high dimensional index structures for\nfast retrieval of matching entities. Our first implementation and experiments\nusing the WebNLG corpus show the effectiveness and the potentials of IDEL. \n\n"}
{"id": "1803.05573", "contents": "Title: Improving GANs Using Optimal Transport Abstract: We present Optimal Transport GAN (OT-GAN), a variant of generative\nadversarial nets minimizing a new metric measuring the distance between the\ngenerator distribution and the data distribution. This metric, which we call\nmini-batch energy distance, combines optimal transport in primal form with an\nenergy distance defined in an adversarially learned feature space, resulting in\na highly discriminative distance function with unbiased mini-batch gradients.\nExperimentally we show OT-GAN to be highly stable when trained with large\nmini-batches, and we present state-of-the-art results on several popular\nbenchmark problems for image generation. \n\n"}
{"id": "1803.06581", "contents": "Title: Variational Knowledge Graph Reasoning Abstract: Inferring missing links in knowledge graphs (KG) has attracted a lot of\nattention from the research community. In this paper, we tackle a practical\nquery answering task involving predicting the relation of a given entity pair.\nWe frame this prediction problem as an inference problem in a probabilistic\ngraphical model and aim at resolving it from a variational inference\nperspective. In order to model the relation between the query entity pair, we\nassume that there exists an underlying latent variable (paths connecting two\nnodes) in the KG, which carries the equivalent semantics of their relations.\nHowever, due to the intractability of connections in large KGs, we propose to\nuse variation inference to maximize the evidence lower bound. More\nspecifically, our framework (\\textsc{Diva}) is composed of three modules, i.e.\na posterior approximator, a prior (path finder), and a likelihood (path\nreasoner). By using variational inference, we are able to incorporate them\nclosely into a unified architecture and jointly optimize them to perform KG\nreasoning. With active interactions among these sub-modules, \\textsc{Diva} is\nbetter at handling noise and coping with more complex reasoning scenarios. In\norder to evaluate our method, we conduct the experiment of the link prediction\ntask on multiple datasets and achieve state-of-the-art performances on both\ndatasets. \n\n"}
{"id": "1803.07416", "contents": "Title: Tensor2Tensor for Neural Machine Translation Abstract: Tensor2Tensor is a library for deep learning models that is well-suited for\nneural machine translation and includes the reference implementation of the\nstate-of-the-art Transformer model. \n\n"}
{"id": "1803.08983", "contents": "Title: Automated Evaluation of Out-of-Context Errors Abstract: We present a new approach to evaluate computational models for the task of\ntext understanding by the means of out-of-context error detection. Through the\nnovel design of our automated modification process, existing large-scale data\nsources can be adopted for a vast number of text understanding tasks. The data\nis thereby altered on a semantic level, allowing models to be tested against a\nchallenging set of modified text passages that require to comprise a broader\nnarrative discourse. Our newly introduced task targets actual real-world\nproblems of transcription and translation systems by inserting authentic\nout-of-context errors. The automated modification process is applied to the\n2016 TEDTalk corpus. Entirely automating the process allows the adoption of\ncomplete datasets at low cost, facilitating supervised learning procedures and\ndeeper networks to be trained and tested. To evaluate the quality of the\nmodification algorithm a language model and a supervised binary classification\nmodel are trained and tested on the altered dataset. A human baseline\nevaluation is examined to compare the results with human performance. The\noutcome of the evaluation task indicates the difficulty to detect semantic\nerrors for machine-learning algorithms and humans, showing that the errors\ncannot be identified when limited to a single sentence. \n\n"}
{"id": "1803.09164", "contents": "Title: Low-Resource Speech-to-Text Translation Abstract: Speech-to-text translation has many potential applications for low-resource\nlanguages, but the typical approach of cascading speech recognition with\nmachine translation is often impossible, since the transcripts needed to train\na speech recognizer are usually not available for low-resource languages.\nRecent work has found that neural encoder-decoder models can learn to directly\ntranslate foreign speech in high-resource scenarios, without the need for\nintermediate transcription. We investigate whether this approach also works in\nsettings where both data and computation are limited. To make the approach\nefficient, we make several architectural changes, including a change from\ncharacter-level to word-level decoding. We find that this choice yields crucial\nspeed improvements that allow us to train with fewer computational resources,\nyet still performs well on frequent words. We explore models trained on between\n20 and 160 hours of data, and find that although models trained on less data\nhave considerably lower BLEU scores, they can still predict words with\nrelatively high precision and recall---around 50% for a model trained on 50\nhours of data, versus around 60% for the full 160 hour model. Thus, they may\nstill be useful for some low-resource scenarios. \n\n"}
{"id": "1803.10525", "contents": "Title: Machine Speech Chain with One-shot Speaker Adaptation Abstract: In previous work, we developed a closed-loop speech chain model based on deep\nlearning, in which the architecture enabled the automatic speech recognition\n(ASR) and text-to-speech synthesis (TTS) components to mutually improve their\nperformance. This was accomplished by the two parts teaching each other using\nboth labeled and unlabeled data. This approach could significantly improve\nmodel performance within a single-speaker speech dataset, but only a slight\nincrease could be gained in multi-speaker tasks. Furthermore, the model is\nstill unable to handle unseen speakers. In this paper, we present a new speech\nchain mechanism by integrating a speaker recognition model inside the loop. We\nalso propose extending the capability of TTS to handle unseen speakers by\nimplementing one-shot speaker adaptation. This enables TTS to mimic voice\ncharacteristics from one speaker to another with only a one-shot speaker\nsample, even from a text without any speaker information. In the speech chain\nloop mechanism, ASR also benefits from the ability to further learn an\narbitrary speaker's characteristics from the generated speech waveform,\nresulting in a significant improvement in the recognition rate. \n\n"}
{"id": "1804.00015", "contents": "Title: ESPnet: End-to-End Speech Processing Toolkit Abstract: This paper introduces a new open source platform for end-to-end speech\nprocessing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech\nrecognition (ASR), and adopts widely-used dynamic neural network toolkits,\nChainer and PyTorch, as a main deep learning engine. ESPnet also follows the\nKaldi ASR toolkit style for data processing, feature extraction/format, and\nrecipes to provide a complete setup for speech recognition and other speech\nprocessing experiments. This paper explains a major architecture of this\nsoftware platform, several important functionalities, which differentiate\nESPnet from other open source ASR toolkits, and experimental results with major\nASR benchmarks. \n\n"}
{"id": "1804.00832", "contents": "Title: Attentive Sequence-to-Sequence Learning for Diacritic Restoration of\n  Yor\\`ub\\'a Language Text Abstract: Yor\\`ub\\'a is a widely spoken West African language with a writing system\nrich in tonal and orthographic diacritics. With very few exceptions, diacritics\nare omitted from electronic texts, due to limited device and application\nsupport. Diacritics provide morphological information, are crucial for lexical\ndisambiguation, pronunciation and are vital for any Yor\\`ub\\'a text-to-speech\n(TTS), automatic speech recognition (ASR) and natural language processing (NLP)\ntasks. Reframing Automatic Diacritic Restoration (ADR) as a machine translation\ntask, we experiment with two different attentive Sequence-to-Sequence neural\nmodels to process undiacritized text. On our evaluation dataset, this approach\nproduces diacritization error rates of less than 5%. We have released\npre-trained models, datasets and source-code as an open-source project to\nadvance efforts on Yor\\`ub\\'a language technology. \n\n"}
{"id": "1804.00982", "contents": "Title: 360{\\deg} Stance Detection Abstract: The proliferation of fake news and filter bubbles makes it increasingly\ndifficult to form an unbiased, balanced opinion towards a topic. To ameliorate\nthis, we propose 360{\\deg} Stance Detection, a tool that aggregates news with\nmultiple perspectives on a topic. It presents them on a spectrum ranging from\nsupport to opposition, enabling the user to base their opinion on multiple\npieces of diverse evidence. \n\n"}
{"id": "1804.01486", "contents": "Title: Clinical Concept Embeddings Learned from Massive Sources of Multimodal\n  Medical Data Abstract: Word embeddings are a popular approach to unsupervised learning of word\nrelationships that are widely used in natural language processing. In this\narticle, we present a new set of embeddings for medical concepts learned using\nan extremely large collection of multimodal medical data. Leaning on recent\ntheoretical insights, we demonstrate how an insurance claims database of 60\nmillion members, a collection of 20 million clinical notes, and 1.7 million\nfull text biomedical journal articles can be combined to embed concepts into a\ncommon space, resulting in the largest ever set of embeddings for 108,477\nmedical concepts. To evaluate our approach, we present a new benchmark\nmethodology based on statistical power specifically designed to test embeddings\nof medical concepts. Our approach, called cui2vec, attains state-of-the-art\nperformance relative to previous methods in most instances. Finally, we provide\na downloadable set of pre-trained embeddings for other researchers to use, as\nwell as an online tool for interactive exploration of the cui2vec embeddings \n\n"}
{"id": "1804.01503", "contents": "Title: Abstractive Tabular Dataset Summarization via Knowledge Base Semantic\n  Embeddings Abstract: This paper describes an abstractive summarization method for tabular data\nwhich employs a knowledge base semantic embedding to generate the summary.\nAssuming the dataset contains descriptive text in headers, columns and/or some\naugmenting metadata, the system employs the embedding to recommend a\nsubject/type for each text segment. Recommendations are aggregated into a small\ncollection of super types considered to be descriptive of the dataset by\nexploiting the hierarchy of types in a pre-specified ontology. Using February\n2015 Wikipedia as the knowledge base, and a corresponding DBpedia ontology as\ntypes, we present experimental results on open data taken from several\nsources--OpenML, CKAN and data.world--to illustrate the effectiveness of the\napproach. \n\n"}
{"id": "1804.02233", "contents": "Title: Forex trading and Twitter: Spam, bots, and reputation manipulation Abstract: Currency trading (Forex) is the largest world market in terms of volume. We\nanalyze trading and tweeting about the EUR-USD currency pair over a period of\nthree years. First, a large number of tweets were manually labeled, and a\nTwitter stance classification model is constructed. The model then classifies\nall the tweets by the trading stance signal: buy, hold, or sell (EUR vs. USD).\nThe Twitter stance is compared to the actual currency rates by applying the\nevent study methodology, well-known in financial economics. It turns out that\nthere are large differences in Twitter stance distribution and potential\ntrading returns between the four groups of Twitter users: trading robots,\nspammers, trading companies, and individual traders. Additionally, we observe\nattempts of reputation manipulation by post festum removal of tweets with poor\npredictions, and deleting/reposting of identical tweets to increase the\nvisibility without tainting one's Twitter timeline. \n\n"}
{"id": "1804.02559", "contents": "Title: Guiding Neural Machine Translation with Retrieved Translation Pieces Abstract: One of the difficulties of neural machine translation (NMT) is the recall and\nappropriate translation of low-frequency words or phrases. In this paper, we\npropose a simple, fast, and effective method for recalling previously seen\ntranslation examples and incorporating them into the NMT decoding process.\nSpecifically, for an input sentence, we use a search engine to retrieve\nsentence pairs whose source sides are similar with the input sentence, and then\ncollect $n$-grams that are both in the retrieved target sentences and aligned\nwith words that match in the source sentences, which we call \"translation\npieces\". We compute pseudo-probabilities for each retrieved sentence based on\nsimilarities between the input sentence and the retrieved source sentences, and\nuse these to weight the retrieved translation pieces. Finally, an existing NMT\nmodel is used to translate the input sentence, with an additional bonus given\nto outputs that contain the collected translation pieces. We show our method\nimproves NMT translation results up to 6 BLEU points on three narrow domain\ntranslation tasks where repetitiveness of the target sentences is particularly\nsalient. It also causes little increase in the translation time, and compares\nfavorably to another alternative retrieval-based method with respect to\naccuracy, speed, and simplicity of implementation. \n\n"}
{"id": "1804.03433", "contents": "Title: Who framed Roger Reindeer? De-censorship of Facebook posts by snippet\n  classification Abstract: This paper considers online news censorship and it concentrates on censorship\nof identities. Obfuscating identities may occur for disparate reasons, from\nmilitary to judiciary ones. In the majority of cases, this happens to protect\nindividuals from being identified and persecuted by hostile people. However,\nbeing the collaborative web characterised by a redundancy of information, it is\nnot unusual that the same fact is reported by multiple sources, which may not\napply the same restriction policies in terms of censorship. Also, the proven\naptitude of social network users to disclose personal information leads to the\nphenomenon that comments to news can reveal the data withheld in the news\nitself. This gives us a mean to figure out who the subject of the censored news\nis. We propose an adaptation of a text analysis approach to unveil censored\nidentities. The approach is tested on a synthesised scenario, which however\nresembles a real use case. Leveraging a text analysis based on a context\nclassifier trained over snippets from posts and comments of Facebook pages, we\nachieve promising results. Despite the quite constrained settings in which we\noperate -- such as considering only snippets of very short length -- our system\nsuccessfully detects the censored name, choosing among 10 different candidate\nnames, in more than 50\\% of the investigated cases. This outperforms the\nresults of two reference baselines. The findings reported in this paper, other\nthan being supported by a thorough experimental methodology and interesting on\ntheir own, also pave the way for further investigation on the insidious issues\nof censorship on the web. \n\n"}
{"id": "1804.03608", "contents": "Title: Imagine This! Scripts to Compositions to Videos Abstract: Imagining a scene described in natural language with realistic layout and\nappearance of entities is the ultimate test of spatial, visual, and semantic\nworld knowledge. Towards this goal, we present the Composition, Retrieval, and\nFusion Network (CRAFT), a model capable of learning this knowledge from\nvideo-caption data and applying it while generating videos from novel captions.\nCRAFT explicitly predicts a temporal-layout of mentioned entities (characters\nand objects), retrieves spatio-temporal entity segments from a video database\nand fuses them to generate scene videos. Our contributions include sequential\ntraining of components of CRAFT while jointly modeling layout and appearances,\nand losses that encourage learning compositional representations for retrieval.\nWe evaluate CRAFT on semantic fidelity to caption, composition consistency, and\nvisual quality. CRAFT outperforms direct pixel generation approaches and\ngeneralizes well to unseen captions and to unseen video databases with no text\nannotations. We demonstrate CRAFT on FLINTSTONES, a new richly annotated\nvideo-caption dataset with over 25000 videos. For a glimpse of videos generated\nby CRAFT, see https://youtu.be/688Vv86n0z8. \n\n"}
{"id": "1804.04003", "contents": "Title: Sentiment Transfer using Seq2Seq Adversarial Autoencoders Abstract: Expressing in language is subjective. Everyone has a different style of\nreading and writing, apparently it all boil downs to the way their mind\nunderstands things (in a specific format). Language style transfer is a way to\npreserve the meaning of a text and change the way it is expressed. Progress in\nlanguage style transfer is lagged behind other domains, such as computer\nvision, mainly because of the lack of parallel data, use cases, and reliable\nevaluation metrics. In response to the challenge of lacking parallel data, we\nexplore learning style transfer from non-parallel data. We propose a model\ncombining seq2seq, autoencoders, and adversarial loss to achieve this goal. The\nkey idea behind the proposed models is to learn separate content\nrepresentations and style representations using adversarial networks.\nConsidering the problem of evaluating style transfer tasks, we frame the\nproblem as sentiment transfer and evaluation using a sentiment classifier to\ncalculate how many sentiments was the model able to transfer. We report our\nresults on several kinds of models. \n\n"}
{"id": "1804.04205", "contents": "Title: Learning Topics using Semantic Locality Abstract: The topic modeling discovers the latent topic probability of the given text\ndocuments. To generate the more meaningful topic that better represents the\ngiven document, we proposed a new feature extraction technique which can be\nused in the data preprocessing stage. The method consists of three steps.\nFirst, it generates the word/word-pair from every single document. Second, it\napplies a two-way TF-IDF algorithm to word/word-pair for semantic filtering.\nThird, it uses the K-means algorithm to merge the word pairs that have the\nsimilar semantic meaning.\n  Experiments are carried out on the Open Movie Database (OMDb), Reuters\nDataset and 20NewsGroup Dataset. The mean Average Precision score is used as\nthe evaluation metric. Comparing our results with other state-of-the-art topic\nmodels, such as Latent Dirichlet allocation and traditional Restricted\nBoltzmann Machines. Our proposed data preprocessing can improve the generated\ntopic accuracy by up to 12.99\\%. \n\n"}
{"id": "1804.04589", "contents": "Title: A Survey on Neural Network-Based Summarization Methods Abstract: Automatic text summarization, the automated process of shortening a text\nwhile reserving the main ideas of the document(s), is a critical research area\nin natural language processing. The aim of this literature review is to survey\nthe recent work on neural-based models in automatic text summarization. We\nexamine in detail ten state-of-the-art neural-based summarizers: five\nabstractive models and five extractive models. In addition, we discuss the\nrelated techniques that can be applied to the summarization tasks and present\npromising paths for future research in neural-based summarization. \n\n"}
{"id": "1804.05435", "contents": "Title: What Happened? Leveraging VerbNet to Predict the Effects of Actions in\n  Procedural Text Abstract: Our goal is to answer questions about paragraphs describing processes (e.g.,\nphotosynthesis). Texts of this genre are challenging because the effects of\nactions are often implicit (unstated), requiring background knowledge and\ninference to reason about the changing world states. To supply this knowledge,\nwe leverage VerbNet to build a rulebase (called the Semantic Lexicon) of the\npreconditions and effects of actions, and use it along with commonsense\nknowledge of persistence to answer questions about change. Our evaluation shows\nthat our system, ProComp, significantly outperforms two strong reading\ncomprehension (RC) baselines. Our contributions are two-fold: the Semantic\nLexicon rulebase itself, and a demonstration of how a simulation-based approach\nto machine reading can outperform RC methods that rely on surface cues alone.\n  Since this work was performed, we have developed neural systems that\noutperform ProComp, described elsewhere (Dalvi et al., NAACL'18). However, the\nSemantic Lexicon remains a novel and potentially useful resource, and its\nintegration with neural systems remains a currently unexplored opportunity for\nfurther improvements in machine reading about processes. \n\n"}
{"id": "1804.05936", "contents": "Title: Learning a Deep Listwise Context Model for Ranking Refinement Abstract: Learning to rank has been intensively studied and widely applied in\ninformation retrieval. Typically, a global ranking function is learned from a\nset of labeled data, which can achieve good performance on average but may be\nsuboptimal for individual queries by ignoring the fact that relevant documents\nfor different queries may have different distributions in the feature space.\nInspired by the idea of pseudo relevance feedback where top ranked documents,\nwhich we refer as the \\textit{local ranking context}, can provide important\ninformation about the query's characteristics, we propose to use the inherent\nfeature distributions of the top results to learn a Deep Listwise Context Model\nthat helps us fine tune the initial ranked list. Specifically, we employ a\nrecurrent neural network to sequentially encode the top results using their\nfeature vectors, learn a local context model and use it to re-rank the top\nresults. There are three merits with our model: (1) Our model can capture the\nlocal ranking context based on the complex interactions between top results\nusing a deep neural network; (2) Our model can be built upon existing\nlearning-to-rank methods by directly using their extracted feature vectors; (3)\nOur model is trained with an attention-based loss function, which is more\neffective and efficient than many existing listwise methods. Experimental\nresults show that the proposed model can significantly improve the\nstate-of-the-art learning to rank methods on benchmark retrieval corpora. \n\n"}
{"id": "1804.05940", "contents": "Title: Approaching Neural Grammatical Error Correction as a Low-Resource\n  Machine Translation Task Abstract: Previously, neural methods in grammatical error correction (GEC) did not\nreach state-of-the-art results compared to phrase-based statistical machine\ntranslation (SMT) baselines. We demonstrate parallels between neural GEC and\nlow-resource neural MT and successfully adapt several methods from low-resource\nMT to neural GEC. We further establish guidelines for trustable results in\nneural GEC and propose a set of model-independent methods for neural GEC that\ncan be easily applied in most GEC settings. Proposed methods include adding\nsource-side noise, domain-adaptation techniques, a GEC-specific\ntraining-objective, transfer learning with monolingual data, and ensembling of\nindependently trained GEC models and language models. The combined effects of\nthese methods result in better than state-of-the-art neural GEC models that\noutperform previously best neural GEC systems by more than 10% M$^2$ on the\nCoNLL-2014 benchmark and 5.9% on the JFLEG test set. Non-neural\nstate-of-the-art systems are outperformed by more than 2% on the CoNLL-2014\nbenchmark and by 4% on JFLEG. \n\n"}
{"id": "1804.06201", "contents": "Title: LCMR: Local and Centralized Memories for Collaborative Filtering with\n  Unstructured Text Abstract: Collaborative filtering (CF) is the key technique for recommender systems.\nPure CF approaches exploit the user-item interaction data (e.g., clicks, likes,\nand views) only and suffer from the sparsity issue. Items are usually\nassociated with content information such as unstructured text (e.g., abstracts\nof articles and reviews of products). CF can be extended to leverage text. In\nthis paper, we develop a unified neural framework to exploit interaction data\nand content information seamlessly. The proposed framework, called LCMR, is\nbased on memory networks and consists of local and centralized memories for\nexploiting content information and interaction data, respectively. By modeling\ncontent information as local memories, LCMR attentively learns what to exploit\nwith the guidance of user-item interaction. On real-world datasets, LCMR shows\nbetter performance by comparing with various baselines in terms of the hit\nratio and NDCG metrics. We further conduct analyses to understand how local and\ncentralized memories work for the proposed framework. \n\n"}
{"id": "1804.06786", "contents": "Title: Quantifying the visual concreteness of words and topics in multimodal\n  datasets Abstract: Multimodal machine learning algorithms aim to learn visual-textual\ncorrespondences. Previous work suggests that concepts with concrete visual\nmanifestations may be easier to learn than concepts with abstract ones. We give\nan algorithm for automatically computing the visual concreteness of words and\ntopics within multimodal datasets. We apply the approach in four settings,\nranging from image captions to images/text scraped from historical books. In\naddition to enabling explorations of concepts in multimodal datasets, our\nconcreteness scores predict the capacity of machine learning algorithms to\nlearn textual/visual relationships. We find that 1) concrete concepts are\nindeed easier to learn; 2) the large number of algorithms we consider have\nsimilar failure cases; 3) the precise positive relationship between\nconcreteness and performance varies between datasets. We conclude with\nrecommendations for using concreteness scores to facilitate future multimodal\nresearch. \n\n"}
{"id": "1804.07583", "contents": "Title: Approaches for Enriching and Improving Textual Knowledge Bases Abstract: Verifiability is one of the core editing principles in Wikipedia, where\neditors are encouraged to provide citations for the added statements.\nStatements can be any arbitrary piece of text, ranging from a sentence up to a\nparagraph. However, in many cases, citations are either outdated, missing, or\nlink to non-existing references (e.g. dead URL, moved content etc.). In total,\n20\\% of the cases such citations refer to news articles and represent the\nsecond most cited source. Even in cases where citations are provided, there are\nno explicit indicators for the span of a citation for a given piece of text. In\naddition to issues related with the verifiability principle, many Wikipedia\nentity pages are incomplete, with relevant information that is already\navailable in online news sources missing. Even for the already existing\ncitations, there is often a delay between the news publication time and the\nreference time.\n  In this thesis, we address the aforementioned issues and propose automated\napproaches that enforce the verifiability principle in Wikipedia, and suggest\nrelevant and missing news references for further enriching Wikipedia entity\npages. \n\n"}
{"id": "1804.07881", "contents": "Title: Event Extraction with Generative Adversarial Imitation Learning Abstract: We propose a new method for event extraction (EE) task based on an imitation\nlearning framework, specifically, inverse reinforcement learning (IRL) via\ngenerative adversarial network (GAN). The GAN estimates proper rewards\naccording to the difference between the actions committed by the expert (or\nground truth) and the agent among complicated states in the environment. EE\ntask benefits from these dynamic rewards because instances and labels yield to\nvarious extents of difficulty and the gains are expected to be diverse -- e.g.,\nan ambiguous but correctly detected trigger or argument should receive high\ngains -- while the traditional RL models usually neglect such differences and\npay equal attention on all instances. Moreover, our experiments also\ndemonstrate that the proposed framework outperforms state-of-the-art methods,\nwithout explicit feature engineering. \n\n"}
{"id": "1804.07931", "contents": "Title: Entire Space Multi-Task Model: An Effective Approach for Estimating\n  Post-Click Conversion Rate Abstract: Estimating post-click conversion rate (CVR) accurately is crucial for ranking\nsystems in industrial applications such as recommendation and advertising.\nConventional CVR modeling applies popular deep learning methods and achieves\nstate-of-the-art performance. However it encounters several task-specific\nproblems in practice, making CVR modeling challenging. For example,\nconventional CVR models are trained with samples of clicked impressions while\nutilized to make inference on the entire space with samples of all impressions.\nThis causes a sample selection bias problem. Besides, there exists an extreme\ndata sparsity problem, making the model fitting rather difficult. In this\npaper, we model CVR in a brand-new perspective by making good use of sequential\npattern of user actions, i.e., impression -> click -> conversion. The proposed\nEntire Space Multi-task Model (ESMM) can eliminate the two problems\nsimultaneously by i) modeling CVR directly over the entire space, ii) employing\na feature representation transfer learning strategy. Experiments on dataset\ngathered from Taobao's recommender system demonstrate that ESMM significantly\noutperforms competitive methods. We also release a sampling version of this\ndataset to enable future research. To the best of our knowledge, this is the\nfirst public dataset which contains samples with sequential dependence of click\nand conversion labels for CVR modeling. \n\n"}
{"id": "1804.07944", "contents": "Title: Variational Inference In Pachinko Allocation Machines Abstract: The Pachinko Allocation Machine (PAM) is a deep topic model that allows\nrepresenting rich correlation structures among topics by a directed acyclic\ngraph over topics. Because of the flexibility of the model, however,\napproximate inference is very difficult. Perhaps for this reason, only a small\nnumber of potential PAM architectures have been explored in the literature. In\nthis paper we present an efficient and flexible amortized variational inference\nmethod for PAM, using a deep inference network to parameterize the approximate\nposterior distribution in a manner similar to the variational autoencoder. Our\ninference method produces more coherent topics than state-of-art inference\nmethods for PAM while being an order of magnitude faster, which allows\nexploration of a wider range of PAM architectures than have previously been\nstudied. \n\n"}
{"id": "1804.08205", "contents": "Title: Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model Abstract: We show how the spellings of known words can help us deal with unknown words\nin open-vocabulary NLP tasks. The method we propose can be used to extend any\nclosed-vocabulary generative model, but in this paper we specifically consider\nthe case of neural language modeling. Our Bayesian generative story combines a\nstandard RNN language model (generating the word tokens in each sentence) with\nan RNN-based spelling model (generating the letters in each word type). These\ntwo RNNs respectively capture sentence structure and word structure, and are\nkept separate as in linguistics. By invoking the second RNN to generate\nspellings for novel words in context, we obtain an open-vocabulary language\nmodel. For known words, embeddings are naturally inferred by combining evidence\nfrom type spelling and token context. Comparing to baselines (including a novel\nstrong baseline), we beat previous work and establish state-of-the-art results\non multiple datasets. \n\n"}
{"id": "1804.08915", "contents": "Title: Scheduled Multi-Task Learning: From Syntax to Translation Abstract: Neural encoder-decoder models of machine translation have achieved impressive\nresults, while learning linguistic knowledge of both the source and target\nlanguages in an implicit end-to-end manner. We propose a framework in which our\nmodel begins learning syntax and translation interleaved, gradually putting\nmore focus on translation. Using this approach, we achieve considerable\nimprovements in terms of BLEU score on relatively large parallel corpus (WMT14\nEnglish to German) and a low-resource (WIT German to English) setup. \n\n"}
{"id": "1804.09028", "contents": "Title: Estimate and Replace: A Novel Approach to Integrating Deep Neural\n  Networks with Existing Applications Abstract: Existing applications include a huge amount of knowledge that is out of reach\nfor deep neural networks. This paper presents a novel approach for integrating\ncalls to existing applications into deep learning architectures. Using this\napproach, we estimate each application's functionality with an estimator, which\nis implemented as a deep neural network (DNN). The estimator is then embedded\ninto a base network that we direct into complying with the application's\ninterface during an end-to-end optimization process. At inference time, we\nreplace each estimator with its existing application counterpart and let the\nbase network solve the task by interacting with the existing application. Using\nthis 'Estimate and Replace' method, we were able to train a DNN end-to-end with\nless data and outperformed a matching DNN that did not interact with the\nexternal application. \n\n"}
{"id": "1804.09530", "contents": "Title: Strong Baselines for Neural Semi-supervised Learning under Domain Shift Abstract: Novel neural models have been proposed in recent years for learning under\ndomain shift. Most models, however, only evaluate on a single task, on\nproprietary datasets, or compare to weak baselines, which makes comparison of\nmodels difficult. In this paper, we re-evaluate classic general-purpose\nbootstrapping approaches in the context of neural networks under domain shifts\nvs. recent neural approaches and propose a novel multi-task tri-training method\nthat reduces the time and space complexity of classic tri-training. Extensive\nexperiments on two benchmarks are negative: while our novel method establishes\na new state-of-the-art for sentiment analysis, it does not fare consistently\nthe best. More importantly, we arrive at the somewhat surprising conclusion\nthat classic tri-training, with some additions, outperforms the state of the\nart. We conclude that classic approaches constitute an important and strong\nbaseline. \n\n"}
{"id": "1804.09661", "contents": "Title: Personalized Language Model for Query Auto-Completion Abstract: Query auto-completion is a search engine feature whereby the system suggests\ncompleted queries as the user types. Recently, the use of a recurrent neural\nnetwork language model was suggested as a method of generating query\ncompletions. We show how an adaptable language model can be used to generate\npersonalized completions and how the model can use online updating to make\npredictions for users not seen during training. The personalized predictions\nare significantly better than a baseline that uses no user information. \n\n"}
{"id": "1804.10974", "contents": "Title: From Credit Assignment to Entropy Regularization: Two New Algorithms for\n  Neural Sequence Prediction Abstract: In this work, we study the credit assignment problem in reward augmented\nmaximum likelihood (RAML) learning, and establish a theoretical equivalence\nbetween the token-level counterpart of RAML and the entropy regularized\nreinforcement learning. Inspired by the connection, we propose two sequence\nprediction algorithms, one extending RAML with fine-grained credit assignment\nand the other improving Actor-Critic with a systematic entropy regularization.\nOn two benchmark datasets, we show the proposed algorithms outperform RAML and\nActor-Critic respectively, providing new alternatives to sequence prediction. \n\n"}
{"id": "1804.11225", "contents": "Title: Automatic Metric Validation for Grammatical Error Correction Abstract: Metric validation in Grammatical Error Correction (GEC) is currently done by\nobserving the correlation between human and metric-induced rankings. However,\nsuch correlation studies are costly, methodologically troublesome, and suffer\nfrom low inter-rater agreement. We propose MAEGE, an automatic methodology for\nGEC metric validation, that overcomes many of the difficulties with existing\npractices. Experiments with \\maege\\ shed a new light on metric quality, showing\nfor example that the standard $M^2$ metric fares poorly on corpus-level\nranking. Moreover, we use MAEGE to perform a detailed analysis of metric\nbehavior, showing that correcting some types of errors is consistently\npenalized by existing metrics. \n\n"}
{"id": "1804.11297", "contents": "Title: Sampling strategies in Siamese Networks for unsupervised speech\n  representation learning Abstract: Recent studies have investigated siamese network architectures for learning\ninvariant speech representations using same-different side information at the\nword level. Here we investigate systematically an often ignored component of\nsiamese networks: the sampling procedure (how pairs of same vs. different\ntokens are selected). We show that sampling strategies taking into account\nZipf's Law, the distribution of speakers and the proportions of same and\ndifferent pairs of words significantly impact the performance of the network.\nIn particular, we show that word frequency compression improves learning across\na large range of variations in number of training pairs. This effect does not\napply to the same extent to the fully unsupervised setting, where the pairs of\nsame-different words are obtained by spoken term discovery. We apply these\nresults to pairs of words discovered using an unsupervised algorithm and show\nan improvement on state-of-the-art in unsupervised representation learning\nusing siamese networks. \n\n"}
{"id": "1805.00063", "contents": "Title: Adversarial Semantic Alignment for Improved Image Captions Abstract: In this paper we study image captioning as a conditional GAN training,\nproposing both a context-aware LSTM captioner and co-attentive discriminator,\nwhich enforces semantic alignment between images and captions. We empirically\nfocus on the viability of two training methods: Self-critical Sequence Training\n(SCST) and Gumbel Straight-Through (ST) and demonstrate that SCST shows more\nstable gradient behavior and improved results over Gumbel ST, even without\naccessing discriminator gradients directly. We also address the problem of\nautomatic evaluation for captioning models and introduce a new semantic score,\nand show its correlation to human judgement. As an evaluation paradigm, we\nargue that an important criterion for a captioner is the ability to generalize\nto compositions of objects that do not usually co-occur together. To this end,\nwe introduce a small captioned Out of Context (OOC) test set. The OOC set,\ncombined with our semantic score, are the proposed new diagnosis tools for the\ncaptioning community. When evaluated on OOC and MS-COCO benchmarks, we show\nthat SCST-based training has a strong performance in both semantic score and\nhuman evaluation, promising to be a valuable new approach for efficient\ndiscrete GAN training. \n\n"}
{"id": "1805.00250", "contents": "Title: Adaptive Scaling for Sparse Detection in Information Extraction Abstract: This paper focuses on detection tasks in information extraction, where\npositive instances are sparsely distributed and models are usually evaluated\nusing F-measure on positive classes. These characteristics often result in\ndeficient performance of neural network based detection models. In this paper,\nwe propose adaptive scaling, an algorithm which can handle the positive\nsparsity problem and directly optimize over F-measure via dynamic\ncost-sensitive learning. To this end, we borrow the idea of marginal utility\nfrom economics and propose a theoretical framework for instance importance\nmeasuring without introducing any additional hyper-parameters. Experiments show\nthat our algorithm leads to a more effective and stable training of neural\nnetwork based detection models. \n\n"}
{"id": "1805.00471", "contents": "Title: \"I ain't tellin' white folks nuthin\": A quantitative exploration of the\n  race-related problem of candour in the WPA slave narratives Abstract: From 1936-38, the Works Progress Administration interviewed thousands of\nformer slaves about their life experiences. While these interviews are crucial\nto understanding the \"peculiar institution\" from the standpoint of the slave\nhimself, issues relating to bias cloud analyses of these interviews. The\nproblem I investigate is the problem of candour in the WPA slave narratives: it\nis widely held in the historical community that the strict racial caste system\nof the Deep South compelled black ex-slaves to tell white interviewers what\nthey thought they wanted to hear, suggesting that there was a significant\ndifference candour depending on whether their interviewer was white or black.\nIn this work, I attempt to quantitatively characterise this race-related\nproblem of candour. Prior work has either been of an impressionistic,\nqualitative nature, or utilised exceedingly simple quantitative methodology. In\ncontrast, I use more sophisticated statistical methods: in particular word\nfrequency and sentiment analysis and comparative topic modelling with LDA to\ntry and identify differences in the content and sentiment expressed by\nex-slaves in front of white interviewers versus black interviewers. While my\nsentiment analysis methodology was ultimately unsuccessful due to the\ncomplexity of the task, my word frequency analysis and comparative topic\nmodelling methods both showed strong evidence that the content expressed in\nfront of white interviewers was different from that of black interviewers. In\nparticular, I found that the ex-slaves spoke much more about unfavourable\naspects of slavery like whipping and slave patrollers in front of interviewers\nof their own race. I hope that my more-sophisticated statistical methodology\nhelps improve the robustness of the argument for the existence of this problem\nof candour in the slave narratives, which some would seek to deny for\nrevisionist purposes. \n\n"}
{"id": "1805.00631", "contents": "Title: Accelerating Neural Transformer via an Average Attention Network Abstract: With parallelizable attention networks, the neural Transformer is very fast\nto train. However, due to the auto-regressive architecture and self-attention\nin the decoder, the decoding procedure becomes slow. To alleviate this issue,\nwe propose an average attention network as an alternative to the self-attention\nnetwork in the decoder of the neural Transformer. The average attention network\nconsists of two layers, with an average layer that models dependencies on\nprevious positions and a gating layer that is stacked over the average layer to\nenhance the expressiveness of the proposed attention network. We apply this\nnetwork on the decoder part of the neural Transformer to replace the original\ntarget-side self-attention model. With masking tricks and dynamic programming,\nour model enables the neural Transformer to decode sentences over four times\nfaster than its original version with almost no loss in training time and\ntranslation performance. We conduct a series of experiments on WMT17\ntranslation tasks, where on 6 different language pairs, we obtain robust and\nconsistent speed-ups in decoding. \n\n"}
{"id": "1805.00977", "contents": "Title: Exploring Users' Perception of Collaborative Explanation Styles Abstract: Collaborative filtering systems heavily depend on user feedback expressed in\nproduct ratings to select and rank items to recommend. In this study we explore\nhow users value different collaborative explanation styles following the\nuser-based or item-based paradigm. Furthermore, we explore how the\ncharacteristics of these rating summarizations, like the total number of\nratings and the mean rating value, influence the decisions of online users.\nResults, based on a choice-based conjoint experimental design, show that the\nmean indicator has a higher impact compared to the total number of ratings.\nFinally, we discuss how these empirical results can serve as an input to\ndeveloping algorithms that foster items with a, consequently, higher\nprobability of choice based on their rating summarizations or their\nexplainability due to these ratings when ranking recommendations. \n\n"}
{"id": "1805.01089", "contents": "Title: A Hierarchical End-to-End Model for Jointly Improving Text Summarization\n  and Sentiment Classification Abstract: Text summarization and sentiment classification both aim to capture the main\nideas of the text but at different levels. Text summarization is to describe\nthe text within a few sentences, while sentiment classification can be regarded\nas a special type of summarization which \"summarizes\" the text into a even more\nabstract fashion, i.e., a sentiment class. Based on this idea, we propose a\nhierarchical end-to-end model for joint learning of text summarization and\nsentiment classification, where the sentiment classification label is treated\nas the further \"summarization\" of the text summarization output. Hence, the\nsentiment classification layer is put upon the text summarization layer, and a\nhierarchical structure is derived. Experimental results on Amazon online\nreviews datasets show that our model achieves better performance than the\nstrong baseline systems on both abstractive summarization and sentiment\nclassification. \n\n"}
{"id": "1805.01555", "contents": "Title: An End-to-end Approach for Handling Unknown Slot Values in Dialogue\n  State Tracking Abstract: We highlight a practical yet rarely discussed problem in dialogue state\ntracking (DST), namely handling unknown slot values. Previous approaches\ngenerally assume predefined candidate lists and thus are not designed to output\nunknown values, especially when the spoken language understanding (SLU) module\nis absent as in many end-to-end (E2E) systems. We describe in this paper an E2E\narchitecture based on the pointer network (PtrNet) that can effectively extract\nunknown slot values while still obtains state-of-the-art accuracy on the\nstandard DSTC2 benchmark. We also provide extensive empirical evidence to show\nthat tracking unknown values can be challenging and our approach can bring\nsignificant improvement with the help of an effective feature dropout\ntechnique. \n\n"}
{"id": "1805.02214", "contents": "Title: Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to\n  Tokens Abstract: Can attention- or gradient-based visualization techniques be used to infer\ntoken-level labels for binary sequence tagging problems, using networks trained\nonly on sentence-level labels? We construct a neural network architecture based\non soft attention, train it as a binary sentence classifier and evaluate\nagainst token-level annotation on four different datasets. Inferring token\nlabels from a network provides a method for quantitatively evaluating what the\nmodel is learning, along with generating useful feedback in assistance systems.\nOur results indicate that attention-based methods are able to predict\ntoken-level labels more accurately, compared to gradient-based methods,\nsometimes even rivaling the supervised oracle network. \n\n"}
{"id": "1805.02396", "contents": "Title: Billion-scale Network Embedding with Iterative Random Projection Abstract: Network embedding, which learns low-dimensional vector representation for\nnodes in the network, has attracted considerable research attention recently.\nHowever, the existing methods are incapable of handling billion-scale networks,\nbecause they are computationally expensive and, at the same time, difficult to\nbe accelerated by distributed computing schemes. To address these problems, we\npropose RandNE (Iterative Random Projection Network Embedding), a novel and\nsimple billion-scale network embedding method. Specifically, we propose a\nGaussian random projection approach to map the network into a low-dimensional\nembedding space while preserving the high-order proximities between nodes. To\nreduce the time complexity, we design an iterative projection procedure to\navoid the explicit calculation of the high-order proximities. Theoretical\nanalysis shows that our method is extremely efficient, and friendly to\ndistributed computing schemes without any communication cost in the\ncalculation. We also design a dynamic updating procedure which can efficiently\nincorporate the dynamic changes of the networks without error aggregation.\nExtensive experimental results demonstrate the efficiency and efficacy of\nRandNE over state-of-the-art methods in several tasks including network\nreconstruction, link prediction and node classification on multiple datasets\nwith different scales, ranging from thousands to billions of nodes and edges. \n\n"}
{"id": "1805.02856", "contents": "Title: Reasoning with Sarcasm by Reading In-between Abstract: Sarcasm is a sophisticated speech act which commonly manifests on social\ncommunities such as Twitter and Reddit. The prevalence of sarcasm on the social\nweb is highly disruptive to opinion mining systems due to not only its tendency\nof polarity flipping but also usage of figurative language. Sarcasm commonly\nmanifests with a contrastive theme either between positive-negative sentiments\nor between literal-figurative scenarios. In this paper, we revisit the notion\nof modeling contrast in order to reason with sarcasm. More specifically, we\npropose an attention-based neural model that looks in-between instead of\nacross, enabling it to explicitly model contrast and incongruity. We conduct\nextensive experiments on six benchmark datasets from Twitter, Reddit and the\nInternet Argument Corpus. Our proposed model not only achieves state-of-the-art\nperformance on all datasets but also enjoys improved interpretability. \n\n"}
{"id": "1805.02917", "contents": "Title: Interpretable Adversarial Perturbation in Input Embedding Space for Text Abstract: Following great success in the image processing field, the idea of\nadversarial training has been applied to tasks in the natural language\nprocessing (NLP) field. One promising approach directly applies adversarial\ntraining developed in the image processing field to the input word embedding\nspace instead of the discrete input space of texts. However, this approach\nabandons such interpretability as generating adversarial texts to significantly\nimprove the performance of NLP tasks. This paper restores interpretability to\nsuch methods by restricting the directions of perturbations toward the existing\nwords in the input embedding space. As a result, we can straightforwardly\nreconstruct each input with perturbations to an actual text by considering the\nperturbations to be the replacement of words in the sentence while maintaining\nor even improving the task performance. \n\n"}
{"id": "1805.03710", "contents": "Title: Incorporating Subword Information into Matrix Factorization Word\n  Embeddings Abstract: The positive effect of adding subword information to word embeddings has been\ndemonstrated for predictive models. In this paper we investigate whether\nsimilar benefits can also be derived from incorporating subwords into counting\nmodels. We evaluate the impact of different types of subwords (n-grams and\nunsupervised morphemes), with results confirming the importance of subword\ninformation in learning representations of rare and out-of-vocabulary words. \n\n"}
{"id": "1805.04104", "contents": "Title: The Capacity of Private Information Retrieval from Uncoded Storage\n  Constrained Databases Abstract: Private information retrieval (PIR) allows a user to retrieve a desired\nmessage from a set of databases without revealing the identity of the desired\nmessage. The replicated databases scenario was considered by Sun and Jafar,\n2016, where $N$ databases can store the same $K$ messages completely. A PIR\nscheme was developed to achieve the optimal download cost given by $\\left(1+\n\\frac{1}{N}+ \\frac{1}{N^{2}}+ \\cdots + \\frac{1}{N^{K-1}}\\right)$. In this work,\nwe consider the problem of PIR from storage constrained databases. Each\ndatabase has a storage capacity of $\\mu KL$ bits, where $L$ is the size of each\nmessage in bits, and $\\mu \\in [1/N, 1]$ is the normalized storage. On one\nextreme, $\\mu=1$ is the replicated databases case. On the other hand, when\n$\\mu= 1/N$, then in order to retrieve a message privately, the user has to\ndownload all the messages from the databases achieving a download cost of\n$1/K$. We aim to characterize the optimal download cost versus storage\ntrade-off for any storage capacity in the range $\\mu \\in [1/N, 1]$. For any\n$(N,K)$, we show that the optimal trade-off between storage, $\\mu$, and the\ndownload cost, $D(\\mu)$, is given by the lower convex hull of the $N$ pairs\n$\\left(\\mu= \\frac{t}{N},D(\\mu) = \\left(1+ \\frac{1}{t}+ \\frac{1}{t^{2}}+ \\cdots\n+ \\frac{1}{t^{K-1}}\\right)\\right)$ for $t=1,2,\\ldots, N$. To prove this result,\nwe first present the storage constrained PIR scheme for any $(N,K)$. We next\nobtain a general lower bound on the download cost for PIR, which is valid for\nthe following storage scenarios: replicated or storage constrained, coded or\nuncoded, and fixed or optimized. We then specialize this bound using the\nuncoded storage assumption to obtain lower bounds matching the achievable\ndownload cost of the storage constrained PIR scheme for any value of the\navailable storage. \n\n"}
{"id": "1805.04612", "contents": "Title: Twitter User Geolocation using Deep Multiview Learning Abstract: Predicting the geographical location of users on social networks like Twitter\nis an active research topic with plenty of methods proposed so far. Most of the\nexisting work follows either a content-based or a network-based approach. The\nformer is based on user-generated content while the latter exploits the\nstructure of the network of users. In this paper, we propose a more generic\napproach, which incorporates not only both content-based and network-based\nfeatures, but also other available information into a unified model. Our\napproach, named Multi-Entry Neural Network (MENET), leverages the latest\nadvances in deep learning and multiview learning. A realization of MENET with\ntextual, network and metadata features results in an effective method for\nTwitter user geolocation, achieving the state of the art on two well-known\ndatasets. \n\n"}
{"id": "1805.04833", "contents": "Title: Hierarchical Neural Story Generation Abstract: We explore story generation: creative systems that can build coherent and\nfluent passages of text about a topic. We collect a large dataset of 300K\nhuman-written stories paired with writing prompts from an online forum. Our\ndataset enables hierarchical story generation, where the model first generates\na premise, and then transforms it into a passage of text. We gain further\nimprovements with a novel form of model fusion that improves the relevance of\nthe story to the prompt, and adding a new gated multi-scale self-attention\nmechanism to model long-range context. Experiments show large improvements over\nstrong baselines on both automated and human evaluations. Human judges prefer\nstories generated by our approach to those from a strong non-hierarchical model\nby a factor of two to one. \n\n"}
{"id": "1805.04836", "contents": "Title: Building Language Models for Text with Named Entities Abstract: Text in many domains involves a significant amount of named entities.\nPredict- ing the entity names is often challenging for a language model as they\nappear less frequent on the training corpus. In this paper, we propose a novel\nand effective approach to building a discriminative language model which can\nlearn the entity names by leveraging their entity type information. We also\nintroduce two benchmark datasets based on recipes and Java programming codes,\non which we evalu- ate the proposed model. Experimental re- sults show that our\nmodel achieves 52.2% better perplexity in recipe generation and 22.06% on code\ngeneration than the state-of-the-art language models. \n\n"}
{"id": "1805.04908", "contents": "Title: On the Practical Computational Power of Finite Precision RNNs for\n  Language Recognition Abstract: While Recurrent Neural Networks (RNNs) are famously known to be Turing\ncomplete, this relies on infinite precision in the states and unbounded\ncomputation time. We consider the case of RNNs with finite precision whose\ncomputation time is linear in the input length. Under these limitations, we\nshow that different RNN variants have different computational power. In\nparticular, we show that the LSTM and the Elman-RNN with ReLU activation are\nstrictly stronger than the RNN with a squashing activation and the GRU. This is\nachieved because LSTMs and ReLU-RNNs can easily implement counting behavior. We\nshow empirically that the LSTM does indeed learn to effectively use the\ncounting mechanism. \n\n"}
{"id": "1805.05202", "contents": "Title: A Dynamic Oracle for Linear-Time 2-Planar Dependency Parsing Abstract: We propose an efficient dynamic oracle for training the 2-Planar\ntransition-based parser, a linear-time parser with over 99% coverage on\nnon-projective syntactic corpora. This novel approach outperforms the static\ntraining strategy in the vast majority of languages tested and scored better on\nmost datasets than the arc-hybrid parser enhanced with the SWAP transition,\nwhich can handle unrestricted non-projectivity. \n\n"}
{"id": "1805.05361", "contents": "Title: NASH: Toward End-to-End Neural Architecture for Generative Semantic\n  Hashing Abstract: Semantic hashing has become a powerful paradigm for fast similarity search in\nmany information retrieval systems. While fairly successful, previous\ntechniques generally require two-stage training, and the binary constraints are\nhandled ad-hoc. In this paper, we present an end-to-end Neural Architecture for\nSemantic Hashing (NASH), where the binary hashing codes are treated as\nBernoulli latent variables. A neural variational inference framework is\nproposed for training, where gradients are directly back-propagated through the\ndiscrete latent variable to optimize the hash function. We also draw\nconnections between proposed method and rate-distortion theory, which provides\na theoretical foundation for the effectiveness of the proposed framework.\nExperimental results on three public datasets demonstrate that our method\nsignificantly outperforms several state-of-the-art models on both unsupervised\nand supervised scenarios. \n\n"}
{"id": "1805.06297", "contents": "Title: A robust self-learning method for fully unsupervised cross-lingual\n  mappings of word embeddings Abstract: Recent work has managed to learn cross-lingual word embeddings without\nparallel data by mapping monolingual embeddings to a shared space through\nadversarial training. However, their evaluation has focused on favorable\nconditions, using comparable corpora or closely-related languages, and we show\nthat they often fail in more realistic scenarios. This work proposes an\nalternative approach based on a fully unsupervised initialization that\nexplicitly exploits the structural similarity of the embeddings, and a robust\nself-learning algorithm that iteratively improves this solution. Our method\nsucceeds in all tested scenarios and obtains the best published results in\nstandard datasets, even surpassing previous supervised systems. Our\nimplementation is released as an open source project at\nhttps://github.com/artetxem/vecmap \n\n"}
{"id": "1805.06375", "contents": "Title: #phramacovigilance - Exploring Deep Learning Techniques for Identifying\n  Mentions of Medication Intake from Twitter Abstract: Mining social media messages for health and drug related information has\nreceived significant interest in pharmacovigilance research. Social media sites\n(e.g., Twitter), have been used for monitoring drug abuse, adverse reactions of\ndrug usage and analyzing expression of sentiments related to drugs. Most of\nthese studies are based on aggregated results from a large population rather\nthan specific sets of individuals. In order to conduct studies at an individual\nlevel or specific cohorts, identifying posts mentioning intake of medicine by\nthe user is necessary. Towards this objective, we train different deep neural\nnetwork classification models on a publicly available annotated dataset and\nstudy their performances on identifying mentions of personal intake of medicine\nin tweets. We also design and train a new architecture of a stacked ensemble of\nshallow convolutional neural network (CNN) ensembles. We use random search for\ntuning the hyperparameters of the models and share the details of the values\ntaken by the hyperparameters for the best learnt model in different deep neural\nnetwork architectures. Our system produces state-of-the-art results, with a\nmicro- averaged F-score of 0.693. \n\n"}
{"id": "1805.06563", "contents": "Title: NPE: Neural Personalized Embedding for Collaborative Filtering Abstract: Matrix factorization is one of the most efficient approaches in recommender\nsystems. However, such algorithms, which rely on the interactions between users\nand items, perform poorly for \"cold-users\" (users with little history of such\ninteractions) and at capturing the relationships between closely related items.\nTo address these problems, we propose a neural personalized embedding (NPE)\nmodel, which improves the recommendation performance for cold-users and can\nlearn effective representations of items. It models a user's click to an item\nin two terms: the personal preference of the user for the item, and the\nrelationships between this item and other items clicked by the user. We show\nthat NPE outperforms competing methods for top-N recommendations, specially for\ncold-user recommendations. We also performed a qualitative analysis that shows\nthe effectiveness of the representations learned by the model. \n\n"}
{"id": "1805.06879", "contents": "Title: Neural language representations predict outcomes of scientific research Abstract: Many research fields codify their findings in standard formats, often by\nreporting correlations between quantities of interest. But the space of all\ntestable correlates is far larger than scientific resources can currently\naddress, so the ability to accurately predict correlations would be useful to\nplan research and allocate resources. Using a dataset of approximately 170,000\ncorrelational findings extracted from leading social science journals, we show\nthat a trained neural network can accurately predict the reported correlations\nusing only the text descriptions of the correlates. Accurate predictive models\nsuch as these can guide scientists towards promising untested correlates,\nbetter quantify the information gained from new findings, and has implications\nfor moving artificial intelligence systems from predicting structures to\npredicting relationships in the real world. \n\n"}
{"id": "1805.07731", "contents": "Title: Generating High-Quality Surface Realizations Using Data Augmentation and\n  Factored Sequence Models Abstract: This work presents a new state of the art in reconstruction of surface\nrealizations from obfuscated text. We identify the lack of sufficient training\ndata as the major obstacle to training high-performing models, and solve this\nissue by generating large amounts of synthetic training data. We also propose\npreprocessing techniques which make the structure contained in the input\nfeatures more accessible to sequence models. Our models were ranked first on\nall evaluation metrics in the English portion of the 2018 Surface Realization\nshared task. \n\n"}
{"id": "1805.07824", "contents": "Title: Validating WordNet Meronymy Relations using Adimen-SUMO Abstract: In this paper, we report on the practical application of a novel approach for\nvalidating the knowledge of WordNet using Adimen-SUMO. In particular, this\npaper focuses on cross-checking the WordNet meronymy relations against the\nknowledge encoded in Adimen-SUMO. Our validation approach tests a large set of\ncompetency questions (CQs), which are derived (semi)-automatically from the\nknowledge encoded in WordNet, SUMO and their mapping, by applying efficient\nfirst-order logic automated theorem provers. Unfortunately, despite of being\ncreated manually, these knowledge resources are not free of errors and\ndiscrepancies. In consequence, some of the resulting CQs are not plausible\naccording to the knowledge included in Adimen-SUMO. Thus, first we focus on\n(semi)-automatically improving the alignment between these knowledge resources,\nand second, we perform a minimal set of corrections in the ontology. Our aim is\nto minimize the manual effort required for an extensive validation process. We\nreport on the strategies followed, the changes made, the effort needed and its\nimpact when validating the WordNet meronymy relations using improved versions\nof the mapping and the ontology. Based on the new results, we discuss the\nimplications of the appropriate corrections and the need of future\nenhancements. \n\n"}
{"id": "1805.08154", "contents": "Title: Numeracy for Language Models: Evaluating and Improving their Ability to\n  Predict Numbers Abstract: Numeracy is the ability to understand and work with numbers. It is a\nnecessary skill for composing and understanding documents in clinical,\nscientific, and other technical domains. In this paper, we explore different\nstrategies for modelling numerals with language models, such as memorisation\nand digit-by-digit composition, and propose a novel neural architecture that\nuses a continuous probability density function to model numerals from an open\nvocabulary. Our evaluation on clinical and scientific datasets shows that using\nhierarchical models to distinguish numerals from words improves a perplexity\nmetric on the subset of numerals by 2 and 4 orders of magnitude, respectively,\nover non-hierarchical models. A combination of strategies can further improve\nperplexity. Our continuous probability density function model reduces mean\nabsolute percentage errors by 18% and 54% in comparison to the second best\nstrategy for each dataset, respectively. \n\n"}
{"id": "1805.08415", "contents": "Title: Estimating the Rating of Reviewers Based on the Text Abstract: User-generated texts such as reviews and social media are valuable sources of\ninformation. Online reviews are important assets for users to buy a product,\nsee a movie, or make a decision. Therefore, rating of a review is one of the\nreliable factors for all users to read and trust the reviews. This paper\nanalyzes the texts of the reviews to evaluate and predict the ratings.\nMoreover, we study the effect of lexical features generated from text as well\nas sentimental words on the accuracy of rating prediction. Our analysis show\nthat words with high information gain score are more efficient compared to\nwords with high TF-IDF value. In addition, we explore the best number of\nfeatures for predicting the ratings of the reviews. \n\n"}
{"id": "1805.08661", "contents": "Title: COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval Abstract: This paper contributes to cross-lingual image annotation and retrieval in\nterms of data and baseline methods. We propose COCO-CN, a novel dataset\nenriching MS-COCO with manually written Chinese sentences and tags. For more\neffective annotation acquisition, we develop a recommendation-assisted\ncollective annotation system, automatically providing an annotator with several\ntags and sentences deemed to be relevant with respect to the pictorial content.\nHaving 20,342 images annotated with 27,218 Chinese sentences and 70,993 tags,\nCOCO-CN is currently the largest Chinese-English dataset that provides a\nunified and challenging platform for cross-lingual image tagging, captioning\nand retrieval. We develop conceptually simple yet effective methods per task\nfor learning from cross-lingual resources. Extensive experiments on the three\ntasks justify the viability of the proposed dataset and methods. Data and code\nare publicly available at https://github.com/li-xirong/coco-cn \n\n"}
{"id": "1805.09208", "contents": "Title: Pushing the bounds of dropout Abstract: We show that dropout training is best understood as performing MAP estimation\nconcurrently for a family of conditional models whose objectives are themselves\nlower bounded by the original dropout objective. This discovery allows us to\npick any model from this family after training, which leads to a substantial\nimprovement on regularisation-heavy language modelling. The family includes\nmodels that compute a power mean over the sampled dropout masks, and their less\nstochastic subvariants with tighter and higher lower bounds than the fully\nstochastic dropout objective. We argue that since the deterministic\nsubvariant's bound is equal to its objective, and the highest amongst these\nmodels, the predominant view of it as a good approximation to MC averaging is\nmisleading. Rather, deterministic dropout is the best available approximation\nto the true objective. \n\n"}
{"id": "1805.09559", "contents": "Title: WSD algorithm based on a new method of vector-word contexts proximity\n  calculation via epsilon-filtration Abstract: The problem of word sense disambiguation (WSD) is considered in the article.\nGiven a set of synonyms (synsets) and sentences with these synonyms. It is\nnecessary to select the meaning of the word in the sentence automatically. 1285\nsentences were tagged by experts, namely, one of the dictionary meanings was\nselected by experts for target words. To solve the WSD-problem, an algorithm\nbased on a new method of vector-word contexts proximity calculation is\nproposed. In order to achieve higher accuracy, a preliminary epsilon-filtering\nof words is performed, both in the sentence and in the set of synonyms. An\nextensive program of experiments was carried out. Four algorithms are\nimplemented, including a new algorithm. Experiments have shown that in a number\nof cases the new algorithm shows better results. The developed software and the\ntagged corpus have an open license and are available online. Wiktionary and\nWikisource are used. A brief description of this work can be viewed in slides\n(https://goo.gl/9ak6Gt). Video lecture in Russian on this research is available\nonline (https://youtu.be/-DLmRkepf58). \n\n"}
{"id": "1805.10043", "contents": "Title: struc2gauss: Structural Role Preserving Network Embedding via Gaussian\n  Embedding Abstract: Network embedding (NE) is playing a principal role in network mining, due to\nits ability to map nodes into efficient low-dimensional embedding vectors.\nHowever, two major limitations exist in state-of-the-art NE methods: role\npreservation and uncertainty modeling. Almost all previous methods represent a\nnode into a point in space and focus on local structural information, i.e.,\nneighborhood information. However, neighborhood information does not capture\nglobal structural information and point vector representation fails in modeling\nthe uncertainty of node representations. In this paper, we propose a new NE\nframework, struc2gauss, which learns node representations in the space of\nGaussian distributions and performs network embedding based on global\nstructural information. struc2gauss first employs a given node similarity\nmetric to measure the global structural information, then generates structural\ncontext for nodes and finally learns node representations via Gaussian\nembedding. Different structural similarity measures of networks and energy\nfunctions of Gaussian embedding are investigated. Experiments conducted on\nreal-world networks demonstrate that struc2gauss effectively captures global\nstructural information while state-of-the-art network embedding methods fail\nto, outperforms other methods on the structure-based clustering and\nclassification task and provides more information on uncertainties of node\nrepresentations. \n\n"}
{"id": "1805.10364", "contents": "Title: Detecting Deceptive Reviews using Generative Adversarial Networks Abstract: In the past few years, consumer review sites have become the main target of\ndeceptive opinion spam, where fictitious opinions or reviews are deliberately\nwritten to sound authentic. Most of the existing work to detect the deceptive\nreviews focus on building supervised classifiers based on syntactic and lexical\npatterns of an opinion. With the successful use of Neural Networks on various\nclassification applications, in this paper, we propose FakeGAN a system that\nfor the first time augments and adopts Generative Adversarial Networks (GANs)\nfor a text classification task, in particular, detecting deceptive reviews.\nUnlike standard GAN models which have a single Generator and Discriminator\nmodel, FakeGAN uses two discriminator models and one generative model. The\ngenerator is modeled as a stochastic policy agent in reinforcement learning\n(RL), and the discriminators use Monte Carlo search algorithm to estimate and\npass the intermediate action-value as the RL reward to the generator. Providing\nthe generator model with two discriminator models avoids the mod collapse issue\nby learning from both distributions of truthful and deceptive reviews. Indeed,\nour experiments show that using two discriminators provides FakeGAN high\nstability, which is a known issue for GAN architectures. While FakeGAN is built\nupon a semi-supervised classifier, known for less accuracy, our evaluation\nresults on a dataset of TripAdvisor hotel reviews show the same performance in\nterms of accuracy as of the state-of-the-art approaches that apply supervised\nmachine learning. These results indicate that GANs can be effective for text\nclassification tasks. Specifically, FakeGAN is effective at detecting deceptive\nreviews. \n\n"}
{"id": "1805.10547", "contents": "Title: Using Syntax to Ground Referring Expressions in Natural Images Abstract: We introduce GroundNet, a neural network for referring expression recognition\n-- the task of localizing (or grounding) in an image the object referred to by\na natural language expression. Our approach to this task is the first to rely\non a syntactic analysis of the input referring expression in order to inform\nthe structure of the computation graph. Given a parse tree for an input\nexpression, we explicitly map the syntactic constituents and relationships\npresent in the tree to a composed graph of neural modules that defines our\narchitecture for performing localization. This syntax-based approach aids\nlocalization of \\textit{both} the target object and auxiliary supporting\nobjects mentioned in the expression. As a result, GroundNet is more\ninterpretable than previous methods: we can (1) determine which phrase of the\nreferring expression points to which object in the image and (2) track how the\nlocalization of the target object is determined by the network. We study this\nproperty empirically by introducing a new set of annotations on the GoogleRef\ndataset to evaluate localization of supporting objects. Our experiments show\nthat GroundNet achieves state-of-the-art accuracy in identifying supporting\nobjects, while maintaining comparable performance in the localization of target\nobjects. \n\n"}
{"id": "1805.10927", "contents": "Title: Scalable and Robust Community Detection with Randomized Sketching Abstract: This article explores and analyzes the unsupervised clustering of large\npartially observed graphs. We propose a scalable and provable randomized\nframework for clustering graphs generated from the stochastic block model. The\nclustering is first applied to a sub-matrix of the graph's adjacency matrix\nassociated with a reduced graph sketch constructed using random sampling. Then,\nthe clusters of the full graph are inferred based on the clusters extracted\nfrom the sketch using a correlation-based retrieval step. Uniform random node\nsampling is shown to improve the computational complexity over clustering of\nthe full graph when the cluster sizes are balanced. A new random degree-based\nnode sampling algorithm is presented which significantly improves upon the\nperformance of the clustering algorithm even when clusters are unbalanced. This\nframework improves the phase transitions for matrix-decomposition-based\nclustering with regard to computational complexity and minimum cluster size,\nwhich are shown to be nearly dimension-free in the low inter-cluster\nconnectivity regime. A third sampling technique is shown to improve balance by\nrandomly sampling nodes based on spatial distribution. We provide analysis and\nnumerical results using a convex clustering algorithm based on matrix\ncompletion. \n\n"}
{"id": "1805.11222", "contents": "Title: Unsupervised Alignment of Embeddings with Wasserstein Procrustes Abstract: We consider the task of aligning two sets of points in high dimension, which\nhas many applications in natural language processing and computer vision. As an\nexample, it was recently shown that it is possible to infer a bilingual\nlexicon, without supervised data, by aligning word embeddings trained on\nmonolingual data. These recent advances are based on adversarial training to\nlearn the mapping between the two embeddings. In this paper, we propose to use\nan alternative formulation, based on the joint estimation of an orthogonal\nmatrix and a permutation matrix. While this problem is not convex, we propose\nto initialize our optimization algorithm by using a convex relaxation,\ntraditionally considered for the graph isomorphism problem. We propose a\nstochastic algorithm to minimize our cost function on large scale problems.\nFinally, we evaluate our method on the problem of unsupervised word\ntranslation, by aligning word embeddings trained on monolingual data. On this\ntask, our method obtains state of the art results, while requiring less\ncomputational resources than competing approaches. \n\n"}
{"id": "1805.11264", "contents": "Title: Disentangling by Partitioning: A Representation Learning Framework for\n  Multimodal Sensory Data Abstract: Multimodal sensory data resembles the form of information perceived by humans\nfor learning, and are easy to obtain in large quantities. Compared to unimodal\ndata, synchronization of concepts between modalities in such data provides\nsupervision for disentangling the underlying explanatory factors of each\nmodality. Previous work leveraging multimodal data has mainly focused on\nretaining only the modality-invariant factors while discarding the rest. In\nthis paper, we present a partitioned variational autoencoder (PVAE) and several\ntraining objectives to learn disentangled representations, which encode not\nonly the shared factors, but also modality-dependent ones, into separate latent\nvariables. Specifically, PVAE integrates a variational inference framework and\na multimodal generative model that partitions the explanatory factors and\nconditions only on the relevant subset of them for generation. We evaluate our\nmodel on two parallel speech/image datasets, and demonstrate its ability to\nlearn disentangled representations by qualitatively exploring within-modality\nand cross-modality conditional generation with semantics and styles specified\nby examples. For quantitative analysis, we evaluate the classification accuracy\nof automatically discovered semantic units. Our PVAE can achieve over 99%\naccuracy on both modalities. \n\n"}
{"id": "1805.11462", "contents": "Title: OpenNMT: Neural Machine Translation Toolkit Abstract: OpenNMT is an open-source toolkit for neural machine translation (NMT). The\nsystem prioritizes efficiency, modularity, and extensibility with the goal of\nsupporting NMT research into model architectures, feature representations, and\nsource modalities, while maintaining competitive performance and reasonable\ntraining requirements. The toolkit consists of modeling and translation\nsupport, as well as detailed pedagogical documentation about the underlying\ntechniques. OpenNMT has been used in several production MT systems, modified\nfor numerous research papers, and is implemented across several deep learning\nframeworks. \n\n"}
{"id": "1805.11611", "contents": "Title: Semantically-informed distance and similarity measures for paraphrase\n  plagiarism identification Abstract: Paraphrase plagiarism identification represents a very complex task given\nthat plagiarized texts are intentionally modified through several rewording\ntechniques. Accordingly, this paper introduces two new measures for evaluating\nthe relatedness of two given texts: a semantically-informed similarity measure\nand a semantically-informed edit distance. Both measures are able to extract\nsemantic information from either an external resource or a distributed\nrepresentation of words, resulting in informative features for training a\nsupervised classifier for detecting paraphrase plagiarism. Obtained results\nindicate that the proposed metrics are consistently good in detecting different\ntypes of paraphrase plagiarism. In addition, results are very competitive\nagainst state-of-the art methods having the advantage of representing a much\nmore simple but equally effective solution. \n\n"}
{"id": "1805.12070", "contents": "Title: Code-Switching Language Modeling using Syntax-Aware Multi-Task Learning Abstract: Lack of text data has been the major issue on code-switching language\nmodeling. In this paper, we introduce multi-task learning based language model\nwhich shares syntax representation of languages to leverage linguistic\ninformation and tackle the low resource data issue. Our model jointly learns\nboth language modeling and Part-of-Speech tagging on code-switched utterances.\nIn this way, the model is able to identify the location of code-switching\npoints and improves the prediction of next word. Our approach outperforms\nstandard LSTM based language model, with an improvement of 9.7% and 7.4% in\nperplexity on SEAME Phase I and Phase II dataset respectively. \n\n"}
{"id": "1805.12164", "contents": "Title: What the Vec? Towards Probabilistically Grounded Embeddings Abstract: Word2Vec (W2V) and GloVe are popular, fast and efficient word embedding\nalgorithms. Their embeddings are widely used and perform well on a variety of\nnatural language processing tasks. Moreover, W2V has recently been adopted in\nthe field of graph embedding, where it underpins several leading algorithms.\nHowever, despite their ubiquity and relatively simple model architecture, a\ntheoretical understanding of what the embedding parameters of W2V and GloVe\nlearn and why that is useful in downstream tasks has been lacking. We show that\ndifferent interactions between PMI vectors reflect semantic word relationships,\nsuch as similarity and paraphrasing, that are encoded in low dimensional word\nembeddings under a suitable projection, theoretically explaining why embeddings\nof W2V and GloVe work. As a consequence, we also reveal an interesting\nmathematical interconnection between the considered semantic relationships\nthemselves. \n\n"}
{"id": "1805.12312", "contents": "Title: Collaborative Multi-modal deep learning for the personalized product\n  retrieval in Facebook Marketplace Abstract: Facebook Marketplace is quickly gaining momentum among consumers as a favored\ncustomer-to-customer (C2C) product trading platform. The recommendation system\nbehind it helps to significantly improve the user experience. Building the\nrecommendation system for Facebook Marketplace is challenging for two reasons:\n1) Scalability: the number of products in Facebook Marketplace is huge. Tens of\nthousands of products need to be scored and recommended within a couple hundred\nmilliseconds for millions of users every day; 2) Cold start: the life span of\nthe C2C products is very short and the user activities on the products are\nsparse. Thus it is difficult to accumulate enough product level signals for\nrecommendation and we are facing a significant cold start issue. In this paper,\nwe propose to address both the scalability and the cold-start issue by building\na collaborative multi-modal deep learning based retrieval system where the\ncompact embeddings for the users and the products are trained with the\nmulti-modal content information. This system shows significant improvement over\nthe benchmark in online and off-line experiments: In the online experiment, it\nincreases the number of messages initiated by the buyer to the seller by\n+26.95%; in the off-line experiment, it improves the prediction accuracy by\n+9.58%. \n\n"}
{"id": "1806.00187", "contents": "Title: Scaling Neural Machine Translation Abstract: Sequence to sequence learning models still require several days to reach\nstate of the art performance on large benchmark datasets using a single\nmachine. This paper shows that reduced precision and large batch training can\nspeedup training by nearly 5x on a single 8-GPU machine with careful tuning and\nimplementation. On WMT'14 English-German translation, we match the accuracy of\nVaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a\nnew state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We\nfurther improve these results to 29.8 BLEU by training on the much larger\nParacrawl dataset. On the WMT'14 English-French task, we obtain a\nstate-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs. \n\n"}
{"id": "1806.00512", "contents": "Title: Structurally Sparsified Backward Propagation for Faster Long Short-Term\n  Memory Training Abstract: Exploiting sparsity enables hardware systems to run neural networks faster\nand more energy-efficiently. However, most prior sparsity-centric optimization\ntechniques only accelerate the forward pass of neural networks and usually\nrequire an even longer training process with iterative pruning and retraining.\nWe observe that artificially inducing sparsity in the gradients of the gates in\nan LSTM cell has little impact on the training quality. Further, we can enforce\nstructured sparsity in the gate gradients to make the LSTM backward pass up to\n45% faster than the state-of-the-art dense approach and 168% faster than the\nstate-of-the-art sparsifying method on modern GPUs. Though the structured\nsparsifying method can impact the accuracy of a model, this performance gap can\nbe eliminated by mixing our sparse training method and the standard dense\ntraining method. Experimental results show that the mixed method can achieve\ncomparable results in a shorter time span than using purely dense training. \n\n"}
{"id": "1806.00628", "contents": "Title: A Novel Framework for Recurrent Neural Networks with Enhancing\n  Information Processing and Transmission between Units Abstract: This paper proposes a novel framework for recurrent neural networks (RNNs)\ninspired by the human memory models in the field of cognitive neuroscience to\nenhance information processing and transmission between adjacent RNNs' units.\nThe proposed framework for RNNs consists of three stages that is working\nmemory, forget, and long-term store. The first stage includes taking input data\ninto sensory memory and transferring it to working memory for preliminary\ntreatment. And the second stage mainly focuses on proactively forgetting the\nsecondary information rather than the primary in the working memory. And\nfinally, we get the long-term store normally using some kind of RNN's unit. Our\nframework, which is generalized and simple, is evaluated on 6 datasets which\nfall into 3 different tasks, corresponding to text classification, image\nclassification and language modelling. Experiments reveal that our framework\ncan obviously improve the performance of traditional recurrent neural networks.\nAnd exploratory task shows the ability of our framework of correctly forgetting\nthe secondary information. \n\n"}
{"id": "1806.01445", "contents": "Title: Embedding Logical Queries on Knowledge Graphs Abstract: Learning low-dimensional embeddings of knowledge graphs is a powerful\napproach used to predict unobserved or missing edges between entities. However,\nan open challenge in this area is developing techniques that can go beyond\nsimple edge prediction and handle more complex logical queries, which might\ninvolve multiple unobserved edges, entities, and variables. For instance, given\nan incomplete biological knowledge graph, we might want to predict \"em what\ndrugs are likely to target proteins involved with both diseases X and Y?\" -- a\nquery that requires reasoning about all possible proteins that {\\em might}\ninteract with diseases X and Y. Here we introduce a framework to efficiently\nmake predictions about conjunctive logical queries -- a flexible but tractable\nsubset of first-order logic -- on incomplete knowledge graphs. In our approach,\nwe embed graph nodes in a low-dimensional space and represent logical operators\nas learned geometric operations (e.g., translation, rotation) in this embedding\nspace. By performing logical operations within a low-dimensional embedding\nspace, our approach achieves a time complexity that is linear in the number of\nquery variables, compared to the exponential complexity required by a naive\nenumeration-based approach. We demonstrate the utility of this framework in two\napplication studies on real-world datasets with millions of relations:\npredicting logical relationships in a network of drug-gene-disease interactions\nand in a graph-based representation of social interactions derived from a\npopular web forum. \n\n"}
{"id": "1806.01483", "contents": "Title: JTAV: Jointly Learning Social Media Content Representation by Fusing\n  Textual, Acoustic, and Visual Features Abstract: Learning social media content is the basis of many real-world applications,\nincluding information retrieval and recommendation systems, among others. In\ncontrast with previous works that focus mainly on single modal or bi-modal\nlearning, we propose to learn social media content by fusing jointly textual,\nacoustic, and visual information (JTAV). Effective strategies are proposed to\nextract fine-grained features of each modality, that is, attBiGRU and DCRNN. We\nalso introduce cross-modal fusion and attentive pooling techniques to integrate\nmulti-modal information comprehensively. Extensive experimental evaluation\nconducted on real-world datasets demonstrates our proposed model outperforms\nthe state-of-the-art approaches by a large margin. \n\n"}
{"id": "1806.02557", "contents": "Title: Emoji-Powered Representation Learning for Cross-Lingual Sentiment\n  Classification Abstract: Sentiment classification typically relies on a large amount of labeled data.\nIn practice, the availability of labels is highly imbalanced among different\nlanguages, e.g., more English texts are labeled than texts in any other\nlanguages, which creates a considerable inequality in the quality of related\ninformation services received by users speaking different languages. To tackle\nthis problem, cross-lingual sentiment classification approaches aim to transfer\nknowledge learned from one language that has abundant labeled examples (i.e.,\nthe source language, usually English) to another language with fewer labels\n(i.e., the target language). The source and the target languages are usually\nbridged through off-the-shelf machine translation tools. Through such a\nchannel, cross-language sentiment patterns can be successfully learned from\nEnglish and transferred into the target languages. This approach, however,\noften fails to capture sentiment knowledge specific to the target language, and\nthus compromises the accuracy of the downstream classification task. In this\npaper, we employ emojis, which are widely available in many languages, as a new\nchannel to learn both the cross-language and the language-specific sentiment\npatterns. We propose a novel representation learning method that uses emoji\nprediction as an instrument to learn respective sentiment-aware representations\nfor each language. The learned representations are then integrated to\nfacilitate cross-lingual sentiment classification. The proposed method\ndemonstrates state-of-the-art performance on benchmark datasets, which is\nsustained even when sentiment labels are scarce. \n\n"}
{"id": "1806.02954", "contents": "Title: Using Social Network Information in Bayesian Truth Discovery Abstract: We investigate the problem of truth discovery based on opinions from multiple\nagents who may be unreliable or biased. We consider the case where agents'\nreliabilities or biases are correlated if they belong to the same community,\nwhich defines a group of agents with similar opinions regarding a particular\nevent. An agent can belong to different communities for different events, and\nthese communities are unknown a priori. We incorporate knowledge of the agents'\nsocial network in our truth discovery framework and develop Laplace variational\ninference methods to estimate agents' reliabilities, communities, and the event\nstates. We also develop a stochastic variational inference method to scale our\nmodel to large social networks. Simulations and experiments on real data\nsuggest that when observations are sparse, our proposed methods perform better\nthan several other inference methods, including majority voting, TruthFinder,\nAccuSim, the Confidence-Aware Truth Discovery method, the Bayesian Classifier\nCombination (BCC) method, and the Community BCC method. \n\n"}
{"id": "1806.03342", "contents": "Title: Discovering Signals from Web Sources to Predict Cyber Attacks Abstract: Cyber attacks are growing in frequency and severity. Over the past year alone\nwe have witnessed massive data breaches that stole personal information of\nmillions of people and wide-scale ransomware attacks that paralyzed critical\ninfrastructure of several countries. Combating the rising cyber threat calls\nfor a multi-pronged strategy, which includes predicting when these attacks will\noccur. The intuition driving our approach is this: during the planning and\npreparation stages, hackers leave digital traces of their activities on both\nthe surface web and dark web in the form of discussions on platforms like\nhacker forums, social media, blogs and the like. These data provide predictive\nsignals that allow anticipating cyber attacks. In this paper, we describe\nmachine learning techniques based on deep neural networks and autoregressive\ntime series models that leverage external signals from publicly available Web\nsources to forecast cyber attacks. Performance of our framework across ground\ntruth data over real-world forecasting tasks shows that our methods yield a\nsignificant lift or increase of F1 for the top signals on predicted cyber\nattacks. Our results suggest that, when deployed, our system will be able to\nprovide an effective line of defense against various types of targeted cyber\nattacks. \n\n"}
{"id": "1806.03743", "contents": "Title: Are All Languages Equally Hard to Language-Model? Abstract: For general modeling methods applied to diverse languages, a natural question\nis: how well should we expect our models to work on languages with differing\ntypological profiles? In this work, we develop an evaluation framework for fair\ncross-linguistic comparison of language models, using translated text so that\nall models are asked to predict approximately the same information. We then\nconduct a study on 21 languages, demonstrating that in some languages, the\ntextual expression of the information is harder to predict with both $n$-gram\nand LSTM language models. We show complex inflectional morphology to be a cause\nof performance differences among languages. \n\n"}
{"id": "1806.04313", "contents": "Title: Embedding Text in Hyperbolic Spaces Abstract: Natural language text exhibits hierarchical structure in a variety of\nrespects. Ideally, we could incorporate our prior knowledge of this\nhierarchical structure into unsupervised learning algorithms that work on text\ndata. Recent work by Nickel & Kiela (2017) proposed using hyperbolic instead of\nEuclidean embedding spaces to represent hierarchical data and demonstrated\nencouraging results when embedding graphs. In this work, we extend their method\nwith a re-parameterization technique that allows us to learn hyperbolic\nembeddings of arbitrarily parameterized objects. We apply this framework to\nlearn word and sentence embeddings in hyperbolic space in an unsupervised\nmanner from text corpora. The resulting embeddings seem to encode certain\nintuitive notions of hierarchy, such as word-context frequency and phrase\nconstituency. However, the implicit continuous hierarchy in the learned\nhyperbolic space makes interrogating the model's learned hierarchies more\ndifficult than for models that learn explicit edges between items. The learned\nhyperbolic embeddings show improvements over Euclidean embeddings in some --\nbut not all -- downstream tasks, suggesting that hierarchical organization is\nmore useful for some tasks than others. \n\n"}
{"id": "1806.04550", "contents": "Title: Deep State Space Models for Unconditional Word Generation Abstract: Autoregressive feedback is considered a necessity for successful\nunconditional text generation using stochastic sequence models. However, such\nfeedback is known to introduce systematic biases into the training process and\nit obscures a principle of generation: committing to global information and\nforgetting local nuances. We show that a non-autoregressive deep state space\nmodel with a clear separation of global and local uncertainty can be built from\nonly two ingredients: An independent noise source and a deterministic\ntransition function. Recent advances on flow-based variational inference can be\nused to train an evidence lower-bound without resorting to annealing, auxiliary\nlosses or similar measures. The result is a highly interpretable generative\nmodel on par with comparable auto-regressive models on the task of word\ngeneration. \n\n"}
{"id": "1806.05499", "contents": "Title: Aspect Sentiment Model for Micro Reviews Abstract: This paper aims at an aspect sentiment model for aspect-based sentiment\nanalysis (ABSA) focused on micro reviews. This task is important in order to\nunderstand short reviews majority of the users write, while existing topic\nmodels are targeted for expert-level long reviews with sufficient co-occurrence\npatterns to observe. Current methods on aggregating micro reviews using\nmetadata information may not be effective as well due to metadata absence,\ntopical heterogeneity, and cold start problems. To this end, we propose a model\ncalled Micro Aspect Sentiment Model (MicroASM). MicroASM is based on the\nobservation that short reviews 1) are viewed with sentiment-aspect word pairs\nas building blocks of information, and 2) can be clustered into larger reviews.\nWhen compared to the current state-of-the-art aspect sentiment models,\nexperiments show that our model provides better performance on aspect-level\ntasks such as aspect term extraction and document-level tasks such as sentiment\nclassification. \n\n"}
{"id": "1806.06176", "contents": "Title: Learning Factorized Multimodal Representations Abstract: Learning multimodal representations is a fundamentally complex research\nproblem due to the presence of multiple heterogeneous sources of information.\nAlthough the presence of multiple modalities provides additional valuable\ninformation, there are two key challenges to address when learning from\nmultimodal data: 1) models must learn the complex intra-modal and cross-modal\ninteractions for prediction and 2) models must be robust to unexpected missing\nor noisy modalities during testing. In this paper, we propose to optimize for a\njoint generative-discriminative objective across multimodal data and labels. We\nintroduce a model that factorizes representations into two sets of independent\nfactors: multimodal discriminative and modality-specific generative factors.\nMultimodal discriminative factors are shared across all modalities and contain\njoint multimodal features required for discriminative tasks such as sentiment\nprediction. Modality-specific generative factors are unique for each modality\nand contain the information required for generating data. Experimental results\nshow that our model is able to learn meaningful multimodal representations that\nachieve state-of-the-art or competitive performance on six multimodal datasets.\nOur model demonstrates flexible generative capabilities by conditioning on\nindependent factors and can reconstruct missing modalities without\nsignificantly impacting performance. Lastly, we interpret our factorized\nrepresentations to understand the interactions that influence multimodal\nlearning. \n\n"}
{"id": "1806.06583", "contents": "Title: Nonparametric Topic Modeling with Neural Inference Abstract: This work focuses on combining nonparametric topic models with Auto-Encoding\nVariational Bayes (AEVB). Specifically, we first propose iTM-VAE, where the\ntopics are treated as trainable parameters and the document-specific topic\nproportions are obtained by a stick-breaking construction. The inference of\niTM-VAE is modeled by neural networks such that it can be computed in a simple\nfeed-forward manner. We also describe how to introduce a hyper-prior into\niTM-VAE so as to model the uncertainty of the prior parameter. Actually, the\nhyper-prior technique is quite general and we show that it can be applied to\nother AEVB based models to alleviate the {\\it collapse-to-prior} problem\nelegantly. Moreover, we also propose HiTM-VAE, where the document-specific\ntopic distributions are generated in a hierarchical manner. HiTM-VAE is even\nmore flexible and can generate topic distributions with better variability.\nExperimental results on 20News and Reuters RCV1-V2 datasets show that the\nproposed models outperform the state-of-the-art baselines significantly. The\nadvantages of the hyper-prior technique and the hierarchical model construction\nare also confirmed by experiments. \n\n"}
{"id": "1806.06671", "contents": "Title: Where to Go Next: A Spatio-temporal LSTM model for Next POI\n  Recommendation Abstract: Next Point-of-Interest (POI) recommendation is of great value for both\nlocation-based service providers and users. Recently Recurrent Neural Networks\n(RNNs) have been proved to be effective on sequential recommendation tasks.\nHowever, existing RNN solutions rarely consider the spatio-temporal intervals\nbetween neighbor check-ins, which are essential for modeling user check-in\nbehaviors in next POI recommendation. In this paper, we propose a new variant\nof LSTM, named STLSTM, which implements time gates and distance gates into LSTM\nto capture the spatio-temporal relation between successive check-ins.\nSpecifically, one-time gate and one distance gate are designed to control\nshort-term interest update, and another time gate and distance gate are\ndesigned to control long-term interest update. Furthermore, to reduce the\nnumber of parameters and improve efficiency, we further integrate coupled input\nand forget gates with our proposed model. Finally, we evaluate the proposed\nmodel using four real-world datasets from various location-based social\nnetworks. Our experimental results show that our model significantly\noutperforms the state-of-the-art approaches for next POI recommendation. \n\n"}
{"id": "1806.06773", "contents": "Title: Towards an efficient deep learning model for musical onset detection Abstract: In this paper, we propose an efficient and reproducible deep learning model\nfor musical onset detection (MOD). We first review the state-of-the-art deep\nlearning models for MOD, and identify their shortcomings and challenges: (i)\nthe lack of hyper-parameter tuning details, (ii) the non-availability of code\nfor training models on other datasets, and (iii) ignoring the network\ncapability when comparing different architectures. Taking the above issues into\naccount, we experiment with seven deep learning architectures. The most\nefficient one achieves equivalent performance to our implementation of the\nstate-of-the-art architecture. However, it has only 28.3% of the total number\nof trainable parameters compared to the state-of-the-art. Our experiments are\nconducted using two different datasets: one mainly consists of instrumental\nmusic excerpts, and another developed by ourselves includes only solo singing\nvoice excerpts. Further, inter-dataset transfer learning experiments are\nconducted. The results show that the model pre-trained on one dataset fails to\ndetect onsets on another dataset, which denotes the importance of providing the\nimplementation code to enable re-training the model for a different dataset.\nDatasets, code and a Jupyter notebook running on Google Colab are publicly\navailable to make this research understandable and easy to reproduce. \n\n"}
{"id": "1806.07832", "contents": "Title: StructVAE: Tree-structured Latent Variable Models for Semi-supervised\n  Semantic Parsing Abstract: Semantic parsing is the task of transducing natural language (NL) utterances\ninto formal meaning representations (MRs), commonly represented as tree\nstructures. Annotating NL utterances with their corresponding MRs is expensive\nand time-consuming, and thus the limited availability of labeled data often\nbecomes the bottleneck of data-driven, supervised models. We introduce\nStructVAE, a variational auto-encoding model for semisupervised semantic\nparsing, which learns both from limited amounts of parallel data, and\nreadily-available unlabeled NL utterances. StructVAE models latent MRs not\nobserved in the unlabeled data as tree-structured latent variables. Experiments\non semantic parsing on the ATIS domain and Python code generation show that\nwith extra unlabeled data, StructVAE outperforms strong supervised models. \n\n"}
{"id": "1806.07944", "contents": "Title: Searching for a Single Community in a Graph Abstract: In standard graph clustering/community detection, one is interested in\npartitioning the graph into more densely connected subsets of nodes. In\ncontrast, the \"search\" problem of this paper aims to only find the nodes in a\n\"single\" such community, the target, out of the many communities that may\nexist. To do so , we are given suitable side information about the target; for\nexample, a very small number of nodes from the target are labeled as such.\n  We consider a general yet simple notion of side information: all nodes are\nassumed to have random weights, with nodes in the target having higher weights\non average. Given these weights and the graph, we develop a variant of the\nmethod of moments that identifies nodes in the target more reliably, and with\nlower computation, than generic community detection methods that do not use\nside information and partition the entire graph. Our empirical results show\nsignificant gains in runtime, and also gains in accuracy over other graph\nclustering algorithms. \n\n"}
{"id": "1806.07956", "contents": "Title: Reconstructing networks with unknown and heterogeneous errors Abstract: The vast majority of network datasets contains errors and omissions, although\nthis is rarely incorporated in traditional network analysis. Recently, an\nincreasing effort has been made to fill this methodological gap by developing\nnetwork reconstruction approaches based on Bayesian inference. These\napproaches, however, rely on assumptions of uniform error rates and on direct\nestimations of the existence of each edge via repeated measurements, something\nthat is currently unavailable for the majority of network data. Here we develop\na Bayesian reconstruction approach that lifts these limitations by not only\nallowing for heterogeneous errors, but also for single edge measurements\nwithout direct error estimates. Our approach works by coupling the inference\napproach with structured generative network models, which enable the\ncorrelations between edges to be used as reliable uncertainty estimates.\nAlthough our approach is general, we focus on the stochastic block model as the\nbasic generative process, from which efficient nonparametric inference can be\nperformed, and yields a principled method to infer hierarchical community\nstructure from noisy data. We demonstrate the efficacy of our approach with a\nvariety of empirical and artificial networks. \n\n"}
{"id": "1806.07963", "contents": "Title: Latent heterogeneous multilayer community detection Abstract: We propose a method for simultaneously detecting shared and unshared\ncommunities in heterogeneous multilayer weighted and undirected networks. The\nmultilayer network is assumed to follow a generative probabilistic model that\ntakes into account the similarities and dissimilarities between the\ncommunities. We make use of a variational Bayes approach for jointly inferring\nthe shared and unshared hidden communities from multilayer network\nobservations. We show that our approach outperforms state-of-the-art algorithms\nin detecting disparate (shared and private) communities on synthetic data as\nwell as on real genome-wide fibroblast proliferation dataset. \n\n"}
{"id": "1806.08309", "contents": "Title: Par4Sim -- Adaptive Paraphrasing for Text Simplification Abstract: Learning from a real-world data stream and continuously updating the model\nwithout explicit supervision is a new challenge for NLP applications with\nmachine learning components. In this work, we have developed an adaptive\nlearning system for text simplification, which improves the underlying\nlearning-to-rank model from usage data, i.e. how users have employed the system\nfor the task of simplification. Our experimental result shows that, over a\nperiod of time, the performance of the embedded paraphrase ranking model\nincreases steadily improving from a score of 62.88% up to 75.70% based on the\nNDCG@10 evaluation metrics. To our knowledge, this is the first study where an\nNLP component is adaptively improved through usage. \n\n"}
{"id": "1806.08727", "contents": "Title: Jack the Reader - A Machine Reading Framework Abstract: Many Machine Reading and Natural Language Understanding tasks require reading\nsupporting text in order to answer questions. For example, in Question\nAnswering, the supporting text can be newswire or Wikipedia articles; in\nNatural Language Inference, premises can be seen as the supporting text and\nhypotheses as questions. Providing a set of useful primitives operating in a\nsingle framework of related tasks would allow for expressive modelling, and\neasier model comparison and replication. To that end, we present Jack the\nReader (Jack), a framework for Machine Reading that allows for quick model\nprototyping by component reuse, evaluation of new models on existing datasets\nas well as integrating new datasets and applying them on a growing set of\nimplemented baseline models. Jack is currently supporting (but not limited to)\nthree tasks: Question Answering, Natural Language Inference, and Link\nPrediction. It is developed with the aim of increasing research efficiency and\ncode reuse. \n\n"}
{"id": "1806.09055", "contents": "Title: DARTS: Differentiable Architecture Search Abstract: This paper addresses the scalability challenge of architecture search by\nformulating the task in a differentiable manner. Unlike conventional approaches\nof applying evolution or reinforcement learning over a discrete and\nnon-differentiable search space, our method is based on the continuous\nrelaxation of the architecture representation, allowing efficient search of the\narchitecture using gradient descent. Extensive experiments on CIFAR-10,\nImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in\ndiscovering high-performance convolutional architectures for image\nclassification and recurrent architectures for language modeling, while being\norders of magnitude faster than state-of-the-art non-differentiable techniques.\nOur implementation has been made publicly available to facilitate further\nresearch on efficient architecture search algorithms. \n\n"}
{"id": "1806.09374", "contents": "Title: Fast ASR-free and almost zero-resource keyword spotting using DTW and\n  CNNs for humanitarian monitoring Abstract: We use dynamic time warping (DTW) as supervision for training a convolutional\nneural network (CNN) based keyword spotting system using a small set of spoken\nisolated keywords. The aim is to allow rapid deployment of a keyword spotting\nsystem in a new language to support urgent United Nations (UN) relief\nprogrammes in parts of Africa where languages are extremely under-resourced and\nthe development of annotated speech resources is infeasible. First, we use 1920\nrecorded keywords (40 keyword types, 34 minutes of speech) as exemplars in a\nDTW-based template matching system and apply it to untranscribed broadcast\nspeech. Then, we use the resulting DTW scores as targets to train a CNN on the\nsame unlabelled speech. In this way we use just 34 minutes of labelled speech,\nbut leverage a large amount of unlabelled data for training. While the\nresulting CNN keyword spotter cannot match the performance of the DTW-based\nsystem, it substantially outperforms a CNN classifier trained only on the\nkeywords, improving the area under the ROC curve from 0.54 to 0.64. Because our\nCNN system is several orders of magnitude faster at runtime than the DTW\nsystem, it represents the most viable keyword spotter on this extremely limited\ndataset. \n\n"}
{"id": "1806.09542", "contents": "Title: Mapping Unparalleled Clinical Professional and Consumer Languages with\n  Embedding Alignment Abstract: Mapping and translating professional but arcane clinical jargons to consumer\nlanguage is essential to improve the patient-clinician communication.\nResearchers have used the existing biomedical ontologies and consumer health\nvocabulary dictionary to translate between the languages. However, such\napproaches are limited by expert efforts to manually build the dictionary,\nwhich is hard to be generalized and scalable. In this work, we utilized the\nembeddings alignment method for the word mapping between unparalleled clinical\nprofessional and consumer language embeddings. To map semantically similar\nwords in two different word embeddings, we first independently trained word\nembeddings on both the corpus with abundant clinical professional terms and the\nother with mainly healthcare consumer terms. Then, we aligned the embeddings by\nthe Procrustes algorithm. We also investigated the approach with the\nadversarial training with refinement. We evaluated the quality of the alignment\nthrough the similar words retrieval both by computing the model precision and\nas well as judging qualitatively by human. We show that the Procrustes\nalgorithm can be performant for the professional consumer language embeddings\nalignment, whereas adversarial training with refinement may find some relations\nbetween two languages. \n\n"}
{"id": "1806.09764", "contents": "Title: Deep Generative Models with Learnable Knowledge Constraints Abstract: The broad set of deep generative models (DGMs) has achieved remarkable\nadvances. However, it is often difficult to incorporate rich structured domain\nknowledge with the end-to-end DGMs. Posterior regularization (PR) offers a\nprincipled framework to impose structured constraints on probabilistic models,\nbut has limited applicability to the diverse DGMs that can lack a Bayesian\nformulation or even explicit density evaluation. PR also requires constraints\nto be fully specified a priori, which is impractical or suboptimal for complex\nknowledge with learnable uncertain parts. In this paper, we establish\nmathematical correspondence between PR and reinforcement learning (RL), and,\nbased on the connection, expand PR to learn constraints as the extrinsic reward\nin RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is\nflexible to adapt arbitrary constraints with the model jointly. Experiments on\nhuman image generation and templated sentence generation show models with\nlearned knowledge constraints by our algorithm greatly improve over base\ngenerative models. \n\n"}
{"id": "1806.09827", "contents": "Title: Unveiling the semantic structure of text documents using paragraph-aware\n  Topic Models Abstract: Classic Topic Models are built under the Bag Of Words assumption, in which\nword position is ignored for simplicity. Besides, symmetric priors are\ntypically used in most applications. In order to easily learn topics with\ndifferent properties among the same corpus, we propose a new line of work in\nwhich the paragraph structure is exploited. Our proposal is based on the\nfollowing assumption: in many text document corpora there are formal\nconstraints shared across all the collection, e.g. sections. When this\nassumption is satisfied, some paragraphs may be related to general concepts\nshared by all documents in the corpus, while others would contain the genuine\ndescription of documents. Assuming each paragraph can be semantically more\ngeneral, specific, or hybrid, we look for ways to measure this, transferring\nthis distinction to topics and being able to learn what we call specific and\ngeneral topics. Experiments show that this is a proper methodology to highlight\ncertain paragraphs in structured documents at the same time we learn\ninteresting and more diverse topics. \n\n"}
{"id": "1806.10201", "contents": "Title: Neural Cross-Lingual Coreference Resolution and its Application to\n  Entity Linking Abstract: We propose an entity-centric neural cross-lingual coreference model that\nbuilds on multi-lingual embeddings and language-independent features. We\nperform both intrinsic and extrinsic evaluations of our model. In the intrinsic\nevaluation, we show that our model, when trained on English and tested on\nChinese and Spanish, achieves competitive results to the models trained\ndirectly on Chinese and Spanish respectively. In the extrinsic evaluation, we\nshow that our English model helps achieve superior entity linking accuracy on\nChinese and Spanish test sets than the top 2015 TAC system without using any\nannotated data from Chinese or Spanish. \n\n"}
{"id": "1806.11330", "contents": "Title: Posthoc Interpretability of Learning to Rank Models using Secondary\n  Training Data Abstract: Predictive models are omnipresent in automated and assisted decision making\nscenarios. But for the most part they are used as black boxes which output a\nprediction without understanding partially or even completely how different\nfeatures influence the model prediction avoiding algorithmic transparency.\nRankings are ordering over items encoding implicit comparisons typically\nlearned using a family of features using learning-to-rank models. In this paper\nwe focus on how best we can understand the decisions made by a ranker in a\npost-hoc model agnostic manner. We operate on the notion of interpretability\nbased on explainability of rankings over an interpretable feature space.\nFurthermore we train a tree based model (inherently interpretable) using labels\nfrom the ranker, called secondary training data to provide explanations.\nConsequently, we attempt to study how well does a subset of features,\npotentially interpretable, explain the full model under different training\nsizes and algorithms. We do experiments on the learning to rank datasets with\n30k queries and report results that serve show in certain settings we can learn\na faithful interpretable ranker. \n\n"}
{"id": "1806.11423", "contents": "Title: Footwear Size Recommendation System Abstract: While shopping for fashion products, customers usually prefer to try-out\nproducts to examine fit, material, overall look and feel. Due to lack of try\nout options during online shopping, it becomes pivotal to provide customers\nwith as much of this information as possible to enhance their shopping\nexperience. Also it becomes essential to provide same experience for new\ncustomers. Our work here focuses on providing a production ready size\nrecommendation system for shoes and address the challenge of providing\nrecommendation for users with no previous purchases on the platform. In our\nwork, we present a probabilistic approach based on user co-purchase data\nfacilitated by generating a brand-brand relationship graph. Specifically we\naddress two challenges that are commonly faced while implementing such\nsolution. 1. Sparse signals for less popular or new products in the system 2.\nExtending the solution for new users. Further we compare and contrast this\napproach with our previous work and show significant improvement both in\nrecommendation precision and coverage. \n\n"}
{"id": "1806.11424", "contents": "Title: Understanding Fashionability: What drives sales of a style? Abstract: We use customer demand data for fashion articles on Myntra, and derive a\nfashionability or style quotient, which represents customer demand for the\nstylistic content of a fashion article, decoupled with its commercials (price,\noffers, etc.). We demonstrate learning for assortment planning in fashion that\nwould aim to keep a healthy mix of breadth and depth across various styles, and\nwe show the relationship between a customer's perception of a style vs a\nmerchandiser's catalogue of styles. We also backtest our method to calculate\nprediction errors in our style quotient and customer demand, and discuss\nvarious implications and findings. \n\n"}
{"id": "1806.11532", "contents": "Title: TextWorld: A Learning Environment for Text-based Games Abstract: We introduce TextWorld, a sandbox learning environment for the training and\nevaluation of RL agents on text-based games. TextWorld is a Python library that\nhandles interactive play-through of text games, as well as backend functions\nlike state tracking and reward assignment. It comes with a curated list of\ngames whose features and challenges we have analyzed. More significantly, it\nenables users to handcraft or automatically generate new games. Its generative\nmechanisms give precise control over the difficulty, scope, and language of\nconstructed games, and can be used to relax challenges inherent to commercial\ntext games like partial observability and sparse rewards. By generating sets of\nvaried but similar games, TextWorld can also be used to study generalization\nand transfer learning. We cast text-based games in the Reinforcement Learning\nformalism, use our framework to develop a set of benchmark games, and evaluate\nseveral baseline agents on this set and the curated list. \n\n"}
{"id": "1807.00228", "contents": "Title: Embedding Models for Episodic Knowledge Graphs Abstract: In recent years a number of large-scale triple-oriented knowledge graphs have\nbeen generated and various models have been proposed to perform learning in\nthose graphs. Most knowledge graphs are static and reflect the world in its\ncurrent state. In reality, of course, the state of the world is changing: a\nhealthy person becomes diagnosed with a disease and a new president is\ninaugurated. In this paper, we extend models for static knowledge graphs to\ntemporal knowledge graphs. This enables us to store episodic data and to\ngeneralize to new facts (inductive learning). We generalize leading learning\nmodels for static knowledge graphs (i.e., Tucker, RESCAL, HolE, ComplEx,\nDistMult) to temporal knowledge graphs. In particular, we introduce a new\ntensor model, ConT, with superior generalization performance. The performances\nof all proposed models are analyzed on two different datasets: the Global\nDatabase of Events, Language, and Tone (GDELT) and the database for Integrated\nConflict Early Warning System (ICEWS). We argue that temporal knowledge graph\nembeddings might be models also for cognitive episodic memory (facts we\nremember and can recollect) and that a semantic memory (current facts we know)\ncan be generated from episodic memory by a marginalization operation. We\nvalidate this episodic-to-semantic projection hypothesis with the ICEWS\ndataset. \n\n"}
{"id": "1807.00560", "contents": "Title: Weight-importance sparse training in keyword spotting Abstract: Large size models are implemented in recently ASR system to deal with complex\nspeech recognition problems. The num- ber of parameters in these models makes\nthem hard to deploy, especially on some resource-short devices such as car\ntablet. Besides this, at most of time, ASR system is used to deal with\nreal-time problem such as keyword spotting (KWS). It is contradictory to the\nfact that large model requires long com- putation time. To deal with this\nproblem, we apply some sparse algo- rithms to reduces number of parameters in\nsome widely used models, Deep Neural Network (DNN) KWS, which requires real\nshort computation time. We can prune more than 90 % even 95% of parameters in\nthe model with tiny effect decline. And the sparse model performs better than\nbaseline models which has same order number of parameters. Besides this, sparse\nalgorithm can lead us to find rational model size au- tomatically for certain\nproblem without concerning choosing an original model size. \n\n"}
{"id": "1807.00914", "contents": "Title: Modeling Language Variation and Universals: A Survey on Typological\n  Linguistics for Natural Language Processing Abstract: Linguistic typology aims to capture structural and semantic variation across\nthe world's languages. A large-scale typology could provide excellent guidance\nfor multilingual Natural Language Processing (NLP), particularly for languages\nthat suffer from the lack of human labeled resources. We present an extensive\nliterature survey on the use of typological information in the development of\nNLP techniques. Our survey demonstrates that to date, the use of information in\nexisting typological databases has resulted in consistent but modest\nimprovements in system performance. We show that this is due to both intrinsic\nlimitations of databases (in terms of coverage and feature granularity) and\nunder-employment of the typological features included in them. We advocate for\na new approach that adapts the broad and discrete nature of typological\ncategories to the contextual and continuous nature of machine learning\nalgorithms used in contemporary NLP. In particular, we suggest that such\napproach could be facilitated by recent developments in data-driven induction\nof typological knowledge. \n\n"}
{"id": "1807.01364", "contents": "Title: Visual Pattern-Driven Exploration of Big Data Abstract: Pattern extraction algorithms are enabling insights into the ever-growing\namount of today's datasets by translating reoccurring data properties into\ncompact representations. Yet, a practical problem arises: With increasing data\nvolumes and complexity also the number of patterns increases, leaving the\nanalyst with a vast result space. Current algorithmic and especially\nvisualization approaches often fail to answer central overview questions\nessential for a comprehensive understanding of pattern distributions and\nsupport, their quality, and relevance to the analysis task. To address these\nchallenges, we contribute a visual analytics pipeline targeted on the\npattern-driven exploration of result spaces in a semi-automatic fashion.\nSpecifically, we combine image feature analysis and unsupervised learning to\npartition the pattern space into interpretable, coherent chunks, which should\nbe given priority in a subsequent in-depth analysis. In our analysis scenarios,\nno ground-truth is given. Thus, we employ and evaluate novel quality metrics\nderived from the distance distributions of our image feature vectors and the\nderived cluster model to guide the feature selection process. We visualize our\nresults interactively, allowing the user to drill down from overview to detail\ninto the pattern space and demonstrate our techniques in a case study on\nbiomedical genomic data. \n\n"}
{"id": "1807.02162", "contents": "Title: Feature Assisted bi-directional LSTM Model for Protein-Protein\n  Interaction Identification from Biomedical Texts Abstract: Knowledge about protein-protein interactions is essential in understanding\nthe biological processes such as metabolic pathways, DNA replication, and\ntranscription etc. However, a majority of the existing Protein-Protein\nInteraction (PPI) systems are dependent primarily on the scientific literature,\nwhich is yet not accessible as a structured database. Thus, efficient\ninformation extraction systems are required for identifying PPI information\nfrom the large collection of biomedical texts. Most of the existing systems\nmodel the PPI extraction task as a classification problem and are tailored to\nthe handcrafted feature set including domain dependent features. In this paper,\nwe present a novel method based on deep bidirectional long short-term memory\n(B-LSTM) technique that exploits word sequences and dependency path related\ninformation to identify PPI information from text. This model leverages joint\nmodeling of proteins and relations in a single unified framework, which we name\nas Shortest Dependency Path B-LSTM (sdpLSTM) model. We perform experiments on\ntwo popular benchmark PPI datasets, namely AiMed & BioInfer. The evaluation\nshows the F1-score values of 86.45% and 77.35% on AiMed and BioInfer,\nrespectively. Comparisons with the existing systems show that our proposed\napproach attains state-of-the-art performance. \n\n"}
{"id": "1807.02322", "contents": "Title: Memory Augmented Policy Optimization for Program Synthesis and Semantic\n  Parsing Abstract: We present Memory Augmented Policy Optimization (MAPO), a simple and novel\nway to leverage a memory buffer of promising trajectories to reduce the\nvariance of policy gradient estimate. MAPO is applicable to deterministic\nenvironments with discrete actions, such as structured prediction and\ncombinatorial optimization tasks. We express the expected return objective as a\nweighted sum of two terms: an expectation over the high-reward trajectories\ninside the memory buffer, and a separate expectation over trajectories outside\nthe buffer. To make an efficient algorithm of MAPO, we propose: (1) memory\nweight clipping to accelerate and stabilize training; (2) systematic\nexploration to discover high-reward trajectories; (3) distributed sampling from\ninside and outside of the memory buffer to scale up training. MAPO improves the\nsample efficiency and robustness of policy gradient, especially on tasks with\nsparse rewards. We evaluate MAPO on weakly supervised program synthesis from\nnatural language (semantic parsing). On the WikiTableQuestions benchmark, we\nimprove the state-of-the-art by 2.6%, achieving an accuracy of 46.3%. On the\nWikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak\nsupervision, outperforming several strong baselines with full supervision. Our\nsource code is available at\nhttps://github.com/crazydonkey200/neural-symbolic-machines \n\n"}
{"id": "1807.02599", "contents": "Title: From Text to Topics in Healthcare Records: An Unsupervised Graph\n  Partitioning Methodology Abstract: Electronic Healthcare Records contain large volumes of unstructured data,\nincluding extensive free text. Yet this source of detailed information often\nremains under-used because of a lack of methodologies to extract interpretable\ncontent in a timely manner. Here we apply network-theoretical tools to analyse\nfree text in Hospital Patient Incident reports from the National Health\nService, to find clusters of documents with similar content in an unsupervised\nmanner at different levels of resolution. We combine deep neural network\nparagraph vector text-embedding with multiscale Markov Stability community\ndetection applied to a sparsified similarity graph of document vectors, and\nshowcase the approach on incident reports from Imperial College Healthcare NHS\nTrust, London. The multiscale community structure reveals different levels of\nmeaning in the topics of the dataset, as shown by descriptive terms extracted\nfrom the clusters of records. We also compare a posteriori against hand-coded\ncategories assigned by healthcare personnel, and show that our approach\noutperforms LDA-based models. Our content clusters exhibit good correspondence\nwith two levels of hand-coded categories, yet they also provide further medical\ndetail in certain areas and reveal complementary descriptors of incidents\nbeyond the external classification taxonomy. \n\n"}
{"id": "1807.02911", "contents": "Title: A Combined CNN and LSTM Model for Arabic Sentiment Analysis Abstract: Deep neural networks have shown good data modelling capabilities when dealing\nwith challenging and large datasets from a wide range of application areas.\nConvolutional Neural Networks (CNNs) offer advantages in selecting good\nfeatures and Long Short-Term Memory (LSTM) networks have proven good abilities\nof learning sequential data. Both approaches have been reported to provide\nimproved results in areas such image processing, voice recognition, language\ntranslation and other Natural Language Processing (NLP) tasks. Sentiment\nclassification for short text messages from Twitter is a challenging task, and\nthe complexity increases for Arabic language sentiment classification tasks\nbecause Arabic is a rich language in morphology. In addition, the availability\nof accurate pre-processing tools for Arabic is another current limitation,\nalong with limited research available in this area. In this paper, we\ninvestigate the benefits of integrating CNNs and LSTMs and report obtained\nimproved accuracy for Arabic sentiment analysis on different datasets.\nAdditionally, we seek to consider the morphological diversity of particular\nArabic words by using different sentiment classification levels. \n\n"}
{"id": "1807.02962", "contents": "Title: Learning to Index for Nearest Neighbor Search Abstract: In this study, we present a novel ranking model based on learning\nneighborhood relationships embedded in the index space. Given a query point,\nconventional approximate nearest neighbor search calculates the distances to\nthe cluster centroids, before ranking the clusters from near to far based on\nthe distances. The data indexed in the top-ranked clusters are retrieved and\ntreated as the nearest neighbor candidates for the query. However, the loss of\nquantization between the data and cluster centroids will inevitably harm the\nsearch accuracy. To address this problem, the proposed model ranks clusters\nbased on their nearest neighbor probabilities rather than the query-centroid\ndistances. The nearest neighbor probabilities are estimated by employing neural\nnetworks to characterize the neighborhood relationships, i.e., the density\nfunction of nearest neighbors with respect to the query. The proposed\nprobability-based ranking can replace the conventional distance-based ranking\nfor finding candidate clusters, and the predicted probability can be used to\ndetermine the data quantity to be retrieved from the candidate cluster. Our\nexperimental results demonstrated that the proposed ranking model could boost\nthe search performance effectively in billion-scale datasets. \n\n"}
{"id": "1807.02974", "contents": "Title: Universal Word Segmentation: Implementation and Interpretation Abstract: Word segmentation is a low-level NLP task that is non-trivial for a\nconsiderable number of languages. In this paper, we present a sequence tagging\nframework and apply it to word segmentation for a wide range of languages with\ndifferent writing systems and typological characteristics. Additionally, we\ninvestigate the correlations between various typological factors and word\nsegmentation accuracy. The experimental results indicate that segmentation\naccuracy is positively related to word boundary markers and negatively to the\nnumber of unique non-segmental terms. Based on the analysis, we design a small\nset of language-specific settings and extensively evaluate the segmentation\nsystem on the Universal Dependencies datasets. Our model obtains\nstate-of-the-art accuracies on all the UD languages. It performs substantially\nbetter on languages that are non-trivial to segment, such as Chinese, Japanese,\nArabic and Hebrew, when compared to previous work. \n\n"}
{"id": "1807.03006", "contents": "Title: A Sequence-to-Sequence Model for Semantic Role Labeling Abstract: We explore a novel approach for Semantic Role Labeling (SRL) by casting it as\na sequence-to-sequence process. We employ an attention-based model enriched\nwith a copying mechanism to ensure faithful regeneration of the input sequence,\nwhile enabling interleaved generation of argument role labels. Here, we apply\nthis model in a monolingual setting, performing PropBank SRL on English\nlanguage data. The constrained sequence generation set-up enforced with the\ncopying mechanism allows us to analyze the performance and special properties\nof the model on manually labeled data and benchmarking against state-of-the-art\nsequence labeling models. We show that our model is able to solve the SRL\nargument labeling task on English data, yet further structural decoding\nconstraints will need to be added to make the model truly competitive. Our work\nrepresents a first step towards more advanced, generative SRL labeling setups. \n\n"}
{"id": "1807.03096", "contents": "Title: NMT-Keras: a Very Flexible Toolkit with a Focus on Interactive NMT and\n  Online Learning Abstract: We present NMT-Keras, a flexible toolkit for training deep learning models,\nwhich puts a particular emphasis on the development of advanced applications of\nneural machine translation systems, such as interactive-predictive translation\nprotocols and long-term adaptation of the translation system via continuous\nlearning. NMT-Keras is based on an extended version of the popular Keras\nlibrary, and it runs on Theano and Tensorflow. State-of-the-art neural machine\ntranslation models are deployed and used following the high-level framework\nprovided by Keras. Given its high modularity and flexibility, it also has been\nextended to tackle different problems, such as image and video captioning,\nsentence classification and visual question answering. \n\n"}
{"id": "1807.03733", "contents": "Title: Network Classification in Temporal Networks Using Motifs Abstract: Network classification has a variety of applications, such as detecting\ncommunities within networks and finding similarities between those representing\ndifferent aspects of the real world. However, most existing work in this area\nfocus on examining static undirected networks without considering directed\nedges or temporality. In this paper, we propose a new methodology that utilizes\nfeature representation for network classification based on the temporal motif\ndistribution of the network and a null model for comparing against random\ngraphs. Experimental results show that our method improves accuracy by up\n$10\\%$ compared to the state-of-the-art embedding method in network\nclassification, for tasks such as classifying network type, identifying\ncommunities in email exchange network, and identifying users given their\napp-switching behaviors. \n\n"}
{"id": "1807.04595", "contents": "Title: Fast Estimation of Causal Interactions using Wold Processes Abstract: We here focus on the task of learning Granger causality matrices for\nmultivariate point processes. In order to accomplish this task, our work is the\nfirst to explore the use of Wold processes. By doing so, we are able to develop\nasymptotically fast MCMC learning algorithms. With $N$ being the total number\nof events and $K$ the number of processes, our learning algorithm has a\n$O(N(\\,\\log(N)\\,+\\,\\log(K)))$ cost per iteration. This is much faster than the\n$O(N^3\\,K^2)$ or $O(K^3)$ for the state of the art. Our approach, called\nGrangerBusca, is validated on nine datasets. This is an advance in relation to\nmost prior efforts which focus mostly on subsets of the Memetracker data.\nRegarding accuracy, GrangerBusca is three times more accurate (in Precision@10)\nthan the state of the art for the commonly explored subsets Memetracker. Due to\nGrangerBusca's much lower training complexity, our approach is the only one\nable to train models for larger, full, sets of data. \n\n"}
{"id": "1807.04715", "contents": "Title: Orthogonal Matching Pursuit for Text Classification Abstract: In text classification, the problem of overfitting arises due to the high\ndimensionality, making regularization essential. Although classic regularizers\nprovide sparsity, they fail to return highly accurate models. On the contrary,\nstate-of-the-art group-lasso regularizers provide better results at the expense\nof low sparsity. In this paper, we apply a greedy variable selection algorithm,\ncalled Orthogonal Matching Pursuit, for the text classification task. We also\nextend standard group OMP by introducing overlapping Group OMP to handle\noverlapping groups of features. Empirical analysis verifies that both OMP and\noverlapping GOMP constitute powerful regularizers, able to produce effective\nand very sparse models. Code and data are available online:\nhttps://github.com/y3nk0/OMP-for-Text-Classification . \n\n"}
{"id": "1807.05560", "contents": "Title: DeepInf: Social Influence Prediction with Deep Learning Abstract: Social and information networking activities such as on Facebook, Twitter,\nWeChat, and Weibo have become an indispensable part of our everyday life, where\nwe can easily access friends' behaviors and are in turn influenced by them.\nConsequently, an effective social influence prediction for each user is\ncritical for a variety of applications such as online recommendation and\nadvertising.\n  Conventional social influence prediction approaches typically design various\nhand-crafted rules to extract user- and network-specific features. However,\ntheir effectiveness heavily relies on the knowledge of domain experts. As a\nresult, it is usually difficult to generalize them into different domains.\nInspired by the recent success of deep neural networks in a wide range of\ncomputing applications, we design an end-to-end framework, DeepInf, to learn\nusers' latent feature representation for predicting social influence. In\ngeneral, DeepInf takes a user's local network as the input to a graph neural\nnetwork for learning her latent social representation. We design strategies to\nincorporate both network structures and user-specific features into\nconvolutional neural and attention networks. Extensive experiments on Open\nAcademic Graph, Twitter, Weibo, and Digg, representing different types of\nsocial and information networks, demonstrate that the proposed end-to-end\nmodel, DeepInf, significantly outperforms traditional feature engineering-based\napproaches, suggesting the effectiveness of representation learning for social\napplications. \n\n"}
{"id": "1807.08089", "contents": "Title: Phonetic-and-Semantic Embedding of Spoken Words with Applications in\n  Spoken Content Retrieval Abstract: Word embedding or Word2Vec has been successful in offering semantics for text\nwords learned from the context of words. Audio Word2Vec was shown to offer\nphonetic structures for spoken words (signal segments for words) learned from\nsignals within spoken words. This paper proposes a two-stage framework to\nperform phonetic-and-semantic embedding on spoken words considering the context\nof the spoken words. Stage 1 performs phonetic embedding with speaker\ncharacteristics disentangled. Stage 2 then performs semantic embedding in\naddition. We further propose to evaluate the phonetic-and-semantic nature of\nthe audio embeddings obtained in Stage 2 by parallelizing with text embeddings.\nIn general, phonetic structure and semantics inevitably disturb each other. For\nexample the words \"brother\" and \"sister\" are close in semantics but very\ndifferent in phonetic structure, while the words \"brother\" and \"bother\" are in\nthe other way around. But phonetic-and-semantic embedding is attractive, as\nshown in the initial experiments on spoken document retrieval. Not only spoken\ndocuments including the spoken query can be retrieved based on the phonetic\nstructures, but spoken documents semantically related to the query but not\nincluding the query can also be retrieved based on the semantics. \n\n"}
{"id": "1807.08228", "contents": "Title: Tree-structured multi-stage principal component analysis (TMPCA): theory\n  and applications Abstract: A PCA based sequence-to-vector (seq2vec) dimension reduction method for the\ntext classification problem, called the tree-structured multi-stage principal\ncomponent analysis (TMPCA) is presented in this paper. Theoretical analysis and\napplicability of TMPCA are demonstrated as an extension to our previous work\n(Su, Huang & Kuo). Unlike conventional word-to-vector embedding methods, the\nTMPCA method conducts dimension reduction at the sequence level without labeled\ntraining data. Furthermore, it can preserve the sequential structure of input\nsequences. We show that TMPCA is computationally efficient and able to\nfacilitate sequence-based text classification tasks by preserving strong mutual\ninformation between its input and output mathematically. It is also\ndemonstrated by experimental results that a dense (fully connected) network\ntrained on the TMPCA preprocessed data achieves better performance than\nstate-of-the-art fastText and other neural-network-based solutions. \n\n"}
{"id": "1807.08465", "contents": "Title: Multimodal Social Media Analysis for Gang Violence Prevention Abstract: Gang violence is a severe issue in major cities across the U.S. and recent\nstudies [Patton et al. 2017] have found evidence of social media communications\nthat can be linked to such violence in communities with high rates of exposure\nto gang activity. In this paper we partnered computer scientists with social\nwork researchers, who have domain expertise in gang violence, to analyze how\npublic tweets with images posted by youth who mention gang associations on\nTwitter can be leveraged to automatically detect psychosocial factors and\nconditions that could potentially assist social workers and violence outreach\nworkers in prevention and early intervention programs. To this end, we\ndeveloped a rigorous methodology for collecting and annotating tweets. We\ngathered 1,851 tweets and accompanying annotations related to visual concepts\nand the psychosocial codes: aggression, loss, and substance use. These codes\nare relevant to social work interventions, as they represent possible pathways\nto violence on social media. We compare various methods for classifying tweets\ninto these three classes, using only the text of the tweet, only the image of\nthe tweet, or both modalities as input to the classifier. In particular, we\nanalyze the usefulness of mid-level visual concepts and the role of different\nmodalities for this tweet classification task. Our experiments show that\nindividually, text information dominates classification performance of the loss\nclass, while image information dominates the aggression and substance use\nclasses. Our multimodal approach provides a very promising improvement (18%\nrelative in mean average precision) over the best single modality approach.\nFinally, we also illustrate the complexity of understanding social media data\nand elaborate on open challenges. \n\n"}
{"id": "1807.09097", "contents": "Title: Algorithm Selection for Collaborative Filtering: the influence of graph\n  metafeatures and multicriteria metatargets Abstract: To select the best algorithm for a new problem is an expensive and difficult\ntask. However, there are automatic solutions to address this problem: using\nMetalearning, which takes advantage of problem characteristics (i.e.\nmetafeatures), one is able to predict the relative performance of algorithms.\nIn the Collaborative Filtering scope, recent works have proposed diverse\nmetafeatures describing several dimensions of this problem. Despite interesting\nand effective findings, it is still unknown whether these are the most\neffective metafeatures. Hence, this work proposes a new set of graph\nmetafeatures, which approach the Collaborative Filtering problem from a Graph\nTheory perspective. Furthermore, in order to understand whether metafeatures\nfrom multiple dimensions are a better fit, we investigate the effects of\ncomprehensive metafeatures. These metafeatures are a selection of the best\nmetafeatures from all existing Collaborative Filtering metafeatures. The impact\nof the most representative metafeatures is investigated in a controlled\nexperimental setup. Another contribution we present is the use of a\nPareto-Efficient ranking procedure to create multicriteria metatargets. These\nnew rankings of algorithms, which take into account multiple evaluation\nmeasures, allow to explore the algorithm selection problem in a fairer and more\ndetailed way. According to the experimental results, the graph metafeatures are\na good alternative to related work metafeatures. However, the results have\nshown that the feature selection procedure used to create the comprehensive\nmetafeatures is is not effective, since there is no gain in predictive\nperformance. Finally, an extensive metaknowledge analysis was conducted to\nidentify the most influential metafeatures. \n\n"}
{"id": "1807.09586", "contents": "Title: Perturb and Combine to Identify Influential Spreaders in Real-World\n  Networks Abstract: Some of the most effective influential spreader detection algorithms are\nunstable to small perturbations of the network structure. Inspired by bagging\nin Machine Learning, we propose the first Perturb and Combine (P&C) procedure\nfor networks. It (1) creates many perturbed versions of a given graph, (2)\napplies a node scoring function separately to each graph, and (3) combines the\nresults. Experiments conducted on real-world networks of various sizes with the\nk-core, generalized k-core, and PageRank algorithms reveal that P&C brings\nsubstantial improvements. Moreover, this performance boost can be obtained at\nalmost no extra cost through parallelization. Finally, a bias-variance analysis\nsuggests that P&C works mainly by reducing bias, and that therefore, it should\nbe capable of improving the performance of all vertex scoring functions,\nincluding stable ones. \n\n"}
{"id": "1807.09596", "contents": "Title: Contextual Stochastic Block Models Abstract: We provide the first information theoretic tight analysis for inference of\nlatent community structure given a sparse graph along with high dimensional\nnode covariates, correlated with the same latent communities. Our work bridges\nrecent theoretical breakthroughs in the detection of latent community structure\nwithout nodes covariates and a large body of empirical work using diverse\nheuristics for combining node covariates with graphs for inference. The\ntightness of our analysis implies in particular, the information theoretical\nnecessity of combining the different sources of information. Our analysis holds\nfor networks of large degrees as well as for a Gaussian version of the model. \n\n"}
{"id": "1807.09597", "contents": "Title: Acoustic-to-Word Recognition with Sequence-to-Sequence Models Abstract: Acoustic-to-Word recognition provides a straightforward solution to\nend-to-end speech recognition without needing external decoding, language model\nre-scoring or lexicon. While character-based models offer a natural solution to\nthe out-of-vocabulary problem, word models can be simpler to decode and may\nalso be able to directly recognize semantically meaningful units. We present\neffective methods to train Sequence-to-Sequence models for direct word-level\nrecognition (and character-level recognition) and show an absolute improvement\nof 4.4-5.0\\% in Word Error Rate on the Switchboard corpus compared to prior\nwork. In addition to these promising results, word-based models are more\ninterpretable than character models, which have to be composed into words using\na separate decoding step. We analyze the encoder hidden states and the\nattention behavior, and show that location-aware attention naturally represents\nwords as a single speech-word-vector, despite spanning multiple frames in the\ninput. We finally show that the Acoustic-to-Word model also learns to segment\nspeech into words with a mean standard deviation of 3 frames as compared with\nhuman annotated forced-alignments for the Switchboard corpus. \n\n"}
{"id": "1807.09644", "contents": "Title: Three hypergraph eigenvector centralities Abstract: Eigenvector centrality is a standard network analysis tool for determining\nthe importance of (or ranking of) entities in a connected system that is\nrepresented by a graph. However, many complex systems and datasets have natural\nmulti-way interactions that are more faithfully modeled by a hypergraph. Here\nwe extend the notion of graph eigenvector centrality to uniform hypergraphs.\nTraditional graph eigenvector centralities are given by a positive eigenvector\nof the adjacency matrix, which is guaranteed to exist by the Perron-Frobenius\ntheorem under some mild conditions. The natural representation of a hypergraph\nis a hypermatrix (colloquially, a tensor). Using recently established\nPerron-Frobenius theory for tensors, we develop three tensor eigenvectors\ncentralities for hypergraphs, each with different interpretations. We show that\nthese centralities can reveal different information on real-world data by\nanalyzing hypergraphs constructed from n-gram frequencies, co-tagging on stack\nexchange, and drug combinations observed in patient emergency room visits. \n\n"}
{"id": "1807.09842", "contents": "Title: Understanding and representing the semantics of large structured\n  documents Abstract: Understanding large, structured documents like scholarly articles, requests\nfor proposals or business reports is a complex and difficult task. It involves\ndiscovering a document's overall purpose and subject(s), understanding the\nfunction and meaning of its sections and subsections, and extracting low level\nentities and facts about them. In this research, we present a deep learning\nbased document ontology to capture the general purpose semantic structure and\ndomain specific semantic concepts from a large number of academic articles and\nbusiness documents. The ontology is able to describe different functional parts\nof a document, which can be used to enhance semantic indexing for a better\nunderstanding by human beings and machines. We evaluate our models through\nextensive experiments on datasets of scholarly articles from arXiv and Request\nfor Proposal documents. \n\n"}
{"id": "1807.09950", "contents": "Title: Variational Memory Encoder-Decoder Abstract: Introducing variability while maintaining coherence is a core task in\nlearning to generate utterances in conversation. Standard neural\nencoder-decoder models and their extensions using conditional variational\nautoencoder often result in either trivial or digressive responses. To overcome\nthis, we explore a novel approach that injects variability into neural\nencoder-decoder via the use of external memory as a mixture model, namely\nVariational Memory Encoder-Decoder (VMED). By associating each memory read with\na mode in the latent mixture distribution at each timestep, our model can\ncapture the variability observed in sequential data such as natural\nconversations. We empirically compare the proposed model against other recent\napproaches on various conversational datasets. The results show that VMED\nconsistently achieves significant improvement over others in both metric-based\nand qualitative evaluations. \n\n"}
{"id": "1807.10854", "contents": "Title: A Survey of the Usages of Deep Learning in Natural Language Processing Abstract: Over the last several years, the field of natural language processing has\nbeen propelled forward by an explosion in the use of deep learning models. This\nsurvey provides a brief introduction to the field and a quick overview of deep\nlearning architectures and methods. It then sifts through the plethora of\nrecent studies and summarizes a large assortment of relevant contributions.\nAnalyzed research areas include several core linguistic processing issues in\naddition to a number of applications of computational linguistics. A discussion\nof the current state of the art is then provided along with recommendations for\nfuture research in the field. \n\n"}
{"id": "1807.11243", "contents": "Title: Active Learning for Interactive Neural Machine Translation of Data\n  Streams Abstract: We study the application of active learning techniques to the translation of\nunbounded data streams via interactive neural machine translation. The main\nidea is to select, from an unbounded stream of source sentences, those worth to\nbe supervised by a human agent. The user will interactively translate those\nsamples. Once validated, these data is useful for adapting the neural machine\ntranslation model.\n  We propose two novel methods for selecting the samples to be validated. We\nexploit the information from the attention mechanism of a neural machine\ntranslation system. Our experiments show that the inclusion of active learning\ntechniques into this pipeline allows to reduce the effort required during the\nprocess, while increasing the quality of the translation system. Moreover, it\nenables to balance the human effort required for achieving a certain\ntranslation quality. Moreover, our neural system outperforms classical\napproaches by a large margin. \n\n"}
{"id": "1808.00300", "contents": "Title: Learning Visual Question Answering by Bootstrapping Hard Attention Abstract: Attention mechanisms in biological perception are thought to select subsets\nof perceptual information for more sophisticated processing which would be\nprohibitive to perform on all sensory inputs. In computer vision, however,\nthere has been relatively little exploration of hard attention, where some\ninformation is selectively ignored, in spite of the success of soft attention,\nwhere information is re-weighted and aggregated, but never filtered out. Here,\nwe introduce a new approach for hard attention and find it achieves very\ncompetitive performance on a recently-released visual question answering\ndatasets, equalling and in some cases surpassing similar soft attention\narchitectures while entirely ignoring some features. Even though the hard\nattention mechanism is thought to be non-differentiable, we found that the\nfeature magnitudes correlate with semantic relevance, and provide a useful\nsignal for our mechanism's attentional selection criterion. Because hard\nattention selects important features of the input information, it can also be\nmore efficient than analogous soft attention mechanisms. This is especially\nimportant for recent approaches that use non-local pairwise operations, whereby\ncomputational and memory costs are quadratic in the size of the set of\nfeatures. \n\n"}
{"id": "1808.01175", "contents": "Title: Content-driven, unsupervised clustering of news articles through\n  multiscale graph partitioning Abstract: The explosion in the amount of news and journalistic content being generated\nacross the globe, coupled with extended and instantaneous access to information\nthrough online media, makes it difficult and time-consuming to monitor news\ndevelopments and opinion formation in real time. There is an increasing need\nfor tools that can pre-process, analyse and classify raw text to extract\ninterpretable content; specifically, identifying topics and content-driven\ngroupings of articles. We present here such a methodology that brings together\npowerful vector embeddings from Natural Language Processing with tools from\nGraph Theory that exploit diffusive dynamics on graphs to reveal natural\npartitions across scales. Our framework uses a recent deep neural network text\nanalysis methodology (Doc2vec) to represent text in vector form and then\napplies a multi-scale community detection method (Markov Stability) to\npartition a similarity graph of document vectors. The method allows us to\nobtain clusters of documents with similar content, at different levels of\nresolution, in an unsupervised manner. We showcase our approach with the\nanalysis of a corpus of 9,000 news articles published by Vox Media over one\nyear. Our results show consistent groupings of documents according to content\nwithout a priori assumptions about the number or type of clusters to be found.\nThe multilevel clustering reveals a quasi-hierarchy of topics and subtopics\nwith increased intelligibility and improved topic coherence as compared to\nexternal taxonomy services and standard topic detection methods. \n\n"}
{"id": "1808.01371", "contents": "Title: Large Scale Language Modeling: Converging on 40GB of Text in Four Hours Abstract: Recent work has shown how to train Convolutional Neural Networks (CNNs)\nrapidly on large image datasets, then transfer the knowledge gained from these\nmodels to a variety of tasks. Following [Radford 2017], in this work, we\ndemonstrate similar scalability and transfer for Recurrent Neural Networks\n(RNNs) for Natural Language tasks. By utilizing mixed precision arithmetic and\na 32k batch size distributed across 128 NVIDIA Tesla V100 GPUs, we are able to\ntrain a character-level 4096-dimension multiplicative LSTM (mLSTM) for\nunsupervised text reconstruction over 3 epochs of the 40 GB Amazon Reviews\ndataset in four hours. This runtime compares favorably with previous work\ntaking one month to train the same size and configuration for one epoch over\nthe same dataset. Converging large batch RNN models can be challenging. Recent\nwork has suggested scaling the learning rate as a function of batch size, but\nwe find that simply scaling the learning rate as a function of batch size leads\neither to significantly worse convergence or immediate divergence for this\nproblem. We provide a learning rate schedule that allows our model to converge\nwith a 32k batch size. Since our model converges over the Amazon Reviews\ndataset in hours, and our compute requirement of 128 Tesla V100 GPUs, while\nsubstantial, is commercially available, this work opens up large scale\nunsupervised NLP training to most commercial applications and deep learning\nresearchers. A model can be trained over most public or private text datasets\novernight. \n\n"}
{"id": "1808.01410", "contents": "Title: Predicting Expressive Speaking Style From Text In End-To-End Speech\n  Synthesis Abstract: Global Style Tokens (GSTs) are a recently-proposed method to learn latent\ndisentangled representations of high-dimensional data. GSTs can be used within\nTacotron, a state-of-the-art end-to-end text-to-speech synthesis system, to\nuncover expressive factors of variation in speaking style. In this work, we\nintroduce the Text-Predicted Global Style Token (TP-GST) architecture, which\ntreats GST combination weights or style embeddings as \"virtual\" speaking style\nlabels within Tacotron. TP-GST learns to predict stylistic renderings from text\nalone, requiring neither explicit labels during training nor auxiliary inputs\nfor inference. We show that, when trained on a dataset of expressive speech,\nour system generates audio with more pitch and energy variation than two\nstate-of-the-art baseline models. We further demonstrate that TP-GSTs can\nsynthesize speech with background noise removed, and corroborate these analyses\nwith positive results on human-rated listener preference audiobook tasks.\nFinally, we demonstrate that multi-speaker TP-GST models successfully factorize\nspeaker identity and speaking style. We provide a website with audio samples\nfor each of our findings. \n\n"}
{"id": "1808.02113", "contents": "Title: Paying Attention to Attention: Highlighting Influential Samples in\n  Sequential Analysis Abstract: In (Yang et al. 2016), a hierarchical attention network (HAN) is created for\ndocument classification. The attention layer can be used to visualize text\ninfluential in classifying the document, thereby explaining the model's\nprediction. We successfully applied HAN to a sequential analysis task in the\nform of real-time monitoring of turn taking in conversations. However, we\ndiscovered instances where the attention weights were uniform at the stopping\npoint (indicating all turns were equivalently influential to the classifier),\npreventing meaningful visualization for real-time human review or classifier\nimprovement. We observed that attention weights for turns fluctuated as the\nconversations progressed, indicating turns had varying influence based on\nconversation state. Leveraging this observation, we develop a method to create\nmore informative real-time visuals (as confirmed by human reviewers) in cases\nof uniform attention weights using the changes in turn importance as a\nconversation progresses over time. \n\n"}
{"id": "1808.02129", "contents": "Title: Probabilistic Causal Analysis of Social Influence Abstract: Mastering the dynamics of social influence requires separating, in a database\nof information propagation traces, the genuine causal processes from temporal\ncorrelation, i.e., homophily and other spurious causes. However, most studies\nto characterize social influence, and, in general, most data-science analyses\nfocus on correlations, statistical independence, or conditional independence.\nOnly recently, there has been a resurgence of interest in \"causal data\nscience\", e.g., grounded on causality theories. In this paper we adopt a\nprincipled causal approach to the analysis of social influence from\ninformation-propagation data, rooted in the theory of probabilistic causation.\n  Our approach consists of two phases. In the first one, in order to avoid the\npitfalls of misinterpreting causation when the data spans a mixture of several\nsubtypes (\"Simpson's paradox\"), we partition the set of propagation traces into\ngroups, in such a way that each group is as less contradictory as possible in\nterms of the hierarchical structure of information propagation. To achieve this\ngoal, we borrow the notion of \"agony\" and define the Agony-bounded Partitioning\nproblem, which we prove being hard, and for which we develop two efficient\nalgorithms with approximation guarantees. In the second phase, for each group\nfrom the first phase, we apply a constrained MLE approach to ultimately learn a\nminimal causal topology. Experiments on synthetic data show that our method is\nable to retrieve the genuine causal arcs w.r.t. a ground-truth generative\nmodel. Experiments on real data show that, by focusing only on the extracted\ncausal structures instead of the whole social graph, the effectiveness of\npredicting influence spread is significantly improved. \n\n"}
{"id": "1808.03227", "contents": "Title: Identifying Protein-Protein Interaction using Tree LSTM and Structured\n  Attention Abstract: Identifying interactions between proteins is important to understand\nunderlying biological processes. Extracting a protein-protein interaction (PPI)\nfrom the raw text is often very difficult. Previous supervised learning methods\nhave used handcrafted features on human-annotated data sets. In this paper, we\npropose a novel tree recurrent neural network with structured attention\narchitecture for doing PPI. Our architecture achieves state of the art results\n(precision, recall, and F1-score) on the AIMed and BioInfer benchmark data\nsets. Moreover, our models achieve a significant improvement over previous best\nmodels without any explicit feature extraction. Our experimental results show\nthat traditional recurrent networks have inferior performance compared to tree\nrecurrent networks for the supervised PPI problem. \n\n"}
{"id": "1808.03712", "contents": "Title: Unsupervised Keyphrase Extraction from Scientific Publications Abstract: We propose a novel unsupervised keyphrase extraction approach that filters\ncandidate keywords using outlier detection. It starts by training word\nembeddings on the target document to capture semantic regularities among the\nwords. It then uses the minimum covariance determinant estimator to model the\ndistribution of non-keyphrase word vectors, under the assumption that these\nvectors come from the same distribution, indicative of their irrelevance to the\nsemantics expressed by the dimensions of the learned vector representation.\nCandidate keyphrases only consist of words that are detected as outliers of\nthis dominant distribution. Empirical results show that our approach\noutperforms state-of-the-art and recent unsupervised keyphrase extraction\nmethods. \n\n"}
{"id": "1808.03733", "contents": "Title: Familia: A Configurable Topic Modeling Framework for Industrial Text\n  Engineering Abstract: In the last decade, a variety of topic models have been proposed for text\nengineering. However, except Probabilistic Latent Semantic Analysis (PLSA) and\nLatent Dirichlet Allocation (LDA), most of existing topic models are seldom\napplied or considered in industrial scenarios. This phenomenon is caused by the\nfact that there are very few convenient tools to support these topic models so\nfar. Intimidated by the demanding expertise and labor of designing and\nimplementing parameter inference algorithms, software engineers are prone to\nsimply resort to PLSA/LDA, without considering whether it is proper for their\nproblem at hand or not. In this paper, we propose a configurable topic modeling\nframework named Familia, in order to bridge the huge gap between academic\nresearch fruits and current industrial practice. Familia supports an important\nline of topic models that are widely applicable in text engineering scenarios.\nIn order to relieve burdens of software engineers without knowledge of Bayesian\nnetworks, Familia is able to conduct automatic parameter inference for a\nvariety of topic models. Simply through changing the data organization of\nFamilia, software engineers are able to easily explore a broad spectrum of\nexisting topic models or even design their own topic models, and find the one\nthat best suits the problem at hand. With its superior extendability, Familia\nhas a novel sampling mechanism that strikes balance between effectiveness and\nefficiency of parameter inference. Furthermore, Familia is essentially a big\ntopic modeling framework that supports parallel parameter inference and\ndistributed parameter storage. The utilities and necessity of Familia are\ndemonstrated in real-life industrial applications. Familia would significantly\nenlarge software engineers' arsenal of topic models and pave the way for\nutilizing highly customized topic models in real-life problems. \n\n"}
{"id": "1808.04164", "contents": "Title: Automatic Reference-Based Evaluation of Pronoun Translation Misses the\n  Point Abstract: We compare the performance of the APT and AutoPRF metrics for pronoun\ntranslation against a manually annotated dataset comprising human judgements as\nto the correctness of translations of the PROTEST test suite. Although there is\nsome correlation with the human judgements, a range of issues limit the\nperformance of the automated metrics. Instead, we recommend the use of\nsemi-automatic metrics and test suites in place of fully automatic metrics. \n\n"}
{"id": "1808.04189", "contents": "Title: Rapid Adaptation of Neural Machine Translation to New Languages Abstract: This paper examines the problem of adapting neural machine translation\nsystems to new, low-resourced languages (LRLs) as effectively and rapidly as\npossible. We propose methods based on starting with massively multilingual\n\"seed models\", which can be trained ahead-of-time, and then continuing training\non data related to the LRL. We contrast a number of strategies, leading to a\nnovel, simple, yet effective method of \"similar-language regularization\", where\nwe jointly train on both a LRL of interest and a similar high-resourced\nlanguage to prevent over-fitting to small LRL data. Experiments demonstrate\nthat massively multilingual models, even without any explicit adaptation, are\nsurprisingly effective, achieving BLEU scores of up to 15.5 with no data from\nthe LRL, and that the proposed similar-language regularization method improves\nover other adaptation methods by 1.7 BLEU points average over 4 LRL settings.\nCode to reproduce experiments at https://github.com/neubig/rapid-adaptation \n\n"}
{"id": "1808.04339", "contents": "Title: Disentangled Representation Learning for Non-Parallel Text Style\n  Transfer Abstract: This paper tackles the problem of disentangling the latent variables of style\nand content in language models. We propose a simple yet effective approach,\nwhich incorporates auxiliary multi-task and adversarial objectives, for label\nprediction and bag-of-words prediction, respectively. We show, both\nqualitatively and quantitatively, that the style and content are indeed\ndisentangled in the latent space. This disentangled latent representation\nlearning method is applied to style transfer on non-parallel corpora. We\nachieve substantially better results in terms of transfer accuracy, content\npreservation and language fluency, in comparison to previous state-of-the-art\napproaches. \n\n"}
{"id": "1808.04446", "contents": "Title: Visual Reasoning with Multi-hop Feature Modulation Abstract: Recent breakthroughs in computer vision and natural language processing have\nspurred interest in challenging multi-modal tasks such as visual\nquestion-answering and visual dialogue. For such tasks, one successful approach\nis to condition image-based convolutional network computation on language via\nFeature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and\nshifting. We propose to generate the parameters of FiLM layers going up the\nhierarchy of a convolutional network in a multi-hop fashion rather than all at\nonce, as in prior work. By alternating between attending to the language input\nand generating FiLM layer parameters, this approach is better able to scale to\nsettings with longer input sequences such as dialogue. We demonstrate that\nmulti-hop FiLM generation achieves state-of-the-art for the short input\nsequence task ReferIt --- on-par with single-hop FiLM generation --- while also\nsignificantly outperforming prior state-of-the-art and single-hop FiLM\ngeneration on the GuessWhat?! visual dialogue task. \n\n"}
{"id": "1808.04928", "contents": "Title: Deep EHR: Chronic Disease Prediction Using Medical Notes Abstract: Early detection of preventable diseases is important for better disease\nmanagement, improved inter-ventions, and more efficient health-care resource\nallocation. Various machine learning approacheshave been developed to utilize\ninformation in Electronic Health Record (EHR) for this task. Majorityof\nprevious attempts, however, focus on structured fields and lose the vast amount\nof information inthe unstructured notes. In this work we propose a general\nmulti-task framework for disease onsetprediction that combines both free-text\nmedical notes and structured information. We compareperformance of different\ndeep learning architectures including CNN, LSTM and hierarchical models.In\ncontrast to traditional text-based prediction models, our approach does not\nrequire disease specificfeature engineering, and can handle negations and\nnumerical values that exist in the text. Ourresults on a cohort of about 1\nmillion patients show that models using text outperform modelsusing just\nstructured data, and that models capable of using numerical values and\nnegations in thetext, in addition to the raw text, further improve performance.\nAdditionally, we compare differentvisualization methods for medical\nprofessionals to interpret model predictions. \n\n"}
{"id": "1808.05558", "contents": "Title: The DALPHI annotation framework & how its pre-annotations can improve\n  annotator efficiency Abstract: Producing the required amounts of training data for machine learning and NLP\ntasks often involves human annotators doing very repetitive and monotonous\nwork. In this paper, we present and evaluate our novel annotation framework\nDALPHI, which facilitates the annotation process by providing the annotator\nwith suggestions generated by an automated, active-learning based assistance\nsystem. In a study with 66 participants, we demonstrate on the exemplary task\nof annotating named entities in text documents that with this assistance system\nthe annotation processes can be improved with respect to the quality and\nquantity of produced annotations, even if the pre-annotations provided by the\nassistance system are at a recall level of only 50%. \n\n"}
{"id": "1808.05988", "contents": "Title: Attainment Ratings for Graph-Query Recommendation Abstract: The video game industry is larger than both the film and music industries\ncombined. Recommender systems for video games have received relatively scant\nacademic attention, despite the uniqueness of the medium and its data. In this\npaper, we introduce a graph-based recommender system that makes use of\ninteractivity, arguably the most significant feature of video gaming. We show\nthat the use of implicit data that tracks user-game interactions and levels of\nattainment (e.g. Sony Playstation Trophies, Microsoft Xbox Achievements) has\nhigh predictive value when making recommendations. Furthermore, we argue that\nthe characteristics of the video gaming hobby (low cost, high duration,\nsocially relevant) make clear the necessity of personalized, individual\nrecommendations that can incorporate social networking information. We\ndemonstrate the natural suitability of graph-query based recommendation for\nthis purpose. \n\n"}
{"id": "1808.06079", "contents": "Title: Community detection in networks without observing edges Abstract: We develop a Bayesian hierarchical model to identify communities in networks\nfor which we do not observe the edges directly, but instead observe a series of\ninterdependent signals for each of the nodes. Fitting the model provides an\nend-to-end community detection algorithm that does not extract information as a\nsequence of point estimates but propagates uncertainties from the raw data to\nthe community labels. Our approach naturally supports multiscale community\ndetection as well as the selection of an optimal scale using model comparison.\nWe study the properties of the algorithm using synthetic data and apply it to\ndaily returns of constituents of the S&P100 index as well as climate data from\nUS cities. \n\n"}
{"id": "1808.06232", "contents": "Title: Lexicosyntactic Inference in Neural Models Abstract: We investigate neural models' ability to capture lexicosyntactic inferences:\ninferences triggered by the interaction of lexical and syntactic information.\nWe take the task of event factuality prediction as a case study and build a\nfactuality judgment dataset for all English clause-embedding verbs in various\nsyntactic contexts. We use this dataset, which we make publicly available, to\nprobe the behavior of current state-of-the-art neural systems, showing that\nthese systems make certain systematic errors that are clearly visible through\nthe lens of factuality prediction. \n\n"}
{"id": "1808.06414", "contents": "Title: Next Item Recommendation with Self-Attention Abstract: In this paper, we propose a novel sequence-aware recommendation model. Our\nmodel utilizes self-attention mechanism to infer the item-item relationship\nfrom user's historical interactions. With self-attention, it is able to\nestimate the relative weights of each item in user interaction trajectories to\nlearn better representations for user's transient interests. The model is\nfinally trained in a metric learning framework, taking both short-term and\nlong-term intentions into consideration. Experiments on a wide range of\ndatasets on different domains demonstrate that our approach outperforms the\nstate-of-the-art by a wide margin. \n\n"}
{"id": "1808.07699", "contents": "Title: End-to-End Neural Entity Linking Abstract: Entity Linking (EL) is an essential task for semantic text understanding and\ninformation extraction. Popular methods separately address the Mention\nDetection (MD) and Entity Disambiguation (ED) stages of EL, without leveraging\ntheir mutual dependency. We here propose the first neural end-to-end EL system\nthat jointly discovers and links entities in a text document. The main idea is\nto consider all possible spans as potential mentions and learn contextual\nsimilarity scores over their entity candidates that are useful for both MD and\nED decisions. Key components are context-aware mention embeddings, entity\nembeddings and a probabilistic mention - entity map, without demanding other\nengineered features. Empirically, we show that our end-to-end method\nsignificantly outperforms popular systems on the Gerbil platform when enough\ntraining data is available. Conversely, if testing datasets follow different\nannotation conventions compared to the training set (e.g. queries/ tweets vs\nnews documents), our ED model coupled with a traditional NER system offers the\nbest or second best EL accuracy. \n\n"}
{"id": "1808.07724", "contents": "Title: Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs Abstract: This paper addresses the problem of mapping natural language text to\nknowledge base entities. The mapping process is approached as a composition of\na phrase or a sentence into a point in a multi-dimensional entity space\nobtained from a knowledge graph. The compositional model is an LSTM equipped\nwith a dynamic disambiguation mechanism on the input word embeddings (a\nMulti-Sense LSTM), addressing polysemy issues. Further, the knowledge base\nspace is prepared by collecting random walks from a graph enhanced with textual\nfeatures, which act as a set of semantic bridges between text and knowledge\nbase entities. The ideas of this work are demonstrated on large-scale\ntext-to-entity mapping and entity classification tasks, with state of the art\nresults. \n\n"}
{"id": "1808.07733", "contents": "Title: Revisiting the Importance of Encoding Logic Rules in Sentiment\n  Classification Abstract: We analyze the performance of different sentiment classification models on\nsyntactically complex inputs like A-but-B sentences. The first contribution of\nthis analysis addresses reproducible research: to meaningfully compare\ndifferent models, their accuracies must be averaged over far more random seeds\nthan what has traditionally been reported. With proper averaging in place, we\nnotice that the distillation model described in arXiv:1603.06318v4 [cs.LG],\nwhich incorporates explicit logic rules for sentiment classification, is\nineffective. In contrast, using contextualized ELMo embeddings\n(arXiv:1802.05365v2 [cs.CL]) instead of logic rules yields significantly better\nperformance. Additionally, we provide analysis and visualizations that\ndemonstrate ELMo's ability to implicitly learn logic rules. Finally, a\ncrowdsourced analysis reveals how ELMo outperforms baseline models even on\nsentences with ambiguous sentiment labels. \n\n"}
{"id": "1808.07910", "contents": "Title: The Importance of Generation Order in Language Modeling Abstract: Neural language models are a critical component of state-of-the-art systems\nfor machine translation, summarization, audio transcription, and other tasks.\nThese language models are almost universally autoregressive in nature,\ngenerating sentences one token at a time from left to right. This paper studies\nthe influence of token generation order on model quality via a novel two-pass\nlanguage model that produces partially-filled sentence \"templates\" and then\nfills in missing tokens. We compare various strategies for structuring these\ntwo passes and observe a surprisingly large variation in model quality. We find\nthe most effective strategy generates function words in the first pass followed\nby content words in the second. We believe these experimental results justify a\nmore extensive investigation of generation order for neural language models. \n\n"}
{"id": "1808.07913", "contents": "Title: Improving Abstraction in Text Summarization Abstract: Abstractive text summarization aims to shorten long text documents into a\nhuman readable form that contains the most important facts from the original\ndocument. However, the level of actual abstraction as measured by novel phrases\nthat do not appear in the source document remains low in existing approaches.\nWe propose two techniques to improve the level of abstraction of generated\nsummaries. First, we decompose the decoder into a contextual network that\nretrieves relevant parts of the source document, and a pretrained language\nmodel that incorporates prior knowledge about language generation. Second, we\npropose a novelty metric that is optimized directly through policy learning to\nencourage the generation of novel phrases. Our model achieves results\ncomparable to state-of-the-art models, as determined by ROUGE scores and human\nevaluations, while achieving a significantly higher level of abstraction as\nmeasured by n-gram overlap with the source document. \n\n"}
{"id": "1808.07982", "contents": "Title: Proximal Policy Optimization and its Dynamic Version for Sequence\n  Generation Abstract: In sequence generation task, many works use policy gradient for model\noptimization to tackle the intractable backpropagation issue when maximizing\nthe non-differentiable evaluation metrics or fooling the discriminator in\nadversarial learning. In this paper, we replace policy gradient with proximal\npolicy optimization (PPO), which is a proved more efficient reinforcement\nlearning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We\ndemonstrate the efficacy of PPO and PPO-dynamic on conditional sequence\ngeneration tasks including synthetic experiment and chit-chat chatbot. The\nresults show that PPO and PPO-dynamic can beat policy gradient by stability and\nperformance. \n\n"}
{"id": "1808.08270", "contents": "Title: Robust Text Classifier on Test-Time Budgets Abstract: We propose a generic and interpretable learning framework for building robust\ntext classification model that achieves accuracy comparable to full models\nunder test-time budget constraints. Our approach learns a selector to identify\nwords that are relevant to the prediction tasks and passes them to the\nclassifier for processing. The selector is trained jointly with the classifier\nand directly learns to incorporate with the classifier. We further propose a\ndata aggregation scheme to improve the robustness of the classifier. Our\nlearning framework is general and can be incorporated with any type of text\nclassification model. On real-world data, we show that the proposed approach\nimproves the performance of a given classifier and speeds up the model with a\nmere loss in accuracy performance. \n\n"}
{"id": "1808.08627", "contents": "Title: Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation Abstract: As opposed to manual feature engineering which is tedious and difficult to\nscale, network representation learning has attracted a surge of research\ninterests as it automates the process of feature learning on graphs. The\nlearned low-dimensional node vector representation is generalizable and eases\nthe knowledge discovery process on graphs by enabling various off-the-shelf\nmachine learning tools to be directly applied. Recent research has shown that\nthe past decade of network embedding approaches either explicitly factorize a\ncarefully designed matrix to obtain the low-dimensional node vector\nrepresentation or are closely related to implicit matrix factorization, with\nthe fundamental assumption that the factorized node connectivity matrix is\nlow-rank. Nonetheless, the global low-rank assumption does not necessarily hold\nespecially when the factorized matrix encodes complex node interactions, and\nthe resultant single low-rank embedding matrix is insufficient to capture all\nthe observed connectivity patterns. In this regard, we propose a novel\nmulti-level network embedding framework BoostNE, which can learn multiple\nnetwork embedding representations of different granularity from coarse to fine\nwithout imposing the prevalent global low-rank assumption. The proposed BoostNE\nmethod is also in line with the successful gradient boosting method in ensemble\nlearning as multiple weak embeddings lead to a stronger and more effective one.\nWe assess the effectiveness of the proposed BoostNE framework by comparing it\nwith existing state-of-the-art network embedding methods on various datasets,\nand the experimental results corroborate the superiority of the proposed\nBoostNE network embedding framework. \n\n"}
{"id": "1808.08744", "contents": "Title: Comparing Attention-based Convolutional and Recurrent Neural Networks:\n  Success and Limitations in Machine Reading Comprehension Abstract: We propose a machine reading comprehension model based on the\ncompare-aggregate framework with two-staged attention that achieves\nstate-of-the-art results on the MovieQA question answering dataset. To\ninvestigate the limitations of our model as well as the behavioral difference\nbetween convolutional and recurrent neural networks, we generate adversarial\nexamples to confuse the model and compare to human performance. Furthermore, we\nassess the generalizability of our model by analyzing its differences to human\ninference, \n\n"}
{"id": "1808.08780", "contents": "Title: Improving Cross-Lingual Word Embeddings by Meeting in the Middle Abstract: Cross-lingual word embeddings are becoming increasingly important in\nmultilingual NLP. Recently, it has been shown that these embeddings can be\neffectively learned by aligning two disjoint monolingual vector spaces through\nlinear transformations, using no more than a small bilingual dictionary as\nsupervision. In this work, we propose to apply an additional transformation\nafter the initial alignment step, which moves cross-lingual synonyms towards a\nmiddle point between them. By applying this transformation our aim is to obtain\na better cross-lingual integration of the vector spaces. In addition, and\nperhaps surprisingly, the monolingual spaces also improve by this\ntransformation. This is in contrast to the original alignment, which is\ntypically learned such that the structure of the monolingual spaces is\npreserved. Our experiments confirm that the resulting cross-lingual embeddings\noutperform state-of-the-art models in both monolingual and cross-lingual\nevaluation tasks. \n\n"}
{"id": "1808.08836", "contents": "Title: A strong baseline for question relevancy ranking Abstract: The best systems at the SemEval-16 and SemEval-17 community question\nanswering shared tasks -- a task that amounts to question relevancy ranking --\ninvolve complex pipelines and manual feature engineering. Despite this, many of\nthese still fail at beating the IR baseline, i.e., the rankings provided by\nGoogle's search engine. We present a strong baseline for question relevancy\nranking by training a simple multi-task feed forward network on a bag of 14\ndistance measures for the input question pair. This baseline model, which is\nfast to train and uses only language-independent features, outperforms the best\nshared task systems on the task of retrieving relevant previously asked\nquestions. \n\n"}
{"id": "1808.09407", "contents": "Title: Implementation Notes for the Soft Cosine Measure Abstract: The standard bag-of-words vector space model (VSM) is efficient, and\nubiquitous in information retrieval, but it underestimates the similarity of\ndocuments with the same meaning, but different terminology. To overcome this\nlimitation, Sidorov et al. proposed the Soft Cosine Measure (SCM) that\nincorporates term similarity relations. Charlet and Damnati showed that the SCM\nis highly effective in question answering (QA) systems. However, the\northonormalization algorithm proposed by Sidorov et al. has an impractical time\ncomplexity of $\\mathcal O(n^4)$, where n is the size of the vocabulary.\n  In this paper, we prove a tighter lower worst-case time complexity bound of\n$\\mathcal O(n^3)$. We also present an algorithm for computing the similarity\nbetween documents and we show that its worst-case time complexity is $\\mathcal\nO(1)$ given realistic conditions. Lastly, we describe implementation in\ngeneral-purpose vector databases such as Annoy, and Faiss and in the inverted\nindices of text search engines such as Apache Lucene, and ElasticSearch. Our\nresults enable the deployment of the SCM in real-world information retrieval\nsystems. \n\n"}
{"id": "1808.09602", "contents": "Title: Multi-Task Identification of Entities, Relations, and Coreference for\n  Scientific Knowledge Graph Construction Abstract: We introduce a multi-task setup of identifying and classifying entities,\nrelations, and coreference clusters in scientific articles. We create SciERC, a\ndataset that includes annotations for all three tasks and develop a unified\nframework called Scientific Information Extractor (SciIE) for with shared span\nrepresentations. The multi-task setup reduces cascading errors between tasks\nand leverages cross-sentence relations through coreference links. Experiments\nshow that our multi-task model outperforms previous models in scientific\ninformation extraction without using any domain-specific features. We further\nshow that the framework supports construction of a scientific knowledge graph,\nwhich we use to analyze information in scientific literature. \n\n"}
{"id": "1808.10009", "contents": "Title: Learning a Policy for Opportunistic Active Learning Abstract: Active learning identifies data points to label that are expected to be the\nmost useful in improving a supervised model. Opportunistic active learning\nincorporates active learning into interactive tasks that constrain possible\nqueries during interactions. Prior work has shown that opportunistic active\nlearning can be used to improve grounding of natural language descriptions in\nan interactive object retrieval task. In this work, we use reinforcement\nlearning for such an object retrieval task, to learn a policy that effectively\ntrades off task completion with model improvement that would benefit future\ntasks. \n\n"}
{"id": "1808.10261", "contents": "Title: Centroid estimation based on symmetric KL divergence for Multinomial\n  text classification problem Abstract: We define a new method to estimate centroid for text classification based on\nthe symmetric KL-divergence between the distribution of words in training\ndocuments and their class centroids. Experiments on several standard data sets\nindicate that the new method achieves substantial improvements over the\ntraditional classifiers. \n\n"}
{"id": "1808.10432", "contents": "Title: Attaining the Unattainable? Reassessing Claims of Human Parity in Neural\n  Machine Translation Abstract: We reassess a recent study (Hassan et al., 2018) that claimed that machine\ntranslation (MT) has reached human parity for the translation of news from\nChinese into English, using pairwise ranking and considering three variables\nthat were not taken into account in that previous study: the language in which\nthe source side of the test set was originally written, the translation\nproficiency of the evaluators, and the provision of inter-sentential context.\nIf we consider only original source text (i.e. not translated from another\nlanguage, or translationese), then we find evidence showing that human parity\nhas not been achieved. We compare the judgments of professional translators\nagainst those of non-experts and discover that those of the experts result in\nhigher inter-annotator agreement and better discrimination between human and\nmachine translations. In addition, we analyse the human translations of the\ntest set and identify important translation issues. Finally, based on these\nfindings, we provide a set of recommendations for future human evaluations of\nMT. \n\n"}
{"id": "1808.10584", "contents": "Title: Learning to Describe Differences Between Pairs of Similar Images Abstract: In this paper, we introduce the task of automatically generating text to\ndescribe the differences between two similar images. We collect a new dataset\nby crowd-sourcing difference descriptions for pairs of image frames extracted\nfrom video-surveillance footage. Annotators were asked to succinctly describe\nall the differences in a short paragraph. As a result, our novel dataset\nprovides an opportunity to explore models that align language and vision, and\ncapture visual salience. The dataset may also be a useful benchmark for\ncoherent multi-sentence generation. We perform a firstpass visual analysis that\nexposes clusters of differing pixels as a proxy for object-level differences.\nWe propose a model that captures visual salience by using a latent variable to\nalign clusters of differing pixels with output sentences. We find that, for\nboth single-sentence generation and as well as multi-sentence generation, the\nproposed model outperforms the models that use attention alone. \n\n"}
{"id": "1809.00378", "contents": "Title: Neural Character-based Composition Models for Abuse Detection Abstract: The advent of social media in recent years has fed into some highly\nundesirable phenomena such as proliferation of offensive language, hate speech,\nsexist remarks, etc. on the Internet. In light of this, there have been several\nefforts to automate the detection and moderation of such abusive content.\nHowever, deliberate obfuscation of words by users to evade detection poses a\nserious challenge to the effectiveness of these efforts. The current state of\nthe art approaches to abusive language detection, based on recurrent neural\nnetworks, do not explicitly address this problem and resort to a generic OOV\n(out of vocabulary) embedding for unseen words. However, in using a single\nembedding for all unseen words we lose the ability to distinguish between\nobfuscated and non-obfuscated or rare words. In this paper, we address this\nproblem by designing a model that can compose embeddings for unseen words. We\nexperimentally demonstrate that our approach significantly advances the current\nstate of the art in abuse detection on datasets from two different domains,\nnamely Twitter and Wikipedia talk page. \n\n"}
{"id": "1809.00388", "contents": "Title: MTNT: A Testbed for Machine Translation of Noisy Text Abstract: Noisy or non-standard input text can cause disastrous mistranslations in most\nmodern Machine Translation (MT) systems, and there has been growing research\ninterest in creating noise-robust MT systems. However, as of yet there are no\npublicly available parallel corpora of with naturally occurring noisy inputs\nand translations, and thus previous work has resorted to evaluating on\nsynthetically created datasets. In this paper, we propose a benchmark dataset\nfor Machine Translation of Noisy Text (MTNT), consisting of noisy comments on\nReddit (www.reddit.com) and professionally sourced translations. We\ncommissioned translations of English comments into French and Japanese, as well\nas French and Japanese comments into English, on the order of 7k-37k sentences\nper language pair. We qualitatively and quantitatively examine the types of\nnoise included in this dataset, then demonstrate that existing MT models fail\nbadly on a number of noise-related phenomena, even after performing adaptation\non a small training set of in-domain data. This indicates that this dataset can\nprovide an attractive testbed for methods tailored to handling noisy text in\nMT. The data is publicly available at www.cs.cmu.edu/~pmichel1/mtnt/. \n\n"}
{"id": "1809.00494", "contents": "Title: Belittling the Source: Trustworthiness Indicators to Obfuscate Fake News\n  on the Web Abstract: With the growth of the internet, the number of fake-news online has been\nproliferating every year. The consequences of such phenomena are manifold,\nranging from lousy decision-making process to bullying and violence episodes.\nTherefore, fact-checking algorithms became a valuable asset. To this aim, an\nimportant step to detect fake-news is to have access to a credibility score for\na given information source. However, most of the widely used Web indicators\nhave either been shut-down to the public (e.g., Google PageRank) or are not\nfree for use (Alexa Rank). Further existing databases are short-manually\ncurated lists of online sources, which do not scale. Finally, most of the\nresearch on the topic is theoretical-based or explore confidential data in a\nrestricted simulation environment. In this paper we explore current research,\nhighlight the challenges and propose solutions to tackle the problem of\nclassifying websites into a credibility scale. The proposed model automatically\nextracts source reputation cues and computes a credibility factor, providing\nvaluable insights which can help in belittling dubious and confirming trustful\nunknown websites. Experimental results outperform state of the art in the\n2-classes and 5-classes setting. \n\n"}
{"id": "1809.00640", "contents": "Title: Deep learning for language understanding of mental health concepts\n  derived from Cognitive Behavioural Therapy Abstract: In recent years, we have seen deep learning and distributed representations\nof words and sentences make impact on a number of natural language processing\ntasks, such as similarity, entailment and sentiment analysis. Here we introduce\na new task: understanding of mental health concepts derived from Cognitive\nBehavioural Therapy (CBT). We define a mental health ontology based on the CBT\nprinciples, annotate a large corpus where this phenomena is exhibited and\nperform understanding using deep learning and distributed representations. Our\nresults show that the performance of deep learning models combined with word\nembeddings or sentence embeddings significantly outperform non-deep-learning\nmodels in this difficult task. This understanding module will be an essential\ncomponent of a statistical dialogue system delivering therapy. \n\n"}
{"id": "1809.00782", "contents": "Title: Open Domain Question Answering Using Early Fusion of Knowledge Bases and\n  Text Abstract: Open Domain Question Answering (QA) is evolving from complex pipelined\nsystems to end-to-end deep neural networks. Specialized neural models have been\ndeveloped for extracting answers from either text alone or Knowledge Bases\n(KBs) alone. In this paper we look at a more practical setting, namely QA over\nthe combination of a KB and entity-linked text, which is appropriate when an\nincomplete KB is available with a large text corpus. Building on recent\nadvances in graph representation learning we propose a novel model, GRAFT-Net,\nfor extracting answers from a question-specific subgraph containing text and KB\nentities and relations. We construct a suite of benchmark tasks for this\nproblem, varying the difficulty of questions, the amount of training data, and\nKB completeness. We show that GRAFT-Net is competitive with the\nstate-of-the-art when tested using either KBs or text alone, and vastly\noutperforms existing methods in the combined setting. Source code is available\nat https://github.com/OceanskySun/GraftNet . \n\n"}
{"id": "1809.00832", "contents": "Title: Improving the Expressiveness of Deep Learning Frameworks with Recursion Abstract: Recursive neural networks have widely been used by researchers to handle\napplications with recursively or hierarchically structured data. However,\nembedded control flow deep learning frameworks such as TensorFlow, Theano,\nCaffe2, and MXNet fail to efficiently represent and execute such neural\nnetworks, due to lack of support for recursion. In this paper, we add recursion\nto the programming model of existing frameworks by complementing their design\nwith recursive execution of dataflow graphs as well as additional APIs for\nrecursive definitions. Unlike iterative implementations, which can only\nunderstand the topological index of each node in recursive data structures, our\nrecursive implementation is able to exploit the recursive relationships between\nnodes for efficient execution based on parallel computation. We present an\nimplementation on TensorFlow and evaluation results with various recursive\nneural network models, showing that our recursive implementation not only\nconveys the recursive nature of recursive neural networks better than other\nimplementations, but also uses given resources more effectively to reduce\ntraining and inference time. \n\n"}
{"id": "1809.01074", "contents": "Title: A Novel Neural Sequence Model with Multiple Attentions for Word Sense\n  Disambiguation Abstract: Word sense disambiguation (WSD) is a well researched problem in computational\nlinguistics. Different research works have approached this problem in different\nways. Some state of the art results that have been achieved for this problem\nare by supervised models in terms of accuracy, but they often fall behind\nflexible knowledge-based solutions which use engineered features as well as\nhuman annotators to disambiguate every target word. This work focuses on\nbridging this gap using neural sequence models incorporating the well-known\nattention mechanism. The main gist of our work is to combine multiple\nattentions on different linguistic features through weights and to provide a\nunified framework for doing this. This weighted attention allows the model to\neasily disambiguate the sense of an ambiguous word by attending over a suitable\nportion of a sentence. Our extensive experiments show that multiple attention\nenables a more versatile encoder-decoder model leading to state of the art\nresults. \n\n"}
{"id": "1809.01299", "contents": "Title: Policy Shaping and Generalized Update Equations for Semantic Parsing\n  from Denotations Abstract: Semantic parsing from denotations faces two key challenges in model training:\n(1) given only the denotations (e.g., answers), search for good candidate\nsemantic parses, and (2) choose the best model update algorithm. We propose\neffective and general solutions to each of them. Using policy shaping, we bias\nthe search procedure towards semantic parses that are more compatible to the\ntext, which provide better supervision signals for training. In addition, we\npropose an update equation that generalizes three different families of\nlearning algorithms, which enables fast model exploration. When experimented on\na recently proposed sequential question answering dataset, our framework leads\nto a new state-of-the-art model that outperforms previous work by 5.0% absolute\non exact match accuracy. \n\n"}
{"id": "1809.01452", "contents": "Title: Sentylic at IEST 2018: Gated Recurrent Neural Network and Capsule\n  Network Based Approach for Implicit Emotion Detection Abstract: In this paper, we present the system we have used for the Implicit WASSA 2018\nImplicit Emotion Shared Task. The task is to predict the emotion of a tweet of\nwhich the explicit mentions of emotion terms have been removed. The idea is to\ncome up with a model which has the ability to implicitly identify the emotion\nexpressed given the context words. We have used a Gated Recurrent Neural\nNetwork (GRU) and a Capsule Network based model for the task. Pre-trained word\nembeddings have been utilized to incorporate contextual knowledge about words\ninto the model. GRU layer learns latent representations using the input word\nembeddings. Subsequent Capsule Network layer learns high-level features from\nthat hidden representation. The proposed model managed to achieve a macro-F1\nscore of 0.692. \n\n"}
{"id": "1809.01477", "contents": "Title: A Supervised Learning Approach For Heading Detection Abstract: As the Portable Document Format (PDF) file format increases in popularity,\nresearch in analysing its structure for text extraction and analysis is\nnecessary. Detecting headings can be a crucial component of classifying and\nextracting meaningful data. This research involves training a supervised\nlearning model to detect headings with features carefully selected through\nrecursive feature elimination. The best performing classifier had an accuracy\nof 96.95%, sensitivity of 0.986 and a specificity of 0.953. This research into\nheading detection contributes to the field of PDF based text extraction and can\nbe applied to the automation of large scale PDF text analysis in a variety of\nprofessional and policy based contexts. \n\n"}
{"id": "1809.01478", "contents": "Title: Weakly-Supervised Neural Text Classification Abstract: Deep neural networks are gaining increasing popularity for the classic text\nclassification task, due to their strong expressive power and less requirement\nfor feature engineering. Despite such attractiveness, neural text\nclassification models suffer from the lack of training data in many real-world\napplications. Although many semi-supervised and weakly-supervised text\nclassification models exist, they cannot be easily applied to deep neural\nmodels and meanwhile support limited supervision types. In this paper, we\npropose a weakly-supervised method that addresses the lack of training data in\nneural text classification. Our method consists of two modules: (1) a\npseudo-document generator that leverages seed information to generate\npseudo-labeled documents for model pre-training, and (2) a self-training module\nthat bootstraps on real unlabeled data for model refinement. Our method has the\nflexibility to handle different types of weak supervision and can be easily\nintegrated into existing deep neural models for text classification. We have\nperformed extensive experiments on three real-world datasets from different\ndomains. The results demonstrate that our proposed method achieves inspiring\nperformance without requiring excessive training data and outperforms baseline\nmethods significantly. \n\n"}
{"id": "1809.01479", "contents": "Title: UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification Abstract: The Fact Extraction and VERification (FEVER) shared task was launched to\nsupport the development of systems able to verify claims by extracting\nsupporting or refuting facts from raw text. The shared task organizers provide\na large-scale dataset for the consecutive steps involved in claim verification,\nin particular, document retrieval, fact extraction, and claim classification.\nIn this paper, we present our claim verification pipeline approach, which,\naccording to the preliminary results, scored third in the shared task, out of\n23 competing systems. For the document retrieval, we implemented a new entity\nlinking approach. In order to be able to rank candidate facts and classify a\nclaim on the basis of several selected facts, we introduce two extensions to\nthe Enhanced LSTM (ESIM). \n\n"}
{"id": "1809.01499", "contents": "Title: Extractive Adversarial Networks: High-Recall Explanations for\n  Identifying Personal Attacks in Social Media Posts Abstract: We introduce an adversarial method for producing high-recall explanations of\nneural text classifier decisions. Building on an existing architecture for\nextractive explanations via hard attention, we add an adversarial layer which\nscans the residual of the attention for remaining predictive signal. Motivated\nby the important domain of detecting personal attacks in social media comments,\nwe additionally demonstrate the importance of manually setting a semantically\nappropriate `default' behavior for the model by explicitly manipulating its\nbias term. We develop a validation set of human-annotated personal attacks to\nevaluate the impact of these changes. \n\n"}
{"id": "1809.01703", "contents": "Title: HyperML: A Boosting Metric Learning Approach in Hyperbolic Space for\n  Recommender Systems Abstract: This paper investigates the notion of learning user and item representations\nin non-Euclidean space. Specifically, we study the connection between metric\nlearning in hyperbolic space and collaborative filtering by exploring Mobius\ngyrovector spaces where the formalism of the spaces could be utilized to\ngeneralize the most common Euclidean vector operations. Overall, this work aims\nto bridge the gap between Euclidean and hyperbolic geometry in recommender\nsystems through metric learning approach. We propose HyperML (Hyperbolic Metric\nLearning), a conceptually simple but highly effective model for boosting the\nperformance. Via a series of extensive experiments, we show that our proposed\nHyperML not only outperforms their Euclidean counterparts, but also achieves\nstate-of-the-art performance on multiple benchmark datasets, demonstrating the\neffectiveness of personalized recommendation in hyperbolic geometry. \n\n"}
{"id": "1809.01812", "contents": "Title: Noise Contrastive Estimation and Negative Sampling for Conditional\n  Models: Consistency and Statistical Efficiency Abstract: Noise Contrastive Estimation (NCE) is a powerful parameter estimation method\nfor log-linear models, which avoids calculation of the partition function or\nits derivatives at each training step, a computationally demanding step in many\ncases. It is closely related to negative sampling methods, now widely used in\nNLP. This paper considers NCE-based estimation of conditional models.\nConditional models are frequently encountered in practice; however there has\nnot been a rigorous theoretical analysis of NCE in this setting, and we will\nargue there are subtle but important questions when generalizing NCE to the\nconditional case. In particular, we analyze two variants of NCE for conditional\nmodels: one based on a classification objective, the other based on a ranking\nobjective. We show that the ranking-based variant of NCE gives consistent\nparameter estimates under weaker assumptions than the classification-based\nmethod; we analyze the statistical efficiency of the ranking-based and\nclassification-based variants of NCE; finally we describe experiments on\nsynthetic data and language modeling showing the effectiveness and trade-offs\nof both methods. \n\n"}
{"id": "1809.01816", "contents": "Title: Visual Coreference Resolution in Visual Dialog using Neural Module\n  Networks Abstract: Visual dialog entails answering a series of questions grounded in an image,\nusing dialog history as context. In addition to the challenges found in visual\nquestion answering (VQA), which can be seen as one-round dialog, visual dialog\nencompasses several more. We focus on one such problem called visual\ncoreference resolution that involves determining which words, typically noun\nphrases and pronouns, co-refer to the same entity/object instance in an image.\nThis is crucial, especially for pronouns (e.g., `it'), as the dialog agent must\nfirst link it to a previous coreference (e.g., `boat'), and only then can rely\non the visual grounding of the coreference `boat' to reason about the pronoun\n`it'. Prior work (in visual dialog) models visual coreference resolution either\n(a) implicitly via a memory network over history, or (b) at a coarse level for\nthe entire question; and not explicitly at a phrase level of granularity. In\nthis work, we propose a neural module network architecture for visual dialog by\nintroducing two novel modules - Refer and Exclude - that perform explicit,\ngrounded, coreference resolution at a finer word level. We demonstrate the\neffectiveness of our model on MNIST Dialog, a visually simple yet\ncoreference-wise complex dataset, by achieving near perfect accuracy, and on\nVisDial, a large and challenging visual dialog dataset on real images, where\nour model outperforms other approaches, and is more interpretable, grounded,\nand consistent qualitatively. \n\n"}
{"id": "1809.02079", "contents": "Title: Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue\n  Models Abstract: We present two categories of model-agnostic adversarial strategies that\nreveal the weaknesses of several generative, task-oriented dialogue models:\nShould-Not-Change strategies that evaluate over-sensitivity to small and\nsemantics-preserving edits, as well as Should-Change strategies that test if a\nmodel is over-stable against subtle yet semantics-changing modifications. We\nnext perform adversarial training with each strategy, employing a max-margin\napproach for negative generative examples. This not only makes the target\ndialogue model more robust to the adversarial inputs, but also helps it perform\nsignificantly better on the original inputs. Moreover, training on all\nstrategies combined achieves further improvements, achieving a new\nstate-of-the-art performance on the original task (also verified via human\nevaluation). In addition to adversarial training, we also address the\nrobustness task at the model-level, by feeding it subword units as both inputs\nand outputs, and show that the resulting model is equally competitive, requires\nonly 1/4 of the original vocabulary size, and is robust to one of the\nadversarial strategies (to which the original model is vulnerable) even without\nadversarial training. \n\n"}
{"id": "1809.02337", "contents": "Title: Information-Theoretic Active Learning for Content-Based Image Retrieval Abstract: We propose Information-Theoretic Active Learning (ITAL), a novel batch-mode\nactive learning method for binary classification, and apply it for acquiring\nmeaningful user feedback in the context of content-based image retrieval.\nInstead of combining different heuristics such as uncertainty, diversity, or\ndensity, our method is based on maximizing the mutual information between the\npredicted relevance of the images and the expected user feedback regarding the\nselected batch. We propose suitable approximations to this computationally\ndemanding problem and also integrate an explicit model of user behavior that\naccounts for possible incorrect labels and unnameable instances. Furthermore,\nour approach does not only take the structure of the data but also the expected\nmodel output change caused by the user feedback into account. In contrast to\nother methods, ITAL turns out to be highly flexible and provides\nstate-of-the-art performance across various datasets, such as MIRFLICKR and\nImageNet. \n\n"}
{"id": "1809.02494", "contents": "Title: Meteorologists and Students: A resource for language grounding of\n  geographical descriptors Abstract: We present a data resource which can be useful for research purposes on\nlanguage grounding tasks in the context of geographical referring expression\ngeneration. The resource is composed of two data sets that encompass 25\ndifferent geographical descriptors and a set of associated graphical\nrepresentations, drawn as polygons on a map by two groups of human subjects:\nteenage students and expert meteorologists. \n\n"}
{"id": "1809.02735", "contents": "Title: Operations Guided Neural Networks for High Fidelity Data-To-Text\n  Generation Abstract: Recent neural models for data-to-text generation are mostly based on\ndata-driven end-to-end training over encoder-decoder networks. Even though the\ngenerated texts are mostly fluent and informative, they often generate\ndescriptions that are not consistent with the input structured data. This is a\ncritical issue especially in domains that require inference or calculations\nover raw data. In this paper, we attempt to improve the fidelity of neural\ndata-to-text generation by utilizing pre-executed symbolic operations. We\npropose a framework called Operation-guided Attention-based\nsequence-to-sequence network (OpAtt), with a specifically designed gating\nmechanism as well as a quantization module for operation results to utilize\ninformation from pre-executed operations. Experiments on two sports datasets\nshow our proposed method clearly improves the fidelity of the generated texts\nto the input structured data. \n\n"}
{"id": "1809.02790", "contents": "Title: The Lower The Simpler: Simplifying Hierarchical Recurrent Models Abstract: To improve the training efficiency of hierarchical recurrent models without\ncompromising their performance, we propose a strategy named as `the lower the\nsimpler', which is to simplify the baseline models by making the lower layers\nsimpler than the upper layers. We carry out this strategy to simplify two\ntypical hierarchical recurrent models, namely Hierarchical Recurrent\nEncoder-Decoder (HRED) and R-NET, whose basic building block is GRU.\nSpecifically, we propose Scalar Gated Unit (SGU), which is a simplified variant\nof GRU, and use it to replace the GRUs at the middle layers of HRED and R-NET.\nBesides, we also use Fixed-size Ordinally-Forgetting Encoding (FOFE), which is\nan efficient encoding method without any trainable parameter, to replace the\nGRUs at the bottom layers of HRED and R-NET. The experimental results show that\nthe simplified HRED and the simplified R-NET contain significantly less\ntrainable parameters, consume significantly less training time, and achieve\nslightly better performance than their baseline models. \n\n"}
{"id": "1809.03632", "contents": "Title: Detecting Gang-Involved Escalation on Social Media Using Context Abstract: Gang-involved youth in cities such as Chicago have increasingly turned to\nsocial media to post about their experiences and intents online. In some\nsituations, when they experience the loss of a loved one, their online\nexpression of emotion may evolve into aggression towards rival gangs and\nultimately into real-world violence. In this paper, we present a novel system\nfor detecting Aggression and Loss in social media. Our system features the use\nof domain-specific resources automatically derived from a large unlabeled\ncorpus, and contextual representations of the emotional and semantic content of\nthe user's recent tweets as well as their interactions with other users.\nIncorporating context in our Convolutional Neural Network (CNN) leads to a\nsignificant improvement. \n\n"}
{"id": "1809.03901", "contents": "Title: Mitigating Confirmation Bias on Twitter by Recommending Opposing Views Abstract: In this work, we propose a content-based recommendation approach to increase\nexposure to opposing beliefs and opinions. Our aim is to help provide users\nwith more diverse viewpoints on issues, which are discussed in partisan groups\nfrom different perspectives. Since due to the backfire effect, people's\noriginal beliefs tend to strengthen when challenged with counter evidence, we\nneed to expose them to opposing viewpoints at the right time. The preliminary\nwork presented here describes our first step into this direction. As\nillustrative showcase, we take the political debate on Twitter around the\npresidency of Donald Trump. \n\n"}
{"id": "1809.03985", "contents": "Title: On The Alignment Problem In Multi-Head Attention-Based Neural Machine\n  Translation Abstract: This work investigates the alignment problem in state-of-the-art multi-head\nattention models based on the transformer architecture. We demonstrate that\nalignment extraction in transformer models can be improved by augmenting an\nadditional alignment head to the multi-head source-to-target attention\ncomponent. This is used to compute sharper attention weights. We describe how\nto use the alignment head to achieve competitive performance. To study the\neffect of adding the alignment head, we simulate a dictionary-guided\ntranslation task, where the user wants to guide translation using pre-defined\ndictionary entries. Using the proposed approach, we achieve up to $3.8$ % BLEU\nimprovement when using the dictionary, in comparison to $2.4$ % BLEU in the\nbaseline case. We also propose alignment pruning to speed up decoding in\nalignment-based neural machine translation (ANMT), which speeds up translation\nby a factor of $1.8$ without loss in translation performance. We carry out\nexperiments on the shared WMT 2016 English$\\to$Romanian news task and the BOLT\nChinese$\\to$English discussion forum task. \n\n"}
{"id": "1809.04047", "contents": "Title: AWE: Asymmetric Word Embedding for Textual Entailment Abstract: Textual entailment is a fundamental task in natural language processing. It\nrefers to the directional relation between text fragments such that the\n\"premise\" can infer \"hypothesis\". In recent years deep learning methods have\nachieved great success in this task. Many of them have considered the\ninter-sentence word-word interactions between the premise-hypothesis pairs,\nhowever, few of them considered the \"asymmetry\" of these interactions.\nDifferent from paraphrase identification or sentence similarity evaluation,\ntextual entailment is essentially determining a directional (asymmetric)\nrelation between the premise and the hypothesis. In this paper, we propose a\nsimple but effective way to enhance existing textual entailment algorithms by\nusing asymmetric word embeddings. Experimental results on SciTail and SNLI\ndatasets show that the learned asymmetric word embeddings could significantly\nimprove the word-word interaction based textual entailment models. It is\nnoteworthy that the proposed AWE-DeIsTe model can get 2.1% accuracy improvement\nover prior state-of-the-art on SciTail. \n\n"}
{"id": "1809.04163", "contents": "Title: Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word\n  Vector Specialization Abstract: Semantic specialization is the process of fine-tuning pre-trained\ndistributional word vectors using external lexical knowledge (e.g., WordNet) to\naccentuate a particular semantic relation in the specialized vector space.\nWhile post-processing specialization methods are applicable to arbitrary\ndistributional vectors, they are limited to updating only the vectors of words\noccurring in external lexicons (i.e., seen words), leaving the vectors of all\nother words unchanged. We propose a novel approach to specializing the full\ndistributional vocabulary. Our adversarial post-specialization method\npropagates the external lexical knowledge to the full distributional space. We\nexploit words seen in the resources as training examples for learning a global\nspecialization function. This function is learned by combining a standard\nL2-distance loss with an adversarial loss: the adversarial component produces\nmore realistic output vectors. We show the effectiveness and robustness of the\nproposed method across three languages and on three tasks: word similarity,\ndialog state tracking, and lexical simplification. We report consistent\nimprovements over distributional word vectors and vectors specialized by other\nstate-of-the-art specialization frameworks. Finally, we also propose a\ncross-lingual transfer method for zero-shot specialization which successfully\nspecializes a full target distributional space without any lexical knowledge in\nthe target language and without any bilingual data. \n\n"}
{"id": "1809.04214", "contents": "Title: Automatic, Personalized, and Flexible Playlist Generation using\n  Reinforcement Learning Abstract: Songs can be well arranged by professional music curators to form a riveting\nplaylist that creates engaging listening experiences. However, it is\ntime-consuming for curators to timely rearrange these playlists for fitting\ntrends in future. By exploiting the techniques of deep learning and\nreinforcement learning, in this paper, we consider music playlist generation as\na language modeling problem and solve it by the proposed attention language\nmodel with policy gradient. We develop a systematic and interactive approach so\nthat the resulting playlists can be tuned flexibly according to user\npreferences. Considering a playlist as a sequence of words, we first train our\nattention RNN language model on baseline recommended playlists. By optimizing\nsuitable imposed reward functions, the model is thus refined for corresponding\npreferences. The experimental results demonstrate that our approach not only\ngenerates coherent playlists automatically but is also able to flexibly\nrecommend personalized playlists for diversity, novelty and freshness. \n\n"}
{"id": "1809.04267", "contents": "Title: Knowledge Based Machine Reading Comprehension Abstract: Machine reading comprehension (MRC) requires reasoning about both the\nknowledge involved in a document and knowledge about the world. However,\nexisting datasets are typically dominated by questions that can be well solved\nby context matching, which fail to test this capability. To encourage the\nprogress on knowledge-based reasoning in MRC, we present knowledge-based MRC in\nthis paper, and build a new dataset consisting of 40,047 question-answer pairs.\nThe annotation of this dataset is designed so that successfully answering the\nquestions requires understanding and the knowledge involved in a document. We\nimplement a framework consisting of both a question answering model and a\nquestion generation model, both of which take the knowledge extracted from the\ndocument as well as relevant facts from an external knowledge base such as\nFreebase/ProBase/Reverb/NELL. Results show that incorporating side information\nfrom external KB improves the accuracy of the baseline question answer system.\nWe compare it with a standard MRC model BiDAF, and also provide the difficulty\nof the dataset and lay out remaining challenges. \n\n"}
{"id": "1809.04271", "contents": "Title: Knowledge-Aware Conversational Semantic Parsing Over Web Tables Abstract: Conversational semantic parsing over tables requires knowledge acquiring and\nreasoning abilities, which have not been well explored by current\nstate-of-the-art approaches. Motivated by this fact, we propose a\nknowledge-aware semantic parser to improve parsing performance by integrating\nvarious types of knowledge. In this paper, we consider three types of\nknowledge, including grammar knowledge, expert knowledge, and external resource\nknowledge. First, grammar knowledge empowers the model to effectively replicate\npreviously generated logical form, which effectively handles the co-reference\nand ellipsis phenomena in conversation Second, based on expert knowledge, we\npropose a decomposable model, which is more controllable compared with\ntraditional end-to-end models that put all the burdens of learning on\ntrial-and-error in an end-to-end way. Third, external resource knowledge, i.e.,\nprovided by a pre-trained language model or an entity typing model, is used to\nimprove the representation of question and table for a better semantic\nunderstanding. We conduct experiments on the SequentialQA dataset. Results show\nthat our knowledge-aware model outperforms the state-of-the-art approaches.\nIncremental experimental results also prove the usefulness of various\nknowledge. Further analysis shows that our approach has the ability to derive\nthe meaning representation of a context-dependent utterance by leveraging\npreviously generated outcomes. \n\n"}
{"id": "1809.04313", "contents": "Title: Chinese Poetry Generation with a Salient-Clue Mechanism Abstract: As a precious part of the human cultural heritage, Chinese poetry has\ninfluenced people for generations. Automatic poetry composition is a challenge\nfor AI. In recent years, significant progress has been made in this area\nbenefiting from the development of neural networks. However, the coherence in\nmeaning, theme or even artistic conception for a generated poem as a whole\nstill remains a big problem. In this paper, we propose a novel Salient-Clue\nmechanism for Chinese poetry generation. Different from previous work which\ntried to exploit all the context information, our model selects the most\nsalient characters automatically from each so-far generated line to gradually\nform a salient clue, which is utilized to guide successive poem generation\nprocess so as to eliminate interruptions and improve coherence. Besides, our\nmodel can be flexibly extended to control the generated poem in different\naspects, for example, poetry style, which further enhances the coherence.\nExperimental results show that our model is very effective, outperforming three\nstrong baselines. \n\n"}
{"id": "1809.04458", "contents": "Title: Unsupervised Representation Learning of Speech for Dialect\n  Identification Abstract: In this paper, we explore the use of a factorized hierarchical variational\nautoencoder (FHVAE) model to learn an unsupervised latent representation for\ndialect identification (DID). An FHVAE can learn a latent space that separates\nthe more static attributes within an utterance from the more dynamic attributes\nby encoding them into two different sets of latent variables. Useful factors\nfor dialect identification, such as phonetic or linguistic content, are encoded\nby a segmental latent variable, while irrelevant factors that are relatively\nconstant within a sequence, such as a channel or a speaker information, are\nencoded by a sequential latent variable. The disentanglement property makes the\nsegmental latent variable less susceptible to channel and speaker variation,\nand thus reduces degradation from channel domain mismatch. We demonstrate that\non fully-supervised DID tasks, an end-to-end model trained on the features\nextracted from the FHVAE model achieves the best performance, compared to the\nsame model trained on conventional acoustic features and an i-vector based\nsystem. Moreover, we also show that the proposed approach can leverage a large\namount of unlabeled data for FHVAE training to learn domain-invariant features\nfor DID, and significantly improve the performance in a low-resource condition,\nwhere the labels for the in-domain data are not available. \n\n"}
{"id": "1809.05296", "contents": "Title: Skeleton-to-Response: Dialogue Generation Guided by Retrieval Memory Abstract: For dialogue response generation, traditional generative models generate\nresponses solely from input queries. Such models rely on insufficient\ninformation for generating a specific response since a certain query could be\nanswered in multiple ways. Consequentially, those models tend to output generic\nand dull responses, impeding the generation of informative utterances.\nRecently, researchers have attempted to fill the information gap by exploiting\ninformation retrieval techniques. When generating a response for a current\nquery, similar dialogues retrieved from the entire training data are considered\nas an additional knowledge source. While this may harvest massive information,\nthe generative models could be overwhelmed, leading to undesirable performance.\nIn this paper, we propose a new framework which exploits retrieval results via\na skeleton-then-response paradigm. At first, a skeleton is generated by\nrevising the retrieved responses. Then, a novel generative model uses both the\ngenerated skeleton and the original query for response generation. Experimental\nresults show that our approaches significantly improve the diversity and\ninformativeness of the generated responses. \n\n"}
{"id": "1809.05685", "contents": "Title: Commentary on Quantum-Inspired Information Retrieval Abstract: There have been suggestions within the Information Retrieval (IR) community\nthat quantum mechanics (QM) can be used to help formalise the foundations of\nIR. The invoked connection to QM is mathematical rather than physical. The\nproposed ideas are concerned with information which is encoded, processed and\naccessed in classical computers. However, some of the suggestions have been\nthoroughly muddled with questions about applying techniques of quantum\ninformation theory in IR, and it is often unclear whether or not the suggestion\nis to perform actual quantum information processing on the information. This\npaper is an attempt to provide some conceptual clarity on the emerging issues. \n\n"}
{"id": "1809.05814", "contents": "Title: Development of deep learning algorithms to categorize free-text notes\n  pertaining to diabetes: convolution neural networks achieve higher accuracy\n  than support vector machines Abstract: Health professionals can use natural language processing (NLP) technologies\nwhen reviewing electronic health records (EHR). Machine learning free-text\nclassifiers can help them identify problems and make critical decisions. We aim\nto develop deep learning neural network algorithms that identify EHR progress\nnotes pertaining to diabetes and validate the algorithms at two institutions.\nThe data used are 2,000 EHR progress notes retrieved from patients with\ndiabetes and all notes were annotated manually as diabetic or non-diabetic.\nSeveral deep learning classifiers were developed, and their performances were\nevaluated with the area under the ROC curve (AUC). The convolutional neural\nnetwork (CNN) model with a separable convolution layer accurately identified\ndiabetes-related notes in the Brigham and Womens Hospital testing set with the\nhighest AUC of 0.975. Deep learning classifiers can be used to identify EHR\nprogress notes pertaining to diabetes. In particular, the CNN-based classifier\ncan achieve a higher AUC than an SVM-based classifier. \n\n"}
{"id": "1809.05916", "contents": "Title: Curriculum-Based Neighborhood Sampling For Sequence Prediction Abstract: The task of multi-step ahead prediction in language models is challenging\nconsidering the discrepancy between training and testing. At test time, a\nlanguage model is required to make predictions given past predictions as input,\ninstead of the past targets that are provided during training. This difference,\nknown as exposure bias, can lead to the compounding of errors along a generated\nsequence at test time.\n  In order to improve generalization in neural language models and address\ncompounding errors, we propose a curriculum learning based method that\ngradually changes an initially deterministic teacher policy to a gradually more\nstochastic policy, which we refer to as \\textit{Nearest-Neighbor Replacement\nSampling}. A chosen input at a given timestep is replaced with a sampled\nnearest neighbor of the past target with a truncated probability proportional\nto the cosine similarity between the original word and its top $k$ most similar\nwords. This allows the teacher to explore alternatives when the teacher\nprovides a sub-optimal policy or when the initial policy is difficult for the\nlearner to model. The proposed strategy is straightforward, online and requires\nlittle additional memory requirements. We report our main findings on two\nlanguage modelling benchmarks and find that the proposed approach performs\nparticularly well when used in conjunction with scheduled sampling, that too\nattempts to mitigate compounding errors in language models. \n\n"}
{"id": "1809.06432", "contents": "Title: Node Classification for Signed Social Networks Using Diffuse Interface\n  Methods Abstract: Signed networks contain both positive and negative kinds of interactions like\nfriendship and enmity. The task of node classification in non-signed graphs has\nproven to be beneficial in many real world applications, yet extensions to\nsigned networks remain largely unexplored. In this paper we introduce the first\nanalysis of node classification in signed social networks via diffuse interface\nmethods based on the Ginzburg-Landau functional together with different\nextensions of the graph Laplacian to signed networks. We show that blending the\ninformation from both positive and negative interactions leads to performance\nimprovement in real signed social networks, consistently outperforming the\ncurrent state of the art. \n\n"}
{"id": "1809.06444", "contents": "Title: Robust Spoken Language Understanding via Paraphrasing Abstract: Learning intents and slot labels from user utterances is a fundamental step\nin all spoken language understanding (SLU) and dialog systems. State-of-the-art\nneural network based methods, after deployment, often suffer from performance\ndegradation on encountering paraphrased utterances, and out-of-vocabulary\nwords, rarely observed in their training set. We address this challenging\nproblem by introducing a novel paraphrasing based SLU model which can be\nintegrated with any existing SLU model in order to improve their overall\nperformance. We propose two new paraphrase generators using RNN and\nsequence-to-sequence based neural networks, which are suitable for our\napplication. Our experiments on existing benchmark and in house datasets\ndemonstrate the robustness of our models to rare and complex paraphrased\nutterances, even under adversarial test distributions. \n\n"}
{"id": "1809.06858", "contents": "Title: FRAGE: Frequency-Agnostic Word Representation Abstract: Continuous word representation (aka word embedding) is a basic building block\nin many neural network-based models used in natural language processing tasks.\nAlthough it is widely accepted that words with similar semantics should be\nclose to each other in the embedding space, we find that word embeddings\nlearned in several tasks are biased towards word frequency: the embeddings of\nhigh-frequency and low-frequency words lie in different subregions of the\nembedding space, and the embedding of a rare word and a popular word can be far\nfrom each other even if they are semantically similar. This makes learned word\nembeddings ineffective, especially for rare words, and consequently limits the\nperformance of these neural network models. In this paper, we develop a neat,\nsimple yet effective way to learn \\emph{FRequency-AGnostic word Embedding}\n(FRAGE) using adversarial training. We conducted comprehensive studies on ten\ndatasets across four natural language processing tasks, including word\nsimilarity, language modeling, machine translation and text classification.\nResults show that with FRAGE, we achieve higher performance than the baselines\nin all tasks. \n\n"}
{"id": "1809.06963", "contents": "Title: Multi-task Learning with Sample Re-weighting for Machine Reading\n  Comprehension Abstract: We propose a multi-task learning framework to learn a joint Machine Reading\nComprehension (MRC) model that can be applied to a wide range of MRC tasks in\ndifferent domains. Inspired by recent ideas of data selection in machine\ntranslation, we develop a novel sample re-weighting scheme to assign\nsample-specific weights to the loss. Empirical study shows that our approach\ncan be applied to many existing MRC models. Combined with contextual\nrepresentations from pre-trained language models (such as ELMo), we achieve new\nstate-of-the-art results on a set of MRC benchmark datasets. We release our\ncode at https://github.com/xycforgithub/MultiTask-MRC. \n\n"}
{"id": "1809.07257", "contents": "Title: MTLE: A Multitask Learning Encoder of Visual Feature Representations for\n  Video and Movie Description Abstract: Learning visual feature representations for video analysis is a daunting task\nthat requires a large amount of training samples and a proper generalization\nframework. Many of the current state of the art methods for video captioning\nand movie description rely on simple encoding mechanisms through recurrent\nneural networks to encode temporal visual information extracted from video\ndata. In this paper, we introduce a novel multitask encoder-decoder framework\nfor automatic semantic description and captioning of video sequences. In\ncontrast to current approaches, our method relies on distinct decoders that\ntrain a visual encoder in a multitask fashion. Our system does not depend\nsolely on multiple labels and allows for a lack of training data working even\nwith datasets where only one single annotation is viable per video. Our method\nshows improved performance over current state of the art methods in several\nmetrics on multi-caption and single-caption datasets. To the best of our\nknowledge, our method is the first method to use a multitask approach for\nencoding video features. Our method demonstrates its robustness on the Large\nScale Movie Description Challenge (LSMDC) 2017 where our method won the movie\ndescription task and its results were ranked among other competitors as the\nmost helpful for the visually impaired. \n\n"}
{"id": "1809.07688", "contents": "Title: Inferring Multiplex Diffusion Network via Multivariate Marked Hawkes\n  Process Abstract: Understanding the diffusion in social network is an important task. However,\nthis task is challenging since (1) the network structure is usually hidden with\nonly observations of events like \"post\" or \"repost\" associated with each node,\nand (2) the interactions between nodes encompass multiple distinct patterns\nwhich in turn affect the diffusion patterns. For instance, social interactions\nseldom develop on a single channel, and multiple relationships can bind pairs\nof people due to their various common interests. Most previous work considers\nonly one of these two challenges which is apparently unrealistic. In this\npaper, we study the problem of \\emph{inferring multiplex network} in social\nnetworks. We propose the Multiplex Diffusion Model (MDM) which incorporates the\nmultivariate marked Hawkes process and topic model to infer the multiplex\nstructure of social network. A MCMC based algorithm is developed to infer the\nlatent multiplex structure and to estimate the node-related parameters. We\nevaluate our model based on both synthetic and real-world datasets. The results\nshow that our model is more effective in terms of uncovering the multiplex\nnetwork structure. \n\n"}
{"id": "1809.07691", "contents": "Title: A Survey on Theoretical Advances of Community Detection in Networks Abstract: Real-world networks usually have community structure, that is, nodes are\ngrouped into densely connected communities. Community detection is one of the\nmost popular and best-studied research topics in network science and has\nattracted attention in many different fields, including computer science,\nstatistics, social sciences, among others. Numerous approaches for community\ndetection have been proposed in literature, from ad-hoc algorithms to\nsystematic model-based approaches. The large number of available methods leads\nto a fundamental question: whether a certain method can provide consistent\nestimates of community labels. The stochastic blockmodel (SBM) and its variants\nprovide a convenient framework for the study of such problems. This article is\na survey on the recent theoretical advances of community detection. The authors\nreview a number of community detection methods and their theoretical\nproperties, including graph cut methods, profile likelihoods, the\npseudo-likelihood method, the variational method, belief propagation, spectral\nclustering, and semidefinite relaxations of the SBM. The authors also briefly\ndiscuss other research topics in community detection such as robust community\ndetection, community detection with nodal covariates and model selection, as\nwell as suggest a few possible directions for future research. \n\n"}
{"id": "1809.07697", "contents": "Title: Higher-order Graph Convolutional Networks Abstract: Following the success of deep convolutional networks in various vision and\nspeech related tasks, researchers have started investigating generalizations of\nthe well-known technique for graph-structured data. A recently-proposed method\ncalled Graph Convolutional Networks has been able to achieve state-of-the-art\nresults in the task of node classification. However, since the proposed method\nrelies on localized first-order approximations of spectral graph convolutions,\nit is unable to capture higher-order interactions between nodes in the graph.\nIn this work, we propose a motif-based graph attention model, called Motif\nConvolutional Networks (MCNs), which generalizes past approaches by using\nweighted multi-hop motif adjacency matrices to capture higher-order\nneighborhoods. A novel attention mechanism is used to allow each individual\nnode to select the most relevant neighborhood to apply its filter. Experiments\nshow that our proposed method is able to achieve state-of-the-art results on\nthe semi-supervised node classification task. \n\n"}
{"id": "1809.07703", "contents": "Title: Fighting Redundancy and Model Decay with Embeddings Abstract: Every day, hundreds of millions of new Tweets containing over 40 languages of\never-shifting vernacular flow through Twitter. Models that attempt to extract\ninsight from this firehose of information must face the torrential covariate\nshift that is endemic to the Twitter platform. While regularly-retrained\nalgorithms can maintain performance in the face of this shift, fixed model\nfeatures that fail to represent new trends and tokens can quickly become stale,\nresulting in performance degradation. To mitigate this problem we employ\nlearned features, or embedding models, that can efficiently represent the most\nrelevant aspects of a data distribution. Sharing these embedding models across\nteams can also reduce redundancy and multiplicatively increase cross-team\nmodeling productivity. In this paper, we detail the commoditized tools,\nalgorithms and pipelines that we have developed and are developing at Twitter\nto regularly generate high quality, up-to-date embeddings and share them\nbroadly across the company. \n\n"}
{"id": "1809.07754", "contents": "Title: PriPeARL: A Framework for Privacy-Preserving Analytics and Reporting at\n  LinkedIn Abstract: Preserving privacy of users is a key requirement of web-scale analytics and\nreporting applications, and has witnessed a renewed focus in light of recent\ndata breaches and new regulations such as GDPR. We focus on the problem of\ncomputing robust, reliable analytics in a privacy-preserving manner, while\nsatisfying product requirements. We present PriPeARL, a framework for\nprivacy-preserving analytics and reporting, inspired by differential privacy.\nWe describe the overall design and architecture, and the key modeling\ncomponents, focusing on the unique challenges associated with privacy,\ncoverage, utility, and consistency. We perform an experimental study in the\ncontext of ads analytics and reporting at LinkedIn, thereby demonstrating the\ntradeoffs between privacy and utility needs, and the applicability of\nprivacy-preserving mechanisms to real-world data. We also highlight the lessons\nlearned from the production deployment of our system at LinkedIn. \n\n"}
{"id": "1809.08004", "contents": "Title: Multi-Dimensional, Multilayer, Nonlinear and Dynamic HITS Abstract: We introduce a ranking model for temporal multi-dimensional weighted and\ndirected networks based on the Perron eigenvector of a multi-homogeneous\norder-preserving map. The model extends to the temporal multilayer setting the\nHITS algorithm and defines five centrality vectors: two for the nodes, two for\nthe layers, and one for the temporal stamps. Nonlinearity is introduced in the\nstandard HITS model in order to guarantee existence and uniqueness of these\ncentrality vectors for any network, without any requirement on its connectivity\nstructure. We introduce a globally convergent power iteration like algorithm\nfor the computation of the centrality vectors. Numerical experiments on\nreal-world networks are performed in order to assess the effectiveness of the\nproposed model and showcase the performance of the accompanying algorithm. \n\n"}
{"id": "1809.08198", "contents": "Title: Low rank methods for multiple network alignment Abstract: Multiple network alignment is the problem of identifying similar and related\nregions in a given set of networks. While there are a large number of effective\ntechniques for pairwise problems with two networks that scale in terms of\nedges, these cannot be readily extended to align multiple networks as the\ncomputational complexity will tend to grow exponentially with the number of\nnetworks.In this paper we introduce a new multiple network alignment algorithm\nand framework that is effective at aligning thousands of networks with\nthousands of nodes. The key enabling technique of our algorithm is identifying\nan exact and easy to compute low-rank tensor structure inside of a principled\nheuristic procedure for pairwise network alignment called IsoRank. This can be\ncombined with a new algorithm for $k$-dimensional matching problems on low-rank\ntensors to produce the alignment. We demonstrate results on synthetic and\nreal-world problems that show our technique (i) is as good or better in terms\nof quality as existing methods, when they work on small problems, while running\nconsiderably faster and (ii) is able to scale to aligning a number of networks\nunreachable by current methods. We show in this paper that our method is the\nrealistic choice for aligning multiple networks when no prior information is\npresent. \n\n"}
{"id": "1809.08927", "contents": "Title: Adversarial Training in Affective Computing and Sentiment Analysis:\n  Recent Advances and Perspectives Abstract: Over the past few years, adversarial training has become an extremely active\nresearch topic and has been successfully applied to various Artificial\nIntelligence (AI) domains. As a potentially crucial technique for the\ndevelopment of the next generation of emotional AI systems, we herein provide a\ncomprehensive overview of the application of adversarial training to affective\ncomputing and sentiment analysis. Various representative adversarial training\nalgorithms are explained and discussed accordingly, aimed at tackling diverse\nchallenges associated with emotional AI systems. Further, we highlight a range\nof potential future research directions. We expect that this overview will help\nfacilitate the development of adversarial training for affective computing and\nsentiment analysis in both the academic and industrial communities. \n\n"}
{"id": "1809.10274", "contents": "Title: Semantically Invariant Text-to-Image Generation Abstract: Image captioning has demonstrated models that are capable of generating\nplausible text given input images or videos. Further, recent work in image\ngeneration has shown significant improvements in image quality when text is\nused as a prior. Our work ties these concepts together by creating an\narchitecture that can enable bidirectional generation of images and text. We\ncall this network Multi-Modal Vector Representation (MMVR). Along with MMVR, we\npropose two improvements to the text conditioned image generation. Firstly, a\nn-gram metric based cost function is introduced that generalizes the caption\nwith respect to the image. Secondly, multiple semantically similar sentences\nare shown to help in generating better images. Qualitative and quantitative\nevaluations demonstrate that MMVR improves upon existing text conditioned image\ngeneration results by over 20%, while integrating visual and text modalities. \n\n"}
{"id": "1809.10745", "contents": "Title: A Short Survey of Topological Data Analysis in Time Series and Systems\n  Analysis Abstract: Topological Data Analysis (TDA) is the collection of mathematical tools that\ncapture the structure of shapes in data. Despite computational topology and\ncomputational geometry, the utilization of TDA in time series and signal\nprocessing is relatively new. In some recent contributions, TDA has been\nutilized as an alternative to the conventional signal processing methods.\nSpecifically, TDA is been considered to deal with noisy signals and time\nseries. In these applications, TDA is used to find the shapes in data as the\nmain properties, while the other properties are assumed much less informative.\nIn this paper, we will review recent developments and contributions where\ntopological data analysis especially persistent homology has been applied to\ntime series analysis, dynamical systems and signal processing. We will cover\nproblem statements such as stability determination, risk analysis, systems\nbehaviour, and predicting critical transitions in financial markets. \n\n"}
{"id": "1809.10770", "contents": "Title: Point-of-Interest Recommendation: Exploiting Self-Attentive Autoencoders\n  with Neighbor-Aware Influence Abstract: The rapid growth of Location-based Social Networks (LBSNs) provides a great\nopportunity to satisfy the strong demand for personalized Point-of-Interest\n(POI) recommendation services. However, with the tremendous increase of users\nand POIs, POI recommender systems still face several challenging problems: (1)\nthe hardness of modeling non-linear user-POI interactions from implicit\nfeedback; (2) the difficulty of incorporating context information such as POIs'\ngeographical coordinates. To cope with these challenges, we propose a novel\nautoencoder-based model to learn the non-linear user-POI relations, namely\n\\textit{SAE-NAD}, which consists of a self-attentive encoder (SAE) and a\nneighbor-aware decoder (NAD). In particular, unlike previous works equally\ntreat users' checked-in POIs, our self-attentive encoder adaptively\ndifferentiates the user preference degrees in multiple aspects, by adopting a\nmulti-dimensional attention mechanism. To incorporate the geographical context\ninformation, we propose a neighbor-aware decoder to make users' reachability\nhigher on the similar and nearby neighbors of checked-in POIs, which is\nachieved by the inner product of POI embeddings together with the radial basis\nfunction (RBF) kernel. To evaluate the proposed model, we conduct extensive\nexperiments on three real-world datasets with many state-of-the-art baseline\nmethods and evaluation metrics. The experimental results demonstrate the\neffectiveness of our model. \n\n"}
{"id": "1810.00494", "contents": "Title: Ranking Paragraphs for Improving Answer Recall in Open-Domain Question\n  Answering Abstract: Recently, open-domain question answering (QA) has been combined with machine\ncomprehension models to find answers in a large knowledge source. As\nopen-domain QA requires retrieving relevant documents from text corpora to\nanswer questions, its performance largely depends on the performance of\ndocument retrievers. However, since traditional information retrieval systems\nare not effective in obtaining documents with a high probability of containing\nanswers, they lower the performance of QA systems. Simply extracting more\ndocuments increases the number of irrelevant documents, which also degrades the\nperformance of QA systems. In this paper, we introduce Paragraph Ranker which\nranks paragraphs of retrieved documents for a higher answer recall with less\nnoise. We show that ranking paragraphs and aggregating answers using Paragraph\nRanker improves performance of open-domain QA pipeline on the four open-domain\nQA datasets by 7.8% on average. \n\n"}
{"id": "1810.00679", "contents": "Title: Direct optimization of F-measure for retrieval-based personal question\n  answering Abstract: Recent advances in spoken language technologies and the introduction of many\ncustomer facing products, have given rise to a wide customer reliance on smart\npersonal assistants for many of their daily tasks. In this paper, we present a\nsystem to reduce users' cognitive load by extending personal assistants with\nlong-term personal memory where users can store and retrieve by voice,\narbitrary pieces of information. The problem is framed as a neural retrieval\nbased question answering system where answers are selected from previously\nstored user memories. We propose to directly optimize the end-to-end retrieval\nperformance, measured by the F1-score, using reinforcement learning, leading to\nbetter performance on our experimental test set(s). \n\n"}
{"id": "1810.00956", "contents": "Title: Challenges of Using Text Classifiers for Causal Inference Abstract: Causal understanding is essential for many kinds of decision-making, but\ncausal inference from observational data has typically only been applied to\nstructured, low-dimensional datasets. While text classifiers produce\nlow-dimensional outputs, their use in causal inference has not previously been\nstudied. To facilitate causal analyses based on language data, we consider the\nrole that text classifiers can play in causal inference through established\nmodeling mechanisms from the causality literature on missing data and\nmeasurement error. We demonstrate how to conduct causal analyses using text\nclassifiers on simulated and Yelp data, and discuss the opportunities and\nchallenges of future work that uses text data in causal inference. \n\n"}
{"id": "1810.02832", "contents": "Title: ResumeNet: A Learning-based Framework for Automatic Resume Quality\n  Assessment Abstract: Recruitment of appropriate people for certain positions is critical for any\ncompanies or organizations. Manually screening to select appropriate candidates\nfrom large amounts of resumes can be exhausted and time-consuming. However,\nthere is no public tool that can be directly used for automatic resume quality\nassessment (RQA). This motivates us to develop a method for automatic RQA.\nSince there is also no public dataset for model training and evaluation, we\nbuild a dataset for RQA by collecting around 10K resumes, which are provided by\na private resume management company. By investigating the dataset, we identify\nsome factors or features that could be useful to discriminate good resumes from\nbad ones, e.g., the consistency between different parts of a resume. Then a\nneural-network model is designed to predict the quality of each resume, where\nsome text processing techniques are incorporated. To deal with the label\ndeficiency issue in the dataset, we propose several variants of the model by\neither utilizing the pair/triplet-based loss, or introducing some\nsemi-supervised learning technique to make use of the abundant unlabeled data.\nBoth the presented baseline model and its variants are general and easy to\nimplement. Various popular criteria including the receiver operating\ncharacteristic (ROC) curve, F-measure and ranking-based average precision (AP)\nare adopted for model evaluation. We compare the different variants with our\nbaseline model. Since there is no public algorithm for RQA, we further compare\nour results with those obtained from a website that can score a resume.\nExperimental results in terms of different criteria demonstrate the\neffectiveness of the proposed method. We foresee that our approach would\ntransform the way of future human resources management. \n\n"}
{"id": "1810.02938", "contents": "Title: Co-Stack Residual Affinity Networks with Multi-level Attention\n  Refinement for Matching Text Sequences Abstract: Learning a matching function between two text sequences is a long standing\nproblem in NLP research. This task enables many potential applications such as\nquestion answering and paraphrase identification. This paper proposes Co-Stack\nResidual Affinity Networks (CSRAN), a new and universal neural architecture for\nthis problem. CSRAN is a deep architecture, involving stacked (multi-layered)\nrecurrent encoders. Stacked/Deep architectures are traditionally difficult to\ntrain, due to the inherent weaknesses such as difficulty with feature\npropagation and vanishing gradients. CSRAN incorporates two novel components to\ntake advantage of the stacked architecture. Firstly, it introduces a new\nbidirectional alignment mechanism that learns affinity weights by fusing\nsequence pairs across stacked hierarchies. Secondly, it leverages a multi-level\nattention refinement component between stacked recurrent layers. The key\nintuition is that, by leveraging information across all network hierarchies, we\ncan not only improve gradient flow but also improve overall performance. We\nconduct extensive experiments on six well-studied text sequence matching\ndatasets, achieving state-of-the-art performance on all. \n\n"}
{"id": "1810.02959", "contents": "Title: Higher-order Spectral Clustering for Heterogeneous Graphs Abstract: Higher-order connectivity patterns such as small induced sub-graphs called\ngraphlets (network motifs) are vital to understand the important components\n(modules/functional units) governing the configuration and behavior of complex\nnetworks. Existing work in higher-order clustering has focused on simple\nhomogeneous graphs with a single node/edge type. However, heterogeneous graphs\nconsisting of nodes and edges of different types are seemingly ubiquitous in\nthe real-world. In this work, we introduce the notion of typed-graphlet that\nexplicitly captures the rich (typed) connectivity patterns in heterogeneous\nnetworks. Using typed-graphlets as a basis, we develop a general principled\nframework for higher-order clustering in heterogeneous networks. The framework\nprovides mathematical guarantees on the optimality of the higher-order\nclustering obtained. The experiments demonstrate the effectiveness of the\nframework quantitatively for three important applications including (i)\nclustering, (ii) link prediction, and (iii) graph compression. In particular,\nthe approach achieves a mean improvement of 43x over all methods and graphs for\nclustering while achieving a 18.7% and 20.8% improvement for link prediction\nand graph compression, respectively. \n\n"}
{"id": "1810.03581", "contents": "Title: Improving the Transformer Translation Model with Document-Level Context Abstract: Although the Transformer translation model (Vaswani et al., 2017) has\nachieved state-of-the-art performance in a variety of translation tasks, how to\nuse document-level context to deal with discourse phenomena problematic for\nTransformer still remains a challenge. In this work, we extend the Transformer\nmodel with a new context encoder to represent document-level context, which is\nthen incorporated into the original encoder and decoder. As large-scale\ndocument-level parallel corpora are usually not available, we introduce a\ntwo-step training method to take full advantage of abundant sentence-level\nparallel corpora and limited document-level parallel corpora. Experiments on\nthe NIST Chinese-English datasets and the IWSLT French-English datasets show\nthat our approach improves over Transformer significantly. \n\n"}
{"id": "1810.03717", "contents": "Title: Comparing Models of Associative Meaning: An Empirical Investigation of\n  Reference in Simple Language Games Abstract: Simple reference games are of central theoretical and empirical importance in\nthe study of situated language use. Although language provides rich,\ncompositional truth-conditional semantics to facilitate reference, speakers and\nlisteners may sometimes lack the overall lexical and cognitive resources to\nguarantee successful reference through these means alone. However, language\nalso has rich associational structures that can serve as a further resource for\nachieving successful reference. Here we investigate this use of associational\ninformation in a setting where only associational information is available: a\nsimplified version of the popular game Codenames. Using optimal experiment\ndesign techniques, we compare a range of models varying in the type of\nassociative information deployed and in level of pragmatic sophistication\nagainst human behavior. In this setting, we find that listeners' behavior\nreflects direct bigram collocational associations more strongly than\nword-embedding or semantic knowledge graph-based associations and that there is\nlittle evidence for pragmatically sophisticated behavior by either speakers or\nlisteners of the type that might be predicted by recursive-reasoning models\nsuch as the Rational Speech Acts theory. These results shed light on the nature\nof the lexical resources that speakers and listeners can bring to bear in\nachieving reference through associative meaning alone. \n\n"}
{"id": "1810.05201", "contents": "Title: Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns Abstract: Coreference resolution is an important task for natural language\nunderstanding, and the resolution of ambiguous pronouns a longstanding\nchallenge. Nonetheless, existing corpora do not capture ambiguous pronouns in\nsufficient volume or diversity to accurately indicate the practical utility of\nmodels. Furthermore, we find gender bias in existing corpora and systems\nfavoring masculine entities. To address this, we present and release GAP, a\ngender-balanced labeled corpus of 8,908 ambiguous pronoun-name pairs sampled to\nprovide diverse coverage of challenges posed by real-world text. We explore a\nrange of baselines which demonstrate the complexity of the challenge, the best\nachieving just 66.9% F1. We show that syntactic structure and continuous neural\nmodels provide promising, complementary cues for approaching the challenge. \n\n"}
{"id": "1810.05682", "contents": "Title: Building Dynamic Knowledge Graphs from Text using Machine Reading\n  Comprehension Abstract: We propose a neural machine-reading model that constructs dynamic knowledge\ngraphs from procedural text. It builds these graphs recurrently for each step\nof the described procedure, and uses them to track the evolving states of\nparticipant entities. We harness and extend a recently proposed machine reading\ncomprehension (MRC) model to query for entity states, since these states are\ngenerally communicated in spans of text and MRC models perform well in\nextracting entity-centric spans. The explicit, structured, and evolving\nknowledge graph representations that our model constructs can be used in\ndownstream question answering tasks to improve machine comprehension of text,\nas we demonstrate empirically. On two comprehension tasks from the recently\nproposed PROPARA dataset (Dalvi et al., 2018), our model achieves\nstate-of-the-art results. We further show that our model is competitive on the\nRECIPES dataset (Kiddon et al., 2015), suggesting it may be generally\napplicable. We present some evidence that the model's knowledge graphs help it\nto impose commonsense constraints on its predictions. \n\n"}
{"id": "1810.07382", "contents": "Title: Analysis of Railway Accidents' Narratives Using Deep Learning Abstract: Automatic understanding of domain specific texts in order to extract useful\nrelationships for later use is a non-trivial task. One such relationship would\nbe between railroad accidents' causes and their correspondent descriptions in\nreports. From 2001 to 2016 rail accidents in the U.S. cost more than $4.6B.\nRailroads involved in accidents are required to submit an accident report to\nthe Federal Railroad Administration (FRA). These reports contain a variety of\nfixed field entries including primary cause of the accidents (a coded variable\nwith 389 values) as well as a narrative field which is a short text description\nof the accident. Although these narratives provide more information than a\nfixed field entry, the terminologies used in these reports are not easy to\nunderstand by a non-expert reader. Therefore, providing an assisting method to\nfill in the primary cause from such domain specific texts(narratives) would\nhelp to label the accidents with more accuracy. Another important question for\ntransportation safety is whether the reported accident cause is consistent with\nnarrative description. To address these questions, we applied deep learning\nmethods together with powerful word embeddings such as Word2Vec and GloVe to\nclassify accident cause values for the primary cause field using the text in\nthe narratives. The results show that such approaches can both accurately\nclassify accident causes based on report narratives and find important\ninconsistencies in accident reporting. \n\n"}
{"id": "1810.08398", "contents": "Title: STACL: Simultaneous Translation with Implicit Anticipation and\n  Controllable Latency using Prefix-to-Prefix Framework Abstract: Simultaneous translation, which translates sentences before they are\nfinished, is useful in many scenarios but is notoriously difficult due to\nword-order differences. While the conventional seq-to-seq framework is only\nsuitable for full-sentence translation, we propose a novel prefix-to-prefix\nframework for simultaneous translation that implicitly learns to anticipate in\na single translation model. Within this framework, we present a very simple yet\nsurprisingly effective wait-k policy trained to generate the target sentence\nconcurrently with the source sentence, but always k words behind. Experiments\nshow our strategy achieves low latency and reasonable quality (compared to\nfull-sentence translation) on 4 directions: zh<->en and de<->en. \n\n"}
{"id": "1810.08802", "contents": "Title: Hierarchical Text Generation using an Outline Abstract: Many challenges in natural language processing require generating text,\nincluding language translation, dialogue generation, and speech recognition.\nFor all of these problems, text generation becomes more difficult as the text\nbecomes longer. Current language models often struggle to keep track of\ncoherence for long pieces of text. Here, we attempt to have the model construct\nand use an outline of the text it generates to keep it focused. We find that\nthe usage of an outline improves perplexity. We do not find that using the\noutline improves human evaluation over a simpler baseline, revealing a\ndiscrepancy in perplexity and human perception. Similarly, hierarchical\ngeneration is not found to improve human evaluation scores. \n\n"}
{"id": "1810.09176", "contents": "Title: Node Representation Learning for Directed Graphs Abstract: We propose a novel approach for learning node representations in directed\ngraphs, which maintains separate views or embedding spaces for the two distinct\nnode roles induced by the directionality of the edges. We argue that the\nprevious approaches either fail to encode the edge directionality or their\nencodings cannot be generalized across tasks. With our simple \\emph{alternating\nrandom walk} strategy, we generate role specific vertex neighborhoods and train\nnode embeddings in their corresponding source/target roles while fully\nexploiting the semantics of directed graphs. We also unearth the limitations of\nevaluations on directed graphs in previous works and propose a clear strategy\nfor evaluating link prediction and graph reconstruction in directed graphs. We\nconduct extensive experiments to showcase our effectiveness on several\nreal-world datasets on link prediction, node classification and graph\nreconstruction tasks. We show that the embeddings from our approach are indeed\nrobust, generalizable and well performing across multiple kinds of tasks and\ngraphs. We show that we consistently outperform all baselines for node\nclassification task. In addition to providing a theoretical interpretation of\nour method we also show that we are considerably more robust than the other\ndirected graph approaches. \n\n"}
{"id": "1810.09305", "contents": "Title: WikiHow: A Large Scale Text Summarization Dataset Abstract: Sequence-to-sequence models have recently gained the state of the art\nperformance in summarization. However, not too many large-scale high-quality\ndatasets are available and almost all the available ones are mainly news\narticles with specific writing style. Moreover, abstractive human-style systems\ninvolving description of the content at a deeper level require data with higher\nlevels of abstraction. In this paper, we present WikiHow, a dataset of more\nthan 230,000 article and summary pairs extracted and constructed from an online\nknowledge base written by different human authors. The articles span a wide\nrange of topics and therefore represent high diversity styles. We evaluate the\nperformance of the existing methods on WikiHow to present its challenges and\nset some baselines to further improve it. \n\n"}
{"id": "1810.09379", "contents": "Title: Linguistic Legal Concept Extraction in Portuguese Abstract: This work investigates legal concepts and their expression in Portuguese,\nconcentrating on the \"Order of Attorneys of Brazil\" Bar exam. Using a corpus\nformed by a collection of multiple-choice questions, three norms related to the\nEthics part of the OAB exam, language resources (Princeton WordNet and\nOpenWordNet-PT) and tools (AntConc and Freeling), we began to investigate the\nconcepts and words missing from our repertory of concepts and words in\nPortuguese, the knowledge base OpenWordNet-PT. We add these concepts and words\nto OpenWordNet-PT and hence obtain a representation of these texts that is\n\"contained\" in the lexical knowledge base. \n\n"}
{"id": "1810.09587", "contents": "Title: Towards Universal Dialogue State Tracking Abstract: Dialogue state tracking is the core part of a spoken dialogue system. It\nestimates the beliefs of possible user's goals at every dialogue turn. However,\nfor most current approaches, it's difficult to scale to large dialogue domains.\nThey have one or more of following limitations: (a) Some models don't work in\nthe situation where slot values in ontology changes dynamically; (b) The number\nof model parameters is proportional to the number of slots; (c) Some models\nextract features based on hand-crafted lexicons. To tackle these challenges, we\npropose StateNet, a universal dialogue state tracker. It is independent of the\nnumber of values, shares parameters across all slots, and uses pre-trained word\nvectors instead of explicit semantic dictionaries. Our experiments on two\ndatasets show that our approach not only overcomes the limitations, but also\nsignificantly outperforms the performance of state-of-the-art approaches. \n\n"}
{"id": "1810.10183", "contents": "Title: Multi-Head Attention with Disagreement Regularization Abstract: Multi-head attention is appealing for the ability to jointly attend to\ninformation from different representation subspaces at different positions. In\nthis work, we introduce a disagreement regularization to explicitly encourage\nthe diversity among multiple attention heads. Specifically, we propose three\ntypes of disagreement regularization, which respectively encourage the\nsubspace, the attended positions, and the output representation associated with\neach attention head to be different from other heads. Experimental results on\nwidely-used WMT14 English-German and WMT17 Chinese-English translation tasks\ndemonstrate the effectiveness and universality of the proposed approach. \n\n"}
{"id": "1810.10222", "contents": "Title: Universal Language Model Fine-Tuning with Subword Tokenization for\n  Polish Abstract: Universal Language Model for Fine-tuning [arXiv:1801.06146] (ULMFiT) is one\nof the first NLP methods for efficient inductive transfer learning.\nUnsupervised pretraining results in improvements on many NLP tasks for English.\nIn this paper, we describe a new method that uses subword tokenization to adapt\nULMFiT to languages with high inflection. Our approach results in a new\nstate-of-the-art for the Polish language, taking first place in Task 3 of\nPolEval'18. After further training, our final model outperformed the second\nbest model by 35%. We have open-sourced our pretrained models and code. \n\n"}
{"id": "1810.11906", "contents": "Title: Semi-Supervised Translation with MMD Networks Abstract: This work aims to improve semi-supervised learning in a neural network\narchitecture by introducing a hybrid supervised and unsupervised cost function.\nThe unsupervised component is trained using a differentiable estimator of the\nMaximum Mean Discrepancy (MMD) distance between the network output and the\ntarget dataset. We introduce the notion of an $n$-channel network and several\nmethods to improve performance of these nets based on supervised\npre-initialization, and multi-scale kernels. This work investigates the\neffectiveness of these methods on language translation where very few quality\ntranslations are known \\textit{a priori}. We also present a thorough\ninvestigation of the hyper-parameter space of this method on both synthetic\ndata. \n\n"}
{"id": "1810.11954", "contents": "Title: A Knowledge-Grounded Multimodal Search-Based Conversational Agent Abstract: Multimodal search-based dialogue is a challenging new task: It extends\nvisually grounded question answering systems into multi-turn conversations with\naccess to an external database. We address this new challenge by learning a\nneural response generation system from the recently released Multimodal\nDialogue (MMD) dataset (Saha et al., 2017). We introduce a knowledge-grounded\nmultimodal conversational model where an encoded knowledge base (KB)\nrepresentation is appended to the decoder input. Our model substantially\noutperforms strong baselines in terms of text-based similarity measures (over 9\nBLEU points, 3 of which are solely due to the use of additional information\nfrom the KB. \n\n"}
{"id": "1810.12085", "contents": "Title: Extractive Summarization of EHR Discharge Notes Abstract: Patient summarization is essential for clinicians to provide coordinated care\nand practice effective communication. Automated summarization has the potential\nto save time, standardize notes, aid clinical decision making, and reduce\nmedical errors. Here we provide an upper bound on extractive summarization of\ndischarge notes and develop an LSTM model to sequentially label topics of\nhistory of present illness notes. We achieve an F1 score of 0.876, which\nindicates that this model can be employed to create a dataset for evaluation of\nextractive summarization methods. \n\n"}
{"id": "1810.12091", "contents": "Title: Embedding Geographic Locations for Modelling the Natural Environment\n  using Flickr Tags and Structured Data Abstract: Meta-data from photo-sharing websites such as Flickr can be used to obtain\nrich bag-of-words descriptions of geographic locations, which have proven\nvaluable, among others, for modelling and predicting ecological features. One\nimportant insight from previous work is that the descriptions obtained from\nFlickr tend to be complementary to the structured information that is available\nfrom traditional scientific resources. To better integrate these two diverse\nsources of information, in this paper we consider a method for learning vector\nspace embeddings of geographic locations. We show experimentally that this\nmethod improves on existing approaches, especially in cases where structured\ninformation is available. \n\n"}
{"id": "1810.12366", "contents": "Title: Do Explanations make VQA Models more Predictable to a Human? Abstract: A rich line of research attempts to make deep neural networks more\ntransparent by generating human-interpretable 'explanations' of their decision\nprocess, especially for interactive tasks like Visual Question Answering (VQA).\nIn this work, we analyze if existing explanations indeed make a VQA model --\nits responses as well as failures -- more predictable to a human. Surprisingly,\nwe find that they do not. On the other hand, we find that human-in-the-loop\napproaches that treat the model as a black-box do. \n\n"}
{"id": "1810.12406", "contents": "Title: Learning to Screen for Fast Softmax Inference on Large Vocabulary Neural\n  Networks Abstract: Neural language models have been widely used in various NLP tasks, including\nmachine translation, next word prediction and conversational agents. However,\nit is challenging to deploy these models on mobile devices due to their slow\nprediction speed, where the bottleneck is to compute top candidates in the\nsoftmax layer. In this paper, we introduce a novel softmax layer approximation\nalgorithm by exploiting the clustering structure of context vectors. Our\nalgorithm uses a light-weight screening model to predict a much smaller set of\ncandidate words based on the given context, and then conducts an exact softmax\nonly within that subset. Training such a procedure end-to-end is challenging as\ntraditional clustering methods are discrete and non-differentiable, and thus\nunable to be used with back-propagation in the training process. Using the\nGumbel softmax, we are able to train the screening model end-to-end on the\ntraining set to exploit data distribution. The algorithm achieves an order of\nmagnitude faster inference than the original softmax layer for predicting\ntop-$k$ words in various tasks such as beam search in machine translation or\nnext words prediction. For example, for machine translation task on German to\nEnglish dataset with around 25K vocabulary, we can achieve 20.4 times speed up\nwith 98.9\\% precision@1 and 99.3\\% precision@5 with the original softmax layer\nprediction, while state-of-the-art ~\\citep{MSRprediction} only achieves 6.7x\nspeedup with 98.7\\% precision@1 and 98.1\\% precision@5 for the same task. \n\n"}
{"id": "1810.12686", "contents": "Title: Evaluating Text GANs as Language Models Abstract: Generative Adversarial Networks (GANs) are a promising approach for text\ngeneration that, unlike traditional language models (LM), does not suffer from\nthe problem of ``exposure bias''. However, A major hurdle for understanding the\npotential of GANs for text generation is the lack of a clear evaluation metric.\nIn this work, we propose to approximate the distribution of text generated by a\nGAN, which permits evaluating them with traditional probability-based LM\nmetrics. We apply our approximation procedure on several GAN-based models and\nshow that they currently perform substantially worse than state-of-the-art LMs.\nOur evaluation procedure promotes better understanding of the relation between\nGANs and LMs, and can accelerate progress in GAN-based text generation. \n\n"}
{"id": "1810.12735", "contents": "Title: Spoken Language Understanding on the Edge Abstract: We consider the problem of performing Spoken Language Understanding (SLU) on\nsmall devices typical of IoT applications. Our contributions are twofold.\nFirst, we outline the design of an embedded, private-by-design SLU system and\nshow that it has performance on par with cloud-based commercial solutions.\nSecond, we release the datasets used in our experiments in the interest of\nreproducibility and in the hope that they can prove useful to the SLU\ncommunity. \n\n"}
{"id": "1810.12752", "contents": "Title: Long Short-Term Attention Abstract: Attention is an important cognition process of humans, which helps humans\nconcentrate on critical information during their perception and learning.\nHowever, although many machine learning models can remember information of\ndata, they have no the attention mechanism. For example, the long short-term\nmemory (LSTM) network is able to remember sequential information, but it cannot\npay special attention to part of the sequences. In this paper, we present a\nnovel model called long short-term attention (LSTA), which seamlessly\nintegrates the attention mechanism into the inner cell of LSTM. More than\nprocessing long short term dependencies, LSTA can focus on important\ninformation of the sequences with the attention mechanism. Extensive\nexperiments demonstrate that LSTA outperforms LSTM and related models on the\nsequence learning tasks. \n\n"}
{"id": "1810.12754", "contents": "Title: Recurrent Attention Unit Abstract: Recurrent Neural Network (RNN) has been successfully applied in many sequence\nlearning problems. Such as handwriting recognition, image description, natural\nlanguage processing and video motion analysis. After years of development,\nresearchers have improved the internal structure of the RNN and introduced many\nvariants. Among others, Gated Recurrent Unit (GRU) is one of the most widely\nused RNN model. However, GRU lacks the capability of adaptively paying\nattention to certain regions or locations, so that it may cause information\nredundancy or loss during leaning. In this paper, we propose a RNN model,\ncalled Recurrent Attention Unit (RAU), which seamlessly integrates the\nattention mechanism into the interior of GRU by adding an attention gate. The\nattention gate can enhance GRU's ability to remember long-term memory and help\nmemory cells quickly discard unimportant content. RAU is capable of extracting\ninformation from the sequential data by adaptively selecting a sequence of\nregions or locations and pay more attention to the selected regions during\nlearning. Extensive experiments on image classification, sentiment\nclassification and language modeling show that RAU consistently outperforms GRU\nand other baseline methods. \n\n"}
{"id": "1810.12897", "contents": "Title: Topic-Specific Sentiment Analysis Can Help Identify Political Ideology Abstract: Ideological leanings of an individual can often be gauged by the sentiment\none expresses about different issues. We propose a simple framework that\nrepresents a political ideology as a distribution of sentiment polarities\ntowards a set of topics. This representation can then be used to detect\nideological leanings of documents (speeches, news articles, etc.) based on the\nsentiments expressed towards different topics. Experiments performed using a\nwidely used dataset show the promise of our proposed approach that achieves\ncomparable performance to other methods despite being much simpler and more\ninterpretable. \n\n"}
{"id": "1811.00379", "contents": "Title: Helping each Other: A Framework for Customer-to-Customer Suggestion\n  Mining using a Semi-supervised Deep Neural Network Abstract: Suggestion mining is increasingly becoming an important task along with\nsentiment analysis. In today's cyberspace world, people not only express their\nsentiments and dispositions towards some entities or services, but they also\nspend considerable time sharing their experiences and advice to fellow\ncustomers and the product/service providers with two-fold agenda: helping\nfellow customers who are likely to share a similar experience, and motivating\nthe producer to bring specific changes in their offerings which would be more\nappreciated by the customers. In our current work, we propose a hybrid deep\nlearning model to identify whether a review text contains any suggestion. The\nmodel employs semi-supervised learning to leverage the useful information from\nthe large amount of unlabeled data. We evaluate the performance of our proposed\nmodel on a benchmark customer review dataset, comprising of the reviews of\nHotel and Electronics domains. Our proposed approach shows the F-scores of\n65.6% and 65.5% for the Hotel and Electronics review datasets, respectively.\nThese performances are significantly better compared to the existing\nstate-of-the-art system. \n\n"}
{"id": "1811.00403", "contents": "Title: Truly unsupervised acoustic word embeddings using weak top-down\n  constraints in encoder-decoder models Abstract: We investigate unsupervised models that can map a variable-duration speech\nsegment to a fixed-dimensional representation. In settings where unlabelled\nspeech is the only available resource, such acoustic word embeddings can form\nthe basis for \"zero-resource\" speech search, discovery and indexing systems.\nMost existing unsupervised embedding methods still use some supervision, such\nas word or phoneme boundaries. Here we propose the encoder-decoder\ncorrespondence autoencoder (EncDec-CAE), which, instead of true word segments,\nuses automatically discovered segments: an unsupervised term discovery system\nfinds pairs of words of the same unknown type, and the EncDec-CAE is trained to\nreconstruct one word given the other as input. We compare it to a standard\nencoder-decoder autoencoder (AE), a variational AE with a prior over its latent\nembedding, and downsampling. EncDec-CAE outperforms its closest competitor by\n24% relative in average precision on two languages in a word discrimination\ntask. \n\n"}
{"id": "1811.00625", "contents": "Title: Incorporating Structured Commonsense Knowledge in Story Completion Abstract: The ability to select an appropriate story ending is the first step towards\nperfect narrative comprehension. Story ending prediction requires not only the\nexplicit clues within the context, but also the implicit knowledge (such as\ncommonsense) to construct a reasonable and consistent story. However, most\nprevious approaches do not explicitly use background commonsense knowledge. We\npresent a neural story ending selection model that integrates three types of\ninformation: narrative sequence, sentiment evolution and commonsense knowledge.\nExperiments show that our model outperforms state-of-the-art approaches on a\npublic dataset, ROCStory Cloze Task , and the performance gain from adding the\nadditional commonsense knowledge is significant. \n\n"}
{"id": "1811.00641", "contents": "Title: Online Embedding Compression for Text Classification using Low Rank\n  Matrix Factorization Abstract: Deep learning models have become state of the art for natural language\nprocessing (NLP) tasks, however deploying these models in production system\nposes significant memory constraints. Existing compression methods are either\nlossy or introduce significant latency. We propose a compression method that\nleverages low rank matrix factorization during training,to compress the word\nembedding layer which represents the size bottleneck for most NLP models. Our\nmodels are trained, compressed and then further re-trained on the downstream\ntask to recover accuracy while maintaining the reduced size. Empirically, we\nshow that the proposed method can achieve 90% compression with minimal impact\nin accuracy for sentence classification tasks, and outperforms alternative\nmethods like fixed-point quantization or offline word embedding compression. We\nalso analyze the inference time and storage space for our method through FLOP\ncalculations, showing that we can compress DNN models by a configurable ratio\nand regain accuracy loss without introducing additional latency compared to\nfixed point quantization. Finally, we introduce a novel learning rate schedule,\nthe Cyclically Annealed Learning Rate (CALR), which we empirically demonstrate\nto outperform other popular adaptive learning rate algorithms on a sentence\nclassification benchmark. \n\n"}
{"id": "1811.00839", "contents": "Title: ATP: Directed Graph Embedding with Asymmetric Transitivity Preservation Abstract: Directed graphs have been widely used in Community Question Answering\nservices (CQAs) to model asymmetric relationships among different types of\nnodes in CQA graphs, e.g., question, answer, user. Asymmetric transitivity is\nan essential property of directed graphs, since it can play an important role\nin downstream graph inference and analysis. Question difficulty and user\nexpertise follow the characteristic of asymmetric transitivity. Maintaining\nsuch properties, while reducing the graph to a lower dimensional vector\nembedding space, has been the focus of much recent research. In this paper, we\ntackle the challenge of directed graph embedding with asymmetric transitivity\npreservation and then leverage the proposed embedding method to solve a\nfundamental task in CQAs: how to appropriately route and assign newly posted\nquestions to users with the suitable expertise and interest in CQAs. The\ntechnique incorporates graph hierarchy and reachability information naturally\nby relying on a non-linear transformation that operates on the core\nreachability and implicit hierarchy within such graphs. Subsequently, the\nmethodology levers a factorization-based approach to generate two embedding\nvectors for each node within the graph, to capture the asymmetric transitivity.\nExtensive experiments show that our framework consistently and significantly\noutperforms the state-of-the-art baselines on two diverse real-world tasks:\nlink prediction, and question difficulty estimation and expert finding in\nonline forums like Stack Exchange. Particularly, our framework can support\ninductive embedding learning for newly posted questions (unseen nodes during\ntraining), and therefore can properly route and assign these kinds of questions\nto experts in CQAs. \n\n"}
{"id": "1811.01302", "contents": "Title: Adversarial Gain Abstract: Adversarial examples can be defined as inputs to a model which induce a\nmistake - where the model output is different than that of an oracle, perhaps\nin surprising or malicious ways. Original models of adversarial attacks are\nprimarily studied in the context of classification and computer vision tasks.\nWhile several attacks have been proposed in natural language processing (NLP)\nsettings, they often vary in defining the parameters of an attack and what a\nsuccessful attack would look like. The goal of this work is to propose a\nunifying model of adversarial examples suitable for NLP tasks in both\ngenerative and classification settings. We define the notion of adversarial\ngain: based in control theory, it is a measure of the change in the output of a\nsystem relative to the perturbation of the input (caused by the so-called\nadversary) presented to the learner. This definition, as we show, can be used\nunder different feature spaces and distance conditions to determine attack or\ndefense effectiveness across different intuitive manifolds. This notion of\nadversarial gain not only provides a useful way for evaluating adversaries and\ndefenses, but can act as a building block for future work in robustness under\nadversaries due to its rooted nature in stability and manifold theory. \n\n"}
{"id": "1811.01355", "contents": "Title: Semi-Supervised Confidence Network aided Gated Attention based Recurrent\n  Neural Network for Clickbait Detection Abstract: Clickbaits are catchy headlines that are frequently used by social media\noutlets in order to allure its viewers into clicking them and thus leading them\nto dubious content. Such venal schemes thrive on exploiting the curiosity of\nnaive social media users, directing traffic to web pages that won't be visited\notherwise. In this paper, we propose a novel, semi-supervised classification\nbased approach, that employs attentions sampled from a Gumbel-Softmax\ndistribution to distill contexts that are fairly important in clickbait\ndetection. An additional loss over the attention weights is used to encode\nprior knowledge. Furthermore, we propose a confidence network that enables\nlearning over weak labels and improves robustness to noisy labels. We show that\nwith merely 30% of strongly labeled samples we can achieve over 97% of the\naccuracy, of current state of the art methods in clickbait detection. \n\n"}
{"id": "1811.01382", "contents": "Title: Neural CRF transducers for sequence labeling Abstract: Conditional random fields (CRFs) have been shown to be one of the most\nsuccessful approaches to sequence labeling. Various linear-chain neural CRFs\n(NCRFs) are developed to implement the non-linear node potentials in CRFs, but\nstill keeping the linear-chain hidden structure. In this paper, we propose NCRF\ntransducers, which consists of two RNNs, one extracting features from\nobservations and the other capturing (theoretically infinite) long-range\ndependencies between labels. Different sequence labeling methods are evaluated\nover POS tagging, chunking and NER (English, Dutch). Experiment results show\nthat NCRF transducers achieve consistent improvements over linear-chain NCRFs\nand RNN transducers across all the four tasks, and can improve state-of-the-art\nresults. \n\n"}
{"id": "1811.01690", "contents": "Title: Cycle-consistency training for end-to-end speech recognition Abstract: This paper presents a method to train end-to-end automatic speech recognition\n(ASR) models using unpaired data. Although the end-to-end approach can\neliminate the need for expert knowledge such as pronunciation dictionaries to\nbuild ASR systems, it still requires a large amount of paired data, i.e.,\nspeech utterances and their transcriptions. Cycle-consistency losses have been\nrecently proposed as a way to mitigate the problem of limited paired data.\nThese approaches compose a reverse operation with a given transformation, e.g.,\ntext-to-speech (TTS) with ASR, to build a loss that only requires unsupervised\ndata, speech in this example. Applying cycle consistency to ASR models is not\ntrivial since fundamental information, such as speaker traits, are lost in the\nintermediate text bottleneck. To solve this problem, this work presents a loss\nthat is based on the speech encoder state sequence instead of the raw speech\nsignal. This is achieved by training a Text-To-Encoder model and defining a\nloss based on the encoder reconstruction error. Experimental results on the\nLibriSpeech corpus show that the proposed cycle-consistency training reduced\nthe word error rate by 14.7% from an initial model trained with 100-hour paired\ndata, using an additional 360 hours of audio data without transcriptions. We\nalso investigate the use of text-only data mainly for language modeling to\nfurther improve the performance in the unpaired data training scenario. \n\n"}
{"id": "1811.01710", "contents": "Title: Weakly Supervised Grammatical Error Correction using Iterative Decoding Abstract: We describe an approach to Grammatical Error Correction (GEC) that is\neffective at making use of models trained on large amounts of weakly supervised\nbitext. We train the Transformer sequence-to-sequence model on 4B tokens of\nWikipedia revisions and employ an iterative decoding strategy that is tailored\nto the loosely-supervised nature of the Wikipedia training corpus. Finetuning\non the Lang-8 corpus and ensembling yields an F0.5 of 58.3 on the CoNLL'14\nbenchmark and a GLEU of 62.4 on JFLEG. The combination of weakly supervised\ntraining and iterative decoding obtains an F0.5 of 48.2 on CoNLL'14 even\nwithout using any labeled GEC data. \n\n"}
{"id": "1811.01727", "contents": "Title: AttentionXML: Label Tree-based Attention-Aware Deep Model for\n  High-Performance Extreme Multi-Label Text Classification Abstract: Extreme multi-label text classification (XMTC) is an important problem in the\nera of big data, for tagging a given text with the most relevant multiple\nlabels from an extremely large-scale label set. XMTC can be found in many\napplications, such as item categorization, web page tagging, and news\nannotation. Traditionally most methods used bag-of-words (BOW) as inputs,\nignoring word context as well as deep semantic information. Recent attempts to\novercome the problems of BOW by deep learning still suffer from 1) failing to\ncapture the important subtext for each label and 2) lack of scalability against\nthe huge number of labels. We propose a new label tree-based deep learning\nmodel for XMTC, called AttentionXML, with two unique features: 1) a multi-label\nattention mechanism with raw text as input, which allows to capture the most\nrelevant part of text to each label; and 2) a shallow and wide probabilistic\nlabel tree (PLT), which allows to handle millions of labels, especially for\n\"tail labels\". We empirically compared the performance of AttentionXML with\nthose of eight state-of-the-art methods over six benchmark datasets, including\nAmazon-3M with around 3 million labels. AttentionXML outperformed all competing\nmethods under all experimental settings. Experimental results also show that\nAttentionXML achieved the best performance against tail labels among label\ntree-based methods. The code and datasets are available at\nhttp://github.com/yourh/AttentionXML . \n\n"}
{"id": "1811.01747", "contents": "Title: The Knowref Coreference Corpus: Removing Gender and Number Cues for\n  Difficult Pronominal Anaphora Resolution Abstract: We introduce a new benchmark for coreference resolution and NLI, Knowref,\nthat targets common-sense understanding and world knowledge. Previous\ncoreference resolution tasks can largely be solved by exploiting the number and\ngender of the antecedents, or have been handcrafted and do not reflect the\ndiversity of naturally occurring text. We present a corpus of over 8,000\nannotated text passages with ambiguous pronominal anaphora. These instances are\nboth challenging and realistic. We show that various coreference systems,\nwhether rule-based, feature-rich, or neural, perform significantly worse on the\ntask than humans, who display high inter-annotator agreement. To explain this\nperformance gap, we show empirically that state-of-the art models often fail to\ncapture context, instead relying on the gender or number of candidate\nantecedents to make a decision. We then use problem-specific insights to\npropose a data-augmentation trick called antecedent switching to alleviate this\ntendency in models. Finally, we show that antecedent switching yields promising\nresults on other tasks as well: we use it to achieve state-of-the-art results\non the GAP coreference task. \n\n"}
{"id": "1811.01778", "contents": "Title: How Reasonable are Common-Sense Reasoning Tasks: A Case-Study on the\n  Winograd Schema Challenge and SWAG Abstract: Recent studies have significantly improved the state-of-the-art on\ncommon-sense reasoning (CSR) benchmarks like the Winograd Schema Challenge\n(WSC) and SWAG. The question we ask in this paper is whether improved\nperformance on these benchmarks represents genuine progress towards\ncommon-sense-enabled systems. We make case studies of both benchmarks and\ndesign protocols that clarify and qualify the results of previous work by\nanalyzing threats to the validity of previous experimental designs. Our\nprotocols account for several properties prevalent in common-sense benchmarks\nincluding size limitations, structural regularities, and variable instance\ndifficulty. \n\n"}
{"id": "1811.01786", "contents": "Title: A human-editable Sign Language representation for software editing---and\n  a writing system? Abstract: To equip SL with software properly, we need an input system to represent and\nmanipulate signed contents in the same way that every day software allows to\nprocess written text. Refuting the claim that video is good enough a medium to\nserve the purpose, we propose to build a representation that is: editable,\nqueryable, synthesisable and user-friendly---we define those terms upfront. The\nissue being functionally and conceptually linked to that of writing, we study\nexisting writing systems, namely those in use for vocal languages, those\ndesigned and proposed for SLs, and more spontaneous ways in which SL users put\ntheir language in writing. Observing each paradigm in turn, we move on to\npropose a new approach to satisfy our goals of integration in software. We\nfinally open the prospect of our proposition being used outside of this\nrestricted scope, as a writing system in itself, and compare its properties to\nthe other writing systems presented. \n\n"}
{"id": "1811.01824", "contents": "Title: Structured Neural Summarization Abstract: Summarization of long sequences into a concise statement is a core problem in\nnatural language processing, requiring non-trivial understanding of the input.\nBased on the promising results of graph neural networks on highly structured\ndata, we develop a framework to extend existing sequence encoders with a graph\ncomponent that can reason about long-distance relationships in weakly\nstructured data such as text. In an extensive evaluation, we show that the\nresulting hybrid sequence-graph models outperform both pure sequence models as\nwell as pure graph models on a range of summarization tasks. \n\n"}
{"id": "1811.02117", "contents": "Title: Modeling and Predicting Popularity Dynamics via Deep Learning Attention\n  Mechanism Abstract: An ability to predict the popularity dynamics of individual items within a\ncomplex evolving system has important implications in a wide range of domains.\nHere we propose a deep learning attention mechanism to model the process\nthrough which individual items gain their popularity. We analyze the\ninterpretability of the model with the four key phenomena confirmed\nindependently in the previous studies of long-term popularity dynamics\nquantification, including the intrinsic quality, the aging effect, the recency\neffect and the Matthew effect. We analyze the effectiveness of introducing\nattention model in popularity dynamics prediction. Extensive experiments on a\nreal-large citation data set demonstrate that the designed deep learning\nattention mechanism possesses remarkable power at predicting the long-term\npopularity dynamics. It consistently outperforms the existing methods, and\nachieves a significant performance improvement. \n\n"}
{"id": "1811.02356", "contents": "Title: Code-switching Sentence Generation by Generative Adversarial Networks\n  and its Application to Data Augmentation Abstract: Code-switching is about dealing with alternative languages in speech or text.\nIt is partially speaker-depend and domain-related, so completely explaining the\nphenomenon by linguistic rules is challenging. Compared to most monolingual\ntasks, insufficient data is an issue for code-switching. To mitigate the issue\nwithout expensive human annotation, we proposed an unsupervised method for\ncode-switching data augmentation. By utilizing a generative adversarial\nnetwork, we can generate intra-sentential code-switching sentences from\nmonolingual sentences. We applied proposed method on two corpora, and the\nresult shows that the generated code-switching sentences improve the\nperformance of code-switching language models. \n\n"}
{"id": "1811.02602", "contents": "Title: Fast Neural Chinese Word Segmentation for Long Sentences Abstract: Rapidly developed neural models have achieved competitive performance in\nChinese word segmentation (CWS) as their traditional counterparts. However,\nmost of methods encounter the computational inefficiency especially for long\nsentences because of the increasing model complexity and slower decoders. This\npaper presents a simple neural segmenter which directly labels the gap\nexistence between adjacent characters to alleviate the existing drawback. Our\nsegmenter is fully end-to-end and capable of performing segmentation very fast.\nWe also show a performance difference with different tag sets. The experiments\nshow that our segmenter can provide comparable performance with\nstate-of-the-art. \n\n"}
{"id": "1811.02736", "contents": "Title: Learning acoustic word embeddings with phonetically associated triplet\n  network Abstract: Previous researches on acoustic word embeddings used in query-by-example\nspoken term detection have shown remarkable performance improvements when using\na triplet network. However, the triplet network is trained using only a limited\ninformation about acoustic similarity between words. In this paper, we propose\na novel architecture, phonetically associated triplet network (PATN), which\naims at increasing discriminative power of acoustic word embeddings by\nutilizing phonetic information as well as word identity. The proposed model is\nlearned to minimize a combined loss function that was made by introducing a\ncross entropy loss to the lower layer of LSTM-based triplet network. We\nobserved that the proposed method performs significantly better than the\nbaseline triplet network on a word discrimination task with the WSJ dataset\nresulting in over 20% relative improvement in recall rate at 1.0 false alarm\nper hour. Finally, we examined the generalization ability by conducting the\nout-of-domain test on the RM dataset. \n\n"}
{"id": "1811.03115", "contents": "Title: Blockwise Parallel Decoding for Deep Autoregressive Models Abstract: Deep autoregressive sequence-to-sequence models have demonstrated impressive\nperformance across a wide variety of tasks in recent years. While common\narchitecture classes such as recurrent, convolutional, and self-attention\nnetworks make different trade-offs between the amount of computation needed per\nlayer and the length of the critical path at training time, generation still\nremains an inherently sequential process. To overcome this limitation, we\npropose a novel blockwise parallel decoding scheme in which we make predictions\nfor multiple time steps in parallel then back off to the longest prefix\nvalidated by a scoring model. This allows for substantial theoretical\nimprovements in generation speed when applied to architectures that can process\noutput sequences in parallel. We verify our approach empirically through a\nseries of experiments using state-of-the-art self-attention models for machine\ntranslation and image super-resolution, achieving iteration reductions of up to\n2x over a baseline greedy decoder with no loss in quality, or up to 7x in\nexchange for a slight decrease in performance. In terms of wall-clock time, our\nfastest models exhibit real-time speedups of up to 4x over standard greedy\ndecoding. \n\n"}
{"id": "1811.03189", "contents": "Title: Towards Fluent Translations from Disfluent Speech Abstract: When translating from speech, special consideration for conversational speech\nphenomena such as disfluencies is necessary. Most machine translation training\ndata consists of well-formed written texts, causing issues when translating\nspontaneous speech. Previous work has introduced an intermediate step between\nspeech recognition (ASR) and machine translation (MT) to remove disfluencies,\nmaking the data better-matched to typical translation text and significantly\nimproving performance. However, with the rise of end-to-end speech translation\nsystems, this intermediate step must be incorporated into the\nsequence-to-sequence architecture. Further, though translated speech datasets\nexist, they are typically news or rehearsed speech without many disfluencies\n(e.g. TED), or the disfluencies are translated into the references (e.g.\nFisher). To generate clean translations from disfluent speech, cleaned\nreferences are necessary for evaluation. We introduce a corpus of cleaned\ntarget data for the Fisher Spanish-English dataset for this task. We compare\nhow different architectures handle disfluencies and provide a baseline for\nremoving disfluencies in end-to-end translation. \n\n"}
{"id": "1811.03865", "contents": "Title: Multimodal Grounding for Sequence-to-Sequence Speech Recognition Abstract: Humans are capable of processing speech by making use of multiple sensory\nmodalities. For example, the environment where a conversation takes place\ngenerally provides semantic and/or acoustic context that helps us to resolve\nambiguities or to recall named entities. Motivated by this, there have been\nmany works studying the integration of visual information into the speech\nrecognition pipeline. Specifically, in our previous work, we propose a\nmultistep visual adaptive training approach which improves the accuracy of an\naudio-based Automatic Speech Recognition (ASR) system. This approach, however,\nis not end-to-end as it requires fine-tuning the whole model with an adaptation\nlayer. In this paper, we propose novel end-to-end multimodal ASR systems and\ncompare them to the adaptive approach by using a range of visual\nrepresentations obtained from state-of-the-art convolutional neural networks.\nWe show that adaptive training is effective for S2S models leading to an\nabsolute improvement of 1.4% in word error rate. As for the end-to-end systems,\nalthough they perform better than baseline, the improvements are slightly less\nthan adaptive training, 0.8 absolute WER reduction in single-best models. Using\nensemble decoding, end-to-end models reach a WER of 15% which is the lowest\nscore among all systems. \n\n"}
{"id": "1811.03873", "contents": "Title: Long Short-Term Memory with Dynamic Skip Connections Abstract: In recent years, long short-term memory (LSTM) has been successfully used to\nmodel sequential data of variable length. However, LSTM can still experience\ndifficulty in capturing long-term dependencies. In this work, we tried to\nalleviate this problem by introducing a dynamic skip connection, which can\nlearn to directly connect two dependent words. Since there is no dependency\ninformation in the training data, we propose a novel reinforcement\nlearning-based method to model the dependency relationship and connect\ndependent words. The proposed model computes the recurrent transition functions\nbased on the skip connections, which provides a dynamic skipping advantage over\nRNNs that always tackle entire sentences sequentially. Our experimental results\non three natural language processing tasks demonstrate that the proposed method\ncan achieve better performance than existing methods. In the number prediction\nexperiment, the proposed model outperformed LSTM with respect to accuracy by\nnearly 20%. \n\n"}
{"id": "1811.04201", "contents": "Title: Adversarially-Trained Normalized Noisy-Feature Auto-Encoder for Text\n  Generation Abstract: This article proposes Adversarially-Trained Normalized Noisy-Feature\nAuto-Encoder (ATNNFAE) for byte-level text generation. An ATNNFAE consists of\nan auto-encoder where the internal code is normalized on the unit sphere and\ncorrupted by additive noise. Simultaneously, a replica of the decoder (sharing\nthe same parameters as the AE decoder) is used as the generator and fed with\nrandom latent vectors. An adversarial discriminator is trained to distinguish\ntraining samples reconstructed from the AE from samples produced through the\nrandom-input generator, making the entire generator-discriminator path\ndifferentiable for discrete data like text. The combined effect of noise\ninjection in the code and shared weights between the decoder and the generator\ncan prevent the mode collapsing phenomenon commonly observed in GANs. Since\nperplexity cannot be applied to non-sequential text generation, we propose a\nnew evaluation method using the total variance distance between frequencies of\nhash-coded byte-level n-grams (NGTVD). NGTVD is a single benchmark that can\ncharacterize both the quality and the diversity of the generated texts.\nExperiments are offered in 6 large-scale datasets in Arabic, Chinese and\nEnglish, with comparisons against n-gram baselines and recurrent neural\nnetworks (RNNs). Ablation study on both the noise level and the discriminator\nis performed. We find that RNNs have trouble competing with the n-gram\nbaselines, and the ATNNFAE results are generally competitive. \n\n"}
{"id": "1811.04319", "contents": "Title: Playing by the Book: An Interactive Game Approach for Action Graph\n  Extraction from Text Abstract: Understanding procedural text requires tracking entities, actions and effects\nas the narrative unfolds. We focus on the challenging real-world problem of\naction-graph extraction from material science papers, where language is highly\nspecialized and data annotation is expensive and scarce. We propose a novel\napproach, Text2Quest, where procedural text is interpreted as instructions for\nan interactive game. A learning agent completes the game by executing the\nprocedure correctly in a text-based simulated lab environment. The framework\ncan complement existing approaches and enables richer forms of learning\ncompared to static texts. We discuss potential limitations and advantages of\nthe approach, and release a prototype proof-of-concept, hoping to encourage\nresearch in this direction. \n\n"}
{"id": "1811.04369", "contents": "Title: User Modeling for Task Oriented Dialogues Abstract: We introduce end-to-end neural network based models for simulating users of\ntask-oriented dialogue systems. User simulation in dialogue systems is crucial\nfrom two different perspectives: (i) automatic evaluation of different dialogue\nmodels, and (ii) training task-oriented dialogue systems. We design a\nhierarchical sequence-to-sequence model that first encodes the initial user\ngoal and system turns into fixed length representations using Recurrent Neural\nNetworks (RNN). It then encodes the dialogue history using another RNN layer.\nAt each turn, user responses are decoded from the hidden representations of the\ndialogue level RNN. This hierarchical user simulator (HUS) approach allows the\nmodel to capture undiscovered parts of the user goal without the need of an\nexplicit dialogue state tracking. We further develop several variants by\nutilizing a latent variable model to inject random variations into user\nresponses to promote diversity in simulated user responses and a novel goal\nregularization mechanism to penalize divergence of user responses from the\ninitial user goal. We evaluate the proposed models on movie ticket booking\ndomain by systematically interacting each user simulator with various dialogue\nsystem policies trained with different objectives and users. \n\n"}
{"id": "1811.04375", "contents": "Title: Attentive Aspect Modeling for Review-aware Recommendation Abstract: In recent years, many studies extract aspects from user reviews and integrate\nthem with ratings for improving the recommendation performance. The common\naspects mentioned in a user's reviews and a product's reviews indicate indirect\nconnections between the user and product. However, these aspect-based methods\nsuffer from two problems. First, the common aspects are usually very sparse,\nwhich is caused by the sparsity of user-product interactions and the diversity\nof individual users' vocabularies. Second, a user's interests on aspects could\nbe different with respect to different products, which are usually assumed to\nbe static in existing methods. In this paper, we propose an Attentive\nAspect-based Recommendation Model (AARM) to tackle these challenges. For the\nfirst problem, to enrich the aspect connections between user and product,\nbesides common aspects, AARM also models the interactions between synonymous\nand similar aspects. For the second problem, a neural attention network which\nsimultaneously considers user, product and aspect information is constructed to\ncapture a user's attention towards aspects when examining different products.\nExtensive quantitative and qualitative experiments show that AARM can\neffectively alleviate the two aforementioned problems and significantly\noutperforms several state-of-the-art recommendation methods on top-N\nrecommendation task. \n\n"}
{"id": "1811.04852", "contents": "Title: Quantum-inspired sublinear classical algorithms for solving low-rank\n  linear systems Abstract: We present classical sublinear-time algorithms for solving low-rank linear\nsystems of equations. Our algorithms are inspired by the HHL quantum algorithm\nfor solving linear systems and the recent breakthrough by Tang of dequantizing\nthe quantum algorithm for recommendation systems. Let $A \\in \\mathbb{C}^{m\n\\times n}$ be a rank-$k$ matrix, and $b \\in \\mathbb{C}^m$ be a vector. We\npresent two algorithms: a \"sampling\" algorithm that provides a sample from\n$A^{-1}b$ and a \"query\" algorithm that outputs an estimate of an entry of\n$A^{-1}b$, where $A^{-1}$ denotes the Moore-Penrose pseudo-inverse. Both of our\nalgorithms have query and time complexity $O(\\mathrm{poly}(k, \\kappa, \\|A\\|_F,\n1/\\epsilon)\\,\\mathrm{polylog}(m, n))$, where $\\kappa$ is the condition number\nof $A$ and $\\epsilon$ is the precision parameter. Note that the algorithms we\nconsider are sublinear time, so they cannot write and read the whole matrix or\nvectors. In this paper, we assume that $A$ and $b$ come with well-known\nlow-overhead data structures such that entries of $A$ and $b$ can be sampled\naccording to some natural probability distributions. Alternatively, when $A$ is\npositive semidefinite, our algorithms can be adapted so that the sampling\nassumption on $b$ is not required. \n\n"}
{"id": "1811.04897", "contents": "Title: Multi-encoder multi-resolution framework for end-to-end speech\n  recognition Abstract: Attention-based methods and Connectionist Temporal Classification (CTC)\nnetwork have been promising research directions for end-to-end Automatic Speech\nRecognition (ASR). The joint CTC/Attention model has achieved great success by\nutilizing both architectures during multi-task training and joint decoding. In\nthis work, we present a novel Multi-Encoder Multi-Resolution (MEMR) framework\nbased on the joint CTC/Attention model. Two heterogeneous encoders with\ndifferent architectures, temporal resolutions and separate CTC networks work in\nparallel to extract complimentary acoustic information. A hierarchical\nattention mechanism is then used to combine the encoder-level information. To\ndemonstrate the effectiveness of the proposed model, experiments are conducted\non Wall Street Journal (WSJ) and CHiME-4, resulting in relative Word Error Rate\n(WER) reduction of 18.0-32.1%. Moreover, the proposed MEMR model achieves 3.6%\nWER in the WSJ eval92 test set, which is the best WER reported for an\nend-to-end system on this benchmark. \n\n"}
{"id": "1811.05121", "contents": "Title: Modeling Local Dependence in Natural Language with Multi-channel\n  Recurrent Neural Networks Abstract: Recurrent Neural Networks (RNNs) have been widely used in processing natural\nlanguage tasks and achieve huge success. Traditional RNNs usually treat each\ntoken in a sentence uniformly and equally. However, this may miss the rich\nsemantic structure information of a sentence, which is useful for understanding\nnatural languages. Since semantic structures such as word dependence patterns\nare not parameterized, it is a challenge to capture and leverage structure\ninformation. In this paper, we propose an improved variant of RNN,\nMulti-Channel RNN (MC-RNN), to dynamically capture and leverage local semantic\nstructure information. Concretely, MC-RNN contains multiple channels, each of\nwhich represents a local dependence pattern at a time. An attention mechanism\nis introduced to combine these patterns at each step, according to the semantic\ninformation. Then we parameterize structure information by adaptively selecting\nthe most appropriate connection structures among channels. In this way, diverse\nlocal structures and dependence patterns in sentences can be well captured by\nMC-RNN. To verify the effectiveness of MC-RNN, we conduct extensive experiments\non typical natural language processing tasks, including neural machine\ntranslation, abstractive summarization, and language modeling. Experimental\nresults on these tasks all show significant improvements of MC-RNN over current\ntop systems. \n\n"}
{"id": "1811.05242", "contents": "Title: A Multi-layer LSTM-based Approach for Robot Command Interaction Modeling Abstract: As the first robotic platforms slowly approach our everyday life, we can\nimagine a near future where service robots will be easily accessible by\nnon-expert users through vocal interfaces. The capability of managing natural\nlanguage would indeed speed up the process of integrating such platform in the\nordinary life. Semantic parsing is a fundamental task of the Natural Language\nUnderstanding process, as it allows extracting the meaning of a user utterance\nto be used by a machine. In this paper, we present a preliminary study to\nsemantically parse user vocal commands for a House Service robot, using a\nmulti-layer Long-Short Term Memory neural network with attention mechanism. The\nsystem is trained on the Human Robot Interaction Corpus, and it is\npreliminarily compared with previous approaches. \n\n"}
{"id": "1811.05711", "contents": "Title: From Free Text to Clusters of Content in Health Records: An Unsupervised\n  Graph Partitioning Approach Abstract: Electronic Healthcare records contain large volumes of unstructured data in\ndifferent forms. Free text constitutes a large portion of such data, yet this\nsource of richly detailed information often remains under-used in practice\nbecause of a lack of suitable methodologies to extract interpretable content in\na timely manner. Here we apply network-theoretical tools to the analysis of\nfree text in Hospital Patient Incident reports in the English National Health\nService, to find clusters of reports in an unsupervised manner and at different\nlevels of resolution based directly on the free text descriptions contained\nwithin them. To do so, we combine recently developed deep neural network\ntext-embedding methodologies based on paragraph vectors with multi-scale Markov\nStability community detection applied to a similarity graph of documents\nobtained from sparsified text vector similarities. We showcase the approach\nwith the analysis of incident reports submitted in Imperial College Healthcare\nNHS Trust, London. The multiscale community structure reveals levels of meaning\nwith different resolution in the topics of the dataset, as shown by relevant\ndescriptive terms extracted from the groups of records, as well as by comparing\na posteriori against hand-coded categories assigned by healthcare personnel.\nOur content communities exhibit good correspondence with well-defined\nhand-coded categories, yet our results also provide further medical detail in\ncertain areas as well as revealing complementary descriptors of incidents\nbeyond the external classification. We also discuss how the method can be used\nto monitor reports over time and across different healthcare providers, and to\ndetect emerging trends that fall outside of pre-existing categories. \n\n"}
{"id": "1811.05927", "contents": "Title: Improvements on SCORE, Especially for Weak Signals Abstract: A network may have weak signals and severe degree heterogeneity, and may be\nvery sparse in one occurrence but very dense in another. SCORE (Jin, 2015) is a\nrecent approach to network community detection. It accommodates severe degree\nheterogeneity and is adaptive to different levels of sparsity, but its\nperformance for networks with weak signals is unclear. In this paper, we show\nthat in a broad class of network settings where we allow for weak signals,\nsevere degree heterogeneity, and a wide range of network sparsity, SCORE\nachieves prefect clustering and has the so-called \"exponential rate\" in Hamming\nclustering errors. The proof uses the most recent advancement on entry-wise\nbounds for the leading eigenvectors of the network adjacency matrix.\n  The theoretical analysis assures us that SCORE continues to work well in the\nweak signal settings, but it does not rule out the possibility that SCORE may\nbe further improved to have better performance in real applications, especially\nfor networks with weak signals. As a second contribution of the paper, we\npropose SCORE+ as an improved version of SCORE. We investigate SCORE+ with 8\nnetwork data sets and found that it outperforms several representative\napproaches. In particular, for the 6 data sets with relatively strong signals,\nSCORE+ has similar performance as that of SCORE, but for the 2 data sets\n(Simmons, Caltech) with possibly weak signals, SCORE+ has much lower error\nrates. SCORE+ proposes several changes to SCORE. We carefully explain the\nrationale underlying each of these changes, using a mixture of theoretical and\nnumerical study. \n\n"}
{"id": "1811.07234", "contents": "Title: Improving Automatic Source Code Summarization via Deep Reinforcement\n  Learning Abstract: Code summarization provides a high level natural language description of the\nfunction performed by code, as it can benefit the software maintenance, code\ncategorization and retrieval. To the best of our knowledge, most\nstate-of-the-art approaches follow an encoder-decoder framework which encodes\nthe code into a hidden space and then decode it into natural language space,\nsuffering from two major drawbacks: a) Their encoders only consider the\nsequential content of code, ignoring the tree structure which is also critical\nfor the task of code summarization, b) Their decoders are typically trained to\npredict the next word by maximizing the likelihood of next ground-truth word\nwith previous ground-truth word given. However, it is expected to generate the\nentire sequence from scratch at test time. This discrepancy can cause an\n\\textit{exposure bias} issue, making the learnt decoder suboptimal. In this\npaper, we incorporate an abstract syntax tree structure as well as sequential\ncontent of code snippets into a deep reinforcement learning framework (i.e.,\nactor-critic network). The actor network provides the confidence of predicting\nthe next word according to current state. On the other hand, the critic network\nevaluates the reward value of all possible extensions of the current state and\ncan provide global guidance for explorations. We employ an advantage reward\ncomposed of BLEU metric to train both networks. Comprehensive experiments on a\nreal-world dataset show the effectiveness of our proposed model when compared\nwith some state-of-the-art methods. \n\n"}
{"id": "1811.07240", "contents": "Title: Representation Mixing for TTS Synthesis Abstract: Recent character and phoneme-based parametric TTS systems using deep learning\nhave shown strong performance in natural speech generation. However, the choice\nbetween character or phoneme input can create serious limitations for practical\ndeployment, as direct control of pronunciation is crucial in certain cases. We\ndemonstrate a simple method for combining multiple types of linguistic\ninformation in a single encoder, named representation mixing, enabling flexible\nchoice between character, phoneme, or mixed representations during inference.\nExperiments and user studies on a public audiobook corpus show the efficacy of\nour approach. \n\n"}
{"id": "1811.07550", "contents": "Title: Switch-based Active Deep Dyna-Q: Efficient Adaptive Planning for\n  Task-Completion Dialogue Policy Learning Abstract: Training task-completion dialogue agents with reinforcement learning usually\nrequires a large number of real user experiences. The Dyna-Q algorithm extends\nQ-learning by integrating a world model, and thus can effectively boost\ntraining efficiency using simulated experiences generated by the world model.\nThe effectiveness of Dyna-Q, however, depends on the quality of the world model\n- or implicitly, the pre-specified ratio of real vs. simulated experiences used\nfor Q-learning. To this end, we extend the recently proposed Deep Dyna-Q (DDQ)\nframework by integrating a switcher that automatically determines whether to\nuse a real or simulated experience for Q-learning. Furthermore, we explore the\nuse of active learning for improving sample efficiency, by encouraging the\nworld model to generate simulated experiences in the state-action space where\nthe agent has not (fully) explored. Our results show that by combining switcher\nand active learning, the new framework named as Switch-based Active Deep Dyna-Q\n(Switch-DDQ), leads to significant improvement over DDQ and Q-learning\nbaselines in both simulation and human evaluations. \n\n"}
{"id": "1811.07609", "contents": "Title: Outlier Aware Network Embedding for Attributed Networks Abstract: Attributed network embedding has received much interest from the research\ncommunity as most of the networks come with some content in each node, which is\nalso known as node attributes. Existing attributed network approaches work well\nwhen the network is consistent in structure and attributes, and nodes behave as\nexpected. But real world networks often have anomalous nodes. Typically these\noutliers, being relatively unexplainable, affect the embeddings of other nodes\nin the network. Thus all the downstream network mining tasks fail miserably in\nthe presence of such outliers. Hence an integrated approach to detect anomalies\nand reduce their overall effect on the network embedding is required.\n  Towards this end, we propose an unsupervised outlier aware network embedding\nalgorithm (ONE) for attributed networks, which minimizes the effect of the\noutlier nodes, and hence generates robust network embeddings. We align and\njointly optimize the loss functions coming from structure and attributes of the\nnetwork. To the best of our knowledge, this is the first generic network\nembedding approach which incorporates the effect of outliers for an attributed\nnetwork without any supervision. We experimented on publicly available real\nnetworks and manually planted different types of outliers to check the\nperformance of the proposed algorithm. Results demonstrate the superiority of\nour approach to detect the network outliers compared to the state-of-the-art\napproaches. We also consider different downstream machine learning applications\non networks to show the efficiency of ONE as a generic network embedding\ntechnique. The source code is made available at\nhttps://github.com/sambaranban/ONE. \n\n"}
{"id": "1811.07746", "contents": "Title: An Empirical Assessment of the Complexity and Realism of Synthetic\n  Social Contact Networks Abstract: We use multiple measures of graph complexity to evaluate the realism of\nsynthetically-generated networks of human activity, in comparison with several\nstylized network models as well as a collection of empirical networks from the\nliterature. The synthetic networks are generated by integrating data about\nhuman populations from several sources, including the Census, transportation\nsurveys, and geographical data. The resulting networks represent an\napproximation of daily or weekly human interaction. Our results indicate that\nthe synthetically generated graphs according to our methodology are closer to\nthe real world graphs, as measured across multiple structural measures, than a\nrange of stylized graphs generated using common network models from the\nliterature. \n\n"}
{"id": "1811.08019", "contents": "Title: Role action embeddings: scalable representation of network positions Abstract: We consider the question of embedding nodes with similar local neighborhoods\ntogether in embedding space, commonly referred to as \"role embeddings.\" We\npropose RAE, an unsupervised framework that learns role embeddings. It combines\na within-node loss function and a graph neural network (GNN) architecture to\nplace nodes with similar local neighborhoods close in embedding space. We also\npropose a faster way of generating negative examples called neighbor shuffling,\nwhich quickly creates negative examples directly within batches. These\ntechniques can be easily combined with existing GNN methods to create\nunsupervised role embeddings at scale. We then explore role action embeddings,\nwhich summarize the non-structural features in a node's neighborhood, leading\nto better performance on node classification tasks. We find that the model\narchitecture proposed here provides strong performance on both graph and node\nclassification tasks, in some cases competitive with semi-supervised methods. \n\n"}
{"id": "1811.08040", "contents": "Title: Unsupervised Pseudo-Labeling for Extractive Summarization on Electronic\n  Health Records Abstract: Extractive summarization is very useful for physicians to better manage and\ndigest Electronic Health Records (EHRs). However, the training of a supervised\nmodel requires disease-specific medical background and is thus very expensive.\nWe studied how to utilize the intrinsic correlation between multiple EHRs to\ngenerate pseudo-labels and train a supervised model with no external\nannotation. Experiments on real-patient data validate that our model is\neffective in summarizing crucial disease-specific information for patients. \n\n"}
{"id": "1811.08129", "contents": "Title: Alignment Analysis of Sequential Segmentation of Lexicons to Improve\n  Automatic Cognate Detection Abstract: Ranking functions in information retrieval are often used in search engines\nto recommend the relevant answers to the query. This paper makes use of this\nnotion of information retrieval and applies onto the problem domain of cognate\ndetection. The main contributions of this paper are: (1) positional\nsegmentation, which incorporates the sequential notion; (2) graphical error\nmodelling, which deduces the transformations. The current research work focuses\non classification problem; which is distinguishing whether a pair of words are\ncognates. This paper focuses on a harder problem, whether we could predict a\npossible cognate from the given input. Our study shows that when language\nmodelling smoothing methods are applied as the retrieval functions and used in\nconjunction with positional segmentation and error modelling gives better\nresults than competing baselines, in both classification and prediction of\ncognates.\n  Source code is at: https://github.com/pranav-ust/cognates \n\n"}
{"id": "1811.08366", "contents": "Title: Temporal Graph Offset Reconstruction: Towards Temporally Robust Graph\n  Representation Learning Abstract: Graphs are a commonly used construct for representing relationships between\nelements in complex high dimensional datasets. Many real-world phenomenon are\ndynamic in nature, meaning that any graph used to represent them is inherently\ntemporal. However, many of the machine learning models designed to capture\nknowledge about the structure of these graphs ignore this rich temporal\ninformation when creating representations of the graph. This results in models\nwhich do not perform well when used to make predictions about the future state\nof the graph -- especially when the delta between time stamps is not small. In\nthis work, we explore a novel training procedure and an associated unsupervised\nmodel which creates graph representations optimised to predict the future state\nof the graph. We make use of graph convolutional neural networks to encode the\ngraph into a latent representation, which we then use to train our temporal\noffset reconstruction method, inspired by auto-encoders, to predict a later\ntime point -- multiple time steps into the future. Using our method, we\ndemonstrate superior performance for the task of future link prediction\ncompared with none-temporal state-of-the-art baselines. We show our approach to\nbe capable of outperforming non-temporal baselines by 38% on a real world\ndataset. \n\n"}
{"id": "1811.08615", "contents": "Title: Unsupervised Multimodal Representation Learning across Medical Images\n  and Reports Abstract: Joint embeddings between medical imaging modalities and associated radiology\nreports have the potential to offer significant benefits to the clinical\ncommunity, ranging from cross-domain retrieval to conditional generation of\nreports to the broader goals of multimodal representation learning. In this\nwork, we establish baseline joint embedding results measured via both local and\nglobal retrieval methods on the soon to be released MIMIC-CXR dataset\nconsisting of both chest X-ray images and the associated radiology reports. We\nexamine both supervised and unsupervised methods on this task and show that for\ndocument retrieval tasks with the learned representations, only a limited\namount of supervision is needed to yield results comparable to those of\nfully-supervised methods. \n\n"}
{"id": "1811.09386", "contents": "Title: Explicit Interaction Model towards Text Classification Abstract: Text classification is one of the fundamental tasks in natural language\nprocessing. Recently, deep neural networks have achieved promising performance\nin the text classification task compared to shallow models. Despite of the\nsignificance of deep models, they ignore the fine-grained (matching signals\nbetween words and classes) classification clues since their classifications\nmainly rely on the text-level representations. To address this problem, we\nintroduce the interaction mechanism to incorporate word-level matching signals\ninto the text classification task. In particular, we design a novel framework,\nEXplicit interAction Model (dubbed as EXAM), equipped with the interaction\nmechanism. We justified the proposed approach on several benchmark datasets\nincluding both multi-label and multi-class text classification tasks. Extensive\nexperimental results demonstrate the superiority of the proposed method. As a\nbyproduct, we have released the codes and parameter settings to facilitate\nother researches. \n\n"}
{"id": "1811.10773", "contents": "Title: Verb Argument Structure Alternations in Word and Sentence Embeddings Abstract: Verbs occur in different syntactic environments, or frames. We investigate\nwhether artificial neural networks encode grammatical distinctions necessary\nfor inferring the idiosyncratic frame-selectional properties of verbs. We\nintroduce five datasets, collectively called FAVA, containing in aggregate\nnearly 10k sentences labeled for grammatical acceptability, illustrating\ndifferent verbal argument structure alternations. We then test whether models\ncan distinguish acceptable English verb-frame combinations from unacceptable\nones using a sentence embedding alone. For converging evidence, we further\nconstruct LaVA, a corresponding word-level dataset, and investigate whether the\nsame syntactic features can be extracted from word embeddings. Our models\nperform reliable classifications for some verbal alternations but not others,\nsuggesting that while these representations do encode fine-grained lexical\ninformation, it is incomplete or can be hard to extract. Further, differences\nbetween the word- and sentence-level models show that some information present\nin word embeddings is not passed on to the down-stream sentence embeddings. \n\n"}
{"id": "1811.10789", "contents": "Title: Flexible Attributed Network Embedding Abstract: Network embedding aims to find a way to encode network by learning an\nembedding vector for each node in the network. The network often has property\ninformation which is highly informative with respect to the node's position and\nrole in the network. Most network embedding methods fail to utilize this\ninformation during network representation learning. In this paper, we propose a\nnovel framework, FANE, to integrate structure and property information in the\nnetwork embedding process. In FANE, we design a network to unify heterogeneity\nof the two information sources, and define a new random walking strategy to\nleverage property information and make the two information compensate. FANE is\nconceptually simple and empirically powerful. It improves over the\nstate-of-the-art methods on Cora dataset classification task by over 5%, more\nthan 10% on WebKB dataset classification task. Experiments also show that the\nresults improve more than the state-of-the-art methods as increasing training\nsize. Moreover, qualitative visualization show that our framework is helpful in\nnetwork property information exploration. In all, we present a new way for\nefficiently learning state-of-the-art task-independent representations in\ncomplex attributed networks. The source code and datasets of this paper can be\nobtained from https://github.com/GraphWorld/FANE. \n\n"}
{"id": "1811.10810", "contents": "Title: A Scalable Optimization Mechanism for Pairwise based Discrete Hashing Abstract: Maintaining the pair similarity relationship among originally\nhigh-dimensional data into a low-dimensional binary space is a popular strategy\nto learn binary codes. One simiple and intutive method is to utilize two\nidentical code matrices produced by hash functions to approximate a pairwise\nreal label matrix. However, the resulting quartic problem is difficult to\ndirectly solve due to the non-convex and non-smooth nature of the objective. In\nthis paper, unlike previous optimization methods using various relaxation\nstrategies, we aim to directly solve the original quartic problem using a novel\nalternative optimization mechanism to linearize the quartic problem by\nintroducing a linear regression model. Additionally, we find that gradually\nlearning each batch of binary codes in a sequential mode, i.e. batch by batch,\nis greatly beneficial to the convergence of binary code learning. Based on this\nsignificant discovery and the proposed strategy, we introduce a scalable\nsymmetric discrete hashing algorithm that gradually and smoothly updates each\nbatch of binary codes. To further improve the smoothness, we also propose a\ngreedy symmetric discrete hashing algorithm to update each bit of batch binary\ncodes. Moreover, we extend the proposed optimization mechanism to solve the\nnon-convex optimization problems for binary code learning in many other\npairwise based hashing algorithms. Extensive experiments on benchmark\nsingle-label and multi-label databases demonstrate the superior performance of\nthe proposed mechanism over recent state-of-the-art methods. \n\n"}
{"id": "1811.11320", "contents": "Title: User-Guided Clustering in Heterogeneous Information Networks via\n  Motif-Based Comprehensive Transcription Abstract: Heterogeneous information networks (HINs) with rich semantics are ubiquitous\nin real-world applications. For a given HIN, many reasonable clustering results\nwith distinct semantic meaning can simultaneously exist. User-guided clustering\nis hence of great practical value for HINs where users provide labels to a\nsmall portion of nodes. To cater to a broad spectrum of user guidance evidenced\nby different expected clustering results, carefully exploiting the signals\nresiding in the data is potentially useful. Meanwhile, as one type of complex\nnetworks, HINs often encapsulate higher-order interactions that reflect the\ninterlocked nature among nodes and edges. Network motifs, sometimes referred to\nas meta-graphs, have been used as tools to capture such higher-order\ninteractions and reveal the many different semantics. We therefore approach the\nproblem of user-guided clustering in HINs with network motifs. In this process,\nwe identify the utility and importance of directly modeling higher-order\ninteractions without collapsing them to pairwise interactions. To achieve this,\nwe comprehensively transcribe the higher-order interaction signals to a series\nof tensors via motifs and propose the MoCHIN model based on joint non-negative\ntensor factorization. This approach applies to arbitrarily many, arbitrary\nforms of HIN motifs. An inference algorithm with speed-up methods is also\nproposed to tackle the challenge that tensor size grows exponentially as the\nnumber of nodes in a motif increases. We validate the effectiveness of the\nproposed method on two real-world datasets and three tasks, and MoCHIN\noutperforms all baselines in three evaluation tasks under three different\nmetrics. Additional experiments demonstrated the utility of motifs and the\nbenefit of directly modeling higher-order information especially when user\nguidance is limited. \n\n"}
{"id": "1811.11456", "contents": "Title: GIRNet: Interleaved Multi-Task Recurrent State Sequence Models Abstract: In several natural language tasks, labeled sequences are available in\nseparate domains (say, languages), but the goal is to label sequences with\nmixed domain (such as code-switched text). Or, we may have available models for\nlabeling whole passages (say, with sentiments), which we would like to exploit\ntoward better position-specific label inference (say, target-dependent\nsentiment annotation). A key characteristic shared across such tasks is that\ndifferent positions in a primary instance can benefit from different `experts'\ntrained from auxiliary data, but labeled primary instances are scarce, and\nlabeling the best expert for each position entails unacceptable cognitive\nburden. We propose GITNet, a unified position-sensitive multi-task recurrent\nneural network (RNN) architecture for such applications. Auxiliary and primary\ntasks need not share training instances. Auxiliary RNNs are trained over\nauxiliary instances. A primary instance is also submitted to each auxiliary\nRNN, but their state sequences are gated and merged into a novel composite\nstate sequence tailored to the primary inference task. Our approach is in sharp\ncontrast to recent multi-task networks like the cross-stitch and sluice\nnetwork, which do not control state transfer at such fine granularity. We\ndemonstrate the superiority of GIRNet using three applications: sentiment\nclassification of code-switched passages, part-of-speech tagging of\ncode-switched text, and target position-sensitive annotation of sentiment in\nmonolingual passages. In all cases, we establish new state-of-the-art\nperformance beyond recent competitive baselines. \n\n"}
{"id": "1811.11728", "contents": "Title: Attributed Network Embedding for Incomplete Attributed Networks Abstract: Attributed networks are ubiquitous since a network often comes with auxiliary\nattribute information e.g. a social network with user profiles. Attributed\nNetwork Embedding (ANE) has recently attracted considerable attention, which\naims to learn unified low dimensional node embeddings while preserving both\nstructural and attribute information. The resulting node embeddings can then\nfacilitate various network downstream tasks e.g. link prediction. Although\nthere are several ANE methods, most of them cannot deal with incomplete\nattributed networks with missing links and/or missing node attributes, which\noften occur in real-world scenarios. To address this issue, we propose a robust\nANE method, the general idea of which is to reconstruct a unified denser\nnetwork by fusing two sources of information for information enhancement, and\nthen employ a random walks based network embedding method for learning node\nembeddings. The experiments of link prediction, node classification,\nvisualization, and parameter sensitivity analysis on six real-world datasets\nvalidate the effectiveness of our method to incomplete attributed networks. \n\n"}
{"id": "1811.12156", "contents": "Title: Improved Deep Embeddings for Inferencing with Multi-Layered Networks Abstract: Inferencing with network data necessitates the mapping of its nodes into a\nvector space, where the relationships are preserved. However, with\nmulti-layered networks, where multiple types of relationships exist for the\nsame set of nodes, it is crucial to exploit the information shared between\nlayers, in addition to the distinct aspects of each layer. In this paper, we\npropose a novel approach that first obtains node embeddings in all layers\njointly via DeepWalk on a \\textit{supra} graph, which allows interactions\nbetween layers, and then fine-tunes the embeddings to encourage cohesive\nstructure in the latent space. With empirical studies in node classification,\nlink prediction and multi-layered community detection, we show that the\nproposed approach outperforms existing single- and multi-layered network\nembedding algorithms on several benchmarks. In addition to effectively scaling\nto a large number of layers (tested up to $37$), our approach consistently\nproduces highly modular community structure, even when compared to methods that\ndirectly optimize for the modularity function. \n\n"}
{"id": "1811.12162", "contents": "Title: Effective Resistance-based Germination of Seed Sets for Community\n  Detection Abstract: Community detection is, at its core, an attempt to attach an interpretable\nfunction to an otherwise indecipherable form. The importance of labeling\ncommunities has obvious implications for identifying clusters in social\nnetworks, but it has a number of equally relevant applications in product\nrecommendations, biological systems, and many forms of classification. The\nlocal variety of community detection starts with a small set of labeled seed\nnodes, and aims to estimate the community containing these nodes. One of the\nmost ubiquitous methods - due to its simplicity and efficiency - is\npersonalized PageRank. The most obvious bottleneck for deploying this form of\nPageRank successfully is the quality of the seeds. We introduce a \"germination\"\nstage for these seeds, where an effective resistance-based approach is used to\nincrease the quality and number of seeds from which a community is detected. By\nbreaking seed set expansion into a two-step process, we aim to utilize two\ndistinct random walk-based approaches in the regimes in which they excel. In\nsynthetic and real network data, a simple, greedy algorithm which minimizes the\neffective resistance diameter combined with PageRank achieves clear\nimprovements in precision and recall over a standalone PageRank procedure. \n\n"}
{"id": "1811.12169", "contents": "Title: Predicting Opioid Relapse Using Social Media Data Abstract: Opioid addiction is a severe public health threat in the U.S, causing massive\ndeaths and many social problems. Accurate relapse prediction is of practical\nimportance for recovering patients since relapse prediction promotes timely\nrelapse preventions that help patients stay clean. In this paper, we introduce\na Generative Adversarial Networks (GAN) model to predict the addiction relapses\nbased on sentiment images and social influences. Experimental results on real\nsocial media data from Reddit.com demonstrate that the GAN model delivers a\nbetter performance than comparable alternative techniques. The sentiment images\ngenerated by the model show that relapse is closely connected with two emotions\n`joy' and `negative'. This work is one of the first attempts to predict\nrelapses using massive social media data and generative adversarial nets. The\nproposed method, combined with knowledge of social media mining, has the\npotential to revolutionize the practice of opioid addiction prevention and\ntreatment. \n\n"}
{"id": "1811.12181", "contents": "Title: What Should I Learn First: Introducing LectureBank for NLP Education and\n  Prerequisite Chain Learning Abstract: Recent years have witnessed the rising popularity of Natural Language\nProcessing (NLP) and related fields such as Artificial Intelligence (AI) and\nMachine Learning (ML). Many online courses and resources are available even for\nthose without a strong background in the field. Often the student is curious\nabout a specific topic but does not quite know where to begin studying. To\nanswer the question of \"what should one learn first,\" we apply an\nembedding-based method to learn prerequisite relations for course concepts in\nthe domain of NLP. We introduce LectureBank, a dataset containing 1,352 English\nlecture files collected from university courses which are each classified\naccording to an existing taxonomy as well as 208 manually-labeled prerequisite\nrelation topics, which is publicly available. The dataset will be useful for\neducational purposes such as lecture preparation and organization as well as\napplications such as reading list generation. Additionally, we experiment with\nneural graph-based networks and non-neural classifiers to learn these\nprerequisite relations from our dataset. \n\n"}
{"id": "1811.12254", "contents": "Title: The Effect of Heterogeneous Data for Alzheimer's Disease Detection from\n  Speech Abstract: Speech datasets for identifying Alzheimer's disease (AD) are generally\nrestricted to participants performing a single task, e.g. describing an image\nshown to them. As a result, models trained on linguistic features derived from\nsuch datasets may not be generalizable across tasks. Building on prior work\ndemonstrating that same-task data of healthy participants helps improve AD\ndetection on a single-task dataset of pathological speech, we augment an\nAD-specific dataset consisting of subjects describing a picture with multi-task\nhealthy data. We demonstrate that normative data from multiple speech-based\ntasks helps improve AD detection by up to 9%. Visualization of decision\nboundaries reveals that models trained on a combination of structured picture\ndescriptions and unstructured conversational speech have the least out-of-task\nerror and show the most potential to generalize to multiple tasks. We analyze\nthe impact of age of the added samples and if they affect fairness in\nclassification. We also provide explanations for a possible inductive bias\neffect across tasks using model-agnostic feature anchors. This work highlights\nthe need for heterogeneous datasets for encoding changes in multiple facets of\ncognition and for developing a task-independent AD detection model. \n\n"}
{"id": "1811.12739", "contents": "Title: Neural separation of observed and unobserved distributions Abstract: Separating mixed distributions is a long standing challenge for machine\nlearning and signal processing. Most current methods either rely on making\nstrong assumptions on the source distributions or rely on having training\nsamples of each source in the mixture. In this work, we introduce a new\nmethod---Neural Egg Separation---to tackle the scenario of extracting a signal\nfrom an unobserved distribution additively mixed with a signal from an observed\ndistribution. Our method iteratively learns to separate the known distribution\nfrom progressively finer estimates of the unknown distribution. In some\nsettings, Neural Egg Separation is initialization sensitive, we therefore\nintroduce Latent Mixture Masking which ensures a good initialization. Extensive\nexperiments on audio and image separation tasks show that our method\noutperforms current methods that use the same level of supervision, and often\nachieves similar performance to full supervision. \n\n"}
{"id": "1812.00073", "contents": "Title: TF-Ranking: Scalable TensorFlow Library for Learning-to-Rank Abstract: Learning-to-Rank deals with maximizing the utility of a list of examples\npresented to the user, with items of higher relevance being prioritized. It has\nseveral practical applications such as large-scale search, recommender systems,\ndocument summarization and question answering. While there is widespread\nsupport for classification and regression based learning, support for\nlearning-to-rank in deep learning has been limited. We propose TensorFlow\nRanking, the first open source library for solving large-scale ranking problems\nin a deep learning framework. It is highly configurable and provides\neasy-to-use APIs to support different scoring mechanisms, loss functions and\nevaluation metrics in the learning-to-rank setting. Our library is developed on\ntop of TensorFlow and can thus fully leverage the advantages of this platform.\nFor example, it is highly scalable, both in training and in inference, and can\nbe used to learn ranking models over massive amounts of user activity data,\nwhich can include heterogeneous dense and sparse features. We empirically\ndemonstrate the effectiveness of our library in learning ranking functions for\nlarge-scale search and recommendation applications in Gmail and Google Drive.\nWe also show that ranking models built using our model scale well for\ndistributed training, without significant impact on metrics. The proposed\nlibrary is available to the open source community, with the hope that it\nfacilitates further academic research and industrial applications in the field\nof learning-to-rank. \n\n"}
{"id": "1812.00141", "contents": "Title: A Dynamic Network and Representation LearningApproach for Quantifying\n  Economic Growth fromSatellite Imagery Abstract: Quantifying the improvement in human living standard, as well as the city\ngrowth in developing countries, is a challenging problem due to the lack of\nreliable economic data. Therefore, there is a fundamental need for alternate,\nlargely unsupervised, computational methods that can estimate the economic\nconditions in the developing regions. To this end, we propose a new network\nscience- and representation learning-based approach that can quantify economic\nindicators and visualize the growth of various regions. More precisely, we\nfirst create a dynamic network drawn out of high-resolution nightlight\nsatellite images. We then demonstrate that using representation learning to\nmine the resulting network, our proposed approach can accurately predict\nspatial gross economic expenditures over large regions. Our method, which\nrequires only nightlight images and limited survey data, can capture\ncity-growth, as well as how people's living standard is changing; this can\nultimately facilitate the decision makers' understanding of growth without\nheavily relying on expensive and time-consuming surveys. \n\n"}
{"id": "1812.00161", "contents": "Title: QADiver: Interactive Framework for Diagnosing QA Models Abstract: Question answering (QA) extracting answers from text to the given question in\nnatural language, has been actively studied and existing models have shown a\npromise of outperforming human performance when trained and evaluated with\nSQuAD dataset. However, such performance may not be replicated in the actual\nsetting, for which we need to diagnose the cause, which is non-trivial due to\nthe complexity of model. We thus propose a web-based UI that provides how each\nmodel contributes to QA performances, by integrating visualization and analysis\ntools for model explanation. We expect this framework can help QA model\nresearchers to refine and improve their models. \n\n"}
{"id": "1812.00271", "contents": "Title: Learning Speaker Representations with Mutual Information Abstract: Learning good representations is of crucial importance in deep learning.\nMutual Information (MI) or similar measures of statistical dependence are\npromising tools for learning these representations in an unsupervised way. Even\nthough the mutual information between two random variables is hard to measure\ndirectly in high dimensional spaces, some recent studies have shown that an\nimplicit optimization of MI can be achieved with an encoder-discriminator\narchitecture similar to that of Generative Adversarial Networks (GANs). In this\nwork, we learn representations that capture speaker identities by maximizing\nthe mutual information between the encoded representations of chunks of speech\nrandomly sampled from the same sentence. The proposed encoder relies on the\nSincNet architecture and transforms raw speech waveform into a compact feature\nvector. The discriminator is fed by either positive samples (of the joint\ndistribution of encoded chunks) or negative samples (from the product of the\nmarginals) and is trained to separate them. We report experiments showing that\nthis approach effectively learns useful speaker representations, leading to\npromising results on speaker identification and verification tasks. Our\nexperiments consider both unsupervised and semi-supervised settings and compare\nthe performance achieved with different objective functions. \n\n"}
{"id": "1812.00381", "contents": "Title: Towards Automatic Discovery of Cybercrime Supply Chains Abstract: Cybercrime forums enable modern criminal entrepreneurs to collaborate with\nother criminals into increasingly efficient and sophisticated criminal\nendeavors. Understanding the connections between different products and\nservices can often illuminate effective interventions. However, generating this\nunderstanding of supply chains currently requires time-consuming manual effort.\n  In this paper, we propose a language-agnostic method to automatically extract\nsupply chains from cybercrime forum posts and replies. Our supply chain\ndetection algorithm can identify 36% and 58% relevant chains within major\nEnglish and Russian forums, respectively, showing improvements over the\nbaselines of 13% and 36%, respectively. Our analysis of the automatically\ngenerated supply chains demonstrates underlying connections between products\nand services within these forums. For example, the extracted supply chain\nilluminated the connection between hack-for-hire services and the selling of\nrare and valuable `OG' accounts, which has only recently been reported. The\nunderstanding of connections between products and services exposes potentially\neffective intervention points. \n\n"}
{"id": "1812.00677", "contents": "Title: Clinical Document Classification Using Labeled and Unlabeled Data Across\n  Hospitals Abstract: Reviewing radiology reports in emergency departments is an essential but\nlaborious task. Timely follow-up of patients with abnormal cases in their\nradiology reports may dramatically affect the patient's outcome, especially if\nthey have been discharged with a different initial diagnosis. Machine learning\napproaches have been devised to expedite the process and detect the cases that\ndemand instant follow up. However, these approaches require a large amount of\nlabeled data to train reliable predictive models. Preparing such a large\ndataset, which needs to be manually annotated by health professionals, is\ncostly and time-consuming. This paper investigates a semi-supervised learning\nframework for radiology report classification across three hospitals. The main\ngoal is to leverage clinical unlabeled data in order to augment the learning\nprocess where limited labeled data is available. To further improve the\nclassification performance, we also integrate a transfer learning technique\ninto the semi-supervised learning pipeline . Our experimental findings show\nthat (1) convolutional neural networks (CNNs), while being independent of any\nproblem-specific feature engineering, achieve significantly higher\neffectiveness compared to conventional supervised learning approaches, (2)\nleveraging unlabeled data in training a CNN-based classifier reduces the\ndependency on labeled data by more than 50% to reach the same performance of a\nfully supervised CNN, and (3) transferring the knowledge gained from available\nlabeled data in an external source hospital significantly improves the\nperformance of a semi-supervised CNN model over their fully supervised\ncounterparts in a target hospital. \n\n"}
{"id": "1812.01190", "contents": "Title: EENMF: An End-to-End Neural Matching Framework for E-Commerce Sponsored\n  Search Abstract: E-commerce sponsored search contributes an important part of revenue for the\ne-commerce company. In consideration of effectiveness and efficiency, a\nlarge-scale sponsored search system commonly adopts a multi-stage architecture.\nWe name these stages as ad retrieval, ad pre-ranking and ad ranking. Ad\nretrieval and ad pre-ranking are collectively referred to as ad matching in\nthis paper. We propose an end-to-end neural matching framework (EENMF) to model\ntwo tasks---vector-based ad retrieval and neural networks based ad pre-ranking.\nUnder the deep matching framework, vector-based ad retrieval harnesses user\nrecent behavior sequence to retrieve relevant ad candidates without the\nconstraint of keyword bidding. Simultaneously, the deep model is employed to\nperform the global pre-ranking of ad candidates from multiple retrieval paths\neffectively and efficiently. Besides, the proposed model tries to optimize the\npointwise cross-entropy loss which is consistent with the objective of predict\nmodels in the ranking stage. We conduct extensive evaluation to validate the\nperformance of the proposed framework. In the real traffic of a large-scale\ne-commerce sponsored search, the proposed approach significantly outperforms\nthe baseline. \n\n"}
{"id": "1812.01504", "contents": "Title: Fighting Fire with Fire: Using Antidote Data to Improve Polarization and\n  Fairness of Recommender Systems Abstract: The increasing role of recommender systems in many aspects of society makes\nit essential to consider how such systems may impact social good. Various\nmodifications to recommendation algorithms have been proposed to improve their\nperformance for specific socially relevant measures. However, previous\nproposals are often not easily adapted to different measures, and they\ngenerally require the ability to modify either existing system inputs, the\nsystem's algorithm, or the system's outputs. As an alternative, in this paper\nwe introduce the idea of improving the social desirability of recommender\nsystem outputs by adding more data to the input, an approach we view as\nproviding `antidote' data to the system. We formalize the antidote data\nproblem, and develop optimization-based solutions. We take as our model system\nthe matrix factorization approach to recommendation, and we propose a set of\nmeasures to capture the polarization or fairness of recommendations. We then\nshow how to generate antidote data for each measure, pointing out a number of\ncomputational efficiencies, and discuss the impact on overall system accuracy.\nOur experiments show that a modest budget for antidote data can lead to\nsignificant improvements in the polarization or fairness of recommendations. \n\n"}
{"id": "1812.02353", "contents": "Title: Top-K Off-Policy Correction for a REINFORCE Recommender System Abstract: Industrial recommender systems deal with extremely large action spaces --\nmany millions of items to recommend. Moreover, they need to serve billions of\nusers, who are unique at any point in time, making a complex user state space.\nLuckily, huge quantities of logged implicit feedback (e.g., user clicks, dwell\ntime) are available for learning. Learning from the logged feedback is however\nsubject to biases caused by only observing feedback on recommendations selected\nby the previous versions of the recommender. In this work, we present a general\nrecipe of addressing such biases in a production top-K recommender system at\nYoutube, built with a policy-gradient-based algorithm, i.e. REINFORCE. The\ncontributions of the paper are: (1) scaling REINFORCE to a production\nrecommender system with an action space on the orders of millions; (2) applying\noff-policy correction to address data biases in learning from logged feedback\ncollected from multiple behavior policies; (3) proposing a novel top-K\noff-policy correction to account for our policy recommending multiple items at\na time; (4) showcasing the value of exploration. We demonstrate the efficacy of\nour approaches through a series of simulations and multiple live experiments on\nYoutube. \n\n"}
{"id": "1812.02646", "contents": "Title: RepeatNet: A Repeat Aware Neural Recommendation Machine for\n  Session-based Recommendation Abstract: Recurrent neural networks for session-based recommendation have attracted a\nlot of attention recently because of their promising performance. repeat\nconsumption is a common phenomenon in many recommendation scenarios (e.g.,\ne-commerce, music, and TV program recommendations), where the same item is\nre-consumed repeatedly over time. However, no previous studies have emphasized\nrepeat consumption with neural networks. An effective neural approach is needed\nto decide when to perform repeat recommendation. In this paper, we incorporate\na repeat-explore mechanism into neural networks and propose a new model, called\nRepeatNet, with an encoder-decoder structure. RepeatNet integrates a regular\nneural recommendation approach in the decoder with a new repeat recommendation\nmechanism that can choose items from a user's history and recommends them at\nthe right time. We report on extensive experiments on three benchmark datasets.\nRepeatNet outperforms state-of-the-art baselines on all three datasets in terms\nof MRR and Recall. Furthermore, as the dataset size and the repeat ratio\nincrease, the improvements of RepeatNet over the baselines also increase, which\ndemonstrates its advantage in handling repeat recommendation scenarios. \n\n"}
{"id": "1812.03483", "contents": "Title: To Reverse the Gradient or Not: An Empirical Comparison of Adversarial\n  and Multi-task Learning in Speech Recognition Abstract: Transcribed datasets typically contain speaker identity for each instance in\nthe data. We investigate two ways to incorporate this information during\ntraining: Multi-Task Learning and Adversarial Learning. In multi-task learning,\nthe goal is speaker prediction; we expect a performance improvement with this\njoint training if the two tasks of speech recognition and speaker recognition\nshare a common set of underlying features. In contrast, adversarial learning is\na means to learn representations invariant to the speaker. We then expect\nbetter performance if this learnt invariance helps generalizing to new\nspeakers. While the two approaches seem natural in the context of speech\nrecognition, they are incompatible because they correspond to opposite\ngradients back-propagated to the model. In order to better understand the\neffect of these approaches in terms of error rates, we compare both strategies\nin controlled settings. Moreover, we explore the use of additional\nuntranscribed data in a semi-supervised, adversarial learning manner to improve\nerror rates. Our results show that deep models trained on big datasets already\ndevelop invariant representations to speakers without any auxiliary loss. When\nconsidering adversarial learning and multi-task learning, the impact on the\nacoustic model seems minor. However, models trained in a semi-supervised manner\ncan improve error-rates. \n\n"}
{"id": "1812.03919", "contents": "Title: Pretraining by Backtranslation for End-to-end ASR in Low-Resource\n  Settings Abstract: We explore training attention-based encoder-decoder ASR in low-resource\nsettings. These models perform poorly when trained on small amounts of\ntranscribed speech, in part because they depend on having sufficient\ntarget-side text to train the attention and decoder networks. In this paper we\naddress this shortcoming by pretraining our network parameters using only\ntext-based data and transcribed speech from other languages. We analyze the\nrelative contributions of both sources of data. Across 3 test languages, our\ntext-based approach resulted in a 20% average relative improvement over a\ntext-based augmentation technique without pretraining. Using transcribed speech\nfrom nearby languages gives a further 20-30% relative reduction in character\nerror rate. \n\n"}
{"id": "1812.04160", "contents": "Title: Delta Embedding Learning Abstract: Unsupervised word embeddings have become a popular approach of word\nrepresentation in NLP tasks. However there are limitations to the semantics\nrepresented by unsupervised embeddings, and inadequate fine-tuning of\nembeddings can lead to suboptimal performance. We propose a novel learning\ntechnique called Delta Embedding Learning, which can be applied to general NLP\ntasks to improve performance by optimized tuning of the word embeddings. A\nstructured regularization is applied to the embeddings to ensure they are tuned\nin an incremental way. As a result, the tuned word embeddings become better\nword representations by absorbing semantic information from supervision without\n\"forgetting.\" We apply the method to various NLP tasks and see a consistent\nimprovement in performance. Evaluation also confirms the tuned word embeddings\nhave better semantic properties. \n\n"}
{"id": "1812.04224", "contents": "Title: On the Dimensionality of Word Embedding Abstract: In this paper, we provide a theoretical understanding of word embedding and\nits dimensionality. Motivated by the unitary-invariance of word embedding, we\npropose the Pairwise Inner Product (PIP) loss, a novel metric on the\ndissimilarity between word embeddings. Using techniques from matrix\nperturbation theory, we reveal a fundamental bias-variance trade-off in\ndimensionality selection for word embeddings. This bias-variance trade-off\nsheds light on many empirical observations which were previously unexplained,\nfor example the existence of an optimal dimensionality. Moreover, new insights\nand discoveries, like when and how word embeddings are robust to over-fitting,\nare revealed. By optimizing over the bias-variance trade-off of the PIP loss,\nwe can explicitly answer the open question of dimensionality selection for word\nembedding. \n\n"}
{"id": "1812.04606", "contents": "Title: Deep Anomaly Detection with Outlier Exposure Abstract: It is important to detect anomalous inputs when deploying machine learning\nsystems. The use of larger and more complex inputs in deep learning magnifies\nthe difficulty of distinguishing between anomalous and in-distribution\nexamples. At the same time, diverse image and text data are available in\nenormous quantities. We propose leveraging these data to improve deep anomaly\ndetection by training anomaly detectors against an auxiliary dataset of\noutliers, an approach we call Outlier Exposure (OE). This enables anomaly\ndetectors to generalize and detect unseen anomalies. In extensive experiments\non natural language processing and small- and large-scale vision tasks, we find\nthat Outlier Exposure significantly improves detection performance. We also\nobserve that cutting-edge generative models trained on CIFAR-10 may assign\nhigher likelihoods to SVHN images than to CIFAR-10 images; we use OE to\nmitigate this issue. We also analyze the flexibility and robustness of Outlier\nExposure, and identify characteristics of the auxiliary dataset that improve\nperformance. \n\n"}
{"id": "1812.05271", "contents": "Title: TextBugger: Generating Adversarial Text Against Real-world Applications Abstract: Deep Learning-based Text Understanding (DLTU) is the backbone technique\nbehind various applications, including question answering, machine translation,\nand text classification. Despite its tremendous popularity, the security\nvulnerabilities of DLTU are still largely unknown, which is highly concerning\ngiven its increasing use in security-sensitive applications such as sentiment\nanalysis and toxic content detection. In this paper, we show that DLTU is\ninherently vulnerable to adversarial text attacks, in which maliciously crafted\ntexts trigger target DLTU systems and services to misbehave. Specifically, we\npresent TextBugger, a general attack framework for generating adversarial\ntexts. In contrast to prior works, TextBugger differs in significant ways: (i)\neffective -- it outperforms state-of-the-art attacks in terms of attack success\nrate; (ii) evasive -- it preserves the utility of benign text, with 94.9\\% of\nthe adversarial text correctly recognized by human readers; and (iii) efficient\n-- it generates adversarial text with computational complexity sub-linear to\nthe text length. We empirically evaluate TextBugger on a set of real-world DLTU\nsystems and services used for sentiment analysis and toxic content detection,\ndemonstrating its effectiveness, evasiveness, and efficiency. For instance,\nTextBugger achieves 100\\% success rate on the IMDB dataset based on Amazon AWS\nComprehend within 4.61 seconds and preserves 97\\% semantic similarity. We\nfurther discuss possible defense mechanisms to mitigate such attack and the\nadversary's potential countermeasures, which leads to promising directions for\nfurther research. \n\n"}
{"id": "1812.05473", "contents": "Title: Learning Features of Network Structures Using Graphlets Abstract: Networks are fundamental to the study of complex systems, ranging from social\ncontacts, message transactions, to biological regulations and economical\nnetworks. In many realistic applications, these networks may vary over time.\nModeling and analyzing such temporal properties is of additional interest as it\ncan provide a richer characterization of relations between nodes in networks.\nIn this paper, we explore the role of \\emph{graphlets} in network\nclassification for both static and temporal networks. Graphlets are small\nnon-isomorphic induced subgraphs representing connected patterns in a network\nand their frequency can be used to assess network structures. We show that\ngraphlet features, which are not captured by state-of-the-art methods, play a\nsignificant role in enhancing the performance of network classification. To\nthat end, we propose two novel graphlet-based techniques, \\emph{gl2vec} for\nnetwork embedding, and \\emph{gl-DCNN} for diffusion-convolutional neural\nnetworks. We demonstrate the efficacy and usability of \\emph{gl2vec} and\n\\emph{gl-DCNN} through extensive experiments using several real-world static\nand temporal networks. We find that features learned from graphlets can bring\nnotable performance increases to state-of-the-art methods in network analysis. \n\n"}
{"id": "1812.06158", "contents": "Title: Few-shot classification in Named Entity Recognition Task Abstract: For many natural language processing (NLP) tasks the amount of annotated data\nis limited. This urges a need to apply semi-supervised learning techniques,\nsuch as transfer learning or meta-learning. In this work we tackle Named Entity\nRecognition (NER) task using Prototypical Network - a metric learning\ntechnique. It learns intermediate representations of words which cluster well\ninto named entity classes. This property of the model allows classifying words\nwith extremely limited number of training examples, and can potentially be used\nas a zero-shot learning method. By coupling this technique with transfer\nlearning we achieve well-performing classifiers trained on only 20 instances of\na target class. \n\n"}
{"id": "1812.06834", "contents": "Title: A Tutorial on Deep Latent Variable Models of Natural Language Abstract: There has been much recent, exciting work on combining the complementary\nstrengths of latent variable models and deep learning. Latent variable modeling\nmakes it easy to explicitly specify model constraints through conditional\nindependence properties, while deep learning makes it possible to parameterize\nthese conditional likelihoods with powerful function approximators. While these\n\"deep latent variable\" models provide a rich, flexible framework for modeling\nmany real-world phenomena, difficulties exist: deep parameterizations of\nconditional likelihoods usually make posterior inference intractable, and\nlatent variable objectives often complicate backpropagation by introducing\npoints of non-differentiability. This tutorial explores these issues in depth\nthrough the lens of variational inference. \n\n"}
{"id": "1812.07104", "contents": "Title: Reading Industrial Inspection Sheets by Inferring Visual Relations Abstract: The traditional mode of recording faults in heavy factory equipment has been\nvia hand marked inspection sheets, wherein a machine engineer manually marks\nthe faulty machine regions on a paper outline of the machine. Over the years,\nmillions of such inspection sheets have been recorded and the data within these\nsheets has remained inaccessible. However, with industries going digital and\nwaking up to the potential value of fault data for machine health monitoring,\nthere is an increased impetus towards digitization of these hand marked\ninspection records. To target this digitization, we propose a novel visual\npipeline combining state of the art deep learning models, with domain knowledge\nand low level vision techniques, followed by inference of visual relationships.\nOur framework is robust to the presence of both static and non-static\nbackground in the document, variability in the machine template diagrams,\nunstructured shape of graphical objects to be identified and variability in the\nstrokes of handwritten text. The proposed pipeline incorporates a capsule and\nspatial transformer network based classifier for accurate text reading, and a\ncustomized CTPN network for text detection in addition to hybrid techniques for\narrow detection and dialogue cloud removal. We have tested our approach on a\nreal world dataset of 50 inspection sheets for large containers and boilers.\nThe results are visually appealing and the pipeline achieved an accuracy of\n87.1% for text detection and 94.6% for text reading. \n\n"}
{"id": "1812.07108", "contents": "Title: Learning Private Neural Language Modeling with Attentive Aggregation Abstract: Mobile keyboard suggestion is typically regarded as a word-level language\nmodeling problem. Centralized machine learning technique requires massive user\ndata collected to train on, which may impose privacy concerns for sensitive\npersonal typing data of users. Federated learning (FL) provides a promising\napproach to learning private language modeling for intelligent personalized\nkeyboard suggestion by training models in distributed clients rather than\ntraining in a central server. To obtain a global model for prediction, existing\nFL algorithms simply average the client models and ignore the importance of\neach client during model aggregation. Furthermore, there is no optimization for\nlearning a well-generalized global model on the central server. To solve these\nproblems, we propose a novel model aggregation with the attention mechanism\nconsidering the contribution of clients models to the global model, together\nwith an optimization technique during server aggregation. Our proposed\nattentive aggregation method minimizes the weighted distance between the server\nmodel and client models through iterative parameters updating while attends the\ndistance between the server model and client models. Through experiments on two\npopular language modeling datasets and a social media dataset, our proposed\nmethod outperforms its counterparts in terms of perplexity and communication\ncost in most settings of comparison. \n\n"}
{"id": "1812.07127", "contents": "Title: Deep reinforcement learning for search, recommendation, and online\n  advertising: a survey Abstract: Search, recommendation, and online advertising are the three most important\ninformation-providing mechanisms on the web. These information seeking\ntechniques, satisfying users' information needs by suggesting users\npersonalized objects (information or services) at the appropriate time and\nplace, play a crucial role in mitigating the information overload problem. With\nrecent great advances in deep reinforcement learning (DRL), there have been\nincreasing interests in developing DRL based information seeking techniques.\nThese DRL based techniques have two key advantages -- (1) they are able to\ncontinuously update information seeking strategies according to users'\nreal-time feedback, and (2) they can maximize the expected cumulative long-term\nreward from users where reward has different definitions according to\ninformation seeking applications such as click-through rate, revenue, user\nsatisfaction and engagement. In this paper, we give an overview of deep\nreinforcement learning for search, recommendation, and online advertising from\nmethodologies to applications, review representative algorithms, and discuss\nsome appealing research directions. \n\n"}
{"id": "1812.07135", "contents": "Title: Globalness Detection in Online Social Network Abstract: Classification problems have made significant progress due to the maturity of\nartificial intelligence (AI). However, differentiating items from categories\nwithout noticeable boundaries is still a huge challenge for machines -- which\nis also crucial for machines to be intelligent.\n  In order to study the fuzzy concept on classification, we define and propose\na globalness detection with the four-stage operational flow. We then\ndemonstrate our framework on Facebook public pages inter-like graph with their\ngeo-location. Our prediction algorithm achieves high precision (89%) and recall\n(88%) of local pages. We evaluate the results on both states and countries\nlevel, finding that the global node ratios are relatively high in those states\n(NY, CA) having large and international cities. Several global nodes examples\nhave also been shown and studied in this paper.\n  It is our hope that our results unveil the perfect value from every\nclassification problem and provide a better understanding of global and local\nnodes in Online Social Networks (OSNs). \n\n"}
{"id": "1812.08879", "contents": "Title: Variational Cross-domain Natural Language Generation for Spoken Dialogue\n  Systems Abstract: Cross-domain natural language generation (NLG) is still a difficult task\nwithin spoken dialogue modelling. Given a semantic representation provided by\nthe dialogue manager, the language generator should generate sentences that\nconvey desired information. Traditional template-based generators can produce\nsentences with all necessary information, but these sentences are not\nsufficiently diverse. With RNN-based models, the diversity of the generated\nsentences can be high, however, in the process some information is lost. In\nthis work, we improve an RNN-based generator by considering latent information\nat the sentence level during generation using the conditional variational\nautoencoder architecture. We demonstrate that our model outperforms the\noriginal RNN-based generator, while yielding highly diverse sentences. In\naddition, our model performs better when the training data is limited. \n\n"}
{"id": "1812.08972", "contents": "Title: COSINE: Compressive Network Embedding on Large-scale Information\n  Networks Abstract: There is recently a surge in approaches that learn low-dimensional embeddings\nof nodes in networks. As there are many large-scale real-world networks, it's\ninefficient for existing approaches to store amounts of parameters in memory\nand update them edge after edge. With the knowledge that nodes having similar\nneighborhood will be close to each other in embedding space, we propose COSINE\n(COmpresSIve NE) algorithm which reduces the memory footprint and accelerates\nthe training process by parameters sharing among similar nodes. COSINE applies\ngraph partitioning algorithms to networks and builds parameter sharing\ndependency of nodes based on the result of partitioning. With parameters\nsharing among similar nodes, COSINE injects prior knowledge about higher\nstructural information into training process which makes network embedding more\nefficient and effective. COSINE can be applied to any embedding lookup method\nand learn high-quality embeddings with limited memory and shorter training\ntime. We conduct experiments of multi-label classification and link prediction,\nwhere baselines and our model have the same memory usage. Experimental results\nshow that COSINE gives baselines up to 23% increase on classification and up to\n25% increase on link prediction. Moreover, time of all representation learning\nmethods using COSINE decreases from 30% to 70%. \n\n"}
{"id": "1812.09195", "contents": "Title: Learning to Navigate the Web Abstract: Learning in environments with large state and action spaces, and sparse\nrewards, can hinder a Reinforcement Learning (RL) agent's learning through\ntrial-and-error. For instance, following natural language instructions on the\nWeb (such as booking a flight ticket) leads to RL settings where input\nvocabulary and number of actionable elements on a page can grow very large.\nEven though recent approaches improve the success rate on relatively simple\nenvironments with the help of human demonstrations to guide the exploration,\nthey still fail in environments where the set of possible instructions can\nreach millions. We approach the aforementioned problems from a different\nperspective and propose guided RL approaches that can generate unbounded amount\nof experience for an agent to learn from. Instead of learning from a\ncomplicated instruction with a large vocabulary, we decompose it into multiple\nsub-instructions and schedule a curriculum in which an agent is tasked with a\ngradually increasing subset of these relatively easier sub-instructions. In\naddition, when the expert demonstrations are not available, we propose a novel\nmeta-learning framework that generates new instruction following tasks and\ntrains the agent more effectively. We train DQN, deep reinforcement learning\nagent, with Q-value function approximated with a novel QWeb neural network\narchitecture on these smaller, synthetic instructions. We evaluate the ability\nof our agent to generalize to new instructions on World of Bits benchmark, on\nforms with up to 100 elements, supporting 14 million possible instructions. The\nQWeb agent outperforms the baseline without using any human demonstration\nachieving 100% success rate on several difficult environments. \n\n"}
{"id": "1812.09323", "contents": "Title: Unsupervised Speech Recognition via Segmental Empirical Output\n  Distribution Matching Abstract: We consider the problem of training speech recognition systems without using\nany labeled data, under the assumption that the learner can only access to the\ninput utterances and a phoneme language model estimated from a non-overlapping\ncorpus. We propose a fully unsupervised learning algorithm that alternates\nbetween solving two sub-problems: (i) learn a phoneme classifier for a given\nset of phoneme segmentation boundaries, and (ii) refining the phoneme\nboundaries based on a given classifier. To solve the first sub-problem, we\nintroduce a novel unsupervised cost function named Segmental Empirical Output\nDistribution Matching, which generalizes the work in (Liu et al., 2017) to\nsegmental structures. For the second sub-problem, we develop an approximate MAP\napproach to refining the boundaries obtained from Wang et al. (2017).\nExperimental results on TIMIT dataset demonstrate the success of this fully\nunsupervised phoneme recognition system, which achieves a phone error rate\n(PER) of 41.6%. Although it is still far away from the state-of-the-art\nsupervised systems, we show that with oracle boundaries and matching language\nmodel, the PER could be improved to 32.5%.This performance approaches the\nsupervised system of the same model architecture, demonstrating the great\npotential of the proposed method. \n\n"}
{"id": "1812.10119", "contents": "Title: Sequence to Sequence Learning for Query Expansion Abstract: Using sequence to sequence algorithms for query expansion has not been\nexplored yet in Information Retrieval literature nor in Question-Answering's.\nWe tried to fill this gap in the literature with a custom Query Expansion\nengine trained and tested on open datasets. Starting from open datasets, we\nbuilt a Query Expansion training set using sentence-embeddings-based Keyword\nExtraction. We therefore assessed the ability of the Sequence to Sequence\nneural networks to capture expanding relations in the words embeddings' space. \n\n"}
{"id": "1812.10234", "contents": "Title: A New Concept of Deep Reinforcement Learning based Augmented General\n  Sequence Tagging System Abstract: In this paper, a new deep reinforcement learning based augmented general\nsequence tagging system is proposed. The new system contains two parts: a deep\nneural network (DNN) based sequence tagging model and a deep reinforcement\nlearning (DRL) based augmented tagger. The augmented tagger helps improve\nsystem performance by modeling the data with minority tags. The new system is\nevaluated on SLU and NLU sequence tagging tasks using ATIS and CoNLL-2003\nbenchmark datasets, to demonstrate the new system's outstanding performance on\ngeneral tagging tasks. Evaluated by F1 scores, it shows that the new system\noutperforms the current state-of-the-art model on ATIS dataset by 1.9% and that\non CoNLL-2003 dataset by 1.4%. \n\n"}
{"id": "1812.10387", "contents": "Title: Same but Different: Distant Supervision for Predicting and Understanding\n  Entity Linking Difficulty Abstract: Entity Linking (EL) is the task of automatically identifying entity mentions\nin a piece of text and resolving them to a corresponding entity in a reference\nknowledge base like Wikipedia. There is a large number of EL tools available\nfor different types of documents and domains, yet EL remains a challenging task\nwhere the lack of precision on particularly ambiguous mentions often spoils the\nusefulness of automated disambiguation results in real applications. A priori\napproximations of the difficulty to link a particular entity mention can\nfacilitate flagging of critical cases as part of semi-automated EL systems,\nwhile detecting latent factors that affect the EL performance, like\ncorpus-specific features, can provide insights on how to improve a system based\non the special characteristics of the underlying corpus. In this paper, we\nfirst introduce a consensus-based method to generate difficulty labels for\nentity mentions on arbitrary corpora. The difficulty labels are then exploited\nas training data for a supervised classification task able to predict the EL\ndifficulty of entity mentions using a variety of features. Experiments over a\ncorpus of news articles show that EL difficulty can be estimated with high\naccuracy, revealing also latent features that affect EL performance. Finally,\nevaluation results demonstrate the effectiveness of the proposed method to\ninform semi-automated EL pipelines. \n\n"}
{"id": "1812.10401", "contents": "Title: Word Embedding based on Low-Rank Doubly Stochastic Matrix Decomposition Abstract: Word embedding, which encodes words into vectors, is an important starting\npoint in natural language processing and commonly used in many text-based\nmachine learning tasks. However, in most current word embedding approaches, the\nsimilarity in embedding space is not optimized in the learning. In this paper\nwe propose a novel neighbor embedding method which directly learns an embedding\nsimplex where the similarities between the mapped words are optimal in terms of\nminimal discrepancy to the input neighborhoods. Our method is built upon\ntwo-step random walks between words via topics and thus able to better reveal\nthe topics among the words. Experiment results indicate that our method,\ncompared with another existing word embedding approach, is more favorable for\nvarious queries. \n\n"}
{"id": "1812.10408", "contents": "Title: Hyperbolic Deep Learning for Chinese Natural Language Understanding Abstract: Recently hyperbolic geometry has proven to be effective in building\nembeddings that encode hierarchical and entailment information. This makes it\nparticularly suited to modelling the complex asymmetrical relationships between\nChinese characters and words. In this paper we first train a large scale\nhyperboloid skip-gram model on a Chinese corpus, then apply the character\nembeddings to a downstream hyperbolic Transformer model derived from the\nprinciples of gyrovector space for Poincare disk model. In our experiments the\ncharacter-based Transformer outperformed its word-based Euclidean equivalent.\nTo the best of our knowledge, this is the first time in Chinese NLP that a\ncharacter-based model outperformed its word-based counterpart, allowing the\ncircumvention of the challenging and domain-dependent task of Chinese Word\nSegmentation (CWS). \n\n"}
{"id": "1812.10962", "contents": "Title: A Variational Topological Neural Model for Cascade-based Diffusion in\n  Networks Abstract: Many works have been proposed in the literature to capture the dynamics of\ndiffusion in networks. While some of them define graphical markovian models to\nextract temporal relationships between node infections in networks, others\nconsider diffusion episodes as sequences of infections via recurrent neural\nmodels. In this paper we propose a model at the crossroads of these two\nextremes, which embeds the history of diffusion in infected nodes as hidden\ncontinuous states. Depending on the trajectory followed by the content before\nreaching a given node, the distribution of influence probabilities may vary.\nHowever, content trajectories are usually hidden in the data, which induces\nchallenging learning problems. We propose a topological recurrent neural model\nwhich exhibits good experimental performances for diffusion modelling and\nprediction. \n\n"}
{"id": "1812.11158", "contents": "Title: MEETING BOT: Reinforcement Learning for Dialogue Based Meeting\n  Scheduling Abstract: In this paper we present Meeting Bot, a reinforcement learning based\nconversational system that interacts with multiple users to schedule meetings.\nThe system is able to interpret user utterences and map them to preferred time\nslots, which are then fed to a reinforcement learning (RL) system with the goal\nof converging on an agreeable time slot. The RL system is able to adapt to user\npreferences and environmental changes in meeting arrival rate while still\nscheduling effectively. Learning is performed via policy gradient with\nexploration, by utilizing an MLP as an approximator of the policy function.\nResults demonstrate that the system outperforms standard scheduling algorithms\nin terms of overall scheduling efficiency. Additionally, the system is able to\nadapt its strategy to situations when users consistently reject or accept\nmeetings in certain slots (such as Friday afternoon versus Thursday morning),\nor when the meeting is called by members who are at a more senior designation. \n\n"}
{"id": "1812.11448", "contents": "Title: Removing Malicious Nodes from Networks Abstract: A fundamental challenge in networked systems is detection and removal of\nsuspected malicious nodes. In reality, detection is always imperfect, and the\ndecision about which potentially malicious nodes to remove must trade off false\npositives (erroneously removing benign nodes) and false negatives (mistakenly\nfailing to remove malicious nodes). However, in network settings this\nconventional tradeoff must now account for node connectivity. In particular,\nmalicious nodes may exert malicious influence, so that mistakenly leaving some\nof these in the network may cause damage to spread. On the other hand, removing\nbenign nodes causes direct harm to these, and indirect harm to their benign\nneighbors who would wish to communicate with them.\n  We formalize the problem of removing potentially malicious nodes from a\nnetwork under uncertainty through an objective that takes connectivity into\naccount. We show that optimally solving the resulting problem is NP-Hard. We\nthen propose a tractable solution approach based on a convex relaxation of the\nobjective. Finally, we experimentally demonstrate that our approach\nsignificantly outperforms both a simple baseline that ignores network\nstructure, as well as a state-of-the-art approach for a related problem, on\nboth synthetic and real-world datasets. \n\n"}
{"id": "1812.11760", "contents": "Title: Multilingual Constituency Parsing with Self-Attention and Pre-Training Abstract: We show that constituency parsing benefits from unsupervised pre-training\nacross a variety of languages and a range of pre-training conditions. We first\ncompare the benefits of no pre-training, fastText, ELMo, and BERT for English\nand find that BERT outperforms ELMo, in large part due to increased model\ncapacity, whereas ELMo in turn outperforms the non-contextual fastText\nembeddings. We also find that pre-training is beneficial across all 11\nlanguages tested; however, large model sizes (more than 100 million parameters)\nmake it computationally expensive to train separate models for each language.\nTo address this shortcoming, we show that joint multilingual pre-training and\nfine-tuning allows sharing all but a small number of parameters between ten\nlanguages in the final model. The 10x reduction in model size compared to\nfine-tuning one model per language causes only a 3.2% relative error increase\nin aggregate. We further explore the idea of joint fine-tuning and show that it\ngives low-resource languages a way to benefit from the larger datasets of other\nlanguages. Finally, we demonstrate new state-of-the-art results for 11\nlanguages, including English (95.8 F1) and Chinese (91.8 F1). \n\n"}
{"id": "1901.00439", "contents": "Title: Deep Representation Learning for Clustering of Health Tweets Abstract: Twitter has been a prominent social media platform for mining\npopulation-level health data and accurate clustering of health-related tweets\ninto topics is important for extracting relevant health insights. In this work,\nwe propose deep convolutional autoencoders for learning compact representations\nof health-related tweets, further to be employed in clustering. We compare our\nmethod to several conventional tweet representation methods including\nbag-of-words, term frequency-inverse document frequency, Latent Dirichlet\nAllocation and Non-negative Matrix Factorization with 3 different clustering\nalgorithms. Our results show that the clustering performance using proposed\nrepresentation learning scheme significantly outperforms that of conventional\nmethods for all experiments of different number of clusters. In addition, we\npropose a constraint on the learned representations during the neural network\ntraining in order to further enhance the clustering performance. All in all,\nthis study introduces utilization of deep neural network-based architectures,\ni.e., deep convolutional autoencoders, for learning informative representations\nof health-related tweets. \n\n"}
{"id": "1901.01122", "contents": "Title: Machine Translation: A Literature Review Abstract: Machine translation (MT) plays an important role in benefiting linguists,\nsociologists, computer scientists, etc. by processing natural language to\ntranslate it into some other natural language. And this demand has grown\nexponentially over past couple of years, considering the enormous exchange of\ninformation between different regions with different regional languages.\nMachine Translation poses numerous challenges, some of which are: a) Not all\nwords in one language has equivalent word in another language b) Two given\nlanguages may have completely different structures c) Words can have more than\none meaning. Owing to these challenges, along with many others, MT has been\nactive area of research for more than five decades. Numerous methods have been\nproposed in the past which either aim at improving the quality of the\ntranslations generated by them, or study the robustness of these systems by\nmeasuring their performance on many different languages. In this literature\nreview, we discuss statistical approaches (in particular word-based and\nphrase-based) and neural approaches which have gained widespread prominence\nowing to their state-of-the-art results across multiple major languages. \n\n"}
{"id": "1901.02081", "contents": "Title: Team EP at TAC 2018: Automating data extraction in systematic reviews of\n  environmental agents Abstract: We describe our entry for the Systematic Review Information Extraction track\nof the 2018 Text Analysis Conference. Our solution is an end-to-end, deep\nlearning, sequence tagging model based on the BI-LSTM-CRF architecture.\nHowever, we use interleaved, alternating LSTM layers with highway connections\ninstead of the more traditional approach, where last hidden states of both\ndirections are concatenated to create an input to the next layer. We also make\nextensive use of pre-trained word embeddings, namely GloVe and ELMo. Thanks to\na number of regularization techniques, we were able to achieve relatively large\ncapacity of the model (31.3M+ of trainable parameters) for the size of training\nset (100 documents, less than 200K tokens). The system's official score was\n60.9% (micro-F1) and it ranked first for the Task 1. Additionally, after\nrectifying an obvious mistake in the submission format, the system scored\n67.35%. \n\n"}
{"id": "1901.02348", "contents": "Title: Improving noise robustness of automatic speech recognition via parallel\n  data and teacher-student learning Abstract: For real-world speech recognition applications, noise robustness is still a\nchallenge. In this work, we adopt the teacher-student (T/S) learning technique\nusing a parallel clean and noisy corpus for improving automatic speech\nrecognition (ASR) performance under multimedia noise. On top of that, we apply\na logits selection method which only preserves the k highest values to prevent\nwrong emphasis of knowledge from the teacher and to reduce bandwidth needed for\ntransferring data. We incorporate up to 8000 hours of untranscribed data for\ntraining and present our results on sequence trained models apart from cross\nentropy trained ones. The best sequence trained student model yields relative\nword error rate (WER) reductions of approximately 10.1%, 28.7% and 19.6% on our\nclean, simulated noisy and real test sets respectively comparing to a sequence\ntrained teacher. \n\n"}
{"id": "1901.02671", "contents": "Title: Is it Time to Swish? Comparing Deep Learning Activation Functions Across\n  NLP tasks Abstract: Activation functions play a crucial role in neural networks because they are\nthe nonlinearities which have been attributed to the success story of deep\nlearning. One of the currently most popular activation functions is ReLU, but\nseveral competitors have recently been proposed or 'discovered', including\nLReLU functions and swish. While most works compare newly proposed activation\nfunctions on few tasks (usually from image classification) and against few\ncompetitors (usually ReLU), we perform the first large-scale comparison of 21\nactivation functions across eight different NLP tasks. We find that a largely\nunknown activation function performs most stably across all tasks, the\nso-called penalized tanh function. We also show that it can successfully\nreplace the sigmoid and tanh gates in LSTM cells, leading to a 2 percentage\npoint (pp) improvement over the standard choices on a challenging NLP task. \n\n"}
{"id": "1901.02860", "contents": "Title: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Abstract: Transformers have a potential of learning longer-term dependency, but are\nlimited by a fixed-length context in the setting of language modeling. We\npropose a novel neural architecture Transformer-XL that enables learning\ndependency beyond a fixed length without disrupting temporal coherence. It\nconsists of a segment-level recurrence mechanism and a novel positional\nencoding scheme. Our method not only enables capturing longer-term dependency,\nbut also resolves the context fragmentation problem. As a result,\nTransformer-XL learns dependency that is 80% longer than RNNs and 450% longer\nthan vanilla Transformers, achieves better performance on both short and long\nsequences, and is up to 1,800+ times faster than vanilla Transformers during\nevaluation. Notably, we improve the state-of-the-art results of bpc/perplexity\nto 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion\nWord, and 54.5 on Penn Treebank (without finetuning). When trained only on\nWikiText-103, Transformer-XL manages to generate reasonably coherent, novel\ntext articles with thousands of tokens. Our code, pretrained models, and\nhyperparameters are available in both Tensorflow and PyTorch. \n\n"}
{"id": "1901.03829", "contents": "Title: Predicting Diffusion Reach Probabilities via Representation Learning on\n  Social Networks Abstract: Diffusion reach probability between two nodes on a network is defined as the\nprobability of a cascade originating from one node reaching to another node. An\ninfinite number of cascades would enable calculation of true diffusion reach\nprobabilities between any two nodes. However, there exists only a finite number\nof cascades and one usually has access only to a small portion of all available\ncascades. In this work, we addressed the problem of estimating diffusion reach\nprobabilities given only a limited number of cascades and partial information\nabout underlying network structure. Our proposed strategy employs node\nrepresentation learning to generate and feed node embeddings into machine\nlearning algorithms to create models that predict diffusion reach\nprobabilities. We provide experimental analysis using synthetically generated\ncascades on two real-world social networks. Results show that proposed method\nis superior to using values calculated from available cascades when the portion\nof cascades is small. \n\n"}
{"id": "1901.04085", "contents": "Title: Passage Re-ranking with BERT Abstract: Recently, neural models pretrained on a language modeling task, such as ELMo\n(Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et\nal., 2018), have achieved impressive results on various natural language\nprocessing tasks such as question-answering and natural language inference. In\nthis paper, we describe a simple re-implementation of BERT for query-based\npassage re-ranking. Our system is the state of the art on the TREC-CAR dataset\nand the top entry in the leaderboard of the MS MARCO passage retrieval task,\noutperforming the previous state of the art by 27% (relative) in MRR@10. The\ncode to reproduce our results is available at\nhttps://github.com/nyu-dl/dl4marco-bert \n\n"}
{"id": "1901.04097", "contents": "Title: Search Efficient Binary Network Embedding Abstract: Traditional network embedding primarily focuses on learning a continuous\nvector representation for each node, preserving network structure and/or node\ncontent information, such that off-the-shelf machine learning algorithms can be\neasily applied to the vector-format node representations for network analysis.\nHowever, the learned continuous vector representations are inefficient for\nlarge-scale similarity search, which often involves finding nearest neighbors\nmeasured by distance or similarity in a continuous vector space. In this paper,\nwe propose a search efficient binary network embedding algorithm called\nBinaryNE to learn a binary code for each node, by simultaneously modeling node\ncontext relations and node attribute relations through a three-layer neural\nnetwork. BinaryNE learns binary node representations through a stochastic\ngradient descent based online learning algorithm. The learned binary encoding\nnot only reduces memory usage to represent each node, but also allows fast\nbit-wise comparisons to support faster node similarity search than using\nEuclidean distance or other distance measures. Extensive experiments and\ncomparisons demonstrate that BinaryNE not only delivers more than 25 times\nfaster search speed, but also provides comparable or better search quality than\ntraditional continuous vector based network embedding methods. The binary codes\nlearned by BinaryNE also render competitive performance on node classification\nand node clustering tasks. The source code of this paper is available at\nhttps://github.com/daokunzhang/BinaryNE. \n\n"}
{"id": "1901.04277", "contents": "Title: Natural Disasters Detection in Social Media and Satellite imagery: a\n  survey Abstract: The analysis of natural disaster-related multimedia content got great\nattention in recent years. Being one of the most important sources of\ninformation, social media have been crawled over the years to collect and\nanalyze disaster-related multimedia content. Satellite imagery has also been\nwidely explored for disasters analysis. In this paper, we survey the existing\nliterature on disaster detection and analysis of the retrieved information from\nsocial media and satellites. Literature on disaster detection and analysis of\nrelated multimedia content on the basis of the nature of the content can be\ncategorized into three groups, namely (i) disaster detection in text; (ii)\nanalysis of disaster-related visual content from social media; and (iii)\ndisaster detection in satellite imagery. We extensively review different\napproaches proposed in these three domains. Furthermore, we also review\nbenchmarking datasets available for the evaluation of disaster detection\nframeworks. Moreover, we provide a detailed discussion on the insights obtained\nfrom the literature review, and identify future trends and challenges, which\nwill provide an important starting point for the researchers in the field. \n\n"}
{"id": "1901.04321", "contents": "Title: Large-scale Collaborative Filtering with Product Embeddings Abstract: The application of machine learning techniques to large-scale personalized\nrecommendation problems is a challenging task. Such systems must make sense of\nenormous amounts of implicit feedback in order to understand user preferences\nacross numerous product categories. This paper presents a deep learning based\nsolution to this problem within the collaborative filtering with implicit\nfeedback framework. Our approach combines neural attention mechanisms, which\nallow for context dependent weighting of past behavioral signals, with\nrepresentation learning techniques to produce models which obtain extremely\nhigh coverage, can easily incorporate new information as it becomes available,\nand are computationally efficient. Offline experiments demonstrate significant\nperformance improvements when compared to several alternative methods from the\nliterature. Results from an online setting show that the approach compares\nfavorably with current production techniques used to produce personalized\nproduct recommendations. \n\n"}
{"id": "1901.07005", "contents": "Title: DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition\n  and Linking in Tweets Abstract: In recent years, with the prevalence of social media and smart devices,\npeople causally reveal their locations such as shops, hotels, and restaurants\nin their tweets. Recognizing and linking such fine-grained location mentions to\nwell-defined location profiles are beneficial for retrieval and recommendation\nsystems. In this paper, we propose DLocRL, a new deep learning pipeline for\nfine-grained location recognition and linking in tweets, and verify its\neffectiveness on a real-world Twitter dataset. \n\n"}
{"id": "1901.07199", "contents": "Title: Transfer Meets Hybrid: A Synthetic Approach for Cross-Domain\n  Collaborative Filtering with Text Abstract: Collaborative filtering (CF) is the key technique for recommender systems\n(RSs). CF exploits user-item behavior interactions (e.g., clicks) only and\nhence suffers from the data sparsity issue. One research thread is to integrate\nauxiliary information such as product reviews and news titles, leading to\nhybrid filtering methods. Another thread is to transfer knowledge from other\nsource domains such as improving the movie recommendation with the knowledge\nfrom the book domain, leading to transfer learning methods. In real-world life,\nno single service can satisfy a user's all information needs. Thus it motivates\nus to exploit both auxiliary and source information for RSs in this paper. We\npropose a novel neural model to smoothly enable Transfer Meeting Hybrid (TMH)\nmethods for cross-domain recommendation with unstructured text in an end-to-end\nmanner. TMH attentively extracts useful content from unstructured text via a\nmemory module and selectively transfers knowledge from a source domain via a\ntransfer network. On two real-world datasets, TMH shows better performance in\nterms of three ranking metrics by comparing with various baselines. We conduct\nthorough analyses to understand how the text content and transferred knowledge\nhelp the proposed model. \n\n"}
{"id": "1901.07786", "contents": "Title: Self-Attentive Model for Headline Generation Abstract: Headline generation is a special type of text summarization task. While the\namount of available training data for this task is almost unlimited, it still\nremains challenging, as learning to generate headlines for news articles\nimplies that the model has strong reasoning about natural language. To overcome\nthis issue, we applied recent Universal Transformer architecture paired with\nbyte-pair encoding technique and achieved new state-of-the-art results on the\nNew York Times Annotated corpus with ROUGE-L F1-score 24.84 and ROUGE-2\nF1-score 13.48. We also present the new RIA corpus and reach ROUGE-L F1-score\n36.81 and ROUGE-2 F1-score 22.15 on it. \n\n"}
{"id": "1901.07878", "contents": "Title: \"Is this an example image?\" -- Predicting the Relative Abstractness\n  Level of Image and Text Abstract: Successful multimodal search and retrieval requires the automatic\nunderstanding of semantic cross-modal relations, which, however, is still an\nopen research problem. Previous work has suggested the metrics cross-modal\nmutual information and semantic correlation to model and predict cross-modal\nsemantic relations of image and text. In this paper, we present an approach to\npredict the (cross-modal) relative abstractness level of a given image-text\npair, that is whether the image is an abstraction of the text or vice versa.\nFor this purpose, we introduce a new metric that captures this specific\nrelationship between image and text at the Abstractness Level (ABS). We present\na deep learning approach to predict this metric, which relies on an autoencoder\narchitecture that allows us to significantly reduce the required amount of\nlabeled training data. A comprehensive set of publicly available scientific\ndocuments has been gathered. Experimental results on a challenging test set\ndemonstrate the feasibility of the approach. \n\n"}
{"id": "1901.07880", "contents": "Title: Phonetic-enriched Text Representation for Chinese Sentiment Analysis\n  with Reinforcement Learning Abstract: The Chinese pronunciation system offers two characteristics that distinguish\nit from other languages: deep phonemic orthography and intonation variations.\nWe are the first to argue that these two important properties can play a major\nrole in Chinese sentiment analysis. Particularly, we propose two effective\nfeatures to encode phonetic information. Next, we develop a Disambiguate\nIntonation for Sentiment Analysis (DISA) network using a reinforcement network.\nIt functions as disambiguating intonations for each Chinese character (pinyin).\nThus, a precise phonetic representation of Chinese is learned. Furthermore, we\nalso fuse phonetic features with textual and visual features in order to mimic\nthe way humans read and understand Chinese text. Experimental results on five\ndifferent Chinese sentiment analysis datasets show that the inclusion of\nphonetic features significantly and consistently improves the performance of\ntextual and visual representations and outshines the state-of-the-art Chinese\ncharacter level representations. \n\n"}
{"id": "1901.08286", "contents": "Title: Neural IR Meets Graph Embedding: A Ranking Model for Product Search Abstract: Recently, neural models for information retrieval are becoming increasingly\npopular. They provide effective approaches for product search due to their\ncompetitive advantages in semantic matching. However, it is challenging to use\ngraph-based features, though proved very useful in IR literature, in these\nneural approaches. In this paper, we leverage the recent advances in graph\nembedding techniques to enable neural retrieval models to exploit\ngraph-structured data for automatic feature extraction. The proposed approach\ncan not only help to overcome the long-tail problem of click-through data, but\nalso incorporate external heterogeneous information to improve search results.\nExtensive experiments on a real-world e-commerce dataset demonstrate\nsignificant improvement achieved by our proposed approach over multiple strong\nbaselines both as an individual retrieval model and as a feature used in\nlearning-to-rank frameworks. \n\n"}
{"id": "1901.09296", "contents": "Title: Variational Smoothing in Recurrent Neural Network Language Models Abstract: We present a new theoretical perspective of data noising in recurrent neural\nnetwork language models (Xie et al., 2017). We show that each variant of data\nnoising is an instance of Bayesian recurrent neural networks with a particular\nvariational distribution (i.e., a mixture of Gaussians whose weights depend on\nstatistics derived from the corpus such as the unigram distribution). We use\nthis insight to propose a more principled method to apply at prediction time\nand propose natural extensions to data noising under the variational framework.\nIn particular, we propose variational smoothing with tied input and output\nembedding matrices and an element-wise variational smoothing method. We\nempirically verify our analysis on two benchmark language modeling datasets and\ndemonstrate performance improvements over existing data noising methods. \n\n"}
{"id": "1901.09656", "contents": "Title: EXIT Analysis for Community Detection Abstract: This paper employs the extrinsic information transfer (EXIT) method, a\ntechnique imported from the analysis of the iterative decoding of error control\ncodes, to study the performance of belief propagation in community detection in\nthe presence of side information. We consider both the detection of a single\n(hidden) community, as well as the problem of identifying two symmetric\ncommunities. For single community detection, this paper demonstrates the\nsuitability of EXIT to predict the asymptotic phase transition for weak\nrecovery. More importantly, EXIT analysis is leveraged to produce useful\ninsights such as the performance of belief propagation near the threshold. For\ntwo symmetric communities, the asymptotic residual error for belief propagation\nis calculated under finite-alphabet side information, generalizing a previous\nresult with noisy labels. EXIT analysis is used to illuminate the effect of\nside information on community detection, its relative importance depending on\nthe correlation of the graphical information with node labels, as well as the\neffect of side information on residual errors. \n\n"}
{"id": "1901.09674", "contents": "Title: Deep Generative Graph Distribution Learning for Synthetic Power Grids Abstract: Power system studies require the topological structures of real-world power\nnetworks; however, such data is confidential due to important security\nconcerns. Thus, power grid synthesis (PGS), i.e., creating realistic power\ngrids that imitate actual power networks, has gained significant attention. In\nthis letter, we cast PGS into a graph distribution learning (GDL) problem where\nthe probability distribution functions (PDFs) of the nodes (buses) and edges\n(lines) are captured. A novel deep GDL (DeepGDL) model is proposed to learn the\ntopological patterns of buses/lines with their physical features (e.g., power\ninjection and line impedance). Having a deep nonlinear recurrent structure,\nDeepGDL understands complex nonlinear topological properties and captures the\ngraph PDF. Sampling from the obtained PDF, we are able to create a large set of\nrealistic networks that all resemble the original power grid. Simulation\nresults show the significant accuracy of our created synthetic power grids in\nterms of various topological metrics and power flow measurements. \n\n"}
{"id": "1901.09680", "contents": "Title: The Intrinsic Scale of Networks is Small Abstract: We define the intrinsic scale at which a network begins to reveal its\nidentity as the scale at which subgraphs in the network (created by a random\nwalk) are distinguishable from similar sized subgraphs in a perturbed copy of\nthe network. We conduct an extensive study of intrinsic scale for several\nnetworks, ranging from structured (e.g. road networks) to ad-hoc and\nunstructured (e.g. crowd sourced information networks), to biological. We find:\n(a) The intrinsic scale is surprisingly small (7-20 vertices), even though the\nnetworks are many orders of magnitude larger. (b) The intrinsic scale\nquantifies ``structure'' in a network -- networks which are explicitly\nconstructed for specific tasks have smaller intrinsic scale. (c) The structure\nat different scales can be fragile (easy to disrupt) or robust. \n\n"}
{"id": "1901.09681", "contents": "Title: Network Lens: Node Classification in Topologically Heterogeneous\n  Networks Abstract: We study the problem of identifying different behaviors occurring in\ndifferent parts of a large heterogenous network. We zoom in to the network\nusing lenses of different sizes to capture the local structure of the network.\nThese network signatures are then weighted to provide a set of predicted labels\nfor every node. We achieve a peak accuracy of $\\sim42\\%$ (random=$11\\%$) on two\nnetworks with $\\sim100,000$ and $\\sim1,000,000$ nodes each. Further, we perform\nbetter than random even when the given node is connected to up to 5 different\ntypes of networks. Finally, we perform this analysis on homogeneous networks\nand show that highly structured networks have high homogeneity. \n\n"}
{"id": "1901.09715", "contents": "Title: Revisiting the Bethe-Hessian: Improved Community Detection in Sparse\n  Heterogeneous Graphs Abstract: Spectral clustering is one of the most popular, yet still incompletely\nunderstood, methods for community detection on graphs. This article studies\nspectral clustering based on the Bethe-Hessian matrix $H_r = (r^2-1)I_n + D-rA$\nfor sparse heterogeneous graphs (following the degree-corrected stochastic\nblock model) in a two-class setting. For a specific value $r = \\zeta$,\nclustering is shown to be insensitive to the degree heterogeneity. We then\nstudy the behavior of the informative eigenvector of $H_{\\zeta}$ and, as a\nresult, predict the clustering accuracy. The article concludes with an overview\nof the generalization to more than two classes along with extensive simulations\non synthetic and real networks corroborating our findings. \n\n"}
{"id": "1901.09821", "contents": "Title: Squeezed Very Deep Convolutional Neural Networks for Text Classification Abstract: Most of the research in convolutional neural networks has focused on\nincreasing network depth to improve accuracy, resulting in a massive number of\nparameters which restricts the trained network to platforms with memory and\nprocessing constraints. We propose to modify the structure of the Very Deep\nConvolutional Neural Networks (VDCNN) model to fit mobile platforms constraints\nand keep performance. In this paper, we evaluate the impact of Temporal\nDepthwise Separable Convolutions and Global Average Pooling in the network\nparameters, storage size, and latency. The squeezed model (SVDCNN) is between\n10x and 20x smaller, depending on the network depth, maintaining a maximum size\nof 6MB. Regarding accuracy, the network experiences a loss between 0.4% and\n1.3% and obtains lower latencies compared to the baseline model. \n\n"}
{"id": "1901.11117", "contents": "Title: The Evolved Transformer Abstract: Recent works have highlighted the strength of the Transformer architecture on\nsequence tasks while, at the same time, neural architecture search (NAS) has\nbegun to outperform human-designed models. Our goal is to apply NAS to search\nfor a better alternative to the Transformer. We first construct a large search\nspace inspired by the recent advances in feed-forward sequence models and then\nrun evolutionary architecture search with warm starting by seeding our initial\npopulation with the Transformer. To directly search on the computationally\nexpensive WMT 2014 English-German translation task, we develop the Progressive\nDynamic Hurdles method, which allows us to dynamically allocate more resources\nto more promising candidate models. The architecture found in our experiments\n-- the Evolved Transformer -- demonstrates consistent improvement over the\nTransformer on four well-established language tasks: WMT 2014 English-German,\nWMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size,\nthe Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8\non WMT'14 English-German; at smaller sizes, it achieves the same quality as the\noriginal \"big\" Transformer with 37.6% less parameters and outperforms the\nTransformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters. \n\n"}
{"id": "cmp-lg/9406030", "contents": "Title: The complexity of normal form rewrite sequences for Associativity Abstract: The complexity of a particular term-rewrite system is considered: the rule of\nassociativity (x*y)*z --> x*(y*z). Algorithms and exact calculations are given\nfor the longest and shortest sequences of applications of --> that result in\nnormal form (NF). The shortest NF sequence for a term x is always n-drm(x),\nwhere n is the number of occurrences of * in x and drm(x) is the depth of the\nrightmost leaf of x. The longest NF sequence for any term is of length\nn(n-1)/2. \n\n"}
{"id": "cmp-lg/9608014", "contents": "Title: Classifiers in Japanese-to-English Machine Translation Abstract: This paper proposes an analysis of classifiers into four major types: UNIT,\nMETRIC, GROUP and SPECIES, based on properties of both Japanese and English.\nThe analysis makes possible a uniform and straightforward treatment of noun\nphrases headed by classifiers in Japanese-to-English machine translation, and\nhas been implemented in the MT system ALT-J/E. Although the analysis is based\non the characteristics of, and differences between, Japanese and English, it is\nshown to be also applicable to the unrelated language Thai. \n\n"}
{"id": "cs/0111007", "contents": "Title: Explaining Scenarios for Information Personalization Abstract: Personalization customizes information access. The PIPE (\"Personalization is\nPartial Evaluation\") modeling methodology represents interaction with an\ninformation space as a program. The program is then specialized to a user's\nknown interests or information seeking activity by the technique of partial\nevaluation. In this paper, we elaborate PIPE by considering requirements\nanalysis in the personalization lifecycle. We investigate the use of scenarios\nas a means of identifying and analyzing personalization requirements. As our\nfirst result, we show how designing a PIPE representation can be cast as a\nsearch within a space of PIPE models, organized along a partial order. This\nallows us to view the design of a personalization system, itself, as\nspecialized interpretation of an information space. We then exploit the\nunderlying equivalence of explanation-based generalization (EBG) and partial\nevaluation to realize high-level goals and needs identified in scenarios; in\nparticular, we specialize (personalize) an information space based on the\nexplanation of a user scenario in that information space, just as EBG\nspecializes a theory based on the explanation of an example in that theory. In\nthis approach, personalization becomes the transformation of information spaces\nto support the explanation of usage scenarios. An example application is\ndescribed. \n\n"}
{"id": "cs/0204020", "contents": "Title: Seven Dimensions of Portability for Language Documentation and\n  Description Abstract: The process of documenting and describing the world's languages is undergoing\nradical transformation with the rapid uptake of new digital technologies for\ncapture, storage, annotation and dissemination. However, uncritical adoption of\nnew tools and technologies is leading to resources that are difficult to reuse\nand which are less portable than the conventional printed resources they\nreplace. We begin by reviewing current uses of software tools and digital\ntechnologies for language documentation and description. This sheds light on\nhow digital language documentation and description are created and managed,\nleading to an analysis of seven portability problems under the following\nheadings: content, format, discovery, access, citation, preservation and\nrights. After characterizing each problem we provide a series of value\nstatements, and this provides the framework for a broad range of best practice\nrecommendations. \n\n"}
{"id": "cs/9810012", "contents": "Title: Ultrametric Distance in Syntax Abstract: Phrase structure trees have a hierarchical structure. In many subjects, most\nnotably in Taxonomy such tree structures have been studied using ultrametrics.\nHere syntactical hierarchical phrase trees are subject to a similar analysis,\nwhich is much siompler as the branching structure is more readily discernible\nand switched. The occurence of hierarchical structure elsewhere in linguistics\nis mentioned. The phrase tree can be represented by a matrix and the elements\nof the matrix can be represented by triangles. The height at which branching\noccurs is not prescribed in previous syntatic models, but it is by using the\nultrametric matrix. The ambiguity of which branching height to choose is\nresolved by postulating that branching occurs at the lowest height available.\nAn ultrametric produces a measure of the complexity of sentences: presumably\nthe complexity of sentence increases as a language is aquired so that this can\nbe tested. A All ultrametric triangles are equilateral or isocles, here it is\nshown that X structur implies that there are no equilateral triangles.\nRestricting attention to simple syntax a minium ultrametric distance between\nlexical categories is calculatex. This ultrametric distance is shown to be\ndifferent than the matrix obtasined from feaures. It is shown that the\ndefinition of c-commabnd can be replaced by an equivalent ultrametric\ndefinition. The new definition invokes a minimum distance between nodes and\nthis is more aesthetically satisfing than previouv varieties of definitions.\n  From the new definition of c-command follows a new definition of government. \n\n"}
{"id": "cs/9811018", "contents": "Title: P-model Alternative to the T-model Abstract: Standard linguistic analysis of syntax uses the T-model. This model requires\nthe ordering: D-structure $>$ S-structure $>$ LF. Between each of these\nrepresentations there is movement which alters the order of the constituent\nwords; movement is achieved using the principles and parameters of syntactic\ntheory. Psychological serial models do not accommodate the T-model immediately\nso that here a new model called the P-model is introduced. Here it is argued\nthat the LF representation should be replaced by a variant of Frege's three\nqualities. In the F-representation the order of elements is not necessarily the\nsame as that in LF and it is suggested that the correct ordering is:\nF-representation $>$ D-structure $>$ S-structure. Within this framework\nmovement originates as the outcome of emphasis applied to the sentence. \n\n"}
{"id": "math/0607507", "contents": "Title: In-Degree and PageRank of Web pages: Why do they follow similar power\n  laws? Abstract: The PageRank is a popularity measure designed by Google to rank Web pages.\nExperiments confirm that the PageRank obeys a `power law' with the same\nexponent as the In-Degree. This paper presents a novel mathematical model that\nexplains this phenomenon. The relation between the PageRank and In-Degree is\nmodelled through a stochastic equation, which is inspired by the original\ndefinition of the PageRank, and is analogous to the well-known distributional\nidentity for the busy period in the M/G/1 queue. Further, we employ the theory\nof regular variation and Tauberian theorems to analytically prove that the tail\nbehavior of the PageRank and the In-Degree differ only by a multiplicative\nfactor, for which we derive a closed-form expression. Our analytical results\nare in good agreement with experimental data. \n\n"}
