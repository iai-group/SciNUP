{"id": "0705.4226", "contents": "Title: Second-Order Type Isomorphisms Through Game Semantics Abstract: The characterization of second-order type isomorphisms is a purely\nsyntactical problem that we propose to study under the enlightenment of game\nsemantics. We study this question in the case of second-order\n&#955;$\\mu$-calculus, which can be seen as an extension of system F to\nclassical logic, and for which we de&#64257;ne a categorical framework: control\nhyperdoctrines. Our game model of &#955;$\\mu$-calculus is based on polymorphic\narenas (closely related to Hughes' hyperforests) which evolve during the play\n(following the ideas of Murawski-Ong). We show that type isomorphisms coincide\nwith the \"equality\" on arenas associated with types. Finally we deduce the\nequational characterization of type isomorphisms from this equality. We also\nrecover from the same model Roberto Di Cosmo's characterization of type\nisomorphisms for system F. This approach leads to a geometrical comprehension\non the question of second order type isomorphisms, which can be easily extended\nto some other polymorphic calculi including additional programming features. \n\n"}
{"id": "0705.4566", "contents": "Title: Loop corrections for message passing algorithms in continuous variable\n  models Abstract: In this paper we derive the equations for Loop Corrected Belief Propagation\non a continuous variable Gaussian model. Using the exactness of the averages\nfor belief propagation for Gaussian models, a different way of obtaining the\ncovariances is found, based on Belief Propagation on cavity graphs. We discuss\nthe relation of this loop correction algorithm to Expectation Propagation\nalgorithms for the case in which the model is no longer Gaussian, but slightly\nperturbed by nonlinear terms. \n\n"}
{"id": "0706.3060", "contents": "Title: N-Body Simulations on GPUs Abstract: Commercial graphics processors (GPUs) have high compute capacity at very low\ncost, which makes them attractive for general purpose scientific computing. In\nthis paper we show how graphics processors can be used for N-body simulations\nto obtain improvements in performance over current generation CPUs. We have\ndeveloped a highly optimized algorithm for performing the O(N^2) force\ncalculations that constitute the major part of stellar and molecular dynamics\nsimulations. In some of the calculations, we achieve sustained performance of\nnearly 100 GFlops on an ATI X1900XTX. The performance on GPUs is comparable to\nspecialized processors such as GRAPE-6A and MDGRAPE-3, but at a fraction of the\ncost. Furthermore, the wide availability of GPUs has significant implications\nfor cluster computing and distributed computing efforts like Folding@Home. \n\n"}
{"id": "0707.2630", "contents": "Title: Multi-physics Extension of OpenFMO Framework Abstract: OpenFMO framework, an open-source software (OSS) platform for Fragment\nMolecular Orbital (FMO) method, is extended to multi-physics simulations (MPS).\nAfter reviewing the several FMO implementations on distributed computer\nenvironments, the subsequent development planning corresponding to MPS is\npresented. It is discussed which should be selected as a scientific software,\nlightweight and reconfigurable form or large and self-contained form. \n\n"}
{"id": "0709.1205", "contents": "Title: Normalisation Control in Deep Inference via Atomic Flows Abstract: We introduce `atomic flows': they are graphs obtained from derivations by\ntracing atom occurrences and forgetting the logical structure. We study simple\nmanipulations of atomic flows that correspond to complex reductions on\nderivations. This allows us to prove, for propositional logic, a new and very\ngeneral normalisation theorem, which contains cut elimination as a special\ncase. We operate in deep inference, which is more general than other syntactic\nparadigms, and where normalisation is more difficult to control. We argue that\natomic flows are a significant technical advance for normalisation theory,\nbecause 1) the technique they support is largely independent of syntax; 2)\nindeed, it is largely independent of logical inference rules; 3) they\nconstitute a powerful geometric formalism, which is more intuitive than syntax. \n\n"}
{"id": "0711.2618", "contents": "Title: A System for Distributed Mechanisms: Design, Implementation and\n  Applications Abstract: We describe here a structured system for distributed mechanism design\nappropriate for both Intranet and Internet applications. In our approach the\nplayers dynamically form a network in which they know neither their neighbours\nnor the size of the network and interact to jointly take decisions. The only\nassumption concerning the underlying communication layer is that for each pair\nof processes there is a path of neighbours connecting them. This allows us to\ndeal with arbitrary network topologies.\n  We also discuss the implementation of this system which consists of a\nsequence of layers. The lower layers deal with the operations that implement\nthe basic primitives of distributed computing, namely low level communication\nand distributed termination, while the upper layers use these primitives to\nimplement high level communication among players, including broadcasting and\nmulticasting, and distributed decision making.\n  This yields a highly flexible distributed system whose specific applications\nare realized as instances of its top layer. This design is implemented in Java.\n  The system supports at various levels fault-tolerance and includes a\nprovision for distributed policing the purpose of which is to exclude\n`dishonest' players. Also, it can be used for repeated creation of dynamically\nformed networks of players interested in a joint decision making implemented by\nmeans of a tax-based mechanism. We illustrate its flexibility by discussing a\nnumber of implemented examples. \n\n"}
{"id": "0802.0865", "contents": "Title: Combining generic judgments with recursive definitions Abstract: Many semantical aspects of programming languages, such as their operational\nsemantics and their type assignment calculi, are specified by describing\nappropriate proof systems. Recent research has identified two proof-theoretic\nfeatures that allow direct, logic-based reasoning about such descriptions: the\ntreatment of atomic judgments as fixed points (recursive definitions) and an\nencoding of binding constructs via generic judgments. However, the logics\nencompassing these two features have thus far treated them orthogonally: that\nis, they do not provide the ability to define object-logic properties that\nthemselves depend on an intrinsic treatment of binding. We propose a new and\nsimple integration of these features within an intuitionistic logic enhanced\nwith induction over natural numbers and we show that the resulting logic is\nconsistent. The pivotal benefit of the integration is that it allows recursive\ndefinitions to not just encode simple, traditional forms of atomic judgments\nbut also to capture generic properties pertaining to such judgments. The\nusefulness of this logic is illustrated by showing how it can provide elegant\ntreatments of object-logic contexts that appear in proofs involving typing\ncalculi and of arbitrarily cascading substitutions that play a role in\nreducibility arguments. \n\n"}
{"id": "0802.3950", "contents": "Title: Belief Propagation and Loop Series on Planar Graphs Abstract: We discuss a generic model of Bayesian inference with binary variables\ndefined on edges of a planar graph. The Loop Calculus approach of [1, 2] is\nused to evaluate the resulting series expansion for the partition function. We\nshow that, for planar graphs, truncating the series at single-connected loops\nreduces, via a map reminiscent of the Fisher transformation [3], to evaluating\nthe partition function of the dimer matching model on an auxiliary planar\ngraph. Thus, the truncated series can be easily re-summed, using the Pfaffian\nformula of Kasteleyn [4]. This allows to identify a big class of\ncomputationally tractable planar models reducible to a dimer model via the\nBelief Propagation (gauge) transformation. The Pfaffian representation can also\nbe extended to the full Loop Series, in which case the expansion becomes a sum\nof Pfaffian contributions, each associated with dimer matchings on an extension\nto a subgraph of the original graph. Algorithmic consequences of the Pfaffian\nrepresentation, as well as relations to quantum and non-planar models, are\ndiscussed. \n\n"}
{"id": "0803.4074", "contents": "Title: Reflective visualization and verbalization of unconscious preference Abstract: A new method is presented, that can help a person become aware of his or her\nunconscious preferences, and convey them to others in the form of verbal\nexplanation. The method combines the concepts of reflection, visualization, and\nverbalization. The method was tested in an experiment where the unconscious\npreferences of the subjects for various artworks were investigated. In the\nexperiment, two lessons were learned. The first is that it helps the subjects\nbecome aware of their unconscious preferences to verbalize weak preferences as\ncompared with strong preferences through discussion over preference diagrams.\nThe second is that it is effective to introduce an adjustable factor into\nvisualization to adapt to the differences in the subjects and to foster their\nmutual understanding. \n\n"}
{"id": "0808.3693", "contents": "Title: Providing Virtual Execution Environments: A Twofold Illustration Abstract: Platform virtualization helps solving major grid computing challenges: share\nresource with flexible, user-controlled and custom execution environments and\nin the meanwhile, isolate failures and malicious code. Grid resource management\ntools will evolve to embrace support for virtual resource.\n  We present two open source projects that transparently supply virtual\nexecution environments. Tycoon has been developed at HP Labs to optimise\nresource usage in creating an economy where users bid to access virtual\nmachines and compete for CPU cycles. SmartDomains provides a peer-to-peer layer\nthat automates virtual machines deployment using a description language and\ndeployment engine from HP Labs. These projects demonstrate both client-server\nand peer-to-peer approaches to virtual resource management. The first case\nmakes extensive use of virtual machines features for dynamic resource\nallocation. The second translates virtual machines capabilities into a\nsophisticated language where resource management components can be plugged in\nconfigurations and architectures defined at deployment time.\n  We propose to share our experience at CERN openlab developing SmartDomains\nand deploying Tycoon to give an illustrative introduction to emerging research\nin virtual resource management. \n\n"}
{"id": "0812.4848", "contents": "Title: The Complexity of Generalized Satisfiability for Linear Temporal Logic Abstract: In a seminal paper from 1985, Sistla and Clarke showed that satisfiability\nfor Linear Temporal Logic (LTL) is either NP-complete or PSPACE-complete,\ndepending on the set of temporal operators used. If, in contrast, the set of\npropositional operators is restricted, the complexity may decrease. This paper\nundertakes a systematic study of satisfiability for LTL formulae over\nrestricted sets of propositional and temporal operators. Since every\npropositional operator corresponds to a Boolean function, there exist\ninfinitely many propositional operators. In order to systematically cover all\npossible sets of them, we use Post's lattice. With its help, we determine the\ncomputational complexity of LTL satisfiability for all combinations of temporal\noperators and all but two classes of propositional functions. Each of these\ninfinitely many problems is shown to be either PSPACE-complete, NP-complete, or\nin P. \n\n"}
{"id": "0901.4664", "contents": "Title: Square root meadows Abstract: Let Q_0 denote the rational numbers expanded to a meadow by totalizing\ninversion such that 0^{-1}=0. Q_0 can be expanded by a total sign function s\nthat extracts the sign of a rational number. In this paper we discuss an\nextension Q_0(s ,\\sqrt) of the signed rationals in which every number has a\nunique square root. \n\n"}
{"id": "0902.3631", "contents": "Title: Distributed Agreement in Tile Self-Assembly Abstract: Laboratory investigations have shown that a formal theory of fault-tolerance\nwill be essential to harness nanoscale self-assembly as a medium of\ncomputation. Several researchers have voiced an intuition that self-assembly\nphenomena are related to the field of distributed computing. This paper\nformalizes some of that intuition. We construct tile assembly systems that are\nable to simulate the solution of the wait-free consensus problem in some\ndistributed systems. (For potential future work, this may allow binding errors\nin tile assembly to be analyzed, and managed, with positive results in\ndistributed computing, as a \"blockage\" in our tile assembly model is analogous\nto a crash failure in a distributed computing model.) We also define a\nstrengthening of the \"traditional\" consensus problem, to make explicit an\nexpectation about consensus algorithms that is often implicit in distributed\ncomputing literature. We show that solution of this strengthened consensus\nproblem can be simulated by a two-dimensional tile assembly model only for two\nprocesses, whereas a three-dimensional tile assembly model can simulate its\nsolution in a distributed system with any number of processes. \n\n"}
{"id": "0903.2914", "contents": "Title: A process calculus with finitary comprehended terms Abstract: We introduce the notion of an ACP process algebra and the notion of a meadow\nenriched ACP process algebra. The former notion originates from the models of\nthe axiom system ACP. The latter notion is a simple generalization of the\nformer notion to processes in which data are involved, the mathematical\nstructure of data being a meadow. Moreover, for all associative operators from\nthe signature of meadow enriched ACP process algebras that are not of an\nauxiliary nature, we introduce variable-binding operators as generalizations.\nThese variable-binding operators, which give rise to comprehended terms, have\nthe property that they can always be eliminated. Thus, we obtain a process\ncalculus whose terms can be interpreted in all meadow enriched ACP process\nalgebras. Use of the variable-binding operators can have a major impact on the\nsize of terms. \n\n"}
{"id": "0904.0125", "contents": "Title: Coherence for rewriting 2-theories Abstract: General coherence theorems are constructed that yield explicit presentations\nof categorical and algebraic objects. The categorical structures involved are\nfinitary discrete Lawvere 2-theories, though they are approached within the\nlanguage of term rewriting theory. Two general coherence theorems are obtained.\nThe first applies to terminating and confluent rewriting 2-theories. This\nresult is exploited to construct systematic presentations for the higher\nThompson groups and the Higman-Thompson groups. The presentations are\ncategorically interesting as they arise from higher-arity analogues of the\nStasheff/Mac Lane coherence axioms, which involve phenomena not present in the\nclassical binary axioms. The second general coherence theorem holds for\n2-theories that are not necessarily confluent or terminating and is used to\nconstruct a new proof of coherence for iterated monoidal categories, which\narise as categorical models of iterated loop spaces and fail to be confluent. \n\n"}
{"id": "0905.2473", "contents": "Title: On the Workings of Genetic Algorithms: The Genoclique Fixing Hypothesis Abstract: We recently reported that the simple genetic algorithm (SGA) is capable of\nperforming a remarkable form of sublinear computation which has a\nstraightforward connection with the general problem of interacting attributes\nin data-mining. In this paper we explain how the SGA can leverage this\ncomputational proficiency to perform efficient adaptation on a broad class of\nfitness functions. Based on the relative ease with which a practical fitness\nfunction might belong to this broad class, we submit a new hypothesis about the\nworkings of genetic algorithms. We explain why our hypothesis is superior to\nthe building block hypothesis, and, by way of empirical validation, we present\nthe results of an experiment in which the use of a simple mechanism called\nclamping dramatically improved the performance of an SGA with uniform crossover\non large, randomly generated instances of the MAX 3-SAT problem. \n\n"}
{"id": "0905.4567", "contents": "Title: Confluence Results for a Quantum Lambda Calculus with Measurements Abstract: A strong confluence result for Q*, a quantum lambda-calculus with\nmeasurements, is proved. More precisely, confluence is shown to hold both for\nfinite and infinite computations. The technique used in the confluence proof is\nsyntactical but innovative. This makes Q* different from similar quantum lambda\ncalculi, which are either measurement-free or provided with a reduction\nstrategy. \n\n"}
{"id": "0906.0065", "contents": "Title: Managing Distributed MARF with SNMP Abstract: The scope of this project's work focuses on the research and prototyping of\nthe extension of the Distributed MARF such that its services can be managed\nthrough the most popular management protocol familiarly, SNMP. The rationale\nbehind SNMP vs. MARF's proprietary management protocols, is that can be\nintegrated with the use of common network service and device management, so the\nadministrators can manage MARF nodes via a already familiar protocol, as well\nas monitor their performance, gather statistics, set desired configuration,\netc. perhaps using the same management tools they've been using for other\nnetwork devices and application servers. \n\n"}
{"id": "0906.4725", "contents": "Title: Interacting Quantum Observables: Categorical Algebra and Diagrammatics Abstract: This paper has two tightly intertwined aims: (i) To introduce an intuitive\nand universal graphical calculus for multi-qubit systems, the ZX-calculus,\nwhich greatly simplifies derivations in the area of quantum computation and\ninformation. (ii) To axiomatise complementarity of quantum observables within a\ngeneral framework for physical theories in terms of dagger symmetric monoidal\ncategories. We also axiomatize phase shifts within this framework.\n  Using the well-studied canonical correspondence between graphical calculi and\nsymmetric monoidal categories, our results provide a purely graphical\nformalisation of complementarity for quantum observables. Each individual\nobservable, represented by a commutative special dagger Frobenius algebra,\ngives rise to an abelian group of phase shifts, which we call the phase group.\nWe also identify a strong form of complementarity, satisfied by the Z and X\nspin observables, which yields a scaled variant of a bialgebra. \n\n"}
{"id": "0908.3889", "contents": "Title: Integrating Post-Newtonian Equations on Graphics Processing Units Abstract: We report on early results of a numerical and statistical study of binary\nblack hole inspirals. The two black holes are evolved using post-Newtonian\napproximations starting with initially randomly distributed spin vectors. We\ncharacterize certain aspects of the distribution shortly before merger. In\nparticular we note the uniform distribution of black hole spin vector dot\nproducts shortly before merger and a high correlation between the initial and\nfinal black hole spin vector dot products in the equal-mass, maximally spinning\ncase. These simulations were performed on Graphics Processing Units, and we\ndemonstrate a speed-up of a factor 50 over a more conventional CPU\nimplementation. \n\n"}
{"id": "0909.2859", "contents": "Title: Electric routing and concurrent flow cutting Abstract: We investigate an oblivious routing scheme, amenable to distributed\ncomputation and resilient to graph changes, based on electrical flow. Our main\ntechnical contribution is a new rounding method which we use to obtain a bound\non the L1->L1 operator norm of the inverse graph Laplacian. We show how this\nnorm reflects both latency and congestion of electric routing. \n\n"}
{"id": "0909.5097", "contents": "Title: On the Scope of the Universal-Algebraic Approach to Constraint\n  Satisfaction Abstract: The universal-algebraic approach has proved a powerful tool in the study of\nthe complexity of CSPs. This approach has previously been applied to the study\nof CSPs with finite or (infinite) omega-categorical templates, and relies on\ntwo facts. The first is that in finite or omega-categorical structures A, a\nrelation is primitive positive definable if and only if it is preserved by the\npolymorphisms of A. The second is that every finite or omega-categorical\nstructure is homomorphically equivalent to a core structure. In this paper, we\npresent generalizations of these facts to infinite structures that are not\nnecessarily omega-categorical. (This abstract has been severely curtailed by\nthe space constraints of arXiv -- please read the full abstract in the\narticle.) Finally, we present applications of our general results to the\ndescription and analysis of the complexity of CSPs. In particular, we give\ngeneral hardness criteria based on the absence of polymorphisms that depend on\nmore than one argument, and we present a polymorphism-based description of\nthose CSPs that are first-order definable (and therefore can be solved in\npolynomial time). \n\n"}
{"id": "0910.2743", "contents": "Title: DILAND: An Algorithm for Distributed Sensor Localization with Noisy\n  Distance Measurements Abstract: In this correspondence, we present an algorithm for distributed sensor\nlocalization with noisy distance measurements (DILAND) that extends and makes\nthe DLRE more robust. DLRE is a distributed sensor localization algorithm in\n$\\mathbb{R}^m$ $(m\\geq1)$ introduced in \\cite{usman_loctsp:08}. DILAND operates\nwhen (i) the communication among the sensors is noisy; (ii) the communication\nlinks in the network may fail with a non-zero probability; and (iii) the\nmeasurements performed to compute distances among the sensors are corrupted\nwith noise. The sensors (which do not know their locations) lie in the convex\nhull of at least $m+1$ anchors (nodes that know their own locations.) Under\nminimal assumptions on the connectivity and triangulation of each sensor in the\nnetwork, this correspondence shows that, under the broad random phenomena\ndescribed above, DILAND converges almost surely (a.s.) to the exact sensor\nlocations. \n\n"}
{"id": "0911.3195", "contents": "Title: Efficient Distributed Random Walks with Applications Abstract: We focus on the problem of performing random walks efficiently in a\ndistributed network. Given bandwidth constraints, the goal is to minimize the\nnumber of rounds required to obtain a random walk sample. We first present a\nfast sublinear time distributed algorithm for performing random walks whose\ntime complexity is sublinear in the length of the walk. Our algorithm performs\na random walk of length $\\ell$ in $\\tilde{O}(\\sqrt{\\ell D})$ rounds (with high\nprobability) on an undirected network, where $D$ is the diameter of the\nnetwork. This improves over the previous best algorithm that ran in\n$\\tilde{O}(\\ell^{2/3}D^{1/3})$ rounds (Das Sarma et al., PODC 2009). We further\nextend our algorithms to efficiently perform $k$ independent random walks in\n$\\tilde{O}(\\sqrt{k\\ell D} + k)$ rounds. We then show that there is a\nfundamental difficulty in improving the dependence on $\\ell$ any further by\nproving a lower bound of $\\Omega(\\sqrt{\\frac{\\ell}{\\log \\ell}} + D)$ under a\ngeneral model of distributed random walk algorithms. Our random walk algorithms\nare useful in speeding up distributed algorithms for a variety of applications\nthat use random walks as a subroutine. We present two main applications. First,\nwe give a fast distributed algorithm for computing a random spanning tree (RST)\nin an arbitrary (undirected) network which runs in $\\tilde{O}(\\sqrt{m}D)$\nrounds (with high probability; here $m$ is the number of edges). Our second\napplication is a fast decentralized algorithm for estimating mixing time and\nrelated parameters of the underlying network. Our algorithm is fully\ndecentralized and can serve as a building block in the design of\ntopologically-aware networks. \n\n"}
{"id": "0912.0419", "contents": "Title: An affine-intuitionistic system of types and effects: confluence and\n  termination Abstract: We present an affine-intuitionistic system of types and effects which can be\nregarded as an extension of Barber-Plotkin Dual Intuitionistic Linear Logic to\nmulti-threaded programs with effects. In the system, dynamically generated\nvalues such as references or channels are abstracted into a finite set of\nregions. We introduce a discipline of region usage that entails the confluence\n(and hence determinacy) of the typable programs. Further, we show that a\ndiscipline of region stratification guarantees termination. \n\n"}
{"id": "0912.0931", "contents": "Title: Orthomodular lattices, Foulis Semigroups and Dagger Kernel Categories Abstract: This paper is a sequel to arXiv:0902.2355 and continues the study of quantum\nlogic via dagger kernel categories. It develops the relation between these\ncategories and both orthomodular lattices and Foulis semigroups. The relation\nbetween the latter two notions has been uncovered in the 1960s. The current\ncategorical perspective gives a broader context and reconstructs this\nrelationship between orthomodular lattices and Foulis semigroups as special\ninstance. \n\n"}
{"id": "0912.2572", "contents": "Title: QR Factorization of Tall and Skinny Matrices in a Grid Computing\n  Environment Abstract: Previous studies have reported that common dense linear algebra operations do\nnot achieve speed up by using multiple geographical sites of a computational\ngrid. Because such operations are the building blocks of most scientific\napplications, conventional supercomputers are still strongly predominant in\nhigh-performance computing and the use of grids for speeding up large-scale\nscientific problems is limited to applications exhibiting parallelism at a\nhigher level. We have identified two performance bottlenecks in the distributed\nmemory algorithms implemented in ScaLAPACK, a state-of-the-art dense linear\nalgebra library. First, because ScaLAPACK assumes a homogeneous communication\nnetwork, the implementations of ScaLAPACK algorithms lack locality in their\ncommunication pattern. Second, the number of messages sent in the ScaLAPACK\nalgorithms is significantly greater than other algorithms that trade flops for\ncommunication. In this paper, we present a new approach for computing a QR\nfactorization -- one of the main dense linear algebra kernels -- of tall and\nskinny matrices in a grid computing environment that overcomes these two\nbottlenecks. Our contribution is to articulate a recently proposed algorithm\n(Communication Avoiding QR) with a topology-aware middleware (QCG-OMPI) in\norder to confine intensive communications (ScaLAPACK calls) within the\ndifferent geographical sites. An experimental study conducted on the Grid'5000\nplatform shows that the resulting performance increases linearly with the\nnumber of geographical sites on large-scale problems (and is in particular\nconsistently higher than ScaLAPACK's). \n\n"}
{"id": "1001.2569", "contents": "Title: Virtual Private Overlays: Secure Group Commounication in NAT-Constrained\n  Environments Abstract: Structured P2P overlays provide a framework for building distributed\napplications that are self-configuring, scalable, and resilient to node\nfailures. Such systems have been successfully adopted in large-scale Internet\nservices such as content delivery networks and file sharing; however,\nwidespread adoption in small/medium scales has been limited due in part to\nsecurity concerns and difficulty bootstrapping in NAT-constrained environments.\nNonetheless, P2P systems can be designed to provide guaranteed lookup times,\nNAT traversal, point-to-point overlay security, and distributed data stores. In\nthis paper we propose a novel way of creating overlays that are both secure and\nprivate and a method to bootstrap them using a public overlay. Private overlay\nnodes use the public overlay's distributed data store to discover each other,\nand the public overlay's connections to assist with NAT hole punching and as\nrelays providing STUN and TURN NAT traversal techniques. The security framework\nutilizes groups, which are created and managed by users through a web based\nuser interface. Each group acts as a Public Key Infrastructure (PKI) relying on\nthe use of a centrally-managed web site providing an automated Certificate\nAuthority (CA). We present a reference implementation which has been used in a\nP2P VPN (Virtual Private Network). To evaluate our contributions, we apply our\ntechniques to an overlay network modeler, event-driven simulations using\nsimulated time delays, and deployment in the PlanetLab wide-area testbed. \n\n"}
{"id": "1003.5831", "contents": "Title: The Computable Universe Hypothesis Abstract: When can a model of a physical system be regarded as computable? We provide\nthe definition of a computable physical model to answer this question. The\nconnection between our definition and Kreisel's notion of a mechanistic theory\nis discussed, and several examples of computable physical models are given,\nincluding models which feature discrete motion, a model which features\nnon-discrete continuous motion, and probabilistic models such as radioactive\ndecay. We show how computable physical models on effective topological spaces\ncan be formulated using the theory of type-two effectivity (TTE). Various\ncommon operations on computable physical models are described, such as the\noperation of coarse-graining and the formation of statistical ensembles. The\ndefinition of a computable physical model also allows for a precise\nformalization of the computable universe hypothesis--the claim that all the\nlaws of physics are computable. \n\n"}
{"id": "1005.5662", "contents": "Title: On the contribution of backward jumps to instruction sequence\n  expressiveness Abstract: We investigate the expressiveness of backward jumps in a framework of\nformalized sequential programming called program algebra. We show that - if\nexpressiveness is measured in terms of the computability of partial Boolean\nfunctions - then backward jumps are superfluous. If we, however, want to\nprevent explosion of the length of programs, then backward jumps are essential. \n\n"}
{"id": "1007.1324", "contents": "Title: Separating the basic logics of the basic recurrences Abstract: This paper shows that, even at the most basic level, the parallel, countable\nbranching and uncountable branching recurrences of Computability Logic (see\nhttp://www.cis.upenn.edu/~giorgi/cl.html) validate different principles. \n\n"}
{"id": "1007.3303", "contents": "Title: A New Approach to Abstract Machines - Introduction to the Theory of\n  Configuration Machines Abstract: An abstract machine is a theoretical model designed to perform a rigorous\nstudy of computation. Such a model usually consists of configurations,\ninstructions, programs, inputs and outputs for the machine. In this paper we\nformalize these notions as a very simple algebraic system, called a\nconfiguration machine. If an abstract machine is defined as a configuration\nmachine consisting of primitive recursive functions then the functions computed\nby the machine are always recursive. The theory of configuration machines\nprovides a useful tool to study universal machines. \n\n"}
{"id": "1008.0135", "contents": "Title: Interactive Visualization of the Largest Radioastronomy Cubes Abstract: 3D visualization is an important data analysis and knowledge discovery tool,\nhowever, interactive visualization of large 3D astronomical datasets poses a\nchallenge for many existing data visualization packages. We present a solution\nto interactively visualize larger-than-memory 3D astronomical data cubes by\nutilizing a heterogeneous cluster of CPUs and GPUs. The system partitions the\ndata volume into smaller sub-volumes that are distributed over the rendering\nworkstations. A GPU-based ray casting volume rendering is performed to generate\nimages for each sub-volume, which are composited to generate the whole volume\noutput, and returned to the user. Datasets including the HI Parkes All Sky\nSurvey (HIPASS - 12 GB) southern sky and the Galactic All Sky Survey (GASS - 26\nGB) data cubes were used to demonstrate our framework's performance. The\nframework can render the GASS data cube with a maximum render time < 0.3 second\nwith 1024 x 1024 pixels output resolution using 3 rendering workstations and 8\nGPUs. Our framework will scale to visualize larger datasets, even of Terabyte\norder, if proper hardware infrastructure is available. \n\n"}
{"id": "1009.1341", "contents": "Title: Component Specification in the Cactus Framework: The Cactus\n  Configuration Language Abstract: Component frameworks are complex systems that rely on many layers of\nabstraction to function properly. One essential requirement is a consistent\nmeans of describing each individual component and how it relates to both other\ncomponents and the whole framework. As component frameworks are designed to be\nflexible by nature, the description method should be simultaneously powerful,\nlead to efficient code, and be easy to use, so that new users can quickly adapt\ntheir own code to work with the framework. In this paper, we discuss the Cactus\nConfiguration Language (CCL) which is used to describe components (\"thorns'')\nin the Cactus Framework. The CCL provides a description language for the\nvariables, parameters, functions, scheduling and compilation of a component and\nincludes concepts such as interface and implementation which allow thorns\nproviding the same capabilities to be easily interchanged. We include several\napplication examples which illustrate how community toolkits use the CCL and\nCactus and identify needed additions to the language. \n\n"}
{"id": "1009.3291", "contents": "Title: Rebuilding for Array Codes in Distributed Storage Systems Abstract: In distributed storage systems that use coding, the issue of minimizing the\ncommunication required to rebuild a storage node after a failure arises. We\nconsider the problem of repairing an erased node in a distributed storage\nsystem that uses an EVENODD code. EVENODD codes are maximum distance separable\n(MDS) array codes that are used to protect against erasures, and only require\nXOR operations for encoding and decoding. We show that when there are two\nredundancy nodes, to rebuild one erased systematic node, only 3/4 of the\ninformation needs to be transmitted. Interestingly, in many cases, the required\ndisk I/O is also minimized. \n\n"}
{"id": "1009.5028", "contents": "Title: What is a space? Computations in emergent algebras and the front end\n  visual system Abstract: With the help of link diagrams with decorated crossings, I explain\ncomputations in emergent algebras, introduced in arXiv:0907.1520, as the kind\nof computations done in the front end visual system. \n\n"}
{"id": "1010.0485", "contents": "Title: Distributed Storage Codes Meet Multiple-Access Wiretap Channels Abstract: We consider {\\it i)} the overhead minimization of maximum-distance separable\n(MDS) storage codes for the repair of a single failed node and {\\it ii)} the\ntotal secure degrees-of-freedom (S-DoF) maximization in a multiple-access\ncompound wiretap channel. We show that the two problems are connected.\nSpecifically, the overhead minimization for a single node failure of an {\\it\noptimal} MDS code, i.e. one that can achieve the information theoretic overhead\nminimum, is equivalent to maximizing the S-DoF in a multiple-access compound\nwiretap channel. Additionally, we show that maximizing the S-DoF in a\nmultiple-access compound wiretap channel is equivalent to minimizing the\noverhead of an MDS code for the repair of a departed node. An optimal MDS code\nmaps to a full S-DoF channel and a full S-DoF channel maps to an MDS code with\nminimum repair overhead for one failed node. We also state a general framework\nfor code-to-channel and channel-to-code mappings and performance bounds between\nthe two settings. The underlying theme for all connections presented is\ninterference alignment (IA). The connections between the two problems become\napparent when we restate IA as an optimization problem. Specifically, we\nformulate the overhead minimization and the S-DoF maximization as rank\nconstrained, sum-rank and max-rank minimization problems respectively. The\nderived connections allow us to map repair strategies of recently discovered\nrepair codes to beamforming matrices and characterize the maximum S-DoF for the\nsingle antenna multiple-access compound wiretap channel. \n\n"}
{"id": "1011.4330", "contents": "Title: Uniform Memory and Serialization for Lambda Calculus Abstract: This paper introduces a special type of systems, defines their properties,\nand then demonstrates that a reduction machine for pure untyped extensional\nlambda calculus can be implemented as a system of the introduced type.\nSpecifically, we discuss uniform memory as a special kind of graphs and real\ntime operation of state machines that use the uniform memory as their state.\nAlso, we consider a special case of serialization, the latter being useful for\nthe mechanism that compares results during computation, not after the\ncomputation is done. However, we start with detailed explanation of our\nmotivation for this work. \n\n"}
{"id": "1012.0729", "contents": "Title: Agnostic Learning of Monomials by Halfspaces is Hard Abstract: We prove the following strong hardness result for learning: Given a\ndistribution of labeled examples from the hypercube such that there exists a\nmonomial consistent with $(1-\\eps)$ of the examples, it is NP-hard to find a\nhalfspace that is correct on $(1/2+\\eps)$ of the examples, for arbitrary\nconstants $\\eps > 0$. In learning theory terms, weak agnostic learning of\nmonomials is hard, even if one is allowed to output a hypothesis from the much\nbigger concept class of halfspaces. This hardness result subsumes a long line\nof previous results, including two recent hardness results for the proper\nlearning of monomials and halfspaces. As an immediate corollary of our result\nwe show that weak agnostic learning of decision lists is NP-hard.\n  Our techniques are quite different from previous hardness proofs for\nlearning. We define distributions on positive and negative examples for\nmonomials whose first few moments match. We use the invariance principle to\nargue that regular halfspaces (all of whose coefficients have small absolute\nvalue relative to the total $\\ell_2$ norm) cannot distinguish between\ndistributions whose first few moments match. For highly non-regular subspaces,\nwe use a structural lemma from recent work on fooling halfspaces to argue that\nthey are ``junta-like'' and one can zero out all but the top few coefficients\nwithout affecting the performance of the halfspace. The top few coefficients\nform the natural list decoding of a halfspace in the context of dictatorship\ntests/Label Cover reductions.\n  We note that unlike previous invariance principle based proofs which are only\nknown to give Unique-Games hardness, we are able to reduce from a version of\nLabel Cover problem that is known to be NP-hard. This has inspired follow-up\nwork on bypassing the Unique Games conjecture in some optimal geometric\ninapproximability results. \n\n"}
{"id": "1101.0309", "contents": "Title: Concrete Sentence Spaces for Compositional Distributional Models of\n  Meaning Abstract: Coecke, Sadrzadeh, and Clark (arXiv:1003.4394v1 [cs.CL]) developed a\ncompositional model of meaning for distributional semantics, in which each word\nin a sentence has a meaning vector and the distributional meaning of the\nsentence is a function of the tensor products of the word vectors. Abstractly\nspeaking, this function is the morphism corresponding to the grammatical\nstructure of the sentence in the category of finite dimensional vector spaces.\nIn this paper, we provide a concrete method for implementing this linear\nmeaning map, by constructing a corpus-based vector space for the type of\nsentence. Our construction method is based on structured vector spaces whereby\nmeaning vectors of all sentences, regardless of their grammatical structure,\nlive in the same vector space. Our proposed sentence space is the tensor\nproduct of two noun spaces, in which the basis vectors are pairs of words each\naugmented with a grammatical role. This enables us to compare meanings of\nsentences by simply taking the inner product of their vectors. \n\n"}
{"id": "1101.3417", "contents": "Title: Categorical Abstract Rewriting Systems and Functoriality of Graph\n  Transformation Abstract: Rewriting systems are often defined as binary relations over a given set of\nobjects. This simple definition is used to describe various properties of\nrewriting such as termination, confluence, normal forms etc. In this paper, we\nintroduce a new notion of abstract rewriting in the framework of categories.\nThen, we define the functoriality property of rewriting systems. This property\nis sometimes called vertical composition. We show that most of graph\ntransformation systems are functorial and provide a counter-example of graph\ntransformation systems which is not functorial. \n\n"}
{"id": "1104.5243", "contents": "Title: Pushing the limits for medical image reconstruction on recent standard\n  multicore processors Abstract: Volume reconstruction by backprojection is the computational bottleneck in\nmany interventional clinical computed tomography (CT) applications. Today\nvendors in this field replace special purpose hardware accelerators by standard\nhardware like multicore chips and GPGPUs. Medical imaging algorithms are on the\nverge of employing High Performance Computing (HPC) technology, and are\ntherefore an interesting new candidate for optimization. This paper presents\nlow-level optimizations for the backprojection algorithm, guided by a thorough\nperformance analysis on four generations of Intel multicore processors\n(Harpertown, Westmere, Westmere EX, and Sandy Bridge).\n  We choose the RabbitCT benchmark, a standardized testcase well supported in\nindustry, to ensure transparent and comparable results. Our aim is to provide\nnot only the fastest possible implementation but also compare to performance\nmodels and hardware counter data in order to fully understand the results. We\nseparate the influence of algorithmic optimizations, parallelization, SIMD\nvectorization, and microarchitectural issues and pinpoint problems with current\nSIMD instruction set extensions on standard CPUs (SSE, AVX). The use of\nassembly language is mandatory for best performance. Finally we compare our\nresults to the best GPGPU implementations available for this open competition\nbenchmark. \n\n"}
{"id": "1106.1652", "contents": "Title: Distributed Storage Codes through Hadamard Designs Abstract: In distributed storage systems that employ erasure coding, the issue of\nminimizing the total {\\it repair bandwidth} required to exactly regenerate a\nstorage node after a failure arises. This repair bandwidth depends on the\nstructure of the storage code and the repair strategies used to restore the\nlost data. Minimizing it requires that undesired data during a repair align in\nthe smallest possible spaces, using the concept of interference alignment (IA).\nHere, a points-on-a-lattice representation of the symbol extension IA of\nCadambe {\\it et al.} provides cues to perfect IA instances which we combine\nwith fundamental properties of Hadamard matrices to construct a new storage\ncode with favorable repair properties. Specifically, we build an explicit\n$(k+2,k)$ storage code over $\\mathbb{GF}(3)$, whose single systematic node\nfailures can be repaired with bandwidth that matches exactly the theoretical\nminimum. Moreover, the repair of single parity node failures generates at most\nthe same repair bandwidth as any systematic node failure. Our code can tolerate\nany single node failure and any pair of failures that involves at most one\nsystematic failure. \n\n"}
{"id": "1106.2275", "contents": "Title: Byzantine Fault Tolerance of Regenerating Codes Abstract: Recent years have witnessed a slew of coding techniques custom designed for\nnetworked storage systems. Network coding inspired regenerating codes are the\nmost prolifically studied among these new age storage centric codes. A lot of\neffort has been invested in understanding the fundamental achievable trade-offs\nof storage and bandwidth usage to maintain redundancy in presence of different\nmodels of failures, showcasing the efficacy of regenerating codes with respect\nto traditional erasure coding techniques. For practical usability in open and\nadversarial environments, as is typical in peer-to-peer systems, we need\nhowever not only resilience against erasures, but also from (adversarial)\nerrors. In this paper, we study the resilience of generalized regenerating\ncodes (supporting multi-repairs, using collaboration among newcomers) in the\npresence of two classes of Byzantine nodes, relatively benign selfish\n(non-cooperating) nodes, as well as under more active, malicious polluting\nnodes. We give upper bounds on the resilience capacity of regenerating codes,\nand show that the advantages of collaborative repair can turn to be detrimental\nin the presence of Byzantine nodes. We further exhibit that system mechanisms\ncan be combined with regenerating codes to mitigate the effect of rogue nodes. \n\n"}
{"id": "1107.3129", "contents": "Title: Homomorphic Self-repairing Codes for Agile Maintenance of Distributed\n  Storage Systems Abstract: Distributed data storage systems are essential to deal with the need to store\nmassive volumes of data. In order to make such a system fault-tolerant, some\nform of redundancy becomes crucial, incurring various overheads - most\nprominently in terms of storage space and maintenance bandwidth requirements.\nErasure codes, originally designed for communication over lossy channels,\nprovide a storage efficient alternative to replication based redundancy,\nhowever entailing high communication overhead for maintenance, when some of the\nencoded fragments need to be replenished in news ones after failure of some\nstorage devices. We propose as an alternative a new family of erasure codes\ncalled self-repairing codes (SRC) taking into account the peculiarities of\ndistributed storage systems, specifically the maintenance process. SRC has the\nfollowing salient features: (a) encoded fragments can be repaired directly from\nother subsets of encoded fragments by downloading less data than the size of\nthe complete object, ensuring that (b) a fragment is repaired from a fixed\nnumber of encoded fragments, the number depending only on how many encoded\nblocks are missing and independent of which specific blocks are missing. This\npaper lays the foundations by defining the novel self-repairing codes,\nelaborating why the defined characteristics are desirable for distributed\nstorage systems. Then homomorphic self-repairing codes (HSRC) are proposed as a\nconcrete instance, whose various aspects and properties are studied and\ncompared - quantitatively or qualitatively with respect to other codes\nincluding traditional erasure codes as well as other recent codes designed\nspecifically for storage applications. \n\n"}
{"id": "1108.5893", "contents": "Title: Edge-preserving self-healing: keeping network backbones densely\n  connected Abstract: Healing algorithms play a crucial part in distributed P2P networks where\nfailures occur continuously and frequently. Several self-healing algorithms\nhave been suggested recently [IPDPS'08, PODC'08, PODC'09, PODC'11] in a line of\nwork that has yielded gradual improvements in the properties ensured on the\ngraph. This work motivates a strong general phenomenon of edge-preserving\nhealing that aims at obtaining self-healing algorithms with the constraint that\nall original edges in the graph (not deleted by the adversary), be retained in\nevery intermediate graph.\n  The previous algorithms, in their nascent form, are not explicitly edge\npreserving. In this paper, we show they can be suitably modified (We introduce\nXheal+, an edge-preserving version of Xheal[PODC'11]). Towards this end, we\npresent a general self-healing model that unifies the previous models. The main\ncontribution of this paper is not in the technical complexity, rather in the\nsimplicity with which the edge-preserving property can be ensured and the\nmessage that this is a crucial property with several benefits. In particular,\nwe highlight this by showing that, almost as an immediate corollary, subgraph\ndensities are preserved or increased. Maintaining density is a notion motivated\nby the fact that in certain distributed networks, certain nodes may require and\ninitially have a larger number of inter-connections. It is vital that a healing\nalgorithm, even amidst failures, respect these requirements. Our suggested\nmodifications yield such subgraph density preservation as a by product. In\naddition, edge preservation helps maintain any subgraph induced property that\nis monotonic. Also, algorithms that are edge-preserving require minimal\nalteration of edges which can be an expensive cost in healing - something that\nhas not been modeled in any of the past work. \n\n"}
{"id": "1109.1363", "contents": "Title: Modelling Spatial Interactions in the Arbuscular Mycorrhizal Symbiosis\n  using the Calculus of Wrapped Compartments Abstract: Arbuscular mycorrhiza (AM) is the most wide-spread plant-fungus symbiosis on\nearth. Investigating this kind of symbiosis is considered one of the most\npromising ways to develop methods to nurture plants in more natural manners,\navoiding the complex chemical productions used nowadays to produce artificial\nfertilizers. In previous work we used the Calculus of Wrapped Compartments\n(CWC) to investigate different phases of the AM symbiosis. In this paper, we\ncontinue this line of research by modelling the colonisation of the plant root\ncells by the fungal hyphae spreading in the soil. This study requires the\ndescription of some spatial interaction. Although CWC has no explicit feature\nmodelling a spatial geometry, the compartment labelling feature can be\neffectively exploited to define a discrete surface topology outlining the\nrelevant sectors which determine the spatial properties of the system under\nconsideration. Different situations and interesting spatial properties can be\nmodelled and analysed in such a lightweight framework (which has not an\nexplicit notion of geometry with coordinates and spatial metrics), thus\nexploiting the existing CWC simulation tool. \n\n"}
{"id": "1109.1368", "contents": "Title: Multiple verification in computational modeling of bone pathologies Abstract: We introduce a model checking approach to diagnose the emerging of bone\npathologies. The implementation of a new model of bone remodeling in PRISM has\nled to an interesting characterization of osteoporosis as a defective bone\nremodeling dynamics with respect to other bone pathologies. Our approach allows\nto derive three types of model checking-based diagnostic estimators. The first\ndiagnostic measure focuses on the level of bone mineral density, which is\ncurrently used in medical practice. In addition, we have introduced a novel\ndiagnostic estimator which uses the full patient clinical record, here\nsimulated using the modeling framework. This estimator detects rapid (months)\nnegative changes in bone mineral density. Independently of the actual bone\nmineral density, when the decrease occurs rapidly it is important to alarm the\npatient and monitor him/her more closely to detect insurgence of other bone\nco-morbidities. A third estimator takes into account the variance of the bone\ndensity, which could address the investigation of metabolic syndromes, diabetes\nand cancer. Our implementation could make use of different logical combinations\nof these statistical estimators and could incorporate other biomarkers for\nother systemic co-morbidities (for example diabetes and thalassemia). We are\ndelighted to report that the combination of stochastic modeling with formal\nmethods motivate new diagnostic framework for complex pathologies. In\nparticular our approach takes into consideration important properties of\nbiosystems such as multiscale and self-adaptiveness. The multi-diagnosis could\nbe further expanded, inching towards the complexity of human diseases. Finally,\nwe briefly introduce self-adaptiveness in formal methods which is a key\nproperty in the regulative mechanisms of biological systems and well known in\nother mathematical and engineering areas. \n\n"}
{"id": "1109.2317", "contents": "Title: An Overview of Codes Tailor-made for Better Repairability in Networked\n  Distributed Storage Systems Abstract: The continuously increasing amount of digital data generated by today's\nsociety asks for better storage solutions. This survey looks at a new\ngeneration of coding techniques designed specifically for the needs of\ndistributed networked storage systems, trying to reach the best compromise\namong storage space efficiency, fault tolerance, and maintenance overheads.\nFour families of codes tailor-made for distributed settings, namely - pyramid,\nhierarchical, regenerating and self-repairing codes - are presented at a high\nlevel, emphasizing the main ideas behind each of these codes, and discussing\ntheir pros and cons, before concluding with a quantitative comparison among\nthem. This survey deliberately excluded technical details for the codes, nor\ndoes it provide an exhaustive summary of the numerous works. Instead, it\nprovides an overview of the major code families in a manner easily accessible\nto a broad audience, by presenting the big picture of advances in coding\ntechniques for distributed storage solutions. \n\n"}
{"id": "1109.5036", "contents": "Title: Testing first-order properties for subclasses of sparse graphs Abstract: We present a linear-time algorithm for deciding first-order (FO) properties\nin classes of graphs with bounded expansion, a notion recently introduced by\nNesetril and Ossona de Mendez. This generalizes several results from the\nliterature, because many natural classes of graphs have bounded expansion:\ngraphs of bounded tree-width, all proper minor-closed classes of graphs, graphs\nof bounded degree, graphs with no subgraph isomorphic to a subdivision of a\nfixed graph, and graphs that can be drawn in a fixed surface in such a way that\neach edge crosses at most a constant number of other edges. We deduce that\nthere is an almost linear-time algorithm for deciding FO properties in classes\nof graphs with locally bounded expansion.\n  More generally, we design a dynamic data structure for graphs belonging to a\nfixed class of graphs of bounded expansion. After a linear-time initialization\nthe data structure allows us to test an FO property in constant time, and the\ndata structure can be updated in constant time after addition/deletion of an\nedge, provided the list of possible edges to be added is known in advance and\ntheir simultaneous addition results in a graph in the class. All our results\nalso hold for relational structures and are based on the seminal result of\nNesetril and Ossona de Mendez on the existence of low tree-depth colorings. \n\n"}
{"id": "1110.2613", "contents": "Title: Trichromatic Open Digraphs for Understanding Qubits Abstract: We introduce a trichromatic graphical calculus for quantum computing. The\ngenerators represent three complementary observables that are treated on equal\nfooting, hence reflecting the symmetries of the Bloch sphere. We derive the\nEuler angle decomposition of the Hadamard gate within it as well as the\nso-called supplementary relationships, which are valid equations for qubits\nthat were not derivable within Z/X-calculus of Coecke and Duncan. More\nspecifically, we have: dichromatic Z/X-calculus + Euler angle decomposition of\nthe Hadamard gate = trichromatic calculus. \n\n"}
{"id": "1112.6128", "contents": "Title: Holographic Grid Cloud, a futurable high storage technology for the next\n  generation astronomical facilities Abstract: In the immediate future holographic technology will be available to store a\nvery large amount of data in HVD (Holographic Versatile Disk) devices. This\ntechnology make extensive use of the WORM (Write-Once-Read-Many) paradigm: this\nmeans that such devices allow for a simultaneous and parallel reading of\nmillions of volumetric pixels (i.e. voxels). This characteristic will make\naccessible wherever the acquired data from a telescope (or satellite) in a\nquite-simultaneous way.\n  With the support of this new technology the aim of this paper is to identify\nthe guidelines for the implementation of a distributed RAID system, a sort of\n\"storage block\" to distribute astronomical data over different geographical\nsites acting as a single remote device as an effect of a property of\ndistributed computing, the abstraction of resources. The end user will only\nhave to take care on connecting in a opportune and secure mode (using personal\ncertificates) to the remote device and will have access to all (or part) of\nthis potential technology.\n  A Storage-Block+Services engineered on such a platform will allow rapid\nscalability of resources, creating a \"network-distributed cloud\" of services\nfor an instrument or a mission. It is recommended the use of a dedicated\ngrid-infrastructure within each single cloud to enhance some critical tasks and\nto speed-up services working on the redundant, encrypted and compressed\nscientific data. The power, the accessibility, the degree of parallelism and of\nredundancy will only depend on the number of distributed storage-blocks: the\nhigher this amount, the greater will be throughput of the IT-system. A\nstorage-block of this kind is a meeting point between two technologies and two\nantithetical computing paradigms: the Grid-Computing and Cloud-Computing. \n\n"}
{"id": "1202.0457", "contents": "Title: Exact Scalar Minimum Storage Coordinated Regenerating Codes Abstract: We study the exact and optimal repair of multiple failures in codes for\ndistributed storage. More particularly, we examine the use of interference\nalignment to build exact scalar minimum storage coordinated regenerating codes\n(MSCR). We show that it is possible to build codes for the case of k = 2 and d\n> k by aligning interferences independently but that this technique cannot be\napplied as soon as k > 2 and d > k. Our results also apply to adaptive\nregenerating codes. \n\n"}
{"id": "1202.1056", "contents": "Title: Building a Framework for Predictive Science Abstract: Key questions that scientists and engineers typically want to address can be\nformulated in terms of predictive science. Questions such as: \"How well does my\ncomputational model represent reality?\", \"What are the most important\nparameters in the problem?\", and \"What is the best next experiment to perform?\"\nare fundamental in solving scientific problems. Mystic is a framework for\nmassively-parallel optimization and rigorous sensitivity analysis that enables\nthese motivating questions to be addressed quantitatively as global\noptimization problems. Often realistic physics, engineering, and materials\nmodels may have hundreds of input parameters, hundreds of constraints, and may\nrequire execution times of seconds or longer. In more extreme cases, realistic\nmodels may be multi-scale, and require the use of high-performance computing\nclusters for their evaluation. Predictive calculations, formulated as a global\noptimization over a potential surface in design parameter space, may require an\nalready prohibitively large simulation to be performed hundreds, if not\nthousands, of times. The need to prepare, schedule, and monitor thousands of\nmodel evaluations, and dynamically explore and analyze results, is a\nchallenging problem that requires a software infrastructure capable of\ndistributing and managing computations on large-scale heterogeneous resources.\nIn this paper, we present the design behind an optimization framework, and also\na framework for heterogeneous computing, that when utilized together, can make\ncomputationally intractable sensitivity and optimization problems much more\ntractable. \n\n"}
{"id": "1202.2167", "contents": "Title: Abstract Representations and Frequent Pattern Discovery Abstract: We discuss the frequent pattern mining problem in a general setting. From an\nanalysis of abstract representations, summarization and frequent pattern\nmining, we arrive at a generalization of the problem. Then, we show how the\nproblem can be cast into the powerful language of algorithmic information\ntheory. This allows us to formulate a simple algorithm to mine for all frequent\npatterns. \n\n"}
{"id": "1202.2293", "contents": "Title: Remarks on Category-Based Routing in Social Networks Abstract: It is well known that individuals can route messages on short paths through\nsocial networks, given only simple information about the target and using only\nlocal knowledge about the topology. Sociologists conjecture that people find\nroutes greedily by passing the message to an acquaintance that has more in\ncommon with the target than themselves, e.g. if a dentist in Saarbr\\\"ucken\nwants to send a message to a specific lawyer in Munich, he may forward it to\nsomeone who is a lawyer and/or lives in Munich. Modelling this setting,\nEppstein et al. introduced the notion of category-based routing. The goal is to\nassign a set of categories to each node of a graph such that greedy routing is\npossible. By proving bounds on the number of categories a node has to be in we\ncan argue about the plausibility of the underlying sociological model. In this\npaper we substantially improve the upper bounds introduced by Eppstein et al.\nand prove new lower bounds. \n\n"}
{"id": "1202.4190", "contents": "Title: Generalized FMD Detection for Spectrum Sensing Under Low Signal-to-Noise\n  Ratio Abstract: Spectrum sensing is a fundamental problem in cognitive radio. We propose a\nfunction of covariance matrix based detection algorithm for spectrum sensing in\ncognitive radio network. Monotonically increasing property of function of\nmatrix involving trace operation is utilized as the cornerstone for this\nalgorithm. The advantage of proposed algorithm is it works under extremely low\nsignal-to-noise ratio, like lower than -30 dB with limited sample data.\nTheoretical analysis of threshold setting for the algorithm is discussed. A\nperformance comparison between the proposed algorithm and other\nstate-of-the-art methods is provided, by the simulation on captured digital\ntelevision (DTV) signal. \n\n"}
{"id": "1202.6094", "contents": "Title: Iterative Approximate Byzantine Consensus in Arbitrary Directed Graphs -\n  Part II: Synchronous and Asynchronous Systems Abstract: This report contains two related sets of results with different assumptions\non synchrony. The first part is about iterative algorithms in synchronous\nsystems. Following our previous work on synchronous iterative approximate\nByzantine consensus (IABC) algorithms, we provide a more intuitive tight\nnecessary and sufficient condition for the existence of such algorithms in\nsynchronous networks1. We believe this condition and the previous results also\nhold in partially asynchronous algorithmic model.\n  In the second part of the report, we explore the problem in asynchronous\nnetworks. While the traditional Byzantine consensus is not solvable in\nasynchronous systems, approximate Byzantine consensus can be solved using\niterative algorithms. \n\n"}
{"id": "1203.1505", "contents": "Title: Performance of a Distributed Stochastic Approximation Algorithm Abstract: In this paper, a distributed stochastic approximation algorithm is studied.\nApplications of such algorithms include decentralized estimation, optimization,\ncontrol or computing. The algorithm consists in two steps: a local step, where\neach node in a network updates a local estimate using a stochastic\napproximation algorithm with decreasing step size, and a gossip step, where a\nnode computes a local weighted average between its estimates and those of its\nneighbors. Convergence of the estimates toward a consensus is established under\nweak assumptions. The approach relies on two main ingredients: the existence of\na Lyapunov function for the mean field in the agreement subspace, and a\ncontraction property of the random matrices of weights in the subspace\northogonal to the agreement subspace. A second order analysis of the algorithm\nis also performed under the form of a Central Limit Theorem. The\nPolyak-averaged version of the algorithm is also considered. \n\n"}
{"id": "1203.4938", "contents": "Title: Advanced Programming Platform for efficient use of Data Parallel\n  Hardware Abstract: Graphics processing units (GPU) had evolved from a specialized hardware\ncapable to render high quality graphics in games to a commodity hardware for\neffective processing blocks of data in a parallel schema. This evolution is\nparticularly interesting for scientific groups, which traditionally use mainly\nCPU as a work horse, and now can profit of the arrival of GPU hardware to HPC\nclusters. This new GPU hardware promises a boost in peak performance, but it is\nnot trivial to use. In this article a programming platform designed to promote\na direct use of this specialized hardware is presented. This platform includes\na visual editor of parallel data flows and it is oriented to the execution in\ndistributed clusters with GPUs. Examples of application in two characteristic\nproblems, Fast Fourier Transform and Image Compression, are also shown. \n\n"}
{"id": "1204.0347", "contents": "Title: The lambda-mu-T-calculus Abstract: Calculi with control operators have been studied as extensions of simple type\ntheory. Real programming languages contain datatypes, so to really understand\ncontrol operators, one should also include these in the calculus. As a first\nstep in that direction, we introduce lambda-mu-T, a combination of Parigot's\nlambda-mu-calculus and G\\\"odel's T, to extend a calculus with control operators\nwith a datatype of natural numbers with a primitive recursor.\n  We consider the problem of confluence on raw terms, and that of strong\nnormalization for the well-typed terms. Observing some problems with extending\nthe proofs of Baba at al. and Parigot's original confluence proof, we provide\nnew, and improved, proofs of confluence (by complete developments) and strong\nnormalization (by reducibility and a postponement argument) for our system.\n  We conclude with some remarks about extensions, choices, and prospects for an\nimproved presentation. \n\n"}
{"id": "1204.1373", "contents": "Title: Spectra: Robust Estimation of Distribution Functions in Networks Abstract: Distributed aggregation allows the derivation of a given global aggregate\nproperty from many individual local values in nodes of an interconnected\nnetwork system. Simple aggregates such as minima/maxima, counts, sums and\naverages have been thoroughly studied in the past and are important tools for\ndistributed algorithms and network coordination. Nonetheless, this kind of\naggregates may not be comprehensive enough to characterize biased data\ndistributions or when in presence of outliers, making the case for richer\nestimates of the values on the network. This work presents Spectra, a\ndistributed algorithm for the estimation of distribution functions over large\nscale networks. The estimate is available at all nodes and the technique\ndepicts important properties, namely: robust when exposed to high levels of\nmessage loss, fast convergence speed and fine precision in the estimate. It can\nalso dynamically cope with changes of the sampled local property, not requiring\nalgorithm restarts, and is highly resilient to node churn. The proposed\napproach is experimentally evaluated and contrasted to a competing state of the\nart distribution aggregation technique. \n\n"}
{"id": "1204.2248", "contents": "Title: Robust Spatio-Temporal Signal Recovery from Noisy Counts in Social Media Abstract: Many real-world phenomena can be represented by a spatio-temporal signal:\nwhere, when, and how much. Social media is a tantalizing data source for those\nwho wish to monitor such signals. Unlike most prior work, we assume that the\ntarget phenomenon is known and we are given a method to count its occurrences\nin social media. However, counting is plagued by sample bias, incomplete data,\nand, paradoxically, data scarcity -- issues inadequately addressed by prior\nwork. We formulate signal recovery as a Poisson point process estimation\nproblem. We explicitly incorporate human population bias, time delays and\nspatial distortions, and spatio-temporal regularization into the model to\naddress the noisy count issues. We present an efficient optimization algorithm\nand discuss its theoretical properties. We show that our model is more accurate\nthan commonly-used baselines. Finally, we present a case study on wildlife\nroadkill monitoring, where our model produces qualitatively convincing results. \n\n"}
{"id": "1204.3283", "contents": "Title: Automated synthesis of reliable and efficient systems through game\n  theory: a case study Abstract: Reactive computer systems bear inherent complexity due to continuous\ninteractions with their environment. While this environment often proves to be\nuncontrollable, we still want to ensure that critical computer systems will not\nfail, no matter what they face. Examples are legion: railway traffic, power\nplants, plane navigation systems, etc. Formal verification of a system may\nensure that it satisfies a given specification, but only applies to an already\nexisting model of a system. In this work, we address the problem of synthesis:\nstarting from a specification of the desired behavior, we show how to build a\nsuitable system controller that will enforce this specification. In particular,\nwe discuss recent developments of that approach for systems that must ensure\nBoolean behaviors (e.g., reachability, liveness) along with quantitative\nrequirements over their execution (e.g., never drop out of fuel, ensure a\nsuitable mean response time). We notably illustrate a powerful, practically\nuseable algorithm for the automated synthesis of provably safe reactive\nsystems. \n\n"}
{"id": "1204.4639", "contents": "Title: Logic Characterization of Floyd Languages Abstract: Floyd languages (FL), alias Operator Precedence Languages, have recently\nreceived renewed attention thanks to their closure properties and local\nparsability which allow one to apply automatic verification techniques (e.g.\nmodel checking) and parallel and incremental parsing. They properly include\nvarious other classes, noticeably Visual Pushdown languages. In this paper we\nprovide a characterization of FL in terms a monadic second order logic (MSO),\nin the same style as Buchi's one for regular languages. We prove the\nequivalence between automata recognizing FL and the MSO formalization. \n\n"}
{"id": "1204.4685", "contents": "Title: A Query Language for Formal Mathematical Libraries Abstract: One of the most promising applications of mathematical knowledge management\nis search: Even if we restrict attention to the tiny fragment of mathematics\nthat has been formalized, the amount exceeds the comprehension of an individual\nhuman.\n  Based on the generic representation language MMT, we introduce the\nmathematical query language QMT: It combines simplicity, expressivity, and\nscalability while avoiding a commitment to a particular logical formalism. QMT\ncan integrate various search paradigms such as unification, semantic web, or\nXQuery style queries, and QMT queries can span different mathematical\nlibraries.\n  We have implemented QMT as a part of the MMT API. This combination provides a\nscalable indexing and query engine that can be readily applied to any library\nof mathematical knowledge. While our focus here is on libraries that are\navailable in a content markup language, QMT naturally extends to presentation\nand narration markup languages. \n\n"}
{"id": "1204.4736", "contents": "Title: Model Checking with Probabilistic Tabled Logic Programming Abstract: We present a formulation of the problem of probabilistic model checking as\none of query evaluation over probabilistic logic programs. To the best of our\nknowledge, our formulation is the first of its kind, and it covers a rich class\nof probabilistic models and probabilistic temporal logics. The inference\nalgorithms of existing probabilistic logic-programming systems are well defined\nonly for queries with a finite number of explanations. This restriction\nprohibits the encoding of probabilistic model checkers, where explanations\ncorrespond to executions of the system being model checked. To overcome this\nrestriction, we propose a more general inference algorithm that uses finite\ngenerative structures (similar to automata) to represent families of\nexplanations. The inference algorithm computes the probability of a possibly\ninfinite set of explanations directly from the finite generative structure. We\nhave implemented our inference algorithm in XSB Prolog, and use this\nimplementation to encode probabilistic model checkers for a variety of temporal\nlogics, including PCTL and GPL (which subsumes PCTL*). Our experiment results\nshow that, despite the highly declarative nature of their encodings, the model\ncheckers constructed in this manner are competitive with their native\nimplementations. \n\n"}
{"id": "1204.5631", "contents": "Title: A Constructive Interpretation of Ramsey's Theorem via the Product of\n  Selection Functions Abstract: We use G\\\"{o}del's Dialectica interpretation to produce a computational\nversion of the well known proof of Ramsey's theorem by Erd\\H{o}s and Rado. Our\nproof makes use of the product of selection functions, which forms an intuitive\nalternative to Spector's bar recursion when interpreting proofs in analysis.\nThis case study is another instance of the application of proof theoretic\ntechniques in mathematics. \n\n"}
{"id": "1204.6675", "contents": "Title: On the Locality of Some NP-Complete Problems Abstract: We consider the distributed message-passing {LOCAL} model. In this model a\ncommunication network is represented by a graph where vertices host processors,\nand communication is performed over the edges. Computation proceeds in\nsynchronous rounds. The running time of an algorithm is the number of rounds\nfrom the beginning until all vertices terminate. Local computation is free. An\nalgorithm is called {local} if it terminates within a constant number of\nrounds. The question of what problems can be computed locally was raised by\nNaor and Stockmayer \\cite{NS93} in their seminal paper in STOC'93. Since then\nthe quest for problems with local algorithms, and for problems that cannot be\ncomputed locally, has become a central research direction in the field of\ndistributed algorithms \\cite{KMW04,KMW10,LOW08,PR01}.\n  We devise the first local algorithm for an {NP-complete} problem.\nSpecifically, our randomized algorithm computes, with high probability, an\nO(n^{1/2 + epsilon} \\cdot chi)-coloring within O(1) rounds, where epsilon > 0\nis an arbitrarily small constant, and chi is the chromatic number of the input\ngraph. (This problem was shown to be NP-complete in \\cite{Z07}.) On our way to\nthis result we devise a constant-time algorithm for computing (O(1), O(n^{1/2 +\nepsilon}))-network-decompositions. Network-decompositions were introduced by\nAwerbuch et al. \\cite{AGLP89}, and are very useful for solving various\ndistributed problems. The best previously-known algorithm for\nnetwork-decomposition has a polylogarithmic running time (but is applicable for\na wider range of parameters) \\cite{LS93}. We also devise a Delta^{1 +\nepsilon}-coloring algorithm for graphs with sufficiently large maximum degree\nDelta that runs within O(1) rounds. It improves the best previously-known\nresult for this family of graphs, which is O(\\log-star n) \\cite{SW10}. \n\n"}
{"id": "1205.0722", "contents": "Title: Generalized Complexity of ALC Subsumption Abstract: The subsumption problem with respect to terminologies in the description\nlogic ALC is EXPTIME-complete. We investigate the computational complexity of\nfragments of this problem by means of allowed Boolean operators. Hereto we make\nuse of the notion of clones in the context of Post's lattice. Furthermore we\nconsider all four possible quantifier combinations for each fragment\nparameterized by a clone. We will see that depending on what quantifiers are\navailable the classification will be either tripartite or a quartering. \n\n"}
{"id": "1205.5871", "contents": "Title: Squeezing out the Cloud via Profit-Maximizing Resource Allocation\n  Policies Abstract: We study the problem of maximizing the average hourly profit earned by a\nSoftware-as-a-Service (SaaS) provider who runs a software service on behalf of\na customer using servers rented from an Infrastructure-as-a-Service (IaaS)\nprovider. The SaaS provider earns a fee per successful transaction and incurs\ncosts proportional to the number of server-hours it uses. A number of resource\nallocation policies for this or similar problems have been proposed in previous\nwork. However, to the best of our knowledge, these policies have not been\ncomparatively evaluated in a cloud environment. This paper reports on an\nempirical evaluation of three policies using a replica of Wikipedia deployed on\nthe Amazon EC2 cloud. Experimental results show that a policy based on a\nsolution to an optimization problem derived from the SaaS provider's utility\nfunction outperforms well-known heuristics that have been proposed for similar\nproblems. It is also shown that all three policies outperform a \"reactive\"\nallocation approach based on Amazon's auto-scaling feature. \n\n"}
{"id": "1205.6557", "contents": "Title: Interaction Graphs: Additives Abstract: Geometry of Interaction (GoI) is a kind of semantics of linear logic proofs\nthat aims at accounting for the dynamical aspects of cut-elimination. We\npresent here a parametrized construction of a Geometry of Interaction for\nMultiplicative Additive Linear Logic (MALL) in which proofs are represented by\nfamilies of directed weighted graphs. Contrarily to former constructions\ndealing with additive connectives, we are able to solve the known issue of\nobtaining a denotational semantics for MALL by introducing a notion of\nobservational equivalence. Moreover, our setting has the advantage of being the\nfirst construction dealing with additives where proofs of MALL are interpreted\nby finite objects. The fact that we obtain a denotational model of MALL relies\non a single geometric property, which we call the trefoil property, from which\nwe obtain, for each value of the parameter, adjunctions. We then proceed to\nshow how this setting is related to Girard's various constructions: particular\nchoices of the parameter respectively give a combinatorial version of his\nlatest GoI, a refined version of older Geometries of Interaction based on\nnilpotency. This shows the importance of the trefoil property underlying our\nconstructions since all known GoI construction to this day rely on particular\ncases of it. \n\n"}
{"id": "1207.1188", "contents": "Title: On the toggling-branching recurrence of Computability Logic Abstract: We introduce a new, substantially simplified version of the\ntoggling-branching recurrence operation of Computability Logic, prove its\nequivalence to Japaridze's old, \"canonical\" version, and also prove that both\nversions preserve the static property of their arguments. \n\n"}
{"id": "1207.1271", "contents": "Title: Automated Verification of Quantum Protocols using MCMAS Abstract: We present a methodology for the automated verification of quantum protocols\nusing MCMAS, a symbolic model checker for multi-agent systems The method is\nbased on the logical framework developed by D'Hondt and Panangaden for\ninvestigating epistemic and temporal properties, built on the model for\nDistributed Measurement-based Quantum Computation (DMC), an extension of the\nMeasurement Calculus to distributed quantum systems. We describe the\ntranslation map from DMC to interpreted systems, the typical formalism for\nreasoning about time and knowledge in multi-agent systems. Then, we introduce\ndmc2ispl, a compiler into the input language of the MCMAS model checker. We\ndemonstrate the technique by verifying the Quantum Teleportation Protocol, and\ndiscuss the performance of the tool. \n\n"}
{"id": "1207.2479", "contents": "Title: Bisimilarity on Basic Process Algebra is in 2-ExpTime (an explicit\n  proof) Abstract: Burkart, Caucal, Steffen (1995) showed a procedure deciding bisimulation\nequivalence of processes in Basic Process Algebra (BPA), i.e. of sequential\nprocesses generated by context-free grammars. They improved the previous\ndecidability result of Christensen, H\\\"uttel, Stirling (1992), since their\nprocedure has obviously an elementary time complexity and the authors claim\nthat a close analysis would reveal a double exponential upper bound. Here a\nself-contained direct proof of the membership in 2-ExpTime is given. This is\ndone via a Prover-Refuter game which shows that there is an alternating Turing\nmachine deciding the problem in exponential space. The proof uses similar\ningredients (size-measures, decompositions, bases) as the previous proofs, but\none new simplifying factor is an explicit addition of infinite regular strings\nto the state space. An auxiliary claim also shows an explicit exponential upper\nbound on the equivalence level of nonbisimilar normed BPA processes. The\nimportance of clarifying the 2-ExpTime upper bound for BPA bisimilarity has\nrecently increased due to the shift of the known lower bound from PSpace (Srba,\n2002) to ExpTime (Kiefer, 2012). \n\n"}
{"id": "1207.2892", "contents": "Title: A Web Interface for Matita Abstract: This article describes a prototype implementation of a web interface for the\nMatita proof assistant. The interface supports all basic functionalities of the\nlocal Gtk interface, but takes advantage of the markup to enrich the document\nwith several kinds of annotations or active elements. Annotations may have both\na presentational/hypertextual nature, aimed to improve the quality of the proof\nscript as a human readable document, or a more semantic nature, aimed to help\nthe system in its processing of the script. The latter kind comprises\ninformation automatically generated by the proof assistant during previous\ncompilations, and stored to improve the performance of re-executing expensive\noperations like disambiguation or automation. \n\n"}
{"id": "1207.5014", "contents": "Title: Checking Satisfiability by Dependency Sequents Abstract: We introduce a new algorithm for checking satisfiability based on a calculus\nof Dependency sequents (D-sequents). Given a CNF formula F(X), a D-sequent is a\nrecord stating that under a partial assignment a set of variables of X is\nredundant in formula \\exists{X}[F]. The D-sequent calculus is based on\noperation join that forms a new D-sequent from two existing D-sequents. The new\nalgorithm solves the quantified version of SAT. That is, given a satisfiable\nformula F, it, in general, does not produce an assignment satisfying F.\n  The new algorithm is called DS-QSAT where DS stands for Dependency Sequent\nand Q for Quantified.\n  Importantly, a DPLL-like procedure is only a special case of DS-QSAT where a\nvery restricted kind of D-sequents is used. We argue that this restriction a)\nadversely affects scalability of SAT-solvers and b) is caused by looking for an\nexplicit satisfying assignment rather than just proving satisfiability. We give\nexperimental results substantiating these claims. \n\n"}
{"id": "1207.6253", "contents": "Title: On When and How to use SAT to Mine Frequent Itemsets Abstract: A new stream of research was born in the last decade with the goal of mining\nitemsets of interest using Constraint Programming (CP). This has promoted a\nnatural way to combine complex constraints in a highly flexible manner.\nAlthough CP state-of-the-art solutions formulate the task using Boolean\nvariables, the few attempts to adopt propositional Satisfiability (SAT)\nprovided an unsatisfactory performance. This work deepens the study on when and\nhow to use SAT for the frequent itemset mining (FIM) problem by defining\ndifferent encodings with multiple task-driven enumeration options and search\nstrategies. Although for the majority of the scenarios SAT-based solutions\nappear to be non-competitive with CP peers, results show a variety of\ninteresting cases where SAT encodings are the best option. \n\n"}
{"id": "1208.5205", "contents": "Title: Monoidal computer I: Basic computability by string diagrams Abstract: We present a new model of computation, described in terms of monoidal\ncategories. It conforms the Church-Turing Thesis, and captures the same\ncomputable functions as the standard models. It provides a succinct categorical\ninterface to most of them, free of their diverse implementation details, using\nthe ideas and structures that in the meantime emerged from research in\nsemantics of computation and programming. The salient feature of the language\nof monoidal categories is that it is supported by a sound and complete\ngraphical formalism, string diagrams, which provide a concrete and intuitive\ninterface for abstract reasoning about computation. The original motivation and\nthe ultimate goal of this effort is to provide a convenient high level\nprogramming language for a theory of computational resources, such as one-way\nfunctions, and trapdoor functions, by adopting the methods for hiding the low\nlevel implementation details that emerged from practice. In the present paper,\nwe make a first step towards this ambitious goal, and sketch a path to reach\nit. This path is pursued in three sequel papers, that are in preparation. \n\n"}
{"id": "1208.5915", "contents": "Title: Relaxed Operational Semantics of Concurrent Programming Languages Abstract: We propose a novel, operational framework to formally describe the semantics\nof concurrent programs running within the context of a relaxed memory model.\nOur framework features a \"temporary store\" where the memory operations issued\nby the threads are recorded, in program order. A memory model then specifies\nthe conditions under which a pending operation from this sequence is allowed to\nbe globally performed, possibly out of order. The memory model also involves a\n\"write grain,\" accounting for architectures where a thread may read a write\nthat is not yet globally visible. Our formal model is supported by a software\nsimulator, allowing us to run litmus tests in our semantics. \n\n"}
{"id": "1209.3332", "contents": "Title: High-throughput Execution of Hierarchical Analysis Pipelines on Hybrid\n  Cluster Platforms Abstract: We propose, implement, and experimentally evaluate a runtime middleware to\nsupport high-throughput execution on hybrid cluster machines of large-scale\nanalysis applications. A hybrid cluster machine consists of computation nodes\nwhich have multiple CPUs and general purpose graphics processing units (GPUs).\nOur work targets scientific analysis applications in which datasets are\nprocessed in application-specific data chunks, and the processing of a data\nchunk is expressed as a hierarchical pipeline of operations. The proposed\nmiddleware system combines a bag-of-tasks style execution with coarse-grain\ndataflow execution. Data chunks and associated data processing pipelines are\nscheduled across cluster nodes using a demand driven approach, while within a\nnode operations in a given pipeline instance are scheduled across CPUs and\nGPUs. The runtime system implements several optimizations, including\nperformance aware task scheduling, architecture aware process placement, data\nlocality conscious task assignment, and data prefetching and asynchronous data\ncopy, to maximize utilization of the aggregate computing power of CPUs and GPUs\nand minimize data copy overheads. The application and performance benefits of\nthe runtime middleware are demonstrated using an image analysis application,\nwhich is employed in a brain cancer study, on a state-of-the-art hybrid cluster\nin which each node has two 6-core CPUs and three GPUs. Our results show that\nimplementing and scheduling application data processing as a set of fine-grain\noperations provide more opportunities for runtime optimizations and attain\nbetter performance than a coarser-grain, monolithic implementation. The\nproposed runtime system can achieve high-throughput processing of large\ndatasets - we were able to process an image dataset consisting of 36,848\n4Kx4K-pixel image tiles at about 150 tiles/second rate on 100 nodes. \n\n"}
{"id": "1209.3904", "contents": "Title: A Distributed Algorithm for Gathering Many Fat Mobile Robots in the\n  Plane Abstract: In this work we consider the problem of gathering autonomous robots in the\nplane. In particular, we consider non-transparent unit-disc robots (i.e., fat)\nin an asynchronous setting. Vision is the only mean of coordination. Using a\nstate-machine representation we formulate the gathering problem and develop a\ndistributed algorithm that solves the problem for any number of robots.\n  The main idea behind our algorithm is for the robots to reach a configuration\nin which all the following hold: (a) The robots' centers form a convex hull in\nwhich all robots are on the convex, (b) Each robot can see all other robots,\nand (c) The configuration is connected, that is, every robot touches another\nrobot and all robots together form a connected formation. We show that starting\nfrom any initial configuration, the robots, making only local decisions and\ncoordinate by vision, eventually reach such a configuration and terminate,\nyielding a solution to the gathering problem. \n\n"}
{"id": "1209.3914", "contents": "Title: Theorem Proving in Large Formal Mathematics as an Emerging AI Field Abstract: In the recent years, we have linked a large corpus of formal mathematics with\nautomated theorem proving (ATP) tools, and started to develop combined AI/ATP\nsystems working in this setting. In this paper we first relate this project to\nthe earlier large-scale automated developments done by Quaife with McCune's\nOtter system, and to the discussions of the QED project about formalizing a\nsignificant part of mathematics. Then we summarize our adventure so far, argue\nthat the QED dreams were right in anticipating the creation of a very\ninteresting semantic AI field, and discuss its further research directions. \n\n"}
{"id": "1209.4935", "contents": "Title: Adaptive Real Time Imaging Synthesis Telescopes Abstract: The digital revolution is transforming astronomy from a data-starved to a\ndata-submerged science. Instruments such as the Atacama Large Millimeter Array\n(ALMA), the Large Synoptic Survey Telescope (LSST), and the Square Kilometer\nArray (SKA) will measure their accumulated data in petabytes. The capacity to\nproduce enormous volumes of data must be matched with the computing power to\nprocess that data and produce meaningful results. In addition to handling huge\ndata rates, we need adaptive calibration and beamforming to handle atmospheric\nfluctuations and radio frequency interference, and to provide a user\nenvironment which makes the full power of large telescope arrays accessible to\nboth expert and non-expert users. Delayed calibration and analysis limit the\nscience which can be done. To make the best use of both telescope and human\nresources we must reduce the burden of data reduction.\n  Our instrumentation comprises of a flexible correlator, beam former and\nimager with digital signal processing closely coupled with a computing cluster.\nThis instrumentation will be highly accessible to scientists, engineers, and\nstudents for research and development of real-time processing algorithms, and\nwill tap into the pool of talented and innovative students and visiting\nscientists from engineering, computing, and astronomy backgrounds.\n  Adaptive real-time imaging will transform radio astronomy by providing\nreal-time feedback to observers. Calibration of the data is made in close to\nreal time using a model of the sky brightness distribution. The derived\ncalibration parameters are fed back into the imagers and beam formers. The\nregions imaged are used to update and improve the a-priori model, which becomes\nthe final calibrated image by the time the observations are complete. \n\n"}
{"id": "1210.0187", "contents": "Title: External Memory based Distributed Generation of Massive Scale Social\n  Networks on Small Clusters Abstract: Small distributed systems are limited by their main memory to generate\nmassively large graphs. Trivial extension to current graph generators to\nutilize external memory leads to large amount of random I/O hence do not scale\nwith size. In this work we offer a technique to generate massive scale graphs\non small cluster of compute nodes with limited main memory. We develop several\ndistributed and external memory algorithms, primarily, shuffle, relabel,\nredistribute, and, compressed-sparse-row (csr) convert. The algorithms are\nimplemented in MPI/pthread model to help parallelize the operations across\nmulticores within each core. Using our scheme it is feasible to generate a\ngraph of size $2^{38}$ nodes (scale 38) using only 64 compute nodes. This can\nbe compared with the current scheme would require at least 8192 compute node,\nassuming 64GB of main memory.\n  Our work has broader implications for external memory graph libraries such as\nSTXXL and graph processing on SSD-based supercomputers such as Dash and Gordon\n[1][2]. \n\n"}
{"id": "1210.1931", "contents": "Title: D-FLAT: Declarative Problem Solving Using Tree Decompositions and\n  Answer-Set Programming Abstract: In this work, we propose Answer-Set Programming (ASP) as a tool for rapid\nprototyping of dynamic programming algorithms based on tree decompositions. In\nfact, many such algorithms have been designed, but only a few of them found\ntheir way into implementation. The main obstacle is the lack of easy-to-use\nsystems which (i) take care of building a tree decomposition and (ii) provide\nan interface for declarative specifications of dynamic programming algorithms.\nIn this paper, we present D-FLAT, a novel tool that relieves the user of having\nto handle all the technical details concerned with parsing, tree decomposition,\nthe handling of data structures, etc. Instead, it is only the dynamic\nprogramming algorithm itself which has to be specified in the ASP language.\nD-FLAT employs an ASP solver in order to compute the local solutions in the\ndynamic programming algorithm. In the paper, we give a few examples\nillustrating the use of D-FLAT and describe the main features of the system.\nMoreover, we report experiments which show that ASP-based D-FLAT encodings for\nsome problems outperform monolithic ASP encodings on instances of small\ntreewidth. \n\n"}
{"id": "1210.2276", "contents": "Title: A Map-Reduce Parallel Approach to Automatic Synthesis of Control\n  Software Abstract: Many Control Systems are indeed Software Based Control Systems, i.e. control\nsystems whose controller consists of control software running on a\nmicrocontroller device. This motivates investigation on Formal Model Based\nDesign approaches for automatic synthesis of control software.\n  Available algorithms and tools (e.g., QKS) may require weeks or even months\nof computation to synthesize control software for large-size systems. This\nmotivates search for parallel algorithms for control software synthesis.\n  In this paper, we present a Map-Reduce style parallel algorithm for control\nsoftware synthesis when the controlled system (plant) is modeled as discrete\ntime linear hybrid system. Furthermore we present an MPI-based implementation\nPQKS of our algorithm. To the best of our knowledge, this is the first parallel\napproach for control software synthesis.\n  We experimentally show effectiveness of PQKS on two classical control\nsynthesis problems: the inverted pendulum and the multi-input buck DC/DC\nconverter. Experiments show that PQKS efficiency is above 65%. As an example,\nPQKS requires about 16 hours to complete the synthesis of control software for\nthe pendulum on a cluster with 60 processors, instead of the 25 days needed by\nthe sequential algorithm in QKS. \n\n"}
{"id": "1211.3831", "contents": "Title: Objective Improvement in Information-Geometric Optimization Abstract: Information-Geometric Optimization (IGO) is a unified framework of stochastic\nalgorithms for optimization problems. Given a family of probability\ndistributions, IGO turns the original optimization problem into a new\nmaximization problem on the parameter space of the probability distributions.\nIGO updates the parameter of the probability distribution along the natural\ngradient, taken with respect to the Fisher metric on the parameter manifold,\naiming at maximizing an adaptive transform of the objective function. IGO\nrecovers several known algorithms as particular instances: for the family of\nBernoulli distributions IGO recovers PBIL, for the family of Gaussian\ndistributions the pure rank-mu CMA-ES update is recovered, and for exponential\nfamilies in expectation parametrization the cross-entropy/ML method is\nrecovered. This article provides a theoretical justification for the IGO\nframework, by proving that any step size not greater than 1 guarantees monotone\nimprovement over the course of optimization, in terms of q-quantile values of\nthe objective function f. The range of admissible step sizes is independent of\nf and its domain. We extend the result to cover the case of different step\nsizes for blocks of the parameters in the IGO algorithm. Moreover, we prove\nthat expected fitness improves over time when fitness-proportional selection is\napplied, in which case the RPP algorithm is recovered. \n\n"}
{"id": "1211.6778", "contents": "Title: Efficient parallel algorithms for tandem queueing system simulation Abstract: Parallel algorithms designed for simulation and performance evaluation of\nsingle-server tandem queueing systems with both infinite and finite buffers are\npresented. The algorithms exploit a simple computational procedure based on\nrecursive equations as a representation of system dynamics. A brief analysis of\nthe performance of the algorithms are given to show that they involve low time\nand memory requirements. \n\n"}
{"id": "1212.1941", "contents": "Title: Amortized communication complexity of an equality predicate Abstract: We study the communication complexity of a direct sum of independent copies\nof the equality predicate. We prove that the probabilistic communication\ncomplexity of this problem is equal to O(N); computational complexity of the\nproposed protocol is polynomial in size of inputs. Our protocol improves the\nresult achieved in 1995(Feder, Kushilevitz, Naor, Nisan). Our construction is\nbased on two techniques: Nisan's pseudorandom generator (1992) and Smith's\nstring synchronization algorithm (2007). \n\n"}
{"id": "1212.2314", "contents": "Title: Tree Projections and Structural Decomposition Methods: Minimality and\n  Game-Theoretic Characterization Abstract: Tree projections provide a mathematical framework that encompasses all the\nvarious (purely) structural decomposition methods that have been proposed in\nthe literature to single out classes of nearly-acyclic (hyper)graphs, such as\nthe tree decomposition method, which is the most powerful decomposition method\non graphs, and the (generalized) hypertree decomposition method, which is its\nnatural counterpart on arbitrary hypergraphs. The paper analyzes this\nframework, by focusing in particular on \"minimal\" tree projections, that is, on\ntree projections without useless redundancies. First, it is shown that minimal\ntree projections enjoy a number of properties that are usually required for\nnormal form decompositions in various structural decomposition methods. In\nparticular, they enjoy the same kind of connection properties as (minimal) tree\ndecompositions of graphs, with the result being tight in the light of the\nnegative answer that is provided to the open question about whether they enjoy\na slightly stronger notion of connection property, defined to speed-up the\ncomputation of hypertree decompositions. Second, it is shown that tree\nprojections admit a natural game-theoretic characterization in terms of the\nCaptain and Robber game. In this game, as for the Robber and Cops game\ncharacterizing tree decompositions, the existence of winning strategies implies\nthe existence of monotone ones. As a special case, the Captain and Robber game\ncan be used to characterize the generalized hypertree decomposition method,\nwhere such a game-theoretic characterization was missing and asked for. Besides\ntheir theoretical interest, these results have immediate algorithmic\napplications both for the general setting and for structural decomposition\nmethods that can be recast in terms of tree projections. \n\n"}
{"id": "1212.3879", "contents": "Title: Interacting via the Heap in the Presence of Recursion Abstract: Almost all modern imperative programming languages include operations for\ndynamically manipulating the heap, for example by allocating and deallocating\nobjects, and by updating reference fields. In the presence of recursive\nprocedures and local variables the interactions of a program with the heap can\nbecome rather complex, as an unbounded number of objects can be allocated\neither on the call stack using local variables, or, anonymously, on the heap\nusing reference fields. As such a static analysis is, in general, undecidable.\n  In this paper we study the verification of recursive programs with unbounded\nallocation of objects, in a simple imperative language for heap manipulation.\nWe present an improved semantics for this language, using an abstraction that\nis precise. For any program with a bounded visible heap, meaning that the\nnumber of objects reachable from variables at any point of execution is\nbounded, this abstraction is a finitary representation of its behaviour, even\nthough an unbounded number of objects can appear in the state. As a\nconsequence, for such programs model checking is decidable.\n  Finally we introduce a specification language for temporal properties of the\nheap, and discuss model checking these properties against heap-manipulating\nprograms. \n\n"}
{"id": "1212.5880", "contents": "Title: Local Thresholding in General Network Graphs Abstract: Local thresholding algorithms were first presented more than a decade ago and\nhave since been applied to a variety of data mining tasks in peer-to-peer\nsystems, wireless sensor networks, and in grid systems. One critical assumption\nmade by those algorithms has always been cycle-free routing. The existence of\neven one cycle may lead all peers to the wrong outcome. Outside the lab,\nunfortunately, cycle freedom is not easy to achieve.\n  This work is the first to lift the requirement of cycle freedom by presenting\na local thresholding algorithm suitable for general network graphs. The\nalgorithm relies on a new repositioning of the problem in weighted vector\narithmetics, on a new stopping rule, whose proof does not require that the\nnetwork be cycle free, and on new methods for balance correction when the\nstopping rule fails.\n  The new stopping and update rules permit calculation of the very same\nfunctions that were calculable using previous algorithms, which do assume cycle\nfreedom. The algorithm is implemented on a standard peer-to-peer simulator and\nis validated for networks of up to 80,000 peers, organized in three different\ntopologies, which are representative of the topology of major current\ndistributed systems: the Internet, structured peer-to-peer systems, and\nwireless sensor networks. \n\n"}
{"id": "1301.3350", "contents": "Title: On Recursive Operations Over Logic LTS Abstract: Recently, in order to mix algebraic and logic styles of specification in a\nuniform framework, the notion of a logic labelled transition system (Logic LTS\nor LLTS for short) has been introduced and explored. A variety of constructors\nover LLTS, including usual process-algebraic operators, logic connectives\n(conjunction and disjunction) and standard temporal operators (always and\nunless), have been given. However, no attempt has made so far to develop\ngeneral theory concerning (nested) recursive operations over LLTSs and a few\nfundamental problems are still open. This paper intends to study this issue in\npure process-algebraic style. A few fundamental properties, including\nprecongruence and the uniqueness of consistent solutions for equations, will be\nestablished. \n\n"}
{"id": "1301.3791", "contents": "Title: XORing Elephants: Novel Erasure Codes for Big Data Abstract: Distributed storage systems for large clusters typically use replication to\nprovide reliability. Recently, erasure codes have been used to reduce the large\nstorage overhead of three-replicated systems. Reed-Solomon codes are the\nstandard design choice and their high repair cost is often considered an\nunavoidable price to pay for high storage efficiency and high reliability.\n  This paper shows how to overcome this limitation. We present a novel family\nof erasure codes that are efficiently repairable and offer higher reliability\ncompared to Reed-Solomon codes. We show analytically that our codes are optimal\non a recently identified tradeoff between locality and minimum distance.\n  We implement our new codes in Hadoop HDFS and compare to a currently deployed\nHDFS module that uses Reed-Solomon codes. Our modified HDFS implementation\nshows a reduction of approximately 2x on the repair disk I/O and repair network\ntraffic. The disadvantage of the new coding scheme is that it requires 14% more\nstorage compared to Reed-Solomon codes, an overhead shown to be information\ntheoretically optimal to obtain locality. Because the new codes repair failures\nfaster, this provides higher reliability, which is orders of magnitude higher\ncompared to replication. \n\n"}
{"id": "1302.3741", "contents": "Title: Upper bounds for Newton's method on monotone polynomial systems, and\n  P-time model checking of probabilistic one-counter automata Abstract: A central computational problem for analyzing and model checking various\nclasses of infinite-state recursive probabilistic systems (including\nquasi-birth-death processes, multi-type branching processes, stochastic\ncontext-free grammars, probabilistic pushdown automata and recursive Markov\nchains) is the computation of {\\em termination probabilities}, and computing\nthese probabilities in turn boils down to computing the {\\em least fixed point}\n(LFP) solution of a corresponding {\\em monotone polynomial system} (MPS) of\nequations, denoted x=P(x).\n  It was shown by Etessami & Yannakakis that a decomposed variant of Newton's\nmethod converges monotonically to the LFP solution for any MPS that has a\nnon-negative solution. Subsequently, Esparza, Kiefer, & Luttenberger obtained\nupper bounds on the convergence rate of Newton's method for certain classes of\nMPSs. More recently, better upper bounds have been obtained for special classes\nof MPSs. However, prior to this paper, for arbitrary (not necessarily\nstrongly-connected) MPSs, no upper bounds at all were known on the convergence\nrate of Newton's method as a function of the encoding size |P| of the input\nMPS, x=P(x).\n  In this paper we provide worst-case upper bounds, as a function of both the\ninput encoding size |P|, and epsilon > 0, on the number of iterations required\nfor decomposed Newton's method (even with rounding) to converge within additive\nerror epsilon > 0 of q^*, for any MPS with LFP solution q^*. Our upper bounds\nare essentially optimal in terms of several important parameters.\n  Using our upper bounds, and building on prior work, we obtain the first\nP-time algorithm (in the standard Turing model of computation) for quantitative\nmodel checking, to within desired precision, of discrete-time QBDs and\n(equivalently) probabilistic 1-counter automata, with respect to any (fixed)\nomega-regular or LTL property. \n\n"}
{"id": "1302.3860", "contents": "Title: ScalienDB: Designing and Implementing a Distributed Database using Paxos Abstract: ScalienDB is a scalable, replicated database built on top of the Paxos\nalgorithm. It was developed from 2010 to 2012, when the startup backing it\nfailed. This paper discusses the design decisions of the distributed database,\ndescribes interesting parts of the C++ codebase and enumerates lessons learned\nputting ScalienDB into production at a handful of clients. The source code is\navailable on Github under the AGPL license, but it is no longer developed or\nmaintained. \n\n"}
{"id": "1302.6890", "contents": "Title: A Graphical Language for Proof Strategies Abstract: Complex automated proof strategies are often difficult to extract, visualise,\nmodify, and debug. Traditional tactic languages, often based on stack-based\ngoal propagation, make it easy to write proofs that obscure the flow of goals\nbetween tactics and are fragile to minor changes in input, proof structure or\nchanges to tactics themselves. Here, we address this by introducing a graphical\nlanguage called PSGraph for writing proof strategies. Strategies are\nconstructed visually by \"wiring together\" collections of tactics and evaluated\nby propagating goal nodes through the diagram via graph rewriting. Tactic nodes\ncan have many output wires, and use a filtering procedure based on goal-types\n(predicates describing the features of a goal) to decide where best to send\nnewly-generated sub-goals.\n  In addition to making the flow of goal information explicit, the graphical\nlanguage can fulfil the role of many tacticals using visual idioms like\nbranching, merging, and feedback loops. We argue that this language enables\ndevelopment of more robust proof strategies and provide several examples, along\nwith a prototype implementation in Isabelle. \n\n"}
{"id": "1303.4431", "contents": "Title: Generalized Thompson Sampling for Sequential Decision-Making and Causal\n  Inference Abstract: Recently, it has been shown how sampling actions from the predictive\ndistribution over the optimal action-sometimes called Thompson sampling-can be\napplied to solve sequential adaptive control problems, when the optimal policy\nis known for each possible environment. The predictive distribution can then be\nconstructed by a Bayesian superposition of the optimal policies weighted by\ntheir posterior probability that is updated by Bayesian inference and causal\ncalculus. Here we discuss three important features of this approach. First, we\ndiscuss in how far such Thompson sampling can be regarded as a natural\nconsequence of the Bayesian modeling of policy uncertainty. Second, we show how\nThompson sampling can be used to study interactions between multiple adaptive\nagents, thus, opening up an avenue of game-theoretic analysis. Third, we show\nhow Thompson sampling can be applied to infer causal relationships when\ninteracting with an environment in a sequential fashion. In summary, our\nresults suggest that Thompson sampling might not merely be a useful heuristic,\nbut a principled method to address problems of adaptive sequential\ndecision-making and causal inference. \n\n"}
{"id": "1304.1007", "contents": "Title: Linear-in-$\\Delta$ Lower Bounds in the LOCAL Model Abstract: By prior work, there is a distributed algorithm that finds a maximal\nfractional matching (maximal edge packing) in $O(\\Delta)$ rounds, where\n$\\Delta$ is the maximum degree of the graph. We show that this is optimal:\nthere is no distributed algorithm that finds a maximal fractional matching in\n$o(\\Delta)$ rounds.\n  Our work gives the first linear-in-$\\Delta$ lower bound for a natural graph\nproblem in the standard model of distributed computing---prior lower bounds for\na wide range of graph problems have been at best logarithmic in $\\Delta$. \n\n"}
{"id": "1304.1697", "contents": "Title: Verification of Artifact-Centric Systems: Decidability and Modeling\n  Issues Abstract: Artifact-centric business processes have recently emerged as an approach in\nwhich processes are centred around the evolution of business entities, called\nartifacts, giving equal importance to control-flow and data. The recent\nGuard-State-Milestone (GSM) approach provides means for specifying business\nartifacts lifecycles in a declarative manner, using constructs that match how\nexecutive-level stakeholders think about their business. However, it turns out\nthat formal verification of GSM is undecidable even for very simple\npropositional temporal properties. We attack this challenging problem by\ntranslating GSM into a well-studied formal framework. We exploit this\ntranslation to isolate an interesting class of state-bounded GSM models for\nwhich verification of sophisticated temporal properties is decidable. We then\nintroduce some guidelines to turn an arbitrary GSM model into a state-bounded,\nverifiable model. \n\n"}
{"id": "1305.1121", "contents": "Title: Storage and Search in Dynamic Peer-to-Peer Networks Abstract: We study robust and efficient distributed algorithms for searching, storing,\nand maintaining data in dynamic Peer-to-Peer (P2P) networks. P2P networks are\nhighly dynamic networks that experience heavy node churn (i.e., nodes join and\nleave the network continuously over time). Our goal is to guarantee, despite\nhigh node churn rate, that a large number of nodes in the network can store,\nretrieve, and maintain a large number of data items. Our main contributions are\nfast randomized distributed algorithms that guarantee the above with high\nprobability (whp) even under high adversarial churn:\n  1. A randomized distributed search algorithm that (whp) guarantees that\nsearches from as many as $n - o(n)$ nodes ($n$ is the stable network size)\nsucceed in ${O}(\\log n)$-rounds despite ${O}(n/\\log^{1+\\delta} n)$ churn, for\nany small constant $\\delta > 0$, per round. We assume that the churn is\ncontrolled by an oblivious adversary (that has complete knowledge and control\nof what nodes join and leave and at what time, but is oblivious to the random\nchoices made by the algorithm).\n  2. A storage and maintenance algorithm that guarantees (whp) data items can\nbe efficiently stored (with only $\\Theta(\\log{n})$ copies of each data item)\nand maintained in a dynamic P2P network with churn rate up to\n${O}(n/\\log^{1+\\delta} n)$ per round. Our search algorithm together with our\nstorage and maintenance algorithm guarantees that as many as $n - o(n)$ nodes\ncan efficiently store, maintain, and search even under ${O}(n/\\log^{1+\\delta}\nn)$ churn per round. Our algorithms require only polylogarithmic in $n$ bits to\nbe processed and sent (per round) by each node.\n  To the best of our knowledge, our algorithms are the first-known,\nfully-distributed storage and search algorithms that provably work under highly\ndynamic settings (i.e., high churn rates per step). \n\n"}
{"id": "1305.5786", "contents": "Title: Graphic lambda calculus Abstract: We introduce and study graphic lambda calculus, a visual language which can\nbe used for representing untyped lambda calculus, but it can also be used for\ncomputations in emergent algebras or for representing Reidemeister moves of\nlocally planar tangle diagrams. \n\n"}
{"id": "1305.7360", "contents": "Title: Pervasive Parallelism in Highly-Trustable Interactive Theorem Proving\n  Systems Abstract: This is an overview of the Paral-ITP project, which intents to make the proof\nassistants Isabelle and Coq fit for the multicore era. \n\n"}
{"id": "1306.2697", "contents": "Title: Probabilistic Concurrent Kleene Algebra Abstract: We provide an extension of concurrent Kleene algebras to account for\nprobabilistic properties. The algebra yields a unified framework containing\nnondeterminism, concurrency and probability and is sound with respect to the\nset of probabilistic automata modulo probabilistic simulation. We use the\nresulting algebra to generalise the algebraic formulation of a variant of\nJones' rely/guarantee calculus. \n\n"}
{"id": "1306.4242", "contents": "Title: Certified Impossibility Results for Byzantine-Tolerant Mobile Robots Abstract: We propose a framework to build formal developments for robot networks using\nthe COQ proof assistant, to state and to prove formally various properties. We\nfocus in this paper on impossibility proofs, as it is natural to take advantage\nof the COQ higher order calculus to reason about algorithms as abstract\nobjects. We present in particular formal proofs of two impossibility results\nforconvergence of oblivious mobile robots if respectively more than one half\nand more than one third of the robots exhibit Byzantine failures, starting from\nthe original theorems by Bouzid et al.. Thanks to our formalization, the\ncorresponding COQ developments are quite compact. To our knowledge, these are\nthe first certified (in the sense of formally proved) impossibility results for\nrobot networks. \n\n"}
{"id": "1306.6843", "contents": "Title: Error AMP Chain Graphs Abstract: Any regular Gaussian probability distribution that can be represented by an\nAMP chain graph (CG) can be expressed as a system of linear equations with\ncorrelated errors whose structure depends on the CG. However, the CG represents\nthe errors implicitly, as no nodes in the CG correspond to the errors. We\npropose in this paper to add some deterministic nodes to the CG in order to\nrepresent the errors explicitly. We call the result an EAMP CG. We will show\nthat, as desired, every AMP CG is Markov equivalent to its corresponding EAMP\nCG under marginalization of the error nodes. We will also show that every EAMP\nCG under marginalization of the error nodes is Markov equivalent to some LWF CG\nunder marginalization of the error nodes, and that the latter is Markov\nequivalent to some directed and acyclic graph (DAG) under marginalization of\nthe error nodes and conditioning on some selection nodes. This is important\nbecause it implies that the independence model represented by an AMP CG can be\naccounted for by some data generating process that is partially observed and\nhas selection bias. Finally, we will show that EAMP CGs are closed under\nmarginalization. This is a desirable feature because it guarantees parsimonious\nmodels under marginalization. \n\n"}
{"id": "1307.1270", "contents": "Title: A heterogeneous many-core platform for experiments on scalable custom\n  interconnects and management of fault and critical events, applied to\n  many-process applications: Vol. II, 2012 technical report Abstract: This is the second of a planned collection of four yearly volumes describing\nthe deployment of a heterogeneous many-core platform for experiments on\nscalable custom interconnects and management of fault and critical events,\napplied to many-process applications. This volume covers several topics, among\nwhich: 1- a system for awareness of faults and critical events (named LO|FA|MO)\non experimental heterogeneous many-core hardware platforms; 2- the integration\nand test of the experimental hardware heterogeneous many-core platform QUoNG,\nbased on the APEnet+ custom interconnect; 3- the design of a\nSoftware-Programmable Distributed Network Processor architecture (DNP) using\nASIP technology; 4- the initial stages of design of a new DNP generation onto a\n28nm FPGA. These developments were performed in the framework of the EURETILE\nEuropean Project under the Grant Agreement no. 247846. \n\n"}
{"id": "1307.1332", "contents": "Title: Byzantine Convex Consensus: An Optimal Algorithm Abstract: Much of the past work on asynchronous approximate Byzantine consensus has\nassumed scalar inputs at the nodes [4, 8]. Recent work has yielded approximate\nByzantine consensus algorithms for the case when the input at each node is a\nd-dimensional vector, and the nodes must reach consensus on a vector in the\nconvex hull of the input vectors at the fault-free nodes [9, 13]. The\nd-dimensional vectors can be equivalently viewed as points in the d-dimensional\nEuclidean space. Thus, the algorithms in [9, 13] require the fault-free nodes\nto decide on a point in the d-dimensional space.\n  In our recent work [arXiv:/1307.1051], we proposed a generalization of the\nconsensus problem, namely Byzantine convex consensus (BCC), which allows the\ndecision to be a convex polytope in the d-dimensional space, such that the\ndecided polytope is within the convex hull of the input vectors at the\nfault-free nodes. We also presented an asynchronous approximate BCC algorithm.\n  In this paper, we propose a new BCC algorithm with optimal fault-tolerance\nthat also agrees on a convex polytope that is as large as possible under\nadversarial conditions. Our prior work [arXiv:/1307.1051] does not guarantee\nthe optimality of the output polytope. \n\n"}
{"id": "1307.3231", "contents": "Title: Certification of Bounds of Non-linear Functions: the Templates Method Abstract: The aim of this work is to certify lower bounds for real-valued multivariate\nfunctions, defined by semialgebraic or transcendental expressions. The\ncertificate must be, eventually, formally provable in a proof system such as\nCoq. The application range for such a tool is widespread; for instance Hales'\nproof of Kepler's conjecture yields thousands of inequalities. We introduce an\napproximation algorithm, which combines ideas of the max-plus basis method (in\noptimal control) and of the linear templates method developed by Manna et al.\n(in static analysis). This algorithm consists in bounding some of the\nconstituents of the function by suprema of quadratic forms with a well chosen\ncurvature. This leads to semialgebraic optimization problems, solved by\nsum-of-squares relaxations. Templates limit the blow up of these relaxations at\nthe price of coarsening the approximation. We illustrate the efficiency of our\nframework with various examples from the literature and discuss the interfacing\nwith Coq. \n\n"}
{"id": "1307.3544", "contents": "Title: Distributed Bayesian Detection with Byzantine Data Abstract: In this paper, we consider the problem of distributed Bayesian detection in\nthe presence of Byzantines in the network. It is assumed that a fraction of the\nnodes in the network are compromised and reprogrammed by an adversary to\ntransmit false information to the fusion center (FC) to degrade detection\nperformance. The problem of distributed detection is formulated as a binary\nhypothesis test at the FC based on 1-bit data sent by the sensors. The\nexpression for minimum attacking power required by the Byzantines to blind the\nFC is obtained. More specifically, we show that above a certain fraction of\nByzantine attackers in the network, the detection scheme becomes completely\nincapable of utilizing the sensor data for detection. We analyze the problem\nunder different attacking scenarios and derive results for different\nnon-asymptotic cases. It is found that existing asymptotics-based results do\nnot hold under several non-asymptotic scenarios. When the fraction of\nByzantines is not sufficient to blind the FC, we also provide closed form\nexpressions for the optimal attacking strategies for the Byzantines that most\ndegrade the detection performance. \n\n"}
{"id": "1307.3802", "contents": "Title: Probability Distinguishes Different Types of Conditional Statements Abstract: The language of probability is used to define several different types of\nconditional statements. There are four principal types: subjunctive, material,\nexistential, and feasibility. Two further types of conditionals are defined\nusing the propositional calculus and Boole's mathematical logic:\ntruth-functional and Boolean feasibility (which turn out to be special cases of\nprobabilistic conditionals). Each probabilistic conditional is quantified by a\nfractional parameter between zero and one that says whether it is purely\naffirmative, purely negative, or intermediate in its sense. Conditionals can be\nspecialized further by their content to express factuality and\ncounterfactuality, and revised or reformulated to account for exceptions and\nconfounding factors. The various conditionals have distinct mathematical\nrepresentations: through intermediate probability expressions and logical\nformulas, each conditional is eventually translated into a set of polynomial\nequations and inequalities (with real coefficients). The polynomial systems\nfrom different types of conditionals exhibit different patterns of behavior,\nconcerning for example opposing conditionals or false antecedents. Interesting\nresults can be computed from the relevant polynomial systems using well-known\nmethods from algebra and computer science. Among other benefits, the proposed\nframework of analysis offers paraconsistent procedures for logical deduction\nthat produce such familiar results as modus ponens, transitivity, disjunction\nintroduction, and disjunctive syllogism; all while avoiding any explosion of\nconsequences from inconsistent premises. Several example problems from Goodman\nand Adams are analyzed. A new perspective called polylogicism is presented:\nmathematical logic that respects the diversity among conditionals in particular\nand logic problems in general. \n\n"}
{"id": "1307.6958", "contents": "Title: Simplifying proofs of linearisability using layers of abstraction Abstract: Linearisability has become the standard correctness criterion for concurrent\ndata structures, ensuring that every history of invocations and responses of\nconcurrent operations has a matching sequential history. Existing proofs of\nlinearisability require one to identify so-called linearisation points within\nthe operations under consideration, which are atomic statements whose execution\ncauses the effect of an operation to be felt. However, identification of\nlinearisation points is a non-trivial task, requiring a high degree of\nexpertise. For sophisticated algorithms such as Heller et al's lazy set, it\neven is possible for an operation to be linearised by the concurrent execution\nof a statement outside the operation being verified. This paper proposes an\nalternative method for verifying linearisability that does not require\nidentification of linearisation points. Instead, using an interval-based logic,\nwe show that every behaviour of each concrete operation over any interval is a\npossible behaviour of a corresponding abstraction that executes with\ncoarse-grained atomicity. This approach is applied to Heller et al's lazy set\nto show that verification of linearisability is possible without having to\nconsider linearisation points within the program code. \n\n"}
{"id": "1307.7048", "contents": "Title: Pivoting makes the ZX-calculus complete for real stabilizers Abstract: We show that pivoting property of graph states cannot be derived from the\naxioms of the ZX-calculus, and that pivoting does not imply local\ncomplementation of graph states. Therefore the ZX-calculus augmented with\npivoting is strictly weaker than the calculus augmented with the Euler\ndecomposition of the Hadamard gate. We derive an angle-free version of the\nZX-calculus and show that it is complete for real stabilizer quantum mechanics. \n\n"}
{"id": "1308.0497", "contents": "Title: A note on Turing's 1936 Abstract: To a close reading of the original Turing article of 1936, we can learn it is\nbased on the claim to have defined a number which is not computable, arguing\nthat there can be no machine computing the diagonal on the enumeration of the\ncomputable sequences. This article carefully analyses the original 1936\nargument, displaying how it cannot be considered a demonstration, and that\nthere is indeed no evidence of such a defined number that is not computable. \n\n"}
{"id": "1308.4171", "contents": "Title: Towards an Effective Decision Procedure for LTL formulas with\n  Constraints Abstract: This paper presents an ongoing work that is part of a more wide-ranging\nproject whose final scope is to define a method to validate LTL formulas w.r.t.\na program written in the timed concurrent constraint language tccp, which is a\nlogic concurrent constraint language based on the concurrent constraint\nparadigm of Saraswat. Some inherent notions to tccp processes are\nnon-determinism, dealing with partial information in states and the monotonic\nevolution of the information. In order to check an LTL property for a process,\nour approach is based on the abstract diagnosis technique. The concluding step\nof this technique needs to check the validity of an LTL formula (with\nconstraints) in an effective way.\n  In this paper, we present a decision method for the validity of temporal\nlogic formulas (with constraints) built by our abstract diagnosis technique. \n\n"}
{"id": "1308.6774", "contents": "Title: Separable Approximations and Decomposition Methods for the Augmented\n  Lagrangian Abstract: In this paper we study decomposition methods based on separable\napproximations for minimizing the augmented Lagrangian. In particular, we study\nand compare the Diagonal Quadratic Approximation Method (DQAM) of Mulvey and\nRuszczy\\'{n}ski and the Parallel Coordinate Descent Method (PCDM) of\nRicht\\'arik and Tak\\'a\\v{c}. We show that the two methods are equivalent for\nfeasibility problems up to the selection of a single step-size parameter.\nFurthermore, we prove an improved complexity bound for PCDM under strong\nconvexity, and show that this bound is at least $8(L'/\\bar{L})(\\omega-1)^2$\ntimes better than the best known bound for DQAM, where $\\omega$ is the degree\nof partial separability and $L'$ and $\\bar{L}$ are the maximum and average of\nthe block Lipschitz constants of the gradient of the quadratic penalty\nappearing in the augmented Lagrangian. \n\n"}
{"id": "1309.0787", "contents": "Title: Online Tensor Methods for Learning Latent Variable Models Abstract: We introduce an online tensor decomposition based approach for two latent\nvariable modeling problems namely, (1) community detection, in which we learn\nthe latent communities that the social actors in social networks belong to, and\n(2) topic modeling, in which we infer hidden topics of text articles. We\nconsider decomposition of moment tensors using stochastic gradient descent. We\nconduct optimization of multilinear operations in SGD and avoid directly\nforming the tensors, to save computational and storage costs. We present\noptimized algorithm in two platforms. Our GPU-based implementation exploits the\nparallelism of SIMD architectures to allow for maximum speed-up by a careful\noptimization of storage and data transfer, whereas our CPU-based implementation\nuses efficient sparse matrix computations and is suitable for large sparse\ndatasets. For the community detection problem, we demonstrate accuracy and\ncomputational efficiency on Facebook, Yelp and DBLP datasets, and for the topic\nmodeling problem, we also demonstrate good performance on the New York Times\ndataset. We compare our results to the state-of-the-art algorithms such as the\nvariational method, and report a gain of accuracy and a gain of several orders\nof magnitude in the execution time. \n\n"}
{"id": "1309.1524", "contents": "Title: Guided Self-Organization of Input-Driven Recurrent Neural Networks Abstract: We review attempts that have been made towards understanding the\ncomputational properties and mechanisms of input-driven dynamical systems like\nRNNs, and reservoir computing networks in particular. We provide details on\nmethods that have been developed to give quantitative answers to the questions\nabove. Following this, we show how self-organization may be used to improve\nreservoirs for better performance, in some cases guided by the measures\npresented before. We also present a possible way to quantify task performance\nusing an information-theoretic approach, and finally discuss promising future\ndirections aimed at a better understanding of how these systems perform their\ncomputations and how to best guide self-organized processes for their\noptimization. \n\n"}
{"id": "1309.3783", "contents": "Title: SWIFT: Fast algorithms for multi-resolution SPH on multi-core\n  architectures Abstract: This paper describes a novel approach to neighbour-finding in Smoothed\nParticle Hydrodynamics (SPH) simulations with large dynamic range in smoothing\nlength. This approach is based on hierarchical cell decompositions, sorted\ninteractions, and a task-based formulation. It is shown to be faster than\ntraditional tree-based codes, and to scale better than domain\ndecomposition-based approaches on shared-memory parallel architectures such as\nmulti-cores. \n\n"}
{"id": "1309.4693", "contents": "Title: Modelling Probabilistic Wireless Networks Abstract: We propose a process calculus to model high level wireless systems, where the\ntopology of a network is described by a digraph. The calculus enjoys features\nwhich are proper of wireless networks, namely broadcast communication and\nprobabilistic behaviour. We first focus on the problem of composing wireless\nnetworks, then we present a compositional theory based on a probabilistic\ngeneralisation of the well known may-testing and must-testing pre- orders.\nAlso, we define an extensional semantics for our calculus, which will be used\nto define both simulation and deadlock simulation preorders for wireless\nnetworks. We prove that our simulation preorder is sound with respect to the\nmay-testing preorder; similarly, the deadlock simulation pre- order is sound\nwith respect to the must-testing preorder, for a large class of networks. We\nalso provide a counterexample showing that completeness of the simulation\npreorder, with respect to the may testing one, does not hold. We conclude the\npaper with an application of our theory to probabilistic routing protocols. \n\n"}
{"id": "1310.0794", "contents": "Title: Formal verification in Coq of program properties involving the global\n  state effect Abstract: The syntax of an imperative language does not mention explicitly the state,\nwhile its denotational semantics has to mention it. In this paper we present a\nframework for the verification in Coq of properties of programs manipulating\nthe global state effect. These properties are expressed in a proof system which\nis close to the syntax, as in effect systems, in the sense that the state does\nnot appear explicitly in the type of expressions which manipulate it. Rather,\nthe state appears via decorations added to terms and to equations. In this\nsystem, proofs of programs thus present two aspects: properties can be verified\n{\\em up to effects} or the effects can be taken into account. The design of our\nCoq library consequently reflects these two aspects: our framework is centered\naround the construction of two inductive and dependent types, one for terms up\nto effects and one for the manipulation of decorations. \n\n"}
{"id": "1310.3438", "contents": "Title: On Optimal Probabilities in Stochastic Coordinate Descent Methods Abstract: We propose and analyze a new parallel coordinate descent method---`NSync---in\nwhich at each iteration a random subset of coordinates is updated, in parallel,\nallowing for the subsets to be chosen non-uniformly. We derive convergence\nrates under a strong convexity assumption, and comment on how to assign\nprobabilities to the sets to optimize the bound. The complexity and practical\nperformance of the method can outperform its uniform variant by an order of\nmagnitude. Surprisingly, the strategy of updating a single randomly selected\ncoordinate per iteration---with optimal probabilities---may require less\niterations, both in theory and practice, than the strategy of updating all\ncoordinates at every iteration. \n\n"}
{"id": "1310.4502", "contents": "Title: 2HOT: An Improved Parallel Hashed Oct-Tree N-Body Algorithm for\n  Cosmological Simulation Abstract: We report on improvements made over the past two decades to our adaptive\ntreecode N-body method (HOT). A mathematical and computational approach to the\ncosmological N-body problem is described, with performance and scalability\nmeasured up to 256k ($2^{18}$) processors. We present error analysis and\nscientific application results from a series of more than ten 69 billion\n($4096^3$) particle cosmological simulations, accounting for $4 \\times 10^{20}$\nfloating point operations. These results include the first simulations using\nthe new constraints on the standard model of cosmology from the Planck\nsatellite. Our simulations set a new standard for accuracy and scientific\nthroughput, while meeting or exceeding the computational efficiency of the\nlatest generation of hybrid TreePM N-body methods. \n\n"}
{"id": "1310.7556", "contents": "Title: First Evaluation of the CPU, GPGPU and MIC Architectures for Real Time\n  Particle Tracking based on Hough Transform at the LHC Abstract: Recent innovations focused around {\\em parallel} processing, either through\nsystems containing multiple processors or processors containing multiple cores,\nhold great promise for enhancing the performance of the trigger at the LHC and\nextending its physics program. The flexibility of the CMS/ATLAS trigger system\nallows for easy integration of computational accelerators, such as NVIDIA's\nTesla Graphics Processing Unit (GPU) or Intel's \\xphi, in the High Level\nTrigger. These accelerators have the potential to provide faster or more energy\nefficient event selection, thus opening up possibilities for new complex\ntriggers that were not previously feasible. At the same time, it is crucial to\nexplore the performance limits achievable on the latest generation multicore\nCPUs with the use of the best software optimization methods. In this article, a\nnew tracking algorithm based on the Hough transform will be evaluated for the\nfirst time on a multi-core Intel Xeon E5-2697v2 CPU, an NVIDIA Tesla K20c GPU,\nand an Intel \\xphi\\ 7120 coprocessor. Preliminary time performance will be\npresented. \n\n"}
{"id": "1312.2552", "contents": "Title: Abstract Interpretation of Temporal Concurrent Constraint Programs Abstract: Timed Concurrent Constraint Programming (tcc) is a declarative model for\nconcurrency offering a logic for specifying reactive systems, i.e. systems that\ncontinuously interact with the environment. The universal tcc formalism (utcc)\nis an extension of tcc with the ability to express mobility. Here mobility is\nunderstood as communication of private names as typically done for mobile\nsystems and security protocols. In this paper we consider the denotational\nsemantics for tcc, and we extend it to a \"collecting\" semantics for utcc based\non closure operators over sequences of constraints. Relying on this semantics,\nwe formalize a general framework for data flow analyses of tcc and utcc\nprograms by abstract interpretation techniques. The concrete and abstract\nsemantics we propose are compositional, thus allowing us to reduce the\ncomplexity of data flow analyses. We show that our method is sound and\nparametric with respect to the abstract domain. Thus, different analyses can be\nperformed by instantiating the framework. We illustrate how it is possible to\nreuse abstract domains previously defined for logic programming to perform, for\ninstance, a groundness analysis for tcc programs. We show the applicability of\nthis analysis in the context of reactive systems. Furthermore, we make also use\nof the abstract semantics to exhibit a secrecy flaw in a security protocol. We\nalso show how it is possible to make an analysis which may show that tcc\nprograms are suspension free. This can be useful for several purposes, such as\nfor optimizing compilation or for debugging. \n\n"}
{"id": "1312.3020", "contents": "Title: Sparse Allreduce: Efficient Scalable Communication for Power-Law Data Abstract: Many large datasets exhibit power-law statistics: The web graph, social\nnetworks, text data, click through data etc. Their adjacency graphs are termed\nnatural graphs, and are known to be difficult to partition. As a consequence\nmost distributed algorithms on these graphs are communication intensive. Many\nalgorithms on natural graphs involve an Allreduce: a sum or average of\npartitioned data which is then shared back to the cluster nodes. Examples\ninclude PageRank, spectral partitioning, and many machine learning algorithms\nincluding regression, factor (topic) models, and clustering. In this paper we\ndescribe an efficient and scalable Allreduce primitive for power-law data. We\npoint out scaling problems with existing butterfly and round-robin networks for\nSparse Allreduce, and show that a hybrid approach improves on both.\nFurthermore, we show that Sparse Allreduce stages should be nested instead of\ncascaded (as in the dense case). And that the optimum throughput Allreduce\nnetwork should be a butterfly of heterogeneous degree where degree decreases\nwith depth into the network. Finally, a simple replication scheme is introduced\nto deal with node failures. We present experiments showing significant\nimprovements over existing systems such as PowerGraph and Hadoop. \n\n"}
{"id": "1312.3797", "contents": "Title: Infinite Games Specified by 2-Tape Automata Abstract: We prove that the determinacy of Gale-Stewart games whose winning sets are\ninfinitary rational relations accepted by 2-tape B\\\"uchi automata is equivalent\nto the determinacy of (effective) analytic Gale-Stewart games which is known to\nbe a large cardinal assumption. Then we prove that winning strategies, when\nthey exist, can be very complex, i.e. highly non-effective, in these games. We\nprove the same results for Gale-Stewart games with winning sets accepted by\nreal-time 1-counter B\\\"uchi automata, then extending previous results obtained\nabout these games. Then we consider the strenghs of determinacy for these\ngames, and we prove that there is a transfinite sequence of 2-tape B\\\"uchi\nautomata (respectively, of real-time 1-counter B\\\"uchi automata) $A_\\alpha$,\nindexed by recursive ordinals, such that the games $G(L(A_\\alpha))$ have\nstrictly increasing strenghs of determinacy. Moreover there is a 2-tape B\\\"uchi\nautomaton (respectively, a real-time 1-counter B\\\"uchi automaton) B such that\nthe determinacy of G(L(B)) is equivalent to the (effective) analytic\ndeterminacy and thus has the maximal strength of determinacy. We show also that\nthe determinacy of Wadge games between two players in charge of infinitary\nrational relations accepted by 2-tape B\\\"uchi automata is equivalent to the\n(effective) analytic determinacy, and thus not provable in ZFC. \n\n"}
{"id": "1312.3938", "contents": "Title: Transparent Checkpoint-Restart over InfiniBand Abstract: InfiniBand is widely used for low-latency, high-throughput cluster computing.\nSaving the state of the InfiniBand network as part of distributed checkpointing\nhas been a long-standing challenge for researchers. Because of a lack of a\nsolution, typical MPI implementations have included custom checkpoint-restart\nservices that \"tear down\" the network, checkpoint each node as if the node were\na standalone computer, and then re-connect the network again. We present the\nfirst example of transparent, system-initiated checkpoint-restart that directly\nsupports InfiniBand. The new approach is independent of any particular Linux\nkernel, thus simplifying the current practice of using a kernel-based module,\nsuch as BLCR. This direct approach results in checkpoints that are found to be\nfaster than with the use of a checkpoint-restart service. The generality of\nthis approach is shown not only by checkpointing an MPI computation, but also a\nnative UPC computation (Berkeley Unified Parallel C), which does not use MPI.\nScalability is shown by checkpointing 2,048 MPI processes across 128 nodes\n(with 16 cores per node). In addition, a cost-effective debugging approach is\nalso enabled, in which a checkpoint image from an InfiniBand-based production\ncluster is copied to a local Ethernet-based cluster, where it can be restarted\nand an interactive debugger can be attached to it. This work is based on a\nplugin that extends the DMTCP (Distributed MultiThreaded CheckPointing)\ncheckpoint-restart package. \n\n"}
{"id": "1312.4722", "contents": "Title: Big Data Computing and Clouds: Trends and Future Directions Abstract: This paper discusses approaches and environments for carrying out analytics\non Clouds for Big Data applications. It revolves around four important areas of\nanalytics and Big Data, namely (i) data management and supporting\narchitectures; (ii) model development and scoring; (iii) visualisation and user\ninteraction; and (iv) business models. Through a detailed survey, we identify\npossible gaps in technology and provide recommendations for the research\ncommunity on future directions on Cloud-supported Big Data computing and\nanalytics solutions. \n\n"}
{"id": "1312.4892", "contents": "Title: A Fast Algorithm for Sparse Controller Design Abstract: We consider the task of designing sparse control laws for large-scale systems\nby directly minimizing an infinite horizon quadratic cost with an $\\ell_1$\npenalty on the feedback controller gains. Our focus is on an improved algorithm\nthat allows us to scale to large systems (i.e. those where sparsity is most\nuseful) with convergence times that are several orders of magnitude faster than\nexisting algorithms. In particular, we develop an efficient proximal Newton\nmethod which minimizes per-iteration cost with a coordinate descent active set\napproach and fast numerical solutions to the Lyapunov equations. Experimentally\nwe demonstrate the appeal of this approach on synthetic examples and real power\nnetworks significantly larger than those previously considered in the\nliterature. \n\n"}
{"id": "1312.7275", "contents": "Title: Arithmetical Foundations - Recursion. Evaluation. Consistency Abstract: Primitive recursion, mu-recursion, universal object and universe theories,\ncomplexity controlled iteration, code evaluation, soundness, decidability,\nG\\\"odel incompleteness theorems, inconsistency provability for set theory,\nconstructive consistency. \n\n"}
{"id": "1312.7305", "contents": "Title: Probabilistic Computability and Choice Abstract: We study the computational power of randomized computations on infinite\nobjects, such as real numbers. In particular, we introduce the concept of a Las\nVegas computable multi-valued function, which is a function that can be\ncomputed on a probabilistic Turing machine that receives a random binary\nsequence as auxiliary input. The machine can take advantage of this random\nsequence, but it always has to produce a correct result or to stop the\ncomputation after finite time if the random advice is not successful. With\npositive probability the random advice has to be successful. We characterize\nthe class of Las Vegas computable functions in the Weihrauch lattice with the\nhelp of probabilistic choice principles and Weak Weak K\\H{o}nig's Lemma. Among\nother things we prove an Independent Choice Theorem that implies that Las Vegas\ncomputable functions are closed under composition. In a case study we show that\nNash equilibria are Las Vegas computable, while zeros of continuous functions\nwith sign changes cannot be computed on Las Vegas machines. However, we show\nthat the latter problem admits randomized algorithms with weaker failure\nrecognition mechanisms. The last mentioned results can be interpreted such that\nthe Intermediate Value Theorem is reducible to the jump of Weak Weak\nK\\H{o}nig's Lemma, but not to Weak Weak K\\H{o}nig's Lemma itself. These\nexamples also demonstrate that Las Vegas computable functions form a proper\nsuperclass of the class of computable functions and a proper subclass of the\nclass of non-deterministically computable functions. We also study the impact\nof specific lower bounds on the success probabilities, which leads to a strict\nhierarchy of classes. In particular, the classical technique of probability\namplification fails for computations on infinite objects. We also investigate\nthe dependency on the underlying probability space. \n\n"}
{"id": "1401.1393", "contents": "Title: Domain Representations Induced by Dyadic Subbases Abstract: We study domain representations induced by dyadic subbases and show that a\nproper dyadic subbase S of a second-countable regular space X induces an\nembedding of X in the set of minimal limit elements of a subdomain D of\n$\\{0,1,\\perp\\}\\omega$. In particular, if X is compact, then X is a retract of\nthe set of limit elements of D. \n\n"}
{"id": "1401.3733", "contents": "Title: BSMBench: a flexible and scalable supercomputer benchmark from\n  computational particle physics Abstract: Lattice Quantum ChromoDynamics (QCD), and by extension its parent field,\nLattice Gauge Theory (LGT), make up a significant fraction of supercomputing\ncycles worldwide. As such, it would be irresponsible not to evaluate machines'\nsuitability for such applications. To this end, a benchmark has been developed\nto assess the performance of LGT applications on modern HPC platforms. Distinct\nfrom previous QCD-based benchmarks, this allows probing the behaviour of a\nvariety of theories, which allows varying the ratio of demands between on-node\ncomputations and inter-node communications. The results of testing this\nbenchmark on various recent HPC platforms are presented, and directions for\nfuture development are discussed. \n\n"}
{"id": "1401.7234", "contents": "Title: Propositional dynamic logic for searching games with errors Abstract: We investigate some finitely-valued generalizations of propositional dynamic\nlogic with tests. We start by introducing the (n+1)-valued Kripke models and a\ncorresponding language based on a modal extension of {\\L}ukasiewicz many-valued\nlogic. We illustrate the definitions by providing a framework for an analysis\nof the R\\'enyi - Ulam searching game with errors.\n  Our main result is the axiomatization of the theory of the (n+1)-valued\nKripke models. This result is obtained through filtration of the canonical\nmodel of the smallest (n+1)-valued propositional dynamic logic. \n\n"}
{"id": "1402.2676", "contents": "Title: Ranking via Robust Binary Classification and Parallel Parameter\n  Estimation in Large-Scale Data Abstract: We propose RoBiRank, a ranking algorithm that is motivated by observing a\nclose connection between evaluation metrics for learning to rank and loss\nfunctions for robust classification. The algorithm shows a very competitive\nperformance on standard benchmark datasets against other representative\nalgorithms in the literature. On the other hand, in large scale problems where\nexplicit feature vectors and scores are not given, our algorithm can be\nefficiently parallelized across a large number of machines; for a task that\nrequires 386,133 x 49,824,519 pairwise interactions between items to be ranked,\nour algorithm finds solutions that are of dramatically higher quality than that\ncan be found by a state-of-the-art competitor algorithm, given the same amount\nof wall-clock time for computation. \n\n"}
{"id": "1402.3281", "contents": "Title: Partitioning Complex Networks via Size-constrained Clustering Abstract: The most commonly used method to tackle the graph partitioning problem in\npractice is the multilevel approach. During a coarsening phase, a multilevel\ngraph partitioning algorithm reduces the graph size by iteratively contracting\nnodes and edges until the graph is small enough to be partitioned by some other\nalgorithm. A partition of the input graph is then constructed by successively\ntransferring the solution to the next finer graph and applying a local search\nalgorithm to improve the current solution.\n  In this paper, we describe a novel approach to partition graphs effectively\nespecially if the networks have a highly irregular structure. More precisely,\nour algorithm provides graph coarsening by iteratively contracting\nsize-constrained clusterings that are computed using a label propagation\nalgorithm. The same algorithm that provides the size-constrained clusterings\ncan also be used during uncoarsening as a fast and simple local search\nalgorithm.\n  Depending on the algorithm's configuration, we are able to compute partitions\nof very high quality outperforming all competitors, or partitions that are\ncomparable to the best competitor in terms of quality, hMetis, while being\nnearly an order of magnitude faster on average. The fastest configuration\npartitions the largest graph available to us with 3.3 billion edges using a\nsingle machine in about ten minutes while cutting less than half of the edges\nthan the fastest competitor, kMetis. \n\n"}
{"id": "1402.6889", "contents": "Title: Lazy Model Expansion: Interleaving Grounding with Search Abstract: Finding satisfying assignments for the variables involved in a set of\nconstraints can be cast as a (bounded) model generation problem: search for\n(bounded) models of a theory in some logic. The state-of-the-art approach for\nbounded model generation for rich knowledge representation languages, like ASP,\nFO(.) and Zinc, is ground-and-solve: reduce the theory to a ground or\npropositional one and apply a search algorithm to the resulting theory.\n  An important bottleneck is the blowup of the size of the theory caused by the\nreduction phase. Lazily grounding the theory during search is a way to overcome\nthis bottleneck. We present a theoretical framework and an implementation in\nthe context of the FO(.) knowledge representation language. Instead of\ngrounding all parts of a theory, justifications are derived for some parts of\nit. Given a partial assignment for the grounded part of the theory and valid\njustifications for the formulas of the non-grounded part, the justifications\nprovide a recipe to construct a complete assignment that satisfies the\nnon-grounded part. When a justification for a particular formula becomes\ninvalid during search, a new one is derived; if that fails, the formula is\nsplit in a part to be grounded and a part that can be justified.\n  The theoretical framework captures existing approaches for tackling the\ngrounding bottleneck such as lazy clause generation and grounding-on-the-fly,\nand presents a generalization of the 2-watched literal scheme. We present an\nalgorithm for lazy model expansion and integrate it in a model generator for\nFO(ID), a language extending first-order logic with inductive definitions. The\nalgorithm is implemented as part of the state-of-the-art FO(ID) Knowledge-Base\nSystem IDP. Experimental results illustrate the power and generality of the\napproach. \n\n"}
{"id": "1402.6964", "contents": "Title: Scalable methods for nonnegative matrix factorizations of near-separable\n  tall-and-skinny matrices Abstract: Numerous algorithms are used for nonnegative matrix factorization under the\nassumption that the matrix is nearly separable. In this paper, we show how to\nmake these algorithms efficient for data matrices that have many more rows than\ncolumns, so-called \"tall-and-skinny matrices\". One key component to these\nimproved methods is an orthogonal matrix transformation that preserves the\nseparability of the NMF problem. Our final methods need a single pass over the\ndata matrix and are suitable for streaming, multi-core, and MapReduce\narchitectures. We demonstrate the efficacy of these algorithms on\nterabyte-sized synthetic matrices and real-world matrices from scientific\ncomputing and bioinformatics. \n\n"}
{"id": "1403.0603", "contents": "Title: Efficient Distributed Online Prediction and Stochastic Optimization with\n  Approximate Distributed Averaging Abstract: We study distributed methods for online prediction and stochastic\noptimization. Our approach is iterative: in each round nodes first perform\nlocal computations and then communicate in order to aggregate information and\nsynchronize their decision variables. Synchronization is accomplished through\nthe use of a distributed averaging protocol. When an exact distributed\naveraging protocol is used, it is known that the optimal regret bound of\n$\\mathcal{O}(\\sqrt{m})$ can be achieved using the distributed mini-batch\nalgorithm of Dekel et al. (2012), where $m$ is the total number of samples\nprocessed across the network. We focus on methods using approximate distributed\naveraging protocols and show that the optimal regret bound can also be achieved\nin this setting. In particular, we propose a gossip-based optimization method\nwhich achieves the optimal regret bound. The amount of communication required\ndepends on the network topology through the second largest eigenvalue of the\ntransition matrix of a random walk on the network. In the setting of stochastic\noptimization, the proposed gossip-based approach achieves nearly-linear\nscaling: the optimization error is guaranteed to be no more than $\\epsilon$\nafter $\\mathcal{O}(\\frac{1}{n \\epsilon^2})$ rounds, each of which involves\n$\\mathcal{O}(\\log n)$ gossip iterations, when nodes communicate over a\nwell-connected graph. This scaling law is also observed in numerical\nexperiments on a cluster. \n\n"}
{"id": "1403.4099", "contents": "Title: High-speed detection of emergent market clustering via an unsupervised\n  parallel genetic algorithm Abstract: We implement a master-slave parallel genetic algorithm (PGA) with a bespoke\nlog-likelihood fitness function to identify emergent clusters within price\nevolutions. We use graphics processing units (GPUs) to implement a PGA and\nvisualise the results using disjoint minimal spanning trees (MSTs). We\ndemonstrate that our GPU PGA, implemented on a commercially available general\npurpose GPU, is able to recover stock clusters in sub-second speed, based on a\nsubset of stocks in the South African market. This represents a pragmatic\nchoice for low-cost, scalable parallel computing and is significantly faster\nthan a prototype serial implementation in an optimised C-based\nfourth-generation programming language, although the results are not directly\ncomparable due to compiler differences. Combined with fast online intraday\ncorrelation matrix estimation from high frequency data for cluster\nidentification, the proposed implementation offers cost-effective,\nnear-real-time risk assessment for financial practitioners. \n\n"}
{"id": "1403.6270", "contents": "Title: Distributed Edge Partitioning for Graph Processing Abstract: The availability of larger and larger graph datasets, growing exponentially\nover the years, has created several new algorithmic challenges to be addressed.\nSequential approaches have become unfeasible, while interest on parallel and\ndistributed algorithms has greatly increased.\n  Appropriately partitioning the graph as a preprocessing step can improve the\ndegree of parallelism of its analysis. A number of heuristic algorithms have\nbeen developed to solve this problem, but many of them subdivide the graph on\nits vertex set, thus obtaining a vertex-partitioned graph.\n  Aim of this paper is to explore a completely different approach based on edge\npartitioning, in which edges, rather than vertices, are partitioned into\ndisjoint subsets. Contribution of this paper is twofold: first, we introduce a\ngraph processing framework based on edge partitioning, that is flexible enough\nto be applied to several different graph problems. Second, we show the\nfeasibility of these ideas by presenting a distributed edge partitioning\nalgorithm called d-fep.\n  Our framework is thoroughly evaluated, using both simulations and an Hadoop\nimplementation running on the Amazon EC2 cloud. The experiments show that d-fep\nis efficient, scalable and obtains consistently good partitions. The resulting\nedge-partitioned graph can be exploited to obtain more efficient\nimplementations of graph analysis algorithms. \n\n"}
{"id": "1404.0144", "contents": "Title: Modal Independence Logic Abstract: This paper introduces modal independence logic MIL, a modal logic that can\nexplicitly talk about independence among propositional variables. Formulas of\nMIL are not evaluated in worlds but in sets of worlds, so called teams. In this\nvein, MIL can be seen as a variant of V\\\"a\\\"an\\\"anen's modal dependence logic\nMDL. We show that MIL embeds MDL and is strictly more expressive. However, on\nsingleton teams, MIL is shown to be not more expressive than usual modal logic,\nbut MIL is exponentially more succinct. Making use of a new form of\nbisimulation, we extend these expressivity results to modal logics extended by\nvarious generalized dependence atoms. We demonstrate the expressive power of\nMIL by giving a specification of the anonymity requirement of the dining\ncryptographers protocol in MIL. We also study complexity issues of MIL and show\nthat, though it is more expressive, its satisfiability and model checking\nproblem have the same complexity as for MDL. \n\n"}
{"id": "1404.0816", "contents": "Title: On Pocrims and Hoops Abstract: Pocrims and suitable specialisations thereof are structures that provide the\nnatural algebraic semantics for a minimal affine logic and its extensions.\nHoops comprise a special class of pocrims that provide algebraic semantics for\nwhat we view as an intuitionistic analogue of the classical multi-valued\n{\\L}ukasiewicz logic. We present some contributions to the theory of these\nalgebraic structures. We give a new proof that the class of hoops is a variety.\nWe use a new indirect method to establish several important identities in the\ntheory of hoops: in particular, we prove that the double negation mapping in a\nhoop is a homormorphism. This leads to an investigation of algebraic analogues\nof the various double negation translations that are well-known from proof\ntheory. We give an algebraic framework for studying the semantics of double\nnegation translations and use it to prove new results about the applicability\nof the double negation translations due to Gentzen and Glivenko. \n\n"}
{"id": "1404.0837", "contents": "Title: Reasoning about Knowledge and Strategies: Epistemic Strategy Logic Abstract: In this paper we introduce Epistemic Strategy Logic (ESL), an extension of\nStrategy Logic with modal operators for individual knowledge. This enhanced\nframework allows us to represent explicitly and to reason about the knowledge\nagents have of their own and other agents' strategies. We provide a semantics\nto ESL in terms of epistemic concurrent game models, and consider the\ncorresponding model checking problem. We show that the complexity of model\nchecking ESL is not worse than (non-epistemic) Strategy Logic \n\n"}
{"id": "1404.2644", "contents": "Title: A Distributed Frank-Wolfe Algorithm for Communication-Efficient Sparse\n  Learning Abstract: Learning sparse combinations is a frequent theme in machine learning. In this\npaper, we study its associated optimization problem in the distributed setting\nwhere the elements to be combined are not centrally located but spread over a\nnetwork. We address the key challenges of balancing communication costs and\noptimization errors. To this end, we propose a distributed Frank-Wolfe (dFW)\nalgorithm. We obtain theoretical guarantees on the optimization error\n$\\epsilon$ and communication cost that do not depend on the total number of\ncombining elements. We further show that the communication cost of dFW is\noptimal by deriving a lower-bound on the communication cost required to\nconstruct an $\\epsilon$-approximate solution. We validate our theoretical\nanalysis with empirical studies on synthetic and real-world data, which\ndemonstrate that dFW outperforms both baselines and competing methods. We also\nstudy the performance of dFW when the conditions of our analysis are relaxed,\nand show that dFW is fairly robust. \n\n"}
{"id": "1404.7169", "contents": "Title: Revisiting the Complexity of Stability of Continuous and Hybrid Systems Abstract: We develop a framework to give upper bounds on the \"practical\" computational\ncomplexity of stability problems for a wide range of nonlinear continuous and\nhybrid systems. To do so, we describe stability properties of dynamical systems\nusing first-order formulas over the real numbers, and reduce stability problems\nto the delta-decision problems of these formulas. The framework allows us to\nobtain a precise characterization of the complexity of different notions of\nstability for nonlinear continuous and hybrid systems. We prove that bounded\nversions of the stability problems are generally decidable, and give upper\nbounds on their complexity. The unbounded versions are generally undecidable,\nfor which we give upper bounds on their degrees of unsolvability. \n\n"}
{"id": "1404.7634", "contents": "Title: Testing Temporal Connectivity in Sparse Dynamic Graphs Abstract: We address the problem of testing whether a given dynamic graph is temporally\nconnected, {\\it i.e} a temporal path (also called a {\\em journey}) exists\nbetween all pairs of vertices. We consider a discrete version of the problem,\nwhere the topology is given as an evolving graph ${\\cal\nG}=\\{G_1,G_2,...,G_{k}\\}$ whose set of vertices is invariant and the set of\n(directed) edges varies over time. Two cases are studied, depending on whether\na single edge or an unlimited number of edges can be crossed in a same $G_i$\n(strict journeys {\\it vs} non-strict journeys).\n  In the case of {\\em strict} journeys, a number of existing algorithms\ndesigned for more general problems can be adapted. We adapt one of them to the\nabove formulation of the problem and characterize its running time complexity.\nThe parameters of interest are the length of the graph sequence $k=|{\\cal G}|$,\nthe maximum {\\em instant} density $\\mu=max(|E_i|)$, and the {\\em cumulated}\ndensity $m=|\\cup E_i|$. Our algorithm has a time complexity of $O(k\\mu n)$,\nwhere $n$ is the number of nodes. This complexity is compared to that of the\nother solutions: one is always more costly (keep in mind that is solves a more\ngeneral problem), the other one is more or less costly depending on the\ninterplay between instant density and cumulated density. The length $k$ of the\nsequence also plays a role. We characterize the key values of $k, \\mu$ and $m$\nfor which either algorithm should be used.\n  In the case of {\\em non-strict} journeys, for which no algorithm is known, we\nshow that some pre-processing of the input graph allows us to re-use the same\nalgorithm than before. By chance, these operations happens to cost again\n$O(k\\mu n)$ time, which implies that the second problem is not more difficult\nthan the first. \n\n"}
{"id": "1405.0386", "contents": "Title: Fatal Attractors in Parity Games: Building Blocks for Partial Solvers Abstract: Attractors in parity games are a technical device for solving \"alternating\"\nreachability of given node sets. A well known solver of parity games -\nZielonka's algorithm - uses such attractor computations recursively. We here\npropose new forms of attractors that are monotone in that they are aware of\nspecific static patterns of colors encountered in reaching a given node set in\nalternating fashion. Then we demonstrate how these new forms of attractors can\nbe embedded within greatest fixed-point computations to design solvers of\nparity games that run in polynomial time but are partial in that they may not\ndecide the winning status of all nodes in the input game.\n  Experimental results show that our partial solvers completely solve\nbenchmarks that were constructed to challenge existing full solvers. Our\npartial solvers also have encouraging run times in practice. For one partial\nsolver we prove that its run-time is at most cubic in the number of nodes in\nthe parity game, that its output game is independent of the order in which\nmonotone attractors are computed, and that it solves all Buechi games and weak\ngames.\n  We then define and study a transformation that converts partial solvers into\nmore precise partial solvers, and we prove that this transformation is sound\nunder very reasonable conditions on the input partial solvers. Noting that one\nof our partial solvers meets these conditions, we apply its transformation on\n1.6 million randomly generated games and so experimentally validate that the\ntransformation can be very effective in increasing the precision of partial\nsolvers. \n\n"}
{"id": "1405.1229", "contents": "Title: Three Semantics for Modular Systems Abstract: In this paper, we further develop the framework of Modular Systems that lays\nmodel-theoretic foundations for combining different declarative languages,\nagents and solvers. We introduce a multi-language logic of modular systems. We\ndefine two novel semantics, a structural operational semantics, and an\ninference-based semantics. We prove the new semantics are equivalent to the\noriginal model-theoretic semantics and describe future research directions. \n\n"}
{"id": "1405.2642", "contents": "Title: An Abductive Framework for Horn Knowledge Base Dynamics Abstract: The dynamics of belief and knowledge is one of the major components of any\nautonomous system that should be able to incorporate new pieces of information.\nWe introduced the Horn knowledge base dynamics to deal with two important\npoints: first, to handle belief states that need not be deductively closed; and\nthe second point is the ability to declare certain parts of the belief as\nimmutable. In this paper, we address another, radically new approach to this\nproblem. This approach is very close to the Hansson's dyadic representation of\nbelief. Here, we consider the immutable part as defining a new logical system.\nBy a logical system, we mean that it defines its own consequence relation and\nclosure operator. Based on this, we provide an abductive framework for Horn\nknowledge base dynamics. \n\n"}
{"id": "1405.3623", "contents": "Title: Mining State-Based Models from Proof Corpora Abstract: Interactive theorem provers have been used extensively to reason about\nvarious software/hardware systems and mathematical theorems. The key challenge\nwhen using an interactive prover is finding a suitable sequence of proof steps\nthat will lead to a successful proof requires a significant amount of human\nintervention. This paper presents an automated technique that takes as input\nexamples of successful proofs and infers an Extended Finite State Machine as\noutput. This can in turn be used to generate proofs of new conjectures. Our\npreliminary experiments show that the inferred models are generally accurate\n(contain few false-positive sequences) and that representing existing proofs in\nsuch a way can be very useful when guiding new ones. \n\n"}
{"id": "1405.3792", "contents": "Title: Minimum Model Semantics for Extensional Higher-order Logic Programming\n  with Negation Abstract: Extensional higher-order logic programming has been introduced as a\ngeneralization of classical logic programming. An important characteristic of\nthis paradigm is that it preserves all the well-known properties of traditional\nlogic programming. In this paper we consider the semantics of negation in the\ncontext of the new paradigm. Using some recent results from non-monotonic\nfixed-point theory, we demonstrate that every higher-order logic program with\nnegation has a unique minimum infinite-valued model. In this way we obtain the\nfirst purely model-theoretic semantics for negation in extensional higher-order\nlogic programming. Using our approach, we resolve an old paradox that was\nintroduced by W. W. Wadge in order to demonstrate the semantic difficulties of\nhigher-order logic programming. \n\n"}
{"id": "1405.5145", "contents": "Title: Set Consensus: Captured by a Set of Runs with Ramifications Abstract: Are (set)-consensus objects necessary? This paper answer is negative.\n  We show that the availability of consensus objects can be replaced by\nrestricting the set of runs we consider. In particular we concentrate of the\nset of runs of the Immediate-Snapshot-Model (IIS), and given the object we\nidentify this restricted subset of IIS runs.\n  We further show that given an $(m,k)$-set consensus, an object that provides\n$k$-set consensus among $m$ processors, in a system of $n$, $n>m$ processors,\nwe do not need to use the precise power of the objects but rather their\neffective cumulative set consensus power. E.g. when $n=3, m=2,$ and $k=1$ and\nall the 3 processors are active then we only use 2-set consensus among the 3\nprocessors, as if 2-processors consensus is not available. We do this until at\nleast one of the 3 processors obtains an output. We show that this suggests a\nnew direction in the design of algorithms when consensus objects are involved. \n\n"}
{"id": "1405.5646", "contents": "Title: Mathematical Programming Strategies for Solving the Minimum Common\n  String Partition Problem Abstract: The minimum common string partition problem is an NP-hard combinatorial\noptimization problem with applications in computational biology. In this work\nwe propose the first integer linear programming model for solving this problem.\nMoreover, on the basis of the integer linear programming model we develop a\ndeterministic 2-phase heuristic which is applicable to larger problem\ninstances. The results show that provenly optimal solutions can be obtained for\nproblem instances of small and medium size from the literature by solving the\nproposed integer linear programming model with CPLEX. Furthermore, new\nbest-known solutions are obtained for all considered problem instances from the\nliterature. Concerning the heuristic, we were able to show that it outperforms\nheuristic competitors from the related literature. \n\n"}
{"id": "1405.7923", "contents": "Title: Bisimulation Equivalence of First-Order Grammars Abstract: A decidability proof for bisimulation equivalence of first-order grammars\n(finite sets of labelled rules for rewriting roots of first-order terms) is\npresented. The equivalence generalizes the DPDA (deterministic pushdown\nautomata) equivalence, and the result corresponds to the result achieved by\nSenizergues (1998, 2005) in the framework of equational graphs, or of PDA with\nrestricted epsilon-steps. The framework of classical first-order terms seems\nparticularly useful for providing a proof that should be understandable for a\nwider audience. We also discuss an extension to branching bisimilarity,\nannounced by Fu and Yin (2014). \n\n"}
{"id": "1406.1974", "contents": "Title: Communication Complexity of the Fast Multipole Method and its Algebraic\n  Variants Abstract: A combination of hierarchical tree-like data structures and data access\npatterns from fast multipole methods and hierarchical low-rank approximation of\nlinear operators from H-matrix methods appears to form an algorithmic path\nforward for efficient implementation of many linear algebraic operations of\nscientific computing at the exascale. The combination provides asymptotically\noptimal computational and communication complexity and applicability to large\nclasses of operators that commonly arise in scientific computing applications.\nA convergence of the mathematical theories of the fast multipole and H-matrix\nmethods has been underway for over a decade. We recap this mathematical\nunification and describe implementation aspects of a hybrid of these two\ncompelling hierarchical algorithms on hierarchical distributed-shared memory\narchitectures, which are likely to be the first to reach the exascale. We\npresent a new communication complexity estimate for fast multipole methods on\nsuch architectures. We also show how the data structures and access patterns of\nH-matrices for low-rank operators map onto those of fast multipole, leading to\nan algebraically generalized form of fast multipole that compromises none of\nits architecturally ideal properties. \n\n"}
{"id": "1406.2602", "contents": "Title: Graph Approximation and Clustering on a Budget Abstract: We consider the problem of learning from a similarity matrix (such as\nspectral clustering and lowd imensional embedding), when computing pairwise\nsimilarities are costly, and only a limited number of entries can be observed.\nWe provide a theoretical analysis using standard notions of graph\napproximation, significantly generalizing previous results (which focused on\nspectral clustering with two clusters). We also propose a new algorithmic\napproach based on adaptive sampling, which experimentally matches or improves\non previous methods, while being considerably more general and computationally\ncheaper. \n\n"}
{"id": "1406.7386", "contents": "Title: Contextual Semantics: From Quantum Mechanics to Logic, Databases,\n  Constraints, and Complexity Abstract: We discuss quantum non-locality and contextuality, emphasising logical and\nstructural aspects. We also show how the same mathematical structures arise in\nvarious areas of classical computation. \n\n"}
{"id": "1407.2002", "contents": "Title: Discovering Beaten Paths in Collaborative Ontology-Engineering Projects\n  using Markov Chains Abstract: Biomedical taxonomies, thesauri and ontologies in the form of the\nInternational Classification of Diseases (ICD) as a taxonomy or the National\nCancer Institute Thesaurus as an OWL-based ontology, play a critical role in\nacquiring, representing and processing information about human health. With\nincreasing adoption and relevance, biomedical ontologies have also\nsignificantly increased in size. For example, the 11th revision of the ICD,\nwhich is currently under active development by the WHO contains nearly 50,000\nclasses representing a vast variety of different diseases and causes of death.\nThis evolution in terms of size was accompanied by an evolution in the way\nontologies are engineered. Because no single individual has the expertise to\ndevelop such large-scale ontologies, ontology-engineering projects have evolved\nfrom small-scale efforts involving just a few domain experts to large-scale\nprojects that require effective collaboration between dozens or even hundreds\nof experts, practitioners and other stakeholders. Understanding how these\nstakeholders collaborate will enable us to improve editing environments that\nsupport such collaborations. We uncover how large ontology-engineering\nprojects, such as the ICD in its 11th revision, unfold by analyzing usage logs\nof five different biomedical ontology-engineering projects of varying sizes and\nscopes using Markov chains. We discover intriguing interaction patterns (e.g.,\nwhich properties users subsequently change) that suggest that large\ncollaborative ontology-engineering projects are governed by a few general\nprinciples that determine and drive development. From our analysis, we identify\ncommonalities and differences between different projects that have implications\nfor project managers, ontology editors, developers and contributors working on\ncollaborative ontology-engineering projects and tools in the biomedical domain. \n\n"}
{"id": "1407.3211", "contents": "Title: Possibility neutrosophic soft sets with applications in decision making\n  and similarity measure Abstract: In this paper, concept of possibility neutrosophic soft set and its\noperations are defined, and their properties are studied. An application of\nthis theory in decision making is investigated. Also a similarity measure of\ntwo possibility neutrosophic soft sets is introduced and discussed. Finally an\napplication of this similarity measure is given to select suitable person for\nposition in a firm. \n\n"}
{"id": "1407.3519", "contents": "Title: Showing invariance compositionally for a process algebra for network\n  protocols Abstract: This paper presents the mechanization of a process algebra for Mobile Ad hoc\nNetworks and Wireless Mesh Networks, and the development of a compositional\nframework for proving invariant properties. Mechanizing the core process\nalgebra in Isabelle/HOL is relatively standard, but its layered structure\nnecessitates special treatment. The control states of reactive processes, such\nas nodes in a network, are modelled by terms of the process algebra. We propose\na technique based on these terms to streamline proofs of inductive invariance.\nThis is not sufficient, however, to state and prove invariants that relate\nstates across multiple processes (entire networks). To this end, we propose a\nnovel compositional technique for lifting global invariants stated at the level\nof individual nodes to networks of nodes. \n\n"}
{"id": "1407.5656", "contents": "Title: PGMHD: A Scalable Probabilistic Graphical Model for Massive Hierarchical\n  Data Problems Abstract: In the big data era, scalability has become a crucial requirement for any\nuseful computational model. Probabilistic graphical models are very useful for\nmining and discovering data insights, but they are not scalable enough to be\nsuitable for big data problems. Bayesian Networks particularly demonstrate this\nlimitation when their data is represented using few random variables while each\nrandom variable has a massive set of values. With hierarchical data - data that\nis arranged in a treelike structure with several levels - one would expect to\nsee hundreds of thousands or millions of values distributed over even just a\nsmall number of levels. When modeling this kind of hierarchical data across\nlarge data sets, Bayesian networks become infeasible for representing the\nprobability distributions for the following reasons: i) Each level represents a\nsingle random variable with hundreds of thousands of values, ii) The number of\nlevels is usually small, so there are also few random variables, and iii) The\nstructure of the network is predefined since the dependency is modeled top-down\nfrom each parent to each of its child nodes, so the network would contain a\nsingle linear path for the random variables from each parent to each child\nnode. In this paper we present a scalable probabilistic graphical model to\novercome these limitations for massive hierarchical data. We believe the\nproposed model will lead to an easily-scalable, more readable, and expressive\nimplementation for problems that require probabilistic-based solutions for\nmassive amounts of hierarchical data. We successfully applied this model to\nsolve two different challenging probabilistic-based problems on massive\nhierarchical data sets for different domains, namely, bioinformatics and latent\nsemantic discovery over search logs. \n\n"}
{"id": "1407.7274", "contents": "Title: Isomorphism within Naive Type Theory Abstract: We provide a treatment of isomorphism within a set-theoretic formulation of\ndependent type theory. Type expressions are assigned their natural\nset-theoretic compositional meaning. Types are divided into small and large\ntypes --- sets and proper classes respectively. Each proper class, such as\n\"group\" or \"topological space\", has an associated notion of isomorphism in\ncorrespondence with standard definitions. Isomorphism is handled by definging a\ngroupoid structure on the space of all definable values. The values are\nsimultaneously objects (oids) and morphism --- they are \"morphoids\". Soundness\ncan then be proved for simple and natural inference rules deriving isomorphisms\nand for the substitution of isomorphics. \n\n"}
{"id": "1408.0620", "contents": "Title: Approximate Consensus in Highly Dynamic Networks: The Role of Averaging\n  Algorithms Abstract: In this paper, we investigate the approximate consensus problem in highly\ndynamic networks in which topology may change continually and unpredictably. We\nprove that in both synchronous and partially synchronous systems, approximate\nconsensus is solvable if and only if the communication graph in each round has\na rooted spanning tree, i.e., there is a coordinator at each time. The striking\npoint in this result is that the coordinator is not required to be unique and\ncan change arbitrarily from round to round. Interestingly, the class of\naveraging algorithms, which are memoryless and require no process identifiers,\nentirely captures the solvability issue of approximate consensus in that the\nproblem is solvable if and only if it can be solved using any averaging\nalgorithm. Concerning the time complexity of averaging algorithms, we show that\napproximate consensus can be achieved with precision of $\\varepsilon$ in a\ncoordinated network model in $O(n^{n+1} \\log\\frac{1}{\\varepsilon})$ synchronous\nrounds, and in $O(\\Delta n^{n\\Delta+1} \\log\\frac{1}{\\varepsilon})$ rounds when\nthe maximum round delay for a message to be delivered is $\\Delta$. While in\ngeneral, an upper bound on the time complexity of averaging algorithms has to\nbe exponential, we investigate various network models in which this exponential\nbound in the number of nodes reduces to a polynomial bound. We apply our\nresults to networked systems with a fixed topology and classical benign fault\nmodels, and deduce both known and new results for approximate consensus in\nthese systems. In particular, we show that for solving approximate consensus, a\ncomplete network can tolerate up to 2n-3 arbitrarily located link faults at\nevery round, in contrast with the impossibility result established by Santoro\nand Widmayer (STACS '89) showing that exact consensus is not solvable with n-1\nlink faults per round originating from the same node. \n\n"}
{"id": "1408.0979", "contents": "Title: Distributed Markov Chains Abstract: The formal verification of large probabilistic models is important and\nchallenging. Exploiting the concurrency that is often present is one way to\naddress this problem. Here we study a restricted class of asynchronous\ndistributed probabilistic systems in which the synchronizations determine the\nprobability distribution for the next moves of the participating agents. The\nkey restriction we impose is that the synchronizations are deterministic, in\nthe sense that any two simultaneously enabled synchronizations must involve\ndisjoint sets of agents. As a result, this network of agents can be viewed as a\nsuccinct and distributed presentation of a large global Markov chain. A rich\nclass of Markov chains can be represented this way.\n  We define an interleaved semantics for our model in terms of the local\nsynchronization actions. The network structure induces an independence relation\non these actions, which, in turn, induces an equivalence relation over the\ninterleaved runs in the usual way. We construct a natural probability measure\nover these equivalence classes of runs by exploiting Mazurkiewicz trace theory\nand the probability measure space of the associated global Markov chain.\n  It turns out that verification of our model, called DMCs (distributed Markov\nchains), can often be efficiently carried out by exploiting the partial order\nnature of the interleaved semantics. To demonstrate this, we develop a\nstatistical model checking (SMC) procedure and use it to verify two large\ndistributed probabilistic networks. \n\n"}
{"id": "1408.2287", "contents": "Title: In principle determination of generic priors Abstract: Probability theory as extended logic is completed such that essentially any\nprobability may be determined. This is done by considering propositional logic\n(as opposed to predicate logic) as syntactically suffcient and imposing a\nsymmetry from propositional logic. It is shown how the notions of `possibility'\nand `property' may be suffciently represented in propositional logic such that\n1) the principle of indifference drops out and becomes essentially combinatoric\nin nature and 2) one may appropriately represent assumptions where one assumes\nthere is a space of possibilities but does not assume the size of the space. \n\n"}
{"id": "1408.3079", "contents": "Title: A Lightweight Approach for Improving the Lookup Performance in\n  Kademlia-type Systems Abstract: Discovery of nodes and content in large-scale distributed systems is\ngenerally based on Kademlia, today. Understanding Kademlia-type systems to\nimprove their performance is essential for maintaining a high service quality\nfor an increased number of participants, particularly when those systems are\nadopted by latency-sensitive applications.\n  This paper contributes to the understanding of Kademlia by studying the\nimpact of \\emph{diversifying} neighbours' identifiers within each routing table\nbucket on the lookup performance. We propose a new, yet backward-compatible,\nneighbour selection scheme that attempts to maximize the aforementioned\ndiversity. The scheme does not cause additional overhead except negligible\ncomputations for comparing the diversity of identifiers. We present a\ntheoretical model for the actual impact of the new scheme on the lookup's hop\ncount and validate it against simulations of three exemplary Kademlia-type\nsystems. We also measure the performance gain enabled by a partial deployment\nfor the scheme in the real KAD system. The results confirm the superiority of\nthe systems that incorporate our scheme. \n\n"}
{"id": "1408.6806", "contents": "Title: Mathematical Knowledge Representation: Semantic Models and Formalisms Abstract: The paper provides a survey of semantic methods for solution of fundamental\ntasks in mathematical knowledge management. Ontological models and formalisms\nare discussed. We propose an ontology of mathematical knowledge, covering a\nwide range of fields of mathematics. We demonstrate applications of this\nrepresentation in mathematical formula search, and learning. \n\n"}
{"id": "1409.2088", "contents": "Title: Introducing Molly: Distributed Memory Parallelization with LLVM Abstract: Programming for distributed memory machines has always been a tedious task,\nbut necessary because compilers have not been sufficiently able to optimize for\nsuch machines themselves. Molly is an extension to the LLVM compiler toolchain\nthat is able to distribute and reorganize workload and data if the program is\norganized in statically determined loop control-flows. These are represented as\npolyhedral integer-point sets that allow program transformations applied on\nthem. Memory distribution and layout can be declared by the programmer as\nneeded and the necessary asynchronous MPI communication is generated\nautomatically. The primary motivation is to run Lattice QCD simulations on IBM\nBlue Gene/Q supercomputers, but since the implementation is not yet completed,\nthis paper shows the capabilities on Conway's Game of Life. \n\n"}
{"id": "1409.2908", "contents": "Title: A Framework for Practical Parallel Fast Matrix Multiplication Abstract: Matrix multiplication is a fundamental computation in many scientific\ndisciplines. In this paper, we show that novel fast matrix multiplication\nalgorithms can significantly outperform vendor implementations of the classical\nalgorithm and Strassen's fast algorithm on modest problem sizes and shapes.\nFurthermore, we show that the best choice of fast algorithm depends not only on\nthe size of the matrices but also the shape. We develop a code generation tool\nto automatically implement multiple sequential and shared-memory parallel\nvariants of each fast algorithm, including our novel parallelization scheme.\nThis allows us to rapidly benchmark over 20 fast algorithms on several problem\nsizes. Furthermore, we discuss a number of practical implementation issues for\nthese algorithms on shared-memory machines that can direct further research on\nmaking fast algorithms practical. \n\n"}
{"id": "1409.3428", "contents": "Title: How constructive is constructing measures? Abstract: Given some set, how hard is it to construct a measure supported by it? We\nclassify some variations of this task in the Weihrauch lattice. Particular\nattention is paid to Frostman measures on sets with positive Hausdorff\ndimension. As a side result, the Weihrauch degree of Hausdorff dimension itself\nis determined. \n\n"}
{"id": "1409.3560", "contents": "Title: Descriptive Control Theory: A Proposal Abstract: Logic is playing an increasingly important role in the engineering of\nreal-time, hybrid, and cyber-physical systems, but mostly in the form of\nposterior verification and high-level analysis. The core methodology in the\ndesign of real-world systems consists mainly of control theory and numerical\nanalysis, and has remained mostly free of logic and formal approaches. As a\nresult, besides facing extreme difficulty in guaranteeing the reliability of\nthese systems, engineers are also missing out the computational power of\nlogic-based methods that has greatly advanced in the past decades. To change\nthis situation, we need a logical and computational foundation for control\ntheory. The name \"descriptive control theory\" emphasizes the overarching theme\nof using logic to express, analyze, and solve problems in control theory. If\nthe program is successfully carried out, logical approaches will significantly\nextend existing engineering methods towards a unified methodology for handling\nnonlinear and hybrid systems, and bring design automation and reliability to an\nunprecedented level in the broad field of engineering. \n\n"}
{"id": "1409.6041", "contents": "Title: Domain Adaptive Neural Networks for Object Recognition Abstract: We propose a simple neural network model to deal with the domain adaptation\nproblem in object recognition. Our model incorporates the Maximum Mean\nDiscrepancy (MMD) measure as a regularization in the supervised learning to\nreduce the distribution mismatch between the source and target domains in the\nlatent space. From experiments, we demonstrate that the MMD regularization is\nan effective tool to provide good domain adaptation models on both SURF\nfeatures and raw image pixels of a particular image data set. We also show that\nour proposed model, preceded by the denoising auto-encoder pretraining,\nachieves better performance than recent benchmark models on the same data sets.\nThis work represents the first study of MMD measure in the context of neural\nnetworks. \n\n"}
{"id": "1409.7591", "contents": "Title: Topic Similarity Networks: Visual Analytics for Large Document Sets Abstract: We investigate ways in which to improve the interpretability of LDA topic\nmodels by better analyzing and visualizing their outputs. We focus on examining\nwhat we refer to as topic similarity networks: graphs in which nodes represent\nlatent topics in text collections and links represent similarity among topics.\nWe describe efficient and effective approaches to both building and labeling\nsuch networks. Visualizations of topic models based on these networks are shown\nto be a powerful means of exploring, characterizing, and summarizing large\ncollections of unstructured text documents. They help to \"tease out\"\nnon-obvious connections among different sets of documents and provide insights\ninto how topics form larger themes. We demonstrate the efficacy and\npracticality of these approaches through two case studies: 1) NSF grants for\nbasic research spanning a 14 year period and 2) the entire English portion of\nWikipedia. \n\n"}
{"id": "1410.3125", "contents": "Title: Relational Linear Programs Abstract: We propose relational linear programming, a simple framework for combing\nlinear programs (LPs) and logic programs. A relational linear program (RLP) is\na declarative LP template defining the objective and the constraints through\nthe logical concepts of objects, relations, and quantified variables. This\nallows one to express the LP objective and constraints relationally for a\nvarying number of individuals and relations among them without enumerating\nthem. Together with a logical knowledge base, effectively a logical program\nconsisting of logical facts and rules, it induces a ground LP. This ground LP\nis solved using lifted linear programming. That is, symmetries within the\nground LP are employed to reduce its dimensionality, if possible, and the\nreduced program is solved using any off-the-shelf LP solver. In contrast to\nmainstream LP template languages like AMPL, which features a mixture of\ndeclarative and imperative programming styles, RLP's relational nature allows a\nmore intuitive representation of optimization problems over relational domains.\nWe illustrate this empirically by experiments on approximate inference in\nMarkov logic networks using LP relaxations, on solving Markov decision\nprocesses, and on collective inference using LP support vector machines. \n\n"}
{"id": "1410.4477", "contents": "Title: Analysis of incremental augmented affine projection algorithm for\n  distributed estimation of complex signals Abstract: This paper considers the problem of distributed estimation in an incremental\nnetwork when the measurements taken by the node follow a widely linear model.\nThe proposed algorithm which we refer to it as incremental augmented affine\nprojection algorithm (incAAPA) utilizes the full second order statistical\ninformation in the complex domain. Moreover, it exploits spatio-temporal\ndiversity to improve the estimation performance. We derive steady-state\nperformance metric of the incAAPA in terms of the mean-square deviation (MSD).\nWe further derive sufficient conditions to ensure mean-square convergence. Our\nanalysis illustrate that the proposed algorithm is able to process both second\norder circular (proper) and noncircular (improper) signals. The validity of the\ntheoretical results and the good performance of the proposed algorithm are\ndemonstrated by several computer simulations. \n\n"}
{"id": "1410.4615", "contents": "Title: Learning to Execute Abstract: Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are\nwidely used because they are expressive and are easy to train. Our interest\nlies in empirically evaluating the expressiveness and the learnability of LSTMs\nin the sequence-to-sequence regime by training them to evaluate short computer\nprograms, a domain that has traditionally been seen as too complex for neural\nnetworks. We consider a simple class of programs that can be evaluated with a\nsingle left-to-right pass using constant memory. Our main result is that LSTMs\ncan learn to map the character-level representations of such programs to their\ncorrect outputs. Notably, it was necessary to use curriculum learning, and\nwhile conventional curriculum learning proved ineffective, we developed a new\nvariant of curriculum learning that improved our networks' performance in all\nexperimental conditions. The improved curriculum had a dramatic impact on an\naddition problem, making it possible to train an LSTM to add two 9-digit\nnumbers with 99% accuracy. \n\n"}
{"id": "1410.5242", "contents": "Title: Performance Engineering of the Kernel Polynomial Method on Large-Scale\n  CPU-GPU Systems Abstract: The Kernel Polynomial Method (KPM) is a well-established scheme in quantum\nphysics and quantum chemistry to determine the eigenvalue density and spectral\nproperties of large sparse matrices. In this work we demonstrate the high\noptimization potential and feasibility of peta-scale heterogeneous CPU-GPU\nimplementations of the KPM. At the node level we show that it is possible to\ndecouple the sparse matrix problem posed by KPM from main memory bandwidth both\non CPU and GPU. To alleviate the effects of scattered data access we combine\nloosely coupled outer iterations with tightly coupled block sparse matrix\nmultiple vector operations, which enables pure data streaming. All\noptimizations are guided by a performance analysis and modelling process that\nindicates how the computational bottlenecks change with each optimization step.\nFinally we use the optimized node-level KPM with a hybrid-parallel framework to\nperform large scale heterogeneous electronic structure calculations for novel\ntopological materials on a petascale-class Cray XC30 system. \n\n"}
{"id": "1410.7256", "contents": "Title: Interactive Consistency in practical, mostly-asynchronous systems Abstract: Interactive consistency is the problem in which n nodes, where up to t may be\nbyzantine, each with its own private value, run an algorithm that allows all\nnon-faulty nodes to infer the values of each other node. This problem is\nrelevant to critical applications that rely on the combination of the opinions\nof multiple peers to provide a service. Examples include monitoring a content\nsource to prevent equivocation or to track variability in the content provided,\nand resolving divergent state amongst the nodes of a distributed system.\nPrevious works assume a fully synchronous system, where one can make strong\nassumptions such as negligible message delivery delays and/or detection of\nabsent messages. However, practical, real-world systems are mostly\nasynchronous, i.e., they exhibit only some periods of synchrony during which\nmessage delivery is timely, thus requiring a different approach. In this paper,\nwe present a thorough study on practical interactive consistency. We leverage\nthe vast prior work on broadcast and byzantine consensus algorithms to design,\nimplement and evaluate a set of algorithms, with varying timing assumptions and\nmessage complexity, that can be used to achieve interactive consistency in\nreal-world distributed systems. We provide a complete, open-source\nimplementation of each proposed interactive consistency algorithm by building a\nmulti-layered stack of protocols that include several broadcast protocols, as\nwell as a binary and a multi-valued consensus protocol. Most of these protocols\nhave never been implemented and evaluated in a real system before. We analyze\nthe performance of our suite of algorithms experimentally by engaging in both\nsingle instance and multiple parallel instances of each alternative. \n\n"}
{"id": "1411.0659", "contents": "Title: Approximate Counting in SMT and Value Estimation for Probabilistic\n  Programs Abstract: #SMT, or model counting for logical theories, is a well-known hard problem\nthat generalizes such tasks as counting the number of satisfying assignments to\na Boolean formula and computing the volume of a polytope. In the realm of\nsatisfiability modulo theories (SMT) there is a growing need for model counting\nsolvers, coming from several application domains (quantitative information\nflow, static analysis of probabilistic programs). In this paper, we show a\nreduction from an approximate version of #SMT to SMT.\n  We focus on the theories of integer arithmetic and linear real arithmetic. We\npropose model counting algorithms that provide approximate solutions with\nformal bounds on the approximation error. They run in polynomial time and make\na polynomial number of queries to the SMT solver for the underlying theory,\nexploiting \"for free\" the sophisticated heuristics implemented within modern\nSMT solvers. We have implemented the algorithms and used them to solve the\nvalue problem for a model of loop-free probabilistic programs with\nnondeterminism. \n\n"}
{"id": "1411.5282", "contents": "Title: Reaching Approximate Byzantine Consensus with Multi-hop Communication Abstract: We address the problem of reaching consensus in the presence of Byzantine\nfaults. In particular, we are interested in investigating the impact of\nmessages relay on the network connectivity for a correct iterative approximate\nByzantine consensus algorithm to exist. The network is modeled by a simple\ndirected graph. We assume a node can send messages to another node that is up\nto $l$ hops away via forwarding by the intermediate nodes on the routes, where\n$l\\in \\mathbb{N}$ is a natural number. We characterize the necessary and\nsufficient topological conditions on the network structure. The tight\nconditions we found are consistent with the tight conditions identified for\n$l=1$, where only local communication is allowed, and are strictly weaker for\n$l>1$. Let $l^*$ denote the length of a longest path in the given network. For\n$l\\ge l^*$ and undirected graphs, our conditions hold if and only if $n\\ge\n3f+1$ and the node-connectivity of the given graph is at least $2f+1$ , where\n$n$ is the total number of nodes and $f$ is the maximal number of Byzantine\nnodes; and for $l\\ge l^*$ and directed graphs, our conditions is equivalent to\nthe tight condition found for exact Byzantine consensus.\n  Our sufficiency is shown by constructing a correct algorithm, wherein the\ntrim function is constructed based on investigating a newly introduced minimal\nmessages cover property. The trim function proposed also works over\nmulti-graphs. \n\n"}
{"id": "1411.5433", "contents": "Title: Using Volunteer Computing for Mounting SAT-based Cryptographic Attacks Abstract: In this paper we describe the volunteer computing project SAT@home, developed\nand maintained by us. This project is aimed at solving hard instances of the\nBoolean satisfiability problem (SAT). We believe that this project can be a\nuseful tool for computational study of inversion problems of some cryptographic\nfunctions. In particular we describe a series of experiments performed in\nSAT@home on the cryptanalysis of the widely known keystream generator A5/1. In\nall experiments we analyzed one known burst (114 bits) of keystream produced by\nA5/1. Before the cryptanalysis itself there is a stage on which the\npartitioning of the original problem to a family of subproblems is carried out.\nEach of subproblems should be easy enough so that it could be solved in\nrelatively small amount of time by volunteer's PC. We construct such\npartitioning using the special technique based on the Monte Carlo method and\ndiscrete optimization algorithms for special predictive functions. Besides this\nin the paper we describe the technique for reducing inversion problems of\ncryptographic functions to SAT. \n\n"}
{"id": "1412.0542", "contents": "Title: Budget Imbalance Criteria for Auctions: A Formalized Theorem Abstract: We present an original theorem in auction theory: it specifies general\nconditions under which the sum of the payments of all bidders is necessarily\nnot identically zero, and more generally not constant. Moreover, it explicitly\nsupplies a construction for a finite minimal set of possible bids on which such\na sum is not constant. In particular, this theorem applies to the important\ncase of a second-price Vickrey auction, where it reduces to a basic result of\nwhich a novel proof is given. To enhance the confidence in this new theorem, it\nhas been formalized in Isabelle/HOL: the main results and definitions of the\nformal proof are re- produced here in common mathematical language, and are\naccompanied by an informal discussion about the underlying ideas. \n\n"}
{"id": "1412.0691", "contents": "Title: RoboBrain: Large-Scale Knowledge Engine for Robots Abstract: In this paper we introduce a knowledge engine, which learns and shares\nknowledge representations, for robots to carry out a variety of tasks. Building\nsuch an engine brings with it the challenge of dealing with multiple data\nmodalities including symbols, natural language, haptic senses, robot\ntrajectories, visual features and many others. The \\textit{knowledge} stored in\nthe engine comes from multiple sources including physical interactions that\nrobots have while performing tasks (perception, planning and control),\nknowledge bases from the Internet and learned representations from several\nrobotics research groups.\n  We discuss various technical aspects and associated challenges such as\nmodeling the correctness of knowledge, inferring latent information and\nformulating different robotic tasks as queries to the knowledge engine. We\ndescribe the system architecture and how it supports different mechanisms for\nusers and robots to interact with the engine. Finally, we demonstrate its use\nin three important research areas: grounding natural language, perception, and\nplanning, which are the key building blocks for many robotic tasks. This\nknowledge engine is a collaborative effort and we call it RoboBrain. \n\n"}
{"id": "1412.2123", "contents": "Title: Distributed Multi-Depot Routing without Communications Abstract: We consider and formulate a class of distributed multi-depot routing\nproblems, where servers are to visit a set of requests, with the aim of\nminimizing the total distance travelled by all servers. These problems fall\ninto two categories: distributed offline routing problems where all the\nrequests that need to be visited are known from the start; distributed online\nrouting problems where the requests come to be known incrementally. A critical\nand novel feature of our formulations is that communications are not allowed\namong the servers, hence posing an interesting and challenging question: what\nperformance can be achieved in comparison to the best possible solution\nobtained from an omniscience planner with perfect communication capabilities?\nThe worst-case (over all possible request-set instances) performance metrics\nare given by the approximation ratio (offline case) and the competitive ratio\n(online case).\n  Our first result indicates that the online and offline problems are\neffectively equivalent: for the same request-set instance, the approximation\nratio and the competitive ratio differ by at most an additive factor of 2,\nirrespective of the release dates in the online case. Therefore, we can\nrestrict our attention to the offline problem. For the offline problem, we show\nthat the approximation ratio given by the Voronoi partition is m (the number of\nservers). For two classes of depot configurations, when the depots form a line\nand when the ratios between the distances of pairs of depots are upper bounded\nby a sublinear function f(m) (i.e., f(m) = o(m)), we give partition schemes\nwith sublinear approximation ratios O(log m) and {\\Theta}(f(m)) respectively.\nWe also discuss several interesting open problems in our formulations: in\nparticular, how our initial results (on the two deliberately chosen classes of\ndepots) shape our conjecture on the open problems. \n\n"}
{"id": "1412.2309", "contents": "Title: Visual Causal Feature Learning Abstract: We provide a rigorous definition of the visual cause of a behavior that is\nbroadly applicable to the visually driven behavior in humans, animals, neurons,\nrobots and other perceiving systems. Our framework generalizes standard\naccounts of causal learning to settings in which the causal variables need to\nbe constructed from micro-variables. We prove the Causal Coarsening Theorem,\nwhich allows us to gain causal knowledge from observational data with minimal\nexperimental effort. The theorem provides a connection to standard inference\ntechniques in machine learning that identify features of an image that\ncorrelate with, but may not cause, the target behavior. Finally, we propose an\nactive learning scheme to learn a manipulator function that performs optimal\nmanipulations on the image to automatically identify the visual cause of a\ntarget behavior. We illustrate our inference and learning algorithms in\nexperiments based on both synthetic and real data. \n\n"}
{"id": "1412.2812", "contents": "Title: Unsupervised Induction of Semantic Roles within a Reconstruction-Error\n  Minimization Framework Abstract: We introduce a new approach to unsupervised estimation of feature-rich\nsemantic role labeling models. Our model consists of two components: (1) an\nencoding component: a semantic role labeling model which predicts roles given a\nrich set of syntactic and lexical features; (2) a reconstruction component: a\ntensor factorization model which relies on roles to predict argument fillers.\nWhen the components are estimated jointly to minimize errors in argument\nreconstruction, the induced roles largely correspond to roles defined in\nannotated resources. Our method performs on par with most accurate role\ninduction methods on English and German, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguages. \n\n"}
{"id": "1412.6572", "contents": "Title: Explaining and Harnessing Adversarial Examples Abstract: Several machine learning models, including neural networks, consistently\nmisclassify adversarial examples---inputs formed by applying small but\nintentionally worst-case perturbations to examples from the dataset, such that\nthe perturbed input results in the model outputting an incorrect answer with\nhigh confidence. Early attempts at explaining this phenomenon focused on\nnonlinearity and overfitting. We argue instead that the primary cause of neural\nnetworks' vulnerability to adversarial perturbation is their linear nature.\nThis explanation is supported by new quantitative results while giving the\nfirst explanation of the most intriguing fact about them: their generalization\nacross architectures and training sets. Moreover, this view yields a simple and\nfast method of generating adversarial examples. Using this approach to provide\nexamples for adversarial training, we reduce the test set error of a maxout\nnetwork on the MNIST dataset. \n\n"}
{"id": "1412.6703", "contents": "Title: Quantifying Natural and Artificial Intelligence in Robots and Natural\n  Systems with an Algorithmic Behavioural Test Abstract: One of the most important aims of the fields of robotics, artificial\nintelligence and artificial life is the design and construction of systems and\nmachines as versatile and as reliable as living organisms at performing high\nlevel human-like tasks. But how are we to evaluate artificial systems if we are\nnot certain how to measure these capacities in living systems, let alone how to\ndefine life or intelligence? Here I survey a concrete metric towards measuring\nabstract properties of natural and artificial systems, such as the ability to\nreact to the environment and to control one's own behaviour. \n\n"}
{"id": "1412.7364", "contents": "Title: Erasure coding for fault oblivious linear system solvers Abstract: Dealing with hardware and software faults is an important problem as parallel\nand distributed systems scale to millions of processing cores and wide area\nnetworks. Traditional methods for dealing with faults include\ncheckpoint-restart, active replicas, and deterministic replay. Each of these\ntechniques has associated resource overheads and constraints. In this paper, we\npropose an alternate approach to dealing with faults, based on input\naugmentation. This approach, which is an algorithmic analog of erasure coded\nstorage, applies a minimally modified algorithm on the augmented input to\nproduce an augmented output. The execution of such an algorithm proceeds\ncompletely oblivious to faults in the system. In the event of one or more\nfaults, the real solution is recovered using a rapid reconstruction method from\nthe augmented output. We demonstrate this approach on the problem of solving\nsparse linear systems using a conjugate gradient solver. We present input\naugmentation and output recovery techniques. Through detailed experiments, we\nshow that our approach can be made oblivious to a large number of faults with\nlow computational overhead. Specifically, we demonstrate cases where a single\nfault can be corrected with less than 10% overhead in time, and even in extreme\ncases (fault rates of 20%), our approach is able to compute a solution with\nreasonable overhead. These results represent a significant improvement over the\nstate of the art. \n\n"}
{"id": "1412.7998", "contents": "Title: Propositional Logics of Dependence Abstract: In this paper, we study logics of dependence on the propositional level. We\nprove that several interesting propositional logics of dependence, including\npropositional dependence logic, propositional intuitionistic dependence logic\nas well as propositional inquisitive logic, are expressively complete and have\ndisjunctive or conjunctive normal forms. We provide deduction systems and prove\nthe completeness theorems for these logics. \n\n"}
{"id": "1412.8324", "contents": "Title: A Constructive Proof on the Compositionality of Linearizability Abstract: Linearizability is the strongest correctness property for both shared memory\nand message passing systems. One of its useful features is the\ncompositionality: a history (execution) is linearizable if and only if each\nobject (component) subhistory is linearizable. In this paper, we propose a new\nhierarchical system model to address challenges in modular development of cloud\nsystems. Object are defined by induction from the most fundamental atomic\nBoolean registers, and histories are represented as countable well-ordered\nstructures of events to deal with both finite and infinite executions. Then, we\npresent a new constructive proof on the compositionality theorem of\nlinearizability inspired by Multiway Merge. This proof deduces a theoretically\nefficient algorithm which generates linearization in O(N*logP) running time\nwith O(N) space, where P and N are process/event numbers respectively. \n\n"}
{"id": "1412.8532", "contents": "Title: Crash-Tolerant Consensus in Directed Graphs Abstract: This work considers a point-to-point network of n nodes connected by directed\nlinks, and proves tight necessary and sufficient conditions on the underlying\ncommunication graphs for achieving consensus among these nodes under crash\nfaults. We identify the conditions in both synchronous and asynchronous systems \n\n"}
{"id": "1501.02036", "contents": "Title: Improving the Deductive System DES with Persistence by Using SQL DBMS's Abstract: This work presents how persistent predicates have been included in the\nin-memory deductive system DES by relying on external SQL database management\nsystems. We introduce how persistence is supported from a user-point of view\nand the possible applications the system opens up, as the deductive expressive\npower is projected to relational databases. Also, we describe how it is\npossible to intermix computations of the deductive engine and the external\ndatabase, explaining its implementation and some optimizations. Finally, a\nperformance analysis is undertaken, comparing the system with current\nrelational database systems. \n\n"}
{"id": "1501.02629", "contents": "Title: Scaling-up Empirical Risk Minimization: Optimization of Incomplete\n  U-statistics Abstract: In a wide range of statistical learning problems such as ranking, clustering\nor metric learning among others, the risk is accurately estimated by\n$U$-statistics of degree $d\\geq 1$, i.e. functionals of the training data with\nlow variance that take the form of averages over $k$-tuples. From a\ncomputational perspective, the calculation of such statistics is highly\nexpensive even for a moderate sample size $n$, as it requires averaging\n$O(n^d)$ terms. This makes learning procedures relying on the optimization of\nsuch data functionals hardly feasible in practice. It is the major goal of this\npaper to show that, strikingly, such empirical risks can be replaced by\ndrastically computationally simpler Monte-Carlo estimates based on $O(n)$ terms\nonly, usually referred to as incomplete $U$-statistics, without damaging the\n$O_{\\mathbb{P}}(1/\\sqrt{n})$ learning rate of Empirical Risk Minimization (ERM)\nprocedures. For this purpose, we establish uniform deviation results describing\nthe error made when approximating a $U$-process by its incomplete version under\nappropriate complexity assumptions. Extensions to model selection, fast rate\nsituations and various sampling techniques are also considered, as well as an\napplication to stochastic gradient descent for ERM. Finally, numerical examples\nare displayed in order to provide strong empirical evidence that the approach\nwe promote largely surpasses more naive subsampling techniques. \n\n"}
{"id": "1501.02729", "contents": "Title: On the Energy Proportionality of Scale-Out Workloads Abstract: Our increasing reliance on the cloud has led to the emergence of scale-out\nworkloads. These scale-out workloads are latency-sensitive as they are user\ndriven. In order to meet strict latency constraints, they require massive\ncomputing infrastructure, which consume significant amount of energy and\ncontribute to operational costs. This cost is further aggravated by the lack of\nenergy proportionality in servers. As Internet services become even more\nubiquitous, scale-out workloads will need increasingly larger cluster\ninstallations. As such, we desire an investigation into the energy\nproportionality and the mechanisms to improve the power consumption of\nscale-out workloads.\n  Therefore, in this paper, we study the energy proportionality and power\nconsumption of clusters in the context of scale-out workloads. Towards this\nend, we evaluate the potential of power and resource provisioning to improve\nthe energy proportionality for this class of workloads. Using data serving, web\nsearching and data caching as our representative workloads, we first analyze\nthe component-level power distribution on a cluster. Second, we characterize\nhow these workloads utilize the cluster. Third, we analyze the potential of\npower provisioning techniques (i.e., active low-power, turbo and idle low-power\nmodes) to improve the energy proportionality of scale-out workloads. We then\ndescribe the ability of active low-power modes to provide trade-offs in power\nand latency. Finally, we compare and contrast power provisioning and resource\nprovisioning techniques. Our study reveals various insights which will help\nimprove the energy proportionality and power consumption of scale-out\nworkloads. \n\n"}
{"id": "1501.06059", "contents": "Title: !-Graphs with Trivial Overlap are Context-Free Abstract: String diagrams are a powerful tool for reasoning about composite structures\nin symmetric monoidal categories. By representing string diagrams as graphs,\nequational reasoning can be done automatically by double-pushout rewriting.\n!-graphs give us the means of expressing and proving properties about whole\nfamilies of these graphs simultaneously. While !-graphs provide elegant proofs\nof surprisingly powerful theorems, little is known about the formal properties\nof the graph languages they define. This paper takes the first step in\ncharacterising these languages by showing that an important subclass of\n!-graphs--those whose repeated structures only overlap trivially--can be\nencoded using a (context-free) vertex replacement grammar. \n\n"}
{"id": "1502.02725", "contents": "Title: Why Transactional Memory Should Not Be Obstruction-Free Abstract: Transactional memory (TM) is an inherently optimistic abstraction: it allows\nconcurrent processes to execute sequences of shared-data accesses\n(transactions) speculatively, with an option of aborting them in the future.\nEarly TM designs avoided using locks and relied on non-blocking synchronization\nto ensure obstruction-freedom: a transaction that encounters no step contention\nis not allowed to abort. However, it was later observed that obstruction-free\nTMs perform poorly and, as a result, state-of-the-art TM implementations are\nnowadays blocking, allowing aborts because of data conflicts rather than step\ncontention.\n  In this paper, we explain this shift in the TM practice theoretically, via\ncomplexity bounds. We prove a few important lower bounds on obstruction-free\nTMs. Then we present a lock-based TM implementation that beats all of these\nlower bounds. In sum, our results exhibit a considerable complexity gap between\nnon-blocking and blocking TM implementations. \n\n"}
{"id": "1502.03645", "contents": "Title: Numerical simulation of skin transport using Parareal Abstract: In-silico investigation of skin permeation is an important but also\ncomputationally demanding problem. To resolve all scales involved in full\ndetail will not only require exascale computing capacities but also suitable\nparallel algorithms. This article investigates the applicability of the\ntime-parallel Parareal algorithm to a brick and mortar setup, a precursory\nproblem to skin permeation. The C++ library Lib4PrM implementing Parareal is\ncombined with the UG4 simulation framework, which provides the spatial\ndiscretization and parallelization. The combination's performance is studied\nwith respect to convergence and speedup. It is confirmed that anisotropies in\nthe domain and jumps in diffusion coefficients only have a minor impact on\nParareal's convergence. The influence of load imbalances in time due to\ndifferences in number of iterations required by the spatial solver as well as\nspatio-temporal weak scaling is discussed. \n\n"}
{"id": "1502.04052", "contents": "Title: Computer-aided verification in mechanism design Abstract: In mechanism design, the gold standard solution concepts are dominant\nstrategy incentive compatibility and Bayesian incentive compatibility. These\nsolution concepts relieve the (possibly unsophisticated) bidders from the need\nto engage in complicated strategizing. While incentive properties are simple to\nstate, their proofs are specific to the mechanism and can be quite complex.\nThis raises two concerns. From a practical perspective, checking a complex\nproof can be a tedious process, often requiring experts knowledgeable in\nmechanism design. Furthermore, from a modeling perspective, if unsophisticated\nagents are unconvinced of incentive properties, they may strategize in\nunpredictable ways.\n  To address both concerns, we explore techniques from computer-aided\nverification to construct formal proofs of incentive properties. Because formal\nproofs can be automatically checked, agents do not need to manually check the\nproperties, or even understand the proof. To demonstrate, we present the\nverification of a sophisticated mechanism: the generic reduction from Bayesian\nincentive compatible mechanism design to algorithm design given by Hartline,\nKleinberg, and Malekian. This mechanism presents new challenges for formal\nverification, including essential use of randomness from both the execution of\nthe mechanism and from the prior type distributions. As an immediate\nconsequence, our work also formalizes Bayesian incentive compatibility for the\nentire family of mechanisms derived via this reduction. Finally, as an\nintermediate step in our formalization, we provide the first formal\nverification of incentive compatibility for the celebrated\nVickrey-Clarke-Groves mechanism. \n\n"}
{"id": "1502.04908", "contents": "Title: Progressive Transactional Memory in Time and Space Abstract: Transactional memory (TM) allows concurrent processes to organize sequences\nof operations on shared \\emph{data items} into atomic transactions. A\ntransaction may commit, in which case it appears to have executed sequentially\nor it may \\emph{abort}, in which case no data item is updated.\n  The TM programming paradigm emerged as an alternative to conventional\nfine-grained locking techniques, offering ease of programming and\ncompositionality. Though typically themselves implemented using locks, TMs hide\nthe inherent issues of lock-based synchronization behind a nice transactional\nprogramming interface.\n  In this paper, we explore inherent time and space complexity of lock-based\nTMs, with a focus of the most popular class of \\emph{progressive} lock-based\nTMs. We derive that a progressive TM might enforce a read-only transaction to\nperform a quadratic (in the number of the data items it reads) number of steps\nand access a linear number of distinct memory locations, closing the question\nof inherent cost of \\emph{read validation} in TMs. We then show that the total\nnumber of \\emph{remote memory references} (RMRs) that take place in an\nexecution of a progressive TM in which $n$ concurrent processes perform\ntransactions on a single data item might reach $\\Omega(n \\log n)$, which\nappears to be the first RMR complexity lower bound for transactional memory. \n\n"}
{"id": "1502.06882", "contents": "Title: On Reducing Linearizability to State Reachability Abstract: Efficient implementations of atomic objects such as concurrent stacks and\nqueues are especially susceptible to programming errors, and necessitate\nautomatic verification. Unfortunately their correctness criteria -\nlinearizability with respect to given ADT specifications - are hard to verify.\nEven on classes of implementations where the usual temporal safety properties\nlike control-state reachability are decidable, linearizability is undecidable.\n  In this work we demonstrate that verifying linearizability for certain fixed\nADT specifications is reducible to control-state reachability, despite being\nharder for arbitrary ADTs. We effectuate this reduction for several of the most\npopular atomic objects. This reduction yields the first decidability results\nfor verification without bounding the number of concurrent threads.\nFurthermore, it enables the application of existing safety-verification tools\nto linearizability verification. \n\n"}
{"id": "1502.07884", "contents": "Title: Characterising Modal Definability of Team-Based Logics via the Universal\n  Modality Abstract: We study model and frame definability of various modal logics. Let ML(A+)\ndenote the fragment of modal logic extended with the universal modality in\nwhich the universal modality occurs only positively. We show that a class of\nKripke models is definable in ML(A+) if and only if the class is elementary and\nclosed under disjoint unions and surjective bisimulations. We also characterise\nthe definability of ML(A+) in the spirit of the well-known Goldblatt--Thomason\ntheorem. We show that an elementary class F of Kripke frames is definable in\nML(A+) if and only if F is closed under taking generated subframes and bounded\nmorphic images, and reflects ultrafilter extensions and finitely generated\nsubframes. In addition we study frame definability relative to finite\ntransitive frames and give an analogous characterisation of ML(A+)-definability\nrelative to finite transitive frames. Finally, we initiate the study of model\nand frame definability in team-based logics. We study (extended) modal\ndependence logic, (extended) modal inclusion logic, and modal team logic. We\nestablish strict linear hierarchies with respect to model definability and\nframe definability, respectively. We show that, with respect to model and frame\ndefinability, the before mentioned team-based logics, except modal dependence\nlogic, either coincide with ML(A+) or plain modal logic ML. Thus as a corollary\nwe obtain model theoretic characterisation of model and frame definability for\nthe team-based logics. \n\n"}
{"id": "1503.00362", "contents": "Title: NEXP-completeness and Universal Hardness Results for Justification Logic Abstract: We provide a lower complexity bound for the satisfiability problem of a\nmulti-agent justification logic, establishing that the general NEXP upper bound\nfrom our previous work is tight. We then use a simple modification of the\ncorresponding reduction to prove that satisfiability for all multi-agent\njustification logics from there is hard for the Sigma 2 p class of the second\nlevel of the polynomial hierarchy - given certain reasonable conditions. Our\nmethods improve on these required conditions for the same lower bound for the\nsingle-agent justification logics, proven by Buss and Kuznets in 2009, thus\nanswering one of their open questions. \n\n"}
{"id": "1503.00576", "contents": "Title: Counting Triangles in Large Graphs on GPU Abstract: The clustering coefficient and the transitivity ratio are concepts often used\nin network analysis, which creates a need for fast practical algorithms for\ncounting triangles in large graphs. Previous research in this area focused on\nsequential algorithms, MapReduce parallelization, and fast approximations.\n  In this paper we propose a parallel triangle counting algorithm for CUDA GPU.\nWe describe the implementation details necessary to achieve high performance\nand present the experimental evaluation of our approach. Our algorithm achieves\n8 to 15 times speedup over the CPU implementation and is capable of finding 3.8\nbillion triangles in an 89 million edges graph in less than 10 seconds on the\nNvidia Tesla C2050 GPU. \n\n"}
{"id": "1503.00886", "contents": "Title: On Geometry of Interaction for Polarized Linear Logic Abstract: We present Geometry of Interaction (GoI) models for Multiplicative Polarized\nLinear Logic, MLLP, which is the multiplicative fragment of Olivier Laurent's\nPolarized Linear Logic. This is done by uniformly adding multipoints to various\ncategorical models of GoI. Multipoints are shown to play an essential role in\nsemantically characterizing the dynamics of proof networks in polarized proof\ntheory. For example, they permit us to characterize the key feature of\npolarization, focusing, as well as being fundamental to our construction of\nconcrete polarized GoI models.\n  Our approach to polarized GoI involves two independent studies, based on\ndifferent categorical perspectives of GoI.\n  (i) Inspired by the work of Abramsky, Haghverdi, and Scott, a polarized GoI\nsituation is defined in which multipoints are added to a traced monoidal\ncategory equipped with a reflexive object $U$. Using this framework,\ncategorical versions of Girard's Execution formula are defined, as well as the\nGoI interpretation of MLLP, proofs. Running the Execution formula is shown to\ncharacterize the focusing property (and thus polarities) as well as the\ndynamics of cut-elimination.\n  (ii) The Int construction of Joyal-Street-Verity is another fundamental\ncategorical structure for modelling GoI. Here, we investigate it in a\nmultipointed setting. Our presentation yields a compact version of\nHamano-Scott's polarized categories, and thus denotational models of MLLP.\nThese arise from a contravariant duality between monoidal categories of\npositive and negative objects, along with an appropriate bimodule structure\n(representing \"non-focused proofs\") between them.\n  Finally, as a special case of (ii) above, a compact model of MLLP is also\npresented based on Rel (the category of sets and relations) equipped with\nmulti-points. \n\n"}
{"id": "1503.04918", "contents": "Title: Lucretia - intersection type polymorphism for scripting languages Abstract: Scripting code may present maintenance problems in the long run. There is,\nthen, the call for methodologies that make it possible to control the\nproperties of programs written in dynamic languages in an automatic fashion. We\nintroduce Lucretia, a core language with an introspection primitive. Lucretia\nis equipped with a (retrofitted) static type system based on local updates of\ntypes that describe the structure of objects being used. In this way, we deal\nwith one of the most dynamic features of scripting languages, that is, the\nruntime modification of object interfaces. Judgements in our systems have a\nHoare-like shape, as they have a precondition and a postcondition part.\nPreconditions describe static approximations of the interfaces of visible\nobjects before a certain expression has been executed and postconditions\ndescribe them after its execution. The field update operation complicates the\nissue of aliasing in the system. We cope with it by introducing intersection\ntypes in method signatures. \n\n"}
{"id": "1503.07220", "contents": "Title: Individual Planning in Agent Populations: Exploiting Anonymity and\n  Frame-Action Hypergraphs Abstract: Interactive partially observable Markov decision processes (I-POMDP) provide\na formal framework for planning for a self-interested agent in multiagent\nsettings. An agent operating in a multiagent environment must deliberate about\nthe actions that other agents may take and the effect these actions have on the\nenvironment and the rewards it receives. Traditional I-POMDPs model this\ndependence on the actions of other agents using joint action and model spaces.\nTherefore, the solution complexity grows exponentially with the number of\nagents thereby complicating scalability. In this paper, we model and extend\nanonymity and context-specific independence -- problem structures often present\nin agent populations -- for computational gain. We empirically demonstrate the\nefficiency from exploiting these problem structures by solving a new multiagent\nproblem involving more than 1,000 agents. \n\n"}
{"id": "1504.01438", "contents": "Title: Convergence Time of Quantized Metropolis Consensus Over Time-Varying\n  Networks Abstract: We consider the quantized consensus problem on undirected time-varying\nconnected graphs with n nodes, and devise a protocol with fast convergence time\nto the set of consensus points. Specifically, we show that when the edges of\neach network in a sequence of connected time-varying networks are activated\nbased on Poisson processes with Metropolis rates, the expected convergence time\nto the set of consensus points is at most O(n^2 log^2 n), where each node\nperforms a constant number of updates per unit time. \n\n"}
{"id": "1504.04759", "contents": "Title: On the Identity Type as the Type of Computational Paths Abstract: We introduce a new way of formalizing the intensional identity type based on\nthe fact that a entity known as computational paths can be interpreted as terms\nof the identity type. Our approach enjoys the fact that our elimination rule is\neasy to understand and use. We make this point clear constructing terms of some\nrelevant types using our proposed elimination rule. We also show that the\nidentity type, as defined by our approach, induces a groupoid structure. This\nresult is on par with the fact that the traditional identity type induces a\ngroupoid, as exposed by Hofmann \\& Streicher (1994). \n\n"}
{"id": "1504.05811", "contents": "Title: Learning of Behavior Trees for Autonomous Agents Abstract: Definition of an accurate system model for Automated Planner (AP) is often\nimpractical, especially for real-world problems. Conversely, off-the-shelf\nplanners fail to scale up and are domain dependent. These drawbacks are\ninherited from conventional transition systems such as Finite State Machines\n(FSMs) that describes the action-plan execution generated by the AP. On the\nother hand, Behavior Trees (BTs) represent a valid alternative to FSMs\npresenting many advantages in terms of modularity, reactiveness, scalability\nand domain-independence. In this paper, we propose a model-free AP framework\nusing Genetic Programming (GP) to derive an optimal BT for an autonomous agent\nto achieve a given goal in unknown (but fully observable) environments. We\nillustrate the proposed framework using experiments conducted with an open\nsource benchmark Mario AI for automated generation of BTs that can play the\ngame character Mario to complete a certain level at various levels of\ndifficulty to include enemies and obstacles. \n\n"}
{"id": "1504.06187", "contents": "Title: LTL Fragments are Hard for Standard Parameterisations Abstract: We classify the complexity of the LTL satisfiability and model checking\nproblems for several standard parameterisations. The investigated parameters\nare temporal depth, number of propositional variables and formula treewidth,\nresp., pathwidth. We show that all operator fragments of LTL under the\ninvestigated parameterisations are intractable in the sense of parameterised\ncomplexity. \n\n"}
{"id": "1504.06316", "contents": "Title: Interactive Communication with Unknown Noise Rate Abstract: Alice and Bob want to run a protocol over a noisy channel, where a certain\nnumber of bits are flipped adversarially. Several results take a protocol\nrequiring $L$ bits of noise-free communication and make it robust over such a\nchannel. In a recent breakthrough result, Haeupler described an algorithm that\nsends a number of bits that is conjectured to be near optimal in such a model.\nHowever, his algorithm critically requires $a \\ priori$ knowledge of the number\nof bits that will be flipped by the adversary.\n  We describe an algorithm requiring no such knowledge. If an adversary flips\n$T$ bits, our algorithm sends $L + O\\left(\\sqrt{L(T+1)\\log L} + T\\right)$ bits\nin expectation and succeeds with high probability in $L$. It does so without\nany $a \\ priori$ knowledge of $T$. Assuming a conjectured lower bound by\nHaeupler, our result is optimal up to logarithmic factors.\n  Our algorithm critically relies on the assumption of a private channel. We\nshow that privacy is necessary when the amount of noise is unknown. \n\n"}
{"id": "1504.07056", "contents": "Title: A Deterministic Almost-Tight Distributed Algorithm for Approximating\n  Single-Source Shortest Paths Abstract: We present a deterministic $(1+o(1))$-approximation\n$(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for solving the single-source\nshortest paths problem on distributed weighted networks (the CONGEST model);\nhere $n$ is the number of nodes in the network and $D$ is its (hop) diameter.\nThis is the first non-trivial deterministic algorithm for this problem. It also\nimproves (i) the running time of the randomized $(1+o(1))$-approximation\n$\\tilde O(n^{1/2}D^{1/4}+D)$-time algorithm of Nanongkai [STOC 2014] by a\nfactor of as large as $n^{1/8}$, and (ii) the $O(\\epsilon^{-1}\\log\n\\epsilon^{-1})$-approximation factor of Lenzen and Patt-Shamir's $\\tilde\nO(n^{1/2+\\epsilon}+D)$-time algorithm [STOC 2013] within the same running time.\nOur running time matches the known time lower bound of $\\Omega(n^{1/2}/\\log n +\nD)$ [Elkin STOC 2004] up to subpolynomial factors, thus essentially settling\nthe status of this problem which was raised at least a decade ago [Elkin SIGACT\nNews 2004]. It also implies a $(2+o(1))$-approximation\n$(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for approximating a network's\nweighted diameter which almost matches the lower bound by Holzer and Pinsker\n[OPODIS 2015]. In achieving this result, we develop two techniques which might\nbe of independent interest and useful in other settings: (i) a deterministic\nprocess that replaces the \"hitting set argument\" commonly used for shortest\npaths computation in various settings, and (ii) a simple, deterministic,\nconstruction of an $(n^{o(1)}, o(1))$-hop set of size $n^{1+o(1)}$. We combine\nthese techniques with many distributed algorithmic techniques, some of which\nfrom problems that are not directly related to shortest paths, e.g., ruling\nsets [Goldberg et al. STOC 1987], source detection [Lenzen and Peleg PODC\n2013], and partial distance estimation [Lenzen and Patt-Shamir PODC 2015]. \n\n"}
{"id": "1504.07313", "contents": "Title: Private Disclosure of Information in Health Tele-monitoring Abstract: We present a novel framework, called Private Disclosure of Information (PDI),\nwhich is aimed to prevent an adversary from inferring certain sensitive\ninformation about subjects using the data that they disclosed during\ncommunication with an intended recipient. We show cases where it is possible to\nachieve perfect privacy regardless of the adversary's auxiliary knowledge while\npreserving full utility of the information to the intended recipient and\nprovide sufficient conditions for such cases. We also demonstrate the\napplicability of PDI on a real-world data set that simulates a health\ntele-monitoring scenario. \n\n"}
{"id": "1505.00138", "contents": "Title: Compositional Distributional Semantics with Compact Closed Categories\n  and Frobenius Algebras Abstract: This thesis contributes to ongoing research related to the categorical\ncompositional model for natural language of Coecke, Sadrzadeh and Clark in\nthree ways: Firstly, I propose a concrete instantiation of the abstract\nframework based on Frobenius algebras (joint work with Sadrzadeh). The theory\nimproves shortcomings of previous proposals, extends the coverage of the\nlanguage, and is supported by experimental work that improves existing results.\nThe proposed framework describes a new class of compositional models that find\nintuitive interpretations for a number of linguistic phenomena. Secondly, I\npropose and evaluate in practice a new compositional methodology which\nexplicitly deals with the different levels of lexical ambiguity (joint work\nwith Pulman). A concrete algorithm is presented, based on the separation of\nvector disambiguation from composition in an explicit prior step. Extensive\nexperimental work shows that the proposed methodology indeed results in more\naccurate composite representations for the framework of Coecke et al. in\nparticular and every other class of compositional models in general. As a last\ncontribution, I formalize the explicit treatment of lexical ambiguity in the\ncontext of the categorical framework by resorting to categorical quantum\nmechanics (joint work with Coecke). In the proposed extension, the concept of a\ndistributional vector is replaced with that of a density matrix, which\ncompactly represents a probability distribution over the potential different\nmeanings of the specific word. Composition takes the form of quantum\nmeasurements, leading to interesting analogies between quantum physics and\nlinguistics. \n\n"}
{"id": "1505.01098", "contents": "Title: Towards concept analysis in categories: limit inferior as algebra, limit\n  superior as coalgebra Abstract: While computer programs and logical theories begin by declaring the concepts\nof interest, be it as data types or as predicates, network computation does not\nallow such global declarations, and requires *concept mining* and *concept\nanalysis* to extract shared semantics for different network nodes. Powerful\nsemantic analysis systems have been the drivers of nearly all paradigm shifts\non the web. In categorical terms, most of them can be described as\nbicompletions of enriched matrices, generalizing the Dedekind-MacNeille-style\ncompletions from posets to suitably enriched categories. Yet it has been well\nknown for more than 40 years that ordinary categories themselves in general do\nnot permit such completions. Armed with this new semantical view of\nDedekind-MacNeille completions, and of matrix bicompletions, we take another\nlook at this ancient mystery. It turns out that simple categorical versions of\nthe *limit superior* and *limit inferior* operations characterize a general\nnotion of Dedekind-MacNeille completion, that seems to be appropriate for\nordinary categories, and boils down to the more familiar enriched versions when\nthe limits inferior and superior coincide. This explains away the apparent gap\namong the completions of ordinary categories, and broadens the path towards\ncategorical concept mining and analysis, opened in previous work. \n\n"}
{"id": "1505.02091", "contents": "Title: Weihrauch-completeness for layerwise computability Abstract: We introduce the notion of being Weihrauch-complete for layerwise\ncomputability and provide several natural examples related to complex\noscillations, the law of the iterated logarithm and Birkhoff's theorem. We also\nconsider hitting time operators, which share the Weihrauch degree of the former\nexamples but fail to be layerwise computable. \n\n"}
{"id": "1505.04308", "contents": "Title: Time vs. Information Tradeoffs for Leader Election in Anonymous Trees Abstract: The leader election task calls for all nodes of a network to agree on a\nsingle node. If the nodes of the network are anonymous, the task of leader\nelection is formulated as follows: every node $v$ of the network must output a\nsimple path, coded as a sequence of port numbers, such that all these paths end\nat a common node, the leader. In this paper, we study deterministic leader\nelection in anonymous trees.\n  Our aim is to establish tradeoffs between the allocated time $\\tau$ and the\namount of information that has to be given $\\textit{a priori}$ to the nodes to\nenable leader election in time $\\tau$ in all trees for which leader election in\nthis time is at all possible. Following the framework of $\\textit{algorithms\nwith advice}$, this information (a single binary string) is provided to all\nnodes at the start by an oracle knowing the entire tree. The length of this\nstring is called the $\\textit{size of advice}$. For an allocated time $\\tau$,\nwe give upper and lower bounds on the minimum size of advice sufficient to\nperform leader election in time $\\tau$.\n  We consider $n$-node trees of diameter $diam \\leq D$. While leader election\nin time $diam$ can be performed without any advice, for time $diam-1$ we give\ntight upper and lower bounds of $\\Theta (\\log D)$. For time $diam-2$ we give\ntight upper and lower bounds of $\\Theta (\\log D)$ for even values of $diam$,\nand tight upper and lower bounds of $\\Theta (\\log n)$ for odd values of $diam$.\nFor the time interval $[\\beta \\cdot diam, diam-3]$ for constant $\\beta >1/2$,\nwe prove an upper bound of $O(\\frac{n\\log n}{D})$ and a lower bound of\n$\\Omega(\\frac{n}{D})$, the latter being valid whenever $diam$ is odd or when\nthe time is at most $diam-4$. Finally, for time $\\alpha \\cdot diam$ for any\nconstant $\\alpha <1/2$ (except for the case of very small diameters), we give\ntight upper and lower bounds of $\\Theta (n)$. \n\n"}
{"id": "1505.04694", "contents": "Title: Thread Parallelism for Highly Irregular Computation in Anisotropic Mesh\n  Adaptation Abstract: Thread-level parallelism in irregular applications with mutable data\ndependencies presents challenges because the underlying data is extensively\nmodified during execution of the algorithm and a high degree of parallelism\nmust be realized while keeping the code race-free. In this article we describe\na methodology for exploiting thread parallelism for a class of graph-mutating\nworklist algorithms, which guarantees safe parallel execution via processing in\nrounds of independent sets and using a deferred update strategy to commit\nchanges in the underlying data structures. Scalability is assisted by atomic\nfetch-and-add operations to create worklists and work-stealing to balance the\nshared-memory workload. This work is motivated by mesh adaptation algorithms,\nfor which we show a parallel efficiency of 60% and 50% on Intel(R) Xeon(R)\nSandy Bridge and AMD Opteron(tm) Magny-Cours systems, respectively, using these\ntechniques. \n\n"}
{"id": "1506.00893", "contents": "Title: SkILL - a Stochastic Inductive Logic Learner Abstract: Probabilistic Inductive Logic Programming (PILP) is a rel- atively unexplored\narea of Statistical Relational Learning which extends classic Inductive Logic\nProgramming (ILP). This work introduces SkILL, a Stochastic Inductive Logic\nLearner, which takes probabilistic annotated data and produces First Order\nLogic theories. Data in several domains such as medicine and bioinformatics\nhave an inherent degree of uncer- tainty, that can be used to produce models\ncloser to reality. SkILL can not only use this type of probabilistic data to\nextract non-trivial knowl- edge from databases, but it also addresses\nefficiency issues by introducing a novel, efficient and effective search\nstrategy to guide the search in PILP environments. The capabilities of SkILL\nare demonstrated in three dif- ferent datasets: (i) a synthetic toy example\nused to validate the system, (ii) a probabilistic adaptation of a well-known\nbiological metabolism ap- plication, and (iii) a real world medical dataset in\nthe breast cancer domain. Results show that SkILL can perform as well as a\ndeterministic ILP learner, while also being able to incorporate probabilistic\nknowledge that would otherwise not be considered. \n\n"}
{"id": "1506.01602", "contents": "Title: A Concurrency Problem with Exponential DPLL(T) Proofs Abstract: Many satisfiability modulo theories solvers implement a variant of the DPLL(T\n) framework which separates theory-specific reasoning from reasoning on the\npropositional abstraction of the formula. Such solvers conclude that a formula\nis unsatisfiable once they have learned enough theory conflicts to derive a\npropositional contradiction. However some problems, such as the diamonds\nproblem, require learning exponentially many conflicts. We give a general\ncriterion for establishing lower bounds on the number of theory conflicts in\nany DPLL(T ) proof for a given problem. We apply our criterion to two different\nstate-of-the-art symbolic partial-order encodings of a simple, yet\nrepresentative concurrency problem. Even though one of the encodings is\nasymptotically smaller than the other, we establish the same exponential lower\nbound proof complexity for both. Our experiments confirm this theoretical lower\nbound across multiple solvers and theory combinations. \n\n"}
{"id": "1506.03378", "contents": "Title: On the Prior Sensitivity of Thompson Sampling Abstract: The empirically successful Thompson Sampling algorithm for stochastic bandits\nhas drawn much interest in understanding its theoretical properties. One\nimportant benefit of the algorithm is that it allows domain knowledge to be\nconveniently encoded as a prior distribution to balance exploration and\nexploitation more effectively. While it is generally believed that the\nalgorithm's regret is low (high) when the prior is good (bad), little is known\nabout the exact dependence. In this paper, we fully characterize the\nalgorithm's worst-case dependence of regret on the choice of prior, focusing on\na special yet representative case. These results also provide insights into the\ngeneral sensitivity of the algorithm to the choice of priors. In particular,\nwith $p$ being the prior probability mass of the true reward-generating model,\nwe prove $O(\\sqrt{T/p})$ and $O(\\sqrt{(1-p)T})$ regret upper bounds for the\nbad- and good-prior cases, respectively, as well as \\emph{matching} lower\nbounds. Our proofs rely on the discovery of a fundamental property of Thompson\nSampling and make heavy use of martingale theory, both of which appear novel in\nthe literature, to the best of our knowledge. \n\n"}
{"id": "1506.06057", "contents": "Title: Multi-sorted logic, models and logical geometry Abstract: Let $\\Theta$ be a variety of algebras, $(H, \\Psi, f)$ be a model, where $H$\nis an algebra from $\\Theta$, $\\Psi$ is a set of relation symbols $\\varphi$, $f$\nis an interpretation of all $\\varphi$ in $H$. Let $X^0$ be an infinite set of\nvariables, $\\Gamma$ be a collection of all finite subsets in $X^0$ (collection\nof sorts), $\\widetilde\\Phi$ be the multi-sorted algebra of formulas. These data\ndefine a knowledge base $KB(H,\\Psi, f)$. In the paper the notion of isomorphism\nof knowledge bases is considered. We give sufficient conditions which provide\nisomorphism of knowledge bases. We also study the problem of necessary and\nsufficient conditions for isomorphism of two knowledge bases. \n\n"}
{"id": "1506.08726", "contents": "Title: The First Reactive Synthesis Competition (SYNTCOMP 2014) Abstract: We introduce the reactive synthesis competition (SYNTCOMP), a long-term\neffort intended to stimulate and guide advances in the design and application\nof synthesis procedures for reactive systems. The first iteration of SYNTCOMP\nis based on the controller synthesis problem for finite-state systems and\nsafety specifications. We provide an overview of this problem and existing\napproaches to solve it, and report on the design and results of the first\nSYNTCOMP. This includes the definition of the benchmark format, the collection\nof benchmarks, the rules of the competition, and the five synthesis tools that\nparticipated. We present and analyze the results of the competition and draw\nconclusions on the state of the art. Finally, we give an outlook on future\ndirections of SYNTCOMP. \n\n"}
{"id": "1506.09067", "contents": "Title: The Potential of the Intel Xeon Phi for Supervised Deep Learning Abstract: Supervised learning of Convolutional Neural Networks (CNNs), also known as\nsupervised Deep Learning, is a computationally demanding process. To find the\nmost suitable parameters of a network for a given application, numerous\ntraining sessions are required. Therefore, reducing the training time per\nsession is essential to fully utilize CNNs in practice. While numerous research\ngroups have addressed the training of CNNs using GPUs, so far not much\nattention has been paid to the Intel Xeon Phi coprocessor. In this paper we\ninvestigate empirically and theoretically the potential of the Intel Xeon Phi\nfor supervised learning of CNNs. We design and implement a parallelization\nscheme named CHAOS that exploits both the thread- and SIMD-parallelism of the\ncoprocessor. Our approach is evaluated on the Intel Xeon Phi 7120P using the\nMNIST dataset of handwritten digits for various thread counts and CNN\narchitectures. Results show a 103.5x speed up when training our large network\nfor 15 epochs using 244 threads, compared to one thread on the coprocessor.\nMoreover, we develop a performance model and use it to assess our\nimplementation and answer what-if questions. \n\n"}
{"id": "1507.00772", "contents": "Title: Optimal and Resilient Pheromone Utilization in Ant Foraging Abstract: Pheromones are a chemical substance produced and released by ants as means of\ncommunication. In this work we present the minimum amount of pheromones\nnecessary and sufficient for a colony of ants (identical mobile agents) to\ndeterministically find a food source (treasure), assuming that each ant has the\ncomputational capabilities of either a Finite State Machine (FSM) or a Turing\nMachine (TM). In addition, we provide pheromone-based foraging algorithms\ncapable of handling fail-stop faults.\n  In more detail, we consider the case where $k$ identical ants, initially\nlocated at the center (nest) of an infinite two-dimensional grid and\ncommunicate only through pheromones, perform a collaborative search for an\nadversarially hidden treasure placed at an unknown distance $D$. We begin by\nproving a tight lower bound of $\\Omega(D)$ on the amount of pheromones required\nby any number of FSM based ants to complete the search, and continue to reduce\nthe lower bound to $\\Omega(k)$ for the stronger ants modeled as TM. We provide\nalgorithms which match the aforementioned lower bounds, and still terminate in\noptimal $\\mathcal{O}(D + D^2 / k)$ time, under both the synchronous and\nasynchronous models. Furthermore, we consider a more realistic setting, where\nan unknown number $f < k$ of ants may fail-stop at any time; we provide\nfault-tolerant FSM algorithms (synchronous and asynchronous), that terminate in\n$\\mathcal{O}(D + D^2/(k-f) + Df)$ rounds and emit no more than the same\nasymptotic minimum number of $\\mathcal{O}(D)$ pheromones overall. \n\n"}
{"id": "1507.00909", "contents": "Title: Node Labels in Local Decision Abstract: The role of unique node identifiers in network computing is well understood\nas far as symmetry breaking is concerned. However, the unique identifiers also\nleak information about the computing environment - in particular, they provide\nsome nodes with information related to the size of the network. It was recently\nproved that in the context of local decision, there are some decision problems\nsuch that (1) they cannot be solved without unique identifiers, and (2) unique\nnode identifiers leak a sufficient amount of information such that the problem\nbecomes solvable (PODC 2013).\n  In this work we give study what is the minimal amount of information that we\nneed to leak from the environment to the nodes in order to solve local decision\nproblems. Our key results are related to scalar oracles $f$ that, for any given\n$n$, provide a multiset $f(n)$ of $n$ labels; then the adversary assigns the\nlabels to the $n$ nodes in the network. This is a direct generalisation of the\nusual assumption of unique node identifiers. We give a complete\ncharacterisation of the weakest oracle that leaks at least as much information\nas the unique identifiers.\n  Our main result is the following dichotomy: we classify scalar oracles as\nlarge and small, depending on their asymptotic behaviour, and show that (1) any\nlarge oracle is at least as powerful as the unique identifiers in the context\nof local decision problems, while (2) for any small oracle there are local\ndecision problems that still benefit from unique identifiers. \n\n"}
{"id": "1507.01461", "contents": "Title: Revisiting Large Scale Distributed Machine Learning Abstract: Nowadays, with the widespread of smartphones and other portable gadgets\nequipped with a variety of sensors, data is ubiquitous available and the focus\nof machine learning has shifted from being able to infer from small training\nsamples to dealing with large scale high-dimensional data. In domains such as\npersonal healthcare applications, which motivates this survey, distributed\nmachine learning is a promising line of research, both for scaling up learning\nalgorithms, but mostly for dealing with data which is inherently produced at\ndifferent locations. This report offers a thorough overview of and\nstate-of-the-art algorithms for distributed machine learning, for both\nsupervised and unsupervised learning, ranging from simple linear logistic\nregression to graphical models and clustering. We propose future directions for\nmost categories, specific to the potential personal healthcare applications.\nWith this in mind, the report focuses on how security and low communication\noverhead can be assured in the specific case of a strictly client-server\narchitectural model. As particular directions we provides an exhaustive\npresentation of an empirical clustering algorithm, k-windows, and proposed an\nasynchronous distributed machine learning algorithm that would scale well and\nalso would be computationally cheap and easy to implement. \n\n"}
{"id": "1507.02563", "contents": "Title: Managing Autonomous Mobility on Demand Systems for Better Passenger\n  Experience Abstract: Autonomous mobility on demand systems, though still in their infancy, have\nvery promising prospects in providing urban population with sustainable and\nsafe personal mobility in the near future. While much research has been\nconducted on both autonomous vehicles and mobility on demand systems, to the\nbest of our knowledge, this is the first work that shows how to manage\nautonomous mobility on demand systems for better passenger experience. We\nintroduce the Expand and Target algorithm which can be easily integrated with\nthree different scheduling strategies for dispatching autonomous vehicles. We\nimplement an agent-based simulation platform and empirically evaluate the\nproposed approaches with the New York City taxi data. Experimental results\ndemonstrate that the algorithm significantly improve passengers' experience by\nreducing the average passenger waiting time by up to 29.82% and increasing the\ntrip success rate by up to 7.65%. \n\n"}
{"id": "1507.03979", "contents": "Title: Planning as Tabled Logic Programming Abstract: This paper describes Picat's planner, its implementation, and planning models\nfor several domains used in International Planning Competition (IPC) 2014.\nPicat's planner is implemented by use of tabling. During search, every state\nencountered is tabled, and tabled states are used to effectively perform\nresource-bounded search. In Picat, structured data can be used to avoid\nenumerating all possible permutations of objects, and term sharing is used to\navoid duplication of common state data. This paper presents several modeling\ntechniques through the example models, ranging from designing state\nrepresentations to facilitate data sharing and symmetry breaking, encoding\nactions with operations for efficient precondition checking and state updating,\nto incorporating domain knowledge and heuristics. Broadly, this paper\ndemonstrates the effectiveness of tabled logic programming for planning, and\nargues the importance of modeling despite recent significant progress in\ndomain-independent PDDL planners. \n\n"}
{"id": "1507.05086", "contents": "Title: Parallel Correlation Clustering on Big Graphs Abstract: Given a similarity graph between items, correlation clustering (CC) groups\nsimilar items together and dissimilar ones apart. One of the most popular CC\nalgorithms is KwikCluster: an algorithm that serially clusters neighborhoods of\nvertices, and obtains a 3-approximation ratio. Unfortunately, KwikCluster in\npractice requires a large number of clustering rounds, a potential bottleneck\nfor large graphs.\n  We present C4 and ClusterWild!, two algorithms for parallel correlation\nclustering that run in a polylogarithmic number of rounds and achieve nearly\nlinear speedups, provably. C4 uses concurrency control to enforce\nserializability of a parallel clustering process, and guarantees a\n3-approximation ratio. ClusterWild! is a coordination free algorithm that\nabandons consistency for the benefit of better scaling; this leads to a\nprovably small loss in the 3-approximation ratio.\n  We provide extensive experimental results for both algorithms, where we\noutperform the state of the art, both in terms of clustering accuracy and\nrunning time. We show that our algorithms can cluster billion-edge graphs in\nunder 5 seconds on 32 cores, while achieving a 15x speedup. \n\n"}
{"id": "1507.05581", "contents": "Title: On Automated Lemma Generation for Separation Logic with Inductive\n  Definitions Abstract: Separation Logic with inductive definitions is a well-known approach for\ndeductive verification of programs that manipulate dynamic data structures.\nDeciding verification conditions in this context is usually based on\nuser-provided lemmas relating the inductive definitions. We propose a novel\napproach for generating these lemmas automatically which is based on simple\nsyntactic criteria and deterministic strategies for applying them. Our approach\nfocuses on iterative programs, although it can be applied to recursive programs\nas well, and specifications that describe not only the shape of the data\nstructures, but also their content or their size. Empirically, we find that our\napproach is powerful enough to deal with sophisticated benchmarks, e.g.,\niterative procedures for searching, inserting, or deleting elements in sorted\nlists, binary search tress, red-black trees, and AVL trees, in a very efficient\nway. \n\n"}
{"id": "1508.01504", "contents": "Title: Resource Oblivious Sorting on Multicores Abstract: We present a deterministic sorting algorithm, SPMS (Sample, Partition, and\nMerge Sort), that interleaves the partitioning of a sample sort with merging.\nSequentially, it sorts $n$ elements in $O(n \\log n)$ time cache-obliviously\nwith an optimal number of cache misses. The parallel complexity (or critical\npath length) of the algorithm is $O(\\log n \\cdot \\log\\log n)$, which improves\non previous bounds for optimal cache oblivious sorting. The algorithm also has\nlow false sharing costs. When scheduled by a work-stealing scheduler in a\nmulticore computing environment with a global shared memory and $p$ cores, each\nhaving a cache of size $M$ organized in blocks of size $B$, the costs of the\nadditional cache misses and false sharing misses due to this parallel execution\nare bounded by the cost of $O(S\\cdot M/B)$ and $O(S \\cdot B)$ cache misses\nrespectively, where $S$ is the number of steals performed during the execution.\nFinally, SPMS is resource oblivious in Athat the dependence on machine\nparameters appear only in the analysis of its performance, and not within the\nalgorithm itself. \n\n"}
{"id": "1508.02344", "contents": "Title: Local Algorithms for Block Models with Side Information Abstract: There has been a recent interest in understanding the power of local\nalgorithms for optimization and inference problems on sparse graphs. Gamarnik\nand Sudan (2014) showed that local algorithms are weaker than global algorithms\nfor finding large independent sets in sparse random regular graphs. Montanari\n(2015) showed that local algorithms are suboptimal for finding a community with\nhigh connectivity in the sparse Erd\\H{o}s-R\\'enyi random graphs. For the\nsymmetric planted partition problem (also named community detection for the\nblock models) on sparse graphs, a simple observation is that local algorithms\ncannot have non-trivial performance.\n  In this work we consider the effect of side information on local algorithms\nfor community detection under the binary symmetric stochastic block model. In\nthe block model with side information each of the $n$ vertices is labeled $+$\nor $-$ independently and uniformly at random; each pair of vertices is\nconnected independently with probability $a/n$ if both of them have the same\nlabel or $b/n$ otherwise. The goal is to estimate the underlying vertex\nlabeling given 1) the graph structure and 2) side information in the form of a\nvertex labeling positively correlated with the true one. Assuming that the\nratio between in and out degree $a/b$ is $\\Theta(1)$ and the average degree $\n(a+b) / 2 = n^{o(1)}$, we characterize three different regimes under which a\nlocal algorithm, namely, belief propagation run on the local neighborhoods,\nmaximizes the expected fraction of vertices labeled correctly. Thus, in\ncontrast to the case of symmetric block models without side information, we\nshow that local algorithms can achieve optimal performance for the block model\nwith side information. \n\n"}
{"id": "1508.03519", "contents": "Title: On the Voting Time of the Deterministic Majority Process Abstract: In the deterministic binary majority process we are given a simple graph\nwhere each node has one out of two initial opinions. In every round, every node\nadopts the majority opinion among its neighbors. By using a potential argument\nfirst discovered by Goles and Olivos (1980), it is known that this process\nalways converges in $O(|E|)$ rounds to a two-periodic state in which every node\neither keeps its opinion or changes it in every round.\n  It has been shown by Frischknecht, Keller, and Wattenhofer (2013) that the\n$O(|E|)$ bound on the convergence time of the deterministic binary majority\nprocess is indeed tight even for dense graphs. However, in many graphs such as\nthe complete graph, from any initial opinion assignment, the process converges\nin just a constant number of rounds.\n  By carefully exploiting the structure of the potential function by Goles and\nOlivos (1980), we derive a new upper bound on the convergence time of the\ndeterministic binary majority process that accounts for such exceptional cases.\nWe show that it is possible to identify certain modules of a graph $G$ in order\nto obtain a new graph $G^\\Delta$ with the property that the worst-case\nconvergence time of $G^\\Delta$ is an upper bound on that of $G$. Moreover, even\nthough our upper bound can be computed in linear time, we show that, given an\ninteger $k$, it is NP-hard to decide whether there exists an initial opinion\nassignment for which it takes more than $k$ rounds to converge to the\ntwo-periodic state. \n\n"}
{"id": "1508.04849", "contents": "Title: Orchestrated Session Compliance Abstract: We investigate the notion of orchestrated compliance for client/server\ninteractions in the context of session contracts. Devising the notion of\norchestrator in such a context makes it possible to have orchestrators with\nunbounded buffering capabilities and at the same time to guarantee any message\nfrom the client to be eventually delivered by the orchestrator to the server,\nwhile preventing the server from sending messages which are kept indefinitely\ninside the orchestrator. The compliance relation is shown to be decidable by\nmeans of 1) a procedure synthesising the orchestrators, if any, making a client\ncompliant with a server, and 2) a procedure for deciding whether an\norchestrator behaves in a proper way as mentioned before. \n\n"}
{"id": "1508.05465", "contents": "Title: A representation of antimatroids by Horn rules and its application to\n  educational systems Abstract: We study a representation of an antimatroid by Horn rules, motivated by its\nrecent application to computer-aided educational systems. We associate any set\n$\\mathcal{R}$ of Horn rules with the unique maximal antimatroid\n$\\mathcal{A}(\\mathcal{R})$ that is contained in the union-closed family\n$\\mathcal{K}(\\mathcal{R})$ naturally determined by ${\\cal R}$. We address\nalgorithmic and Boolean function theoretic aspects on the association ${\\cal R}\n\\mapsto \\mathcal{A}(\\mathcal{R})$, where ${\\cal R}$ is viewed as the input. We\npresent linear time algorithms to solve the membership problem and the\ninference problem for ${\\cal A}({\\cal R})$. We also provide efficient\nalgorithms for generating all members and all implicates of ${\\cal A}({\\cal\nR})$. We show that this representation is essentially equivalent to the\nKorte-Lov\\'{a}sz representation of antimatroids by rooted sets. Based on the\nequivalence, we provide a quadratic time algorithm to construct the\nuniquely-determined minimal representation. % These results have potential\napplications to computer-aided educational systems, where an antimatroid is\nused as a model of the space of possible knowledge states of learners, and is\nconstructed by giving Horn queries to a human expert. \n\n"}
{"id": "1508.06731", "contents": "Title: NETCS: A New Simulator of Population Protocols and Network Constructors Abstract: Network Constructors are an extension of the standard population protocol\nmodel in which finite-state agents interact in pairs under the control of an\nadversary scheduler. In this work we present NETCS, a simulator designed to\nevaluate the performance of various network constructors and population\nprotocols under different schedulers and network configurations. Our simulator\nprovides researchers with an intuitive user interface and a quick\nexperimentation environment to evaluate their work. It also harnesses the power\nof the cloud, as experiments are executed remotely and scheduled through the\nweb interface provided. To prove the validity and quality of our simulator we\nprovide an extensive evaluation of multiple protocols with more than 100000\nexperiments for different network sizes and configurations that validate the\ncorrectness of the theoretical analysis of existing protocols and estimate the\nreal values of the hidden asymptotic coefficients. We also show experimentally\n(with more than 40000 experiments) that a probabilistic algorithm is capable of\ncounting the actual size of the network in bounded time given a unique leader. \n\n"}
{"id": "1508.06924", "contents": "Title: Using Thought-Provoking Children's Questions to Drive Artificial\n  Intelligence Research Abstract: We propose to use thought-provoking children's questions (TPCQs), namely\nHighlights BrainPlay questions, as a new method to drive artificial\nintelligence research and to evaluate the capabilities of general-purpose AI\nsystems. These questions are designed to stimulate thought and learning in\nchildren, and they can be used to do the same thing in AI systems, while\ndemonstrating the system's reasoning capabilities to the evaluator. We\nintroduce the TPCQ task, which which takes a TPCQ question as input and\nproduces as output (1) answers to the question and (2) learned generalizations.\nWe discuss how BrainPlay questions stimulate learning. We analyze 244 BrainPlay\nquestions, and we report statistics on question type, question class, answer\ncardinality, answer class, types of knowledge needed, and types of reasoning\nneeded. We find that BrainPlay questions span many aspects of intelligence.\nBecause the answers to BrainPlay questions and the generalizations learned from\nthem are often highly open-ended, we suggest using human judges for evaluation. \n\n"}
{"id": "1509.01183", "contents": "Title: Parallel Knowledge Embedding with MapReduce on a Multi-core Processor Abstract: This article firstly attempts to explore parallel algorithms of learning\ndistributed representations for both entities and relations in large-scale\nknowledge repositories with {\\it MapReduce} programming model on a multi-core\nprocessor. We accelerate the training progress of a canonical knowledge\nembedding method, i.e. {\\it translating embedding} ({\\bf TransE}) model, by\ndividing a whole knowledge repository into several balanced subsets, and\nfeeding each subset into an individual core where local embeddings can\nconcurrently run updating during the {\\it Map} phase. However, it usually\nsuffers from inconsistent low-dimensional vector representations of the same\nkey, which are collected from different {\\it Map} workers, and further leads to\nconflicts when conducting {\\it Reduce} to merge the various vectors associated\nwith the same key. Therefore, we try several strategies to acquire the merged\nembeddings which may not only retain the performance of {\\it entity inference},\n{\\it relation prediction}, and even {\\it triplet classification} evaluated by\nthe single-thread {\\bf TransE} on several well-known knowledge bases such as\nFreebase and NELL, but also scale up the learning speed along with the number\nof cores within a processor. So far, the empirical studies show that we could\nachieve comparable results as the single-thread {\\bf TransE} performs by the\n{\\it stochastic gradient descend} (SGD) algorithm, as well as increase the\ntraining speed multiple times via adapting the {\\it batch gradient descend}\n(BGD) algorithm for {\\it MapReduce} paradigm. \n\n"}
{"id": "1509.01481", "contents": "Title: A Comparison of Computational Models for the Extracellular Potential of\n  Neurons Abstract: The extracellular space has an ambiguous role in neuroscience. It is present\nin every physiologically relevant system and often used as a measurement site\nin experimental recordings, but it has received subordinate attention compared\nto the intracellular domain. In computational modeling, it is often regarded as\na passive, homogeneous resistive medium with a constant conductivity, which\ngreatly simplifies the computation of extracellular potentials. However, recent\nstudies have shown that local ionic diffusion and capacitive effects of\nelectrically active membranes can have a substantial impact on the\nextracellular potential. These effects can not be described by traditional\nmodels, and they have been subject to theoretical and experimental analyses. We\nstrive to give an overview over recent progress in modeling the extracellular\nspace with special regard towards the concentration and potential dynamics on\ndifferent temporal and spatial scales. Three models with distinct assumptions\nand levels of detail are compared both theoretically and by means of numerical\nsimulations: the classical volume conductor (VC) model, which is most\nfrequently used in form of the line source approximation (LSA); the very\ndetailed, but computationally intensive Poisson-Nernst-Planck model of\nelectrodiffusion (PNP); and an intermediate one called the electroneutral model\n(EN). The results clearly show that there is no one model for all applications,\nas they show significantly different responses especially close to neuronal\nmembranes. Finally, we list some common use cases for model simulations and\ngive recommendations on which model to use in each situation. \n\n"}
{"id": "1509.01506", "contents": "Title: Analyzing large-scale DNA Sequences on Multi-core Architectures Abstract: Rapid analysis of DNA sequences is important in preventing the evolution of\ndifferent viruses and bacteria during an early phase, early diagnosis of\ngenetic predispositions to certain diseases (cancer, cardiovascular diseases),\nand in DNA forensics. However, real-world DNA sequences may comprise several\nGigabytes and the process of DNA analysis demands adequate computational\nresources to be completed within a reasonable time. In this paper we present a\nscalable approach for parallel DNA analysis that is based on Finite Automata,\nand which is suitable for analyzing very large DNA segments. We evaluate our\napproach for real-world DNA segments of mouse (2.7GB), cat (2.4GB), dog\n(2.4GB), chicken (1GB), human (3.2GB) and turkey (0.2GB). Experimental results\non a dual-socket shared-memory system with 24 physical cores show speed-ups of\nup to 17.6x. Our approach is up to 3x faster than a pattern-based parallel\napproach that uses the RE2 library. \n\n"}
{"id": "1509.02256", "contents": "Title: Matrix Computations and Optimization in Apache Spark Abstract: We describe matrix computations available in the cluster programming\nframework, Apache Spark. Out of the box, Spark provides abstractions and\nimplementations for distributed matrices and optimization routines using these\nmatrices. When translating single-node algorithms to run on a distributed\ncluster, we observe that often a simple idea is enough: separating matrix\noperations from vector operations and shipping the matrix operations to be ran\non the cluster, while keeping vector operations local to the driver. In the\ncase of the Singular Value Decomposition, by taking this idea to an extreme, we\nare able to exploit the computational power of a cluster, while running code\nwritten decades ago for a single core. Another example is our Spark port of the\npopular TFOCS optimization package, originally built for MATLAB, which allows\nfor solving Linear programs as well as a variety of other convex programs. We\nconclude with a comprehensive set of benchmarks for hardware accelerated matrix\ncomputations from the JVM, which is interesting in its own right, as many\ncluster programming frameworks use the JVM. The contributions described in this\npaper are already merged into Apache Spark and available on Spark installations\nby default, and commercially supported by a slew of companies which provide\nfurther services. \n\n"}
{"id": "1509.02464", "contents": "Title: Characterizing and Adapting the Consistency-Latency Tradeoff in\n  Distributed Key-value Stores Abstract: The CAP theorem is a fundamental result that applies to distributed storage\nsystems. In this paper, we first present and prove two CAP-like impossibility\ntheorems. To state these theorems, we present probabilistic models to\ncharacterize the three important elements of the CAP theorem: consistency (C),\navailability or latency (A), and partition tolerance (P). The theorems show the\nun-achievable envelope, i.e., which combinations of the parameters of the three\nmodels make them impossible to achieve together. Next, we present the design of\na class of systems called PCAP that perform close to the envelope described by\nour theorems. In addition, these systems allow applications running on a single\ndata-center to specify either a latency SLA or a consistency SLA. The PCAP\nsystems automatically adapt, in real-time and under changing network\nconditions, to meet the SLA while optimizing the other C/A metric. We\nincorporate PCAP into two popular key-value stores -- Apache Cassandra and\nRiak. Our experiments with these two deployments, under realistic workloads,\nreveal that the PCAP system satisfactorily meets SLAs, and performs close to\nthe achievable envelope. We also extend PCAP from a single data-center to\nmultiple geo-distributed data-centers. \n\n"}
{"id": "1509.03371", "contents": "Title: Efficient Convolutional Neural Networks for Pixelwise Classification on\n  Heterogeneous Hardware Systems Abstract: This work presents and analyzes three convolutional neural network (CNN)\nmodels for efficient pixelwise classification of images. When using\nconvolutional neural networks to classify single pixels in patches of a whole\nimage, a lot of redundant computations are carried out when using sliding\nwindow networks. This set of new architectures solve this issue by either\nremoving redundant computations or using fully convolutional architectures that\ninherently predict many pixels at once.\n  The implementations of the three models are accessible through a new utility\non top of the Caffe library. The utility provides support for a wide range of\nimage input and output formats, pre-processing parameters and methods to\nequalize the label histogram during training. The Caffe library has been\nextended by new layers and a new backend for availability on a wider range of\nhardware such as CPUs and GPUs through OpenCL.\n  On AMD GPUs, speedups of $54\\times$ (SK-Net), $437\\times$ (U-Net) and\n$320\\times$ (USK-Net) have been observed, taking the SK equivalent SW (sliding\nwindow) network as the baseline. The label throughput is up to one megapixel\nper second.\n  The analyzed neural networks have distinctive characteristics that apply\nduring training or processing, and not every data set is suitable to every\narchitecture. The quality of the predictions is assessed on two neural tissue\ndata sets, of which one is the ISBI 2012 challenge data set. Two different loss\nfunctions, Malis loss and Softmax loss, were used during training.\n  The whole pipeline, consisting of models, interface and modified Caffe\nlibrary, is available as Open Source software under the working title Project\nGreentea. \n\n"}
{"id": "1509.03391", "contents": "Title: Modal Characterisations of Behavioural Pseudometrics Abstract: For the model of probabilistic labelled transition systems that allow for the\nco-existence of nondeterminism and probabilities, we present two notions of\nbisimulation metrics: one is state-based and the other is distribution-based.\nWe provide a sound and complete modal characterisation for each of them, using\nreal-valued modal logics based on the Hennessy-Milner logic. The logic for\ncharacterising the state-based metric is much simpler than an earlier logic by\nDesharnais et al. as it uses only two non-expansive operators rather than the\ngeneral class of non-expansive operators. \n\n"}
{"id": "1509.04210", "contents": "Title: Model Accuracy and Runtime Tradeoff in Distributed Deep Learning:A\n  Systematic Study Abstract: This paper presents Rudra, a parameter server based distributed computing\nframework tuned for training large-scale deep neural networks. Using variants\nof the asynchronous stochastic gradient descent algorithm we study the impact\nof synchronization protocol, stale gradient updates, minibatch size, learning\nrates, and number of learners on runtime performance and model accuracy. We\nintroduce a new learning rate modulation strategy to counter the effect of\nstale gradients and propose a new synchronization protocol that can effectively\nbound the staleness in gradients, improve runtime performance and achieve good\nmodel accuracy. Our empirical investigation reveals a principled approach for\ndistributed training of neural networks: the mini-batch size per learner should\nbe reduced as more learners are added to the system to preserve the model\naccuracy. We validate this approach using commonly-used image classification\nbenchmarks: CIFAR10 and ImageNet. \n\n"}
{"id": "1509.07175", "contents": "Title: Exploration and Exploitation of Victorian Science in Darwin's Reading\n  Notebooks Abstract: Search in an environment with an uncertain distribution of resources involves\na trade-off between exploitation of past discoveries and further exploration.\nThis extends to information foraging, where a knowledge-seeker shifts between\nreading in depth and studying new domains. To study this decision-making\nprocess, we examine the reading choices made by one of the most celebrated\nscientists of the modern era: Charles Darwin. From the full-text of books\nlisted in his chronologically-organized reading journals, we generate topic\nmodels to quantify his local (text-to-text) and global (text-to-past) reading\ndecisions using Kullback-Liebler Divergence, a cognitively-validated,\ninformation-theoretic measure of relative surprise. Rather than a pattern of\nsurprise-minimization, corresponding to a pure exploitation strategy, Darwin's\nbehavior shifts from early exploitation to later exploration, seeking unusually\nhigh levels of cognitive surprise relative to previous eras. These shifts,\ndetected by an unsupervised Bayesian model, correlate with major intellectual\nepochs of his career as identified both by qualitative scholarship and Darwin's\nown self-commentary. Our methods allow us to compare his consumption of texts\nwith their publication order. We find Darwin's consumption more exploratory\nthan the culture's production, suggesting that underneath gradual societal\nchanges are the explorations of individual synthesis and discovery. Our\nquantitative methods advance the study of cognitive search through a framework\nfor testing interactions between individual and collective behavior and between\nshort- and long-term consumption choices. This novel application of topic\nmodeling to characterize individual reading complements widespread studies of\ncollective scientific behavior. \n\n"}
{"id": "1510.00331", "contents": "Title: Multimodal Hierarchical Dirichlet Process-based Active Perception Abstract: In this paper, we propose an active perception method for recognizing object\ncategories based on the multimodal hierarchical Dirichlet process (MHDP). The\nMHDP enables a robot to form object categories using multimodal information,\ne.g., visual, auditory, and haptic information, which can be observed by\nperforming actions on an object. However, performing many actions on a target\nobject requires a long time. In a real-time scenario, i.e., when the time is\nlimited, the robot has to determine the set of actions that is most effective\nfor recognizing a target object. We propose an MHDP-based active perception\nmethod that uses the information gain (IG) maximization criterion and lazy\ngreedy algorithm. We show that the IG maximization criterion is optimal in the\nsense that the criterion is equivalent to a minimization of the expected\nKullback--Leibler divergence between a final recognition state and the\nrecognition state after the next set of actions. However, a straightforward\ncalculation of IG is practically impossible. Therefore, we derive an efficient\nMonte Carlo approximation method for IG by making use of a property of the\nMHDP. We also show that the IG has submodular and non-decreasing properties as\na set function because of the structure of the graphical model of the MHDP.\nTherefore, the IG maximization problem is reduced to a submodular maximization\nproblem. This means that greedy and lazy greedy algorithms are effective and\nhave a theoretical justification for their performance. We conducted an\nexperiment using an upper-torso humanoid robot and a second one using synthetic\ndata. The experimental results show that the method enables the robot to select\na set of actions that allow it to recognize target objects quickly and\naccurately. The results support our theoretical outcomes. \n\n"}
{"id": "1510.01463", "contents": "Title: Local Rademacher Complexity Bounds based on Covering Numbers Abstract: This paper provides a general result on controlling local Rademacher\ncomplexities, which captures in an elegant form to relate the complexities with\nconstraint on the expected norm to the corresponding ones with constraint on\nthe empirical norm. This result is convenient to apply in real applications and\ncould yield refined local Rademacher complexity bounds for function classes\nsatisfying general entropy conditions. We demonstrate the power of our\ncomplexity bounds by applying them to derive effective generalization error\nbounds. \n\n"}
{"id": "1510.02836", "contents": "Title: A Model for Interactive Scores with Temporal Constraints and Conditional\n  Branching Abstract: Interactive Scores (IS) are a formalism for the design and performance of\ninteractive multimedia scenarios. IS provide temporal relations (TR), but they\ncannot represent conditional branching and TRs simultaneously. We propose an\nextension to Allombert et al.'s IS model by including a condition on the TRs.\nWe found out that in order to have a coherent model in all possible scenarios,\ndurations must be flexible; however, sometimes it is possible to have fixed\ndurations. To show the relevance of our model, we modeled an existing\nmultimedia installation called Mariona. In Mariona there is choice, random\ndurations and loops. Whether we can represent all the TRs available in\nAllombert et al.'s model into ours, or we have to choose between a timed\nconditional branching model and a pure temporal model before writing a\nscenario, still remains as an open question. \n\n"}
{"id": "1510.05076", "contents": "Title: A categorical approach to open and interconnected dynamical systems Abstract: We develop a sound and complete graphical theory for discrete linear\ntime-invariant dynamical systems. The graphical syntax, as in previous work, is\nclosely related to the classical notion of signal flow diagrams, differently\nfrom previous work, these are understood as multi-input multi-output\ntransducers that process streams with an \\emph{infinite past} as well as an\ninfinite future. This extended semantics features non-controllable systems, and\nwe develop a novel, structural characterisation of controllability. Our\napproach is formalised through the theory of props, extending the work of\nBonchi, Zanasi and the third author. \n\n"}
{"id": "1510.06658", "contents": "Title: Type-checking Liveness for Collaborative Processes with Bounded and\n  Unbounded Recursion Abstract: We present the first session typing system guaranteeing request-response\nliveness properties for possibly non-terminating communicating processes. The\ntypes augment the branch and select types of the standard binary session types\nwith a set of required responses, indicating that whenever a particular label\nis selected, a set of other labels, its responses, must eventually also be\nselected. We prove that these extended types are strictly more expressive than\nstandard session types. We provide a type system for a process calculus similar\nto a subset of collaborative BPMN processes with internal (data-based) and\nexternal (event-based) branching, message passing, bounded and unbounded\nlooping. We prove that this type system is sound, i.e., it guarantees\nrequest-response liveness for dead-lock free processes. We exemplify the use of\nthe calculus and type system on a concrete example of an infinite state system. \n\n"}
{"id": "1510.06706", "contents": "Title: ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional\n  Networks on Multi-Core and Many-Core Shared Memory Machines Abstract: Convolutional networks (ConvNets) have become a popular approach to computer\nvision. It is important to accelerate ConvNet training, which is\ncomputationally costly. We propose a novel parallel algorithm based on\ndecomposition into a set of tasks, most of which are convolutions or FFTs.\nApplying Brent's theorem to the task dependency graph implies that linear\nspeedup with the number of processors is attainable within the PRAM model of\nparallel computation, for wide network architectures. To attain such\nperformance on real shared-memory machines, our algorithm computes convolutions\nconverging on the same node of the network with temporal locality to reduce\ncache misses, and sums the convergent convolution outputs via an almost\nwait-free concurrent method to reduce time spent in critical sections. We\nimplement the algorithm with a publicly available software package called ZNN.\nBenchmarking with multi-core CPUs shows that ZNN can attain speedup roughly\nequal to the number of physical cores. We also show that ZNN can attain over\n90x speedup on a many-core CPU (Xeon Phi Knights Corner). These speedups are\nachieved for network architectures with widths that are in common use. The task\nparallelism of the ZNN algorithm is suited to CPUs, while the SIMD parallelism\nof previous algorithms is compatible with GPUs. Through examples, we show that\nZNN can be either faster or slower than certain GPU implementations depending\non specifics of the network architecture, kernel sizes, and density and size of\nthe output patch. ZNN may be less costly to develop and maintain, due to the\nrelative ease of general-purpose CPU programming. \n\n"}
{"id": "1510.07623", "contents": "Title: Partial Key Grouping: Load-Balanced Partitioning of Distributed Streams Abstract: We study the problem of load balancing in distributed stream processing\nengines, which is exacerbated in the presence of skew. We introduce Partial Key\nGrouping (PKG), a new stream partitioning scheme that adapts the classical\n\"power of two choices\" to a distributed streaming setting by leveraging two\nnovel techniques: key splitting and local load estimation. In so doing, it\nachieves better load balancing than key grouping while being more scalable than\nshuffle grouping.\n  We test PKG on several large datasets, both real-world and synthetic.\nCompared to standard hashing, PKG reduces the load imbalance by up to several\norders of magnitude, and often achieves nearly-perfect load balance. This\nresult translates into an improvement of up to 175% in throughput and up to 45%\nin latency when deployed on a real Storm cluster. PKG has been integrated in\nApache Storm v0.10. \n\n"}
{"id": "1510.08545", "contents": "Title: High Energy Physics Forum for Computational Excellence: Working Group\n  Reports (I. Applications Software II. Software Libraries and Tools III.\n  Systems) Abstract: Computing plays an essential role in all aspects of high energy physics. As\ncomputational technology evolves rapidly in new directions, and data throughput\nand volume continue to follow a steep trend-line, it is important for the HEP\ncommunity to develop an effective response to a series of expected challenges.\nIn order to help shape the desired response, the HEP Forum for Computational\nExcellence (HEP-FCE) initiated a roadmap planning activity with two key\noverlapping drivers -- 1) software effectiveness, and 2) infrastructure and\nexpertise advancement. The HEP-FCE formed three working groups, 1) Applications\nSoftware, 2) Software Libraries and Tools, and 3) Systems (including systems\nsoftware), to provide an overview of the current status of HEP computing and to\npresent findings and opportunities for the desired HEP computational roadmap.\nThe final versions of the reports are combined in this document, and are\npresented along with introductory material. \n\n"}
{"id": "1510.09102", "contents": "Title: Trace Refinement in Labelled Markov Decision Processes Abstract: Given two labelled Markov decision processes (MDPs), the trace-refinement\nproblem asks whether for all strategies of the first MDP there exists a\nstrategy of the second MDP such that the induced labelled Markov chains are\ntrace-equivalent. We show that this problem is decidable in polynomial time if\nthe second MDP is a Markov chain. The algorithm is based on new results on a\nparticular notion of bisimulation between distributions over the states.\nHowever, we show that the general trace-refinement problem is undecidable, even\nif the first MDP is a Markov chain. Decidability of those problems was stated\nas open in 2008. We further study the decidability and complexity of the\ntrace-refinement problem provided that the strategies are restricted to be\nmemoryless. \n\n"}
{"id": "1511.00346", "contents": "Title: Lattice-Theoretic Progress Measures and Coalgebraic Model Checking (with\n  Appendices) Abstract: In the context of formal verification in general and model checking in\nparticular, parity games serve as a mighty vehicle: many problems are encoded\nas parity games, which are then solved by the seminal algorithm by Jurdzinski.\nIn this paper we identify the essence of this workflow to be the notion of\nprogress measure, and formalize it in general, possibly infinitary,\nlattice-theoretic terms. Our view on progress measures is that they are to\nnested/alternating fixed points what invariants are to safety/greatest fixed\npoints, and what ranking functions are to liveness/least fixed points. That is,\nprogress measures are combination of the latter two notions (invariant and\nranking function) that have been extensively studied in the context of\n(program) verification.\n  We then apply our theory of progress measures to a general model-checking\nframework, where systems are categorically presented as coalgebras. The\nframework's theoretical robustness is witnessed by a smooth transfer from the\nbranching-time setting to the linear-time one. Although the framework can be\nused to derive some decision procedures for finite settings, we also expect the\nproposed framework to form a basis for sound proof methods for some\nundecidable/infinitary problems. \n\n"}
{"id": "1511.00830", "contents": "Title: The Variational Fair Autoencoder Abstract: We investigate the problem of learning representations that are invariant to\ncertain nuisance or sensitive factors of variation in the data while retaining\nas much of the remaining information as possible. Our model is based on a\nvariational autoencoding architecture with priors that encourage independence\nbetween sensitive and latent factors of variation. Any subsequent processing,\nsuch as classification, can then be performed on this purged latent\nrepresentation. To remove any remaining dependencies we incorporate an\nadditional penalty term based on the \"Maximum Mean Discrepancy\" (MMD) measure.\nWe discuss how these architectures can be efficiently trained on data and show\nin experiments that this method is more effective than previous work in\nremoving unwanted sources of variation while maintaining informative latent\nrepresentations. \n\n"}
{"id": "1511.01287", "contents": "Title: Local Conflict Coloring Abstract: Locally finding a solution to symmetry-breaking tasks such as\nvertex-coloring, edge-coloring, maximal matching, maximal independent set,\netc., is a long-standing challenge in distributed network computing. More\nrecently, it has also become a challenge in the framework of centralized local\ncomputation. We introduce conflict coloring as a general symmetry-breaking task\nthat includes all the aforementioned tasks as specific instantiations ---\nconflict coloring includes all locally checkable labeling tasks from\n[Naor\\&Stockmeyer, STOC 1993]. Conflict coloring is characterized by two\nparameters $l$ and $d$, where the former measures the amount of freedom given\nto the nodes for selecting their colors, and the latter measures the number of\nconstraints which colors of adjacent nodes are subject to.We show that, in the\nstandard LOCAL model for distributed network computing, if $l/d \\textgreater{}\n\\Delta$, then conflict coloring can be solved in $\\tilde\nO(\\sqrt{\\Delta})+\\log^*n$ rounds in $n$-node graphs with maximum degree\n$\\Delta$, where $\\tilde O$ ignores the polylog factors in $\\Delta$. The\ndependency in~$n$ is optimal, as a consequence of the $\\Omega(\\log^*n)$ lower\nbound by [Linial, SIAM J. Comp. 1992] for $(\\Delta+1)$-coloring. An important\nspecial case of our result is a significant improvement over the best known\nalgorithm for distributed $(\\Delta+1)$-coloring due to [Barenboim, PODC 2015],\nwhich required $\\tilde O(\\Delta^{3/4})+\\log^*n$ rounds. Improvements for other\nvariants of coloring, including $(\\Delta+1)$-list-coloring,\n$(2\\Delta-1)$-edge-coloring, $T$-coloring, etc., also follow from our general\nresult on conflict coloring. Likewise, in the framework of centralized local\ncomputation algorithms (LCAs), our general result yields an LCA which requires\na smaller number of probes than the previously best known algorithm for\nvertex-coloring, and works for a wide range of coloring problems. \n\n"}
{"id": "1511.01570", "contents": "Title: Quotient-Comprehension Chains Abstract: Quotients and comprehension are fundamental mathematical constructions that\ncan be described via adjunctions in categorical logic. This paper reveals that\nquotients and comprehension are related to measurement, not only in quantum\nlogic, but also in probabilistic and classical logic. This relation is\npresented by a long series of examples, some of them easy, and some also highly\nnon-trivial (esp. for von Neumann algebras). We have not yet identified a\nunifying theory. Nevertheless, the paper contributes towards such a theory by\nintroducing the new quotient-and-comprehension perspective on measurement\ninstruments, and by describing the examples on which such a theory should be\nbuilt. \n\n"}
{"id": "1511.03609", "contents": "Title: Automated Dynamic Firmware Analysis at Scale: A Case Study on Embedded\n  Web Interfaces Abstract: Embedded devices are becoming more widespread, interconnected, and\nweb-enabled than ever. However, recent studies showed that these devices are\nfar from being secure. Moreover, many embedded systems rely on web interfaces\nfor user interaction or administration. Unfortunately, web security is known to\nbe difficult, and therefore the web interfaces of embedded systems represent a\nconsiderable attack surface.\n  In this paper, we present the first fully automated framework that applies\ndynamic firmware analysis techniques to achieve, in a scalable manner,\nautomated vulnerability discovery within embedded firmware images. We apply our\nframework to study the security of embedded web interfaces running in\nCommercial Off-The-Shelf (COTS) embedded devices, such as routers, DSL/cable\nmodems, VoIP phones, IP/CCTV cameras. We introduce a methodology and implement\na scalable framework for discovery of vulnerabilities in embedded web\ninterfaces regardless of the vendor, device, or architecture. To achieve this\ngoal, our framework performs full system emulation to achieve the execution of\nfirmware images in a software-only environment, i.e., without involving any\nphysical embedded devices. Then, we analyze the web interfaces within the\nfirmware using both static and dynamic tools. We also present some interesting\ncase-studies, and discuss the main challenges associated with the dynamic\nanalysis of firmware images and their web interfaces and network services. The\nobservations we make in this paper shed light on an important aspect of\nembedded devices which was not previously studied at a large scale.\n  We validate our framework by testing it on 1925 firmware images from 54\ndifferent vendors. We discover important vulnerabilities in 185 firmware\nimages, affecting nearly a quarter of vendors in our dataset. These\nexperimental results demonstrate the effectiveness of our approach. \n\n"}
{"id": "1511.06341", "contents": "Title: Communicating Semantics: Reference by Description Abstract: Messages often refer to entities such as people, places and events. Correct\nidentification of the intended reference is an essential part of communication.\nLack of shared unique names often complicates entity reference. Shared\nknowledge can be used to construct uniquely identifying descriptive references\nfor entities with ambiguous names. We introduce a mathematical model for\n`Reference by Description', derive results on the conditions under which, with\nhigh probability, programs can construct unambiguous references to most\nentities in the domain of discourse and provide empirical validation of these\nresults. \n\n"}
{"id": "1511.06481", "contents": "Title: Variance Reduction in SGD by Distributed Importance Sampling Abstract: Humans are able to accelerate their learning by selecting training materials\nthat are the most informative and at the appropriate level of difficulty. We\npropose a framework for distributing deep learning in which one set of workers\nsearch for the most informative examples in parallel while a single worker\nupdates the model on examples selected by importance sampling. This leads the\nmodel to update using an unbiased estimate of the gradient which also has\nminimum variance when the sampling proposal is proportional to the L2-norm of\nthe gradient. We show experimentally that this method reduces gradient variance\neven in a context where the cost of synchronization across machines cannot be\nignored, and where the factors for importance sampling are not updated\ninstantly across the training set. \n\n"}
{"id": "1511.07569", "contents": "Title: A Survey of Signed Network Mining in Social Media Abstract: Many real-world relations can be represented by signed networks with positive\nand negative links, as a result of which signed network analysis has attracted\nincreasing attention from multiple disciplines. With the increasing prevalence\nof social media networks, signed network analysis has evolved from developing\nand measuring theories to mining tasks. In this article, we present a review of\nmining signed networks in the context of social media and discuss some\npromising research directions and new frontiers. We begin by giving basic\nconcepts and unique properties and principles of signed networks. Then we\nclassify and review tasks of signed network mining with representative\nalgorithms. We also delineate some tasks that have not been extensively studied\nwith formal definitions and also propose research directions to expand the\nfield of signed network mining. \n\n"}
{"id": "1511.07663", "contents": "Title: Approximate Probabilistic Inference via Word-Level Counting Abstract: Hashing-based model counting has emerged as a promising approach for\nlarge-scale probabilistic inference on graphical models. A key component of\nthese techniques is the use of xor-based 2-universal hash functions that\noperate over Boolean domains. Many counting problems arising in probabilistic\ninference are, however, naturally encoded over finite discrete domains.\nTechniques based on bit-level (or Boolean) hash functions require these\nproblems to be propositionalized, making it impossible to leverage the\nremarkable progress made in SMT (Satisfiability Modulo Theory) solvers that can\nreason directly over words (or bit-vectors). In this work, we present the first\napproximate model counter that uses word-level hashing functions, and can\ndirectly leverage the power of sophisticated SMT solvers. Empirical evaluation\nover an extensive suite of benchmarks demonstrates the promise of the approach. \n\n"}
{"id": "1511.07693", "contents": "Title: A Distributed System for Storing and Processing Data from\n  Earth-observing Satellites: System Design and Performance Evaluation of the\n  Visualisation Tool Abstract: We present a distributed system for storage, processing, three-dimensional\nvisualisation and basic analysis of data from Earth-observing satellites. The\ndatabase and the server have been designed for high performance and\nscalability, whereas the client is highly portable thanks to having been\ndesigned as a HTML5- and WebGL-based Web application. The system is based on\nthe so-called MEAN stack, a modern replacement for LAMP which has steadily been\ngaining traction among high-performance Web applications. We demonstrate the\nperformance of the system from the perspective of an user operating the client. \n\n"}
{"id": "1511.08130", "contents": "Title: A Roadmap towards Machine Intelligence Abstract: The development of intelligent machines is one of the biggest unsolved\nchallenges in computer science. In this paper, we propose some fundamental\nproperties these machines should have, focusing in particular on communication\nand learning. We discuss a simple environment that could be used to\nincrementally teach a machine the basics of natural-language-based\ncommunication, as a prerequisite to more complex interaction with human users.\nWe also present some conjectures on the sort of algorithms the machine should\nsupport in order to profitably learn from the environment. \n\n"}
{"id": "1512.01274", "contents": "Title: MXNet: A Flexible and Efficient Machine Learning Library for\n  Heterogeneous Distributed Systems Abstract: MXNet is a multi-language machine learning (ML) library to ease the\ndevelopment of ML algorithms, especially for deep neural networks. Embedded in\nthe host language, it blends declarative symbolic expression with imperative\ntensor computation. It offers auto differentiation to derive gradients. MXNet\nis computation and memory efficient and runs on various heterogeneous systems,\nranging from mobile devices to distributed GPU clusters.\n  This paper describes both the API design and the system implementation of\nMXNet, and explains how embedding of both symbolic expression and tensor\noperation is handled in a unified fashion. Our preliminary experiments reveal\npromising results on large scale deep neural network applications using\nmultiple GPU machines. \n\n"}
{"id": "1512.01370", "contents": "Title: Locally Adaptive Translation for Knowledge Graph Embedding Abstract: Knowledge graph embedding aims to represent entities and relations in a\nlarge-scale knowledge graph as elements in a continuous vector space. Existing\nmethods, e.g., TransE and TransH, learn embedding representation by defining a\nglobal margin-based loss function over the data. However, the optimal loss\nfunction is determined during experiments whose parameters are examined among a\nclosed set of candidates. Moreover, embeddings over two knowledge graphs with\ndifferent entities and relations share the same set of candidate loss\nfunctions, ignoring the locality of both graphs. This leads to the limited\nperformance of embedding related applications. In this paper, we propose a\nlocally adaptive translation method for knowledge graph embedding, called\nTransA, to find the optimal loss function by adaptively determining its margin\nover different knowledge graphs. Experiments on two benchmark data sets\ndemonstrate the superiority of the proposed method, as compared to\nthe-state-of-the-art ones. \n\n"}
{"id": "1512.02727", "contents": "Title: Distributed Balanced Partitioning via Linear Embedding Abstract: Balanced partitioning is often a crucial first step in solving large-scale\ngraph optimization problems, e.g., in some cases, a big graph can be chopped\ninto pieces that fit on one machine to be processed independently before\nstitching the results together. In other cases, links between different parts\nmay show up in the running time and/or network communications cost. We study a\ndistributed balanced partitioning problem where the goal is to partition the\nvertices of a given graph into k pieces so as to minimize the total cut size.\nOur algorithm is composed of a few steps that are easily implementable in\ndistributed computation frameworks. The algorithm first embeds nodes of the\ngraph onto a line, and then processes nodes in a distributed manner guided by\nthe linear embedding order. We examine various ways to find the first\nembedding, e.g., via a hierarchical clustering or Hilbert curves. Then we apply\nfour different techniques including local swaps, minimum cuts on the boundaries\nof partitions, as well as contraction and dynamic programming. As our empirical\nstudy, we compare the above techniques with each other, and also to previous\nwork in distributed graph algorithms, e.g., a label propagation method, FENNEL\nand Spinner. We report our results both on a private map graph and several\npublic social networks, and show that our results beat previous distributed\nalgorithms: e.g., compared to the label propagation algorithm, we report an\nimprovement of 15-25% in the cut value. We also observe that our algorithms\nallow for scalable distributed implementation for any number of partitions.\nFinally, we apply our techniques for the Google Maps Driving Directions to\nminimize the number of multi-shard queries with the goal of saving in CPU\nusage. During live experiments, we observe an ~40% drop in the number of\nmulti-shard queries when comparing our method with a standard geography-based\nmethod. \n\n"}
{"id": "1512.02737", "contents": "Title: Using Symmetry to Schedule Classical Matrix Multiplication Abstract: Presented with a new machine with a specific interconnect topology, algorithm\ndesigners use intuition about the symmetry of the algorithm to design time and\ncommunication-efficient schedules that map the algorithm to the machine. Is\nthere a systematic procedure for designing schedules? We present a new\ntechnique to design schedules for algorithms with no non-trivial dependencies,\nfocusing on the classical matrix multiplication algorithm.\n  We model the symmetry of algorithm with the set of instructions $X$ as the\naction of the group formed by the compositions of bijections from the set $X$\nto itself. We model the machine as the action of the group $N\\times \\Delta$,\nwhere $N$ and $\\Delta$ represent the interconnect topology and time increments\nrespectively, on the set $P\\times T$ of processors iterated over time steps. We\nmodel schedules as symmetry-preserving equivariant maps between the set $X$ and\na subgroup of its symmetry and the set $P\\times T$ with the symmetry\n$N\\times\\Delta$. Such equivariant maps are the solutions of a set of algebraic\nequations involving group homomorphisms. We associate time and communication\ncosts with the solutions to these equations.\n  We solve these equations for the classical matrix multiplication algorithm\nand show that equivariant maps correspond to time- and communication-efficient\nschedules for many topologies. We recover well known variants including the\nCannon's algorithm and the communication-avoiding \"2.5D\" algorithm for toroidal\ninterconnects, systolic computation for planar hexagonal VLSI arrays, recursive\nalgorithms for fat-trees, the cache-oblivious algorithm for the ideal cache\nmodel, and the space-bounded schedule for the parallel memory hierarchy model.\nThis suggests that the design of a schedule for a new class of machines can be\nmotivated by solutions to algebraic equations. \n\n"}
{"id": "1512.02832", "contents": "Title: Connectivity Preserving Network Transformers Abstract: The Population Protocol model is a distributed model that concerns systems of\nvery weak computational entities that cannot control the way they interact. The\nmodel of Network Constructors is a variant of Population Protocols capable of\n(algorithmically) constructing abstract networks. Both models are characterized\nby a fundamental inability to terminate. In this work, we investigate the\nminimal strengthenings of the latter that could overcome this inability. Our\nmain conclusion is that initial connectivity of the communication topology\ncombined with the ability of the protocol to transform the communication\ntopology plus a few other local and realistic assumptions are sufficient to\nguarantee not only termination but also the maximum computational power that\none can hope for in this family of models. The technique is to transform any\ninitial connected topology to a less symmetric and detectable topology without\never breaking its connectivity during the transformation. The target topology\nof all of our transformers is the spanning line and we call Terminating Line\nTransformation the corresponding problem. We first study the case in which\nthere is a pre-elected unique leader and give a time-optimal protocol for\nTerminating Line Transformation. We then prove that dropping the leader without\nadditional assumptions leads to a strong impossibility result. In an attempt to\novercome this, we equip the nodes with the ability to tell, during their\npairwise interactions, whether they have at least one neighbor in common.\nInterestingly, it turns out that this local and realistic mechanism is\nsufficient to make the problem solvable. In particular, we give a very\nefficient protocol that solves Terminating Line Transformation when all nodes\nare initially identical. The latter implies that the model computes with\ntermination any symmetric predicate computable by a Turing Machine of space\n$\\Theta(n^2)$. \n\n"}
{"id": "1512.03024", "contents": "Title: Comparing representations for function spaces in computable analysis Abstract: This paper compares different representations (in the sense of computable\nanalysis) of a number of function spaces that are of interest in analysis. In\nparticular subspace representations inherited from a larger function space are\ncompared to more natural representations for these spaces. The formal framework\nfor the comparisons is provided by Weihrauch reducibility.\n  The centrepiece of the paper considers several representations of the\nanalytic functions on the unit disk and their mutual translations. All\ntranslations that are not already computable are shown to be Weihrauch\nequivalent to closed choice on the natural numbers. Subsequently some similar\nconsiderations are carried out for representations of polynomials. In this case\nin addition to closed choice the Weihrauch degree LPO* shows up as the\ndifficulty of finding the degree or the zeros. As a final example, the smooth\nfunctions are contrasted with functions with bounded support and Schwartz\nfunctions. Here closed choice on the natural numbers and the lim degree appear. \n\n"}
{"id": "1512.06789", "contents": "Title: Information-Theoretic Bounded Rationality Abstract: Bounded rationality, that is, decision-making and planning under resource\nlimitations, is widely regarded as an important open problem in artificial\nintelligence, reinforcement learning, computational neuroscience and economics.\nThis paper offers a consolidated presentation of a theory of bounded\nrationality based on information-theoretic ideas. We provide a conceptual\njustification for using the free energy functional as the objective function\nfor characterizing bounded-rational decisions. This functional possesses three\ncrucial properties: it controls the size of the solution space; it has Monte\nCarlo planners that are exact, yet bypass the need for exhaustive search; and\nit captures model uncertainty arising from lack of evidence or from interacting\nwith other agents having unknown intentions. We discuss the single-step\ndecision-making case, and show how to extend it to sequential decisions using\nequivalence transformations. This extension yields a very general class of\ndecision problems that encompass classical decision rules (e.g. EXPECTIMAX and\nMINIMAX) as limit cases, as well as trust- and risk-sensitive planning. \n\n"}
{"id": "1512.06992", "contents": "Title: On the Differential Privacy of Bayesian Inference Abstract: We study how to communicate findings of Bayesian inference to third parties,\nwhile preserving the strong guarantee of differential privacy. Our main\ncontributions are four different algorithms for private Bayesian inference on\nproba-bilistic graphical models. These include two mechanisms for adding noise\nto the Bayesian updates, either directly to the posterior parameters, or to\ntheir Fourier transform so as to preserve update consistency. We also utilise a\nrecently introduced posterior sampling mechanism, for which we prove bounds for\nthe specific but general case of discrete Bayesian networks; and we introduce a\nmaximum-a-posteriori private mechanism. Our analysis includes utility and\nprivacy bounds, with a novel focus on the influence of graph structure on\nprivacy. Worked examples and experiments with Bayesian na{\\\"i}ve Bayes and\nBayesian linear regression illustrate the application of our mechanisms. \n\n"}
{"id": "1512.07679", "contents": "Title: Deep Reinforcement Learning in Large Discrete Action Spaces Abstract: Being able to reason in an environment with a large number of discrete\nactions is essential to bringing reinforcement learning to a larger class of\nproblems. Recommender systems, industrial plants and language models are only\nsome of the many real-world tasks involving large numbers of discrete actions\nfor which current methods are difficult or even often impossible to apply. An\nability to generalize over the set of actions as well as sub-linear complexity\nrelative to the size of the set are both necessary to handle such tasks.\nCurrent approaches are not able to provide both of these, which motivates the\nwork in this paper. Our proposed approach leverages prior information about the\nactions to embed them in a continuous space upon which it can generalize.\nAdditionally, approximate nearest-neighbor methods allow for logarithmic-time\nlookup complexity relative to the number of actions, which is necessary for\ntime-wise tractable training. This combined approach allows reinforcement\nlearning methods to be applied to large-scale learning problems previously\nintractable with current methods. We demonstrate our algorithm's abilities on a\nseries of tasks having up to one million actions. \n\n"}
{"id": "1512.08949", "contents": "Title: Simple, Robust and Optimal Ranking from Pairwise Comparisons Abstract: We consider data in the form of pairwise comparisons of n items, with the\ngoal of precisely identifying the top k items for some value of k < n, or\nalternatively, recovering a ranking of all the items. We analyze the Copeland\ncounting algorithm that ranks the items in order of the number of pairwise\ncomparisons won, and show it has three attractive features: (a) its\ncomputational efficiency leads to speed-ups of several orders of magnitude in\ncomputation time as compared to prior work; (b) it is robust in that\ntheoretical guarantees impose no conditions on the underlying matrix of\npairwise-comparison probabilities, in contrast to some prior work that applies\nonly to the BTL parametric model; and (c) it is an optimal method up to\nconstant factors, meaning that it achieves the information-theoretic limits for\nrecovering the top k-subset. We extend our results to obtain sharp guarantees\nfor approximate recovery under the Hamming distortion metric, and more\ngenerally, to any arbitrary error requirement that satisfies a simple and\nnatural monotonicity condition. \n\n"}
{"id": "1601.00372", "contents": "Title: Mutual Information and Diverse Decoding Improve Neural Machine\n  Translation Abstract: Sequence-to-sequence neural translation models learn semantic and syntactic\nrelations between sentence pairs by optimizing the likelihood of the target\ngiven the source, i.e., $p(y|x)$, an objective that ignores other potentially\nuseful sources of information. We introduce an alternative objective function\nfor neural MT that maximizes the mutual information between the source and\ntarget sentences, modeling the bi-directional dependency of sources and\ntargets. We implement the model with a simple re-ranking method, and also\nintroduce a decoding algorithm that increases diversity in the N-best list\nproduced by the first pass. Applied to the WMT German/English and\nFrench/English tasks, the proposed models offers a consistent performance boost\non both standard LSTM and attention-based neural MT architectures. \n\n"}
{"id": "1601.00901", "contents": "Title: Joint learning of ontology and semantic parser from text Abstract: Semantic parsing methods are used for capturing and representing semantic\nmeaning of text. Meaning representation capturing all the concepts in the text\nmay not always be available or may not be sufficiently complete. Ontologies\nprovide a structured and reasoning-capable way to model the content of a\ncollection of texts. In this work, we present a novel approach to joint\nlearning of ontology and semantic parser from text. The method is based on\nsemi-automatic induction of a context-free grammar from semantically annotated\ntext. The grammar parses the text into semantic trees. Both, the grammar and\nthe semantic trees are used to learn the ontology on several levels -- classes,\ninstances, taxonomic and non-taxonomic relations. The approach was evaluated on\nthe first sentences of Wikipedia pages describing people. \n\n"}
{"id": "1601.02327", "contents": "Title: A Synthetic Approach for Recommendation: Combining Ratings, Social\n  Relations, and Reviews Abstract: Recommender systems (RSs) provide an effective way of alleviating the\ninformation overload problem by selecting personalized choices. Online social\nnetworks and user-generated content provide diverse sources for recommendation\nbeyond ratings, which present opportunities as well as challenges for\ntraditional RSs. Although social matrix factorization (Social MF) can integrate\nratings with social relations and topic matrix factorization can integrate\nratings with item reviews, both of them ignore some useful information. In this\npaper, we investigate the effective data fusion by combining the two\napproaches, in two steps. First, we extend Social MF to exploit the graph\nstructure of neighbors. Second, we propose a novel framework MR3 to jointly\nmodel these three types of information effectively for rating prediction by\naligning latent factors and hidden topics. We achieve more accurate rating\nprediction on two real-life datasets. Furthermore, we measure the contribution\nof each data source to the proposed framework. \n\n"}
{"id": "1601.03980", "contents": "Title: An Elastic Middleware Platform for Concurrent and Distributed Cloud and\n  MapReduce Simulations Abstract: Cloud Computing researches involve a tremendous amount of entities such as\nusers, applications, and virtual machines. Due to the limited access and often\nvariable availability of such resources, researchers have their prototypes\ntested against the simulation environments, opposed to the real cloud\nenvironments. Existing cloud simulation environments such as CloudSim and\nEmuSim are executed sequentially, where a more advanced cloud simulation tool\ncould be created extending them, leveraging the latest technologies as well as\nthe availability of multi-core computers and the clusters in the research\nlaboratories. While computing has been evolving with multi-core programming,\nMapReduce paradigms, and middleware platforms, cloud and MapReduce simulations\nstill fail to exploit these developments themselves. This research develops\nCloud2Sim, which tries to fill the gap between the simulations and the actual\ntechnology that they are trying to simulate.\n  First, Cloud2Sim provides a concurrent and distributed cloud simulator, by\nextending CloudSim cloud simulator, using Hazelcast in-memory key-value store.\nThen, it also provides a quick assessment to MapReduce implementations of\nHazelcast and Infinispan, adaptively distributing the execution to a cluster,\nproviding means of simulating MapReduce executions. The dynamic scaler solution\nscales out the cloud and MapReduce simulations to multiple nodes running\nHazelcast and Infinispan, based on load. The distributed execution model and\nadaptive scaling solution could be leveraged as a general purpose auto scaler\nmiddleware for a multi-tenanted deployment. \n\n"}
{"id": "1601.04037", "contents": "Title: Funnel Libraries for Real-Time Robust Feedback Motion Planning Abstract: We consider the problem of generating motion plans for a robot that are\nguaranteed to succeed despite uncertainty in the environment, parametric model\nuncertainty, and disturbances. Furthermore, we consider scenarios where these\nplans must be generated in real-time, because constraints such as obstacles in\nthe environment may not be known until they are perceived (with a noisy sensor)\nat runtime. Our approach is to pre-compute a library of \"funnels\" along\ndifferent maneuvers of the system that the state is guaranteed to remain within\n(despite bounded disturbances) when the feedback controller corresponding to\nthe maneuver is executed. We leverage powerful computational machinery from\nconvex optimization (sums-of-squares programming in particular) to compute\nthese funnels. The resulting funnel library is then used to sequentially\ncompose motion plans at runtime while ensuring the safety of the robot. A major\nadvantage of the work presented here is that by explicitly taking into account\nthe effect of uncertainty, the robot can evaluate motion plans based on how\nvulnerable they are to disturbances.\n  We demonstrate and validate our method using extensive hardware experiments\non a small fixed-wing airplane avoiding obstacles at high speed (~12 mph),\nalong with thorough simulation experiments of ground vehicle and quadrotor\nmodels navigating through cluttered environments. To our knowledge, these\ndemonstrations constitute one of the first examples of provably safe and robust\ncontrol for robotic systems with complex nonlinear dynamics that need to plan\nin real-time in environments with complex geometric constraints. \n\n"}
{"id": "1601.05527", "contents": "Title: Single- and Multi-level Network Sparsification by Algebraic Distance Abstract: Network sparsification methods play an important role in modern network\nanalysis when fast estimation of computationally expensive properties (such as\nthe diameter, centrality indices, and paths) is required. We propose a method\nof network sparsification that preserves a wide range of structural properties.\nDepending on the analysis goals, the method allows to distinguish between local\nand global range edges that can be filtered out during the sparsification.\nFirst we rank edges by their algebraic distances and then we sample them. We\nalso introduce a multilevel framework for sparsification that can be used to\ncontrol the sparsification process at various coarse-grained resolutions. Based\nprimarily on the matrix-vector multiplications, our method is easily\nparallelized for different architectures. \n\n"}
{"id": "1601.05904", "contents": "Title: Improving GPU-accelerated Adaptive IDW Interpolation Algorithm Using\n  Fast kNN Search Abstract: This paper presents an efficient parallel Adaptive Inverse Distance Weighting\n(AIDW) interpolation algorithm on modern Graphics Processing Unit (GPU). The\npresented algorithm is an improvement of our previous GPU-accelerated AIDW\nalgorithm by adopting fast k-Nearest Neighbors (kNN) search. In AIDW, it needs\nto find several nearest neighboring data points for each interpolated point to\nadaptively determine the power parameter; and then the desired prediction value\nof the interpolated point is obtained by weighted interpolating using the power\nparameter. In this work, we develop a fast kNN search approach based on the\nspace-partitioning data structure, even grid, to improve the previous\nGPU-accelerated AIDW algorithm. The improved algorithm is composed of the\nstages of kNN search and weighted interpolating. To evaluate the performance of\nthe improved algorithm, we perform five groups of experimental tests.\nExperimental results show that: (1) the improved algorithm can achieve a\nspeedup of up to 1017 over the corresponding serial algorithm; (2) the improved\nalgorithm is at least two times faster than our previous GPU-accelerated AIDW\nalgorithm; and (3) the utilization of fast kNN search can significantly improve\nthe computational efficiency of the entire GPU-accelerated AIDW algorithm. \n\n"}
{"id": "1602.00591", "contents": "Title: NEXT: In-Network Nonconvex Optimization Abstract: We study nonconvex distributed optimization in multi-agent networks with\ntime-varying (nonsymmetric) connectivity. We introduce the first algorithmic\nframework for the distributed minimization of the sum of a smooth (possibly\nnonconvex and nonseparable) function - the agents' sum-utility - plus a convex\n(possibly nonsmooth and nonseparable) regularizer. The latter is usually\nemployed to enforce some structure in the solution, typically sparsity. The\nproposed method hinges on successive convex approximation techniques while\nleveraging dynamic consensus as a mechanism to distribute the computation among\nthe agents: each agent first solves (possibly inexactly) a local convex\napproximation of the nonconvex original problem, and then performs local\naveraging operations. Asymptotic convergence to (stationary) solutions of the\nnonconvex problem is established. Our algorithmic framework is then customized\nto a variety of convex and nonconvex problems in several fields, including\nsignal processing, communications, networking, and machine learning. Numerical\nresults show that the new method compares favorably to existing distributed\nalgorithms on both convex and nonconvex problems. \n\n"}
{"id": "1602.00753", "contents": "Title: Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects Abstract: Human vision greatly benefits from the information about sizes of objects.\nThe role of size in several visual reasoning tasks has been thoroughly explored\nin human perception and cognition. However, the impact of the information about\nsizes of objects is yet to be determined in AI. We postulate that this is\nmainly attributed to the lack of a comprehensive repository of size\ninformation. In this paper, we introduce a method to automatically infer object\nsizes, leveraging visual and textual information from web. By maximizing the\njoint likelihood of textual and visual observations, our method learns reliable\nrelative size estimates, with no explicit human supervision. We introduce the\nrelative size dataset and show that our method outperforms competitive textual\nand visual baselines in reasoning about size comparisons. \n\n"}
{"id": "1602.00991", "contents": "Title: Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks Abstract: This paper presents to the best of our knowledge the first end-to-end object\ntracking approach which directly maps from raw sensor input to object tracks in\nsensor space without requiring any feature engineering or system identification\nin the form of plant or sensor models. Specifically, our system accepts a\nstream of raw sensor data at one end and, in real-time, produces an estimate of\nthe entire environment state at the output including even occluded objects. We\nachieve this by framing the problem as a deep learning task and exploit\nsequence models in the form of recurrent neural networks to learn a mapping\nfrom sensor measurements to object tracks. In particular, we propose a learning\nmethod based on a form of input dropout which allows learning in an\nunsupervised manner, only based on raw, occluded sensor data without access to\nground-truth annotations. We demonstrate our approach using a synthetic dataset\ndesigned to mimic the task of tracking objects in 2D laser data -- as commonly\nencountered in robotics applications -- and show that it learns to track many\ndynamic objects despite occlusions and the presence of sensor noise. \n\n"}
{"id": "1602.01171", "contents": "Title: The Second Reactive Synthesis Competition (SYNTCOMP 2015) Abstract: We report on the design and results of the second reactive synthesis\ncompetition (SYNTCOMP 2015). We describe our extended benchmark library, with 6\ncompletely new sets of benchmarks, and additional challenging instances for 4\nof the benchmark sets that were already used in SYNTCOMP 2014. To enhance the\nanalysis of experimental results, we introduce an extension of our benchmark\nformat with meta-information, including a difficulty rating and a reference\nsize for solutions. Tools are evaluated on a set of 250 benchmarks, selected to\nprovide a good coverage of benchmarks from all classes and difficulties. We\nreport on changes of the evaluation scheme and the experimental setup. Finally,\nwe describe the entrants into SYNTCOMP 2015, as well as the results of our\nexperimental evaluation. In our analysis, we emphasize progress over the tools\nthat participated last year. \n\n"}
{"id": "1602.01352", "contents": "Title: Universality of causal graph dynamics Abstract: Causal Graph Dynamics generalize Cellular Automata, extending them to bounded\ndegree, time varying graphs. The dynamics rewrite the graph at each time step\nwith respect to two physics-like symmetries: causality (bounded speed of\ninformation) and homogeneity (the rewriting acts the same everywhere on the\ngraph, at every time step). Universality is the ability simulating every other\ninstances of another (or the same) model of computation. In this work, we study\nthree different notions of simulation for Causal Graph Dynamics, each of them\nleading to a definition of universality. \n\n"}
{"id": "1602.01365", "contents": "Title: On the Semantics of Intensionality Abstract: In this paper we propose a categorical theory of intensionality. We first\nrevisit the notion of intensionality, and discuss we its relevance to logic and\ncomputer science. It turns out that 1-category theory is not the most\nappropriate vehicle for studying the interplay of extension and intension. We\nare thus led to consider the P-categories of \\v{C}ubri\\'{c}, Dybjer and Scott,\nwhich are categories only up to a partial equivalence relation (PER). In this\nsetting, we introduce a new P-categorical construct, that of exposures.\nExposures are very nearly functors, except that they do not preserve the PERs\nof the P-category. Inspired by the categorical semantics of modal logic, we\nbegin to develop their theory. Our leading examples demonstrate that an\nexposure is an abstraction of well-behaved intensional devices, such as G\\\"odel\nnumberings. The outcome is a unifying framework in which classic results of\nKleene, G\\\"odel, Tarski and Rice find concise, clear formulations, and where\neach logical device or assumption involved in their proofs can be expressed in\nthe same algebraic manner. \n\n"}
{"id": "1602.02864", "contents": "Title: Semi-External Memory Sparse Matrix Multiplication for Billion-Node\n  Graphs Abstract: Sparse matrix multiplication is traditionally performed in memory and scales\nto large matrices using the distributed memory of multiple nodes. In contrast,\nwe scale sparse matrix multiplication beyond memory capacity by implementing\nsparse matrix dense matrix multiplication (SpMM) in a semi-external memory\n(SEM) fashion; i.e., we keep the sparse matrix on commodity SSDs and dense\nmatrices in memory. Our SEM-SpMM incorporates many in-memory optimizations for\nlarge power-law graphs. It outperforms the in-memory implementations of\nTrilinos and Intel MKL and scales to billion-node graphs, far beyond the\nlimitations of memory. Furthermore, on a single large parallel machine, our\nSEM-SpMM operates as fast as the distributed implementations of Trilinos using\nfive times as much processing power. We also run our implementation in memory\n(IM-SpMM) to quantify the overhead of keeping data on SSDs. SEM-SpMM achieves\nalmost 100% performance of IM-SpMM on graphs when the dense matrix has more\nthan four columns; it achieves at least 65% performance of IM-SpMM on all\ninputs. We apply our SpMM to three important data analysis tasks--PageRank,\neigensolving, and non-negative matrix factorization--and show that our SEM\nimplementations significantly advance the state of the art. \n\n"}
{"id": "1602.04353", "contents": "Title: The algebraic dichotomy conjecture for infinite domain Constraint\n  Satisfaction Problems Abstract: We prove that an $\\omega$-categorical core structure primitively positively\ninterprets all finite structures with parameters if and only if some stabilizer\nof its polymorphism clone has a homomorphism to the clone of projections, and\nthat this happens if and only if its polymorphism clone does not contain\noperations $\\alpha$, $\\beta$, $s$ satisfying the identity $\\alpha\ns(x,y,x,z,y,z) \\approx \\beta s(y,x,z,x,z,y)$.\n  This establishes an algebraic criterion equivalent to the conjectured\nborderline between P and NP-complete CSPs over reducts of finitely bounded\nhomogenous structures, and accomplishes one of the steps of a proposed strategy\nfor reducing the infinite domain CSP dichotomy conjecture to the finite case.\n  Our theorem is also of independent mathematical interest, characterizing a\ntopological property of any $\\omega$-categorical core structure (the existence\nof a continuous homomorphism of a stabilizer of its polymorphism clone to the\nprojections) in purely algebraic terms (the failure of an identity as above). \n\n"}
{"id": "1602.05573", "contents": "Title: Gravitational wave astrophysics, data analysis and multimessenger\n  astronomy Abstract: This paper reviews gravitational wave sources and their detection. One of the\nmost exciting potential sources of gravitational waves are coalescing binary\nblack hole systems. They can occur on all mass scales and be formed in numerous\nways, many of which are not understood. They are generally invisible in\nelectromagnetic waves, and they provide opportunities for deep investigation of\nEinstein's general theory of relativity. Sect. 1 of this paper considers ways\nthat binary black holes can be created in the universe, and includes the\nprediction that binary black hole coalescence events are likely to be the first\ngravitational wave sources to be detected. The next parts of this paper address\nthe detection of chirp waveforms from coalescence events in noisy data. Such\nanalysis is computationally intensive. Sect. 2 reviews a new and powerful\nmethod of signal detection based on the GPU-implemented summed parallel\ninfinite impulse response filters. Such filters are intrinsically real time\nalorithms, that can be used to rapidly detect and localise signals. Sect. 3 of\nthe paper reviews the use of GPU processors for rapid searching for\ngravitational wave bursts that can arise from black hole births and\ncoalescences. In sect. 4 the use of GPU processors to enable fast efficient\nstatistical significance testing of gravitational wave event candidates is\nreviewed. Sect. 5 of this paper addresses the method of multimessenger\nastronomy where the discovery of electromagnetic counterparts of gravitational\nwave events can be used to identify sources, understand their nature and obtain\nmuch greater science outcomes from each identified event. \n\n"}
{"id": "1602.06489", "contents": "Title: Distributed Private Online Learning for Social Big Data Computing over\n  Data Center Networks Abstract: With the rapid growth of Internet technologies, cloud computing and social\nnetworks have become ubiquitous. An increasing number of people participate in\nsocial networks and massive online social data are obtained. In order to\nexploit knowledge from copious amounts of data obtained and predict social\nbehavior of users, we urge to realize data mining in social networks. Almost\nall online websites use cloud services to effectively process the large scale\nof social data, which are gathered from distributed data centers. These data\nare so large-scale, high-dimension and widely distributed that we propose a\ndistributed sparse online algorithm to handle them. Additionally,\nprivacy-protection is an important point in social networks. We should not\ncompromise the privacy of individuals in networks, while these social data are\nbeing learned for data mining. Thus we also consider the privacy problem in\nthis article. Our simulations shows that the appropriate sparsity of data would\nenhance the performance of our algorithm and the privacy-preserving method does\nnot significantly hurt the performance of the proposed algorithm. \n\n"}
{"id": "1602.06771", "contents": "Title: Rewriting modulo symmetric monoidal structure Abstract: String diagrams are a powerful and intuitive graphical syntax for terms of\nsymmetric monoidal categories (SMCs). They find many applications in computer\nscience and are becoming increasingly relevant in other fields such as physics\nand control theory.\n  An important role in many such approaches is played by equational theories of\ndiagrams, typically oriented and applied as rewrite rules. This paper lays a\ncomprehensive foundation of this form of rewriting. We interpret diagrams\ncombinatorially as typed hypergraphs and establish the precise correspondence\nbetween diagram rewriting modulo the laws of SMCs on the one hand and double\npushout (DPO) rewriting of hypergraphs, subject to a soundness condition called\nconvexity, on the other. This result rests on a more general characterisation\ntheorem in which we show that typed hypergraph DPO rewriting amounts to diagram\nrewriting modulo the laws of SMCs with a chosen special Frobenius structure.\n  We illustrate our approach with a proof of termination for the theory of\nnon-commutative bimonoids. \n\n"}
{"id": "1602.07868", "contents": "Title: Weight Normalization: A Simple Reparameterization to Accelerate Training\n  of Deep Neural Networks Abstract: We present weight normalization: a reparameterization of the weight vectors\nin a neural network that decouples the length of those weight vectors from\ntheir direction. By reparameterizing the weights in this way we improve the\nconditioning of the optimization problem and we speed up convergence of\nstochastic gradient descent. Our reparameterization is inspired by batch\nnormalization but does not introduce any dependencies between the examples in a\nminibatch. This means that our method can also be applied successfully to\nrecurrent models such as LSTMs and to noise-sensitive applications such as deep\nreinforcement learning or generative models, for which batch normalization is\nless well suited. Although our method is much simpler, it still provides much\nof the speed-up of full batch normalization. In addition, the computational\noverhead of our method is lower, permitting more optimization steps to be taken\nin the same amount of time. We demonstrate the usefulness of our method on\napplications in supervised image recognition, generative modelling, and deep\nreinforcement learning. \n\n"}
{"id": "1602.08166", "contents": "Title: An Exponential Separation Between Randomized and Deterministic\n  Complexity in the LOCAL Model Abstract: Over the past 30 years numerous algorithms have been designed for symmetry\nbreaking problems in the LOCAL model, such as maximal matching, MIS, vertex\ncoloring, and edge-coloring. For most problems the best randomized algorithm is\nat least exponentially faster than the best deterministic algorithm. In this\npaper we prove that these exponential gaps are necessary and establish\nconnections between the deterministic and randomized complexities in the LOCAL\nmodel. Each result has a very compelling take-away message:\n  1. Fast $\\Delta$-coloring of trees requires random bits: Building on the\nrecent lower bounds of Brandt et al., we prove that the randomized complexity\nof $\\Delta$-coloring a tree with maximum degree $\\Delta\\ge 55$ is\n$\\Theta(\\log_\\Delta\\log n)$, whereas its deterministic complexity is\n$\\Theta(\\log_\\Delta n)$ for any $\\Delta\\ge 3$. This also establishes a large\nseparation between the deterministic complexity of $\\Delta$-coloring and\n$(\\Delta+1)$-coloring trees.\n  2. Randomized lower bounds imply deterministic lower bounds: We prove that\nany deterministic algorithm for a natural class of problems that runs in\n$O(1)+o(\\log_\\Delta n)$ rounds can be transformed to run in\n$O(\\log^*n-\\log^*\\Delta+1)$ rounds. If the transformed algorithm violates a\nlower bound (even allowing randomization), then one can conclude that the\nproblem requires $\\Omega(\\log_\\Delta n)$ time deterministically.\n  3. Deterministic lower bounds imply randomized lower bounds: We prove that\nthe randomized complexity of any natural problem on instances of size $n$ is at\nleast its deterministic complexity on instances of size $\\sqrt{\\log n}$. This\nshows that a deterministic $\\Omega(\\log_\\Delta n)$ lower bound for any problem\nimplies a randomized $\\Omega(\\log_\\Delta\\log n)$ lower bound. It also\nillustrates that the graph shattering technique is absolutely essential to the\nLOCAL model. \n\n"}
{"id": "1603.02148", "contents": "Title: Complete Elgot Monads and Coalgebraic Resumptions Abstract: Monads are extensively used nowadays to abstractly model a wide range of\ncomputational effects such as nondeterminism, statefulness, and exceptions. It\nturns out that equipping a monad with a (uniform) iteration operator satisfying\na set of natural axioms allows for modelling iterative computations just as\nabstractly. The emerging monads are called complete Elgot monads. It has been\nshown recently that extending complete Elgot monads with free effects (e.g.\noperations of sending/receiving messages over channels) canonically leads to\ngeneralized coalgebraic resumption monads, previously used as semantic domains\nfor non-wellfounded guarded processes. In this paper, we continue the study of\nthe relationship between abstract complete Elgot monads and those that capture\ncoalgebraic resumptions, by comparing the corresponding categories of\n(Eilenberg-Moore) algebras. To this end we first provide a characterization of\nthe latter category; even more generally, we formulate this characterization in\nterms of Uustalu's parametrized monads. This is further used for establishing a\ncharacterization of complete Elgot monads as precisely those monads whose\nalgebras are coherently equipped with the structure of algebras of coalgebraic\nresumption monads. \n\n"}
{"id": "1603.02188", "contents": "Title: Self-stabilizing Balls & Bins in Batches Abstract: A fundamental problem in distributed computing is the distribution of\nrequests to a set of uniform servers without a centralized controller.\nClassically, such problems are modeled as static balls into bins processes,\nwhere $m$ balls (tasks) are to be distributed to $n$ bins (servers). In a\nseminal work, Azar et al. proposed the sequential strategy \\greedy{d} for\n$n=m$. When thrown, a ball queries the load of $d$ random bins and is allocated\nto a least loaded of these. Azar et al. showed that $d=2$ yields an exponential\nimprovement compared to $d=1$. Berenbrink et al. extended this to $m\\gg n$,\nshowing that the maximal load difference is independent of $m$ for $d=2$ (in\ncontrast to $d=1$).\n  We propose a new variant of an \\emph{infinite} balls into bins process. Each\nround an expected number of $\\lambda n$ new balls arrive and are distributed\n(in parallel) to the bins. Each non-empty bin deletes one of its balls. This\nsetting models a set of servers processing incoming requests, where clients can\nquery a server's current load but receive no information about parallel\nrequests. We study the \\greedy{d} distribution scheme in this setting and show\na strong self-stabilizing property: For \\emph{any} arrival rate\n$\\lambda=\\lambda(n)<1$, the system load is time-invariant. Moreover, for\n\\emph{any} (even super-exponential) round $t$, the maximum system load is\n(w.h.p.) $O(\\frac{1}{1-\\lambda}\\cdot\\log\\frac{n}{1-\\lambda})$ for $d=1$ and\n$O(\\log\\frac{n}{1-\\lambda})$ for $d=2$. In particular, \\greedy{2} has an\nexponentially smaller system load for high arrival rates. \n\n"}
{"id": "1603.02297", "contents": "Title: TTC: A high-performance Compiler for Tensor Transpositions Abstract: We present TTC, an open-source parallel compiler for multidimensional tensor\ntranspositions. In order to generate high-performance C++ code, TTC explores a\nnumber of optimizations, including software prefetching, blocking,\nloop-reordering, and explicit vectorization. To evaluate the performance of\nmultidimensional transpositions across a range of possible use-cases, we also\nrelease a benchmark covering arbitrary transpositions of up to six dimensions.\nPerformance results show that the routines generated by TTC achieve close to\npeak memory bandwidth on both the Intel Haswell and the AMD Steamroller\narchitectures, and yield significant performance gains over modern compilers.\nBy implementing a set of pruning heuristics, TTC allows users to limit the\nnumber of potential solutions; this option is especially useful when dealing\nwith high-dimensional tensors, as the search space might become prohibitively\nlarge. Experiments indicate that when only 100 potential solutions are\nconsidered, the resulting performance is about 99% of that achieved with\nexhaustive search. \n\n"}
{"id": "1603.02655", "contents": "Title: Study and evaluation of an Irregular Graph Algorithm on Multicore and\n  GPU Processor Architectures Abstract: One area of Computing applications which poses significant challenge of\nperformance scalability on Chip Multiprocessors(CMP's) are Irregular\napplications. Such applications have very little computation and unpredictable\nmemory access patterns making them memory-bound in contrast to compute-bound\napplications. Since the gap between processor and memory performance continues\nto exist, difficulty to hide and decrease this gap is one of the important\nfactors which results in poor performance of these applications on CMP's.\n  The goal of this thesis is to overcome many such challenges posed during\nperformance acceleration of an irregular graph algorithm called Triad Census.\nWe accelerated the Triad Census algorithm on two significantly different Chip\nMultiprocessors: Dual-socket Intel Xeon Multicore (8 hardware threads per\nsocket) and 240-processor core NVIDIA Tesla C1060 GPGPU(128 hardware threads\nper core).\n  The experimental results obtained on Intel Multicore Xeon system shows\nperformance speedups (w.r.t baseline sequential) of maximum 56x , average 33x\nand minimum 8.3x for real world graph data sets. On NVIDIA Tesla C1060 GPGPU,\nwe were able to match almost equally the Multicore results - 58.4x maximum,\n32.8x average and 4.2x minimum speedups w.r.t baseline sequential. In terms of\nraw performance, for the graph data set called Patents network, our results on\nIntel Xeon Multicore(16 hw threads) were 1.27x times faster than previous\nresults on Cray XMT(16 hw threads) while results achieved on GPGPU were\ncomparatively slower(0.72x). To the best of our knowledge, this algorithm has\nonly been accelerated on supercomputer class computer named Cray XMT and no\nwork exists that demonstrates performance evaluation and comparison of this\nalgorithm on relatively lower-cost Multicore and GPGPU based platforms. \n\n"}
{"id": "1603.02981", "contents": "Title: Ant-Inspired Density Estimation via Random Walks Abstract: Many ant species employ distributed population density estimation in\napplications ranging from quorum sensing [Pra05], to task allocation [Gor99],\nto appraisal of enemy colony strength [Ada90]. It has been shown that ants\nestimate density by tracking encounter rates -- the higher the population\ndensity, the more often the ants bump into each other [Pra05,GPT93].\n  We study distributed density estimation from a theoretical perspective. We\nprove that a group of anonymous agents randomly walking on a grid are able to\nestimate their density within a small multiplicative error in few steps by\nmeasuring their rates of encounter with other agents. Despite dependencies\ninherent in the fact that nearby agents may collide repeatedly (and, worse,\ncannot recognize when this happens), our bound nearly matches what would be\nrequired to estimate density by independently sampling grid locations.\n  From a biological perspective, our work helps shed light on how ants and\nother social insects can obtain relatively accurate density estimates via\nencounter rates. From a technical perspective, our analysis provides new tools\nfor understanding complex dependencies in the collision probabilities of\nmultiple random walks. We bound the strength of these dependencies using\n$local\\ mixing\\ properties$ of the underlying graph. Our results extend beyond\nthe grid to more general graphs and we discuss applications to size estimation\nfor social networks and density estimation for robot swarms. \n\n"}
{"id": "1603.03980", "contents": "Title: On Learning High Dimensional Structured Single Index Models Abstract: Single Index Models (SIMs) are simple yet flexible semi-parametric models for\nmachine learning, where the response variable is modeled as a monotonic\nfunction of a linear combination of features. Estimation in this context\nrequires learning both the feature weights and the nonlinear function that\nrelates features to observations. While methods have been described to learn\nSIMs in the low dimensional regime, a method that can efficiently learn SIMs in\nhigh dimensions, and under general structural assumptions, has not been\nforthcoming. In this paper, we propose computationally efficient algorithms for\nSIM inference in high dimensions with structural constraints. Our general\napproach specializes to sparsity, group sparsity, and low-rank assumptions\namong others. Experiments show that the proposed method enjoys superior\npredictive performance when compared to generalized linear models, and achieves\nresults comparable to or better than single layer feedforward neural networks\nwith significantly less computational cost. \n\n"}
{"id": "1603.05163", "contents": "Title: Accelerating Data Regeneration for Distributed Storage Systems with\n  Heterogeneous Link Capacities Abstract: Distributed storage systems provide large-scale reliable data storage\nservices by spreading redundancy across a large group of storage nodes. In such\na large system, node failures take place on a regular basis. When a storage\nnode breaks down, a replacement node is expected to regenerate the redundant\ndata as soon as possible in order to maintain the same level of redundancy.\nPrevious results have been mainly focused on the minimization of network\ntraffic in regeneration. However, in practical networks, where link capacities\nvary in a wide range, minimizing network traffic does not always yield the\nminimum regeneration time. In this paper, we investigate two approaches to the\nproblem of minimizing regeneration time in networks with heterogeneous link\ncapacities. The first approach is to download different amounts of repair data\nfrom the helping nodes according to the link capacities. The second approach\ngeneralizes the conventional star-structured regeneration topology to\ntree-structured topologies so that we can utilize the links between helping\nnodes with bypassing low-capacity links. Simulation results show that the\nflexible tree-structured regeneration scheme that combines the advantages of\nboth approaches can achieve a substantial reduction in the regeneration time. \n\n"}
{"id": "1603.05627", "contents": "Title: Hypergraph Partitioning for Sparse Matrix-Matrix Multiplication Abstract: We propose a fine-grained hypergraph model for sparse matrix-matrix\nmultiplication (SpGEMM), a key computational kernel in scientific computing and\ndata analysis whose performance is often communication bound. This model\ncorrectly describes both the interprocessor communication volume along a\ncritical path in a parallel computation and also the volume of data moving\nthrough the memory hierarchy in a sequential computation. We show that\nidentifying a communication-optimal algorithm for particular input matrices is\nequivalent to solving a hypergraph partitioning problem. Our approach is\nsparsity dependent, meaning that we seek the best algorithm for the given input\nmatrices.\n  In addition to our (3D) fine-grained model, we also propose coarse-grained 1D\nand 2D models that correspond to simpler SpGEMM algorithms. We explore the\nrelations between our models theoretically, and we study their performance\nexperimentally in the context of three applications that use SpGEMM as a key\ncomputation. For each application, we find that at least one coarse-grained\nmodel is as communication efficient as the fine-grained model. We also observe\nthat different applications have affinities for different algorithms.\n  Our results demonstrate that hypergraphs are an accurate model for reasoning\nabout the communication costs of SpGEMM as well as a practical tool for\nexploring the SpGEMM algorithm design space. \n\n"}
{"id": "1603.05789", "contents": "Title: Stuttering equivalence is too slow! Abstract: Groote and Wijs recently described an algorithm for deciding stuttering\nequivalence and branching bisimulation equivalence, acclaimed to run in\n$\\mathcal{O}(m \\log n)$ time. Unfortunately, the algorithm does not always meet\nthe acclaimed running time. In this paper, we present two counterexamples where\nthe algorithms uses $\\Omega(md)$ time. A third example shows that the\ncorrection is not trivial. In order to analyse the problem we present\npseudocode of the algorithm, and indicate the time that can be spent on each\npart of the algorithm in order to meet the desired bound. We also propose fixes\nto the algorithm such that it indeed runs in $\\mathcal{O}(m \\log n)$ time. \n\n"}
{"id": "1603.06288", "contents": "Title: Multi-fidelity Gaussian Process Bandit Optimisation Abstract: In many scientific and engineering applications, we are tasked with the\nmaximisation of an expensive to evaluate black box function $f$. Traditional\nsettings for this problem assume just the availability of this single function.\nHowever, in many cases, cheap approximations to $f$ may be obtainable. For\nexample, the expensive real world behaviour of a robot can be approximated by a\ncheap computer simulation. We can use these approximations to eliminate low\nfunction value regions cheaply and use the expensive evaluations of $f$ in a\nsmall but promising region and speedily identify the optimum. We formalise this\ntask as a \\emph{multi-fidelity} bandit problem where the target function and\nits approximations are sampled from a Gaussian process. We develop MF-GP-UCB, a\nnovel method based on upper confidence bound techniques. In our theoretical\nanalysis we demonstrate that it exhibits precisely the above behaviour, and\nachieves better regret than strategies which ignore multi-fidelity information.\nEmpirically, MF-GP-UCB outperforms such naive strategies and other\nmulti-fidelity methods on several synthetic and real experiments. \n\n"}
{"id": "1603.07030", "contents": "Title: Descriptive complexity of graph spectra Abstract: Two graphs are co-spectral if their respective adjacency matrices have the\nsame multi-set of eigenvalues. A graph is said to be determined by its spectrum\nif all graphs that are co-spectral with it are isomorphic to it. We consider\nthese properties in relation to logical definability. We show that any pair of\ngraphs that are elementarily equivalent with respect to the three-variable\ncounting first-order logic $C^3$ are co-spectral, and this is not the case with\n$C^2$, nor with any number of variables if we exclude counting quantifiers. We\nalso show that the class of graphs that are determined by their spectra is\ndefinable in partial fixed-point logic with counting. We relate these\nproperties to other algebraic and combinatorial problems. \n\n"}
{"id": "1603.08561", "contents": "Title: Shuffle and Learn: Unsupervised Learning using Temporal Order\n  Verification Abstract: In this paper, we present an approach for learning a visual representation\nfrom the raw spatiotemporal signals in videos. Our representation is learned\nwithout supervision from semantic labels. We formulate our method as an\nunsupervised sequential verification task, i.e., we determine whether a\nsequence of frames from a video is in the correct temporal order. With this\nsimple task and no semantic labels, we learn a powerful visual representation\nusing a Convolutional Neural Network (CNN). The representation contains\ncomplementary information to that learned from supervised image datasets like\nImageNet. Qualitative results show that our method captures information that is\ntemporally varying, such as human pose. When used as pre-training for action\nrecognition, our method gives significant gains over learning without external\ndata on benchmark datasets like UCF101 and HMDB51. To demonstrate its\nsensitivity to human pose, we show results for pose estimation on the FLIC and\nMPII datasets that are competitive, or better than approaches using\nsignificantly more supervision. Our method can be combined with supervised\nrepresentations to provide an additional boost in accuracy. \n\n"}
{"id": "1603.08988", "contents": "Title: Towards Practical Bayesian Parameter and State Estimation Abstract: Joint state and parameter estimation is a core problem for dynamic Bayesian\nnetworks. Although modern probabilistic inference toolkits make it relatively\neasy to specify large and practically relevant probabilistic models, the silver\nbullet---an efficient and general online inference algorithm for such\nproblems---remains elusive, forcing users to write special-purpose code for\neach application. We propose a novel blackbox algorithm -- a hybrid of particle\nfiltering for state variables and assumed density filtering for parameter\nvariables. It has following advantages: (a) it is efficient due to its online\nnature, and (b) it is applicable to both discrete and continuous parameter\nspaces . On a variety of toy and real models, our system is able to generate\nmore accurate results within a fixed computation budget. This preliminary\nevidence indicates that the proposed approach is likely to be of practical use. \n\n"}
{"id": "1603.09727", "contents": "Title: Neural Language Correction with Character-Based Attention Abstract: Natural language correction has the potential to help language learners\nimprove their writing skills. While approaches with separate classifiers for\ndifferent error types have high precision, they do not flexibly handle errors\nsuch as redundancy or non-idiomatic phrasing. On the other hand, word and\nphrase-based machine translation methods are not designed to cope with\northographic errors, and have recently been outpaced by neural models.\nMotivated by these issues, we present a neural network-based approach to\nlanguage correction. The core component of our method is an encoder-decoder\nrecurrent neural network with an attention mechanism. By operating at the\ncharacter level, the network avoids the problem of out-of-vocabulary words. We\nillustrate the flexibility of our approach on dataset of noisy, user-generated\ntext collected from an English learner forum. When combined with a language\nmodel, our method achieves a state-of-the-art $F_{0.5}$-score on the CoNLL 2014\nShared Task. We further demonstrate that training the network on additional\ndata with synthesized errors can improve performance. \n\n"}
{"id": "1604.00921", "contents": "Title: A Review of Theoretical and Practical Challenges of Trusted Autonomy in\n  Big Data Abstract: Despite the advances made in artificial intelligence, software agents, and\nrobotics, there is little we see today that we can truly call a fully\nautonomous system. We conjecture that the main inhibitor for advancing autonomy\nis lack of trust. Trusted autonomy is the scientific and engineering field to\nestablish the foundations and ground work for developing trusted autonomous\nsystems (robotics and software agents) that can be used in our daily life, and\ncan be integrated with humans seamlessly, naturally and efficiently.\n  In this paper, we review this literature to reveal opportunities for\nresearchers and practitioners to work on topics that can create a leap forward\nin advancing the field of trusted autonomy. We focus the paper on the `trust'\ncomponent as the uniting technology between humans and machines. Our inquiry\ninto this topic revolves around three sub-topics: (1) reviewing and positioning\nthe trust modelling literature for the purpose of trusted autonomy; (2)\nreviewing a critical subset of sensor technologies that allow a machine to\nsense human states; and (3) distilling some critical questions for advancing\nthe field of trusted autonomy. The inquiry is augmented with conceptual models\nthat we propose along the way by recompiling and reshaping the literature into\nforms that enables trusted autonomous systems to become a reality. The paper\noffers a vision for a Trusted Cyborg Swarm, an extension of our previous\nCognitive Cyber Symbiosis concept, whereby humans and machines meld together in\na harmonious, seamless, and coordinated manner. \n\n"}
{"id": "1604.00981", "contents": "Title: Revisiting Distributed Synchronous SGD Abstract: Distributed training of deep learning models on large-scale training data is\ntypically conducted with asynchronous stochastic optimization to maximize the\nrate of updates, at the cost of additional noise introduced from asynchrony. In\ncontrast, the synchronous approach is often thought to be impractical due to\nidle time wasted on waiting for straggling workers. We revisit these\nconventional beliefs in this paper, and examine the weaknesses of both\napproaches. We demonstrate that a third approach, synchronous optimization with\nbackup workers, can avoid asynchronous noise while mitigating for the worst\nstragglers. Our approach is empirically validated and shown to converge faster\nand to better test accuracies. \n\n"}
{"id": "1604.00981", "contents": "Title: Revisiting Distributed Synchronous SGD Abstract: Distributed training of deep learning models on large-scale training data is\ntypically conducted with asynchronous stochastic optimization to maximize the\nrate of updates, at the cost of additional noise introduced from asynchrony. In\ncontrast, the synchronous approach is often thought to be impractical due to\nidle time wasted on waiting for straggling workers. We revisit these\nconventional beliefs in this paper, and examine the weaknesses of both\napproaches. We demonstrate that a third approach, synchronous optimization with\nbackup workers, can avoid asynchronous noise while mitigating for the worst\nstragglers. Our approach is empirically validated and shown to converge faster\nand to better test accuracies. \n\n"}
{"id": "1604.01416", "contents": "Title: dMath: A Scalable Linear Algebra and Math Library for Heterogeneous\n  GP-GPU Architectures Abstract: A new scalable parallel math library, dMath, is presented in this paper that\ndemonstrates leading scaling when using intranode, or internode,\nhybrid-parallelism for deep-learning. dMath provides easy-to-use distributed\nbase primitives and a variety of domain-specific algorithms. These include\nmatrix multiplication, convolutions, and others allowing for rapid development\nof highly scalable applications, including Deep Neural Networks (DNN), whereas\npreviously one was restricted to libraries that provided effective primitives\nfor only a single GPU, like Nvidia cublas and cudnn or DNN primitives from\nNervana neon framework. Development of HPC software is difficult,\nlabor-intensive work, requiring a unique skill set. dMath allows a wide range\nof developers to utilize parallel and distributed hardware easily. One\ncontribution of this approach is that data is stored persistently on the GPU\nhardware, avoiding costly transfers between host and device. Advanced memory\nmanagement techniques are utilized, including caching of transferred data and\nmemory reuse through pooling. A key contribution of dMath is that it delivers\nperformance, portability, and productivity to its specific domain of support.\nIt enables algorithm and application programmers to quickly solve problems\nwithout managing the significant complexity associated with multi-level\nparallelism. \n\n"}
{"id": "1604.01696", "contents": "Title: A Corpus and Evaluation Framework for Deeper Understanding of\n  Commonsense Stories Abstract: Representation and learning of commonsense knowledge is one of the\nfoundational problems in the quest to enable deep language understanding. This\nissue is particularly challenging for understanding casual and correlational\nrelationships between events. While this topic has received a lot of interest\nin the NLP community, research has been hindered by the lack of a proper\nevaluation framework. This paper attempts to address this problem with a new\nframework for evaluating story understanding and script learning: the 'Story\nCloze Test'. This test requires a system to choose the correct ending to a\nfour-sentence story. We created a new corpus of ~50k five-sentence commonsense\nstories, ROCStories, to enable this evaluation. This corpus is unique in two\nways: (1) it captures a rich set of causal and temporal commonsense relations\nbetween daily events, and (2) it is a high quality collection of everyday life\nstories that can also be used for story generation. Experimental evaluation\nshows that a host of baselines and state-of-the-art models based on shallow\nlanguage understanding struggle to achieve a high score on the Story Cloze\nTest. We discuss these implications for script and story learning, and offer\nsuggestions for deeper language understanding. \n\n"}
{"id": "1604.02504", "contents": "Title: Fault Tolerant QR Factorization for General Matrices Abstract: This paper presents a fault-tolerant algorithm for the QR factorization of\ngeneral matrices. It relies on the communication-avoiding algorithm, and uses\nthe structure of the reduction of each part of the computation to introduce\nredundancies that are sufficient to recover the state of a failed process.\nAfter a process has failed, its state can be recovered based on the data held\nby one process only. Besides, it does not add any significant operation in the\ncritical path during failure-free execution. \n\n"}
{"id": "1604.03655", "contents": "Title: A Discrete and Bounded Envy-Free Cake Cutting Protocol for Any Number of\n  Agents Abstract: We consider the well-studied cake cutting problem in which the goal is to\nfind an envy-free allocation based on queries from $n$ agents. The problem has\nreceived attention in computer science, mathematics, and economics. It has been\na major open problem whether there exists a discrete and bounded envy-free\nprotocol. We resolve the problem by proposing a discrete and bounded envy-free\nprotocol for any number of agents. The maximum number of queries required by\nthe protocol is $n^{n^{n^{n^{n^n}}}}$. We additionally show that even if we do\nnot run our protocol to completion, it can find in at most $n^3{(n^2)}^n$\nqueries a partial allocation of the cake that achieves proportionality (each\nagent gets at least $1/n$ of the value of the whole cake) and envy-freeness.\nFinally we show that an envy-free partial allocation can be computed in at most\n$n^3{(n^2)}^n$ queries such that each agent gets a connected piece that gives\nthe agent at least $1/(3n)$ of the value of the whole cake. \n\n"}
{"id": "1604.03853", "contents": "Title: Hierarchical Compound Poisson Factorization Abstract: Non-negative matrix factorization models based on a hierarchical\nGamma-Poisson structure capture user and item behavior effectively in extremely\nsparse data sets, making them the ideal choice for collaborative filtering\napplications. Hierarchical Poisson factorization (HPF) in particular has proved\nsuccessful for scalable recommendation systems with extreme sparsity. HPF,\nhowever, suffers from a tight coupling of sparsity model (absence of a rating)\nand response model (the value of the rating), which limits the expressiveness\nof the latter. Here, we introduce hierarchical compound Poisson factorization\n(HCPF) that has the favorable Gamma-Poisson structure and scalability of HPF to\nhigh-dimensional extremely sparse matrices. More importantly, HCPF decouples\nthe sparsity model from the response model, allowing us to choose the most\nsuitable distribution for the response. HCPF can capture binary, non-negative\ndiscrete, non-negative continuous, and zero-inflated continuous responses. We\ncompare HCPF with HPF on nine discrete and three continuous data sets and\nconclude that HCPF captures the relationship between sparsity and response\nbetter than HPF. \n\n"}
{"id": "1604.06061", "contents": "Title: Compositionality and String Diagrams for Game Theory Abstract: We introduce string diagrams as a formal mathematical, graphical language to\nrepresent, compose, program and reason about games. The language is well\nestablished in quantum physics, quantum computing and quantum linguistic with\nthe semantics given by category theory. We apply this language to the game\ntheoretical setting and show examples how to use it for some economic games\nwhere we highlight the compositional nature of our higher-order game theory. \n\n"}
{"id": "1604.06853", "contents": "Title: Adaptive Content-based Routing using Subscription Subgrouping in\n  Structured Overlays Abstract: Cyclic or general overlays may provide multiple paths between publishers and\nsubscribers. However, an advertisement tree and a matching subscription\nactivates only one path for notifications routing in publish/subscribe systems.\nThis poses serious challenges in handling network conditions like congestion,\nand link or broker failures. Further, content-based dynamic routing of\nnotifications requires instantaneous updates in routing paths, which is not a\nscalable option. This paper introduces a clustering approach with a bit-vector\ntechnique for inter-cluster dynamic routing of notifications in a structured\ncyclic topology that provides multiple paths between publishers and interested\nsubscribers. The advertisement forwarding process exploits the structured\nnature of the overlay topology to generate advertisement trees of length 1\nwithout generating duplicate messages in the advertisement forwarding process.\nIssued subscriptions are divided into multiple disjoint subgropus, where each\nsubscription is broadcast to a cluster, which is a limited part of the\nstructured cyclic overlay network. We implemented novel static and\nintra-cluster dynamic routing algorithms in the proposed overlay topology for\nour advertisement-based publish/subscribe system, called OctopiA. We also\nperformed a pragmatic comparison of our two algorithms with the\nstate-of-the-art. Experiments on a cluster testbed show that our approach\ngenerates fewer inter-broker messages, and is scalable. \n\n"}
{"id": "1604.07371", "contents": "Title: Do the Hard Stuff First: Scheduling Dependent Computations in\n  Data-Analytics Clusters Abstract: We present a scheduler that improves cluster utilization and job completion\ntimes by packing tasks having multi-resource requirements and\ninter-dependencies. While the problem is algorithmically very hard, we achieve\nnear-optimality on the job DAGs that appear in production clusters at a large\nenterprise and in benchmarks such as TPC-DS. A key insight is that carefully\nhandling the long-running tasks and those with tough-to-pack resource needs\nwill produce good-enough schedules. However, which subset of tasks to treat\ncarefully is not clear (and intractable to discover). Hence, we offer a search\nprocedure that evaluates various possibilities and outputs a preferred schedule\norder over tasks. An online component enforces the schedule orders desired by\nthe various jobs running on the cluster. In addition, it packs tasks, overbooks\nthe fungible resources and guarantees bounded unfairness for a variety of\ndesirable fairness schemes. Relative to the state-of-the art schedulers, we\nspeed up 50% of the jobs by over 30% each. \n\n"}
{"id": "1604.07515", "contents": "Title: Parallel Local Graph Clustering Abstract: Graph clustering has many important applications in computing, but due to\ngrowing sizes of graphs, even traditionally fast clustering methods such as\nspectral partitioning can be computationally expensive for real-world graphs of\ninterest. Motivated partly by this, so-called local algorithms for graph\nclustering have received significant interest due to the fact that they can\nfind good clusters in a graph with work proportional to the size of the cluster\nrather than that of the entire graph. This feature has proven to be crucial in\nmaking such graph clustering and many of its downstream applications efficient\nin practice. While local clustering algorithms are already faster than\ntraditional algorithms that touch the entire graph, they are sequential and\nthere is an opportunity to make them even more efficient via parallelization.\nIn this paper, we show how to parallelize many of these algorithms in the\nshared-memory multicore setting, and we analyze the parallel complexity of\nthese algorithms. We present comprehensive experiments on large-scale graphs\nshowing that our parallel algorithms achieve good parallel speedups on a modern\nmulticore machine, thus significantly speeding up the analysis of local graph\nclusters in the very large-scale setting. \n\n"}
{"id": "1604.08348", "contents": "Title: On the algebraic structure of Weihrauch degrees Abstract: We introduce two new operations (compositional products and implication) on\nWeihrauch degrees, and investigate the overall algebraic structure. The\nvalidity of the various distributivity laws is studied and forms the basis for\na comparison with similar structures such as residuated lattices and concurrent\nKleene algebras. Introducing the notion of an ideal with respect to the\ncompositional product, we can consider suitable quotients of the Weihrauch\ndegrees. We also prove some specific characterizations using the implication.\nIn order to introduce and study compositional products and implications, we\nintroduce and study a function space of multi-valued continuous functions. This\nspace turns out to be particularly well-behaved for effectively traceable\nspaces that are closely related to admissibly represented spaces. \n\n"}
{"id": "1604.08484", "contents": "Title: Architectural Impact on Performance of In-memory Data Analytics: Apache\n  Spark Case Study Abstract: While cluster computing frameworks are continuously evolving to provide\nreal-time data analysis capabilities, Apache Spark has managed to be at the\nforefront of big data analytics for being a unified framework for both, batch\nand stream data processing. However, recent studies on micro-architectural\ncharacterization of in-memory data analytics are limited to only batch\nprocessing workloads. We compare micro-architectural performance of batch\nprocessing and stream processing workloads in Apache Spark using hardware\nperformance counters on a dual socket server. In our evaluation experiments, we\nhave found that batch processing are stream processing workloads have similar\nmicro-architectural characteristics and are bounded by the latency of frequent\ndata access to DRAM. For data accesses we have found that simultaneous\nmulti-threading is effective in hiding the data latencies. We have also\nobserved that (i) data locality on NUMA nodes can improve the performance by\n10% on average and(ii) disabling next-line L1-D prefetchers can reduce the\nexecution time by up-to 14\\% and (iii) multiple small executors can provide\nup-to 36\\% speedup over single large executor. \n\n"}
{"id": "1604.08618", "contents": "Title: Stringer: Balancing Latency and Resource Usage in Service Function Chain\n  Provisioning Abstract: Network Functions Virtualization, or NFV, enables telecommunications\ninfrastructure providers to replace special-purpose networking equipment with\ncommodity servers running virtualized network functions (VNFs). A service\nprovider utilizing NFV technology faces the SFC provisioning problem of\nassigning VNF instances to nodes in the physical infrastructure (e.g., a\ndatacenter), and routing Service Function Chains (sequences of functions\nrequired by customers, a.k.a. SFCs) in the physical network. In doing so, the\nprovider must balance between various competing goals of performance and\nresource usage. We present an approach for SFC provisioning, consisting of\nthree elements. The first element is a fast, scalable round-robin heuristic.\nThe second element is a Mixed Integer Programming (MIP) based approach. The\nthird element is a queueing-theoretic model to estimate the average latency\nassociated with any SFC provisioning solution. Combined, these elements create\nan approach that generates a set of SFC provisioning solutions, reflecting\ndifferent tradeoffs between resource usage and performance. \n\n"}
{"id": "1604.08880", "contents": "Title: Deep, Convolutional, and Recurrent Models for Human Activity Recognition\n  using Wearables Abstract: Human activity recognition (HAR) in ubiquitous computing is beginning to\nadopt deep learning to substitute for well-established analysis techniques that\nrely on hand-crafted feature extraction and classification techniques. From\nthese isolated applications of custom deep architectures it is, however,\ndifficult to gain an overview of their suitability for problems ranging from\nthe recognition of manipulative gestures to the segmentation and identification\nof physical activities like running or ascending stairs. In this paper we\nrigorously explore deep, convolutional, and recurrent approaches across three\nrepresentative datasets that contain movement data captured with wearable\nsensors. We describe how to train recurrent approaches in this setting,\nintroduce a novel regularisation approach, and illustrate how they outperform\nthe state-of-the-art on a large benchmark dataset. Across thousands of\nrecognition experiments with randomly sampled model configurations we\ninvestigate the suitability of each model for different tasks in HAR, explore\nthe impact of hyperparameters using the fANOVA framework, and provide\nguidelines for the practitioner who wants to apply deep learning in their\nproblem setting. \n\n"}
{"id": "1605.01018", "contents": "Title: A Solution to Time-Varying Markov Decision Processes Abstract: We consider a decision-making problem where the environment varies both in\nspace and time. Such problems arise naturally when considering e.g., the\nnavigation of an underwater robot amidst ocean currents or the navigation of an\naerial vehicle in wind. To model such spatiotemporal variation, we extend the\nstandard Markov Decision Process (MDP) to a new framework called the\nTime-Varying Markov Decision Process (TVMDP). The TVMDP has a time-varying\nstate transition model and transforms the standard MDP that considers only\nimmediate and static uncertainty descriptions of state transitions, to a\nframework that is able to adapt to future time-varying transition dynamics over\nsome horizon. We show how to solve a TVMDP via a redesign of the MDP value\npropagation mechanisms by incorporating the introduced dynamics along the\ntemporal dimension. We validate our framework in a marine robotics navigation\nsetting using spatiotemporal ocean data and show that it outperforms prior\nefforts. \n\n"}
{"id": "1605.03719", "contents": "Title: Distributed Testing of Excluded Subgraphs Abstract: We study property testing in the context of distributed computing, under the\nclassical CONGEST model. It is known that testing whether a graph is\ntriangle-free can be done in a constant number of rounds, where the constant\ndepends on how far the input graph is from being triangle-free. We show that,\nfor every connected 4-node graph H, testing whether a graph is H-free can be\ndone in a constant number of rounds too. The constant also depends on how far\nthe input graph is from being H-free, and the dependence is identical to the\none in the case of testing triangles. Hence, in particular, testing whether a\ngraph is K_4-free, and testing whether a graph is C_4-free can be done in a\nconstant number of rounds (where K_k denotes the k-node clique, and C_k denotes\nthe k-node cycle). On the other hand, we show that testing K_k-freeness and\nC_k-freeness for k>4 appear to be much harder. Specifically, we investigate two\nnatural types of generic algorithms for testing H-freeness, called DFS tester\nand BFS tester. The latter captures the previously known algorithm to test the\npresence of triangles, while the former captures our generic algorithm to test\nthe presence of a 4-node graph pattern H. We prove that both DFS and BFS\ntesters fail to test K_k-freeness and C_k-freeness in a constant number of\nrounds for k>4. \n\n"}
{"id": "1605.04056", "contents": "Title: Causal Discovery for Manufacturing Domains Abstract: Yield and quality improvement is of paramount importance to any manufacturing\ncompany. One of the ways of improving yield is through discovery of the root\ncausal factors affecting yield. We propose the use of data-driven interpretable\ncausal models to identify key factors affecting yield. We focus on factors that\nare measured in different stages of production and testing in the manufacturing\ncycle of a product. We apply causal structure learning techniques on real data\ncollected from this line. Specifically, the goal of this work is to learn\ninterpretable causal models from observational data produced by manufacturing\nlines.\n  Emphasis has been given to the interpretability of the models to make them\nactionable in the field of manufacturing. We highlight the challenges presented\nby assembly line data and propose ways to alleviate them.We also identify\nunique characteristics of data originating from assembly lines and how to\nleverage them in order to improve causal discovery. Standard evaluation\ntechniques for causal structure learning shows that the learned causal models\nseem to closely represent the underlying latent causal relationship between\ndifferent factors in the production process. These results were also validated\nby manufacturing domain experts who found them promising. This work\ndemonstrates how data mining and knowledge discovery can be used for root cause\nanalysis in the domain of manufacturing and connected industry. \n\n"}
{"id": "1605.04580", "contents": "Title: TwinCG: Dual Thread Redundancy with Forward Recovery for Conjugate\n  Gradient Methods Abstract: Even though iterative solvers like the Conjugate Gradients method (CG) have\nbeen studied for over fifty years, fault tolerance for such solvers has seen\nmuch attention in recent years. For iterative solvers, two major reliable\nstrategies of recovery exist: checkpoint-restart for backward recovery, or some\ntype of redundancy technique for forward recovery. Important redundancy\ntechniques like ABFT techniques for sparse matrix-vector products (SpMxV) have\nrecently been proposed, which increase the resilience of CG methods. These\ntechniques offer limited recovery options, and introduce a tolerable overhead.\nIn this work, we study a more powerful resilience concept, which is redundant\nmultithreading. It offers more generic and stronger recovery guarantees,\nincluding any soft faults in CG iterations (among others covering ABFT SpMxV),\nbut also requires more resources. We carefully study this redundancy/efficiency\nconflict. We propose a fault tolerant CG method, called TwinCG, which\nintroduces minimal wallclock time overhead, and significant advantages in\ndetection and correction strategies. Our method uses Dual Modular Redundancy\ninstead of the more expensive Triple Modular Redundancy; still, it retains the\nTMR advantages of fault correction. We describe, implement, and benchmark our\niterative solver, and compare it in terms of efficiency and fault tolerance\ncapabilities to state-of-the-art techniques. We find that before\nparallelization, TwinCG introduces around 5-6% runtime overhead compared to\nstandard CG, and after parallelization efficiently uses BLAS. In the presence\nof faults, it reliably performs forward recovery for a range of problems,\noutperforming SpMxV ABFT solutions. \n\n"}
{"id": "1605.05104", "contents": "Title: Abstract Program Slicing: an Abstract Interpretation-based approach to\n  Program Slicing Abstract: In the present paper we formally define the notion of abstract program\nslicing, a general form of program slicing where properties of data are\nconsidered instead of their exact value. This approach is applied to a language\nwith numeric and reference values, and relies on the notion of abstract\ndependencies between program components (statements).\n  The different forms of (backward) abstract slicing are added to an existing\nformal framework where traditional, non-abstract forms of slicing could be\ncompared. The extended framework allows us to appreciate that abstract slicing\nis a generalization of traditional slicing, since traditional slicing (dealing\nwith syntactic dependencies) is generalized by (semantic) non-abstract forms of\nslicing, which are actually equivalent to an abstract form where the identity\nabstraction is performed on data.\n  Sound algorithms for computing abstract dependencies and a systematic\ncharacterization of program slices are provided, which rely on the notion of\nagreement between program states. \n\n"}
{"id": "1605.06838", "contents": "Title: Causality on Longitudinal Data: Stable Specification Search in\n  Constrained Structural Equation Modeling Abstract: A typical problem in causal modeling is the instability of model structure\nlearning, i.e., small changes in finite data can result in completely different\noptimal models. The present work introduces a novel causal modeling algorithm\nfor longitudinal data, that is robust for finite samples based on recent\nadvances in stability selection using subsampling and selection algorithms. Our\napproach uses exploratory search but allows incorporation of prior knowledge,\ne.g., the absence of a particular causal relationship between two specific\nvariables. We represent causal relationships using structural equation models.\nModels are scored along two objectives: the model fit and the model complexity.\nSince both objectives are often conflicting we apply a multi-objective\nevolutionary algorithm to search for Pareto optimal models. To handle the\ninstability of small finite data samples, we repeatedly subsample the data and\nselect those substructures (from the optimal models) that are both stable and\nparsimonious. These substructures can be visualized through a causal graph. Our\nmore exploratory approach achieves at least comparable performance as, but\noften a significant improvement over state-of-the-art alternative approaches on\na simulated data set with a known ground truth. We also present the results of\nour method on three real-world longitudinal data sets on chronic fatigue\nsyndrome, Alzheimer disease, and chronic kidney disease. The findings obtained\nwith our approach are generally in line with results from more\nhypothesis-driven analyses in earlier studies and suggest some novel\nrelationships that deserve further research. \n\n"}
{"id": "1605.06844", "contents": "Title: Information-Theoretic Lower Bounds on the Storage Cost of Shared Memory\n  Emulation Abstract: The focus of this paper is to understand storage costs of emulating an atomic\nshared memory over an asynchronous, distributed message passing system.\nPrevious literature has developed several shared memory emulation algorithms\nbased on replication and erasure coding techniques. In this paper, we present\ninformation-theoretic lower bounds on the storage costs incurred by shared\nmemory emulation algorithms. Our storage cost lower bounds are universally\napplicable, that is, we make no assumption on the structure of the algorithm or\nthe method of encoding the data.\n  We consider an arbitrary algorithm $A$ that implements an atomic multi-writer\nsingle-reader (MWSR) shared memory variable whose values come from a finite set\n$\\mathcal{V}$ over a system of $N$ servers connected by point-to-point\nasynchronous links. We require that in every fair execution of algorithm $A$\nwhere the number of server failures is smaller than a parameter $f$, every\noperation invoked at a non-failing client terminates. We define the storage\ncost of a server in algorithm $A$ as the logarithm (to base 2) of number of\nstates it can take on; the total-storage cost of algorithm $A$ is the sum of\nthe storage cost of all servers.\n  Our results are as follows. (i) We show that if algorithm $A$ does not use\nserver gossip, then the total storage cost is lower bounded by $2\n\\frac{N}{N-f+1}\\log_2|\\mathcal{V}|-o(\\log_2|\\mathcal{V}|)$. (ii) The total\nstorage cost is at least $2 \\frac{N}{N-f+2}\n\\log_{2}|\\mathcal{V}|-o(\\log_{2}|\\mathcal{V}|)$ even if the algorithm uses\nserver gossip. (iii) We consider algorithms where the write protocol sends\ninformation about the value in at most one phase. We show that the total\nstorage cost is at least $\\nu^* \\frac{N}{N-f+\\nu^*-1} \\log_2( |\\mathcal{V}|)-\no(\\log_2(|\\mathcal{V}|),$ where $\\nu^*$ is the minimum of $f+1$ and the number\nof active write operations of an execution. \n\n"}
{"id": "1605.07422", "contents": "Title: Computing Web-scale Topic Models using an Asynchronous Parameter Server Abstract: Topic models such as Latent Dirichlet Allocation (LDA) have been widely used\nin information retrieval for tasks ranging from smoothing and feedback methods\nto tools for exploratory search and discovery. However, classical methods for\ninferring topic models do not scale up to the massive size of today's publicly\navailable Web-scale data sets. The state-of-the-art approaches rely on custom\nstrategies, implementations and hardware to facilitate their asynchronous,\ncommunication-intensive workloads.\n  We present APS-LDA, which integrates state-of-the-art topic modeling with\ncluster computing frameworks such as Spark using a novel asynchronous parameter\nserver. Advantages of this integration include convenient usage of existing\ndata processing pipelines and eliminating the need for disk writes as data can\nbe kept in memory from start to finish. Our goal is not to outperform highly\ncustomized implementations, but to propose a general high-performance topic\nmodeling framework that can easily be used in today's data processing\npipelines. We compare APS-LDA to the existing Spark LDA implementations and\nshow that our system can, on a 480-core cluster, process up to 135 times more\ndata and 10 times more topics without sacrificing model quality. \n\n"}
{"id": "1605.07753", "contents": "Title: Deciding Maxmin Reachability in Half-Blind Stochastic Games Abstract: Two-player, turn-based, stochastic games with reachability conditions are\nconsidered, where the maximizer has no information (he is blind) and is\nrestricted to deterministic strategies whereas the minimizer is perfectly\ninformed. We ask the question of whether the game has maxmin 1, in other words\nwe ask whether for all $\\epsilon>0$ there exists a deterministic strategy for\nthe (blind) maximizer such that against all the strategies of the minimizer, it\nis possible to reach the set of final states with probability larger than\n$1-\\epsilon$. This problem is undecidable in general, but we define a class of\ngames, called leaktight half-blind games where the problem becomes decidable.\nWe also show that mixed strategies in general are stronger for both players and\nthat optimal strategies for the minimizer might require infinite-memory. \n\n"}
{"id": "1605.08374", "contents": "Title: Kronecker Determinantal Point Processes Abstract: Determinantal Point Processes (DPPs) are probabilistic models over all\nsubsets a ground set of $N$ items. They have recently gained prominence in\nseveral applications that rely on \"diverse\" subsets. However, their\napplicability to large problems is still limited due to the $\\mathcal O(N^3)$\ncomplexity of core tasks such as sampling and learning. We enable efficient\nsampling and learning for DPPs by introducing KronDPP, a DPP model whose kernel\nmatrix decomposes as a tensor product of multiple smaller kernel matrices. This\ndecomposition immediately enables fast exact sampling. But contrary to what one\nmay expect, leveraging the Kronecker product structure for speeding up DPP\nlearning turns out to be more difficult. We overcome this challenge, and derive\nbatch and stochastic optimization algorithms for efficiently learning the\nparameters of a KronDPP. \n\n"}
{"id": "1605.08563", "contents": "Title: Towards the Automated Verification of Cyber-Physical Security Protocols:\n  Bounding the Number of Timed Intruders Abstract: Timed Intruder Models have been proposed for the verification of\nCyber-Physical Security Protocols (CPSP) amending the traditional Dolev-Yao\nintruder to obey the physical restrictions of the environment. Since to learn a\nmessage, a Timed Intruder needs to wait for a message to arrive, mounting an\nattack may depend on where Timed Intruders are. It may well be the case that in\nthe presence of a great number of intruders there is no attack, but there is an\nattack in the presence of a small number of well placed intruders. Therefore, a\nmajor challenge for the automated verification of CPSP is to determine how many\nTimed Intruders to use and where should they be placed. This paper answers this\nquestion by showing it is enough to use the same number of Timed Intruders as\nthe number of participants. We also report on some preliminary experimental\nresults in discovering attacks in CPSP. \n\n"}
{"id": "1605.09293", "contents": "Title: Internal Guidance for Satallax Abstract: We propose a new internal guidance method for automated theorem provers based\non the given-clause algorithm. Our method influences the choice of unprocessed\nclauses using positive and negative examples from previous proofs. To this end,\nwe present an efficient scheme for Naive Bayesian classification by\ngeneralising label occurrences to types with monoid structure. This makes it\npossible to extend existing fast classifiers, which consider only positive\nexamples, with negative ones. We implement the method in the higher-order logic\nprover Satallax, where we modify the delay with which propositions are\nprocessed. We evaluated our method on a simply-typed higher-order logic version\nof the Flyspeck project, where it solves 26% more problems than Satallax\nwithout internal guidance. \n\n"}
{"id": "1606.01833", "contents": "Title: Analyzing Distributed Join-Idle-Queue: A Fluid Limit Approach Abstract: In the context of load balancing, Lu et al. introduced the distributed\nJoin-Idle-Queue algorithm, where a group of dispatchers distribute jobs to a\ncluster of parallel servers. Each dispatcher maintains a queue of idle servers;\nwhen a job arrives to a dispatcher, it sends it to a server on its queue, or to\na random server if the queue is empty. In turn, when a server has no jobs, it\nrequests to be placed on the idle queue of a randomly chosen dispatcher.\n  Although this algorithm was shown to be quite effective, the original\nasymptotic analysis makes simplifying assumptions that become increasingly\ninaccurate as the system load increases. Further, the analysis does not\nnaturally generalize to interesting variations, such as having a server request\nto be placed on the idle queue of a dispatcher before it has completed all\njobs, which can be beneficial under high loads.\n  We provide a new asymptotic analysis of Join-Idle-Queue systems based on mean\nfield fluid limit methods, deriving families of differential equations that\ndescribe these systems. Our analysis avoids previous simplifying assumptions,\nis empirically more accurate, and generalizes naturally to the variation\ndescribed above, as well as other simple variations. Our theoretical and\nempirical analyses shed further light on the performance of Join-Idle-Queue,\nincluding potential performance pitfalls under high load. \n\n"}
{"id": "1606.01855", "contents": "Title: Bayesian Poisson Tucker Decomposition for Learning the Structure of\n  International Relations Abstract: We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling\ncountry--country interaction event data. These data consist of interaction\nevents of the form \"country $i$ took action $a$ toward country $j$ at time\n$t$.\" BPTD discovers overlapping country--community memberships, including the\nnumber of latent communities. In addition, it discovers directed\ncommunity--community interaction networks that are specific to \"topics\" of\naction types and temporal \"regimes.\" We show that BPTD yields an efficient MCMC\ninference algorithm and achieves better predictive performance than related\nmodels. We also demonstrate that it discovers interpretable latent structure\nthat agrees with our knowledge of international relations. \n\n"}
{"id": "1606.02020", "contents": "Title: Program Derivation by Correctness Enhacements Abstract: Relative correctness is the property of a program to be more-correct than\nanother program with respect to a given specification. Among the many\nproperties of relative correctness, that which we found most intriguing is the\nproperty that program P' refines program P if and only if P' is more-correct\nthan P with respect to any specification. This inspires us to reconsider\nprogram derivation by successive refinements: each step of this process\nmandates that we transform a program P into a program P' that refines P, i.e.\nP' is more-correct than P with respect to any specification. This raises the\nquestion: why should we want to make P' more-correct than P with respect to any\nspecification, when we only have to satisfy specification R? In this paper, we\ndiscuss a process of program derivation that replaces traditional sequence of\nrefinement-based correctness-preserving transformations starting from\nspecification R by a sequence of relative correctness-based\ncorrectness-enhancing transformations starting from abort. \n\n"}
{"id": "1606.02396", "contents": "Title: Deep Successor Reinforcement Learning Abstract: Learning robust value functions given raw observations and rewards is now\npossible with model-free and model-based deep reinforcement learning\nalgorithms. There is a third alternative, called Successor Representations\n(SR), which decomposes the value function into two components -- a reward\npredictor and a successor map. The successor map represents the expected future\nstate occupancy from any given state and the reward predictor maps states to\nscalar rewards. The value function of a state can be computed as the inner\nproduct between the successor map and the reward weights. In this paper, we\npresent DSR, which generalizes SR within an end-to-end deep reinforcement\nlearning framework. DSR has several appealing properties including: increased\nsensitivity to distal reward changes due to factorization of reward and world\ndynamics, and the ability to extract bottleneck states (subgoals) given\nsuccessor maps trained under a random policy. We show the efficacy of our\napproach on two diverse environments given raw pixel observations -- simple\ngrid-world domains (MazeBase) and the Doom game engine. \n\n"}
{"id": "1606.02421", "contents": "Title: Gossip Dual Averaging for Decentralized Optimization of Pairwise\n  Functions Abstract: In decentralized networks (of sensors, connected objects, etc.), there is an\nimportant need for efficient algorithms to optimize a global cost function, for\ninstance to learn a global model from the local data collected by each\ncomputing unit. In this paper, we address the problem of decentralized\nminimization of pairwise functions of the data points, where these points are\ndistributed over the nodes of a graph defining the communication topology of\nthe network. This general problem finds applications in ranking, distance\nmetric learning and graph inference, among others. We propose new gossip\nalgorithms based on dual averaging which aims at solving such problems both in\nsynchronous and asynchronous settings. The proposed framework is flexible\nenough to deal with constrained and regularized variants of the optimization\nproblem. Our theoretical analysis reveals that the proposed algorithms preserve\nthe convergence rate of centralized dual averaging up to an additive bias term.\nWe present numerical simulations on Area Under the ROC Curve (AUC) maximization\nand metric learning problems which illustrate the practical interest of our\napproach. \n\n"}
{"id": "1606.02738", "contents": "Title: SWIFT: Using task-based parallelism, fully asynchronous communication,\n  and graph partition-based domain decomposition for strong scaling on more\n  than 100,000 cores Abstract: We present a new open-source cosmological code, called SWIFT, designed to\nsolve the equations of hydrodynamics using a particle-based approach (Smooth\nParticle Hydrodynamics) on hybrid shared/distributed-memory architectures.\nSWIFT was designed from the bottom up to provide excellent strong scaling on\nboth commodity clusters (Tier-2 systems) and Top100-supercomputers (Tier-0\nsystems), without relying on architecture-specific features or specialized\naccelerator hardware. This performance is due to three main computational\napproaches: (1) Task-based parallelism for shared-memory parallelism, which\nprovides fine-grained load balancing and thus strong scaling on large numbers\nof cores. (2) Graph-based domain decomposition, which uses the task graph to\ndecompose the simulation domain such that the work, as opposed to just the\ndata, as is the case with most partitioning schemes, is equally distributed\nacross all nodes. (3) Fully dynamic and asynchronous communication, in which\ncommunication is modelled as just another task in the task-based scheme,\nsending data whenever it is ready and deferring on tasks that rely on data from\nother nodes until it arrives. In order to use these approaches, the code had to\nbe re-written from scratch, and the algorithms therein adapted to the\ntask-based paradigm. As a result, we can show upwards of 60% parallel\nefficiency for moderate-sized problems when increasing the number of cores\n512-fold, on both x86-based and Power8-based architectures. \n\n"}
{"id": "1606.03044", "contents": "Title: The \"Horse'' Inside: Seeking Causes Behind the Behaviours of Music\n  Content Analysis Systems Abstract: Building systems that possess the sensitivity and intelligence to identify\nand describe high-level attributes in music audio signals continues to be an\nelusive goal, but one that surely has broad and deep implications for a wide\nvariety of applications. Hundreds of papers have so far been published toward\nthis goal, and great progress appears to have been made. Some systems produce\nremarkable accuracies at recognising high-level semantic concepts, such as\nmusic style, genre and mood. However, it might be that these numbers do not\nmean what they seem. In this paper, we take a state-of-the-art music content\nanalysis system and investigate what causes it to achieve exceptionally high\nperformance in a benchmark music audio dataset. We dissect the system to\nunderstand its operation, determine its sensitivities and limitations, and\npredict the kinds of knowledge it could and could not possess about music. We\nperform a series of experiments to illuminate what the system has actually\nlearned to do, and to what extent it is performing the intended music listening\ntask. Our results demonstrate how the initial manifestation of music\nintelligence in this state-of-the-art can be deceptive. Our work provides\nconstructive directions toward developing music content analysis systems that\ncan address the music information and creation needs of real-world users. \n\n"}
{"id": "1606.03335", "contents": "Title: WordNet2Vec: Corpora Agnostic Word Vectorization Method Abstract: A complex nature of big data resources demands new methods for structuring\nespecially for textual content. WordNet is a good knowledge source for\ncomprehensive abstraction of natural language as its good implementations exist\nfor many languages. Since WordNet embeds natural language in the form of a\ncomplex network, a transformation mechanism WordNet2Vec is proposed in the\npaper. It creates vectors for each word from WordNet. These vectors encapsulate\ngeneral position - role of a given word towards all other words in the natural\nlanguage. Any list or set of such vectors contains knowledge about the context\nof its component within the whole language. Such word representation can be\neasily applied to many analytic tasks like classification or clustering. The\nusefulness of the WordNet2Vec method was demonstrated in sentiment analysis,\ni.e. classification with transfer learning for the real Amazon opinion textual\ndataset. \n\n"}
{"id": "1606.03418", "contents": "Title: Asynchronous Distributed Hypothesis Testing in the Presence of Crash\n  Failures Abstract: This paper addresses the problem of distributed hypothesis testing in\nmulti-agent networks, where agents repeatedly collect local observations about\nan unknown state of the world, and try to collaboratively detect the true state\nthrough information exchange. We focus on the impact of failures and asynchrony\n(two fundamental factors in distributed systems) on the performance of\nconsensus-based non-Bayesian learning. In particular, we consider the scenario\nwhere the networked agents may suffer crash faults, and messages delay can be\narbitrarily long but finite. We identify the minimal global detectability of\nthe network for non-Bayesian rule to succeed. In addition, we obtain a\ngeneralization of a celebrated result by Wolfowitz and Hajnal to submatrices,\nwhich might be of independent interest. \n\n"}
{"id": "1606.03475", "contents": "Title: De-identification of Patient Notes with Recurrent Neural Networks Abstract: Objective: Patient notes in electronic health records (EHRs) may contain\ncritical information for medical investigations. However, the vast majority of\nmedical investigators can only access de-identified notes, in order to protect\nthe confidentiality of patients. In the United States, the Health Insurance\nPortability and Accountability Act (HIPAA) defines 18 types of protected health\ninformation (PHI) that needs to be removed to de-identify patient notes. Manual\nde-identification is impractical given the size of EHR databases, the limited\nnumber of researchers with access to the non-de-identified notes, and the\nfrequent mistakes of human annotators. A reliable automated de-identification\nsystem would consequently be of high value.\n  Materials and Methods: We introduce the first de-identification system based\non artificial neural networks (ANNs), which requires no handcrafted features or\nrules, unlike existing systems. We compare the performance of the system with\nstate-of-the-art systems on two datasets: the i2b2 2014 de-identification\nchallenge dataset, which is the largest publicly available de-identification\ndataset, and the MIMIC de-identification dataset, which we assembled and is\ntwice as large as the i2b2 2014 dataset.\n  Results: Our ANN model outperforms the state-of-the-art systems. It yields an\nF1-score of 97.85 on the i2b2 2014 dataset, with a recall 97.38 and a precision\nof 97.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with\na recall 99.25 and a precision of 99.06.\n  Conclusion: Our findings support the use of ANNs for de-identification of\npatient notes, as they show better performance than previously published\nsystems while requiring no feature engineering. \n\n"}
{"id": "1606.03935", "contents": "Title: A framework for redescription set construction Abstract: Redescription mining is a field of knowledge discovery that aims at finding\ndifferent descriptions of similar subsets of instances in the data. These\ndescriptions are represented as rules inferred from one or more disjoint sets\nof attributes, called views. As such, they support knowledge discovery process\nand help domain experts in formulating new hypotheses or constructing new\nknowledge bases and decision support systems. In contrast to previous\napproaches that typically create one smaller set of redescriptions satisfying a\npre-defined set of constraints, we introduce a framework that creates large and\nheterogeneous redescription set from which user/expert can extract compact sets\nof differing properties, according to its own preferences. Construction of\nlarge and heterogeneous redescription set relies on CLUS-RM algorithm and a\nnovel, conjunctive refinement procedure that facilitates generation of larger\nand more accurate redescription sets. The work also introduces the variability\nof redescription accuracy when missing values are present in the data, which\nsignificantly extends applicability of the method. Crucial part of the\nframework is the redescription set extraction based on heuristic\nmulti-objective optimization procedure that allows user to define importance\nlevels towards one or more redescription quality criteria. We provide both\ntheoretical and empirical comparison of the novel framework against current\nstate of the art redescription mining algorithms and show that it represents\nmore efficient and versatile approach for mining redescriptions from data. \n\n"}
{"id": "1606.05385", "contents": "Title: D2O - a distributed data object for parallel high-performance computing\n  in Python Abstract: We introduce D2O, a Python module for cluster-distributed multi-dimensional\nnumerical arrays. It acts as a layer of abstraction between the algorithm code\nand the data-distribution logic. The main goal is to achieve usability without\nlosing numerical performance and scalability. D2O's global interface is similar\nto the one of a numpy.ndarray, whereas the cluster node's local data is\ndirectly accessible for use in customized high-performance modules. D2O is\nwritten in pure Python which makes it portable and easy to use and modify.\nExpensive operations are carried out by dedicated external libraries like numpy\nand mpi4py. The performance of D2O is on a par with numpy for serial\napplications and scales well when moving to an MPI cluster. D2O is open-source\nsoftware available under the GNU General Public License v3 (GPL-3) at\nhttps://gitlab.mpcdf.mpg.de/ift/D2O \n\n"}
{"id": "1606.05916", "contents": "Title: On the homotopy groups of spheres in homotopy type theory Abstract: The goal of this thesis is to prove that $\\pi_4(S^3) \\simeq\n\\mathbb{Z}/2\\mathbb{Z}$ in homotopy type theory. In particular it is a\nconstructive and purely homotopy-theoretic proof. We first recall the basic\nconcepts of homotopy type theory, and we prove some well-known results about\nthe homotopy groups of spheres: the computation of the homotopy groups of the\ncircle, the triviality of those of the form $\\pi_k(S^n)$ with $k < n$, and the\nconstruction of the Hopf fibration. We then move to more advanced tools. In\nparticular, we define the James construction which allows us to prove the\nFreudenthal suspension theorem and the fact that there exists a natural number\n$n$ such that $\\pi_4(S^3) \\simeq \\mathbb{Z}/n\\mathbb{Z}$. Then we study the\nsmash product of spheres, we construct the cohomology ring of a space, and we\nintroduce the Hopf invariant, allowing us to narrow down the $n$ to either $1$\nor $2$. The Hopf invariant also allows us to prove that all the groups of the\nform $\\pi_{4n-1}(S^{2n})$ are infinite. Finally we construct the Gysin exact\nsequence, allowing us to compute the cohomology of $\\mathbb{C}P^2$ and to prove\nthat $\\pi_4(S^3) \\simeq \\mathbb{Z}/2\\mathbb{Z}$ and that more generally\n$\\pi_{n+1}(S^n) \\simeq \\mathbb{Z}/2\\mathbb{Z}$ for every $n \\ge 3$. \n\n"}
{"id": "1606.06379", "contents": "Title: Answer-Type Modification without Tears: Prompt-Passing Style Translation\n  for Typed Delimited-Control Operators Abstract: The salient feature of delimited-control operators is their ability to modify\nanswer types during computation. The feature, answer-type modification (ATM for\nshort), allows one to express various interesting programs such as typed printf\ncompactly and nicely, while it makes it difficult to embed these operators in\nstandard functional languages.\n  In this paper, we present a typed translation of delimited-control operators\nshift and reset with ATM into a familiar language with multi-prompt shift and\nreset without ATM, which lets us use ATM in standard languages without\nmodifying the type system. Our translation generalizes Kiselyov's direct-style\nimplementation of typed printf, which uses two prompts to emulate the\nmodification of answer types, and passes them during computation. We prove that\nour translation preserves typing. As the naive prompt-passing style translation\ngenerates and passes many prompts even for pure terms, we show an optimized\ntranslation that generate prompts only when needed, which is also\ntype-preserving. Finally, we give an implementation in the tagless-final style\nwhich respects typing by construction. \n\n"}
{"id": "1606.06461", "contents": "Title: Neighborhood Mixture Model for Knowledge Base Completion Abstract: Knowledge bases are useful resources for many natural language processing\ntasks, however, they are far from complete. In this paper, we define a novel\nentity representation as a mixture of its neighborhood in the knowledge base\nand apply this technique on TransE-a well-known embedding model for knowledge\nbase completion. Experimental results show that the neighborhood information\nsignificantly helps to improve the results of the TransE model, leading to\nbetter performance than obtained by other state-of-the-art embedding models on\nthree benchmark datasets for triple classification, entity prediction and\nrelation prediction tasks. \n\n"}
{"id": "1606.06629", "contents": "Title: Parallel Galton Watson Process Abstract: In this paper, we study a parallel version of Galton-Watson processes for the\nrandom generation of tree-shaped structures. Random trees are useful in many\nsituations (testing, binary search, simulation of physics phenomena,...) as\nattests more than 49000 citations on Google scholar. Using standard analytic\ncombinatorics, we first give a theoretical, average-case study of the random\nprocess in order to evaluate how parallelism can be extracted from this\nprocess, and we deduce a parallel generation algorithm. Then we present how it\ncan be implemented in a task-based parallel paradigm for shared memory (here,\nIntel Cilk). This implementation faces several challenges, among which\nefficient, thread-safe random bit generation, memory management and algorithmic\nmodifications for small-grain parallelism. Finally, we evaluate the performance\nof our implementation and the impact of different choices and parameters. We\nobtain a significant efficiency improvement for the generation of big trees. We\nalso conduct empirical and theoretical studies of the average behaviour of our\nalgorithm. \n\n"}
{"id": "1606.07822", "contents": "Title: Efficient Parallel Learning of Word2Vec Abstract: Since its introduction, Word2Vec and its variants are widely used to learn\nsemantics-preserving representations of words or entities in an embedding\nspace, which can be used to produce state-of-art results for various Natural\nLanguage Processing tasks. Existing implementations aim to learn efficiently by\nrunning multiple threads in parallel while operating on a single model in\nshared memory, ignoring incidental memory update collisions. We show that these\ncollisions can degrade the efficiency of parallel learning, and propose a\nstraightforward caching strategy that improves the efficiency by a factor of 4. \n\n"}
{"id": "1606.07860", "contents": "Title: Non-Monotonic Spatial Reasoning with Answer Set Programming Modulo\n  Theories Abstract: The systematic modelling of dynamic spatial systems is a key requirement in a\nwide range of application areas such as commonsense cognitive robotics,\ncomputer-aided architecture design, and dynamic geographic information systems.\nWe present ASPMT(QS), a novel approach and fully-implemented prototype for\nnon-monotonic spatial reasoning -a crucial requirement within dynamic spatial\nsystems- based on Answer Set Programming Modulo Theories (ASPMT).\n  ASPMT(QS) consists of a (qualitative) spatial representation module (QS) and\na method for turning tight ASPMT instances into Satisfiability Modulo Theories\n(SMT) instances in order to compute stable models by means of SMT solvers. We\nformalise and implement concepts of default spatial reasoning and spatial frame\naxioms. Spatial reasoning is performed by encoding spatial relations as systems\nof polynomial constraints, and solving via SMT with the theory of real\nnonlinear arithmetic. We empirically evaluate ASPMT(QS) in comparison with\nother contemporary spatial reasoning systems both within and outside the\ncontext of logic programming. ASPMT(QS) is currently the only existing system\nthat is capable of reasoning about indirect spatial effects (i.e., addressing\nthe ramification problem), and integrating geometric and qualitative spatial\ninformation within a non-monotonic spatial reasoning context.\n  This paper is under consideration for publication in TPLP. \n\n"}
{"id": "1606.08359", "contents": "Title: Lifted Rule Injection for Relation Embeddings Abstract: Methods based on representation learning currently hold the state-of-the-art\nin many natural language processing and knowledge base inference tasks. Yet, a\nmajor challenge is how to efficiently incorporate commonsense knowledge into\nsuch models. A recent approach regularizes relation and entity representations\nby propositionalization of first-order logic rules. However,\npropositionalization does not scale beyond domains with only few entities and\nrules. In this paper we present a highly efficient method for incorporating\nimplication rules into distributed representations for automated knowledge base\nconstruction. We map entity-tuple embeddings into an approximately Boolean\nspace and encourage a partial ordering over relation embeddings based on\nimplication rules mined from WordNet. Surprisingly, we find that the strong\nrestriction of the entity-tuple embedding space does not hurt the\nexpressiveness of the model and even acts as a regularizer that improves\ngeneralization. By incorporating few commonsense rules, we achieve an increase\nof 2 percentage points mean average precision over a matrix factorization\nbaseline, while observing a negligible increase in runtime. \n\n"}
{"id": "1606.08815", "contents": "Title: Undecidable Cases of Model Checking Probabilistic Temporal-Epistemic\n  Logic (Extended Abstract) Abstract: We investigate the decidability of model-checking logics of time, knowledge\nand probability, with respect to two epistemic semantics: the clock and\nsynchronous perfect recall semantics in partially observed discrete-time Markov\nchains. Decidability results are known for certain restricted logics with\nrespect to these semantics, subject to a variety of restrictions that are\neither unexplained or involve a longstanding unsolved mathematical problem. We\nshow that mild generalizations of the known decidable cases suffice to render\nthe model checking problem definitively undecidable. In particular, for a\nsynchronous perfect recall, a generalization from temporal operators with\nfinite reach to operators with infinite reach renders model checking\nundecidable. The case of the clock semantics is closely related to a monadic\nsecond order logic of time and probability that is known to be decidable,\nexcept on a set of measure zero. We show that two distinct extensions of this\nlogic make model checking undecidable. One of these involves polynomial\ncombinations of probability terms, the other involves monadic second order\nquantification into the scope of probability operators. These results explain\nsome of the restrictions in previous work. \n\n"}
{"id": "1606.09242", "contents": "Title: Swift: Compiled Inference for Probabilistic Programming Languages Abstract: A probabilistic program defines a probability measure over its semantic\nstructures. One common goal of probabilistic programming languages (PPLs) is to\ncompute posterior probabilities for arbitrary models and queries, given\nobserved evidence, using a generic inference engine. Most PPL inference\nengines---even the compiled ones---incur significant runtime interpretation\noverhead, especially for contingent and open-universe models. This paper\ndescribes Swift, a compiler for the BLOG PPL. Swift-generated code incorporates\noptimizations that eliminate interpretation overhead, maintain dynamic\ndependencies efficiently, and handle memory management for possible worlds of\nvarying sizes. Experiments comparing Swift with other PPL engines on a variety\nof inference problems demonstrate speedups ranging from 12x to 326x. \n\n"}
{"id": "1606.09274", "contents": "Title: Compression of Neural Machine Translation Models via Pruning Abstract: Neural Machine Translation (NMT), like many other deep learning domains,\ntypically suffers from over-parameterization, resulting in large storage sizes.\nThis paper examines three simple magnitude-based pruning schemes to compress\nNMT models, namely class-blind, class-uniform, and class-distribution, which\ndiffer in terms of how pruning thresholds are computed for the different\nclasses of weights in the NMT architecture. We demonstrate the efficacy of\nweight pruning as a compression technique for a state-of-the-art NMT system. We\nshow that an NMT model with over 200 million parameters can be pruned by 40%\nwith very little performance loss as measured on the WMT'14 English-German\ntranslation task. This sheds light on the distribution of redundancy in the NMT\narchitecture. Our main result is that with retraining, we can recover and even\nsurpass the original performance with an 80%-pruned model. \n\n"}
{"id": "1606.09399", "contents": "Title: Coalgebraic Trace Semantics for Buechi and Parity Automata Abstract: Despite its success in producing numerous general results on state-based\ndynamics, the theory of coalgebra has struggled to accommodate the Buechi\nacceptance condition---a basic notion in the theory of automata for infinite\nwords or trees. In this paper we present a clean answer to the question that\nbuilds on the \"maximality\" characterization of infinite traces (by Jacobs and\nCirstea): the accepted language of a Buechi automaton is characterized by two\ncommuting diagrams, one for a least homomorphism and the other for a greatest,\nmuch like in a system of (least and greatest) fixed-point equations. This\ncharacterization works uniformly for the nondeterministic branching and the\nprobabilistic one; and for words and trees alike. We present our results in\nterms of the parity acceptance condition that generalizes Buechi's. \n\n"}
{"id": "1606.09449", "contents": "Title: Clique-Width and Directed Width Measures for Answer-Set Programming Abstract: Disjunctive Answer Set Programming (ASP) is a powerful declarative\nprogramming paradigm whose main decision problems are located on the second\nlevel of the polynomial hierarchy. Identifying tractable fragments and\ndeveloping efficient algorithms for such fragments are thus important\nobjectives in order to complement the sophisticated ASP systems available to\ndate. Hard problems can become tractable if some problem parameter is bounded\nby a fixed constant; such problems are then called fixed-parameter tractable\n(FPT). While several FPT results for ASP exist, parameters that relate to\ndirected or signed graphs representing the program at hand have been neglected\nso far. In this paper, we first give some negative observations showing that\ndirected width measures on the dependency graph of a program do not lead to FPT\nresults. We then consider the graph parameter of signed clique-width and\npresent a novel dynamic programming algorithm that is FPT w.r.t. this\nparameter. Clique-width is more general than the well-known treewidth, and, to\nthe best of our knowledge, ours is the first FPT algorithm for bounded\nclique-width for reasoning problems beyond SAT. \n\n"}
{"id": "1607.00976", "contents": "Title: Modelling Context with User Embeddings for Sarcasm Detection in Social\n  Media Abstract: We introduce a deep neural network for automated sarcasm detection. Recent\nwork has emphasized the need for models to capitalize on contextual features,\nbeyond lexical and syntactic cues present in utterances. For example, different\nspeakers will tend to employ sarcasm regarding different subjects and, thus,\nsarcasm detection models ought to encode such speaker information. Current\nmethods have achieved this by way of laborious feature engineering. By\ncontrast, we propose to automatically learn and then exploit user embeddings,\nto be used in concert with lexical signals to recognize sarcasm. Our approach\ndoes not require elaborate feature engineering (and concomitant data scraping);\nfitting user embeddings requires only the text from their previous posts. The\nexperimental results show that our model outperforms a state-of-the-art\napproach leveraging an extensive set of carefully crafted features. \n\n"}
{"id": "1607.02533", "contents": "Title: Adversarial examples in the physical world Abstract: Most existing machine learning classifiers are highly vulnerable to\nadversarial examples. An adversarial example is a sample of input data which\nhas been modified very slightly in a way that is intended to cause a machine\nlearning classifier to misclassify it. In many cases, these modifications can\nbe so subtle that a human observer does not even notice the modification at\nall, yet the classifier still makes a mistake. Adversarial examples pose\nsecurity concerns because they could be used to perform an attack on machine\nlearning systems, even if the adversary has no access to the underlying model.\nUp to now, all previous work have assumed a threat model in which the adversary\ncan feed data directly into the machine learning classifier. This is not always\nthe case for systems operating in the physical world, for example those which\nare using signals from cameras and other sensors as an input. This paper shows\nthat even in such physical world scenarios, machine learning systems are\nvulnerable to adversarial examples. We demonstrate this by feeding adversarial\nimages obtained from cell-phone camera to an ImageNet Inception classifier and\nmeasuring the classification accuracy of the system. We find that a large\nfraction of adversarial examples are classified incorrectly even when perceived\nthrough the camera. \n\n"}
{"id": "1607.02951", "contents": "Title: Design Patterns in Beeping Algorithms: Examples, Emulation, and Analysis Abstract: We consider networks of processes which interact with beeps. In the basic\nmodel defined by Cornejo and Kuhn (2010), processes can choose in each round\neither to beep or to listen. Those who beep are unable to detect simultaneous\nbeeps. Those who listen can only distinguish between silence and the presence\nof at least one beep. We refer to this model as $BL$ (beep or listen). Stronger\nmodels exist where the nodes can detect collision while they are beeping\n($B_{cd}L$), listening ($BL_{cd}$), or both ($B_{cd}L_{cd}$). Beeping models\nare weak in essence and even simple tasks are difficult or unfeasible within.\n  We present a set of generic building blocks (design patterns) which seem to\noccur frequently in the design of beeping algorithms. They include multi-slot\nphases: the fact of dividing the main loop into a number of specialised slots;\nexclusive beeps: having a single node beep at a time in a neighbourhood (within\none or two hops); adaptive probability: increasing or decreasing the\nprobability of beeping to produce more exclusive beeps; internal (resp.\nperipheral) collision detection: for detecting collision while beeping (resp.\nlistening). Based on these patterns, we provide algorithms for a number of\nbasic problems, including colouring, 2-hop colouring, degree computation, 2-hop\nMIS, and collision detection (in $BL$). The patterns make it possible to\nformulate these algorithms in a rather concise and elegant way. Their analyses\nare more technical; one of them improves significantly upon that of the best\nknown MIS algorithm by Jeavons et al. (2016). Finally, inspired by a technique\nfrom Afek et al. (2013), our last contribution is to show that any Las Vegas\nalgorithm relying on collision detection can be transposed into a Monte Carlo\nalgorithm without collision detection at the cost of a logarithmic slowdown,\nwhich we prove is optimal. \n\n"}
{"id": "1607.04500", "contents": "Title: Number representations and term rewriting Abstract: In this paper we examine a number of term rewriting system for integer number\nrepresentations, building further upon the datatype defining systems described\nin [2]. In particular, we look at automated methods for proving confluence and\ntermination in binary and decimal term rewriting systems for both append and\ntree constructor functions. We find that some of these term rewriting systems\nare not strongly terminating, which we resolve with minor changes to these\nsystems. Moreover, most of the term rewriting systems discussed do not exhibit\nthe confluence property, which seems more difficult to resolve. \n\n"}
{"id": "1607.05212", "contents": "Title: Polynomial Lower Bound for Distributed Graph Coloring in a Weak LOCAL\n  Model Abstract: We show an $\\Omega\\big(\\Delta^{\\frac{1}{3}-\\frac{\\eta}{3}}\\big)$ lower bound\non the runtime of any deterministic distributed\n$\\mathcal{O}\\big(\\Delta^{1+\\eta}\\big)$-graph coloring algorithm in a weak\nvariant of the \\LOCAL\\ model.\n  In particular, given a network graph \\mbox{$G=(V,E)$}, in the weak \\LOCAL\\\nmodel nodes communicate in synchronous rounds and they can use unbounded local\ncomputation. We assume that the nodes have no identifiers, but that instead,\nthe computation starts with an initial valid vertex coloring. A node can\n\\textbf{broadcast} a \\textbf{single} message of \\textbf{unbounded} size to its\nneighbors and receives the \\textbf{set of messages} sent to it by its\nneighbors. That is, if two neighbors of a node $v\\in V$ send the same message\nto $v$, $v$ will receive this message only a single time; without any further\nknowledge, $v$ cannot know whether a received message was sent by only one or\nmore than one neighbor.\n  Neighborhood graphs have been essential in the proof of lower bounds for\ndistributed coloring algorithms, e.g., \\cite{linial92,Kuhn2006On}. Our proof\nanalyzes the recursive structure of the neighborhood graph of the respective\nmodel to devise an $\\Omega\\big(\\Delta^{\\frac{1}{3}-\\frac{\\eta}{3}}\\big)$ lower\nbound on the runtime for any deterministic distributed\n$\\mathcal{O}\\big(\\Delta^{1+\\eta}\\big)$-graph coloring algorithm.\n  Furthermore, we hope that the proof technique improves the understanding of\nneighborhood graphs in general and that it will help towards finding a lower\n(runtime) bound for distributed graph coloring in the standard \\LOCAL\\ model.\nOur proof technique works for one-round algorithms in the standard \\LOCAL\\\nmodel and provides a simpler and more intuitive proof for an existing\n$\\Omega(\\Delta^2)$ lower bound. \n\n"}
{"id": "1607.07348", "contents": "Title: Spark Parameter Tuning via Trial-and-Error Abstract: Spark has been established as an attractive platform for big data analysis,\nsince it manages to hide most of the complexities related to parallelism, fault\ntolerance and cluster setting from developers. However, this comes at the\nexpense of having over 150 configurable parameters, the impact of which cannot\nbe exhaustively examined due to the exponential amount of their combinations.\nThe default values allow developers to quickly deploy their applications but\nleave the question as to whether performance can be improved open. In this\nwork, we investigate the impact of the most important of the tunable Spark\nparameters on the application performance and guide developers on how to\nproceed to changes to the default values. We conduct a series of experiments\nwith known benchmarks on the MareNostrum petascale supercomputer to test the\nperformance sensitivity. More importantly, we offer a trial-and-error\nmethodology for tuning parameters in arbitrary applications based on evidence\nfrom a very small number of experimental runs. We test our methodology in three\ncase studies, where we manage to achieve speedups of more than 10 times. \n\n"}
{"id": "1607.08316", "contents": "Title: Efficient Hyperparameter Optimization of Deep Learning Algorithms Using\n  Deterministic RBF Surrogates Abstract: Automatically searching for optimal hyperparameter configurations is of\ncrucial importance for applying deep learning algorithms in practice. Recently,\nBayesian optimization has been proposed for optimizing hyperparameters of\nvarious machine learning algorithms. Those methods adopt probabilistic\nsurrogate models like Gaussian processes to approximate and minimize the\nvalidation error function of hyperparameter values. However, probabilistic\nsurrogates require accurate estimates of sufficient statistics (e.g.,\ncovariance) of the error distribution and thus need many function evaluations\nwith a sizeable number of hyperparameters. This makes them inefficient for\noptimizing hyperparameters of deep learning algorithms, which are highly\nexpensive to evaluate. In this work, we propose a new deterministic and\nefficient hyperparameter optimization method that employs radial basis\nfunctions as error surrogates. The proposed mixed integer algorithm, called\nHORD, searches the surrogate for the most promising hyperparameter values\nthrough dynamic coordinate search and requires many fewer function evaluations.\nHORD does well in low dimensions but it is exceptionally better in higher\ndimensions. Extensive evaluations on MNIST and CIFAR-10 for four deep neural\nnetworks demonstrate HORD significantly outperforms the well-established\nBayesian optimization methods such as GP, SMAC, and TPE. For instance, on\naverage, HORD is more than 6 times faster than GP-EI in obtaining the best\nconfiguration of 19 hyperparameters. \n\n"}
{"id": "1608.00695", "contents": "Title: The blockchain: a new framework for robotic swarm systems Abstract: Swarms of robots will revolutionize many industrial applications, from\ntargeted material delivery to precision farming. However, several of the\nheterogeneous characteristics that make them ideal for certain future\napplications --- robot autonomy, decentralized control, collective emergent\nbehavior, etc. --- hinder the evolution of the technology from academic\ninstitutions to real-world problems. Blockchain, an emerging technology\noriginated in the Bitcoin field, demonstrates that by combining peer-to-peer\nnetworks with cryptographic algorithms a group of agents can reach an agreement\non a particular state of affairs and record that agreement without the need for\na controlling authority. The combination of blockchain with other distributed\nsystems, such as robotic swarm systems, can provide the necessary capabilities\nto make robotic swarm operations more secure, autonomous, flexible and even\nprofitable. This work explains how blockchain technology can provide innovative\nsolutions to four emergent issues in the swarm robotics research field. New\nsecurity, decision making, behavior differentiation and business models for\nswarm robotic systems are described by providing case scenarios and examples.\nFinally, limitations and possible future problems that arise from the\ncombination of these two technologies are described. \n\n"}
{"id": "1608.00726", "contents": "Title: Infinite Unlimited Churn Abstract: We study unlimited infinite churn in peer-to-peer overlay networks. Under\nthis churn, arbitrary many peers may concurrently request to join or leave the\noverlay network; moreover these requests may never stop coming. We prove that\nunlimited adversarial churn, where processes may just exit the overlay network,\nis unsolvable. We focus on cooperative churn where exiting processes\nparticipate in the churn handling algorithm. We define the problem of unlimited\ninfinite churn in this setting. We distinguish the fair version of the problem,\nwhere each request is eventually satisfied, from the unfair version that just\nguarantees progress. We focus on local solutions to the problem, and prove that\na local solution to the Fair Infinite Unlimited Churn is impossible. We then\npresent and prove correct an algorithm UIUC that solves the Unfair Infinite\nUnlimited Churn Problem for a linearized peer-to-peer overlay network. We\nextend this solution to skip lists and skip graphs. \n\n"}
{"id": "1608.01689", "contents": "Title: Derandomizing Local Distributed Algorithms under Bandwidth Restrictions Abstract: This paper addresses the cornerstone family of \\emph{local problems} in\ndistributed computing, and investigates the curious gap between randomized and\ndeterministic solutions under bandwidth restrictions.\n  Our main contribution is in providing tools for derandomizing solutions to\nlocal problems, when the $n$ nodes can only send $O(\\log n)$-bit messages in\neach round of communication. We combine bounded independence, which we show to\nbe sufficient for some algorithms, with the method of conditional expectations\nand with additional machinery, to obtain the following results.\n  Our techniques give a deterministic maximal independent set (MIS) algorithm\nin the CONGEST model, where the communication graph is identical to the input\ngraph, in $O(D\\log^2 n)$ rounds, where $D$ is the diameter of the graph. The\nbest known running time in terms of $n$ alone is $2^{O(\\sqrt{\\log n})}$, which\nis super-polylogarithmic, and requires large messages. For the CONGEST model,\nthe only known previous solution is a coloring-based $O(\\Delta + \\log^*\nn)$-round algorithm, where $\\Delta$ is the maximal degree in the graph.\n  On the way to obtaining the above, we show that in the \\emph{Congested\nClique} model, which allows all-to-all communication, there is a deterministic\nMIS algorithm that runs in $O(\\log \\Delta \\log n)$ rounds.%, where $\\Delta$ is\nthe maximum degree. When $\\Delta=O(n^{1/3})$, the bound improves to $O(\\log\n\\Delta)$ and holds also for $(\\Delta+1)$-coloring.\n  In addition, we deterministically construct a $(2k-1)$-spanner with\n$O(kn^{1+1/k}\\log n)$ edges in $O(k \\log n)$ rounds. For comparison, in the\nmore stringent CONGEST model, the best deterministic algorithm for constructing\na $(2k-1)$-spanner with $O(kn^{1+1/k})$ edges runs in $O(n^{1-1/k})$ rounds. \n\n"}
{"id": "1608.02644", "contents": "Title: Holophrasm: a neural Automated Theorem Prover for higher-order logic Abstract: I propose a system for Automated Theorem Proving in higher order logic using\ndeep learning and eschewing hand-constructed features. Holophrasm exploits the\nformalism of the Metamath language and explores partial proof trees using a\nneural-network-augmented bandit algorithm and a sequence-to-sequence model for\naction enumeration. The system proves 14% of its test theorems from Metamath's\nset.mm module. \n\n"}
{"id": "1608.03000", "contents": "Title: Neural Generation of Regular Expressions from Natural Language with\n  Minimal Domain Knowledge Abstract: This paper explores the task of translating natural language queries into\nregular expressions which embody their meaning. In contrast to prior work, the\nproposed neural model does not utilize domain-specific crafting, learning to\ntranslate directly from a parallel corpus. To fully explore the potential of\nneural models, we propose a methodology for collecting a large corpus of\nregular expression, natural language pairs. Our resulting model achieves a\nperformance gain of 19.6% over previous state-of-the-art models. \n\n"}
{"id": "1608.03866", "contents": "Title: Distributed Optimization for Client-Server Architecture with Negative\n  Gradient Weights Abstract: Availability of both massive datasets and computing resources have made\nmachine learning and predictive analytics extremely pervasive. In this work we\npresent a synchronous algorithm and architecture for distributed optimization\nmotivated by privacy requirements posed by applications in machine learning. We\npresent an algorithm for the recently proposed multi-parameter-server\narchitecture. We consider a group of parameter servers that learn a model based\non randomized gradients received from clients. Clients are computational\nentities with private datasets (inducing a private objective function), that\nevaluate and upload randomized gradients to the parameter servers. The\nparameter servers perform model updates based on received gradients and share\nthe model parameters with other servers. We prove that the proposed algorithm\ncan optimize the overall objective function for a very general architecture\ninvolving $C$ clients connected to $S$ parameter servers in an arbitrary time\nvarying topology and the parameter servers forming a connected network. \n\n"}
{"id": "1608.05368", "contents": "Title: Scaling Bounded Model Checking By Transforming Programs With Arrays Abstract: Bounded Model Checking is one the most successful techniques for finding bugs\nin program. However, for programs with loops iterating over large-sized arrays,\nbounded model checkers often exceed the limit of resources available to them.\nWe present a transformation that enables bounded model checkers to verify a\ncertain class of array properties. Our technique transforms an\narray-manipulating program in ANSI-C to an array-free and loop-free program.\nThe transformed program can efficiently be verified by an off-the-shelf bounded\nmodel checker. Though the transformed program is, in general, an abstraction of\nthe original program, we formally characterize the properties for which the\ntransformation is precise. We demonstrate the applicability and usefulness of\nour technique on both industry code as well as academic benchmarks. \n\n"}
{"id": "1608.07249", "contents": "Title: Benchmarking State-of-the-Art Deep Learning Software Tools Abstract: Deep learning has been shown as a successful machine learning method for a\nvariety of tasks, and its popularity results in numerous open-source deep\nlearning software tools. Training a deep network is usually a very\ntime-consuming process. To address the computational challenge in deep\nlearning, many tools exploit hardware features such as multi-core CPUs and\nmany-core GPUs to shorten the training time. However, different tools exhibit\ndifferent features and running performance when training different types of\ndeep networks on different hardware platforms, which makes it difficult for end\nusers to select an appropriate pair of software and hardware. In this paper, we\naim to make a comparative study of the state-of-the-art GPU-accelerated deep\nlearning software tools, including Caffe, CNTK, MXNet, TensorFlow, and Torch.\nWe first benchmark the running performance of these tools with three popular\ntypes of neural networks on two CPU platforms and three GPU platforms. We then\nbenchmark some distributed versions on multiple GPUs. Our contribution is\ntwo-fold. First, for end users of deep learning tools, our benchmarking results\ncan serve as a guide to selecting appropriate hardware platforms and software\ntools. Second, for software developers of deep learning tools, our in-depth\nanalysis points out possible future directions to further optimize the running\nperformance. \n\n"}
{"id": "1608.07792", "contents": "Title: A Generalized Resolution Proof Schema and the Pigeonhole Principle Abstract: The schematic CERES method is a method of cut elimination for proof schemata,\nthat is a sequence of proofs with a recursive construction. Proof schemata can\nbe thought of as a way to circumvent the addition of an induction rule to the\nLK-calculus. In this work, we formalize a schematic version of the Infinitary\nPigeonhole Principle (IPP), in the LKS-calculus, and analyse the extracted\nclause set schema. However, the refutation we find cannot be expressed as a\nresolution proof schema because there is no clear ordering of the terms\nindexing the recursion, every ordering is used in the refutation. Interesting\nenough, the clause set and its refutation is very close to a canonical form\nfound in cut elimination of LK-proofs. Not being able to handle refutations of\nthis form is problematic in that proof schema, when instantiated, are\nLK-proofs. Based on the structure of our refutation and structural results, we\ndevelop a generalized resolution proof schema based on recursion over a special\ntype of list, and provide a refutation, using our generalization, of the clause\nset extracted from our formal proof of IPP. We also extract a Herbrand System\nfrom the refutation. \n\n"}
{"id": "1608.08754", "contents": "Title: Towards Concolic Testing for Hybrid Systems Abstract: Hybrid systems exhibit both continuous and discrete behavior. Analyzing\nhybrid systems is known to be hard. Inspired by the idea of concolic testing\n(of programs), we investigate whether we can combine random sampling and\nsymbolic execution in order to effectively verify hybrid systems. We identify a\nsufficient condition under which such a combination is more effective than\nrandom sampling. Furthermore, we analyze different strategies of combining\nrandom sampling and symbolic execution and propose an algorithm which allows us\nto dynamically switch between them so as to reduce the overall cost. Our method\nhas been implemented as a web-based checker named HyChecker. HyChecker has been\nevaluated with benchmark hybrid systems and a water treatment system in order\nto test its effectiveness. \n\n"}
{"id": "1609.01479", "contents": "Title: A Lightweight Approach to Performance Portability with targetDP Abstract: Leading HPC systems achieve their status through use of highly parallel\ndevices such as NVIDIA GPUs or Intel Xeon Phi many-core CPUs. The concept of\nperformance portability across such architectures, as well as traditional CPUs,\nis vital for the application programmer. In this paper we describe targetDP, a\nlightweight abstraction layer which allows grid-based applications to target\ndata parallel hardware in a platform agnostic manner. We demonstrate the\neffectiveness of our pragmatic approach by presenting performance results for a\ncomplex fluid application (with which the model was co-designed), plus a\nseparate lattice QCD particle physics code. For each application, a single\nsource code base is seen to achieve portable performance, as assessed within\nthe context of the Roofline model. TargetDP can be combined with MPI to allow\nuse on systems containing multiple nodes: we demonstrate this through provision\nof scaling results on traditional and GPU-accelerated large scale\nsupercomputers. \n\n"}
{"id": "1609.02104", "contents": "Title: A Consumer-Centric Market for Database Computation in the Cloud Abstract: The availability of public computing resources in the cloud has\nrevolutionized data analysis, but requesting cloud resources often involves\ncomplex decisions for consumers. Under the current pricing mechanisms, cloud\nservice providers offer several service options and charge consumers based on\nthe resources they use. Before they can decide which cloud resources to\nrequest, consumers have to estimate the completion time and cost of their\ncomputational tasks for different service options and possibly for different\nservice providers. This estimation is challenging even for expert cloud users.\nWe propose a new market-based framework for pricing computational tasks in the\ncloud. Our framework introduces an agent between consumers and cloud providers.\nThe agent takes data and computational tasks from users, estimates time and\ncost for evaluating the tasks, and returns to consumers contracts that specify\nthe price and completion time. Our framework can be applied directly to\nexisting cloud markets without altering the way cloud providers offer and price\nservices. In addition, it simplifies cloud use for consumers by allowing them\nto compare contracts, rather than choose resources directly. We present design,\nanalytical, and algorithmic contributions focusing on pricing computation\ncontracts, analyzing their properties, and optimizing them in complex\nworkflows. We conduct an experimental evaluation of our market framework over a\nreal-world cloud service and demonstrate empirically that our market ensures\nthree key properties: competitiveness, fairness, and resilience. Finally, we\npresent a fine-grained pricing mechanism for complex workflows and show that it\ncan increase agent profits by more than an order of magnitude in some cases. \n\n"}
{"id": "1609.02993", "contents": "Title: Episodic Exploration for Deep Deterministic Policies: An Application to\n  StarCraft Micromanagement Tasks Abstract: We consider scenarios from the real-time strategy game StarCraft as new\nbenchmarks for reinforcement learning algorithms. We propose micromanagement\ntasks, which present the problem of the short-term, low-level control of army\nmembers during a battle. From a reinforcement learning point of view, these\nscenarios are challenging because the state-action space is very large, and\nbecause there is no obvious feature representation for the state-action\nevaluation function. We describe our approach to tackle the micromanagement\nscenarios with deep neural network controllers from raw state features given by\nthe game engine. In addition, we present a heuristic reinforcement learning\nalgorithm which combines direct exploration in the policy space and\nbackpropagation. This algorithm allows for the collection of traces for\nlearning using deterministic policies, which appears much more efficient than,\nfor example, {\\epsilon}-greedy exploration. Experiments show that with this\nalgorithm, we successfully learn non-trivial strategies for scenarios with\narmies of up to 15 agents, where both Q-learning and REINFORCE struggle. \n\n"}
{"id": "1609.03784", "contents": "Title: A Fast Proximal Gradient Algorithm for Decentralized Composite\n  Optimization over Directed Networks Abstract: This paper proposes a fast decentralized algorithm for solving a consensus\noptimization problem defined in a directed networked multi-agent system, where\nthe local objective functions have the smooth+nonsmooth composite form, and are\npossibly nonconvex. Examples of such problems include decentralized compressed\nsensing and constrained quadratic programming problems, as well as many\ndecentralized regularization problems. We extend the existing algorithms\nPG-EXTRA and ExtraPush to a new algorithm PG-ExtraPush for composite consensus\noptimization over a directed network. This algorithm takes advantage of the\nproximity operator like in PG-EXTRA to deal with the nonsmooth term, and\nemploys the push-sum protocol like in ExtraPush to tackle the bias introduced\nby the directed network. With a proper step size, we show that PG-ExtraPush\nconverges to an optimal solution at a linear rate under some regular\nassumptions. We conduct a series of numerical experiments to show the\neffectiveness of the proposed algorithm. Specifically, with a proper step size,\nPG-ExtraPush performs linear rates in most of cases, even in some nonconvex\ncases, and is significantly faster than Subgradient-Push, even if the latter\nuses a hand-optimized step size. The established theoretical results are also\nverified by the numerical results. \n\n"}
{"id": "1609.04436", "contents": "Title: Bayesian Reinforcement Learning: A Survey Abstract: Bayesian methods for machine learning have been widely investigated, yielding\nprincipled methods for incorporating prior information into inference\nalgorithms. In this survey, we provide an in-depth review of the role of\nBayesian methods for the reinforcement learning (RL) paradigm. The major\nincentives for incorporating Bayesian reasoning in RL are: 1) it provides an\nelegant approach to action-selection (exploration/exploitation) as a function\nof the uncertainty in learning; and 2) it provides a machinery to incorporate\nprior knowledge into the algorithms. We first discuss models and methods for\nBayesian inference in the simple single-step Bandit model. We then review the\nextensive recent literature on Bayesian methods for model-based RL, where prior\ninformation can be expressed on the parameters of the Markov model. We also\npresent Bayesian methods for model-free RL, where priors are expressed over the\nvalue function or policy class. The objective of the paper is to provide a\ncomprehensive survey on Bayesian RL algorithms and their theoretical and\nempirical properties. \n\n"}
{"id": "1609.09444", "contents": "Title: Contextual RNN-GANs for Abstract Reasoning Diagram Generation Abstract: Understanding, predicting, and generating object motions and transformations\nis a core problem in artificial intelligence. Modeling sequences of evolving\nimages may provide better representations and models of motion and may\nultimately be used for forecasting, simulation, or video generation.\nDiagrammatic Abstract Reasoning is an avenue in which diagrams evolve in\ncomplex patterns and one needs to infer the underlying pattern sequence and\ngenerate the next image in the sequence. For this, we develop a novel\nContextual Generative Adversarial Network based on Recurrent Neural Networks\n(Context-RNN-GANs), where both the generator and the discriminator modules are\nbased on contextual history (modeled as RNNs) and the adversarial discriminator\nguides the generator to produce realistic images for the particular time step\nin the image sequence. We evaluate the Context-RNN-GAN model (and its variants)\non a novel dataset of Diagrammatic Abstract Reasoning, where it performs\ncompetitively with 10th-grade human performance but there is still scope for\ninteresting improvements as compared to college-grade human performance. We\nalso evaluate our model on a standard video next-frame prediction task,\nachieving improved performance over comparable state-of-the-art. \n\n"}
{"id": "1610.00956", "contents": "Title: Embracing data abundance: BookTest Dataset for Reading Comprehension Abstract: There is a practically unlimited amount of natural language data available.\nStill, recent work in text comprehension has focused on datasets which are\nsmall relative to current computing possibilities. This article is making a\ncase for the community to move to larger data and as a step in that direction\nit is proposing the BookTest, a new dataset similar to the popular Children's\nBook Test (CBT), however more than 60 times larger. We show that training on\nthe new data improves the accuracy of our Attention-Sum Reader model on the\noriginal CBT test data by a much larger margin than many recent attempts to\nimprove the model architecture. On one version of the dataset our ensemble even\nexceeds the human baseline provided by Facebook. We then show in our own human\nstudy that there is still space for further improvement. \n\n"}
{"id": "1610.01465", "contents": "Title: Visual Question Answering: Datasets, Algorithms, and Future Challenges Abstract: Visual Question Answering (VQA) is a recent problem in computer vision and\nnatural language processing that has garnered a large amount of interest from\nthe deep learning, computer vision, and natural language processing\ncommunities. In VQA, an algorithm needs to answer text-based questions about\nimages. Since the release of the first VQA dataset in 2014, additional datasets\nhave been released and many algorithms have been proposed. In this review, we\ncritically examine the current state of VQA in terms of problem formulation,\nexisting datasets, evaluation metrics, and algorithms. In particular, we\ndiscuss the limitations of current datasets with regard to their ability to\nproperly train and assess VQA algorithms. We then exhaustively review existing\nalgorithms for VQA. Finally, we discuss possible future directions for VQA and\nimage understanding research. \n\n"}
{"id": "1610.02891", "contents": "Title: Personalizing a Dialogue System with Transfer Reinforcement Learning Abstract: It is difficult to train a personalized task-oriented dialogue system because\nthe data collected from each individual is often insufficient. Personalized\ndialogue systems trained on a small dataset can overfit and make it difficult\nto adapt to different user needs. One way to solve this problem is to consider\na collection of multiple users' data as a source domain and an individual\nuser's data as a target domain, and to perform a transfer learning from the\nsource to the target domain. By following this idea, we propose\n\"PETAL\"(PErsonalized Task-oriented diALogue), a transfer-learning framework\nbased on POMDP to learn a personalized dialogue system. The system first learns\ncommon dialogue knowledge from the source domain and then adapts this knowledge\nto the target user. This framework can avoid the negative transfer problem by\nconsidering differences between source and target users. The policy in the\npersonalized POMDP can learn to choose different actions appropriately for\ndifferent users. Experimental results on a real-world coffee-shopping data and\nsimulation data show that our personalized dialogue system can choose different\noptimal actions for different users, and thus effectively improve the dialogue\nquality under the personalized setting. \n\n"}
{"id": "1610.03295", "contents": "Title: Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving Abstract: Autonomous driving is a multi-agent setting where the host vehicle must apply\nsophisticated negotiation skills with other road users when overtaking, giving\nway, merging, taking left and right turns and while pushing ahead in\nunstructured urban roadways. Since there are many possible scenarios, manually\ntackling all possible cases will likely yield a too simplistic policy.\nMoreover, one must balance between unexpected behavior of other\ndrivers/pedestrians and at the same time not to be too defensive so that normal\ntraffic flow is maintained.\n  In this paper we apply deep reinforcement learning to the problem of forming\nlong term driving strategies. We note that there are two major challenges that\nmake autonomous driving different from other robotic tasks. First, is the\nnecessity for ensuring functional safety - something that machine learning has\ndifficulty with given that performance is optimized at the level of an\nexpectation over many instances. Second, the Markov Decision Process model\noften used in robotics is problematic in our case because of unpredictable\nbehavior of other agents in this multi-agent scenario. We make three\ncontributions in our work. First, we show how policy gradient iterations can be\nused without Markovian assumptions. Second, we decompose the problem into a\ncomposition of a Policy for Desires (which is to be learned) and trajectory\nplanning with hard constraints (which is not learned). The goal of Desires is\nto enable comfort of driving, while hard constraints guarantees the safety of\ndriving. Third, we introduce a hierarchical temporal abstraction we call an\n\"Option Graph\" with a gating mechanism that significantly reduces the effective\nhorizon and thereby reducing the variance of the gradient estimation even\nfurther. \n\n"}
{"id": "1610.04707", "contents": "Title: Reasoning in the Bernays-Schoenfinkel-Ramsey Fragment of Separation\n  Logic Abstract: Separation Logic (SL) is a well-known assertion language used in Hoare-style\nmodular proof systems for programs with dynamically allocated data structures.\nIn this paper we investigate the fragment of first-order SL restricted to the\nBernays-Schoenfinkel-Ramsey quantifier prefix $\\exists^*\\forall^*$, where the\nquantified variables range over the set of memory locations. When this set is\nuninterpreted (has no associated theory) the fragment is PSPACE-complete, which\nmatches the complexity of the quantifier-free fragment. However, SL becomes\nundecidable when the quantifier prefix belongs to $\\exists^*\\forall^*\\exists^*$\ninstead, or when the memory locations are interpreted as integers with linear\narithmetic constraints, thus setting a sharp boundary for decidability within\nSL. We have implemented a decision procedure for the decidable fragment of\n$\\exists^*\\forall^*$SL as a specialized solver inside a DPLL($T$) architecture,\nwithin the CVC4 SMT solver. The evaluation of our implementation was carried\nout using two sets of verification conditions, produced by (i) unfolding\ninductive predicates, and (ii) a weakest precondition-based verification\ncondition generator. Experimental data shows that automated quantifier\ninstantiation has little overhead, compared to manual model-based\ninstantiation. \n\n"}
{"id": "1610.06309", "contents": "Title: Non-Asymptotic Delay Bounds for Multi-Server Systems with\n  Synchronization Constraints Abstract: Multi-server systems have received increasing attention with important\nimplementations such as Google MapReduce, Hadoop, and Spark. Common to these\nsystems are a fork operation, where jobs are first divided into tasks that are\nprocessed in parallel, and a later join operation, where completed tasks wait\nuntil the results of all tasks of a job can be combined and the job leaves the\nsystem. The synchronization constraint of the join operation makes the analysis\nof fork-join systems challenging and few explicit results are known. In this\nwork, we model fork-join systems using a max-plus server model that enables us\nto derive statistical bounds on waiting and sojourn times for general arrival\nand service time processes. We contribute end-to-end delay bounds for\nmulti-stage fork-join networks that grow in $\\mathcal{O}(h \\ln k)$ for $h$\nfork-join stages, each with $k$ parallel servers. We perform a detailed\ncomparison of different multi-server configurations and highlight their pros\nand cons. We also include an analysis of single-queue fork-join systems that\nare non-idling and achieve a fundamental performance gain, and compare these\nresults to both simulation and a live Spark system. \n\n"}
{"id": "1610.07183", "contents": "Title: How to be Fair and Diverse? Abstract: Due to the recent cases of algorithmic bias in data-driven decision-making,\nmachine learning methods are being put under the microscope in order to\nunderstand the root cause of these biases and how to correct them. Here, we\nconsider a basic algorithmic task that is central in machine learning:\nsubsampling from a large data set. Subsamples are used both as an end-goal in\ndata summarization (where fairness could either be a legal, political or moral\nrequirement) and to train algorithms (where biases in the samples are often a\nsource of bias in the resulting model). Consequently, there is a growing effort\nto modify either the subsampling methods or the algorithms themselves in order\nto ensure fairness. However, in doing so, a question that seems to be\noverlooked is whether it is possible to produce fair subsamples that are also\nadequately representative of the feature space of the data set - an important\nand classic requirement in machine learning. Can diversity and fairness be\nsimultaneously ensured? We start by noting that, in some applications,\nguaranteeing one does not necessarily guarantee the other, and a new approach\nis required. Subsequently, we present an algorithmic framework which allows us\nto produce both fair and diverse samples. Our experimental results on an image\nsummarization task show marked improvements in fairness without compromising\nfeature diversity by much, giving us the best of both the worlds. \n\n"}
{"id": "1610.07236", "contents": "Title: Hybrid Static/Dynamic Schedules for Tiled Polyhedral Programs Abstract: Polyhedral compilers perform optimizations such as tiling and\nparallelization; when doing both, they usually generate code that executes\n\"barrier-synchronized wavefronts\" of tiles. We present a system to express and\ngenerate code for hybrid schedules, where some constraints are automatically\nsatisfied through the structure of the code, and the remainder are dynamically\nenforced at run-time with data flow mechanisms. We prove bounds on the added\noverheads that are better, by at least one polynomial degree, than those of\nprevious techniques.\n  We propose a generic mechanism to implement the needed synchronization, and\nshow it can be easily realized for a variety of targets: OpenMP, Pthreads, GPU\n(CUDA or OpenCL) code, languages like X10, Habanero, Cilk, as well as data flow\nplatforms like DAGuE, and OpenStream and MPI. We also provide a simple concrete\nimplementation that works without the need of any sophisticated run-time\nmechanism.\n  Our experiments show our simple implementation to be competitive or better\nthan the wavefront-synchronized code generated by other systems. We also show\nhow the proposed mechanism can achieve 24% to 70% reduction in energy. \n\n"}
{"id": "1610.08077", "contents": "Title: A statistical framework for fair predictive algorithms Abstract: Predictive modeling is increasingly being employed to assist human\ndecision-makers. One purported advantage of replacing human judgment with\ncomputer models in high stakes settings-- such as sentencing, hiring, policing,\ncollege admissions, and parole decisions-- is the perceived \"neutrality\" of\ncomputers. It is argued that because computer models do not hold personal\nprejudice, the predictions they produce will be equally free from prejudice.\nThere is growing recognition that employing algorithms does not remove the\npotential for bias, and can even amplify it, since training data were\ninevitably generated by a process that is itself biased. In this paper, we\nprovide a probabilistic definition of algorithmic bias. We propose a method to\nremove bias from predictive models by removing all information regarding\nprotected variables from the permitted training data. Unlike previous work in\nthis area, our framework is general enough to accommodate arbitrary data types,\ne.g. binary, continuous, etc. Motivated by models currently in use in the\ncriminal justice system that inform decisions on pre-trial release and\nparoling, we apply our proposed method to a dataset on the criminal histories\nof individuals at the time of sentencing to produce \"race-neutral\" predictions\nof re-arrest. In the process, we demonstrate that the most common approach to\ncreating \"race-neutral\" models-- omitting race as a covariate-- still results\nin racially disparate predictions. We then demonstrate that the application of\nour proposed method to these data removes racial disparities from predictions\nwith minimal impact on predictive accuracy. \n\n"}
{"id": "1611.01137", "contents": "Title: A Memory Bandwidth-Efficient Hybrid Radix Sort on GPUs Abstract: Sorting is at the core of many database operations, such as index creation,\nsort-merge joins, and user-requested output sorting. As GPUs are emerging as a\npromising platform to accelerate various operations, sorting on GPUs becomes a\nviable endeavour. Over the past few years, several improvements have been\nproposed for sorting on GPUs, leading to the first radix sort implementations\nthat achieve a sorting rate of over one billion 32-bit keys per second. Yet,\nstate-of-the-art approaches are heavily memory bandwidth-bound, as they require\nsubstantially more memory transfers than their CPU-based counterparts.\n  Our work proposes a novel approach that almost halves the amount of memory\ntransfers and, therefore, considerably lifts the memory bandwidth limitation.\nBeing able to sort two gigabytes of eight-byte records in as little as 50\nmilliseconds, our approach achieves a 2.32-fold improvement over the\nstate-of-the-art GPU-based radix sort for uniform distributions, sustaining a\nminimum speed-up of no less than a factor of 1.66 for skewed distributions.\n  To address inputs that either do not reside on the GPU or exceed the\navailable device memory, we build on our efficient GPU sorting approach with a\npipelined heterogeneous sorting algorithm that mitigates the overhead\nassociated with PCIe data transfers. Comparing the end-to-end sorting\nperformance to the state-of-the-art CPU-based radix sort running 16 threads,\nour heterogeneous approach achieves a 2.06-fold and a 1.53-fold improvement for\nsorting 64 GB key-value pairs with a skewed and a uniform distribution,\nrespectively. \n\n"}
{"id": "1611.01236", "contents": "Title: Adversarial Machine Learning at Scale Abstract: Adversarial examples are malicious inputs designed to fool machine learning\nmodels. They often transfer from one model to another, allowing attackers to\nmount black box attacks without knowledge of the target model's parameters.\nAdversarial training is the process of explicitly training a model on\nadversarial examples, in order to make it more robust to attack or to reduce\nits test error on clean inputs. So far, adversarial training has primarily been\napplied to small problems. In this research, we apply adversarial training to\nImageNet. Our contributions include: (1) recommendations for how to succesfully\nscale adversarial training to large models and datasets, (2) the observation\nthat adversarial training confers robustness to single-step attack methods, (3)\nthe finding that multi-step attack methods are somewhat less transferable than\nsingle-step attack methods, so single-step attacks are the best for mounting\nblack-box attacks, and (4) resolution of a \"label leaking\" effect that causes\nadversarially trained models to perform better on adversarial examples than on\nclean examples, because the adversarial example construction process uses the\ntrue label and the model can learn to exploit regularities in the construction\nprocess. \n\n"}
{"id": "1611.02053", "contents": "Title: Reinforcement-based Simultaneous Algorithm and its Hyperparameters\n  Selection Abstract: Many algorithms for data analysis exist, especially for classification\nproblems. To solve a data analysis problem, a proper algorithm should be\nchosen, and also its hyperparameters should be selected. In this paper, we\npresent a new method for the simultaneous selection of an algorithm and its\nhyperparameters. In order to do so, we reduced this problem to the multi-armed\nbandit problem. We consider an algorithm as an arm and algorithm\nhyperparameters search during a fixed time as the corresponding arm play. We\nalso suggest a problem-specific reward function. We performed the experiments\non 10 real datasets and compare the suggested method with the existing one\nimplemented in Auto-WEKA. The results show that our method is significantly\nbetter in most of the cases and never worse than the Auto-WEKA. \n\n"}
{"id": "1611.02108", "contents": "Title: Cubical Type Theory: a constructive interpretation of the univalence\n  axiom Abstract: This paper presents a type theory in which it is possible to directly\nmanipulate $n$-dimensional cubes (points, lines, squares, cubes, etc.) based on\nan interpretation of dependent type theory in a cubical set model. This enables\nnew ways to reason about identity types, for instance, function extensionality\nis directly provable in the system. Further, Voevodsky's univalence axiom is\nprovable in this system. We also explain an extension with some higher\ninductive types like the circle and propositional truncation. Finally we\nprovide semantics for this cubical type theory in a constructive meta-theory. \n\n"}
{"id": "1611.04535", "contents": "Title: Learning-Theoretic Foundations of Algorithm Configuration for\n  Combinatorial Partitioning Problems Abstract: Max-cut, clustering, and many other partitioning problems that are of\nsignificant importance to machine learning and other scientific fields are\nNP-hard, a reality that has motivated researchers to develop a wealth of\napproximation algorithms and heuristics. Although the best algorithm to use\ntypically depends on the specific application domain, a worst-case analysis is\noften used to compare algorithms. This may be misleading if worst-case\ninstances occur infrequently, and thus there is a demand for optimization\nmethods which return the algorithm configuration best suited for the given\napplication's typical inputs. We address this problem for clustering, max-cut,\nand other partitioning problems, such as integer quadratic programming, by\ndesigning computationally efficient and sample efficient learning algorithms\nwhich receive samples from an application-specific distribution over problem\ninstances and learn a partitioning algorithm with high expected performance.\nOur algorithms learn over common integer quadratic programming and clustering\nalgorithm families: SDP rounding algorithms and agglomerative clustering\nalgorithms with dynamic programming. For our sample complexity analysis, we\nprovide tight bounds on the pseudodimension of these algorithm classes, and\nshow that surprisingly, even for classes of algorithms parameterized by a\nsingle parameter, the pseudo-dimension is superconstant. In this way, our work\nboth contributes to the foundations of algorithm configuration and pushes the\nboundaries of learning theory, since the algorithm classes we analyze consist\nof multi-stage optimization procedures and are significantly more complex than\nclasses typically studied in learning theory. \n\n"}
{"id": "1611.05546", "contents": "Title: Zero-Shot Visual Question Answering Abstract: Part of the appeal of Visual Question Answering (VQA) is its promise to\nanswer new questions about previously unseen images. Most current methods\ndemand training questions that illustrate every possible concept, and will\ntherefore never achieve this capability, since the volume of required training\ndata would be prohibitive. Answering general questions about images requires\nmethods capable of Zero-Shot VQA, that is, methods able to answer questions\nbeyond the scope of the training questions. We propose a new evaluation\nprotocol for VQA methods which measures their ability to perform Zero-Shot VQA,\nand in doing so highlights significant practical deficiencies of current\napproaches, some of which are masked by the biases in current datasets. We\npropose and evaluate several strategies for achieving Zero-Shot VQA, including\nmethods based on pretrained word embeddings, object classifiers with semantic\nembeddings, and test-time retrieval of example images. Our extensive\nexperiments are intended to serve as baselines for Zero-Shot VQA, and they also\nachieve state-of-the-art performance in the standard VQA evaluation setting. \n\n"}
{"id": "1611.06213", "contents": "Title: GaDei: On Scale-up Training As A Service For Deep Learning Abstract: Deep learning (DL) training-as-a-service (TaaS) is an important emerging\nindustrial workload. The unique challenge of TaaS is that it must satisfy a\nwide range of customers who have no experience and resources to tune DL\nhyper-parameters, and meticulous tuning for each user's dataset is\nprohibitively expensive. Therefore, TaaS hyper-parameters must be fixed with\nvalues that are applicable to all users. IBM Watson Natural Language Classifier\n(NLC) service, the most popular IBM cognitive service used by thousands of\nenterprise-level clients around the globe, is a typical TaaS service. By\nevaluating the NLC workloads, we show that only the conservative\nhyper-parameter setup (e.g., small mini-batch size and small learning rate) can\nguarantee acceptable model accuracy for a wide range of customers. We further\njustify theoretically why such a setup guarantees better model convergence in\ngeneral. Unfortunately, the small mini-batch size causes a high volume of\ncommunication traffic in a parameter-server based system. We characterize the\nhigh communication bandwidth requirement of TaaS using representative\nindustrial deep learning workloads and demonstrate that none of the\nstate-of-the-art scale-up or scale-out solutions can satisfy such a\nrequirement. We then present GaDei, an optimized shared-memory based scale-up\nparameter server design. We prove that the designed protocol is deadlock-free\nand it processes each gradient exactly once. Our implementation is evaluated on\nboth commercial benchmarks and public benchmarks to demonstrate that it\nsignificantly outperforms the state-of-the-art parameter-server based\nimplementation while maintaining the required accuracy and our implementation\nreaches near the best possible runtime performance, constrained only by the\nhardware limitation. Furthermore, to the best of our knowledge, GaDei is the\nonly scale-up DL system that provides fault-tolerance. \n\n"}
{"id": "1611.07238", "contents": "Title: Time and Space Optimal Counting in Population Protocols Abstract: This work concerns the general issue of combined optimality in terms of time\nand space complexity. In this context, we study the problem of (exact) counting\nresource-limited and passively mobile nodes in the model of population\nprotocols, in which the space complexity is crucial. The counted nodes are\nmemory-limited anonymous devices (called agents) communicating asynchronously\nin pairs (according to a fairness condition). Moreover, we assume that these\nagents are prone to failures so that they cannot be correctly initialized. This\nstudy considers two classical fairness conditions, and for each we investigate\nthe issue of time optimality of counting given the optimal space per agent. In\nthe case of randomly interacting agents (probabilistic fairness), as usual, the\nconvergence time is measured in terms of parallel time (or parallel\ninteractions), which is defined as the number of pairwise interactions until\nconvergence, divided by n (the number of agents). In case of weak fairness,\nwhere it is only required that every pair of agents interacts infinitely often,\nthe convergence time is defined in terms of non-null transitions, i.e, the\ntransitions that affect the states of the interacting agents.First, assuming\nprobabilistic fairness, we present a \"non-guessing\" time optimal protocol of\nO(n log n) expected time given an optimal space of only one bit, and we prove\nthe time optimality of this protocol. Then, for weak fairness, we show that a\nspace optimal (semi-uniform) solution cannot converge faster than in\n$\\Omega$(2^n) time (non-null transitions). This result, together with the time\ncomplexity analysis of an already known space optimal protocol, shows that it\nis also optimal in time (given the optimal space constrains). \n\n"}
{"id": "1611.07567", "contents": "Title: Feature Importance Measure for Non-linear Learning Algorithms Abstract: Complex problems may require sophisticated, non-linear learning methods such\nas kernel machines or deep neural networks to achieve state of the art\nprediction accuracies. However, high prediction accuracies are not the only\nobjective to consider when solving problems using machine learning. Instead,\nparticular scientific applications require some explanation of the learned\nprediction function. Unfortunately, most methods do not come with out of the\nbox straight forward interpretation. Even linear prediction functions are not\nstraight forward to explain if features exhibit complex correlation structure.\n  In this paper, we propose the Measure of Feature Importance (MFI). MFI is\ngeneral and can be applied to any arbitrary learning machine (including kernel\nmachines and deep learning). MFI is intrinsically non-linear and can detect\nfeatures that by itself are inconspicuous and only impact the prediction\nfunction through their interaction with other features. Lastly, MFI can be used\nfor both --- model-based feature importance and instance-based feature\nimportance (i.e, measuring the importance of a feature for a particular data\npoint). \n\n"}
{"id": "1611.09434", "contents": "Title: Input Switched Affine Networks: An RNN Architecture Designed for\n  Interpretability Abstract: There exist many problem domains where the interpretability of neural network\nmodels is essential for deployment. Here we introduce a recurrent architecture\ncomposed of input-switched affine transformations - in other words an RNN\nwithout any explicit nonlinearities, but with input-dependent recurrent\nweights. This simple form allows the RNN to be analyzed via straightforward\nlinear methods: we can exactly characterize the linear contribution of each\ninput to the model predictions; we can use a change-of-basis to disentangle\ninput, output, and computational hidden unit subspaces; we can fully\nreverse-engineer the architecture's solution to a simple task. Despite this\nease of interpretation, the input switched affine network achieves reasonable\nperformance on a text modeling tasks, and allows greater computational\nefficiency than networks with standard nonlinearities. \n\n"}
{"id": "1611.09528", "contents": "Title: Flexible Scheduling of Distributed Analytic Applications Abstract: This work addresses the problem of scheduling user-defined analytic\napplications, which we define as high-level compositions of frameworks, their\ncomponents, and the logic necessary to carry out work. The key idea in our\napplication definition, is to distinguish classes of components, including\nrigid and elastic types: the first being required for an application to make\nprogress, the latter contributing to reduced execution times. We show that the\nproblem of scheduling such applications poses new challenges, which existing\napproaches address inefficiently.\n  Thus, we present the design and evaluation of a novel, flexible heuristic to\nschedule analytic applications, that aims at high system responsiveness, by\nallocating resources efficiently. Our algorithm is evaluated using trace-driven\nsimulations, with large-scale real system traces: our flexible scheduler\noutperforms a baseline approach across a variety of metrics, including\napplication turnaround times, and resource allocation efficiency.\n  We also present the design and evaluation of a full-fledged system, which we\nhave called Zoe, that incorporates the ideas presented in this paper, and\nreport concrete improvements in terms of efficiency and performance, with\nrespect to prior generations of our system. \n\n"}
{"id": "1611.09830", "contents": "Title: NewsQA: A Machine Comprehension Dataset Abstract: We present NewsQA, a challenging machine comprehension dataset of over\n100,000 human-generated question-answer pairs. Crowdworkers supply questions\nand answers based on a set of over 10,000 news articles from CNN, with answers\nconsisting of spans of text from the corresponding articles. We collect this\ndataset through a four-stage process designed to solicit exploratory questions\nthat require reasoning. A thorough analysis confirms that NewsQA demands\nabilities beyond simple word matching and recognizing textual entailment. We\nmeasure human performance on the dataset and compare it to several strong\nneural models. The performance gap between humans and machines (0.198 in F1)\nindicates that significant progress can be made on NewsQA through future\nresearch. The dataset is freely available at\nhttps://datasets.maluuba.com/NewsQA. \n\n"}
{"id": "1612.01086", "contents": "Title: Deep Learning of Robotic Tasks without a Simulator using Strong and Weak\n  Human Supervision Abstract: We propose a scheme for training a computerized agent to perform complex\nhuman tasks such as highway steering. The scheme is designed to follow a\nnatural learning process whereby a human instructor teaches a computerized\ntrainee. The learning process consists of five elements: (i) unsupervised\nfeature learning; (ii) supervised imitation learning; (iii) supervised reward\ninduction; (iv) supervised safety module construction; and (v) reinforcement\nlearning. We implemented the last four elements of the scheme using deep\nconvolutional networks and applied it to successfully create a computerized\nagent capable of autonomous highway steering over the well-known racing game\nAssetto Corsa. We demonstrate that the use of the last four elements is\nessential to effectively carry out the steering task using vision alone,\nwithout access to a driving simulator internals, and operating in wall-clock\ntime. This is made possible also through the introduction of a safety network,\na novel way for preventing the agent from performing catastrophic mistakes\nduring the reinforcement learning stage. \n\n"}
{"id": "1612.01091", "contents": "Title: A new rule for almost-certain termination of probabilistic- and demonic\n  programs Abstract: Extending our own and others' earlier approaches to reasoning about\ntermination of probabilistic programs, we propose and prove a new rule for\ntermination with probability one, also known as \"almost-certain termination\".\nThe rule uses both (non-strict) super martingales and guarantees of progress,\ntogether, and it seems to cover significant cases that earlier methods do not.\nIn particular, it suffices for termination of the unbounded symmetric random\nwalk in both one- and two dimensions: for the first, we give a proof; for the\nsecond, we use a theorem of Foster to argue that a proof exists.\nNon-determinism (i.e. demonic choice) is supported; but we do currently\nrestrict to discrete distributions. \n\n"}
{"id": "1612.02916", "contents": "Title: Solida: A Blockchain Protocol Based on Reconfigurable Byzantine\n  Consensus Abstract: The decentralized cryptocurrency Bitcoin has experienced great success but\nalso encountered many challenges. One of the challenges has been the long\nconfirmation time. Another challenge is the lack of incentives at certain steps\nof the protocol, raising concerns for transaction withholding, selfish mining,\netc. To address these challenges, we propose Solida, a decentralized blockchain\nprotocol based on reconfigurable Byzantine consensus augmented by\nproof-of-work. Solida improves on Bitcoin in confirmation time, and provides\nsafety and liveness assuming the adversary control less than (roughly)\none-third of the total mining power. \n\n"}
{"id": "1612.03471", "contents": "Title: Reinforcement Learning With Temporal Logic Rewards Abstract: Reinforcement learning (RL) depends critically on the choice of reward\nfunctions used to capture the de- sired behavior and constraints of a robot.\nUsually, these are handcrafted by a expert designer and represent heuristics\nfor relatively simple tasks. Real world applications typically involve more\ncomplex tasks with rich temporal and logical structure. In this paper we take\nadvantage of the expressive power of temporal logic (TL) to specify complex\nrules the robot should follow, and incorporate domain knowledge into learning.\nWe propose Truncated Linear Temporal Logic (TLTL) as specifications language,\nthat is arguably well suited for the robotics applications, together with\nquantitative semantics, i.e., robustness degree. We propose a RL approach to\nlearn tasks expressed as TLTL formulae that uses their associated robustness\ndegree as reward functions, instead of the manually crafted heuristics trying\nto capture the same specifications. We show in simulated trials that learning\nis faster and policies obtained using the proposed approach outperform the ones\nlearned using heuristic rewards in terms of the robustness degree, i.e., how\nwell the tasks are satisfied. Furthermore, we demonstrate the proposed RL\napproach in a toast-placing task learned by a Baxter robot. \n\n"}
{"id": "1612.04936", "contents": "Title: Learning through Dialogue Interactions by Asking Questions Abstract: A good dialogue agent should have the ability to interact with users by both\nresponding to questions and by asking questions, and importantly to learn from\nboth types of interaction. In this work, we explore this direction by designing\na simulator and a set of synthetic tasks in the movie domain that allow such\ninteractions between a learner and a teacher. We investigate how a learner can\nbenefit from asking questions in both offline and online reinforcement learning\nsettings, and demonstrate that the learner improves when asking questions.\nFinally, real experiments with Mechanical Turk validate the approach. Our work\nrepresents a first step in developing such end-to-end learned interactive\ndialogue agents. \n\n"}
{"id": "1612.06038", "contents": "Title: Context and Interference Effects in the Combinations of Natural Concepts Abstract: The mathematical formalism of quantum theory exhibits significant\neffectiveness when applied to cognitive phenomena that have resisted\ntraditional (set theoretical) modeling. Relying on a decade of research on the\noperational foundations of micro-physical and conceptual entities, we present a\ntheoretical framework for the representation of concepts and their conjunctions\nand disjunctions that uses the quantum formalism. This framework provides a\nunified solution to the 'conceptual combinations problem' of cognitive\npsychology, explaining the observed deviations from classical (Boolean, fuzzy\nset and Kolmogorovian) structures in terms of genuine quantum effects. In\nparticular, natural concepts 'interfere' when they combine to form more complex\nconceptual entities, and they also exhibit a 'quantum-type context-dependence',\nwhich are responsible of the 'over- and under-extension' that are\nsystematically observed in experiments on membership judgments. \n\n"}
{"id": "1612.06043", "contents": "Title: An Empirical Study of Adequate Vision Span for Attention-Based Neural\n  Machine Translation Abstract: Recently, the attention mechanism plays a key role to achieve high\nperformance for Neural Machine Translation models. However, as it computes a\nscore function for the encoder states in all positions at each decoding step,\nthe attention model greatly increases the computational complexity. In this\npaper, we investigate the adequate vision span of attention models in the\ncontext of machine translation, by proposing a novel attention framework that\nis capable of reducing redundant score computation dynamically. The term\n\"vision span\" means a window of the encoder states considered by the attention\nmodel in one step. In our experiments, we found that the average window size of\nvision span can be reduced by over 50% with modest loss in accuracy on\nEnglish-Japanese and German-English translation tasks.% This results indicate\nthat the conventional attention mechanism performs a significant amount of\nredundant computation. \n\n"}
{"id": "1612.06302", "contents": "Title: Hybrid Transactional Replication: State-Machine and Deferred-Update\n  Replication Combined Abstract: We propose Hybrid Transactional Replication (HTR), a novel replication scheme\nfor highly dependable services. It combines two schemes: a transaction is\nexecuted either optimistically by only one service replica in the deferred\nupdate mode (DU), or deterministically by all replicas in the state machine\nmode (SM); the choice is made by an oracle. The DU mode allows for parallelism\nand thus takes advantage of multicore hardware. In contrast to DU, the SM mode\nguarantees abort-free execution, so it is suitable for irrevocable operations\nand transactions generating high contention. For expressiveness, transactions\ncan be discarded or retried on demand. We formally prove that the higher\nflexibility of the scheme does not come at the cost of weaker guarantees for\nclients: HTR satisfies strong consistency guarantees akin to those provided by\nother popular transactional replication schemes such as Deferred Update\nReplication. We developed HTR-enabled Paxos STM, an object-based distributed\ntransactional memory system, and evaluated it thoroughly under various\nworkloads. We show the benefits of using a novel oracle, which relies on\nmachine learning techniques for automatic adaptation to changing conditions. In\nour tests, the ML-based oracle provides up to 50% improvement in throughput\nwhen compared to the system running with DU-only or SM-only oracles. Our\napproach is inspired by a well known algorithm used in the context of the\nmulti-armed bandit problem. \n\n"}
{"id": "1612.06340", "contents": "Title: Computing Human-Understandable Strategies Abstract: Algorithms for equilibrium computation generally make no attempt to ensure\nthat the computed strategies are understandable by humans. For instance the\nstrategies for the strongest poker agents are represented as massive binary\nfiles. In many situations, we would like to compute strategies that can\nactually be implemented by humans, who may have computational limitations and\nmay only be able to remember a small number of features or components of the\nstrategies that have been computed. We study poker games where private\ninformation distributions can be arbitrary. We create a large training set of\ngame instances and solutions, by randomly selecting the information\nprobabilities, and present algorithms that learn from the training instances in\norder to perform well in games with unseen information distributions. We are\nable to conclude several new fundamental rules about poker strategy that can be\neasily implemented by humans. \n\n"}
{"id": "1612.06476", "contents": "Title: Computational Complexity of Testing Proportional Justified\n  Representation Abstract: We consider a committee voting setting in which each voter approves of a\nsubset of candidates and based on the approvals, a target number of candidates\nare selected. Aziz et al. (2015) proposed two representation axioms called\njustified representation and extended justified representation. Whereas the\nformer can be tested as well as achieved in polynomial time, the latter\nproperty is coNP-complete to test and no polynomial-time algorithm is known to\nachieve it. Interestingly, S{\\'a}nchez-Fern{\\'a}ndez et~al. (2016) proposed an\nintermediate property called proportional justified representation that admits\na polynomial-time algorithm to achieve. The complexity of testing proportional\njustified representation has remained an open problem. In this paper, we settle\nthe complexity by proving that testing proportional justified representation is\ncoNP-complete. We complement the complexity result by showing that the problem\nadmits efficient algorithms if any of the following parameters are bounded: (1)\nnumber of voters (2) number of candidates (3) maximum number of candidates\napproved by a voter (4) maximum number of voters approving a given candidate. \n\n"}
{"id": "1612.06704", "contents": "Title: Action-Driven Object Detection with Top-Down Visual Attentions Abstract: A dominant paradigm for deep learning based object detection relies on a\n\"bottom-up\" approach using \"passive\" scoring of class agnostic proposals. These\napproaches are efficient but lack of holistic analysis of scene-level context.\nIn this paper, we present an \"action-driven\" detection mechanism using our\n\"top-down\" visual attention model. We localize an object by taking sequential\nactions that the attention model provides. The attention model conditioned with\nan image region provides required actions to get closer toward a target object.\nAn action at each time step is weak itself but an ensemble of the sequential\nactions makes a bounding-box accurately converge to a target object boundary.\nThis attention model we call AttentionNet is composed of a convolutional neural\nnetwork. During our whole detection procedure, we only utilize the actions from\na single AttentionNet without any modules for object proposals nor post\nbounding-box regression. We evaluate our top-down detection mechanism over the\nPASCAL VOC series and ILSVRC CLS-LOC dataset, and achieve state-of-the-art\nperformances compared to the major bottom-up detection methods. In particular,\nour detection mechanism shows a strong advantage in elaborate localization by\noutperforming Faster R-CNN with a margin of +7.1% over PASCAL VOC 2007 when we\nincrease the IoU threshold for positive detection to 0.7. \n\n"}
{"id": "1612.07195", "contents": "Title: Certifying Confluence Proofs via Relative Termination and Rule Labeling Abstract: The rule labeling heuristic aims to establish confluence of (left-)linear\nterm rewrite systems via decreasing diagrams. We present a formalization of a\nconfluence criterion based on the interplay of relative termination and the\nrule labeling in the theorem prover Isabelle. Moreover, we report on the\nintegration of this result into the certifier CeTA, facilitating the checking\nof confluence certificates based on decreasing diagrams. The power of the\nmethod is illustrated by an experimental evaluation on a (standard) collection\nof confluence problems. \n\n"}
{"id": "1612.07551", "contents": "Title: Equations in oligomorphic clones and the Constraint Satisfaction Problem\n  for $\\omega$-categorical structures Abstract: There exist two conjectures for constraint satisfaction problems (CSPs) of\nreducts of finitely bounded homogeneous structures: the first one states that\ntractability of the CSP of such a structure is, when the structure is a\nmodel-complete core, equivalent to its polymorphism clone satisfying a certain\nnon-trivial linear identity modulo outer embeddings. The second conjecture,\nchallenging the approach via model-complete cores by reflections, states that\ntractability is equivalent to the linear identities (without outer embeddings)\nsatisfied by its polymorphisms clone, together with the natural uniformity on\nit, being non-trivial.\n  We prove that the identities satisfied in the polymorphism clone of a\nstructure allow for conclusions about the orbit growth of its automorphism\ngroup, and apply this to show that the two conjectures are equivalent. We\ncontrast this with a counterexample showing that $\\omega$-categoricity alone is\ninsufficient to imply the equivalence of the two conditions above in a\nmodel-complete core.\n  Taking a different approach, we then show how the Ramsey property of a\nhomogeneous structure can be utilized for obtaining a similar equivalence under\ndifferent conditions.\n  We then prove that any polymorphism of sufficiently large arity which is\ntotally symmetric modulo outer embeddings of a finitely bounded structure can\nbe turned into a non-trivial system of linear identities, and obtain\nnon-trivial linear identities for all tractable cases of reducts of the\nrational order, the random graph, and the random poset.\n  Finally, we provide a new and short proof, in the language of monoids, of the\ntheorem stating that every $\\omega$-categorical structure is homomorphically\nequivalent to a model-complete core. \n\n"}
{"id": "1612.07919", "contents": "Title: EnhanceNet: Single Image Super-Resolution Through Automated Texture\n  Synthesis Abstract: Single image super-resolution is the task of inferring a high-resolution\nimage from a single low-resolution input. Traditionally, the performance of\nalgorithms for this task is measured using pixel-wise reconstruction measures\nsuch as peak signal-to-noise ratio (PSNR) which have been shown to correlate\npoorly with the human perception of image quality. As a result, algorithms\nminimizing these metrics tend to produce over-smoothed images that lack\nhigh-frequency textures and do not look natural despite yielding high PSNR\nvalues.\n  We propose a novel application of automated texture synthesis in combination\nwith a perceptual loss focusing on creating realistic textures rather than\noptimizing for a pixel-accurate reproduction of ground truth images during\ntraining. By using feed-forward fully convolutional neural networks in an\nadversarial training setting, we achieve a significant boost in image quality\nat high magnification ratios. Extensive experiments on a number of datasets\nshow the effectiveness of our approach, yielding state-of-the-art results in\nboth quantitative and qualitative benchmarks. \n\n"}
{"id": "1612.08843", "contents": "Title: FastMask: Segment Multi-scale Object Candidates in One Shot Abstract: Objects appear to scale differently in natural images. This fact requires\nmethods dealing with object-centric tasks (e.g. object proposal) to have robust\nperformance over variances in object scales. In the paper, we present a novel\nsegment proposal framework, namely FastMask, which takes advantage of\nhierarchical features in deep convolutional neural networks to segment\nmulti-scale objects in one shot. Innovatively, we adapt segment proposal\nnetwork into three different functional components (body, neck and head). We\nfurther propose a weight-shared residual neck module as well as a\nscale-tolerant attentional head module for efficient one-shot inference. On MS\nCOCO benchmark, the proposed FastMask outperforms all state-of-the-art segment\nproposal methods in average recall being 2~5 times faster. Moreover, with a\nslight trade-off in accuracy, FastMask can segment objects in near real time\n(~13 fps) with 800*600 resolution images, demonstrating its potential in\npractical applications. Our implementation is available on\nhttps://github.com/voidrank/FastMask. \n\n"}
{"id": "1612.09029", "contents": "Title: Distributed Convex Optimization with Inequality Constraints over\n  Time-varying Unbalanced Digraphs Abstract: This paper considers a distributed convex optimization problem with\ninequality constraints over time-varying unbalanced digraphs, where the cost\nfunction is a sum of local objectives, and each node of the graph only knows\nits local objective and inequality constraints. Although there is a vast\nliterature on distributed optimization, most of them require the graph to be\nbalanced, which is quite restrictive and not necessary. Very recently, the\nunbalanced problem has been resolved only for either time-invariant graphs or\nunconstrained optimization. This work addresses the unbalancedness by focusing\non an epigraph form of the constrained optimization. A striking feature is that\nthis novel idea can be easily used to study time-varying unbalanced digraphs.\nUnder local communications, a simple iterative algorithm is then designed for\neach node. We prove that if the graph is uniformly jointly strongly connected,\neach node asymptotically converges to some common optimal solution. \n\n"}
{"id": "1701.00503", "contents": "Title: Distributed Graph Layout for Scalable Small-world Network Analysis Abstract: The in-memory graph layout or organization has a considerable impact on the\ntime and energy efficiency of distributed memory graph computations. It affects\nmemory locality, inter-task load balance, communication time, and overall\nmemory utilization. Graph layout could refer to partitioning or replication of\nvertex and edge arrays, selective replication of data structures that hold\nmeta-data, and reordering vertex and edge identifiers. In this work, we present\nDGL, a fast, parallel, and memory-efficient distributed graph layout strategy\nthat is specifically designed for small-world networks (low-diameter graphs\nwith skewed vertex degree distributions). Label propagation-based partitioning\nand a scalable BFS-based ordering are the main steps in the layout strategy. We\nshow that the DGL layout can significantly improve end-to-end performance of\nfive challenging graph analytics workloads: PageRank, a parallel subgraph\nenumeration program, tuned implementations of breadth-first search and\nsingle-source shortest paths, and RDF3X-MPI, a distributed SPARQL query\nprocessing engine. Using these benchmarks, we additionally offer a\ncomprehensive analysis on how graph layout affects the performance of graph\nanalytics with variable computation and communication characteristics. \n\n"}
{"id": "1701.00656", "contents": "Title: On the Cohomology of Contextuality Abstract: Recent work by Abramsky and Brandenburger used sheaf theory to give a\nmathematical formulation of non-locality and contextuality. By adopting this\nviewpoint, it has been possible to define cohomological obstructions to the\nexistence of global sections. In the present work, we illustrate new insights\ninto different aspects of this theory. We shed light on the power of detection\nof the cohomological obstruction by showing that it is not a complete invariant\nfor strong contextuality even under symmetry and connectedness restrictions on\nthe measurement cover, disproving a previous conjecture. We generalise\nobstructions to higher cohomology groups and show that they give rise to a\nrefinement of the notion of cohomological contextuality: different \"levels\" of\ncontextuality are organised in a hierarchy of logical implications. Finally, we\npresent an alternative description of the first cohomology group in terms of\ntorsors, resulting in a new interpretation of the cohomological obstructions. \n\n"}
{"id": "1701.01539", "contents": "Title: Algorithms for Optimal Replica Placement Under Correlated Failure in\n  Hierarchical Failure Domains Abstract: In data centers, data replication is the primary method used to ensure\navailability of customer data. To avoid correlated failure, cloud storage\ninfrastructure providers model hierarchical failure domains using a tree, and\navoid placing a large number of data replicas within the same failure domain\n(i.e. on the same branch of the tree). Typical best practices ensure that\nreplicas are distributed across failure domains, but relatively little is known\nconcerning optimization algorithms for distributing data replicas. Using a\nhierarchical model, we answer how to distribute replicas across failure domains\noptimally. We formulate a novel optimization problem for replica placement in\ndata centers. As part of our problem, we formalize and explain a new criterion\nfor optimizing a replica placement. Our overall goal is to choose placements in\nwhich correlated failures disable as few replicas as possible. We provide two\noptimization algorithms for dependency models represented by trees. We first\npresent an $O(n + \\rho \\log \\rho)$ time dynamic programming algorithm for\nplacing $\\rho$ replicas of a single file on the leaves (representing servers)\nof a tree with $n$ vertices. We next consider the problem of placing replicas\nof $m$ blocks of data, where each block may have different replication factors.\nFor this problem, we give an exact algorithm which runs in polynomial time when\nthe skew, the difference in the number of replicas between the largest and\nsmallest blocks of data, is constant. \n\n"}
{"id": "1701.02648", "contents": "Title: Why Can't You Behave? Non-termination Analysis of Direct Recursive Rules\n  with Constraints Abstract: This paper is concerned with rule-based programs that go wrong. The unwanted\nbehavior of rule applications is non-termination or failure of a computation.\nWe propose a static program analysis of the non-termination problem for\nrecursion in the Constraint Handling Rules (CHR) language.\n  CHR is an advanced concurrent declarative language involving constraint\nreasoning. It has been closely related to many other rule-based approaches, so\nthe results are of a more general interest. In such languages, non-termination\nis due to infinite applications of recursive rules. Failure is due to\naccumulation of contradicting constraints during the computation.\n  We give theorems with so-called misbehavior conditions for potential\nnon-termination and failure (as well as definite termination) of linear direct\nrecursive simplification rules. Logical relationships between the constraints\nin a recursive rule play a crucial role in this kind of program analysis. We\nthink that our approach can be extended to other types of recursion and to a\nmore general class of rules. Therefore this paper can serve as a basic\nreference and a starting point for further research. \n\n"}
{"id": "1701.02810", "contents": "Title: OpenNMT: Open-Source Toolkit for Neural Machine Translation Abstract: We describe an open-source toolkit for neural machine translation (NMT). The\ntoolkit prioritizes efficiency, modularity, and extensibility with the goal of\nsupporting NMT research into model architectures, feature representations, and\nsource modalities, while maintaining competitive performance and reasonable\ntraining requirements. The toolkit consists of modeling and translation\nsupport, as well as detailed pedagogical documentation about the underlying\ntechniques. \n\n"}
{"id": "1701.03037", "contents": "Title: Towards Smart Proof Search for Isabelle Abstract: Despite the recent progress in automatic theorem provers, proof engineers are\nstill suffering from the lack of powerful proof automation. In this position\npaper we first report our proof strategy language based on a meta-tool\napproach. Then, we propose an AI-based approach to drastically improve proof\nautomation for Isabelle, while identifying three major challenges we plan to\naddress for this objective. \n\n"}
{"id": "1701.03043", "contents": "Title: Robust Group LASSO Over Decentralized Networks Abstract: This paper considers the recovery of group sparse signals over a multi-agent\nnetwork, where the measurements are subject to sparse errors. We first\ninvestigate the robust group LASSO model and its centralized algorithm based on\nthe alternating direction method of multipliers (ADMM), which requires a\ncentral fusion center to compute a global row-support detector. To implement it\nin a decentralized network environment, we then adopt dynamic average consensus\nstrategies that enable dynamic tracking of the global row-support detector.\nNumerical experiments demonstrate the effectiveness of the proposed algorithms. \n\n"}
{"id": "1701.03297", "contents": "Title: Solutions to twisted word equations and equations in virtually free\n  groups Abstract: It is well known that the problem solving equations in virtually free groups\ncan be reduced to the problem of solving twisted word equations with regular\nconstraints over free monoids with involution. In this paper we prove that the\nset of all solutions of a twisted word equation is an EDT0L language whose\nspecification can be computed in $\\mathsf{PSPACE}$. Within the same complexity\nbound we can decide whether the solution set is empty, finite, or infinite.\n  In the second part of the paper we apply the results for twisted equations to\nobtain in $\\mathsf{PSPACE}$ an EDT0L description of the solution set of\nequations with rational constraints for finitely generated virtually free\ngroups in standard normal forms with respect to a natural set of generators. If\nthe rational constraints are given by a homomorphism into a fixed (or \"small\nenough\") finite monoid, then our algorithms can be implemented in\n$\\mathsf{NSPACE}(n^2\\log n)$, that is, in quasi-quadratic nondeterministic\nspace.\n  Our results generalize the work by Lohrey and S\\'enizergues (ICALP 2006) and\nDahmani and Guirardel (J. of Topology 2010) with respect to both complexity and\nexpressive power. Neither paper gave any concrete complexity bound and the\nresults in these papers are stated for subsets of solutions only, whereas our\nresults concern all solutions. \n\n"}
{"id": "1701.03757", "contents": "Title: Deep Probabilistic Programming Abstract: We propose Edward, a Turing-complete probabilistic programming language.\nEdward defines two compositional representations---random variables and\ninference. By treating inference as a first class citizen, on a par with\nmodeling, we show that probabilistic programming can be as flexible and\ncomputationally efficient as traditional deep learning. For flexibility, Edward\nmakes it easy to fit the same model using a variety of composable inference\nmethods, ranging from point estimation to variational inference to MCMC. In\naddition, Edward can reuse the modeling representation as part of inference,\nfacilitating the design of rich variational models and generative adversarial\nnetworks. For efficiency, Edward is integrated into TensorFlow, providing\nsignificant speedups over existing probabilistic systems. For example, we show\non a benchmark logistic regression task that Edward is at least 35x faster than\nStan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it\nis as fast as handwritten TensorFlow. \n\n"}
{"id": "1701.04391", "contents": "Title: Congruence Closure in Intensional Type Theory Abstract: Congruence closure procedures are used extensively in automated reasoning and\nare a core component of most satisfiability modulo theories solvers. However,\nno known congruence closure algorithms can support any of the expressive logics\nbased on intensional type theory (ITT), which form the basis of many\ninteractive theorem provers. The main source of expressiveness in these logics\nis dependent types, and yet existing congruence closure procedures found in\ninteractive theorem provers based on ITT do not handle dependent types at all\nand only work on the simply-typed subsets of the logics. Here we present an\nefficient and proof-producing congruence closure procedure that applies to\nevery function in ITT no matter how many dependencies exist among its\narguments, and that only relies on the commonly assumed uniqueness of identity\nproofs axiom. We demonstrate its usefulness by solving interesting verification\nproblems involving functions with dependent types. \n\n"}
{"id": "1701.05451", "contents": "Title: Feasibility of Fog Computing Abstract: As billions of devices get connected to the Internet, it will not be\nsustainable to use the cloud as a centralised server. The way forward is to\ndecentralise computations away from the cloud towards the edge of the network\ncloser to the user. This reduces the latency of communication between a user\ndevice and the cloud, and is the premise of 'fog computing' defined in this\npaper. The aim of this paper is to highlight the feasibility and the benefits\nin improving the Quality-of-Service and Experience by using fog computing. For\nan online game use-case, we found that the average response time for a user is\nimproved by 20% when using the edge of the network in comparison to using a\ncloud-only model. It was also observed that the volume of traffic between the\nedge and the cloud server is reduced by over 90% for the use-case. The\npreliminary results highlight the potential of fog computing in achieving a\nsustainable computing model and highlights the benefits of integrating the edge\nof the network into the computing ecosystem. \n\n"}
{"id": "1701.05617", "contents": "Title: Parametricity, automorphisms of the universe, and excluded middle Abstract: It is known that one can construct non-parametric functions by assuming\nclassical axioms. Our work is a converse to that: we prove classical axioms in\ndependent type theory assuming specific instances of non-parametricity. We also\naddress the interaction between classical axioms and the existence of\nautomorphisms of a type universe. We work over intensional Martin-L\\\"of\ndependent type theory, and in some results assume further principles including\nfunction extensionality, propositional extensionality, propositional\ntruncation, and the univalence axiom. \n\n"}
{"id": "1701.08330", "contents": "Title: SOS-based Modal Decomposition on Nondeterministic Probabilistic\n  Processes Abstract: We propose a method for the decomposition of modal formulae on processes with\nnondeterminism and probability with respect to Structural Operational\nSemantics. The purpose is to reduce the satisfaction problem of a formula for a\nprocess to verifying whether its subprocesses satisfy certain formulae obtained\nfrom the decomposition. To deal with the probabilistic behavior of processes,\nand thus with the decomposition of formulae characterizing it, we introduce a\nSOS-like machinery allowing for the specification of the behavior of open\ndistribution terms. By our decomposition, we obtain (pre)congruence formats for\nprobabilistic bisimilarity, ready similarity and similarity. \n\n"}
{"id": "1702.00934", "contents": "Title: Y-Calculus: A Language for Real Matrices Derived from the ZX-Calculus Abstract: We introduce a ZX-like diagrammatic language devoted to manipulating real\nmatrices - and rebits -, with its own set of axioms. We prove the necessity of\nsome non trivial axioms of these. We show that some restriction of the language\nis complete. We exhibit two interpretations to and from the ZX-Calculus, thus\nshowing the consistency between the two languages. Finally, we derive from our\nwork a way to extract the real or imaginary part of a ZX-diagram, and prove\nthat a restriction of our language is complete if the equivalent restriction of\nthe ZX-calculus is complete. \n\n"}
{"id": "1702.01135", "contents": "Title: Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks Abstract: Deep neural networks have emerged as a widely used and effective means for\ntackling complex, real-world problems. However, a major obstacle in applying\nthem to safety-critical systems is the great difficulty in providing formal\nguarantees about their behavior. We present a novel, scalable, and efficient\ntechnique for verifying properties of deep neural networks (or providing\ncounter-examples). The technique is based on the simplex method, extended to\nhandle the non-convex Rectified Linear Unit (ReLU) activation function, which\nis a crucial ingredient in many modern neural networks. The verification\nprocedure tackles neural networks as a whole, without making any simplifying\nassumptions. We evaluated our technique on a prototype deep neural network\nimplementation of the next-generation airborne collision avoidance system for\nunmanned aircraft (ACAS Xu). Results show that our technique can successfully\nprove properties of networks that are an order of magnitude larger than the\nlargest networks verified using existing methods. \n\n"}
{"id": "1702.03096", "contents": "Title: A set-theoretical approach for ABox reasoning services (Extended\n  Version) Abstract: In this paper we consider the most common ABox reasoning services for the\ndescription logic $\\mathcal{DL}\\langle\n\\mathsf{4LQS^{R,\\!\\times}}\\rangle(\\mathbf{D})$\n($\\mathcal{DL}_{\\mathbf{D}}^{4,\\!\\times}$, for short) and prove their\ndecidability via a reduction to the satisfiability problem for the\nset-theoretic fragment \\flqsr. The description logic\n$\\mathcal{DL}_{\\mathbf{D}}^{4,\\!\\times}$ is very expressive, as it admits\nvarious concept and role constructs, and data types, that allow one to\nrepresent rule-based languages such as SWRL. Decidability results are achieved\nby defining a generalization of the conjunctive query answering problem, called\nHOCQA (Higher Order Conjunctive Query Answering), that can be instantiated to\nthe most wide\\-spread ABox reasoning tasks. We also present a \\ke\\space based\nprocedure for calculating the answer set from\n$\\mathcal{DL}_{\\mathbf{D}}^{4,\\!\\times}$ knowledge bases and higher order\n$\\mathcal{DL}_{\\mathbf{D}}^{4,\\!\\times}$ conjunctive queries, thus providing\nmeans for reasoning on several well-known ABox reasoning tasks. Our calculus\nextends a previously introduced \\ke\\space based decision procedure for the CQA\nproblem. \n\n"}
{"id": "1702.03274", "contents": "Title: Hybrid Code Networks: practical and efficient end-to-end dialog control\n  with supervised and reinforcement learning Abstract: End-to-end learning of recurrent neural networks (RNNs) is an attractive\nsolution for dialog systems; however, current techniques are data-intensive and\nrequire thousands of dialogs to learn simple behaviors. We introduce Hybrid\nCode Networks (HCNs), which combine an RNN with domain-specific knowledge\nencoded as software and system action templates. Compared to existing\nend-to-end approaches, HCNs considerably reduce the amount of training data\nrequired, while retaining the key benefit of inferring a latent representation\nof dialog state. In addition, HCNs can be optimized with supervised learning,\nreinforcement learning, or a mixture of both. HCNs attain state-of-the-art\nperformance on the bAbI dialog dataset, and outperform two commercially\ndeployed customer-facing dialog systems. \n\n"}
{"id": "1702.03380", "contents": "Title: Training Deep Neural Networks via Optimization Over Graphs Abstract: In this work, we propose to train a deep neural network by distributed\noptimization over a graph. Two nonlinear functions are considered: the\nrectified linear unit (ReLU) and a linear unit with both lower and upper\ncutoffs (DCutLU). The problem reformulation over a graph is realized by\nexplicitly representing ReLU or DCutLU using a set of slack variables. We then\napply the alternating direction method of multipliers (ADMM) to update the\nweights of the network layerwise by solving subproblems of the reformulated\nproblem. Empirical results suggest that the ADMM-based method is less sensitive\nto overfitting than the stochastic gradient descent (SGD) and Adam methods. \n\n"}
{"id": "1702.03504", "contents": "Title: Time, Computational Complexity, and Probability in the Analysis of\n  Distance-Bounding Protocols Abstract: Many security protocols rely on the assumptions on the physical properties in\nwhich its protocol sessions will be carried out. For instance, Distance\nBounding Protocols take into account the round trip time of messages and the\ntransmission velocity to infer an upper bound of the distance between two\nagents. We classify such security protocols as Cyber-Physical. Time plays a key\nrole in design and analysis of many of these protocols. This paper investigates\nthe foundational differences and the impacts on the analysis when using models\nwith discrete time and models with dense time. We show that there are attacks\nthat can be found by models using dense time, but not when using discrete time.\nWe illustrate this with a novel attack that can be carried out on most Distance\nBounding Protocols. In this attack, one exploits the execution delay of\ninstructions during one clock cycle to convince a verifier that he is in a\nlocation different from his actual position. We additionally present a\nprobabilistic analysis of this novel attack. As a formal model for representing\nand analyzing Cyber-Physical properties, we propose a Multiset Rewriting model\nwith dense time suitable for specifying cyber-physical security protocols. We\nintroduce Circle-Configurations and show that they can be used to symbolically\nsolve the reachability problem for our model, and show that for the important\nclass of balanced theories the reachability problem is PSPACE-complete. We also\nshow how our model can be implemented using the computational rewriting tool\nMaude, the machinery that automatically searches for such attacks. \n\n"}
{"id": "1702.04263", "contents": "Title: Okapi: Causally Consistent Geo-Replication Made Faster, Cheaper and More\n  Available Abstract: Okapi is a new causally consistent geo-replicated key- value store. Okapi\nleverages two key design choices to achieve high performance. First, it relies\non hybrid logical/physical clocks to achieve low latency even in the presence\nof clock skew. Second, Okapi achieves higher resource efficiency and better\navailability, at the expense of a slight increase in update visibility latency.\nTo this end, Okapi implements a new stabilization protocol that uses a\ncombination of vector and scalar clocks and makes a remote update visible when\nits delivery has been acknowledged by every data center. We evaluate Okapi with\ndifferent workloads on Amazon AWS, using three geographically distributed\nregions and 96 nodes. We compare Okapi with two recent approaches to causal\nconsistency, Cure and GentleRain. We show that Okapi delivers up to two orders\nof magnitude better performance than GentleRain and that Okapi achieves up to\n3.5x lower latency and a 60% reduction of the meta-data overhead with respect\nto Cure. \n\n"}
{"id": "1702.04280", "contents": "Title: DAGER: Deep Age, Gender and Emotion Recognition Using Convolutional\n  Neural Network Abstract: This paper describes the details of Sighthound's fully automated age, gender\nand emotion recognition system. The backbone of our system consists of several\ndeep convolutional neural networks that are not only computationally\ninexpensive, but also provide state-of-the-art results on several competitive\nbenchmarks. To power our novel deep networks, we collected large labeled\ndatasets through a semi-supervised pipeline to reduce the annotation\neffort/time. We tested our system on several public benchmarks and report\noutstanding results. Our age, gender and emotion recognition models are\navailable to developers through the Sighthound Cloud API at\nhttps://www.sighthound.com/products/cloud \n\n"}
{"id": "1702.05051", "contents": "Title: Succinct progress measures for solving parity games Abstract: The recent breakthrough paper by Calude et al. has given the first algorithm\nfor solving parity games in quasi-polynomial time, where previously the best\nalgorithms were mildly subexponential. We devise an alternative\nquasi-polynomial time algorithm based on progress measures, which allows us to\nreduce the space required from quasi-polynomial to nearly linear. Our key\ntechnical tools are a novel concept of ordered tree coding, and a succinct tree\ncoding result that we prove using bounded adaptive multi-counters, both of\nwhich are interesting in their own right. \n\n"}
{"id": "1702.05527", "contents": "Title: Local Redundancy in SAT: Generalizations of Blocked Clauses Abstract: Clause-elimination procedures that simplify formulas in conjunctive normal\nform play an important role in modern SAT solving. Before or during the actual\nsolving process, such procedures identify and remove clauses that are\nirrelevant to the solving result. These simplifications usually rely on\nso-called redundancy properties that characterize cases in which the removal of\na clause does not affect the satisfiability status of a formula. One\nparticularly successful redundancy property is that of blocked clauses, because\nit generalizes several other redundancy properties. To find out whether a\nclause is blocked---and therefore redundant---one only needs to consider its\nresolution environment, i.e., the clauses with which it can be resolved. For\nthis reason, we say that the redundancy property of blocked clauses is local.\nIn this paper, we show that there exist local redundancy properties that are\neven more general than blocked clauses. We present a semantic notion of\nblocking and prove that it constitutes the most general local redundancy\nproperty. We furthermore introduce the syntax-based notions of set-blocking and\nsuper-blocking, and show that the latter coincides with our semantic blocking\nnotion. In addition, we show how semantic blocking can be alternatively\ncharacterized via Davis and Putnam's rule for eliminating atomic formulas.\nFinally, we perform a detailed complexity analysis and relate our novel\nredundancy properties to prominent redundancy properties from the literature. \n\n"}
{"id": "1702.05865", "contents": "Title: Hemingway: Modeling Distributed Optimization Algorithms Abstract: Distributed optimization algorithms are widely used in many industrial\nmachine learning applications. However choosing the appropriate algorithm and\ncluster size is often difficult for users as the performance and convergence\nrate of optimization algorithms vary with the size of the cluster. In this\npaper we make the case for an ML-optimizer that can select the appropriate\nalgorithm and cluster size to use for a given problem. To do this we propose\nbuilding two models: one that captures the system level characteristics of how\ncomputation, communication change as we increase cluster sizes and another that\ncaptures how convergence rates change with cluster sizes. We present\npreliminary results from our prototype implementation called Hemingway and\ndiscuss some of the challenges involved in developing such a system. \n\n"}
{"id": "1702.07134", "contents": "Title: Diverse Weighted Bipartite b-Matching Abstract: Bipartite matching, where agents on one side of a market are matched to\nagents or items on the other, is a classical problem in computer science and\neconomics, with widespread application in healthcare, education, advertising,\nand general resource allocation. A practitioner's goal is typically to maximize\na matching market's economic efficiency, possibly subject to some fairness\nrequirements that promote equal access to resources. A natural balancing act\nexists between fairness and efficiency in matching markets, and has been the\nsubject of much research.\n  In this paper, we study a complementary goal---balancing diversity and\nefficiency---in a generalization of bipartite matching where agents on one side\nof the market can be matched to sets of agents on the other. Adapting a\nclassical definition of the diversity of a set, we propose a quadratic\nprogramming-based approach to solving a supermodular minimization problem that\nbalances diversity and total weight of the solution. We also provide a scalable\ngreedy algorithm with theoretical performance bounds. We then define the price\nof diversity, a measure of the efficiency loss due to enforcing diversity, and\ngive a worst-case theoretical bound. Finally, we demonstrate the efficacy of\nour methods on three real-world datasets, and show that the price of diversity\nis not bad in practice. \n\n"}
{"id": "1702.07403", "contents": "Title: Making Asynchronous Distributed Computations Robust to Noise Abstract: We consider the problem of making distributed computations robust to noise,\nin particular to worst-case (adversarial) corruptions of messages. We give a\ngeneral distributed interactive coding scheme which simulates any asynchronous\ndistributed protocol while tolerating an optimal corruption of a $\\Theta(1/n)$\nfraction of all messages while incurring a moderate blowup of $O(n\\log^2 n)$ in\nthe communication complexity.\n  Our result is the first fully distributed interactive coding scheme in which\nthe topology of the communication network is not known in advance. Prior work\nrequired either a coordinating node to be connected to all other nodes in the\nnetwork or assumed a synchronous network in which all nodes already know the\ncomplete topology of the network. \n\n"}
{"id": "1702.07908", "contents": "Title: CHAOS: A Parallelization Scheme for Training Convolutional Neural\n  Networks on Intel Xeon Phi Abstract: Deep learning is an important component of big-data analytic tools and\nintelligent applications, such as, self-driving cars, computer vision, speech\nrecognition, or precision medicine. However, the training process is\ncomputationally intensive, and often requires a large amount of time if\nperformed sequentially. Modern parallel computing systems provide the\ncapability to reduce the required training time of deep neural networks. In\nthis paper, we present our parallelization scheme for training convolutional\nneural networks (CNN) named Controlled Hogwild with Arbitrary Order of\nSynchronization (CHAOS). Major features of CHAOS include the support for thread\nand vector parallelism, non-instant updates of weight parameters during\nback-propagation without a significant delay, and implicit synchronization in\narbitrary order. CHAOS is tailored for parallel computing systems that are\naccelerated with the Intel Xeon Phi. We evaluate our parallelization approach\nempirically using measurement techniques and performance modeling for various\nnumbers of threads and CNN architectures. Experimental results for the MNIST\ndataset of handwritten digits using the total number of threads on the Xeon Phi\nshow speedups of up to 103x compared to the execution on one thread of the Xeon\nPhi, 14x compared to the sequential execution on Intel Xeon E5, and 58x\ncompared to the sequential execution on Intel Core i5. \n\n"}
{"id": "1702.08193", "contents": "Title: Modularisation of Sequent Calculi for Normal and Non-normal Modalities Abstract: In this work we explore the connections between (linear) nested sequent\ncalculi and ordinary sequent calculi for normal and non-normal modal logics. By\nproposing local versions to ordinary sequent rules we obtain linear nested\nsequent calculi for a number of logics, including to our knowledge the first\nnested sequent calculi for a large class of simply dependent multimodal logics,\nand for many standard non-normal modal logics. The resulting systems are\nmodular and have separate left and right introduction rules for the modalities,\nwhich makes them amenable to specification as bipole clauses. While this\ngranulation of the sequent rules introduces more choices for proof search, we\nshow how linear nested sequent calculi can be restricted to blocked\nderivations, which directly correspond to ordinary sequent derivations. \n\n"}
{"id": "1703.00320", "contents": "Title: Investigating the Characteristics of One-Sided Matching Mechanisms Under\n  Various Preferences and Risk Attitudes Abstract: One-sided matching mechanisms are fundamental for assigning a set of\nindivisible objects to a set of self-interested agents when monetary transfers\nare not allowed. Two widely-studied randomized mechanisms in multiagent\nsettings are the Random Serial Dictatorship (RSD) and the Probabilistic Serial\nRule (PS). Both mechanisms require only that agents specify ordinal preferences\nand have a number of desirable economic and computational properties. However,\nthe induced outcomes of the mechanisms are often incomparable and thus there\nare challenges when it comes to deciding which mechanism to adopt in practice.\nIn this paper, we first consider the space of general ordinal preferences and\nprovide empirical results on the (in)comparability of RSD and PS. We analyze\ntheir respective economic properties under general and lexicographic\npreferences. We then instantiate utility functions with the goal of gaining\ninsights on the manipulability, efficiency, and envyfreeness of the mechanisms\nunder different risk-attitude models. Our results hold under various preference\ndistribution models, which further confirm the broad use of RSD in most\npractical applications. \n\n"}
{"id": "1703.01041", "contents": "Title: Large-Scale Evolution of Image Classifiers Abstract: Neural networks have proven effective at solving difficult problems but\ndesigning their architectures can be challenging, even for image classification\nproblems alone. Our goal is to minimize human participation, so we employ\nevolutionary algorithms to discover such networks automatically. Despite\nsignificant computational requirements, we show that it is now possible to\nevolve models with accuracies within the range of those published in the last\nyear. Specifically, we employ simple evolutionary techniques at unprecedented\nscales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting\nfrom trivial initial conditions and reaching accuracies of 94.6% (95.6% for\nensemble) and 77.0%, respectively. To do this, we use novel and intuitive\nmutation operators that navigate large search spaces; we stress that no human\nparticipation is required once evolution starts and that the output is a\nfully-trained model. Throughout this work, we place special emphasis on the\nrepeatability of results, the variability in the outcomes and the computational\nrequirements. \n\n"}
{"id": "1703.01122", "contents": "Title: First-Order Logic with Counting: At Least, Weak Hanf Normal Forms Always\n  Exist and Can Be Computed! Abstract: We introduce the logic FOCN(P) which extends first-order logic by counting\nand by numerical predicates from a set P, and which can be viewed as a natural\ngeneralisation of various counting logics that have been studied in the\nliterature.\n  We obtain a locality result showing that every FOCN(P)-formula can be\ntransformed into a formula in Hanf normal form that is equivalent on all finite\nstructures of degree at most d. A formula is in Hanf normal form if it is a\nBoolean combination of formulas describing the neighbourhood around its tuple\nof free variables and arithmetic sentences with predicates from P over atomic\nstatements describing the number of realisations of a type with a single\ncentre. The transformation into Hanf normal form can be achieved in time\nelementary in $d$ and the size of the input formula. From this locality result,\nwe infer the following applications: (*) The Hanf-locality rank of first-order\nformulas of bounded quantifier alternation depth only grows polynomially with\nthe formula size. (*) The model checking problem for the fragment FOC(P) of\nFOCN(P) on structures of bounded degree is fixed-parameter tractable (with\nelementary parameter dependence). (*) The query evaluation problem for fixed\nqueries from FOC(P) over fully dynamic databases of degree at most d can be\nsolved efficiently: there is a dynamic algorithm that can enumerate the tuples\nin the query result with constant delay, and that allows to compute the size of\nthe query result and to test if a given tuple belongs to the query result\nwithin constant time after every database update. \n\n"}
{"id": "1703.02245", "contents": "Title: Design of the Artificial: lessons from the biological roots of general\n  intelligence Abstract: Our fascination with intelligent machines goes back to ancient times with the\nmythical automaton Talos, Aristotle's mode of mechanical thought (syllogism)\nand Heron of Alexandria's mechanical machines. However, the quest for\nArtificial General Intelligence (AGI) has been troubled with repeated failures.\nRecently, there has been a shift towards bio-inspired software and hardware,\nbut their singular design focus makes them inefficient in achieving AGI. Which\nset of requirements have to be met in the design of AGI? What are the limits in\nthe design of the artificial? A careful examination of computation in\nbiological systems suggests that evolutionary tinkering of contextual\nprocessing of information enabled by a hierarchical architecture is key to\nbuilding AGI. \n\n"}
{"id": "1703.03076", "contents": "Title: Causal Data Science for Financial Stress Testing Abstract: The most recent financial upheavals have cast doubt on the adequacy of some\nof the conventional quantitative risk management strategies, such as VaR (Value\nat Risk), in many common situations. Consequently, there has been an increasing\nneed for verisimilar financial stress testings, namely simulating and analyzing\nfinancial portfolios in extreme, albeit rare scenarios. Unlike conventional\nrisk management which exploits statistical correlations among financial\ninstruments, here we focus our analysis on the notion of probabilistic\ncausation, which is embodied by Suppes-Bayes Causal Networks (SBCNs); SBCNs are\nprobabilistic graphical models that have many attractive features in terms of\nmore accurate causal analysis for generating financial stress scenarios. In\nthis paper, we present a novel approach for conducting stress testing of\nfinancial portfolios based on SBCNs in combination with classical machine\nlearning classification tools. The resulting method is shown to be capable of\ncorrectly discovering the causal relationships among financial factors that\naffect the portfolios and thus, simulating stress testing scenarios with a\nhigher accuracy and lower computational complexity than conventional Monte\nCarlo Simulations. \n\n"}
{"id": "1703.05424", "contents": "Title: Partially Replicated Causally Consistent Shared Memory: Lower Bounds and\n  An Algorithm Abstract: The focus of this paper is on causal consistency in a {\\em partially\nreplicated} distributed shared memory (DSM) system that provides the\nabstraction of shared read/write registers. Maintaining causal consistency in\ndistributed shared memory systems has received significant attention in the\npast, mostly on {\\em full replication} wherein each replica stores a copy of\nall the registers in the shared memory. To ensure causal consistency, all\ncausally preceding updates must be performed before an update is performed at\nany given replica. Therefore, some mechanism for tracking causal dependencies\nis required, such as vector timestamps with the number of vector elements being\nequal to the number of replicas in the context of full replication. In this\npaper, we investigate causal consistency in {\\em partially replicated systems},\nwherein each replica may store only a subset of the shared registers. Building\non the past work, this paper makes three key contributions: 1. We present a\nnecessary condition on the metadata (which we refer as a {\\em timestamp}) that\nmust be maintained by each replica to be able to track causality accurately.\nThe necessary condition identifies a set of directed edges in a {\\em share\ngraph} that a replica's timestamp must keep track of. 2. We present an\nalgorithm for achieving causal consistency using a timestamp that matches the\nabove necessary condition, thus showing that the condition is necessary and\nsufficient. 3. We define a measurement of timestamp space size and present a\nlower bound (in bits) on the size of the timestamps. The lower bound matches\nour algorithm in several special cases. \n\n"}
{"id": "1703.07758", "contents": "Title: Sample and Computationally Efficient Learning Algorithms under S-Concave\n  Distributions Abstract: We provide new results for noise-tolerant and sample-efficient learning\nalgorithms under $s$-concave distributions. The new class of $s$-concave\ndistributions is a broad and natural generalization of log-concavity, and\nincludes many important additional distributions, e.g., the Pareto distribution\nand $t$-distribution. This class has been studied in the context of efficient\nsampling, integration, and optimization, but much remains unknown about the\ngeometry of this class of distributions and their applications in the context\nof learning. The challenge is that unlike the commonly used distributions in\nlearning (uniform or more generally log-concave distributions), this broader\nclass is not closed under the marginalization operator and many such\ndistributions are fat-tailed. In this work, we introduce new convex geometry\ntools to study the properties of $s$-concave distributions and use these\nproperties to provide bounds on quantities of interest to learning including\nthe probability of disagreement between two halfspaces, disagreement outside a\nband, and the disagreement coefficient. We use these results to significantly\ngeneralize prior results for margin-based active learning, disagreement-based\nactive learning, and passive learning of intersections of halfspaces. Our\nanalysis of geometric properties of $s$-concave distributions might be of\nindependent interest to optimization more broadly. \n\n"}
{"id": "1703.08228", "contents": "Title: Automatically Tuning the GCC Compiler to Optimize the Performance of\n  Applications Running on Embedded Systems Abstract: This paper introduces a novel method for automatically tuning the selection\nof compiler flags to optimize the performance of software intended to run on\nembedded hardware platforms. We begin by developing our approach on code\ncompiled by the GNU C Compiler (GCC) for the ARM Cortex-M3 (CM3) processor; and\nwe show how our method outperforms the industry standard -O3 optimization level\nacross a diverse embedded benchmark suite. First we quantify the potential\ngains by using existing iterative compilation approaches that time-intensively\nsearch for optimal configurations for each benchmark. Then we adapt iterative\ncompilation to output a single configuration that optimizes performance across\nthe entire benchmark suite. Although this is a time-consuming process, our\napproach constructs an optimized variation of -O3, which we call -Ocm3, that\nrealizes nearly two thirds of known available gains on the CM3 and\nsignificantly outperforms a more complex state-of-the-art predictive method in\ncross-validation experiments. Finally, we demonstrate our method on additional\nplatforms by constructing two more optimization levels that find even more\nsignificant speed-ups on the ARM Cortex-A8 and 8-bit AVR processors. \n\n"}
{"id": "1703.09034", "contents": "Title: A Recipe for State-and-Effect Triangles Abstract: In the semantics of programming languages one can view programs as state\ntransformers, or as predicate transformers. Recently the author has introduced\nstate-and-effect triangles which capture this situation categorically,\ninvolving an adjunction between state- and predicate-transformers. The current\npaper exploits a classical result in category theory, part of Jon Beck's\nmonadicity theorem, to systematically construct such a state-and-effect\ntriangle from an adjunction. The power of this construction is illustrated in\nmany examples, covering many monads occurring in program semantics, including\n(probabilistic) power domains. \n\n"}
{"id": "1703.09707", "contents": "Title: Accelerating gravitational microlensing simulations using the Xeon Phi\n  coprocessor Abstract: Recently Graphics Processing Units (GPUs) have been used to speed up very\nCPU-intensive gravitational microlensing simulations. In this work, we use the\nXeon Phi coprocessor to accelerate such simulations and compare its performance\non a microlensing code with that of NVIDIA's GPUs. For the selected set of\nparameters evaluated in our experiment, we find that the speedup by Intel's\nKnights Corner coprocessor is comparable to that by NVIDIA's Fermi family of\nGPUs with compute capability 2.0, but less significant than GPUs with higher\ncompute capabilities such as the Kepler. However, the very recently released\nsecond generation Xeon Phi, Knights Landing, is about 5.8 times faster than the\nKnights Corner, and about 2.9 times faster than the Kepler GPU used in our\nsimulations. We conclude that the Xeon Phi is a very promising alternative to\nGPUs for modern high performance microlensing simulations. \n\n"}
{"id": "1703.09845", "contents": "Title: Bringing Salary Transparency to the World: Computing Robust Compensation\n  Insights via LinkedIn Salary Abstract: The recently launched LinkedIn Salary product has been designed with the goal\nof providing compensation insights to the world's professionals and thereby\nhelping them optimize their earning potential. We describe the overall design\nand architecture of the statistical modeling system underlying this product. We\nfocus on the unique data mining challenges while designing and implementing the\nsystem, and describe the modeling components such as Bayesian hierarchical\nsmoothing that help to compute and present robust compensation insights to\nusers. We report on extensive evaluation with nearly one year of de-identified\ncompensation data collected from over one million LinkedIn users, thereby\ndemonstrating the efficacy of the statistical models. We also highlight the\nlessons learned through the deployment of our system at LinkedIn. \n\n"}
{"id": "1703.10731", "contents": "Title: An analysis of budgeted parallel search on conditional Galton-Watson\n  trees Abstract: Recently Avis and Jordan have demonstrated the efficiency of a simple\ntechnique called budgeting for the parallelization of a number of tree search\nalgorithms. The idea is to limit the amount of work that a processor performs\nbefore it terminates its search and returns any unexplored nodes to a master\nprocess. This limit is set by a critical budget parameter which determines the\noverhead of the process. In this paper we study the behaviour of the budget\nparameter on conditional Galton-Watson trees obtaining asymptotically tight\nbounds on this overhead. We present empirical results to show that this bound\nis surprisingly accurate in practice. \n\n"}
{"id": "1704.00978", "contents": "Title: High-Throughput Computing on High-Performance Platforms: A Case Study Abstract: The computing systems used by LHC experiments has historically consisted of\nthe federation of hundreds to thousands of distributed resources, ranging from\nsmall to mid-size resource. In spite of the impressive scale of the existing\ndistributed computing solutions, the federation of small to mid-size resources\nwill be insufficient to meet projected future demands. This paper is a case\nstudy of how the ATLAS experiment has embraced Titan---a DOE leadership\nfacility in conjunction with traditional distributed high- throughput computing\nto reach sustained production scales of approximately 52M core-hours a years.\nThe three main contributions of this paper are: (i) a critical evaluation of\ndesign and operational considerations to support the sustained, scalable and\nproduction usage of Titan; (ii) a preliminary characterization of a next\ngeneration executor for PanDA to support new workloads and advanced execution\nmodes; and (iii) early lessons for how current and future experimental and\nobservational systems can be integrated with production supercomputers and\nother platforms in a general and extensible manner. \n\n"}
{"id": "1704.02038", "contents": "Title: Treatment-Response Models for Counterfactual Reasoning with\n  Continuous-time, Continuous-valued Interventions Abstract: Treatment effects can be estimated from observational data as the difference\nin potential outcomes. In this paper, we address the challenge of estimating\nthe potential outcome when treatment-dose levels can vary continuously over\ntime. Further, the outcome variable may not be measured at a regular frequency.\nOur proposed solution represents the treatment response curves using linear\ntime-invariant dynamical systems---this provides a flexible means for modeling\nresponse over time to highly variable dose curves. Moreover, for multivariate\ndata, the proposed method: uncovers shared structure in treatment response and\nthe baseline across multiple markers; and, flexibly models challenging\ncorrelation structure both across and within signals over time. For this, we\nbuild upon the framework of multiple-output Gaussian Processes. On simulated\nand a challenging clinical dataset, we show significant gains in accuracy over\nstate-of-the-art models. \n\n"}
{"id": "1704.02254", "contents": "Title: Recurrent Environment Simulators Abstract: Models that can simulate how environments change in response to actions can\nbe used by agents to plan and act efficiently. We improve on previous\nenvironment simulators from high-dimensional pixel observations by introducing\nrecurrent neural networks that are able to make temporally and spatially\ncoherent predictions for hundreds of time-steps into the future. We present an\nin-depth analysis of the factors affecting performance, providing the most\nextensive attempt to advance the understanding of the properties of these\nmodels. We address the issue of computationally inefficiency with a model that\ndoes not need to generate a high-dimensional image at each time-step. We show\nthat our approach can be used to improve exploration and is adaptable to many\ndiverse environments, namely 10 Atari games, a 3D car racing environment, and\ncomplex 3D mazes. \n\n"}
{"id": "1704.02882", "contents": "Title: Dynamic Safe Interruptibility for Decentralized Multi-Agent\n  Reinforcement Learning Abstract: In reinforcement learning, agents learn by performing actions and observing\ntheir outcomes. Sometimes, it is desirable for a human operator to\n\\textit{interrupt} an agent in order to prevent dangerous situations from\nhappening. Yet, as part of their learning process, agents may link these\ninterruptions, that impact their reward, to specific states and deliberately\navoid them. The situation is particularly challenging in a multi-agent context\nbecause agents might not only learn from their own past interruptions, but also\nfrom those of other agents. Orseau and Armstrong defined \\emph{safe\ninterruptibility} for one learner, but their work does not naturally extend to\nmulti-agent systems. This paper introduces \\textit{dynamic safe\ninterruptibility}, an alternative definition more suited to decentralized\nlearning problems, and studies this notion in two learning frameworks:\n\\textit{joint action learners} and \\textit{independent learners}. We give\nrealistic sufficient conditions on the learning algorithm to enable dynamic\nsafe interruptibility in the case of joint action learners, yet show that these\nconditions are not sufficient for independent learners. We show however that if\nagents can detect interruptions, it is possible to prune the observations to\nensure dynamic safe interruptibility even for independent learners. \n\n"}
{"id": "1704.03080", "contents": "Title: Representing operational semantics with enriched Lawvere theories Abstract: Many term calculi, like lambda calculus or pi calculus, involve binders for\nnames, and the mathematics of bound variable names is subtle. Schoenfinkel\nintroduced the SKI combinator calculus in 1924 to clarify the role of\nquantified variables in intuitionistic logic by eliminating them. Yoshida\ndemonstrated how to eliminate the bound names coming from the input prefix in\nthe asynchronous pi calculus, but her combinators still depend on the new\noperator to bind names. Recently, Meredith and Stay showed how to modify\nYoshida's combinators by replacing new and replication with reflective\noperators to provide the first combinator calculus with no bound names into\nwhich the asynchronous pi calculus has a faithful embedding. Here we provide an\nalternative set of combinators built from SKI plus reflection that also\neliminates all nominal phenomena, yet provides a faithful embedding of a\nreflective higher-order pi calculus. We show that with the nominal features\neffectively eliminated as syntactic sugar, multisorted Lawvere theories\nenriched over graphs suffice to capture the operational semantics of the\ncalculus. \n\n"}
{"id": "1704.03292", "contents": "Title: Enumeration Complexity of Poor Man's Propositional Dependence Logic Abstract: Dependence logics are a modern family of logics of independence and\ndependence which mimic notions of database theory. In this paper, we aim to\ninitiate the study of enumeration complexity in the field of dependence logics\nand thereby get a new point of view on enumerating answers of database queries.\nConsequently, as a first step, we investigate the problem of enumerating all\nsatisfying teams of formulas from a given fragment of propositional dependence\nlogic. We distinguish between restricting the team size by arbitrary functions\nand the parametrised version where the parameter is the team size. We show that\na polynomial delay can be reached for polynomials and otherwise in the\nparametrised setting we reach FPT delay. However, the constructed enumeration\nalgorithm with polynomial delay requires exponential space. We show that an\nincremental polynomial delay algorithm exists which uses polynomial space only.\nNegatively, we show that for the general problem without restricting the team\nsize, an enumeration algorithm running in polynomial space cannot exist. \n\n"}
{"id": "1704.03383", "contents": "Title: Portable, high-performance containers for HPC Abstract: Building and deploying software on high-end computing systems is a\nchallenging task. High performance applications have to reliably run across\nmultiple platforms and environments, and make use of site-specific resources\nwhile resolving complicated software-stack dependencies. Containers are a type\nof lightweight virtualization technology that attempt to solve this problem by\npackaging applications and their environments into standard units of software\nthat are: portable, easy to build and deploy, have a small footprint, and low\nruntime overhead. In this work we present an extension to the container runtime\nof Shifter that provides containerized applications with a mechanism to access\nGPU accelerators and specialized networking from the host system, effectively\nenabling performance portability of containers across HPC resources. The\npresented extension makes possible to rapidly deploy high-performance software\non supercomputers from containerized applications that have been developed,\nbuilt, and tested in non-HPC commodity hardware, e.g. the laptop or workstation\nof a researcher. \n\n"}
{"id": "1704.03627", "contents": "Title: Real-time On-Demand Crowd-powered Entity Extraction Abstract: Output-agreement mechanisms such as ESP Game have been widely used in human\ncomputation to obtain reliable human-generated labels. In this paper, we argue\nthat a \"time-limited\" output-agreement mechanism can be used to create a fast\nand robust crowd-powered component in interactive systems, particularly\ndialogue systems, to extract key information from user utterances on the fly.\nOur experiments on Amazon Mechanical Turk using the Airline Travel Information\nSystem (ATIS) dataset showed that the proposed approach achieves high-quality\nresults with an average response time shorter than 9 seconds. \n\n"}
{"id": "1704.04374", "contents": "Title: HPTT: A High-Performance Tensor Transposition C++ Library Abstract: Recently we presented TTC, a domain-specific compiler for tensor\ntranspositions. Despite the fact that the performance of the generated code is\nnearly optimal, due to its offline nature, TTC cannot be utilized in all the\napplication codes in which the tensor sizes and the necessary tensor\npermutations are determined at runtime. To overcome this limitation, we\nintroduce the open-source C++ library High-Performance Tensor Transposition\n(HPTT). Similar to TTC, HPTT incorporates optimizations such as blocking,\nmulti-threading, and explicit vectorization; furthermore it decomposes any\ntransposition into multiple loops around a so called micro-kernel. This modular\ndesign---inspired by BLIS---makes HPTT easy to port to different architectures,\nby only replacing the hand-vectorized micro-kernel (e.g., a 4x4 transpose).\nHPTT also offers an optional autotuning framework---guided by a performance\nmodel---that explores a vast search space of implementations at runtime\n(similar to FFTW). Across a wide range of different tensor transpositions and\narchitectures (e.g., Intel Ivy Bridge, Intel Knights Landing, ARMv7, IBM\nPower7), HPTT attains a bandwidth comparable to that of SAXPY, and yields\nremarkable speedups over Eigen's tensor transposition implementation. Most\nimportantly, the integration of HPTT into the Cyclops Tensor Framework (CTF)\nimproves the overall performance of tensor contractions by up to 3.1x. \n\n"}
{"id": "1704.06297", "contents": "Title: A Time Hierarchy Theorem for the LOCAL Model Abstract: The celebrated Time Hierarchy Theorem for Turing machines states, informally,\nthat more problems can be solved given more time. The extent to which a time\nhierarchy-type theorem holds in the distributed LOCAL model has been open for\nmany years. It is consistent with previous results that all natural problems in\nthe LOCAL model can be classified according to a small constant number of\ncomplexities, such as $O(1),O(\\log^* n), O(\\log n), 2^{O(\\sqrt{\\log n})}$, etc.\n  In this paper we establish the first time hierarchy theorem for the LOCAL\nmodel and prove that several gaps exist in the LOCAL time hierarchy.\n  1. We define an infinite set of simple coloring problems called Hierarchical\n$2\\frac{1}{2}$-Coloring}. A correctly colored graph can be confirmed by simply\nchecking the neighborhood of each vertex, so this problem fits into the class\nof locally checkable labeling (LCL) problems. However, the complexity of the\n$k$-level Hierarchical $2\\frac{1}{2}$-Coloring problem is $\\Theta(n^{1/k})$,\nfor $k\\in\\mathbb{Z}^+$. The upper and lower bounds hold for both general graphs\nand trees, and for both randomized and deterministic algorithms.\n  2. Consider any LCL problem on bounded degree trees. We prove an\nautomatic-speedup theorem that states that any randomized $n^{o(1)}$-time\nalgorithm solving the LCL can be transformed into a deterministic $O(\\log\nn)$-time algorithm. Together with a previous result, this establishes that on\ntrees, there are no natural deterministic complexities in the ranges\n$\\omega(\\log^* n)$---$o(\\log n)$ or $\\omega(\\log n)$---$n^{o(1)}$.\n  3. We expose a gap in the randomized time hierarchy on general graphs. Any\nrandomized algorithm that solves an LCL problem in sublogarithmic time can be\nsped up to run in $O(T_{LLL})$ time, which is the complexity of the distributed\nLovasz local lemma problem, currently known to be $\\Omega(\\log\\log n)$ and\n$O(\\log n)$. \n\n"}
{"id": "1704.06857", "contents": "Title: A Review on Deep Learning Techniques Applied to Semantic Segmentation Abstract: Image semantic segmentation is more and more being of interest for computer\nvision and machine learning researchers. Many applications on the rise need\naccurate and efficient segmentation mechanisms: autonomous driving, indoor\nnavigation, and even virtual or augmented reality systems to name a few. This\ndemand coincides with the rise of deep learning approaches in almost every\nfield or application target related to computer vision, including semantic\nsegmentation or scene understanding. This paper provides a review on deep\nlearning methods for semantic segmentation applied to various application\nareas. Firstly, we describe the terminology of this field as well as mandatory\nbackground concepts. Next, the main datasets and challenges are exposed to help\nresearchers decide which are the ones that best suit their needs and their\ntargets. Then, existing methods are reviewed, highlighting their contributions\nand their significance in the field. Finally, quantitative results are given\nfor the described methods and the datasets in which they were evaluated,\nfollowing up with a discussion of the results. At last, we point out a set of\npromising future works and draw our own conclusions about the state of the art\nof semantic segmentation using deep learning techniques. \n\n"}
{"id": "1704.07069", "contents": "Title: Evaluating and Modelling Hanabi-Playing Agents Abstract: Agent modelling involves considering how other agents will behave, in order\nto influence your own actions. In this paper, we explore the use of agent\nmodelling in the hidden-information, collaborative card game Hanabi. We\nimplement a number of rule-based agents, both from the literature and of our\nown devising, in addition to an Information Set Monte Carlo Tree Search\n(IS-MCTS) agent. We observe poor results from IS-MCTS, so construct a new,\npredictor version that uses a model of the agents with which it is paired. We\nobserve a significant improvement in game-playing strength from this agent in\ncomparison to IS-MCTS, resulting from its consideration of what the other\nagents in a game would do. In addition, we create a flawed rule-based agent to\nhighlight the predictor's capabilities with such an agent. \n\n"}
{"id": "1704.07234", "contents": "Title: Scaling Reliably: Improving the Scalability of the Erlang Distributed\n  Actor Platform Abstract: Distributed actor languages are an effective means of constructing scalable\nreliable systems, and the Erlang programming language has a well-established\nand influential model. While Erlang model conceptually provides reliable\nscalability, it has some inherent scalability limits and these force developers\nto depart from the model at scale. This article establishes the scalability\nlimits of Erlang systems, and reports the work to improve the language\nscalability.\n  We systematically study the scalability limits of Erlang and address the\nissues at the virtual machine (VM), language, and tool levels. More\nspecifically: (1) We have evolved the Erlang VM so that it can work effectively\nin large scale single-host multicore and NUMA architectures. We have made\nimportant architectural improvements to the Erlang/OTP. (2) We have designed\nand implemented Scalable Distributed (SD) Erlang libraries to address\nlanguage-level scalability issues, and provided and validated a set of\nsemantics for the new language constructs. (3) To make large Erlang systems\neasier to deploy, monitor, and debug we have developed and made open source\nreleases of five complementary tools, some specific to SD Erlang.\n  Throughout the article we use two case studies to investigate the\ncapabilities of our new technologies and tools: a distributed hash table based\nOrbit calculation and Ant Colony Optimisation (ACO). Chaos Monkey experiments\nshow that two versions of ACO survive random process failure and hence that SD\nErlang preserves the Erlang reliability model. Even for programs with no global\nrecovery data to maintain, SD Erlang partitions the network to reduce network\ntraffic and hence improves performance of the Orbit and ACO benchmarks above 80\nhosts. ACO measurements show that maintaining global recovery data dramatically\nlimits scalability; however scalability is recovered by partitioning the\nrecovery data. \n\n"}
{"id": "1704.07468", "contents": "Title: GaKCo: a Fast GApped k-mer string Kernel using COunting Abstract: String Kernel (SK) techniques, especially those using gapped $k$-mers as\nfeatures (gk), have obtained great success in classifying sequences like DNA,\nprotein, and text. However, the state-of-the-art gk-SK runs extremely slow when\nwe increase the dictionary size ($\\Sigma$) or allow more mismatches ($M$). This\nis because current gk-SK uses a trie-based algorithm to calculate co-occurrence\nof mismatched substrings resulting in a time cost proportional to\n$O(\\Sigma^{M})$. We propose a \\textbf{fast} algorithm for calculating\n\\underline{Ga}pped $k$-mer \\underline{K}ernel using \\underline{Co}unting\n(GaKCo). GaKCo uses associative arrays to calculate the co-occurrence of\nsubstrings using cumulative counting. This algorithm is fast, scalable to\nlarger $\\Sigma$ and $M$, and naturally parallelizable. We provide a rigorous\nasymptotic analysis that compares GaKCo with the state-of-the-art gk-SK.\nTheoretically, the time cost of GaKCo is independent of the $\\Sigma^{M}$ term\nthat slows down the trie-based approach. Experimentally, we observe that GaKCo\nachieves the same accuracy as the state-of-the-art and outperforms its speed by\nfactors of 2, 100, and 4, on classifying sequences of DNA (5 datasets), protein\n(12 datasets), and character-based English text (2 datasets), respectively.\n  GaKCo is shared as an open source tool at\n\\url{https://github.com/QData/GaKCo-SVM} \n\n"}
{"id": "1704.07503", "contents": "Title: Learning of Human-like Algebraic Reasoning Using Deep Feedforward Neural\n  Networks Abstract: There is a wide gap between symbolic reasoning and deep learning. In this\nresearch, we explore the possibility of using deep learning to improve symbolic\nreasoning. Briefly, in a reasoning system, a deep feedforward neural network is\nused to guide rewriting processes after learning from algebraic reasoning\nexamples produced by humans. To enable the neural network to recognise patterns\nof algebraic expressions with non-deterministic sizes, reduced partial trees\nare used to represent the expressions. Also, to represent both top-down and\nbottom-up information of the expressions, a centralisation technique is used to\nimprove the reduced partial trees. Besides, symbolic association vectors and\nrule application records are used to improve the rewriting processes.\nExperimental results reveal that the algebraic reasoning examples can be\naccurately learnt only if the feedforward neural network has enough hidden\nlayers. Also, the centralisation technique, the symbolic association vectors\nand the rule application records can reduce error rates of reasoning. In\nparticular, the above approaches have led to 4.6% error rate of reasoning on a\ndataset of linear equations, differentials and integrals. \n\n"}
{"id": "1704.07555", "contents": "Title: Molecular De Novo Design through Deep Reinforcement Learning Abstract: This work introduces a method to tune a sequence-based generative model for\nmolecular de novo design that through augmented episodic likelihood can learn\nto generate structures with certain specified desirable properties. We\ndemonstrate how this model can execute a range of tasks such as generating\nanalogues to a query structure and generating compounds predicted to be active\nagainst a biological target. As a proof of principle, the model is first\ntrained to generate molecules that do not contain sulphur. As a second example,\nthe model is trained to generate analogues to the drug Celecoxib, a technique\nthat could be used for scaffold hopping or library expansion starting from a\nsingle molecule. Finally, when tuning the model towards generating compounds\npredicted to be active against the dopamine receptor type 2, the model\ngenerates structures of which more than 95% are predicted to be active,\nincluding experimentally confirmed actives that have not been included in\neither the generative model nor the activity prediction model. \n\n"}
{"id": "1704.07751", "contents": "Title: Fine-Grained Entity Typing with High-Multiplicity Assignments Abstract: As entity type systems become richer and more fine-grained, we expect the\nnumber of types assigned to a given entity to increase. However, most\nfine-grained typing work has focused on datasets that exhibit a low degree of\ntype multiplicity. In this paper, we consider the high-multiplicity regime\ninherent in data sources such as Wikipedia that have semi-open type systems. We\nintroduce a set-prediction approach to this problem and show that our model\noutperforms unstructured baselines on a new Wikipedia-based fine-grained typing\ncorpus. \n\n"}
{"id": "1704.07774", "contents": "Title: A Calculus of Truly Concurrent Mobile Processes Abstract: We make a mixture of Milner's $\\pi$-calculus and our previous work on truly\nconcurrent process algebra, which is called $\\pi_{tc}$. We introduce syntax and\nsemantics of $\\pi_{tc}$, its properties based on strongly truly concurrent\nbisimilarities. Also, we include an axiomatization of $\\pi_{tc}$. $\\pi_{tc}$\ncan be used as a formal tool in verifying mobile systems in a truly concurrent\nflavor. \n\n"}
{"id": "1704.07926", "contents": "Title: From Language to Programs: Bridging Reinforcement Learning and Maximum\n  Marginal Likelihood Abstract: Our goal is to learn a semantic parser that maps natural language utterances\ninto executable programs when only indirect supervision is available: examples\nare labeled with the correct execution result, but not the program itself.\nConsequently, we must search the space of programs for those that output the\ncorrect result, while not being misled by spurious programs: incorrect programs\nthat coincidentally output the correct result. We connect two common learning\nparadigms, reinforcement learning (RL) and maximum marginal likelihood (MML),\nand then present a new learning algorithm that combines the strengths of both.\nThe new algorithm guards against spurious programs by combining the systematic\nsearch traditionally employed in MML with the randomized exploration of RL, and\nby updating parameters such that probability is spread more evenly across\nconsistent programs. We apply our learning algorithm to a new neural semantic\nparser and show significant gains over existing state-of-the-art results on a\nrecent context-dependent semantic parsing task. \n\n"}
{"id": "1704.08273", "contents": "Title: Exploring the Performance Benefit of Hybrid Memory System on HPC\n  Environments Abstract: Hardware accelerators have become a de-facto standard to achieve high\nperformance on current supercomputers and there are indications that this trend\nwill increase in the future. Modern accelerators feature high-bandwidth memory\nnext to the computing cores. For example, the Intel Knights Landing (KNL)\nprocessor is equipped with 16 GB of high-bandwidth memory (HBM) that works\ntogether with conventional DRAM memory. Theoretically, HBM can provide 5x\nhigher bandwidth than conventional DRAM. However, many factors impact the\neffective performance achieved by applications, including the application\nmemory access pattern, the problem size, the threading level and the actual\nmemory configuration. In this paper, we analyze the Intel KNL system and\nquantify the impact of the most important factors on the application\nperformance by using a set of applications that are representative of\nscientific and data-analytics workloads. Our results show that applications\nwith regular memory access benefit from MCDRAM, achieving up to 3x performance\nwhen compared to the performance obtained using only DRAM. On the contrary,\napplications with random memory access pattern are latency-bound and may suffer\nfrom performance degradation when using only MCDRAM. For those applications,\nthe use of additional hardware threads may help hide latency and achieve higher\naggregated bandwidth when using HBM. \n\n"}
{"id": "1704.08483", "contents": "Title: No, This is not a Circle Abstract: A popular curve shown in introductory maths textbooks, seems like a circle.\nBut it is actually a different curve. This paper discusses some elementary\napproaches to identify the geometric object, including novel technological\nmeans by using GeoGebra. We demonstrate two ways to refute the false\nimpression, two suggestions to find a correct conjecture, and four ways to\nconfirm the result by proving it rigorously.\n  All of the discussed approaches can be introduced in classrooms at various\nlevels from middle school to high school. \n\n"}
{"id": "1704.08713", "contents": "Title: Finding the Size and the Diameter of a Radio Network Using Short Labels Abstract: The number of nodes of a network, called its size, and the largest distance\nbetween nodes of a network, called its diameter, are among the most important\nnetwork parameters. Knowing the size and/or diameter is a prerequisite of many\ndistributed network algorithms. A radio network is a collection of nodes, with\nwireless transmission and receiving capabilities. It is modeled as a simple\nundirected graph whose nodes communicate in synchronous rounds. In each round,\na node can either transmit a message to all its neighbors, or stay silent and\nlisten. At the receiving end, a node $v$ hears a message from a neighbor $w$ in\na round $i$, if $v$ listens in round $i$, and if $w$ is its only neighbor that\ntransmits in round $i$. If $v$ listens in a round, and multiple neighbors of\n$v$ transmit in this round, a collision occurs at $v$. If $v$ transmits in a\nround, it does not hear anything. If listening nodes can distinguish collision\nfrom silence, we say that the network has collision detection capability,\notherwise there is no collision detection. We consider the tasks of size\ndiscovery and diameter discovery: finding the size (resp. the diameter) of an\nunknown radio network with collision detection. All nodes have to output the\nsize (resp. the diameter) of the network, using a deterministic algorithm.\nNodes have labels which are binary strings. The length of a labeling scheme is\nthe largest length of a label. We concentrate on the following problems:\n  1. What is the shortest labeling scheme that permits size discovery in all\nradio networks of maximum degree $\\Delta$? 2. What is the shortest labeling\nscheme that permits diameter discovery in all radio networks?\n  We show that the minimum length of a labeling scheme that permits size\ndiscovery is $\\Theta(\\log\\log \\Delta)$. By contrast, we show that diameter\ndiscovery can be done using a labeling scheme of constant length. \n\n"}
{"id": "1705.01146", "contents": "Title: Population protocols for leader election and exact majority with O(log^2\n  n) states and O(log^2 n) convergence time Abstract: We consider the model of population protocols, which can be viewed as a\nsequence of random pairwise interactions of $n$ agents (nodes). We show\npopulation protocols for two problems: the leader election and the exact\nmajority voting. The leader election starts with all agents in the same initial\nstate and the goal is to converge to the (global) state when exactly one agent\nis in a distinct state $L$. The exact majority voting starts with each agent in\none of the two distinct states $A$ or $B$ and the goal is to make all nodes\nknow which of these two states was the initial majority state, even if that\nmajority was just by a single vote.\n  Alistarh and Gelashvili [ICALP 2015] showed a leader-election protocol which\nconverges in $O(\\log^3 n)$ time w.h.p. and in expectation and needs\n$\\Theta(\\log^3 n)$ states per agent. We present a protocol which elects the\nleader in $O(\\log^2 n)$ time w.h.p. and in expectation and uses $\\Theta(\\log^2\nn)$ states per agent. For the exact majority voting, we show a population\nprotocol with the same asymptotic performance: $O(\\log^2 n)$ time and\n$\\Theta(\\log^2 n)$ states per agent. The exact-majority protocol proposed by\nAlistarh et al. [PODC 2015] achieves expected $O(\\log^2 n)$ time, but requires\na relatively high initial imbalance between $A$'s and $B$'s or a large number\nof states per agent. More recently, Alistarh et al. [SODA 2017] showed\n$O(\\log^2 n)$-state protocols for both problems, with the exact majority\nprotocol converging in time $O(\\log^3 n)$, and the leader election protocol\nconverging in time $O(\\log^{6.3} n)$ w.h.p. and $O(\\log^{5.3} n)$ in\nexpectation.\n  Our leader election and exact majority protocols are based on the idea of\nagents counting their local interactions and rely on the probabilistic fact\nthat the uniform random selection would limit the divergence of the individual\ncounts. \n\n"}
{"id": "1705.02671", "contents": "Title: Lightweight Robust Framework for Workload Scheduling in Clouds Abstract: Reliability, security and stability of cloud services without sacrificing too\nmuch resources have become a desired feature in the area of workload management\nin clouds. The paper proposes and evaluates a lightweight framework for\nscheduling a workload which part could be unreliable. This unreliability could\nbe caused by various types of failures or attacks. Our framework for robust\nworkload scheduling efficiently combines classic fault-tolerant and security\ntools, such as packet/job scanning, with workload scheduling, and it does not\nuse any heavy resource-consuming tools, e.g., cryptography or non-linear\noptimization. More specifically, the framework uses a novel objective function\nto allocate jobs to servers and constantly decides which job to scan based on a\nformula associated with the objective function. We show how to set up the\nobjective function and the corresponding scanning procedure to make the system\nprovably stable, provided it satisfies a specific stability condition. As a\nresult, we show that our framework assures cloud stability even if naive\nscanning-all and scanning-none strategies are not stable. We extend the\nframework to decentralized scheduling and evaluate it under several popular\nrouting procedures. \n\n"}
{"id": "1705.03427", "contents": "Title: Rapid Mixing of Local Graph Dynamics Abstract: Graph dynamics arise naturally in many contexts. For instance in peer-to-peer\nnetworks, a participating peer may replace an existing connection with one\nneighbour by a new connection with a neighbour's neighbour. Several such local\nrewiring rules have been proposed to ensure that peer-to-peer networks achieve\ngood connectivity properties (e.g. high expansion) in equilibrium. However it\nhas remained an open question whether there existed such rules that also led to\nfast convergence to equilibrium. In this work we provide an affirmative answer:\nWe exhibit a local rewiring rule that converges to equilibrium after each\nparticipating node has undergone only a number of rewirings that is\npoly-logarithmic in the system size. The proof involves consideration of the\nwhole isoperimetric profile of the graph, and may be of independent interest. \n\n"}
{"id": "1705.03876", "contents": "Title: Constant Space and Non-Constant Time in Distributed Computing Abstract: While the relationship of time and space is an established topic in\ntraditional centralised complexity theory, this is not the case in distributed\ncomputing. We aim to remedy this by studying the time and space complexity of\nalgorithms in a weak message-passing model of distributed computing. While a\nconstant number of communication rounds implies a constant number of states\nvisited during the execution, the other direction is not clear at all. We\nconsider several graph families and show that indeed, there exist non-trivial\ngraph problems that are solvable by constant-space algorithms but that require\na non-constant running time. This provides us with a new complexity class for\ndistributed computing and raises interesting questions about the existence of\nfurther combinations of time and space complexity. \n\n"}
{"id": "1705.05394", "contents": "Title: Probabilistically Safe Policy Transfer Abstract: Although learning-based methods have great potential for robotics, one\nconcern is that a robot that updates its parameters might cause large amounts\nof damage before it learns the optimal policy. We formalize the idea of safe\nlearning in a probabilistic sense by defining an optimization problem: we\ndesire to maximize the expected return while keeping the expected damage below\na given safety limit. We study this optimization for the case of a robot\nmanipulator with safety-based torque limits. We would like to ensure that the\ndamage constraint is maintained at every step of the optimization and not just\nat convergence. To achieve this aim, we introduce a novel method which predicts\nhow modifying the torque limit, as well as how updating the policy parameters,\nmight affect the robot's safety. We show through a number of experiments that\nour approach allows the robot to improve its performance while ensuring that\nthe expected damage constraint is not violated during the learning process. \n\n"}
{"id": "1705.05491", "contents": "Title: Distributed Statistical Machine Learning in Adversarial Settings:\n  Byzantine Gradient Descent Abstract: We consider the problem of distributed statistical machine learning in\nadversarial settings, where some unknown and time-varying subset of working\nmachines may be compromised and behave arbitrarily to prevent an accurate model\nfrom being learned. This setting captures the potential adversarial attacks\nfaced by Federated Learning -- a modern machine learning paradigm that is\nproposed by Google researchers and has been intensively studied for ensuring\nuser privacy. Formally, we focus on a distributed system consisting of a\nparameter server and $m$ working machines. Each working machine keeps $N/m$\ndata samples, where $N$ is the total number of samples. The goal is to\ncollectively learn the underlying true model parameter of dimension $d$.\n  In classical batch gradient descent methods, the gradients reported to the\nserver by the working machines are aggregated via simple averaging, which is\nvulnerable to a single Byzantine failure. In this paper, we propose a Byzantine\ngradient descent method based on the geometric median of means of the\ngradients. We show that our method can tolerate $q \\le (m-1)/2$ Byzantine\nfailures, and the parameter estimate converges in $O(\\log N)$ rounds with an\nestimation error of $\\sqrt{d(2q+1)/N}$, hence approaching the optimal error\nrate $\\sqrt{d/N}$ in the centralized and failure-free setting. The total\ncomputational complexity of our algorithm is of $O((Nd/m) \\log N)$ at each\nworking machine and $O(md + kd \\log^3 N)$ at the central server, and the total\ncommunication cost is of $O(m d \\log N)$. We further provide an application of\nour general results to the linear regression problem.\n  A key challenge arises in the above problem is that Byzantine failures create\narbitrary and unspecified dependency among the iterations and the aggregated\ngradients. We prove that the aggregated gradient converges uniformly to the\ntrue gradient function. \n\n"}
{"id": "1705.05595", "contents": "Title: GraphH: High Performance Big Graph Analytics in Small Clusters Abstract: It is common for real-world applications to analyze big graphs using\ndistributed graph processing systems. Popular in-memory systems require an\nenormous amount of resources to handle big graphs. While several out-of-core\napproaches have been proposed for processing big graphs on disk, the high disk\nI/O overhead could significantly reduce performance. In this paper, we propose\nGraphH to enable high-performance big graph analytics in small clusters.\nSpecifically, we design a two-stage graph partition scheme to evenly divide the\ninput graph into partitions, and propose a GAB (Gather-Apply-Broadcast)\ncomputation model to make each worker process a partition in memory at a time.\nWe use an edge cache mechanism to reduce the disk I/O overhead, and design a\nhybrid strategy to improve the communication performance. GraphH can\nefficiently process big graphs in small clusters or even a single commodity\nserver. Extensive evaluations have shown that GraphH could be up to 7.8x faster\ncompared to popular in-memory systems, such as Pregel+ and PowerGraph when\nprocessing generic graphs, and more than 100x faster than recently proposed\nout-of-core systems, such as GraphD and Chaos when processing big graphs. \n\n"}
{"id": "1705.05646", "contents": "Title: Quadratic and Near-Quadratic Lower Bounds for the CONGEST Model Abstract: We present the first super-linear lower bounds for natural graph problems in\nthe CONGEST model, answering a long-standing open question.\n  Specifically, we show that any exact computation of a minimum vertex cover or\na maximum independent set requires $\\Omega(n^2/\\log^2{n})$ rounds in the worst\ncase in the CONGEST model, as well as any algorithm for $\\chi$-coloring a\ngraph, where $\\chi$ is the chromatic number of the graph. We further show that\nsuch strong lower bounds are not limited to NP-hard problems, by showing two\nsimple graph problems in P which require a quadratic and near-quadratic number\nof rounds.\n  Finally, we address the problem of computing an exact solution to weighted\nall-pairs-shortest-paths (APSP), which arguably may be considered as a\ncandidate for having a super-linear lower bound. We show a simple $\\Omega(n)$\nlower bound for this problem, which implies a separation between the weighted\nand unweighted cases, since the latter is known to have a complexity of\n$\\Theta(n/\\log{n})$. We also formally prove that the standard Alice-Bob\nframework is incapable of providing a super-linear lower bound for exact\nweighted APSP, whose complexity remains an intriguing open question. \n\n"}
{"id": "1705.06271", "contents": "Title: Fast Snapshottable Concurrent Braun Heaps Abstract: This paper proposes a new concurrent heap algorithm, based on a stateless\nshape property, which efficiently maintains balance during insert and removeMin\noperations implemented with hand-over-hand locking. It also provides a O(1)\nlinearizable snapshot operation based on lazy copy-on-write semantics. Such\nsnapshots can be used to provide consistent views of the heap during iteration,\nas well as to make speculative updates (which can later be dropped).\n  The simplicity of the algorithm allows it to be easily proven correct, and\nthe choice of shape property provides priority queue performance which is\ncompetitive with highly optimized skiplist implementations (and has stronger\nbounds on worst-case time complexity).\n  A Scala reference implementation is provided. \n\n"}
{"id": "1705.06936", "contents": "Title: Atari games and Intel processors Abstract: The asynchronous nature of the state-of-the-art reinforcement learning\nalgorithms such as the Asynchronous Advantage Actor-Critic algorithm, makes\nthem exceptionally suitable for CPU computations. However, given the fact that\ndeep reinforcement learning often deals with interpreting visual information, a\nlarge part of the train and inference time is spent performing convolutions. In\nthis work we present our results on learning strategies in Atari games using a\nConvolutional Neural Network, the Math Kernel Library and TensorFlow 0.11rc0\nmachine learning framework. We also analyze effects of asynchronous\ncomputations on the convergence of reinforcement learning algorithms. \n\n"}
{"id": "1705.07120", "contents": "Title: VAE with a VampPrior Abstract: Many different methods to train deep generative models have been introduced\nin the past. In this paper, we propose to extend the variational auto-encoder\n(VAE) framework with a new type of prior which we call \"Variational Mixture of\nPosteriors\" prior, or VampPrior for short. The VampPrior consists of a mixture\ndistribution (e.g., a mixture of Gaussians) with components given by\nvariational posteriors conditioned on learnable pseudo-inputs. We further\nextend this prior to a two layer hierarchical model and show that this\narchitecture with a coupled prior and posterior, learns significantly better\nmodels. The model also avoids the usual local optima issues related to useless\nlatent dimensions that plague VAEs. We provide empirical studies on six\ndatasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes,\nFrey Faces and Histopathology patches, and show that applying the hierarchical\nVampPrior delivers state-of-the-art results on all datasets in the unsupervised\npermutation invariant setting and the best results or comparable to SOTA\nmethods for the approach with convolutional networks. \n\n"}
{"id": "1705.07262", "contents": "Title: Batch Reinforcement Learning on the Industrial Benchmark: First\n  Experiences Abstract: The Particle Swarm Optimization Policy (PSO-P) has been recently introduced\nand proven to produce remarkable results on interacting with academic\nreinforcement learning benchmarks in an off-policy, batch-based setting. To\nfurther investigate the properties and feasibility on real-world applications,\nthis paper investigates PSO-P on the so-called Industrial Benchmark (IB), a\nnovel reinforcement learning (RL) benchmark that aims at being realistic by\nincluding a variety of aspects found in industrial applications, like\ncontinuous state and action spaces, a high dimensional, partially observable\nstate space, delayed effects, and complex stochasticity. The experimental\nresults of PSO-P on IB are compared to results of closed-form control policies\nderived from the model-based Recurrent Control Neural Network (RCNN) and the\nmodel-free Neural Fitted Q-Iteration (NFQ). Experiments show that PSO-P is not\nonly of interest for academic benchmarks, but also for real-world industrial\napplications, since it also yielded the best performing policy in our IB\nsetting. Compared to other well established RL techniques, PSO-P produced\noutstanding results in performance and robustness, requiring only a relatively\nlow amount of effort in finding adequate parameters or making complex design\ndecisions. \n\n"}
{"id": "1705.07830", "contents": "Title: Ask the Right Questions: Active Question Reformulation with\n  Reinforcement Learning Abstract: We frame Question Answering (QA) as a Reinforcement Learning task, an\napproach that we call Active Question Answering. We propose an agent that sits\nbetween the user and a black box QA system and learns to reformulate questions\nto elicit the best possible answers. The agent probes the system with,\npotentially many, natural language reformulations of an initial question and\naggregates the returned evidence to yield the best answer. The reformulation\nsystem is trained end-to-end to maximize answer quality using policy gradient.\nWe evaluate on SearchQA, a dataset of complex questions extracted from\nJeopardy!. The agent outperforms a state-of-the-art base model, playing the\nrole of the environment, and other benchmarks. We also analyze the language\nthat the agent has learned while interacting with the question answering\nsystem. We find that successful question reformulations look quite different\nfrom natural language paraphrases. The agent is able to discover non-trivial\nreformulation strategies that resemble classic information retrieval techniques\nsuch as term re-weighting (tf-idf) and stemming. \n\n"}
{"id": "1705.08210", "contents": "Title: Parallel Accelerated Vector Similarity Calculations for Genomics\n  Applications Abstract: The surge in availability of genomic data holds promise for enabling\ndetermination of genetic causes of observed individual traits, with\napplications to problems such as discovery of the genetic roots of phenotypes,\nbe they molecular phenotypes such as gene expression or metabolite\nconcentrations, or complex phenotypes such as diseases. However, the growing\nsizes of these datasets and the quadratic, cubic or higher scaling\ncharacteristics of the relevant algorithms pose a serious computational\nchallenge necessitating use of leadership scale computing. In this paper we\ndescribe a new approach to performing vector similarity metrics calculations,\nsuitable for parallel systems equipped with graphics processing units (GPUs) or\nIntel Xeon Phi processors. Our primary focus is the Proportional Similarity\nmetric applied to Genome Wide Association Studies (GWAS) and Phenome Wide\nAssociation Studies (PheWAS). We describe the implementation of the algorithms\non accelerated processors, methods used for eliminating redundant calculations\ndue to symmetries, and techniques for efficient mapping of the calculations to\nmany-node parallel systems. Results are presented demonstrating high per-node\nperformance and parallel scalability with rates of more than five quadrillion\nelementwise comparisons achieved per second on the ORNL Titan system. In a\ncompanion paper we describe corresponding techniques applied to calculations of\nthe Custom Correlation Coefficient for comparative genomics applications. \n\n"}
{"id": "1705.08584", "contents": "Title: MMD GAN: Towards Deeper Understanding of Moment Matching Network Abstract: Generative moment matching network (GMMN) is a deep generative model that\ndiffers from Generative Adversarial Network (GAN) by replacing the\ndiscriminator in GAN with a two-sample test based on kernel maximum mean\ndiscrepancy (MMD). Although some theoretical guarantees of MMD have been\nstudied, the empirical performance of GMMN is still not as competitive as that\nof GAN on challenging and large benchmark datasets. The computational\nefficiency of GMMN is also less desirable in comparison with GAN, partially due\nto its requirement for a rather large batch size during the training. In this\npaper, we propose to improve both the model expressiveness of GMMN and its\ncomputational efficiency by introducing adversarial kernel learning techniques,\nas the replacement of a fixed Gaussian kernel in the original GMMN. The new\napproach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN.\nThe new distance measure in MMD GAN is a meaningful loss that enjoys the\nadvantage of weak topology and can be optimized via gradient descent with\nrelatively small batch sizes. In our evaluation on multiple benchmark datasets,\nincluding MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN\nsignificantly outperforms GMMN, and is competitive with other representative\nGAN works. \n\n"}
{"id": "1705.09073", "contents": "Title: Load Balancing for Skewed Streams on Heterogeneous Cluster Abstract: Streaming applications frequently encounter skewed workloads and execute on\nheterogeneous clusters. Optimal resource utilization in such adverse conditions\nbecomes a challenge, as it requires inferring the resource capacities and input\ndistribution at run time. In this paper, we tackle the aforementioned\nchallenges by modeling them as a load balancing problem. We propose a novel\npartitioning strategy called Consistent Grouping (CG), which enables each\nprocessing element instance (PEI) to process the workload according to its\ncapacity. The main idea behind CG is the notion of small, equal-sized virtual\nworkers at the sources, which are assigned to workers based on their\ncapacities. We provide a theoretical analysis of the proposed algorithm and\nshow via extensive empirical evaluation that our proposed scheme outperforms\nthe state-of-the-art approaches, like key grouping. In particular, CG achieves\n3.44x better performance in terms of latency compared to key grouping. \n\n"}
{"id": "1705.09783", "contents": "Title: Good Semi-supervised Learning that Requires a Bad GAN Abstract: Semi-supervised learning methods based on generative adversarial networks\n(GANs) obtained strong empirical results, but it is not clear 1) how the\ndiscriminator benefits from joint training with a generator, and 2) why good\nsemi-supervised classification performance and a good generator cannot be\nobtained at the same time. Theoretically, we show that given the discriminator\nobjective, good semisupervised learning indeed requires a bad generator, and\npropose the definition of a preferred generator. Empirically, we derive a novel\nformulation based on our analysis that substantially improves over feature\nmatching GANs, obtaining state-of-the-art results on multiple benchmark\ndatasets. \n\n"}
{"id": "1705.09905", "contents": "Title: A Unified Optimization Approach for Sparse Tensor Operations on GPUs Abstract: Sparse tensors appear in many large-scale applications with multidimensional\nand sparse data. While multidimensional sparse data often need to be processed\non manycore processors, attempts to develop highly-optimized GPU-based\nimplementations of sparse tensor operations are rare. The irregular computation\npatterns and sparsity structures as well as the large memory footprints of\nsparse tensor operations make such implementations challenging. We leverage the\nfact that sparse tensor operations share similar computation patterns to\npropose a unified tensor representation called F-COO. Combined with\nGPU-specific optimizations, F-COO provides highly-optimized implementations of\nsparse tensor computations on GPUs. The performance of the proposed unified\napproach is demonstrated for tensor-based kernels such as the Sparse Matricized\nTensor- Times-Khatri-Rao Product (SpMTTKRP) and the Sparse Tensor- Times-Matrix\nMultiply (SpTTM) and is used in tensor decomposition algorithms. Compared to\nstate-of-the-art work we improve the performance of SpTTM and SpMTTKRP up to\n3.7 and 30.6 times respectively on NVIDIA Titan-X GPUs. We implement a\nCANDECOMP/PARAFAC (CP) decomposition and achieve up to 14.9 times speedup using\nthe unified method over state-of-the-art libraries on NVIDIA Titan-X GPUs. \n\n"}
{"id": "1705.10195", "contents": "Title: Deterministic subgraph detection in broadcast CONGEST Abstract: We present simple deterministic algorithms for subgraph finding and\nenumeration in the broadcast CONGEST model of distributed computation:\n  -- For any constant $k$, detecting $k$-paths and trees on $k$ nodes can be\ndone in $O(1)$ rounds.\n  -- For any constant $k$, detecting $k$-cycles and pseudotrees on $k$ nodes\ncan be done in $O(n)$ rounds.\n  -- On $d$-degenerate graphs, cliques and $4$-cycles can be enumerated in $O(d\n+ \\log n)$ rounds, and $5$-cycles in $O(d^2 + \\log n)$ rounds.\n  In many cases, these bounds are tight up to logarithmic factors. Moreover, we\nshow that the algorithms for $d$-degenerate graphs can be improved to optimal\ncomplexity $O(d/\\log n)$ and $O(d^2/\\log n)$, respectively, in the supported\nCONGEST model, which can be seen as an intermediate model between CONGEST and\nthe congested clique. \n\n"}
{"id": "1705.10201", "contents": "Title: Machine Learned Learning Machines Abstract: There are two common approaches for optimizing the performance of a machine:\ngenetic algorithms and machine learning. A genetic algorithm is applied over\nmany generations whereas machine learning works by applying feedback until the\nsystem meets a performance threshold. Though these are methods that typically\noperate separately, we combine evolutionary adaptation and machine learning\ninto one approach. Our focus is on machines that can learn during their\nlifetime, but instead of equipping them with a machine learning algorithm we\naim to let them evolve their ability to learn by themselves. We use evolvable\nnetworks of probabilistic and deterministic logic gates, known as Markov\nBrains, as our computational model organism. The ability of Markov Brains to\nlearn is augmented by a novel adaptive component that can change its\ncomputational behavior based on feedback. We show that Markov Brains can indeed\nevolve to incorporate these feedback gates to improve their adaptability to\nvariable environments. By combining these two methods, we now also implemented\na computational model that can be used to study the evolution of learning. \n\n"}
{"id": "1705.10464", "contents": "Title: Polynomial Codes: an Optimal Design for High-Dimensional Coded Matrix\n  Multiplication Abstract: We consider a large-scale matrix multiplication problem where the computation\nis carried out using a distributed system with a master node and multiple\nworker nodes, where each worker can store parts of the input matrices. We\npropose a computation strategy that leverages ideas from coding theory to\ndesign intermediate computations at the worker nodes, in order to efficiently\ndeal with straggling workers. The proposed strategy, named as \\emph{polynomial\ncodes}, achieves the optimum recovery threshold, defined as the minimum number\nof workers that the master needs to wait for in order to compute the output.\nFurthermore, by leveraging the algebraic structure of polynomial codes, we can\nmap the reconstruction problem of the final output to a polynomial\ninterpolation problem, which can be solved efficiently. Polynomial codes\nprovide order-wise improvement over the state of the art in terms of recovery\nthreshold, and are also optimal in terms of several other metrics. Furthermore,\nwe extend this code to distributed convolution and show its order-wise\noptimality. \n\n"}
{"id": "1705.11097", "contents": "Title: A Logic for Non-Deterministic Parallel Abstract State Machines Abstract: We develop a logic which enables reasoning about single steps of\nnon-deterministic parallel Abstract State Machines (ASMs). Our logic builds\nupon the unifying logic introduced by Nanchen and St\\\"ark for reasoning about\nhierarchical (parallel) ASMs. Our main contribution to this regard is the\nhandling of non-determinism (both bounded and unbounded) within the logical\nformalism. Moreover, we do this without sacrificing the completeness of the\nlogic for statements about single steps of non-deterministic parallel ASMs,\nsuch as invariants of rules, consistency conditions for rules, or step-by-step\nequivalence of rules. \n\n"}
{"id": "1706.00035", "contents": "Title: Sequoidal Categories and Transfinite Games: A Coalgebraic Approach to\n  Stateful Objects in Game Semantics Abstract: The non-commutative sequoid operator $\\oslash$ on games was introduced to\ncapture algebraically the presence of state in history-sensitive strategies in\ngame semantics, by imposing a causality relation on the tensor product of\ngames. Coalgebras for the functor $A \\oslash \\_$ - i.e. morphisms from $S$ to\n$A \\oslash S$ - may be viewed as state transformers: if $A \\oslash \\_$ has a\nfinal coalgebra, $!A$, then the anamorphism of such a state transformer\nencapsulates its explicit state, so that it is shared only between successive\ninvocations.\n  We study the conditions under which a final coalgebra $!A$ for $A \\oslash \\_$\nis the carrier of a cofree commutative comonoid on $A$. That is, it is a model\nof the exponential of linear logic in which we can construct imperative objects\nsuch as reference cells coalgebraically, in a game semantics setting. We show\nthat if the tensor decomposes into the sequoid, the final coalgebra $!A$ may be\nendowed with the structure of the cofree commutative comonoid if there is a\nnatural isomorphism from $!(A \\times B)$ to $!A \\otimes !B$. This condition is\nalways satisfied if $!A$ is the bifree algebra for $A \\oslash \\_$, but in\ngeneral it is necessary to impose it, as we establish by giving an example of a\nsequoidally decomposable category of games in which plays will be allowed to\nhave transfinite length. In this category, the final coalgebra for the functor\n$A \\oslash \\_$ is not the cofree commutative comonoid over A: we illustrate\nthis by explicitly contrasting the final sequence for the functor $A \\oslash\n\\_$ with the chain of symmetric tensor powers used in the construction of the\ncofree commutative comonoid as a limit by Melli\\'es, Tabareau and Tasson. \n\n"}
{"id": "1706.00095", "contents": "Title: Using GPI-2 for Distributed Memory Paralleliziation of the Caffe Toolbox\n  to Speed up Deep Neural Network Training Abstract: Deep Neural Network (DNN) are currently of great inter- est in research and\napplication. The training of these net- works is a compute intensive and time\nconsuming task. To reduce training times to a bearable amount at reasonable\ncost we extend the popular Caffe toolbox for DNN with an efficient distributed\nmemory communication pattern. To achieve good scalability we emphasize the\noverlap of computation and communication and prefer fine granu- lar\nsynchronization patterns over global barriers. To im- plement these\ncommunication patterns we rely on the the Global address space Programming\nInterface version 2 (GPI-2) communication library. This interface provides a\nlight-weight set of asynchronous one-sided communica- tion primitives\nsupplemented by non-blocking fine gran- ular data synchronization mechanisms.\nTherefore, Caf- feGPI is the name of our parallel version of Caffe. First\nbenchmarks demonstrate better scaling behavior com- pared with other\nextensions, e.g., the Intel TM Caffe. Even within a single symmetric\nmultiprocessing machine with four graphics processing units, the CaffeGPI\nscales bet- ter than the standard Caffe toolbox. These first results\ndemonstrate that the use of standard High Performance Computing (HPC) hardware\nis a valid cost saving ap- proach to train large DDNs. I/O is an other\nbottleneck to work with DDNs in a standard parallel HPC setting, which we will\nconsider in more detail in a forthcoming paper. \n\n"}
{"id": "1706.00130", "contents": "Title: Teaching Machines to Describe Images via Natural Language Feedback Abstract: Robots will eventually be part of every household. It is thus critical to\nenable algorithms to learn from and be guided by non-expert users. In this\npaper, we bring a human in the loop, and enable a human teacher to give\nfeedback to a learning agent in the form of natural language. We argue that a\ndescriptive sentence can provide a much stronger learning signal than a numeric\nreward in that it can easily point to where the mistakes are and how to correct\nthem. We focus on the problem of image captioning in which the quality of the\noutput can easily be judged by non-experts. We propose a hierarchical\nphrase-based captioning model trained with policy gradients, and design a\nfeedback network that provides reward to the learner by conditioning on the\nhuman-provided feedback. We show that by exploiting descriptive feedback our\nmodel learns to perform better than when given independently written human\ncaptions. \n\n"}
{"id": "1706.00359", "contents": "Title: Discovering Discrete Latent Topics with Neural Variational Inference Abstract: Topic models have been widely explored as probabilistic generative models of\ndocuments. Traditional inference methods have sought closed-form derivations\nfor updating the models, however as the expressiveness of these models grows,\nso does the difficulty of performing fast and accurate inference over their\nparameters. This paper presents alternative neural approaches to topic\nmodelling by providing parameterisable distributions over topics which permit\ntraining by backpropagation in the framework of neural variational inference.\nIn addition, with the help of a stick-breaking construction, we propose a\nrecurrent network that is able to discover a notionally unbounded number of\ntopics, analogous to Bayesian non-parametric topic models. Experimental results\non the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the\neffectiveness and efficiency of these neural topic models. \n\n"}
{"id": "1706.02179", "contents": "Title: Learning to Represent Mechanics via Long-term Extrapolation and\n  Interpolation Abstract: While the basic laws of Newtonian mechanics are well understood, explaining a\nphysical scenario still requires manually modeling the problem with suitable\nequations and associated parameters. In order to adopt such models for\nartificial intelligence, researchers have handcrafted the relevant states, and\nthen used neural networks to learn the state transitions using simulation runs\nas training data. Unfortunately, such approaches can be unsuitable for modeling\ncomplex real-world scenarios, where manually authoring relevant state spaces\ntend to be challenging. In this work, we investigate if neural networks can\nimplicitly learn physical states of real-world mechanical processes only based\non visual data, and thus enable long-term physical extrapolation. We develop a\nrecurrent neural network architecture for this task and also characterize\nresultant uncertainties in the form of evolving variance estimates. We evaluate\nour setup to extrapolate motion of a rolling ball on bowl of varying shape and\norientation using only images as input, and report competitive results with\napproaches that assume access to internal physics models and parameters. \n\n"}
{"id": "1706.02677", "contents": "Title: Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour Abstract: Deep learning thrives with large neural networks and large datasets. However,\nlarger networks and larger datasets result in longer training times that impede\nresearch and development progress. Distributed synchronous SGD offers a\npotential solution to this problem by dividing SGD minibatches over a pool of\nparallel workers. Yet to make this scheme efficient, the per-worker workload\nmust be large, which implies nontrivial growth in the SGD minibatch size. In\nthis paper, we empirically show that on the ImageNet dataset large minibatches\ncause optimization difficulties, but when these are addressed the trained\nnetworks exhibit good generalization. Specifically, we show no loss of accuracy\nwhen training with large minibatch sizes up to 8192 images. To achieve this\nresult, we adopt a hyper-parameter-free linear scaling rule for adjusting\nlearning rates as a function of minibatch size and develop a new warmup scheme\nthat overcomes optimization challenges early in training. With these simple\ntechniques, our Caffe2-based system trains ResNet-50 with a minibatch size of\n8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using\ncommodity hardware, our implementation achieves ~90% scaling efficiency when\nmoving from 8 to 256 GPUs. Our findings enable training visual recognition\nmodels on internet-scale data with high efficiency. \n\n"}
{"id": "1706.03416", "contents": "Title: Learning Large-Scale Topological Maps Using Sum-Product Networks Abstract: In order to perform complex actions in human environments, an autonomous\nrobot needs the ability to understand the environment, that is, to gather and\nmaintain spatial knowledge. Topological map is commonly used for representing\nlarge scale, global maps such as floor plans. Although much work has been done\nin topological map extraction, we have found little previous work on the\nproblem of learning the topological map using a probabilistic model. Learning a\ntopological map means learning the structure of the large-scale space and\ndependency between places, for example, how the evidence of a group of places\ninfluence the attributes of other places. This is an important step towards\nplanning complex actions in the environment. In this thesis, we consider the\nproblem of using probabilistic deep learning model to learn the topological\nmap, which is essentially a sparse undirected graph where nodes represent\nplaces annotated with their semantic attributes (e.g. place category). We\npropose to use a novel probabilistic deep model, Sum-Product Networks (SPNs),\ndue to their unique properties. We present two methods for learning topological\nmaps using SPNs: the place grid method and the template-based method. We\ncontribute an algorithm that builds SPNs for graphs using template models. Our\nexperiments evaluate the ability of our models to enable robots to infer\nsemantic attributes and detect maps with novel semantic attribute arrangements.\nOur results demonstrate their understanding of the topological map structure\nand spatial relations between places. \n\n"}
{"id": "1706.03741", "contents": "Title: Deep reinforcement learning from human preferences Abstract: For sophisticated reinforcement learning (RL) systems to interact usefully\nwith real-world environments, we need to communicate complex goals to these\nsystems. In this work, we explore goals defined in terms of (non-expert) human\npreferences between pairs of trajectory segments. We show that this approach\ncan effectively solve complex RL tasks without access to the reward function,\nincluding Atari games and simulated robot locomotion, while providing feedback\non less than one percent of our agent's interactions with the environment. This\nreduces the cost of human oversight far enough that it can be practically\napplied to state-of-the-art RL systems. To demonstrate the flexibility of our\napproach, we show that we can successfully train complex novel behaviors with\nabout an hour of human time. These behaviors and environments are considerably\nmore complex than any that have been previously learned from human feedback. \n\n"}
{"id": "1706.04052", "contents": "Title: Beyond Monte Carlo Tree Search: Playing Go with Deep Alternative Neural\n  Network and Long-Term Evaluation Abstract: Monte Carlo tree search (MCTS) is extremely popular in computer Go which\ndetermines each action by enormous simulations in a broad and deep search tree.\nHowever, human experts select most actions by pattern analysis and careful\nevaluation rather than brute search of millions of future nteractions. In this\npaper, we propose a computer Go system that follows experts way of thinking and\nplaying. Our system consists of two parts. The first part is a novel deep\nalternative neural network (DANN) used to generate candidates of next move.\nCompared with existing deep convolutional neural network (DCNN), DANN inserts\nrecurrent layer after each convolutional layer and stacks them in an\nalternative manner. We show such setting can preserve more contexts of local\nfeatures and its evolutions which are beneficial for move prediction. The\nsecond part is a long-term evaluation (LTE) module used to provide a reliable\nevaluation of candidates rather than a single probability from move predictor.\nThis is consistent with human experts nature of playing since they can foresee\ntens of steps to give an accurate estimation of candidates. In our system, for\neach candidate, LTE calculates a cumulative reward after several future\ninteractions when local variations are settled. Combining criteria from the two\nparts, our system determines the optimal choice of next move. For more\ncomprehensive experiments, we introduce a new professional Go dataset (PGD),\nconsisting of 253233 professional records. Experiments on GoGoD and PGD\ndatasets show the DANN can substantially improve performance of move prediction\nover pure DCNN. When combining LTE, our system outperforms most relevant\napproaches and open engines based on MCTS. \n\n"}
{"id": "1706.04235", "contents": "Title: A Hybrid Observer for a Distributed Linear System with a Changing\n  Neighbor Graph Abstract: A hybrid observer is described for estimating the state of an $m>0$ channel,\n$n$-dimensional, continuous-time, distributed linear system of the form\n$\\dot{x} = Ax,\\;y_i = C_ix,\\;i\\in\\{1,2,\\ldots, m\\}$. The system's state $x$ is\nsimultaneously estimated by $m$ agents assuming each agent $i$ senses $y_i$ and\nreceives appropriately defined data from each of its current neighbors.\nNeighbor relations are characterized by a time-varying directed graph\n$\\mathbb{N}(t)$ whose vertices correspond to agents and whose arcs depict\nneighbor relations. Agent $i$ updates its estimate $x_i$ of $x$ at \"event\ntimes\" $t_1,t_2,\\ldots $ using a local observer and a local parameter\nestimator. The local observer is a continuous time linear system whose input is\n$y_i$ and whose output $w_i$ is an asymptotically correct estimate of $L_ix$\nwhere $L_i$ a matrix with kernel equaling the unobservable space of $(C_i,A)$.\nThe local parameter estimator is a recursive algorithm designed to estimate,\nprior to each event time $t_j$, a constant parameter $p_j$ which satisfies the\nlinear equations $w_k(t_j-\\tau) =\nL_kp_j+\\mu_k(t_j-\\tau),\\;k\\in\\{1,2,\\ldots,m\\}$, where $\\tau$ is a small\npositive constant and $\\mu_k$ is the state estimation error of local observer\n$k$. Agent $i$ accomplishes this by iterating its parameter estimator state\n$z_i$, $q$ times within the interval $[t_j-\\tau, t_j)$, and by making use of\nthe state of each of its neighbors' parameter estimators at each iteration. The\nupdated value of $x_i$ at event time $t_j$ is then $x_i(t_j) =\ne^{A\\tau}z_i(q)$. Subject to the assumptions that (i) the neighbor graph\n$\\mathbb{N}(t)$ is strongly connected for all time, (ii) the system whose state\nis to be estimated is jointly observable, (iii) $q$ is sufficiently large, it\nis shown that each estimate $x_i$ converges to $x$ exponentially fast as\n$t\\rightarrow \\infty$ at a rate which can be controlled. \n\n"}
{"id": "1706.04582", "contents": "Title: Existence versus Exploitation: The Opacity of Backbones and Backdoors\n  Under a Weak Assumption Abstract: Backdoors and backbones of Boolean formulas are hidden structural properties.\nA natural goal, already in part realized, is that solver algorithms seek to\nobtain substantially better performance by exploiting these structures.\n  However, the present paper is not intended to improve the performance of SAT\nsolvers, but rather is a cautionary paper. In particular, the theme of this\npaper is that there is a potential chasm between the existence of such\nstructures in the Boolean formula and being able to effectively exploit them.\nThis does not mean that these structures are not useful to solvers. It does\nmean that one must be very careful not to assume that it is computationally\neasy to go from the existence of a structure to being able to get one's hands\non it and/or being able to exploit the structure.\n  For example, in this paper we show that, under the assumption that P $\\neq$\nNP, there are easily recognizable families of Boolean formulas with strong\nbackdoors that are easy to find, yet for which it is hard (in fact,\nNP-complete) to determine whether the formulas are satisfiable. We also show\nthat, also under the assumption P $\\neq$ NP, there are easily recognizable sets\nof Boolean formulas for which it is hard (in fact, NP-complete) to determine\nwhether they have a large backbone. \n\n"}
{"id": "1706.04889", "contents": "Title: Improved Set-based Symbolic Algorithms for Parity Games Abstract: Graph games with {\\omega}-regular winning conditions provide a mathematical\nframework to analyze a wide range of problems in the analysis of reactive\nsystems and programs (such as the synthesis of reactive systems, program\nrepair, and the verification of branching time properties). Parity conditions\nare canonical forms to specify {\\omega}-regular winning conditions. Graph games\nwith parity conditions are equivalent to {\\mu}-calculus model checking, and\nthus a very important algorithmic problem. Symbolic algorithms are of great\nsignificance because they provide scalable algorithms for the analysis of large\nfinite-state systems, as well as algorithms for the analysis of infinite-state\nsystems with finite quotient. A set-based symbolic algorithm uses the basic set\noperations and the one-step predecessor operators. We consider graph games with\n$n$ vertices and parity conditions with $c$ priorities. While many explicit\nalgorithms exist for graph games with parity conditions, for set-based symbolic\nalgorithms there are only two algorithms (notice that we use space to refer to\nthe number of sets stored by a symbolic algorithm): (a) the basic algorithm\nthat requires $O(n^c)$ symbolic operations and linear space; and (b) an\nimproved algorithm that requires $O(n^{c/2+1})$ symbolic operations but also\n$O(n^{c/2+1})$ space (i.e., exponential space). In this work we present two\nset-based symbolic algorithms for parity games: (a) our first algorithm\nrequires $O(n^{c/2+1})$ symbolic operations and only requires linear space; and\n(b) developing on our first algorithm, we present an algorithm that requires\n$O(n^{c/3+1})$ symbolic operations and only linear space. We also present the\nfirst linear space set-based symbolic algorithm for parity games that requires\nat most a sub-exponential number of symbolic operations. \n\n"}
{"id": "1706.06060", "contents": "Title: Consistent feature attribution for tree ensembles Abstract: Note that a newer expanded version of this paper is now available at:\narXiv:1802.03888\n  It is critical in many applications to understand what features are important\nfor a model, and why individual predictions were made. For tree ensemble\nmethods these questions are usually answered by attributing importance values\nto input features, either globally or for a single prediction. Here we show\nthat current feature attribution methods are inconsistent, which means changing\nthe model to rely more on a given feature can actually decrease the importance\nassigned to that feature. To address this problem we develop fast exact\nsolutions for SHAP (SHapley Additive exPlanation) values, which were recently\nshown to be the unique additive feature attribution method based on conditional\nexpectations that is both consistent and locally accurate. We integrate these\nimprovements into the latest version of XGBoost, demonstrate the\ninconsistencies of current methods, and show how using SHAP values results in\nsignificantly improved supervised clustering performance. Feature importance\nvalues are a key part of understanding widely used models such as gradient\nboosting trees and random forests, so improvements to them have broad practical\nimplications. \n\n"}
{"id": "1706.06663", "contents": "Title: Grilliot's trick in Nonstandard Analysis Abstract: The technique known as Grilliot's trick constitutes a template for explicitly\ndefining the Turing jump functional $(\\exists^2)$ in terms of a given\neffectively discontinuous type two functional. In this paper, we discuss the\nstandard extensionality trick: a technique similar to Grilliot's trick in\nNonstandard Analysis. This nonstandard trick proceeds by deriving from the\nexistence of certain nonstandard discontinuous functionals, the Transfer\nprinciple from Nonstandard analysis limited to $\\Pi_1^0$-formulas; from this\n(generally ineffective) implication, we obtain an effective implication\nexpressing the Turing jump functional in terms of a discontinuous functional\n(and no longer involving Nonstandard Analysis). The advantage of our\nnonstandard approach is that one obtains effective content without paying\nattention to effective content. We also discuss a new class of functionals\nwhich all seem to fall outside the established categories. These functionals\ndirectly derive from the Standard Part axiom of Nonstandard Analysis. \n\n"}
{"id": "1706.07206", "contents": "Title: Explaining Recurrent Neural Network Predictions in Sentiment Analysis Abstract: Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown\nto deliver insightful explanations in the form of input space relevances for\nunderstanding feed-forward neural network classification decisions. In the\npresent work, we extend the usage of LRP to recurrent neural networks. We\npropose a specific propagation rule applicable to multiplicative connections as\nthey arise in recurrent network architectures such as LSTMs and GRUs. We apply\nour technique to a word-based bi-directional LSTM model on a five-class\nsentiment prediction task, and evaluate the resulting LRP relevances both\nqualitatively and quantitatively, obtaining better results than a\ngradient-based related method which was used in previous work. \n\n"}
{"id": "1706.07351", "contents": "Title: An approach to reachability analysis for feed-forward ReLU neural\n  networks Abstract: We study the reachability problem for systems implemented as feed-forward\nneural networks whose activation function is implemented via ReLU functions. We\ndraw a correspondence between establishing whether some arbitrary output can\never be outputed by a neural system and linear problems characterising a neural\nsystem of interest. We present a methodology to solve cases of practical\ninterest by means of a state-of-the-art linear programs solver. We evaluate the\ntechnique presented by discussing the experimental results obtained by\nanalysing reachability properties for a number of benchmarks in the literature. \n\n"}
{"id": "1706.07881", "contents": "Title: On Sampling Strategies for Neural Network-based Collaborative Filtering Abstract: Recent advances in neural networks have inspired people to design hybrid\nrecommendation algorithms that can incorporate both (1) user-item interaction\ninformation and (2) content information including image, audio, and text.\nDespite their promising results, neural network-based recommendation algorithms\npose extensive computational costs, making it challenging to scale and improve\nupon. In this paper, we propose a general neural network-based recommendation\nframework, which subsumes several existing state-of-the-art recommendation\nalgorithms, and address the efficiency issue by investigating sampling\nstrategies in the stochastic gradient descent training for the framework. We\ntackle this issue by first establishing a connection between the loss functions\nand the user-item interaction bipartite graph, where the loss function terms\nare defined on links while major computation burdens are located at nodes. We\ncall this type of loss functions \"graph-based\" loss functions, for which varied\nmini-batch sampling strategies can have different computational costs. Based on\nthe insight, three novel sampling strategies are proposed, which can\nsignificantly improve the training efficiency of the proposed framework (up to\n$\\times 30$ times speedup in our experiments), as well as improving the\nrecommendation performance. Theoretical analysis is also provided for both the\ncomputational cost and the convergence. We believe the study of sampling\nstrategies have further implications on general graph-based loss functions, and\nwould also enable more research under the neural network-based recommendation\nframework. \n\n"}
{"id": "1706.08146", "contents": "Title: Compressed Factorization: Fast and Accurate Low-Rank Factorization of\n  Compressively-Sensed Data Abstract: What learning algorithms can be run directly on compressively-sensed data? In\nthis work, we consider the question of accurately and efficiently computing\nlow-rank matrix or tensor factorizations given data compressed via random\nprojections. We examine the approach of first performing factorization in the\ncompressed domain, and then reconstructing the original high-dimensional\nfactors from the recovered (compressed) factors. In both the matrix and tensor\nsettings, we establish conditions under which this natural approach will\nprovably recover the original factors. While it is well-known that random\nprojections preserve a number of geometric properties of a dataset, our work\ncan be viewed as showing that they can also preserve certain solutions of\nnon-convex, NP-Hard problems like non-negative matrix factorization. We support\nthese theoretical results with experiments on synthetic data and demonstrate\nthe practical applicability of compressed factorization on real-world gene\nexpression and EEG time series datasets. \n\n"}
{"id": "1707.01873", "contents": "Title: Blockchain Consensus Protocols in the Wild Abstract: A blockchain is a distributed ledger for recording transactions, maintained\nby many nodes without central authority through a distributed cryptographic\nprotocol. All nodes validate the information to be appended to the blockchain,\nand a consensus protocol ensures that the nodes agree on a unique order in\nwhich entries are appended. Consensus protocols for tolerating Byzantine faults\nhave received renewed attention because they also address blockchain systems.\nThis work discusses the process of assessing and gaining confidence in the\nresilience of a consensus protocols exposed to faults and adversarial nodes. We\nadvocate to follow the established practice in cryptography and computer\nsecurity, relying on public reviews, detailed models, and formal proofs; the\ndesigners of several practical systems appear to be unaware of this. Moreover,\nwe review the consensus protocols in some prominent permissioned blockchain\nplatforms with respect to their fault models and resilience against attacks.\nThe protocol comparison covers Hyperledger Fabric, Tendermint, Symbiont,\nR3~Corda, Iroha, Kadena, Chain, Quorum, MultiChain, Sawtooth Lake, Ripple,\nStellar, and IOTA. \n\n"}
{"id": "1707.02260", "contents": "Title: Fair Personalization Abstract: Personalization is pervasive in the online space as, when combined with\nlearning, it leads to higher efficiency and revenue by allowing the most\nrelevant content to be served to each user. However, recent studies suggest\nthat such personalization can propagate societal or systemic biases, which has\nled to calls for regulatory mechanisms and algorithms to combat inequality.\nHere we propose a rigorous algorithmic framework that allows for the\npossibility to control biased or discriminatory personalization with respect to\nsensitive attributes of users without losing all of the benefits of\npersonalization. \n\n"}
{"id": "1707.03300", "contents": "Title: The Intentional Unintentional Agent: Learning to Solve Many Continuous\n  Control Tasks Simultaneously Abstract: This paper introduces the Intentional Unintentional (IU) agent. This agent\nendows the deep deterministic policy gradients (DDPG) agent for continuous\ncontrol with the ability to solve several tasks simultaneously. Learning to\nsolve many tasks simultaneously has been a long-standing, core goal of\nartificial intelligence, inspired by infant development and motivated by the\ndesire to build flexible robot manipulators capable of many diverse behaviours.\nWe show that the IU agent not only learns to solve many tasks simultaneously\nbut it also learns faster than agents that target a single task at-a-time. In\nsome cases, where the single task DDPG method completely fails, the IU agent\nsuccessfully solves the task. To demonstrate this, we build a playroom\nenvironment using the MuJoCo physics engine, and introduce a grounded formal\nlanguage to automatically generate tasks. \n\n"}
{"id": "1707.06403", "contents": "Title: Improved Cloud resource allocation: how INDIGO-DataCloud is overcoming\n  the current limitations in Cloud schedulers Abstract: Performing efficient resource provisioning is a fundamental aspect for any\nresource provider. Local Resource Management Systems (LRMS) have been used in\ndata centers for decades in order to obtain the best usage of the resources,\nproviding their fair usage and partitioning for the users. In contrast, current\ncloud schedulers are normally based on the immediate allocation of resources on\na first-come, first-served basis, meaning that a request will fail if there are\nno resources (e.g. OpenStack) or it will be trivially queued ordered by entry\ntime (e.g. OpenNebula). Moreover, these scheduling strategies are based on a\nstatic partitioning of the resources, meaning that existing quotas cannot be\nexceeded, even if there are idle resources allocated to other projects. This is\na consequence of the fact that cloud instances are not associated with a\nmaximum execution time and leads to a situation where the resources are\nunder-utilized. These facts have been identified by the INDIGO-DataCloud\nproject as being too simplistic for accommodating scientific workloads in an\nefficient way, leading to an underutilization of the resources, a non desirable\nsituation in scientific data centers. In this work, we will present the work\ndone in the scheduling area during the first year of the INDIGO project and the\nforeseen evolutions. \n\n"}
{"id": "1707.07339", "contents": "Title: Finite Inverse Categories as Signatures Abstract: We define a simple dependent type theory and prove that its well-formed types\ncorrespond exactly to finite inverse categories. \n\n"}
{"id": "1707.07397", "contents": "Title: Synthesizing Robust Adversarial Examples Abstract: Standard methods for generating adversarial examples for neural networks do\nnot consistently fool neural network classifiers in the physical world due to a\ncombination of viewpoint shifts, camera noise, and other natural\ntransformations, limiting their relevance to real-world systems. We demonstrate\nthe existence of robust 3D adversarial objects, and we present the first\nalgorithm for synthesizing examples that are adversarial over a chosen\ndistribution of transformations. We synthesize two-dimensional adversarial\nimages that are robust to noise, distortion, and affine transformation. We\napply our algorithm to complex three-dimensional objects, using 3D-printing to\nmanufacture the first physical adversarial objects. Our results demonstrate the\nexistence of 3D adversarial objects in the physical world. \n\n"}
{"id": "1707.07659", "contents": "Title: An Improved Approximate Consensus Algorithm in the Presence of Mobile\n  Faults Abstract: This paper explores the problem of reaching approximate consensus in\nsynchronous point-to-point networks, where each pair of nodes is able to\ncommunicate with each other directly and reliably. We consider the mobile\nByzantine fault model proposed by Garay '94 -- in the model, an omniscient\nadversary can corrupt up to $f$ nodes in each round, and at the beginning of\neach round, faults may \"move\" in the system (i.e., different sets of nodes may\nbecome faulty in different rounds). Recent work by Bonomi et al. '16 proposed a\nsimple iterative approximate consensus algorithm which requires at least $4f+1$\nnodes. This paper proposes a novel technique of using \"confession\" (a mechanism\nto allow others to ignore past behavior) and a variant of reliable broadcast to\nimprove the fault-tolerance level. In particular, we present an approximate\nconsensus algorithm that requires only $\\lceil 7f/2\\rceil + 1$ nodes, an\n$\\lfloor f/2 \\rfloor$ improvement over the state-of-the-art algorithms.\nMoreover, we also show that the proposed algorithm is optimal within a family\nof round-based algorithms. \n\n"}
{"id": "1707.08167", "contents": "Title: On The Robustness of a Neural Network Abstract: With the development of neural networks based machine learning and their\nusage in mission critical applications, voices are rising against the\n\\textit{black box} aspect of neural networks as it becomes crucial to\nunderstand their limits and capabilities. With the rise of neuromorphic\nhardware, it is even more critical to understand how a neural network, as a\ndistributed system, tolerates the failures of its computing nodes, neurons, and\nits communication channels, synapses. Experimentally assessing the robustness\nof neural networks involves the quixotic venture of testing all the possible\nfailures, on all the possible inputs, which ultimately hits a combinatorial\nexplosion for the first, and the impossibility to gather all the possible\ninputs for the second.\n  In this paper, we prove an upper bound on the expected error of the output\nwhen a subset of neurons crashes. This bound involves dependencies on the\nnetwork parameters that can be seen as being too pessimistic in the average\ncase. It involves a polynomial dependency on the Lipschitz coefficient of the\nneurons activation function, and an exponential dependency on the depth of the\nlayer where a failure occurs. We back up our theoretical results with\nexperiments illustrating the extent to which our prediction matches the\ndependencies between the network parameters and robustness. Our results show\nthat the robustness of neural networks to the average crash can be estimated\nwithout the need to neither test the network on all failure configurations, nor\naccess the training set used to train the network, both of which are\npractically impossible requirements. \n\n"}
{"id": "1708.00807", "contents": "Title: Adversarial-Playground: A Visualization Suite Showing How Adversarial\n  Examples Fool Deep Learning Abstract: Recent studies have shown that attackers can force deep learning models to\nmisclassify so-called \"adversarial examples\": maliciously generated images\nformed by making imperceptible modifications to pixel values. With growing\ninterest in deep learning for security applications, it is important for\nsecurity experts and users of machine learning to recognize how learning\nsystems may be attacked. Due to the complex nature of deep learning, it is\nchallenging to understand how deep models can be fooled by adversarial\nexamples. Thus, we present a web-based visualization tool,\nAdversarial-Playground, to demonstrate the efficacy of common adversarial\nmethods against a convolutional neural network (CNN) system.\nAdversarial-Playground is educational, modular and interactive. (1) It enables\nnon-experts to compare examples visually and to understand why an adversarial\nexample can fool a CNN-based image classifier. (2) It can help security experts\nexplore more vulnerability of deep learning as a software module. (3) Building\nan interactive visualization is challenging in this domain due to the large\nfeature space of image classification (generating adversarial examples is slow\nin general and visualizing images are costly). Through multiple novel design\nchoices, our tool can provide fast and accurate responses to user requests.\nEmpirically, we find that our client-server division strategy reduced the\nresponse time by an average of 1.5 seconds per sample. Our other innovation, a\nfaster variant of JSMA evasion algorithm, empirically performed twice as fast\nas JSMA and yet maintains a comparable evasion rate.\n  Project source code and data from our experiments available at:\nhttps://github.com/QData/AdversarialDNN-Playground \n\n"}
{"id": "1708.00898", "contents": "Title: Seating Assignment Using Constrained Signed Spectral Clustering Abstract: In this paper, we present a novel method for constrained cluster size signed\nspectral clustering which allows us to subdivide large groups of people based\non their relationships. In general, signed clustering only requires K hard\nclusters and does not constrain the cluster sizes. We extend signed clustering\nto include cluster size constraints. Using an example of seating assignment, we\nefficiently find groups of people with high social affinity while mitigating\nawkward social interaction between people who dislike each other. \n\n"}
{"id": "1708.01413", "contents": "Title: Distributed Solution of Large-Scale Linear Systems via Accelerated\n  Projection-Based Consensus Abstract: Solving a large-scale system of linear equations is a key step at the heart\nof many algorithms in machine learning, scientific computing, and beyond. When\nthe problem dimension is large, computational and/or memory constraints make it\ndesirable, or even necessary, to perform the task in a distributed fashion. In\nthis paper, we consider a common scenario in which a taskmaster intends to\nsolve a large-scale system of linear equations by distributing subsets of the\nequations among a number of computing machines/cores. We propose an accelerated\ndistributed consensus algorithm, in which at each iteration every machine\nupdates its solution by adding a scaled version of the projection of an error\nsignal onto the nullspace of its system of equations, and where the taskmaster\nconducts an averaging over the solutions with momentum. The convergence\nbehavior of the proposed algorithm is analyzed in detail and analytically shown\nto compare favorably with the convergence rate of alternative distributed\nmethods, namely distributed gradient descent, distributed versions of\nNesterov's accelerated gradient descent and heavy-ball method, the block\nCimmino method, and ADMM. On randomly chosen linear systems, as well as on\nreal-world data sets, the proposed method offers significant speed-up relative\nto all the aforementioned methods. Finally, our analysis suggests a novel\nvariation of the distributed heavy-ball method, which employs a particular\ndistributed preconditioning, and which achieves the same theoretical\nconvergence rate as the proposed consensus-based method. \n\n"}
{"id": "1708.01648", "contents": "Title: 3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks Abstract: The success of various applications including robotics, digital content\ncreation, and visualization demand a structured and abstract representation of\nthe 3D world from limited sensor data. Inspired by the nature of human\nperception of 3D shapes as a collection of simple parts, we explore such an\nabstract shape representation based on primitives. Given a single depth image\nof an object, we present 3D-PRNN, a generative recurrent neural network that\nsynthesizes multiple plausible shapes composed of a set of primitives. Our\ngenerative model encodes symmetry characteristics of common man-made objects,\npreserves long-range structural coherence, and describes objects of varying\ncomplexity with a compact representation. We also propose a method based on\nGaussian Fields to generate a large scale dataset of primitive-based shape\nrepresentations to train our network. We evaluate our approach on a wide range\nof examples and show that it outperforms nearest-neighbor based shape retrieval\nmethods and is on-par with voxel-based generative models while using a\nsignificantly reduced parameter space. \n\n"}
{"id": "1708.02188", "contents": "Title: PowerAI DDL Abstract: As deep neural networks become more complex and input datasets grow larger,\nit can take days or even weeks to train a deep neural network to the desired\naccuracy. Therefore, distributed Deep Learning at a massive scale is a critical\ncapability, since it offers the potential to reduce the training time from\nweeks to hours. In this paper, we present a software-hardware co-optimized\ndistributed Deep Learning system that can achieve near-linear scaling up to\nhundreds of GPUs. The core algorithm is a multi-ring communication pattern that\nprovides a good tradeoff between latency and bandwidth and adapts to a variety\nof system configurations. The communication algorithm is implemented as a\nlibrary for easy use. This library has been integrated into Tensorflow, Caffe,\nand Torch. We train Resnet-101 on Imagenet 22K with 64 IBM Power8 S822LC\nservers (256 GPUs) in about 7 hours to an accuracy of 33.8 % validation\naccuracy. Microsoft's ADAM and Google's DistBelief results did not reach 30 %\nvalidation accuracy for Imagenet 22K. Compared to Facebook AI Research's recent\npaper on 256 GPU training, we use a different communication algorithm, and our\ncombined software and hardware system offers better communication overhead for\nResnet-50. A PowerAI DDL enabled version of Torch completed 90 epochs of\ntraining on Resnet 50 for 1K classes in 50 minutes using 64 IBM Power8 S822LC\nservers (256 GPUs). \n\n"}
{"id": "1708.03418", "contents": "Title: Learning to Attend, Copy, and Generate for Session-Based Query\n  Suggestion Abstract: Users try to articulate their complex information needs during search\nsessions by reformulating their queries. To make this process more effective,\nsearch engines provide related queries to help users in specifying the\ninformation need in their search process. In this paper, we propose a\ncustomized sequence-to-sequence model for session-based query suggestion. In\nour model, we employ a query-aware attention mechanism to capture the structure\nof the session context. is enables us to control the scope of the session from\nwhich we infer the suggested next query, which helps not only handle the noisy\ndata but also automatically detect session boundaries. Furthermore, we observe\nthat, based on the user query reformulation behavior, within a single session a\nlarge portion of query terms is retained from the previously submitted queries\nand consists of mostly infrequent or unseen terms that are usually not included\nin the vocabulary. We therefore empower the decoder of our model to access the\nsource words from the session context during decoding by incorporating a copy\nmechanism. Moreover, we propose evaluation metrics to assess the quality of the\ngenerative models for query suggestion. We conduct an extensive set of\nexperiments and analysis. e results suggest that our model outperforms the\nbaselines both in terms of the generating queries and scoring candidate queries\nfor the task of query suggestion. \n\n"}
{"id": "1708.05448", "contents": "Title: On Ensuring that Intelligent Machines Are Well-Behaved Abstract: Machine learning algorithms are everywhere, ranging from simple data analysis\nand pattern recognition tools used across the sciences to complex systems that\nachieve super-human performance on various tasks. Ensuring that they are\nwell-behaved---that they do not, for example, cause harm to humans or act in a\nracist or sexist way---is therefore not a hypothetical problem to be dealt with\nin the future, but a pressing one that we address here. We propose a new\nframework for designing machine learning algorithms that simplifies the problem\nof specifying and regulating undesirable behaviors. To show the viability of\nthis new framework, we use it to create new machine learning algorithms that\npreclude the sexist and harmful behaviors exhibited by standard machine\nlearning algorithms in our experiments. Our framework for designing machine\nlearning algorithms simplifies the safe and responsible application of machine\nlearning. \n\n"}
{"id": "1708.05629", "contents": "Title: Learning to Transfer Abstract: Transfer learning borrows knowledge from a source domain to facilitate\nlearning in a target domain. Two primary issues to be addressed in transfer\nlearning are what and how to transfer. For a pair of domains, adopting\ndifferent transfer learning algorithms results in different knowledge\ntransferred between them. To discover the optimal transfer learning algorithm\nthat maximally improves the learning performance in the target domain,\nresearchers have to exhaustively explore all existing transfer learning\nalgorithms, which is computationally intractable. As a trade-off, a sub-optimal\nalgorithm is selected, which requires considerable expertise in an ad-hoc way.\nMeanwhile, it is widely accepted in educational psychology that human beings\nimprove transfer learning skills of deciding what to transfer through\nmeta-cognitive reflection on inductive transfer learning practices. Motivated\nby this, we propose a novel transfer learning framework known as Learning to\nTransfer (L2T) to automatically determine what and how to transfer are the best\nby leveraging previous transfer learning experiences. We establish the L2T\nframework in two stages: 1) we first learn a reflection function encrypting\ntransfer learning skills from experiences; and 2) we infer what and how to\ntransfer for a newly arrived pair of domains by optimizing the reflection\nfunction. Extensive experiments demonstrate the L2T's superiority over several\nstate-of-the-art transfer learning algorithms and its effectiveness on\ndiscovering more transferable knowledge. \n\n"}
{"id": "1708.06257", "contents": "Title: A Flow Model of Neural Networks Abstract: Based on a natural connection between ResNet and transport equation or its\ncharacteristic equation, we propose a continuous flow model for both ResNet and\nplain net. Through this continuous model, a ResNet can be explicitly\nconstructed as a refinement of a plain net. The flow model provides an\nalternative perspective to understand phenomena in deep neural networks, such\nas why it is necessary and sufficient to use 2-layer blocks in ResNets, why\ndeeper is better, and why ResNets are even deeper, and so on. It also opens a\ngate to bring in more tools from the huge area of differential equations. \n\n"}
{"id": "1708.06303", "contents": "Title: Network Model Selection for Task-Focused Attributed Network Inference Abstract: Networks are models representing relationships between entities. Often these\nrelationships are explicitly given, or we must learn a representation which\ngeneralizes and predicts observed behavior in underlying individual data (e.g.\nattributes or labels). Whether given or inferred, choosing the best\nrepresentation affects subsequent tasks and questions on the network. This work\nfocuses on model selection to evaluate network representations from data,\nfocusing on fundamental predictive tasks on networks. We present a modular\nmethodology using general, interpretable network models, task neighborhood\nfunctions found across domains, and several criteria for robust model\nselection. We demonstrate our methodology on three online user activity\ndatasets and show that network model selection for the appropriate network task\nvs. an alternate task increases performance by an order of magnitude in our\nexperiments. \n\n"}
{"id": "1708.07231", "contents": "Title: Exploring the Link Between Test Suite Quality and Automatic\n  Specification Inference Abstract: While no one doubts the importance of correct and complete specifications,\nmany industrial systems still do not have formal specifications written out --\nand even when they do, it is hard to check their correctness and completeness.\nThis work explores the possibility of using an invariant extraction tool such\nas Daikon to automatically infer specifications from available test suites with\nthe idea of aiding software engineers to improve the specifications by having\nanother version to compare to. Given that our initial experiments did not\nproduce satisfactory results, in this paper we explore which test suite\nattributes influence the quality of the inferred specification. Following\nfurther study, we found that instruction, branch and method coverage are\ncorrelated to high recall values, reaching up to 97.93%. \n\n"}
{"id": "1708.07280", "contents": "Title: Learning Generalized Reactive Policies using Deep Neural Networks Abstract: We present a new approach to learning for planning, where knowledge acquired\nwhile solving a given set of planning problems is used to plan faster in\nrelated, but new problem instances. We show that a deep neural network can be\nused to learn and represent a \\emph{generalized reactive policy} (GRP) that\nmaps a problem instance and a state to an action, and that the learned GRPs\nefficiently solve large classes of challenging problem instances. In contrast\nto prior efforts in this direction, our approach significantly reduces the\ndependence of learning on handcrafted domain knowledge or feature selection.\nInstead, the GRP is trained from scratch using a set of successful execution\ntraces. We show that our approach can also be used to automatically learn a\nheuristic function that can be used in directed search algorithms. We evaluate\nour approach using an extensive suite of experiments on two challenging\nplanning problem domains and show that our approach facilitates learning\ncomplex decision making policies and powerful heuristic functions with minimal\nhuman input. Videos of our results are available at goo.gl/Hpy4e3. \n\n"}
{"id": "1709.00023", "contents": "Title: R$^3$: Reinforced Reader-Ranker for Open-Domain Question Answering Abstract: In recent years researchers have achieved considerable success applying\nneural network methods to question answering (QA). These approaches have\nachieved state of the art results in simplified closed-domain settings such as\nthe SQuAD (Rajpurkar et al., 2016) dataset, which provides a pre-selected\npassage, from which the answer to a given question may be extracted. More\nrecently, researchers have begun to tackle open-domain QA, in which the model\nis given a question and access to a large corpus (e.g., wikipedia) instead of a\npre-selected passage (Chen et al., 2017a). This setting is more complex as it\nrequires large-scale search for relevant passages by an information retrieval\ncomponent, combined with a reading comprehension model that \"reads\" the\npassages to generate an answer to the question. Performance in this setting\nlags considerably behind closed-domain performance. In this paper, we present a\nnovel open-domain QA system called Reinforced Ranker-Reader $(R^3)$, based on\ntwo algorithmic innovations. First, we propose a new pipeline for open-domain\nQA with a Ranker component, which learns to rank retrieved passages in terms of\nlikelihood of generating the ground-truth answer to a given question. Second,\nwe propose a novel method that jointly trains the Ranker along with an\nanswer-generation Reader model, based on reinforcement learning. We report\nextensive experimental results showing that our method significantly improves\non the state of the art for multiple open-domain QA datasets. \n\n"}
{"id": "1709.00149", "contents": "Title: Learning what to read: Focused machine reading Abstract: Recent efforts in bioinformatics have achieved tremendous progress in the\nmachine reading of biomedical literature, and the assembly of the extracted\nbiochemical interactions into large-scale models such as protein signaling\npathways. However, batch machine reading of literature at today's scale (PubMed\nalone indexes over 1 million papers per year) is unfeasible due to both cost\nand processing overhead. In this work, we introduce a focused reading approach\nto guide the machine reading of biomedical literature towards what literature\nshould be read to answer a biomedical query as efficiently as possible. We\nintroduce a family of algorithms for focused reading, including an intuitive,\nstrong baseline, and a second approach which uses a reinforcement learning (RL)\nframework that learns when to explore (widen the search) or exploit (narrow\nit). We demonstrate that the RL approach is capable of answering more queries\nthan the baseline, while being more efficient, i.e., reading fewer documents. \n\n"}
{"id": "1709.00670", "contents": "Title: Difficulty-level Modeling of Ontology-based Factual Questions Abstract: Semantics based knowledge representations such as ontologies are found to be\nvery useful in automatically generating meaningful factual questions.\nDetermining the difficulty level of these system generated questions is helpful\nto effectively utilize them in various educational and professional\napplications. The existing approaches for finding the difficulty level of\nfactual questions are very simple and are limited to a few basic principles. We\npropose a new methodology for this problem by considering an educational theory\ncalled Item Response Theory (IRT). In the IRT, knowledge proficiency of end\nusers (learners) are considered for assigning difficulty levels, because of the\nassumptions that a given question is perceived differently by learners of\nvarious proficiencies. We have done a detailed study on the features (factors)\nof a question statement which could possibly determine its difficulty level for\nthree learner categories (experts, intermediates and beginners). We formulate\nontology based metrics for the same. We then train three logistic regression\nmodels to predict the difficulty level corresponding to the three learner\ncategories. \n\n"}
{"id": "1709.01434", "contents": "Title: A Generic Approach for Escaping Saddle points Abstract: A central challenge to using first-order methods for optimizing nonconvex\nproblems is the presence of saddle points. First-order methods often get stuck\nat saddle points, greatly deteriorating their performance. Typically, to escape\nfrom saddles one has to use second-order methods. However, most works on\nsecond-order methods rely extensively on expensive Hessian-based computations,\nmaking them impractical in large-scale settings. To tackle this challenge, we\nintroduce a generic framework that minimizes Hessian based computations while\nat the same time provably converging to second-order critical points. Our\nframework carefully alternates between a first-order and a second-order\nsubroutine, using the latter only close to saddle points, and yields\nconvergence results competitive to the state-of-the-art. Empirical results\nsuggest that our strategy also enjoys a good practical performance. \n\n"}
{"id": "1709.01440", "contents": "Title: Locality-Aware Hybrid Coded MapReduce for Server-Rack Architecture Abstract: MapReduce is a widely used framework for distributed computing. Data\nshuffling between the Map phase and Reduce phase of a job involves a large\namount of data transfer across servers, which in turn accounts for increase in\njob completion time. Recently, Coded MapReduce has been proposed to offer\nsavings with respect to the communication cost incurred in data shuffling. This\nis achieved by creating coded multicast opportunities for shuffling through\nrepeating Map tasks at multiple servers. We consider a server-rack architecture\nfor MapReduce and in this architecture, propose to divide the total\ncommunication cost into two: intra-rack communication cost and cross-rack\ncommunication cost. Having noted that cross-rack data transfer operates at\nlower speed as compared to intra-rack data transfer, we present a scheme termed\nas Hybrid Coded MapReduce which results in lower cross-rack communication than\nCoded MapReduce at the cost of increase in intra-rack communication. In\naddition, we pose the problem of assigning Map tasks to servers to maximize\ndata locality in the framework of Hybrid Coded MapReduce as a constrained\ninteger optimization problem. We show through simulations that data locality\ncan be improved considerably by using the solution of optimization to assign\nMap tasks to servers. \n\n"}
{"id": "1709.01494", "contents": "Title: Latency Optimal Broadcasting in Noisy Wireless Mesh Networks Abstract: In this paper, we adopt a new noisy wireless network model introduced very\nrecently by Censor-Hillel et al. in [ACM PODC 2017, CHHZ17]. More specifically,\nfor a given noise parameter $p\\in [0,1],$ any sender has a probability of $p$\nof transmitting noise or any receiver of a single transmission in its\nneighborhood has a probability $p$ of receiving noise.\n  In this paper, we first propose a new asymptotically latency-optimal\napproximation algorithm (under faultless model) that can complete\nsingle-message broadcasting task in $D+O(\\log^2 n)$ time units/rounds in any\nWMN of size $n,$ and diameter $D$. We then show this diameter-linear\nbroadcasting algorithm remains robust under the noisy wireless network model\nand also improves the currently best known result in CHHZ17 by a\n$\\Theta(\\log\\log n)$ factor.\n  In this paper, we also further extend our robust single-message broadcasting\nalgorithm to $k$ multi-message broadcasting scenario and show it can broadcast\n$k$ messages in $O(D+k\\log n+\\log^2 n)$ time rounds. This new robust\nmulti-message broadcasting scheme is not only asymptotically optimal but also\nanswers affirmatively the problem left open in CHHZ17 on the existence of an\nalgorithm that is robust to sender and receiver faults and can broadcast $k$\nmessages in $O(D+k\\log n + polylog(n))$ time rounds. \n\n"}
{"id": "1709.01743", "contents": "Title: Distant decimals of $\\pi$ Abstract: We describe how to compute very far decimals of $$\\pi$$ and how to provide\nformal guarantees that the decimals we compute are correct. In particular, we\nreport on an experiment where 1 million decimals of $$\\pi$$ and the billionth\nhexadecimal (without the preceding ones) have been computed in a formally\nverified way. Three methods have been studied, the first one relying on a\nspigot formula to obtain at a reasonable cost only one distant digit (more\nprecisely a hexadecimal digit, because the numeration basis is 16) and the\nother two relying on arithmetic-geometric means. All proofs and computations\ncan be made inside the Coq system. We detail the new formalized material that\nwas necessary for this achievement and the techniques employed to guarantee the\naccuracy of the computed digits, in spite of the necessity to work with fixed\nprecision numerical computation. \n\n"}
{"id": "1709.01826", "contents": "Title: Foundation for a series of efficient simulation algorithms Abstract: Compute the coarsest simulation preorder included in an initial preorder is\nused to reduce the resources needed to analyze a given transition system. This\ntechnique is applied on many models like Kripke structures, labeled graphs,\nlabeled transition systems or even word and tree automata. Let (Q,\n$\\rightarrow$) be a given transition system and Rinit be an initial preorder\nover Q. Until now, algorithms to compute Rsim , the coarsest simulation\nincluded in Rinit , are either memory efficient or time efficient but not both.\nIn this paper we propose the foundation for a series of efficient simulation\nalgorithms with the introduction of the notion of maximal transitions and the\nnotion of stability of a preorder with respect to a coarser one. As an\nillustration we solve an open problem by providing the first algorithm with the\nbest published time complexity, O(|Psim |.|$\\rightarrow$|), and a bit space\ncomplexity in O(|Psim |^2. log(|Psim |) + |Q|. log(|Q|)), with Psim the\npartition induced by Rsim. \n\n"}
{"id": "1709.02557", "contents": "Title: A Rational Agent Controlling an Autonomous Vehicle: Implementation and\n  Formal Verification Abstract: The development and deployment of Autonomous Vehicles (AVs) on our roads is\nnot only realistic in the near future but can also bring significant benefits.\nIn particular, it can potentially solve several problems relating to vehicles\nand traffic, for instance: (i) possible reduction of traffic congestion, with\nthe consequence of improved fuel economy and reduced driver inactivity; (ii)\npossible reduction in the number of accidents, assuming that an AV can minimise\nthe human errors that often cause traffic accidents; and (iii) increased ease\nof parking, especially when one considers the potential for shared AVs. In\norder to deploy an AV there are significant steps that must be completed in\nterms of hardware and software. As expected, software components play a key\nrole in the complex AV system and so, at least for safety, we should assess the\ncorrectness of these components.\n  In this paper, we are concerned with the high-level software component(s)\nresponsible for the decisions in an AV. We intend to model an AV capable of\nnavigation; obstacle avoidance; obstacle selection (when a crash is\nunavoidable) and vehicle recovery, etc, using a rational agent. To achieve\nthis, we have established the following stages. First, the agent plans and\nactions have been implemented within the Gwendolen agent programming language.\nSecond, we have built a simulated automotive environment in the Java language.\nThird, we have formally specified some of the required agent properties through\nLTL formulae, which are then formally verified with the AJPF verification tool.\nFinally, within the MCAPL framework (which comprises all the tools used in\nprevious stages) we have obtained formal verification of our AV agent in terms\nof its specific behaviours. For example, the agent plans responsible for\nselecting an obstacle with low potential damage, instead of a higher damage\nobstacle (when possible) can be formally verified within MCAPL. We must\nemphasise that the major goal (of our present approach) lies in the formal\nverification of agent plans, rather than evaluating real-world applications.\nFor this reason we utilised a simple matrix representation concerning the\nenvironment used by our agent. \n\n"}
{"id": "1709.03854", "contents": "Title: Meta-QSAR: a large-scale application of meta-learning to drug design and\n  discovery Abstract: We investigate the learning of quantitative structure activity relationships\n(QSARs) as a case-study of meta-learning. This application area is of the\nhighest societal importance, as it is a key step in the development of new\nmedicines. The standard QSAR learning problem is: given a target (usually a\nprotein) and a set of chemical compounds (small molecules) with associated\nbioactivities (e.g. inhibition of the target), learn a predictive mapping from\nmolecular representation to activity. Although almost every type of machine\nlearning method has been applied to QSAR learning there is no agreed single\nbest way of learning QSARs, and therefore the problem area is well-suited to\nmeta-learning. We first carried out the most comprehensive ever comparison of\nmachine learning methods for QSAR learning: 18 regression methods, 6 molecular\nrepresentations, applied to more than 2,700 QSAR problems. (These results have\nbeen made publicly available on OpenML and represent a valuable resource for\ntesting novel meta-learning methods.) We then investigated the utility of\nalgorithm selection for QSAR problems. We found that this meta-learning\napproach outperformed the best individual QSAR learning method (random forests\nusing a molecular fingerprint representation) by up to 13%, on average. We\nconclude that meta-learning outperforms base-learning methods for QSAR\nlearning, and as this investigation is one of the most extensive ever\ncomparisons of base and meta-learning methods ever made, it provides evidence\nfor the general effectiveness of meta-learning over base-learning. \n\n"}
{"id": "1709.04071", "contents": "Title: Variational Reasoning for Question Answering with Knowledge Graph Abstract: Knowledge graph (KG) is known to be helpful for the task of question\nanswering (QA), since it provides well-structured relational information\nbetween entities, and allows one to further infer indirect facts. However, it\nis challenging to build QA systems which can learn to reason over knowledge\ngraphs based on question-answer pairs alone. First, when people ask questions,\ntheir expressions are noisy (for example, typos in texts, or variations in\npronunciations), which is non-trivial for the QA system to match those\nmentioned entities to the knowledge graph. Second, many questions require\nmulti-hop logic reasoning over the knowledge graph to retrieve the answers. To\naddress these challenges, we propose a novel and unified deep learning\narchitecture, and an end-to-end variational learning algorithm which can handle\nnoise in questions, and learn multi-hop reasoning simultaneously. Our method\nachieves state-of-the-art performance on a recent benchmark dataset in the\nliterature. We also derive a series of new benchmark datasets, including\nquestions for multi-hop reasoning, questions paraphrased by neural translation\nmodel, and questions in human voice. Our method yields very promising results\non all these challenging datasets. \n\n"}
{"id": "1709.04382", "contents": "Title: On the decidability of the existence of polyhedral invariants in\n  transition systems Abstract: Automated program verification often proceeds by exhibiting inductive\ninvariants entailing the desired properties.For numerical properties, a\nclassical class of invariants is convex polyhedra: solution sets of system of\nlinear (in)equalities.Forty years of research on convex polyhedral invariants\nhave focused, on the one hand, on identifying \"easier\" subclasses, on the other\nhand on heuristics for finding general convex polyhedra.These heuristics are\nhowever not guaranteed to find polyhedral inductive invariants when they\nexist.To our best knowledge, the existence of polyhedral inductive invariants\nhas never been proved to be undecidable.In this article, we show that the\nexistence of convex polyhedral invariants is undecidable, even if there is only\none control state in addition to the \"bad\" one.The question is still open if\none is not allowed any nonlinear constraint. \n\n"}
{"id": "1709.05047", "contents": "Title: Disentangled Variational Auto-Encoder for Semi-supervised Learning Abstract: Semi-supervised learning is attracting increasing attention due to the fact\nthat datasets of many domains lack enough labeled data. Variational\nAuto-Encoder (VAE), in particular, has demonstrated the benefits of\nsemi-supervised learning. The majority of existing semi-supervised VAEs utilize\na classifier to exploit label information, where the parameters of the\nclassifier are introduced to the VAE. Given the limited labeled data, learning\nthe parameters for the classifiers may not be an optimal solution for\nexploiting label information. Therefore, in this paper, we develop a novel\napproach for semi-supervised VAE without classifier. Specifically, we propose a\nnew model called Semi-supervised Disentangled VAE (SDVAE), which encodes the\ninput data into disentangled representation and non-interpretable\nrepresentation, then the category information is directly utilized to\nregularize the disentangled representation via the equality constraint. To\nfurther enhance the feature learning ability of the proposed VAE, we\nincorporate reinforcement learning to relieve the lack of data. The dynamic\nframework is capable of dealing with both image and text data with its\ncorresponding encoder and decoder networks. Extensive experiments on image and\ntext datasets demonstrate the effectiveness of the proposed framework. \n\n"}
{"id": "1709.05162", "contents": "Title: Certified Non-Confluence with ConCon 1.5 Abstract: We present three methods to check CTRSs for non-confluence: (1) an ad hoc\nmethod for 4-CTRSs, (2) a specialized method for unconditional critical pairs,\nand finally, (3) a method that employs conditional narrowing to find\nnon-confluence witnesses. We shortly describe our implementation of these\nmethods in ConCon, then look into their certification with CeTA, and finally\nconclude with experiments on the confluence problems database (Cops). \n\n"}
{"id": "1709.05601", "contents": "Title: Markov Brains: A Technical Introduction Abstract: Markov Brains are a class of evolvable artificial neural networks (ANN). They\ndiffer from conventional ANNs in many aspects, but the key difference is that\ninstead of a layered architecture, with each node performing the same function,\nMarkov Brains are networks built from individual computational components.\nThese computational components interact with each other, receive inputs from\nsensors, and control motor outputs. The function of the computational\ncomponents, their connections to each other, as well as connections to sensors\nand motors are all subject to evolutionary optimization. Here we describe in\ndetail how a Markov Brain works, what techniques can be used to study them, and\nhow they can be evolved. \n\n"}
{"id": "1709.05635", "contents": "Title: Joining Jolie to Docker - Orchestration of Microservices on a\n  Containers-as-a-Service Layer Abstract: Cloud computing is steadily growing and, as IaaS vendors have started to\noffer pay-as-you-go billing policies, it is fundamental to achieve as much\nelasticity as possible, avoiding over-provisioning that would imply higher\ncosts. In this paper, we briefly analyse the orchestration characteristics of\nPaaSSOA, a proposed architecture already implemented for Jolie microservices,\nand Kubernetes, one of the various orchestration plugins for Docker; then, we\noutline similarities and differences of the two approaches, with respect to\ntheir own domain of application. Furthermore, we investigate some ideas to\nachieve a federation of the two technologies, proposing an architectural\ncomposition of Jolie microservices on Docker Container-as-a-Service layer. \n\n"}
{"id": "1709.05666", "contents": "Title: On Inductive Abilities of Latent Factor Models for Relational Learning Abstract: Latent factor models are increasingly popular for modeling multi-relational\nknowledge graphs. By their vectorial nature, it is not only hard to interpret\nwhy this class of models works so well, but also to understand where they fail\nand how they might be improved. We conduct an experimental survey of\nstate-of-the-art models, not towards a purely comparative end, but as a means\nto get insight about their inductive abilities. To assess the strengths and\nweaknesses of each model, we create simple tasks that exhibit first, atomic\nproperties of binary relations, and then, common inter-relational inference\nthrough synthetic genealogies. Based on these experimental results, we propose\nnew research directions to improve on existing models. \n\n"}
{"id": "1709.06175", "contents": "Title: Blocking Versus Non-Blocking Halo Exchange Abstract: This report describes the design, implementation and analysis of a\nnon-blocking halo exchange routine as an alternative to the blocking halo\nexchange routine in the lattice Boltzmann code Ludwig. The alternative,\nnon-blocking, routine is implemented in such a way to allow work-communication\noverlap. Detailed benchmarks in this report show that the non-blocking version\nis a good alternative even without any work-communication overlap.\nWork-Communication overlap can be used to improve the performance of the\nnon-blocking routine. Development and benchmarking were conducted on the UK\nnational supercomputer, ARCHER. \n\n"}
{"id": "1709.06620", "contents": "Title: Learning of Coordination Policies for Robotic Swarms Abstract: Inspired by biological swarms, robotic swarms are envisioned to solve\nreal-world problems that are difficult for individual agents. Biological swarms\ncan achieve collective intelligence based on local interactions and simple\nrules; however, designing effective distributed policies for large-scale\nrobotic swarms to achieve a global objective can be challenging. Although it is\noften possible to design an optimal centralized strategy for smaller numbers of\nagents, those methods can fail as the number of agents increases. Motivated by\nthe growing success of machine learning, we develop a deep learning approach\nthat learns distributed coordination policies from centralized policies. In\ncontrast to traditional distributed control approaches, which are usually based\non human-designed policies for relatively simple tasks, this learning-based\napproach can be adapted to more difficult tasks. We demonstrate the efficacy of\nour proposed approach on two different tasks, the well-known rendezvous problem\nand a more difficult particle assignment problem. For the latter, no known\ndistributed policy exists. From extensive simulations, it is shown that the\nperformance of the learned coordination policies is comparable to the\ncentralized policies, surpassing state-of-the-art distributed policies.\nThereby, our proposed approach provides a promising alternative for real-world\ncoordination problems that would be otherwise computationally expensive to\nsolve or intangible to explore. \n\n"}
{"id": "1709.06921", "contents": "Title: A Byzantine Fault-Tolerant Ordering Service for the Hyperledger Fabric\n  Blockchain Platform Abstract: Hyperledger Fabric (HLF) is a flexible permissioned blockchain platform\ndesigned for business applications beyond the basic digital coin addressed by\nBitcoin and other existing networks. A key property of HLF is its\nextensibility, and in particular the support for multiple ordering services for\nbuilding the blockchain. Nonetheless, the version 1.0 was launched in early\n2017 without an implementation of a Byzantine fault-tolerant (BFT) ordering\nservice. To overcome this limitation, we designed, implemented, and evaluated a\nBFT ordering service for HLF on top of the BFT-SMaRt state machine\nreplication/consensus library, implementing also optimizations for wide-area\ndeployment. Our results show that HLF with our ordering service can achieve up\nto ten thousand transactions per second and write a transaction irrevocably in\nthe blockchain in half a second, even with peers spread in different\ncontinents. \n\n"}
{"id": "1709.07080", "contents": "Title: A Deep-Reinforcement Learning Approach for Software-Defined Networking\n  Routing Optimization Abstract: In this paper we design and evaluate a Deep-Reinforcement Learning agent that\noptimizes routing. Our agent adapts automatically to current traffic conditions\nand proposes tailored configurations that attempt to minimize the network\ndelay. Experiments show very promising performance. Moreover, this approach\nprovides important operational advantages with respect to traditional\noptimization algorithms. \n\n"}
{"id": "1709.07094", "contents": "Title: Decomposing GR(1) Games with Singleton Liveness Guarantees for Efficient\n  Synthesis Abstract: Temporal logic based synthesis approaches are often used to find trajectories\nthat are correct-by-construction for tasks in systems with complex behavior.\nSome examples of such tasks include synchronization for multi-agent hybrid\nsystems, reactive motion planning for robots. However, the scalability of such\napproaches is of concern and at times a bottleneck when transitioning from\ntheory to practice. In this paper, we identify a class of problems in the GR(1)\nfragment of linear-time temporal logic (LTL) where the synthesis problem allows\nfor a decomposition that enables easy parallelization. This decomposition also\nreduces the alternation depth, resulting in more efficient synthesis. A\nmulti-agent robot gridworld example with coordination tasks is presented to\ndemonstrate the application of the developed ideas and also to perform\nempirical analysis for benchmarking the decomposition-based synthesis approach. \n\n"}
{"id": "1709.07741", "contents": "Title: Subjective Simulation as a Notion of Morphism for Composing Concurrent\n  Resources Abstract: Recent approaches to verifying programs in separation logics for concurrency\nhave used state transition systems (STSs) to specify the atomic operations of\nprograms. A key challenge in the setting has been to compose such STSs into\nlarger ones, while enabling programs specified under one STS to be linked to a\nlarger one, without reverification. This paper develops a notion of morphism\nbetween two STSs which permits such lifting. The morphisms are a constructive\nform of simulation between the STSs, and lead to a general and concise proof\nsystem. We illustrate the concept and its generality on several disparate\nexamples, including staged construction of a readers/writers lock and its\nproof, and of proofs about quiescence when concurrent programs are executed\nwithout external interference. \n\n"}
{"id": "1709.08086", "contents": "Title: The algebra of entanglement and the geometry of composition Abstract: String diagrams turn algebraic equations into topological moves that have\nrecurring shapes, involving the sliding of one diagram past another. We\nindividuate, at the root of this fact, the dual nature of polygraphs as\npresentations of higher algebraic theories, and as combinatorial descriptions\nof \"directed spaces\". Operations of polygraphs modelled on operations of\ntopological spaces are used as the foundation of a compositional universal\nalgebra, where sliding moves arise from tensor products of polygraphs. We\nreconstruct several higher algebraic theories in this framework.\n  In this regard, the standard formalism of polygraphs has some technical\nproblems. We propose a notion of regular polygraph, barring cell boundaries\nthat are not homeomorphic to a disk of the appropriate dimension. We define a\ncategory of non-degenerate shapes, and show how to calculate their tensor\nproducts. Then, we introduce a notion of weak unit to recover weakly degenerate\nboundaries in low dimensions, and prove that the existence of weak units is\nequivalent to a representability property.\n  We then turn to applications of diagrammatic algebra to quantum theory. We\nre-evaluate the category of Hilbert spaces from the perspective of categorical\nuniversal algebra, which leads to a bicategorical refinement. Then, we focus on\nthe axiomatics of fragments of quantum theory, and present the ZW calculus, the\nfirst complete diagrammatic axiomatisation of the theory of qubits.\n  The ZW calculus has several advantages over ZX calculi, including a\ncomputationally meaningful normal form, and a fragment whose diagrams can be\nread as setups of fermionic oscillators. Moreover, its generators reflect an\noperational classification of entangled states of 3 qubits. We conclude with\ngeneralisations of the ZW calculus to higher-dimensional systems, including the\ndefinition of a universal set of generators in each dimension. \n\n"}
{"id": "1709.08767", "contents": "Title: BOSS-LDG: A Novel Computational Framework that Brings Together Blue\n  Waters, Open Science Grid, Shifter and the LIGO Data Grid to Accelerate\n  Gravitational Wave Discovery Abstract: We present a novel computational framework that connects Blue Waters, the\nNSF-supported, leadership-class supercomputer operated by NCSA, to the Laser\nInterferometer Gravitational-Wave Observatory (LIGO) Data Grid via Open Science\nGrid technology. To enable this computational infrastructure, we configured,\nfor the first time, a LIGO Data Grid Tier-1 Center that can submit\nheterogeneous LIGO workflows using Open Science Grid facilities. In order to\nenable a seamless connection between the LIGO Data Grid and Blue Waters via\nOpen Science Grid, we utilize Shifter to containerize LIGO's workflow software.\nThis work represents the first time Open Science Grid, Shifter, and Blue Waters\nare unified to tackle a scientific problem and, in particular, it is the first\ntime a framework of this nature is used in the context of large scale\ngravitational wave data analysis. This new framework has been used in the last\nseveral weeks of LIGO's second discovery campaign to run the most\ncomputationally demanding gravitational wave search workflows on Blue Waters,\nand accelerate discovery in the emergent field of gravitational wave\nastrophysics. We discuss the implications of this novel framework for a wider\necosystem of Higher Performance Computing users. \n\n"}
{"id": "1709.09491", "contents": "Title: Flexible Support for Fast Parallel Commutative Updates Abstract: Privatizing data is a useful strategy for increasing parallelism in a shared\nmemory multithreaded program. Independent cores can compute independently on\nduplicates of shared data, combining their results at the end of their\ncomputations. Conventional approaches to privatization, however, rely on\nexplicit static or dynamic memory allocation for duplicated state, increasing\nmemory footprint and contention for cache resources, especially in shared\ncaches. In this work, we describe CCache, a system for on-demand privatization\nof data manipulated by commutative operations. CCache garners the benefits of\nprivatization, without the increase in memory footprint or cache occupancy.\nEach core in CCache dynamically privatizes commutatively manipulated data,\noperating on a copy. Periodically or at the end of its computation, the core\nmerges its value with the value resident in memory, and when all cores have\nmerged, the in-memory copy contains the up-to-date value. We describe a\nlow-complexity architectural implementation of CCache that extends a\nconventional multicore to support on-demand privatization without using\nadditional memory for private copies. We evaluate CCache on several high-value\napplications, including random access key-value store, clustering, breadth\nfirst search and graph ranking, showing speedups upto 3.2X. \n\n"}
{"id": "1709.09994", "contents": "Title: Premise Selection for Theorem Proving by Deep Graph Embedding Abstract: We propose a deep learning-based approach to the problem of premise\nselection: selecting mathematical statements relevant for proving a given\nconjecture. We represent a higher-order logic formula as a graph that is\ninvariant to variable renaming but still fully preserves syntactic and semantic\ninformation. We then embed the graph into a vector via a novel embedding method\nthat preserves the information of edge ordering. Our approach achieves\nstate-of-the-art results on the HolStep dataset, improving the classification\naccuracy from 83% to 90.3%. \n\n"}
{"id": "1710.00073", "contents": "Title: CARMA: Contention-aware Auction-based Resource Management in\n  Architecture Abstract: As the number of resources on chip multiprocessors (CMPs) increases, the\ncomplexity of how to best allocate these resources increases drastically.\nBecause the higher number of applications makes the interaction and impacts of\nvarious memory levels more complex. Also, the selection of the objective\nfunction to define what \\enquote{best} means for all applications is\nchallenging. Memory-level parallelism (MLP) aware replacement algorithms in\nCMPs try to maximize the overall system performance or equalize each\napplication's performance degradation due to sharing. However, depending on the\nselected \\enquote{performance} metric, these algorithms are not efficiently\nimplemented, because these centralized approaches mostly need some further\ninformation regarding about applications' need. In this paper, we propose a\ncontention-aware game-theoretic resource management approach (CARMA) using\nmarket auction mechanism to find an optimal strategy for each application in a\nresource competition game. The applications learn through repeated interactions\nto choose their action on choosing the shared resources. Specifically, we\nconsider two cases: (i) cache competition game, and (ii) main processor and\nco-processor congestion game. We enforce costs for each resource and derive\nbidding strategy. Accurate evaluation of the proposed approach show that our\ndistributed allocation is scalable and outperforms the static and traditional\napproaches. \n\n"}
{"id": "1710.00489", "contents": "Title: SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Planning\n  and Control Abstract: In this work, we present an approach to deep visuomotor control using\nstructured deep dynamics models. Our deep dynamics model, a variant of\nSE3-Nets, learns a low-dimensional pose embedding for visuomotor control via an\nencoder-decoder structure. Unlike prior work, our dynamics model is structured:\ngiven an input scene, our network explicitly learns to segment salient parts\nand predict their pose-embedding along with their motion modeled as a change in\nthe pose space due to the applied actions. We train our model using a pair of\npoint clouds separated by an action and show that given supervision only in the\nform of point-wise data associations between the frames our network is able to\nlearn a meaningful segmentation of the scene along with consistent poses. We\nfurther show that our model can be used for closed-loop control directly in the\nlearned low-dimensional pose space, where the actions are computed by\nminimizing error in the pose space using gradient-based methods, similar to\ntraditional model-based control. We present results on controlling a Baxter\nrobot from raw depth data in simulation and in the real world and compare\nagainst two baseline deep networks. Our method runs in real-time, achieves good\nprediction of scene dynamics and outperforms the baseline methods on multiple\ncontrol runs. Video results can be found at:\nhttps://rse-lab.cs.washington.edu/se3-structured-deep-ctrl/ \n\n"}
{"id": "1710.00675", "contents": "Title: Sensor Synthesis for POMDPs with Reachability Objectives Abstract: Partially observable Markov decision processes (POMDPs) are widely used in\nprobabilistic planning problems in which an agent interacts with an environment\nusing noisy and imprecise sensors. We study a setting in which the sensors are\nonly partially defined and the goal is to synthesize \"weakest\" additional\nsensors, such that in the resulting POMDP, there is a small-memory policy for\nthe agent that almost-surely (with probability~1) satisfies a reachability\nobjective. We show that the problem is NP-complete, and present a symbolic\nalgorithm by encoding the problem into SAT instances. We illustrate trade-offs\nbetween the amount of memory of the policy and the number of additional sensors\non a simple example. We have implemented our approach and consider three\nclassical POMDP examples from the literature, and show that in all the examples\nthe number of sensors can be significantly decreased (as compared to the\nexisting solutions in the literature) without increasing the complexity of the\npolicies. \n\n"}
{"id": "1710.01437", "contents": "Title: Duality of Graphical Models and Tensor Networks Abstract: In this article we show the duality between tensor networks and undirected\ngraphical models with discrete variables. We study tensor networks on\nhypergraphs, which we call tensor hypernetworks. We show that the tensor\nhypernetwork on a hypergraph exactly corresponds to the graphical model given\nby the dual hypergraph. We translate various notions under duality. For\nexample, marginalization in a graphical model is dual to contraction in the\ntensor network. Algorithms also translate under duality. We show that belief\npropagation corresponds to a known algorithm for tensor network contraction.\nThis article is a reminder that the research areas of graphical models and\ntensor networks can benefit from interaction. \n\n"}
{"id": "1710.04748", "contents": "Title: HyperENTM: Evolving Scalable Neural Turing Machines through HyperNEAT Abstract: Recent developments within memory-augmented neural networks have solved\nsequential problems requiring long-term memory, which are intractable for\ntraditional neural networks. However, current approaches still struggle to\nscale to large memory sizes and sequence lengths. In this paper we show how\naccess to memory can be encoded geometrically through a HyperNEAT-based Neural\nTuring Machine (HyperENTM). We demonstrate that using the indirect HyperNEAT\nencoding allows for training on small memory vectors in a bit-vector copy task\nand then applying the knowledge gained from such training to speed up training\non larger size memory vectors. Additionally, we demonstrate that in some\ninstances, networks trained to copy bit-vectors of size 9 can be scaled to\nsizes of 1,000 without further training. While the task in this paper is\nsimple, these results could open up the problems amendable to networks with\nexternal memories to problems with larger memory vectors and theoretically\nunbounded memory sizes. \n\n"}
{"id": "1710.05763", "contents": "Title: A Hierarchy of Scheduler Classes for Stochastic Automata Abstract: Stochastic automata are a formal compositional model for concurrent\nstochastic timed systems, with general distributions and non-deterministic\nchoices. Measures of interest are defined over schedulers that resolve the\nnondeterminism. In this paper we investigate the power of various theoretically\nand practically motivated classes of schedulers, considering the classic\ncomplete-information view and a restriction to non-prophetic schedulers. We\nprove a hierarchy of scheduler classes w.r.t. unbounded probabilistic\nreachability. We find that, unlike Markovian formalisms, stochastic automata\ndistinguish most classes even in this basic setting. Verification and strategy\nsynthesis methods thus face a tradeoff between powerful and efficient classes.\nUsing lightweight scheduler sampling, we explore this tradeoff and demonstrate\nthe concept of a useful approximative verification technique for stochastic\nautomata. \n\n"}
{"id": "1710.07516", "contents": "Title: An impure solution to the problem of matching fans Abstract: We propose an algorithm to solve the problem of matching fans in interaction\nnet implementations of optimal reduction for the pure untyped lambda calculus\nwithout use of any additional agent types. The algorithm relies upon a specific\ninteraction nets reduction strategy and involves side effects in one of\ninteraction rules. \n\n"}
{"id": "1710.08027", "contents": "Title: Lightweight MPI Communicators with Applications to Perfectly Balanced\n  Quicksort Abstract: MPI uses the concept of communicators to connect groups of processes. It\nprovides nonblocking collective operations on communicators to overlap\ncommunication and computation. Flexible algorithms demand flexible\ncommunicators. E.g., a process can work on different subproblems within\ndifferent process groups simultaneously, new process groups can be created, or\nthe members of a process group can change. Depending on the number of\ncommunicators, the time for communicator creation can drastically increase the\nrunning time of the algorithm. Furthermore, a new communicator synchronizes all\nprocesses as communicator creation routines are blocking collective operations.\n  We present RBC, a communication library based on MPI, that creates\nrange-based communicators in constant time without communication. These RBC\ncommunicators support (non)blocking point-to-point communication as well as\n(non)blocking collective operations. Our experiments show that the library\nreduces the time to create a new communicator by a factor of more than 400\nwhereas the running time of collective operations remains about the same. We\npropose Janus Quicksort, a distributed sorting algorithm that avoids any load\nimbalances. We improved the performance of this algorithm by a factor of 15 for\nmoderate inputs by using RBC communicators. Finally, we discuss different\napproaches to bring nonblocking (local) communicator creation of lightweight\n(range-based) communicators into MPI. \n\n"}
{"id": "1710.08377", "contents": "Title: Listening to the World Improves Speech Command Recognition Abstract: We study transfer learning in convolutional network architectures applied to\nthe task of recognizing audio, such as environmental sound events and speech\ncommands. Our key finding is that not only is it possible to transfer\nrepresentations from an unrelated task like environmental sound classification\nto a voice-focused task like speech command recognition, but also that doing so\nimproves accuracies significantly. We also investigate the effect of increased\nmodel capacity for transfer learning audio, by first validating known results\nfrom the field of Computer Vision of achieving better accuracies with\nincreasingly deeper networks on two audio datasets: UrbanSound8k and the newly\nreleased Google Speech Commands dataset. Then we propose a simple multiscale\ninput representation using dilated convolutions and show that it is able to\naggregate larger contexts and increase classification performance. Further, the\nmodels trained using a combination of transfer learning and multiscale input\nrepresentations need only 40% of the training data to achieve similar\naccuracies as a freshly trained model with 100% of the training data. Finally,\nwe demonstrate a positive interaction effect for the multiscale input and\ntransfer learning, making a case for the joint application of the two\ntechniques. \n\n"}
{"id": "1710.08774", "contents": "Title: High-Performance Code Generation though Fusion and Vectorization Abstract: We present a technique for automatically transforming kernel-based\ncomputations in disparate, nested loops into a fused, vectorized form that can\nreduce intermediate storage needs and lead to improved performance on\ncontemporary hardware.\n  We introduce representations for the abstract relationships and data\ndependencies of kernels in loop nests and algorithms for manipulating them into\nmore efficient form; we similarly introduce techniques for determining data\naccess patterns for stencil-like array accesses and show how this can be used\nto elide storage and improve vectorization.\n  We discuss our prototype implementation of these ideas---named HFAV---and its\nuse of a declarative, inference-based front-end to drive transformations, and\nwe present results for some prominent codes in HPC. \n\n"}
{"id": "1710.08883", "contents": "Title: Avoiding Communication in Proximal Methods for Convex Optimization\n  Problems Abstract: The fast iterative soft thresholding algorithm (FISTA) is used to solve\nconvex regularized optimization problems in machine learning. Distributed\nimplementations of the algorithm have become popular since they enable the\nanalysis of large datasets. However, existing formulations of FISTA communicate\ndata at every iteration which reduces its performance on modern distributed\narchitectures. The communication costs of FISTA, including bandwidth and\nlatency costs, is closely tied to the mathematical formulation of the\nalgorithm. This work reformulates FISTA to communicate data at every k\niterations and reduce data communication when operating on large data sets. We\nformulate the algorithm for two different optimization methods on the Lasso\nproblem and show that the latency cost is reduced by a factor of k while\nbandwidth and floating-point operation costs remain the same. The convergence\nrates and stability properties of the reformulated algorithms are similar to\nthe standard formulations. The performance of communication-avoiding FISTA and\nProximal Newton methods is evaluated on 1 to 1024 nodes for multiple benchmarks\nand demonstrate average speedups of 3-10x with scaling properties that\noutperform the classical algorithms. \n\n"}
{"id": "1710.09844", "contents": "Title: Alone Together: Compositional Reasoning and Inference for Weak Isolation Abstract: Serializability is a well-understood correctness criterion that simplifies\nreasoning about the behavior of concurrent transactions by ensuring they are\nisolated from each other while they execute. However, enforcing serializable\nisolation comes at a steep cost in performance and hence database systems in\npractice support, and often encourage, developers to implement transactions\nusing weaker alternatives. Unfortunately, the semantics of weak isolation is\npoorly understood, and usually explained only informally in terms of low-level\nimplementation artifacts. Consequently, verifying high-level correctness\nproperties in such environments remains a challenging problem.\n  To address this issue, we present a novel program logic that enables\ncompositional reasoning about the behavior of concurrently executing\nweakly-isolated transactions. Recognizing that the proof burden necessary to\nuse this logic may dissuade application developers, we also describe an\ninference procedure based on this foundation that ascertains the weakest\nisolation level that still guarantees the safety of high-level consistency\ninvariants associated with such transactions. The key to effective inference is\nthe observation that weakly-isolated transactions can be viewed as functional\n(monadic) computations over an abstract database state, allowing us to treat\ntheir operations as state transformers over the database. This interpretation\nenables automated verification using off-the-shelf SMT solvers. Case studies\nand experiments of real-world applications (written in an embedded DSL in\nOCaml) demonstrate the utility of our approach. \n\n"}
{"id": "1710.10035", "contents": "Title: Convolutional neural networks on irregular domains based on approximate\n  vertex-domain translations Abstract: We propose a generalization of convolutional neural networks (CNNs) to\nirregular domains, through the use of a translation operator on a graph\nstructure. In regular settings such as images, convolutional layers are\ndesigned by translating a convolutional kernel over all pixels, thus enforcing\ntranslation equivariance. In the case of general graphs however, translation is\nnot a well-defined operation, which makes shifting a convolutional kernel not\nstraightforward. In this article, we introduce a methodology to allow the\ndesign of convolutional layers that are adapted to signals evolving on\nirregular topologies, even in the absence of a natural translation. Using the\ndesigned layers, we build a CNN that we train using the initial set of signals.\nContrary to other approaches that aim at extending CNNs to irregular domains,\nwe incorporate the classical settings of CNNs for 2D signals as a particular\ncase of our approach. Designing convolutional layers in the vertex domain\ndirectly implies weight sharing, which in other approaches is generally\nestimated a posteriori using heuristics. \n\n"}
{"id": "1710.10057", "contents": "Title: Multiwinner Voting with Fairness Constraints Abstract: Multiwinner voting rules are used to select a small representative subset of\ncandidates or items from a larger set given the preferences of voters. However,\nif candidates have sensitive attributes such as gender or ethnicity (when\nselecting a committee), or specified types such as political leaning (when\nselecting a subset of news items), an algorithm that chooses a subset by\noptimizing a multiwinner voting rule may be unbalanced in its selection -- it\nmay under or over represent a particular gender or political orientation in the\nexamples above. We introduce an algorithmic framework for multiwinner voting\nproblems when there is an additional requirement that the selected subset\nshould be \"fair\" with respect to a given set of attributes. Our framework\nprovides the flexibility to (1) specify fairness with respect to multiple,\nnon-disjoint attributes (e.g., ethnicity and gender) and (2) specify a score\nfunction. We study the computational complexity of this constrained multiwinner\nvoting problem for monotone and submodular score functions and present several\napproximation algorithms and matching hardness of approximation results for\nvarious attribute group structure and types of score functions. We also present\nsimulations that suggest that adding fairness constraints may not affect the\nscores significantly when compared to the unconstrained case. \n\n"}
{"id": "1710.10164", "contents": "Title: Towards a new paradigm for assistive technology at home: research\n  challenges, design issues and performance assessment Abstract: Providing elderly and people with special needs, including those suffering\nfrom physical disabilities and chronic diseases, with the possibility of\nretaining their independence at best is one of the most important challenges\nour society is expected to face. Assistance models based on the home care\nparadigm are being adopted rapidly in almost all industrialized and emerging\ncountries. Such paradigms hypothesize that it is necessary to ensure that the\nso-called Activities of Daily Living are correctly and regularly performed by\nthe assisted person to increase the perception of an improved quality of life.\nThis chapter describes the computational inference engine at the core of\nArianna, a system able to understand whether an assisted person performs a\ngiven set of ADL and to motivate him/her in performing them through a\nspeech-mediated motivational dialogue, using a set of nearables to be installed\nin an apartment, plus a wearable to be worn or fit in garments. \n\n"}
{"id": "1710.10196", "contents": "Title: Progressive Growing of GANs for Improved Quality, Stability, and\n  Variation Abstract: We describe a new training methodology for generative adversarial networks.\nThe key idea is to grow both the generator and discriminator progressively:\nstarting from a low resolution, we add new layers that model increasingly fine\ndetails as training progresses. This both speeds the training up and greatly\nstabilizes it, allowing us to produce images of unprecedented quality, e.g.,\nCelebA images at 1024^2. We also propose a simple way to increase the variation\nin generated images, and achieve a record inception score of 8.80 in\nunsupervised CIFAR10. Additionally, we describe several implementation details\nthat are important for discouraging unhealthy competition between the generator\nand discriminator. Finally, we suggest a new metric for evaluating GAN results,\nboth in terms of image quality and variation. As an additional contribution, we\nconstruct a higher-quality version of the CelebA dataset. \n\n"}
{"id": "1710.10205", "contents": "Title: Polymorphism and the obstinate circularity of second order logic: a\n  victims' tale Abstract: The investigations on higher-order type theories and on the related notion of\nparametric polymorphism constitute the technical counterpart of the old\nfoundational problem of the circularity (or impredicativity) of second and\nhigher order logic. However, the epistemological significance of such\ninvestigations, and of their often non trivial results, has not received much\nattention in the contemporary foundational debate. The results recalled in this\npaper suggest that the question of the circularity of second order logic cannot\nbe reduced to the simple assessment of a vicious circle. Through a comparison\nbetween the faulty consistency arguments given by Frege and Martin-L\\\"of,\nrespectively for the logical system of the Grundgesetze (shown inconsistent by\nRussell's paradox) and for the intuitionistic type theory with a type of all\ntypes (shown inconsistent by Girard's paradox), and the normalization argument\nfor second order type theory (or System F), we indicate a bunch of subtle\nmathematical problems and logical concepts hidden behind the hazardous idea of\nimpredicative quantification, constituting a vast (and largely unexplored)\ndomain for foundational research. \n\n"}
{"id": "1710.11424", "contents": "Title: Regret Minimization for Partially Observable Deep Reinforcement Learning Abstract: Deep reinforcement learning algorithms that estimate state and state-action\nvalue functions have been shown to be effective in a variety of challenging\ndomains, including learning control strategies from raw image pixels. However,\nalgorithms that estimate state and state-action value functions typically\nassume a fully observed state and must compensate for partial observations by\nusing finite length observation histories or recurrent networks. In this work,\nwe propose a new deep reinforcement learning algorithm based on counterfactual\nregret minimization that iteratively updates an approximation to an\nadvantage-like function and is robust to partially observed state. We\ndemonstrate that this new algorithm can substantially outperform strong\nbaseline methods on several partially observed reinforcement learning tasks:\nlearning first-person 3D navigation in Doom and Minecraft, and acting in the\npresence of partially observed objects in Doom and Pong. \n\n"}
{"id": "1710.11622", "contents": "Title: Meta-Learning and Universality: Deep Representations and Gradient\n  Descent can Approximate any Learning Algorithm Abstract: Learning to learn is a powerful paradigm for enabling models to learn from\ndata more effectively and efficiently. A popular approach to meta-learning is\nto train a recurrent model to read in a training dataset as input and output\nthe parameters of a learned model, or output predictions for new test inputs.\nAlternatively, a more recent approach to meta-learning aims to acquire deep\nrepresentations that can be effectively fine-tuned, via standard gradient\ndescent, to new tasks. In this paper, we consider the meta-learning problem\nfrom the perspective of universality, formalizing the notion of learning\nalgorithm approximation and comparing the expressive power of the\naforementioned recurrent models to the more recent approaches that embed\ngradient descent into the meta-learner. In particular, we seek to answer the\nfollowing question: does deep representation combined with standard gradient\ndescent have sufficient capacity to approximate any learning algorithm? We find\nthat this is indeed true, and further find, in our experiments, that\ngradient-based meta-learning consistently leads to learning strategies that\ngeneralize more widely compared to those represented by recurrent models. \n\n"}
{"id": "1711.00350", "contents": "Title: Generalization without systematicity: On the compositional skills of\n  sequence-to-sequence recurrent networks Abstract: Humans can understand and produce new utterances effortlessly, thanks to\ntheir compositional skills. Once a person learns the meaning of a new verb\n\"dax,\" he or she can immediately understand the meaning of \"dax twice\" or \"sing\nand dax.\" In this paper, we introduce the SCAN domain, consisting of a set of\nsimple compositional navigation commands paired with the corresponding action\nsequences. We then test the zero-shot generalization capabilities of a variety\nof recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence\nmethods. We find that RNNs can make successful zero-shot generalizations when\nthe differences between training and test commands are small, so that they can\napply \"mix-and-match\" strategies to solve the task. However, when\ngeneralization requires systematic compositional skills (as in the \"dax\"\nexample above), RNNs fail spectacularly. We conclude with a proof-of-concept\nexperiment in neural machine translation, suggesting that lack of systematicity\nmight be partially responsible for neural networks' notorious training data\nthirst. \n\n"}
{"id": "1711.00851", "contents": "Title: Provable defenses against adversarial examples via the convex outer\n  adversarial polytope Abstract: We propose a method to learn deep ReLU-based classifiers that are provably\nrobust against norm-bounded adversarial perturbations on the training data. For\npreviously unseen examples, the approach is guaranteed to detect all\nadversarial examples, though it may flag some non-adversarial examples as well.\nThe basic idea is to consider a convex outer approximation of the set of\nactivations reachable through a norm-bounded perturbation, and we develop a\nrobust optimization procedure that minimizes the worst case loss over this\nouter region (via a linear program). Crucially, we show that the dual problem\nto this linear program can be represented itself as a deep network similar to\nthe backpropagation network, leading to very efficient optimization approaches\nthat produce guaranteed bounds on the robust loss. The end result is that by\nexecuting a few more forward and backward passes through a slightly modified\nversion of the original network (though possibly with much larger batch sizes),\nwe can learn a classifier that is provably robust to any norm-bounded\nadversarial attack. We illustrate the approach on a number of tasks to train\nclassifiers with robust adversarial guarantees (e.g. for MNIST, we produce a\nconvolutional classifier that provably has less than 5.8% test error for any\nadversarial attack with bounded $\\ell_\\infty$ norm less than $\\epsilon = 0.1$),\nand code for all experiments in the paper is available at\nhttps://github.com/locuslab/convex_adversarial. \n\n"}
{"id": "1711.01110", "contents": "Title: A Rudimentary Model for Low-Latency Anonymous Communication Systems Abstract: In this paper we present a rudimentary model for low-latency anonymous\ncommunication systems. Specifically, we study distributed OR algorithm as an\nabstract of the system. Based on our model, we give several satisfactory lower\nbounds of anonymity leakage of a deterministic OR algorithm. Some of them\nreveal a trade-off between anonymity and communication complexity. For the\nrandomized OR algorithm, we only give a relatively trivial but possibly tight\nlower bound when leaving out communication complexity. And we find the\nrelationship between our model and some open case in the study of secret\nsharing scheme, if considering communication complexity. \n\n"}
{"id": "1711.01436", "contents": "Title: Searching for Biophysically Realistic Parameters for Dynamic Neuron\n  Models by Genetic Algorithms from Calcium Imaging Recording Abstract: Individual Neurons in the nervous systems exploit various dynamics. To\ncapture these dynamics for single neurons, we tune the parameters of an\nelectrophysiological model of nerve cells, to fit experimental data obtained by\ncalcium imaging. A search for the biophysical parameters of this model is\nperformed by means of a genetic algorithm, where the model neuron is exposed to\na predefined input current representing overall inputs from other parts of the\nnervous system. The algorithm is then constrained for keeping the ion-channel\ncurrents within reasonable ranges, while producing the best fit to a calcium\nimaging time series of the AVA interneuron, from the brain of the soil-worm, C.\nelegans. Our settings enable us to project a set of biophysical parameters to\nthe the neuron kinetics observed in neuronal imaging. \n\n"}
{"id": "1711.01569", "contents": "Title: Double Q($\\sigma$) and Q($\\sigma, \\lambda$): Unifying Reinforcement\n  Learning Control Algorithms Abstract: Temporal-difference (TD) learning is an important field in reinforcement\nlearning. Sarsa and Q-Learning are among the most used TD algorithms. The\nQ($\\sigma$) algorithm (Sutton and Barto (2017)) unifies both. This paper\nextends the Q($\\sigma$) algorithm to an online multi-step algorithm Q($\\sigma,\n\\lambda$) using eligibility traces and introduces Double Q($\\sigma$) as the\nextension of Q($\\sigma$) to double learning. Experiments suggest that the new\nQ($\\sigma, \\lambda$) algorithm can outperform the classical TD control methods\nSarsa($\\lambda$), Q($\\lambda$) and Q($\\sigma$). \n\n"}
{"id": "1711.01897", "contents": "Title: Simple and efficient GPU parallelization of existing H-Matrix\n  accelerated BEM code Abstract: In this paper, we demonstrate how GPU-accelerated BEM routines can be used in\na simple black-box fashion to accelerate fast boundary element formulations\nbased on Hierarchical Matrices (H-Matrices) with ACA (Adaptive Cross\nApproximation). In particular, we focus on the expensive evaluation of the\ndiscrete weak form of boundary operators associated with the Laplace and the\nHelmholtz equation in three space dimensions. The method is based on offloading\nthe CPU assembly of elements during the ACA assembly onto a GPU device and to\nuse threading strategies across ACA blocks to create sufficient workload for\nthe GPU. The proposed GPU strategy is designed such that it can be implemented\nin existing code with minimal changes to the surrounding application structure.\nThis is in particular interesting for existing legacy code that is not from the\nground-up designed with GPU computing in mind. Our benchmark study gives\nrealistic impressions of the benefits of GPU-accelerated BEM simulations by\nusing state-of-the-art multi-threaded computations on modern high-performance\nCPUs as a reference, rather than drawing synthetic comparisons with\nsingle-threaded codes. Speed-up plots illustrate that performance gains up to a\nfactor of 5.5 could be realized with GPU computing under these conditions. This\nrefers to a boundary element model with about 4 million unknowns, whose\nH-Matrix weak form associated with a real-valued (Laplace) boundary operator is\nset up in only 100 minutes harnessing the two GPUs instead of 9 hours when\nusing the 20 CPU cores at disposal only. The benchmark study is followed by a\nparticularly demanding real-life application, where we compute the scattered\nhigh-frequency sound field of a submarine to demonstrate the increase in\noverall application performance from moving to a GPU-based ACA assembly. \n\n"}
{"id": "1711.01919", "contents": "Title: Fast Integral Histogram Computations on GPU for Real-Time Video\n  Analytics Abstract: In many Multimedia content analytics frameworks feature likelihood maps\nrepresented as histograms play a critical role in the overall algorithm.\nIntegral histograms provide an efficient computational framework for extracting\nmulti-scale histogram-based regional descriptors in constant time which are\nconsidered as the principle building blocks of many video content analytics\nframeworks. We evaluate four different mappings of the integral histogram\ncomputation onto Graphics Processing Units (GPUs) using different kernel\noptimization strategies. Our kernels perform cumulative sums on row and column\nhistograms in a cross-weave or wavefront scan order, use different data\norganization and scheduling methods that is shown to critically affect\nutilization of GPU resources (cores and shared memory). Tiling the 3-D array\ninto smaller regular data blocks significantly speeds up the efficiency of the\ncomputation compared to a strip-based organization. The tiled integral\nhistogram using a diagonal wavefront scan has the best performance of about\n300.4 frames/sec for 640 x 480 images and 32 bins with a speedup factor of\nabout 120 using GTX Titan X graphics card compared to a single threaded\nsequential CPU implementation. Double-buffering has been exploited to overlap\ncomputation and communication across sequence of images. Mapping integral\nhistogram bins computations onto multiple GPUs enables us to process 32 giga\nbytes integral histogram data (of 64MB Image and 128 bins) with a frame rate of\n0.73 Hz and speedup factor of 153X over single-threaded CPU implementation and\nthe speedup of 45X over 16-threaded CPU implementation. \n\n"}
{"id": "1711.02827", "contents": "Title: Inverse Reward Design Abstract: Autonomous agents optimize the reward function we give them. What they don't\nknow is how hard it is for us to design a reward function that actually\ncaptures what we want. When designing the reward, we might think of some\nspecific training scenarios, and make sure that the reward will lead to the\nright behavior in those scenarios. Inevitably, agents encounter new scenarios\n(e.g., new types of terrain) where optimizing that same reward may lead to\nundesired behavior. Our insight is that reward functions are merely\nobservations about what the designer actually wants, and that they should be\ninterpreted in the context in which they were designed. We introduce inverse\nreward design (IRD) as the problem of inferring the true objective based on the\ndesigned reward and the training MDP. We introduce approximate methods for\nsolving IRD problems, and use their solution to plan risk-averse behavior in\ntest MDPs. Empirical results suggest that this approach can help alleviate\nnegative side effects of misspecified reward functions and mitigate reward\nhacking. \n\n"}
{"id": "1711.02976", "contents": "Title: RPYFMM: Parallel Adaptive Fast Multipole Method for\n  Rotne-Prager-Yamakawa Tensor in Biomolecular Hydrodynamics Simulations Abstract: RPYFMM is a software package for the efficient evaluation of the potential\nfield governed by the Rotne-Prager-Yamakawa (RPY) tensor interactions in\nbiomolecular hydrodynamics simulations. In our algorithm, the RPY tensor is\ndecomposed as a linear combination of four Laplace interactions, each of which\nis evaluated using the adaptive fast multipole method (FMM) [1] where the\nexponential expansions are applied to diagonalize the multipole-to-local\ntranslation operators. RPYFMM offers a unified execution on both shared and\ndistributed memory computers by leveraging the DASHMM library [2, 3].\nPreliminary numerical results show that the interactions for a molecular system\nof 15 million particles (beads) can be computed within one second on a Cray\nXC30 cluster using 12, 288 cores, while achieving approximately 54%\nstrong-scaling efficiency. \n\n"}
{"id": "1711.03433", "contents": "Title: h: A Plank for Higher-order Attribute Contraction Schemes Abstract: We present and formalize h, a core (or \"plank\") calculus that can serve as\nthe foundation for several compiler specification languages, notably CRSX\n(Combinatory Reductions Systems with eXtensions), HACS (Higher-order Attribute\nContraction Schemes), and TransScript. We discuss how the h typing and\nformation rules introduce the necessary restrictions to ensure that rewriting\nis well-defined, even in the presence of h's powerful extensions for\nmanipulating free variables and environments as first class elements (including\nin pattern matching). \n\n"}
{"id": "1711.03499", "contents": "Title: Constraints on physical reality arising from a formalization of\n  knowledge Abstract: There are (at least) four ways that an agent can acquire information\nconcerning the state of the universe: via observation, control, prediction, or\nvia retrodiction, i.e., memory. Each of these four ways of acquiring\ninformation seems to rely on a different kind of physical device (resp., an\nobservation device, a control device, etc.). However it turns out that certain\nmathematical structure is common to those four types of device. Any device that\npossesses a certain subset of that structure is known as an \"inference device\"\n(ID).\n  Here I review some of the properties of IDs, including their relation with\nTuring machines, and (more loosely) quantum mechanics. I also review the bounds\nof the joint abilities of any set of IDs to know facts about the physical\nuniverse that contains them. These bounds constrain the possible properties of\nany universe that contains agents who can acquire information concerning that\nuniverse.\n  I then extend this previous work on IDs, by adding to the definition of IDs\nsome of the other mathematical structure that is common to the four ways of\nacquiring information about the universe but is not captured in the (minimal)\ndefinition of IDs. I discuss these extensions of IDs in the context of\nepistemic logic (especially possible worlds formalisms like Kripke structures\nand Aumann structures). In particular, I show that these extensions of IDs are\nnot subject to the problem of logical omniscience that plagues many previously\nstudied forms of epistemic logic. \n\n"}
{"id": "1711.03588", "contents": "Title: A New Proof Rule for Almost-Sure Termination Abstract: An important question for a probabilistic program is whether the probability\nmass of all its diverging runs is zero, that is that it terminates \"almost\nsurely\". Proving that can be hard, and this paper presents a new method for\ndoing so; it is expressed in a program logic, and so applies directly to source\ncode. The programs may contain both probabilistic- and demonic choice, and the\nprobabilistic choices may depend on the current state.\n  As do other researchers, we use variant functions (a.k.a.\n\"super-martingales\") that are real-valued and probabilistically might decrease\non each loop iteration; but our key innovation is that the amount as well as\nthe probability of the decrease are parametric.\n  We prove the soundness of the new rule, indicate where its applicability goes\nbeyond existing rules, and explain its connection to classical results on\ndenumerable (non-demonic) Markov chains. \n\n"}
{"id": "1711.03888", "contents": "Title: In-Depth Exploration of Single-Snapshot Lossy Compression Techniques for\n  N-Body Simulations Abstract: In situ lossy compression allowing user-controlled data loss can\nsignificantly reduce the I/O burden. For large-scale N-body simulations where\nonly one snapshot can be compressed at a time, the lossy compression ratio is\nvery limited because of the fairly low spatial coherence of the particle data.\nIn this work, we assess the state-of-the-art single-snapshot lossy compression\ntechniques of two common N-body simulation models: cosmology and molecular\ndynamics. We design a series of novel optimization techniques based on the two\nrepresentative real-world N-body simulation codes. For molecular dynamics\nsimulation, we propose three compression modes (i.e., best speed, best\ntradeoff, best compression mode) that can refine the tradeoff between the\ncompression rate (a.k.a., speed/throughput) and ratio. For cosmology\nsimulation, we identify that our improved SZ is the best lossy compressor with\nrespect to both compression ratio and rate. Its compression ratio is higher\nthan the second-best compressor by 11% with comparable compression rate.\nExperiments with up to 1024 cores on the Blues supercomputer at Argonne show\nthat our proposed lossy compression method can reduce I/O time by 80% compared\nwith writing data directly to a parallel file system and outperforms the\nsecond-best solution by 60%. Moreover, our proposed lossy compression methods\nhave the best rate-distortion with reasonable compression errors on the tested\nN-body simulation data compared with state-of-the-art compressors. \n\n"}
{"id": "1711.03938", "contents": "Title: CARLA: An Open Urban Driving Simulator Abstract: We introduce CARLA, an open-source simulator for autonomous driving research.\nCARLA has been developed from the ground up to support development, training,\nand validation of autonomous urban driving systems. In addition to open-source\ncode and protocols, CARLA provides open digital assets (urban layouts,\nbuildings, vehicles) that were created for this purpose and can be used freely.\nThe simulation platform supports flexible specification of sensor suites and\nenvironmental conditions. We use CARLA to study the performance of three\napproaches to autonomous driving: a classic modular pipeline, an end-to-end\nmodel trained via imitation learning, and an end-to-end model trained via\nreinforcement learning. The approaches are evaluated in controlled scenarios of\nincreasing difficulty, and their performance is examined via metrics provided\nby CARLA, illustrating the platform's utility for autonomous driving research.\nThe supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E \n\n"}
{"id": "1711.04022", "contents": "Title: Deep Within-Class Covariance Analysis for Robust Audio Representation\n  Learning Abstract: Convolutional Neural Networks (CNNs) can learn effective features, though\nhave been shown to suffer from a performance drop when the distribution of the\ndata changes from training to test data. In this paper we analyze the internal\nrepresentations of CNNs and observe that the representations of unseen data in\neach class, spread more (with higher variance) in the embedding space of the\nCNN compared to representations of the training data. More importantly, this\ndifference is more extreme if the unseen data comes from a shifted\ndistribution. Based on this observation, we objectively evaluate the degree of\nrepresentation's variance in each class via eigenvalue decomposition on the\nwithin-class covariance of the internal representations of CNNs and observe the\nsame behaviour. This can be problematic as larger variances might lead to\nmis-classification if the sample crosses the decision boundary of its class. We\napply nearest neighbor classification on the representations and empirically\nshow that the embeddings with the high variance actually have significantly\nworse KNN classification performances, although this could not be foreseen from\ntheir end-to-end classification results. To tackle this problem, we propose\nDeep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that\nsignificantly reduces the within-class covariance of a DNN's representation,\nimproving performance on unseen test data from a shifted distribution. We\nempirically evaluate DWCCA on two datasets for Acoustic Scene Classification\n(DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA\nsignificantly improve the network's internal representation, it also increases\nthe end-to-end classification accuracy, especially when the test set exhibits a\ndistribution shift. By adding DWCCA to a VGG network, we achieve around 6\npercentage points improvement in the case of a distribution mismatch. \n\n"}
{"id": "1711.04569", "contents": "Title: Multilingual Adaptation of RNN Based ASR Systems Abstract: In this work, we focus on multilingual systems based on recurrent neural\nnetworks (RNNs), trained using the Connectionist Temporal Classification (CTC)\nloss function. Using a multilingual set of acoustic units poses difficulties.\nTo address this issue, we proposed Language Feature Vectors (LFVs) to train\nlanguage adaptive multilingual systems. Language adaptation, in contrast to\nspeaker adaptation, needs to be applied not only on the feature level, but also\nto deeper layers of the network. In this work, we therefore extended our\nprevious approach by introducing a novel technique which we call \"modulation\".\nBased on this method, we modulated the hidden layers of RNNs using LFVs. We\nevaluated this approach in both full and low resource conditions, as well as\nfor grapheme and phone based systems. Lower error rates throughout the\ndifferent conditions could be achieved by the use of the modulation. \n\n"}
{"id": "1711.06100", "contents": "Title: Sequences, Items And Latent Links: Recommendation With Consumed Item\n  Packs Abstract: Recommenders personalize the web content by typically using collaborative\nfiltering to relate users (or items) based on explicit feedback, e.g., ratings.\nThe difficulty of collecting this feedback has recently motivated to consider\nimplicit feedback (e.g., item consumption along with the corresponding time).\n  In this paper, we introduce the notion of consumed item pack (CIP) which\nenables to link users (or items) based on their implicit analogous consumption\nbehavior. Our proposal is generic, and we show that it captures three novel\nimplicit recommenders: a user-based (CIP-U), an item-based (CIP-I), and a word\nembedding-based (DEEPCIP), as well as a state-of-the-art technique using\nimplicit feedback (FISM). We show that our recommenders handle incremental\nupdates incorporating freshly consumed items. We demonstrate that all three\nrecommenders provide a recommendation quality that is competitive with\nstate-of-the-art ones, including one incorporating both explicit and implicit\nfeedback. \n\n"}
{"id": "1711.06920", "contents": "Title: Superlinear Lower Bounds for Distributed Subgraph Detection Abstract: In the distributed subgraph-freeness problem, we are given a graph $H$, and\nasked to determine whether the network graph contains $H$ as a subgraph or not.\nSubgraph-freeness is an extremely local problem: if the network had no\nbandwidth constraints, we could detect any subgraph $H$ in $|H|$ rounds, by\nhaving each node of the network learn its entire $|H|$-neighborhood. However,\nwhen bandwidth is limited, the problem becomes harder.\n  Upper and lower bounds in the presence of congestion have been established\nfor several classes of subgraphs, including cycles, trees, and more complicated\nsubgraphs. All bounds shown so far have been linear or sublinear. We show that\nthe subgraph-freeness problem is not, in general, solvable in linear time: for\nany $k \\geq 2$, there exists a subgraph $H_k$ such that $H_k$-freeness requires\n$\\Omega( n^{2-1/k} / (Bk) )$ rounds to solve. Here $B$ is the bandwidth of each\ncommunication link. The lower bound holds even for diameter-3 subgraphs and\ndiameter-3 network graphs. In particular, taking $k = \\Theta(\\log n)$, we\nobtain a lower bound of $\\Omega(n^2 / (B \\log n))$. \n\n"}
{"id": "1711.07425", "contents": "Title: Modular Continual Learning in a Unified Visual Environment Abstract: A core aspect of human intelligence is the ability to learn new tasks quickly\nand switch between them flexibly. Here, we describe a modular continual\nreinforcement learning paradigm inspired by these abilities. We first introduce\na visual interaction environment that allows many types of tasks to be unified\nin a single framework. We then describe a reward map prediction scheme that\nlearns new tasks robustly in the very large state and action spaces required by\nsuch an environment. We investigate how properties of module architecture\ninfluence efficiency of task learning, showing that a module motif\nincorporating specific design principles (e.g. early bottlenecks, low-order\npolynomial nonlinearities, and symmetry) significantly outperforms more\nstandard neural network motifs, needing fewer training examples and fewer\nneurons to achieve high levels of performance. Finally, we present a\nmeta-controller architecture for task switching based on a dynamic neural\nvoting scheme, which allows new modules to use information learned from\npreviously-seen tasks to substantially improve their own learning efficiency. \n\n"}
{"id": "1711.07632", "contents": "Title: Generating Thematic Chinese Poetry using Conditional Variational\n  Autoencoders with Hybrid Decoders Abstract: Computer poetry generation is our first step towards computer writing.\nWriting must have a theme. The current approaches of using sequence-to-sequence\nmodels with attention often produce non-thematic poems. We present a novel\nconditional variational autoencoder with a hybrid decoder adding the\ndeconvolutional neural networks to the general recurrent neural networks to\nfully learn topic information via latent variables. This approach significantly\nimproves the relevance of the generated poems by representing each line of the\npoem not only in a context-sensitive manner but also in a holistic way that is\nhighly related to the given keyword and the learned topic. A proposed augmented\nword2vec model further improves the rhythm and symmetry. Tests show that the\ngenerated poems by our approach are mostly satisfying with regulated rules and\nconsistent themes, and 73.42% of them receive an Overall score no less than 3\n(the highest score is 5). \n\n"}
{"id": "1711.08699", "contents": "Title: Functorial Semantics for Relational Theories Abstract: We introduce the concept of Frobenius theory as a generalisation of Lawvere's\nfunctorial semantics approach to categorical universal algebra. Whereas the\nuniverse for models of Lawvere theories is the category of sets and functions,\nor more generally cartesian categories, Frobenius theories take their models in\nthe category of sets and relations, or more generally in cartesian\nbicategories. \n\n"}
{"id": "1711.09395", "contents": "Title: Improved Neural Text Attribute Transfer with Non-parallel Data Abstract: Text attribute transfer using non-parallel data requires methods that can\nperform disentanglement of content and linguistic attributes. In this work, we\npropose multiple improvements over the existing approaches that enable the\nencoder-decoder framework to cope with the text attribute transfer from\nnon-parallel data. We perform experiments on the sentiment transfer task using\ntwo datasets. For both datasets, our proposed method outperforms a strong\nbaseline in two of the three employed evaluation metrics. \n\n"}
{"id": "1711.09561", "contents": "Title: HP-GAN: Probabilistic 3D human motion prediction via GAN Abstract: Predicting and understanding human motion dynamics has many applications,\nsuch as motion synthesis, augmented reality, security, and autonomous vehicles.\nDue to the recent success of generative adversarial networks (GAN), there has\nbeen much interest in probabilistic estimation and synthetic data generation\nusing deep neural network architectures and learning algorithms.\n  We propose a novel sequence-to-sequence model for probabilistic human motion\nprediction, trained with a modified version of improved Wasserstein generative\nadversarial networks (WGAN-GP), in which we use a custom loss function designed\nfor human motion prediction. Our model, which we call HP-GAN, learns a\nprobability density function of future human poses conditioned on previous\nposes. It predicts multiple sequences of possible future human poses, each from\nthe same input sequence but a different vector z drawn from a random\ndistribution. Furthermore, to quantify the quality of the non-deterministic\npredictions, we simultaneously train a motion-quality-assessment model that\nlearns the probability that a given skeleton sequence is a real human motion.\n  We test our algorithm on two of the largest skeleton datasets: NTURGB-D and\nHuman3.6M. We train our model on both single and multiple action types. Its\npredictive power for long-term motion estimation is demonstrated by generating\nmultiple plausible futures of more than 30 frames from just 10 frames of input.\nWe show that most sequences generated from the same input have more than 50\\%\nprobabilities of being judged as a real human sequence. We will release all the\ncode used in this paper to Github. \n\n"}
{"id": "1711.09745", "contents": "Title: An edge-fog-cloud platform for anticipatory learning process designed\n  for Internet of Mobile Things Abstract: This paper presents a novel architecture for data analytics targeting an\nanticipatory learning process in the context of the Internet of Mobile Things.\nThe architecture is geo-distributed and composed by edge, fog, and cloud\nresources that operate collectively to support such an anticipatory learning\nprocess. We designed the architecture to manage large volumes of data streams\ncoming from the IoMT devices, analyze in successive phases climbing up in the\nhierarchy of resources from edge, fog and cloud. We discuss the characteristics\nof the analytical tasks at each layer. We notice that the amount of data being\ntransported in the network decreases going from the edge, to the fog and\nfinally to the cloud, while the complexity of the computation increases. Such\ndesign allows to support different kind of analytical needs, from real-time to\nhistorical according to the type of resource being utilized. We have\nimplemented the proposed architecture as a proof-of-concept using the transit\ndata feeds from the area of Greater Moncton, Canada. \n\n"}
{"id": "1711.09883", "contents": "Title: AI Safety Gridworlds Abstract: We present a suite of reinforcement learning environments illustrating\nvarious safety properties of intelligent agents. These problems include safe\ninterruptibility, avoiding side effects, absent supervisor, reward gaming, safe\nexploration, as well as robustness to self-modification, distributional shift,\nand adversaries. To measure compliance with the intended safe behavior, we\nequip each environment with a performance function that is hidden from the\nagent. This allows us to categorize AI safety problems into robustness and\nspecification problems, depending on whether the performance function\ncorresponds to the observed reward function. We evaluate A2C and Rainbow, two\nrecent deep reinforcement learning agents, on our environments and show that\nthey are not able to solve them satisfactorily. \n\n"}
{"id": "1711.09964", "contents": "Title: On the Approximability of Related Machine Scheduling under Arbitrary\n  Precedence Abstract: Distributed computing systems often need to consider the scheduling problem\ninvolving a collection of highly dependent data-processing tasks that must work\nin concert to achieve mission-critical objectives. This paper considers the\nunrelated machine scheduling problem for minimizing weighted sum completion\ntime under arbitrary precedence constraints and on heterogeneous machines with\ndifferent processing speeds. The problem is known to be strongly NP-hard even\nin the single machine setting. By making use of Queyranne's constraint set and\nconstructing a novel Linear Programming relaxation for the scheduling problem\nunder arbitrary precedence constraints, our results in this paper advance the\nstate of the art. We develop a $2(1+(m-1)/D)$-approximation algorithm (and\n$2(1+(m-1)/D)+1$-approximation) for the scheduling problem with zero release\ntime (and arbitrary release time), where $m$ is the number of servers and $D$\nis the task-skewness product. The algorithm can be efficiently computed in\npolynomial time using the Ellipsoid method and achieves nearly optimal\nperformance in practice as $D>O(m)$ when the number of tasks per job to\nschedule is sufficiently larger than the number of machines available. Our\nimplementation and evaluation using a heterogeneous testbed and real-world\nbenchmarks confirms significant improvement in weighted sum completion time for\ndependent computing tasks. \n\n"}
{"id": "1711.10102", "contents": "Title: A Game-theoretic Framework for Revenue Sharing in Edge-Cloud Computing\n  System Abstract: We introduce a game-theoretic framework to ex- plore revenue sharing in an\nEdge-Cloud computing system, in which computing service providers at the edge\nof the Internet (edge providers) and computing service providers at the cloud\n(cloud providers) co-exist and collectively provide computing resources to\nclients (e.g., end users or applications) at the edge. Different from\ntraditional cloud computing, the providers in an Edge-Cloud system are\nindependent and self-interested. To achieve high system-level efficiency, the\nmanager of the system adopts a task distribution mechanism to maximize the\ntotal revenue received from clients and also adopts a revenue sharing mechanism\nto split the received revenue among computing servers (and hence service\nproviders). Under those system-level mechanisms, service providers attempt to\ngame with the system in order to maximize their own utilities, by strategically\nallocating their resources (e.g., computing servers).\n  Our framework models the competition among the providers in an Edge-Cloud\nsystem as a non-cooperative game. Our simulations and experiments on an\nemulation system have shown the existence of Nash equilibrium in such a game.\nWe find that revenue sharing mechanisms have a significant impact on the\nsystem-level efficiency at Nash equilibria, and surprisingly the revenue\nsharing mechanism based directly on actual contributions can result in\nsignificantly worse system efficiency than Shapley value sharing mechanism and\nOrtmann proportional sharing mechanism. Our framework provides an effective\neconomics approach to understanding and designing efficient Edge-Cloud\ncomputing systems. \n\n"}
{"id": "1711.10314", "contents": "Title: Crossmodal Attentive Skill Learner Abstract: This paper presents the Crossmodal Attentive Skill Learner (CASL), integrated\nwith the recently-introduced Asynchronous Advantage Option-Critic (A2OC)\narchitecture [Harb et al., 2017] to enable hierarchical reinforcement learning\nacross multiple sensory inputs. We provide concrete examples where the approach\nnot only improves performance in a single task, but accelerates transfer to new\ntasks. We demonstrate the attention mechanism anticipates and identifies useful\nlatent features, while filtering irrelevant sensor modalities during execution.\nWe modify the Arcade Learning Environment [Bellemare et al., 2013] to support\naudio queries, and conduct evaluations of crossmodal learning in the Atari 2600\ngame Amidar. Finally, building on the recent work of Babaeizadeh et al. [2017],\nwe open-source a fast hybrid CPU-GPU implementation of CASL. \n\n"}
{"id": "1711.10783", "contents": "Title: Partial Consensus and Conservative Fusion of Gaussian Mixtures for\n  Distributed PHD Fusion Abstract: We propose a novel consensus notion, called \"partial consensus\", for\ndistributed GM-PHD (Gaussian mixture probability hypothesis density) fusion\nbased on a peer-to-peer (P2P) sensor network, in which only highly-weighted\nposterior Gaussian components (GCs) are disseminated in the P2P communication\nfor fusion while the insignificant GCs are not involved. The partial consensus\ndoes not only enjoy high efficiency in both network communication and local\nfusion computation, but also significantly reduces the affect of potential\nfalse data (clutter) to the filter, leading to increased signal-to-noise ratio\nat local sensors. Two \"conservative\" mixture reduction schemes are advocated\nfor fusing the shared GCs in a fully distributed manner. One is given by\npairwise averaging GCs between sensors based on Hungarian assignment and the\nother is merging close GCs based a new GM merging scheme. The proposed\napproaches have a close connection to the conservative fusion approaches known\nas covariance union and arithmetic mean density. In parallel, average consensus\nis sought on the cardinality distribution (namely the GM weight sum) among\nsensors. Simulations for tracking either a single target or multiple targets\nthat simultaneously appear are presented based on a sensor network where each\nsensor operates a GM-PHD filter, in order to compare our approaches with the\nbenchmark generalized covariance intersection approach. The results demonstrate\nthat the partial, arithmetic average, consensus outperforms the complete,\ngeometric average, consensus. \n\n"}
{"id": "1711.11225", "contents": "Title: Variational Deep Q Network Abstract: We propose a framework that directly tackles the probability distribution of\nthe value function parameters in Deep Q Network (DQN), with powerful\nvariational inference subroutines to approximate the posterior of the\nparameters. We will establish the equivalence between our proposed surrogate\nobjective and variational inference loss. Our new algorithm achieves efficient\nexploration and performs well on large scale chain Markov Decision Process\n(MDP). \n\n"}
{"id": "1711.11383", "contents": "Title: Learning to Learn from Weak Supervision by Full Supervision Abstract: In this paper, we propose a method for training neural networks when we have\na large set of data with weak labels and a small amount of data with true\nlabels. In our proposed model, we train two neural networks: a target network,\nthe learner and a confidence network, the meta-learner. The target network is\noptimized to perform a given task and is trained using a large set of unlabeled\ndata that are weakly annotated. We propose to control the magnitude of the\ngradient updates to the target network using the scores provided by the second\nconfidence network, which is trained on a small amount of supervised data. Thus\nwe avoid that the weight updates computed from noisy labels harm the quality of\nthe target network model. \n\n"}
{"id": "1712.00193", "contents": "Title: InclusiveFaceNet: Improving Face Attribute Detection with Race and\n  Gender Diversity Abstract: We demonstrate an approach to face attribute detection that retains or\nimproves attribute detection accuracy across gender and race subgroups by\nlearning demographic information prior to learning the attribute detection\ntask. The system, which we call InclusiveFaceNet, detects face attributes by\ntransferring race and gender representations learned from a held-out dataset of\npublic race and gender identities. Leveraging learned demographic\nrepresentations while withholding demographic inference from the downstream\nface attribute detection task preserves potential users' demographic privacy\nwhile resulting in some of the best reported numbers to date on attribute\ndetection in the Faces of the World and CelebA datasets. \n\n"}
{"id": "1712.00725", "contents": "Title: Sentiment Classification using Images and Label Embeddings Abstract: In this project we analysed how much semantic information images carry, and\nhow much value image data can add to sentiment analysis of the text associated\nwith the images. To better understand the contribution from images, we compared\nmodels which only made use of image data, models which only made use of text\ndata, and models which combined both data types. We also analysed if this\napproach could help sentiment classifiers generalize to unknown sentiments. \n\n"}
{"id": "1712.01113", "contents": "Title: Layer by layer - Combining Monads Abstract: We develop a method to incrementally construct programming languages. Our\napproach is categorical: each layer of the language is described as a monad.\nOur method either (i) concretely builds a distributive law between two monads,\ni.e. layers of the language, which then provides a monad structure to the\ncomposition of layers, or (ii) identifies precisely the algebraic obstacles to\nthe existence of a distributive law and gives a best approximant language. The\nrunning example will involve three layers: a basic imperative language enriched\nfirst by adding non-determinism and then probabilistic choice. The first\nextension works seamlessly, but the second encounters an obstacle, which\nresults in a best approximant language structurally very similar to the\nprobabilistic network specification language ProbNetKAT. \n\n"}
{"id": "1712.03086", "contents": "Title: FlagIt: A System for Minimally Supervised Human Trafficking Indicator\n  Mining Abstract: In this paper, we describe and study the indicator mining problem in the\nonline sex advertising domain. We present an in-development system, FlagIt\n(Flexible and adaptive generation of Indicators from text), which combines the\nbenefits of both a lightweight expert system and classical semi-supervision\n(heuristic re-labeling) with recently released state-of-the-art unsupervised\ntext embeddings to tag millions of sentences with indicators that are highly\ncorrelated with human trafficking. The FlagIt technology stack is open source.\nOn preliminary evaluations involving five indicators, FlagIt illustrates\npromising performance compared to several alternatives. The system is being\nactively developed, refined and integrated into a domain-specific search system\nused by over 200 law enforcement agencies to combat human trafficking, and is\nbeing aggressively extended to mine at least six more indicators with minimal\nprogramming effort. FlagIt is a good example of a system that operates in\nlimited label settings, and that requires creative combinations of established\nmachine learning techniques to produce outputs that could be used by real-world\nnon-technical analysts. \n\n"}
{"id": "1712.03141", "contents": "Title: Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning Abstract: Learning-based pattern classifiers, including deep networks, have shown\nimpressive performance in several application domains, ranging from computer\nvision to cybersecurity. However, it has also been shown that adversarial input\nperturbations carefully crafted either at training or at test time can easily\nsubvert their predictions. The vulnerability of machine learning to such wild\npatterns (also referred to as adversarial examples), along with the design of\nsuitable countermeasures, have been investigated in the research field of\nadversarial machine learning. In this work, we provide a thorough overview of\nthe evolution of this research area over the last ten years and beyond,\nstarting from pioneering, earlier work on the security of non-deep learning\nalgorithms up to more recent work aimed to understand the security properties\nof deep learning algorithms, in the context of computer vision and\ncybersecurity tasks. We report interesting connections between these\napparently-different lines of work, highlighting common misconceptions related\nto the security evaluation of machine-learning algorithms. We review the main\nthreat models and attacks defined to this end, and discuss the main limitations\nof current work, along with the corresponding future challenges towards the\ndesign of more secure learning algorithms. \n\n"}
{"id": "1712.04155", "contents": "Title: Toward `verifying' a Water Treatment System Abstract: Modeling and verifying real-world cyber-physical systems is challenging,\nwhich is especially so for complex systems where manually modeling is\ninfeasible. In this work, we report our experience on combining model learning\nand abstraction refinement to analyze a challenging system, i.e., a real-world\nSecure Water Treatment system (SWaT). Given a set of safety requirements, the\nobjective is to either show that the system is safe with a high probability (so\nthat a system shutdown is rarely triggered due to safety violation) or not. As\nthe system is too complicated to be manually modeled, we apply latest automatic\nmodel learning techniques to construct a set of Markov chains through\nabstraction and refinement, based on two long system execution logs (one for\ntraining and the other for testing). For each probabilistic safety property, we\neither report it does not hold with a certain level of probabilistic\nconfidence, or report that it holds by showing the evidence in the form of an\nabstract Markov chain. The Markov chains can subsequently be implemented as\nruntime monitors in SWaT. \n\n"}
{"id": "1712.05087", "contents": "Title: Learning Binary Residual Representations for Domain-specific Video\n  Streaming Abstract: We study domain-specific video streaming. Specifically, we target a streaming\nsetting where the videos to be streamed from a server to a client are all in\nthe same domain and they have to be compressed to a small size for low-latency\ntransmission. Several popular video streaming services, such as the video game\nstreaming services of GeForce Now and Twitch, fall in this category. While\nconventional video compression standards such as H.264 are commonly used for\nthis task, we hypothesize that one can leverage the property that the videos\nare all in the same domain to achieve better video quality. Based on this\nhypothesis, we propose a novel video compression pipeline. Specifically, we\nfirst apply H.264 to compress domain-specific videos. We then train a novel\nbinary autoencoder to encode the leftover domain-specific residual information\nframe-by-frame into binary representations. These binary representations are\nthen compressed and sent to the client together with the H.264 stream. In our\nexperiments, we show that our pipeline yields consistent gains over standard\nH.264 compression across several benchmark datasets while using the same\nchannel bandwidth. \n\n"}
{"id": "1712.05363", "contents": "Title: A Probability Monad as the Colimit of Spaces of Finite Samples Abstract: We define and study a probability monad on the category of complete metric\nspaces and short maps. It assigns to each space the space of Radon probability\nmeasures on it with finite first moment, equipped with the\nKantorovich-Wasserstein distance. This monad is analogous to the Giry monad on\nthe category of Polish spaces, and it extends a construction due to van Breugel\nfor compact and for 1-bounded complete metric spaces.\n  We prove that this Kantorovich monad arises from a colimit construction on\nfinite power-like constructions, which formalizes the intuition that\nprobability measures are limits of finite samples. The proof relies on a\ncriterion for when an ordinary left Kan extension of lax monoidal functors is a\nmonoidal Kan extension. The colimit characterization allows the development of\nintegration theory and the treatment of measures on spaces of measures, without\nmeasure theory.\n  We also show that the category of algebras of the Kantorovich monad is\nequivalent to the category of closed convex subsets of Banach spaces with short\naffine maps as morphisms. \n\n"}
{"id": "1712.06497", "contents": "Title: HERO: Heterogeneous Embedded Research Platform for Exploring RISC-V\n  Manycore Accelerators on FPGA Abstract: Heterogeneous embedded systems on chip (HESoCs) co-integrate a standard host\nprocessor with programmable manycore accelerators (PMCAs) to combine\ngeneral-purpose computing with domain-specific, efficient processing\ncapabilities. While leading companies successfully advance their HESoC\nproducts, research lags behind due to the challenges of building a prototyping\nplatform that unites an industry-standard host processor with an open research\nPMCA architecture. In this work we introduce HERO, an FPGA-based research\nplatform that combines a PMCA composed of clusters of RISC-V cores, implemented\nas soft cores on an FPGA fabric, with a hard ARM Cortex-A multicore host\nprocessor. The PMCA architecture mapped on the FPGA is silicon-proven,\nscalable, configurable, and fully modifiable. HERO includes a complete software\nstack that consists of a heterogeneous cross-compilation toolchain with support\nfor OpenMP accelerator programming, a Linux driver, and runtime libraries for\nboth host and PMCA. HERO is designed to facilitate rapid exploration on all\nsoftware and hardware layers: run-time behavior can be accurately analyzed by\ntracing events, and modifications can be validated through fully automated hard\nware and software builds and executed tests. We demonstrate the usefulness of\nHERO by means of case studies from our research. \n\n"}
{"id": "1712.06536", "contents": "Title: Nonparametric Inference for Auto-Encoding Variational Bayes Abstract: We would like to learn latent representations that are low-dimensional and\nhighly interpretable. A model that has these characteristics is the Gaussian\nProcess Latent Variable Model. The benefits and negative of the GP-LVM are\ncomplementary to the Variational Autoencoder, the former provides interpretable\nlow-dimensional latent representations while the latter is able to handle large\namounts of data and can use non-Gaussian likelihoods. Our inspiration for this\npaper is to marry these two approaches and reap the benefits of both. In order\nto do so we will introduce a novel approximate inference scheme inspired by the\nGP-LVM and the VAE. We show experimentally that the approximation allows the\ncapacity of the generative bottle-neck (Z) of the VAE to be arbitrarily large\nwithout losing a highly interpretable representation, allowing reconstruction\nquality to be unlimited by Z at the same time as a low-dimensional space can be\nused to perform ancestral sampling from as well as a means to reason about the\nembedded data. \n\n"}
{"id": "1712.06568", "contents": "Title: ES Is More Than Just a Traditional Finite-Difference Approximator Abstract: An evolution strategy (ES) variant based on a simplification of a natural\nevolution strategy recently attracted attention because it performs\nsurprisingly well in challenging deep reinforcement learning domains. It\nsearches for neural network parameters by generating perturbations to the\ncurrent set of parameters, checking their performance, and moving in the\naggregate direction of higher reward. Because it resembles a traditional\nfinite-difference approximation of the reward gradient, it can naturally be\nconfused with one. However, this ES optimizes for a different gradient than\njust reward: It optimizes for the average reward of the entire population,\nthereby seeking parameters that are robust to perturbation. This difference can\nchannel ES into distinct areas of the search space relative to gradient\ndescent, and also consequently to networks with distinct properties. This\nunique robustness-seeking property, and its consequences for optimization, are\ndemonstrated in several domains. They include humanoid locomotion, where\nnetworks from policy gradient-based reinforcement learning are significantly\nless robust to parameter perturbation than ES-based policies solving the same\ntask. While the implications of such robustness and robustness-seeking remain\nopen to further study, this work's main contribution is to highlight such\ndifferences and their potential importance. \n\n"}
{"id": "1712.06868", "contents": "Title: Heinrich Behmann's Contributions to Second-Order Quantifier Elimination\n  from the View of Computational Logic Abstract: For relational monadic formulas (the L\\\"owenheim class) second-order\nquantifier elimination, which is closely related to computation of uniform\ninterpolants, projection and forgetting - operations that currently receive\nmuch attention in knowledge processing - always succeeds. The decidability\nproof for this class by Heinrich Behmann from 1922 explicitly proceeds by\nelimination with equivalence preserving formula rewriting. Here we reconstruct\nthe results from Behmann's publication in detail and discuss related issues\nthat are relevant in the context of modern approaches to second-order\nquantifier elimination in computational logic. In addition, an extensive\ndocumentation of the letters and manuscripts in Behmann's bequest that concern\nsecond-order quantifier elimination is given, including a commented register\nand English abstracts of the German sources with focus on technical material.\nIn the late 1920s Behmann attempted to develop an elimination-based decision\nmethod for formulas with predicates whose arity is larger than one. His\nmanuscripts and the correspondence with Wilhelm Ackermann show technical\naspects that are still of interest today and give insight into the genesis of\nAckermann's landmark paper \"Untersuchungen \\\"uber das Eliminationsproblem der\nmathematischen Logik\" from 1935, which laid the foundation of the two\nprevailing modern approaches to second-order quantifier elimination. \n\n"}
{"id": "1712.08091", "contents": "Title: Multiview Deep Learning for Predicting Twitter Users' Location Abstract: The problem of predicting the location of users on large social networks like\nTwitter has emerged from real-life applications such as social unrest detection\nand online marketing. Twitter user geolocation is a difficult and active\nresearch topic with a vast literature. Most of the proposed methods follow\neither a content-based or a network-based approach. The former exploits\nuser-generated content while the latter utilizes the connection or interaction\nbetween Twitter users. In this paper, we introduce a novel method combining the\nstrength of both approaches. Concretely, we propose a multi-entry neural\nnetwork architecture named MENET leveraging the advances in deep learning and\nmultiview learning. The generalizability of MENET enables the integration of\nmultiple data representations. In the context of Twitter user geolocation, we\nrealize MENET with textual, network, and metadata features. Considering the\nnatural distribution of Twitter users across the concerned geographical area,\nwe subdivide the surface of the earth into multi-scale cells and train MENET\nwith the labels of the cells. We show that our method outperforms the state of\nthe art by a large margin on three benchmark datasets. \n\n"}
{"id": "1712.08163", "contents": "Title: Reachable Set Computation and Safety Verification for Neural Networks\n  with ReLU Activations Abstract: Neural networks have been widely used to solve complex real-world problems.\nDue to the complicate, nonlinear, non-convex nature of neural networks, formal\nsafety guarantees for the output behaviors of neural networks will be crucial\nfor their applications in safety-critical systems.In this paper, the output\nreachable set computation and safety verification problems for a class of\nneural networks consisting of Rectified Linear Unit (ReLU) activation functions\nare addressed. A layer-by-layer approach is developed to compute output\nreachable set. The computation is formulated in the form of a set of\nmanipulations for a union of polyhedra, which can be efficiently applied with\nthe aid of polyhedron computation tools. Based on the output reachable set\ncomputation results, the safety verification for a ReLU neural network can be\nperformed by checking the intersections of unsafe regions and output reachable\nset described by a union of polyhedra. A numerical example of a randomly\ngenerated ReLU neural network is provided to show the effectiveness of the\napproach developed in this paper. \n\n"}
{"id": "1712.09098", "contents": "Title: SoA-Fog: Secure Service-Oriented Edge Computing Architecture for Smart\n  Health Big Data Analytics Abstract: The smart health paradigms employ Internet-connected wearables for\ntelemonitoring, diagnosis for providing inexpensive healthcare solutions. Fog\ncomputing reduces latency and increases throughput by processing data near the\nbody sensor network. In this paper, we proposed a secure serviceorientated edge\ncomputing architecture that is validated on recently released public dataset.\nResults and discussions support the applicability of proposed architecture for\nsmart health applications. We proposed SoA-Fog i.e. a three-tier secure\nframework for efficient management of health data using fog devices. It discuss\nthe security aspects in client layer, fog layer and the cloud layer. We design\nthe prototype by using win-win spiral model with use case and sequence diagram.\nOverlay analysis was performed using proposed framework on malaria vector borne\ndisease positive maps of Maharastra state in India from 2011 to 2014. The\nmobile clients were taken as test case. We performed comparative analysis\nbetween proposed secure fog framework and state-of-the art cloud-based\nframework. \n\n"}
{"id": "1712.09381", "contents": "Title: RLlib: Abstractions for Distributed Reinforcement Learning Abstract: Reinforcement learning (RL) algorithms involve the deep nesting of highly\nirregular computation patterns, each of which typically exhibits opportunities\nfor distributed computation. We argue for distributing RL components in a\ncomposable way by adapting algorithms for top-down hierarchical control,\nthereby encapsulating parallelism and resource requirements within\nshort-running compute tasks. We demonstrate the benefits of this principle\nthrough RLlib: a library that provides scalable software primitives for RL.\nThese primitives enable a broad range of algorithms to be implemented with high\nperformance, scalability, and substantial code reuse. RLlib is available at\nhttps://rllib.io/. \n\n"}
{"id": "1712.10050", "contents": "Title: Kernel Robust Bias-Aware Prediction under Covariate Shift Abstract: Under covariate shift, training (source) data and testing (target) data\ndiffer in input space distribution, but share the same conditional label\ndistribution. This poses a challenging machine learning task. Robust Bias-Aware\n(RBA) prediction provides the conditional label distribution that is robust to\nthe worstcase logarithmic loss for the target distribution while matching\nfeature expectation constraints from the source distribution. However,\nemploying RBA with insufficient feature constraints may result in high\ncertainty predictions for much of the source data, while leaving too much\nuncertainty for target data predictions. To overcome this issue, we extend the\nrepresenter theorem to the RBA setting, enabling minimization of regularized\nexpected target risk by a reweighted kernel expectation under the source\ndistribution. By applying kernel methods, we establish consistency guarantees\nand demonstrate better performance of the RBA classifier than competing methods\non synthetically biased UCI datasets as well as datasets that have natural\ncovariate shift. \n\n"}
{"id": "1801.00471", "contents": "Title: TWAM: A Certifying Abstract Machine for Logic Programs Abstract: Type-preserving (or typed) compilation uses typing derivations to certify\ncorrectness properties of compilation. We have designed and implemented a\ntype-preserving compiler for a simply-typed dialect of Prolog we call T-Prolog.\nThe crux of our approach is a new certifying abstract machine which we call the\nTyped Warren Abstract Machine (TWAM). The TWAM has a dependent type system\nstrong enough to specify the semantics of a logic program in the logical\nframework LF. We present a soundness metatheorem which constitutes a partial\ncorrectness guarantee: well-typed programs implement the logic program\nspecified by their type. This metatheorem justifies our design and\nimplementation of a certifying compiler from T-Prolog to TWAM. \n\n"}
{"id": "1801.01037", "contents": "Title: Distributed Memory Techniques for Classical Simulation of Quantum\n  Circuits Abstract: In this paper we describe, implement, and test the performance of distributed\nmemory simulations of quantum circuits on the MSU Laconia Top500 supercomputer.\nUsing OpenMP and MPI hybrid parallelization, we first use a distributed\nmatrix-vector multiplication with one-dimensional partitioning and discuss the\nshortcomings of this method due to the exponential memory requirements in\nsimulating quantum computers. We then describe a more efficient method that\nstores only the $2^n$ amplitudes of the $n$ qubit state vector $|\\psi\\rangle$\nand optimize its single node performance. In our multi-node implementation, we\nuse a single amplitude communication protocol that maximizes the number of\nqubits able to be simulated and minimizes the ratio of qubits that require\ncommunication to those that do not, and we present an algorithm for efficiently\ndetermining communication pairs among processors. We simulate up to 30 qubits\non a single node and 33 qubits with the state vector partitioned across 64\nnodes. Lastly, we discuss the advantages and disadvantages of our communication\nscheme, propose potential improvements, and describe other optimizations such\nas storing the state vector non-sequentially in memory to map communication\nrequirements to idle qubits in the circuit. \n\n"}
{"id": "1801.01174", "contents": "Title: Concurrent and Adaptive Extreme Scale Binding Free Energy Calculations Abstract: The efficacy of drug treatments depends on how tightly small molecules bind\nto their target proteins. The rapid and accurate quantification of the strength\nof these interactions (as measured by binding affinity) is a grand challenge of\ncomputational chemistry, surmounting which could revolutionize drug design and\nprovide the platform for patient-specific medicine. Recent evidence suggests\nthat molecular dynamics (MD) can achieve useful predictive accuracy (< 1\nkcal/mol). For this predictive accuracy to impact clinical decision making,\nbinding free energy computational campaigns must provide results rapidly and\nwithout loss of accuracy. This demands advances in algorithms, scalable\nsoftware systems, and efficient utilization of supercomputing resources. We\nintroduce a framework called HTBAC, designed to support accurate and scalable\ndrug binding affinity calculations, while marshaling large simulation\ncampaigns. We show that HTBAC supports the specification and execution of\nfree-energy protocols at scale. This paper makes three main contributions: (1)\nshows the importance of adaptive execution for ensemble-based free energy\nprotocols to improve binding affinity accuracy; (2) presents and characterizes\nHTBAC -- a software system that enables the scalable and adaptive execution of\nbinding affinity protocols at scale; and (3) for a widely used free-energy\nprotocol (TIES), shows improvements in the accuracy of simulations for a fixed\namount of resource, or reduced resource consumption for a fixed accuracy as a\nconsequence of adaptive execution. \n\n"}
{"id": "1801.02774", "contents": "Title: Adversarial Spheres Abstract: State of the art computer vision models have been shown to be vulnerable to\nsmall adversarial perturbations of the input. In other words, most images in\nthe data distribution are both correctly classified by the model and are very\nclose to a visually similar misclassified image. Despite substantial research\ninterest, the cause of the phenomenon is still poorly understood and remains\nunsolved. We hypothesize that this counter intuitive behavior is a naturally\noccurring result of the high dimensional geometry of the data manifold. As a\nfirst step towards exploring this hypothesis, we study a simple synthetic\ndataset of classifying between two concentric high dimensional spheres. For\nthis dataset we show a fundamental tradeoff between the amount of test error\nand the average distance to nearest error. In particular, we prove that any\nmodel which misclassifies a small constant fraction of a sphere will be\nvulnerable to adversarial perturbations of size $O(1/\\sqrt{d})$. Surprisingly,\nwhen we train several different architectures on this dataset, all of their\nerror sets naturally approach this theoretical bound. As a result of the\ntheory, the vulnerability of neural networks to small adversarial perturbations\nis a logical consequence of the amount of test error observed. We hope that our\ntheoretical analysis of this very simple case will point the way forward to\nexplore how the geometry of complex real-world data sets leads to adversarial\nexamples. \n\n"}
{"id": "1801.03454", "contents": "Title: Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters\n  in Deep Neural Networks Abstract: In an effort to understand the meaning of the intermediate representations\ncaptured by deep networks, recent papers have tried to associate specific\nsemantic concepts to individual neural network filter responses, where\ninteresting correlations are often found, largely by focusing on extremal\nfilter responses. In this paper, we show that this approach can favor\neasy-to-interpret cases that are not necessarily representative of the average\nbehavior of a representation.\n  A more realistic but harder-to-study hypothesis is that semantic\nrepresentations are distributed, and thus filters must be studied in\nconjunction. In order to investigate this idea while enabling systematic\nvisualization and quantification of multiple filter responses, we introduce the\nNet2Vec framework, in which semantic concepts are mapped to vectorial\nembeddings based on corresponding filter responses. By studying such\nembeddings, we are able to show that 1., in most cases, multiple filters are\nrequired to code for a concept, that 2., often filters are not concept specific\nand help encode multiple concepts, and that 3., compared to single filter\nactivations, filter embeddings are able to better characterize the meaning of a\nrepresentation and its relationship to other concepts. \n\n"}
{"id": "1801.03859", "contents": "Title: Oink: an Implementation and Evaluation of Modern Parity Game Solvers Abstract: Parity games have important practical applications in formal verification and\nsynthesis, especially to solve the model-checking problem of the modal\nmu-calculus. They are also interesting from the theory perspective, as they are\nwidely believed to admit a polynomial solution, but so far no such algorithm is\nknown. In recent years, a number of new algorithms and improvements to existing\nalgorithms have been proposed. We implement a new and easy to extend tool Oink,\nwhich is a high-performance implementation of modern parity game algorithms. We\nfurther present a comprehensive empirical evaluation of modern parity game\nalgorithms and solvers, both on real world benchmarks and randomly generated\ngames. Our experiments show that our new tool Oink outperforms the current\nstate-of-the-art. \n\n"}
{"id": "1801.04185", "contents": "Title: A reference model for interaction semantics Abstract: In this article, we introduce a reference model for interaction semantics\namong communicating discrete systems to guide the discourse on\ninteroperability.\n  The necessary set of unifying concepts is small and comprises essentially the\nnotion of discrete systems interacting by exchanging information. It is based\non a simple, but nevertheless complete classification of system interactions\nwith respect to information transport and processing. Information transport can\nonly be uni- or bidirectional and information processing is subclassified along\nthe binary dimensions of state, determinism and synchronicity.\n  For interactions with bidirectional information flow we are able to define a\ncriterion for a layered structure of systems: we name a bidirectional\ninteraction \"horizontal\" if all interacting systems behave the same with\nrespect to state, determinism and synchronicity and we name it \"vertical\" ---\nproviding a semantic direction --- if there is a behavioral asymmetry between\nthe interacting systems with respect to these properties.\n  It is shown that horizontal interactions are essentially stateful,\nasynchronous and nondeterministic and are described by protocols. Vertical\ninteractions are essentially top-down-usage, described by object models or\noperations, and bottom-up-observation, described by anonymous events.\n  The reference model thereby helps us to understand the significant\nrelationships that are created between interacting discrete systems by their\ninteractions and guides us on how to talk about discrete system\ninteroperability.\n  To show its conceptual power, we apply the reference model to assess several\nother architectural models, communication technologies and so called software\ndesign or architectural styles like SOA and REST. \n\n"}
{"id": "1801.04306", "contents": "Title: A Workload Analysis of NSF's Innovative HPC Resources Using XDMoD Abstract: Workload characterization is an integral part of performance analysis of high\nperformance computing (HPC) systems. An understanding of workload properties\nsheds light on resource utilization and can be used to inform performance\noptimization both at the software and system configuration levels. It can\nprovide information on how computational science usage modalities are changing\nthat could potentially aid holistic capacity planning for the wider HPC\necosystem. Here, we report on the results of a detailed workload analysis of\nthe portfolio of supercomputers comprising the NSF Innovative HPC program in\norder to characterize its past and current workload and look for trends to\nunderstand the nature of how the broad portfolio of computational science\nresearch is being supported and how it is changing over time. The workload\nanalysis also sought to illustrate a wide variety of usage patterns and\nperformance requirements for jobs running on these systems. File system\nperformance, memory utilization and the types of parallelism employed by users\n(MPI, threads, etc) were also studied for all systems for which job level\nperformance data was available. \n\n"}
{"id": "1801.04487", "contents": "Title: Better Runtime Guarantees Via Stochastic Domination Abstract: Apart from few exceptions, the mathematical runtime analysis of evolutionary\nalgorithms is mostly concerned with expected runtimes. In this work, we argue\nthat stochastic domination is a notion that should be used more frequently in\nthis area. Stochastic domination allows to formulate much more informative\nperformance guarantees, it allows to decouple the algorithm analysis into the\ntrue algorithmic part of detecting a domination statement and the\nprobability-theoretical part of deriving the desired probabilistic guarantees\nfrom this statement, and it helps finding simpler and more natural proofs. As\nparticular results, we prove a fitness level theorem which shows that the\nruntime is dominated by a sum of independent geometric random variables, we\nprove the first tail bounds for several classic runtime problems, and we give a\nshort and natural proof for Witt's result that the runtime of any $(\\mu,p)$\nmutation-based algorithm on any function with unique optimum is subdominated by\nthe runtime of a variant of the \\oea on the \\onemax function. As side-products,\nwe determine the fastest unbiased (1+1) algorithm for the \\leadingones\nbenchmark problem, both in the general case and when restricted to static\nmutation operators, and we prove a Chernoff-type tail bound for sums of\nindependent coupon collector distributions. \n\n"}
{"id": "1801.04686", "contents": "Title: Hierarchical Coding for Distributed Computing Abstract: Coding for distributed computing supports low-latency computation by\nrelieving the burden of straggling workers. While most existing works assume a\nsimple master-worker model, we consider a hierarchical computational structure\nconsisting of groups of workers, motivated by the need to reflect the\narchitectures of real-world distributed computing systems. In this work, we\npropose a hierarchical coding scheme for this model, as well as analyze its\ndecoding cost and expected computation time. Specifically, we first provide\nupper and lower bounds on the expected computing time of the proposed scheme.\nWe also show that our scheme enables efficient parallel decoding, thus reducing\ndecoding costs by orders of magnitude over non-hierarchical schemes. When\nconsidering both decoding cost and computing time, the proposed hierarchical\ncoding is shown to outperform existing schemes in many practical scenarios. \n\n"}
{"id": "1801.06889", "contents": "Title: Visual Analytics in Deep Learning: An Interrogative Survey for the Next\n  Frontiers Abstract: Deep learning has recently seen rapid development and received significant\nattention due to its state-of-the-art performance on previously-thought hard\nproblems. However, because of the internal complexity and nonlinear structure\nof deep neural networks, the underlying decision making processes for why these\nmodels are achieving such performance are challenging and sometimes mystifying\nto interpret. As deep learning spreads across domains, it is of paramount\nimportance that we equip users of deep learning with tools for understanding\nwhen a model works correctly, when it fails, and ultimately how to improve its\nperformance. Standardized toolkits for building neural networks have helped\ndemocratize deep learning; visual analytics systems have now been developed to\nsupport model explanation, interpretation, debugging, and improvement. We\npresent a survey of the role of visual analytics in deep learning research,\nwhich highlights its short yet impactful history and thoroughly summarizes the\nstate-of-the-art using a human-centered interrogative framework, focusing on\nthe Five W's and How (Why, Who, What, How, When, and Where). We conclude by\nhighlighting research directions and open research problems. This survey helps\nresearchers and practitioners in both visual analytics and deep learning to\nquickly learn key aspects of this young and rapidly growing body of research,\nwhose impact spans a diverse range of domains. \n\n"}
{"id": "1801.07630", "contents": "Title: Task-parallel Analysis of Molecular Dynamics Trajectories Abstract: Different parallel frameworks for implementing data analysis applications\nhave been proposed by the HPC and Big Data communities. In this paper, we\ninvestigate three task-parallel frameworks: Spark, Dask and RADICAL-Pilot with\nrespect to their ability to support data analytics on HPC resources and compare\nthem with MPI. We investigate the data analysis requirements of Molecular\nDynamics (MD) simulations which are significant consumers of supercomputing\ncycles, producing immense amounts of data. A typical large-scale MD simulation\nof a physical system of O(100k) atoms over {\\mu}secs can produce from O(10) GB\nto O(1000) GBs of data. We propose and evaluate different approaches for\nparallelization of a representative set of MD trajectory analysis algorithms,\nin particular the computation of path similarity and leaflet identification. We\nevaluate Spark, Dask and RADICAL-Pilot with respect to their abstractions and\nruntime engine capabilities to support these algorithms. We provide a\nconceptual basis for comparing and understanding different frameworks that\nenable users to select the optimal system for each application. We also provide\na quantitative performance analysis of the different algorithms across the\nthree frameworks. \n\n"}
{"id": "1801.07791", "contents": "Title: PointCNN: Convolution On $\\mathcal{X}$-Transformed Points Abstract: We present a simple and general framework for feature learning from point\nclouds. The key to the success of CNNs is the convolution operator that is\ncapable of leveraging spatially-local correlation in data represented densely\nin grids (e.g. images). However, point clouds are irregular and unordered, thus\ndirectly convolving kernels against features associated with the points, will\nresult in desertion of shape information and variance to point ordering. To\naddress these problems, we propose to learn an $\\mathcal{X}$-transformation\nfrom the input points, to simultaneously promote two causes. The first is the\nweighting of the input features associated with the points, and the second is\nthe permutation of the points into a latent and potentially canonical order.\nElement-wise product and sum operations of the typical convolution operator are\nsubsequently applied on the $\\mathcal{X}$-transformed features. The proposed\nmethod is a generalization of typical CNNs to feature learning from point\nclouds, thus we call it PointCNN. Experiments show that PointCNN achieves on\npar or better performance than state-of-the-art methods on multiple challenging\nbenchmark datasets and tasks. \n\n"}
{"id": "1801.08441", "contents": "Title: Finitary-based Domain Theory in Coq: An Early Report Abstract: In domain theory every finite computable object can be represented by a\nsingle mathematical object instead of a set of objects, using the notion of\nfinitary-basis. In this article we report on our effort to formalize domain\ntheory in Coq in terms of finitary-basis. \n\n"}
{"id": "1801.09597", "contents": "Title: Deep Reinforcement Learning using Capsules in Advanced Game Environments Abstract: Reinforcement Learning (RL) is a research area that has blossomed\ntremendously in recent years and has shown remarkable potential for artificial\nintelligence based opponents in computer games. This success is primarily due\nto vast capabilities of Convolutional Neural Networks (ConvNet), enabling\nalgorithms to extract useful information from noisy environments. Capsule\nNetwork (CapsNet) is a recent introduction to the Deep Learning algorithm group\nand has only barely begun to be explored. The network is an architecture for\nimage classification, with superior performance for classification of the MNIST\ndataset. CapsNets have not been explored beyond image classification.\n  This thesis introduces the use of CapsNet for Q-Learning based game\nalgorithms. To successfully apply CapsNet in advanced game play, three main\ncontributions follow. First, the introduction of four new game environments as\nframeworks for RL research with increasing complexity, namely Flash RL, Deep\nLine Wars, Deep RTS, and Deep Maze. These environments fill the gap between\nrelatively simple and more complex game environments available for RL research\nand are in the thesis used to test and explore the CapsNet behavior.\n  Second, the thesis introduces a generative modeling approach to produce\nartificial training data for use in Deep Learning models including CapsNets. We\nempirically show that conditional generative modeling can successfully generate\ngame data of sufficient quality to train a Deep Q-Network well.\n  Third, we show that CapsNet is a reliable architecture for Deep Q-Learning\nbased algorithms for game AI. A capsule is a group of neurons that determine\nthe presence of objects in the data and is in the literature shown to increase\nthe robustness of training and predictions while lowering the amount training\ndata needed. It should, therefore, be ideally suited for game plays. \n\n"}
{"id": "1801.10437", "contents": "Title: Deep Learning Works in Practice. But Does it Work in Theory? Abstract: Deep learning relies on a very specific kind of neural networks: those\nsuperposing several neural layers. In the last few years, deep learning\nachieved major breakthroughs in many tasks such as image analysis, speech\nrecognition, natural language processing, and so on. Yet, there is no\ntheoretical explanation of this success. In particular, it is not clear why the\ndeeper the network, the better it actually performs.\n  We argue that the explanation is intimately connected to a key feature of the\ndata collected from our surrounding universe to feed the machine learning\nalgorithms: large non-parallelizable logical depth. Roughly speaking, we\nconjecture that the shortest computational descriptions of the universe are\nalgorithms with inherently large computation times, even when a large number of\ncomputers are available for parallelization. Interestingly, this conjecture,\ncombined with the folklore conjecture in theoretical computer science that $ P\n\\neq NC$, explains the success of deep learning. \n\n"}
{"id": "1802.00420", "contents": "Title: Obfuscated Gradients Give a False Sense of Security: Circumventing\n  Defenses to Adversarial Examples Abstract: We identify obfuscated gradients, a kind of gradient masking, as a phenomenon\nthat leads to a false sense of security in defenses against adversarial\nexamples. While defenses that cause obfuscated gradients appear to defeat\niterative optimization-based attacks, we find defenses relying on this effect\ncan be circumvented. We describe characteristic behaviors of defenses\nexhibiting the effect, and for each of the three types of obfuscated gradients\nwe discover, we develop attack techniques to overcome it. In a case study,\nexamining non-certified white-box-secure defenses at ICLR 2018, we find\nobfuscated gradients are a common occurrence, with 7 of 9 defenses relying on\nobfuscated gradients. Our new attacks successfully circumvent 6 completely, and\n1 partially, in the original threat model each paper considers. \n\n"}
{"id": "1802.00699", "contents": "Title: Data Dwarfs: A Lens Towards Fully Understanding Big Data and AI\n  Workloads Abstract: The complexity and diversity of big data and AI workloads make understanding\nthem difficult and challenging. This paper proposes a new approach to\ncharacterizing big data and AI workloads. We consider each big data and AI\nworkload as a pipeline of one or more classes of unit of computations performed\non different initial or intermediate data inputs. Each class of unit of\ncomputation captures the common requirements while being reasonably divorced\nfrom individual implementations, and hence we call it a data dwarf. For the\nfirst time, among a wide variety of big data and AI workloads, we identify\neight data dwarfs that takes up most of run time, including Matrix, Sampling,\nLogic, Transform, Set, Graph, Sort and Statistic. We implement the eight data\ndwarfs on different software stacks as the micro benchmarks of an open-source\nbig data and AI benchmark suite, and perform comprehensive characterization of\nthose data dwarfs from perspective of data sizes, types, sources, and patterns\nas a lens towards fully understanding big data and AI workloads. \n\n"}
{"id": "1802.00748", "contents": "Title: Short-term Memory of Deep RNN Abstract: The extension of deep learning towards temporal data processing is gaining an\nincreasing research interest. In this paper we investigate the properties of\nstate dynamics developed in successive levels of deep recurrent neural networks\n(RNNs) in terms of short-term memory abilities. Our results reveal interesting\ninsights that shed light on the nature of layering as a factor of RNN design.\nNoticeably, higher layers in a hierarchically organized RNN architecture\nresults to be inherently biased towards longer memory spans even prior to\ntraining of the recurrent connections. Moreover, in the context of Reservoir\nComputing framework, our analysis also points out the benefit of a layered\nrecurrent organization as an efficient approach to improve the memory skills of\nreservoir models. \n\n"}
{"id": "1802.03495", "contents": "Title: Generative Adversarial Networks and Probabilistic Graph Models for\n  Hyperspectral Image Classification Abstract: High spectral dimensionality and the shortage of annotations make\nhyperspectral image (HSI) classification a challenging problem. Recent studies\nsuggest that convolutional neural networks can learn discriminative spatial\nfeatures, which play a paramount role in HSI interpretation. However, most of\nthese methods ignore the distinctive spectral-spatial characteristic of\nhyperspectral data. In addition, a large amount of unlabeled data remains an\nunexploited gold mine for efficient data use. Therefore, we proposed an\nintegration of generative adversarial networks (GANs) and probabilistic\ngraphical models for HSI classification. Specifically, we used a\nspectral-spatial generator and a discriminator to identify land cover\ncategories of hyperspectral cubes. Moreover, to take advantage of a large\namount of unlabeled data, we adopted a conditional random field to refine the\npreliminary classification results generated by GANs. Experimental results\nobtained using two commonly studied datasets demonstrate that the proposed\nframework achieved encouraging classification accuracy using a small number of\ndata for training. \n\n"}
{"id": "1802.03700", "contents": "Title: Stochastic Non-preemptive Co-flow Scheduling with Time-Indexed\n  Relaxation Abstract: Co-flows model a modern scheduling setting that is commonly found in a\nvariety of applications in distributed and cloud computing. A stochastic\nco-flow task contains a set of parallel flows with randomly distributed sizes.\nFurther, many applications require non-preemptive scheduling of co-flow tasks.\nThis paper gives an approximation algorithm for stochastic non-preemptive\nco-flow scheduling. The proposed approach uses a time-indexed linear\nrelaxation, and uses its solution to come up with a feasible schedule. This\nalgorithm is shown to achieve a competitive ratio of\n$(2\\log{m}+1)(1+\\sqrt{m}\\Delta)(1+m{\\Delta}){(3+\\Delta)}/{2}$ for zero-release\ntimes, and $(2\\log{m}+1)(1+\\sqrt{m}\\Delta)(1+m\\Delta)(2+\\Delta)$ for general\nrelease times, where $\\Delta$ represents the upper bound of squared coefficient\nof variation of processing times, and $m$ is the number of servers. \n\n"}
{"id": "1802.03875", "contents": "Title: Pseudo-Recursal: Solving the Catastrophic Forgetting Problem in Deep\n  Neural Networks Abstract: In general, neural networks are not currently capable of learning tasks in a\nsequential fashion. When a novel, unrelated task is learnt by a neural network,\nit substantially forgets how to solve previously learnt tasks. One of the\noriginal solutions to this problem is pseudo-rehearsal, which involves learning\nthe new task while rehearsing generated items representative of the previous\ntask/s. This is very effective for simple tasks. However, pseudo-rehearsal has\nnot yet been successfully applied to very complex tasks because in these tasks\nit is difficult to generate representative items. We accomplish\npseudo-rehearsal by using a Generative Adversarial Network to generate items so\nthat our deep network can learn to sequentially classify the CIFAR-10, SVHN and\nMNIST datasets. After training on all tasks, our network loses only 1.67%\nabsolute accuracy on CIFAR-10 and gains 0.24% absolute accuracy on SVHN. Our\nmodel's performance is a substantial improvement compared to the current state\nof the art solution. \n\n"}
{"id": "1802.04211", "contents": "Title: Basic Parallel and Distributed Computing Curriculum Abstract: With the advent of multi-core processors and their fast expansion, it is\nquite clear that {\\em parallel computing} is now a genuine requirement in\nComputer Science and Engineering (and related) curriculum. In addition to the\npervasiveness of parallel computing devices, we should take into account the\nfact that there are lot of existing softwares that are implemented in the\nsequential mode, and thus need to be adapted for a parallel execution.\nTherefore, it is required to the programmer to be able to design parallel\nprograms and also to have some skills in moving from a given sequential code to\nthe corresponding parallel code. In this paper, we present a basic educational\nscenario on how to give a consistent and efficient background in parallel\ncomputing to ordinary computer scientists and engineers. \n\n"}
{"id": "1802.04245", "contents": "Title: Scalarization Methods for Many-Objective Virtual Machine Placement of\n  Elastic Infrastructures in Overbooked Cloud Computing Data Centers Under\n  Uncertainty Abstract: Cloud computing datacenters provide thousands to millions of virtual machines\n(VMs) on-demand in highly dynamic environments, requiring quick placement of\nrequested VMs into available physical machines (PMs). Due to the randomness of\ncustomer requests, the Virtual Machine Placement (VMP) should be formulated as\nan online optimization problem.\n  The first part of this work analyzes alternatives to solve the formulated\nproblem, an experimental comparison of five different online deterministic\nheuristics against an offline memetic algorithm with migration of VMs was\nperformed, considering several experimental workloads. Simulations indicate\nthat First-Fit Decreasing algorithm (A4) outperforms other evaluated heuristics\non average.\n  This work presents a two-phase schema formulation of a VMP problem\nconsidering the optimization of three objective functions in an IaaS\nenvironment with elasticity and overbooking capabilities. The two-phase schema\nformulation describes that the allocation of the VMs can be separated into two\nsub-problems, the incremental allocation (iVMP) and the reconfiguration of a\nplacement (VMPr).\n  To analyze alternatives to solve the formulated problem, an experimental\ncomparison of three different objective function scalarization methods as part\nof the iVMP and VMPr was performed considering several experimental workloads.\nSimulations indicate that the Euclidean distance to origin outperforms other\nevaluated scalarization methods on average.\n  In order to portray the dynamic nature of an IaaS environment a customizable\nworkload trace generator was developed to simulate uncertainty in the scenarios\nwith elasticity and overbooking of resources in VM requests.\n  Experimental results proved that the Euclidean distance is preferable over\nthe other scalarizatiom methods to improve the values of the power consumption\nobjective function. \n\n"}
{"id": "1802.04289", "contents": "Title: Deep Neural Networks for Bot Detection Abstract: The problem of detecting bots, automated social media accounts governed by\nsoftware but disguising as human users, has strong implications. For example,\nbots have been used to sway political elections by distorting online discourse,\nto manipulate the stock market, or to push anti-vaccine conspiracy theories\nthat caused health epidemics. Most techniques proposed to date detect bots at\nthe account level, by processing large amount of social media posts, and\nleveraging information from network structure, temporal dynamics, sentiment\nanalysis, etc.\n  In this paper, we propose a deep neural network based on contextual long\nshort-term memory (LSTM) architecture that exploits both content and metadata\nto detect bots at the tweet level: contextual features are extracted from user\nmetadata and fed as auxiliary input to LSTM deep nets processing the tweet\ntext.\n  Another contribution that we make is proposing a technique based on synthetic\nminority oversampling to generate a large labeled dataset, suitable for deep\nnets training, from a minimal amount of labeled data (roughly 3,000 examples of\nsophisticated Twitter bots). We demonstrate that, from just one single tweet,\nour architecture can achieve high classification accuracy (AUC > 96%) in\nseparating bots from humans.\n  We apply the same architecture to account-level bot detection, achieving\nnearly perfect classification accuracy (AUC > 99%). Our system outperforms\nprevious state of the art while leveraging a small and interpretable set of\nfeatures yet requiring minimal training data. \n\n"}
{"id": "1802.04592", "contents": "Title: A Deep Reinforcement Learning Framework for Rebalancing Dockless Bike\n  Sharing Systems Abstract: Bike sharing provides an environment-friendly way for traveling and is\nbooming all over the world. Yet, due to the high similarity of user travel\npatterns, the bike imbalance problem constantly occurs, especially for dockless\nbike sharing systems, causing significant impact on service quality and company\nrevenue. Thus, it has become a critical task for bike sharing systems to\nresolve such imbalance efficiently. In this paper, we propose a novel deep\nreinforcement learning framework for incentivizing users to rebalance such\nsystems. We model the problem as a Markov decision process and take both\nspatial and temporal features into consideration. We develop a novel deep\nreinforcement learning algorithm called Hierarchical Reinforcement Pricing\n(HRP), which builds upon the Deep Deterministic Policy Gradient algorithm.\nDifferent from existing methods that often ignore spatial information and rely\nheavily on accurate prediction, HRP captures both spatial and temporal\ndependencies using a divide-and-conquer structure with an embedded localized\nmodule. We conduct extensive experiments to evaluate HRP, based on a dataset\nfrom Mobike, a major Chinese dockless bike sharing company. Results show that\nHRP performs close to the 24-timeslot look-ahead optimization, and outperforms\nstate-of-the-art methods in both service level and bike distribution. It also\ntransfers well when applied to unseen areas. \n\n"}
{"id": "1802.04765", "contents": "Title: Progressive Reinforcement Learning with Distillation for Multi-Skilled\n  Motion Control Abstract: Deep reinforcement learning has demonstrated increasing capabilities for\ncontinuous control problems, including agents that can move with skill and\nagility through their environment. An open problem in this setting is that of\ndeveloping good strategies for integrating or merging policies for multiple\nskills, where each individual skill is a specialist in a specific skill and its\nassociated state distribution. We extend policy distillation methods to the\ncontinuous action setting and leverage this technique to combine expert\npolicies, as evaluated in the domain of simulated bipedal locomotion across\ndifferent classes of terrain. We also introduce an input injection method for\naugmenting an existing policy network to exploit new input features. Lastly,\nour method uses transfer learning to assist in the efficient acquisition of new\nskills. The combination of these methods allows a policy to be incrementally\naugmented with new skills. We compare our progressive learning and integration\nvia distillation (PLAID) method against three alternative baselines. \n\n"}
{"id": "1802.05312", "contents": "Title: Learning Deep Disentangled Embeddings with the F-Statistic Loss Abstract: Deep-embedding methods aim to discover representations of a domain that make\nexplicit the domain's class structure and thereby support few-shot learning.\nDisentangling methods aim to make explicit compositional or factorial\nstructure. We combine these two active but independent lines of research and\npropose a new paradigm suitable for both goals. We propose and evaluate a novel\nloss function based on the $F$ statistic, which describes the separation of two\nor more distributions. By ensuring that distinct classes are well separated on\na subset of embedding dimensions, we obtain embeddings that are useful for\nfew-shot learning. By not requiring separation on all dimensions, we encourage\nthe discovery of disentangled representations. Our embedding method matches or\nbeats state-of-the-art, as evaluated by performance on recall@$k$ and few-shot\nlearning tasks. Our method also obtains performance superior to a variety of\nalternatives on disentangling, as evaluated by two key properties of a\ndisentangled representation: modularity and explicitness. The goal of our work\nis to obtain more interpretable, manipulable, and generalizable deep\nrepresentations of concepts and categories. \n\n"}
{"id": "1802.05872", "contents": "Title: Online Machine Learning in Big Data Streams Abstract: The area of online machine learning in big data streams covers algorithms\nthat are (1) distributed and (2) work from data streams with only a limited\npossibility to store past data. The first requirement mostly concerns software\narchitectures and efficient algorithms. The second one also imposes nontrivial\ntheoretical restrictions on the modeling methods: In the data stream model,\nolder data is no longer available to revise earlier suboptimal modeling\ndecisions as the fresh data arrives.\n  In this article, we provide an overview of distributed software architectures\nand libraries as well as machine learning models for online learning. We\nhighlight the most important ideas for classification, regression,\nrecommendation, and unsupervised modeling from streaming data, and we show how\nthey are implemented in various distributed data stream processing systems.\n  This article is a reference material and not a survey. We do not attempt to\nbe comprehensive in describing all existing methods and solutions; rather, we\ngive pointers to the most important resources in the field. All related\nsub-fields, online algorithms, online learning, and distributed data processing\nare hugely dominant in current research and development with conceptually new\nresearch results and software components emerging at the time of writing. In\nthis article, we refer to several survey results, both for distributed data\nprocessing and for online machine learning. Compared to past surveys, our\narticle is different because we discuss recommender systems in extended detail. \n\n"}
{"id": "1802.06476", "contents": "Title: Simultaneous Modeling of Multiple Complications for Risk Profiling in\n  Diabetes Care Abstract: Type 2 diabetes mellitus (T2DM) is a chronic disease that often results in\nmultiple complications. Risk prediction and profiling of T2DM complications is\ncritical for healthcare professionals to design personalized treatment plans\nfor patients in diabetes care for improved outcomes. In this paper, we study\nthe risk of developing complications after the initial T2DM diagnosis from\nlongitudinal patient records. We propose a novel multi-task learning approach\nto simultaneously model multiple complications where each task corresponds to\nthe risk modeling of one complication. Specifically, the proposed method\nstrategically captures the relationships (1) between the risks of multiple T2DM\ncomplications, (2) between the different risk factors, and (3) between the risk\nfactor selection patterns. The method uses coefficient shrinkage to identify an\ninformative subset of risk factors from high-dimensional data, and uses a\nhierarchical Bayesian framework to allow domain knowledge to be incorporated as\npriors. The proposed method is favorable for healthcare applications because in\nadditional to improved prediction performance, relationships among the\ndifferent risks and risk factors are also identified. Extensive experimental\nresults on a large electronic medical claims database show that the proposed\nmethod outperforms state-of-the-art models by a significant margin.\nFurthermore, we show that the risk associations learned and the risk factors\nidentified lead to meaningful clinical insights. \n\n"}
{"id": "1802.07830", "contents": "Title: Proper Semirings and Proper Convex Functors Abstract: Esik and Maletti introduced the notion of a proper semiring and proved that\nsome important (classes of) semirings -- Noetherian semirings, natural numbers\n-- are proper. Properness matters as the equivalence problem for weighted\nautomata over a semiring which is proper and finitely and effectively presented\nis decidable. Milius generalised the notion of properness from a semiring to a\nfunctor. As a consequence, a semiring is proper if and only if its associated\n\"cubic functor\" is proper. Moreover, properness of a functor renders soundness\nand completeness proofs for axiomatizations of equivalent behaviour.\n  In this paper we provide a method for proving properness of functors, and\ninstantiate it to cover both the known cases and several novel ones: (1)\nproperness of the semirings of positive rationals and positive reals, via\nproperness of the corresponding cubic functors; and (2) properness of two\nfunctors on (positive) convex algebras. The latter functors are important for\naxiomatizing trace equivalence of probabilistic transition systems. Our proofs\nrely on results that stretch all the way back to Hilbert and Minkowski. \n\n"}
{"id": "1802.07927", "contents": "Title: The Hidden Vulnerability of Distributed Learning in Byzantium Abstract: While machine learning is going through an era of celebrated success,\nconcerns have been raised about the vulnerability of its backbone: stochastic\ngradient descent (SGD). Recent approaches have been proposed to ensure the\nrobustness of distributed SGD against adversarial (Byzantine) workers sending\npoisoned gradients during the training phase. Some of these approaches have\nbeen proven Byzantine-resilient: they ensure the convergence of SGD despite the\npresence of a minority of adversarial workers.\n  We show in this paper that convergence is not enough. In high dimension $d\n\\gg 1$, an adver\\-sary can build on the loss function's non-convexity to make\nSGD converge to ineffective models. More precisely, we bring to light that\nexisting Byzantine-resilient schemes leave a margin of poisoning of\n$\\Omega\\left(f(d)\\right)$, where $f(d)$ increases at least like $\\sqrt{d~}$.\nBased on this leeway, we build a simple attack, and experimentally show its\nstrong to utmost effectivity on CIFAR-10 and MNIST.\n  We introduce Bulyan, and prove it significantly reduces the attackers leeway\nto a narrow $O( \\frac{1}{\\sqrt{d~}})$ bound. We empirically show that Bulyan\ndoes not suffer the fragility of existing aggregation rules and, at a\nreasonable cost in terms of required batch size, achieves convergence as if\nonly non-Byzantine gradients had been used to update the model. \n\n"}
{"id": "1803.01118", "contents": "Title: Some Considerations on Learning to Explore via Meta-Reinforcement\n  Learning Abstract: We consider the problem of exploration in meta reinforcement learning. Two\nnew meta reinforcement learning algorithms are suggested: E-MAML and\nE-$\\text{RL}^2$. Results are presented on a novel environment we call `Krazy\nWorld' and a set of maze environments. We show E-MAML and E-$\\text{RL}^2$\ndeliver better performance on tasks where exploration is important. \n\n"}
{"id": "1803.02839", "contents": "Title: The emergent algebraic structure of RNNs and embeddings in NLP Abstract: We examine the algebraic and geometric properties of a uni-directional GRU\nand word embeddings trained end-to-end on a text classification task. A\nhyperparameter search over word embedding dimension, GRU hidden dimension, and\na linear combination of the GRU outputs is performed. We conclude that words\nnaturally embed themselves in a Lie group and that RNNs form a nonlinear\nrepresentation of the group. Appealing to these results, we propose a novel\nclass of recurrent-like neural networks and a word embedding scheme. \n\n"}
{"id": "1803.03031", "contents": "Title: Redundancy in Distributed Proofs Abstract: Distributed proofs are mechanisms enabling the nodes of a network to\ncollectivity and efficiently check the correctness of Boolean predicates on the\nstructure of the network, or on data-structures distributed over the nodes\n(e.g., spanning trees or routing tables). We consider mechanisms consisting of\ntwo components: a \\emph{prover} assigning a \\emph{certificate} to each node,\nand a distributed algorithm called \\emph{verifier} that is in charge of\nverifying the distributed proof formed by the collection of all certificates.\n  In this paper, we show that many network predicates have distributed proofs\noffering a high level of redundancy, explicitly or implicitly. We use this\nremarkable property of distributed proofs for establishing perfect tradeoffs\nbetween the \\emph{size of the certificate} stored at every node, and the\n\\emph{number of rounds} of the verification protocol. If we allow every node to\ncommunicate to distance at most $t$, one might expect that the certificate\nsizes can be reduced by a multiplicative factor of at least~$t$. In trees,\ncycles and grids, we show that such tradeoffs can be established for \\emph{all}\nnetwork predicates, i.e., it is always possible to linearly decrease the\ncertificate size. In arbitrary graphs, we show that any part of the\ncertificates common to all nodes can be evenly redistributed among these nodes,\nachieving even a better tradeoff: this common part of the certificate can be\nreduced by the size of a smallest ball of radius $t$ in the network.\n  In addition to these general results, we establish several upper and lower\nbounds on the certificate sizes used for distributed proofs for spanning trees,\nminimum-weight spanning trees, diameter, additive and multiplicative spanners,\nand more, improving and generalizing previous results from the literature. \n\n"}
{"id": "1803.03241", "contents": "Title: Efficient Algorithms for Outlier-Robust Regression Abstract: We give the first polynomial-time algorithm for performing linear or\npolynomial regression resilient to adversarial corruptions in both examples and\nlabels.\n  Given a sufficiently large (polynomial-size) training set drawn i.i.d. from\ndistribution D and subsequently corrupted on some fraction of points, our\nalgorithm outputs a linear function whose squared error is close to the squared\nerror of the best-fitting linear function with respect to D, assuming that the\nmarginal distribution of D over the input space is \\emph{certifiably\nhypercontractive}. This natural property is satisfied by many well-studied\ndistributions such as Gaussian, strongly log-concave distributions and, uniform\ndistribution on the hypercube among others. We also give a simple statistical\nlower bound showing that some distributional assumption is necessary to succeed\nin this setting.\n  These results are the first of their kind and were not known to be even\ninformation-theoretically possible prior to our work.\n  Our approach is based on the sum-of-squares (SoS) method and is inspired by\nthe recent applications of the method for parameter recovery problems in\nunsupervised learning. Our algorithm can be seen as a natural convex relaxation\nof the following conceptually simple non-convex optimization problem: find a\nlinear function and a large subset of the input corrupted sample such that the\nleast squares loss of the function over the subset is minimized over all\npossible large subsets. \n\n"}
{"id": "1803.03248", "contents": "Title: Improved Distributed $\\Delta$-Coloring Abstract: We present a randomized distributed algorithm that computes a\n$\\Delta$-coloring in any non-complete graph with maximum degree $\\Delta \\geq 4$\nin $O(\\log \\Delta) + 2^{O(\\sqrt{\\log\\log n})}$ rounds, as well as a randomized\nalgorithm that computes a $\\Delta$-coloring in $O((\\log \\log n)^2)$ rounds when\n$\\Delta \\in [3, O(1)]$. Both these algorithms improve on an $O(\\log^3 n/\\log\n\\Delta)$-round algorithm of Panconesi and Srinivasan~[STOC'1993], which has\nremained the state of the art for the past 25 years. Moreover, the latter\nalgorithm gets (exponentially) closer to an $\\Omega(\\log\\log n)$ round lower\nbound of Brandt et al.~[STOC'16]. \n\n"}
{"id": "1803.04783", "contents": "Title: A Scalable Near-Memory Architecture for Training Deep Neural Networks on\n  Large In-Memory Datasets Abstract: Most investigations into near-memory hardware accelerators for deep neural\nnetworks have primarily focused on inference, while the potential of\naccelerating training has received relatively little attention so far. Based on\nan in-depth analysis of the key computational patterns in state-of-the-art\ngradient-based training methods, we propose an efficient near-memory\nacceleration engine called NTX that can be used to train state-of-the-art deep\nconvolutional neural networks at scale. Our main contributions are: (i) a loose\ncoupling of RISC-V cores and NTX co-processors reducing offloading overhead by\n7x over previously published results; (ii) an optimized IEEE754 compliant data\npath for fast high-precision convolutions and gradient propagation; (iii)\nevaluation of near-memory computing with NTX embedded into residual area on the\nLogic Base die of a Hybrid Memory Cube; and (iv) a scaling analysis to meshes\nof HMCs in a data center scenario. We demonstrate a 2.7x energy efficiency\nimprovement of NTX over contemporary GPUs at 4.4x less silicon area, and a\ncompute performance of 1.2 Tflop/s for training large state-of-the-art networks\nwith full floating-point precision. At the data center scale, a mesh of NTX\nachieves above 95% parallel and energy efficiency, while providing 2.1x energy\nsavings or 3.1x performance improvement over a GPU-based system. \n\n"}
{"id": "1803.06333", "contents": "Title: Snap ML: A Hierarchical Framework for Machine Learning Abstract: We describe a new software framework for fast training of generalized linear\nmodels. The framework, named Snap Machine Learning (Snap ML), combines recent\nadvances in machine learning systems and algorithms in a nested manner to\nreflect the hierarchical architecture of modern computing systems. We prove\ntheoretically that such a hierarchical system can accelerate training in\ndistributed environments where intra-node communication is cheaper than\ninter-node communication. Additionally, we provide a review of the\nimplementation of Snap ML in terms of GPU acceleration, pipelining,\ncommunication patterns and software architecture, highlighting aspects that\nwere critical for achieving high performance. We evaluate the performance of\nSnap ML in both single-node and multi-node environments, quantifying the\nbenefit of the hierarchical scheme and the data streaming functionality, and\ncomparing with other widely-used machine learning software frameworks. Finally,\nwe present a logistic regression benchmark on the Criteo Terabyte Click Logs\ndataset and show that Snap ML achieves the same test loss an order of magnitude\nfaster than any of the previously reported results, including those obtained\nusing TensorFlow and scikit-learn. \n\n"}
{"id": "1803.06554", "contents": "Title: Fusion of an Ensemble of Augmented Image Detectors for Robust Object\n  Detection Abstract: A significant challenge in object detection is accurate identification of an\nobject's position in image space, whereas one algorithm with one set of\nparameters is usually not enough, and the fusion of multiple algorithms and/or\nparameters can lead to more robust results. Herein, a new computational\nintelligence fusion approach based on the dynamic analysis of agreement among\nobject detection outputs is proposed. Furthermore, we propose an online versus\njust in training image augmentation strategy. Experiments comparing the results\nboth with and without fusion are presented. We demonstrate that the augmented\nand fused combination results are the best, with respect to higher accuracy\nrates and reduction of outlier influences. The approach is demonstrated in the\ncontext of cone, pedestrian and box detection for Advanced Driver Assistance\nSystems (ADAS) applications. \n\n"}
{"id": "1803.07068", "contents": "Title: D$^2$: Decentralized Training over Decentralized Data Abstract: While training a machine learning model using multiple workers, each of which\ncollects data from their own data sources, it would be most useful when the\ndata collected from different workers can be {\\em unique} and {\\em different}.\nIronically, recent analysis of decentralized parallel stochastic gradient\ndescent (D-PSGD) relies on the assumption that the data hosted on different\nworkers are {\\em not too different}. In this paper, we ask the question: {\\em\nCan we design a decentralized parallel stochastic gradient descent algorithm\nthat is less sensitive to the data variance across workers?} In this paper, we\npresent D$^2$, a novel decentralized parallel stochastic gradient descent\nalgorithm designed for large data variance \\xr{among workers} (imprecisely,\n\"decentralized\" data). The core of D$^2$ is a variance blackuction extension of\nthe standard D-PSGD algorithm, which improves the convergence rate from\n$O\\left({\\sigma \\over \\sqrt{nT}} + {(n\\zeta^2)^{\\frac{1}{3}} \\over\nT^{2/3}}\\right)$ to $O\\left({\\sigma \\over \\sqrt{nT}}\\right)$ where $\\zeta^{2}$\ndenotes the variance among data on different workers. As a result, D$^2$ is\nrobust to data variance among workers. We empirically evaluated D$^2$ on image\nclassification tasks where each worker has access to only the data of a limited\nset of labels, and find that D$^2$ significantly outperforms D-PSGD. \n\n"}
{"id": "1803.08833", "contents": "Title: Gaussian and exponential lateral connectivity on distributed spiking\n  neural network simulation Abstract: We measured the impact of long-range exponentially decaying intra-areal\nlateral connectivity on the scaling and memory occupation of a distributed\nspiking neural network simulator compared to that of short-range Gaussian\ndecays. While previous studies adopted short-range connectivity, recent\nexperimental neurosciences studies are pointing out the role of longer-range\nintra-areal connectivity with implications on neural simulation platforms.\nTwo-dimensional grids of cortical columns composed by up to 11 M point-like\nspiking neurons with spike frequency adaption were connected by up to 30 G\nsynapses using short- and long-range connectivity models. The MPI processes\ncomposing the distributed simulator were run on up to 1024 hardware cores,\nhosted on a 64 nodes server platform. The hardware platform was a cluster of\nIBM NX360 M5 16-core compute nodes, each one containing two Intel Xeon Haswell\n8-core E5-2630 v3 processors, with a clock of 2.40 G Hz, interconnected through\nan InfiniBand network, equipped with 4x QDR switches. \n\n"}
{"id": "1803.09297", "contents": "Title: Proof nets and the instantiation overflow property Abstract: Instantiation overflow is the property of those second order types for which\nall instances of full comprehension can be deduced from instances of atomic\ncomprehension. In other words, a type has instantiation overflow when one can\ntype, by atomic polymorphism, \"expansion terms\" which realize instances of the\nfull extraction rule applied to that type. This property was investigated in\nthe case of the types arising from the well-known Russell-Prawitz translation\nof logical connectives into System F, but is not restricted to such types.\nMoreover, it can be related to functorial polymorphism, a well-known categorial\napproach to parametricity in System F. In this paper we investigate the\ninstantiation overflow property by exploiting the representation of derivations\nby means of linear logic proof nets. We develop a geometric approach to\ninstantiation overflow yielding a deeper understanding of the structure of\nexpansion terms and Russell-Prawitz types. Our main result is a\ncharacterization of the class of types of the form $\\forall XA$, where $A$ is a\nsimple type, which enjoy the instantiation overflow property, by means of a\ngeneralization of Russell-Prawitz types. \n\n"}
{"id": "1803.09566", "contents": "Title: BoSy: An Experimentation Framework for Bounded Synthesis Abstract: We present BoSy, a reactive synthesis tool based on the bounded synthesis\napproach. Bounded synthesis ensures the minimality of the synthesized\nimplementation by incrementally increasing a bound on the size of the solutions\nit considers. For each bound, the existence of a solution is encoded as a\nlogical constraint solving problem that is solved by an appropriate solver.\nBoSy constructs bounded synthesis encodings into SAT, QBF, DQBF, EPR, and SMT,\nand interfaces to solvers of the corresponding type. When supported by the\nsolver, BoSy extracts solutions as circuits, which can, if desired, be verified\nwith standard hardware model checkers. BoSy won the LTL synthesis track at\nSYNTCOMP 2016. In addition to its use as a synthesis tool, BoSy can also be\nused as an experimentation and performance evaluation framework for various\ntypes of satisfiability solvers. \n\n"}
{"id": "1803.09984", "contents": "Title: Hiding in the Crowd: A Massively Distributed Algorithm for Private\n  Averaging with Malicious Adversaries Abstract: The amount of personal data collected in our everyday interactions with\nconnected devices offers great opportunities for innovative services fueled by\nmachine learning, as well as raises serious concerns for the privacy of\nindividuals. In this paper, we propose a massively distributed protocol for a\nlarge set of users to privately compute averages over their joint data, which\ncan then be used to learn predictive models. Our protocol can find a solution\nof arbitrary accuracy, does not rely on a third party and preserves the privacy\nof users throughout the execution in both the honest-but-curious and malicious\nadversary models. Specifically, we prove that the information observed by the\nadversary (the set of maliciours users) does not significantly reduce the\nuncertainty in its prediction of private values compared to its prior belief.\nThe level of privacy protection depends on a quantity related to the Laplacian\nmatrix of the network graph and generally improves with the size of the graph.\nFurthermore, we design a verification procedure which offers protection against\nmalicious users joining the service with the goal of manipulating the outcome\nof the algorithm. \n\n"}
{"id": "1803.10080", "contents": "Title: A sequent calculus for a semi-associative law Abstract: We introduce a sequent calculus with a simple restriction of Lambek's product\nrules that precisely captures the classical Tamari order, i.e., the partial\norder on fully-bracketed words (equivalently, binary trees) induced by a\nsemi-associative law (equivalently, right rotation). We establish a focusing\nproperty for this sequent calculus (a strengthening of cut-elimination), which\nyields the following coherence theorem: every valid entailment in the Tamari\norder has exactly one focused derivation. We then describe two main\napplications of the coherence theorem, including: 1. A new proof of the lattice\nproperty for the Tamari order, and 2. A new proof of the Tutte-Chapoton formula\nfor the number of intervals in the Tamari lattice $Y_n$. \n\n"}
{"id": "1803.10113", "contents": "Title: Univalent polymorphism Abstract: We show that Martin Hyland's effective topos can be exhibited as the homotopy\ncategory of a path category $\\mathbb{EFF}$. Path categories are categories of\nfibrant objects in the sense of Brown satisfying two additional properties and\nas such provide a context in which one can interpret many notions from homotopy\ntheory and Homotopy Type Theory. Within the path category $\\mathbb{EFF}$ one\ncan identify a class of discrete fibrations which is closed under push forward\nalong arbitrary fibrations (in other words, this class is polymorphic or closed\nunder impredicative quantification) and satisfies propositional resizing. This\nclass does not have a univalent representation, but if one restricts to those\ndiscrete fibrations whose fibres are propositions in the sense of Homotopy Type\nTheory, then it does. This means that, modulo the usual coherence problems, it\ncan be seen as a model of the Calculus of Constructions with a univalent type\nof propositions. We will also build a more complicated path category in which\nthe class of discrete fibrations whose fibres are sets in the sense of Homotopy\nType Theory has a univalent representation, which means that this will be a\nmodel of the Calculus of Constructions with a univalent type of sets. \n\n"}
{"id": "1803.10227", "contents": "Title: Forward-Backward Reinforcement Learning Abstract: Goals for reinforcement learning problems are typically defined through\nhand-specified rewards. To design such problems, developers of learning\nalgorithms must inherently be aware of what the task goals are, yet we often\nrequire agents to discover them on their own without any supervision beyond\nthese sparse rewards. While much of the power of reinforcement learning derives\nfrom the concept that agents can learn with little guidance, this requirement\ngreatly burdens the training process. If we relax this one restriction and\nendow the agent with knowledge of the reward function, and in particular of the\ngoal, we can leverage backwards induction to accelerate training. To achieve\nthis, we propose training a model to learn to take imagined reversal steps from\nknown goal states. Rather than training an agent exclusively to determine how\nto reach a goal while moving forwards in time, our approach travels backwards\nto jointly predict how we got there. We evaluate our work in Gridworld and\nTowers of Hanoi and empirically demonstrate that it yields better performance\nthan standard DDQN. \n\n"}
{"id": "1803.10901", "contents": "Title: Statistical Validity and Consistency of Big Data Analytics: A General\n  Framework Abstract: Informatics and technological advancements have triggered generation of huge\nvolume of data with varied complexity in its management and analysis. Big Data\nanalytics is the practice of revealing hidden aspects of such data and making\ninferences from it. Although storage, retrieval and management of Big Data seem\npossible through efficient algorithm and system development, concern about\nstatistical consistency remains to be addressed in view of its specific\ncharacteristics. Since Big Data does not conform to standard analytics, we need\nproper modification of the existing statistical theory and tools. Here we\npropose, with illustrations, a general statistical framework and an algorithmic\nprinciple for Big Data analytics that ensure statistical accuracy of the\nconclusions. The proposed framework has the potential to push forward\nadvancement of Big Data analytics in the right direction. The\npartition-repetition approach proposed here is broad enough to encompass all\npractical data analytic problems. \n\n"}
{"id": "1804.00399", "contents": "Title: Towards Scaling Blockchain Systems via Sharding Abstract: Existing blockchain systems scale poorly because of their distributed\nconsensus protocols. Current attempts at improving blockchain scalability are\nlimited to cryptocurrency. Scaling blockchain systems under general workloads\n(i.e., non-cryptocurrency applications) remains an open question. In this work,\nwe take a principled approach to apply sharding, which is a well-studied and\nproven technique to scale out databases, to blockchain systems in order to\nimprove their transaction throughput at scale. This is challenging, however,\ndue to the fundamental difference in failure models between databases and\nblockchain. To achieve our goal, we first enhance the performance of Byzantine\nconsensus protocols, by doing so we improve individual shards' throughput.\nNext, we design an efficient shard formation protocol that leverages a trusted\nrandom beacon to securely assign nodes into shards. We rely on trusted\nhardware, namely Intel SGX, to achieve high performance for both consensus and\nshard formation protocol. Third, we design a general distributed transaction\nprotocol that ensures safety and liveness even when transaction coordinators\nare malicious. Finally, we conduct an extensive evaluation of our design both\non a local cluster and on Google Cloud Platform. The results show that our\nconsensus and shard formation protocols outperform state-of-the-art solutions\nat scale. More importantly, our sharded blockchain reaches a high throughput\nthat can handle Visa-level workloads, and is the largest ever reported in a\nrealistic environment. \n\n"}
{"id": "1804.00617", "contents": "Title: Specification-Driven Multi-Perspective Predictive Business Process\n  Monitoring (Extended Version) Abstract: Predictive analysis in business process monitoring aims at forecasting the\nfuture information of a running business process. The prediction is typically\nmade based on the model extracted from historical process execution logs (event\nlogs). In practice, different business domains might require different kinds of\npredictions. Hence, it is important to have a means for properly specifying the\ndesired prediction tasks, and a mechanism to deal with these various prediction\ntasks. Although there have been many studies in this area, they mostly focus on\na specific prediction task. This work introduces a language for specifying the\ndesired prediction tasks, and this language allows us to express various kinds\nof prediction tasks. This work also presents a mechanism for automatically\ncreating the corresponding prediction model based on the given specification.\nThus, different from previous studies, our approach enables us to deal with\nvarious kinds of prediction tasks based on the given specification. A prototype\nimplementing our approach has been developed and experiments using a real-life\nevent log have been conducted. \n\n"}
{"id": "1804.00982", "contents": "Title: 360{\\deg} Stance Detection Abstract: The proliferation of fake news and filter bubbles makes it increasingly\ndifficult to form an unbiased, balanced opinion towards a topic. To ameliorate\nthis, we propose 360{\\deg} Stance Detection, a tool that aggregates news with\nmultiple perspectives on a topic. It presents them on a spectrum ranging from\nsupport to opposition, enabling the user to base their opinion on multiple\npieces of diverse evidence. \n\n"}
{"id": "1804.01023", "contents": "Title: Attracting Tangles to Solve Parity Games Abstract: Parity games have important practical applications in formal verification and\nsynthesis, especially to solve the model-checking problem of the modal\nmu-calculus. They are also interesting from the theory perspective, because\nthey are widely believed to admit a polynomial solution, but so far no such\nalgorithm is known.\n  We propose a new algorithm to solve parity games based on learning tangles,\nwhich are strongly connected subgraphs for which one player has a strategy to\nwin all cycles in the subgraph. We argue that tangles play a fundamental role\nin the prominent parity game solving algorithms. We show that tangle learning\nis competitive in practice and the fastest solver for large random games. \n\n"}
{"id": "1804.01437", "contents": "Title: Short Proofs for Some Symmetric Quantified Boolean Formulas Abstract: We exploit symmetries to give short proofs for two prominent formula families\nof QBF proof complexity. On the one hand, we employ symmetry breakers. On the\nother hand, we enrich the (relatively weak) QBF resolution calculus Q-Res with\nthe symmetry rule and obtain separations to powerful QBF calculi. \n\n"}
{"id": "1804.01626", "contents": "Title: SBFT: a Scalable and Decentralized Trust Infrastructure Abstract: SBFT is a state of the art Byzantine fault tolerant permissioned blockchain\nsystem that addresses the challenges of scalability, decentralization and\nworld-scale geo-replication. SBFTis optimized for decentralization and can\neasily handle more than 200 active replicas in a real world-scale deployment.\nWe evaluate \\sysname in a world-scale geo-replicated deployment with 209\nreplicas withstanding f=64 Byzantine failures. We provide experiments that show\nhow the different algorithmic ingredients of \\sysname increase its performance\nand scalability. The results show that SBFT simultaneously provides almost 2x\nbetter throughput and about 1.5x better latency relative to a highly optimized\nsystem that implements the PBFT protocol. To achieve this performance\nimprovement, SBFT uses a combination of four ingredients: using collectors and\nthreshold signatures to reduce communication to linear, using an optimistic\nfast path, reducing client communication and utilizing redundant servers for\nthe fast path. \n\n"}
{"id": "1804.02477", "contents": "Title: Programmatically Interpretable Reinforcement Learning Abstract: We present a reinforcement learning framework, called Programmatically\nInterpretable Reinforcement Learning (PIRL), that is designed to generate\ninterpretable and verifiable agent policies. Unlike the popular Deep\nReinforcement Learning (DRL) paradigm, which represents policies by neural\nnetworks, PIRL represents policies using a high-level, domain-specific\nprogramming language. Such programmatic policies have the benefits of being\nmore easily interpreted than neural networks, and being amenable to\nverification by symbolic methods. We propose a new method, called Neurally\nDirected Program Search (NDPS), for solving the challenging nonsmooth\noptimization problem of finding a programmatic policy with maximal reward. NDPS\nworks by first learning a neural policy network using DRL, and then performing\na local search over programmatic policies that seeks to minimize a distance\nfrom this neural \"oracle\". We evaluate NDPS on the task of learning to drive a\nsimulated car in the TORCS car-racing environment. We demonstrate that NDPS is\nable to discover human-readable policies that pass some significant performance\nbars. We also show that PIRL policies can have smoother trajectories, and can\nbe more easily transferred to environments not encountered during training,\nthan corresponding policies discovered by DRL. \n\n"}
{"id": "1804.02772", "contents": "Title: Active Mini-Batch Sampling using Repulsive Point Processes Abstract: The convergence speed of stochastic gradient descent (SGD) can be improved by\nactively selecting mini-batches. We explore sampling schemes where similar data\npoints are less likely to be selected in the same mini-batch. In particular, we\nprove that such repulsive sampling schemes lowers the variance of the gradient\nestimator. This generalizes recent work on using Determinantal Point Processes\n(DPPs) for mini-batch diversification (Zhang et al., 2017) to the broader class\nof repulsive point processes. We first show that the phenomenon of variance\nreduction by diversified sampling generalizes in particular to non-stationary\npoint processes. We then show that other point processes may be computationally\nmuch more efficient than DPPs. In particular, we propose and investigate\nPoisson Disk sampling---frequently encountered in the computer graphics\ncommunity---for this task. We show empirically that our approach improves over\nstandard SGD both in terms of convergence speed as well as final model\nperformance. \n\n"}
{"id": "1804.02816", "contents": "Title: A Generation Method of Immunological Memory in Clonal Selection\n  Algorithm by using Restricted Boltzmann Machines Abstract: Recently, a high technique of image processing is required to extract the\nimage features in real time. In our research, the tourist subject data are\ncollected from the Mobile Phone based Participatory Sensing (MPPS) system. Each\nrecord consists of image files with GPS, geographic location name, user's\nnumerical evaluation, and comments written in natural language at sightseeing\nspots where a user really visits. In our previous research, the famous\nlandmarks in sightseeing spot can be detected by Clonal Selection Algorithm\nwith Immunological Memory Cell (CSAIM). However, some landmarks was not\ndetected correctly by the previous method because they didn't have enough\namount of information for the feature extraction. In order to improve the\nweakness, we propose the generation method of immunological memory by\nRestricted Boltzmann Machines. To verify the effectiveness of the method, some\nexperiments for classification of the subjective data are executed by using\nmachine learning tools for Deep Learning. \n\n"}
{"id": "1804.02884", "contents": "Title: Policy Gradient With Value Function Approximation For Collective\n  Multiagent Planning Abstract: Decentralized (PO)MDPs provide an expressive framework for sequential\ndecision making in a multiagent system. Given their computational complexity,\nrecent research has focused on tractable yet practical subclasses of\nDec-POMDPs. We address such a subclass called CDEC-POMDP where the collective\nbehavior of a population of agents affects the joint-reward and environment\ndynamics. Our main contribution is an actor-critic (AC) reinforcement learning\nmethod for optimizing CDEC-POMDP policies. Vanilla AC has slow convergence for\nlarger problems. To address this, we show how a particular decomposition of the\napproximate action-value function over agents leads to effective updates, and\nalso derive a new way to train the critic based on local reward signals.\nComparisons on a synthetic benchmark and a real-world taxi fleet optimization\nproblem show that our new AC approach provides better quality solutions than\nprevious best approaches. \n\n"}
{"id": "1804.03115", "contents": "Title: AMNet: Memorability Estimation with Attention Abstract: In this paper we present the design and evaluation of an end-to-end\ntrainable, deep neural network with a visual attention mechanism for\nmemorability estimation in still images. We analyze the suitability of transfer\nlearning of deep models from image classification to the memorability task.\nFurther on we study the impact of the attention mechanism on the memorability\nestimation and evaluate our network on the SUN Memorability and the LaMem\ndatasets. Our network outperforms the existing state of the art models on both\ndatasets in terms of the Spearman's rank correlation as well as the mean\nsquared error, closely matching human consistency. \n\n"}
{"id": "1804.03124", "contents": "Title: Leveraging Intra-User and Inter-User Representation Learning for\n  Automated Hate Speech Detection Abstract: Hate speech detection is a critical, yet challenging problem in Natural\nLanguage Processing (NLP). Despite the existence of numerous studies dedicated\nto the development of NLP hate speech detection approaches, the accuracy is\nstill poor. The central problem is that social media posts are short and noisy,\nand most existing hate speech detection solutions take each post as an isolated\ninput instance, which is likely to yield high false positive and negative\nrates. In this paper, we radically improve automated hate speech detection by\npresenting a novel model that leverages intra-user and inter-user\nrepresentation learning for robust hate speech detection on Twitter. In\naddition to the target Tweet, we collect and analyze the user's historical\nposts to model intra-user Tweet representations. To suppress the noise in a\nsingle Tweet, we also model the similar Tweets posted by all other users with\nreinforced inter-user representation learning techniques. Experimentally, we\nshow that leveraging these two representations can significantly improve the\nf-score of a strong bidirectional LSTM baseline model by 10.1%. \n\n"}
{"id": "1804.03235", "contents": "Title: Large scale distributed neural network training through online\n  distillation Abstract: Techniques such as ensembling and distillation promise model quality\nimprovements when paired with almost any base model. However, due to increased\ntest-time cost (for ensembles) and increased complexity of the training\npipeline (for distillation), these techniques are challenging to use in\nindustrial settings. In this paper we explore a variant of distillation which\nis relatively straightforward to use as it does not require a complicated\nmulti-stage setup or many new hyperparameters. Our first claim is that online\ndistillation enables us to use extra parallelism to fit very large datasets\nabout twice as fast. Crucially, we can still speed up training even after we\nhave already reached the point at which additional parallelism provides no\nbenefit for synchronous or asynchronous stochastic gradient descent. Two neural\nnetworks trained on disjoint subsets of the data can share knowledge by\nencouraging each model to agree with the predictions the other model would have\nmade. These predictions can come from a stale version of the other model so\nthey can be safely computed using weights that only rarely get transmitted. Our\nsecond claim is that online distillation is a cost-effective way to make the\nexact predictions of a model dramatically more reproducible. We support our\nclaims using experiments on the Criteo Display Ad Challenge dataset, ImageNet,\nand the largest to-date dataset used for neural language modeling, containing\n$6\\times 10^{11}$ tokens and based on the Common Crawl repository of web data. \n\n"}
{"id": "1804.03758", "contents": "Title: Universal Successor Representations for Transfer Reinforcement Learning Abstract: The objective of transfer reinforcement learning is to generalize from a set\nof previous tasks to unseen new tasks. In this work, we focus on the transfer\nscenario where the dynamics among tasks are the same, but their goals differ.\nAlthough general value function (Sutton et al., 2011) has been shown to be\nuseful for knowledge transfer, learning a universal value function can be\nchallenging in practice. To attack this, we propose (1) to use universal\nsuccessor representations (USR) to represent the transferable knowledge and (2)\na USR approximator (USRA) that can be trained by interacting with the\nenvironment. Our experiments show that USR can be effectively applied to new\ntasks, and the agent initialized by the trained USRA can achieve the goal\nconsiderably faster than random initialization. \n\n"}
{"id": "1804.05045", "contents": "Title: Morita equivalences between algebraic dependent type theories Abstract: We define a notion of equivalence between algebraic dependent type theories\nwhich we call Morita equivalence. This notion has a simple syntactic\ndescription and an equivalent description in terms of models of the theories.\nThe category of models of a type theory often carries a natural structure of a\nmodel category. If this holds for the categories of models of two theories,\nthen a map between them is a Morita equivalence if and only if the adjunction\ngenerated by it is a Quillen equivalence. \n\n"}
{"id": "1804.05435", "contents": "Title: What Happened? Leveraging VerbNet to Predict the Effects of Actions in\n  Procedural Text Abstract: Our goal is to answer questions about paragraphs describing processes (e.g.,\nphotosynthesis). Texts of this genre are challenging because the effects of\nactions are often implicit (unstated), requiring background knowledge and\ninference to reason about the changing world states. To supply this knowledge,\nwe leverage VerbNet to build a rulebase (called the Semantic Lexicon) of the\npreconditions and effects of actions, and use it along with commonsense\nknowledge of persistence to answer questions about change. Our evaluation shows\nthat our system, ProComp, significantly outperforms two strong reading\ncomprehension (RC) baselines. Our contributions are two-fold: the Semantic\nLexicon rulebase itself, and a demonstration of how a simulation-based approach\nto machine reading can outperform RC methods that rely on surface cues alone.\n  Since this work was performed, we have developed neural systems that\noutperform ProComp, described elsewhere (Dalvi et al., NAACL'18). However, the\nSemantic Lexicon remains a novel and potentially useful resource, and its\nintegration with neural systems remains a currently unexplored opportunity for\nfurther improvements in machine reading about processes. \n\n"}
{"id": "1804.05714", "contents": "Title: Developing Synthesis Flows Without Human Knowledge Abstract: Design flows are the explicit combinations of design transformations,\nprimarily involved in synthesis, placement and routing processes, to accomplish\nthe design of Integrated Circuits (ICs) and System-on-Chip (SoC). Mostly, the\nflows are developed based on the knowledge of the experts. However, due to the\nlarge search space of design flows and the increasing design complexity,\ndeveloping Intellectual Property (IP)-specific synthesis flows providing high\nQuality of Result (QoR) is extremely challenging. This work presents a fully\nautonomous framework that artificially produces design-specific synthesis flows\nwithout human guidance and baseline flows, using Convolutional Neural Network\n(CNN). The demonstrations are made by successfully designing logic synthesis\nflows of three large scaled designs. \n\n"}
{"id": "1804.06687", "contents": "Title: The clocks they are adjunctions:Denotational semantics for Clocked Type\n  Theory Abstract: Clocked Type Theory (CloTT) is a type theory for guarded recursion useful for\nprogramming with coinductive types, allowing productivity to be encoded in\ntypes, and for reasoning about advanced programming language features using an\nabstract form of step-indexing. CloTT has previously been shown to enjoy a\nnumber of syntactic properties including strong normalisation, canonicity and\ndecidability of type checking. In this paper we present a denotational\nsemantics for CloTT useful, e.g., for studying future extensions of CloTT with\nconstructions such as path types.\n  The main challenge for constructing this model is to model the notion of\nticks used in CloTT for coinductive reasoning about coinductive types. We build\non a category previously used to model guarded recursion, but in this category\nthere is no object of ticks, so tick-assumptions in a context can not be\nmodelled using standard tools. Instead we show how ticks can be modelled using\nadjoint functors, and how to model the tick constant using a semantic\nsubstitution. \n\n"}
{"id": "1804.06894", "contents": "Title: Dichotomies in Ontology-Mediated Querying with the Guarded Fragment Abstract: We study the complexity of ontology-mediated querying when ontologies are\nformulated in the guarded fragment of first-order logic (GF). Our general aim\nis to classify the data complexity on the level of ontologies where query\nevaluation w.r.t. an ontology O is considered to be in PTime if all (unions of\nconjunctive) queries can be evaluated in PTime w.r.t. O and coNP-hard if at\nleast one query is coNP-hard w.r.t. O. We identify several large and relevant\nfragments of GF that enjoy a dichotomy between PTime and coNP, some of them\nadditionally admitting a form of counting. In fact, almost all ontologies in\nthe BioPortal repository fall into these fragments or can easily be rewritten\nto do so. We then establish a variation of Ladner's Theorem on the existence of\nNP-intermediate problems and use this result to show that for other fragments,\nthere is provably no such dichotomy. Again for other fragments (such as full\nGF), establishing a dichotomy implies the Feder-Vardi conjecture on the\ncomplexity of constraint satisfaction problems. We also link these results to\nDatalog-rewritability and study the decidability of whether a given ontology\nenjoys PTime query evaluation, presenting both positive and negative results. \n\n"}
{"id": "1804.07598", "contents": "Title: OpenFPM: A scalable open framework for particle and particle-mesh codes\n  on parallel computers Abstract: Scalable and efficient numerical simulations continue to gain importance, as\ncomputation is firmly established as the third pillar of discovery, alongside\ntheory and experiment. Meanwhile, the performance of computing hardware grows\nthrough increasing heterogeneous parallelism, enabling simulations of ever more\ncomplex models. However, efficiently implementing scalable codes on\nheterogeneous, distributed hardware systems becomes the bottleneck. This\nbottleneck can be alleviated by intermediate software layers that provide\nhigher-level abstractions closer to the problem domain, hence allowing the\ncomputational scientist to focus on the simulation. Here, we present OpenFPM,\nan open and scalable framework that provides an abstraction layer for numerical\nsimulations using particles and/or meshes. OpenFPM provides transparent and\nscalable infrastructure for shared-memory and distributed-memory\nimplementations of particles-only and hybrid particle-mesh simulations of both\ndiscrete and continuous models, as well as non-simulation codes. This\ninfrastructure is complemented with portable implementations of frequently used\nnumerical routines, as well as interfaces to third-party libraries. We present\nthe architecture and design of OpenFPM, detail the underlying abstractions, and\nbenchmark the framework in applications ranging from Smoothed-Particle\nHydrodynamics (SPH) to Molecular Dynamics (MD), Discrete Element Methods (DEM),\nVortex Methods, stencil codes, high-dimensional Monte Carlo sampling (CMA-ES),\nand Reaction-Diffusion solvers, comparing it to the current state of the art\nand existing software frameworks. \n\n"}
{"id": "1804.07747", "contents": "Title: Cut to Fit: Tailoring the Partitioning to the Computation Abstract: Social Graph Analytics applications are very often built using off-the-shelf\nanalytics frameworks. These, however, are profiled and optimized for the\ngeneral case and have to perform for all kinds of graphs. This paper\ninvestigates how knowledge of the application and the dataset can help optimize\nperformance with minimal effort. We concentrate on the impact of partitioning\nstrategies on the performance of computations on social graphs. We evaluate six\ngraph partitioning algorithms on a set of six social graphs, using four\nstandard graph algorithms by measuring a set of five partitioning metrics.\n  We analyze the performance of each partitioning strategy with respect to (i)\nthe properties of the graph dataset, (ii) each analytics computation,of\npartitions. We discover that communication cost is the best predictor of\nperformance for most -but not all- analytics computations. We also find that\nthe best partitioning strategy for a particular kind of algorithm may not be\nthe best for another, and that optimizing for the general case of all\nalgorithms may not select the optimal partitioning strategy for a given graph\nalgorithm. We conclude with insights on selecting the right data partitioning\nstrategy, which has significant impact on the performance of large graph\nanalytics computations; certainly enough to warrant optimization of the\npartitioning strategy to the computation and to the dataset. \n\n"}
{"id": "1804.08229", "contents": "Title: Task Planning in Robotics: an Empirical Comparison of PDDL-based and\n  ASP-based Systems Abstract: Robots need task planning algorithms to sequence actions toward accomplishing\ngoals that are impossible through individual actions. Off-the-shelf task\nplanners can be used by intelligent robotics practitioners to solve a variety\nof planning problems. However, many different planners exist, each with\ndifferent strengths and weaknesses, and there are no general rules for which\nplanner would be best to apply to a given problem.\n  In this article, we empirically compare the performance of state-of-the-art\nplanners that use either the Planning Domain Description Language (PDDL), or\nAnswer Set Programming (ASP) as the underlying action language. PDDL is\ndesigned for task planning, and PDDL-based planners are widely used for a\nvariety of planning problems. ASP is designed for knowledge-intensive\nreasoning, but can also be used for solving task planning problems. Given\ndomain encodings that are as similar as possible, we find that PDDL-based\nplanners perform better on problems with longer solutions, and ASP-based\nplanners are better on tasks with a large number of objects or in which complex\nreasoning is required to reason about action preconditions and effects. The\nresulting analysis can inform selection among general purpose planning systems\nfor particular robot task planning domains. \n\n"}
{"id": "1804.08265", "contents": "Title: Deterministic and Randomized Diffusion based Iterative Generalized Hard\n  Thresholding (DiFIGHT) for Distributed Sparse Signal Recovery Abstract: In this paper we propose a distributed iterated hard thresholding algorithm\ntermed DiFIGHT over a network that is built on the diffusion mechanism and also\npropose a modification of the proposed algorithm, termed MoDiFIGHT, that has\nlow complexity in terms of communication in the network. We additionally\npropose four different strategies termed RP, RNP, RGP$_r$, and RGNP$_r$ that\nare used to randomly select a subset of nodes that are subsequently activated\nto take part in the distributed algorithm, so as to reduce the mean number of\ncommunications during the run of the distributed algorithm. We present\ntheoretical estimates of the long run communication per unit time for these\ndifferent strategies, when used by the two proposed algorithms. Also, we\npresent analysis of the two proposed algorithms and provide provable bounds on\ntheir recovery performance with or without using the random node selection\nstrategies. Finally we use numerical studies to show that both when the random\nstrategies are used as well as when they are not used, the proposed algorithms\ndisplay performances far superior to distributed IHT algorithm using consensus\nmechanism . \n\n"}
{"id": "1804.08316", "contents": "Title: Bilingual Embeddings with Random Walks over Multilingual Wordnets Abstract: Bilingual word embeddings represent words of two languages in the same space,\nand allow to transfer knowledge from one language to the other without machine\ntranslation. The main approach is to train monolingual embeddings first and\nthen map them using bilingual dictionaries. In this work, we present a novel\nmethod to learn bilingual embeddings based on multilingual knowledge bases (KB)\nsuch as WordNet. Our method extracts bilingual information from multilingual\nwordnets via random walks and learns a joint embedding space in one go. We\nfurther reinforce cross-lingual equivalence adding bilingual con- straints in\nthe loss function of the popular skipgram model. Our experiments involve twelve\ncross-lingual word similarity and relatedness datasets in six lan- guage pairs\ncovering four languages, and show that: 1) random walks over mul- tilingual\nwordnets improve results over just using dictionaries; 2) multilingual wordnets\non their own improve over text-based systems in similarity datasets; 3) the\ngood results are consistent for large wordnets (e.g. English, Spanish), smaller\nwordnets (e.g. Basque) or loosely aligned wordnets (e.g. Italian); 4) the\ncombination of wordnets and text yields the best results, above mapping-based\napproaches. Our method can be applied to richer KBs like DBpedia or Babel- Net,\nand can be easily extended to multilingual embeddings. All software and\nresources are open source. \n\n"}
{"id": "1804.08834", "contents": "Title: Measuring and Computing Database Inconsistency via Repairs Abstract: We propose a generic numerical measure of inconsistency of a database with\nrespect to a set of integrity constraints. It is based on an abstract repair\nsemantics. A particular inconsistency measure associated to cardinality-repairs\nis investigated; and we show that it can be computed via answer-set programs.\n  Keywords: Integrity constraints in databases, inconsistent databases,\ndatabase repairs, inconsistency measure. \n\n"}
{"id": "1804.09447", "contents": "Title: On the satisfiability problem for fragments of the two-variable logic\n  with one transitive relation Abstract: We study the satisfiability problem for the two-variable first-order logic\nover structures with one transitive relation. % We show that the problem is\ndecidable in 2-NExpTime for the fragment consisting of formulas where\nexistential quantifiers are guarded by transitive atoms. As this fragment\nenjoys neither the finite model nor the tree model property, to show\ndecidability we introduce novel model construction technique based on the\ninfinite Ramsey theorem.\n  We also point out why the technique is not sufficient to obtain decidability\nfor the full two-variable logic with one transitive relation, hence contrary to\nour previous claim, [FO$^2$ with one transitive relation is decidable, STACS\n2013: 317-328], the status of the latter problem remains open. \n\n"}
{"id": "1804.09494", "contents": "Title: On Optimizing Distributed Tucker Decomposition for Sparse Tensors Abstract: The Tucker decomposition generalizes the notion of Singular Value\nDecomposition (SVD) to tensors, the higher dimensional analogues of matrices.\nWe study the problem of constructing the Tucker decomposition of sparse tensors\non distributed memory systems via the HOOI procedure, a popular iterative\nmethod. The scheme used for distributing the input tensor among the processors\n(MPI ranks) critically influences the HOOI execution time. Prior work has\nproposed different distribution schemes: an offline scheme based on\nsophisticated hypergraph partitioning method and simple, lightweight\nalternatives that can be used real-time. While the hypergraph based scheme\ntypically results in faster HOOI execution time, being complex, the time taken\nfor determining the distribution is an order of magnitude higher than the\nexecution time of a single HOOI iteration. Our main contribution is a\nlightweight distribution scheme, which achieves the best of both worlds. We\nshow that the scheme is near-optimal on certain fundamental metrics associated\nwith the HOOI procedure and as a result, near-optimal on the computational load\n(FLOPs). Though the scheme may incur higher communication volume, the\ncomputation time is the dominant factor and as the result, the scheme achieves\nbetter performance on the overall HOOI execution time. Our experimental\nevaluation on large real-life tensors (having up to 4 billion elements) shows\nthat the scheme outperforms the prior schemes on the HOOI execution time by a\nfactor of up to 3x. On the other hand, its distribution time is comparable to\nthe prior lightweight schemes and is typically lesser than the execution time\nof a single HOOI iteration. \n\n"}
{"id": "1804.10013", "contents": "Title: Distributed Ledger Technology: Blockchain Compared to Directed Acyclic\n  Graph Abstract: Nowadays, blockchain is becoming a synonym for distributed ledger technology.\nHowever, blockchain is only one of the specializations in the field and is\ncurrently well-covered in existing literature, but mostly from a cryptographic\npoint of view. Besides blockchain technology, a new paradigm is gaining\nmomentum: directed acyclic graphs. The contribution presented in this paper is\ntwofold. Firstly, the paper analyzes distributed ledger technology with an\nemphasis on the features relevant to distributed systems. Secondly, the paper\nanalyses the usage of directed acyclic graph paradigm in the context of\ndistributed ledgers, and compares it with the blockchain-based solutions. The\ntwo paradigms are compared using representative implementations: Bitcoin,\nEthereum and Nano. We examine representative solutions in terms of the applied\ndata structures for maintaining the ledger, consensus mechanisms, transaction\nconfirmation confidence, ledger size, and scalability. \n\n"}
{"id": "1804.10140", "contents": "Title: Securing Distributed Gradient Descent in High Dimensional Statistical\n  Learning Abstract: We consider unreliable distributed learning systems wherein the training data\nis kept confidential by external workers, and the learner has to interact\nclosely with those workers to train a model. In particular, we assume that\nthere exists a system adversary that can adaptively compromise some workers;\nthe compromised workers deviate from their local designed specifications by\nsending out arbitrarily malicious messages.\n  We assume in each communication round, up to $q$ out of the $m$ workers\nsuffer Byzantine faults. Each worker keeps a local sample of size $n$ and the\ntotal sample size is $N=nm$. We propose a secured variant of the gradient\ndescent method that can tolerate up to a constant fraction of Byzantine\nworkers, i.e., $q/m = O(1)$. Moreover, we show the statistical estimation error\nof the iterates converges in $O(\\log N)$ rounds to $O(\\sqrt{q/N} +\n\\sqrt{d/N})$, where $d$ is the model dimension. As long as $q=O(d)$, our\nproposed algorithm achieves the optimal error rate $O(\\sqrt{d/N})$. Our results\nare obtained under some technical assumptions. Specifically, we assume\nstrongly-convex population risk. Nevertheless, the empirical risk (sample\nversion) is allowed to be non-convex. The core of our method is to robustly\naggregate the gradients computed by the workers based on the filtering\nprocedure proposed by Steinhardt et al. On the technical front, deviating from\nthe existing literature on robustly estimating a finite-dimensional mean\nvector, we establish a {\\em uniform} concentration of the sample covariance\nmatrix of gradients, and show that the aggregated gradient, as a function of\nmodel parameter, converges uniformly to the true gradient function. To get a\nnear-optimal uniform concentration bound, we develop a new matrix concentration\ninequality, which might be of independent interest. \n\n"}
{"id": "1804.10331", "contents": "Title: Rateless Codes for Near-Perfect Load Balancing in Distributed\n  Matrix-Vector Multiplication Abstract: Large-scale machine learning and data mining applications require computer\nsystems to perform massive matrix-vector and matrix-matrix multiplication\noperations that need to be parallelized across multiple nodes. The presence of\nstraggling nodes -- computing nodes that unpredictably slowdown or fail -- is a\nmajor bottleneck in such distributed computations. Ideal load balancing\nstrategies that dynamically allocate more tasks to faster nodes require\nknowledge or monitoring of node speeds as well as the ability to quickly move\ndata. Recently proposed fixed-rate erasure coding strategies can handle\nunpredictable node slowdown, but they ignore partial work done by straggling\nnodes thus resulting in a lot of redundant computation. We propose a\n\\emph{rateless fountain coding} strategy that achieves the best of both worlds\n-- we prove that its latency is asymptotically equal to ideal load balancing,\nand it performs asymptotically zero redundant computations. Our idea is to\ncreate linear combinations of the $m$ rows of the matrix and assign these\nencoded rows to different worker nodes. The original matrix-vector product can\nbe decoded as soon as slightly more than $m$ row-vector products are\ncollectively finished by the nodes. We conduct experiments in three computing\nenvironments: local parallel computing, Amazon EC2, and Amazon Lambda, which\nshow that rateless coding gives as much as $3\\times$ speed-up over uncoded\nschemes. \n\n"}
{"id": "1804.10540", "contents": "Title: A theory of linear typings as flows on 3-valent graphs Abstract: Building on recently established enumerative connections between lambda\ncalculus and the theory of embedded graphs (or \"maps\"), this paper develops an\nanalogy between typing (of lambda terms) and coloring (of maps). Our starting\npoint is the classical notion of an abelian group-valued \"flow\" on an abstract\ngraph (Tutte, 1954). Typing a linear lambda term may be naturally seen as\nconstructing a flow (on an embedded 3-valent graph with boundary) valued in a\nmore general algebraic structure consisting of a preordered set equipped with\nan \"implication\" operation and unit satisfying composition, identity, and unit\nlaws. Interesting questions and results from the theory of flows (such as the\nexistence of nowhere-zero flows) may then be re-examined from the standpoint of\nlambda calculus and logic. For example, we give a characterization of when the\nlocal flow relations (across vertices) may be categorically lifted to a global\nflow relation (across the boundary), proving that this holds just in case the\nunderlying map has the orientation of a lambda term. We also develop a basic\ntheory of rewriting of flows that suggests topological meanings for classical\ncompleteness results in combinatory logic, and introduce a polarized notion of\nflow, which draws connections to the theory of proof-nets in linear logic and\nto bidirectional typing. \n\n"}
{"id": "1804.10829", "contents": "Title: Formal Security Analysis of Neural Networks using Symbolic Intervals Abstract: Due to the increasing deployment of Deep Neural Networks (DNNs) in real-world\nsecurity-critical domains including autonomous vehicles and collision avoidance\nsystems, formally checking security properties of DNNs, especially under\ndifferent attacker capabilities, is becoming crucial. Most existing security\ntesting techniques for DNNs try to find adversarial examples without providing\nany formal security guarantees about the non-existence of such adversarial\nexamples. Recently, several projects have used different types of\nSatisfiability Modulo Theory (SMT) solvers to formally check security\nproperties of DNNs. However, all of these approaches are limited by the high\noverhead caused by the solver.\n  In this paper, we present a new direction for formally checking security\nproperties of DNNs without using SMT solvers. Instead, we leverage interval\narithmetic to compute rigorous bounds on the DNN outputs. Our approach, unlike\nexisting solver-based approaches, is easily parallelizable. We further present\nsymbolic interval analysis along with several other optimizations to minimize\noverestimations of output bounds.\n  We design, implement, and evaluate our approach as part of ReluVal, a system\nfor formally checking security properties of Relu-based DNNs. Our extensive\nempirical results show that ReluVal outperforms Reluplex, a state-of-the-art\nsolver-based system, by 200 times on average. On a single 8-core machine\nwithout GPUs, within 4 hours, ReluVal is able to verify a security property\nthat Reluplex deemed inconclusive due to timeout after running for more than 5\ndays. Our experiments demonstrate that symbolic interval analysis is a\npromising new direction towards rigorously analyzing different security\nproperties of DNNs. \n\n"}
{"id": "1804.10938", "contents": "Title: Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge,\n  Deep Architectures, and Beyond Abstract: Automatic understanding of human affect using visual signals is of great\nimportance in everyday human-machine interactions. Appraising human emotional\nstates, behaviors and reactions displayed in real-world settings, can be\naccomplished using latent continuous dimensions (e.g., the circumplex model of\naffect). Valence (i.e., how positive or negative is an emotion) & arousal\n(i.e., power of the activation of the emotion) constitute popular and effective\naffect representations. Nevertheless, the majority of collected datasets this\nfar, although containing naturalistic emotional states, have been captured in\nhighly controlled recording conditions. In this paper, we introduce the\nAff-Wild benchmark for training and evaluating affect recognition algorithms.\nWe also report on the results of the First Affect-in-the-wild Challenge that\nwas organized in conjunction with CVPR 2017 on the Aff-Wild database and was\nthe first ever challenge on the estimation of valence and arousal in-the-wild.\nFurthermore, we design and extensively train an end-to-end deep neural\narchitecture which performs prediction of continuous emotion dimensions based\non visual cues. The proposed deep learning architecture, AffWildNet, includes\nconvolutional & recurrent neural network layers, exploiting the invariant\nproperties of convolutional features, while also modeling temporal dynamics\nthat arise in human behavior via the recurrent layers. The AffWildNet produced\nstate-of-the-art results on the Aff-Wild Challenge. We then exploit the AffWild\ndatabase for learning features, which can be used as priors for achieving best\nperformances both for dimensional, as well as categorical emotion recognition,\nusing the RECOLA, AFEW-VA and EmotiW datasets, compared to all other methods\ndesigned for the same goal. The database and emotion recognition models are\navailable at http://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge. \n\n"}
{"id": "1804.11044", "contents": "Title: Prospects for Declarative Mathematical Modeling of Complex Biological\n  Systems Abstract: Declarative modeling uses symbolic expressions to represent models. With such\nexpressions one can formalize high-level mathematical computations on models\nthat would be difficult or impossible to perform directly on a lower-level\nsimulation program, in a general-purpose programming language. Examples of such\ncomputations on models include model analysis, relatively general-purpose\nmodel-reduction maps, and the initial phases of model implementation, all of\nwhich should preserve or approximate the mathematical semantics of a complex\nbiological model. The potential advantages are particularly relevant in the\ncase of developmental modeling, wherein complex spatial structures exhibit\ndynamics at molecular, cellular, and organogenic levels to relate genotype to\nmulticellular phenotype. Multiscale modeling can benefit from both the\nexpressive power of declarative modeling languages and the application of model\nreduction methods to link models across scale. Based on previous work, here we\ndefine declarative modeling of complex biological systems by defining the\noperator algebra semantics of an increasingly powerful series of declarative\nmodeling languages including reaction-like dynamics of parameterized and\nextended objects; we define semantics-preserving implementation and\nsemantics-approximating model reduction transformations; and we outline a\n\"meta-hierarchy\" for organizing declarative models and the mathematical methods\nthat can fruitfully manipulate them. \n\n"}
{"id": "1805.00280", "contents": "Title: Efficient Graph Computation for Node2Vec Abstract: Node2Vec is a state-of-the-art general-purpose feature learning method for\nnetwork analysis. However, current solutions cannot run Node2Vec on large-scale\ngraphs with billions of vertices and edges, which are common in real-world\napplications. The existing distributed Node2Vec on Spark incurs significant\nspace and time overhead. It runs out of memory even for mid-sized graphs with\nmillions of vertices. Moreover, it considers at most 30 edges for every vertex\nin generating random walks, causing poor result quality. In this paper, we\npropose Fast-Node2Vec, a family of efficient Node2Vec random walk algorithms on\na Pregel-like graph computation framework. Fast-Node2Vec computes transition\nprobabilities during random walks to reduce memory space consumption and\ncomputation overhead for large-scale graphs. The Pregel-like scheme avoids\nspace and time overhead of Spark's read-only RDD structures and shuffle\noperations. Moreover, we propose a number of optimization techniques to further\nreduce the computation overhead for popular vertices with large degrees.\nEmpirical evaluation show that Fast-Node2Vec is capable of computing Node2Vec\non graphs with billions of vertices and edges on a mid-sized machine cluster.\nCompared to Spark-Node2Vec, Fast-Node2Vec achieves 7.7--122x speedups. \n\n"}
{"id": "1805.00289", "contents": "Title: Denotational semantics of recursive types in synthetic guarded domain\n  theory Abstract: Just like any other branch of mathematics, denotational semantics of\nprogramming languages should be formalised in type theory, but adapting\ntraditional domain theoretic semantics, as originally formulated in classical\nset theory to type theory has proven challenging. This paper is part of a\nproject on formulating denotational semantics in type theories with guarded\nrecursion. This should have the benefit of not only giving simpler semantics\nand proofs of properties such as adequacy, but also hopefully in the future to\nscale to languages with advanced features, such as general references, outside\nthe reach of traditional domain theoretic techniques. Working in Guarded\nDependent Type Theory (GDTT), we develop denotational semantics for FPC, the\nsimply typed lambda calculus extended with recursive types, modelling the\nrecursive types of FPC using the guarded recursive types of GDTT. We prove\nsoundness and computational adequacy of the model in GDTT using a logical\nrelation between syntax and semantics constructed also using guarded recursive\ntypes. The denotational semantics is intensional in the sense that it counts\nthe number of unfold-fold reductions needed to compute the value of a term, but\nwe construct a relation relating the denotations of extensionally equal terms,\ni.e., pairs of terms that compute the same value in a different number of\nsteps. Finally we show how the denotational semantics of terms can be executed\ninside type theory and prove that executing the denotation of a boolean term\ncomputes the same value as the operational semantics of FPC. \n\n"}
{"id": "1805.00326", "contents": "Title: I Know How You Feel: Emotion Recognition with Facial Landmarks Abstract: Classification of human emotions remains an important and challenging task\nfor many computer vision algorithms, especially in the era of humanoid robots\nwhich coexist with humans in their everyday life. Currently proposed methods\nfor emotion recognition solve this task using multi-layered convolutional\nnetworks that do not explicitly infer any facial features in the classification\nphase. In this work, we postulate a fundamentally different approach to solve\nemotion recognition task that relies on incorporating facial landmarks as a\npart of the classification loss function. To that end, we extend a recently\nproposed Deep Alignment Network (DAN), that achieves state-of-the-art results\nin the recent facial landmark recognition challenge, with a term related to\nfacial features. Thanks to this simple modification, our model called\nEmotionalDAN is able to outperform state-of-the-art emotion classification\nmethods on two challenging benchmark dataset by up to 5%. \n\n"}
{"id": "1805.00705", "contents": "Title: Investigating Audio, Visual, and Text Fusion Methods for End-to-End\n  Automatic Personality Prediction Abstract: We propose a tri-modal architecture to predict Big Five personality trait\nscores from video clips with different channels for audio, text, and video\ndata. For each channel, stacked Convolutional Neural Networks are employed. The\nchannels are fused both on decision-level and by concatenating their respective\nfully connected layers. It is shown that a multimodal fusion approach\noutperforms each single modality channel, with an improvement of 9.4\\% over the\nbest individual modality (video). Full backpropagation is also shown to be\nbetter than a linear combination of modalities, meaning complex interactions\nbetween modalities can be leveraged to build better models. Furthermore, we can\nsee the prediction relevance of each modality for each trait. The described\nmodel can be used to increase the emotional intelligence of virtual agents. \n\n"}
{"id": "1805.00748", "contents": "Title: One Theorem to Rule Them All: A Unified Translation of LTL into\n  {\\omega}-Automata Abstract: We present a unified translation of LTL formulas into deterministic Rabin\nautomata, limit-deterministic B\\\"uchi automata, and nondeterministic B\\\"uchi\nautomata. The translations yield automata of asymptotically optimal size\n(double or single exponential, respectively). All three translations are\nderived from one single Master Theorem of purely logical nature. The Master\nTheorem decomposes the language of a formula into a positive boolean\ncombination of languages that can be translated into {\\omega}-automata by\nelementary means. In particular, Safra's, ranking, and breakpoint constructions\nused in other translations are not needed. \n\n"}
{"id": "1805.01406", "contents": "Title: Distributed Community Detection via Metastability of the 2-Choices\n  Dynamics Abstract: We investigate the behavior of a simple majority dynamics on networks of\nagents whose interaction topology exhibits a community structure. By leveraging\nrecent advancements in the analysis of dynamics, we prove that, when the states\nof the nodes are randomly initialized, the system rapidly and stably converges\nto a configuration in which the communities maintain internal consensus on\ndifferent states. This is the first analytical result on the behavior of\ndynamics for non-consensus problems on non-complete topologies, based on the\nfirst symmetry-breaking analysis in such setting. Our result has several\nimplications in different contexts in which dynamics are adopted for\ncomputational and biological modeling purposes. In the context of Label\nPropagation Algorithms, a class of widely used heuristics for community\ndetection, it represents the first theoretical result on the behavior of a\ndistributed label propagation algorithm with quasi-linear message complexity.\nIn the context of evolutionary biology, dynamics such as the Moran process have\nbeen used to model the spread of mutations in genetic populations [Lieberman,\nHauert, and Nowak 2005]; our result shows that, when the probability of\nadoption of a given mutation by a node of the evolutionary graph depends\nsuper-linearly on the frequency of the mutation in the neighborhood of the node\nand the underlying evolutionary graph exhibits a community structure, there is\na non-negligible probability for species differentiation to occur. \n\n"}
{"id": "1805.01681", "contents": "Title: Encoding fairness in a synchronous concurrent program algebra: extended\n  version with proofs Abstract: Concurrent program refinement algebra provides a suitable basis for\nsupporting mechanised reasoning about shared-memory concurrent programs in a\ncompositional manner, for example, it supports the rely/guarantee approach of\nJones. The algebra makes use of a synchronous parallel operator motivated by\nAczel's trace model of concurrency and with similarities to Milner's SCCS. This\npaper looks at defining a form of fairness within the program algebra. The\nencoding allows one to reason about the fair execution of a single process in\nisolation as well as define fair-parallel in terms of a base parallel operator,\nof which no fairness properties are assumed. An algebraic theory to support\nfairness and fair-parallel is developed. \n\n"}
{"id": "1805.02010", "contents": "Title: Generalised Dining Philosophers as Feedback Control Abstract: We revisit the Generalised Dining Philosophers problem through the\nperspective of feedback control. The result is a modular development of the\nsolution using the notions of system and system composition (the latter due to\nTabuada) in a formal setting that employs simple equational reasoning. The\nmodular approach separates the solution architecture from the algorithmic\nminutiae and has the benefit of simplifying the design and correctness proofs.\n  Three variants of the problem are considered: N=1, and N>1 with centralised\nand distributed topology. The base case (N=1) reveals important insights into\nthe problem specification and the architecture of the solution. In each case,\nsolving the Generalised Dining Philosophers reduces to designing an appropriate\nfeedback controller. \n\n"}
{"id": "1805.02709", "contents": "Title: Biform Theories: Project Description Abstract: A biform theory is a combination of an axiomatic theory and an algorithmic\ntheory that supports the integration of reasoning and computation. These are\nideal for specifying and reasoning about algorithms that manipulate\nmathematical expressions. However, formalizing biform theories is challenging\nsince it requires the means to express statements about the interplay of what\nthese algorithms do and what their actions mean mathematically. This paper\ndescribes a project to develop a methodology for expressing, manipulating,\nmanaging, and generating mathematical knowledge as a network of biform\ntheories. It is a subproject of MathScheme, a long-term project at McMaster\nUniversity to produce a framework for integrating formal deduction and symbolic\ncomputation. \n\n"}
{"id": "1805.02971", "contents": "Title: Multinomial Logit Bandit with Linear Utility Functions Abstract: Multinomial logit bandit is a sequential subset selection problem which\narises in many applications. In each round, the player selects a\n$K$-cardinality subset from $N$ candidate items, and receives a reward which is\ngoverned by a {\\it multinomial logit} (MNL) choice model considering both item\nutility and substitution property among items. The player's objective is to\ndynamically learn the parameters of MNL model and maximize cumulative reward\nover a finite horizon $T$. This problem faces the exploration-exploitation\ndilemma, and the involved combinatorial nature makes it non-trivial. In recent\nyears, there have developed some algorithms by exploiting specific\ncharacteristics of the MNL model, but all of them estimate the parameters of\nMNL model separately and incur a regret no better than\n$\\tilde{O}\\big(\\sqrt{NT}\\big)$ which is not preferred for large candidate set\nsize $N$. In this paper, we consider the {\\it linear utility} MNL choice model\nwhose item utilities are represented as linear functions of $d$-dimension item\nfeatures, and propose an algorithm, titled {\\bf LUMB}, to exploit the\nunderlying structure. It is proven that the proposed algorithm achieves\n$\\tilde{O}\\big(dK\\sqrt{T}\\big)$ regret which is free of candidate set size.\nExperiments show the superiority of the proposed algorithm. \n\n"}
{"id": "1805.03391", "contents": "Title: Communication Complexity of Byzantine Agreement, Revisited Abstract: As Byzantine Agreement (BA) protocols find application in large-scale\ndecentralized cryptocurrencies, an increasingly important problem is to design\nBA protocols with improved communication complexity. A few existing works have\nshown how to achieve subquadratic BA under an {\\it adaptive} adversary.\nIntriguingly, they all make a common relaxation about the adaptivity of the\nattacker, that is, if an honest node sends a message and then gets corrupted in\nsome round, the adversary {\\it cannot erase the message that was already sent}\n--- henceforth we say that such an adversary cannot perform \"after-the-fact\nremoval\". By contrast, many (super-)quadratic BA protocols in the literature\ncan tolerate after-the-fact removal. In this paper, we first prove that\ndisallowing after-the-fact removal is necessary for achieving\nsubquadratic-communication BA.\n  Next, we show new subquadratic binary BA constructions (of course, assuming\nno after-the-fact removal) that achieves near-optimal resilience and expected\nconstant rounds under standard cryptographic assumptions and a public-key\ninfrastructure (PKI) in both synchronous and partially synchronous settings. In\ncomparison, all known subquadratic protocols make additional strong assumptions\nsuch as random oracles or the ability of honest nodes to erase secrets from\nmemory, and even with these strong assumptions, no prior work can achieve the\nabove properties. Lastly, we show that some setup assumption is necessary for\nachieving subquadratic multicast-based BA. \n\n"}
{"id": "1805.03490", "contents": "Title: Assessing Security and Performances of Consensus algorithms for\n  Permissioned Blockchains Abstract: Blockchain is a novel technology that is rising a lot of interest in the\nindustrial and re- search sectors because its properties of decentralisation,\nimmutability and data integrity. Initially, the underlying consensus mechanism\nhas been designed for permissionless block- chain on trustless network model\nthrough the proof-of-work, i.e. a mathematical challenge which requires high\ncomputational power. This solution suffers of poor performances, hence\nalternative consensus algorithms as the proof-of-stake have been proposed.\nConversely, for permissioned blockchain, where participants are known and\nauthenti- cated, variants of distributed consensus algorithms have been\nemployed. However, most of them comes out without formal expression of security\nanalysis and trust assumptions because the absence of an established knowledge.\nTherefore the lack of adequate analysis on these algorithms hinders any\ncautious evaluation of their effectiveness in a real-world setting where\nsystems are deployed over trustless networks, i.e. Internet ... \n\n"}
{"id": "1805.03691", "contents": "Title: Self-Stabilizing Task Allocation In Spite of Noise Abstract: We study the problem of distributed task allocation inspired by the behavior\nof social insects, which perform task allocation in a setting of limited\ncapabilities and noisy environment feedback. We assume that each task has a\ndemand that should be satisfied but not exceeded, i.e., there is an optimal\nnumber of ants that should be working on this task at a given time. The goal is\nto assign a near-optimal number of workers to each task in a distributed manner\nand without explicit access to the values of the demands nor the number of ants\nworking on the task.\n  We seek to answer the question of how the quality of task allocation depends\non the accuracy of assessing whether too many (overload) or not enough (lack)\nants are currently working on a given task. Concretely, we address the open\nquestion of solving task allocation in the model where each ant receives\nfeedback that depends on the deficit defined as the (possibly negative)\ndifference between the optimal demand and the current number of workers in the\ntask. The feedback is modeled as a random variable that takes value lack or\noverload with probability given by a sigmoid of the deficit. Each ants receives\nthe feedback independently, but the higher the overload or lack of workers for\na task, the more likely it is that all the ants will receive the same, correct\nfeedback from this task; the closer the deficit is to zero, the less reliable\nthe feedback becomes. We measure the performance of task allocation algorithms\nusing the notion of regret, defined as the absolute value of the deficit summed\nover all tasks and summed over time.\n  We propose a simple, constant-memory, self-stabilizing, distributed algorithm\nthat quickly converges from any initial distribution to a near-optimal\nassignment. We also show that our algorithm works not only under stochastic\nnoise but also in an adversarial noise setting. \n\n"}
{"id": "1805.03714", "contents": "Title: Foundations of Sequence-to-Sequence Modeling for Time Series Abstract: The availability of large amounts of time series data, paired with the\nperformance of deep-learning algorithms on a broad class of problems, has\nrecently led to significant interest in the use of sequence-to-sequence models\nfor time series forecasting. We provide the first theoretical analysis of this\ntime series forecasting framework. We include a comparison of\nsequence-to-sequence modeling to classical time series models, and as such our\ntheory can serve as a quantitative guide for practitioners choosing between\ndifferent modeling methodologies. \n\n"}
{"id": "1805.03716", "contents": "Title: Long Short-Term Memory as a Dynamically Computed Element-wise Weighted\n  Sum Abstract: LSTMs were introduced to combat vanishing gradients in simple RNNs by\naugmenting them with gated additive recurrent connections. We present an\nalternative view to explain the success of LSTMs: the gates themselves are\nversatile recurrent models that provide more representational power than\npreviously appreciated. We do this by decoupling the LSTM's gates from the\nembedded simple RNN, producing a new class of RNNs where the recurrence\ncomputes an element-wise weighted sum of context-independent functions of the\ninput. Ablations on a range of problems demonstrate that the gating mechanism\nalone performs as well as an LSTM in most settings, strongly suggesting that\nthe gates are doing much more in practice than just alleviating vanishing\ngradients. \n\n"}
{"id": "1805.03992", "contents": "Title: Order out of Chaos: Proving Linearizability Using Local Views Abstract: Proving the linearizability of highly concurrent data structures, such as\nthose using optimistic concurrency control, is a challenging task. The main\ndifficulty is in reasoning about the view of the memory obtained by the\nthreads, because as they execute, threads observe different fragments of memory\nfrom different points in time. Until today, every linearizability proof has\ntackled this challenge from scratch.\n  We present a unifying proof argument for the correctness of unsynchronized\ntraversals, and apply it to prove the linearizability of several highly\nconcurrent search data structures, including an optimistic self-balancing\nbinary search tree, the Lazy List and a lock-free skip list. Our framework\nharnesses {\\em sequential reasoning} about the view of a thread, considering\nthe thread as if it traverses the data structure without interference from\nother operations. Our key contribution is showing that properties of\nreachability along search paths can be deduced for concurrent traversals from\nsuch interference-free traversals, when certain intuitive conditions are met.\nBasing the correctness of traversals on such \\emph{local view arguments}\ngreatly simplifies linearizability proofs.\n  To apply our framework, the user proves that the data structure satisfies two\nconditions: (1) acyclicity of the order on memory, even when it is considered\nacross intermediate memory states, and (2) preservation of search paths to\nlocations modified by interfering writes. Establishing the conditions, as well\nas the full linearizability proof utilizing our proof argument, reduces to\nsimple concurrent reasoning. The result is a clear and comprehensible\ncorrectness proof, and elucidates common patterns underlying several existing\ndata structures. \n\n"}
{"id": "1805.04071", "contents": "Title: The Energy Complexity of Diameter and Minimum Cut Computation in\n  Bounded-Genus Networks Abstract: This paper investigates the energy complexity of distributed graph problems\nin multi-hop radio networks, where the energy cost of an algorithm is measured\nby the maximum number of awake rounds of a vertex. Recent works revealed that\nsome problems, such as broadcast, breadth-first search, and maximal matching,\ncan be solved with energy-efficient algorithms that consume only $\\text{poly}\n\\log n$ energy. However, there exist some problems, such as computing the\ndiameter of the graph, that require $\\Omega(n)$ energy to solve. To improve\nenergy efficiency for these problems, we focus on a special graph class:\nbounded-genus graphs. We present algorithms for computing the exact diameter,\nthe exact global minimum cut size, and a $(1 \\pm\\epsilon)$-approximate $s$-$t$\nminimum cut size with $\\tilde{O}(\\sqrt{n})$ energy for bounded-genus graphs.\nOur approach is based on a generic framework that divides the vertex set into\nhigh-degree and low-degree parts and leverages the structural properties of\nbounded-genus graphs to control the number of certain connected components in\nthe subgraph induced by the low-degree part. \n\n"}
{"id": "1805.04238", "contents": "Title: Stochastic Approximation for Risk-aware Markov Decision Processes Abstract: We develop a stochastic approximation-type algorithm to solve finite\nstate/action, infinite-horizon, risk-aware Markov decision processes. Our\nalgorithm has two loops. The inner loop computes the risk by solving a\nstochastic saddle-point problem. The outer loop performs $Q$-learning to\ncompute an optimal risk-aware policy. Several widely investigated risk measures\n(e.g. conditional value-at-risk, optimized certainty equivalent, and absolute\nsemi-deviation) are covered by our algorithm. Almost sure convergence and the\nconvergence rate of the algorithm are established. For an error tolerance\n$\\epsilon>0$ for the optimal $Q$-value estimation gap and learning rate\n$k\\in(1/2,\\,1]$, the overall convergence rate of our algorithm is\n$\\Omega((\\ln(1/\\delta\\epsilon)/\\epsilon^{2})^{1/k}+(\\ln(1/\\epsilon))^{1/(1-k)})$\nwith probability at least $1-\\delta$. \n\n"}
{"id": "1805.04770", "contents": "Title: Born Again Neural Networks Abstract: Knowledge Distillation (KD) consists of transferring \u00e2\u0080\u009cknowledge\u00e2\u0080\u009d from one\nmachine learning model (the teacher) to another (the student). Commonly, the\nteacher is a high-capacity model with formidable performance, while the student\nis more compact. By transferring knowledge, one hopes to benefit from the\nstudent\u00e2\u0080\u0099s compactness, without sacrificing too much performance. We study KD\nfrom a new perspective: rather than compressing models, we train students\nparameterized identically to their teachers. Surprisingly, these Born-Again\nNetworks (BANs), outperform their teachers significantly, both on computer\nvision and language modeling tasks. Our experiments with BANs based on\nDenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and\nCIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore\ntwo distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and\n(ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate\nthe essential components of KD, demonstrating the effect of the teacher outputs\non both predicted and non-predicted classes. \n\n"}
{"id": "1805.04955", "contents": "Title: Low-pass Recurrent Neural Networks - A memory architecture for\n  longer-term correlation discovery Abstract: Reinforcement learning (RL) agents performing complex tasks must be able to\nremember observations and actions across sizable time intervals. This is\nespecially true during the initial learning stages, when exploratory behaviour\ncan increase the delay between specific actions and their effects. Many new or\npopular approaches for learning these distant correlations employ\nbackpropagation through time (BPTT), but this technique requires storing\nobservation traces long enough to span the interval between cause and effect.\nBesides memory demands, learning dynamics like vanishing gradients and slow\nconvergence due to infrequent weight updates can reduce BPTT's practicality;\nmeanwhile, although online recurrent network learning is a developing topic,\nmost approaches are not efficient enough to use as replacements. We propose a\nsimple, effective memory strategy that can extend the window over which BPTT\ncan learn without requiring longer traces. We explore this approach empirically\non a few tasks and discuss its implications. \n\n"}
{"id": "1805.05137", "contents": "Title: Gracefully Degrading Gathering in Dynamic Rings Abstract: Gracefully degrading algorithms [Biely \\etal, TCS 2018] are designed to\ncircumvent impossibility results in dynamic systems by adapting themselves to\nthe dynamics. Indeed, such an algorithm solves a given problem under some\ndynamics and, moreover, guarantees that a weaker (but related) problem is\nsolved under a higher dynamics under which the original problem is impossible\nto solve. The underlying intuition is to solve the problem whenever possible\nbut to provide some kind of quality of service if the dynamics become\n(unpredictably) higher.In this paper, we apply for the first time this approach\nto robot networks. We focus on the fundamental problem of gathering a squad of\nautonomous robots on an unknown location of a dynamic ring. In this goal, we\nintroduce a set of weaker variants of this problem. Motivated by a set of\nimpossibility results related to the dynamics of the ring, we propose a\ngracefully degrading gathering algorithm. \n\n"}
{"id": "1805.05603", "contents": "Title: Neural Classification of Malicious Scripts: A study with JavaScript and\n  VBScript Abstract: Malicious scripts are an important computer infection threat vector. Our\nanalysis reveals that the two most prevalent types of malicious scripts include\nJavaScript and VBScript. The percentage of detected JavaScript attacks are on\nthe rise. To address these threats, we investigate two deep recurrent models,\nLaMP (LSTM and Max Pooling) and CPoLS (Convoluted Partitioning of Long\nSequences), which process JavaScript and VBScript as byte sequences. Lower\nlayers capture the sequential nature of these byte sequences while higher\nlayers classify the resulting embedding as malicious or benign. Unlike\npreviously proposed solutions, our models are trained in an end-to-end fashion\nallowing discriminative training even for the sequential processing layers.\nEvaluating these models on a large corpus of 296,274 JavaScript files indicates\nthat the best performing LaMP model has a 65.9% true positive rate (TPR) at a\nfalse positive rate (FPR) of 1.0%. Similarly, the best CPoLS model has a TPR of\n45.3% at an FPR of 1.0%. LaMP and CPoLS yield a TPR of 69.3% and 67.9%,\nrespectively, at an FPR of 1.0% on a collection of 240,504 VBScript files. \n\n"}
{"id": "1805.05827", "contents": "Title: Graph Signal Sampling via Reinforcement Learning Abstract: We formulate the problem of sampling and recovering clustered graph signal as\na multi-armed bandit (MAB) problem. This formulation lends naturally to\nlearning sampling strategies using the well-known gradient MAB algorithm. In\nparticular, the sampling strategy is represented as a probability distribution\nover the individual arms of the MAB and optimized using gradient ascent. Some\nillustrative numerical experiments indicate that the sampling strategies based\non the gradient MAB algorithm outperform existing sampling methods. \n\n"}
{"id": "1805.05874", "contents": "Title: Approximate Distributed Joins in Apache Spark Abstract: The join operation is a fundamental building block of parallel data\nprocessing. Unfortunately, it is very resource-intensive to compute an\nequi-join across massive datasets. The approximate computing paradigm allows\nusers to trade accuracy and latency for expensive data processing operations.\nThe equi-join operator is thus a natural candidate for optimization using\napproximation techniques. Although sampling-based approaches are widely used\nfor approximation, sampling over joins is a compelling but challenging task\nregarding the output quality. Naive approaches, which perform joins over\ndataset samples, would not preserve statistical properties of the join output.\n  To realize this potential, we interweave Bloom filter sketching and\nstratified sampling with the join computation in a new operator, ApproxJoin,\nthat preserves the statistical properties of the join output. ApproxJoin\nleverages a Bloom filter to avoid shuffling non-joinable data items around the\nnetwork and then applies stratified sampling to obtain a representative sample\nof the join output.\n  Our analysis shows that ApproxJoin scales well and significantly reduces data\nmovement, without sacrificing tight error bounds on the accuracy of the final\nresults. We implemented ApproxJoin in Apache Spark and evaluated ApproxJoin\nusing microbenchmarks and real-world case studies. The evaluation shows that\nApproxJoin achieves a speedup of 6-9x over unmodified Spark-based joins with\nthe same sampling rate. Furthermore, the speedup is accompanied by a\nsignificant reduction in the shuffled data volume, which is 5-82x less than\nunmodified Spark-based joins. \n\n"}
{"id": "1805.06504", "contents": "Title: Analogical Reasoning on Chinese Morphological and Semantic Relations Abstract: Analogical reasoning is effective in capturing linguistic regularities. This\npaper proposes an analogical reasoning task on Chinese. After delving into\nChinese lexical knowledge, we sketch 68 implicit morphological relations and 28\nexplicit semantic relations. A big and balanced dataset CA8 is then built for\nthis task, including 17813 questions. Furthermore, we systematically explore\nthe influences of vector representations, context features, and corpora on\nanalogical reasoning. With the experiments, CA8 is proved to be a reliable\nbenchmark for evaluating Chinese word embeddings. \n\n"}
{"id": "1805.06621", "contents": "Title: Generative networks as inverse problems with Scattering transforms Abstract: Generative Adversarial Nets (GANs) and Variational Auto-Encoders (VAEs)\nprovide impressive image generations from Gaussian white noise, but the\nunderlying mathematics are not well understood. We compute deep convolutional\nnetwork generators by inverting a fixed embedding operator. Therefore, they do\nnot require to be optimized with a discriminator or an encoder. The embedding\nis Lipschitz continuous to deformations so that generators transform linear\ninterpolations between input white noise vectors into deformations between\noutput images. This embedding is computed with a wavelet Scattering transform.\nNumerical experiments demonstrate that the resulting Scattering generators have\nsimilar properties as GANs or VAEs, without learning a discriminative network\nor an encoder. \n\n"}
{"id": "1805.06801", "contents": "Title: Dependability in a Multi-tenant Multi-framework Deep Learning\n  as-a-Service Platform Abstract: Deep learning (DL), a form of machine learning, is becoming increasingly\npopular in several application domains. As a result, cloud-based Deep Learning\nas a Service (DLaaS) platforms have become an essential infrastructure in many\norganizations. These systems accept, schedule, manage and execute DL training\njobs at scale.\n  This paper explores dependability in the context of a DLaaS platform used in\nIBM. We begin by explaining how DL training workloads are different, and what\nfeatures ensure dependability in this context. We then describe the\narchitecture, design and implementation of a cloud-based orchestration system\nfor DL training. We show how this system has been architected with\ndependability in mind while also being horizontally scalable, elastic, flexible\nand efficient. We also present an initial empirical evaluation of the overheads\nintroduced by our platform, and discuss tradeoffs between efficiency and\ndependability. \n\n"}
{"id": "1805.07339", "contents": "Title: Scanner: Efficient Video Analysis at Scale Abstract: A growing number of visual computing applications depend on the analysis of\nlarge video collections. The challenge is that scaling applications to operate\non these datasets requires efficient systems for pixel data access and parallel\nprocessing across large numbers of machines. Few programmers have the\ncapability to operate efficiently at these scales, limiting the field's ability\nto explore new applications that leverage big video data. In response, we have\ncreated Scanner, a system for productive and efficient video analysis at scale.\nScanner organizes video collections as tables in a data store optimized for\nsampling frames from compressed video, and executes pixel processing\ncomputations, expressed as dataflow graphs, on these frames. Scanner schedules\nvideo analysis applications expressed using these abstractions onto\nheterogeneous throughput computing hardware, such as multi-core CPUs, GPUs, and\nmedia processing ASICs, for high-throughput pixel processing. We demonstrate\nthe productivity of Scanner by authoring a variety of video processing\napplications including the synthesis of stereo VR video streams from\nmulti-camera rigs, markerless 3D human pose reconstruction from video, and\ndata-mining big video datasets such as hundreds of feature-length films or over\n70,000 hours of TV news. These applications achieve near-expert performance on\na single machine and scale efficiently to hundreds of machines, enabling\nformerly long-running big video data analysis tasks to be carried out in\nminutes to hours. \n\n"}
{"id": "1805.07563", "contents": "Title: Reinforcement Learning of Theorem Proving Abstract: We introduce a theorem proving algorithm that uses practically no domain\nheuristics for guiding its connection-style proof search. Instead, it runs many\nMonte-Carlo simulations guided by reinforcement learning from previous proof\nattempts. We produce several versions of the prover, parameterized by different\nlearning and guiding algorithms. The strongest version of the system is trained\non a large corpus of mathematical problems and evaluated on previously unseen\nproblems. The trained system solves within the same number of inferences over\n40% more problems than a baseline prover, which is an unusually high\nimprovement in this hard AI domain. To our knowledge this is the first time\nreinforcement learning has been convincingly applied to solving general\nmathematical problems on a large scale. \n\n"}
{"id": "1805.07883", "contents": "Title: How Many Samples are Needed to Estimate a Convolutional or Recurrent\n  Neural Network? Abstract: It is widely believed that the practical success of Convolutional Neural\nNetworks (CNNs) and Recurrent Neural Networks (RNNs) owes to the fact that CNNs\nand RNNs use a more compact parametric representation than their\nFully-Connected Neural Network (FNN) counterparts, and consequently require\nfewer training examples to accurately estimate their parameters. We initiate\nthe study of rigorously characterizing the sample-complexity of estimating CNNs\nand RNNs. We show that the sample-complexity to learn CNNs and RNNs scales\nlinearly with their intrinsic dimension and this sample-complexity is much\nsmaller than for their FNN counterparts. For both CNNs and RNNs, we also\npresent lower bounds showing our sample complexities are tight up to\nlogarithmic factors. Our main technical tools for deriving these results are a\nlocalized empirical process analysis and a new technical lemma characterizing\nthe convolutional and recurrent structure. We believe that these tools may\ninspire further developments in understanding CNNs and RNNs. \n\n"}
{"id": "1805.07891", "contents": "Title: Parameter Hub: a Rack-Scale Parameter Server for Distributed Deep Neural\n  Network Training Abstract: Distributed deep neural network (DDNN) training constitutes an increasingly\nimportant workload that frequently runs in the cloud. Larger DNN models and\nfaster compute engines are shifting DDNN training bottlenecks from computation\nto communication. This paper characterizes DDNN training to precisely pinpoint\nthese bottlenecks. We found that timely training requires high performance\nparameter servers (PSs) with optimized network stacks and gradient processing\npipelines, as well as server and network hardware with balanced computation and\ncommunication resources. We therefore propose PHub, a high performance\nmulti-tenant, rack-scale PS design. PHub co-designs the PS software and\nhardware to accelerate rack-level and hierarchical cross-rack parameter\nexchange, with an API compatible with many DDNN training frameworks. PHub\nprovides a performance improvement of up to 2.7x compared to state-of-the-art\ndistributed training techniques for cloud-based ImageNet workloads, with 25%\nbetter throughput per dollar. \n\n"}
{"id": "1805.07956", "contents": "Title: Multiple-Step Greedy Policies in Online and Approximate Reinforcement\n  Learning Abstract: Multiple-step lookahead policies have demonstrated high empirical competence\nin Reinforcement Learning, via the use of Monte Carlo Tree Search or Model\nPredictive Control. In a recent work \\cite{efroni2018beyond}, multiple-step\ngreedy policies and their use in vanilla Policy Iteration algorithms were\nproposed and analyzed. In this work, we study multiple-step greedy algorithms\nin more practical setups. We begin by highlighting a counter-intuitive\ndifficulty, arising with soft-policy updates: even in the absence of\napproximations, and contrary to the 1-step-greedy case, monotonic policy\nimprovement is not guaranteed unless the update stepsize is sufficiently large.\nTaking particular care about this difficulty, we formulate and analyze online\nand approximate algorithms that use such a multi-step greedy operator. \n\n"}
{"id": "1805.08195", "contents": "Title: Depth-Limited Solving for Imperfect-Information Games Abstract: A fundamental challenge in imperfect-information games is that states do not\nhave well-defined values. As a result, depth-limited search algorithms used in\nsingle-agent settings and perfect-information games do not apply. This paper\nintroduces a principled way to conduct depth-limited solving in\nimperfect-information games by allowing the opponent to choose among a number\nof strategies for the remainder of the game at the depth limit. Each one of\nthese strategies results in a different set of values for leaf nodes. This\nforces an agent to be robust to the different strategies an opponent may\nemploy. We demonstrate the effectiveness of this approach by building a\nmaster-level heads-up no-limit Texas hold'em poker AI that defeats two prior\ntop agents using only a 4-core CPU and 16 GB of memory. Developing such a\npowerful agent would have previously required a supercomputer. \n\n"}
{"id": "1805.08296", "contents": "Title: Data-Efficient Hierarchical Reinforcement Learning Abstract: Hierarchical reinforcement learning (HRL) is a promising approach to extend\ntraditional reinforcement learning (RL) methods to solve more complex tasks.\nYet, the majority of current HRL methods require careful task-specific design\nand on-policy training, making them difficult to apply in real-world scenarios.\nIn this paper, we study how we can develop HRL algorithms that are general, in\nthat they do not make onerous additional assumptions beyond standard RL\nalgorithms, and efficient, in the sense that they can be used with modest\nnumbers of interaction samples, making them suitable for real-world problems\nsuch as robotic control. For generality, we develop a scheme where lower-level\ncontrollers are supervised with goals that are learned and proposed\nautomatically by the higher-level controllers. To address efficiency, we\npropose to use off-policy experience for both higher and lower-level training.\nThis poses a considerable challenge, since changes to the lower-level behaviors\nchange the action space for the higher-level policy, and we introduce an\noff-policy correction to remedy this challenge. This allows us to take\nadvantage of recent advances in off-policy model-free RL to learn both higher-\nand lower-level policies using substantially fewer environment interactions\nthan on-policy algorithms. We term the resulting HRL agent HIRO and find that\nit is generally applicable and highly sample-efficient. Our experiments show\nthat HIRO can be used to learn highly complex behaviors for simulated robots,\nsuch as pushing objects and utilizing them to reach target locations, learning\nfrom only a few million samples, equivalent to a few days of real-time\ninteraction. In comparisons with a number of prior HRL methods, we find that\nour approach substantially outperforms previous state-of-the-art techniques. \n\n"}
{"id": "1805.08430", "contents": "Title: RPC Considered Harmful: Fast Distributed Deep Learning on RDMA Abstract: Deep learning emerges as an important new resource-intensive workload and has\nbeen successfully applied in computer vision, speech, natural language\nprocessing, and so on. Distributed deep learning is becoming a necessity to\ncope with growing data and model sizes. Its computation is typically\ncharacterized by a simple tensor data abstraction to model multi-dimensional\nmatrices, a data-flow graph to model computation, and iterative executions with\nrelatively frequent synchronizations, thereby making it substantially different\nfrom Map/Reduce style distributed big data computation.\n  RPC, commonly used as the communication primitive, has been adopted by\npopular deep learning frameworks such as TensorFlow, which uses gRPC. We show\nthat RPC is sub-optimal for distributed deep learning computation, especially\non an RDMA-capable network. The tensor abstraction and data-flow graph, coupled\nwith an RDMA network, offers the opportunity to reduce the unnecessary overhead\n(e.g., memory copy) without sacrificing programmability and generality. In\nparticular, from a data access point of view, a remote machine is abstracted\njust as a \"device\" on an RDMA channel, with a simple memory interface for\nallocating, reading, and writing memory regions. Our graph analyzer looks at\nboth the data flow graph and the tensors to optimize memory allocation and\nremote data access using this interface. The result is up to 25 times speedup\nin representative deep learning benchmarks against the standard gRPC in\nTensorFlow and up to 169% improvement even against an RPC implementation\noptimized for RDMA, leading to faster convergence in the training process. \n\n"}
{"id": "1805.08755", "contents": "Title: Energy-aware tree network formation among computationally weak nodes Abstract: We study the fundamental problem of distributed network formation among\nmobile agents of limited computational power that aim to achieve energy balance\nby wirelessly transmitting and receiving energy in a peer-to-peer manner.\nSpecifically, we design simple distributed protocols consisting of a small\nnumber of states and interaction rules for the formation of arbitrary and k-ary\ntree networks. Furthermore, we evaluate (theoretically and also using computer\nsimulations) a plethora of energy redistribution protocols that exploit\ndifferent levels of knowledge in order to achieve desired energy distributions\namong the agents which require that every agent has exactly or at least twice\nthe energy of the agents of higher depth, according to the structure of the\nnetwork. Our study shows that without using any knowledge about the network\nstructure, such energy distributions cannot be achieved in a timely manner,\nmeaning that there might be high energy loss during the redistribution process.\nOn the other hand, only a few extra bits of information seem to be enough to\nguarantee quick convergence to energy distributions that satisfy particular\nproperties, yielding low energy loss. \n\n"}
{"id": "1805.08893", "contents": "Title: On-the-fly Vertex Reuse for Massively-Parallel Software Geometry\n  Processing Abstract: Compute-mode rendering is becoming more and more attractive for non-standard\nrendering applications, due to the high flexibility of compute-mode execution.\nThese newly designed pipelines often include streaming vertex and geometry\nprocessing stages. In typical triangle meshes, the same transformed vertex is\non average required six times during rendering. To avoid redundant computation,\na post-transform cache is traditionally suggested to enable reuse of vertex\nprocessing results. However, traditional caching neither scales well as the\nhardware becomes more parallel, nor can be efficiently implemented in a\nsoftware design. We investigate alternative strategies to reusing vertex\nshading results on-the-fly for massively parallel software geometry processing.\nForming static and dynamic batching on the data input stream, we analyze the\neffectiveness of identifying potential local reuse based on sorting, hashing,\nand efficient intra-thread-group communication. Altogether, we present four\nvertex reuse strategies, tailored to modern parallel architectures. Our\nsimulations showcase that our batch-based strategies significantly outperform\nparallel caches in terms of reuse. On actual GPU hardware, our evaluation shows\nthat our strategies not only lead to good reuse of processing results, but also\nboost performance by $2-3\\times$ compared to na\\\"ively ignoring reuse in a\nvariety of practical applications. \n\n"}
{"id": "1805.09018", "contents": "Title: Cloud Brokerage: A Systematic Survey Abstract: Background: The proliferation of cloud providers and provisioning levels has\nopened a space for cloud brokerage services. Brokers intermediate between cloud\ncustomers and providers to assist the customer in selecting the most suitable\ncloud service, helping to manage the dimensionality, heterogeneity, and\nuncertainty associated with cloud services. Objective: This paper identifies\nand classifies approaches to realise cloud brokerage. By doing so, this paper\npresents an understanding of the state of the art and a novel taxonomy to\ncharacterise cloud brokers. Method: We conducted a systematic literature survey\nto compile studies related to cloud brokerage and explore how cloud brokers are\nengineered. We analysed the studies from multiple perspectives, such as\nmotivation, functionality, engineering approach, and evaluation methodology.\nResults: The survey resulted in a knowledge base of current proposals for\nrealising cloud brokers. The survey identified surprising differences between\nthe studies' implementations, with engineering efforts directed at combinations\nof market-based solutions, middlewares, toolkits, algorithms, semantic\nframeworks, and conceptual frameworks. Conclusion: Our comprehensive\nmeta-analysis shows that cloud brokerage is still a formative field. There is\nno doubt that progress has been achieved in the field but considerable\nchallenges remain to be addressed. This survey identifies such challenges and\ndirections for future research. \n\n"}
{"id": "1805.09542", "contents": "Title: A sequent calculus with dependent types for classical arithmetic Abstract: In a recent paper, Herbelin developed a calculus dPA$^\\omega$ in which\nconstructive proofs for the axioms of countable and dependent choices could be\nderived via the encoding of a proof of countable universal quantification as a\nstream of it components. However, the property of normalization (and therefore\nthe one of soundness) was only conjectured. The difficulty for the proof of\nnormalization is due to the simultaneous presence of dependent dependent types\n(for the constructive part of the choice), of control operators (for classical\nlogic), of coinductive objects (to encode functions of type $N\\rightarrow A$\ninto streams $(a_0,a_1,\\ldots)$) and of lazy evaluation with sharing (for these\ncoinductive objects).Building on previous works, we introduce in this paper a\nvariant of dPA$^\\omega$ presented as a sequent calculus. On the one hand, we\ntake advantage of a variant of Krivine classical realizability we developed to\nprove the normalization of classical call-by-need. On the other hand, we\nbenefit of dL, a classical sequent calculus with dependent types in which type\nsafety is ensured using delimited continuations together with a syntactic\nrestriction. By combining the techniques developed in these papers, we manage\nto define a realizability interpretation {\\`a} la Krivine of our calculus that\nallows us to prove normalization and soundness. \n\n"}
{"id": "1805.09653", "contents": "Title: Uncertainty-Aware Attention for Reliable Interpretation and Prediction Abstract: Attention mechanism is effective in both focusing the deep learning models on\nrelevant features and interpreting them. However, attentions may be unreliable\nsince the networks that generate them are often trained in a weakly-supervised\nmanner. To overcome this limitation, we introduce the notion of input-dependent\nuncertainty to the attention mechanism, such that it generates attention for\neach feature with varying degrees of noise based on the given input, to learn\nlarger variance on instances it is uncertain about. We learn this\nUncertainty-aware Attention (UA) mechanism using variational inference, and\nvalidate it on various risk prediction tasks from electronic health records on\nwhich our model significantly outperforms existing attention models. The\nanalysis of the learned attentions shows that our model generates attentions\nthat comply with clinicians' interpretation, and provide richer interpretation\nvia learned variance. Further evaluation of both the accuracy of the\nuncertainty calibration and the prediction performance with \"I don't know\"\ndecision show that UA yields networks with high reliability as well. \n\n"}
{"id": "1805.09657", "contents": "Title: Learning compositionally through attentive guidance Abstract: While neural network models have been successfully applied to domains that\nrequire substantial generalisation skills, recent studies have implied that\nthey struggle when solving the task they are trained on requires inferring its\nunderlying compositional structure. In this paper, we introduce Attentive\nGuidance, a mechanism to direct a sequence to sequence model equipped with\nattention to find more compositional solutions. We test it on two tasks,\ndevised precisely to assess the compositional capabilities of neural models,\nand we show that vanilla sequence to sequence models with attention overfit the\ntraining distribution, while the guided versions come up with compositional\nsolutions that fit the training and testing distributions almost equally well.\nMoreover, the learned solutions generalise even in cases where the training and\ntesting distributions strongly diverge. In this way, we demonstrate that\nsequence to sequence models are capable of finding compositional solutions\nwithout requiring extra components. These results helps to disentangle the\ncauses for the lack of systematic compositionality in neural networks, which\ncan in turn fuel future work. \n\n"}
{"id": "1805.09767", "contents": "Title: Local SGD Converges Fast and Communicates Little Abstract: Mini-batch stochastic gradient descent (SGD) is state of the art in large\nscale distributed training. The scheme can reach a linear speedup with respect\nto the number of workers, but this is rarely seen in practice as the scheme\noften suffers from large network delays and bandwidth limits. To overcome this\ncommunication bottleneck recent works propose to reduce the communication\nfrequency. An algorithm of this type is local SGD that runs SGD independently\nin parallel on different workers and averages the sequences only once in a\nwhile.\n  This scheme shows promising results in practice, but eluded thorough\ntheoretical analysis. We prove concise convergence rates for local SGD on\nconvex problems and show that it converges at the same rate as mini-batch SGD\nin terms of number of evaluated gradients, that is, the scheme achieves linear\nspeedup in the number of workers and mini-batch size. The number of\ncommunication rounds can be reduced up to a factor of T^{1/2}---where T denotes\nthe number of total steps---compared to mini-batch SGD. This also holds for\nasynchronous implementations. Local SGD can also be used for large scale\ntraining of deep learning models.\n  The results shown here aim serving as a guideline to further explore the\ntheoretical and practical aspects of local SGD in these applications. \n\n"}
{"id": "1805.09938", "contents": "Title: Automated Verification of Neural Networks: Advances, Challenges and\n  Perspectives Abstract: Neural networks are one of the most investigated and widely used techniques\nin Machine Learning. In spite of their success, they still find limited\napplication in safety- and security-related contexts, wherein assurance about\nnetworks' performances must be provided. In the recent past, automated\nreasoning techniques have been proposed by several researchers to close the gap\nbetween neural networks and applications requiring formal guarantees about\ntheir behavior. In this work, we propose a primer of such techniques and a\ncomprehensive categorization of existing approaches for the automated\nverification of neural networks. A discussion about current limitations and\ndirections for future investigation is provided to foster research on this\ntopic at the crossroads of Machine Learning and Automated Reasoning. \n\n"}
{"id": "1805.10005", "contents": "Title: Finite Sample Analysis of LSTD with Random Projections and Eligibility\n  Traces Abstract: Policy evaluation with linear function approximation is an important problem\nin reinforcement learning. When facing high-dimensional feature spaces, such a\nproblem becomes extremely hard considering the computation efficiency and\nquality of approximations. We propose a new algorithm, LSTD($\\lambda$)-RP,\nwhich leverages random projection techniques and takes eligibility traces into\nconsideration to tackle the above two challenges. We carry out theoretical\nanalysis of LSTD($\\lambda$)-RP, and provide meaningful upper bounds of the\nestimation error, approximation error and total generalization error. These\nresults demonstrate that LSTD($\\lambda$)-RP can benefit from random projection\nand eligibility traces strategies, and LSTD($\\lambda$)-RP can achieve better\nperformances than prior LSTD-RP and LSTD($\\lambda$) algorithms. \n\n"}
{"id": "1805.10817", "contents": "Title: GPGPU Linear Complexity t-SNE Optimization Abstract: The t-distributed Stochastic Neighbor Embedding (tSNE) algorithm has become\nin recent years one of the most used and insightful techniques for the\nexploratory data analysis of high-dimensional data. tSNE reveals clusters of\nhigh-dimensional data points at different scales while it requires only minimal\ntuning of its parameters. Despite these advantages, the computational\ncomplexity of the algorithm limits its application to relatively small\ndatasets. To address this problem, several evolutions of tSNE have been\ndeveloped in recent years, mainly focusing on the scalability of the similarity\ncomputations between data points. However, these contributions are insufficient\nto achieve interactive rates when visualizing the evolution of the tSNE\nembedding for large datasets. In this work, we present a novel approach to the\nminimization of the tSNE objective function that heavily relies on modern\ngraphics hardware and has linear computational complexity. Our technique does\nnot only beat the state of the art, but can even be executed on the client side\nin a browser. We propose to approximate the repulsion forces between data\npoints using adaptive-resolution textures that are drawn at every iteration\nwith WebGL. This approximation allows us to reformulate the tSNE minimization\nproblem as a series of tensor operation that are computed with TensorFlow.js, a\nJavaScript library for scalable tensor computations. \n\n"}
{"id": "1805.10820", "contents": "Title: Local Rule-Based Explanations of Black Box Decision Systems Abstract: The recent years have witnessed the rise of accurate but obscure decision\nsystems which hide the logic of their internal decision processes to the users.\nThe lack of explanations for the decisions of black box systems is a key\nethical issue, and a limitation to the adoption of machine learning components\nin socially sensitive and safety-critical contexts. %Therefore, we need\nexplanations that reveals the reasons why a predictor takes a certain decision.\nIn this paper we focus on the problem of black box outcome explanation, i.e.,\nexplaining the reasons of the decision taken on a specific instance. We propose\nLORE, an agnostic method able to provide interpretable and faithful\nexplanations. LORE first leans a local interpretable predictor on a synthetic\nneighborhood generated by a genetic algorithm. Then it derives from the logic\nof the local interpretable predictor a meaningful explanation consisting of: a\ndecision rule, which explains the reasons of the decision; and a set of\ncounterfactual rules, suggesting the changes in the instance's features that\nlead to a different outcome. Wide experiments show that LORE outperforms\nexisting methods and baselines both in the quality of explanations and in the\naccuracy in mimicking the black box. \n\n"}
{"id": "1805.11074", "contents": "Title: Reward Constrained Policy Optimization Abstract: Solving tasks in Reinforcement Learning is no easy feat. As the goal of the\nagent is to maximize the accumulated reward, it often learns to exploit\nloopholes and misspecifications in the reward signal resulting in unwanted\nbehavior. While constraints may solve this issue, there is no closed form\nsolution for general constraints. In this work we present a novel\nmulti-timescale approach for constrained policy optimization, called `Reward\nConstrained Policy Optimization' (RCPO), which uses an alternative penalty\nsignal to guide the policy towards a constraint satisfying one. We prove the\nconvergence of our approach and provide empirical evidence of its ability to\ntrain constraint satisfying policies. \n\n"}
{"id": "1805.11090", "contents": "Title: GenAttack: Practical Black-box Attacks with Gradient-Free Optimization Abstract: Deep neural networks are vulnerable to adversarial examples, even in the\nblack-box setting, where the attacker is restricted solely to query access.\nExisting black-box approaches to generating adversarial examples typically\nrequire a significant number of queries, either for training a substitute\nnetwork or performing gradient estimation. We introduce GenAttack, a\ngradient-free optimization technique that uses genetic algorithms for\nsynthesizing adversarial examples in the black-box setting. Our experiments on\ndifferent datasets (MNIST, CIFAR-10, and ImageNet) show that GenAttack can\nsuccessfully generate visually imperceptible adversarial examples against\nstate-of-the-art image recognition models with orders of magnitude fewer\nqueries than previous approaches. Against MNIST and CIFAR-10 models, GenAttack\nrequired roughly 2,126 and 2,568 times fewer queries respectively, than ZOO,\nthe prior state-of-the-art black-box attack. In order to scale up the attack to\nlarge-scale high-dimensional ImageNet models, we perform a series of\noptimizations that further improve the query efficiency of our attack leading\nto 237 times fewer queries against the Inception-v3 model than ZOO.\nFurthermore, we show that GenAttack can successfully attack some\nstate-of-the-art ImageNet defenses, including ensemble adversarial training and\nnon-differentiable or randomized input transformations. Our results suggest\nthat evolutionary algorithms open up a promising area of research into\neffective black-box attacks. \n\n"}
{"id": "1805.11752", "contents": "Title: Multi-turn Dialogue Response Generation in an Adversarial Learning\n  Framework Abstract: We propose an adversarial learning approach for generating multi-turn\ndialogue responses. Our proposed framework, hredGAN, is based on conditional\ngenerative adversarial networks (GANs). The GAN's generator is a modified\nhierarchical recurrent encoder-decoder network (HRED) and the discriminator is\na word-level bidirectional RNN that shares context and word embeddings with the\ngenerator. During inference, noise samples conditioned on the dialogue history\nare used to perturb the generator's latent space to generate several possible\nresponses. The final response is the one ranked best by the discriminator. The\nhredGAN shows improved performance over existing methods: (1) it generalizes\nbetter than networks trained using only the log-likelihood criterion, and (2)\nit generates longer, more informative and more diverse responses with high\nutterance and topic relevance even with limited training data. This improvement\nis demonstrated on the Movie triples and Ubuntu dialogue datasets using both\nautomatic and human evaluations. \n\n"}
{"id": "1805.11984", "contents": "Title: Automatic generation of object shapes with desired functionalities Abstract: 3D objects (artefacts) are made to fulfill functions. Designing an object\noften starts with defining a list of functionalities that it should provide,\nalso known as functional requirements. Today, the design of 3D object models is\nstill a slow and largely artisanal activity, with few Computer-Aided Design\n(CAD) tools existing to aid the exploration of the design solution space. To\naccelerate the design process, we introduce an algorithm for generating object\nshapes with desired functionalities. Following the concept of form follows\nfunction, we assume that existing object shapes were rationally chosen to\nprovide desired functionalities. First, we use an artificial neural network to\nlearn a function-to-form mapping by analysing a dataset of objects labeled with\ntheir functionalities. Then, we combine forms providing one or more desired\nfunctions, generating an object shape that is expected to provide all of them.\nFinally, we verify in simulation whether the generated object possesses the\ndesired functionalities, by defining and executing functionality tests on it. \n\n"}
{"id": "1805.12212", "contents": "Title: Monodromy Solver: Sequential and Parallel Abstract: We describe, study, and experiment with an algorithm for finding all\nsolutions of systems of polynomial equations using homotopy continuation and\nmonodromy. This algorithm follows a framework developed in previous work and\ncan operate in the presence of a large number of failures of the homotopy\ncontinuation subroutine. We give special attention to parallelization and\nprobabilistic analysis of a model adapted to parallelization and failures.\nApart from theoretical results, we developed a simulator that allows us to run\na large number of experiments without recomputing the outcomes of the\ncontinuation subroutine. \n\n"}
{"id": "1805.12258", "contents": "Title: Harmonic-summing Module of SKA on FPGA--Optimising the Irregular Memory\n  Accesses Abstract: The Square Kilometre Array (SKA), which will be the world's largest radio\ntelescope, will enhance and boost a large number of science projects, including\nthe search for pulsars. The frequency domain acceleration search is an\nefficient approach to search for binary pulsars. A significant part of it is\nthe harmonic-summing module, which is the research subject of this paper. Most\nof the operations in the harmonic-summing module are relatively cheap\noperations for FPGAs. The main challenge is the large number of point accesses\nto off-chip memory which are not consecutive but irregular. Although\nharmonic-summing alone might not be targeted for FPGA acceleration, it is a\npart of the pulsar search pipeline that contains many other compute-intensive\nmodules, which are efficiently executed on FPGA. Hence having the\nharmonic-summing also on the FPGA will avoid off-board communication, which\ncould destroy other acceleration benefits. Two types of harmonic-summing\napproaches are investigated in this paper: 1) storing intermediate data in\noff-chip memory and 2) processing the input signals directly without storing.\nFor the second type, two approaches of caching data are proposed and evaluated:\n1) preloading points that are frequently touched 2) preloading all necessary\npoints that are used to generate a chunk of output points. OpenCL is adopted to\nimplement the proposed approaches. In an extensive experimental evaluation, the\nsame OpenCL kernel codes are evaluated on FPGA boards and GPU cards. Regarding\nthe proposed preloading methods, preloading all necessary points method while\nreordering the input signals is faster than all the other methods. While in raw\nperformance a single FPGA board cannot compete with a GPU, in terms of energy\ndissipation, GPU costs up to 2.6x times more energy than that of FPGAs in\nexecuting the same NDRange kernels. \n\n"}
{"id": "1805.12487", "contents": "Title: Sequential Attacks on Agents for Long-Term Adversarial Goals Abstract: Reinforcement learning (RL) has advanced greatly in the past few years with\nthe employment of effective deep neural networks (DNNs) on the policy networks.\nWith the great effectiveness came serious vulnerability issues with DNNs that\nsmall adversarial perturbations on the input can change the output of the\nnetwork. Several works have pointed out that learned agents with a DNN policy\nnetwork can be manipulated against achieving the original task through a\nsequence of small perturbations on the input states. In this paper, we\ndemonstrate furthermore that it is also possible to impose an arbitrary\nadversarial reward on the victim policy network through a sequence of attacks.\nOur method involves the latest adversarial attack technique, Adversarial\nTransformer Network (ATN), that learns to generate the attack and is easy to\nintegrate into the policy network. As a result of our attack, the victim agent\nis misguided to optimise for the adversarial reward over time. Our results\nexpose serious security threats for RL applications in safety-critical systems\nincluding drones, medical analysis, and self-driving cars. \n\n"}
{"id": "1806.00931", "contents": "Title: Holographic Neural Architectures Abstract: Representation learning is at the heart of what makes deep learning\neffective. In this work, we introduce a new framework for representation\nlearning that we call \"Holographic Neural Architectures\" (HNAs). In the same\nway that an observer can experience the 3D structure of a holographed object by\nlooking at its hologram from several angles, HNAs derive Holographic\nRepresentations from the training set. These representations can then be\nexplored by moving along a continuous bounded single dimension. We show that\nHNAs can be used to make generative networks, state-of-the-art regression\nmodels and that they are inherently highly resistant to noise. Finally, we\nargue that because of their denoising abilities and their capacity to\ngeneralize well from very few examples, models based upon HNAs are particularly\nwell suited for biological applications where training examples are rare or\nnoisy. \n\n"}
{"id": "1806.01105", "contents": "Title: Performance tuning for deep learning on a many-core processor (master\n  thesis) Abstract: Convolutional neural networks (CNNs) are becoming very successful and popular\nfor a variety of applications. The Loki many-core processor architecture is\nvery promising for achieving specialised hardware performance and efficiency\nwhile being a general purpose solution. Loki combines many simple cores with\nincreased control for the programmer. This freedom can be exploited to produce\nmuch more efficient code than in conventional multiprocessors but it also\ncreates a very big design space for possible optimisations. In this project, I\nexplore possible optimisations for a CNN application, their portability on\ndifferent Loki-specific configurations, convolution parameters and inputs.\nFinally, I investigate the potential for adaptive algorithms for further\nperformance increase. \n\n"}
{"id": "1806.01556", "contents": "Title: Combining Multiple Optimised FPGA-based Pulsar Search Modules Using\n  OpenCL Abstract: Field-Programmable Gate Arrays (FPGAs) are widely used in the central signal\nprocessing design of the Square Kilometre Array (SKA) as acceleration hardware.\nThe frequency domain acceleration search (FDAS) module is an important part of\nthe SKA1-MID pulsar search engine. To develop for a yet to be finalised\nhardware, for cross-discipline interoperability and to achieve fast\nprototyping, OpenCL as a high-level FPGA synthesis approach is employed to\ncreate the sub-modules of FDAS. The FT convolution and the harmonic-summing\nplus some other minor sub-modules are elements in the FDAS module that have\nbeen well-optimised separately before. In this paper, we explore the design\nspace of combining well-optimised designs, dealing with the ensuing need to\ntrade-off and compromise. Pipeline computing is employed to handle multiple\ninput arrays at high speed. The hardware target is to employ multiple high-end\nFPGAs to process the combined FDAS module. The results show interesting\nconsequences, where the best individual solutions are not necessarily the best\nsolutions for the speed of a pipeline where FPGA resources and memory bandwidth\nneed to be shared. By proposing multiple buffering techniques to the pipeline,\nthe combined FDAS module can achieve up to 2x speedup over implementations\nwithout pipeline computing. We perform an extensive experimental evaluation on\nmultiple FPGA boards (Arria 10) hosted in a workstation and compare to a\ntechnology comparable mid-range GPU. \n\n"}
{"id": "1806.01775", "contents": "Title: A Memristor based Unsupervised Neuromorphic System Towards Fast and\n  Energy-Efficient GAN Abstract: Deep Learning has gained immense success in pushing today's artificial\nintelligence forward. To solve the challenge of limited labeled data in the\nsupervised learning world, unsupervised learning has been proposed years ago\nwhile low accuracy hinters its realistic applications. Generative adversarial\nnetwork (GAN) emerges as an unsupervised learning approach with promising\naccuracy and are under extensively study. However, the execution of GAN is\nextremely memory and computation intensive and results in ultra-low speed and\nhigh-power consumption. In this work, we proposed a holistic solution for fast\nand energy-efficient GAN computation through a memristor-based neuromorphic\nsystem. First, we exploited a hardware and software co-design approach to map\nthe computation blocks in GAN efficiently. We also proposed an efficient data\nflow for optimal parallelism training and testing, depending on the computation\ncorrelations between different computing blocks. To compute the unique and\ncomplex loss of GAN, we developed a diff-block with optimized accuracy and\nperformance. The experiment results on big data show that our design achieves\n2.8x speedup and 6.1x energy-saving compared with the traditional GPU\naccelerator, as well as 5.5x speedup and 1.4x energy-saving compared with the\nprevious FPGA-based accelerator. \n\n"}
{"id": "1806.02284", "contents": "Title: Corpus Conversion Service: A Machine Learning Platform to Ingest\n  Documents at Scale Abstract: Over the past few decades, the amount of scientific articles and technical\nliterature has increased exponentially in size. Consequently, there is a great\nneed for systems that can ingest these documents at scale and make the\ncontained knowledge discoverable. Unfortunately, both the format of these\ndocuments (e.g. the PDF format or bitmap images) as well as the presentation of\nthe data (e.g. complex tables) make the extraction of qualitative and\nquantitive data extremely challenging. In this paper, we present a modular,\ncloud-based platform to ingest documents at scale. This platform, called the\nCorpus Conversion Service (CCS), implements a pipeline which allows users to\nparse and annotate documents (i.e. collect ground-truth), train\nmachine-learning classification algorithms and ultimately convert any type of\nPDF or bitmap-documents to a structured content representation format. We will\nshow that each of the modules is scalable due to an asynchronous microservice\narchitecture and can therefore handle massive amounts of documents.\nFurthermore, we will show that our capability to gather ground-truth is\naccelerated by machine-learning algorithms by at least one order of magnitude.\nThis allows us to both gather large amounts of ground-truth in very little time\nand obtain very good precision/recall metrics in the range of 99\\% with regard\nto content conversion to structured output. The CCS platform is currently\ndeployed on IBM internal infrastructure and serving more than 250 active users\nfor knowledge-engineering project engagements. \n\n"}
{"id": "1806.02891", "contents": "Title: Revisiting the Importance of Individual Units in CNNs via Ablation Abstract: We revisit the importance of the individual units in Convolutional Neural\nNetworks (CNNs) for visual recognition. By conducting unit ablation experiments\non CNNs trained on large scale image datasets, we demonstrate that, though\nablating any individual unit does not hurt overall classification accuracy, it\ndoes lead to significant damage on the accuracy of specific classes. This\nresult shows that an individual unit is specialized to encode information\nrelevant to a subset of classes. We compute the correlation between the\naccuracy drop under unit ablation and various attributes of an individual unit\nsuch as class selectivity and weight L1 norm. We confirm that unit attributes\nsuch as class selectivity are a poor predictor for impact on overall accuracy\nas found previously in recent work \\cite{morcos2018importance}. However, our\nresults show that class selectivity along with other attributes are good\npredictors of the importance of one unit to individual classes. We evaluate the\nimpact of random rotation, batch normalization, and dropout to the importance\nof units to specific classes. Our results show that units with high selectivity\nplay an important role in network classification power at the individual class\nlevel. Understanding and interpreting the behavior of these units is necessary\nand meaningful. \n\n"}
{"id": "1806.02939", "contents": "Title: Flexible Load Balancing with Multi-dimensional State-space Collapse:\n  Throughput and Heavy-traffic Delay Optimality Abstract: Heavy traffic analysis for load balancing policies has relied heavily on the\ncondition of state-space collapse onto a single-dimensional line in previous\nworks. In this paper, via Lyapunov-drift analysis, we rigorously prove that\neven under a multi-dimensional state-space collapse, steady-state heavy-traffic\ndelay optimality can still be achieved for a general load balancing system.\nThis result directly implies that achieving steady-state heavy-traffic delay\noptimality simply requires that no server is idling while others are busy at\nheavy loads, thus complementing and extending the result obtained by diffusion\napproximations. Further, we explore the greater flexibility provided by\nallowing a multi-dimensional state-space collapse in designing new load\nbalancing policies that are both throughput optimal and heavy-traffic delay\noptimal in steady state. This is achieved by overcoming various technical\nchallenges, and the methods used in this paper could be of independent\ninterest. \n\n"}
{"id": "1806.03536", "contents": "Title: Representation Learning on Graphs with Jumping Knowledge Networks Abstract: Recent deep learning approaches for representation learning on graphs follow\na neighborhood aggregation procedure. We analyze some important properties of\nthese models, and propose a strategy to overcome those. In particular, the\nrange of \"neighboring\" nodes that a node's representation draws from strongly\ndepends on the graph structure, analogous to the spread of a random walk. To\nadapt to local neighborhood properties and tasks, we explore an architecture --\njumping knowledge (JK) networks -- that flexibly leverages, for each node,\ndifferent neighborhood ranges to enable better structure-aware representation.\nIn a number of experiments on social, bioinformatics and citation networks, we\ndemonstrate that our model achieves state-of-the-art performance. Furthermore,\ncombining the JK framework with models like Graph Convolutional Networks,\nGraphSAGE and Graph Attention Networks consistently improves those models'\nperformance. \n\n"}
{"id": "1806.03915", "contents": "Title: Decentralize and Randomize: Faster Algorithm for Wasserstein Barycenters Abstract: We study the decentralized distributed computation of discrete approximations\nfor the regularized Wasserstein barycenter of a finite set of continuous\nprobability measures distributedly stored over a network. We assume there is a\nnetwork of agents/machines/computers, and each agent holds a private continuous\nprobability measure and seeks to compute the barycenter of all the measures in\nthe network by getting samples from its local measure and exchanging\ninformation with its neighbors. Motivated by this problem, we develop, and\nanalyze, a novel accelerated primal-dual stochastic gradient method for general\nstochastic convex optimization problems with linear equality constraints. Then,\nwe apply this method to the decentralized distributed optimization setting to\nobtain a new algorithm for the distributed semi-discrete regularized\nWasserstein barycenter problem. Moreover, we show explicit non-asymptotic\ncomplexity for the proposed algorithm. \n\n"}
{"id": "1806.04284", "contents": "Title: iParaphrasing: Extracting Visually Grounded Paraphrases via an Image Abstract: A paraphrase is a restatement of the meaning of a text in other words.\nParaphrases have been studied to enhance the performance of many natural\nlanguage processing tasks. In this paper, we propose a novel task iParaphrasing\nto extract visually grounded paraphrases (VGPs), which are different phrasal\nexpressions describing the same visual concept in an image. These extracted\nVGPs have the potential to improve language and image multimodal tasks such as\nvisual question answering and image captioning. How to model the similarity\nbetween VGPs is the key of iParaphrasing. We apply various existing methods as\nwell as propose a novel neural network-based method with image attention, and\nreport the results of the first attempt toward iParaphrasing. \n\n"}
{"id": "1806.04328", "contents": "Title: Broadcast and minimum spanning tree with $o(m)$ messages in the\n  asynchronous CONGEST model Abstract: We provide the first asynchronous distributed algorithms to compute broadcast\nand minimum spanning tree with $o(m)$ bits of communication, in a graph with\n$n$ nodes and $m$ edges. For decades, it was believed that $\\Omega(m)$ bits of\ncommunication are required for any algorithm that constructs a broadcast tree.\nIn 2015, King, Kutten and Thorup showed that in the KT1 model where nodes have\ninitial knowledge of their neighbors' identities it is possible to construct\nMST in $\\tilde{O}(n)$ messages in the synchronous CONGEST model. In the CONGEST\nmodel messages are of size $O(\\log n)$. However, no algorithm with $o(m)$\nmessages were known for the asynchronous case. Here, we provide an algorithm\nthat uses $O(n^{3/2} \\log^{3/2} n)$ messages to find MST in the asynchronous\nCONGEST model. Our algorithm is randomized Monte Carlo and outputs MST with\nhigh probability. We will provide an algorithm for computing a spanning tree\nwith $O(n^{3/2} \\log^{3/2} n)$ messages. Given a spanning tree, we can compute\nMST with $\\tilde{O}(n)$ messages. \n\n"}
{"id": "1806.04552", "contents": "Title: Combining Model-Free Q-Ensembles and Model-Based Approaches for Informed\n  Exploration Abstract: Q-Ensembles are a model-free approach where input images are fed into\ndifferent Q-networks and exploration is driven by the assumption that\nuncertainty is proportional to the variance of the output Q-values obtained.\nThey have been shown to perform relatively well compared to other exploration\nstrategies. Further, model-based approaches, such as encoder-decoder models\nhave been used successfully for next frame prediction given previous frames.\nThis paper proposes to integrate the model-free Q-ensembles and model-based\napproaches with the hope of compounding the benefits of both and achieving\nsuperior exploration as a result. Results show that a model-based trajectory\nmemory approach when combined with Q-ensembles produces superior performance\nwhen compared to only using Q-ensembles. \n\n"}
{"id": "1806.06185", "contents": "Title: EdgeChain: An Edge-IoT Framework and Prototype Based on Blockchain and\n  Smart Contracts Abstract: The emerging Internet of Things (IoT) is facing significant scalability and\nsecurity challenges. On the one hand, IoT devices are \"weak\" and need external\nassistance. Edge computing provides a promising direction addressing the\ndeficiency of centralized cloud computing in scaling massive number of devices.\nOn the other hand, IoT devices are also relatively \"vulnerable\" facing\nmalicious hackers due to resource constraints. The emerging blockchain and\nsmart contracts technologies bring a series of new security features for IoT\nand edge computing. In this paper, to address the challenges, we design and\nprototype an edge-IoT framework named \"EdgeChain\" based on blockchain and smart\ncontracts. The core idea is to integrate a permissioned blockchain and the\ninternal currency or \"coin\" system to link the edge cloud resource pool with\neach IoT device' account and resource usage, and hence behavior of the IoT\ndevices. EdgeChain uses a credit-based resource management system to control\nhow much resource IoT devices can obtain from edge servers, based on\npre-defined rules on priority, application types and past behaviors. Smart\ncontracts are used to enforce the rules and policies to regulate the IoT device\nbehavior in a non-deniable and automated manner. All the IoT activities and\ntransactions are recorded into blockchain for secure data logging and auditing.\nWe implement an EdgeChain prototype and conduct extensive experiments to\nevaluate the ideas. The results show that while gaining the security benefits\nof blockchain and smart contracts, the cost of integrating them into EdgeChain\nis within a reasonable and acceptable range. \n\n"}
{"id": "1806.06192", "contents": "Title: Handling Cold-Start Collaborative Filtering with Reinforcement Learning Abstract: A major challenge in recommender systems is handling new users, whom are also\ncalled $\\textit{cold-start}$ users. In this paper, we propose a novel approach\nfor learning an optimal series of questions with which to interview cold-start\nusers for movie recommender systems. We propose learning interview questions\nusing Deep Q Networks to create user profiles to make better recommendations to\ncold-start users. While our proposed system is trained using a movie\nrecommender system, our Deep Q Network model should generalize across various\ntypes of recommender systems. \n\n"}
{"id": "1806.06411", "contents": "Title: Measuring Semantic Coherence of a Conversation Abstract: Conversational systems have become increasingly popular as a way for humans\nto interact with computers. To be able to provide intelligent responses,\nconversational systems must correctly model the structure and semantics of a\nconversation. We introduce the task of measuring semantic (in)coherence in a\nconversation with respect to background knowledge, which relies on the\nidentification of semantic relations between concepts introduced during a\nconversation. We propose and evaluate graph-based and machine learning-based\napproaches for measuring semantic coherence using knowledge graphs, their\nvector space embeddings and word embedding models, as sources of background\nknowledge. We demonstrate how these approaches are able to uncover different\ncoherence patterns in conversations on the Ubuntu Dialogue Corpus. \n\n"}
{"id": "1806.07304", "contents": "Title: Dynamic Multi-Level Multi-Task Learning for Sentence Simplification Abstract: Sentence simplification aims to improve readability and understandability,\nbased on several operations such as splitting, deletion, and paraphrasing.\nHowever, a valid simplified sentence should also be logically entailed by its\ninput sentence. In this work, we first present a strong pointer-copy mechanism\nbased sequence-to-sequence sentence simplification model, and then improve its\nentailment and paraphrasing capabilities via multi-task learning with related\nauxiliary tasks of entailment and paraphrase generation. Moreover, we propose a\nnovel 'multi-level' layered soft sharing approach where each auxiliary task\nshares different (higher versus lower) level layers of the sentence\nsimplification model, depending on the task's semantic versus lexico-syntactic\nnature. We also introduce a novel multi-armed bandit based training approach\nthat dynamically learns how to effectively switch across tasks during\nmulti-task learning. Experiments on multiple popular datasets demonstrate that\nour model outperforms competitive simplification systems in SARI and FKGL\nautomatic metrics, and human evaluation. Further, we present several ablation\nanalyses on alternative layer sharing methods, soft versus hard sharing,\ndynamic multi-armed bandit sampling approaches, and our model's learned\nentailment and paraphrasing skills. \n\n"}
{"id": "1806.07498", "contents": "Title: Defining Locality for Surrogates in Post-hoc Interpretablity Abstract: Local surrogate models, to approximate the local decision boundary of a\nblack-box classifier, constitute one approach to generate explanations for the\nrationale behind an individual prediction made by the back-box. This paper\nhighlights the importance of defining the right locality, the neighborhood on\nwhich a local surrogate is trained, in order to approximate accurately the\nlocal black-box decision boundary. Unfortunately, as shown in this paper, this\nissue is not only a parameter or sampling distribution challenge and has a\nmajor impact on the relevance and quality of the approximation of the local\nblack-box decision boundary and thus on the meaning and accuracy of the\ngenerated explanation. To overcome the identified problems, quantified with an\nadapted measure and procedure, we propose to generate surrogate-based\nexplanations for individual predictions based on a sampling centered on\nparticular place of the decision boundary, relevant for the prediction to be\nexplained, rather than on the prediction itself as it is classically done. We\nevaluate the novel approach compared to state-of-the-art methods and a\nstraightforward improvement thereof on four UCI datasets. \n\n"}
{"id": "1806.08047", "contents": "Title: Flexible Neural Representation for Physics Prediction Abstract: Humans have a remarkable capacity to understand the physical dynamics of\nobjects in their environment, flexibly capturing complex structures and\ninteractions at multiple levels of detail. Inspired by this ability, we propose\na hierarchical particle-based object representation that covers a wide variety\nof types of three-dimensional objects, including both arbitrary rigid\ngeometrical shapes and deformable materials. We then describe the Hierarchical\nRelation Network (HRN), an end-to-end differentiable neural network based on\nhierarchical graph convolution, that learns to predict physical dynamics in\nthis representation. Compared to other neural network baselines, the HRN\naccurately handles complex collisions and nonrigid deformations, generating\nplausible dynamics predictions at long time scales in novel settings, and\nscaling to large scene configurations. These results demonstrate an\narchitecture with the potential to form the basis of next-generation physics\npredictors for use in computer vision, robotics, and quantitative cognitive\nscience. \n\n"}
{"id": "1806.08568", "contents": "Title: Continuous Learning in Single-Incremental-Task Scenarios Abstract: It was recently shown that architectural, regularization and rehearsal\nstrategies can be used to train deep models sequentially on a number of\ndisjoint tasks without forgetting previously acquired knowledge. However, these\nstrategies are still unsatisfactory if the tasks are not disjoint but\nconstitute a single incremental task (e.g., class-incremental learning). In\nthis paper we point out the differences between multi-task and\nsingle-incremental-task scenarios and show that well-known approaches such as\nLWF, EWC and SI are not ideal for incremental task scenarios. A new approach,\ndenoted as AR1, combining architectural and regularization strategies is then\nspecifically proposed. AR1 overhead (in term of memory and computation) is very\nsmall thus making it suitable for online learning. When tested on CORe50 and\niCIFAR-100, AR1 outperformed existing regularization strategies by a good\nmargin. \n\n"}
{"id": "1806.08686", "contents": "Title: A Predictive Model for Music Based on Learned Interval Representations Abstract: Connectionist sequence models (e.g., RNNs) applied to musical sequences\nsuffer from two known problems: First, they have strictly \"absolute pitch\nperception\". Therefore, they fail to generalize over musical concepts which are\ncommonly perceived in terms of relative distances between pitches (e.g.,\nmelodies, scale types, modes, cadences, or chord types). Second, they fall\nshort of capturing the concepts of repetition and musical form. In this paper\nwe introduce the recurrent gated autoencoder (RGAE), a recurrent neural network\nwhich learns and operates on interval representations of musical sequences. The\nrelative pitch modeling increases generalization and reduces sparsity in the\ninput data. Furthermore, it can learn sequences of copy-and-shift operations\n(i.e. chromatically transposed copies of musical fragments)---a promising\ncapability for learning musical repetition structure. We show that the RGAE\nimproves the state of the art for general connectionist sequence models in\nlearning to predict monophonic melodies, and that ensembles of relative and\nabsolute music processing models improve the results appreciably. Furthermore,\nwe show that the relative pitch processing of the RGAE naturally facilitates\nthe learning and the generation of sequences of copy-and-shift operations,\nwherefore the RGAE greatly outperforms a common absolute pitch recurrent neural\nnetwork on this task. \n\n"}
{"id": "1806.09614", "contents": "Title: Accuracy-based Curriculum Learning in Deep Reinforcement Learning Abstract: In this paper, we investigate a new form of automated curriculum learning\nbased on adaptive selection of accuracy requirements, called accuracy-based\ncurriculum learning. Using a reinforcement learning agent based on the Deep\nDeterministic Policy Gradient algorithm and addressing the Reacher environment,\nwe first show that an agent trained with various accuracy requirements sampled\nrandomly learns more efficiently than when asked to be very accurate at all\ntimes. Then we show that adaptive selection of accuracy requirements, based on\na local measure of competence progress, automatically generates a curriculum\nwhere difficulty progressively increases, resulting in a better learning\nefficiency than sampling randomly. \n\n"}
{"id": "1806.11558", "contents": "Title: A scalable H-matrix approach for the solution of boundary integral\n  equations on multi-GPU clusters Abstract: In this work, we consider the solution of boundary integral equations by\nmeans of a scalable hierarchical matrix approach on clusters equipped with\ngraphics hardware, i.e. graphics processing units (GPUs). To this end, we\nextend our existing single-GPU hierarchical matrix library hmglib such that it\nis able to scale on many GPUs and such that it can be coupled to arbitrary\napplication codes. Using a model GPU implementation of a boundary element\nmethod (BEM) solver, we are able to achieve more than 67 percent relative\nparallel speed-up going from 128 to 1024 GPUs for a model geometry test case\nwith 1.5 million unknowns and a real-world geometry test case with almost 1.2\nmillion unknowns. On 1024 GPUs of the cluster Titan, it takes less than 6\nminutes to solve the 1.5 million unknowns problem, with 5.7 minutes for the\nsetup phase and 20 seconds for the iterative solver. To the best of the\nauthors' knowledge, we here discuss the first fully GPU-based\ndistributed-memory parallel hierarchical matrix Open Source library using the\ntraditional H-matrix format and adaptive cross approximation with an\napplication to BEM problems. \n\n"}
{"id": "1807.01001", "contents": "Title: Modular Vehicle Control for Transferring Semantic Information Between\n  Weather Conditions Using GANs Abstract: Even though end-to-end supervised learning has shown promising results for\nsensorimotor control of self-driving cars, its performance is greatly affected\nby the weather conditions under which it was trained, showing poor\ngeneralization to unseen conditions. In this paper, we show how knowledge can\nbe transferred using semantic maps to new weather conditions without the need\nto obtain new ground truth data. To this end, we propose to divide the task of\nvehicle control into two independent modules: a control module which is only\ntrained on one weather condition for which labeled steering data is available,\nand a perception module which is used as an interface between new weather\nconditions and the fixed control module. To generate the semantic data needed\nto train the perception module, we propose to use a generative adversarial\nnetwork (GAN)-based model to retrieve the semantic information for the new\nconditions in an unsupervised manner. We introduce a master-servant\narchitecture, where the master model (semantic labels available) trains the\nservant model (semantic labels not available). We show that our proposed method\ntrained with ground truth data for a single weather condition is capable of\nachieving similar results on the task of steering angle prediction as an\nend-to-end model trained with ground truth data of 15 different weather\nconditions. \n\n"}
{"id": "1807.01147", "contents": "Title: FastTrack: Minimizing Stalls for CDN-based Over-the-top Video Streaming\n  Systems Abstract: Traffic for internet video streaming has been rapidly increasing and is\nfurther expected to increase with the higher definition videos and IoT\napplications, such as 360 degree videos and augmented virtual reality\napplications. While efficient management of heterogeneous cloud resources to\noptimize the quality of experience is important, existing work in this problem\nspace often left out important factors. In this paper, we present a model for\ndescribing a today's representative system architecture for video streaming\napplications, typically composed of a centralized origin server and several CDN\nsites. Our model comprehensively considers the following factors: limited\ncaching spaces at the CDN sites, allocation of CDN for a video request, choice\nof different ports from the CDN, and the central storage and bandwidth\nallocation. With the model, we focus on minimizing a performance metric, stall\nduration tail probability (SDTP), and present a novel, yet efficient, algorithm\nto solve the formulated optimization problem. The theoretical bounds with\nrespect to the SDTP metric are also analyzed and presented. Our extensive\nsimulation results demonstrate that the proposed algorithms can significantly\nimprove the SDTP metric, compared to the baseline strategies. Small-scale video\nstreaming system implementation in a real cloud environment further validates\nour results. \n\n"}
{"id": "1807.01183", "contents": "Title: Quantified Markov Logic Networks Abstract: Markov Logic Networks (MLNs) are well-suited for expressing statistics such\nas \"with high probability a smoker knows another smoker\" but not for expressing\nstatements such as \"there is a smoker who knows most other smokers\", which is\nnecessary for modeling, e.g. influencers in social networks. To overcome this\nshortcoming, we study quantified MLNs which generalize MLNs by introducing\nstatistical universal quantifiers, allowing to express also the latter type of\nstatistics in a principled way. Our main technical contribution is to show that\nthe standard reasoning tasks in quantified MLNs, maximum a posteriori and\nmarginal inference, can be reduced to their respective MLN counterparts in\npolynomial time. \n\n"}
{"id": "1807.01494", "contents": "Title: A complete system of deduction for Sigma formulas Abstract: The Sigma formulas of the language of arithmetic express semidecidable\nrelations on the natural numbers. More generally, whenever a totality of\nobjects is regarded as incomplete, the Sigma formulas express relations that\nare witnessed in a completed portion of that totality when they hold. In this\nsense, the Sigma formulas are more concrete semantically than other first-order\nformulas.\n  We describe a system of deduction that uses only Sigma formulas. Each axiom,\nan implication between two Sigma formulas, is implemented as a rewriting rule\nfor subformulas. We exhibit a complete class of logical axioms for this system,\nand we observe that a distributive law distinguishes classical reasoning from\nintuitionistic reasoning in this setting.\n  Skolem's theory PRA of primitive recursive arithmetic can be formulated in\nour deductive system. In Skolem's system, free variables are universally\nquantified implicitly, but in our formulation, free variables act as parameters\nto the deduction. In this sense, our formulation is more explicitly finitistic.\nFurthermore, most of our results are themselves finistic, being theorems of\nPRA. In particular, appending our main theorem to a celebrated chain of\nreductions from reverse mathematics, we find that an implication of Sigma\nformulas is derivable in WKL_0 if and only if there is a deduction from the\nantecedent to the consequent in our formulation of PRA. \n\n"}
{"id": "1807.01521", "contents": "Title: Curiosity Driven Exploration of Learned Disentangled Goal Spaces Abstract: Intrinsically motivated goal exploration processes enable agents to\nautonomously sample goals to explore efficiently complex environments with\nhigh-dimensional continuous actions. They have been applied successfully to\nreal world robots to discover repertoires of policies producing a wide\ndiversity of effects. Often these algorithms relied on engineered goal spaces\nbut it was recently shown that one can use deep representation learning\nalgorithms to learn an adequate goal space in simple environments. However, in\nthe case of more complex environments containing multiple objects or\ndistractors, an efficient exploration requires that the structure of the goal\nspace reflects the one of the environment. In this paper we show that using a\ndisentangled goal space leads to better exploration performances than an\nentangled goal space. We further show that when the representation is\ndisentangled, one can leverage it by sampling goals that maximize learning\nprogress in a modular manner. Finally, we show that the measure of learning\nprogress, used to drive curiosity-driven exploration, can be used\nsimultaneously to discover abstract independently controllable features of the\nenvironment. \n\n"}
{"id": "1807.01566", "contents": "Title: Analyzing Big Datasets of Genomic Sequences: Fast and Scalable\n  Collection of k-mer Statistics Abstract: Distributed approaches based on the map-reduce programming paradigm have\nstarted to be proposed in the bioinformatics domain, due to the large amount of\ndata produced by the next-generation sequencing techniques. However, the use of\nmap-reduce and related Big Data technologies and frameworks (e.g., Apache\nHadoop and Spark) does not necessarily produce satisfactory results, in terms\nof both efficiency and effectiveness. We discuss how the development of\ndistributed and Big Data management technologies has affected the analysis of\nlarge datasets of biological sequences. Moreover, we show how the choice of\ndifferent parameter configurations and the careful engineering of the software\nwith respect to the specific framework under consideration may be crucial in\norder to achieve good performance, especially on very large amounts of data. We\nchoose k-mers counting as a case study for our analysis, and Spark as the\nframework to implement FastKmer, a novel approach for the extraction of k-mer\nstatistics from large collection of biological sequences, with arbitrary values\nof k. One of the most relevant contributions of FastKmer is the introduction of\na module for balancing the statistics aggregation workload over the nodes of a\ncomputing cluster, in order to overcome data skew while allowing for a fully\nexploitation of the underly- ing distributed architecture. We also present the\nresults of a comparative experimental analysis showing that our approach is\ncurrently the fastest among the ones based on Big Data technologies, while\nexhibiting a very good scalability. We provide evidence that the usage of\ntechnologies such as Hadoop or Spark for the analysis of big datasets of\nbiological sequences is productive only if the architectural details and the\npeculiar aspects of the considered framework are carefully taken into account\nfor the algorithm design and implementation. \n\n"}
{"id": "1807.01798", "contents": "Title: Regularizing Autoencoder-Based Matrix Completion Models via Manifold\n  Learning Abstract: Autoencoders are popular among neural-network-based matrix completion models\ndue to their ability to retrieve potential latent factors from the partially\nobserved matrices. Nevertheless, when training data is scarce their performance\nis significantly degraded due to overfitting. In this paper, we mit- igate\noverfitting with a data-dependent regularization technique that relies on the\nprinciples of multi-task learning. Specifically, we propose an\nautoencoder-based matrix completion model that performs prediction of the\nunknown matrix values as a main task, and manifold learning as an auxiliary\ntask. The latter acts as an inductive bias, leading to solutions that\ngeneralize better. The proposed model outperforms the existing\nautoencoder-based models designed for matrix completion, achieving high\nreconstruction accuracy in well-known datasets. \n\n"}
{"id": "1807.02189", "contents": "Title: Functional Object-Oriented Network: Construction & Expansion Abstract: We build upon the functional object-oriented network (FOON), a structured\nknowledge representation which is constructed from observations of human\nactivities and manipulations. A FOON can be used for representing object-motion\naffordances. Knowledge retrieval through graph search allows us to obtain novel\nmanipulation sequences using knowledge spanning across many video sources,\nhence the novelty in our approach. However, we are limited to the sources\ncollected. To further improve the performance of knowledge retrieval as a\nfollow up to our previous work, we discuss generalizing knowledge to be applied\nto objects which are similar to what we have in FOON without manually\nannotating new sources of knowledge. We discuss two means of generalization: 1)\nexpanding our network through the use of object similarity to create new\nfunctional units from those we already have, and 2) compressing the functional\nunits by object categories rather than specific objects. We discuss experiments\nwhich compare the performance of our knowledge retrieval algorithm with both\nexpansion and compression by categories. \n\n"}
{"id": "1807.02846", "contents": "Title: Mind My Value: a decentralized infrastructure for fair and trusted IoT\n  data trading Abstract: Internet of Things (IoT) data are increasingly viewed as a new form of\nmassively distributed and large scale digital assets, which are continuously\ngenerated by millions of connected devices. The real value of such assets can\nonly be realized by allowing IoT data trading to occur on a marketplace that\nrewards every single producer and consumer, at a very granular level.\nCrucially, we believe that such a marketplace should not be owned by anybody,\nand should instead fairly and transparently self-enforce a well defined set of\ngovernance rules. In this paper we address some of the technical challenges\ninvolved in realizing such a marketplace. We leverage emerging blockchain\ntechnologies to build a decentralized, trusted, transparent and open\narchitecture for IoT traffic metering and contract compliance, on top of the\nlargely adopted IoT brokered data infrastructure. We discuss an Ethereum-based\nprototype implementation and experimentally evaluate the overhead cost\nassociated with Smart Contract transactions, concluding that a viable business\nmodel can indeed be associated with our technical approach. \n\n"}
{"id": "1807.03341", "contents": "Title: Troubling Trends in Machine Learning Scholarship Abstract: Collectively, machine learning (ML) researchers are engaged in the creation\nand dissemination of knowledge about data-driven algorithms. In a given paper,\nresearchers might aspire to any subset of the following goals, among others: to\ntheoretically characterize what is learnable, to obtain understanding through\nempirically rigorous experiments, or to build a working system that has high\npredictive accuracy. While determining which knowledge warrants inquiry may be\nsubjective, once the topic is fixed, papers are most valuable to the community\nwhen they act in service of the reader, creating foundational knowledge and\ncommunicating as clearly as possible.\n  Recent progress in machine learning comes despite frequent departures from\nthese ideals. In this paper, we focus on the following four patterns that\nappear to us to be trending in ML scholarship: (i) failure to distinguish\nbetween explanation and speculation; (ii) failure to identify the sources of\nempirical gains, e.g., emphasizing unnecessary modifications to neural\narchitectures when gains actually stem from hyper-parameter tuning; (iii)\nmathiness: the use of mathematics that obfuscates or impresses rather than\nclarifies, e.g., by confusing technical and non-technical concepts; and (iv)\nmisuse of language, e.g., by choosing terms of art with colloquial connotations\nor by overloading established technical terms.\n  While the causes behind these patterns are uncertain, possibilities include\nthe rapid expansion of the community, the consequent thinness of the reviewer\npool, and the often-misaligned incentives between scholarship and short-term\nmeasures of success (e.g., bibliometrics, attention, and entrepreneurial\nopportunity). While each pattern offers a corresponding remedy (don't do it),\nwe also discuss some speculative suggestions for how the community might combat\nthese trends. \n\n"}
{"id": "1807.04905", "contents": "Title: Ultra-Fine Entity Typing Abstract: We introduce a new entity typing task: given a sentence with an entity\nmention, the goal is to predict a set of free-form phrases (e.g. skyscraper,\nsongwriter, or criminal) that describe appropriate types for the target entity.\nThis formulation allows us to use a new type of distant supervision at large\nscale: head words, which indicate the type of the noun phrases they appear in.\nWe show that these ultra-fine types can be crowd-sourced, and introduce new\nevaluation sets that are much more diverse and fine-grained than existing\nbenchmarks. We present a model that can predict open types, and is trained\nusing a multitask objective that pools our new head-word supervision with prior\nsupervision from entity linking. Experimental results demonstrate that our\nmodel is effective in predicting entity types at varying granularity; it\nachieves state of the art performance on an existing fine-grained entity typing\nbenchmark, and sets baselines for our newly-introduced datasets. Our data and\nmodel can be downloaded from: http://nlp.cs.washington.edu/entity_type \n\n"}
{"id": "1807.06179", "contents": "Title: Real-Time Index Authentication for Event-Oriented Surveillance Video\n  Query using Blockchain Abstract: Information from surveillance video is essential for situational awareness\n(SAW). Nowadays, a prohibitively large amount of surveillance data is being\ngenerated continuously by ubiquitously distributed video sensors. It is very\nchallenging to immediately identify the objects of interest or zoom in\nsuspicious actions from thousands of video frames. Making the big data\nindexable is critical to tackle this problem. It is ideal to generate pattern\nindexes in a real-time, on-site manner on the video streaming instead of\ndepending on the batch processing at the cloud centers. The modern\nedge-fog-cloud computing paradigm allows implementation of time sensitive tasks\nat the edge of the network. The on-site edge devices collect the information\nsensed in format of frames and extracts useful features. The near-site fog\nnodes conduct the contextualization and classification of the features. The\nremote cloud center is in charge of more data intensive and computing intensive\ntasks. However, exchanging the index information among devices in different\nlayers raises security concerns where an adversary can capture or tamper with\nfeatures to mislead the surveillance system. In this paper, a blockchain\nenabled scheme is proposed to protect the index data through an encrypted\nsecure channel between the edge and fog nodes. It reduces the chance of attacks\non the small edge and fog devices. The feasibility of the proposal is validated\nthrough intensive experimental analysis. \n\n"}
{"id": "1807.06286", "contents": "Title: Preference-Based Monte Carlo Tree Search Abstract: Monte Carlo tree search (MCTS) is a popular choice for solving sequential\nanytime problems. However, it depends on a numeric feedback signal, which can\nbe difficult to define. Real-time MCTS is a variant which may only rarely\nencounter states with an explicit, extrinsic reward. To deal with such cases,\nthe experimenter has to supply an additional numeric feedback signal in the\nform of a heuristic, which intrinsically guides the agent. Recent work has\nshown evidence that in different areas the underlying structure is ordinal and\nnot numerical. Hence erroneous and biased heuristics are inevitable, especially\nin such domains. In this paper, we propose a MCTS variant which only depends on\nqualitative feedback, and therefore opens up new applications for MCTS. We also\nfind indications that translating absolute into ordinal feedback may be\nbeneficial. Using a puzzle domain, we show that our preference-based MCTS\nvariant, wich only receives qualitative feedback, is able to reach a\nperformance level comparable to a regular MCTS baseline, which obtains\nquantitative feedback. \n\n"}
{"id": "1807.07490", "contents": "Title: FuzzerGym: A Competitive Framework for Fuzzing and Learning Abstract: Fuzzing is a commonly used technique designed to test software by\nautomatically crafting program inputs. Currently, the most successful fuzzing\nalgorithms emphasize simple, low-overhead strategies with the ability to\nefficiently monitor program state during execution. Through compile-time\ninstrumentation, these approaches have access to numerous aspects of program\nstate including coverage, data flow, and heterogeneous fault detection and\nclassification. However, existing approaches utilize blind random mutation\nstrategies when generating test inputs. We present a different approach that\nuses this state information to optimize mutation operators using reinforcement\nlearning (RL). By integrating OpenAI Gym with libFuzzer we are able to\nsimultaneously leverage advancements in reinforcement learning as well as\nfuzzing to achieve deeper coverage across several varied benchmarks. Our\ntechnique connects the rich, efficient program monitors provided by LLVM\nSantizers with a deep neural net to learn mutation selection strategies\ndirectly from the input data. The cross-language, asynchronous architecture we\ndeveloped enables us to apply any OpenAI Gym compatible deep reinforcement\nlearning algorithm to any fuzzing problem with minimal slowdown. \n\n"}
{"id": "1807.07801", "contents": "Title: Finding Structure in Dynamic Networks Abstract: This document is the first part of the author's habilitation thesis (HDR),\ndefended on June 4, 2018 at the University of Bordeaux. Given the nature of\nthis document, the contributions that involve the author have been emphasized;\nhowever, these four chapters were specifically written for distribution to a\nlarger audience. We hope they can serve as a broad introduction to the domain\nof highly dynamic networks, with a focus on temporal graph concepts and their\ninteraction with distributed computing. \n\n"}
{"id": "1807.08237", "contents": "Title: Learning Deep Hidden Nonlinear Dynamics from Aggregate Data Abstract: Learning nonlinear dynamics from diffusion data is a challenging problem\nsince the individuals observed may be different at different time points,\ngenerally following an aggregate behaviour. Existing work cannot handle the\ntasks well since they model such dynamics either directly on observations or\nenforce the availability of complete longitudinal individual-level\ntrajectories. However, in most of the practical applications, these\nrequirements are unrealistic: the evolving dynamics may be too complex to be\nmodeled directly on observations, and individual-level trajectories may not be\navailable due to technical limitations, experimental costs and/or privacy\nissues. To address these challenges, we formulate a model of diffusion dynamics\nas the {\\em hidden stochastic process} via the introduction of hidden variables\nfor flexibility, and learn the hidden dynamics directly on {\\em aggregate\nobservations} without any requirement for individual-level trajectories. We\npropose a dynamic generative model with Wasserstein distance for LEarninG dEep\nhidden Nonlinear Dynamics (LEGEND) and prove its theoretical guarantees as\nwell. Experiments on a range of synthetic and real-world datasets illustrate\nthat LEGEND has very strong performance compared to state-of-the-art baselines. \n\n"}
{"id": "1807.08364", "contents": "Title: EnsembleDAgger: A Bayesian Approach to Safe Imitation Learning Abstract: While imitation learning is often used in robotics, the approach frequently\nsuffers from data mismatch and compounding errors. DAgger is an iterative\nalgorithm that addresses these issues by aggregating training data from both\nthe expert and novice policies, but does not consider the impact of safety. We\npresent a probabilistic extension to DAgger, which attempts to quantify the\nconfidence of the novice policy as a proxy for safety. Our method,\nEnsembleDAgger, approximates a Gaussian Process using an ensemble of neural\nnetworks. Using the variance as a measure of confidence, we compute a decision\nrule that captures how much we doubt the novice, thus determining when it is\nsafe to allow the novice to act. With this approach, we aim to maximize the\nnovice's share of actions, while constraining the probability of failure. We\ndemonstrate improved safety and learning performance compared to other DAgger\nvariants and classic imitation learning on an inverted pendulum and in the\nMuJoCo HalfCheetah environment. \n\n"}
{"id": "1807.08430", "contents": "Title: Actor-Action Semantic Segmentation with Region Masks Abstract: In this paper, we study the actor-action semantic segmentation problem, which\nrequires joint labeling of both actor and action categories in video frames.\nOne major challenge for this task is that when an actor performs an action,\ndifferent body parts of the actor provide different types of cues for the\naction category and may receive inconsistent action labeling when they are\nlabeled independently. To address this issue, we propose an end-to-end\nregion-based actor-action segmentation approach which relies on region masks\nfrom an instance segmentation algorithm. Our main novelty is to avoid labeling\npixels in a region mask independently - instead we assign a single action label\nto these pixels to achieve consistent action labeling. When a pixel belongs to\nmultiple region masks, max pooling is applied to resolve labeling conflicts.\nOur approach uses a two-stream network as the front-end (which learns features\ncapturing both appearance and motion information), and uses two region-based\nsegmentation networks as the back-end (which takes the fused features from the\ntwo-stream network as the input and predicts actor-action labeling).\nExperiments on the A2D dataset demonstrate that both the region-based\nsegmentation strategy and the fused features from the two-stream network\ncontribute to the performance improvements. The proposed approach outperforms\nthe state-of-the-art results by more than 8% in mean class accuracy, and more\nthan 5% in mean class IOU, which validates its effectiveness. \n\n"}
{"id": "1807.08894", "contents": "Title: ClusterNet: 3D Instance Segmentation in RGB-D Images Abstract: We propose a method for instance-level segmentation that uses RGB-D data as\ninput and provides detailed information about the location, geometry and number\nof individual objects in the scene. This level of understanding is fundamental\nfor autonomous robots. It enables safe and robust decision-making under the\nlarge uncertainty of the real-world. In our model, we propose to use the first\nand second order moments of the object occupancy function to represent an\nobject instance. We train an hourglass Deep Neural Network (DNN) where each\npixel in the output votes for the 3D position of the corresponding object\ncenter and for the object's size and pose. The final instance segmentation is\nachieved through clustering in the space of moments. The object-centric\ntraining loss is defined on the output of the clustering. Our method\noutperforms the state-of-the-art instance segmentation method on our\nsynthesized dataset. We show that our method generalizes well on real-world\ndata achieving visually better segmentation results. \n\n"}
{"id": "1807.08919", "contents": "Title: The Variational Homoencoder: Learning to learn high capacity generative\n  models from few examples Abstract: Hierarchical Bayesian methods can unify many related tasks (e.g. k-shot\nclassification, conditional and unconditional generation) as inference within a\nsingle generative model. However, when this generative model is expressed as a\npowerful neural network such as a PixelCNN, we show that existing learning\ntechniques typically fail to effectively use latent variables. To address this,\nwe develop a modification of the Variational Autoencoder in which encoded\nobservations are decoded to new elements from the same class. This technique,\nwhich we call a Variational Homoencoder (VHE), produces a hierarchical latent\nvariable model which better utilises latent variables. We use the VHE framework\nto learn a hierarchical PixelCNN on the Omniglot dataset, which outperforms all\nexisting models on test set likelihood and achieves strong performance on\none-shot generation and classification tasks. We additionally validate the VHE\non natural images from the YouTube Faces database. Finally, we develop\nextensions of the model that apply to richer dataset structures such as\nfactorial and hierarchical categories. \n\n"}
{"id": "1807.09466", "contents": "Title: Model Checking Quantum Systems --- A Survey Abstract: This article discusses the essential difficulties in developing\nmodel-checking techniques for quantum systems that are never present in model\nchecking classical systems. It further reviews some early researches on\nchecking quantum communication protocols as well as a new line of researches\npursued by the authors and their collaborators on checking general quantum\nsystems, applicable to both physical systems and quantum programs. \n\n"}
{"id": "1807.09632", "contents": "Title: PaPaS: A Portable, Lightweight, and Generic Framework for Parallel\n  Parameter Studies Abstract: The current landscape of scientific research is widely based on modeling and\nsimulation, typically with complexity in the simulation's flow of execution and\nparameterization properties. Execution flows are not necessarily\nstraightforward since they may need multiple processing tasks and iterations.\nFurthermore, parameter and performance studies are common approaches used to\ncharacterize a simulation, often requiring traversal of a large parameter\nspace. High-performance computers offer practical resources at the expense of\nusers handling the setup, submission, and management of jobs. This work\npresents the design of PaPaS, a portable, lightweight, and generic workflow\nframework for conducting parallel parameter and performance studies. Workflows\nare defined using parameter files based on keyword-value pairs syntax, thus\nremoving from the user the overhead of creating complex scripts to manage the\nworkflow. A parameter set consists of any combination of environment variables,\nfiles, partial file contents, and command line arguments. PaPaS is being\ndeveloped in Python 3 with support for distributed parallelization using SSH,\nbatch systems, and C++ MPI. The PaPaS framework will run as user processes, and\ncan be used in single/multi-node and multi-tenant computing systems. An example\nsimulation using the BehaviorSpace tool from NetLogo and a matrix multiply\nusing OpenMP are presented as parameter and performance studies, respectively.\nThe results demonstrate that the PaPaS framework offers a simple method for\ndefining and managing parameter studies, while increasing resource utilization. \n\n"}
{"id": "1807.09844", "contents": "Title: Modular Mechanistic Networks: On Bridging Mechanistic and\n  Phenomenological Models with Deep Neural Networks in Natural Language\n  Processing Abstract: Natural language processing (NLP) can be done using either top-down (theory\ndriven) and bottom-up (data driven) approaches, which we call mechanistic and\nphenomenological respectively. The approaches are frequently considered to\nstand in opposition to each other. Examining some recent approaches in deep\nlearning we argue that deep neural networks incorporate both perspectives and,\nfurthermore, that leveraging this aspect of deep learning may help in solving\ncomplex problems within language technology, such as modelling language and\nperception in the domain of spatial cognition. \n\n"}
{"id": "1807.10399", "contents": "Title: Applications of Common Entropy for Causal Inference Abstract: We study the problem of discovering the simplest latent variable that can\nmake two observed discrete variables conditionally independent. The minimum\nentropy required for such a latent is known as common entropy in information\ntheory. We extend this notion to Renyi common entropy by minimizing the Renyi\nentropy of the latent variable. To efficiently compute common entropy, we\npropose an iterative algorithm that can be used to discover the trade-off\nbetween the entropy of the latent variable and the conditional mutual\ninformation of the observed variables. We show two applications of common\nentropy in causal inference: First, under the assumption that there are no\nlow-entropy mediators, it can be used to distinguish causation from spurious\ncorrelation among almost all joint distributions on simple causal graphs with\ntwo observed variables. Second, common entropy can be used to improve\nconstraint-based methods such as PC or FCI algorithms in the small-sample\nregime, where these methods are known to struggle. We propose a modification to\nthese constraint-based methods to assess if a separating set found by these\nalgorithms is valid using common entropy. We finally evaluate our algorithms on\nsynthetic and real data to establish their performance. \n\n"}
{"id": "1807.10454", "contents": "Title: Rob-GAN: Generator, Discriminator, and Adversarial Attacker Abstract: We study two important concepts in adversarial deep learning---adversarial\ntraining and generative adversarial network (GAN). Adversarial training is the\ntechnique used to improve the robustness of discriminator by combining\nadversarial attacker and discriminator in the training phase. GAN is commonly\nused for image generation by jointly optimizing discriminator and generator. We\nshow these two concepts are indeed closely related and can be used to\nstrengthen each other---adding a generator to the adversarial training\nprocedure can improve the robustness of discriminators, and adding an\nadversarial attack to GAN training can improve the convergence speed and lead\nto better generators. Combining these two insights, we develop a framework\ncalled Rob-GAN to jointly optimize generator and discriminator in the presence\nof adversarial attacks---the generator generates fake images to fool\ndiscriminator; the adversarial attacker perturbs real images to fool the\ndiscriminator, and the discriminator wants to minimize loss under fake and\nadversarial images. Through this end-to-end training procedure, we are able to\nsimultaneously improve the convergence speed of GAN training, the quality of\nsynthetic images, and the robustness of discriminator under strong adversarial\nattacks. Experimental results demonstrate that the obtained classifier is more\nrobust than the state-of-the-art adversarial training approach, and the\ngenerator outperforms SN-GAN on ImageNet-143. \n\n"}
{"id": "1807.11112", "contents": "Title: Is One Hyperparameter Optimizer Enough? Abstract: Hyperparameter tuning is the black art of automatically finding a good\ncombination of control parameters for a data miner. While widely applied in\nempirical Software Engineering, there has not been much discussion on which\nhyperparameter tuner is best for software analytics. To address this gap in the\nliterature, this paper applied a range of hyperparameter optimizers (grid\nsearch, random search, differential evolution, and Bayesian optimization) to\ndefect prediction problem. Surprisingly, no hyperparameter optimizer was\nobserved to be `best' and, for one of the two evaluation measures studied here\n(F-measure), hyperparameter optimization, in 50\\% cases, was no better than\nusing default configurations.\n  We conclude that hyperparameter optimization is more nuanced than previously\nbelieved. While such optimization can certainly lead to large improvements in\nthe performance of classifiers used in software analytics, it remains to be\nseen which specific optimizers should be applied to a new dataset. \n\n"}
{"id": "1807.11399", "contents": "Title: Who needs category theory? Abstract: In mathematical applications, category theory remains a contentious issue,\nwith enthusiastic fans and a skeptical majority. In a muted form this split\napplies to the authors of this note. When we learned that the only\nmathematically sound foundation of topological quantum computing in the\nliterature is based on category theory, the skeptical author suggested to\n\"decategorize\" the foundation. But we discovered, to our surprise, that\ncategory theory (or something like it) is necessary for the purpose, for\ncomputational reasons. The goal of this note is to give a high-level\nexplanation of that necessity, which avoids details and which suggests that the\ncase of topological quantum computing is far from unique. \n\n"}
{"id": "1808.00829", "contents": "Title: A Systematic Comparison of Dynamic Load Balancing Algorithms for\n  Massively Parallel Rigid Particle Dynamics Abstract: As compute power increases with time, more involved and larger simulations\nbecome possible. However, it gets increasingly difficult to efficiently use the\nprovided computational resources. Especially in particle-based simulations with\na spatial domain partitioning large load imbalances can occur due to the\nsimulation being dynamic. Then a static domain partitioning may not be\nsuitable. This can deteriorate the overall runtime of the simulation\nsignificantly. Sophisticated load balancing strategies must be designed to\nalleviate this problem. In this paper we conduct a systematic evaluation of the\nperformance of six different load balancing algorithms. Our tests cover a wide\nrange of simulation sizes, and employ one of the largest supercomputers\navailable. In particular we study the runtime and memory complexity of all\ncomponents of the simulation carefully. When progressing to extreme scale\nsimulations it is essential to identify bottlenecks and to predict the scaling\nbehaviour. Scaling experiments are shown for up to over one million processes.\nThe performance of each algorithm is analyzed with respect to the quality of\nthe load balancing and its runtime costs. For all tests, the waLBerla\nmultiphysics framework is employed. \n\n"}
{"id": "1808.01358", "contents": "Title: Attributes' Importance for Zero-Shot Pose-Classification Based on\n  Wearable Sensors Abstract: This paper presents a simple yet effective method for improving the\nperformance of zero-shot learning (ZSL). ZSL classifies instances of unseen\nclasses, from which no training data is available, by utilizing the attributes\nof the classes. Conventional ZSL methods have equally dealt with all the\navailable attributes, but this sometimes causes misclassification. This is\nbecause an attribute that is effective for classifying instances of one class\nis not always effective for another class. In this case, a metric of\nclassifying the latter class can be undesirably influenced by the irrelevant\nattribute. This paper solves this problem by taking the importance of each\nattribute for each class into account when calculating the metric. In addition\nto the proposal of this new method, this paper also contributes by providing a\ndataset for pose classification based on wearable sensors, named HDPoseDS. It\ncontains 22 classes of poses performed by 10 subjects with 31 IMU sensors\nacross full body. To the best of our knowledge, it is the richest\nwearable-sensor dataset especially in terms of sensor density, and thus it is\nsuitable for studying zero-shot pose/action recognition. The presented method\nwas evaluated on HDPoseDS and outperformed relative improvement of 5.9% in\ncomparison to the best baseline method. \n\n"}
{"id": "1808.01561", "contents": "Title: Rapido: A Layer2 Payment System for Decentralized Currencies Abstract: Bitcoin blockchain faces the bitcoin scalability problem, for which bitcoin's\nblocks contain the transactions on the bitcoin network. The on-chain\ntransaction processing capacity of the bitcoin network is limited by the\naverage block creation time of 10 minutes and the block size limit. These\njointly constrain the network's throughput. The transaction processing capacity\nmaximum is estimated between 3.3 and 7 transactions per second (TPS). A Layer2\nNetwork, named Lightning Network, is proposed and activated solutions to\naddress this problem. LN operates on top of the bitcoin network as a cache to\nallow payments to be affected that are not immediately put on the blockchain.\nHowever, it also brings some drawbacks. In this paper, we observe a specific\npayment issue among current LN, which requires additional claims to blockchain\nand is time-consuming. We call the issue as shares issue. Therefore, we propose\nRapido to explicitly address the shares issue. Furthermore, a new smart\ncontract, D-HTLC, is equipped with Rapido as the payment protocol. We finally\nprovide a proof of concept implementation and simulation for both Rapido and\nLN, in which Rapdio not only mitigates the shares issue but also mitigates the\nskewness issue thus is proved to be more applicable for various transactions\nthan LN. \n\n"}
{"id": "1808.03380", "contents": "Title: A survey of data transfer and storage techniques in prevalent\n  cryptocurrencies and suggested improvements Abstract: This thesis focuses on aspects related to the functioning of the gossip\nnetworks underlying three relatively popular cryptocurrencies: Ethereum, Nano\nand IOTA.\n  We look at topics such as automatic discovery of peers when a new node joins\nthe network, bandwidth usage of a node, message passing protocols and storage\nschemas and optimizations for the shared ledger. We believe this is a topic\nthat is often overlooked in works about blockchains and cryptocurrencies.\nVulnerabilities and inefficiencies attain a higher significance than ones in a\nregular open source project because of the rather direct financial implications\nof these projects. Barring Bitcoin, a network that has been around for nearly\n10 years, no other project has substantial documentation for its operational\ndetails other than scattered and sparse pages in the source code repositories.\nAlmost all of the content described here has been extracted by studying the\nsource code of the reference implementations of these projects.\n  We evaluate the use of Invertible Bloom Lookup Tables and the Graphene\nprotocol to decrease block propagation times and bandwidth usage of certain\nmessages. We perform realistic simulations that show significant improvements.\nWe provide a complete implementation of Graphene in Geth, Ethereum's main node\nsoftware and test this implementation against the main Ethereum blockchain.\n  We also crawled the chosen cryptocurrency networks for publicly visible nodes\nand provide an Autonomous System-level breakdown of these nodes with the end\ngoal of estimating the ease of performing attacks such as BGP hijacks and their\nimpact.\n  Code written for implementing Graphene in Geth, performing various\nsimulations and for other miscellaneous tasks has been uploaded to Github at\nhttps://github.com/sunfinite/masters-thesis. \n\n"}
{"id": "1808.03737", "contents": "Title: Learning Multi-touch Conversion Attribution with Dual-attention\n  Mechanisms for Online Advertising Abstract: In online advertising, the Internet users may be exposed to a sequence of\ndifferent ad campaigns, i.e., display ads, search, or referrals from multiple\nchannels, before led up to any final sales conversion and transaction. For both\ncampaigners and publishers, it is fundamentally critical to estimate the\ncontribution from ad campaign touch-points during the customer journey\n(conversion funnel) and assign the right credit to the right ad exposure\naccordingly. However, the existing research on the multi-touch attribution\nproblem lacks a principled way of utilizing the users' pre-conversion actions\n(i.e., clicks), and quite often fails to model the sequential patterns among\nthe touch points from a user's behavior data. To make it worse, the current\nindustry practice is merely employing a set of arbitrary rules as the\nattribution model, e.g., the popular last-touch model assigns 100% credit to\nthe final touch-point regardless of actual attributions. In this paper, we\npropose a Dual-attention Recurrent Neural Network (DARNN) for the multi-touch\nattribution problem. It learns the attribution values through an attention\nmechanism directly from the conversion estimation objective. To achieve this,\nwe utilize sequence-to-sequence prediction for user clicks, and combine both\npost-view and post-click attribution patterns together for the final conversion\nestimation. To quantitatively benchmark attribution models, we also propose a\nnovel yet practical attribution evaluation scheme through the proxy of budget\nallocation (under the estimated attributions) over ad channels. The\nexperimental results on two real datasets demonstrate the significant\nperformance gains of our attribution model against the state of the art. \n\n"}
{"id": "1808.04487", "contents": "Title: CLAIRE: A distributed-memory solver for constrained large deformation\n  diffeomorphic image registration Abstract: With this work, we release CLAIRE, a distributed-memory implementation of an\neffective solver for constrained large deformation diffeomorphic image\nregistration problems in three dimensions. We consider an optimal control\nformulation. We invert for a stationary velocity field that parameterizes the\ndeformation map. Our solver is based on a globalized, preconditioned, inexact\nreduced space Gauss--Newton--Krylov scheme. We exploit state-of-the-art\ntechniques in scientific computing to develop an effective solver that scales\nto thousands of distributed memory nodes on high-end clusters. We present the\nformulation, discuss algorithmic features, describe the software package, and\nintroduce an improved preconditioner for the reduced space Hessian to speed up\nthe convergence of our solver. We test registration performance on synthetic\nand real data. We demonstrate registration accuracy on several neuroimaging\ndatasets. We compare the performance of our scheme against different flavors of\nthe Demons algorithm for diffeomorphic image registration. We study convergence\nof our preconditioner and our overall algorithm. We report scalability results\non state-of-the-art supercomputing platforms. We demonstrate that we can solve\nregistration problems for clinically relevant data sizes in two to four minutes\non a standard compute node with 20 cores, attaining excellent data fidelity.\nWith the present work we achieve a speedup of (on average) 5$\\times$ with a\npeak performance of up to 17$\\times$ compared to our former work. \n\n"}
{"id": "1808.05545", "contents": "Title: Limits of bimorphic lenses Abstract: Bimorphic lenses are a simplification of polymorphic lenses that (like\npolymorphic lenses) have a type defined by 4 parameters, but which are defined\nin a monomorphic type system (i.e. an ordinary category with finite products).\nWe show that the category of bimorphic lenses is complete when the base\ncategory is complete, cocomplete and cartesian closed, and so symmetric\nbimorphic lenses can be defined as spans of ordinary bimorphic lenses. This is\nin contrast to monomorphic lenses, which do not have pullbacks, and for which\nthe category of spans can be defined in an ad-hoc way only when the lenses\nsatisfy a certain axiom (the put-get law). This is a step towards a theory of\nsymmetric polymorphic lenses. Bimorphic lenses additionally play an essential\nrole in compositional game theory, and spans of bimorphic lenses are a step\ntowards a compact closed category of open games. \n\n"}
{"id": "1808.05577", "contents": "Title: Deeper Image Quality Transfer: Training Low-Memory Neural Networks for\n  3D Images Abstract: In this paper we address the memory demands that come with the processing of\n3-dimensional, high-resolution, multi-channeled medical images in deep\nlearning. We exploit memory-efficient backpropagation techniques, to reduce the\nmemory complexity of network training from being linear in the network's depth,\nto being roughly constant $ - $ permitting us to elongate deep architectures\nwith negligible memory increase. We evaluate our methodology in the paradigm of\nImage Quality Transfer, whilst noting its potential application to various\ntasks that use deep learning. We study the impact of depth on accuracy and show\nthat deeper models have more predictive power, which may exploit larger\ntraining sets. We obtain substantially better results than the previous\nstate-of-the-art model with a slight memory increase, reducing the\nroot-mean-squared-error by $ 13\\% $. Our code is publicly available. \n\n"}
{"id": "1808.05839", "contents": "Title: Neuromorphic Architecture for the Hierarchical Temporal Memory Abstract: A biomimetic machine intelligence algorithm, that holds promise in creating\ninvariant representations of spatiotemporal input streams is the hierarchical\ntemporal memory (HTM). This unsupervised online algorithm has been demonstrated\non several machine-learning tasks, including anomaly detection. Significant\neffort has been made in formalizing and applying the HTM algorithm to different\nclasses of problems. There are few early explorations of the HTM hardware\narchitecture, especially for the earlier version of the spatial pooler of HTM\nalgorithm. In this article, we present a full-scale HTM architecture for both\nspatial pooler and temporal memory. Synthetic synapse design is proposed to\naddress the potential and dynamic interconnections occurring during learning.\nThe architecture is interweaved with parallel cells and columns that enable\nhigh processing speed for the HTM. The proposed architecture is verified for\ntwo different datasets: MNIST and the European number plate font (EUNF), with\nand without the presence of noise. The spatial pooler architecture is\nsynthesized on Xilinx ZYNQ-7, with 91.16% classification accuracy for MNIST and\n90\\% accuracy for EUNF, with noise. For the temporal memory sequence\nprediction, first and second order predictions are observed for a 5-number long\nsequence generated from EUNF dataset and 95% accuracy is obtained. Moreover,\nthe proposed hardware architecture offers 1364X speedup over the software\nrealization. These results indicate that the proposed architecture can serve as\na digital core to build the HTM in hardware and eventually as a standalone\nself-learning system. \n\n"}
{"id": "1808.06351", "contents": "Title: Lambda Calculus with Explicit Read-back Abstract: This paper introduces a new term rewriting system that is similar to the\nembedded read-back mechanism for interaction nets presented in our previous\nwork, but is easier to follow than in the original setting and thus to analyze\nits properties. Namely, we verify that it correctly represents the lambda\ncalculus. Further, we show that there is exactly one reduction sequence that\nstarts with any term in our term rewriting system. Finally, we represent the\nleftmost strategy which is known to be normalizing. \n\n"}
{"id": "1808.07725", "contents": "Title: Measuring Coverage of Prolog Programs Using Mutation Testing Abstract: Testing is an important aspect in professional software development, both to\navoid and identify bugs as well as to increase maintainability. However,\nincreasing the number of tests beyond a reasonable amount hinders development\nprogress. To decide on the completeness of a test suite, many approaches to\nassert test coverage have been suggested. Yet, frameworks for logic programs\nremain scarce.\n  In this paper, we introduce a framework for Prolog programs measuring test\ncoverage using mutations. We elaborate the main ideas of mutation testing and\ntransfer them to logic programs. To do so, we discuss the usefulness of\ndifferent mutations in the context of Prolog and empirically evaluate them in a\nnew mutation testing framework on different examples. \n\n"}
{"id": "1808.07921", "contents": "Title: SOTER: A Runtime Assurance Framework for Programming Safe Robotics\n  Systems Abstract: The recent drive towards achieving greater autonomy and intelligence in\nrobotics has led to high levels of complexity. Autonomous robots increasingly\ndepend on third party off-the-shelf components and complex machine-learning\ntechniques. This trend makes it challenging to provide strong design-time\ncertification of correct operation.\n  To address these challenges, we present SOTER, a robotics programming\nframework with two key components: (1) a programming language for implementing\nand testing high-level reactive robotics software and (2) an integrated runtime\nassurance (RTA) system that helps enable the use of uncertified components,\nwhile still providing safety guarantees. SOTER provides language primitives to\ndeclaratively construct a RTA module consisting of an advanced,\nhigh-performance controller (uncertified), a safe, lower-performance controller\n(certified), and the desired safety specification. The framework provides a\nformal guarantee that a well-formed RTA module always satisfies the safety\nspecification, without completely sacrificing performance by using higher\nperformance uncertified components whenever safe. SOTER allows the complex\nrobotics software stack to be constructed as a composition of RTA modules,\nwhere each uncertified component is protected using a RTA module.\n  To demonstrate the efficacy of our framework, we consider a real-world\ncase-study of building a safe drone surveillance system. Our experiments both\nin simulation and on actual drones show that the SOTER-enabled RTA ensures the\nsafety of the system, including when untrusted third-party components have bugs\nor deviate from the desired behavior. \n\n"}
{"id": "1808.08512", "contents": "Title: Data Motifs: A Lens Towards Fully Understanding Big Data and AI\n  Workloads Abstract: The complexity and diversity of big data and AI workloads make understanding\nthem difficult and challenging. This paper proposes a new approach to modelling\nand characterizing big data and AI workloads. We consider each big data and AI\nworkload as a pipeline of one or more classes of units of computation performed\non different initial or intermediate data inputs. Each class of unit of\ncomputation captures the common requirements while being reasonably divorced\nfrom individual implementations, and hence we call it a data motif. For the\nfirst time, among a wide variety of big data and AI workloads, we identify\neight data motifs that take up most of the run time of those workloads,\nincluding Matrix, Sampling, Logic, Transform, Set, Graph, Sort and Statistic.\nWe implement the eight data motifs on different software stacks as the micro\nbenchmarks of an open-source big data and AI benchmark suite ---BigDataBench\n4.0 (publicly available from http://prof.ict.ac.cn/BigDataBench), and perform\ncomprehensive characterization of those data motifs from perspective of data\nsizes, types, sources, and patterns as a lens towards fully understanding big\ndata and AI workloads. We believe the eight data motifs are promising\nabstractions and tools for not only big data and AI benchmarking, but also\ndomain-specific hardware and software co-design. \n\n"}
{"id": "1808.10442", "contents": "Title: Application of Self-Play Reinforcement Learning to a Four-Player Game of\n  Imperfect Information Abstract: We introduce a new virtual environment for simulating a card game known as\n\"Big 2\". This is a four-player game of imperfect information with a relatively\ncomplicated action space (being allowed to play 1,2,3,4 or 5 card combinations\nfrom an initial starting hand of 13 cards). As such it poses a challenge for\nmany current reinforcement learning methods. We then use the recently proposed\n\"Proximal Policy Optimization\" algorithm to train a deep neural network to play\nthe game, purely learning via self-play, and find that it is able to reach a\nlevel which outperforms amateur human players after only a relatively short\namount of training time and without needing to search a tree of future game\nstates. \n\n"}
{"id": "1809.00503", "contents": "Title: Improving Convergence Rate Of IC3 Abstract: IC3, a well-known model checker, proves a property of a transition system by\nbuilding a sequence of formulas $F_0,\\dots,F_k$. Formula $F_i$, $0 \\leq i \\leq\nk$ over-approximates the set of states reachable in at most $i$ transitions.\nThe basic algorithm of IC3 cannot guarantee that the value of $k$ never exceeds\nthe reachability diameter of the system. We describe an algorithm called IC4\nthat gives such a guarantee. (IC4 stands for 'IC3 + Improved Convergence'). One\ncan argue that the average convergence rate of IC4 is better than for IC3 as\nwell. Improving convergence can facilitate some other variations of the basic\nalgorithm. As an example, we describe a version of IC4 employing property\ndecomposition. The latter means replacing an original (strong) property with a\nconjunction of weaker properties to prove by IC4. We argue that addressing the\nconvergence problem is important for making the property decomposition approach\nwork. \n\n"}
{"id": "1809.00509", "contents": "Title: DeFactoNLP: Fact Verification using Entity Recognition, TFIDF Vector\n  Comparison and Decomposable Attention Abstract: In this paper, we describe DeFactoNLP, the system we designed for the FEVER\n2018 Shared Task. The aim of this task was to conceive a system that can not\nonly automatically assess the veracity of a claim but also retrieve evidence\nsupporting this assessment from Wikipedia. In our approach, the Wikipedia\ndocuments whose Term Frequency-Inverse Document Frequency (TFIDF) vectors are\nmost similar to the vector of the claim and those documents whose names are\nsimilar to those of the named entities (NEs) mentioned in the claim are\nidentified as the documents which might contain evidence. The sentences in\nthese documents are then supplied to a textual entailment recognition module.\nThis module calculates the probability of each sentence supporting the claim,\ncontradicting the claim or not providing any relevant information to assess the\nveracity of the claim. Various features computed using these probabilities are\nfinally used by a Random Forest classifier to determine the overall\ntruthfulness of the claim. The sentences which support this classification are\nreturned as evidence. Our approach achieved a 0.4277 evidence F1-score, a\n0.5136 label accuracy and a 0.3833 FEVER score. \n\n"}
{"id": "1809.00939", "contents": "Title: Decentralized Search on Decentralized Web Abstract: Decentralized Web, or DWeb, is envisioned as a promising future of the Web.\nBeing decentralized, there are no dedicated web servers in DWeb; Devices that\nretrieve web contents also serve their cached data to peer devices with\nstraight privacy-preserving mechanisms. The fact that contents in DWeb are\ndistributed, replicated, and decentralized lead to a number of key advantages\nover the conventional web. These include better resiliency against network\npartitioning and distributed-denial-of-service attacks (DDoS), and better\nbrowsing experiences in terms of shorter latency and higher throughput.\nMoreover, DWeb provides tamper-proof contents because each content piece is\nuniquely identified by a cryptographic hash. DWeb also clicks well with future\nInternet architectures, such as Named Data Networking (NDN).Search engines have\nbeen an inseparable element of the Web. Contemporary (\"Web 2.0\") search\nengines, however, provide centralized services. They are thus subject to DDoS\nattacks, insider threat, and ethical issues like search bias and censorship. As\nthe web moves from being centralized to being decentralized, search engines\nought to follow. We propose QueenBee, a decentralized search engine for DWeb.\nQueenBee is so named because worker bees and honeycomb are a common metaphor\nfor distributed architectures, with the queen being the one that holds the\ncolony together. QueenBee aims to revolutionize the search engine business\nmodel by offering incentives to both content providers and peers that\nparticipate in QueenBee's page indexing and ranking operations. \n\n"}
{"id": "1809.00969", "contents": "Title: A Deeper Insight into the UnDEMoN: Unsupervised Deep Network for Depth\n  and Ego-Motion Estimation Abstract: This paper presents an unsupervised deep learning framework called UnDEMoN\nfor estimating dense depth map and 6-DoF camera pose information directly from\nmonocular images. The proposed network is trained using unlabeled monocular\nstereo image pairs and is shown to provide superior performance in depth and\nego-motion estimation compared to the existing state-of-the-art. These\nimprovements are achieved by introducing a new objective function that aims to\nminimize spatial as well as temporal reconstruction losses simultaneously.\nThese losses are defined using bi-linear sampling kernel and penalized using\nthe Charbonnier penalty function. The objective function, thus created,\nprovides robustness to image gradient noises thereby improving the overall\nestimation accuracy without resorting to any coarse to fine strategies which\nare currently prevalent in the literature. Another novelty lies in the fact\nthat we combine a disparity-based depth estimation network with a pose\nestimation network to obtain absolute scale-aware 6 DOF Camera pose and\nsuperior depth map. The effectiveness of the proposed approach is demonstrated\nthrough performance comparison with the existing supervised and unsupervised\nmethods on the KITTI driving dataset. \n\n"}
{"id": "1809.01124", "contents": "Title: Straight to the Facts: Learning Knowledge Base Retrieval for Factual\n  Visual Question Answering Abstract: Question answering is an important task for autonomous agents and virtual\nassistants alike and was shown to support the disabled in efficiently\nnavigating an overwhelming environment. Many existing methods focus on\nobservation-based questions, ignoring our ability to seamlessly combine\nobserved content with general knowledge. To understand interactions with a\nknowledge base, a dataset has been introduced recently and keyword matching\ntechniques were shown to yield compelling results despite being vulnerable to\nmisconceptions due to synonyms and homographs. To address this issue, we\ndevelop a learning-based approach which goes straight to the facts via a\nlearned embedding space. We demonstrate state-of-the-art results on the\nchallenging recently introduced fact-based visual question answering dataset,\noutperforming competing methods by more than 5%. \n\n"}
{"id": "1809.01266", "contents": "Title: DeepHunter: Hunting Deep Neural Network Defects via Coverage-Guided\n  Fuzzing Abstract: In company with the data explosion over the past decade, deep neural network\n(DNN) based software has experienced unprecedented leap and is becoming the key\ndriving force of many novel industrial applications, including many\nsafety-critical scenarios such as autonomous driving. Despite great success\nachieved in various human intelligence tasks, similar to traditional software,\nDNNs could also exhibit incorrect behaviors caused by hidden defects causing\nsevere accidents and losses. In this paper, we propose DeepHunter, an automated\nfuzz testing framework for hunting potential defects of general-purpose DNNs.\nDeepHunter performs metamorphic mutation to generate new semantically preserved\ntests, and leverages multiple plugable coverage criteria as feedback to guide\nthe test generation from different perspectives. To be scalable towards\npractical-sized DNNs, DeepHunter maintains multiple tests in a batch, and\nprioritizes the tests selection based on active feedback. The effectiveness of\nDeepHunter is extensively investigated on 3 popular datasets (MNIST, CIFAR-10,\nImageNet) and 7 DNNs with diverse complexities, under a large set of 6 coverage\ncriteria as feedback. The large-scale experiments demonstrate that DeepHunter\ncan (1) significantly boost the coverage with guidance; (2) generate useful\ntests to detect erroneous behaviors and facilitate the DNN model quality\nevaluation; (3) accurately capture potential defects during DNN quantization\nfor platform migration. \n\n"}
{"id": "1809.01560", "contents": "Title: Reinforcement Learning under Threats Abstract: In several reinforcement learning (RL) scenarios, mainly in security\nsettings, there may be adversaries trying to interfere with the reward\ngenerating process. In this paper, we introduce Threatened Markov Decision\nProcesses (TMDPs), which provide a framework to support a decision maker\nagainst a potential adversary in RL. Furthermore, we propose a level-$k$\nthinking scheme resulting in a new learning framework to deal with TMDPs. After\nintroducing our framework and deriving theoretical results, relevant empirical\nevidence is given via extensive experiments, showing the benefits of accounting\nfor adversaries while the agent learns. \n\n"}
{"id": "1809.01604", "contents": "Title: Merging datasets through deep learning Abstract: Merging datasets is a key operation for data analytics. A frequent\nrequirement for merging is joining across columns that have different surface\nforms for the same entity (e.g., the name of a person might be represented as\n\"Douglas Adams\" or \"Adams, Douglas\"). Similarly, ontology alignment can require\nrecognizing distinct surface forms of the same entity, especially when\nontologies are independently developed. However, data management systems are\ncurrently limited to performing merges based on string equality, or at best\nusing string similarity. We propose an approach to performing merges based on\ndeep learning models. Our approach depends on (a) creating a deep learning\nmodel that maps surface forms of an entity into a set of vectors such that\nalternate forms for the same entity are closest in vector space, (b) indexing\nthese vectors using a nearest neighbors algorithm to find the forms that can be\npotentially joined together. To build these models, we had to adapt techniques\nfrom metric learning due to the characteristics of the data; specifically we\ndescribe novel sample selection techniques and loss functions that work for\nthis problem. To evaluate our approach, we used Wikidata as ground truth and\nbuilt models from datasets with approximately 1.1M people's names (200K\nidentities) and 130K company names (70K identities). We developed models that\nallow for joins with precision@1 of .75-.81 and recall of .74-.81. We make the\nmodels available for aligning people or companies across multiple datasets. \n\n"}
{"id": "1809.01997", "contents": "Title: Dual Ask-Answer Network for Machine Reading Comprehension Abstract: There are three modalities in the reading comprehension setting: question,\nanswer and context. The task of question answering or question generation aims\nto infer an answer or a question when given the counterpart based on context.\nWe present a novel two-way neural sequence transduction model that connects\nthree modalities, allowing it to learn two tasks simultaneously and mutually\nbenefit one another. During training, the model receives\nquestion-context-answer triplets as input and captures the cross-modal\ninteraction via a hierarchical attention process. Unlike previous joint\nlearning paradigms that leverage the duality of question generation and\nquestion answering at data level, we solve such dual tasks at the architecture\nlevel by mirroring the network structure and partially sharing components at\ndifferent layers. This enables the knowledge to be transferred from one task to\nanother, helping the model to find a general representation for each modality.\nThe evaluation on four public datasets shows that our dual-learning model\noutperforms the mono-learning counterpart as well as the state-of-the-art joint\nmodels on both question answering and question generation tasks. \n\n"}
{"id": "1809.02079", "contents": "Title: Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue\n  Models Abstract: We present two categories of model-agnostic adversarial strategies that\nreveal the weaknesses of several generative, task-oriented dialogue models:\nShould-Not-Change strategies that evaluate over-sensitivity to small and\nsemantics-preserving edits, as well as Should-Change strategies that test if a\nmodel is over-stable against subtle yet semantics-changing modifications. We\nnext perform adversarial training with each strategy, employing a max-margin\napproach for negative generative examples. This not only makes the target\ndialogue model more robust to the adversarial inputs, but also helps it perform\nsignificantly better on the original inputs. Moreover, training on all\nstrategies combined achieves further improvements, achieving a new\nstate-of-the-art performance on the original task (also verified via human\nevaluation). In addition to adversarial training, we also address the\nrobustness task at the model-level, by feeding it subword units as both inputs\nand outputs, and show that the resulting model is equally competitive, requires\nonly 1/4 of the original vocabulary size, and is robust to one of the\nadversarial strategies (to which the original model is vulnerable) even without\nadversarial training. \n\n"}
{"id": "1809.02941", "contents": "Title: Well Quasiorders and Hierarchy Theory Abstract: We discuss some applications of WQOs to several fields were hierarchies and\nreducibilities are the principal classification tools, notably to Descriptive\nSet Theory, Computability theory and Automata Theory. While the classical\nhierarchies of sets usually degenerate to structures very close to ordinals,\nthe extension of them to functions requires more complicated WQOs, and the same\napplies to reducibilities. We survey some results obtained so far and discuss\nopen problems and possible research directions. \n\n"}
{"id": "1809.03057", "contents": "Title: Variance Reduction in Monte Carlo Counterfactual Regret Minimization\n  (VR-MCCFR) for Extensive Form Games using Baselines Abstract: Learning strategies for imperfect information games from samples of\ninteraction is a challenging problem. A common method for this setting, Monte\nCarlo Counterfactual Regret Minimization (MCCFR), can have slow long-term\nconvergence rates due to high variance. In this paper, we introduce a variance\nreduction technique (VR-MCCFR) that applies to any sampling variant of MCCFR.\nUsing this technique, per-iteration estimated values and updates are\nreformulated as a function of sampled values and state-action baselines,\nsimilar to their use in policy gradient reinforcement learning. The new\nformulation allows estimates to be bootstrapped from other estimates within the\nsame episode, propagating the benefits of baselines along the sampled\ntrajectory; the estimates remain unbiased even when bootstrapping from other\nestimates. Finally, we show that given a perfect baseline, the variance of the\nvalue estimates can be reduced to zero. Experimental evaluation shows that\nVR-MCCFR brings an order of magnitude speedup, while the empirical variance\ndecreases by three orders of magnitude. The decreased variance allows for the\nfirst time CFR+ to be used with sampling, increasing the speedup to two orders\nof magnitude. \n\n"}
{"id": "1809.03299", "contents": "Title: Monte Carlo Tree Search for Verifying Reachability in Markov Decision\n  Processes Abstract: The maximum reachability probabilities in a Markov decision process can be\ncomputed using value iteration (VI). Recently, simulation-based heuristic\nextensions of VI have been introduced, such as bounded real-time dynamic\nprogramming (BRTDP), which often manage to avoid explicit analysis of the whole\nstate space while preserving guarantees on the computed result. In this paper,\nwe introduce a new class of such heuristics, based on Monte Carlo tree search\n(MCTS), a technique celebrated in various machine-learning settings. We provide\na spectrum of algorithms ranging from MCTS to BRTDP. We evaluate these\ntechniques and show that for larger examples, where VI is no more applicable,\nour techniques are more broadly applicable than BRTDP with only a minor\nadditional overhead. \n\n"}
{"id": "1809.03330", "contents": "Title: Neural Allocentric Intuitive Physics Prediction from Real Videos Abstract: Humans are able to make rich predictions about the future dynamics of\nphysical objects from a glance. On the other hand, most existing computer\nvision approaches require strong assumptions about the underlying system,\nad-hoc modeling, or annotated datasets, to carry out even simple predictions.\nTo tackle this gap, we propose a new perspective on the problem of learning\nintuitive physics that is inspired by the spatial memory representation of\nobjects and spaces in human brains, in particular the co-existence of\negocentric and allocentric spatial representations. We present a generic\nframework that learns a layered representation of the physical world, using a\ncascade of invertible modules. In this framework, real images are first\nconverted to a synthetic domain representation that reduces complexity arising\nfrom lighting and texture. Then, an allocentric viewpoint transformer removes\nviewpoint complexity by projecting images to a canonical view. Finally, a novel\nRecurrent Latent Variation Network (RLVN) architecture learns the dynamics of\nthe objects interacting with the environment and predicts future motion,\nleveraging the availability of unlimited synthetic simulations. Predicted\nframes are then projected back to the original camera view and translated back\nto the real world domain. Experimental results show the ability of the\nframework to consistently and accurately predict several frames in the future\nand the ability to adapt to real images. \n\n"}
{"id": "1809.03956", "contents": "Title: Abstraction Learning Abstract: There has been a gap between artificial intelligence and human intelligence.\nIn this paper, we identify three key elements forming human intelligence, and\nsuggest that abstraction learning combines these elements and is thus a way to\nbridge the gap. Prior researches in artificial intelligence either specify\nabstraction by human experts, or take abstraction as a qualitative explanation\nfor the model. This paper aims to learn abstraction directly. We tackle three\nmain challenges: representation, objective function, and learning algorithm.\nSpecifically, we propose a partition structure that contains pre-allocated\nabstraction neurons; we formulate abstraction learning as a constrained\noptimization problem, which integrates abstraction properties; we develop a\nnetwork evolution algorithm to solve this problem. This complete framework is\nnamed ONE (Optimization via Network Evolution). In our experiments on MNIST,\nONE shows elementary human-like intelligence, including low energy consumption,\nknowledge sharing, and lifelong learning. \n\n"}
{"id": "1809.04070", "contents": "Title: Interstellar: Using Halide's Scheduling Language to Analyze DNN\n  Accelerators Abstract: We show that DNN accelerator micro-architectures and their program mappings\nrepresent specific choices of loop order and hardware parallelism for computing\nthe seven nested loops of DNNs, which enables us to create a formal taxonomy of\nall existing dense DNN accelerators. Surprisingly, the loop transformations\nneeded to create these hardware variants can be precisely and concisely\nrepresented by Halide's scheduling language. By modifying the Halide compiler\nto generate hardware, we create a system that can fairly compare these prior\naccelerators. As long as proper loop blocking schemes are used, and the\nhardware can support mapping replicated loops, many different hardware\ndataflows yield similar energy efficiency with good performance. This is\nbecause the loop blocking can ensure that most data references stay on-chip\nwith good locality and the processing units have high resource utilization. How\nresources are allocated, especially in the memory system, has a large impact on\nenergy and performance. By optimizing hardware resource allocation while\nkeeping throughput constant, we achieve up to 4.2X energy improvement for\nConvolutional Neural Networks (CNNs), 1.6X and 1.8X improvement for Long\nShort-Term Memories (LSTMs) and multi-layer perceptrons (MLPs), respectively. \n\n"}
{"id": "1809.04322", "contents": "Title: Reinforcement Learning in Topology-based Representation for Human Body\n  Movement with Whole Arm Manipulation Abstract: Moving a human body or a large and bulky object can require the strength of\nwhole arm manipulation (WAM). This type of manipulation places the load on the\nrobot's arms and relies on global properties of the interaction to\nsucceed---rather than local contacts such as grasping or non-prehensile\npushing. In this paper, we learn to generate motions that enable WAM for\nholding and transporting of humans in certain rescue or patient care scenarios.\nWe model the task as a reinforcement learning problem in order to provide a\nbehavior that can directly respond to external perturbation and human motion.\nFor this, we represent global properties of the robot-human interaction with\ntopology-based coordinates that are computed from arm and torso positions.\nThese coordinates also allow transferring the learned policy to other body\nshapes and sizes. For training and evaluation, we simulate a dynamic sea rescue\nscenario and show in quantitative experiments that the policy can solve unseen\nscenarios with differently-shaped humans, floating humans, or with perception\nnoise. Our qualitative experiments show the subsequent transporting after\nholding is achieved and we demonstrate that the policy can be directly\ntransferred to a real world setting. \n\n"}
{"id": "1809.04497", "contents": "Title: Hyperprior Induced Unsupervised Disentanglement of Latent\n  Representations Abstract: We address the problem of unsupervised disentanglement of latent\nrepresentations learnt via deep generative models. In contrast to current\napproaches that operate on the evidence lower bound (ELBO), we argue that\nstatistical independence in the latent space of VAEs can be enforced in a\nprincipled hierarchical Bayesian manner. To this effect, we augment the\nstandard VAE with an inverse-Wishart (IW) prior on the covariance matrix of the\nlatent code. By tuning the IW parameters, we are able to encourage (or\ndiscourage) independence in the learnt latent dimensions. Extensive\nexperimental results on a range of datasets (2DShapes, 3DChairs, 3DFaces and\nCelebA) show our approach to outperform the $\\beta$-VAE and is competitive with\nthe state-of-the-art FactorVAE. Our approach achieves significantly better\ndisentanglement and reconstruction on a new dataset (CorrelatedEllipses) which\nintroduces correlations between the factors of variation. \n\n"}
{"id": "1809.05070", "contents": "Title: Physical Primitive Decomposition Abstract: Objects are made of parts, each with distinct geometry, physics,\nfunctionality, and affordances. Developing such a distributed, physical,\ninterpretable representation of objects will facilitate intelligent agents to\nbetter explore and interact with the world. In this paper, we study physical\nprimitive decomposition---understanding an object through its components, each\nwith physical and geometric attributes. As annotated data for object parts and\nphysics are rare, we propose a novel formulation that learns physical\nprimitives by explaining both an object's appearance and its behaviors in\nphysical events. Our model performs well on block towers and tools in both\nsynthetic and real scenarios; we also demonstrate that visual and physical\nobservations often provide complementary signals. We further present ablation\nand behavioral studies to better understand our model and contrast it with\nhuman performance. \n\n"}
{"id": "1809.05247", "contents": "Title: Revisiting Random Binning Features: Fast Convergence and Strong\n  Parallelizability Abstract: Kernel method has been developed as one of the standard approaches for\nnonlinear learning, which however, does not scale to large data set due to its\nquadratic complexity in the number of samples. A number of kernel approximation\nmethods have thus been proposed in the recent years, among which the random\nfeatures method gains much popularity due to its simplicity and direct\nreduction of nonlinear problem to a linear one. The Random Binning (RB)\nfeature, proposed in the first random-feature paper \\cite{rahimi2007random},\nhas drawn much less attention than the Random Fourier (RF) feature. In this\nwork, we observe that the RB features, with right choice of optimization\nsolver, could be orders-of-magnitude more efficient than other random features\nand kernel approximation methods under the same requirement of accuracy. We\nthus propose the first analysis of RB from the perspective of optimization,\nwhich by interpreting RB as a Randomized Block Coordinate Descent in the\ninfinite-dimensional space, gives a faster convergence rate compared to that of\nother random features. In particular, we show that by drawing $R$ random grids\nwith at least $\\kappa$ number of non-empty bins per grid in expectation, RB\nmethod achieves a convergence rate of $O(1/(\\kappa R))$, which not only\nsharpens its $O(1/\\sqrt{R})$ rate from Monte Carlo analysis, but also shows a\n$\\kappa$ times speedup over other random features under the same analysis\nframework. In addition, we demonstrate another advantage of RB in the\nL1-regularized setting, where unlike other random features, a RB-based\nCoordinate Descent solver can be parallelized with guaranteed speedup\nproportional to $\\kappa$. Our extensive experiments demonstrate the superior\nperformance of the RB features over other random features and kernel\napproximation methods. Our code and data is available at {\n\\url{https://github.com/teddylfwu/RB_GEN}}. \n\n"}
{"id": "1809.05888", "contents": "Title: An investigation of a deep learning based malware detection system Abstract: We investigate a Deep Learning based system for malware detection. In the\ninvestigation, we experiment with different combination of Deep Learning\narchitectures including Auto-Encoders, and Deep Neural Networks with varying\nlayers over Malicia malware dataset on which earlier studies have obtained an\naccuracy of (98%) with an acceptable False Positive Rates (1.07%). But these\nresults were done using extensive man-made custom domain features and investing\ncorresponding feature engineering and design efforts. In our proposed approach,\nbesides improving the previous best results (99.21% accuracy and a False\nPositive Rate of 0.19%) indicates that Deep Learning based systems could\ndeliver an effective defense against malware. Since it is good in automatically\nextracting higher conceptual features from the data, Deep Learning based\nsystems could provide an effective, general and scalable mechanism for\ndetection of existing and unknown malware. \n\n"}
{"id": "1809.06646", "contents": "Title: Model-Free Adaptive Optimal Control of Episodic Fixed-Horizon\n  Manufacturing Processes using Reinforcement Learning Abstract: A self-learning optimal control algorithm for episodic fixed-horizon\nmanufacturing processes with time-discrete control actions is proposed and\nevaluated on a simulated deep drawing process. The control model is built\nduring consecutive process executions under optimal control via reinforcement\nlearning, using the measured product quality as reward after each process\nexecution. Prior model formulation, which is required by state-of-the-art\nalgorithms from model predictive control and approximate dynamic programming,\nis therefore obsolete. This avoids several difficulties namely in system\nidentification, accurate modelling, and runtime complexity, that arise when\ndealing with processes subject to nonlinear dynamics and stochastic influences.\nInstead of using pre-created process and observation models, value\nfunction-based reinforcement learning algorithms build functions of expected\nfuture reward, which are used to derive optimal process control decisions. The\nexpectation functions are learned online, by interacting with the process. The\nproposed algorithm takes stochastic variations of the process conditions into\naccount and is able to cope with partial observability. A Q-learning-based\nmethod for adaptive optimal control of partially observable episodic\nfixed-horizon manufacturing processes is developed and studied. The resulting\nalgorithm is instantiated and evaluated by applying it to a simulated\nstochastic optimal control problem in metal sheet deep drawing. \n\n"}
{"id": "1809.07141", "contents": "Title: A survey of advances in epistemic logic program solvers Abstract: Recent research in extensions of Answer Set Programming has included a\nrenewed interest in the language of Epistemic Specifications, which adds modal\noperators K (\"known\") and M (\"may be true\") to provide for more powerful\nintrospective reasoning and enhanced capability, particularly when reasoning\nwith incomplete information. An epistemic logic program is a set of rules in\nthis language. Infused with the research has been the desire for an efficient\nsolver to enable the practical use of such programs for problem solving. In\nthis paper, we report on the current state of development of epistemic logic\nprogram solvers. \n\n"}
{"id": "1809.07599", "contents": "Title: Sparsified SGD with Memory Abstract: Huge scale machine learning problems are nowadays tackled by distributed\noptimization algorithms, i.e. algorithms that leverage the compute power of\nmany devices for training. The communication overhead is a key bottleneck that\nhinders perfect scalability. Various recent works proposed to use quantization\nor sparsification techniques to reduce the amount of data that needs to be\ncommunicated, for instance by only sending the most significant entries of the\nstochastic gradient (top-k sparsification). Whilst such schemes showed very\npromising performance in practice, they have eluded theoretical analysis so\nfar.\n  In this work we analyze Stochastic Gradient Descent (SGD) with\nk-sparsification or compression (for instance top-k or random-k) and show that\nthis scheme converges at the same rate as vanilla SGD when equipped with error\ncompensation (keeping track of accumulated errors in memory). That is,\ncommunication can be reduced by a factor of the dimension of the problem\n(sometimes even more) whilst still converging at the same rate. We present\nnumerical experiments to illustrate the theoretical findings and the better\nscalability for distributed applications. \n\n"}
{"id": "1809.07878", "contents": "Title: Learning Quickly to Plan Quickly Using Modular Meta-Learning Abstract: Multi-object manipulation problems in continuous state and action spaces can\nbe solved by planners that search over sampled values for the continuous\nparameters of operators. The efficiency of these planners depends critically on\nthe effectiveness of the samplers used, but effective sampling in turn depends\non details of the robot, environment, and task. Our strategy is to learn\nfunctions called \"specializers\" that generate values for continuous operator\nparameters, given a state description and values for the discrete parameters.\nRather than trying to learn a single specializer for each operator from large\namounts of data on a single task, we take a modular meta-learning approach. We\ntrain on multiple tasks and learn a variety of specializers that, on a new\ntask, can be quickly adapted using relatively little data -- thus, our system\n\"learns quickly to plan quickly\" using these specializers. We validate our\napproach experimentally in simulated 3D pick-and-place tasks with continuous\nstate and action spaces. Visit http://tinyurl.com/chitnis-icra-19 for a\nsupplementary video. \n\n"}
{"id": "1809.08098", "contents": "Title: Efficient Formal Safety Analysis of Neural Networks Abstract: Neural networks are increasingly deployed in real-world safety-critical\ndomains such as autonomous driving, aircraft collision avoidance, and malware\ndetection. However, these networks have been shown to often mispredict on\ninputs with minor adversarial or even accidental perturbations. Consequences of\nsuch errors can be disastrous and even potentially fatal as shown by the recent\nTesla autopilot crash. Thus, there is an urgent need for formal analysis\nsystems that can rigorously check neural networks for violations of different\nsafety properties such as robustness against adversarial perturbations within a\ncertain $L$-norm of a given image. An effective safety analysis system for a\nneural network must be able to either ensure that a safety property is\nsatisfied by the network or find a counterexample, i.e., an input for which the\nnetwork will violate the property. Unfortunately, most existing techniques for\nperforming such analysis struggle to scale beyond very small networks and the\nones that can scale to larger networks suffer from high false positives and\ncannot produce concrete counterexamples in case of a property violation. In\nthis paper, we present a new efficient approach for rigorously checking\ndifferent safety properties of neural networks that significantly outperforms\nexisting approaches by multiple orders of magnitude. Our approach can check\ndifferent safety properties and find concrete counterexamples for networks that\nare 10$\\times$ larger than the ones supported by existing analysis techniques.\nWe believe that our approach to estimating tight output bounds of a network for\na given input range can also help improve the explainability of neural networks\nand guide the training process of more robust neural networks. \n\n"}
{"id": "1809.08694", "contents": "Title: Second-order Guarantees of Distributed Gradient Algorithms Abstract: We consider distributed smooth nonconvex unconstrained optimization over\nnetworks, modeled as a connected graph. We examine the behavior of distributed\ngradient-based algorithms near strict saddle points. Specifically, we establish\nthat (i) the renowned Distributed Gradient Descent (DGD) algorithm likely\nconverges to a neighborhood of a Second-order Stationary (SoS) solution; and\n(ii) the more recent class of distributed algorithms based on gradient\ntracking--implementable also over digraphs--likely converges to exact SoS\nsolutions, thus avoiding (strict) saddle-points. Furthermore, new convergence\nrate results to first-order critical points is established for the latter class\nof algorithms. \n\n"}
{"id": "1809.09331", "contents": "Title: Early Identification of Pathogenic Social Media Accounts Abstract: Pathogenic Social Media (PSM) accounts such as terrorist supporters exploit\nlarge communities of supporters for conducting attacks on social media. Early\ndetection of these accounts is crucial as they are high likely to be key users\nin making a harmful message \"viral\". In this paper, we make the first attempt\non utilizing causal inference to identify PSMs within a short time frame around\ntheir activity. We propose a time-decay causality metric and incorporate it\ninto a causal community detection-based algorithm. The proposed algorithm is\napplied to groups of accounts sharing similar causality features and is\nfollowed by a classification algorithm to classify accounts as PSM or not.\nUnlike existing techniques that take significant time to collect information\nsuch as network, cascade path, or content, our scheme relies solely on action\nlog of users. Results on a real-world dataset from Twitter demonstrate\neffectiveness and efficiency of our approach. We achieved precision of 0.84 for\ndetecting PSMs only based on their first 10 days of activity; the misclassified\naccounts were then detected 10 days later. \n\n"}
{"id": "1809.10315", "contents": "Title: Smooth Inter-layer Propagation of Stabilized Neural Networks for\n  Classification Abstract: Recent work has studied the reasons for the remarkable performance of deep\nneural networks in image classification. We examine batch normalization on the\none hand and the dynamical systems view of residual networks on the other hand.\nOur goal is in understanding the notions of stability and smoothness of the\ninter-layer propagation of ResNets so as to explain when they contribute to\nsignificantly enhanced performance. We postulate that such stability is of\nimportance for the trained ResNet to transfer. \n\n"}
{"id": "1809.10438", "contents": "Title: Wafer Quality Inspection using Memristive LSTM, ANN, DNN and HTM Abstract: The automated wafer inspection and quality control is a complex and\ntime-consuming task, which can speed up using neuromorphic memristive\narchitectures, as a separate inspection device or integrating directly into\nsensors. This paper presents the performance analysis and comparison of\ndifferent neuromorphic architectures for patterned wafer quality inspection and\nclassification. The application of non-volatile memristive devices in these\narchitectures ensures low power consumption, small on-chip area scalability. We\ndemonstrate that Long-Short Term Memory (LSTM) outperforms other architectures\nfor the same number of training iterations, and has relatively low on-chip area\nand power consumption. \n\n"}
{"id": "1809.10596", "contents": "Title: Predicting the confirmation time of Bitcoin transactions Abstract: We study the probabilistic distribution of the confirmation time of Bitcoin\ntransactions, conditional on the current memory pool (i.e., the queue of\ntransactions awaiting confirmation). The results of this paper are particularly\ninteresting for users that want to make a Bitcoin transaction during\n`heavy-traffic situations', when the transaction demand exceeds the block\ncapacity. In such situations, Bitcoin users tend to bid up the transaction\nfees, in order to gain priority over other users that pay a lower fee. We argue\nthat the time until a Bitcoin transaction is confirmed can be modelled as a\nparticular stochastic fluid queueing process (to be precise: a\nCram\\'er-Lundberg process). We approximate the queueing process in two\ndifferent ways. The first approach leads to a lower bound on the confirmation\nprobability, which becomes increasingly tight as traffic decreases. The second\napproach relies on a diffusion approximation with a continuity correction,\nwhich becomes increasingly accurate as traffic intensifies. The accuracy of the\napproximations under different traffic loads are evaluated in a simulation\nstudy. \n\n"}
{"id": "1809.10624", "contents": "Title: dynamicMF: A Matrix Factorization Approach to Monitor Resource Usage in\n  High Performance Computing Systems Abstract: High performance computing (HPC) facilities consist of a large number of\ninterconnected computing units (or nodes) that execute highly complex\nscientific simulations to support scientific research. Monitoring such\nfacilities, in real-time, is essential to ensure that the system operates at\npeak efficiency. Such systems are typically monitored using a variety of\nmeasurement and log data which capture the state of the various components\nwithin the system at regular intervals of time. As modern HPC systems grow in\ncapacity and complexity, the data produced by current resource monitoring tools\nis at a scale that it is no longer feasible to be visually monitored by\nanalysts. We propose a method that transforms the multi-dimensional output of\nresource monitoring tools to a low dimensional representation that facilitates\nthe understanding of the behavior of a High Performance Computing (HPC) system.\nThe proposed method automatically extracts the low-dimensional signal in the\ndata which can be used to track the system efficiency and identify performance\nanomalies. The method models the resource usage data as a three dimensional\ntensor (capturing resource usage of all compute nodes for difference resources\nover time). A dynamic matrix factorization algorithm, called dynamicMF, is\nproposed to extract a low-dimensional temporal signal for each node, which is\nsubsequently fed into an anomaly detector. Results on resource usage data\ncollected from the Lonestar 4 system at the Texas Advanced Computing Center\nshow that the identified anomalies are correlated with actual anomalous events\nreported in the system log messages. \n\n"}
{"id": "1810.00147", "contents": "Title: M$^3$RL: Mind-aware Multi-agent Management Reinforcement Learning Abstract: Most of the prior work on multi-agent reinforcement learning (MARL) achieves\noptimal collaboration by directly controlling the agents to maximize a common\nreward. In this paper, we aim to address this from a different angle. In\nparticular, we consider scenarios where there are self-interested agents (i.e.,\nworker agents) which have their own minds (preferences, intentions, skills,\netc.) and can not be dictated to perform tasks they do not wish to do. For\nachieving optimal coordination among these agents, we train a super agent\n(i.e., the manager) to manage them by first inferring their minds based on both\ncurrent and past observations and then initiating contracts to assign suitable\ntasks to workers and promise to reward them with corresponding bonuses so that\nthey will agree to work together. The objective of the manager is maximizing\nthe overall productivity as well as minimizing payments made to the workers for\nad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent\nManagement Reinforcement Learning (M^3RL), which consists of agent modeling and\npolicy learning. We have evaluated our approach in two environments, Resource\nCollection and Crafting, to simulate multi-agent management problems with\nvarious task settings and multiple designs for the worker agents. The\nexperimental results have validated the effectiveness of our approach in\nmodeling worker agents' minds online, and in achieving optimal ad-hoc teaming\nwith good generalization and fast adaptation. \n\n"}
{"id": "1810.00305", "contents": "Title: Resource Management in Fog/Edge Computing: A Survey Abstract: Contrary to using distant and centralized cloud data center resources,\nemploying decentralized resources at the edge of a network for processing data\ncloser to user devices, such as smartphones and tablets, is an upcoming\ncomputing paradigm, referred to as fog/edge computing. Fog/edge resources are\ntypically resource-constrained, heterogeneous, and dynamic compared to the\ncloud, thereby making resource management an important challenge that needs to\nbe addressed. This article reviews publications as early as 1991, with 85% of\nthe publications between 2013-2018, to identify and classify the architectures,\ninfrastructure, and underlying algorithms for managing resources in fog/edge\ncomputing. \n\n"}
{"id": "1810.00510", "contents": "Title: Interactive Agent Modeling by Learning to Probe Abstract: The ability of modeling the other agents, such as understanding their\nintentions and skills, is essential to an agent's interactions with other\nagents. Conventional agent modeling relies on passive observation from\ndemonstrations. In this work, we propose an interactive agent modeling scheme\nenabled by encouraging an agent to learn to probe. In particular, the probing\nagent (i.e. a learner) learns to interact with the environment and with a\ntarget agent (i.e., a demonstrator) to maximize the change in the observed\nbehaviors of that agent. Through probing, rich behaviors can be observed and\nare used for enhancing the agent modeling to learn a more accurate mind model\nof the target agent. Our framework consists of two learning processes: i)\nimitation learning for an approximated agent model and ii) pure\ncuriosity-driven reinforcement learning for an efficient probing policy to\ndiscover new behaviors that otherwise can not be observed. We have validated\nour approach in four different tasks. The experimental results suggest that the\nagent model learned by our approach i) generalizes better in novel scenarios\nthan the ones learned by passive observation, random probing, and other\ncuriosity-driven approaches do, and ii) can be used for enhancing performance\nin multiple applications including distilling optimal planning to a policy net,\ncollaboration, and competition. A video demo is available at\nhttps://www.dropbox.com/s/8mz6rd3349tso67/Probing_Demo.mov?dl=0 \n\n"}
{"id": "1810.00555", "contents": "Title: Probabilistic Meta-Representations Of Neural Networks Abstract: Existing Bayesian treatments of neural networks are typically characterized\nby weak prior and approximate posterior distributions according to which all\nthe weights are drawn independently. Here, we consider a richer prior\ndistribution in which units in the network are represented by latent variables,\nand the weights between units are drawn conditionally on the values of the\ncollection of those variables. This allows rich correlations between related\nweights, and can be seen as realizing a function prior with a Bayesian\ncomplexity regularizer ensuring simple solutions. We illustrate the resulting\nmeta-representations and representations, elucidating the power of this prior. \n\n"}
{"id": "1810.01165", "contents": "Title: Semi-supervised Text Regression with Conditional Generative Adversarial\n  Networks Abstract: Enormous online textual information provides intriguing opportunities for\nunderstandings of social and economic semantics. In this paper, we propose a\nnovel text regression model based on a conditional generative adversarial\nnetwork (GAN), with an attempt to associate textual data and social outcomes in\na semi-supervised manner. Besides promising potential of predicting\ncapabilities, our superiorities are twofold: (i) the model works with\nunbalanced datasets of limited labelled data, which align with real-world\nscenarios; and (ii) predictions are obtained by an end-to-end framework,\nwithout explicitly selecting high-level representations. Finally we point out\nrelated datasets for experiments and future research directions. \n\n"}
{"id": "1810.01835", "contents": "Title: Human-Centered Autonomous Vehicle Systems: Principles of Effective\n  Shared Autonomy Abstract: Building effective, enjoyable, and safe autonomous vehicles is a lot harder\nthan has historically been considered. The reason is that, simply put, an\nautonomous vehicle must interact with human beings. This interaction is not a\nrobotics problem nor a machine learning problem nor a psychology problem nor an\neconomics problem nor a policy problem. It is all of these problems put into\none. It challenges our assumptions about the limitations of human beings at\ntheir worst and the capabilities of artificial intelligence systems at their\nbest. This work proposes a set of principles for designing and building\nautonomous vehicles in a human-centered way that does not run away from the\ncomplexity of human nature but instead embraces it. We describe our development\nof the Human-Centered Autonomous Vehicle (HCAV) as an illustrative case study\nof implementing these principles in practice. \n\n"}
{"id": "1810.01871", "contents": "Title: Grounding the Experience of a Visual Field through Sensorimotor\n  Contingencies Abstract: Artificial perception is traditionally handled by hand-designing task\nspecific algorithms. However, a truly autonomous robot should develop\nperceptive abilities on its own, by interacting with its environment, and\nadapting to new situations. The sensorimotor contingencies theory proposes to\nground the development of those perceptive abilities in the way the agent can\nactively transform its sensory inputs. We propose a sensorimotor approach,\ninspired by this theory, in which the agent explores the world and discovers\nits properties by capturing the sensorimotor regularities they induce. This\nwork presents an application of this approach to the discovery of a so-called\nvisual field as the set of regularities that a visual sensor imposes on a naive\nagent's experience. A formalism is proposed to describe how those regularities\ncan be captured in a sensorimotor predictive model. Finally, the approach is\nevaluated on a simulated system coarsely inspired from the human retina. \n\n"}
{"id": "1810.02106", "contents": "Title: Distributed Reconfiguration of Maximal Independent Sets Abstract: In this paper, we investigate a distributed maximal independent set (MIS)\nreconfiguration problem, in which there are two maximal independent sets for\nwhich every node is given its membership status, and the nodes need to\ncommunicate with their neighbors in order to find a reconfiguration schedule\nthat switches from the first MIS to the second. Such a schedule is a list of\nindependent sets that is restricted by forbidding two neighbors to change their\nmembership status at the same step. In addition, these independent sets should\nprovide some covering guarantee. We show that obtaining an actual MIS (and even\na 3-dominating set) in each intermediate step is impossible. However, we\nprovide efficient solutions when the intermediate sets are only required to be\nindependent and 4-dominating, which is almost always possible, as we fully\ncharacterize. Consequently, our goal is to pin down the tradeoff between the\npossible length of the schedule and the number of communication rounds. We\nprove that a constant length schedule can be found in\n$O(\\texttt{MIS}+\\texttt{R32})$ rounds, where $\\texttt{MIS}$ is the complexity\nof finding an MIS in a worst-case graph and $\\texttt{R32}$ is the complexity of\nfinding a $(3,2)$-ruling set. For bounded degree graphs, this is $O(\\log^*n)$\nrounds and we show that it is necessary. On the other extreme, we show that\nwith a constant number of rounds we can find a linear length schedule. \n\n"}
{"id": "1810.02749", "contents": "Title: Demonstration Abstract: A Toolkit for Specifying Service Level\n  Agreements for IoT applications Abstract: Today we see the use of the Internet of Things (IoT) in various application\ndomains such as healthcare, smart homes, smart cars, and smart-x applications\nin smart cities. The number of applications based on IoT and cloud computing is\nprojected to increase rapidly over the next few years. IoT-based services must\nmeet the guaranteed levels of quality of service (QoS) to match users'\nexpectations. Ensuring QoS through specifying the QoS constraints using Service\nLevel Agreements (SLAs) is crucial. Therefore, as a first step toward SLA\nmanagement, it is essential to provide an SLA specification in a\nmachine-readable format. In this paper, we demonstrate a toolkit for creating\nSLA specifications for IoT applications. The toolkit is used to simplify the\nprocess of capturing the requirements of IoT applications. We present a\ndemonstration of the toolkit using a Remote Health Monitoring Service (RHMS)\nusecase. The toolkit supports the following: (1) specifying the Service-Level\nObjectives (SLO) of an IoT application at the application level; (2) specifying\nthe workflow activities of the IoT application; (3) mapping each activity to\nthe required software and hardware resources and specifying the constraints of\nSLOs and other configuration- related metrics of the required hardware and\nsoftware; and (4) creating the composed SLA in JSON format. \n\n"}
{"id": "1810.03298", "contents": "Title: Multi-Stream Opportunistic Network Decoupling: Relay Selection and\n  Interference Management Abstract: We study multi-stream transmission in the $K \\times N \\times K$ channel with\ninterfering relay nodes, consisting of $K$ multi-antenna source--destination\n(S--D) pairs and $N$ single-antenna half-duplex relay nodes between the S--D\npairs. We propose a new achievable scheme operating with partial effective\nchannel gain, termed multi-stream opportunistic network decoupling (MS-OND),\nwhich achieves the optimal degrees of freedom (DoF) under a certain relay\nscaling law. Our protocol is built upon the conventional OND that leads to\nvirtual full-duplex mode with one data stream transmission per S--D pair,\ngeneralizing the idea of OND to multi-stream scenarios by leveraging relay\nselection and interference management. Specifically, two subsets of relay nodes\nare opportunistically selected using alternate relaying in terms of producing\nor receiving the minimum total interference level. For interference management,\neach source node sends $S \\,(1 \\le S \\le M)$ data streams to selected relay\nnodes with random beamforming for the first hop, while each destination node\nreceives its desired $S$ streams from the selected relay nodes via\nopportunistic interference alignment for the second hop, where $M$ is the\nnumber of antennas at each source or destination node. Our analytical results\nare validated by numerical evaluation. \n\n"}
{"id": "1810.03538", "contents": "Title: Combinatorial Attacks on Binarized Neural Networks Abstract: Binarized Neural Networks (BNNs) have recently attracted significant interest\ndue to their computational efficiency. Concurrently, it has been shown that\nneural networks may be overly sensitive to \"attacks\" - tiny adversarial changes\nin the input - which may be detrimental to their use in safety-critical\ndomains. Designing attack algorithms that effectively fool trained models is a\nkey step towards learning robust neural networks. The discrete,\nnon-differentiable nature of BNNs, which distinguishes them from their\nfull-precision counterparts, poses a challenge to gradient-based attacks. In\nthis work, we study the problem of attacking a BNN through the lens of\ncombinatorial and integer optimization. We propose a Mixed Integer Linear\nProgramming (MILP) formulation of the problem. While exact and flexible, the\nMILP quickly becomes intractable as the network and perturbation space grow. To\naddress this issue, we propose IProp, a decomposition-based algorithm that\nsolves a sequence of much smaller MILP problems. Experimentally, we evaluate\nboth proposed methods against the standard gradient-based attack (FGSM) on\nMNIST and Fashion-MNIST, and show that IProp performs favorably compared to\nFGSM, while scaling beyond the limits of the MILP. \n\n"}
{"id": "1810.03993", "contents": "Title: Model Cards for Model Reporting Abstract: Trained machine learning models are increasingly used to perform high-impact\ntasks in areas such as law enforcement, medicine, education, and employment. In\norder to clarify the intended use cases of machine learning models and minimize\ntheir usage in contexts for which they are not well suited, we recommend that\nreleased models be accompanied by documentation detailing their performance\ncharacteristics. In this paper, we propose a framework that we call model\ncards, to encourage such transparent model reporting. Model cards are short\ndocuments accompanying trained machine learning models that provide benchmarked\nevaluation in a variety of conditions, such as across different cultural,\ndemographic, or phenotypic groups (e.g., race, geographic location, sex,\nFitzpatrick skin type) and intersectional groups (e.g., age and race, or sex\nand Fitzpatrick skin type) that are relevant to the intended application\ndomains. Model cards also disclose the context in which models are intended to\nbe used, details of the performance evaluation procedures, and other relevant\ninformation. While we focus primarily on human-centered machine learning models\nin the application fields of computer vision and natural language processing,\nthis framework can be used to document any trained machine learning model. To\nsolidify the concept, we provide cards for two supervised models: One trained\nto detect smiling faces in images, and one trained to detect toxic comments in\ntext. We propose model cards as a step towards the responsible democratization\nof machine learning and related AI technology, increasing transparency into how\nwell AI technology works. We hope this work encourages those releasing trained\nmachine learning models to accompany model releases with similar detailed\nevaluation numbers and other relevant documentation. \n\n"}
{"id": "1810.04989", "contents": "Title: Listening for Sirens: Locating and Classifying Acoustic Alarms in City\n  Scenes Abstract: This paper is about alerting acoustic event detection and sound source\nlocalisation in an urban scenario. Specifically, we are interested in spotting\nthe presence of horns, and sirens of emergency vehicles. In order to obtain a\nreliable system able to operate robustly despite the presence of traffic noise,\nwhich can be copious, unstructured and unpredictable, we propose to treat the\nspectrograms of incoming stereo signals as images, and apply semantic\nsegmentation, based on a Unet architecture, to extract the target sound from\nthe background noise. In a multi-task learning scheme, together with signal\ndenoising, we perform acoustic event classification to identify the nature of\nthe alerting sound. Lastly, we use the denoised signals to localise the\nacoustic source on the horizon plane, by regressing the direction of arrival of\nthe sound through a CNN architecture. Our experimental evaluation shows an\naverage classification rate of 94%, and a median absolute error on the\nlocalisation of 7.5{\\deg} when operating on audio frames of 0.5s, and of\n2.5{\\deg} when operating on frames of 2.5s. The system offers excellent\nperformance in particularly challenging scenarios, where the noise level is\nremarkably high. \n\n"}
{"id": "1810.05937", "contents": "Title: End-to-End Service Level Agreement Specification for IoT Applications Abstract: The Internet of Things (IoT) promises to help solve a wide range of issues\nthat relate to our wellbeing within application domains that include smart\ncities, healthcare monitoring, and environmental monitoring. IoT is bringing\nnew wireless sensor use cases by taking advantage of the computing power and\nflexibility provided by Edge and Cloud Computing. However, the software and\nhardware resources used within such applications must perform correctly and\noptimally. Especially in applications where a failure of resources can be\ncritical. Service Level Agreements (SLA) where the performance requirements of\nsuch applications are defined, need to be specified in a standard way that\nreflects the end-to-end nature of IoT application domains, accounting for the\nQuality of Service (QoS) metrics within every layer including the Edge, Network\nGateways, and Cloud. In this paper, we propose a conceptual model that captures\nthe key entities of an SLA and their relationships, as a prior step for\nend-to-end SLA specification and composition. Service level objective (SLO)\nterms are also considered to express the QoS constraints. Moreover, we propose\na new SLA grammar which considers workflow activities and the multi-layered\nnature of IoT applications. Accordingly, we develop a tool for SLA\nspecification and composition that can be used as a template to generate SLAs\nin a machine-readable format. We demonstrate the effectiveness of the proposed\nspecification language through a literature survey that includes an SLA\nlanguage comparison analysis, and via reflecting the user satisfaction results\nof a usability study. \n\n"}
{"id": "1810.06839", "contents": "Title: Sharp Analysis of Learning with Discrete Losses Abstract: The problem of devising learning strategies for discrete losses (e.g.,\nmultilabeling, ranking) is currently addressed with methods and theoretical\nanalyses ad-hoc for each loss. In this paper we study a least-squares framework\nto systematically design learning algorithms for discrete losses, with\nquantitative characterizations in terms of statistical and computational\ncomplexity. In particular we improve existing results by providing explicit\ndependence on the number of labels for a wide class of losses and faster\nlearning rates in conditions of low-noise. Theoretical results are complemented\nwith experiments on real datasets, showing the effectiveness of the proposed\ngeneral approach. \n\n"}
{"id": "1810.07000", "contents": "Title: Influence of A-Posteriori Subcell Limiting on Fault Frequency in\n  Higher-Order DG Schemes Abstract: Soft error rates are increasing as modern architectures require increasingly\nsmall features at low voltages. Due to the large number of components used in\nHPC architectures, these are particularly vulnerable to soft errors. Hence,\nwhen designing applications that run for long time periods on large machines,\nalgorithmic resilience must be taken into account. In this paper we analyse the\ninherent resiliency of a-posteriori limiting procedures in the context of the\nexplicit ADER DG hyperbolic PDE solver ExaHyPE. The a-posteriori limiter checks\nelement-local high-order DG solutions for physical admissibility, and can thus\nbe expected to also detect hardware-induced errors. Algorithmically, it can be\ninterpreted as element-local checkpointing and restarting of the solver with a\nmore robust finite volume scheme on a fine subgrid. We show that the limiter\nindeed increases the resilience of the DG algorithm, detecting and correcting\nparticularly those faults which would otherwise lead to a fatal failure. \n\n"}
{"id": "1810.07167", "contents": "Title: Composable Action-Conditioned Predictors: Flexible Off-Policy Learning\n  for Robot Navigation Abstract: A general-purpose intelligent robot must be able to learn autonomously and be\nable to accomplish multiple tasks in order to be deployed in the real world.\nHowever, standard reinforcement learning approaches learn separate\ntask-specific policies and assume the reward function for each task is known a\npriori. We propose a framework that learns event cues from off-policy data, and\ncan flexibly combine these event cues at test time to accomplish different\ntasks. These event cue labels are not assumed to be known a priori, but are\ninstead labeled using learned models, such as computer vision detectors, and\nthen `backed up' in time using an action-conditioned predictive model. We show\nthat a simulated robotic car and a real-world RC car can gather data and train\nfully autonomously without any human-provided labels beyond those needed to\ntrain the detectors, and then at test-time be able to accomplish a variety of\ndifferent tasks. Videos of the experiments and code can be found at\nhttps://github.com/gkahn13/CAPs \n\n"}
{"id": "1810.07852", "contents": "Title: Distributed $k$-Clustering for Data with Heavy Noise Abstract: In this paper, we consider the $k$-center/median/means clustering with\noutliers problems (or the $(k, z)$-center/median/means problems) in the\ndistributed setting. Most previous distributed algorithms have their\ncommunication costs linearly depending on $z$, the number of outliers. Recently\nGuha et al. overcame this dependence issue by considering bi-criteria\napproximation algorithms that output solutions with $2z$ outliers. For the case\nwhere $z$ is large, the extra $z$ outliers discarded by the algorithms might be\ntoo large, considering that the data gathering process might be costly. In this\npaper, we improve the number of outliers to the best possible $(1+\\epsilon)z$,\nwhile maintaining the $O(1)$-approximation ratio and independence of\ncommunication cost on $z$. The problems we consider include the $(k, z)$-center\nproblem, and $(k, z)$-median/means problems in Euclidean metrics.\nImplementation of the our algorithm for $(k, z)$-center shows that it\noutperforms many previous algorithms, both in terms of the communication cost\nand quality of the output solution. \n\n"}
{"id": "1810.08038", "contents": "Title: Toward a Uniform Approach to the Unfolding of Nets Abstract: In this paper we introduce the notion of spread net. Spread nets are (safe)\nPetri nets equipped with vector clocks on places and with ticking functions on\ntransitions, and are such that vector clocks are consistent with the ticking of\ntransitions. Such nets generalize previous families of nets like unfoldings,\nmerged processes and trellis processes, and can thus be used to represent runs\nof a net in a true concurrency semantics through an operation called the\nspreading of a net. By contrast with previous constructions, which may identify\nconflicts, spread nets allow loops in time \n\n"}
{"id": "1810.08092", "contents": "Title: Deconstructing the Blockchain to Approach Physical Limits Abstract: Transaction throughput, confirmation latency and confirmation reliability are\nfundamental performance measures of any blockchain system in addition to its\nsecurity. In a decentralized setting, these measures are limited by two\nunderlying physical network attributes: communication capacity and\nspeed-of-light propagation delay. Existing systems operate far away from these\nphysical limits. In this work we introduce Prism, a new proof-of-work\nblockchain protocol, which can achieve 1) security against up to 50%\nadversarial hashing power; 2) optimal throughput up to the capacity C of the\nnetwork; 3) confirmation latency for honest transactions proportional to the\npropagation delay D, with confirmation error probability exponentially small in\nCD ; 4) eventual total ordering of all transactions. Our approach to the design\nof this protocol is based on deconstructing the blockchain into its basic\nfunctionalities and systematically scaling up these functionalities to approach\ntheir physical limits. \n\n"}
{"id": "1810.08544", "contents": "Title: New and Simplified Distributed Algorithms for Weighted All Pairs\n  Shortest Paths Abstract: We consider the problem of computing all pairs shortest paths (APSP) and\nshortest paths for k sources in a weighted graph in the distributed CONGEST\nmodel. For graphs with non-negative integer edge weights (including zero\nweights) we build on a recent pipelined algorithm to obtain\n$\\tilde{O}(\\lambda^{1/4}\\cdot n^{5/4})$ in graphs with non-negative integer\nedge-weight at most $\\lambda$, and $\\tilde{O}(n \\cdot \\bigtriangleup^{1/3})$\nrounds for shortest path distances at most $\\bigtriangleup$. Additionally, we\nsimplify some of the procedures in the earlier APSP algorithms for non-negative\nedge weights in [HNS17,ARKP18]. We also present results for computing h-hop\nshortest paths and shortest paths from $k$ given sources.\n  In other results, we present a randomized exact APSP algorithm for graphs\nwith arbitrary edge weights that runs in $\\tilde{O}(n^{4/3})$ rounds w.h.p. in\nn, which improves the previous best $\\tilde{O}(n^{3/2})$ bound, which is\ndeterministic. We also present an $\\tilde{O}(n/\\epsilon^2)$-round deterministic\n$(1+\\epsilon)$ approximation algorithm for graphs with non-negative $poly(n)$\ninteger weights (including zero edge-weights), improving results in\n[Nanongkai14,LP15] that hold only for positive integer weights. \n\n"}
{"id": "1810.10191", "contents": "Title: Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal\n  Representations for Contact-Rich Tasks Abstract: Contact-rich manipulation tasks in unstructured environments often require\nboth haptic and visual feedback. However, it is non-trivial to manually design\na robot controller that combines modalities with very different\ncharacteristics. While deep reinforcement learning has shown success in\nlearning control policies for high-dimensional inputs, these algorithms are\ngenerally intractable to deploy on real robots due to sample complexity. We use\nself-supervision to learn a compact and multimodal representation of our\nsensory inputs, which can then be used to improve the sample efficiency of our\npolicy learning. We evaluate our method on a peg insertion task, generalizing\nover different geometry, configurations, and clearances, while being robust to\nexternal perturbations. Results for simulated and real robot experiments are\npresented. \n\n"}
{"id": "1810.10499", "contents": "Title: Multi-Multi-View Learning: Multilingual and Multi-Representation Entity\n  Typing Abstract: Knowledge bases (KBs) are paramount in NLP. We employ multiview learning for\nincreasing accuracy and coverage of entity type information in KBs. We rely on\ntwo metaviews: language and representation. For language, we consider\nhigh-resource and low-resource languages from Wikipedia. For representation, we\nconsider representations based on the context distribution of the entity (i.e.,\non its embedding), on the entity's name (i.e., on its surface form) and on its\ndescription in Wikipedia. The two metaviews language and representation can be\nfreely combined: each pair of language and representation (e.g., German\nembedding, English description, Spanish name) is a distinct view. Our\nexperiments on entity typing with fine-grained classes demonstrate the\neffectiveness of multiview learning. We release MVET, a large multiview - and,\nin particular, multilingual - entity typing dataset we created. Mono- and\nmultilingual fine-grained entity typing systems can be evaluated on this\ndataset. \n\n"}
{"id": "1810.11580", "contents": "Title: Attacks Meet Interpretability: Attribute-steered Detection of\n  Adversarial Samples Abstract: Adversarial sample attacks perturb benign inputs to induce DNN misbehaviors.\nRecent research has demonstrated the widespread presence and the devastating\nconsequences of such attacks. Existing defense techniques either assume prior\nknowledge of specific attacks or may not work well on complex models due to\ntheir underlying assumptions. We argue that adversarial sample attacks are\ndeeply entangled with interpretability of DNN models: while classification\nresults on benign inputs can be reasoned based on the human perceptible\nfeatures/attributes, results on adversarial samples can hardly be explained.\nTherefore, we propose a novel adversarial sample detection technique for face\nrecognition models, based on interpretability. It features a novel\nbi-directional correspondence inference between attributes and internal neurons\nto identify neurons critical for individual attributes. The activation values\nof critical neurons are enhanced to amplify the reasoning part of the\ncomputation and the values of other neurons are weakened to suppress the\nuninterpretable part. The classification results after such transformation are\ncompared with those of the original model to detect adversaries. Results show\nthat our technique can achieve 94% detection accuracy for 7 different kinds of\nattacks with 9.91% false positives on benign inputs. In contrast, a\nstate-of-the-art feature squeezing technique can only achieve 55% accuracy with\n23.3% false positives. \n\n"}
{"id": "1810.11787", "contents": "Title: A Hitchhiker's Guide On Distributed Training of Deep Neural Networks Abstract: Deep learning has led to tremendous advancements in the field of Artificial\nIntelligence. One caveat however is the substantial amount of compute needed to\ntrain these deep learning models. Training a benchmark dataset like ImageNet on\na single machine with a modern GPU can take upto a week, distributing training\non multiple machines has been observed to drastically bring this time down.\nRecent work has brought down ImageNet training time to a time as low as 4\nminutes by using a cluster of 2048 GPUs. This paper surveys the various\nalgorithms and techniques used to distribute training and presents the current\nstate of the art for a modern distributed training framework. More\nspecifically, we explore the synchronous and asynchronous variants of\ndistributed Stochastic Gradient Descent, various All Reduce gradient\naggregation strategies and best practices for obtaining higher throughout and\nlower latency over a cluster such as mixed precision training, large batch\ntraining and gradient compression. \n\n"}
{"id": "1810.12162", "contents": "Title: Model-Based Active Exploration Abstract: Efficient exploration is an unsolved problem in Reinforcement Learning which\nis usually addressed by reactively rewarding the agent for fortuitously\nencountering novel situations. This paper introduces an efficient active\nexploration algorithm, Model-Based Active eXploration (MAX), which uses an\nensemble of forward models to plan to observe novel events. This is carried out\nby optimizing agent behaviour with respect to a measure of novelty derived from\nthe Bayesian perspective of exploration, which is estimated using the\ndisagreement between the futures predicted by the ensemble members. We show\nempirically that in semi-random discrete environments where directed\nexploration is critical to make progress, MAX is at least an order of magnitude\nmore efficient than strong baselines. MAX scales to high-dimensional continuous\nenvironments where it builds task-agnostic models that can be used for any\ndownstream task. \n\n"}
{"id": "1810.12488", "contents": "Title: Re-evaluating Continual Learning Scenarios: A Categorization and Case\n  for Strong Baselines Abstract: Continual learning has received a great deal of attention recently with\nseveral approaches being proposed. However, evaluations involve a diverse set\nof scenarios making meaningful comparison difficult. This work provides a\nsystematic categorization of the scenarios and evaluates them within a\nconsistent framework including strong baselines and state-of-the-art methods.\nThe results provide an understanding of the relative difficulty of the\nscenarios and that simple baselines (Adagrad, L2 regularization, and naive\nrehearsal strategies) can surprisingly achieve similar performance to current\nmainstream methods. We conclude with several suggestions for creating harder\nevaluation scenarios and future research directions. The code is available at\nhttps://github.com/GT-RIPL/Continual-Learning-Benchmark \n\n"}
{"id": "1810.13084", "contents": "Title: Provably Accelerated Randomized Gossip Algorithms Abstract: In this work we present novel provably accelerated gossip algorithms for\nsolving the average consensus problem. The proposed protocols are inspired from\nthe recently developed accelerated variants of the randomized Kaczmarz method -\na popular method for solving linear systems. In each gossip iteration all nodes\nof the network update their values but only a pair of them exchange their\nprivate information. Numerical experiments on popular wireless sensor networks\nshowing the benefits of our protocols are also presented. \n\n"}
{"id": "1810.13325", "contents": "Title: A Concurrent Unbounded Wait-Free Graph Abstract: In this paper, we propose an efficient concurrent wait-free algorithm to\nconstruct an unbounded directed graph for shared memory architecture. To the\nbest of our knowledge that this is the first wait-free algorithm for an\nunbounded directed graph where insertion and deletion of vertices and/or edges\ncan happen concurrently. To achieve wait-freedom in a dynamic setting, threads\nhelp each other to perform the desired tasks using operator descriptors by\nother threads. To enhance performance, we also developed an optimized wait-free\ngraph based on the principle of fast-path-slow-path. We also prove that all\ngraph operations are wait-free and linearizable. We implemented our algorithms\nin C++ and tested its performance through several micro-benchmarks. Our\nexperimental results show an average of 9x improvement over the global\nlock-based implementation. \n\n"}
{"id": "1811.00128", "contents": "Title: Towards a Simple Approach to Multi-step Model-based Reinforcement\n  Learning Abstract: When environmental interaction is expensive, model-based reinforcement\nlearning offers a solution by planning ahead and avoiding costly mistakes.\nModel-based agents typically learn a single-step transition model. In this\npaper, we propose a multi-step model that predicts the outcome of an action\nsequence with variable length. We show that this model is easy to learn, and\nthat the model can make policy-conditional predictions. We report preliminary\nresults that show a clear advantage for the multi-step model compared to its\none-step counterpart. \n\n"}
{"id": "1811.00196", "contents": "Title: Towards Explainable NLP: A Generative Explanation Framework for Text\n  Classification Abstract: Building explainable systems is a critical problem in the field of Natural\nLanguage Processing (NLP), since most machine learning models provide no\nexplanations for the predictions. Existing approaches for explainable machine\nlearning systems tend to focus on interpreting the outputs or the connections\nbetween inputs and outputs. However, the fine-grained information is often\nignored, and the systems do not explicitly generate the human-readable\nexplanations. To better alleviate this problem, we propose a novel generative\nexplanation framework that learns to make classification decisions and generate\nfine-grained explanations at the same time. More specifically, we introduce the\nexplainable factor and the minimum risk training approach that learn to\ngenerate more reasonable explanations. We construct two new datasets that\ncontain summaries, rating scores, and fine-grained reasons. We conduct\nexperiments on both datasets, comparing with several strong neural network\nbaseline systems. Experimental results show that our method surpasses all\nbaselines on both datasets, and is able to generate concise explanations at the\nsame time. \n\n"}
{"id": "1811.00222", "contents": "Title: CariGANs: Unpaired Photo-to-Caricature Translation Abstract: Facial caricature is an art form of drawing faces in an exaggerated way to\nconvey humor or sarcasm. In this paper, we propose the first Generative\nAdversarial Network (GAN) for unpaired photo-to-caricature translation, which\nwe call \"CariGANs\". It explicitly models geometric exaggeration and appearance\nstylization using two components: CariGeoGAN, which only models the\ngeometry-to-geometry transformation from face photos to caricatures, and\nCariStyGAN, which transfers the style appearance from caricatures to face\nphotos without any geometry deformation. In this way, a difficult cross-domain\ntranslation problem is decoupled into two easier tasks. The perceptual study\nshows that caricatures generated by our CariGANs are closer to the hand-drawn\nones, and at the same time better persevere the identity, compared to\nstate-of-the-art methods. Moreover, our CariGANs allow users to control the\nshape exaggeration degree and change the color/texture style by tuning the\nparameters or giving an example caricature. \n\n"}
{"id": "1811.00631", "contents": "Title: MDFS - MultiDimensional Feature Selection Abstract: Identification of informative variables in an information system is often\nperformed using simple one-dimensional filtering procedures that discard\ninformation about interactions between variables. Such approach may result in\nremoving some relevant variables from consideration. Here we present an R\npackage MDFS (MultiDimensional Feature Selection) that performs identification\nof informative variables taking into account synergistic interactions between\nmultiple descriptors and the decision variable. MDFS is an implementation of an\nalgorithm based on information theory. Computational kernel of the package is\nimplemented in C++. A high-performance version implemented in CUDA C is also\navailable. The applications of MDFS are demonstrated using the well-known\nMadelon dataset that has synergistic variables by design. The dataset comes\nfrom the UCI Machine Learning Repository. It is shown that multidimensional\nanalysis is more sensitive than one-dimensional tests and returns more reliable\nrankings of importance. \n\n"}
{"id": "1811.01118", "contents": "Title: Learning to Rank Query Graphs for Complex Question Answering over\n  Knowledge Graphs Abstract: In this paper, we conduct an empirical investigation of neural query graph\nranking approaches for the task of complex question answering over knowledge\ngraphs. We experiment with six different ranking models and propose a novel\nself-attention based slot matching model which exploits the inherent structure\nof query graphs, our logical form of choice. Our proposed model generally\noutperforms the other models on two QA datasets over the DBpedia knowledge\ngraph, evaluated in different settings. In addition, we show that transfer\nlearning from the larger of those QA datasets to the smaller dataset yields\nsubstantial improvements, effectively offsetting the general lack of training\ndata. \n\n"}
{"id": "1811.01270", "contents": "Title: Multi-Round Cooperative Search Games with Multiple Players Abstract: Assume that a treasure is placed in one of $M$ boxes according to a known\ndistribution and that $k$ searchers are searching for it in parallel during $T$\nrounds. We study the question of how to incentivize selfish players so that the\nsuccess probability, namely, the probability that at least one player finds the\ntreasure, would be maximized. We focus on congestion policies $C(s)$ that\nspecify the reward that a player receives if it is one of $s$ players that\n(simultaneously) find the treasure for the first time. Our main technical\ncontribution is proving that the exclusive policy, in which $C(1)=1$ and\n$C(s)=0$ for $s>1$, yields a price of anarchy of $(1-(1-{1}/{k})^{k})^{-1}$,\nand that this is the best possible price among all symmetric reward mechanisms.\nFor this policy we also have an explicit description of a symmetric\nequilibrium, which is in some sense unique, and moreover enjoys the best\nsuccess probability among all symmetric profiles. For general congestion\npolicies, we show how to polynomially find, for any $e>0$, a symmetric\nmultiplicative $(1+e)(1+C(k))$-equilibrium. Together with an appropriate reward\npolicy, a central entity can suggest players to play a particular profile at\nequilibrium. As our main conceptual contribution, we advocate the use of\nsymmetric equilibria for such purposes. Besides being fair, we argue that in\nmany cases, despite the fact that some small fraction of players crash,\nsymmetric equilibria remain efficient in terms of their group performances and,\nat the same time, serve as approximate equilibria. We show that this principle\nholds for a class of games, which we call monotonously scalable games. This\napplies in particular to our search game, assuming the sharing policy, in which\n$C(s)=1/s$. For the exclusive policy, this general result does not hold, but we\nshow that the symmetric equilibrium is nevertheless robust under mild\nassumptions. \n\n"}
{"id": "1811.01299", "contents": "Title: SimplerVoice: A Key Message & Visual Description Generator System for\n  Illiteracy Abstract: We introduce SimplerVoice: a key message and visual description generator\nsystem to help low-literate adults navigate the information-dense world with\nconfidence, on their own. SimplerVoice can automatically generate sensible\nsentences describing an unknown object, extract semantic meanings of the object\nusage in the form of a query string, then, represent the string as multiple\ntypes of visual guidance (pictures, pictographs, etc.). We demonstrate\nSimplerVoice system in a case study of generating grocery products' manuals\nthrough a mobile application. To evaluate, we conducted a user study on\nSimplerVoice's generated description in comparison to the information\ninterpreted by users from other methods: the original product package and\nsearch engines' top result, in which SimplerVoice achieved the highest\nperformance score: 4.82 on 5-point mean opinion score scale. Our result shows\nthat SimplerVoice is able to provide low-literate end-users with simple yet\ninformative components to help them understand how to use the grocery products,\nand that the system may potentially provide benefits in other real-world use\ncases \n\n"}
{"id": "1811.01848", "contents": "Title: Plan Online, Learn Offline: Efficient Learning and Exploration via\n  Model-Based Control Abstract: We propose a plan online and learn offline (POLO) framework for the setting\nwhere an agent, with an internal model, needs to continually act and learn in\nthe world. Our work builds on the synergistic relationship between local\nmodel-based control, global value function learning, and exploration. We study\nhow local trajectory optimization can cope with approximation errors in the\nvalue function, and can stabilize and accelerate value function learning.\nConversely, we also study how approximate value functions can help reduce the\nplanning horizon and allow for better policies beyond local solutions. Finally,\nwe also demonstrate how trajectory optimization can be used to perform\ntemporally coordinated exploration in conjunction with estimating uncertainty\nin value function approximation. This exploration is critical for fast and\nstable learning of the value function. Combining these components enable\nsolutions to complex simulated control tasks, like humanoid locomotion and\ndexterous in-hand manipulation, in the equivalent of a few minutes of\nexperience in the real world. \n\n"}
{"id": "1811.02540", "contents": "Title: Regret Circuits: Composability of Regret Minimizers Abstract: Regret minimization is a powerful tool for solving large-scale problems; it\nwas recently used in breakthrough results for large-scale extensive-form game\nsolving. This was achieved by composing simplex regret minimizers into an\noverall regret-minimization framework for extensive-form game strategy spaces.\nIn this paper we study the general composability of regret minimizers. We\nderive a calculus for constructing regret minimizers for composite convex sets\nthat are obtained from convexity-preserving operations on simpler convex sets.\nWe show that local regret minimizers for the simpler sets can be combined with\nadditional regret minimizers into an aggregate regret minimizer for the\ncomposite set. As one application, we show that the CFR framework can be\nconstructed easily from our framework. We also show ways to include curtailing\n(constraining) operations into our framework. For one, they enables the\nconstruction of CFR generalization for extensive-form games with general convex\nstrategy constraints that can cut across decision points. \n\n"}
{"id": "1811.02760", "contents": "Title: Weighted Matchings via Unweighted Augmentations Abstract: We design a generic method for reducing the task of finding weighted\nmatchings to that of finding short augmenting paths in unweighted graphs. This\nmethod enables us to provide efficient implementations for approximating\nweighted matchings in the streaming model and in the massively parallel\ncomputation (MPC) model.\n  In the context of streaming with random edge arrivals, our techniques yield a\n$(1/2+c)$-approximation algorithm thus breaking the natural barrier of $1/2$.\nFor multi-pass streaming and the MPC model, we show that any algorithm\ncomputing a $(1-\\delta)$-approximate unweighted matching in bipartite graphs\ncan be translated into an algorithm that computes a\n$(1-\\varepsilon(\\delta))$-approximate maximum weighted matching. Furthermore,\nthis translation incurs only a constant factor (that depends on $\\varepsilon>\n0$) overhead in the complexity. Instantiating this with the current best\nmulti-pass streaming and MPC algorithms for unweighted matchings yields the\nfollowing results for maximum weighted matchings:\n  * A $(1-\\varepsilon)$-approximation streaming algorithm that uses\n$O_\\varepsilon(1)$ passes and $O_\\varepsilon(n\\, \\text{poly} (\\log n))$ memory.\nThis is the first $(1-\\varepsilon)$-approximation streaming algorithm for\nweighted matchings that uses a constant number of passes (only depending on\n$\\varepsilon$).\n  * A $(1 - \\varepsilon)$-approximation algorithm in the MPC model that uses\n$O_\\varepsilon(\\log \\log n)$ rounds, $O(m/n)$ machines per round, and\n$O_\\varepsilon(n\\, \\text{poly}(\\log n))$ memory per machine. This improves upon\nthe previous best approximation guarantee of $(1/2-\\varepsilon)$ for weighted\ngraphs. \n\n"}
{"id": "1811.02883", "contents": "Title: SCALE-Sim: Systolic CNN Accelerator Simulator Abstract: Systolic Arrays are one of the most popular compute substrates within Deep\nLearning accelerators today, as they provide extremely high efficiency for\nrunning dense matrix multiplications. However, the research community lacks\ntools to insights on both the design trade-offs and efficient mapping\nstrategies for systolic-array based accelerators. We introduce Systolic CNN\nAccelerator Simulator (SCALE-Sim), which is a configurable systolic array based\ncycle accurate DNN accelerator simulator. SCALE-Sim exposes various\nmicro-architectural features as well as system integration parameters to the\ndesigner to enable comprehensive design space exploration. This is the first\nsystolic-array simulator tuned for running DNNs to the best of our knowledge.\nUsing SCALE-Sim, we conduct a suite of case studies and demonstrate the effect\nof bandwidth, data flow and aspect ratio on the overall runtime and energy of\nDeep Learning kernels across vision, speech, text, and games. We believe that\nthese insights will be highly beneficial to architects and ML practitioners. \n\n"}
{"id": "1811.03205", "contents": "Title: Robustness of Conditional GANs to Noisy Labels Abstract: We study the problem of learning conditional generators from noisy labeled\nsamples, where the labels are corrupted by random noise. A standard training of\nconditional GANs will not only produce samples with wrong labels, but also\ngenerate poor quality samples. We consider two scenarios, depending on whether\nthe noise model is known or not. When the distribution of the noise is known,\nwe introduce a novel architecture which we call Robust Conditional GAN (RCGAN).\nThe main idea is to corrupt the label of the generated sample before feeding to\nthe adversarial discriminator, forcing the generator to produce samples with\nclean labels. This approach of passing through a matching noisy channel is\njustified by corresponding multiplicative approximation bounds between the loss\nof the RCGAN and the distance between the clean real distribution and the\ngenerator distribution. This shows that the proposed approach is robust, when\nused with a carefully chosen discriminator architecture, known as projection\ndiscriminator. When the distribution of the noise is not known, we provide an\nextension of our architecture, which we call RCGAN-U, that learns the noise\nmodel simultaneously while training the generator. We show experimentally on\nMNIST and CIFAR-10 datasets that both the approaches consistently improve upon\nbaseline approaches, and RCGAN-U closely matches the performance of RCGAN. \n\n"}
{"id": "1811.03433", "contents": "Title: Explainable cardiac pathology classification on cine MRI with motion\n  characterization by semi-supervised learning of apparent flow Abstract: We propose a method to classify cardiac pathology based on a novel approach\nto extract image derived features to characterize the shape and motion of the\nheart. An original semi-supervised learning procedure, which makes efficient\nuse of a large amount of non-segmented images and a small amount of images\nsegmented manually by experts, is developed to generate pixel-wise apparent\nflow between two time points of a 2D+t cine MRI image sequence. Combining the\napparent flow maps and cardiac segmentation masks, we obtain a local apparent\nflow corresponding to the 2D motion of myocardium and ventricular cavities.\nThis leads to the generation of time series of the radius and thickness of\nmyocardial segments to represent cardiac motion. These time series of motion\nfeatures are reliable and explainable characteristics of pathological cardiac\nmotion. Furthermore, they are combined with shape-related features to classify\ncardiac pathologies. Using only nine feature values as input, we propose an\nexplainable, simple and flexible model for pathology classification. On ACDC\ntraining set and testing set, the model achieves 95% and 94% respectively as\nclassification accuracy. Its performance is hence comparable to that of the\nstate-of-the-art. Comparison with various other models is performed to outline\nsome advantages of our model. \n\n"}
{"id": "1811.03868", "contents": "Title: Suggesting Cooking Recipes Through Simulation and Bayesian Optimization Abstract: Cooking typically involves a plethora of decisions about ingredients and\ntools that need to be chosen in order to write a good cooking recipe. Cooking\ncan be modelled in an optimization framework, as it involves a search space of\ningredients, kitchen tools, cooking times or temperatures. If we model as an\nobjective function the quality of the recipe, several problems arise. No\nanalytical expression can model all the recipes, so no gradients are available.\nThe objective function is subjective, in other words, it contains noise.\nMoreover, evaluations are expensive both in time and human resources. Bayesian\nOptimization (BO) emerges as an ideal methodology to tackle problems with these\ncharacteristics. In this paper, we propose a methodology to suggest recipe\nrecommendations based on a Machine Learning (ML) model that fits real and\nsimulated data and BO. We provide empirical evidence with two experiments that\nsupport the adequacy of the methodology. \n\n"}
{"id": "1811.04801", "contents": "Title: On Weisfeiler-Leman Invariance: Subgraph Counts and Related Graph\n  Properties Abstract: The $k$-dimensional Weisfeiler-Leman algorithm ($k$-WL) is a fruitful\napproach to the Graph Isomorphism problem. 2-WL corresponds to the original\nalgorithm suggested by Weisfeiler and Leman over 50 years ago. 1-WL is the\nclassical color refinement routine. Indistinguishability by $k$-WL is an\nequivalence relation on graphs that is of fundamental importance for\nisomorphism testing, descriptive complexity theory, and graph similarity\ntesting which is also of some relevance in artificial intelligence. Focusing on\ndimensions $k=1,2$, we investigate subgraph patterns whose counts are $k$-WL\ninvariant, and whose occurrence is $k$-WL invariant. We achieve a complete\ndescription of all such patterns for dimension $k=1$ and considerably extend\nthe previous results known for $k=2$. \n\n"}
{"id": "1811.05013", "contents": "Title: Blindfold Baselines for Embodied QA Abstract: We explore blindfold (question-only) baselines for Embodied Question\nAnswering. The EmbodiedQA task requires an agent to answer a question by\nintelligently navigating in a simulated environment, gathering necessary visual\ninformation only through first-person vision before finally answering.\nConsequently, a blindfold baseline which ignores the environment and visual\ninformation is a degenerate solution, yet we show through our experiments on\nthe EQAv1 dataset that a simple question-only baseline achieves\nstate-of-the-art results on the EmbodiedQA task in all cases except when the\nagent is spawned extremely close to the object. \n\n"}
{"id": "1811.05685", "contents": "Title: Layout Design for Intelligent Warehouse by Evolution with Fitness\n  Approximation Abstract: With the rapid growth of the express industry, intelligent warehouses that\nemploy autonomous robots for carrying parcels have been widely used to handle\nthe vast express volume. For such warehouses, the warehouse layout design plays\na key role in improving the transportation efficiency. However, this work is\nstill done by human experts, which is expensive and leads to suboptimal\nresults. In this paper, we aim to automate the warehouse layout designing\nprocess. We propose a two-layer evolutionary algorithm to efficiently explore\nthe warehouse layout space, where an auxiliary objective fitness approximation\nmodel is introduced to predict the outcome of the designed warehouse layout and\na two-layer population structure is proposed to incorporate the approximation\nmodel into the ordinary evolution framework. Empirical experiments show that\nour method can efficiently design effective warehouse layouts that outperform\nboth heuristic-designed and vanilla evolution-designed warehouse layouts. \n\n"}
{"id": "1811.07029", "contents": "Title: Modelling the Dynamic Joint Policy of Teammates with Attention\n  Multi-agent DDPG Abstract: Modelling and exploiting teammates' policies in cooperative multi-agent\nsystems have long been an interest and also a big challenge for the\nreinforcement learning (RL) community. The interest lies in the fact that if\nthe agent knows the teammates' policies, it can adjust its own policy\naccordingly to arrive at proper cooperations; while the challenge is that the\nagents' policies are changing continuously due to they are learning\nconcurrently, which imposes difficulty to model the dynamic policies of\nteammates accurately. In this paper, we present \\emph{ATTention Multi-Agent\nDeep Deterministic Policy Gradient} (ATT-MADDPG) to address this challenge.\nATT-MADDPG extends DDPG, a single-agent actor-critic RL method, with two\nspecial designs. First, in order to model the teammates' policies, the agent\nshould get access to the observations and actions of teammates. ATT-MADDPG\nadopts a centralized critic to collect such information. Second, to model the\nteammates' policies using the collected information in an effective way,\nATT-MADDPG enhances the centralized critic with an attention mechanism. This\nattention mechanism introduces a special structure to explicitly model the\ndynamic joint policy of teammates, making sure that the collected information\ncan be processed efficiently. We evaluate ATT-MADDPG on both benchmark tasks\nand the real-world packet routing tasks. Experimental results show that it not\nonly outperforms the state-of-the-art RL-based methods and rule-based methods\nby a large margin, but also achieves better performance in terms of scalability\nand robustness. \n\n"}
{"id": "1811.07308", "contents": "Title: A Variational Dirichlet Framework for Out-of-Distribution Detection Abstract: With the recently rapid development in deep learning, deep neural networks\nhave been widely adopted in many real-life applications. However, deep neural\nnetworks are also known to have very little control over its uncertainty for\nunseen examples, which potentially causes very harmful and annoying\nconsequences in practical scenarios. In this paper, we are particularly\ninterested in designing a higher-order uncertainty metric for deep neural\nnetworks and investigate its effectiveness under the out-of-distribution\ndetection task proposed by~\\cite{hendrycks2016baseline}. Our method first\nassumes there exists an underlying higher-order distribution $\\mathbb{P}(z)$,\nwhich controls label-wise categorical distribution $\\mathbb{P}(y)$ over classes\non the K-dimension simplex, and then approximate such higher-order distribution\nvia parameterized posterior function $p_{\\theta}(z|x)$ under variational\ninference framework, finally we use the entropy of learned posterior\ndistribution $p_{\\theta}(z|x)$ as uncertainty measure to detect\nout-of-distribution examples. Further, we propose an auxiliary objective\nfunction to discriminate against synthesized adversarial examples to further\nincrease the robustness of the proposed uncertainty measure. Through\ncomprehensive experiments on various datasets, our proposed framework is\ndemonstrated to consistently outperform competing algorithms. \n\n"}
{"id": "1811.07407", "contents": "Title: Multimodal Densenet Abstract: Humans make accurate decisions by interpreting complex data from multiple\nsources. Medical diagnostics, in particular, often hinge on human\ninterpretation of multi-modal information. In order for artificial intelligence\nto make progress in automated, objective, and accurate diagnosis and prognosis,\nmethods to fuse information from multiple medical imaging modalities are\nrequired. However, combining information from multiple data sources has several\nchallenges, as current deep learning architectures lack the ability to extract\nuseful representations from multimodal information, and often simple\nconcatenation is used to fuse such information. In this work, we propose\nMultimodal DenseNet, a novel architecture for fusing multimodal data. Instead\nof focusing on concatenation or early and late fusion, our proposed\narchitectures fuses information over several layers and gives the model\nflexibility in how it combines information from multiple sources. We apply this\narchitecture to the challenge of polyp characterization and landmark\nidentification in endoscopy. Features from white light images are fused with\nfeatures from narrow band imaging or depth maps. This study demonstrates that\nMultimodal DenseNet outperforms monomodal classification as well as other\nmultimodal fusion techniques by a significant margin on two different datasets. \n\n"}
{"id": "1811.08186", "contents": "Title: Analysing Results from AI Benchmarks: Key Indicators and How to Obtain\n  Them Abstract: Item response theory (IRT) can be applied to the analysis of the evaluation\nof results from AI benchmarks. The two-parameter IRT model provides two\nindicators (difficulty and discrimination) on the side of the item (or AI\nproblem) while only one indicator (ability) on the side of the respondent (or\nAI agent). In this paper we analyse how to make this set of indicators dual, by\nadding a fourth indicator, generality, on the side of the respondent.\nGenerality is meant to be dual to discrimination, and it is based on\ndifficulty. Namely, generality is defined as a new metric that evaluates\nwhether an agent is consistently good at easy problems and bad at difficult\nones. With the addition of generality, we see that this set of four key\nindicators can give us more insight on the results of AI benchmarks. In\nparticular, we explore two popular benchmarks in AI, the Arcade Learning\nEnvironment (Atari 2600 games) and the General Video Game AI competition. We\nprovide some guidelines to estimate and interpret these indicators for other AI\nbenchmarks and competitions. \n\n"}
{"id": "1811.08596", "contents": "Title: SuperNeurons: FFT-based Gradient Sparsification in the Distributed\n  Training of Deep Neural Networks Abstract: The performance and efficiency of distributed training of Deep Neural\nNetworks highly depend on the performance of gradient averaging among all\nparticipating nodes, which is bounded by the communication between nodes. There\nare two major strategies to reduce communication overhead: one is to hide\ncommunication by overlapping it with computation, and the other is to reduce\nmessage sizes. The first solution works well for linear neural architectures,\nbut latest networks such as ResNet and Inception offer limited opportunity for\nthis overlapping. Therefore, researchers have paid more attention to minimizing\ncommunication. In this paper, we present a novel gradient compression framework\nderived from insights of real gradient distributions, and which strikes a\nbalance between compression ratio, accuracy, and computational overhead. Our\nframework has two major novel components: sparsification of gradients in the\nfrequency domain, and a range-based floating point representation to quantize\nand further compress gradients frequencies. Both components are dynamic, with\ntunable parameters that achieve different compression ratio based on the\naccuracy requirement and systems' platforms, and achieve very high throughput\non GPUs. We prove that our techniques guarantee the convergence with a\ndiminishing compression ratio. Our experiments show that the proposed\ncompression framework effectively improves the scalability of most popular\nneural networks on a 32 GPU cluster to the baseline of no compression, without\ncompromising the accuracy and convergence speed. \n\n"}
{"id": "1811.09047", "contents": "Title: Fog Computing Architecture: Survey and Challenges Abstract: Emerging technologies that generate a huge amount of data such as the\nInternet of Things (IoT) services need latency aware computing platforms to\nsupport time-critical applications. Due to the on-demand services and\nscalability features of cloud computing, Big Data application processing is\ndone in the cloud infrastructure. Managing Big Data applications exclusively in\nthe cloud is not an efficient solution for latency-sensitive applications\nrelated to smart transportation systems, healthcare solutions, emergency\nresponse systems and content delivery applications. Thus, the Fog computing\nparadigm that allows applications to perform computing operations in-between\nthe cloud and the end devices has emerged. In Fog architecture, IoT devices and\nsensors are connected to the Fog devices which are located in close proximity\nto the users and it is also responsible for intermediate computation and\nstorage. Most computations will be done on the edge by eliminating full\ndependencies on the cloud resources. In this chapter, we investigate and survey\nFog computing architectures which have been proposed over the past few years.\nMoreover, we study the requirements of IoT applications and platforms, and the\nlimitations faced by cloud systems when executing IoT applications. Finally, we\nreview current research works that particularly focus on Big Data application\nexecution on Fog and address several open challenges as well as future research\ndirections. \n\n"}
{"id": "1811.11190", "contents": "Title: Semantically-aware population health risk analyses Abstract: One primary task of population health analysis is the identification of risk\nfactors that, for some subpopulation, have a significant association with some\nhealth condition. Examples include finding lifestyle factors associated with\nchronic diseases and finding genetic mutations associated with diseases in\nprecision health. We develop a combined semantic and machine learning system\nthat uses a health risk ontology and knowledge graph (KG) to dynamically\ndiscover risk factors and their associated subpopulations. Semantics and the\nnovel supervised cadre model make our system explainable. Future population\nhealth studies are easily performed and documented with provenance by\nspecifying additional input and output KG cartridges. \n\n"}
{"id": "1811.11707", "contents": "Title: Few-Shot Generalization Across Dialogue Tasks Abstract: Machine-learning based dialogue managers are able to learn complex behaviors\nin order to complete a task, but it is not straightforward to extend their\ncapabilities to new domains. We investigate different policies' ability to\nhandle uncooperative user behavior, and how well expertise in completing one\ntask (such as restaurant reservations) can be reapplied when learning a new one\n(e.g. booking a hotel). We introduce the Recurrent Embedding Dialogue Policy\n(REDP), which embeds system actions and dialogue states in the same vector\nspace. REDP contains a memory component and attention mechanism based on a\nmodified Neural Turing Machine, and significantly outperforms a baseline LSTM\nclassifier on this task. We also show that both our architecture and baseline\nsolve the bAbI dialogue task, achieving 100% test accuracy. \n\n"}
{"id": "1811.12354", "contents": "Title: Touchdown: Natural Language Navigation and Spatial Reasoning in Visual\n  Street Environments Abstract: We study the problem of jointly reasoning about language and vision through a\nnavigation and spatial reasoning task. We introduce the Touchdown task and\ndataset, where an agent must first follow navigation instructions in a\nreal-life visual urban environment, and then identify a location described in\nnatural language to find a hidden object at the goal position. The data\ncontains 9,326 examples of English instructions and spatial descriptions paired\nwith demonstrations. Empirical analysis shows the data presents an open\nchallenge to existing methods, and qualitative linguistic analysis shows that\nthe data displays richer use of spatial reasoning compared to related\nresources. \n\n"}
{"id": "1811.12560", "contents": "Title: An Introduction to Deep Reinforcement Learning Abstract: Deep reinforcement learning is the combination of reinforcement learning (RL)\nand deep learning. This field of research has been able to solve a wide range\nof complex decision-making tasks that were previously out of reach for a\nmachine. Thus, deep RL opens up many new applications in domains such as\nhealthcare, robotics, smart grids, finance, and many more. This manuscript\nprovides an introduction to deep reinforcement learning models, algorithms and\ntechniques. Particular focus is on the aspects related to generalization and\nhow deep RL can be used for practical applications. We assume the reader is\nfamiliar with basic machine learning concepts. \n\n"}
{"id": "1811.12787", "contents": "Title: A Tutorial for Weighted Bipolar Argumentation with Continuous Dynamical\n  Systems and the Java Library Attractor Abstract: Weighted bipolar argumentation frameworks allow modeling decision problems\nand online discussions by defining arguments and their relationships. The\nstrength of arguments can be computed based on an initial weight and the\nstrength of attacking and supporting arguments. While previous approaches\nassumed an acyclic argumentation graph and successively set arguments' strength\nbased on the strength of their parents, recently continuous dynamical systems\nhave been proposed as an alternative. Continuous models update arguments'\nstrength simultaneously and continuously. While there are currently no\nanalytical guarantees for convergence in general graphs, experiments show that\ncontinuous models can converge quickly in large cyclic graphs with thousands of\narguments. Here, we focus on the high-level ideas of this approach and explain\nkey results and applications. We also introduce Attractor, a Java library that\ncan be used to solve weighted bipolar argumentation problems. Attractor\ncontains implementations of several discrete and continuous models and\nnumerical algorithms to compute solutions. It also provides base classes that\ncan be used to implement, to evaluate and to compare continuous models easily. \n\n"}
{"id": "1811.12941", "contents": "Title: On the Computational Inefficiency of Large Batch Sizes for Stochastic\n  Gradient Descent Abstract: Increasing the mini-batch size for stochastic gradient descent offers\nsignificant opportunities to reduce wall-clock training time, but there are a\nvariety of theoretical and systems challenges that impede the widespread\nsuccess of this technique. We investigate these issues, with an emphasis on\ntime to convergence and total computational cost, through an extensive\nempirical analysis of network training across several architectures and problem\ndomains, including image classification, image segmentation, and language\nmodeling. Although it is common practice to increase the batch size in order to\nfully exploit available computational resources, we find a substantially more\nnuanced picture. Our main finding is that across a wide range of network\narchitectures and problem domains, increasing the batch size beyond a certain\npoint yields no decrease in wall-clock time to convergence for \\emph{either}\ntrain or test loss. This batch size is usually substantially below the capacity\nof current systems. We show that popular training strategies for large batch\nsize optimization begin to fail before we can populate all available compute\nresources, and we show that the point at which these methods break down depends\nmore on attributes like model architecture and data complexity than it does\ndirectly on the size of the dataset. \n\n"}
{"id": "1812.01161", "contents": "Title: A Spectral Regularizer for Unsupervised Disentanglement Abstract: A generative model with a disentangled representation allows for independent\ncontrol over different aspects of the output. Learning disentangled\nrepresentations has been a recent topic of great interest, but it remains\npoorly understood. We show that even for GANs that do not possess disentangled\nrepresentations, one can find curved trajectories in latent space over which\nlocal disentanglement occurs. These trajectories are found by iteratively\nfollowing the leading right-singular vectors of the Jacobian of the generator\nwith respect to its input. Based on this insight, we describe an efficient\nregularizer that aligns these vectors with the coordinate axes, and show that\nit can be used to induce disentangled representations in GANs, in a completely\nunsupervised manner. \n\n"}
{"id": "1812.01214", "contents": "Title: Prototype-based Neural Network Layers: Incorporating Vector Quantization Abstract: Neural networks currently dominate the machine learning community and they do\nso for good reasons. Their accuracy on complex tasks such as image\nclassification is unrivaled at the moment and with recent improvements they are\nreasonably easy to train. Nevertheless, neural networks are lacking robustness\nand interpretability. Prototype-based vector quantization methods on the other\nhand are known for being robust and interpretable. For this reason, we propose\ntechniques and strategies to merge both approaches. This contribution will\nparticularly highlight the similarities between them and outline how to\nconstruct a prototype-based classification layer for multilayer networks.\nAdditionally, we provide an alternative, prototype-based, approach to the\nclassical convolution operation. Numerical results are not part of this report,\ninstead the focus lays on establishing a strong theoretical framework. By\npublishing our framework and the respective theoretical considerations and\njustifications before finalizing our numerical experiments we hope to\njump-start the incorporation of prototype-based learning in neural networks and\nvice versa. \n\n"}
{"id": "1812.01804", "contents": "Title: Random Spiking and Systematic Evaluation of Defenses Against Adversarial\n  Examples Abstract: Image classifiers often suffer from adversarial examples, which are generated\nby strategically adding a small amount of noise to input images to trick\nclassifiers into misclassification. Over the years, many defense mechanisms\nhave been proposed, and different researchers have made seemingly contradictory\nclaims on their effectiveness. We present an analysis of possible adversarial\nmodels, and propose an evaluation framework for comparing different defense\nmechanisms. As part of the framework, we introduce a more powerful and\nrealistic adversary strategy. Furthermore, we propose a new defense mechanism\ncalled Random Spiking (RS), which generalizes dropout and introduces random\nnoises in the training process in a controlled manner. Evaluations under our\nproposed framework suggest RS delivers better protection against adversarial\nexamples than many existing schemes. \n\n"}
{"id": "1812.01837", "contents": "Title: ADARES: Adaptive Resource Management for Virtual Machines Abstract: Virtual execution environments allow for consolidation of multiple\napplications onto the same physical server, thereby enabling more efficient use\nof server resources. However, users often statically configure the resources of\nvirtual machines through guesswork, resulting in either insufficient resource\nallocations that hinder VM performance, or excessive allocations that waste\nprecious data center resources. In this paper, we first characterize real-world\nresource allocation and utilization of VMs through the analysis of an extensive\ndataset, consisting of more than 250k VMs from over 3.6k private enterprise\nclusters. Our large-scale analysis confirms that VMs are often misconfigured,\neither overprovisioned or underprovisioned, and that this problem is pervasive\nacross a wide range of private clusters. We then propose ADARES, an adaptive\nsystem that dynamically adjusts VM resources using machine learning techniques.\nIn particular, ADARES leverages the contextual bandits framework to effectively\nmanage the adaptations. Our system exploits easily collectible data, at the\ncluster, node, and VM levels, to make more sensible allocation decisions, and\nuses transfer learning to safely explore the configurations space and speed up\ntraining. Our empirical evaluation shows that ADARES can significantly improve\nsystem utilization without sacrificing performance. For instance, when compared\nto threshold and prediction-based baselines, it achieves more predictable\nVM-level performance and also reduces the amount of virtual CPUs and memory\nprovisioned by up to 35% and 60% respectively for synthetic workloads on real\nclusters. \n\n"}
{"id": "1812.01853", "contents": "Title: Termination of $\\lambda$$\\Pi$ modulo rewriting using the size-change\n  principle (work in progress) Abstract: The Size-Change Termination principle was first introduced to study the\ntermination of first-order functional programs. In this work, we show that it\ncan also be used to study the termination of higher-order rewriting in a system\nof dependent types extending LF. \n\n"}
{"id": "1812.03651", "contents": "Title: Serverless Computing: One Step Forward, Two Steps Back Abstract: Serverless computing offers the potential to program the cloud in an\nautoscaling, pay-as-you go manner. In this paper we address critical gaps in\nfirst-generation serverless computing, which place its autoscaling potential at\nodds with dominant trends in modern computing: notably data-centric and\ndistributed computing, but also open source and custom hardware. Put together,\nthese gaps make current serverless offerings a bad fit for cloud innovation and\nparticularly bad for data systems innovation. In addition to pinpointing some\nof the main shortfalls of current serverless architectures, we raise a set of\nchallenges we believe must be met to unlock the radical potential that the\ncloud---with its exabytes of storage and millions of cores---should offer to\ninnovative developers. \n\n"}
{"id": "1812.04048", "contents": "Title: Compressed Distributed Gradient Descent: Communication-Efficient\n  Consensus over Networks Abstract: Network consensus optimization has received increasing attention in recent\nyears and has found important applications in many scientific and engineering\nfields. To solve network consensus optimization problems, one of the most\nwell-known approaches is the distributed gradient descent method (DGD).\nHowever, in networks with slow communication rates, DGD's performance is\nunsatisfactory for solving high-dimensional network consensus problems due to\nthe communication bottleneck. This motivates us to design a\ncommunication-efficient DGD-type algorithm based on compressed information\nexchanges. Our contributions in this paper are three-fold: i) We develop a\ncommunication-efficient algorithm called amplified-differential compression DGD\n(ADC-DGD) and show that it converges under {\\em any} unbiased compression\noperator; ii) We rigorously prove the convergence performances of ADC-DGD and\nshow that they match with those of DGD without compression; iii) We reveal an\ninteresting phase transition phenomenon in the convergence speed of ADC-DGD.\nCollectively, our findings advance the state-of-the-art of network consensus\noptimization theory. \n\n"}
{"id": "1812.04070", "contents": "Title: SIMD-X: Programming and Processing of Graph Algorithms on GPUs Abstract: With high computation power and memory bandwidth, graphics processing units\n(GPUs) lend themselves to accelerate data-intensive analytics, especially when\nsuch applications fit the single instruction multiple data (SIMD) model.\nHowever, graph algorithms such as breadth-first search and k-core, often fail\nto take full advantage of GPUs, due to irregularity in memory access and\ncontrol flow. To address this challenge, we have developed SIMD-X, for\nprogramming and processing of single instruction multiple, complex, data on\nGPUs. Specifically, the new Active-Compute-Combine (ACC) model not only\nprovides ease of programming to programmers, but more importantly creates\nopportunities for system-level optimizations. To this end, SIMD-X utilizes\njust-in-time task management which filters out inactive vertices at runtime and\nintelligently maps various tasks to different amount of GPU cores in pursuit of\nworkload balancing. In addition, SIMD-X leverages push-pull based kernel fusion\nthat, with the help of a new deadlock-free global barrier, reduces a large\nnumber of computation kernels to very few. Using SIMD-X, a user can program a\ngraph algorithm in tens of lines of code, while achieving 3?, 6?, 24?, 3?\nspeedup over Gunrock, Galois, CuSha, and Ligra, respectively. \n\n"}
{"id": "1812.04380", "contents": "Title: DRONE: a Distributed Subgraph-Centric Framework for Processing Large\n  Scale Power-law Graphs Abstract: Nowadays, in the big data era, social networks, graph databases, knowledge\ngraphs, electronic commerce etc. demand efficient and scalable capability to\nprocess an ever increasing volume of graph-structured data. To meet the\nchallenge, two mainstream distributed programming models, vertex-centric (VC)\nand subgraph-centric (SC) were proposed. Compared to the VC model, the SC model\nconverges faster with less communication overhead on well-partitioned graphs,\nand is easy to program due to the \"think like a graph\" philosophy. The edge-cut\nmethod is considered as a natural choice of subgraph-centric model for graph\npartitioning, and has been adopted by Giraph++, Blogel and GRAPE. However, the\nedge-cut method causes significant performance bottleneck for processing large\nscale power-law graphs. Thus, the SC model is less competitive in practice. In\nthis paper, we present an innovative distributed graph computing framework,\nDRONE (Distributed gRaph cOmputiNg Engine). It combines the subgraph-centric\nmodel and the vertex-cut graph partitioning strategy. Experiments show that\nDRONE outperforms the state-of-art distributed graph computing engines on\nreal-world graphs and synthetic power-law graphs. DRONE is capable of scaling\nup to process one-trillion-edge synthetic power-law graphs, which is orders of\nmagnitude larger than previously reported by existing SC-based frameworks. \n\n"}
{"id": "1812.05070", "contents": "Title: Enhancing Selection Hyper-heuristics via Feature Transformations Abstract: Hyper-heuristics are a novel tool. They deal with complex optimization\nproblems where standalone solvers exhibit varied performance. Among such a tool\nreside selection hyper-heuristics. By combining the strengths of each solver,\nthis kind of hyper-heuristic offers a more robust tool. However, their\neffectiveness is highly dependent on the 'features' used to link them with the\nproblem that is being solved. Aiming at enhancing selection hyper-heuristics,\nin this paper we propose two types of transformation: explicit and implicit.\nThe first one directly changes the distribution of critical points within the\nfeature domain while using a Euclidean distance to measure proximity. The\nsecond one operates indirectly by preserving the distribution of critical\npoints but changing the distance metric through a kernel function. We focus on\nanalyzing the effect of each kind of transformation, and of their combinations.\nWe test our ideas in the domain of constraint satisfaction problems because of\ntheir popularity and many practical applications. In this work, we compare the\nperformance of our proposals against those of previously published data.\nFurthermore, we expand on previous research by increasing the number of\nanalyzed features. We found that, by incorporating transformations into the\nmodel of selection hyper-heuristics, overall performance can be improved,\nyielding more stable results. However, combining implicit and explicit\ntransformations was not as fruitful. Additionally, we ran some confirmatory\ntests on the domain of knapsack problems. Again, we observed improved\nstability, leading to the generation of hyper-heuristics whose profit had a\nstandard deviation between 20% and 30% smaller. \n\n"}
{"id": "1812.05352", "contents": "Title: Efficient Dispersion of Mobile Robots on Arbitrary Graphs and Grids Abstract: The mobile robot dispersion problem on graphs asks $k\\leq n$ robots placed\ninitially arbitrarily on the nodes of an $n$-node anonymous graph to reposition\nautonomously to reach a configuration in which each robot is on a distinct node\nof the graph. This problem is of significant interest due to its relationship\nto other fundamental robot coordination problems, such as exploration,\nscattering, load balancing, and relocation of self-driven electric cars\n(robots) to recharge stations (nodes). In this paper, we provide two novel\ndeterministic algorithms for dispersion, one for arbitrary graphs and another\nfor grid graphs, in a synchronous setting where all robots perform their\nactions in every time step. Our algorithm for arbitrary graphs has\n$O(\\min(m,k\\Delta) \\cdot \\log k)$ steps runtime using $O(\\log n)$ bits of\nmemory at each robot, where $m$ is the number of edges and $\\Delta$ is the\nmaximum degree of the graph. This is an exponential improvement over the\n$O(mk)$ steps best previously known algorithm. In particular, the runtime of\nour algorithm is optimal (up to a $O(\\log k)$ factor) in constant-degree\narbitrary graphs. Our algorithm for grid graphs has $O(\\min(k,\\sqrt{n}))$ steps\nruntime using $\\Theta(\\log k)$ bits at each robot. This is the first algorithm\nfor dispersion in grid graphs. Moreover, this algorithm is optimal for both\nmemory and time when $k=\\Omega(n)$. \n\n"}
{"id": "1812.05444", "contents": "Title: Pluralize: a Trustworthy Framework for High-Level Smart Contract-Draft Abstract: The paper presents Pluralize a formal logical framework able to extend the\nexecution of blockchain transactions to events coming from external oracles,\nlike external time, sensor data, human-made declarations, etc. These events are\nby essence non-reliable, since transaction execution can be triggered by\ninformation whose veracity cannot be established by the blockchain. To overcome\nthis problem, the language features a first-order logic and an authority\nalgebra to allow formal reasoning and establish accountability of agents for\nblockchain-enabled transactions. We provide an accountability model that allows\nto formally prove the accountability of agents by a formal proof locally\nexecutable by each agent of the blockchain. \n\n"}
{"id": "1812.05765", "contents": "Title: Graphical Regular Logic Abstract: Regular logic can be regarded as the internal language of regular categories,\nbut the logic itself is generally not given a categorical treatment. In this\npaper, we understand the syntax and proof rules of regular logic in terms of\nthe free regular category $\\mathsf{FRg}(\\mathrm{T})$ on a set $\\mathrm{T}$.\nFrom this point of view, regular theories are certain monoidal 2-functors from\na suitable 2-category of contexts---the 2-category of relations in\n$\\mathsf{FRg}(\\mathrm{T})$---to the 2-category of posets. Such functors assign\nto each context the set of formulas in that context, ordered by entailment. We\nrefer to such a 2-functor as a regular calculus because it naturally gives rise\nto a graphical string diagram calculus in the spirit of Joyal and Street. Our\nkey aim to prove that the category of regular categories is essentially\nreflective in that of regular calculi. Along the way, we demonstrate how to use\nthis graphical calculus. \n\n"}
{"id": "1812.06492", "contents": "Title: Performance Evaluation of Big Data Processing Strategies for\n  Neuroimaging Abstract: Neuroimaging datasets are rapidly growing in size as a result of advancements\nin image acquisition methods, open-science and data sharing. However, the\nadoption of Big Data processing strategies by neuroimaging processing engines\nremains limited. Here, we evaluate three Big Data processing strategies\n(in-memory computing, data locality and lazy evaluation) on typical\nneuroimaging use cases, represented by the BigBrain dataset. We contrast these\nvarious strategies using Apache Spark and Nipype as our representative Big Data\nand neuroimaging processing engines, on Dell EMC's Top-500 cluster. Big Data\nthresholds were modelled by comparing the data-write rate of the application to\nthe filesystem bandwidth and number of concurrent processes. This model\nacknowledges the fact that page caching provided by the Linux kernel is\ncritical to the performance of Big Data applications. Results show that\nin-memory computing alone speeds-up executions by a factor of up to 1.6,\nwhereas when combined with data locality, this factor reaches 5.3. Lazy\nevaluation strategies were found to increase the likelihood of cache hits,\nfurther improving processing time. Such important speed-up values are likely to\nbe observed on typical image processing operations performed on images of size\nlarger than 75GB. A ballpark speculation from our model showed that in-memory\ncomputing alone will not speed-up current functional MRI analyses unless\ncoupled with data locality and processing around 280 subjects concurrently.\nFurthermore, we observe that emulating in-memory computing using in-memory file\nsystems (tmpfs) does not reach the performance of an in-memory engine,\npresumably due to swapping to disk and the lack of data cleanup. We conclude\nthat Big Data processing strategies are worth developing for neuroimaging\napplications. \n\n"}
{"id": "1812.06535", "contents": "Title: Deep Clustering Based on a Mixture of Autoencoders Abstract: In this paper we propose a Deep Autoencoder MIxture Clustering (DAMIC)\nalgorithm based on a mixture of deep autoencoders where each cluster is\nrepresented by an autoencoder. A clustering network transforms the data into\nanother space and then selects one of the clusters. Next, the autoencoder\nassociated with this cluster is used to reconstruct the data-point. The\nclustering algorithm jointly learns the nonlinear data representation and the\nset of autoencoders. The optimal clustering is found by minimizing the\nreconstruction loss of the mixture of autoencoder network. Unlike other deep\nclustering algorithms, no regularization term is needed to avoid data\ncollapsing to a single point. Our experimental evaluations on image and text\ncorpora show significant improvement over state-of-the-art methods. \n\n"}
{"id": "1812.08806", "contents": "Title: Blockchain and Cryptocurrency: A comparative framework of the main\n  Architectural Drivers Abstract: Blockchain is a decentralized transaction and data management solution, the\ntechnological weapon-of-choice behind the success of Bitcoin and other\ncryptocurrencies. As the number and variety of existing blockchain\nimplementations continues to increase, adopters should focus on selecting the\nbest one to support their decentralized applications (dApps), rather than\ndeveloping new ones from scratch. In this paper we present a framework to aid\nsoftware architects, developers, tool selectors and decision makers to adopt\nthe right blockchain technology for their problem at hand. The framework\nexposes the correlation between technological decisions and architectural\nfeatures, capturing the knowledge from existing industrial products, technical\nforums/blogs, experts' feedback and academic literature; plus our own\nexperience using and developing blockchain-based applications. We validate our\nframework by applying it to dissect the most outstanding blockchain platforms,\ni.e., the ones behind the top 10 cryptocurrencies apart from Bitcoin. Then, we\nshow how we applied it to a real-world case study in the insurtech domain. \n\n"}
{"id": "1812.09111", "contents": "Title: Generative Models from the perspective of Continual Learning Abstract: Which generative model is the most suitable for Continual Learning? This\npaper aims at evaluating and comparing generative models on disjoint sequential\nimage generation tasks. We investigate how several models learn and forget,\nconsidering various strategies: rehearsal, regularization, generative replay\nand fine-tuning. We used two quantitative metrics to estimate the generation\nquality and memory ability. We experiment with sequential tasks on three\ncommonly used benchmarks for Continual Learning (MNIST, Fashion MNIST and\nCIFAR10). We found that among all models, the original GAN performs best and\namong Continual Learning strategies, generative replay outperforms all other\nmethods. Even if we found satisfactory combinations on MNIST and Fashion MNIST,\ntraining generative models sequentially on CIFAR10 is particularly instable,\nand remains a challenge. Our code is available online\n\\footnote{\\url{https://github.com/TLESORT/Generative\\_Continual\\_Learning}}. \n\n"}
{"id": "1812.09459", "contents": "Title: Optimal Cache Placement for Modified Coded Caching with Arbitrary Cache\n  Size Abstract: We consider content caching between a service provider and multiple\ncache-enabled users, using the recently proposed modified coded caching scheme\n(MCCS) that provides an improved delivery strategy for random user requests. We\ndevelop the optimal cache placement solution for the MCCS with arbitrary cache\nsize by formulating the cache placement as an optimization problem to minimize\nthe average rate during the delivery phase under random user requests. Through\nreformulation, we show that the problem is a linear programming problem. By\nexploring the properties in the caching constraints, we obtain the optimal\ncache placement solution in closed-form. We verify that the existing cache\nplacement scheme obtained at specific cache sizes is a special case of our\nsolution. Numerical studies show how the caching gain changes as the user\npopulation increases, as a result of different cache placement patterns. \n\n"}
{"id": "1812.10668", "contents": "Title: An efficient cloud scheduler design supporting preemptible instances Abstract: Maximizing resource utilization by performing an efficient resource\nprovisioning is a key factor for any cloud provider: commercial actors can\nmaximize their revenues, whereas scientific and non-commercial providers can\nmaximize their infrastructure utilization. Traditionally, batch systems have\nallowed data centers to fill their resources as much as possible by using\nbackfilling and similar techniques. However, in an IaaS cloud, where virtual\nmachines are supposed to live indefinitely, or at least as long as the user is\nable to pay for them, these policies are not easily implementable. In this work\nwe present a new scheduling algorithm for IaaS providers that is able to\nsupport preemptible instances, that can be stopped by higher priority requests\nwithout introducing large modifications in the current cloud schedulers. This\nscheduler enables the implementation of new cloud usage and payment models that\nallow more efficient usage of the resources and potential new revenue sources\nfor commercial providers. We also study the correctness and the performace\noverhead of the proposed scheduler agains existing solutions. \n\n"}
{"id": "1812.10844", "contents": "Title: AT2: Asynchronous Trustworthy Transfers Abstract: Many blockchain-based protocols, such as Bitcoin, implement a decentralized\nasset transfer (or exchange) system. As clearly stated in the original paper by\nNakamoto, the crux of this problem lies in prohibiting any participant from\nengaging in double-spending. There seems to be a common belief that consensus\nis necessary for solving the double-spending problem. Indeed, whether it is for\na permissionless or a permissioned environment, the typical solution uses\nconsensus to build a totally ordered ledger of submitted transfers. In this\npaper we show that this common belief is false: consensus is not needed to\nimplement of a decentralized asset transfer system. We do so by introducing AT2\n(Asynchronous Trustworthy Transfers), a class of consensusless algorithms. To\nshow formally that consensus is unnecessary for asset transfers, we consider\nthis problem first in the shared-memory context. We introduce AT2$_{SM}$, a\nwait-free algorithm that asynchronously implements asset transfer in the\nread-write shared-memory model. In other words, we show that the consensus\nnumber of an asset-transfer object is one. In the message passing model with\nByzantine faults, we introduce a generic asynchronous algorithm called\nAT2$_{MP}$ and discuss two instantiations of this solution. First, AT2$_{D}$\nensures deterministic guarantees and consequently targets a small scale\ndeployment (tens to hundreds of nodes), typically for a permissioned\nenvironment. Second, AT2$_{P}$ provides probabilistic guarantees and scales\nwell to a very large system size (tens of thousands of nodes), ensuring\nlogarithmic latency and communication complexity. Instead of consensus, we\nconstruct AT2$_{D}$ and AT2$_{P}$ on top of a broadcast primitive with causal\nordering guarantees offering deterministic and probabilistic properties,\nrespectively. \n\n"}
{"id": "1812.11314", "contents": "Title: Meta Reinforcement Learning with Distribution of Exploration Parameters\n  Learned by Evolution Strategies Abstract: In this paper, we propose a novel meta-learning method in a reinforcement\nlearning setting, based on evolution strategies (ES), exploration in parameter\nspace and deterministic policy gradients. ES methods are easy to parallelize,\nwhich is desirable for modern training architectures; however, such methods\ntypically require a huge number of samples for effective training. We use\ndeterministic policy gradients during adaptation and other techniques to\ncompensate for the sample-efficiency problem while maintaining the inherent\nscalability of ES methods. We demonstrate that our method achieves good results\ncompared to gradient-based meta-learning in high-dimensional control tasks in\nthe MuJoCo simulator. In addition, because of gradient-free methods in the\nmeta-training phase, which do not need information about gradients and policies\nin adaptation training, we predict and confirm our algorithm performs better in\ntasks that need multi-step adaptation. \n\n"}
{"id": "1812.11677", "contents": "Title: ADMM-NN: An Algorithm-Hardware Co-Design Framework of DNNs Using\n  Alternating Direction Method of Multipliers Abstract: To facilitate efficient embedded and hardware implementations of deep neural\nnetworks (DNNs), two important categories of DNN model compression techniques:\nweight pruning and weight quantization are investigated. The former leverages\nthe redundancy in the number of weights, whereas the latter leverages the\nredundancy in bit representation of weights. However, there lacks a systematic\nframework of joint weight pruning and quantization of DNNs, thereby limiting\nthe available model compression ratio. Moreover, the computation reduction,\nenergy efficiency improvement, and hardware performance overhead need to be\naccounted for besides simply model size reduction.\n  To address these limitations, we present ADMM-NN, the first\nalgorithm-hardware co-optimization framework of DNNs using Alternating\nDirection Method of Multipliers (ADMM), a powerful technique to deal with\nnon-convex optimization problems with possibly combinatorial constraints. The\nfirst part of ADMM-NN is a systematic, joint framework of DNN weight pruning\nand quantization using ADMM. It can be understood as a smart regularization\ntechnique with regularization target dynamically updated in each ADMM\niteration, thereby resulting in higher performance in model compression than\nprior work. The second part is hardware-aware DNN optimizations to facilitate\nhardware-level implementations.\n  Without accuracy loss, we can achieve 85$\\times$ and 24$\\times$ pruning on\nLeNet-5 and AlexNet models, respectively, significantly higher than prior work.\nThe improvement becomes more significant when focusing on computation\nreductions. Combining weight pruning and quantization, we achieve 1,910$\\times$\nand 231$\\times$ reductions in overall model size on these two benchmarks, when\nfocusing on data storage. Highly promising results are also observed on other\nrepresentative DNNs such as VGGNet and ResNet-50. \n\n"}
{"id": "1901.00898", "contents": "Title: Imminent Collision Mitigation with Reinforcement Learning and Vision Abstract: This work examines the role of reinforcement learning in reducing the\nseverity of on-road collisions by controlling velocity and steering in\nsituations in which contact is imminent. We construct a model, given camera\nimages as input, that is capable of learning and predicting the dynamics of\nobstacles, cars and pedestrians, and train our policy using this model. Two\npolicies that control both braking and steering are compared against a baseline\nwhere the only action taken is (conventional) braking in a straight line. The\ntwo policies are trained using two distinct reward structures, one where any\nand all collisions incur a fixed penalty, and a second one where the penalty is\ncalculated based on already established delta-v models of injury severity. The\nresults show that both policies exceed the performance of the baseline, with\nthe policy trained using injury models having the highest performance. \n\n"}
{"id": "1901.01331", "contents": "Title: The ISTI Rapid Response on Exploring Cloud Computing 2018 Abstract: This report describes eighteen projects that explored how commercial cloud\ncomputing services can be utilized for scientific computation at national\nlaboratories. These demonstrations ranged from deploying proprietary software\nin a cloud environment to leveraging established cloud-based analytics\nworkflows for processing scientific datasets. By and large, the projects were\nsuccessful and collectively they suggest that cloud computing can be a valuable\ncomputational resource for scientific computation at national laboratories. \n\n"}
{"id": "1901.01854", "contents": "Title: Age-of-Information for Computation-Intensive Messages in Mobile Edge\n  Computing Abstract: Age-of-information (AoI) is a novel metric that measures the freshness of\ninformation in status update scenarios. It is essential for real-time\napplications to transmit status update packets to the destination node as\ntimely as possible. However, for some applications, status information embedded\nin the packets is not revealed until complicated data processing, which is\ncomputational expensive and time consuming. As mobile edge server has\nsufficient computational resource and is placed close to users, mobile edge\ncomputing (MEC) is expected to reduce age for computation-intensive messages.\nIn this paper, we study the AoI for computation-intensive data in MEC, and\nconsider two schemes: local computing by user itself and remote computing at\nMEC server. The two computing models are unified into a two-node tandem queuing\nmodel. Zero-wait policy is adopted, i.e., a new message is generated once the\nprevious one leaves the first node. We consider exponentially distributed\nservice time and infinite queue size, and hence, the second node can be seen as\na First-Come-First-Served (FCFS) M/M/1 system. Closed-form average AoI is\nderived for the two computing schemes. The region where remote computing\noutperforms local computing is characterized. Simulation results show that the\nremote computing is greatly superior to the local computing when the remote\ncomputing rate is large enough, and that there exists an optimal transmission\nrate so that remote computing is better than local computing for a largest\nrange. \n\n"}
{"id": "1901.02875", "contents": "Title: Learning to Infer and Execute 3D Shape Programs Abstract: Human perception of 3D shapes goes beyond reconstructing them as a set of\npoints or a composition of geometric primitives: we also effortlessly\nunderstand higher-level shape structure such as the repetition and reflective\nsymmetry of object parts. In contrast, recent advances in 3D shape sensing\nfocus more on low-level geometry but less on these higher-level relationships.\nIn this paper, we propose 3D shape programs, integrating bottom-up recognition\nsystems with top-down, symbolic program structure to capture both low-level\ngeometry and high-level structural priors for 3D shapes. Because there are no\nannotations of shape programs for real shapes, we develop neural modules that\nnot only learn to infer 3D shape programs from raw, unannotated shapes, but\nalso to execute these programs for shape reconstruction. After initial\nbootstrapping, our end-to-end differentiable model learns 3D shape programs by\nreconstructing shapes in a self-supervised manner. Experiments demonstrate that\nour model accurately infers and executes 3D shape programs for highly complex\nshapes from various categories. It can also be integrated with an\nimage-to-shape module to infer 3D shape programs directly from an RGB image,\nleading to 3D shape reconstructions that are both more accurate and more\nphysically plausible. \n\n"}
{"id": "1901.03161", "contents": "Title: Harnessing the Power of Serverless Runtimes for Large-Scale Optimization Abstract: The event-driven and elastic nature of serverless runtimes makes them a very\nefficient and cost-effective alternative for scaling up computations. So far,\nthey have mostly been used for stateless, data parallel and ephemeral\ncomputations. In this work, we propose using serverless runtimes to solve\ngeneric, large-scale optimization problems. Specifically, we build a\nmaster-worker setup using AWS Lambda as the source of our workers, implement a\nparallel optimization algorithm to solve a regularized logistic regression\nproblem, and show that relative speedups up to 256 workers and efficiencies\nabove 70% up to 64 workers can be expected. We also identify possible\nalgorithmic and system-level bottlenecks, propose improvements, and discuss the\nlimitations and challenges in realizing these improvements. \n\n"}
{"id": "1901.04629", "contents": "Title: Separation and approximate separation of multipartite quantum gates Abstract: The number of qubits of current quantum computers is one of the most\ndominating restrictions for applications. So it is naturally conceived to use\ntwo or more small capacity quantum computers to form a larger capacity quantum\ncomputing system by quantum parallel programming. To design the parallel\nprogram for quantum computers, the primary obstacle is to decompose quantum\ngates in the whole circuit to the tensor product of local gates. In the paper,\nwe first devote to analyzing theoretically separability conditions of\nmultipartite quantum gates on finite or infinite dimensional systems.\nFurthermore, we perform the separation experiments for $n$-qubit quantum gates\non the IBM's quantum computers by the software Q$|SI\\rangle$. Not surprisedly,\nit is showed that there exist few separable ones among multipartite quantum\ngates. Therefore, we pay our attention to the approximate separation problems\nof multipartite gates, i.e., how a multipartite gate can be closed to separable\nones. \n\n"}
{"id": "1901.04713", "contents": "Title: Global-to-local Memory Pointer Networks for Task-Oriented Dialogue Abstract: End-to-end task-oriented dialogue is challenging since knowledge bases are\nusually large, dynamic and hard to incorporate into a learning framework. We\npropose the global-to-local memory pointer (GLMP) networks to address this\nissue. In our model, a global memory encoder and a local memory decoder are\nproposed to share external knowledge. The encoder encodes dialogue history,\nmodifies global contextual representation, and generates a global memory\npointer. The decoder first generates a sketch response with unfilled slots.\nNext, it passes the global memory pointer to filter the external knowledge for\nrelevant information, then instantiates the slots via the local memory\npointers. We empirically show that our model can improve copy accuracy and\nmitigate the common out-of-vocabulary problem. As a result, GLMP is able to\nimprove over the previous state-of-the-art models in both simulated bAbI\nDialogue dataset and human-human Stanford Multi-domain Dialogue dataset on\nautomatic and human evaluation. \n\n"}
{"id": "1901.04985", "contents": "Title: NNStreamer: Stream Processing Paradigm for Neural Networks, Toward\n  Efficient Development and Execution of On-Device AI Applications Abstract: We propose nnstreamer, a software system that handles neural networks as\nfilters of stream pipelines, applying the stream processing paradigm to neural\nnetwork applications. A new trend with the wide-spread of deep neural network\napplications is on-device AI; i.e., processing neural networks directly on\nmobile devices or edge/IoT devices instead of cloud servers. Emerging privacy\nissues, data transmission costs, and operational costs signifies the need for\non-device AI especially when a huge number of devices with real-time data\nprocessing are deployed. Nnstreamer efficiently handles neural networks with\ncomplex data stream pipelines on devices, improving the overall performance\nsignificantly with minimal efforts. Besides, nnstreamer simplifies the neural\nnetwork pipeline implementations and allows reusing off-shelf multimedia stream\nfilters directly; thus it reduces the developmental costs significantly.\nNnstreamer is already being deployed with a product releasing soon and is open\nsource software applicable to a wide range of hardware architectures and\nsoftware platforms. \n\n"}
{"id": "1901.04988", "contents": "Title: A Survey of FPGA Based Deep Learning Accelerators: Challenges and\n  Opportunities Abstract: With the rapid development of in-depth learning, neural network and deep\nlearning algorithms have been widely used in various fields, e.g., image, video\nand voice processing. However, the neural network model is getting larger and\nlarger, which is expressed in the calculation of model parameters. Although a\nwealth of existing efforts on GPU platforms currently used by researchers for\nimproving computing performance, dedicated hardware solutions are essential and\nemerging to provide advantages over pure software solutions. In this paper, we\nsystematically investigate the neural network accelerator based on FPGA.\nSpecifically, we respectively review the accelerators designed for specific\nproblems, specific algorithms, algorithm features, and general templates. We\nalso compared the design and implementation of the accelerator based on FPGA\nunder different devices and network models and compared it with the versions of\nCPU and GPU. Finally, we present to discuss the advantages and disadvantages of\naccelerators on FPGA platforms and to further explore the opportunities for\nfuture research. \n\n"}
{"id": "1901.05049", "contents": "Title: Bonseyes AI Pipeline -- bringing AI to you. End-to-end integration of\n  data, algorithms and deployment tools Abstract: Next generation of embedded Information and Communication Technology (ICT)\nsystems are collaborative systems able to perform autonomous tasks. The\nremarkable expansion of the embedded ICT market, together with the rise and\nbreakthroughs of Artificial Intelligence (AI), have put the focus on the Edge\nas it stands as one of the keys for the next technological revolution: the\nseamless integration of AI in our daily life. However, training and deployment\nof custom AI solutions on embedded devices require a fine-grained integration\nof data, algorithms, and tools to achieve high accuracy. Such integration\nrequires a high level of expertise that becomes a real bottleneck for small and\nmedium enterprises wanting to deploy AI solutions on the Edge which,\nultimately, slows down the adoption of AI on daily-life applications. In this\nwork, we present a modular AI pipeline as an integrating framework to bring\ndata, algorithms, and deployment tools together. By removing the integration\nbarriers and lowering the required expertise, we can interconnect the different\nstages of tools and provide a modular end-to-end development of AI products for\nembedded devices. Our AI pipeline consists of four modular main steps: i) data\ningestion, ii) model training, iii) deployment optimization and, iv) the IoT\nhub integration. To show the effectiveness of our pipeline, we provide examples\nof different AI applications during each of the steps. Besides, we integrate\nour deployment framework, LPDNN, into the AI pipeline and present its\nlightweight architecture and deployment capabilities for embedded devices.\nFinally, we demonstrate the results of the AI pipeline by showing the\ndeployment of several AI applications such as keyword spotting, image\nclassification and object detection on a set of well-known embedded platforms,\nwhere LPDNN consistently outperforms all other popular deployment frameworks. \n\n"}
{"id": "1901.05123", "contents": "Title: Memory Augmented Deep Generative models for Forecasting the Next Shot\n  Location in Tennis Abstract: This paper presents a novel framework for predicting shot location and type\nin tennis. Inspired by recent neuroscience discoveries we incorporate neural\nmemory modules to model the episodic and semantic memory components of a tennis\nplayer. We propose a Semi Supervised Generative Adversarial Network\narchitecture that couples these memory models with the automatic feature\nlearning power of deep neural networks and demonstrate methodologies for\nlearning player level behavioural patterns with the proposed framework. We\nevaluate the effectiveness of the proposed model on tennis tracking data from\nthe 2012 Australian Tennis open and exhibit applications of the proposed method\nin discovering how players adapt their style depending on the match context. \n\n"}
{"id": "1901.05856", "contents": "Title: Amplifying the Imitation Effect for Reinforcement Learning of UCAV's\n  Mission Execution Abstract: This paper proposes a new reinforcement learning (RL) algorithm that enhances\nexploration by amplifying the imitation effect (AIE). This algorithm consists\nof self-imitation learning and random network distillation algorithms. We argue\nthat these two algorithms complement each other and that combining these two\nalgorithms can amplify the imitation effect for exploration. In addition, by\nadding an intrinsic penalty reward to the state that the RL agent frequently\nvisits and using replay memory for learning the feature state when using an\nexploration bonus, the proposed approach leads to deep exploration and deviates\nfrom the current converged policy. We verified the exploration performance of\nthe algorithm through experiments in a two-dimensional grid environment. In\naddition, we applied the algorithm to a simulated environment of unmanned\ncombat aerial vehicle (UCAV) mission execution, and the empirical results show\nthat AIE is very effective for finding the UCAV's shortest flight path to avoid\nan enemy's missiles. \n\n"}
{"id": "1901.06803", "contents": "Title: Active Learning with Gaussian Processes for High Throughput Phenotyping Abstract: A looming question that must be solved before robotic plant phenotyping\ncapabilities can have significant impact to crop improvement programs is\nscalability. High Throughput Phenotyping (HTP) uses robotic technologies to\nanalyze crops in order to determine species with favorable traits, however, the\ncurrent practices rely on exhaustive coverage and data collection from the\nentire crop field being monitored under the breeding experiment. This works\nwell in relatively small agricultural fields but can not be scaled to the\nlarger ones, thus limiting the progress of genetics research. In this work, we\npropose an active learning algorithm to enable an autonomous system to collect\nthe most informative samples in order to accurately learn the distribution of\nphenotypes in the field with the help of a Gaussian Process model. We\ndemonstrate the superior performance of our proposed algorithm compared to the\ncurrent practices on sorghum phenotype data collection. \n\n"}
{"id": "1901.06949", "contents": "Title: Differential Privacy for Power Grid Obfuscation Abstract: The availability of high-fidelity energy networks brings significant value to\nacademic and commercial research. However, such releases also raise fundamental\nconcerns related to privacy and security as they can reveal sensitive\ncommercial information and expose system vulnerabilities. This paper\ninvestigates how to release power networks where the parameters of transmission\nlines and transformers are obfuscated. It does so by using the framework of\nDifferential Privacy (DP), that provides strong privacy guarantees and has\nattracted significant attention in recent years. Unfortunately, simple DP\nmechanisms often result in AC-infeasible networks. To address these concerns,\nthis paper presents a novel differential privacy mechanism that guarantees\nAC-feasibility and largely preserves the fidelity of the obfuscated network.\nExperimental results also show that the obfuscation significantly reduces the\npotential damage of an attacker exploiting the release of the dataset. \n\n"}
{"id": "1901.07294", "contents": "Title: SVE-enabling Lattice QCD Codes Abstract: Optimization of applications for supercomputers of the highest performance\nclass requires parallelization at multiple levels using different techniques.\nIn this contribution we focus on parallelization of particle physics\nsimulations through vector instructions. With the advent of the Scalable Vector\nExtension (SVE) ISA, future ARM-based processors are expected to provide a\nsignificant level of parallelism at this level. \n\n"}
{"id": "1901.07302", "contents": "Title: IOTA-based Directed Acyclic Graphs without Orphans Abstract: Directed Acylic Graphs (DAGs) are emerging as an attractive alternative to\ntraditional blockchain architectures for distributed ledger technology (DLT).\nIn particular DAG ledgers with stochastic attachment mechanisms potentially\noffer many advantages over blockchain, including scalability and faster\ntransaction speeds. However, the random nature of the attachment mechanism\ncoupled with the requirement of protection against double-spend transactions\nleaves open the possibility that not all transactions will be eventually\nvalidated. Such transactions are said to be orphaned, and will never be\nvalidated. Our principal contribution is to propose a simple modification to\nthe attachment mechanism for the Tangle (the IOTA DAG architecture). This\nmodification ensures that all transactions are validated in finite time, and\npreserves essential features of the popular Monte-Carlo selection algorithm. In\norder to demonstrate these results we derive a fluid approximation for the\nTangle (in the limit of infinite arrival rate) and prove that this fluid model\nexhibits the desired behavior. We also present simulations which validate the\nresults for finite arrival rates. \n\n"}
{"id": "1901.08162", "contents": "Title: Causal Reasoning from Meta-reinforcement Learning Abstract: Discovering and exploiting the causal structure in the environment is a\ncrucial challenge for intelligent agents. Here we explore whether causal\nreasoning can emerge via meta-reinforcement learning. We train a recurrent\nnetwork with model-free reinforcement learning to solve a range of problems\nthat each contain causal structure. We find that the trained agent can perform\ncausal reasoning in novel situations in order to obtain rewards. The agent can\nselect informative interventions, draw causal inferences from observational\ndata, and make counterfactual predictions. Although established formal causal\nreasoning algorithms also exist, in this paper we show that such reasoning can\narise from model-free reinforcement learning, and suggest that causal reasoning\nin complex settings may benefit from the more end-to-end learning-based\napproaches presented here. This work also offers new strategies for structured\nexploration in reinforcement learning, by providing agents with the ability to\nperform -- and interpret -- experiments. \n\n"}
{"id": "1901.08326", "contents": "Title: A stack-vector routing protocol for automatic tunneling Abstract: In a network, a tunnel is a part of a path where a protocol is encapsulated\nin another one. A tunnel starts with an encapsulation and ends with the\ncorresponding decapsulation. Several tunnels can be nested at some stage,\nforming a protocol stack. Tunneling is very important nowadays and it is\ninvolved in several tasks: IPv4/IPv6 transition, VPNs, security (IPsec, onion\nrouting), etc. However, tunnel establishment is mainly performed manually or by\nscript, which present obvious scalability issues. Some works attempt to\nautomate a part of the process (e.g., TSP, ISATAP, etc.). However, the\ndetermination of the tunnel(s) endpoints is not fully automated, especially in\nthe case of an arbitrary number of nested tunnels. The lack of routing\nprotocols performing automatic tunneling is due to the unavailability of path\ncomputation algorithms taking into account encapsulations and decapsulations.\nThere is a polynomial centralized algorithm to perform the task. However, to\nthe best of our knowledge, no fully distributed path computation algorithm is\nknown. Here, we propose the first fully distributed algorithm for path\ncomputation with automatic tunneling, i.e., taking into account encapsulation,\ndecapsulation and conversion of protocols. Our algorithm is a generalization of\nthe distributed Bellman-Ford algorithm, where the distance vector is replaced\nby a protocol stack vector. This allows to know how to route a packet with some\nprotocol stack. We prove that the messages size of our algorithm is polynomial,\neven if the shortest path can be of exponential length. We also prove that the\nalgorithm converges after a polynomial number of steps in a synchronized\nsetting. We adapt our algorithm into a proto-protocol for routing with\nautomatic tunneling and we show its efficiency through simulations. \n\n"}
{"id": "1901.08460", "contents": "Title: Fully Decentralized Joint Learning of Personalized Models and\n  Collaboration Graphs Abstract: We consider the fully decentralized machine learning scenario where many\nusers with personal datasets collaborate to learn models through local\npeer-to-peer exchanges, without a central coordinator. We propose to train\npersonalized models that leverage a collaboration graph describing the\nrelationships between user personal tasks, which we learn jointly with the\nmodels. Our fully decentralized optimization procedure alternates between\ntraining nonlinear models given the graph in a greedy boosting manner, and\nupdating the collaboration graph (with controlled sparsity) given the models.\nThroughout the process, users exchange messages only with a small number of\npeers (their direct neighbors when updating the models, and a few random users\nwhen updating the graph), ensuring that the procedure naturally scales with the\nnumber of users. Overall, our approach is communication-efficient and avoids\nexchanging personal data. We provide an extensive analysis of the convergence\nrate, memory and communication complexity of our approach, and demonstrate its\nbenefits compared to competing techniques on synthetic and real datasets. \n\n"}
{"id": "1901.08705", "contents": "Title: Ambitious Data Science Can Be Painless Abstract: Modern data science research can involve massive computational\nexperimentation; an ambitious PhD in computational fields may do experiments\nconsuming several million CPU hours. Traditional computing practices, in which\nresearchers use laptops or shared campus-resident resources, are inadequate for\nexperiments at the massive scale and varied scope that we now see in data\nscience. On the other hand, modern cloud computing promises seemingly unlimited\ncomputational resources that can be custom configured, and seems to offer a\npowerful new venue for ambitious data-driven science. Exploiting the cloud\nfully, the amount of work that could be completed in a fixed amount of time can\nexpand by several orders of magnitude.\n  As potentially powerful as cloud-based experimentation may be in the\nabstract, it has not yet become a standard option for researchers in many\nacademic disciplines. The prospect of actually conducting massive computational\nexperiments in today's cloud systems confronts the potential user with daunting\nchallenges. Leading considerations include: (i) the seeming complexity of\ntoday's cloud computing interface, (ii) the difficulty of executing an\noverwhelmingly large number of jobs, and (iii) the difficulty of monitoring and\ncombining a massive collection of separate results. Starting a massive\nexperiment `bare-handed' seems therefore highly problematic and prone to rapid\n`researcher burn out'.\n  New software stacks are emerging that render massive cloud experiments\nrelatively painless. Such stacks simplify experimentation by systematizing\nexperiment definition, automating distribution and management of tasks, and\nallowing easy harvesting of results and documentation. In this article, we\ndiscuss several painless computing stacks that abstract away the difficulties\nof massive experimentation, thereby allowing a proliferation of ambitious\nexperiments for scientific discovery. \n\n"}
{"id": "1901.09865", "contents": "Title: Asynchronous Accelerated Proximal Stochastic Gradient for Strongly\n  Convex Distributed Finite Sums Abstract: In this work, we study the problem of minimizing the sum of strongly convex\nfunctions split over a network of $n$ nodes. We propose the decentralized and\nasynchronous algorithm ADFS to tackle the case when local functions are\nthemselves finite sums with $m$ components. ADFS converges linearly when local\nfunctions are smooth, and matches the rates of the best known finite sum\nalgorithms when executed on a single machine. On several machines, ADFS enjoys\na $O (\\sqrt{n})$ or $O(n)$ speed-up depending on the leading complexity term as\nlong as the diameter of the network is not too big with respect to $m$. This\nalso leads to a $\\sqrt{m}$ speed-up over state-of-the-art distributed batch\nmethods, which is the expected speed-up for finite sum algorithms. In terms of\ncommunication times and network parameters, ADFS scales as well as optimal\ndistributed batch algorithms. As a side contribution, we give a generalized\nversion of the accelerated proximal coordinate gradient algorithm using\narbitrary sampling that we apply to a well-chosen dual problem to derive ADFS.\nYet, ADFS uses primal proximal updates that only require solving\none-dimensional problems for many standard machine learning applications.\nFinally, ADFS can be formulated for non-smooth objectives with equally good\nscaling properties. We illustrate the improvement of ADFS over state-of-the-art\napproaches with simulations. \n\n"}
{"id": "1901.10008", "contents": "Title: The OoO VLIW JIT Compiler for GPU Inference Abstract: Current trends in Machine Learning~(ML) inference on hardware accelerated\ndevices (e.g., GPUs, TPUs) point to alarmingly low utilization. As ML inference\nis increasingly time-bounded by tight latency SLOs, increasing data parallelism\nis not an option. The need for better efficiency motivates GPU multiplexing.\nFurthermore, existing GPU programming abstractions force programmers to\nmicro-manage GPU resources in an early-binding, context-free fashion. We\npropose a VLIW-inspired Out-of-Order (OoO) Just-in-Time (JIT) compiler that\ncoalesces and reorders execution kernels at runtime for throughput-optimal\ndevice utilization while satisfying latency SLOs. We quantify the\ninefficiencies of space-only and time-only multiplexing alternatives and\ndemonstrate an achievable 7.7x opportunity gap through spatial coalescing. \n\n"}
{"id": "1901.10114", "contents": "Title: Optimising Clifford Circuits with Quantomatic Abstract: We present a system of equations between Clifford circuits, all derivable in\nthe ZX-calculus, and formalised as rewrite rules in the Quantomatic proof\nassistant. By combining these rules with some non-trivial simplification\nprocedures defined in the Quantomatic tactic language, we demonstrate the use\nof Quantomatic as a circuit optimisation tool. We prove that the system always\nreduces Clifford circuits of one or two qubits to their minimal form, and give\nnumerical results demonstrating its performance on larger Clifford circuits. \n\n"}
{"id": "1901.10348", "contents": "Title: Stochastic Frank-Wolfe for Composite Convex Minimization Abstract: A broad class of convex optimization problems can be formulated as a\nsemidefinite program (SDP), minimization of a convex function over the\npositive-semidefinite cone subject to some affine constraints. The majority of\nclassical SDP solvers are designed for the deterministic setting where problem\ndata is readily available. In this setting, generalized conditional gradient\nmethods (aka Frank-Wolfe-type methods) provide scalable solutions by leveraging\nthe so-called linear minimization oracle instead of the projection onto the\nsemidefinite cone. Most problems in machine learning and modern engineering\napplications, however, contain some degree of stochasticity. In this work, we\npropose the first conditional-gradient-type method for solving stochastic\noptimization problems under affine constraints. Our method guarantees\n$\\mathcal{O}(k^{-1/3})$ convergence rate in expectation on the objective\nresidual and $\\mathcal{O}(k^{-5/12})$ on the feasibility gap. \n\n"}
{"id": "1901.10405", "contents": "Title: Constraint Satisfaction Propagation: Non-stationary Policy Synthesis for\n  Temporal Logic Planning Abstract: Problems arise when using reward functions to capture dependencies between\nsequential time-constrained goal states because the state-space must be\nprohibitively expanded to accommodate a history of successfully achieved\nsub-goals. Also, policies and value functions derived with stationarity\nassumptions are not readily decomposable, leading to a tension between reward\nmaximization and task generalization. We demonstrate a logic-compatible\napproach using model-based knowledge of environment dynamics and deadline\ninformation to directly infer non-stationary policies composed of reusable\nstationary policies. The policies are constructed to maximize the probability\nof satisfying time-sensitive goals while respecting time-varying obstacles. Our\napproach explicitly maintains two different spaces, a high-level logical task\nspecification where the task-variables are grounded onto the low-level\nstate-space of a Markov decision process. Computing satisfiability at the\ntask-level is made possible by a Bellman-like equation which operates on a\ntensor that links the temporal relationship between the two spaces; the\nequation solves for a value function that can be explicitly interpreted as the\nprobability of sub-goal satisfaction under the synthesized non-stationary\npolicy, an approach we term Constraint Satisfaction Propagation (CSP). \n\n"}
{"id": "1901.10653", "contents": "Title: Evaluating Bregman Divergences for Probability Learning from Crowd Abstract: The crowdsourcing scenarios are a good example of having a probability\ndistribution over some categories showing what the people in a global\nperspective thinks. Learn a predictive model of this probability distribution\ncan be of much more valuable that learn only a discriminative model that gives\nthe most likely category of the data. Here we present differents models that\nadapts having probability distribution as target to train a machine learning\nmodel. We focus on the Bregman divergences framework to used as objective\nfunction to minimize. The results show that special care must be taken when\nbuild a objective function and consider a equal optimization on neural network\nin Keras framework. \n\n"}
{"id": "1901.11459", "contents": "Title: Funnelling: A New Ensemble Method for Heterogeneous Transfer Learning\n  and its Application to Cross-Lingual Text Classification Abstract: Cross-lingual Text Classification (CLC) consists of automatically\nclassifying, according to a common set C of classes, documents each written in\none of a set of languages L, and doing so more accurately than when naively\nclassifying each document via its corresponding language-specific classifier.\nIn order to obtain an increase in the classification accuracy for a given\nlanguage, the system thus needs to also leverage the training examples written\nin the other languages. We tackle multilabel CLC via funnelling, a new ensemble\nlearning method that we propose here. Funnelling consists of generating a\ntwo-tier classification system where all documents, irrespectively of language,\nare classified by the same (2nd-tier) classifier. For this classifier all\ndocuments are represented in a common, language-independent feature space\nconsisting of the posterior probabilities generated by 1st-tier,\nlanguage-dependent classifiers. This allows the classification of all test\ndocuments, of any language, to benefit from the information present in all\ntraining documents, of any language. We present substantial experiments, run on\npublicly available multilingual text collections, in which funnelling is shown\nto significantly outperform a number of state-of-the-art baselines. All code\nand datasets (in vector form) are made publicly available. \n\n"}
{"id": "1901.11515", "contents": "Title: ProBO: Versatile Bayesian Optimization Using Any Probabilistic\n  Programming Language Abstract: Optimizing an expensive-to-query function is a common task in science and\nengineering, where it is beneficial to keep the number of queries to a minimum.\nA popular strategy is Bayesian optimization (BO), which leverages probabilistic\nmodels for this task. Most BO today uses Gaussian processes (GPs), or a few\nother surrogate models. However, there is a broad set of Bayesian modeling\ntechniques that could be used to capture complex systems and reduce the number\nof queries in BO. Probabilistic programming languages (PPLs) are modern tools\nthat allow for flexible model definition, prior specification, model\ncomposition, and automatic inference. In this paper, we develop ProBO, a BO\nprocedure that uses only standard operations common to most PPLs. This allows a\nuser to drop in a model built with an arbitrary PPL and use it directly in BO.\nWe describe acquisition functions for ProBO, and strategies for efficiently\noptimizing these functions given complex models or costly inference procedures.\nUsing existing PPLs, we implement new models to aid in a few challenging\noptimization settings, and demonstrate these on model hyperparameter and\narchitecture search tasks. \n\n"}
{"id": "cond-mat/9812344", "contents": "Title: Parallelization of a Dynamic Monte Carlo Algorithm: a Partially\n  Rejection-Free Conservative Approach Abstract: We experiment with a massively parallel implementation of an algorithm for\nsimulating the dynamics of metastable decay in kinetic Ising models. The\nparallel scheme is directly applicable to a wide range of stochastic cellular\nautomata where the discrete events (updates) are Poisson arrivals. For high\nperformance, we utilize a continuous-time, asynchronous parallel version of the\nn-fold way rejection-free algorithm. Each processing element carries an lxl\nblock of spins, and we employ the fast SHMEM-library routines on the Cray T3E\ndistributed-memory parallel architecture. Different processing elements have\ndifferent local simulated times. To ensure causality, the algorithm handles the\nasynchrony in a conservative fashion. Despite relatively low utilization and an\nintricate relationship between the average time increment and the size of the\nspin blocks, we find that for sufficiently large l the algorithm outperforms\nits corresponding parallel Metropolis (non-rejection-free) counterpart. As an\nexample application, we present results for metastable decay in a model\nferromagnetic or ferroelectric film, observed with a probe of area smaller than\nthe total system. \n\n"}
{"id": "cond-mat/9909114", "contents": "Title: From Massively Parallel Algorithms and Fluctuating Time Horizons to\n  Non-equilibrium Surface Growth Abstract: We study the asymptotic scaling properties of a massively parallel algorithm\nfor discrete-event simulations where the discrete events are Poisson arrivals.\nThe evolution of the simulated time horizon is analogous to a non-equilibrium\nsurface. Monte Carlo simulations and a coarse-grained approximation indicate\nthat the macroscopic landscape in the steady state is governed by the\nEdwards-Wilkinson Hamiltonian. Since the efficiency of the algorithm\ncorresponds to the density of local minima in the associated surface, our\nresults imply that the algorithm is asymptotically scalable. \n\n"}
{"id": "cs/0101019", "contents": "Title: General Loss Bounds for Universal Sequence Prediction Abstract: The Bayesian framework is ideally suited for induction problems. The\nprobability of observing $x_t$ at time $t$, given past observations\n$x_1...x_{t-1}$ can be computed with Bayes' rule if the true distribution $\\mu$\nof the sequences $x_1x_2x_3...$ is known. The problem, however, is that in many\ncases one does not even have a reasonable estimate of the true distribution. In\norder to overcome this problem a universal distribution $\\xi$ is defined as a\nweighted sum of distributions $\\mu_i\\inM$, where $M$ is any countable set of\ndistributions including $\\mu$. This is a generalization of Solomonoff\ninduction, in which $M$ is the set of all enumerable semi-measures. Systems\nwhich predict $y_t$, given $x_1...x_{t-1}$ and which receive loss $l_{x_t y_t}$\nif $x_t$ is the true next symbol of the sequence are considered. It is proven\nthat using the universal $\\xi$ as a prior is nearly as good as using the\nunknown true distribution $\\mu$. Furthermore, games of chance, defined as a\nsequence of bets, observations, and rewards are studied. The time needed to\nreach the winning zone is bounded in terms of the relative entropy of $\\mu$ and\n$\\xi$. Extensions to arbitrary alphabets, partial and delayed prediction, and\nmore active systems are discussed. \n\n"}
{"id": "cs/0205079", "contents": "Title: Connectives in Quantum and other Cumulative Logics Abstract: Cumulative logics are studied in an abstract setting, i.e., without\nconnectives, very much in the spirit of Makinson's early work. A powerful\nrepresentation theorem characterizes those logics by choice functions that\nsatisfy a weakening of Sen's property alpha, in the spirit of the author's\n\"Nonmonotonic Logics and Semantics\" (JLC). The representation results obtained\nare surprisingly smooth: in the completeness part the choice function may be\ndefined on any set of worlds, not only definable sets and no\ndefinability-preservation property is required in the soundness part. For\nabstract cumulative logics, proper conjunction and negation may be defined.\nContrary to the situation studied in \"Nonmonotonic Logics and Semantics\" no\nproper disjunction seems to be definable in general. The cumulative relations\nof KLM that satisfy some weakening of the consistency preservation property all\ndefine cumulative logics with a proper negation. Quantum Logics, as defined by\nEngesser and Gabbay are such cumulative logics but the negation defined by\northogonal complement does not provide a proper negation. \n\n"}
{"id": "cs/0211004", "contents": "Title: The DLV System for Knowledge Representation and Reasoning Abstract: This paper presents the DLV system, which is widely considered the\nstate-of-the-art implementation of disjunctive logic programming, and addresses\nseveral aspects. As for problem solving, we provide a formal definition of its\nkernel language, function-free disjunctive logic programs (also known as\ndisjunctive datalog), extended by weak constraints, which are a powerful tool\nto express optimization problems. We then illustrate the usage of DLV as a tool\nfor knowledge representation and reasoning, describing a new declarative\nprogramming methodology which allows one to encode complex problems (up to\n$\\Delta^P_3$-complete problems) in a declarative fashion. On the foundational\nside, we provide a detailed analysis of the computational complexity of the\nlanguage of DLV, and by deriving new complexity results we chart a complete\npicture of the complexity of this language and important fragments thereof.\n  Furthermore, we illustrate the general architecture of the DLV system which\nhas been influenced by these results. As for applications, we overview\napplication front-ends which have been developed on top of DLV to solve\nspecific knowledge representation tasks, and we briefly describe the main\ninternational projects investigating the potential of the system for industrial\nexploitation. Finally, we report about thorough experimentation and\nbenchmarking, which has been carried out to assess the efficiency of the\nsystem. The experimental results confirm the solidity of DLV and highlight its\npotential for emerging application areas like knowledge management and\ninformation integration. \n\n"}
{"id": "cs/0307036", "contents": "Title: Small-World File-Sharing Communities Abstract: Web caches, content distribution networks, peer-to-peer file sharing\nnetworks, distributed file systems, and data grids all have in common that they\ninvolve a community of users who generate requests for shared data. In each\ncase, overall system performance can be improved significantly if we can first\nidentify and then exploit interesting structure within a community's access\npatterns. To this end, we propose a novel perspective on file sharing based on\nthe study of the relationships that form among users based on the files in\nwhich they are interested.\n  We propose a new structure that captures common user interests in data--the\ndata-sharing graph-- and justify its utility with studies on three\ndata-distribution systems: a high-energy physics collaboration, the Web, and\nthe Kazaa peer-to-peer network. We find small-world patterns in the\ndata-sharing graphs of all three communities. We analyze these graphs and\npropose some probable causes for these emergent small-world patterns. The\nsignificance of small-world patterns is twofold: it provides a rigorous support\nto intuition and, perhaps most importantly, it suggests ways to design\nmechanisms that exploit these naturally emerging patterns. \n\n"}
{"id": "cs/0310030", "contents": "Title: A Particular Bug Trap: Execution Replay Using Virtual Machines Abstract: Execution-replay (ER) is well known in the literature but has been restricted\nto special system architectures for many years. Improved hardware resources and\nthe maturity of virtual machine technology promise to make ER useful for a\nbroader range of development projects.\n  This paper describes an approach to create a practical, generic ER\ninfrastructure for desktop PC systems using virtual machine technology. In the\ncreated VM environment arbitrary application programs will run and be replayed\nunmodified, neither instrumentation nor recompilation are required. \n\n"}
{"id": "cs/0402014", "contents": "Title: Self-Organising Networks for Classification: developing Applications to\n  Science Analysis for Astroparticle Physics Abstract: Physics analysis in astroparticle experiments requires the capability of\nrecognizing new phenomena; in order to establish what is new, it is important\nto develop tools for automatic classification, able to compare the final result\nwith data from different detectors. A typical example is the problem of Gamma\nRay Burst detection, classification, and possible association to known sources:\nfor this task physicists will need in the next years tools to associate data\nfrom optical databases, from satellite experiments (EGRET, GLAST), and from\nCherenkov telescopes (MAGIC, HESS, CANGAROO, VERITAS). \n\n"}
{"id": "cs/0408043", "contents": "Title: The Arithmetical Complexity of Dimension and Randomness Abstract: Constructive dimension and constructive strong dimension are effectivizations\nof the Hausdorff and packing dimensions, respectively. Each infinite binary\nsequence A is assigned a dimension dim(A) in [0,1] and a strong dimension\nDim(A) in [0,1].\n  Let DIM^alpha and DIMstr^alpha be the classes of all sequences of dimension\nalpha and of strong dimension alpha, respectively. We show that DIM^0 is\nproperly Pi^0_2, and that for all Delta^0_2-computable alpha in (0,1],\nDIM^alpha is properly Pi^0_3.\n  To classify the strong dimension classes, we use a more powerful effective\nBorel hierarchy where a co-enumerable predicate is used rather than a\nenumerable predicate in the definition of the Sigma^0_1 level. For all\nDelta^0_2-computable alpha in [0,1), we show that DIMstr^alpha is properly in\nthe Pi^0_3 level of this hierarchy. We show that DIMstr^1 is properly in the\nPi^0_2 level of this hierarchy.\n  We also prove that the class of Schnorr random sequences and the class of\ncomputably random sequences are properly Pi^0_3. \n\n"}
{"id": "cs/0501021", "contents": "Title: Large-scale lattice Boltzmann simulations of complex fluids: advances\n  through the advent of computational grids Abstract: During the last two years the RealityGrid project has allowed us to be one of\nthe few scientific groups involved in the development of computational grids.\nSince smoothly working production grids are not yet available, we have been\nable to substantially influence the direction of software development and grid\ndeployment within the project. In this paper we review our results from large\nscale three-dimensional lattice Boltzmann simulations performed over the last\ntwo years. We describe how the proactive use of computational steering and\nadvanced job migration and visualization techniques enabled us to do our\nscientific work more efficiently. The projects reported on in this paper are\nstudies of complex fluid flows under shear or in porous media, as well as\nlarge-scale parameter searches, and studies of the self-organisation of liquid\ncubic mesophases.\n  Movies are available at\nhttp://www.ica1.uni-stuttgart.de/~jens/pub/05/05-PhilTransReview.html \n\n"}
{"id": "cs/0501084", "contents": "Title: Towards Automated Integration of Guess and Check Programs in Answer Set\n  Programming: A Meta-Interpreter and Applications Abstract: Answer set programming (ASP) with disjunction offers a powerful tool for\ndeclaratively representing and solving hard problems. Many NP-complete problems\ncan be encoded in the answer set semantics of logic programs in a very concise\nand intuitive way, where the encoding reflects the typical \"guess and check\"\nnature of NP problems: The property is encoded in a way such that polynomial\nsize certificates for it correspond to stable models of a program. However, the\nproblem-solving capacity of full disjunctive logic programs (DLPs) is beyond\nNP, and captures a class of problems at the second level of the polynomial\nhierarchy. While these problems also have a clear \"guess and check\" structure,\nfinding an encoding in a DLP reflecting this structure may sometimes be a\nnon-obvious task, in particular if the \"check\" itself is a coNP-complete\nproblem; usually, such problems are solved by interleaving separate guess and\ncheck programs, where the check is expressed by inconsistency of the check\nprogram. In this paper, we present general transformations of head-cycle free\n(extended) disjunctive logic programs into stratified and positive (extended)\ndisjunctive logic programs based on meta-interpretation techniques. The answer\nsets of the original and the transformed program are in simple correspondence,\nand, moreover, inconsistency of the original program is indicated by a\ndesignated answer set of the transformed program. Our transformations\nfacilitate the integration of separate \"guess\" and \"check\" programs, which are\noften easy to obtain, automatically into a single disjunctive logic program.\nOur results complement recent results on meta-interpretation in ASP, and extend\nmethods and techniques for a declarative \"guess and check\" problem solving\nparadigm through ASP. \n\n"}
{"id": "cs/0502078", "contents": "Title: Semantical Characterizations and Complexity of Equivalences in Answer\n  Set Programming Abstract: In recent research on non-monotonic logic programming, repeatedly strong\nequivalence of logic programs P and Q has been considered, which holds if the\nprograms P union R and Q union R have the same answer sets for any other\nprogram R. This property strengthens equivalence of P and Q with respect to\nanswer sets (which is the particular case for R is the empty set), and has its\napplications in program optimization, verification, and modular logic\nprogramming. In this paper, we consider more liberal notions of strong\nequivalence, in which the actual form of R may be syntactically restricted. On\nthe one hand, we consider uniform equivalence, where R is a set of facts rather\nthan a set of rules. This notion, which is well known in the area of deductive\ndatabases, is particularly useful for assessing whether programs P and Q are\nequivalent as components of a logic program which is modularly structured. On\nthe other hand, we consider relativized notions of equivalence, where R ranges\nover rules over a fixed alphabet, and thus generalize our results to\nrelativized notions of strong and uniform equivalence. For all these notions,\nwe consider disjunctive logic programs in the propositional (ground) case, as\nwell as some restricted classes, provide semantical characterizations and\nanalyze the computational complexity. Our results, which naturally extend to\nanswer set semantics for programs with strong negation, complement the results\non strong equivalence of logic programs and pave the way for optimizations in\nanswer set solvers as a tool for input-based problem solving. \n\n"}
{"id": "cs/0506034", "contents": "Title: A Taxonomy of Data Grids for Distributed Data Sharing, Management and\n  Processing Abstract: Data Grids have been adopted as the platform for scientific communities that\nneed to share, access, transport, process and manage large data collections\ndistributed worldwide. They combine high-end computing technologies with\nhigh-performance networking and wide-area storage management techniques. In\nthis paper, we discuss the key concepts behind Data Grids and compare them with\nother data sharing and distribution paradigms such as content delivery\nnetworks, peer-to-peer networks and distributed databases. We then provide\ncomprehensive taxonomies that cover various aspects of architecture, data\ntransportation, data replication and resource allocation and scheduling.\nFinally, we map the proposed taxonomy to various Data Grid systems not only to\nvalidate the taxonomy but also to identify areas for future exploration.\nThrough this taxonomy, we aim to categorise existing systems to better\nunderstand their goals and their methodology. This would help evaluate their\napplicability for solving similar problems. This taxonomy also provides a \"gap\nanalysis\" of this area through which researchers can potentially identify new\nissues for investigation. Finally, we hope that the proposed taxonomy and\nmapping also helps to provide an easy way for new practitioners to understand\nthis complex area of research. \n\n"}
{"id": "cs/0506059", "contents": "Title: Existentially Restricted Quantified Constraint Satisfaction Abstract: The quantified constraint satisfaction problem (QCSP) is a powerful framework\nfor modelling computational problems. The general intractability of the QCSP\nhas motivated the pursuit of restricted cases that avoid its maximal\ncomplexity. In this paper, we introduce and study a new model for investigating\nQCSP complexity in which the types of constraints given by the existentially\nquantified variables, is restricted. Our primary technical contribution is the\ndevelopment and application of a general technology for proving positive\nresults on parameterizations of the model, of inclusion in the complexity class\ncoNP. \n\n"}
{"id": "cs/0603084", "contents": "Title: Random 3CNF formulas elude the Lovasz theta function Abstract: Let $\\phi$ be a 3CNF formula with n variables and m clauses. A simple\nnonconstructive argument shows that when m is sufficiently large compared to n,\nmost 3CNF formulas are not satisfiable. It is an open question whether there is\nan efficient refutation algorithm that for most such formulas proves that they\nare not satisfiable. A possible approach to refute a formula $\\phi$ is: first,\ntranslate it into a graph $G_{\\phi}$ using a generic reduction from 3-SAT to\nmax-IS, then bound the maximum independent set of $G_{\\phi}$ using the Lovasz\n$\\vartheta$ function. If the $\\vartheta$ function returns a value $< m$, this\nis a certificate for the unsatisfiability of $\\phi$. We show that for random\nformulas with $m < n^{3/2 -o(1)}$ clauses, the above approach fails, i.e. the\n$\\vartheta$ function is likely to return a value of m. \n\n"}
{"id": "cs/9811015", "contents": "Title: An Emptiness Algorithm for Regular Types with Set Operators Abstract: An algorithm to decide the emptiness of a regular type expression with set\noperators given a set of parameterised type definitions is presented. The\nalgorithm can also be used to decide the equivalence of two regular type\nexpressions and the inclusion of one regular type expression in another. The\nalgorithm strictly generalises previous work in that tuple distributivity is\nnot assumed and set operators are permitted in type expressions. \n\n"}
{"id": "math/0506553", "contents": "Title: Introduction to Cirquent Calculus and Abstract Resource Semantics Abstract: This paper introduces a refinement of the sequent calculus approach called\ncirquent calculus. While in Gentzen-style proof trees sibling (or cousin, etc.)\nsequents are disjoint sequences of formulas, in cirquent calculus they are\npermitted to share elements. Explicitly allowing or disallowing shared\nresources and thus taking to a more subtle level the resource-awareness\nintuitions underlying substructural logics, cirquent calculus offers much\ngreater flexibility and power than sequent calculus does. A need for\nsubstantially new deductive tools came with the birth of computability logic\n(see http://www.cis.upenn.edu/~giorgi/cl.html) - the semantically constructed\nformal theory of computational resources, which has stubbornly resisted any\naxiomatization attempts within the framework of traditional syntactic\napproaches. Cirquent calculus breaks the ice. Removing contraction from the\nfull collection of its rules yields a sound and complete system for the basic\nfragment CL5 of computability logic. Doing the same in sequent calculus, on the\nother hand, throws out the baby with the bath water, resulting in the strictly\nweaker affine logic. An implied claim of computability logic is that it is CL5\nrather than affine logic that adequately materializes the resource philosophy\ntraditionally associated with the latter. To strengthen this claim, the paper\nfurther introduces an abstract resource semantics and shows the soundness and\ncompleteness of CL5 with respect to it. \n\n"}
{"id": "math/0508533", "contents": "Title: On the cascade rollback synchronization Abstract: We consider a cascade model of $N$ different processors performing a\ndistributed parallel simulation. The main goal of the study is to show that the\nlong-time dynamics of the system has a cluster behavior. To attack this problem\nwe combine two methods: stochastic comparison and Foster-Lyapunov functions. \n\n"}
