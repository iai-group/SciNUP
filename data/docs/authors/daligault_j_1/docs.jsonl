{"id": "0704.2505", "contents": "Title: Algebraic Distributed Space-Time Codes with Low ML Decoding Complexity Abstract: \"Extended Clifford algebras\" are introduced as a means to obtain low ML\ndecoding complexity space-time block codes. Using left regular matrix\nrepresentations of two specific classes of extended Clifford algebras, two\nsystematic algebraic constructions of full diversity Distributed Space-Time\nCodes (DSTCs) are provided for any power of two number of relays. The left\nregular matrix representation has been shown to naturally result in space-time\ncodes meeting the additional constraints required for DSTCs. The DSTCs so\nconstructed have the salient feature of reduced Maximum Likelihood (ML)\ndecoding complexity. In particular, the ML decoding of these codes can be\nperformed by applying the lattice decoder algorithm on a lattice of four times\nlesser dimension than what is required in general. Moreover these codes have a\nuniform distribution of power among the relays and in time, thus leading to a\nlow Peak to Average Power Ratio at the relays. \n\n"}
{"id": "0704.2596", "contents": "Title: Computing Extensions of Linear Codes Abstract: This paper deals with the problem of increasing the minimum distance of a\nlinear code by adding one or more columns to the generator matrix. Several\nmethods to compute extensions of linear codes are presented. Many codes\nimproving the previously known lower bounds on the minimum distance have been\nfound. \n\n"}
{"id": "0706.2725", "contents": "Title: The Complexity of Determining Existence a Hamiltonian Cycle is $O(n^3)$ Abstract: The Hamiltonian cycle problem in digraph is mapped into a matching cover\nbipartite graph. Based on this mapping, it is proved that determining existence\na Hamiltonian cycle in graph is $O(n^3)$. \n\n"}
{"id": "0709.0170", "contents": "Title: Untangling a Planar Graph Abstract: A straight-line drawing $\\delta$ of a planar graph $G$ need not be plane, but\ncan be made so by \\emph{untangling} it, that is, by moving some of the vertices\nof $G$. Let shift$(G,\\delta)$ denote the minimum number of vertices that need\nto be moved to untangle $\\delta$. We show that shift$(G,\\delta)$ is NP-hard to\ncompute and to approximate. Our hardness results extend to a version of\n\\textsc{1BendPointSetEmbeddability}, a well-known graph-drawing problem.\n  Further we define fix$(G,\\delta)=n-shift(G,\\delta)$ to be the maximum number\nof vertices of a planar $n$-vertex graph $G$ that can be fixed when untangling\n$\\delta$. We give an algorithm that fixes at least $\\sqrt{((\\log n)-1)/\\log\n\\log n}$ vertices when untangling a drawing of an $n$-vertex graph $G$. If $G$\nis outerplanar, the same algorithm fixes at least $\\sqrt{n/2}$ vertices. On the\nother hand we construct, for arbitrarily large $n$, an $n$-vertex planar graph\n$G$ and a drawing $\\delta_G$ of $G$ with fix$(G,\\delta_G) \\le \\sqrt{n-2}+1$ and\nan $n$-vertex outerplanar graph $H$ and a drawing $\\delta_H$ of $H$ with\nfix$(H,\\delta_H) \\le 2 \\sqrt{n-1}+1$. Thus our algorithm is asymptotically\nworst-case optimal for outerplanar graphs. \n\n"}
{"id": "0712.2857", "contents": "Title: Single-Exclusion Number and the Stopping Redundancy of MDS Codes Abstract: For a linear block code C, its stopping redundancy is defined as the smallest\nnumber of check nodes in a Tanner graph for C, such that there exist no\nstopping sets of size smaller than the minimum distance of C. Schwartz and\nVardy conjectured that the stopping redundancy of an MDS code should only\ndepend on its length and minimum distance.\n  We define the (n,t)-single-exclusion number, S(n,t) as the smallest number of\nt-subsets of an n-set, such that for each i-subset of the n-set, i=1,...,t+1,\nthere exists a t-subset that contains all but one element of the i-subset. New\nupper bounds on the single-exclusion number are obtained via probabilistic\nmethods, recurrent inequalities, as well as explicit constructions. The new\nbounds are used to better understand the stopping redundancy of MDS codes. In\nparticular, it is shown that for [n,k=n-d+1,d] MDS codes, as n goes to\ninfinity, the stopping redundancy is asymptotic to S(n,d-2), if d=o(\\sqrt{n}),\nor if k=o(\\sqrt{n}) and k goes to infinity, thus giving partial confirmation of\nthe Schwartz-Vardy conjecture in the asymptotic sense. \n\n"}
{"id": "0712.3203", "contents": "Title: Solving Medium-Density Subset Sum Problems in Expected Polynomial Time:\n  An Enumeration Approach Abstract: The subset sum problem (SSP) can be briefly stated as: given a target integer\n$E$ and a set $A$ containing $n$ positive integer $a_j$, find a subset of $A$\nsumming to $E$. The \\textit{density} $d$ of an SSP instance is defined by the\nratio of $n$ to $m$, where $m$ is the logarithm of the largest integer within\n$A$. Based on the structural and statistical properties of subset sums, we\npresent an improved enumeration scheme for SSP, and implement it as a complete\nand exact algorithm (EnumPlus). The algorithm always equivalently reduces an\ninstance to be low-density, and then solve it by enumeration. Through this\napproach, we show the possibility to design a sole algorithm that can\nefficiently solve arbitrary density instance in a uniform way. Furthermore, our\nalgorithm has considerable performance advantage over previous algorithms.\nFirstly, it extends the density scope, in which SSP can be solved in expected\npolynomial time. Specifically, It solves SSP in expected $O(n\\log{n})$ time\nwhen density $d \\geq c\\cdot \\sqrt{n}/\\log{n}$, while the previously best\ndensity scope is $d \\geq c\\cdot n/(\\log{n})^{2}$. In addition, the overall\nexpected time and space requirement in the average case are proven to be\n$O(n^5\\log n)$ and $O(n^5)$ respectively. Secondly, in the worst case, it\nslightly improves the previously best time complexity of exact algorithms for\nSSP. Specifically, the worst-case time complexity of our algorithm is proved to\nbe $O((n-6)2^{n/2}+n)$, while the previously best result is $O(n2^{n/2})$. \n\n"}
{"id": "0801.2838", "contents": "Title: An Algorithm for Road Coloring Abstract: A coloring of edges of a finite directed graph turns the graph into\nfinite-state automaton. The synchronizing word of a deterministic automaton is\na word in the alphabet of colors (considered as letters) of its edges that maps\nthe automaton to a single state. A coloring of edges of a directed graph of\nuniform outdegree (constant outdegree of any vertex) is synchronizing if the\ncoloring turns the graph into a deterministic finite automaton possessing a\nsynchronizing word. The road coloring problem is the problem of synchronizing\ncoloring of a directed finite strongly connected graph of uniform outdegree if\nthe greatest common divisor of the lengths of all its cycles is one. The\nproblem posed in 1970 had evoked a noticeable interest among the specialists in\nthe theory of graphs, automata, codes, symbolic dynamics as well as among the\nwide mathematical community. A polynomial time algorithm of $O(n^3)$ complexity\nin the most worst case and quadratic in majority of studied cases for the road\ncoloring of the considered graph is presented below. The work is based on\nrecent positive solution of the road coloring problem. The algorithm was\nimplemented in the package TESTAS \n\n"}
{"id": "0801.4130", "contents": "Title: Solving Min-Max Problems with Applications to Games Abstract: We refine existing general network optimization techniques, give new\ncharacterizations for the class of problems to which they can be applied, and\nshow that they can also be used to solve various two-player games in almost\nlinear time. Among these is a new variant of the network interdiction problem,\nwhere the interdictor wants to destroy high-capacity paths from the source to\nthe destination using a vertex-wise limited budget of arc removals. We also\nshow that replacing the limit average in mean payoff games by the maximum\nweight results in a class of games amenable to these techniques. \n\n"}
{"id": "0802.2184", "contents": "Title: Set Covering Problems with General Objective Functions Abstract: We introduce a parameterized version of set cover that generalizes several\npreviously studied problems. Given a ground set V and a collection of subsets\nS_i of V, a feasible solution is a partition of V such that each subset of the\npartition is included in one of the S_i. The problem involves maximizing the\nmean subset size of the partition, where the mean is the generalized mean of\nparameter p, taken over the elements. For p=-1, the problem is equivalent to\nthe classical minimum set cover problem. For p=0, it is equivalent to the\nminimum entropy set cover problem, introduced by Halperin and Karp. For p=1,\nthe problem includes the maximum-edge clique partition problem as a special\ncase. We prove that the greedy algorithm simultaneously approximates the\nproblem within a factor of (p+1)^1/p for any p in R^+, and that this is the\nbest possible unless P=NP. These results both generalize and simplify previous\nresults for special cases. We also consider the corresponding graph coloring\nproblem, and prove several tractability and inapproximability results. Finally,\nwe consider a further generalization of the set cover problem in which we aim\nat minimizing the sum of some concave function of the part sizes. As an\napplication, we derive an approximation ratio for a Rent-or-Buy set cover\nproblem. \n\n"}
{"id": "0803.1416", "contents": "Title: New formulas for Stirling-like numbers and Dobinski-like formulas Abstract: Extensions of the $Stirling$ numbers of the second kind and $Dobinski$ -like\nformulas are proposed in a series of exercises for graduates. Some of these new\nformulas recently discovered by me are to be found in the source paper $ [1]$.\nThese extensions naturally encompass the well known $q$- extensions. The\nindicatory references are to point at a part of the vast domain of the\nfoundations of computer science in arxiv affiliation. \n\n"}
{"id": "0805.1401", "contents": "Title: Approximation Algorithms for Shortest Descending Paths in Terrains Abstract: A path from s to t on a polyhedral terrain is descending if the height of a\npoint p never increases while we move p along the path from s to t. No\nefficient algorithm is known to find a shortest descending path (SDP) from s to\nt in a polyhedral terrain. We give two approximation algorithms (more\nprecisely, FPTASs) that solve the SDP problem on general terrains. Both\nalgorithms are simple, robust and easy to implement. \n\n"}
{"id": "0806.3626", "contents": "Title: On multi F-nomial coefficients and Inversion formula for F-nomial\n  coefficients Abstract: In response to [6], we discover the looked for inversion formula for F-nomial\ncoefficients. Before supplying its proof, we generalize F-nomial coefficients\nto multi F-nomial coefficients and we give their combinatorial interpretation\nin cobweb posets language, as the number of maximal-disjoint blocks of the form\nsP_{k_1,k_2,...,k_s} of layer <Phi_1-->Phi_n>. Then we present inversion\nformula for F-nomial coefficients using multi F-nomial coefficients for all\ncobweb-admissible sequences. To this end we infer also some identities as\nconclusions of that inversion formula for the case of binomial, Gaussian and\nFibonomial coefficients. \n\n"}
{"id": "0806.3668", "contents": "Title: Approximating Multi-Criteria Max-TSP Abstract: We present randomized approximation algorithms for multi-criteria Max-TSP.\nFor Max-STSP with k > 1 objective functions, we obtain an approximation ratio\nof $1/k - \\eps$ for arbitrarily small $\\eps > 0$. For Max-ATSP with k objective\nfunctions, we obtain an approximation ratio of $1/(k+1) - \\eps$. \n\n"}
{"id": "0807.0087", "contents": "Title: Path lengths in tree-child time consistent hybridization networks Abstract: Hybridization networks are representations of evolutionary histories that\nallow for the inclusion of reticulate events like recombinations,\nhybridizations, or lateral gene transfers. The recent growth in the number of\nhybridization network reconstruction algorithms has led to an increasing\ninterest in the definition of metrics for their comparison that can be used to\nassess the accuracy or robustness of these methods. In this paper we establish\nsome basic results that make it possible the generalization to tree-child time\nconsistent (TCTC) hybridization networks of some of the oldest known metrics\nfor phylogenetic trees: those based on the comparison of the vectors of path\nlengths between leaves. More specifically, we associate to each hybridization\nnetwork a suitably defined vector of `splitted' path lengths between its\nleaves, and we prove that if two TCTC hybridization networks have the same such\nvectors, then they must be isomorphic. Thus, comparing these vectors by means\nof a metric for real-valued vectors defines a metric for TCTC hybridization\nnetworks. We also consider the case of fully resolved hybridization networks,\nwhere we prove that simpler, `non-splitted' vectors can be used. \n\n"}
{"id": "0807.0644", "contents": "Title: Greedy D-Approximation Algorithm for Covering with Arbitrary Constraints\n  and Submodular Cost Abstract: This paper describes a simple greedy D-approximation algorithm for any\ncovering problem whose objective function is submodular and non-decreasing, and\nwhose feasible region can be expressed as the intersection of arbitrary (closed\nupwards) covering constraints, each of which constrains at most D variables of\nthe problem. (A simple example is Vertex Cover, with D = 2.) The algorithm\ngeneralizes previous approximation algorithms for fundamental covering problems\nand online paging and caching problems. \n\n"}
{"id": "0808.3331", "contents": "Title: Efficient algorithms for the basis of finite Abelian groups Abstract: Let $G$ be a finite abelian group $G$ with $N$ elements. In this paper we\ngive a O(N) time algorithm for computing a basis of $G$. Furthermore, we obtain\nan algorithm for computing a basis from a generating system of $G$ with $M$\nelements having time complexity $O(M\\sum_{p|N} e(p)\\lceil\np^{1/2}\\rceil^{\\mu(p)})$, where $p$ runs over all the prime divisors of $N$,\nand $p^{e(p)}$, $\\mu(p)$ are the exponent and the number of cyclic groups which\nare direct factors of the $p$-primary component of $G$, respectively. In case\nwhere $G$ is a cyclic group having a generating system with $M$ elements, a\n$O(MN^{\\epsilon})$ time algorithm for the computation of a basis of $G$ is\nobtained. \n\n"}
{"id": "0808.3881", "contents": "Title: Counting Hexagonal Patches and Independent Sets in Circle Graphs Abstract: A hexagonal patch is a plane graph in which inner faces have length 6, inner\nvertices have degree 3, and boundary vertices have degree 2 or 3. We consider\nthe following counting problem: given a sequence of twos and threes, how many\nhexagonal patches exist with this degree sequence along the outer face? This\nproblem is motivated by the study of benzenoid hydrocarbons and fullerenes in\ncomputational chemistry. We give the first polynomial time algorithm for this\nproblem. We show that it can be reduced to counting maximum independent sets in\ncircle graphs, and give a simple and fast algorithm for this problem. \n\n"}
{"id": "0810.2717", "contents": "Title: A Class of Graph-Geodetic Distances Generalizing the Shortest-Path and\n  the Resistance Distances Abstract: A new class of distances for graph vertices is proposed. This class contains\nparametric families of distances which reduce to the shortest-path, weighted\nshortest-path, and the resistance distances at the limiting values of the\nfamily parameters. The main property of the class is that all distances it\ncomprises are graph-geodetic: $d(i,j)+d(j,k)=d(i,k)$ if and only if every path\nfrom $i$ to $k$ passes through $j$. The construction of the class is based on\nthe matrix forest theorem and the transition inequality. \n\n"}
{"id": "0811.2546", "contents": "Title: Phase transition for Local Search on planted SAT Abstract: The Local Search algorithm (or Hill Climbing, or Iterative Improvement) is\none of the simplest heuristics to solve the Satisfiability and\nMax-Satisfiability problems. It is a part of many satisfiability and\nmax-satisfiability solvers, where it is used to find a good starting point for\na more sophisticated heuristics, and to improve a candidate solution. In this\npaper we give an analysis of Local Search on random planted 3-CNF formulas. We\nshow that if there is k<7/6 such that the clause-to-variable ratio is less than\nk ln(n) (n is the number of variables in a CNF) then Local Search whp does not\nfind a satisfying assignment, and if there is k>7/6 such that the\nclause-to-variable ratio is greater than k ln(n)$ then the local search whp\nfinds a satisfying assignment. As a byproduct we also show that for any\nconstant r there is g such that Local Search applied to a random (not\nnecessarily planted) 3-CNF with clause-to-variable ratio r produces an\nassignment that satisfies at least gn clauses less than the maximal number of\nsatisfiable clauses. \n\n"}
{"id": "0812.2636", "contents": "Title: Approximating the least hypervolume contributor: NP-hard in general, but\n  fast in practice Abstract: The hypervolume indicator is an increasingly popular set measure to compare\nthe quality of two Pareto sets. The basic ingredient of most hypervolume\nindicator based optimization algorithms is the calculation of the hypervolume\ncontribution of single solutions regarding a Pareto set. We show that exact\ncalculation of the hypervolume contribution is #P-hard while its approximation\nis NP-hard. The same holds for the calculation of the minimal contribution. We\nalso prove that it is NP-hard to decide whether a solution has the least\nhypervolume contribution. Even deciding whether the contribution of a solution\nis at most $(1+\\eps)$ times the minimal contribution is NP-hard. This implies\nthat it is neither possible to efficiently find the least contributing solution\n(unless $P = NP$) nor to approximate it (unless $NP = BPP$).\n  Nevertheless, in the second part of the paper we present a fast approximation\nalgorithm for this problem. We prove that for arbitrarily given $\\eps,\\delta>0$\nit calculates a solution with contribution at most $(1+\\eps)$ times the minimal\ncontribution with probability at least $(1-\\delta)$. Though it cannot run in\npolynomial time for all instances, it performs extremely fast on various\nbenchmark datasets. The algorithm solves very large problem instances which are\nintractable for exact algorithms (e.g., 10000 solutions in 100 dimensions)\nwithin a few seconds. \n\n"}
{"id": "0812.4073", "contents": "Title: Multi-level algorithms for modularity clustering Abstract: Modularity is one of the most widely used quality measures for graph\nclusterings. Maximizing modularity is NP-hard, and the runtime of exact\nalgorithms is prohibitive for large graphs. A simple and effective class of\nheuristics coarsens the graph by iteratively merging clusters (starting from\nsingletons), and optionally refines the resulting clustering by iteratively\nmoving individual vertices between clusters. Several heuristics of this type\nhave been proposed in the literature, but little is known about their relative\nperformance.\n  This paper experimentally compares existing and new coarsening- and\nrefinement-based heuristics with respect to their effectiveness (achieved\nmodularity) and efficiency (runtime). Concerning coarsening, it turns out that\nthe most widely used criterion for merging clusters (modularity increase) is\noutperformed by other simple criteria, and that a recent algorithm by Schuetz\nand Caflisch is no improvement over simple greedy coarsening for these\ncriteria. Concerning refinement, a new multi-level algorithm is shown to\nproduce significantly better clusterings than conventional single-level\nalgorithms. A comparison with published benchmark results and algorithm\nimplementations shows that combinations of coarsening and multi-level\nrefinement are competitive with the best algorithms in the literature. \n\n"}
{"id": "0901.0920", "contents": "Title: A Determination of the Spin of the Black Hole Primary in LMC X-1 Abstract: The first extragalactic X-ray binary, LMC X-1, was discovered in 1969. In the\n1980s, its compact primary was established as the fourth dynamical black-hole\ncandidate. Recently, we published accurate values for the mass of the black\nhole and the orbital inclination angle of the binary system. Building on these\nresults, we have analyzed 53 X-ray spectra obtained by RXTE and, using a\nselected sample of 18 of these spectra, we have determined the dimensionless\nspin parameter of the black hole to be a* = 0.92(-0.07,+0.05). This result\ntakes into account all sources of observational and model-parameter\nuncertainties. The standard deviation around the mean value of a* for these 18\nX-ray spectra, which were obtained over a span of several years, is only 0.02.\nWhen we consider our complete sample of 53 RXTE spectra, we find a somewhat\nhigher value of the spin parameter and a larger standard deviation. Finally, we\nshow that our results based on RXTE data are confirmed by our analyses of\nselected X-ray spectra obtained by the XMM-Newton, BeppoSAX and Ginga missions. \n\n"}
{"id": "0901.4302", "contents": "Title: Different types of ultraluminous X-ray sources in NGC 4631 Abstract: We have re-examined the most luminous X-ray sources in the starburst galaxy\nNGC 4631, using XMM-Newton, Chandra and ROSAT data. The most interesting source\nis a highly variable supersoft ULX. We suggest that its bolometric luminosity ~\na few 10^{39} erg/s in the high/supersoft state: this is an order of magnitude\nlower than estimated in previous studies, thus reducing the need for extreme or\nexotic scenarios. Moreover, we find that this source was in a non-canonical\nlow/soft (kT ~ 0.1-0.3 keV) state during the Chandra observation. By comparing\nthe high and low state, we argue that the spectral properties may not be\nconsistent with the expected behaviour of an accreting intermediate-mass black\nhole. We suggest that recurrent super-Eddington outbursts with photospheric\nexpansion from a massive white dwarf (M_{wd} >~ 1.3 M_{sun}), powered by\nnon-steady nuclear burning, may be a viable possibility, in alternative to the\npreviously proposed scenario of a super-Eddington outflow from an accreting\nstellar-mass black hole. The long-term average accretion rate required for\nnuclear burning to power such white-dwarf outbursts in this source and perhaps\nin other supersoft ULXs is ~ 5-10 x 10^{-6} M_{sun}/yr: this is comparable to\nthe thermal-timescale mass transfer rate invoked to explain the most luminous\nhard-spectrum ULXs (powered by black hole accretion). The other four most\nluminous X-ray sources in NGC 4631 (three of which can be classified as ULXs)\nappear to be typical accreting black holes, in four different spectral states:\nhigh/soft, convex-spectrum, power-law with soft excess, and simple power-law.\nNone of them requires masses >~ 50 M_{sun}. \n\n"}
{"id": "0902.0720", "contents": "Title: Topology of magnetars external field. I. Axially symmetric fields Abstract: There is an increasing theoretical and observational evidence that the\nexternal magnetic field of magnetars may contain a toroidal component, likely\nof the same order of the poloidal one. Such \"twisted magnetospheres\" are\nthreaded by currents flowing along the closed field lines which can efficiently\ninteract with soft thermal photons via resonant cyclotron scatterings (RCS).\nActually, RCS spectral models proved quite successful in explaining the\npersistent ~1-10 keV emission from the magnetar candidates, the soft gamma-ray\nrepeaters (SGRs) and the anomalous X-ray pulsars (AXPs). Moreover, it has been\nproposed that, in presence of highly relativistic electrons, the same process\ncan give rise to the observed hard X-ray spectral tails extending up to ~200\nkeV. Spectral calculations have been restricted up to now to the case of a\nglobally twisted dipolar magnetosphere, although there are indications that the\ntwist may be confined only to a portion of the magnetosphere, and/or that the\nlarge scale field is more complex than a simple dipole. In this paper we\ninvestigate multipolar, force-free magnetospheres of ultra-magnetized neutron\nstars. We first discuss a general method to generate multipolar solutions of\nthe Grad- Schluter-Shafranov equation, and analyze in detail dipolar,\nquadrupolar and octupolar fields. The spectra and lightcurves for these\nmultipolar, globally twisted fields are then computed using a Monte Carlo code\nand compared with those of a purely dipolar configuration. Finally the\nphase-resolved spectra and energy-dependent lightcurves obtained with a simple\nmodel of a locally sheared field are confronted with the INTEGRAL observations\nof the AXPs 1RXS J1708-4009 and 4U 0142+61. Results support a picture in which\nthe field in these two sources is not globally twisted. \n\n"}
{"id": "0902.0761", "contents": "Title: The redshift and afterglow of the extremely energetic gamma-ray burst\n  GRB 080916C Abstract: The detection of GeV photons from gamma-ray bursts (GRBs) has important\nconsequences for the interpretation and modelling of these most-energetic\ncosmological explosions. The full exploitation of the high-energy measurements\nrelies, however, on the accurate knowledge of the distance to the events. Here\nwe report on the discovery of the afterglow and subsequent redshift\ndetermination of GRB 080916C, the first GRB detected by the Fermi Gamma-Ray\nSpace Telescope with high significance detection of photons at >0.1 GeV.\nObservations were done with 7-channel imager GROND at the 2.2m MPI/ESO\ntelescope, the SIRIUS instrument at the Nagoya-SAAO 1.4m telescope in South\nAfrica, and the GMOS instrument at Gemini-S. The afterglow photometric redshift\nof z=4.35+-0.15, based on simultaneous 7-filter observations with the Gamma-Ray\nOptical and Near-infrared Detector (GROND), places GRB 080916C among the top 5%\nmost distant GRBs, and makes it the most energetic GRB known to date. The\ndetection of GeV photons from such a distant event is rather surprising.\n  The observed gamma-ray variability in the prompt emission together with the\nredshift suggests a lower limit for the Lorentz factor of the\nultra-relativistic ejecta of Gamma > 1090. This value rivals any previous\nmeasurements of Gamma in GRBs and strengthens the extreme nature of GRB\n080916C. \n\n"}
{"id": "0902.1340", "contents": "Title: Fermi Large Area Telescope Bright Gamma-ray Source List Abstract: Following its launch in June 2008, the Fermi Gamma-ray Space Telescope\n(Fermi) began a sky survey in August. The Large Area Telescope (LAT) on Fermi\nin 3 months produced a deeper and better-resolved map of the gamma-ray sky than\nany previous space mission. We present here initial results for energies above\n100 MeV for the 205 most significant (statistical significance greater than\n~10-sigma) gamma-ray sources in these data. These are the best-characterized\nand best-localized point-like (i.e., spatially unresolved) gamma-ray sources in\nthe early-mission data. \n\n"}
{"id": "0902.1693", "contents": "Title: Fast Evaluation of Interlace Polynomials on Graphs of Bounded Treewidth Abstract: We consider the multivariate interlace polynomial introduced by Courcelle\n(2008), which generalizes several interlace polynomials defined by Arratia,\nBollobas, and Sorkin (2004) and by Aigner and van der Holst (2004). We present\nan algorithm to evaluate the multivariate interlace polynomial of a graph with\nn vertices given a tree decomposition of the graph of width k. The best\npreviously known result (Courcelle 2008) employs a general logical framework\nand leads to an algorithm with running time f(k)*n, where f(k) is doubly\nexponential in k. Analyzing the GF(2)-rank of adjacency matrices in the context\nof tree decompositions, we give a faster and more direct algorithm. Our\nalgorithm uses 2^{3k^2+O(k)}*n arithmetic operations and can be efficiently\nimplemented in parallel. \n\n"}
{"id": "0902.1830", "contents": "Title: Optical flashes, reverse shocks and magnetization Abstract: Despite the pre-Swift expectation that bright optical flashes from reverse\nshocks would be prevalent in early-time afterglow emission, rapid response\nobservations show this not to be the case. Although very bright at early times,\nsome GRBs such as GRB 061007 and GRB 060418, lack the short-lived optical flash\nfrom the reverse shock within minutes after the GRB. In contrast, other optical\nafterglows, such as those of GRB 990123, GRB 021211, GRB 060111B, GRB 060117,\nGRB 061126, and recently GRB 080319B, show a steep-to-flat transition within\nfirst 10^3 s typical of a rapidly evolving reverse + forward shock combination.\nWe review the presence and absence of the reverse shock components in optical\nafterglows and discuss the implications for the standard model and the\nmagnetization of the fireball. We show that the previously predicted optical\nflashes are likely to occur at lower wavelengths, perhaps as low as radio\nwavelengths and, by using the case of GRB 061126 we show that the magnetic\nenergy density in the ejecta, expressed as a fraction of the equipartion value,\nis a key physical parameter. \n\n"}
{"id": "0902.2959", "contents": "Title: First AGILE Catalog of High Confidence Gamma-Ray Sources Abstract: We present the first catalog of high-confidence gamma-ray sources detected by\nthe AGILE satellite during observations performed from July 9, 2007 to June 30,\n2008. Catalogued sources are detected by merging all the available data over\nthe entire time period. AGILE, launched in April 2007, is an ASI mission\ndevoted to gamma-ray observations in the 30 MeV - 50 GeV energy range, with\nsimultaneous X-ray imaging capability in the 18-60 keV band. This catalog is\nbased on Gamma-Ray Imaging Detector (GRID) data for energies greater than 100\nMeV. For the first AGILE catalog we adopted a conservative analysis, with a\nhigh-quality event filter optimized to select gamma-ray events within the\ncentral zone of the instrument Field of View (radius of 40 degrees). This is a\nsignificance-limited (4 sigma) catalog, and it is not a complete flux-limited\nsample due to the non-uniform first year AGILE sky coverage. The catalog\nincludes 47 sources, 21 of which are associated with confirmed or candidate\npulsars, 13 with Blazars (7 FSRQ, 4 BL Lacs, 2 unknown type), 2 with HMXRBs, 2\nwith SNRs, 1 with a colliding-wind binary system, 8 with unidentified sources. \n\n"}
{"id": "0902.4039", "contents": "Title: Continuous frequency spectrum of the global hydromagnetic oscillations\n  of a magnetically confined mountain on an accreting neutron star Abstract: We compute the continuous part of the ideal-magnetohydrodynamic (ideal-MHD)\nfrequency spectrum of a polar mountain produced by magnetic burial on an\naccreting neutron star. Applying the formalism developed by Hellsten & Spies\n(1979), extended to include gravity, we solve the singular eigenvalue problem\nsubject to line-tying boundary conditions. This spectrum divides into an\nAlfv\\'{e}n part and a cusp part. The eigenfunctions are chirped and anharmonic\nwith an exponential envelope, and the eigenfrequencies cover the whole spectrum\nabove a minimum $\\omega_\\mathrm{low}$. For equilibria with accreted mass $1.2\n\\times 10^{-6} \\la M_a/M_\\odot \\la 1.7 \\times 10^{-4}$ and surface magnetic\nfields $10^{11} \\la B_\\ast/\\mathrm{G} \\la 10^{13}$, $\\omega_\\mathrm{low}$ is\napproximately independent of $B_\\ast$, and increases with $M_a$. The results\nare consistent with the Alfv\\'{e}n spectrum excited in numerical simulations\nwith the \\textsc{zeus-mp} solver. The spectrum is modified substantially by the\nCoriolis force in neutron stars spinning faster than $\\sim 100$ Hz. The\nimplications for gravitational wave searches for low-mass X-ray binaries are\nconsidered briefly. \n\n"}
{"id": "0903.0317", "contents": "Title: Sommerfeld enhancement for a Yukawa potential Abstract: We show how easy it is to get the Sommerfeld enhancement for a Yukawa\npotential, for definite partial waves, beyond the S wave analyzed in previous\nliterature. In particular, we report results for the P wave (for which there is\na resonant pattern and the enhancement can be of several orders of magnitude\neven far from the resonance) that could be relevant for the analysis of\nexperimental cosmic rays data possibly signaling the annihilation of dark\nmatter particles. \n\n"}
{"id": "0903.0544", "contents": "Title: A constructive proof of the general Lovasz Local Lemma Abstract: The Lovasz Local Lemma [EL75] is a powerful tool to non-constructively prove\nthe existence of combinatorial objects meeting a prescribed collection of\ncriteria. In his breakthrough paper [Bec91], Beck demonstrated that a\nconstructive variant can be given under certain more restrictive conditions.\nSimplifications of his procedure and relaxations of its restrictions were\nsubsequently exhibited in several publications [Alo91, MR98, CS00, Mos06,\nSri08, Mos08]. In [Mos09], a constructive proof was presented that works under\nnegligible restrictions, formulated in terms of the Bounded Occurrence\nSatisfiability problem. In the present paper, we reformulate and improve upon\nthese findings so as to directly apply to almost all known applications of the\ngeneral Local Lemma. \n\n"}
{"id": "0903.2981", "contents": "Title: Energy Spectra of the Soft X-ray Diffuse Emission in Fourteen Fields\n  Observed with Suzaku Abstract: The soft diffuse X-ray emission of twelve fields observed with Suzaku are\npresented together with two additional fields from previous analyses. All have\ngalactic longitudes 65 deg < l < 295 deg to avoid contributions from the very\nbright diffuse source that extends at least 30 deg from the Galactic center.\nThe surface brightnesses of the Suzaku nine fields for which apparently\nuncontaminated ROSAT All Sky Survey (RASS) were available were statistically\nconsistent with the RASS values, with an upper limit for differences of 17 x\n10^{-6} c s^{-1} amin^{-2} in R45}-band. The Ovii and Oviii intensities are\nwell correlated to each other, and Ovii emission shows an intensity floor at ~2\nphotons s^{-1} cm^{-2 str^{-1} (LU). The high-latitude Oviii emission shows a\ntight correlation with excess of Ovii emission above the floor, with (Oviii\nintensity) = 0.5 x [(Ovii intensity) -2 LU], suggesting that temperatures\naveraged over different line-of-sight show a narrow distribution around ~0.2\nkeV. We consider that the offset intensity of Ovii arises from the Heliospheric\nsolar wind charge exchange and perhaps from the local hot bubble, and that the\nexcess Ovii (2-7 LU) is emission from more distant parts of the Galaxy. The\ntotal bolometric luminosity of this galactic emission is estimated to be 4 x\n10^{39} erg s^{-1}, and its characteristic temperature may be related to the\nvirial temperature of the Galaxy. \n\n"}
{"id": "0904.4911", "contents": "Title: On the Algorithmic Complexity of the Mastermind Game with Black-Peg\n  Results Abstract: In this paper, we study the algorithmic complexity of the Mastermind game,\nwhere results are single-color black pegs. This differs from the usual\ndual-color version of the game, but better corresponds to applications in\ngenetics. We show that it is NP-complete to determine if a sequence of\nsingle-color Mastermind results have a satisfying vector. We also show how to\ndevise efficient algorithms for discovering a hidden vector through\nsingle-color queries. Indeed, our algorithm improves a previous method of\nChvatal by almost a factor of 2. \n\n"}
{"id": "0905.0145", "contents": "Title: An ultracompact X-ray binary in the globular cluster NGC 1851 Abstract: We present far-ultraviolet photometry obtained with the Hubble Space\nTelescope of the low-mass X-ray binary 4U 0513-40 in the globular cluster NGC\n1851. Our observations reveal a clear, roughly sinusoidal periodic signal with\n$P \\simeq 17$ min and amplitude 3%-10%. The signal appears fully coherent and\ncan be modelled as a simple reprocessing effect associated with the changing\nprojected area presented by the irradiated face of a white dwarf donor star in\nthe system. All of these properties suggest that the signal we have detected is\norbital in nature, thus confirming 4U 0513-40 as an ultracompact X-ray binary\n(UCXB). All four confirmed UCXBs in globular clusters have orbital periods\nbelow 30 minutes, whereas almost all UCXBs in the Galactic field have orbital\nperiods longer than this. This suggests that the dynamical formation processes\ndominate UCXB production in clusters, producing a different orbital period\ndistribution than observed among field UCXBs. Based on the likely system\nparameters, we show that 4U 0513-40 should be a strong gravitational wave\nsource and may be detectable by LISA over the course of a multi-year mission. \n\n"}
{"id": "0905.1162", "contents": "Title: A new interpretation of the gamma-ray observations of active galactic\n  nuclei Abstract: Gamma-ray telescopes have reported some surprising observations of multi-TeV\nphotons from distant active galactic nuclei (AGN), which show no significant\nattenuation due to pair production on either the extragalactic background light\n(EBL), or the photons near the source. We suggest a new interpretation of these\nobservations, which is consistent with both the EBL calculations and the AGN\nmodels. Cosmic rays with energies below 50 EeV, produced by AGN, can cross\ncosmological distances, interact with EBL relatively close to Earth, and\ngenerate the secondary photons observed by gamma-ray telescopes. We calculate\nthe spectrum of the secondary photons and find that it agrees with the\ngamma-ray data. The delays in the proton arrival times can explain the orphan\nflares, the lack of time correlations, and the mismatch of the variability time\nscales inferred from the multiwavelength observations. The gamma-ray data are\nconsistent with the detection of the secondary photons, which has important\nramifications for gamma-ray astronomy, cosmic ray physics, EBL, and the\nintergalactic magnetic fields (IGMF). \n\n"}
{"id": "0905.3949", "contents": "Title: t-Pebbling and Extensions Abstract: Graph pebbling is the study of moving discrete pebbles from certain initial\ndistributions on the vertices of a graph to various target distributions via\npebbling moves. A pebbling move removes two pebbles from a vertex and places\none pebble on one of its neighbors (losing the other as a toll). For t >= 1 the\nt-pebbling number of a graph is the minimum number of pebbles necessary so that\nfrom any initial distribution of them it is possible to move t pebbles to any\nvertex. We provide the best possible upper bound on the t-pebbling number of a\ndiameter two graph, proving a conjecture of Curtis, et al., in the process. We\nalso give a linear time (in the number of edges) algorithm to t-pebble such\ngraphs, as well as a quartic time (in the number of vertices) algorithm to\ncompute the pebbling number of such graphs, improving the best known result of\nBekmetjev and Cusack. Furthermore, we show that, for complete graphs, cycles,\ntrees, and cubes, we can allow the target to be any distribution of t pebbles\nwithout increasing the corresponding t-pebbling numbers; we conjecture that\nthis behavior holds for all graphs. Finally, we explore fractional and optimal\nfractional versions of pebbling, proving the fractional pebbling number\nconjecture of Hurlbert and using linear optimization to reveal results on the\noptimal fractional pebbling number of vertex-transitive graphs. \n\n"}
{"id": "0906.0575", "contents": "Title: The blazar S5 0014+813: a real or apparent monster? Abstract: A strong hard X-ray luminosity from a blazar flags the presence of a very\npowerful jet. If the jet power is in turn related to the mass accretion rate,\nthe most luminous hard X-ray blazars should pinpoint the largest accretion\nrates, and therefore the largest black hole masses. These ideas are confirmed\nby the Swift satellite observations of the blazar S5 0014+813, at the redshift\nz=3.366. Swift detected this source with all its three instruments, from the\noptical to the hard X-rays. Through the construction of its spectral energy\ndistribution we are confident that its optical-UV emission is thermal in\norigin. Associating it to the emission of a standard optically thick\ngeometrically thin accretion disk, we find a black hole mass of 40 billion\nsolar masses, radiating at 40% the Eddington value. The derived mass is among\nthe largest ever found. Super-Eddington slim disks or thick disks with the\npresence of a collimating funnel can in principle reduce the black hole mass\nestimate, but tends to produce spectra bluer than observed. \n\n"}
{"id": "0906.2577", "contents": "Title: Chandra Localizations and Spectra of INTEGRAL Sources in the Galactic\n  Plane: The Cycle 9 Sample Abstract: We report on 0.3-10 keV X-ray observations by the Chandra X-ray Observatory\nof the fields of 22 sources that were discovered as hard X-ray (20-100 keV)\nsources by the INTEGRAL satellite (IGR sources). The purpose of the Chandra\nobservations is to localize the sources and to measure their soft X-ray spectra\nin order to determine the nature of the sources. We find very likely Chandra\ncounterparts for 18 of the 22 sources. We discuss the implications for each\nsource, considering previous results and new optical or IR identifications, and\nwe identify or suggest identifications for the nature of 16 of the sources. Two\nof the sources, IGR J14003-6326 and IGR J17448-3232, are extended on arcminute\nscales. We identify the former as a pulsar wind nebula (PWN) with a surrounding\nsupernova remnant (SNR) and the latter as a SNR. In the group of 242 IGR\nsources, there is only one other source that has previously been identified as\na SNR. We confirm a previous identification of IGR J14331-6112 as an High-Mass\nX-ray Binary (HMXB), and we suggest that IGR J17404-3655, IGR J16287-5021, IGR\nJ17354-3255, IGR J17507-2647, IGR J17586-2129, and IGR J13186-6257 are\ncandidate HMXBs. Our results indicate or confirm that IGR J19267+1325, IGR\nJ18173-2509, and IGR J18308-1232 are Cataclysmic Variables (CVs), and we\nsuggest that IGR J15529-5029 may also be a CV. We confirm that IGR J14471-6414\nis an Active Galactic Nucleus (AGN), and we also suggest that IGR J19443+2117\nand IGR J18485-0047 may be AGN. Finally, we found Chandra counterparts for IGR\nJ11098-6457 and IGR J18134-1636, but more information is required to determine\nthe nature of these two sources. \n\n"}
{"id": "0906.3411", "contents": "Title: The radio counterpart of the likely TeV binary HESS J0632+057 Abstract: The few known gamma-ray binary systems are all associated with variable radio\nand X-ray emission. The TeV source HESS J0632+057, apparently associated with\nthe Be star MWC148, is plausibly a new member of this class. Following the\nidentification of a variable X-ray counterpart to the TeV source we conducted\nGMRT and VLA observations in June-September 2008 to search for the radio\ncounterpart of this object. A point-like radio source at the position of the\nstar is detected in both 1280 MHz GMRT and 5 GHz VLA observations, with an\naverage spectral index, alpha, of ~0.6. In the VLA data there is significant\nflux variability on ~month timescales around the mean flux density of ~0.3 mJy.\nThese radio properties (and the overall spectral energy distribution) are\nconsistent with an interpretation of HESS J0632+057 as a lower power analogue\nof the established gamma-ray binary systems. \n\n"}
{"id": "0907.0385", "contents": "Title: Instrument simulation for the analysis of cosmic ray electron with the\n  Fermi LAT Abstract: The Fermi LAT collaboration has built up a detailed Monte Carlo simulation to\ncharacterize the instrument response and tune its performance. The simulation\ncode is built around the widely used GEANT4 toolkit and was carefully validated\nagainst beam test and flight data. This poster shows how the full LAT\nsimulation is used to develop the event selection for the Cosmic-Ray Electron\n(\\emph{CRE}) analysis so as to optimize the instrument performance. In\nparticular, we will show in detail the determination of the geometry factor and\nthe residual hadron contamination. The very accurate MC simulation proved to be\nfundamental to control the systematic uncertainties on the CRE spectrum\nmeasured by the Fermi LAT. \n\n"}
{"id": "0907.0559", "contents": "Title: The GALPROP Cosmic-Ray Propagation Code Abstract: The Galactic cosmic-ray propagation code GALPROP is designed to make\npredictions of many kinds of data self-consistently, including direct\ncosmic-ray measurements, gamma rays and synchrotron radiation. In the decade\nsince its conception it has undergone considerable development, which is\ncontinuing. A public version was made available a few years ago, supported by a\nwebsite. We describe the new features of the current version which will become\npart of the next public release. Plans for future developments are also\nmentioned. \n\n"}
{"id": "0907.4573", "contents": "Title: Solving MAX-r-SAT Above a Tight Lower Bound Abstract: We present an exact algorithm that decides, for every fixed $r \\geq 2$ in\ntime $O(m) + 2^{O(k^2)}$ whether a given multiset of $m$ clauses of size $r$\nadmits a truth assignment that satisfies at least $((2^r-1)m+k)/2^r$ clauses.\nThus \\textsc{Max-$r$-Sat} is fixed-parameter tractable when parameterized by\nthe number of satisfied clauses above the tight lower bound $(1-2^{-r})m$. This\nsolves an open problem of Mahajan et al. (J. Comput. System Sci., 75, 2009).\n  Our algorithm is based on a polynomial-time data reduction procedure that\nreduces a problem instance to an equivalent algebraically represented problem\nwith $O(k^2)$ variables. This is done by representing the instance as an\nappropriate polynomial, and by applying a probabilistic argument combined with\nsome simple tools from Harmonic analysis to show that if the polynomial cannot\nbe reduced to one of size $O(k^2)$, then there is a truth assignment satisfying\nthe required number of clauses.\n  We introduce a new notion of bikernelization from a parameterized problem to\nanother one and apply it to prove that the above-mentioned parameterized\n\\textsc{Max-$r$-Sat} admits a polynomial-size kernel.\n  Combining another probabilistic argument with tools from graph matching\ntheory and signed graphs, we show that if an instance of \\textsc{Max-2-Sat}\nwith $m$ clauses has at least $3k$ variables after application of certain\npolynomial time reduction rules to it, then there is a truth assignment that\nsatisfies at least $(3m+k)/4$ clauses.\n  We also outline how the fixed-parameter tractability and polynomial-size\nkernel results on \\textsc{Max-$r$-Sat} can be extended to more general families\nof Boolean Constraint Satisfaction Problems. \n\n"}
{"id": "0907.4778", "contents": "Title: Galactic Substructure and Energetic Neutrinos from the Sun and the Earth Abstract: We consider the effects of Galactic substructure on energetic neutrinos from\nannihilation of weakly-interacting massive particles (WIMPs) that have been\ncaptured by the Sun and Earth. Substructure gives rise to a time-varying\ncapture rate and thus to time variation in the annihilation rate and resulting\nenergetic-neutrino flux. However, there may be a time lag between the capture\nand annihilation rates. The energetic-neutrino flux may then be determined by\nthe density of dark matter in the Solar System's past trajectory, rather than\nthe local density. The signature of such an effect may be sought in the ratio\nof the direct- to indirect-detection rates. \n\n"}
{"id": "0907.5372", "contents": "Title: Speedup in the Traveling Repairman Problem with Unit Time Windows Abstract: The input to the unrooted traveling repairman problem is an undirected metric\ngraph and a subset of nodes, each of which has a time window of unit length.\nGiven that a repairman can start at any location, the goal is to plan a route\nthat visits as many nodes as possible during their respective time windows. A\npolynomial-time bicriteria approximation algorithm is presented for this\nproblem, gaining an increased fraction of repairman visits for increased\nspeedup of repairman motion. For speedup $s$, we find a $6\\gamma/(s +\n1)$-approximation for $s$ in the range $1 \\leq s \\leq 2$ and a\n$4\\gamma/s$-approximation for $s$ in the range $2 \\leq s \\leq 4$, where $\\gamma\n= 1$ on tree-shaped networks and $\\gamma = 2 + \\epsilon$ on general metric\ngraphs. \n\n"}
{"id": "0908.0375", "contents": "Title: Deterministic Algorithms for the Lovasz Local Lemma Abstract: The Lovasz Local Lemma (LLL) is a powerful result in probability theory that\nstates that the probability that none of a set of bad events happens is nonzero\nif the probability of each event is small compared to the number of events that\ndepend on it. It is often used in combination with the probabilistic method for\nnon-constructive existence proofs. A prominent application is to k-CNF\nformulas, where LLL implies that, if every clause in the formula shares\nvariables with at most d <= 2^k/e other clauses then such a formula has a\nsatisfying assignment. Recently, a randomized algorithm to efficiently\nconstruct a satisfying assignment was given by Moser. Subsequently Moser and\nTardos gave a randomized algorithm to construct the structures guaranteed by\nthe LLL in a very general algorithmic framework. We address the main problem\nleft open by Moser and Tardos of derandomizing these algorithms efficiently.\nSpecifically, for a k-CNF formula with m clauses and d <= 2^{k/(1+\\eps)}/e for\nany \\eps\\in (0,1), we give an algorithm that finds a satisfying assignment in\ntime \\tilde{O}(m^{2(1+1/\\eps)}). This improves upon the deterministic\nalgorithms of Moser and of Moser-Tardos with running time m^{\\Omega(k^2)} which\nis superpolynomial for k=\\omega(1) and upon other previous algorithms which\nwork only for d\\leq 2^{k/16}/4. Our algorithm works efficiently for a general\nversion of LLL under the algorithmic framework of Moser and Tardos, and is also\nparallelizable, i.e., has polylogarithmic running time using polynomially many\nprocessors. \n\n"}
{"id": "0908.2059", "contents": "Title: Early phase observations of extremely luminous Type Ia Supernova 2009dc Abstract: We present early phase observations in optical and near-infrared wavelengths\nfor the extremely luminous Type Ia supernova (SN Ia) 2009dc. The decline rate\nof the light curve is $\\Delta m_{15}(B)=0.65\\pm 0.03$, which is one of the\nslowest among SNe Ia. The peak $V$-band absolute magnitude is $M_{V}=-19.90\\pm\n0.15$ mag even if the host extinction is $A_{V}=0$ mag. It reaches\n$M_{V}=-20.19\\pm 0.19$ mag for the host extinction of $A_{V}=0.29$ mag as\ninferred from the observed Na {\\sc i} D line absorption in the host. Our\n$JHK_{s}$-band photometry shows that the SN is one of the most luminous SNe Ia\nalso in near-infrared wavelengths. These results indicate that SN 2009dc\nbelongs to the most luminous class of SNe Ia, like SN 2003fg and SN 2006gz. We\nestimate the ejected $^{56}$Ni mass of $1.2\\pm 0.3$ $\\Msun$ for no host\nextinction case (or 1.6$\\pm$ 0.4 M$_{\\odot}$ for the host extinction of\n$A_{V}=0.29$ mag). The C {\\sc ii} $\\lambda$6580 absorption line keeps visible\nuntil a week after maximum, which diminished in SN 2006gz before its maximum\nbrightness. The line velocity of Si {\\sc ii} $\\lambda$6355 is about 8000 km\ns$^{-1}$ around the maximum, being considerably slower than that of SN 2006gz,\nwhile comparable to that of SN 2003fg. The velocity of the C {\\sc ii} line is\nalmost comparable to that of the Si {\\sc ii}. The presence of the carbon line\nsuggests that thick unburned C+O layers remain after the explosion. SN 2009dc\nis a plausible candidate of the super-Chandrasekhar mass SNe Ia. \n\n"}
{"id": "0909.1062", "contents": "Title: New Approximation Algorithms for Minimum Enclosing Convex Shapes Abstract: Given $n$ points in a $d$ dimensional Euclidean space, the Minimum Enclosing\nBall (MEB) problem is to find the ball with the smallest radius which contains\nall $n$ points. We give a $O(nd\\Qcal/\\sqrt{\\epsilon})$ approximation algorithm\nfor producing an enclosing ball whose radius is at most $\\epsilon$ away from\nthe optimum (where $\\Qcal$ is an upper bound on the norm of the points). This\nimproves existing results using \\emph{coresets}, which yield a $O(nd/\\epsilon)$\ngreedy algorithm. Finding the Minimum Enclosing Convex Polytope (MECP) is a\nrelated problem wherein a convex polytope of a fixed shape is given and the aim\nis to find the smallest magnification of the polytope which encloses the given\npoints. For this problem we present a $O(mnd\\Qcal/\\epsilon)$ approximation\nalgorithm, where $m$ is the number of faces of the polytope. Our algorithms\nborrow heavily from convex duality and recently developed techniques in\nnon-smooth optimization, and are in contrast with existing methods which rely\non geometric arguments. In particular, we specialize the excessive gap\nframework of \\citet{Nesterov05a} to obtain our results. \n\n"}
{"id": "0909.2572", "contents": "Title: 'Disc-jet' coupling in black hole X-ray binaries and active galactic\n  nuclei Abstract: In this chapter I will review the status of our phenomenological\nunderstanding of the relation between accretion and outflows in accreting black\nhole systems. This understanding arises primarily from observing the relation\nbetween X-ray and longer wavelength (infrared, radio) emission. The view is\nnecessarily a biased one, beginning with observations of X-ray binary systems,\nand attempting to see if they match with the general observational properties\nof active galactic nuclei. \n\n"}
{"id": "0909.2643", "contents": "Title: Shock Breakout from Type Ia Supernova Abstract: The mode of explosive burning in Type Ia SNe remains an outstanding problem.\nIt is generally thought to begin as a subsonic deflagration, but this may\ntransition into a supersonic detonation (the DDT). We argue that this\ntransition leads to a breakout shock, which would provide the first unambiguous\nevidence that DDTs occur. Its main features are a hard X-ray flash (~20 keV)\nlasting ~0.01 s with a total radiated energy of ~10^{40} ergs, followed by a\ncooling tail. This creates a distinct feature in the visual light curve, which\nis separate from the nickel decay. This cooling tail has a maximum absolute\nvisual magnitude of M_V = -9 to -10 at approximately 1 day, which depends most\nsensitively on the white dwarf radius at the time of the DDT. As the thermal\ndiffusion wave moves in, the composition of these surface layers may be\nimprinted as spectral features, which would help to discern between SN Ia\nprogenitor models. Since this feature should accompany every SNe Ia, future\ndeep surveys (e.g., m=24) will see it out to a distance of approximately 80\nMpc, giving a maximum rate of ~60/yr. Archival data sets can also be used to\nstudy the early rise dictated by the shock heating (at about 20 days before\nmaximum B-band light). A similar and slightly brighter event may also accompany\ncore bounce during the accretion induced collapse to a neutron star, but with a\nlower occurrence rate. \n\n"}
{"id": "0909.2767", "contents": "Title: On disjoint matchings in cubic graphs: maximum 2- and 3-edge-colorable\n  subgraphs Abstract: We show that any $2-$factor of a cubic graph can be extended to a maximum\n$3-$edge-colorable subgraph. We also show that the sum of sizes of maximum $2-$\nand $3-$edge-colorable subgraphs of a cubic graph is at least twice of its\nnumber of vertices. Finally, for a cubic graph $G$, consider the pairs of\nedge-disjoint matchings whose union consists of as many edges as possible. Let\n$H$ be the largest matching among such pairs. Let $M$ be a maximum matching of\n$G$. We show that 9/8 is a tight upper bound for $|M|/|H|$. \n\n"}
{"id": "0909.4727", "contents": "Title: A regularity lemma, and low-weight approximators, for low-degree\n  polynomial threshold functions Abstract: We give a \"regularity lemma\" for degree-d polynomial threshold functions\n(PTFs) over the Boolean cube {-1,1}^n. This result shows that every degree-d\nPTF can be decomposed into a constant number of subfunctions such that almost\nall of the subfunctions are close to being regular PTFs. Here a \"regular PTF is\na PTF sign(p(x)) where the influence of each variable on the polynomial p(x) is\na small fraction of the total influence of p.\n  As an application of this regularity lemma, we prove that for any constants d\n\\geq 1, \\eps \\geq 0, every degree-d PTF over n variables has can be\napproximated to accuracy eps by a constant-degree PTF that has integer weights\nof total magnitude O(n^d). This weight bound is shown to be optimal up to\nconstant factors. \n\n"}
{"id": "0909.4927", "contents": "Title: Constraints on Lorentz invariance violation from gamma-ray burst\n  GRB090510 Abstract: We obtain modified dispersion relations by requiring the vanishing of\ndeterminant of inverse of modified photon propagators in Lorentz invariance\nviolation (LIV) theory. Inspired by these dispersion relations, we give a more\ngeneral dispersion relation with less assumption and apply it to the recent\nobserved gamma-ray burst GRB090510 to extract various constraints on LIV\nparameters. We find that the constraint on quantum gravity mass is slightly\nlarger than the Planck mass but is consistent with the other recent\nobservations, so the corresponding LIV coefficient $\\xi_1$ has reached the\nnatural order ($o(1)$) as one expects. From our analysis, the linear LIV\ncorrections to photon group velocity might be not excluded yet. \n\n"}
{"id": "0910.0110", "contents": "Title: Improved Hardness of Approximation for Stackelberg Shortest-Path Pricing Abstract: We consider the Stackelberg shortest-path pricing problem, which is defined\nas follows. Given a graph G with fixed-cost and pricable edges and two distinct\nvertices s and t, we may assign prices to the pricable edges. Based on the\npredefined fixed costs and our prices, a customer purchases a cheapest s-t-path\nin G and we receive payment equal to the sum of prices of pricable edges\nbelonging to the path. Our goal is to find prices maximizing the payment\nreceived from the customer. While Stackelberg shortest-path pricing was known\nto be APX-hard before, we provide the first explicit approximation threshold\nand prove hardness of approximation within 2-o(1). \n\n"}
{"id": "0910.0443", "contents": "Title: Stackelberg Pricing is Hard to Approximate within $2-\\epsilon$ Abstract: Stackelberg Pricing Games is a two-level combinatorial pricing problem\nstudied in the Economics, Operation Research, and Computer Science communities.\nIn this paper, we consider the decade-old shortest path version of this problem\nwhich is the first and most studied problem in this family.\n  The game is played on a graph (representing a network) consisting of {\\em\nfixed cost} edges and {\\em pricable} or {\\em variable cost} edges. The fixed\ncost edges already have some fixed price (representing the competitor's\nprices). Our task is to choose prices for the variable cost edges. After that,\na client will buy the cheapest path from a node $s$ to a node $t$, using any\ncombination of fixed cost and variable cost edges. The goal is to maximize the\nrevenue on variable cost edges.\n  In this paper, we show that the problem is hard to approximate within\n$2-\\epsilon$, improving the previous \\APX-hardness result by Joret [to appear\nin {\\em Networks}]. Our technique combines the existing ideas with a new\ninsight into the price structure and its relation to the hardness of the\ninstances. \n\n"}
{"id": "0910.0883", "contents": "Title: X-ray Isophotes in a Rapidly Rotating Elliptical Galaxy: Evidence of\n  Inflowing Gas Abstract: We describe two-dimensional gasdynamical computations of the X-ray emitting\ngas in the rotating elliptical galaxy NGC 4649 that indicate an inflow of about\none solar mass per year at every radius. Such a large instantaneous inflow\ncannot have persisted over a Hubble time. The central constant-entropy\ntemperature peak recently observed in the innermost 150 parsecs is explained by\ncompressive heating as gas flows toward the central massive black hole. Since\nthe cooling time of this gas is only a few million years, NGC 4649 provides the\nmost acutely concentrated known example of the cooling flow problem in which\nthe time-integrated apparent mass that has flowed into the galactic core\nexceeds the total mass observed there. This paradox can be resolved by\nintermittent outflows of energy or mass driven by accretion energy released\nnear the black hole. Inflowing gas is also required at intermediate kpc radii\nto explain the ellipticity of X-ray isophotes due to spin-up by mass ejected by\nstars that rotate with the galaxy and to explain local density and temperature\nprofiles. We provide evidence that many luminous elliptical galaxies undergo\nsimilar inflow spin-up. A small turbulent viscosity is required in NGC 4649 to\navoid forming large X-ray luminous disks that are not observed, but the\nturbulent pressure is small and does not interfere with mass determinations\nthat assume hydrostatic equilibrium. \n\n"}
{"id": "0910.2397", "contents": "Title: Singularity and entropy of the viscosity dark energy model Abstract: In this paper bulk viscosity is introduced to describe the effects of cosmic\nnon-perfect fluid on the cosmos evolution and to build the unified dark energy\n(DE) with (dark) matter models. Also we derive a general relation between the\nbulk viscosity form and Hubble parameter that can provide a procedure for the\nviscosity DE model building. Especially, a redshift dependent viscosity\nparameter $\\zeta\\propto\\lambda_{0}+\\lambda_{1}(1+z)^{n}$ proposed in the\nprevious work by X.H.Meng and X.Dou in 2009\\cite{md} is investigated\nextensively in this present work. Further more we use the recently released\nsupernova dataset (the Constitution dataset) to constrain the model parameters.\nIn order to differentiate the proposed concrete dark energy models from the\nwell known $\\Lambda$CDM model, statefinder diagnostic method is applied to this\nbulk viscosity model, as a complementary to the $Om$ parameter diagnostic and\nthe deceleration parameter analysis performed by us before. The DE model\nevolution behavior and tendency are shown in the plane of the statefinder\ndiagnostic parameter pair \\{$r,s$\\} where the fixed point represents the\n$\\Lambda$CDM model. The possible singularity property in this bulk viscosity\ncosmology is also discussed to which we can conclude that in the different\nparameter regions chosen properly, this concrete viscosity DE model can have\nvarious late evolution behaviors and the late time singularity could be\navoided. We also calculate the cosmic entropy in the bulk viscosity dark energy\nframe, and find that the total entropy in the viscosity DE model increases\nmonotonously with respect to the scale factor evolution, thus this monotonous\nincreasing property can indicate an arrow of time in the universe evolution,\nthough the quantum version of the arrow of time is still puzzling. \n\n"}
{"id": "0910.2459", "contents": "Title: GeV emission from Gamma Ray Bursts: a radiative fireball? Abstract: We study the emission observed at energies greater than 100 MeV of 11 Gamma\nRay Bursts (GRBs) detected by the Fermi/Large Area Telescope (LAT) until\nOctober 2009. The GeV emission has three main properties: (i) its duration is\noften longer than the duration of the softer emission detected by the Gamma\nBurst Monitor (GBM) onboard Fermi [this confirms earlier results from the\nEnergetic Gamma-Ray Experiment Telescope (EGRET)]; (ii) its spectrum is\nconsistent with F(v) propto v^(-1) and does not show strong spectral evolution;\n(iii) for the brightest bursts, the flux detected by the LAT decays as a power\nlaw with a typical slope: t^(-1.5). We argue that the observed >0.1 GeV flux\ncan be interpreted as afterglow emission shortly following the start of the\nprompt phase emission as seen at smaller frequencies. The decay slope is what\nexpected if the fireball emission is produced in the radiative regime, i.e. all\ndissipated energy is radiated away. We also argue that the detectability in the\nGeV energy range depends on the bulk Lorentz factor Gamma of the bursts, being\nstrongly favoured in the case of large Gamma. This implies that the fraction of\nbursts detected at high energies corresponds to the fraction of bursts having\nthe largest Gamma. The radiative interpretation can help to explain why the\nobserved X-ray and optical afterglow energetics are much smaller than the\nenergetics emitted during the prompt phase, despite the fact that the collision\nwith the external medium should be more efficient than internal shocks in\nproducing the radiation we see. \n\n"}
{"id": "0910.2877", "contents": "Title: On Neutral Absorption and Spectral Evolution in X-ray Binaries Abstract: Current X-ray observatories make it possible to follow the evolution of\ntransient and variable X-ray binaries across a broad range in luminosity and\nsource behavior. In such studies, it can be unclear whether evolution in the\nlow energy portion of the spectrum should be attributed to evolution in the\nsource, or instead to evolution in neutral photoelectric absorption. Dispersive\nspectrometers make it possible to address this problem. We have analyzed a\nsmall but diverse set of X-ray binaries observed with the Chandra High Energy\nTransmission Grating Spectrometer across a range in luminosity and different\nspectral states. The column density in individual photoelectric absorption\nedges remains constant with luminosity, both within and across source spectral\nstates. This finding suggests that absorption in the interstellar medium\nstrongly dominates the neutral column density observed in spectra of X-ray\nbinaries. Consequently, evolution in the low energy spectrum of X-ray binaries\nshould properly be attributed to evolution in the source spectrum. We discuss\nour results in the context of X-ray binary spectroscopy with current and future\nX-ray missions. \n\n"}
{"id": "0910.3565", "contents": "Title: Fermi Large Area Telescope Gamma-Ray Detection of the Radio Galaxy M87 Abstract: We report the Fermi-LAT discovery of high-energy (MeV/GeV) gamma-ray emission\npositionally consistent with the center of the radio galaxy M87, at a source\nsignificance of over 10 sigma in ten-months of all-sky survey data. Following\nthe detections of Cen A and Per A, this makes M87 the third radio galaxy seen\nwith the LAT. The faint point-like gamma-ray source has a >100 MeV flux of 2.45\n(+/- 0.63) x 10^-8 ph cm^-2 s^-1 (photon index = 2.26 +/- 0.13) with no\nsignificant variability detected within the LAT observation. This flux is\ncomparable with the previous EGRET upper limit (< 2.18 x 10^-8 ph cm^-2 s^-1, 2\nsigma), thus there is no evidence for a significant MeV/GeV flare on decade\ntimescales. Contemporaneous Chandra and VLBA data indicate low activity in the\nunresolved X-ray and radio core relative to previous observations, suggesting\nM87 is in a quiescent overall level over the first year of Fermi-LAT\nobservations. The LAT gamma-ray spectrum is modeled as synchrotron self-Compton\n(SSC) emission from the electron population producing the radio-to-X-ray\nemission in the core. The resultant SSC spectrum extrapolates smoothly from the\nLAT band to the historical-minimum TeV emission. Alternative models for the\ncore and possible contributions from the kiloparsec-scale jet in M87 are\nconsidered, and can not be excluded. \n\n"}
{"id": "0910.4192", "contents": "Title: Fermi observations of high-energy gamma-ray emission from GRB 080825C Abstract: The Fermi Gamma-ray Space Telescope (FGST) has opened a new high-energy\nwindow in the study of Gamma-Ray Bursts (GRBs). Here we present a thorough\nanalysis of GRB 080825C, which triggered the Fermi Gamma-ray Burst Monitor\n(GBM), and was the first firm detection of a GRB by the Fermi Large Area\nTelescope (LAT). We discuss the LAT event selections, background estimation,\nsignificance calculations, and localization for Fermi GRBs in general and GRB\n080825C in particular. We show the results of temporal and time-resolved\nspectral analysis of the GBM and LAT data. We also present some theoretical\ninterpretation of GRB 080825C observations as well as some common features\nobserved in other LAT GRBs. \n\n"}
{"id": "0911.0015", "contents": "Title: Stochastic conversions of TeV photons into axion-like particles in\n  extragalactic magnetic fields Abstract: Very-high energy photons emitted by distant cosmic sources are absorbed on\nthe extragalactic background light (EBL) during their propagation. This effect\ncan be characterized in terms of a photon transfer function at Earth. The\npresence of extragalactic magnetic fields could also induce conversions between\nvery high-energy photons and hypothetical axion-like particles (ALPs). The\nturbulent structure of the extragalactic magnetic fields would produce a\nstochastic behaviour in these conversions, leading to a statistical\ndistribution of the photon transfer functions for the different realizations of\nthe random magnetic fields. To characterize this effect, we derive new\nequations to calculate the mean and the variance of this distribution. We find\nthat, in presence of ALP conversions, the photon transfer functions on\ndifferent lines of sight could have relevant deviations with respect to the\nmean value, producing both an enhancement or a suppression in the observable\nphoton flux with respect to the expectations with only absorption. As a\nconsequence, the most striking signature of the mixing with ALPs would be a\nreconstructed EBL density from TeV photon observations which appears to vary\nover different directions of the sky: consistent with standard expectations in\nsome regions, but inconsistent in others. \n\n"}
{"id": "0911.1419", "contents": "Title: Belief Propagation and Loop Calculus for the Permanent of a Non-Negative\n  Matrix Abstract: We consider computation of permanent of a positive $(N\\times N)$ non-negative\nmatrix, $P=(P_i^j|i,j=1,\\cdots,N)$, or equivalently the problem of weighted\ncounting of the perfect matchings over the complete bipartite graph $K_{N,N}$.\nThe problem is known to be of likely exponential complexity. Stated as the\npartition function $Z$ of a graphical model, the problem allows exact Loop\nCalculus representation [Chertkov, Chernyak '06] in terms of an interior\nminimum of the Bethe Free Energy functional over non-integer doubly stochastic\nmatrix of marginal beliefs, $\\beta=(\\beta_i^j|i,j=1,\\cdots,N)$, also\ncorrespondent to a fixed point of the iterative message-passing algorithm of\nthe Belief Propagation (BP) type. Our main result is an explicit expression of\nthe exact partition function (permanent) in terms of the matrix of BP\nmarginals, $\\beta$, as $Z=\\mbox{Perm}(P)=Z_{BP}\n\\mbox{Perm}(\\beta_i^j(1-\\beta_i^j))/\\prod_{i,j}(1-\\beta_i^j)$, where $Z_{BP}$\nis the BP expression for the permanent stated explicitly in terms if $\\beta$.\nWe give two derivations of the formula, a direct one based on the Bethe Free\nEnergy and an alternative one combining the Ihara graph-$\\zeta$ function and\nthe Loop Calculus approaches. Assuming that the matrix $\\beta$ of the Belief\nPropagation marginals is calculated, we provide two lower bounds and one\nupper-bound to estimate the multiplicative term. Two complementary lower bounds\nare based on the Gurvits-van der Waerden theorem and on a relation between the\nmodified permanent and determinant respectively. \n\n"}
{"id": "0911.3363", "contents": "Title: Spinning Black Holes as Particle Accelerators Abstract: It has recently been pointed out that particles falling freely from rest at\ninfinity outside a Kerr black hole can in principle collide with arbitrarily\nhigh center of mass energy in the limiting case of maximal black hole spin.\nHere we aim to elucidate the mechanism for this fascinating result, and to\npoint out its practical limitations, which imply that ultra-energetic\ncollisions cannot occur near black holes in nature. \n\n"}
{"id": "0911.3950", "contents": "Title: Randomized Interior Point methods for Sampling and Optimization Abstract: We present a Markov chain (Dikin walk) for sampling from a convex body\nequipped with a self-concordant barrier, whose mixing time from a \"central\npoint\" is strongly polynomial in the description of the convex set. The mixing\ntime of this chain is invariant under affine transformations of the convex set,\nthus eliminating the need for first placing the body in an isotropic position.\nThis recovers and extends previous results of from polytopes to more general\nconvex sets. On every convex set of dimension $n$, there exists a\nself-concordant barrier whose \"complexity\" is polynomially bounded.\nConsequently, a rapidly mixing Markov chain of the kind we describe can be\ndefined on any convex set. We use these results to design an algorithm\nconsisting of a single random walk for optimizing a linear function on a convex\nset. We show that this random walk reaches an approximately optimal point in\npolynomial time with high probability and that the corresponding objective\nvalues converge with probability 1 to the optimal objective value as the number\nof steps tends to infinity. One technical contribution is a family of lower\nbounds for the isoperimetric constants of (weighted) Riemannian manifolds on\nwhich, interior point methods perform a kind of steepest descent. Using results\nof Barthe \\cite{barthe} and Bobkov and Houdr\\'e, on the isoperimetry of\nproducts of (weighted) Riemannian manifolds, we obtain sharper upper bounds on\nthe mixing time of Dikin walk on products of convex sets than the bounds\nobtained from a direct application of the Localization Lemma, on which, since\n(Lov\\'asz and Simonovits), the analyses of all random walks on convex sets have\nrelied. \n\n"}
{"id": "0911.4719", "contents": "Title: SN 2008iy: An Unusual Type IIn Supernova with an Enduring 400 Day Rise\n  Time Abstract: We present spectroscopic and photometric observations of the Type IIn\nsupernova (SN) 2008iy. SN 2008iy showed an unprecedentedly long rise time of\n~400 days, making it the first SN to take significantly longer than 100 days to\nreach peak optical luminosity. The peak absolute magnitude of SN 2008iy was M_r\n~ -19.1 mag, and the total radiated energy over the first ~700 days was ~2 x\n10^50 erg. Spectroscopically, SN 2008iy is very similar to the Type IIn SN\n1988Z at late times, and, like SN 1988Z, it is a luminous X-ray source (both\nsupernovae had an X-ray luminosity L_ X > 10^41 erg/s). The Halpha emission\nprofile of SN 2008iy shows a narrow P Cygni absorption component, implying a\npre-SN wind speed of ~100 km/s. We argue that the luminosity of SN 2008iy is\npowered via the interaction of the SN ejecta with a dense, clumpy circumstellar\nmedium. The ~400 day rise time can be understood if the number density of\nclumps increases with distance over a radius ~1.7 x 10^16 cm from the\nprogenitor. This scenario is possible if the progenitor experienced an episodic\nphase of enhanced mass-loss < 1 century prior to explosion or the progenitor\nwind speed increased during the decades before core collapse. We favour the\nformer scenario, which is reminiscent of the eruptive mass-loss episodes\nobserved for luminous blue variable (LBV) stars. The progenitor wind speed and\nincreased mass-loss rates serve as further evidence that at least some, and\nperhaps all, Type IIn supernovae experience LBV-like eruptions shortly before\ncore collapse. We also discuss the host galaxy of SN 2008iy, a subluminous\ndwarf galaxy, and offer a few reasons why the recent suggestion that unusual,\nluminous supernovae preferentially occur in dwarf galaxies may be the result of\nobservational biases. \n\n"}
{"id": "0911.5487", "contents": "Title: Strong Spatial Mixing for Binary Markov Random Fields Abstract: Gibbs distribution of binary Markov random fields on a sparse on average\ngraph is considered in this paper. The strong spatial mixing is proved under\nthe condition that the `external field' is uniformly large or small. Such\ncondition on `external field' is meaningful in physics. \n\n"}
{"id": "0912.0517", "contents": "Title: Neutralino dark matter annihilation to monoenergetic gamma rays as a\n  signal of low mass superstrings Abstract: We consider extensions of the standard model based on open strings ending on\nD-branes, in which gauge bosons and their associated gauginos exist as strings\nattached to stacks of D-branes, and chiral matter exists as strings stretching\nbetween intersecting D-branes. Under the assumptions that the fundamental\nstring scale is in the TeV range and the theory is weakly coupled, we study\nmodels of supersymmetry for which signals of annihilating neutralino dark\nmatter are observable. In particular, we construct a model with a\nsupersymmetric R-symmetry violating (but R-parity conserving) effective\nLagrangian that allows for the s-wave annihilation of neutralinos, once\ngauginos acquire mass through an unspecified mechanism. The model yields\nbino-like neutralinos (with the measured relic abundance) that annihilate to a\ngamma-gamma final state with a substantial branching fraction (~ 10%) that is\norders of magnitude larger than in the minimal supersymmetric standard model. A\nvery bright gamma-ray spectral line could be observed by gamma-ray telescopes. \n\n"}
{"id": "0912.2346", "contents": "Title: Hidden Hot Dark Matter as Cold Dark Matter Abstract: We show that hidden hot dark matter, hidden-sector dark matter with\ninteractions that decouple when it is relativistic, is a viable dark matter\ncandidate provided it has never been in thermal equilibrium with the particles\nof the standard model. This hidden hot dark matter may reheat to a lower\ntemperature and number density than the visible Universe and thus account,\nsimply with its thermal abundance, for all the dark matter in the Universe\nwhile evading the typical constraints on hot dark matter arising from structure\nformation. We find masses ranging from ~3 keV to ~10 TeV. While never in\nequilibrium with the standard model, this class of models may have unique\nobservational signatures in the matter power spectrum or via extra-weak\ninteractions with standard model particles. \n\n"}
{"id": "0912.3929", "contents": "Title: Metric inequalities for polygons Abstract: Let $A_1,A_2,...,A_n$ be the vertices of a polygon with unit perimeter, that\nis $\\sum_{i=1}^n |A_i A_{i+1}|=1$. We derive various tight estimates on the\nminimum and maximum values of the sum of pairwise distances, and respectively\nsum of pairwise squared distances among its vertices. In most cases such\nestimates on these sums in the literature were known only for convex polygons.\n  In the second part, we turn to a problem of Bra\\ss\\ regarding the maximum\nperimeter of a simple $n$-gon ($n$ odd) contained in a disk of unit radius. The\nproblem was solved by Audet et al. \\cite{AHM09b}, who gave an exact formula.\nHere we present an alternative simpler proof of this formula. We then examine\nwhat happens if the simplicity condition is dropped, and obtain an exact\nformula for the maximum perimeter in this case as well. \n\n"}
{"id": "0912.4776", "contents": "Title: A resonance model with magnetic connection for 3:2 HFQPO pairs in black\n  hole binaries Abstract: We apply epicyclic resonances to the magnetic connection (MC) of a black hole\n(BH) with a relativistic accretion disc, interpreting the high frequency\nquasi-periodic oscillations (HFQPOs) with 3:2 pairs observed in three BH X-ray\nbinaries. It turns out that the 3:2 HFQPO pairs are associated with the steep\npower-law states, and the severe damping can be overcome by transferring energy\nand angular momentum from a spinning BH to the inner disc in the MC process. \n\n"}
{"id": "1001.4503", "contents": "Title: Searching for 511 keV annihilation line emission from galactic compact\n  objects with IBIS Abstract: The IBIS imager on board the INTEGRAL satellite, thanks to the large field of\nview and good sensitivity, gave us a unique opportunity to search for possible\n511 keV point sources either previously unknown or associated to known objects\nsuch as X-ray binaries or supernovae. The IBIS sensitivity at 511 keV depends\non the gamma ray detector quantum efficiency at this energy and on the\nbackground. Both these quantities have been estimated. Reducing all the\navailable IBIS data up to April 2008 we have produced a 5 years full sky 511\nkeV map with 10 Ms exposure in the Galactic Center. We did not find any\nsignificant signal at this energy. The lack of detection of 511 keV galactic\npoint sources is in agreement with the idea that a significant part of the\ngalactic positrons originates in compact objects and then propagates in the\ninterstellar medium. \n\n"}
{"id": "1001.5019", "contents": "Title: Lower Bounds for the Complexity of Monadic Second-Order Logic Abstract: Courcelle's famous theorem from 1990 states that any property of graphs\ndefinable in monadic second-order logic (MSO) can be decided in linear time on\nany class of graphs of bounded treewidth, or in other words, MSO is\nfixed-parameter tractable in linear time on any such class of graphs. From a\nlogical perspective, Courcelle's theorem establishes a sufficient condition, or\nan upper bound, for tractability of MSO-model checking.\n  Whereas such upper bounds on the complexity of logics have received\nsignificant attention in the literature, almost nothing is known about\ncorresponding lower bounds. In this paper we establish a strong lower bound for\nthe complexity of monadic second-order logic. In particular, we show that if C\nis any class of graphs which is closed under taking subgraphs and whose\ntreewidth is not bounded by a polylogarithmic function (in fact, $\\log^c n$ for\nsome small c suffices) then MSO-model checking is intractable on C (under a\nsuitable assumption from complexity theory). \n\n"}
{"id": "1003.0632", "contents": "Title: Detailed Classification of Swift's Gamma-Ray Bursts Abstract: Earlier classification analyses found three types of gamma-ray bursts (short,\nlong and intermediate in duration) in the BATSE sample. Recent works have shown\nthat these three groups are also present in the RHESSI and the BeppoSAX\ndatabases. The duration distribution analysis of the bursts observed by the\nSwift satellite also favors the three-component model. In this paper, we extend\nthe analysis of the Swift data with spectral information. We show, using the\nspectral hardness and the duration simultaneously, that the maximum likelihood\nmethod favors the three-component against the two-component model. The\nlikelihood also shows that a fourth component is not needed. \n\n"}
{"id": "1004.1485", "contents": "Title: Are there any good digraph width measures? Abstract: Several different measures for digraph width have appeared in the last few\nyears. However, none of them shares all the \"nice\" properties of treewidth:\nFirst, being \\emph{algorithmically useful} i.e. admitting polynomial-time\nalgorithms for all $\\MS1$-definable problems on digraphs of bounded width. And,\nsecond, having nice \\emph{structural properties} i.e. being monotone under\ntaking subdigraphs and some form of arc contractions. As for the former,\n(undirected) $\\MS1$ seems to be the least common denominator of all reasonably\nexpressive logical languages on digraphs that can speak about the edge/arc\nrelation on the vertex set.The latter property is a necessary condition for a\nwidth measure to be characterizable by some version of the cops-and-robber game\ncharacterizing the ordinary treewidth. Our main result is that \\emph{any\nreasonable} algorithmically useful and structurally nice digraph measure cannot\nbe substantially different from the treewidth of the underlying undirected\ngraph. Moreover, we introduce \\emph{directed topological minors} and argue that\nthey are the weakest useful notion of minors for digraphs. \n\n"}
{"id": "1004.1963", "contents": "Title: A self-consistent approach to the hard and soft states of 4U 1705-44 Abstract: We analyzed two XMM-Newton observations of the bright atoll source 4U\n1705-44, which can be considered a prototype of the class of the persistent NS\nLMXBs showing both hard and soft states. The first observation was performed\nwhen the source was in a hard low flux state, the second during a soft,\nhigh-flux state. Both the spectra show broad iron emission lines. We fit the\nspectra using a two-component model, together with a reflection model\nspecifically suited to the case of a neutron star, where the incident spectrum\nhas a blackbody shape. In the soft state, the reflection model, convolved with\na relativistic smearing component, consistently describes the broad features\npresent in the spectrum, and we find a clear relation between the temperature\nof the incident flux and the temperature of the harder X-ray component that we\ninterpret as the boundary layer emission. In this state we find converging\nevidence that the boundary layer outer radius is ~ 2 times the neutron star\nradius. In the low flux state, we observe a change in the continuum shape of\nthe spectrum with respect to the soft state. Still, the broad local emission\nfeatures can be associated with a disk reflecting matter, but in a lower\nionization state, and possibly produced in an accretion disk truncated at\ngreater distance. Our analysis provides strong evidence that the reflection\ncomponent in soft states of LMXBs comes from to hard X-ray thermal irradiation,\nwhich we identify with the boundary layer emission, also present in the\ncontinuum model. In the hard state, the broad iron line if also produced by\nreflection, and the continuum disk emission can be self-consistently accounted\nif the disk is truncated at a greater distance than the soft state. \n\n"}
{"id": "1004.2537", "contents": "Title: The Pierre Auger Project and Enhancements Abstract: The current status of the scientific results of the Auger Observatory will be\ndiscussed which include spectrum, anisotropy in arrival directions, chemical\ncomposition analyses, and limits on neutrino and photon fluxes. A review of the\nObservatory detection systems will be presented. Auger has started the\nconstruction of its second phase which encompasses antennae for radio detection\nof cosmic rays, high-elevation telescopes, and surface plus muon detectors.\nDetails will be presented on the latter, AMIGA (Auger Muons and Infill for the\nGround Array), an Auger project consisting of 85 detector pairs each one\ncomposed of a surface water-Cherenkov detector and a buried muon counter. The\ndetector pairs are arranged in an array with spacings of 433 and 750 m in order\nto perform a detailed study of the 10^17 eV to 10^19 eV spectrum region.\nPreliminary results on the performance of the 750 m array of surface detectors\nand the first muon counter prototype will be presented. \n\n"}
{"id": "1004.2778", "contents": "Title: Tropical polar cones, hypergraph transversals, and mean payoff games Abstract: We discuss the tropical analogues of several basic questions of convex\nduality. In particular, the polar of a tropical polyhedral cone represents the\nset of linear inequalities that its elements satisfy. We characterize the\nextreme rays of the polar in terms of certain minimal set covers which may be\nthought of as weighted generalizations of minimal transversals in hypergraphs.\nWe also give a tropical analogue of Farkas lemma, which allows one to check\nwhether a linear inequality is implied by a finite family of linear\ninequalities. Here, the certificate is a strategy of a mean payoff game. We\ndiscuss examples, showing that the number of extreme rays of the polar of the\ntropical cyclic polyhedral cone is polynomially bounded, and that there is no\nunique minimal system of inequalities defining a given tropical polyhedral\ncone. \n\n"}
{"id": "1004.3311", "contents": "Title: The Diffuse Supernova Neutrino Background Abstract: The Diffuse Supernova Neutrino Background (DSNB) is the weak glow of MeV\nneutrinos and antineutrinos from distant core-collapse supernovae. The DSNB has\nnot been detected yet, but the Super-Kamiokande (SK) 2003 upper limit on the\nelectron antineutrino flux is close to predictions, now quite precise, based on\nastrophysical data. If SK is modified with dissolved gadolinium to reduce\ndetector backgrounds and increase the energy range for analysis, then it should\ndetect the DSNB at a rate of a few events per year, providing a new probe of\nsupernova neutrino emission and the cosmic core-collapse rate. If the DSNB is\nnot detected, then new physics will be required. Neutrino astronomy, while\nuniquely powerful, has proven extremely difficult -- only the Sun and the\nnearby Supernova 1987A have been detected to date -- so the promise of\ndetecting new sources soon is exciting indeed. \n\n"}
{"id": "1005.3324", "contents": "Title: An LP with Integrality Gap 1+epsilon for Multidimensional Knapsack Abstract: In this note we study packing or covering integer programs with at most k\nconstraints, which are also known as k-dimensional knapsack problems. For any\ninteger k > 0 and real epsilon > 0, we observe there is a polynomial-sized LP\nfor the k-dimensional knapsack problem with integrality gap at most 1+epsilon.\nThe variables may be unbounded or have arbitrary upper bounds. In the packing\ncase, we can also remove the dependence of the LP on the cost-function,\nyielding a polyhedral approximation of the integer hull. This generalizes a\nrecent result of Bienstock on the classical knapsack problem. \n\n"}
{"id": "1006.0262", "contents": "Title: The Impact of Non-Equipartition on Cosmological Parameter Estimation\n  from Sunyaev-Zel'dovich Surveys Abstract: The collisionless accretion shock at the outer boundary of a galaxy cluster\nshould primarily heat the ions instead of electrons since they carry most of\nthe kinetic energy of the infalling gas. Near the accretion shock, the density\nof the intracluster medium is very low and the Coulomb collisional timescale is\nlonger than the accretion timescale. Electrons and ions may not achieve\nequipartition in these regions. Numerical simulations have shown that the\nSunyaev-Zel'dovich observables (e.g., the integrated Comptonization parameter\nY) for relaxed clusters can be biased by a few percent. The Y-mass relation can\nbe biased if non-equipartition effects are not properly taken into account.\nUsing a set of hydrodynamical simulations, we have calculated three potential\nsystematic biases in the Y-mass relations introduced by non-equipartition\neffects during the cross-calibration or self-calibration when using the galaxy\ncluster abundance technique to constraint cosmological parameters. We then use\na semi-analytic technique to estimate the non-equipartition effects on the\ndistribution functions of Y (Y functions) determined from the extended\nPress-Schechter theory. Depending on the calibration method, we find that\nnon-equipartition effects can induce systematic biases on the Y functions, and\nthe values of the cosmological parameters Omega_8, sigma_8, and the dark energy\nequation of state parameter w can be biased by a few percent. In particular,\nnon-equipartition effects can introduce an apparent evolution in w of a few\npercent in all of the systematic cases we considered. Techniques are suggested\nto take into account the non-equipartition effect empirically when using the\ncluster abundance technique to study precision cosmology. We conclude that\nsystematic uncertainties in the Y-mass relation of even a few percent can\nintroduce a comparable level of biases in cosmological parameter measurements. \n\n"}
{"id": "1006.3585", "contents": "Title: A Derandomized Sparse Johnson-Lindenstrauss Transform Abstract: Recent work of [Dasgupta-Kumar-Sarlos, STOC 2010] gave a sparse\nJohnson-Lindenstrauss transform and left as a main open question whether their\nconstruction could be efficiently derandomized. We answer their question\naffirmatively by giving an alternative proof of their result requiring only\nbounded independence hash functions. Furthermore, the sparsity bound obtained\nin our proof is improved. The main ingredient in our proof is a spectral moment\nbound for quadratic forms that was recently used in [Diakonikolas-Kane-Nelson,\nFOCS 2010]. \n\n"}
{"id": "1006.4608", "contents": "Title: Evolving Graph Representation and Visualization Abstract: The study of evolution of networks has received increased interest with the\nrecent discovery that many real-world networks possess many things in common,\nin particular the manner of evolution of such networks. By adding a dimension\nof time to graph analysis, evolving graphs present opportunities and challenges\nto extract valuable information. This paper introduces the Evolving Graph\nMarkup Language (EGML), an XML application for representing evolving graphs and\nrelated results. Along with EGML, a software tool is provided for the study of\nevolving graphs. New evolving graph drawing techniques based on the\nforce-directed graph layout algorithm are also explored. Our evolving graph\ntechniques reduce vertex movements between graph instances, so that an evolving\ngraph can be viewed with smooth transitions \n\n"}
{"id": "1006.4665", "contents": "Title: Masses of Neutron Stars in High-Mass X-ray Binaries with Optical\n  Astrometry Abstract: Determining the type of matter that is inside a neutron star (NS) has been a\nlong-standing goal of astrophysics. Despite this, most of the NS equations of\nstate (EOS) that predict maximum masses in the range 1.4-2.8 solar masses are\nstill viable. Most of the precise NS mass measurements that have been made to\ndate show values close to 1.4 solar masses, but a reliable measurement of an\nover-massive NS would constrain the EOS possibilities. Here, we investigate how\noptical astrometry at the microarcsecond level can be used to map out the\norbits of High-Mass X-ray Binaries (HMXBs), leading to tight constraints on NS\nmasses. While previous studies by Unwin and co-workers and Tomsick and\nco-workers discuss the fact that the future Space Interferometry Mission should\nbe capable of making such measurements, the current work describes detailed\nsimulations for 6 HMXB systems, including predicted constraints on all orbital\nparameters. We find that the direct NS masses can be measured to an accuracy of\n2.5% (1-sigma) in the best case (X Per), to 6.5% for Vela X-1, and to 10% for\ntwo other HMXBs. \n\n"}
{"id": "1007.1604", "contents": "Title: Infectious Random Walks Abstract: We study the dynamics of information (or virus) dissemination by $m$ mobile\nagents performing independent random walks on an $n$-node grid. We formulate\nour results in terms of two scenarios: broadcasting and gossiping. In the\nbroadcasting scenario, the mobile agents are initially placed uniformly at\nrandom among the grid nodes. At time 0, one agent is informed of a rumor and\nstarts a random walk. When an informed agent meets an uninformed agent, the\nlatter becomes informed and starts a new random walk. We study the broadcasting\ntime of the system, that is, the time it takes for all agents to know the\nrumor. In the gossiping scenario, each agent is given a distinct rumor at time\n0 and all agents start random walks. When two agents meet, they share all\nrumors they are aware of. We study the gossiping time of the system, that is,\nthe time it takes for all agents to know all rumors. We prove that both the\nbroadcasting and the gossiping times are $\\tilde\\Theta(n/\\sqrt{m})$ w.h.p.,\nthus achieving a tight characterization up to logarithmic factors. Previous\nresults for the grid provided bounds which were weaker and only concerned\naverage times. In the context of virus infection, a corollary of our results is\nthat static and dynamically moving agents are infected at about the same speed. \n\n"}
{"id": "1007.4879", "contents": "Title: Short-term VHE variability in blazars: PKS 2155-304 Abstract: Context: The $\\gamma$-ray blazar PKS 2155-304 has attracted considerable\nattention because of its extreme TeV variability characteristics during an\nexceptional flaring period in 2006. Among the observed key findings are (i) a\nminimum variability timescale as short as $\\sim 200$ sec and (ii) highly\nvariable TeV emission, which in the frequency interval [$10^{-4}$ Hz, $10^{-2}$\nHz] can be described by a log-normal distribution and suggests an underlying\nmultiplicative (and not additive) process. Aims: Simultaneously accounting for\nthese findings appears difficult within conventional approaches. Following\nearlier suggestions for the TeV blazar Mkn 501, we explore a possible scenario\nwhere PKS 2155-304 is supposed to harbor a supermassive binary black hole\nsystem and where the observed TeV variability is dominated by emission from the\nless massive black hole. Methods: We analyze the constraints on the very high\nenergy (VHE) source imposed by the observed variability characteristics and the\nintegrated VHE luminosity output, and discuss its implications for a binary\nblack hole system. Results: We show that for a secondary mass of $m_{\\rm BH}\n\\sim 10^7 M_{\\odot}$, fluctuations in the disk accretion rate that feed the jet\ncould account for the observed red-noise type variability process down to\nfrequencies of $\\sim 10^{-2}$ Hz. Jet curvature induced by orbital motion, on\nthe other hand, could further relax constraints on the intrinsic jet speeds.\nConclusions: Because a binary system can lead to different (yet not\nindependent) periodicities in different energy bands, a longterm (quasi-)\nperiodicity analysis could offer important insights into the real nature of the\ncentral engine of PKS~2155-304. \n\n"}
{"id": "1008.0957", "contents": "Title: Neutron stars with small radii -- the role of delta resonances Abstract: Recent neutron star observations suggest that the masses and radii of neutron\nstars may be smaller than previously considered, which would disfavor a purely\nnucleonic equation of state. In our model, we use a the flavor SU(3) sigma\nmodel that includes delta resonances and hyperons in the equation of state. We\nfind that if the coupling of the delta resonances to the vector mesons is\nslightly smaller than that of the nucleons, we can reproduce both the measured\nmass-radius relationship and the extrapolated equation of state. \n\n"}
{"id": "1008.1650", "contents": "Title: Representing Small Ordinals by Finite Automata Abstract: It is known that an ordinal is the order type of the lexicographic ordering\nof a regular language if and only if it is less than omega^omega. We design a\npolynomial time algorithm that constructs, for each well-ordered regular\nlanguage L with respect to the lexicographic ordering, given by a deterministic\nfinite automaton, the Cantor Normal Form of its order type. It follows that\nthere is a polynomial time algorithm to decide whether two deterministic finite\nautomata accepting well-ordered regular languages accept isomorphic languages.\nWe also give estimates on the size of the smallest automaton representing an\nordinal less than omega^omega, together with an algorithm that translates each\nsuch ordinal to an automaton. \n\n"}
{"id": "1008.4563", "contents": "Title: Shortest paths between shortest paths and independent sets Abstract: We study problems of reconfiguration of shortest paths in graphs. We prove\nthat the shortest reconfiguration sequence can be exponential in the size of\nthe graph and that it is NP-hard to compute the shortest reconfiguration\nsequence even when we know that the sequence has polynomial length. Moreover,\nwe also study reconfiguration of independent sets in three different models and\nanalyze relationships between these models, observing that shortest path\nreconfiguration is a special case of independent set reconfiguration in perfect\ngraphs, under any of the three models. Finally, we give polynomial results for\nrestricted classes of graphs (even-hole-free and $P_4$-free graphs). \n\n"}
{"id": "1008.5042", "contents": "Title: The X-ray and radio-emitting plasma lobes of 4C23.56: further evidence\n  of recurrent jet activity and high acceleration energies Abstract: New Chandra observations of the giant (0.5 Mpc) radio galaxy 4C23.56 at z =\n2.5 show X-rays in a linear structure aligned with its radio emission, but\nanti-correlated with the detailed radio structure. Consistent with the\npowerful, high-z giant radio galaxies we have studied previously, X-rays seem\nto be invariably found where the lobe plasma is oldest even where the radio\nemission has long since faded. The hotspot complexes seem to show structures\nresembling the double shock structure exhibited by the largest radio quasar\n4C74.26, with the X-ray shock again being offset closer to the nucleus than the\nradio synchrotron shock. In the current paper, the offsets between these shocks\nare even larger at 35kpc. Unusually for a classical double (FRII) radio source,\nthere is smooth low surface-brightness radio emission associated with the\nregions beyond the hotspots (further away from the nucleus than the hotspots\nthemselves), which seems to be symmetric for the ends of both jets. We consider\npossible explanations for this phenomenon, and conclude that it arises from\nhigh-energy electrons, recently accelerated in the nearby radio hotspots that\nare leaking into a pre-existing weakly-magnetized plasma that are symmetric\nrelic lobes fed from a previous episode of jet activity. This contrasts with\nother manifestations of previous epochs of jet ejection in various examples of\nclassical double radio sources namely (1) double-double radio galaxies by e.g.\nSchoenmakers et al, (2) the double-double X-ray/radio galaxies by Laskar et al\nand (3) the presence of a relic X-ray counter-jet in the prototypical classical\ndouble radio galaxy, Cygnus A by Steenbrugge et al. The occurrence of\nmulti-episodic jet activity in powerful radio galaxies and quasars indicates\nthat they may have a longer lasting influence on the on-going structure\nformation processes in their environs than previously presumed. \n\n"}
{"id": "1008.5341", "contents": "Title: Corotating light cylinders and Alfv\\'en waves Abstract: Exact relativistic force free fields with cylindrical symmetry are explored.\nSuch fields are generated in the interstellar gas via their connection to\npulsar magnetospheres both inside and outside their light cylinders. The\npossibility of much enhanced interstellar fields wound on cylinders of Solar\nsystem dimensions is discussed but these are most likely unstable. \n\n"}
{"id": "1009.0600", "contents": "Title: Young Supernova Remnants and the Knee in the Cosmic Ray Sectrum Abstract: It has recently been suggested that neutron stars inside the shells of young\nsupernova remnants (SNR) are the sources of PeV cosmic rays and that the\ninteraction of the particles with the radiation field in the SNR causes\nelectron pair production, which has relevance to recent observations of 'high'\npositron fluxes. Furthermore, the character of the interaction is such that the\nwell-known knee in the cosmic ray energy spectrum can be explained. Our\nexamination of the mechanism leads us to believe that the required parameters\nof SN and pulses are so uncommon that the knee and positron fraction can only\nbe explained if a single, local and recent SN - and associated pulsar - are\nconcerned. \n\n"}
{"id": "1009.1381", "contents": "Title: A Branch-and-Reduce Algorithm for Finding a Minimum Independent\n  Dominating Set Abstract: An independent dominating set D of a graph G = (V,E) is a subset of vertices\nsuch that every vertex in V \\ D has at least one neighbor in D and D is an\nindependent set, i.e. no two vertices of D are adjacent in G. Finding a minimum\nindependent dominating set in a graph is an NP-hard problem. Whereas it is hard\nto cope with this problem using parameterized and approximation algorithms,\nthere is a simple exact O(1.4423^n)-time algorithm solving the problem by\nenumerating all maximal independent sets. In this paper we improve the latter\nresult, providing the first non trivial algorithm computing a minimum\nindependent dominating set of a graph in time O(1.3569^n). Furthermore, we give\na lower bound of \\Omega(1.3247^n) on the worst-case running time of this\nalgorithm, showing that the running time analysis is almost tight. \n\n"}
{"id": "1009.2307", "contents": "Title: Quasi-randomness of graph balanced cut properties Abstract: Quasi-random graphs can be informally described as graphs whose edge\ndistribution closely resembles that of a truly random graph of the same edge\ndensity. Recently, Shapira and Yuster proved the following result on\nquasi-randomness of graphs. Let $k \\ge 2$ be a fixed integer,\n$\\alpha_1,...,\\alpha_k$ be positive reals satisfying $\\sum_{i} \\alpha_i = 1$\nand $(\\alpha_1,..., \\alpha_k) \\neq (1/k,...,1/k)$, and $G$ be a graph on $n$\nvertices. If for every partition of the vertices of $G$ into sets $V_1,...,\nV_k$ of size $\\alpha_1 n,..., \\alpha_k n$, the number of complete graphs on $k$\nvertices which have exactly one vertex in each of these sets is similar to what\nwe would expect in a random graph, then the graph is quasi-random. However, the\nmethod of quasi-random hypergraphs they used did not provide enough information\nto resolve the case $(1/k,..., 1/k)$ for graphs. In their work, Shapira and\nYuster asked whether this case also forces the graph to be quasi-random. Janson\nalso posed the same question in his study of quasi-randomness under the\nframework of graph limits. In this paper, we positively answer their question. \n\n"}
{"id": "1009.3319", "contents": "Title: Electron Injection by Whistler Waves in Non-relativistic Shocks Abstract: Electron acceleration to non-thermal, ultra-relativistic energies (~ 10-100\nTeV) is revealed by radio and X-ray observations of shocks in young supernova\nremnants (SNRs). The diffusive shock acceleration (DSA) mechanism is usually\ninvoked to explain this acceleration, but the way in which electrons are\ninitially energized or 'injected' into this acceleration process starting from\nthermal energies is an unresolved problem. In this paper we study the initial\nacceleration of electrons in non-relativistic shocks from first principles,\nusing two- and three-dimensional particle-in-cell (PIC) plasma simulations. We\nsystematically explore the space of shock parameters (the Alfv\\'enic Mach\nnumber, M_A, the shock velocity, v_{sh}, the angle between the upstream\nmagnetic field and the shock normal, theta_{Bn}, and the ion to electron mass\nratio, m_i/m_e). We find that significant non-thermal acceleration occurs due\nto the growth of oblique whistler waves in the foot of quasi-perpendicular\nshocks. The obtained electron energy distributions show power law tails with\nspectral indices up to alpha ~ 3-4. The maximum energies of the accelerated\nparticles are consistent with the electron Larmor radii being comparable to\nthat of the ions, indicating potential injection into the subsequent DSA\nprocess. This injection mechanism, however, requires the shock waves to have\nfairly low Alf\\'enic Mach numbers, M_A <~ 20, which is consistent with the\ntheoretical conditions for the growth of whistler waves in the shock foot (M_A\n<~ (m_i/m_e)^{1/2}). Thus, if the whistler mechanism is the only robust\nelectron injection process at work in SNR shocks, then SNRs that display\nnon-thermal emission must have significantly amplified upstream magnetic\nfields. Such field amplification is likely achieved by the escaping cosmic\nrays, so electron and proton acceleration in SNR shocks must be interconnected. \n\n"}
{"id": "1010.0558", "contents": "Title: Analyzing Network Coding Gossip Made Easy Abstract: We give a new technique to analyze the stopping time of gossip protocols that\nare based on random linear network coding (RLNC). Our analysis drastically\nsimplifies, extends and strengthens previous results. We analyze RLNC gossip in\na general framework for network and communication models that encompasses and\nunifies the models used previously in this context. We show, in most settings\nfor the first time, that it converges with high probability in the\ninformation-theoretically optimal time. Most stopping times are of the form O(k\n+ T) where k is the number of messages to be distributed and T is the time it\ntakes to disseminate one message. This means RLNC gossip achieves \"perfect\npipelining\". Our analysis directly extends to highly dynamic networks in which\nthe topology can change completely at any time. This remains true even if the\nnetwork dynamics are controlled by a fully adaptive adversary that knows the\ncomplete network state. Virtually nothing besides simple O(kT) sequential\nflooding protocols was previously known for such a setting. While RLNC gossip\nworks in this wide variety of networks its analysis remains the same and\nextremely simple. This contrasts with more complex proofs that were put forward\nto give less strong results for various special cases. \n\n"}
{"id": "1010.2921", "contents": "Title: Electrical Flows, Laplacian Systems, and Faster Approximation of Maximum\n  Flow in Undirected Graphs Abstract: We introduce a new approach to computing an approximately maximum s-t flow in\na capacitated, undirected graph. This flow is computed by solving a sequence of\nelectrical flow problems. Each electrical flow is given by the solution of a\nsystem of linear equations in a Laplacian matrix, and thus may be approximately\ncomputed in nearly-linear time.\n  Using this approach, we develop the fastest known algorithm for computing\napproximately maximum s-t flows. For a graph having n vertices and m edges, our\nalgorithm computes a (1-\\epsilon)-approximately maximum s-t flow in time\n\\tilde{O}(mn^{1/3} \\epsilon^{-11/3}). A dual version of our approach computes a\n(1+\\epsilon)-approximately minimum s-t cut in time\n\\tilde{O}(m+n^{4/3}\\eps^{-8/3}), which is the fastest known algorithm for this\nproblem as well. Previously, the best dependence on m and n was achieved by the\nalgorithm of Goldberg and Rao (J. ACM 1998), which can be used to compute\napproximately maximum s-t flows in time \\tilde{O}(m\\sqrt{n}\\epsilon^{-1}), and\napproximately minimum s-t cuts in time \\tilde{O}(m+n^{3/2}\\epsilon^{-3}). \n\n"}
{"id": "1010.3091", "contents": "Title: Near-Optimal Bayesian Active Learning with Noisy Observations Abstract: We tackle the fundamental problem of Bayesian active learning with noise,\nwhere we need to adaptively select from a number of expensive tests in order to\nidentify an unknown hypothesis sampled from a known prior distribution. In the\ncase of noise-free observations, a greedy algorithm called generalized binary\nsearch (GBS) is known to perform near-optimally. We show that if the\nobservations are noisy, perhaps surprisingly, GBS can perform very poorly. We\ndevelop EC2, a novel, greedy active learning algorithm and prove that it is\ncompetitive with the optimal policy, thus obtaining the first competitiveness\nguarantees for Bayesian active learning with noisy observations. Our bounds\nrely on a recently discovered diminishing returns property called adaptive\nsubmodularity, generalizing the classical notion of submodular set functions to\nadaptive policies. Our results hold even if the tests have non-uniform cost and\ntheir noise is correlated. We also propose EffECXtive, a particularly fast\napproximation of EC2, and evaluate it on a Bayesian experimental design problem\ninvolving human subjects, intended to tease apart competing economic theories\nof how people make decisions under uncertainty. \n\n"}
{"id": "1010.3250", "contents": "Title: Electromagnetic Counterparts to Black Hole Mergers Abstract: During the final moments of a binary black hole (BH) merger, the\ngravitational wave (GW) luminosity of the system is greater than the combined\nelectromagnetic output of the entire observable universe. However, the\nextremely weak coupling between GWs and ordinary matter makes these waves very\ndifficult to detect directly. Fortunately, the inspiraling BH system will\ninteract strongly--on a purely Newtonian level--with any surrounding material\nin the host galaxy, and this matter can in turn produce unique electromagnetic\n(EM) signals detectable at Earth. By identifying EM counterparts to GW sources,\nwe will be able to study the host environments of the merging BHs, in turn\ngreatly expanding the scientific yield of a mission like LISA. \n\n"}
{"id": "1010.5996", "contents": "Title: Cosmic rays: current status, historical context Abstract: The ISVHECRI conference series emphasizes the connection between high energy\nphysics and cosmic ray physics--the study of elementary particles and nuclei\nfrom accelerators in the lab and from space. In this introductory paper on\ncosmic rays, I comment on several current topics in the field while also\nproviding some historical context. \n\n"}
{"id": "1011.4016", "contents": "Title: Nowhere dense graph classes, stability, and the independence property Abstract: A class of graphs is nowhere dense if for every integer r there is a finite\nupper bound on the size of cliques that occur as (topological) r-minors. We\nobserve that this tameness notion from algorithmic graph theory is essentially\nthe earlier stability theoretic notion of superflatness. For subgraph-closed\nclasses of graphs we prove equivalence to stability and to not having the\nindependence property. \n\n"}
{"id": "1011.4406", "contents": "Title: Multidimensional simulations of magnetic field amplification and\n  electron acceleration to near-energy equipartition with ions by a mildly\n  relativistic quasi-parallel plasma collision Abstract: The energetic electromagnetic eruptions observed during the prompt phase of\ngamma-ray bursts are attributed to synchrotron emissions. The internal shocks\nmoving through the ultrarelativistic jet, which is ejected by an imploding\nsupermassive star, are the likely source of this radiation. Synchrotron\nemissions at the observed strength require the simultaneous presence of\npowerful magnetic fields and highly relativistic electrons. We explore with one\nand three-dimensional relativistic particle-in-cell simulations the transition\nlayer of a shock, that evolves out of the collision of two plasma clouds at a\nspeed 0.9c and in the presence of a quasi-parallel magnetic field. The cloud\ndensities vary by a factor of 10. The number densities of ions and electrons in\neach cloud, which have the mass ratio 250, are equal. The peak Lorentz factor\nof the electrons is determined in the 1D simulation, as well as the orientation\nand the strength of the magnetic field at the boundary of the two colliding\nclouds. The relativistic masses of the electrons and ions close to the shock\ntransition layer are comparable as in previous work. The 3D simulation shows\nrapid and strong plasma filamentation behind the transient precursor. The\nmagnetic field component orthogonal to the initial field direction is amplified\nin both simulations to values that exceed those expected from the shock\ncompression by over an order of magnitude. The forming shock is\nquasi-perpendicular due to this amplification. The simultaneous presence of\nhighly relativistic electrons and strong magnetic fields will give rise to\nsignificant synchrotron emissions. \n\n"}
{"id": "1011.5599", "contents": "Title: HyperANF: Approximating the Neighbourhood Function of Very Large Graphs\n  on a Budget Abstract: The neighbourhood function N(t) of a graph G gives, for each t, the number of\npairs of nodes <x, y> such that y is reachable from x in less that t hops. The\nneighbourhood function provides a wealth of information about the graph (e.g.,\nit easily allows one to compute its diameter), but it is very expensive to\ncompute it exactly. Recently, the ANF algorithm (approximate neighbourhood\nfunction) has been proposed with the purpose of approximating NG(t) on large\ngraphs. We describe a breakthrough improvement over ANF in terms of speed and\nscalability. Our algorithm, called HyperANF, uses the new HyperLogLog counters\nand combines them efficiently through broadword programming; our implementation\nuses overdecomposition to exploit multi-core parallelism. With HyperANF, for\nthe first time we can compute in a few hours the neighbourhood function of\ngraphs with billions of nodes with a small error and good confidence using a\nstandard workstation. Then, we turn to the study of the distribution of the\nshortest paths between reachable nodes (that can be efficiently approximated by\nmeans of HyperANF), and discover the surprising fact that its index of\ndispersion provides a clear-cut characterisation of proper social networks vs.\nweb graphs. We thus propose the spid (Shortest-Paths Index of Dispersion) of a\ngraph as a new, informative statistics that is able to discriminate between the\nabove two types of graphs. We believe this is the first proposal of a\nsignificant new non-local structural index for complex networks whose\ncomputation is highly scalable. \n\n"}
{"id": "1011.6350", "contents": "Title: Three-Dimensional Simulations of MHD Turbulence Behind Relativistic\n  Shock Waves and Their Implications for GRBs Abstract: Relativistic astrophysical phenomena such as gamma-ray bursts (GRBs) and\nactive galactic nuclei often require long-lived strong magnetic field. Here, we\nreport on three-dimensional special-relativistic magnetohydrodynamic (MHD)\nsimulations to explore the amplification and decay of macroscopic turbulence\ndynamo excited by the so-called Richtmyer-Meshkov instability (RMI; a\nRayleigh-Taylor type instability). This instability is an inevitable outcome of\ninteractions between shock and ambient density fluctuations. We find that the\nmagnetic energy grows exponentially in a few eddy turnover times, and then,\nfollowing the decay of kinetic turbulence, decays with a temporal power-law\nexponent of -0.7. The magnetic-energy fraction can reach $epsilon_B \\sim$ 0.1\nbut depends on the initial magnetic field strength. We find that the magnetic\nenergy grows by at least two orders of magnitude compared to the magnetic\nenergy immediately behind the shock. This minimum degree of the amplification\ndoes not depend on the amplitude of the initial density fluctuations, while the\ngrowth timescale and the maximum magnetic energy depend on the degree of\ninhomogeneity in the density. The transition from Kolmogorov cascade to MHD\ncritical balance cascade occurs at $\\sim$ 1/10th the initial inhomogeneity\nscale, which limits the maximum synchrotron polarization to less than 2%. New\nresults include the avoidance of electron cooling with RMI turbulence, the\nturbulent photosphere model via RMI, and the shallow decay of the early\nafterglow from RMI. We also performed a simulation of freely decaying\nturbulence with relativistic velocity dispersion. We find that relativistic\nturbulence begins to decay much faster than one eddy-turnover time because of\nfast shock dissipation, which does not support the relativistic turbulence\nmodel by Narayan & Kumar. \n\n"}
{"id": "1011.6397", "contents": "Title: Almost Optimal Explicit Johnson-Lindenstrauss Transformations Abstract: The Johnson-Lindenstrauss lemma is a fundamental result in probability with\nseveral applications in the design and analysis of algorithms in high\ndimensional geometry. Most known constructions of linear embeddings that\nsatisfy the Johnson-Lindenstrauss property involve randomness. We address the\nquestion of explicitly constructing such embedding families and provide a\nconstruction with an almost optimal use of randomness: we use\nO(log(n/delta)log(log(n/delta)/epsilon)) random bits for embedding n dimensions\nto O(log(1/delta)/epsilon^2) dimensions with error probability at most delta,\nand distortion at most epsilon.\n  In particular, for delta = 1/poly(n) and fixed epsilon, we use O(log n loglog\nn) random bits. Previous constructions required at least O(log^2 n) random bits\nto get polynomially small error. \n\n"}
{"id": "1012.3018", "contents": "Title: On the size of data structures used in symbolic model checking Abstract: Temporal Logic Model Checking is a verification method in which we describe a\nsystem, the model, and then we verify whether some properties, expressed in a\ntemporal logic formula, hold in the system. It has many industrial\napplications. In order to improve performance, some tools allow preprocessing\nof the model, verifying on-line a set of properties reusing the same compiled\nmodel; we prove that the complexity of the Model Checking problem, without any\npreprocessing or preprocessing the model or the formula in a polynomial data\nstructure, is the same. As a result preprocessing does not always exponentially\nimprove performance.\n  Symbolic Model Checking algorithms work by manipulating sets of states, and\nthese sets are often represented by BDDs. It has been observed that the size of\nBDDs may grow exponentially as the model and formula increase in size. As a\nside result, we formally prove that a superpolynomial increase of the size of\nthese BDDs is unavoidable in the worst case. While this exponential growth has\nbeen empirically observed, to the best of our knowledge it has never been\nproved so far in general terms. This result not only holds for all types of\nBDDs regardless of the variable ordering, but also for more powerful data\nstructures, such as BEDs, RBCs, MTBDDs, and ADDs. \n\n"}
{"id": "1012.5913", "contents": "Title: All liaisons are dangerous when all your friends are known to us Abstract: Online Social Networks (OSNs) are used by millions of users worldwide.\nAcademically speaking, there is little doubt about the usefulness of\ndemographic studies conducted on OSNs and, hence, methods to label unknown\nusers from small labeled samples are very useful. However, from the general\npublic point of view, this can be a serious privacy concern. Thus, both topics\nare tackled in this paper: First, a new algorithm to perform user profiling in\nsocial networks is described, and its performance is reported and discussed.\nSecondly, the experiments --conducted on information usually considered\nsensitive-- reveal that by just publicizing one's contacts privacy is at risk\nand, thus, measures to minimize privacy leaks due to social graph data mining\nare outlined. \n\n"}
{"id": "1101.3960", "contents": "Title: Speedup in the Traveling Repairman Problem with Constrained Time Windows Abstract: A bicriteria approximation algorithm is presented for the unrooted traveling\nrepairman problem, realizing increased profit in return for increased speedup\nof repairman motion. The algorithm generalizes previous results from the case\nin which all time windows are the same length to the case in which their\nlengths can range between l and 2. This analysis can extend to any range of\ntime window lengths, following our earlier techniques. This relationship\nbetween repairman profit and speedup is applicable over a range of values that\nis dependent on the cost of putting the input in an especially desirable form,\ninvolving what are called \"trimmed windows.\" For time windows with lengths\nbetween 1 and 2, the range of values for speedup $s$ for which our analysis\nholds is $1 \\leq s \\leq 6$. In this range, we establish an approximation ratio\nthat is constant for any specific value of $s$. \n\n"}
{"id": "1101.4450", "contents": "Title: Adaptive Submodular Optimization under Matroid Constraints Abstract: Many important problems in discrete optimization require maximization of a\nmonotonic submodular function subject to matroid constraints. For these\nproblems, a simple greedy algorithm is guaranteed to obtain near-optimal\nsolutions. In this article, we extend this classic result to a general class of\nadaptive optimization problems under partial observability, where each choice\ncan depend on observations resulting from past choices. Specifically, we prove\nthat a natural adaptive greedy algorithm provides a $1/(p+1)$ approximation for\nthe problem of maximizing an adaptive monotone submodular function subject to\n$p$ matroid constraints, and more generally over arbitrary $p$-independence\nsystems. We illustrate the usefulness of our result on a complex adaptive\nmatch-making application. \n\n"}
{"id": "1101.4602", "contents": "Title: Accretion of Chaplygin gas upon black holes: Formation of faster\n  outflowing winds Abstract: We study the accretion of modified Chaplygin gas upon different types of\nblack hole. Modified Chaplygin gas is one of the best candidates for a combined\nmodel of dark matter and dark energy. In addition, from a field theoretical\npoint of view the modified Chaplygin gas model is equivalent to that of a\nscalar field having a self-interacting potential. We formulate the equations\nrelated to both spherical accretion and disc accretion, and respective winds.\nThe corresponding numerical solutions of the flow, particularly of velocity,\nare presented and are analyzed. We show that the accretion-wind system of\nmodified Chaplygin gas dramatically alters the wind solutions, producing faster\nwinds, upon changes in physical parameters, while accretion solutions\nqualitatively remain unaffected. This implies that modified Chaplygin gas is\nmore prone to produce outflow which is the natural consequence of the dark\nenergy into the system. \n\n"}
{"id": "1101.4609", "contents": "Title: Tight Bounds on Information Dissemination in Sparse Mobile Networks Abstract: Motivated by the growing interest in mobile systems, we study the dynamics of\ninformation dissemination between agents moving independently on a plane.\nFormally, we consider $k$ mobile agents performing independent random walks on\nan $n$-node grid. At time $0$, each agent is located at a random node of the\ngrid and one agent has a rumor. The spread of the rumor is governed by a\ndynamic communication graph process ${G_t(r) | t \\geq 0}$, where two agents are\nconnected by an edge in $G_t(r)$ iff their distance at time $t$ is within their\ntransmission radius $r$. Modeling the physical reality that the speed of radio\ntransmission is much faster than the motion of the agents, we assume that the\nrumor can travel throughout a connected component of $G_t$ before the graph is\naltered by the motion. We study the broadcast time $T_B$ of the system, which\nis the time it takes for all agents to know the rumor. We focus on the sparse\ncase (below the percolation point $r_c \\approx \\sqrt{n/k}$) where, with high\nprobability, no connected component in $G_t$ has more than a logarithmic number\nof agents and the broadcast time is dominated by the time it takes for many\nindependent random walks to meet each other. Quite surprisingly, we show that\nfor a system below the percolation point the broadcast time does not depend on\nthe relation between the mobility speed and the transmission radius. In fact,\nwe prove that $T_B = \\tilde{O}(n / \\sqrt{k})$ for any $0 \\leq r < r_c$, even\nwhen the transmission range is significantly larger than the mobility range in\none step, giving a tight characterization up to logarithmic factors. Our result\ncomplements a recent result of Peres et al. (SODA 2011) who showed that above\nthe percolation point the broadcast time is polylogarithmic in $k$. \n\n"}
{"id": "1101.5357", "contents": "Title: Hysteresis in the spectral states of the neutron star low-mass X-ray\n  binary EXO 1745-248 Abstract: We study the low-frequency timing properties and the spectral state evolution\nof the transient neutron star low-mass X-ray binary EXO 1745-248 using the\nentire Rossi X-ray Timing Explorer Proportional Counter Array data. We\ntentatively conclude that EXO 1745-248 is an atoll source, and report the\ndiscovery of a ~ 0.45 Hz low-frequency quasi-periodic oscillation and ~ 10 Hz\npeaked noises. If it is an atoll, this source is unusual because (1) instead of\na `C'-like curve, it traced a clear overall clockwise hysteresis curve in each\nof the colour-colour diagram and the hardness-intensity diagram; and (2) the\nsource took at least 2.5 months to trace the softer banana state, as opposed to\na few hours to a day, which is typical for an atoll source. The shape of the\nhysteresis track was intermediate between the characteristic `q'-like curves of\nseveral black hole systems and `C'-like curves of atolls, implying that EXO\n1745-248 is an important source for the unification of the black hole and\nneutron star accretion processes. \n\n"}
{"id": "1102.0045", "contents": "Title: Renormalized kinetic theory of classical fluids in and out of\n  equilibrium Abstract: We present a theory for the construction of renormalized kinetic equations to\ndescribe the dynamics of classical systems of particles in or out of\nequilibrium. A closed, self-consistent set of evolution equations is derived\nfor the single-particle phase-space distribution function $f$, the correlation\nfunction $C=<\\delta f\\delta f >$, the retarded and advanced density response\nfunctions $\\chi^{R,A}=\\delta f/\\delta\\phi$ to an external potential $\\phi$, and\nthe associated memory functions $\\Sigma^{R,A,C}$. The basis of the theory is an\neffective action functional $\\Omega$ of external potentials $\\phi$ that\ncontains all information about the dynamical properties of the system. In\nparticular, its functional derivatives generate successively the\nsingle-particle phase-space density $f$ and all the correlation and density\nresponse functions, which are coupled through an infinite hierarchy of\nevolution equations. Traditional renormalization techniques are then used to\nperform the closure of the hierarchy through memory functions. The latter\nsatisfy functional equations that can be used to devise systematic\napproximations. The present formulation can be equally regarded as (i) a\ngeneralization to dynamical problems of the density functional theory of fluids\nin equilibrium and (ii) as the classical mechanical counterpart of the theory\nof non-equilibrium Green's functions in quantum field theory. It unifies and\nencompasses previous results for classical Hamiltonian systems with any initial\nconditions. For equilibrium states, the theory reduces to the equilibrium\nmemory function approach. For non-equilibrium fluids, popular closures (e.g.\nLandau, Boltzmann, Lenard-Balescu) are simply recovered and we discuss the\ncorrespondence with the seminal approaches of Martin-Siggia-Rose and of\nRose.and we discuss the correspondence with the seminal approaches of\nMartin-Siggia-Rose and of Rose. \n\n"}
{"id": "1102.0908", "contents": "Title: Linear-Time Algorithms for Graphs of Bounded Rankwidth: A Fresh Look\n  Using Game Theory Abstract: We present an alternative proof of a theorem by Courcelle, Makowski and\nRotics which states that problems expressible in MSO are solvable in linear\ntime for graphs of bounded rankwidth. Our proof uses a game-theoretic approach\nand has the advantage of being self-contained, intuitive, and fairly easy to\nfollow. In particular, our presentation does not assume any background in logic\nor automata theory. We believe that it is good to have alternative proofs of\nthis important result. Moreover our approach can be generalized to prove other\nresults of a similar flavor, for example, that of Courcelle's Theorem for\ntreewidth. \n\n"}
{"id": "1102.0975", "contents": "Title: Spread of Matter over a Neutron-Star Surface During Disk Accretion:\n  Deceleration of Rapid Rotation Abstract: The problem of disk accretion onto the surface of a neutron star with a weak\nmagnetic field at a luminosity exceeding several percent of Eddington is\nreduced to the problem of the braking of a hypersonic flow with a velocity that\nis 0.4-0.5 of the speed of light above the base of the spreading layer -- a\ndense atmosphere made up of previously fallen matter. We show that turbulent\nbraking in the Prandtl-Karman model with universally accepted coefficients for\nterrestrial conditions and laboratory experiments and a ladder of interacting\ngravity waves in a stratified quasi-exponential atmosphere at standard\nRichardson numbers lead to a spin-up of the massive zone that extends to the\nocean made up of a plasma with degenerate electrons. Turbulent braking in the\nocean at the boundary with the outer solid crust reduces the rotation velocity\nto the solid-body rotation velocity of the star. This situation should lead to\nstrong heating of deep atmospheric layers and to the switch-off of the\nexplosive helium burning mechanism. Obviously, a more efficient mechanism for\nthe dissipation of a fast azimuthal flow in the atmosphere should operate in\nX-ray bursters. We show that a giant solitary gravity wave in the atmosphere\ncan lead to energy dissipation and to a sharp decrease in azimuthal velocity in\nfairly rarefied atmospheric layers above the zone of explosive helium burning\nnuclear reactions. We discuss the reasons why this wave, that has no direct\nanalog in the Earth's atmosphere or ocean, appears and its stability. We pose\nthe question as to whether neutron stars with massive atmospheres, spun up to\nhigh velocities by accreting matter from a disk, can exist among the observed\nGalactic X-ray sources. \n\n"}
{"id": "1102.4005", "contents": "Title: Approximating the Online Set Multicover Problems Via Randomized\n  Winnowing Abstract: In this paper, we consider the weighted online set k-multicover problem. In\nthis problem, we have a universe V of elements, a family S of subsets of V with\na positive real cost for every set in S and a \"coverage factor\" (positive\ninteger) k. A subset of elements are presented online in an arbitrary order.\nWhen each element, say i, is presented, we are also told the collection of all\n(at least k) sets and their costs to which i belongs and we need to select\nadditional sets from these sets containing i, if necessary, such that our\ncollection of selected sets contains at least k sets that contain the element\ni. The goal is to minimize the total cost of the selected sets (our algorithm\nand competitive ratio bounds can be extended to the case when a set can be\nselected at most a pre-specified number of times instead of just once; we do\nnot report these extensions for simplicity and also because they have no\nrelevance to the biological applications that motivated our work). In this\npaper, we describe a new randomized algorithm for the online multicover problem\nbased on a randomized version of the winnowing approach of Littlestone. This\nalgorithm generalizes and improves some earlier results by N. Alon, B.\nAwerbuch, Y. Azar, N. Buchbinder, and J. Naor. We also discuss lower bounds on\ncompetitive ratios for deterministic algorithms for general $k$. \n\n"}
{"id": "1102.5538", "contents": "Title: Pseudo-random graphs and bit probe schemes with one-sided error Abstract: We study probabilistic bit-probe schemes for the membership problem. Given a\nset A of at most n elements from the universe of size m we organize such a\nstructure that queries of type \"Is x in A?\" can be answered very quickly.\nH.Buhrman, P.B.Miltersen, J.Radhakrishnan, and S.Venkatesh proposed a bit-probe\nscheme based on expanders. Their scheme needs space of $O(n\\log m)$ bits, and\nrequires to read only one randomly chosen bit from the memory to answer a\nquery. The answer is correct with high probability with two-sided errors. In\nthis paper we show that for the same problem there exists a bit-probe scheme\nwith one-sided error that needs space of $O(n\\log^2 m+\\poly(\\log m))$ bits. The\ndifference with the model of Buhrman, Miltersen, Radhakrishnan, and Venkatesh\nis that we consider a bit-probe scheme with an auxiliary word. This means that\nin our scheme the memory is split into two parts of different size: the main\nstorage of $O(n\\log^2 m)$ bits and a short word of $\\log^{O(1)}m$ bits that is\npre-computed once for the stored set A and `cached'. To answer a query \"Is x in\nA?\" we allow to read the whole cached word and only one bit from the main\nstorage. For some reasonable values of parameters our space bound is better\nthan what can be achieved by any scheme without cached data. \n\n"}
{"id": "1103.0572", "contents": "Title: A multiwavelength study on the high-energy behaviour of Fermi/LAT\n  pulsars Abstract: Using archival as well as freshly acquired data, we assess the X-ray\nbehaviour of the Fermi/LAT gamma-ray pulsars listed in the First Fermi source\ncatalog. After revisiting the relationships between the pulsars' rotational\nenergy losses and their X and gamma-ray luminosities, we focus on the\ndistance-indipendent gamma to X-ray flux ratios. When plotting our Fgamma/Fx\nvalues as a function of the pulsars' rotational energy losses, one immediately\nsees that pulsars with similar energetics have Fgamma/Fx spanning 3 decades.\nSuch spread, most probably stemming from vastly different geometrical\nconfigurations of the X and gamma-ray emitting regions, defies any\nstraightforward interpretation of the plot. Indeed, while energetic pulsars do\nhave low Fgamma/Fx values, little can be said for the bulk of the Fermi neutron\nstars. Dividing our pulsar sample into radio-loud and radio-quiet subsamples,\nwe find that, on average, radio-quiet pulsars do have higher values of\nFgamma/Fx, implying an intrinsec faintness of their X-ray emission and/or a\ndifferent geometrical configuration. Moreover, despite the large spread\nmentioned above, statistical tests show a lower scatter in the radio-quiet\ndataset with respect to the radio-loud one, pointing to a somewhat more\nconstrained geometry for the radio-quiet objects with respect to the radio-loud\nones. \n\n"}
{"id": "1103.1138", "contents": "Title: Toward a standard Gamma Ray Burst: tight correlations between the prompt\n  and the afterglow plateau phase emission Abstract: To reveal and understand astrophysical processes responsible for the Gamma\nRay Burst (GRB) phenomenon, it is crucial to discover and understand relations\nbetween their observational properties. The presented study is performed in the\nGRB rest frames and it uses a sample of 62 long GRBs from our sample of 77\nSwift GRBs with known redshifts. Following the earlier analysis of the\nafterglow {\\it characteristic luminosity $L^*_a$ -- break time $T^*_a$}\ncorrelation for a sample of long GRBs \\citep{Dainotti2010} we extend it to\ncorrelations between the afterglow and the prompt emission GRB physical\nparameters. We reveal a tight physical scaling between the mentioned afterglow\nluminosity $ L^*_a$ and the prompt emission {\\it mean} luminosity $<L^*_p>_{45}\n\\equiv E_{iso}/T^*_{45}$. The distribution, with the Spearman correlation\ncoefficient reaching 0.95 for the data subsample with most regular light\ncurves, can be fitted with approximately $L^*_a \\propto {<L^*_p>_{45}}^{0.7}$.\nWe also analyzed correlations of $L^*_a$ with several other prompt emission\nparameters, including the isotropic energy $E_{iso}$, the peak energy in the\n$\\nu F_{\\nu}$ spectrum, $E_{peak}$, and the variability parameter, $V$, defined\nby \\cite{N000}. As a result, we reveal significant correlations also between\nthese quantities, with an exception of the variability parameter. The main\nresult of the present study is the discovery that the highest correlated GRB\nsubsample in the \\citet{Dainotti2010} afterglow analysis, for the GRBs with\ncanonical X\\,-\\,ray light curves, leads also to the highest {\\it\nprompt-afterglow} correlations and such events can be considered to form a\nsample of standard GRBs for astrophysics and cosmology. \n\n"}
{"id": "1103.3010", "contents": "Title: A VLA search for 5 GHz radio transients and variables at low Galactic\n  latitudes Abstract: We present the results of a 5 GHz survey with the Very Large Array, designed\nto search for short-lived (<1 day) transients and to characterize the\nvariability of radio sources at milli-Jansky levels. A total sky area of 2.66\ndeg^2, spread over 141 fields at low Galactic latitudes was observed 16 times\nwith a cadence sampling timescales of days, months and years. Most of the data\nwere searched for transients in near real time. Candidates were followed up\nusing visible light telescopes (1-2 hr delays) and the X-Ray Telescope on board\nthe Swift satellite. The final processing of the data revealed a single\npossible transient with a flux density of 2.4 mJy. This implies a transients,\n>1.8 mJy, sky surface density of 0.039 (-0.032/+0.13) deg^-2. This areal\ndensity is consistent with the sky surface density of transients from the Bower\net al. survey extrapolated to 1.8 mJy. Our observed transient areal density is\nconsistent with a Neutron Stars (NSs) origin for these events. Furthermore, we\nuse the data to measure the sources variability on days to years time scales,\nand we present the variability structure function of 5 GHz sources. The mean\nstructure function shows a fast increase on ~1 day time scale, followed by a\nslower increase on time scales of up to 10 days. On time scales between 10-60\ndays the structure function is roughly constant. We find that >30% of the\nunresolved sources brighter than 1.8 mJy are variable at the >4 sigma\nconfidence level, presumably due mainly to refractive scintillation. \n\n"}
{"id": "1103.3250", "contents": "Title: Gamma-ray observations of the Be/pulsar binary 1A 0535+262 during a\n  giant X-ray outburst Abstract: Giant X-ray outbursts, with luminosities of about $ 10^{37}$ erg s$^{-1}$,\nare observed roughly every 5 years from the nearby Be/pulsar binary 1A\n0535+262. In this article, we present observations of the source with VERITAS\nat very-high energies (VHE; E$>$100 GeV) triggered by the X-ray outburst in\nDecember 2009. The observations started shortly after the onset of the\noutburst, and they provided comprehensive coverage of the episode, as well as\nthe 111-day binary orbit. No VHE emission is evident at any time. We also\nexamined data from the contemporaneous observations of 1A 0535+262 with the\nFermi/LAT at high energy photons (HE; E$>$0.1 GeV) and failed to detect the\nsource at GeV energies. The X-ray continua measured with the Swift/XRT and the\nRXTE/PCA can be well described by the combination of blackbody and Comptonized\nemission from thermal electrons. Therefore, the gamma-ray and X-ray\nobservations suggest the absence of a significant population of non-thermal\nparticles in the system. This distinguishes 1A~0535+262 from those Be X-ray\nbinaries (such as PSR B1259--63 and LS I +61$^{\\circ}$303) that have been\ndetected at GeV--TeV energies. We discuss the implications of the results on\ntheoretical models. \n\n"}
{"id": "1103.5102", "contents": "Title: Data-Oblivious External-Memory Algorithms for the Compaction, Selection,\n  and Sorting of Outsourced Data Abstract: We present data-oblivious algorithms in the external-memory model for\ncompaction, selection, and sorting. Motivation for such problems comes from\nclients who use outsourced data storage services and wish to mask their data\naccess patterns. We show that compaction and selection can be done\ndata-obliviously using $O(N/B)$ I/Os, and sorting can be done, with a high\nprobability of success, using $O((N/B)\\log_{M/B} (N/B))$ I/Os. Our methods use\na number of new algorithmic techniques, including data-oblivious uses of\ninvertible Bloom lookup tables, a butterfly-like compression network,\nrandomized data thinning, and \"shuffle-and-deal\" data perturbation. In\naddition, since data-oblivious sorting is the bottleneck in the \"inner loop\" in\nexisting oblivious RAM simulations, our sorting result improves the amortized\ntime overhead to do oblivious RAM simulation by a logarithmic factor in the\nexternal-memory model. \n\n"}
{"id": "1104.1135", "contents": "Title: Simultaneously Satisfying Linear Equations Over $\\mathbb{F}_2$: MaxLin2\n  and Max-$r$-Lin2 Parameterized Above Average Abstract: In the parameterized problem \\textsc{MaxLin2-AA}[$k$], we are given a system\nwith variables $x_1,...,x_n$ consisting of equations of the form $\\prod_{i \\in\nI}x_i = b$, where $x_i,b \\in \\{-1, 1\\}$ and $I\\subseteq [n],$ each equation has\na positive integral weight, and we are to decide whether it is possible to\nsimultaneously satisfy equations of total weight at least $W/2+k$, where $W$ is\nthe total weight of all equations and $k$ is the parameter (if $k=0$, the\npossibility is assured). We show that \\textsc{MaxLin2-AA}[$k$] has a kernel\nwith at most $O(k^2\\log k)$ variables and can be solved in time $2^{O(k\\log\nk)}(nm)^{O(1)}$. This solves an open problem of Mahajan et al. (2006).\n  The problem \\textsc{Max-$r$-Lin2-AA}[$k,r$] is the same as\n\\textsc{MaxLin2-AA}[$k$] with two differences: each equation has at most $r$\nvariables and $r$ is the second parameter. We prove a theorem on\n\\textsc{Max-$r$-Lin2-AA}[$k,r$] which implies that\n\\textsc{Max-$r$-Lin2-AA}[$k,r$] has a kernel with at most $(2k-1)r$ variables\nimproving a number of results including one by Kim and Williams (2010). The\ntheorem also implies a lower bound on the maximum of a function $f:\\ \\{-1,1\\}^n\n\\rightarrow \\mathbb{R}$ of degree $r$. We show applicability of the lower bound\nby giving a new proof of the Edwards-Erd{\\H o}s bound (each connected graph on\n$n$ vertices and $m$ edges has a bipartite subgraph with at least $m/2 +\n(n-1)/4$ edges) and obtaining a generalization. \n\n"}
{"id": "1104.2875", "contents": "Title: The supernova remnant CTB 37B and its associated magnetar CXOU\n  J171405.7-381031: evidence for a magnetar-driven remnant Abstract: We discuss in this Letter the association of the candidate magnetar CXOU\nJ171405.7-381031 with the supernova remnant CTB 37B. The recent detection of\nthe period derivative of the object allowed an estimation of a young\ncharacteristic age of only $\\sim 1000 yr$. This value is too small to be\ncompatible even with the minimum radius of the remnant $\\geq 10 pc$, the value\ncorresponding to the {\\it lower} limit of the estimated distance of $10.2 \\pm\n3.5 kpc$, unless the true distance happens to be even smaller than the lower\nlimit. We argue that a consistent scenario for the remnant origin, in which the\nlatter is powered by the energy injected by a young magnetar, is indeed more\naccurate to explain the young age, and points out to its non-standard (i.e.\nmagnetar-driven) nature. \n\n"}
{"id": "1104.3146", "contents": "Title: Localizing Sagittarius A* and M87 on Microarcsecond Scales with\n  Millimeter VLBI Abstract: With the advent of the Event Horizon Telescope (EHT), a\nmillimeter/sub-millimeter very-long baseline interferometer (VLBI), it has\nbecome possible to image a handful of black holes with sub-horizon resolutions.\nHowever, these images do not translate into microarcsecond absolute positions\ndue to the lack of absolute phase information when an external phase reference\nis not used. Due to the short atmospheric coherence time at these wavelengths,\nnodding between the source and phase reference is impractical. However, here we\nsuggest an alternative scheme which makes use of the fact that many of the VLBI\nstations within the EHT are arrays in their own right. With this we show that\nit should be possible to absolutely position the supermassive black holes at\nthe centers of the Milky Way (Sgr A*) and M87 relative to nearby objects with\nprecisions of roughly 1 microarcsecond. This is sufficient to detect the\nperturbations to Sgr A*'s position resulting from interactions with the stars\nand stellar-mass black holes in the Galactic cusp on year timescales, and\nseverely constrain the astrophysically relevant parameter space for an orbiting\nintermediate mass black hole, implicated in some mechanisms for producing the\nyoung massive stars in the Galactic center. For M87, it allows the registering\nof millimeter images, in which the black hole may be identified by its\nsilhouette against nearby emission, and existing larger scale radio images,\neliminating present ambiguities in the nature of the radio core and\ninclination, opening angle, and source of the radio jet. \n\n"}
{"id": "1104.3720", "contents": "Title: Solving the Closest Vector Problem with respect to l_p Norms Abstract: In this paper, we present a deterministic algorithm for the closest vector\nproblem for all l_p-norms, 1 < p < \\infty, and all polyhedral norms, especially\nfor the l_1-norm and the l_{\\infty}-norm. We achieve our results by introducing\na new lattice problem, the lattice membership problem. We describe a\ndeterministic algorithm for the lattice membership problem, which is a\ngeneralization of Lenstra's algorithm for integer programming. We also describe\na polynomial time reduction from the closest vector problem to the lattice\nmembership problem. This approach leads to a deterministic algorithm that\nsolves the closest vector problem for all l_p-norms, 1 < p < \\infty, in time p\nlog_2 (r)^{O (1)} n^{(5/2+o(1))n} and for all polyhedral norms in time (s log_2\n(r))^{O (1)} n^{(2+o(1))n}, where s is the number of constraints defining the\npolytope and r is an upper bound on the coefficients used to describe the\nconvex body. \n\n"}
{"id": "1104.4954", "contents": "Title: On the Complexity of Solving a Bivariate Polynomial System Abstract: We study the complexity of computing the real solutions of a bivariate\npolynomial system using the recently proposed algorithm BISOLVE. BISOLVE is a\nclassical elimination method which first projects the solutions of a system\nonto the $x$- and $y$-axes and, then, selects the actual solutions from the so\ninduced candidate set. However, unlike similar algorithms, BISOLVE requires no\ngenericity assumption on the input nor it needs any change of the coordinate\nsystem. Furthermore, extensive benchmarks from \\cite{bes-bisolve-2011} confirm\nthat the algorithm outperforms state of the art approaches by a large factor.\nIn this work, we show that, for two polynomials $f,g\\in\\mathbb{Z}[x,y]$ of\ntotal degree at most $n$ with integer coefficients bounded by $2^\\tau$, BISOLVE\ncomputes isolating boxes for all real solutions of the system $f=g=0$ using\n$\\Otilde(n^8\\tau^{2})$ bit operations, thereby improving the previous record\nbound by a factor of at least $n^{2}$. \n\n"}
{"id": "1104.5172", "contents": "Title: Gamma-ray halos as a measure of intergalactic magnetic fields: a\n  classical moment problem Abstract: The presence of weak intergalactic magnetic fields can be studied by their\neffect on electro-magnetic cascades induced by multi-TeV gamma-rays in the\ncosmic radiation background. Small deflections of secondary electrons and\npositrons as the cascade develops extend the apparent size of the emission\nregion of distant TeV gamma-ray sources. These gamma-ray halos can be\nresolvable in imaging atmospheric Cherenkov telescopes and serve as a measure\nof the intergalactic magnetic field strength and coherence length. We present a\nmethod of calculating the gamma-ray halo for isotropically emitting sources by\ntreating magnetic deflections in the cascade as a diffusion process. With this\nansatz the moments of the halo follow from a set of simple diffusion-cascade\nequations. The reconstruction of the angular distribution is then equivalent to\na classical moment problem. We present a simple solution using Pade\napproximations of the moment's generating function. \n\n"}
{"id": "1104.5187", "contents": "Title: A Search for a Diffuse Flux of Astrophysical Muon Neutrinos with the\n  IceCube 40-String Detector Abstract: The IceCube Neutrino Observatory is a 1 km$^{3}$ detector currently taking\ndata at the South Pole. One of the main strategies used to look for\nastrophysical neutrinos with IceCube is the search for a diffuse flux of\nhigh-energy neutrinos from unresolved sources. A hard energy spectrum of\nneutrinos from isotropically distributed astrophysical sources could manifest\nitself as a detectable signal that may be differentiated from the atmospheric\nneutrino background by spectral measurement. This analysis uses data from the\nIceCube detector collected in its half completed configuration which operated\nbetween April 2008 and May 2009 to search for a diffuse flux of astrophysical\nmuon neutrinos. A total of 12,877 upward going candidate neutrino events have\nbeen selected for this analysis. No evidence for a diffuse flux of\nastrophysical muon neutrinos was found in the data set leading to a 90 percent\nC.L. upper limit on the normalization of an $E^{-2}$ astrophysical $\\nu_{\\mu}$\nflux of $8.9 \\times 10^{-9} \\ \\mathrm{GeV \\ cm^{-2} \\ s^{-1} \\ sr^{-1}}$. The\nanalysis is sensitive in the energy range between $35 \\ \\mathrm{TeV} - 7 \\\n\\mathrm{PeV}$. The 12,877 candidate neutrino events are consistent with\natmospheric muon neutrinos measured from 332 GeV to 84 TeV and no evidence for\na prompt component to the atmospheric neutrino spectrum is found. \n\n"}
{"id": "1104.5597", "contents": "Title: A Static Optimality Transformation with Applications to Planar Point\n  Location Abstract: Over the last decade, there have been several data structures that, given a\nplanar subdivision and a probability distribution over the plane, provide a way\nfor answering point location queries that is fine-tuned for the distribution.\nAll these methods suffer from the requirement that the query distribution must\nbe known in advance.\n  We present a new data structure for point location queries in planar\ntriangulations. Our structure is asymptotically as fast as the optimal\nstructures, but it requires no prior information about the queries. This is a\n2D analogue of the jump from Knuth's optimum binary search trees (discovered in\n1971) to the splay trees of Sleator and Tarjan in 1985. While the former need\nto know the query distribution, the latter are statically optimal. This means\nthat we can adapt to the query sequence and achieve the same asymptotic\nperformance as an optimum static structure, without needing any additional\ninformation. \n\n"}
{"id": "1105.0255", "contents": "Title: Photopolarimetric Monitoring of Blazars in the Optical and Near-Infrared\n  Bands with the Kanata Telescope. I. Correlations between Flux, Color, and\n  Polarization Abstract: We report on the correlation between the flux, color and polarization\nvariations on time scales of days--months in blazars, and discuss their\nuniversal aspects. We performed monitoring of 42 blazars in the optical and\nnear-infrared bands from 2008 to 2010 using TRISPEC attached to the \"Kanata\"\n1.5-m telescope. We found that 28 blazars exhibited \"bluer-when-brighter\"\ntrends in their whole or a part of time-series data sets. This corresponds to\n88% of objects that were observed for >10 days. Thus, our observation\nunambiguously confirmed that the \"bluer-when-brighter\" trend is common in the\nemission from blazar jets. This trend was apparently generated by a variation\ncomponent with a constant and relatively blue color and an underlying red\ncomponent. Prominent short-term flares on time scales of days--weeks tended to\nexhibit a spectral hysteresis; their rising phases were bluer than their decay\nphases around the flare maxima. In contrast to the strong flux--color\ncorrelation, the correlation of the flux and polarization degree was relatively\nweak; only 10 objects showed significant positive correlations. Rotations of\npolarization were detected only in three objects: PKS 1510-089, 3C 454.3, and\nPKS 1749+096, and possibly in S5 0716+714. We also investigated the dependence\nof the degree of variability on the luminosity and the synchrotron peak\nfrequency, \\nu_peak. As a result, we found that lower luminosity and higher\n\\nu_peak objects had smaller variations in their amplitudes both in the flux,\ncolor, and polarization degree. Our observation suggests the presence of\nseveral distinct emitting sources, which have different variation time-scales,\ncolors, and polarizations. We propose that the energy injection by, for\nexample, internal shocks in relativistic shells is a major factor for blazar\nvariations on time scales of both days and months. \n\n"}
{"id": "1105.1842", "contents": "Title: Property Testing for Cyclic Groups and Beyond Abstract: This paper studies the problem of testing if an input (Gamma,*), where Gamma\nis a finite set of unknown size and * is a binary operation over Gamma given as\nan oracle, is close to a specified class of groups. Friedl et al. [Efficient\ntesting of groups, STOC'05] have constructed an efficient tester using\npoly(log|Gamma|) queries for the class of abelian groups. We focus in this\npaper on subclasses of abelian groups, and show that these problems are much\nharder: Omega(|Gamma|^{1/6}) queries are necessary to test if the input is\nclose to a cyclic group, and Omega(|Gamma|^c) queries for some constant c are\nnecessary to test more generally if the input is close to an abelian group\ngenerated by k elements, for any fixed integer k>0. We also show that knowledge\nof the size of the ground set Gamma helps only for k=1, in which case we\nconstruct an efficient tester using poly(log|Gamma|) queries; for any other\nvalue k>1 the query complexity remains Omega(|Gamma|^c). All our upper and\nlower bounds hold for both the edit distance and the Hamming distance. These\nare, to the best of our knowledge, the first nontrivial lower bounds for such\ngroup-theoretic problems in the property testing model and, in particular, they\nimply the first exponential separations between the classical and quantum query\ncomplexities of testing closeness to classes of groups. \n\n"}
{"id": "1105.3807", "contents": "Title: On the black hole limit of electrically counterpoised dust\n  configurations Abstract: By means of a simple scaling transformation any asymptotically flat\nPapapetrou-Majumdar solution of the Einstein-Maxwell equations corresponding to\na localized regular distribution of electrically counterpoised dust can be\nreformulated as a one-parameter family of solutions admitting a black hole\nlimit. In the limit, a characteristic separation of spacetimes occurs: From the\nexterior point of view, the extreme Reissner-Nordstrom metric outside the event\nhorizon is formed. From the interior point of view, a regular,\nnon-asymptotically flat (and in general non-spherically symmetric) spacetime\nwith the extreme Reissner-Nordstrom near-horizon geometry at spatial infinity\nresults. \n\n"}
{"id": "1105.5298", "contents": "Title: Simplicial blowups and discrete normal surfaces in simpcomp Abstract: simpcomp is an extension to GAP, the well known system for computational\ndiscrete algebra. It allows the user to work with simplicial complexes. In the\nlatest version, support for simplicial blowups and discrete normal surfaces was\nadded, both features unique to simpcomp. Furthermore, new functions for\nconstructing certain infinite series of triangulations have been implemented\nand interfaces to other software packages have been improved to previous\nversions. \n\n"}
{"id": "1105.5933", "contents": "Title: The Cell Probe Complexity of Dynamic Range Counting Abstract: In this paper we develop a new technique for proving lower bounds on the\nupdate time and query time of dynamic data structures in the cell probe model.\nWith this technique, we prove the highest lower bound to date for any explicit\nproblem, namely a lower bound of $t_q=\\Omega((\\lg n/\\lg(wt_u))^2)$. Here $n$ is\nthe number of update operations, $w$ the cell size, $t_q$ the query time and\n$t_u$ the update time. In the most natural setting of cell size $w=\\Theta(\\lg\nn)$, this gives a lower bound of $t_q=\\Omega((\\lg n/\\lg \\lg n)^2)$ for any\npolylogarithmic update time. This bound is almost a quadratic improvement over\nthe highest previous lower bound of $\\Omega(\\lg n)$, due to P\\v{a}tra\\c{s}cu\nand Demaine [SICOMP'06].\n  We prove the lower bound for the fundamental problem of weighted orthogonal\nrange counting. In this problem, we are to support insertions of\ntwo-dimensional points, each assigned a $\\Theta(\\lg n)$-bit integer weight. A\nquery to this problem is specified by a point $q=(x,y)$, and the goal is to\nreport the sum of the weights assigned to the points dominated by $q$, where a\npoint $(x',y')$ is dominated by $q$ if $x' \\leq x$ and $y' \\leq y$. In addition\nto being the highest cell probe lower bound to date, the lower bound is also\ntight for data structures with update time $t_u = \\Omega(\\lg^{2+\\eps}n)$, where\n$\\eps>0$ is an arbitrarily small constant. \n\n"}
{"id": "1105.6145", "contents": "Title: Maximum lilkelihood estimation in the $\\beta$-model Abstract: We study maximum likelihood estimation for the statistical model for\nundirected random graphs, known as the $\\beta$-model, in which the degree\nsequences are minimal sufficient statistics. We derive necessary and sufficient\nconditions, based on the polytope of degree sequences, for the existence of the\nmaximum likelihood estimator (MLE) of the model parameters. We characterize in\na combinatorial fashion sample points leading to a nonexistent MLE, and\nnonestimability of the probability parameters under a nonexistent MLE. We\nformulate conditions that guarantee that the MLE exists with probability\ntending to one as the number of nodes increases. \n\n"}
{"id": "1106.0683", "contents": "Title: Towards P = NP via k-SAT: A k-SAT Algorithm Using Linear Algebra on\n  Finite Fields Abstract: The problem of P vs. NP is very serious, and solutions to the problem can\nhelp save lives. This article is an attempt at solving the problem using a\ncomputer algorithm. It is presented in a fashion that will hopefully allow for\neasy understanding for many people and scientists from many diverse fields.\n  In technical terms, a novel method for solving k-SAT is explained. This\nmethod is primarily based on linear algebra and finite fields. Evidence is\ngiven that this method may require rougly O(n^3) time and space for\ndeterministic models. More specifically the algorithm runs in time O(P\nV(n+V)^2) with mistaking satisfiable Boolean expressions as unsatisfiable with\nan approximate probablity 1 / \\Theta(V(n+V)^2)^P, where n is the number of\nclauses and V is the number of variables. It's concluded that significant\nevidence exists that P=NP.\n  There is a forum devoted to this paper at http://482527.ForumRomanum.com. All\nare invited to correspond here and help with the analysis of the algorithm.\nSource code for the associated algorithm can be found at\nhttps://sourceforge.net/p/la3sat. \n\n"}
{"id": "1106.1348", "contents": "Title: Fermi-LAT Observations of Markarian 421: the Missing Piece of its\n  Spectral Energy Distribution Abstract: We report on the gamma-ray activity of the high-synchrotron-peaked BL\nLacertae object Mrk 421 during the first 1.5 years of Fermi operation, from\n2008 August 5 to 2010 March 12. We find that the Large Area Telescope (LAT)\ngamma-ray spectrum above 0.3 GeV can be well-described by a power-law function\nwith photon index Gamma=1.78 +/- 0.02 and average photon flux F(>0.3 GeV)=(7.23\n+/- 0.16) x 10^{-8} ph cm^{-2} s^{-1}. Over this time period, the Fermi-LAT\nspectrum above 0.3 GeV was evaluated on 7-day-long time intervals, showing\nsignificant variations in the photon flux (up to a factor ~3 from the minimum\nto the maximum flux), but mild spectral variations. The variability amplitude\nat X-ray frequencies measured by RXTE/ASM and Swift/BAT is substantially larger\nthan that in gamma-rays measured by Fermi-LAT, and these two energy ranges are\nnot significantly correlated. We also present the first results from the\n4.5-month-long multifrequency campaign on Mrk 421, which included the VLBA,\nSwift, RXTE, MAGIC, the F-GAMMA, GASP-WEBT, and other collaborations and\ninstruments which provided excellent temporal and energy coverage of the source\nthroughout the entire campaign (2009 January 19 to 2009 June 1). During this\ncampaign, Mrk 421 showed a low activity at all wavebands. The extensive\nmulti-instrument (radio to TeV) data set provides an unprecedented, complete\nlook at the quiescent spectral energy distribution (SED) for this source. The\nbroad band SED was reproduced with a leptonic (one-zone Synchrotron\nSelf-Compton) and a hadronic model (Synchrotron Proton Blazar). Both frameworks\nare able to describe the average SED reasonably well, implying comparable jet\npowers but very different characteristics for the blazar emission site. \n\n"}
{"id": "1106.3913", "contents": "Title: Torsional oscillations in tensor-vector-scalar theory Abstract: With the Cowling approximation, the torsional oscillations on relativistic\nstars in tensor-vector-scalar (TeVeS) theory are examined. The spectrum\nfeatures in TeVeS are very similar to those in general relativity (GR), but the\ntorsional frequencies in TeVeS become larger than those expected in GR. We find\nthat, compared with the fluid oscillations with polar parity, the torsional\nfrequencies depend strongly on the gravitational theory. Since the dependences\nof fundamental frequencies on the gravitational theory and on the equation of\nstate are different from those of overtone, it could be possible to distinguish\nTeVeS from GR in the strong-field regime via observations of this type of\noscillations with the help of the observation of stellar mass. \n\n"}
{"id": "1107.1327", "contents": "Title: On counting untyped lambda terms Abstract: We present several results on counting untyped lambda terms, i.e., on telling\nhow many terms belong to such or such class, according to the size of the terms\nand/or to the number of free variables. \n\n"}
{"id": "1107.2379", "contents": "Title: Data Stability in Clustering: A Closer Look Abstract: We consider the model introduced by Bilu and Linial (2010), who study\nproblems for which the optimal clustering does not change when distances are\nperturbed. They show that even when a problem is NP-hard, it is sometimes\npossible to obtain efficient algorithms for instances resilient to certain\nmultiplicative perturbations, e.g. on the order of $O(\\sqrt{n})$ for max-cut\nclustering. Awasthi et al. (2010) consider center-based objectives, and Balcan\nand Liang (2011) analyze the $k$-median and min-sum objectives, giving\nefficient algorithms for instances resilient to certain constant multiplicative\nperturbations.\n  Here, we are motivated by the question of to what extent these assumptions\ncan be relaxed while allowing for efficient algorithms. We show there is little\nroom to improve these results by giving NP-hardness lower bounds for both the\n$k$-median and min-sum objectives. On the other hand, we show that constant\nmultiplicative resilience parameters can be so strong as to make the clustering\nproblem trivial, leaving only a narrow range of resilience parameters for which\nclustering is interesting. We also consider a model of additive perturbations\nand give a correspondence between additive and multiplicative notions of\nstability. Our results provide a close examination of the consequences of\nassuming stability in data. \n\n"}
{"id": "1107.3406", "contents": "Title: Measurement of Cosmic Ray antiproton/proton flux ratio at TeV energies\n  with ARGO-YBJ Abstract: Cosmic ray antiprotons provide an important probe for the study of cosmic-ray\npropagation in the interstellar space and to investigate the existence of\nGalactic dark matter. The ARGO-YBJ experiment, located at the Yangbajing Cosmic\nRay Laboratory (Tibet, P.R. China, 4300 m a.s.l., 606 g/cm$^2$), is the only\nexperiment exploiting the full coverage approach at very high altitude\npresently at work. The ARGO-YBJ experiment is particularly effective in\nmeasuring the cosmic ray antimatter content via the observation of the cosmic\nrays Moon shadowing effect. Based on all the data recorded during the period\nfrom July 2006 through November 2009 and a full Monte Carlo simulation, we\nsearched for the existence of the shadow produced by antiprotons at the few-TeV\nenergy region. No evidence of the existence of antiprotons was found in this\nenergy region. Upper limits to the antip/p flux ratio are set to 5 % at a\nmedian energy of 2 TeV and 6 % at 5 TeV with a confidence level of 90 %. In the\nfew-TeV energy range this result is the lowest available. \n\n"}
{"id": "1108.2464", "contents": "Title: Grothendieck-type inequalities in combinatorial optimization Abstract: We survey connections of the Grothendieck inequality and its variants to\ncombinatorial optimization and computational complexity. \n\n"}
{"id": "1108.3756", "contents": "Title: On the Intersection of All Critical Sets of a Unicyclic Graph Abstract: A set S is independent in a graph G if no two vertices from S are adjacent.\nThe independence number alpha(G) is the cardinality of a maximum independent\nset, while mu(G) is the size of a maximum matching in G. If alpha(G)+mu(G)=|V|,\nthen G=(V,E) is called a Konig-Egervary graph. The number\nd_{c}(G)=max{|A|-|N(A)|} is called the critical difference of G (Zhang, 1990).\nBy core(G) (corona(G)) we denote the intersection (union, respectively) of all\nmaximum independent sets, while by ker(G) we mean the intersection of all\ncritical independent sets. A connected graph having only one cycle is called\nunicyclic. It is known that ker(G) is a subset of core(G) for every graph G,\nwhile the equality is true for bipartite graphs (Levit and Mandrescu, 2011).\nFor Konig-Egervary unicyclic graphs, the difference |core(G)|-|ker(G)| may\nequal any non-negative integer. In this paper we prove that if G is a\nnon-Konig-Egervary unicyclic graph, then: (i) ker(G)= core(G) and (ii)\n|corona(G)|+|core(G)|=2*alpha(G)+1. Pay attention that\n|corona(G)|+|core(G)|=2*alpha(G) holds for every Konig-Egervary graph. \n\n"}
{"id": "1108.4803", "contents": "Title: Constraint Satisfaction Problems Parameterized Above or Below Tight\n  Bounds: A Survey Abstract: We consider constraint satisfaction problems parameterized above or below\ntight bounds. One example is MaxSat parameterized above $m/2$: given a CNF\nformula $F$ with $m$ clauses, decide whether there is a truth assignment that\nsatisfies at least $m/2+k$ clauses, where $k$ is the parameter. Among other\nproblems we deal with are MaxLin2-AA (given a system of linear equations over\n$\\mathbb{F}_2$ in which each equation has a positive integral weight, decide\nwhether there is an assignment to the variables that satisfies equations of\ntotal weight at least $W/2+k$, where $W$ is the total weight of all equations),\nMax-$r$-Lin2-AA (the same as MaxLin2-AA, but each equation has at most $r$\nvariables, where $r$ is a constant) and Max-$r$-Sat-AA (given a CNF formula $F$\nwith $m$ clauses in which each clause has at most $r$ literals, decide whether\nthere is a truth assignment satisfying at least $\\sum_{i=1}^m(1-2^{r_i})+k$\nclauses, where $k$ is the parameter, $r_i$ is the number of literals in Clause\n$i$, and $r$ is a constant). We also consider Max-$r$-CSP-AA, a natural\ngeneralization of both Max-$r$-Lin2-AA and Max-$r$-Sat-AA, order (or,\npermutation) constraint satisfaction problems of arities 2 and 3 parameterized\nabove the average value and some other problems related to MaxSat. We discuss\nresults, both polynomial kernels and parameterized algorithms, obtained for the\nproblems mainly in the last few years as well as some open questions. \n\n"}
{"id": "1108.6277", "contents": "Title: A search for the near-infrared counterpart of the eclipsing millisecond\n  X-ray pulsar Swift J1749.4-2807 Abstract: Swift J1749.4-2807 is a transient accreting millisecond X-ray pulsars, the\nfirst that displayed X-ray eclipses. Therefore it holds a great potential for\naccurate mass measurements in a low mass X-ray binary system. The determination\nof the companion star radial velocity would make it possible to fully resolve\nthe system and to accurately measure the mass of the neutron star based on\ndynamical measurements. Unfortunately, no optical/NIR counterpart has been\nidentified to date for this system, either in outburst or in quiescence. We\nperformed a photometric study of the field of Swift J1749.4-2807 during\nquiescence in order to search for the presence of a variable counterpart. The\nsource direction lies on the Galactic plane, making any search for its\noptical/NIR counterpart challenging. To minimize the effects of field crowding\nand interstellar extinction, we carried out our observations using the adaptive\noptics near-infrared imager NACO mounted at the ESO Very Large Telescope. From\nthe analysis of Swift X-ray data obtained during outburst, we derived the most\nprecise (1.6\" radius) position for this source. Due to the extreme stellar\ncrowding of the field, 41 sources are detected in our VLT images within the\nX-ray error circle, with some of them possibly showing variability consistent\nwith the expectations. We carried out the first deep imaging campaign devoted\nto the search of the quiescent NIR counterpart of Swift J1749.4-2807. Our\nresults allow to provide constraints on the nature of the companion star of\nthis system. Furthermore, they suggest that future phase-resolved NIR\nobservations (performed with large aperture telescopes and adaptive optics)\ncovering the full orbital period of the system are likely to identify the\nquiescent counterpart of Swift J1749.4-2807, through the measure of its orbital\nvariability, opening the possibility of dynamical studies of this unique\nsource. \n\n"}
{"id": "1109.0345", "contents": "Title: Planar and Poly-Arc Lombardi Drawings Abstract: In Lombardi drawings of graphs, edges are represented as circular arcs, and\nthe edges incident on vertices have perfect angular resolution. However, not\nevery graph has a Lombardi drawing, and not every planar graph has a planar\nLombardi drawing. We introduce k-Lombardi drawings, in which each edge may be\ndrawn with k circular arcs, noting that every graph has a smooth 2-Lombardi\ndrawing. We show that every planar graph has a smooth planar 3-Lombardi drawing\nand further investigate topics connecting planarity and Lombardi drawings. \n\n"}
{"id": "1109.1693", "contents": "Title: Graph Expansion and Communication Costs of Fast Matrix Multiplication Abstract: The communication cost of algorithms (also known as I/O-complexity) is shown\nto be closely related to the expansion properties of the corresponding\ncomputation graphs. We demonstrate this on Strassen's and other fast matrix\nmultiplication algorithms, and obtain first lower bounds on their communication\ncosts.\n  In the sequential case, where the processor has a fast memory of size $M$,\ntoo small to store three $n$-by-$n$ matrices, the lower bound on the number of\nwords moved between fast and slow memory is, for many of the matrix\nmultiplication algorithms, $\\Omega((\\frac{n}{\\sqrt M})^{\\omega_0}\\cdot M)$,\nwhere $\\omega_0$ is the exponent in the arithmetic count (e.g., $\\omega_0 = \\lg\n7$ for Strassen, and $\\omega_0 = 3$ for conventional matrix multiplication).\nWith $p$ parallel processors, each with fast memory of size $M$, the lower\nbound is $p$ times smaller.\n  These bounds are attainable both for sequential and for parallel algorithms\nand hence optimal. These bounds can also be attained by many fast algorithms in\nlinear algebra (e.g., algorithms for LU, QR, and solving the Sylvester\nequation). \n\n"}
{"id": "1109.4729", "contents": "Title: Parameterized Complexity of Firefighting Revisited Abstract: The Firefighter problem is to place firefighters on the vertices of a graph\nto prevent a fire with known starting point from lighting up the entire graph.\nIn each time step, a firefighter may be permanently placed on an unburned\nvertex and the fire spreads to its neighborhood in the graph in so far no\nfirefighters are protecting those vertices. The goal is to let as few vertices\nburn as possible. This problem is known to be NP-complete, even when restricted\nto bipartite graphs or to trees of maximum degree three. Initial study showed\nthe Firefighter problem to be fixed-parameter tractable on trees in various\nparameterizations. We complete these results by showing that the problem is in\nFPT on general graphs when parameterized by the number of burned vertices, but\nhas no polynomial kernel on trees, resolving an open problem. Conversely, we\nshow that the problem is W[1]-hard when parameterized by the number of unburned\nvertices, even on bipartite graphs. For both parameterizations, we additionally\ngive refined algorithms on trees, improving on the running times of the known\nalgorithms. \n\n"}
{"id": "1109.5036", "contents": "Title: Testing first-order properties for subclasses of sparse graphs Abstract: We present a linear-time algorithm for deciding first-order (FO) properties\nin classes of graphs with bounded expansion, a notion recently introduced by\nNesetril and Ossona de Mendez. This generalizes several results from the\nliterature, because many natural classes of graphs have bounded expansion:\ngraphs of bounded tree-width, all proper minor-closed classes of graphs, graphs\nof bounded degree, graphs with no subgraph isomorphic to a subdivision of a\nfixed graph, and graphs that can be drawn in a fixed surface in such a way that\neach edge crosses at most a constant number of other edges. We deduce that\nthere is an almost linear-time algorithm for deciding FO properties in classes\nof graphs with locally bounded expansion.\n  More generally, we design a dynamic data structure for graphs belonging to a\nfixed class of graphs of bounded expansion. After a linear-time initialization\nthe data structure allows us to test an FO property in constant time, and the\ndata structure can be updated in constant time after addition/deletion of an\nedge, provided the list of possible edges to be added is known in advance and\ntheir simultaneous addition results in a graph in the class. All our results\nalso hold for relational structures and are based on the seminal result of\nNesetril and Ossona de Mendez on the existence of low tree-depth colorings. \n\n"}
{"id": "1109.5664", "contents": "Title: Deterministic Feature Selection for $k$-means Clustering Abstract: We study feature selection for $k$-means clustering. Although the literature\ncontains many methods with good empirical performance, algorithms with provable\ntheoretical behavior have only recently been developed. Unfortunately, these\nalgorithms are randomized and fail with, say, a constant probability. We\naddress this issue by presenting a deterministic feature selection algorithm\nfor k-means with theoretical guarantees. At the heart of our algorithm lies a\ndeterministic method for decompositions of the identity. \n\n"}
{"id": "1109.6131", "contents": "Title: On automatic infinite permutations Abstract: An infinite permutation $\\alpha$ is a linear ordering of $\\mathbb N$. We\nstudy properties of infinite permutations analogous to those of infinite words,\nand show some resemblances and some differences between permutations and words.\nIn this paper, we try to extend to permutations the notion of automaticity. As\nwe shall show, the standard definitions which are equivalent in the case of\nwords are not equivalent in the context of permutations. We investigate the\nrelationships between these definitions and prove that they constitute a chain\nof inclusions. We also construct and study an automaton generating the\nThue-Morse permutation. \n\n"}
{"id": "1109.6619", "contents": "Title: New Bounds for Edge-Cover by Random Walk Abstract: We show that the expected time for a random walk on a (multi-)graph $G$ to\ntraverse all $m$ edges of $G$, and return to its starting point, is at most\n$2m^2$; if each edge must be traversed in both directions, the bound is $3m^2$.\nBoth bounds are tight and may be applied to graphs with arbitrary edge lengths,\nwith implications for Brownian motion on a finite or infinite network of total\nedge-length $m$. \n\n"}
{"id": "1110.0728", "contents": "Title: Encoding and Constructing 1-Nested Phylogenetic Networks with Trinets Abstract: Phylogenetic networks are a generalization of phylogenetic trees that are\nused in biology to represent reticulate or non-treelike evolution. Recently,\nseveral algorithms have been developed which aim to construct phylogenetic\nnetworks from biological data using {\\em triplets}, i.e. binary phylogenetic\ntrees on 3-element subsets of a given set of species. However, a fundamental\nproblem with this approach is that the triplets displayed by a phylogenetic\nnetwork do not necessary uniquely determine or {\\em encode} the network. Here\nwe propose an alternative approach to encoding and constructing phylogenetic\nnetworks, which uses phylogenetic networks on 3-element subsets of a set, or\n{\\em trinets}, rather than triplets. More specifically, we show that for a\nspecial, well-studied type of phylogenetic network called a 1-nested network,\nthe trinets displayed by a 1-nested network always encode the network. We also\npresent an efficient algorithm for deciding whether a {\\em dense} set of\ntrinets (i.e. one that contains a trinet on every 3-element subset of a set)\ncan be displayed by a 1-nested network or not and, if so, constructs that\nnetwork. In addition, we discuss some potential new directions that this new\napproach opens up for constructing and comparing phylogenetic networks. \n\n"}
{"id": "1110.1894", "contents": "Title: On the Efficiency of Influence-and-Exploit Strategies for Revenue\n  Maximization under Positive Externalities Abstract: We study the problem of revenue maximization in the marketing model for\nsocial networks introduced by (Hartline, Mirrokni, Sundararajan, WWW '08). We\nrestrict our attention to the Uniform Additive Model and mostly focus on\nInfluence-and-Exploit (IE) marketing strategies. We obtain a comprehensive\ncollection of results on the efficiency and the approximability of IE\nstrategies, which also imply a significant improvement on the best known\napproximation ratios for revenue maximization. Specifically, we show that in\nthe Uniform Additive Model, both computing the optimal marketing strategy and\ncomputing the best IE strategy are $\\NP$-hard for undirected social networks.\nWe observe that allowing IE strategies to offer prices smaller than the myopic\nprice in the exploit step leads to a measurable improvement on their\nperformance. Thus, we show that the best IE strategy approximates the maximum\nrevenue within a factor of 0.911 for undirected and of roughly 0.553 for\ndirected networks. Moreover, we present a natural generalization of IE\nstrategies, with more than two pricing classes, and show that they approximate\nthe maximum revenue within a factor of roughly 0.7 for undirected and of\nroughly 0.35 for directed networks. Utilizing a connection between good IE\nstrategies and large cuts in the underlying social network, we obtain\npolynomial-time algorithms that approximate the revenue of the best IE strategy\nwithin a factor of roughly 0.9. Hence, we significantly improve on the best\nknown approximation ratio for revenue maximization to 0.8229 for undirected and\nto 0.5011 for directed networks (from 2/3 and 1/3, respectively, by Hartline et\nal.). \n\n"}
{"id": "1110.4882", "contents": "Title: Strongly polynomial algorithm for a class of minimum-cost flow problems\n  with separable convex objectives Abstract: A well-studied nonlinear extension of the minimum-cost flow problem is to\nminimize the objective $\\sum_{ij\\in E} C_{ij}(f_{ij})$ over feasible flows $f$,\nwhere on every arc $ij$ of the network, $C_{ij}$ is a convex function. We give\na strongly polynomial algorithm for the case when all $C_{ij}$'s are convex\nquadratic functions, settling an open problem raised e.g. by Hochbaum [1994].\nWe also give strongly polynomial algorithms for computing market equilibria in\nFisher markets with linear utilities and with spending constraint utilities,\nthat can be formulated in this framework (see Shmyrev [2009], Devanur et al.\n[2011]). For the latter class this resolves an open question raised by Vazirani\n[2010]. The running time is $O(m^4\\log m)$ for quadratic costs,\n$O(n^4+n^2(m+n\\log n)\\log n)$ for Fisher's markets with linear utilities and\n$O(mn^3 +m^2(m+n\\log n)\\log m)$ for spending constraint utilities.\n  All these algorithms are presented in a common framework that addresses the\ngeneral problem setting. Whereas it is impossible to give a strongly polynomial\nalgorithm for the general problem even in an approximate sense (see Hochbaum\n[1994]), we show that assuming the existence of certain black-box oracles, one\ncan give an algorithm using a strongly polynomial number of arithmetic\noperations and oracle calls only. The particular algorithms can be derived by\nimplementing these oracles in the respective settings. \n\n"}
{"id": "1110.5273", "contents": "Title: VLT observations of Fermi pulsars Abstract: Many energetic gamma-ray pulsars discovered by Fermi are promising candidates\nfor optical follow-ups. We present the results of the first deep optical\nobservations of the two Vela-like Fermi pulsars PSR J1357-6429 and PSR\nJ1048-5832 performed with the VLT. However, they have not been detected down to\nV~27 and V~27.6, respectively (3 sigma). These upper limits suggest an\nefficiency in converting spin-down power into optical luminosity < 7x10d-7 and\n<6x10d-6, respectively, lower than the Crab pulsar and, possibly, more\ncompatible with the spin-down age of these two pulsars. \n\n"}
{"id": "1111.0966", "contents": "Title: A Compact Degenerate Primary-Star Progenitor of SN 2011fe Abstract: While a white dwarf is, from a theoretical perspective, the most plausible\nprimary star in Type Ia supernova (SN Ia), many other candidates have not been\nformally ruled out. Shock energy deposited in the envelope of any exploding\nprimary contributes to the early SN brightness and, since this radiation energy\nis degraded by expansion after the explosion, the diffusive luminosity depends\non the initial primary radius. We present a new non-detection limit of the\nnearby SN Ia 2011fe, obtained what appears to be just 4 hours after explosion,\nallowing us to directly constrain the initial primary radius, R_p. Coupled with\nthe non-detection of a quiescent X-ray counterpart and the inferred synthesized\nNi mass, we show that R_p <~ 0.02 R_sun (a factor of 5 smaller than previously\ninferred), that the average density of the primary must be rho_p > 10,000 gm\ncm^{-3}, and that the effective temperature must be less than a few x 10^5 K.\nThis rules out hydrogen burning main sequence stars and giants. Constructing\nthe helium-burning main sequence and carbon-burning main sequence, we find such\nobjects are also excluded. By process of elimination, we find that only\ndegeneracy-supported compact objects---WDs and neutron stars---are viable as\nthe primary star of SN 2011fe. With few caveats, we also restrict the companion\n(secondary) star radius to R_ c <~ 0.1 R_sun, excluding Roche-Lobe overflowing\nred giant and main-sequence companions to high significance. \n\n"}
{"id": "1111.1382", "contents": "Title: Quasi-spherical accretion in X-ray pulsars Abstract: Quasi-spherical accretion in wind-fed X-ray pulsars is discussed. At X-ray\nluminosities <4 10^{36} erg/s, a hot convective shell is formed around the\nneutron star magnetosphere, and subsonic settling accretion regime sets in. In\nthis regime, accretion rate onto neutron star is determined by the ability of\nplasma to enter magnetosphere via Rayleigh-Taylor instability. A gas-dynamic\ntheory of settling accretion is constructed taking into account anisotropic\nturbulence. The angular momentum can be transferred through the quasi-static\nshell via large-scale convective motions initiating turbulence cascade. The\nangular velocity distribution in the shell is found depending on the turbulent\nviscosity prescription. Comparison with observations of long-period X-ray\nwind-fed pulsars shows that an almost iso-angular-momentum distribution is most\nlikely realized in their shells. The theory explains long-term spin-down in\nwind- fed accreting pulsars (e.g. GX 1+4) and properties of short-term\ntorque-luminosity correlations. The theory can be applied to slowly rotating\nlow-luminosity X-ray pulsars and non-stationary accretion phenomena observed in\nsome SFXTs. \n\n"}
{"id": "1111.2045", "contents": "Title: Relativistic Outflow Drives Gamma-Ray Emission in 3C345 Abstract: Aims: 3C345 was recently identified as a gamma-ray emitter, based on the\nfirst 20 months of Fermi-LAT data and optical monitoring. In this paper, a\nconnection between the gamma-ray and optical variability of 3C345 and\nproperties of its parsec-scale radio emission is investigated. Methods: The\nFermi-LAT data of 3C345, covering an energy range of 0.1-300 GeV, were combined\nwith 32 Very Long Baseline Array observations of the object made at 43.2 GHz in\nthe period of January 2008 - March 2010. Results: The VLBA data reveal\nmorphology and kinematics of the flow on scales of up to ~5 milliarcseconds\n(mas; deprojected linear distances of 380 parsecs). The brightness temperature,\nT_b(r), measured along the jet first decreases with distance proportional to\nr^-(0.95 +/-0.69) and later exhibits a break at ~0.3 mas, with T_b(r)\nproportional to r^-(4.11 +/-0.85) at larger separations. Variations of the\ngamma-ray, optical and parsec-scale radio emission show a similar long-term\ntrend persistent during the entire VLBA monitoring period. The gamma-ray and\noptical variations on shorter time scales are related to structural changes in\nthe jet on scales of ~0.3 mas (~23 parsecs, deprojected), with the gamma-ray\nand optical flares possibly related to the evolution of four distinct\nsuperluminal components identified in the flow. Conclusions: The observations\nindicate that both the quiescent and flaring components of the gamma-ray\nemission are produced in a region of the jet of ~23 pc in extent. This region\nmay mark the Compton-loss dominated zone of the flow and its large extent may\nfavor the synchrotron self-Compton mechanism for gamma-ray production in the\nrelativistic jet of the quasar 3C345. \n\n"}
{"id": "1111.2195", "contents": "Title: Representative sets and irrelevant vertices: New tools for kernelization Abstract: The existence of a polynomial kernel for Odd Cycle Transversal was a\nnotorious open problem in parameterized complexity. Recently, this was settled\nby the present authors (Kratsch and Wahlstr\\\"om, SODA 2012), with a randomized\npolynomial kernel for the problem, using matroid theory to encode flow\nquestions over a set of terminals in size polynomial in the number of\nterminals.\n  In the current work we further establish the usefulness of matroid theory to\nkernelization by showing applications of a result on representative sets due to\nLov\\'asz (Combinatorial Surveys 1977) and Marx (TCS 2009). We show how\nrepresentative sets can be used to give a polynomial kernel for the elusive\nAlmost 2-SAT problem. We further apply the representative sets tool to the\nproblem of finding irrelevant vertices in graph cut problems, i.e., vertices\nwhich can be made undeletable without affecting the status of the problem. This\ngives the first significant progress towards a polynomial kernel for the\nMultiway Cut problem; in particular, we get a kernel of O(k^{s+1}) vertices for\nMultiway Cut instances with at most s terminals. Both these kernelization\nresults have significant spin-off effects, producing the first polynomial\nkernels for a range of related problems.\n  More generally, the irrelevant vertex results have implications for covering\nmin-cuts in graphs. For a directed graph G=(V,E) and sets S, T \\subseteq V, let\nr be the size of a minimum (S,T)-vertex cut (which may intersect S and T). We\ncan find a set Z \\subseteq V of size O(|S|*|T|*r) which contains a minimum\n(A,B)-vertex cut for every A \\subseteq S, B \\subseteq T. Similarly, for an\nundirected graph G=(V,E), a set of terminals X \\subseteq V, and a constant s,\nwe can find a set Z\\subseteq V of size O(|X|^{s+1}) which contains a minimum\nmultiway cut for any partition of X into at most s pairwise disjoint subsets. \n\n"}
{"id": "1111.2346", "contents": "Title: The spectrum of the recycled PSR J0437-4715 and its white dwarf\n  companion Abstract: We present extensive spectral and photometric observations of the recycled\npulsar/white-dwarf binary containing PSR J0437-4715, which we analyzed together\nwith archival X-ray and gamma-ray data, to obtain the complete mid-infrared to\ngamma-ray spectrum. We first fit each part of the spectrum separately, and then\nthe whole multi-wavelength spectrum. We find that the optical-infrared part of\nthe spectrum is well fit by a cool white dwarf atmosphere model with pure\nhydrogen composition. The model atmosphere (Teff = 3950pm150K, log\ng=6.98pm0.15, R_WD=(1.9pm0.2)e9 cm) fits our spectral data remarkably well for\nthe known mass and distance (M=0.25pm0.02Msun, d=156.3pm1.3pc), yielding the\nwhite dwarf age (tau=6.0pm0.5Gyr). In the UV, we find a spectral shape\nconsistent with thermal emission from the bulk of the neutron star surface,\nwith surface temperature between 1.25e5 and 3.5e5K. The temperature of the\nthermal spectrum suggests that some heating mechanism operates throughout the\nlife of the neutron star. The temperature distribution on the neutron star\nsurface is non-uniform. In the X-rays, we confirm the presence of a high-energy\ntail which is consistent with a continuation of the cut-off power-law component\n(Gamma=1.56pm0.01, Ecut=1.1pm0.2GeV) that is seen in gamma-rays and perhaps\neven extends to the near-UV. \n\n"}
{"id": "1111.3097", "contents": "Title: The tile assembly model is intrinsically universal Abstract: We prove that the abstract Tile Assembly Model (aTAM) of nanoscale\nself-assembly is intrinsically universal. This means that there is a single\ntile assembly system U that, with proper initialization, simulates any tile\nassembly system T. The simulation is \"intrinsic\" in the sense that the\nself-assembly process carried out by U is exactly that carried out by T, with\neach tile of T represented by an m x m \"supertile\" of U. Our construction works\nfor the full aTAM at any temperature, and it faithfully simulates the\ndeterministic or nondeterministic behavior of each T.\n  Our construction succeeds by solving an analog of the cell differentiation\nproblem in developmental biology: Each supertile of U, starting with those in\nthe seed assembly, carries the \"genome\" of the simulated system T. At each\nlocation of a potential supertile in the self-assembly of U, a decision is made\nwhether and how to express this genome, i.e., whether to generate a supertile\nand, if so, which tile of T it will represent. This decision must be achieved\nusing asynchronous communication under incomplete information, but it achieves\nthe correct global outcome(s). \n\n"}
{"id": "1111.3964", "contents": "Title: Constraining Sources of Ultra High Energy Cosmic Rays Using High Energy\n  Observations with the Fermi Satellite Abstract: We analyze the conditions that enable acceleration of particles to ultra-high\nenergies, ~10^{20} eV (UHECRs). We show that broad band photon data recently\nprovided by WMAP, ISOCAM, Swift and Fermi satellites, yield constraints on the\nability of active galactic nuclei (AGN) to produce UHECRs. The high energy (MeV\n- GeV) photons are produced by Compton scattering of the emitted low energy\nphotons and the cosmic microwave background or extra-galactic background light.\nThe ratio of the luminosities at high and low photon energies can therefore be\nused as a probe of the physical conditions in the acceleration site. We find\nthat existing data excludes core regions of nearby radio-loud AGN as possible\nacceleration sites of UHECR protons. However, we show that giant radio lobes\nare not excluded. We apply our method to Cen A, and show that acceleration of\nprotons to ~10^{20} eV can only occur at distances >~ 100 kpc from the core. \n\n"}
{"id": "1111.5395", "contents": "Title: A graph theoretical Gauss-Bonnet-Chern Theorem Abstract: We prove a discrete Gauss-Bonnet-Chern theorem which states where summing the\ncurvature over all vertices of a finite graph G=(V,E) gives the Euler\ncharacteristic of G. \n\n"}
{"id": "1111.6551", "contents": "Title: A diagnostic test for determining the location of the GeV emission in\n  powerful blazars Abstract: An issue currently under debate in the literature is how far from the black\nhole is the Fermi-observed GeV emission of powerful blazars emitted. Here we\npresent a clear diagnostic tool for testing whether the GeV emission site is\nlocated within the sub-pc broad emission line (BLR) region or further out in\nthe few pc scale molecular torus (MT) environment. Within the BLR the\nscattering takes place at the onset of the Klein-Nishina regime, causing the\nelectron cooling time to become almost energy independent and as a result, the\nvariation of high-energy emission is expected to be achromatic. Contrarily, if\nthe emission site is located outside the BLR, the expected GeV variability is\nenergy-dependent and with amplitude increasing with energy. We demonstrate this\nusing time-dependent numerical simulations of blazar variability. \n\n"}
{"id": "1112.1558", "contents": "Title: Implications of the radio spectral index transition in LS I +61{\\deg}303\n  for its INTEGRAL data analysis Abstract: The TeV emitting X-ray binary LS I +61{\\deg}303 has two radio periodicities\nthat correspond to a large periodic outburst with the same period as the orbit,\n26.5 days (phase \\Phi), and a second periodicity of 1667 days (phase \\Theta),\nwhich modulates the orbital phase and amplitude of the large outburst. Analyses\nof the radio spectral index revealed in LS I +61{\\deg}303 the presence of the\ncritical transition typical for microquasars from optically thick emission\n(related to a steady jet) to an optically thin outburst (related to a transient\njet), and found that it occurs at \\Phi_{crit}, which is modulated by \\Theta:\n\\Phi_{crit}=f(\\Theta). We examine the possible implications of averaging high\nenergy data over large \\Theta and \\Phi intervals in the light of puzzling\npublished INTEGRAL results, which differ for different averaging of the data.\nIn microquasars, a simultaneous transition between two X-ray states occurs at\nthe switch from optically thick radio emission to an optically thin radio\noutburst, from the low/hard to the steep power-law state. Assuming that the\nsame transition occurs in LS I +61{\\deg}303 at \\Phi_{crit}, we can show\nqualitatively the effect of averaging high energy data on \\Theta, by analysing\nthe effects of averaging radio spectral index data across the same \\Theta\ninterval. We then model the two X-ray states, low/hard and steep power-law\nstate, and show quantitatively how their mixing can affect the results. When\nfolded over too large a \\Theta interval, spectral data from INTEGRAL can yield\na false picture of the emission behaviour of the source along the orbit because\nit may be mixing two spectral states. Furthermore, averaging the data along the\norbit may result in a dominant low/hard spectral state, which, for\ninsufficiently extended sampling, might appear without a cut-off. \n\n"}
{"id": "1112.2275", "contents": "Title: On Problems as Hard as CNFSAT Abstract: The field of exact exponential time algorithms for NP-hard problems has\nthrived over the last decade. While exhaustive search remains asymptotically\nthe fastest known algorithm for some basic problems, difficult and non-trivial\nexponential time algorithms have been found for a myriad of problems, including\nGraph Coloring, Hamiltonian Path, Dominating Set and 3-CNF-Sat. In some\ninstances, improving these algorithms further seems to be out of reach. The\nCNF-Sat problem is the canonical example of a problem for which the trivial\nexhaustive search algorithm runs in time O(2^n), where n is the number of\nvariables in the input formula. While there exist non-trivial algorithms for\nCNF-Sat that run in time o(2^n), no algorithm was able to improve the growth\nrate 2 to a smaller constant, and hence it is natural to conjecture that 2 is\nthe optimal growth rate. The strong exponential time hypothesis (SETH) by\nImpagliazzo and Paturi [JCSS 2001] goes a little bit further and asserts that,\nfor every epsilon<1, there is a (large) integer k such that that k-CNF-Sat\ncannot be computed in time 2^{epsilon n}.\n  In this paper, we show that, for every epsilon < 1, the problems Hitting Set,\nSet Splitting, and NAE-Sat cannot be computed in time O(2^{epsilon n}) unless\nSETH fails. Here n is the number of elements or variables in the input. For\nthese problems, we actually get an equivalence to SETH in a certain sense. We\nconjecture that SETH implies a similar statement for Set Cover, and prove that,\nunder this assumption, the fastest known algorithms for Steinter Tree,\nConnected Vertex Cover, Set Partitioning, and the pseudo-polynomial time\nalgorithm for Subset Sum cannot be significantly improved. Finally, we justify\nour assumption about the hardness of Set Cover by showing that the parity of\nthe number of set covers cannot be computed in time O(2^{epsilon n}) for any\nepsilon<1 unless SETH fails. \n\n"}
{"id": "1112.2865", "contents": "Title: The observation of Gamma Ray Bursts and Terrestrial Gamma-ray Flashes\n  with AGILE Abstract: Since its early phases of operation, the AGILE mission is successfully\nobserving Gamma Ray Bursts (GRBs) in the hard X-ray band with the SuperAGILE\nimager and in the MeV range with the Mini-Calorimeter. Up to now, three firm\nGRB detections were obtained above 25 MeV and some bursts were detected with\nlower statistical confidence in the same energy band. When a GRB is localized,\neither by SuperAGILE or Swift/BAT or INTEGRAL/IBIS or Fermi/GBM or IPN, inside\nthe field of view of the Gamma Ray Imager of AGILE, a detection is searched for\nin the gamma ray band or an upper limit is provided. A promising result of\nAGILE is the detection of very short gamma ray transients, a few ms in duration\nand possibly identified with Terrestrial Gamma-ray Flashes. In this paper we\nshow the current status of the observation of Gamma Ray Bursts and Terrestrial\nGamma-ray Flashes with AGILE. \n\n"}
{"id": "1112.3337", "contents": "Title: Search by quantum walks on two-dimensional grid without amplitude\n  amplification Abstract: We study search by quantum walk on a finite two dimensional grid. The\nalgorithm of Ambainis, Kempe, Rivosh (quant-ph/0402107) takes O(\\sqrt{N log N})\nsteps and finds a marked location with probability O(1/log N) for grid of size\n\\sqrt{N} * \\sqrt{N}. This probability is small, thus amplitude amplification is\nneeded to achieve \\Theta(1) success probability. The amplitude amplification\nadds an additional O(\\sqrt{log N}) factor to the number of steps, making it\nO(\\sqrt{N} log N).\n  In this paper, we show that despite a small probability to find a marked\nlocation, the probability to be within an O(\\sqrt{N}) neighbourhood (at an\nO(\\sqrt[4]{N}) distance) of the marked location is \\Theta(1). This allows to\nskip amplitude amplification step and leads to an O(\\sqrt{log N}) speed-up.\n  We describe the results of numerical experiments supporting this idea, and we\nprove this fact analytically. \n\n"}
{"id": "1112.3351", "contents": "Title: Gravitational Waves from Quasicircular Extreme Mass-Ratio Inspirals as\n  Probes of Scalar-Tensor Theories Abstract: A stellar-mass compact object spiraling into a supermassive black hole, an\nextreme-mass-ratio inspiral (EMRI), is one of the targets of future\ngravitational-wave detectors and it offers a unique opportunity to test General\nRelativity (GR) in the strong-field. We study whether generic scalar-tensor\n(ST) theories can be further constrained with EMRIs. We show that in the EMRI\nlimit, all such theories universally reduce to massive or massless Brans-Dicke\ntheory and that black holes do not emit dipolar radiation to all orders in\npost-Newtonian (PN) theory. For massless theories, we calculate the scalar\nenergy flux in the Teukolsky formalism to all orders in PN theory and fit it to\na high-order PN expansion. We derive the PN ST corrections to the Fourier\ntransform of the gravitational wave response and map it to the parameterized\npost-Einsteinian framework. We use the effective-one-body framework adapted to\nEMRIs to calculate the ST modifications to the gravitational waveform. We find\nthat such corrections are smaller than those induced in the early inspiral of\ncomparable-mass binaries, leading to projected bounds on the coupling that are\nworse than current Solar System ones. Brans-Dicke theory modifies the\nweak-field, with deviations in the energy flux that are largest at small\nvelocities. For massive theories, superradiance can lead to resonances in the\nscalar energy flux that can lead to floating orbits outside the innermost\nstable circular orbit and that last until the supermassive black hole loses\nenough mass and spin-angular momentum. If such floating orbits occur in the\nfrequency band of LISA, they would lead to a large dephasing (~1e6 rads),\npreventing detection with GR templates. A detection that is consistent with GR\nwould then rule out floating resonances at frequencies lower than the lowest\nobserved frequency, allowing for the strongest constraints yet on massive ST\ntheories. \n\n"}
{"id": "1112.3506", "contents": "Title: Max-Cut Parameterized Above the Edwards-Erd\\H{o}s Bound Abstract: We study the boundary of tractability for the Max-Cut problem in graphs. Our\nmain result shows that Max-Cut above the Edwards-Erd\\H{o}s bound is\nfixed-parameter tractable: we give an algorithm that for any connected graph\nwith n vertices and m edges finds a cut of size m/2 + (n-1)/4 + k in time\n2^O(k)n^4, or decides that no such cut exists. This answers a long-standing\nopen question from parameterized complexity that has been posed several times\nover the past 15 years. Our algorithm is asymptotically optimal, under the\nExponential Time Hypothesis, and is strengthened by a polynomial-time\ncomputable kernel of polynomial size. \n\n"}
{"id": "1112.5749", "contents": "Title: On the Dimension and Euler characteristic of random graphs Abstract: The inductive dimension dim(G) of a finite undirected graph G=(V,E) is a\nrational number defined inductively as 1 plus the arithmetic mean of the\ndimensions of the unit spheres dim(S(x)) at vertices x primed by the\nrequirement that the empty graph has dimension -1. We look at the distribution\nof the random variable \"dim\" on the Erdos-Renyi probability space G(n,p), where\neach of the n(n-1)/2 edges appears independently with probability p. We show\nhere that the average dimension E[dim] is a computable polynomial of degree\nn(n-1)/2 in p. The explicit formulas allow experimentally to explore limiting\nlaws for the dimension of large graphs. We also study the expectation E[X] of\nthe Euler characteristic X, considered as a random variable on G(n,p). We look\nexperimentally at the statistics of curvature K(v) and local dimension dim(v) =\n1+dim(S(v)) which satisfy the Gauss-Bonnet formula X(G) = sum K(v) and by\ndefinition dim(G) = sum dim(v)/|V|. We also look at the signature functions\nf(p)=E[dim], g(p)=E[X] and matrix values functions A(p) = Cov[{dim(v),dim(w)],\nB(p) = Cov[K(v),K(w)] on the probability space G(p) of all subgraphs of a host\ngraph G=(V,E) with the same vertex set V, where each edge is turned on with\nprobability p. If G is the complete graph or a union of cyclic graphs with have\nexplicit formulas for the signature polynomials f and g. \n\n"}
{"id": "1201.0253", "contents": "Title: A Lower Bound for Estimating High Moments of a Data Stream Abstract: We show an improved lower bound for the Fp estimation problem in a data\nstream setting for p>2. A data stream is a sequence of items from the domain\n[n] with possible repetitions. The frequency vector x is an n-dimensional\nnon-negative integer vector x such that x(i) is the number of occurrences of i\nin the sequence. Given an accuracy parameter Omega(n^{-1/p}) < \\epsilon < 1,\nthe problem of estimating Fp is to estimate \\norm{x}_p^p = \\sum_{i \\in [n]}\n\\abs{x(i)}^p correctly to within a relative accuracy of 1\\pm \\epsilon with high\nconstant probability in an online fashion and using as little space as\npossible. The current space lower bound for this problem is Omega(n^{1-2/p}\n\\epsilon^{-2/p}+ n^{1-2/p}\\epsilon^{-4/p}/ \\log^{O(1)}(n)+ (\\epsilon^{-2} +\n\\log (n))). The first term in the lower bound expression was proved in\n\\cite{B-YJKS:stoc02,cks:ccc03}, the second in \\cite{wz:arxiv11} and the third\nin \\cite{wood:soda04}. In this note, we show an Omega(p^2 n^{1-2/p}\n\\epsilon^{-2}/\\log (n)) bits space bound, for Omega(pn^{-1/p}) \\le \\epsilon \\le\n1/10. \n\n"}
{"id": "1201.0749", "contents": "Title: There is no 16-Clue Sudoku: Solving the Sudoku Minimum Number of Clues\n  Problem Abstract: The sudoku minimum number of clues problem is the following question: what is\nthe smallest number of clues that a sudoku puzzle can have? For several years\nit had been conjectured that the answer is 17. We have performed an exhaustive\ncomputer search for 16-clue sudoku puzzles, and did not find any, thus proving\nthat the answer is indeed 17. In this article we describe our method and the\nactual search. As a part of this project we developed a novel way for\nenumerating hitting sets. The hitting set problem is computationally hard; it\nis one of Karp's 21 classic NP-complete problems. A standard backtracking\nalgorithm for finding hitting sets would not be fast enough to search for a\n16-clue sudoku puzzle exhaustively, even at today's supercomputer speeds. To\nmake an exhaustive search possible, we designed an algorithm that allowed us to\nefficiently enumerate hitting sets of a suitable size. \n\n"}
{"id": "1201.0856", "contents": "Title: Complexity Classification in Infinite-Domain Constraint Satisfaction Abstract: A constraint satisfaction problem (CSP) is a computational problem where the\ninput consists of a finite set of variables and a finite set of constraints,\nand where the task is to decide whether there exists a satisfying assignment of\nvalues to the variables. Depending on the type of constraints that we allow in\nthe input, a CSP might be tractable, or computationally hard. In recent years,\ngeneral criteria have been discovered that imply that a CSP is polynomial-time\ntractable, or that it is NP-hard. Finite-domain CSPs have become a major common\nresearch focus of graph theory, artificial intelligence, and finite model\ntheory. It turned out that the key questions for complexity classification of\nCSPs are closely linked to central questions in universal algebra.\n  This thesis studies CSPs where the variables can take values from an infinite\ndomain. This generalization enhances dramatically the range of computational\nproblems that can be modeled as a CSP. Many problems from areas that have so\nfar seen no interaction with constraint satisfaction theory can be formulated\nusing infinite domains, e.g. problems from temporal and spatial reasoning,\nphylogenetic reconstruction, and operations research.\n  It turns out that the universal-algebraic approach can also be applied to\nstudy large classes of infinite-domain CSPs, yielding elegant complexity\nclassification results. A new tool in this thesis that becomes relevant\nparticularly for infinite domains is Ramsey theory. We demonstrate the\nfeasibility of our approach with two complete complexity classification\nresults: one on CSPs in temporal reasoning, the other on a generalization of\nSchaefer's theorem for propositional logic to logic over graphs. We also study\nthe limits of complexity classification, and present classes of computational\nproblems provably do not exhibit a complexity dichotomy into hard and easy\nproblems. \n\n"}
{"id": "1201.2780", "contents": "Title: Linear Kernels on Graphs Excluding Topological Minors Abstract: We show that problems which have finite integer index and satisfy a\nrequirement we call treewidth-bounding admit linear kernels on the class of\n$H$-topological-minor free graphs, for an arbitrary fixed graph $H$. This\nbuilds on earlier results by Fomin et al.\\ on linear kernels for $H$-minor-free\ngraphs and by Bodlaender et al.\\ on graphs of bounded genus. Our framework\nencompasses several problems, the prominent ones being Chordal Vertex Deletion,\nFeedback Vertex Set and Edge Dominating Set. \n\n"}
{"id": "1202.0348", "contents": "Title: Radiative Models of Sagittarius A* and M87 from Relativistic MHD\n  Simulations Abstract: Ongoing millimeter VLBI observations with the Event Horizon Telescope allow\nunprecedented study of the innermost portion of black hole accretion flows.\nInterpreting the observations requires relativistic, time-dependent physical\nmodeling. We discuss the comparison of radiative transfer calculations from\ngeneral relativistic MHD simulations of Sagittarius A* and M87 with current and\nfuture mm-VLBI observations. This comparison allows estimates of the viewing\ngeometry and physical conditions of the Sgr A* accretion flow. The viewing\ngeometry for M87 is already constrained from observations of its large-scale\njet, but, unlike Sgr A*, there is no consensus for its millimeter emission\ngeometry or electron population. Despite this uncertainty, as long as the\nemission region is compact, robust predictions for the size of its jet\nlaunching region can be made. For both sources, the black hole shadow may be\ndetected with future observations including ALMA and/or the LMT, which would\nconstitute the first direct evidence for a black hole event horizon. \n\n"}
{"id": "1202.1569", "contents": "Title: Nonrepetitive Colourings of Planar Graphs with $O(\\log n)$ Colours Abstract: A vertex colouring of a graph is \\emph{nonrepetitive} if there is no path for\nwhich the first half of the path is assigned the same sequence of colours as\nthe second half. The \\emph{nonrepetitive chromatic number} of a graph $G$ is\nthe minimum integer $k$ such that $G$ has a nonrepetitive $k$-colouring.\nWhether planar graphs have bounded nonrepetitive chromatic number is one of the\nmost important open problems in the field. Despite this, the best known upper\nbound is $O(\\sqrt{n})$ for $n$-vertex planar graphs. We prove a $O(\\log n)$\nupper bound. \n\n"}
{"id": "1202.1866", "contents": "Title: Long-term monitoring of the high-energy gamma-ray emission from LS I\n  +61{\\deg} 303 and LS 5039 Abstract: The Fermi Large Area Telescope (LAT) reported the first definitive GeV\ndetections of the binaries LS I +61\\degree 303 and LS 5039 in the first year\nafter its launch in June, 2008. These detections were unambiguous as a\nconsequence of the reduced positional uncertainty and the detection of\nmodulated gamma-ray emission on the corresponding orbital periods. An analysis\nof new data from the LAT, comprising 30 months of observations, identifies a\nchange in the gamma-ray behavior of LS I +61\\degree 303. An increase in flux is\ndetected in March 2009 and a steady decline in the orbital flux modulation is\nobserved. Significant emission up to 30GeV is detected by the LAT; prior\ndatasets led to upper limits only. Contemporaneous TeV observations no longer\ndetected the source, or found it -in one orbit- close to periastron, far from\nthe phases at which the source previously appeared at TeV energies. The\ndetailed numerical simulations and models that exist within the literature do\nnot predict or explain many of these features now observed at GeV and TeV\nenergies. New ideas and models are needed to fully explain and understand this\nbehavior. A detailed phase-resolved analysis of the spectral characterization\nof LS I +61\\degree 303 in the GeV regime ascribes a power law with an\nexponential cutoff spectrum along each analyzed portion of the system's orbit.\nThe on-source exposure of LS 5039 is also substantially increased with respect\nto our prior publication. In this case, whereas the general gamma-ray\nproperties remain consistent, the increased statistics of the current dataset\nallows for a deeper investigation of its orbital and spectral evolution. \n\n"}
{"id": "1202.2910", "contents": "Title: Revolutionaries and spies: Spy-good and spy-bad graphs Abstract: We study a game on a graph $G$ played by $r$ {\\it revolutionaries} and $s$\n{\\it spies}. Initially, revolutionaries and then spies occupy vertices. In each\nsubsequent round, each revolutionary may move to a neighboring vertex or not\nmove, and then each spy has the same option. The revolutionaries win if $m$ of\nthem meet at some vertex having no spy (at the end of a round); the spies win\nif they can avoid this forever.\n  Let $\\sigma(G,m,r)$ denote the minimum number of spies needed to win. To\navoid degenerate cases, assume $|V(G)|\\ge r-m+1\\ge\\floor{r/m}\\ge 1$. The easy\nbounds are then $\\floor{r/m}\\le \\sigma(G,m,r)\\le r-m+1$. We prove that the\nlower bound is sharp when $G$ has a rooted spanning tree $T$ such that every\nedge of $G$ not in $T$ joins two vertices having the same parent in $T$. As a\nconsequence, $\\sigma(G,m,r)\\le\\gamma(G)\\floor{r/m}$, where $\\gamma(G)$ is the\ndomination number; this bound is nearly sharp when $\\gamma(G)\\le m$.\n  For the random graph with constant edge-probability $p$, we obtain constants\n$c$ and $c'$ (depending on $m$ and $p$) such that $\\sigma(G,m,r)$ is near the\ntrivial upper bound when $r<c\\ln n$ and at most $c'$ times the trivial lower\nbound when $r>c'\\ln n$. For the hypercube $Q_d$ with $d\\ge r$, we have\n$\\sigma(G,m,r)=r-m+1$ when $m=2$, and for $m\\ge 3$ at least $r-39m$ spies are\nneeded.\n  For complete $k$-partite graphs with partite sets of size at least $2r$, the\nleading term in $\\sigma(G,m,r)$ is approximately $\\frac{k}{k-1}\\frac{r}{m}$\nwhen $k\\ge m$. For $k=2$, we have\n$\\sigma(G,2,r)=\\bigl\\lceil{\\frac{\\floor{7r/2}-3}5}\\bigr\\rceil$ and\n$\\sigma(G,3,r)=\\floor{r/2}$, and in general $\\frac{3r}{2m}-3\\le\n\\sigma(G,m,r)\\le\\frac{(1+1/\\sqrt3)r}{m}$. \n\n"}
{"id": "1202.4331", "contents": "Title: Strong Backdoors to Nested Satisfiability Abstract: Knuth (1990) introduced the class of nested formulas and showed that their\nsatisfiability can be decided in polynomial time. We show that, parameterized\nby the size of a smallest strong backdoor set to the target class of nested\nformulas, checking the satisfiability of any CNF formula is fixed-parameter\ntractable. Thus, for any k>0, the satisfiability problem can be solved in\npolynomial time for any formula F for which there exists a variable set B of\nsize at most k such that for every truth assignment t to B, the formula F[t] is\nnested; moreover, the degree of the polynomial is independent of k.\n  Our algorithm uses the grid-minor theorem of Robertson and Seymour (1986) to\neither find that the incidence graph of the formula has bounded treewidth - a\ncase that is solved using model checking for monadic second order logic - or to\nfind many vertex-disjoint obstructions in the incidence graph. For the latter\ncase, new combinatorial arguments are used to find a small backdoor set.\nCombining both cases leads to an approximation algorithm producing a strong\nbackdoor set whose size is upper bounded by a function of the optimum. Going\nthrough all assignments to this set of variables and using Knuth's algorithm,\nthe satisfiability of the input formula is decided. \n\n"}
{"id": "1202.4970", "contents": "Title: A polynomial time approximation scheme for computing the supremum of\n  Gaussian processes Abstract: We give a polynomial time approximation scheme (PTAS) for computing the\nsupremum of a Gaussian process. That is, given a finite set of vectors\n$V\\subseteq\\mathbb{R}^d$, we compute a $(1+\\varepsilon)$-factor approximation\nto $\\mathop {\\mathbb{E}}_{X\\leftarrow\\mathcal{N}^d}[\\sup_{v\\in V}|\\langle\nv,X\\rangle|]$ deterministically in time $\\operatorname\n{poly}(d)\\cdot|V|^{O_{\\varepsilon}(1)}$. Previously, only a constant factor\ndeterministic polynomial time approximation algorithm was known due to the work\nof Ding, Lee and Peres [Ann. of Math. (2) 175 (2012) 1409-1471]. This answers\nan open question of Lee (2010) and Ding [Ann. Probab. 42 (2014) 464-496]. The\nstudy of supremum of Gaussian processes is of considerable importance in\nprobability with applications in functional analysis, convex geometry, and in\nlight of the recent breakthrough work of Ding, Lee and Peres [Ann. of Math. (2)\n175 (2012) 1409-1471], to random walks on finite graphs. As such our result\ncould be of use elsewhere. In particular, combining with the work of Ding [Ann.\nProbab. 42 (2014) 464-496], our result yields a PTAS for computing the cover\ntime of bounded-degree graphs. Previously, such algorithms were known only for\ntrees. Along the way, we also give an explicit oblivious estimator for\nsemi-norms in Gaussian space with optimal query complexity. Our algorithm and\nits analysis are elementary in nature, using two classical comparison\ninequalities, Slepian's lemma and Kanter's lemma. \n\n"}
{"id": "1202.5749", "contents": "Title: Fixed-parameter tractability of multicut in directed acyclic graphs Abstract: The MULTICUT problem, given a graph G, a set of terminal pairs T={(s_i,t_i) |\n1 <= i <= r} and an integer p, asks whether one can find a cutset consisting of\nat most p non-terminal vertices that separates all the terminal pairs, i.e.,\nafter removing the cutset, t_i is not reachable from s_i for each 1 <= i <= r.\nThe fixed-parameter tractability of MULTICUT in undirected graphs,\nparameterized by the size of the cutset only, has been recently proven by Marx\nand Razgon (STOC'11) and, independently, by Bousquet et al. (STOC'11), after\nresisting attacks as a long-standing open problem. In this paper we prove that\nMULTICUT is fixed-parameter tractable on directed acyclic graphs, when\nparameterized both by the size of the cutset and the number of terminal pairs.\nWe complement this result by showing that this is implausible for\nparameterization by the size of the cutset only, as this version of the problem\nremains W[1]-hard. \n\n"}
{"id": "1203.0833", "contents": "Title: Faster Parameterized Algorithms using Linear Programming Abstract: We investigate the parameterized complexity of Vertex Cover parameterized by\nthe difference between the size of the optimal solution and the value of the\nlinear programming (LP) relaxation of the problem. By carefully analyzing the\nchange in the LP value in the branching steps, we argue that combining\npreviously known preprocessing rules with the most straightforward branching\nalgorithm yields an $O^*((2.618)^k)$ algorithm for the problem. Here $k$ is the\nexcess of the vertex cover size over the LP optimum, and we write $O^*(f(k))$\nfor a time complexity of the form $O(f(k)n^{O(1)})$, where $f (k)$ grows\nexponentially with $k$. We proceed to show that a more sophisticated branching\nalgorithm achieves a runtime of $O^*(2.3146^k)$.\n  Following this, using known and new reductions, we give $O^*(2.3146^k)$\nalgorithms for the parameterized versions of Above Guarantee Vertex Cover, Odd\nCycle Transversal, Split Vertex Deletion and Almost 2-SAT, and an\n$O^*(1.5214^k)$ algorithm for Ko\\\"nig Vertex Deletion, Vertex Cover Param by\nOCT and Vertex Cover Param by KVD. These algorithms significantly improve the\nbest known bounds for these problems. The most notable improvement is the new\nbound for Odd Cycle Transversal - this is the first algorithm which beats the\ndependence on $k$ of the seminal $O^*(3^k)$ algorithm of Reed, Smith and Vetta.\nFinally, using our algorithm, we obtain a kernel for the standard\nparameterization of Vertex Cover with at most $2k - c \\log k$ vertices. Our\nkernel is simpler than previously known kernels achieving the same size bound. \n\n"}
{"id": "1203.1057", "contents": "Title: Constraints on the ICM velocity power spectrum from the X-ray lines\n  width and shift Abstract: Future X-ray observations of galaxy clusters by high spectral resolution\nmissions will provide spatially resolved measurements of the energy and width\nfor the brightest emission lines in the intracluster medium (ICM) spectrum. In\nthis paper we discuss various ways of using these high resolution data to\nconstrain velocity power spectrum in galaxy clusters. We argue that variations\nof these quantities with the projected distance R in cool core clusters contain\nimportant information on the velocity field length scales in the ICM. The\neffective length $l_{\\rm eff}$ along the line of sight, which provides dominant\ncontribution to the line flux, increases with R, allowing one to probe the\namplitude of the velocity variations at different spatial scales. In\nparticular, we show that the width of the line as a function of R is closely\nlinked to the structure function of the 3D velocity field. Yet another easily\nobtainable proxy of the velocity field length scales is the ratio of the\namplitude of the projected velocity field (line energy) variations to the\ndispersion of the velocity along the line of sight (line width). Finally the\nprojected velocity field can be easily converted into 3D velocity field,\nespecially for clusters like Coma with an extended flat core in the surface\nbrightness. Under assumption of a homogeneous isotropic Gaussian 3D velocity\nfield we derived simple expressions relating the power spectrum of the 3D\nvelocity field (or structure function) and the observables. The uncertainties\nin the observables, caused by stochastic nature of the velocity field, are\nestimated by making multiple realizations of the random Gaussian velocity field\nand evaluating the scatter in observables. If large scale motions are present\nin the ICM these uncertainties may dominate the statistical errors of line\nwidth and shift measurements. \n\n"}
{"id": "1203.1607", "contents": "Title: Neutrino scattering and flavor transformation in supernovae Abstract: We argue that the small fraction of neutrinos that undergo direction-changing\nscattering outside of the neutrinosphere could have significant influence on\nneutrino flavor transformation in core-collapse supernova environments. We show\nthat the standard treatment for collective neutrino flavor transformation is\nadequate at late times, but could be inadequate in the crucial shock\nrevival/explosion epoch of core-collapse supernovae, where the potentials that\ngovern neutrino flavor evolution are affected by the scattered neutrinos.\nTaking account of this effect, and the way it couples to entropy and\ncomposition, will require a new paradigm in supernova modeling. \n\n"}
{"id": "1203.1754", "contents": "Title: Known algorithms for EDGE CLIQUE COVER are probably optimal Abstract: In the EDGE CLIQUE COVER (ECC) problem, given a graph G and an integer k, we\nask whether the edges of G can be covered with k complete subgraphs of G or,\nequivalently, whether G admits an intersection model on k-element universe.\nGramm et al. [JEA 2008] have shown a set of simple rules that reduce the number\nof vertices of G to 2^k, and no algorithm is known with significantly better\nrunning time bound than a brute-force search on this reduced instance. In this\npaper we show that the approach of Gramm et al. is essentially optimal: we\npresent a polynomial time algorithm that reduces an arbitrary 3-CNF-SAT formula\nwith n variables and m clauses to an equivalent ECC instance (G,k) with k =\nO(log n) and |V(G)| = O(n + m). Consequently, there is no 2^{2^{o(k)}}poly(n)\ntime algorithm for the ECC problem, unless the Exponential Time Hypothesis\nfails. To the best of our knowledge, these are the first results for a natural,\nfixed-parameter tractable problem, and proving that a doubly-exponential\ndependency on the parameter is essentially necessary. \n\n"}
{"id": "1203.2365", "contents": "Title: Increasing Forests and Quadrangulations via a Bijective Approach Abstract: In this work, we expose four bijections each allowing to increase (or\ndecrease) one parameter in either uniform random forests with a fixed number of\nedges and trees, or quadrangulations with a boundary having a fixed number of\nfaces and a fixed boundary length. In particular, this gives a way to sample a\nuniform quadrangulation with n + 1 faces from a uniform quadrangulation with n\nfaces or a uniform forest with n+1 edges and p trees from a uniform forest with\nn edges and p trees. \n\n"}
{"id": "1204.0136", "contents": "Title: Near-Optimal Algorithms for Online Matrix Prediction Abstract: In several online prediction problems of recent interest the comparison class\nis composed of matrices with bounded entries. For example, in the online\nmax-cut problem, the comparison class is matrices which represent cuts of a\ngiven graph and in online gambling the comparison class is matrices which\nrepresent permutations over n teams. Another important example is online\ncollaborative filtering in which a widely used comparison class is the set of\nmatrices with a small trace norm. In this paper we isolate a property of\nmatrices, which we call (beta,tau)-decomposability, and derive an efficient\nonline learning algorithm, that enjoys a regret bound of O*(sqrt(beta tau T))\nfor all problems in which the comparison class is composed of\n(beta,tau)-decomposable matrices. By analyzing the decomposability of cut\nmatrices, triangular matrices, and low trace-norm matrices, we derive near\noptimal regret bounds for online max-cut, online gambling, and online\ncollaborative filtering. In particular, this resolves (in the affirmative) an\nopen problem posed by Abernethy (2010); Kleinberg et al (2010). Finally, we\nderive lower bounds for the three problems and show that our upper bounds are\noptimal up to logarithmic factors. In particular, our lower bound for the\nonline collaborative filtering problem resolves another open problem posed by\nShamir and Srebro (2011). \n\n"}
{"id": "1204.1086", "contents": "Title: Sharp Bounds on Davenport-Schinzel Sequences of Every Order Abstract: One of the longest-standing open problems in computational geometry is to\nbound the lower envelope of $n$ univariate functions, each pair of which\ncrosses at most $s$ times, for some fixed $s$. This problem is known to be\nequivalent to bounding the length of an order-$s$ Davenport-Schinzel sequence,\nnamely a sequence over an $n$-letter alphabet that avoids alternating\nsubsequences of the form $a \\cdots b \\cdots a \\cdots b \\cdots$ with length\n$s+2$. These sequences were introduced by Davenport and Schinzel in 1965 to\nmodel a certain problem in differential equations and have since been applied\nto bounding the running times of geometric algorithms, data structures, and the\ncombinatorial complexity of geometric arrangements.\n  Let $\\lambda_s(n)$ be the maximum length of an order-$s$ DS sequence over $n$\nletters. What is $\\lambda_s$ asymptotically? This question has been answered\nsatisfactorily (by Hart and Sharir, Agarwal, Sharir, and Shor, Klazar, and\nNivasch) when $s$ is even or $s\\le 3$. However, since the work of Agarwal,\nSharir, and Shor in the mid-1980s there has been a persistent gap in our\nunderstanding of the odd orders.\n  In this work we effectively close the problem by establishing sharp bounds on\nDavenport-Schinzel sequences of every order $s$. Our results reveal that,\ncontrary to one's intuition, $\\lambda_s(n)$ behaves essentially like\n$\\lambda_{s-1}(n)$ when $s$ is odd. This refutes conjectures due to Alon et al.\n(2008) and Nivasch (2010). \n\n"}
{"id": "1204.1111", "contents": "Title: Faster Algorithms for Rectangular Matrix Multiplication Abstract: Let {\\alpha} be the maximal value such that the product of an n x n^{\\alpha}\nmatrix by an n^{\\alpha} x n matrix can be computed with n^{2+o(1)} arithmetic\noperations. In this paper we show that \\alpha>0.30298, which improves the\nprevious record \\alpha>0.29462 by Coppersmith (Journal of Complexity, 1997).\nMore generally, we construct a new algorithm for multiplying an n x n^k matrix\nby an n^k x n matrix, for any value k\\neq 1. The complexity of this algorithm\nis better than all known algorithms for rectangular matrix multiplication. In\nthe case of square matrix multiplication (i.e., for k=1), we recover exactly\nthe complexity of the algorithm by Coppersmith and Winograd (Journal of\nSymbolic Computation, 1990).\n  These new upper bounds can be used to improve the time complexity of several\nknown algorithms that rely on rectangular matrix multiplication. For example,\nwe directly obtain a O(n^{2.5302})-time algorithm for the all-pairs shortest\npaths problem over directed graphs with small integer weights, improving over\nthe O(n^{2.575})-time algorithm by Zwick (JACM 2002), and also improve the time\ncomplexity of sparse square matrix multiplication. \n\n"}
{"id": "1204.2124", "contents": "Title: Finding vertex-surjective graph homomorphisms Abstract: The Surjective Homomorphism problem is to test whether a given graph G called\nthe guest graph allows a vertex-surjective homomorphism to some other given\ngraph H called the host graph. The bijective and injective homomorphism\nproblems can be formulated in terms of spanning subgraphs and subgraphs, and as\nsuch their computational complexity has been extensively studied. What about\nthe surjective variant? Because this problem is NP-complete in general, we\nrestrict the guest and the host graph to belong to graph classes G and H,\nrespectively. We determine to what extent a certain choice of G and H\ninfluences its computational complexity. We observe that the problem is\npolynomial-time solvable if H is the class of paths, whereas it is NP-complete\nif G is the class of paths. Moreover, we show that the problem is even\nNP-complete on many other elementary graph classes, namely linear forests,\nunions of complete graphs, cographs, proper interval graphs, split graphs and\ntrees of pathwidth at most 2. In contrast, we prove that the problem is\nfixed-parameter tractable in k if G is the class of trees and H is the class of\ntrees with at most k leaves, or if G and H are equal to the class of graphs\nwith vertex cover number at most k. \n\n"}
{"id": "1204.2131", "contents": "Title: On Thresholds for the Appearance of 2-cores in Mixed Hypergraphs Abstract: We study thresholds for the appearance of a 2-core in random hypergraphs that\nare a mixture of a constant number of random uniform hypergraphs each with a\nlinear number of edges but with different edge sizes. For the case of two\noverlapping hypergraphs we give a solution for the optimal (expected) number of\nedges of each size such that the 2-core threshold for the resulting mixed\nhypergraph is maximized. We show that for adequate edge sizes this threshold\nexceeds the maximum 2-core threshold for any random uniform hypergraph, which\ncan be used to improve the space utilization of several data structures that\nrely on this parameter. \n\n"}
{"id": "1204.3445", "contents": "Title: Spin-down age: the key to magnetic field decay Abstract: The properties of the spin-down age are investigated. Based on assumption\nabout a uniform magnetic field decay law we suggest a new method which allows\nus to shed light on magnetic field decay. This method is applied for following\nselection: isolated non-millisecond pulsars from the ATNF catalog are chosen.\nPulsars in the selection are with the spin-down ages from 4 \\cdot 10^4 to 2\n\\cdot 10^6 years. In order to avoid observational selection we take into\naccount only pulsars which are closer to the Sun than 10 kpc. For this\nselection we restore the uniform magnetic field decay law. It appears that the\nmagnetic field decays three times from 4 \\cdot 10^4 to 3.5 \\cdot 10^5 years.\nThis function is approximated by modified power-law. We also estimate the\nbirthrate of pulsars in our Galaxy and find that it should be about 2.9 pulsars\nper century. \n\n"}
{"id": "1204.3589", "contents": "Title: Exploring the relation between (sub-)millimeter radiation and gamma-ray\n  emission in blazars with Planck and Fermi Abstract: The coexistence of Planck and Fermi satellites in orbit has enabled the\nexploration of the connection between the (sub-)millimeter and gamma-ray\nemission in a large sample of blazars. We find that the gamma-ray emission and\nthe (sub-)mm luminosities are correlated over five orders of magnitude.\nHowever, this correlation is not significant at some frequency bands when\nsimultaneous observations are considered. The most significant statistical\ncorrelations, on the other hand, arise when observations are quasi-simultaneous\nwithin 2 months. Moreover, we find that sources with an approximate spectral\nturnover in the middle of the mm-wave regime are more likely to be strong\ngamma-ray emitters. These results suggest a physical relation between the newly\ninjected plasma components in the jet and the high levels of gamma-ray\nemission. \n\n"}
{"id": "1204.4703", "contents": "Title: Fermi-LAT Observation of Supernova Remnant S147 Abstract: We present an analysis of gamma-ray data obtained with the Large Area\nTelescope (LAT) onboard the Fermi Gamma-ray Space Telescope in the region\naround SNR S147 (G180.0-1.7). A spatially extended gamma-ray source detected in\nan energy range of 0.2--10 GeV is found to coincide with SNR S147. We confirm\nits spatial extension at >5sigma confidence level. The gamma-ray flux is (3.8\n\\pm 0.6) x 10^{-8} photons cm^{-2} s^{-1}, corresponding to a luminosity of 1.3\nx 10^{34} (d/1.3 kpc)^2 erg s^{-1} in this energy range. The gamma-ray emission\nexhibits a possible spatial correlation with prominent Halpha filaments of\nS147. There is no indication that the gamma-ray emission comes from the\nassociated pulsar PSR J0538+2817. The gamma-ray spectrum integrated over the\nremnant is likely dominated by the decay of neutral pi mesons produced through\nthe proton--proton collisions in the filaments. Reacceleration of pre-existing\nCRs and subsequent adiabatic compression in the filaments is sufficient to\nprovide the required energy density of high-energy protons. \n\n"}
{"id": "1204.6391", "contents": "Title: Extending partial representations of function graphs and permutation\n  graphs Abstract: Function graphs are graphs representable by intersections of continuous\nreal-valued functions on the interval [0,1] and are known to be exactly the\ncomplements of comparability graphs. As such they are recognizable in\npolynomial time. Function graphs generalize permutation graphs, which arise\nwhen all functions considered are linear.\n  We focus on the problem of extending partial representations, which\ngeneralizes the recognition problem. We observe that for permutation graphs an\neasy extension of Golumbic's comparability graph recognition algorithm can be\nexploited. This approach fails for function graphs. Nevertheless, we present a\npolynomial-time algorithm for extending a partial representation of a graph by\nfunctions defined on the entire interval [0,1] provided for some of the\nvertices. On the other hand, we show that if a partial representation consists\nof functions defined on subintervals of [0,1], then the problem of extending\nthis representation to functions on the entire interval [0,1] becomes\nNP-complete. \n\n"}
{"id": "1205.1271", "contents": "Title: Directed Subset Feedback Vertex Set is Fixed-Parameter Tractable Abstract: Given a graph $G$ and an integer $k$, the Feedback Vertex Set (FVS) problem\nasks if there is a vertex set $T$ of size at most $k$ that hits all cycles in\nthe graph. The fixed-parameter tractability status of FVS in directed graphs\nwas a long-standing open problem until Chen et al. (STOC '08) showed that it is\nFPT by giving a $4^{k}k!n^{O(1)}$ time algorithm. In the subset versions of\nthis problems, we are given an additional subset $S$ of vertices (resp., edges)\nand we want to hit all cycles passing through a vertex of $S$ (resp. an edge of\n$S$). Recently, the Subset Feedback Vertex Set in undirected graphs was shown\nto be FPT by Cygan et al. (ICALP '11) and independently by Kakimura et al.\n(SODA '12). We generalize the result of Chen et al. (STOC '08) by showing that\nSubset Feedback Vertex Set in directed graphs can be solved in time\n$2^{O(k^3)}n^{O(1)}$. By our result, we complete the picture for feedback\nvertex set problems and their subset versions in undirected and directed\ngraphs. Besides proving the fixed-parameter tractability of Directed Subset\nFeedback Vertex Set, we reformulate the random sampling of important separators\ntechnique in an abstract way that can be used for a general family of\ntransversal problems. Moreover, we modify the probability distribution used in\nthe technique to achieve better running time; in particular, this gives an\nimprovement from $2^{2^{O(k)}}$ to $2^{O(k^2)}$ in the parameter dependence of\nthe Directed Multiway Cut algorithm of Chitnis et al. (SODA '12). \n\n"}
{"id": "1205.2234", "contents": "Title: Approximation Algorithms for Semi-random Graph Partitioning Problems Abstract: In this paper, we propose and study a new semi-random model for graph\npartitioning problems. We believe that it captures many properties of\nreal--world instances. The model is more flexible than the semi-random model of\nFeige and Kilian and planted random model of Bui, Chaudhuri, Leighton and\nSipser.\n  We develop a general framework for solving semi-random instances and apply it\nto several problems of interest. We present constant factor bi-criteria\napproximation algorithms for semi-random instances of the Balanced Cut,\nMulticut, Min Uncut, Sparsest Cut and Small Set Expansion problems. We also\nshow how to almost recover the optimal solution if the instance satisfies an\nadditional expanding condition. Our algorithms work in a wider range of\nparameters than most algorithms for previously studied random and semi-random\nmodels.\n  Additionally, we study a new planted algebraic expander model and develop\nconstant factor bi-criteria approximation algorithms for graph partitioning\nproblems in this model. \n\n"}
{"id": "1205.3160", "contents": "Title: Violation of Chandrasekhar Mass Limit: The Exciting Potential of\n  Strongly Magnetized White Dwarfs Abstract: We consider a relativistic, degenerate, electron gas under the influence of a\nstrong magnetic field, which describes magnetized white dwarfs. Landau\nquantization changes the density of states available to the electrons, thus\nmodifying the underlying equation of state. In the presence of very strong\nmagnetic fields a maximum of either one, two or three Landau level(s) is/are\noccupied. We obtain the mass-radius relations for such white dwarfs and their\ndetailed investigation leads us to propose the existence of white dwarfs having\na mass ~2.3M_Sun, which overwhelmingly exceeds the Chandrasekhar mass limit. \n\n"}
{"id": "1205.3728", "contents": "Title: Parameterized Domination in Circle Graphs Abstract: A circle graph is the intersection graph of a set of chords in a circle. Keil\n[Discrete Applied Mathematics, 42(1):51-63, 1993] proved that Dominating Set,\nConnected Dominating Set, and Total Dominating Set are NP-complete in circle\ngraphs. To the best of our knowledge, nothing was known about the parameterized\ncomplexity of these problems in circle graphs. In this paper we prove the\nfollowing results, which contribute in this direction:\n  - Dominating Set, Independent Dominating Set, Connected Dominating Set, Total\nDominating Set, and Acyclic Dominating Set are W[1]-hard in circle graphs,\nparameterized by the size of the solution.\n  - Whereas both Connected Dominating Set and Acyclic Dominating Set are\nW[1]-hard in circle graphs, it turns out that Connected Acyclic Dominating Set\nis polynomial-time solvable in circle graphs.\n  - If T is a given tree, deciding whether a circle graph has a dominating set\nisomorphic to T is NP-complete when T is in the input, and FPT when\nparameterized by |V(T)|. We prove that the FPT algorithm is subexponential. \n\n"}
{"id": "1205.3751", "contents": "Title: Tests of the universality of free fall for strongly self-gravitating\n  bodies with radio pulsars Abstract: In this paper, we review tests of the strong equivalence principle (SEP)\nderived from pulsar-white dwarf data. The extreme difference in binding energy\nbetween both components and the precise measurement of the orbital motion\nprovided by pulsar timing allow the only current precision SEP tests for\nstrongly self-gravitating bodies. We start by highlighting why such tests are\nconceptually important. We then review previous work where limits on SEP\nviolation are obtained with an ensemble of wide binary systems with small\neccentricity orbits. Then we propose a new SEP violation test based on the\nmeasurement of the variation of the orbital eccentricity de/dt. This new method\nhas the following advantages: a) unlike previous methods it is not based on\nprobabilistic considerations, b) it can make a direct detection of SEP\nviolation, c) the measurement of de/dt is not contaminated by any known\nexternal effects, which implies that this SEP test is only restricted by the\nmeasurement precision of de/dt. In the final part of the review, we\nconceptually compare the SEP test with the test for dipolar radiation damping,\na phenomenon closely related to SEP violation, and speculate on future\nprospects by new types of tests in globular clusters and future triple systems. \n\n"}
{"id": "1205.6868", "contents": "Title: Baryon Loading of AGN Jets Mediated by Neutrons Abstract: Plasmas of geometrically thick, black hole (BH) accretion flows in active\ngalactic nuclei (AGNs) are generally collisionless for protons, and involve\nmagnetic field turbulence. Under such conditions a fraction of protons can be\naccelerated stochastically and create relativistic neutrons via nuclear\ncollisions. These neutrons can freely escape from the accretion flow and decay\ninto protons in dilute polar region above the rotating BH to form relativistic\njets. We calculate geometric efficiencies of the neutron energy and mass\ninjections into the polar region, and show that this process can deposit\nluminosity as high as L_j ~ 2e-3 dot{M} c^2 and mass loading dot{M}_j ~ 6e-4\ndot{M} for the case of the BH mass M ~ 1e8 M_sun, where dot{M} is mass\naccretion rate. The terminal Lorentz factors of the jets are Gamma ~ 3, and\nthey may explain the AGN jets having low luminosities. For higher luminosity\njets, which can be produced by additional energy inputs such as Poynting flux,\nthe neutron decay still can be a dominant mass loading process, leading to\ne.g., Gamma ~ 50 for L_{j,tot} ~ 3e-2 dot{M}c^2. \n\n"}
{"id": "1205.6871", "contents": "Title: The Neutron Star Mass-Radius Relation and the Equation of State of Dense\n  Matter Abstract: The equation of state (EOS) of dense matter has been a long-sought goal of\nnuclear physics. Equations of state generate unique mass versus radius (M-R)\nrelations for neutron stars, the ultra-dense remnants of stellar evolution. In\nthis work, we determine the neutron star mass-radius relation and, based on\nrecent observations of both transiently accreting and bursting sources, we show\nthat the radius of a 1.4 solar mass neutron star lies between 10.4 and 12.9 km,\nindependent of assumptions about the composition of the core. We show, for the\nfirst time, that these constraints remain valid upon removal from our sample of\nthe most extreme transient sources or of the entire set of bursting sources;\nour constraints also apply even if deconfined quark matter exists in the\nneutron star core. Our results significantly constrain the dense matter EOS and\nare, furthermore, consistent with constraints from both heavy-ion collisions\nand theoretical studies of neutron matter. We predict a relatively weak\ndependence of the symmetry energy on the density and a value for the neutron\nskin thickness of lead which is less than 0.20 fm, results that are testable in\nforthcoming experiments. \n\n"}
{"id": "1206.1698", "contents": "Title: Generating spherical multiquadrangulations by restricted vertex\n  splittings and the reducibility of equilibrium classes Abstract: A quadrangulation is a graph embedded on the sphere such that each face is\nbounded by a walk of length 4, parallel edges allowed. All quadrangulations can\nbe generated by a sequence of graph operations called vertex splitting,\nstarting from the path P_2 of length 2. We define the degree D of a splitting S\nand consider restricted splittings S_{i,j} with i <= D <= j. It is known that\nS_{2,3} generate all simple quadrangulations.\n  Here we investigate the cases S_{1,2}, S_{1,3}, S_{1,1}, S_{2,2}, S_{3,3}.\nFirst we show that the splittings S_{1,2} are exactly the monotone ones in the\nsense that the resulting graph contains the original as a subgraph. Then we\nshow that they define a set of nontrivial ancestors beyond P_2 and each\nquadrangulation has a unique ancestor.\n  Our results have a direct geometric interpretation in the context of\nmechanical equilibria of convex bodies. The topology of the equilibria\ncorresponds to a 2-coloured quadrangulation with independent set sizes s, u.\nThe numbers s, u identify the primary equilibrium class associated with the\nbody by V\\'arkonyi and Domokos. We show that both S_{1,1} and S_{2,2} generate\nall primary classes from a finite set of ancestors which is closely related to\ntheir geometric results.\n  If, beyond s and u, the full topology of the quadrangulation is considered,\nwe arrive at the more refined secondary equilibrium classes. As Domokos,\nL\\'angi and Szab\\'o showed recently, one can create the geometric counterparts\nof unrestricted splittings to generate all secondary classes. Our results show\nthat S_{1,2} can only generate a limited range of secondary classes from the\nsame ancestor. The geometric interpretation of the additional ancestors defined\nby monotone splittings shows that minimal polyhedra play a key role in this\nprocess. We also present computational results on the number of secondary\nclasses and multiquadrangulations. \n\n"}
{"id": "1206.2379", "contents": "Title: On the astrophysical robustness of neutron star merger r-process Abstract: In this study we explore the nucleosynthesis in the dynamic ejecta of compact\nbinary mergers. We are particularly interested in the question how sensitive\nthe resulting abundance patterns are to the parameters of the merging system.\nTherefore, we systematically investigate combinations of neutron star masses in\nthe range from 1.0 to 2.0 \\Msun and, for completeness, we compare the results\nwith those from two simulations of a neutron star black hole merger. The ejecta\nmasses vary by a factor of five for the studied systems, but all amounts are\n(within the uncertainties of the merger rates) compatible with being a major\nsource of cosmic r-process. The ejecta undergo a robust r-process\nnucleosynthesis which produces all the elements from the second to the third\npeak in close-to-solar ratios. Most strikingly, this r-process is extremely\nrobust, all 23 investigated binary systems yield practically identical\nabundance patterns. This is mainly the result of the ejecta being extremely\nneutron rich (\\ye $\\approx0.04$) and the r-process path meandering along the\nneutron drip line so that the abundances are determined entirely by nuclear\nrather than by astrophysical properties. This robustness together with the ease\nwith which both the second and third peak are reproduced make compact binary\nmergers the prime candidate for the source of the observed unique heavy\nr-process component. \n\n"}
{"id": "1206.2384", "contents": "Title: Bounding the fractional chromatic number of $K_\\Delta$-free graphs Abstract: King, Lu, and Peng recently proved that for $\\Delta\\geq 4$, any\n$K_\\Delta$-free graph with maximum degree $\\Delta$ has fractional chromatic\nnumber at most $\\Delta-\\tfrac{2}{67}$ unless it is isomorphic to $C_5\\boxtimes\nK_2$ or $C_8^2$. Using a different approach we give improved bounds for\n$\\Delta\\geq 6$ and pose several related conjectures. Our proof relies on a\nweighted local generalization of the fractional relaxation of Reed's $\\omega$,\n$\\Delta$, $\\chi$ conjecture. \n\n"}
{"id": "1206.3204", "contents": "Title: Improved Spectral-Norm Bounds for Clustering Abstract: Aiming to unify known results about clustering mixtures of distributions\nunder separation conditions, Kumar and Kannan[2010] introduced a deterministic\ncondition for clustering datasets. They showed that this single deterministic\ncondition encompasses many previously studied clustering assumptions. More\nspecifically, their proximity condition requires that in the target\n$k$-clustering, the projection of a point $x$ onto the line joining its cluster\ncenter $\\mu$ and some other center $\\mu'$, is a large additive factor closer to\n$\\mu$ than to $\\mu'$. This additive factor can be roughly described as $k$\ntimes the spectral norm of the matrix representing the differences between the\ngiven (known) dataset and the means of the (unknown) target clustering.\nClearly, the proximity condition implies center separation -- the distance\nbetween any two centers must be as large as the above mentioned bound.\n  In this paper we improve upon the work of Kumar and Kannan along several\naxes. First, we weaken the center separation bound by a factor of $\\sqrt{k}$,\nand secondly we weaken the proximity condition by a factor of $k$. Using these\nweaker bounds we still achieve the same guarantees when all points satisfy the\nproximity condition. We also achieve better guarantees when only\n$(1-\\epsilon)$-fraction of the points satisfy the weaker proximity condition.\nThe bulk of our analysis relies only on center separation under which one can\nproduce a clustering which (i) has low error, (ii) has low $k$-means cost, and\n(iii) has centers very close to the target centers.\n  Our improved separation condition allows us to match the results of the\nPlanted Partition Model of McSherry[2001], improve upon the results of\nOstrovsky et al[2006], and improve separation results for mixture of Gaussian\nmodels in a particular setting. \n\n"}
{"id": "1206.4135", "contents": "Title: On the origin of a highly-dispersed coherent radio burst Abstract: We discuss the possible source of a highly-dispersed radio transient\ndiscovered in the Parkes Multi-beam Pulsar Survey (PMPS). The pulse has a\ndispersion measure of $746\\mathrm{cm}^{-3}\\mathrm{pc}$, a peak flux density of\n400 mJy for the observed pulse width of 7.8 ms, and a flat spectrum across a\n288-MHz band centred on 1374 MHz. The flat spectrum suggests that the pulse did\nnot originate from a pulsar, but is consistent with radio-emitting magnetar\nspectra. The non-detection of subsequent bursts constrains any possible pulsar\nperiod to $\\gtrsim1$ s, and the pulse energy distribution to being much flatter\nthan typical giant pulse emitting pulsars. The burst is also consistent with\nthe radio signal theorised from an annihilating mini black hole. Extrapolating\nthe PMPS detection rate, provides a limit of\n$\\Omega_{BH}\\lesssim5\\times10^{-14}$ on the density of these objects. We\ninvestigate the consistency of these two scenarios, plus several other possible\nsolutions, as potential explanations to the origin of the pulse, as well as for\nanother transient with similar properties: the Lorimer Burst. \n\n"}
{"id": "1206.4912", "contents": "Title: Preprocessing Subgraph and Minor Problems: When Does a Small Vertex\n  Cover Help? Abstract: We prove a number of results around kernelization of problems parameterized\nby the size of a given vertex cover of the input graph. We provide three sets\nof simple general conditions characterizing problems admitting kernels of\npolynomial size. Our characterizations not only give generic explanations for\nthe existence of many known polynomial kernels for problems like q-Coloring,\nOdd Cycle Transversal, Chordal Deletion, Eta Transversal, or Long Path,\nparameterized by the size of a vertex cover, but also imply new polynomial\nkernels for problems like F-Minor-Free Deletion, which is to delete at most k\nvertices to obtain a graph with no minor from a fixed finite set F.\n  While our characterization captures many interesting problems, the\nkernelization complexity landscape of parameterizations by vertex cover is much\nmore involved. We demonstrate this by several results about induced subgraph\nand minor containment testing, which we find surprising. While it was known\nthat testing for an induced complete subgraph has no polynomial kernel unless\nNP is in coNP/poly, we show that the problem of testing if a graph contains a\ncomplete graph on t vertices as a minor admits a polynomial kernel. On the\nother hand, it was known that testing for a path on t vertices as a minor\nadmits a polynomial kernel, but we show that testing for containment of an\ninduced path on t vertices is unlikely to admit a polynomial kernel. \n\n"}
{"id": "1207.0672", "contents": "Title: Octants are Cover-Decomposable into Many Coverings Abstract: We prove that octants are cover-decomposable into multiple coverings, i.e.,\nfor any k there is an m(k) such that any m(k)-fold covering of any subset of\nthe space with a finite number of translates of a given octant can be\ndecomposed into k coverings. As a corollary, we obtain that any m(k)-fold\ncovering of any subset of the plane with a finite number of homothetic copies\nof a given triangle can be decomposed into k coverings. Previously only some\nweaker bounds were known for related problems. \n\n"}
{"id": "1207.1788", "contents": "Title: Improved Bounds for Online Preemptive Matching Abstract: When designing a preemptive online algorithm for the maximum matching\nproblem, we wish to maintain a valid matching M while edges of the underlying\ngraph are presented one after the other. When presented with an edge e, the\nalgorithm should decide whether to augment the matching M by adding e (in which\ncase e may be removed later on) or to keep M in its current form without adding\ne (in which case e is lost for good). The objective is to eventually hold a\nmatching M with maximum weight.\n  The main contribution of this paper is to establish new lower and upper\nbounds on the competitive ratio achievable by preemptive online algorithms:\n  1. We provide a lower bound of 1+ln 2~1.693 on the competitive ratio of any\nrandomized algorithm for the maximum cardinality matching problem, thus\nimproving on the currently best known bound of e/(e-1)~1.581 due to Karp,\nVazirani, and Vazirani [STOC'90].\n  2. We devise a randomized algorithm that achieves an expected competitive\nratio of 5.356 for maximum weight matching. This finding demonstrates the power\nof randomization in this context, showing how to beat the tight bound of 3\n+2\\sqrt{2}~5.828 for deterministic algorithms, obtained by combining the 5.828\nupper bound of McGregor [APPROX'05] and the recent 5.828 lower bound of\nVaradaraja [ICALP'11]. \n\n"}
{"id": "1207.1794", "contents": "Title: Design, Evaluation and Analysis of Combinatorial Optimization Heuristic\n  Algorithms Abstract: Combinatorial optimization is widely applied in a number of areas nowadays.\nUnfortunately, many combinatorial optimization problems are NP-hard which\nusually means that they are unsolvable in practice. However, it is often\nunnecessary to have an exact solution. In this case one may use heuristic\napproach to obtain a near-optimal solution in some reasonable time.\n  We focus on two combinatorial optimization problems, namely the Generalized\nTraveling Salesman Problem and the Multidimensional Assignment Problem. The\nfirst problem is an important generalization of the Traveling Salesman Problem;\nthe second one is a generalization of the Assignment Problem for an arbitrary\nnumber of dimensions. Both problems are NP-hard and have hosts of applications.\n  In this work, we discuss different aspects of heuristics design and\nevaluation. A broad spectrum of related subjects, covered in this research,\nincludes test bed generation and analysis, implementation and performance\nissues, local search neighborhoods and efficient exploration algorithms,\nmetaheuristics design and population sizing in memetic algorithm.\n  The most important results are obtained in the areas of local search and\nmemetic algorithms for the considered problems. In both cases we have\nsignificantly advanced the existing knowledge on the local search neighborhoods\nand algorithms by systematizing and improving the previous results. We have\nproposed a number of efficient heuristics which dominate the existing\nalgorithms in a wide range of time/quality requirements.\n  Several new approaches, introduced in our memetic algorithms, make them the\nstate-of-the-art metaheuristics for the corresponding problems. Population\nsizing is one of the most promising among these approaches; it is expected to\nbe applicable to virtually any memetic algorithm. \n\n"}
{"id": "1207.3807", "contents": "Title: Light Spanner and Monotone Tree Abstract: In approximation algorithm design, light spanners has applications in\ngraph-metric problems such as metric TSP (the traveling salesman problem). We\nhave developed an efficient algorithm for light spanners in bounded pathwidth\ngraphs, based on an intermediate data structure called monotone tree. In this\npaper, we extended the results to include bounded catwidth graphs. \n\n"}
{"id": "1207.4415", "contents": "Title: Online and quasi-online colorings of wedges and intervals Abstract: We consider proper online colorings of hypergraphs defined by geometric\nregions. We prove that there is an online coloring algorithm that colors $N$\nintervals of the real line using $\\Theta(\\log N/k)$ colors such that for every\npoint $p$, contained in at least $k$ intervals, not all the intervals\ncontaining $p$ have the same color. We also prove the corresponding result\nabout online coloring a family of wedges (quadrants) in the plane that are the\ntranslates of a given fixed wedge. These results contrast the results of the\nfirst and third author showing that in the quasi-online setting 12 colors are\nenough to color wedges (independent of $N$ and $k$). We also consider\nquasi-online coloring of intervals. In all cases we present efficient coloring\nalgorithms. \n\n"}
{"id": "1207.6175", "contents": "Title: A Bijection Between the Recurrent Configurations of a Hereditary\n  Chip-Firing Model and Spanning Trees Abstract: Hereditary chip-firing models generalize the Abelian sandpile model and the\ncluster firing model to an exponential family of games induced by covers of the\nvertex set. This generalization retains some desirable properties, e.g.\nstabilization is independent of firings chosen and each chip-firing equivalence\nclass contains a unique recurrent configuration. In this paper we present an\nexplicit bijection between the recurrent configurations of a hereditary\nchip-firing model on a graph and its spanning trees. \n\n"}
{"id": "1207.7134", "contents": "Title: Improved approximation algorithms for low-density instances of the\n  Minimum Entropy Set Cover Problem Abstract: We study the approximability of instances of the minimum entropy set cover\nproblem, parameterized by the average frequency of a random element in the\ncovering sets. We analyze an algorithm combining a greedy approach with another\none biased towards large sets. The algorithm is controled by the percentage of\nelements to which we apply the biased approach. The optimal parameter choice\nhas a phase transition around average density $e$ and leads to improved\napproximation guarantees when average element frequency is less than $e$. \n\n"}
{"id": "1208.0906", "contents": "Title: On the masses of OJ287 black holes Abstract: Two multifrequency campaigns were carried out on OJ287 in 2005: in April when\nit was in its pre-outburst state, and in November, during the main 12 yr cycle\noutburst. The wavelength coverage was from radio to X-rays. In the\noptical-to-UV range the differential spectrum between the observations has a\nbremsstrahlung spectral shape, consistent with gas at $3 \\times 10^{5}K$\ntemperature. Our result supports the hydrogen column density of the OJ287 host\ngalaxy of $\\sim9.3\\times 10^{20} cm^{-2}$, the average value found by Gosh &\nSoundararajaperumal. The $3 \\times 10^{5}K$ bremsstrahlung radiation was\npredicted in the binary black hole model of OJ287, and it arises from a hot\nbubble of gas which is torn off the accretion disc by the impact of the\nsecondary. As this radiation is not Doppler boosted, the brightness of the\noutburst provides an estimate for the mass of the secondary black hole,\n$\\sim1.4\\times10^{8}$ solar mass. In order to estimate the mass of the primary\nblack hole, we ask what is the minimum mass ratio in a binary system which\nallows the stability of the accretion disc. By using particle simulations, we\nfind that the ratio is $\\sim1.3\\times10^{2}$. This makes the minimum mass of\nthe primary $\\sim1.8\\times10^{10}$ solar mass, in agreement with the mass\ndetermined from the orbit solution, $1.84 \\times 10^{10}$ solar mass. With this\nmass value and the measured K-magnitude of the bulge of the host galaxy of\nOJ287, the system lies almost exactly on the previously established correlation\nin the black hole mass vs. K-magnitude diagramme. It supports the extension of\nthis correlation to brighter magnitudes and to more massive black holes than\nhas been done previously. \n\n"}
{"id": "1208.1565", "contents": "Title: Fuel Efficient Computation in Passive Self-Assembly Abstract: In this paper we show that passive self-assembly in the context of the tile\nself-assembly model is capable of performing fuel efficient, universal\ncomputation. The tile self-assembly model is a premiere model of self-assembly\nin which particles are modeled by four-sided squares with glue types assigned\nto each tile edge. The assembly process is driven by positive and negative\nforce interactions between glue types, allowing for tile assemblies floating in\nthe plane to combine and break apart over time. We refer to this type of\nassembly model as passive in that the constituent parts remain unchanged\nthroughout the assembly process regardless of their interactions. A\ncomputationally universal system is said to be fuel efficient if the number of\ntiles used up per computation step is bounded by a constant. Work within this\nmodel has shown how fuel guzzling tile systems can perform universal\ncomputation with only positive strength glue interactions. Recent work has\nintroduced space-efficient, fuel-guzzling universal computation with the\naddition of negative glue interactions and the use of a powerful non-diagonal\nclass of glue interactions. Other recent work has shown how to achieve fuel\nefficient computation within active tile self-assembly. In this paper we\nutilize negative interactions in the tile self-assembly model to achieve the\nfirst computationally universal passive tile self-assembly system that is both\nspace and fuel-efficient. In addition, we achieve this result using a limited\ndiagonal class of glue interactions. \n\n"}
{"id": "1208.1986", "contents": "Title: Evaluating Systematic Dependencies of Type Ia Supernovae: The Influence\n  of Central Density Abstract: We present a study exploring a systematic effect on the brightness of type Ia\nsupernovae using numerical models that assume the single-degenerate paradigm.\nOur investigation varied the central density of the progenitor white dwarf at\nflame ignition, and considered its impact on the explosion yield, particularly\nthe production and distribution of radioactive Ni-56, which powers the light\ncurve. We performed a suite of two-dimensional simulations with randomized\ninitial conditions, allowing us to characterize the statistical trends that we\npresent. The simulations indicate that production of Fe-group material is\nstatistically independent of progenitor central density, but the mass of stable\nFe-group isotopes is tightly correlated with central density, with a decrease\nin the production of Ni-56 at higher central densities. These results imply\nprogenitors with higher central densities produce dimmer events. We provide\ndetails of the post-explosion distribution of Ni-56 in the models, including\nthe lack of a consistent centrally-located deficit of Ni-56, which may be\ncompared to observed remnants. By performing a self-consistent extrapolation of\nour model yields and considering the main-sequence lifetime of the progenitor\nstar and the elapsed time between the formation of the white dwarf and the\nonset of accretion, we develop a brightness-age relation that improves our\nprediction of the expected trend for single degenerates and we compare this\nrelation with observations. \n\n"}
{"id": "1208.2294", "contents": "Title: Learning pseudo-Boolean k-DNF and Submodular Functions Abstract: We prove that any submodular function f: {0,1}^n -> {0,1,...,k} can be\nrepresented as a pseudo-Boolean 2k-DNF formula. Pseudo-Boolean DNFs are a\nnatural generalization of DNF representation for functions with integer range.\nEach term in such a formula has an associated integral constant. We show that\nan analog of Hastad's switching lemma holds for pseudo-Boolean k-DNFs if all\nconstants associated with the terms of the formula are bounded.\n  This allows us to generalize Mansour's PAC-learning algorithm for k-DNFs to\npseudo-Boolean k-DNFs, and hence gives a PAC-learning algorithm with membership\nqueries under the uniform distribution for submodular functions of the form\nf:{0,1}^n -> {0,1,...,k}. Our algorithm runs in time polynomial in n, k^{O(k\n\\log k / \\epsilon)}, 1/\\epsilon and log(1/\\delta) and works even in the\nagnostic setting. The line of previous work on learning submodular functions\n[Balcan, Harvey (STOC '11), Gupta, Hardt, Roth, Ullman (STOC '11), Cheraghchi,\nKlivans, Kothari, Lee (SODA '12)] implies only n^{O(k)} query complexity for\nlearning submodular functions in this setting, for fixed epsilon and delta.\n  Our learning algorithm implies a property tester for submodularity of\nfunctions f:{0,1}^n -> {0, ..., k} with query complexity polynomial in n for\nk=O((\\log n/ \\loglog n)^{1/2}) and constant proximity parameter \\epsilon. \n\n"}
{"id": "1208.2432", "contents": "Title: Pirates and Treasure Abstract: In this paper we introduce a new game; in this game there are two players who\nplay as rival pirate gangs. The goal is to gather more treasure than your\nrival. The game is played on a graph and a player gathers treasure by moving to\nan unvisited vertex. At the end of the game, the player with the most treasure\nwins. We will show that this game is NP-Hard, and we will also look at the\nstructure of this game under the disjunctive sum. We will show that there are\ncases where this game behaves like a normal play game, and cases where it\nbehaves like a mis`ere play game. We then leave an open problem about scoring\nplay games in general. \n\n"}
{"id": "1208.2763", "contents": "Title: Intrinsic Simulations between Stochastic Cellular Automata Abstract: The paper proposes a simple formalism for dealing with deterministic,\nnon-deterministic and stochastic cellular automata in a unifying and composable\nmanner. Armed with this formalism, we extend the notion of intrinsic simulation\nbetween deterministic cellular automata, to the non-deterministic and\nstochastic settings. We then provide explicit tools to prove or disprove the\nexistence of such a simulation between two stochastic cellular automata, even\nthough the intrinsic simulation relation is shown to be undecidable in\ndimension two and higher. The key result behind this is the caracterization of\nequality of stochastic global maps by the existence of a coupling between the\nrandom sources. We then prove that there is a universal non-deterministic\ncellular automaton, but no universal stochastic cellular automaton. Yet we\nprovide stochastic cellular automata achieving optimal partial universality. \n\n"}
{"id": "1208.2770", "contents": "Title: Strictly Temporally Periodic Points in Cellular Automata Abstract: We study the set of strictly periodic points in surjective cellular automata,\ni.e., the set of those configurations which are temporally periodic for a given\nautomaton but they not spatially periodic. This set turns out to be dense for\nalmost equicontinuous surjective cellular automata while it is empty for the\npositively expansive ones. In the class of additive cellular automata, the set\nof strictly periodic points can be either dense or empty. The latter happens if\nand only if the cellular automaton is topologically transitive. \n\n"}
{"id": "1209.0194", "contents": "Title: Counting Plane Graphs: Cross-Graph Charging Schemes Abstract: We study cross-graph charging schemes for graphs drawn in the plane. These\nare charging schemes where charge is moved across vertices of different graphs.\nSuch methods have been recently applied to obtain various properties of\ntriangulations that are embedded over a fixed set of points in the plane. We\nshow how this method can be generalized to obtain results for various other\ntypes of graphs that are embedded in the plane. Specifically, we obtain a new\nbound of $O^*(187.53^N)$ (where the $O^*()$ notation hides polynomial factors)\nfor the maximum number of crossing-free straight-edge graphs that can be\nembedded over any specific set of $N$ points in the plane (improving upon the\nprevious best upper bound $207.85^N$ in Hoffmann et al.). We also derive upper\nbounds for numbers of several other types of plane graphs (such as connected\nand bi-connected plane graphs), and obtain various bounds on expected\nvertex-degrees in graphs that are uniformly chosen from the set of all\ncrossing-free straight-edge graphs that can be embedded over a specific point\nset.\n  We then show how to apply the cross-graph charging-scheme method for graphs\nthat allow certain types of crossings. Specifically, we consider graphs with no\nset of $k$ pairwise-crossing edges (more commonly known as $k$-quasi-planar\ngraphs). For $k=3$ and $k=4$, we prove that, for any set $S$ of $N$ points in\nthe plane, the number of graphs that have a straight-edge $k$-quasi-planar\nembedding over $S$ is only exponential in $N$. \n\n"}
{"id": "1209.1597", "contents": "Title: Sensitivity and block sensitivity of nested canalyzing function Abstract: Based on a recent characterization of nested canalyzing function (NCF), we\nobtain the formula of the sensitivity of any NCF. Hence we find that any\nsensitivity of NCF is between $\\frac{n+1}{2}$ and $n$. Both lower and upper\nbounds are tight. We prove that the block sensitivity, hence the $l$-block\nsensitivity, is same to the sensitivity. It is well known that monotone\nfunction also has this property. We eventually find all the functions which are\nboth monotone and nested canalyzing (MNCF). The cardinality of all the MNCF is\nalso provided. \n\n"}
{"id": "1209.1945", "contents": "Title: Lense-Thirring Precession in Pleba\\'nski-Demia\\'nski spacetimes Abstract: An exact expression of Lense-Thirring precession rate is derived for\nnon-extremal and extremal Pleba\\'nski-Demia\\'nski spacetimes. This formula is\nused to find the exact Lense-Thirring precession rate in various axisymmetric\nspacetimes, like: Kerr, Kerr-Newman, Kerr-de Sitter etc. We also show, if the\nKerr parameter vanishes in Pleba\\'nski-Demia\\'nski(PD) spacetime, the\nLense-Thirring precession does not vanish due to the existence of NUT charge.\nTo derive the LT precession rate in extremal Pleba\\'nski-Demia\\'nski we first\nderive the general extremal condition for PD spacetimes. This general result\ncould be applied to get the extremal limit in any stationary and axisymmetric\nspacetimes. \n\n"}
{"id": "1209.2053", "contents": "Title: A method for localizing energy dissipation in blazars using Fermi\n  variability Abstract: The distance of the Fermi-detected blazar gamma-ray emission site from the\nsupermassive black hole is a matter of active debate. Here we present a method\nfor testing if the GeV emission of powerful blazars is produced within the\nsub-pc scale broad line region (BLR) or farther out in the pc-scale molecular\ntorus (MT) environment. If the GeV emission takes place within the BLR, the\ninverse Compton (IC) scattering of the BLR ultraviolet (UV) seed photons that\nproduces the gamma-rays takes place at the onset of the Klein-Nishina regime.\nThis causes the electron cooling time to become practically energy independent\nand the variation of the gamma-ray emission to be almost achromatic. If on the\nother hand the gamma-ray emission is produced farther out in the pc-scale MT,\nthe IC scattering of the infrared (IR) MT seed photons that produces the\ngamma-rays takes place in the Thomson regime, resulting to energy-dependent\nelectron cooling times, manifested as faster cooling times for higher Fermi\nenergies. We demonstrate these characteristics and discuss the applicability\nand limitations of our method. \n\n"}
{"id": "1209.2342", "contents": "Title: Seeking Counterparts to Advanced LIGO/Virgo Transients with Swift Abstract: Binary neutron star (NS) mergers are among the most promising astrophysical\nsources of gravitational wave emission for Advanced LIGO and Advanced Virgo,\nexpected to be operational in 2015. Finding electromagnetic counterparts to\nthese signals will be essential to placing them in an astronomical context. The\nSwift satellite carries a sensitive X-ray telescope (XRT), and can respond to\ntarget-of-opportunity requests within 1-2 hours, and so is uniquely poised to\nfind the X-ray counterparts to LIGO/Virgo triggers. Assuming NS mergers are the\nprogenitors of short gamma-ray bursts (GRBs), some percentage of LIGO/Virgo\ntriggers will be accompanied by X-ray band afterglows that are brighter than\n10^-12 erg/s/cm^2 in the XRT band one day after the trigger time. We find that\na soft X-ray transient of this flux is bright enough to be extremely rare, and\nso could be confidently associated with even a moderately localized GW signal.\nWe examine two possible search strategies with the Swift XRT to find bright\ntransients in LIGO/Virgo error boxes. In the first strategy, XRT could search a\nvolume of space with a ~100 Mpc radius by observing ~30 galaxies over the\ncourse of a day, with sufficient depth to observe the expected X-ray afterglow.\nFor an extended LIGO/Virgo horizon distance, the XRT could employ 100 s\nexposures to cover an area of ~ 35 square degrees in about a day, remain\nsensitive enough to image GW discovered GRB afterglows. These strategies\ndemonstrate that discovery of X-ray band counterparts to GW triggers will be\npossible. \n\n"}
{"id": "1209.3495", "contents": "Title: The Collatz conjecture and De Bruijn graphs Abstract: We study variants of the well-known Collatz graph, by considering the action\nof the 3n+1 function on congruence classes. For moduli equal to powers of 2,\nthese graphs are shown to be isomorphic to binary De Bruijn graphs. Unlike the\nCollatz graph, these graphs are very structured, and have several interesting\nproperties. We then look at a natural generalization of these finite graphs to\nthe 2-adic integers, and show that the isomorphism between these infinite\ngraphs is exactly the conjugacy map previously studied by Bernstein and\nLagarias. Finally, we show that for generalizations of the 3n+1 function, we\nget similar relations with 2-adic and p-adic De Bruijn graphs. \n\n"}
{"id": "1209.4501", "contents": "Title: Kaluza-Klein models with spherical compactification: observational\n  constraints and possible examples Abstract: We consider Kaluza-Klein models with background matter in the form of a\nmulticomponent perfect fluid. This matter provides spherical compactification\nof the internal space with an arbitrary number of dimensions. The gravitating\nsource has the dust-like equation of state in the external/our space and an\narbitrary equation of state (with the parameter $\\Omega$) in the internal\nspace. In the single-component case, tension ($\\Omega=-1/2$) is the necessary\ncondition to satisfy both the gravitational tests in the solar system and the\nthermodynamical observations. In the multicomponent case, we propose two models\nsatisfying both of these observations. One of them also requires tension\n$\\Omega=-1/2$, but the second one is of special interest because is free of\ntension, i.e. $\\Omega=0$. To get this result, we need to impose certain\nconditions. \n\n"}
{"id": "1209.5860", "contents": "Title: Reversible MCMC on Markov equivalence classes of sparse directed acyclic\n  graphs Abstract: Graphical models are popular statistical tools which are used to represent\ndependent or causal complex systems. Statistically equivalent causal or\ndirected graphical models are said to belong to a Markov equivalent class. It\nis of great interest to describe and understand the space of such classes.\nHowever, with currently known algorithms, sampling over such classes is only\nfeasible for graphs with fewer than approximately 20 vertices. In this paper,\nwe design reversible irreducible Markov chains on the space of Markov\nequivalent classes by proposing a perfect set of operators that determine the\ntransitions of the Markov chain. The stationary distribution of a proposed\nMarkov chain has a closed form and can be computed easily. Specifically, we\nconstruct a concrete perfect set of operators on sparse Markov equivalence\nclasses by introducing appropriate conditions on each possible operator.\nAlgorithms and their accelerated versions are provided to efficiently generate\nMarkov chains and to explore properties of Markov equivalence classes of sparse\ndirected acyclic graphs (DAGs) with thousands of vertices. We find\nexperimentally that in most Markov equivalence classes of sparse DAGs, (1) most\nedges are directed, (2) most undirected subgraphs are small and (3) the number\nof these undirected subgraphs grows approximately linearly with the number of\nvertices. The article contains supplement arXiv:1303.0632,\nhttp://dx.doi.org/10.1214/13-AOS1125SUPP \n\n"}
{"id": "1209.5932", "contents": "Title: Efficient quantum algorithm to construct arbitrary Dicke states Abstract: In this paper, we study efficient algorithms towards the construction of any\narbitrary Dicke state. Our contribution is to use proper symmetric Boolean\nfunctions that involve manipulations with Krawtchouk polynomials. Deutsch-Jozsa\nalgorithm, Grover algorithm and the parity measurement technique are stitched\ntogether to devise the complete algorithm. Further, motivated by the work of\nChilds et al (2002), we explore how one can plug the biased Hadamard\ntransformation in our strategy. Our work compares fairly with the results of\nChilds et al (2002). \n\n"}
{"id": "1209.6287", "contents": "Title: Nonlinear magnetic response of the magnetized vacuum to applied electric\n  field Abstract: We find first nonlinear correction to the field, produced by a static charge\nat rest in a background constant magnetic field. It is quadratic in the charge\nand purely magnetic. The third-rank polarization tensor - the nonlinear\nresponse function - is written within the local approximation of the effective\naction in an otherwise model- and approximation-independent way within any\nP-invariant nonlinear electrodynamics, QED included. \n\n"}
{"id": "1210.1847", "contents": "Title: Constraints on the Universe as a Numerical Simulation Abstract: Observable consequences of the hypothesis that the observed universe is a\nnumerical simulation performed on a cubic space-time lattice or grid are\nexplored. The simulation scenario is first motivated by extrapolating current\ntrends in computational resource requirements for lattice QCD into the future.\nUsing the historical development of lattice gauge theory technology as a guide,\nwe assume that our universe is an early numerical simulation with unimproved\nWilson fermion discretization and investigate potentially-observable\nconsequences. Among the observables that are considered are the muon g-2 and\nthe current differences between determinations of alpha, but the most stringent\nbound on the inverse lattice spacing of the universe, b^(-1) >~ 10^(11) GeV, is\nderived from the high-energy cut off of the cosmic ray spectrum. The numerical\nsimulation scenario could reveal itself in the distributions of the highest\nenergy cosmic rays exhibiting a degree of rotational symmetry breaking that\nreflects the structure of the underlying lattice. \n\n"}
{"id": "1210.4552", "contents": "Title: Herschel PACS and SPIRE observations of blazar PKS 1510-089: a case for\n  two blazar zones Abstract: We present the results of observations of blazar PKS 1510-089 with the\nHerschel Space Observatory PACS and SPIRE instruments, together with\nmultiwavelength data from Fermi/LAT, Swift, SMARTS and SMA. The source was\nfound in a quiet state, and its far-infrared spectrum is consistent with a\npower-law with a spectral index of alpha ~ 0.7. Our Herschel observations were\npreceded by two 'orphan' gamma-ray flares. The near-infrared data reveal the\nhigh-energy cut-off in the main synchrotron component, which cannot be\nassociated with the main gamma-ray component in a one-zone leptonic model. This\nis because in such a model the luminosity ratio of the External-Compton and\nsynchrotron components is tightly related to the frequency ratio of these\ncomponents, and in this particular case an unrealistically high energy density\nof the external radiation would be implied. Therefore, we consider a\nwell-constrained two-zone blazar model to interpret the entire dataset. In this\nframework, the observed infrared emission is associated with the synchrotron\ncomponent produced in the hot-dust region at the supra-pc scale, while the\ngamma-ray emission is associated with the External-Compton component produced\nin the broad-line region at the sub-pc scale. In addition, the optical/UV\nemission is associated with the accretion disk thermal emission, with the\naccretion disk corona likely contributing to the X-ray emission. \n\n"}
{"id": "1210.5048", "contents": "Title: Convergence of SDP hierarchies for polynomial optimization on the\n  hypersphere Abstract: We show how to bound the accuracy of a family of semi-definite programming\nrelaxations for the problem of polynomial optimization on the hypersphere. Our\nmethod is inspired by a set of results from quantum information known as\nquantum de Finetti theorems. In particular, we prove a de Finetti theorem for a\nspecial class of real symmetric matrices to establish the existence of\napproximate representing measures for moment matrix relaxations. \n\n"}
{"id": "1210.6661", "contents": "Title: Local Study of Accretion Disks with a Strong Vertical Magnetic Field:\n  Magnetorotational Instability and Disk Outflow Abstract: We perform 3D vertically-stratified local shearing-box ideal MHD simulations\nof the magnetorotational instability (MRI) that include a net vertical magnetic\nflux, which is characterized by beta_0 (ratio of gas pressure to magnetic\npressure of the net vertical field at midplane). We have considered\nbeta_0=10^2, 10^3 and 10^4 and in the first two cases the most unstable linear\nMRI modes are well resolved in the simulations. We find that the behavior of\nthe MRI turbulence strongly depends on beta_0: The radial transport of angular\nmomentum increases with net vertical flux, achieving alpha=0.08 for beta_0=10^4\nand alpha>1.0 for beta_0=100, where alpha is the Shakura-Sunyaev parameter. A\ncritical value lies at beta_0=10^3: For beta_0>10^3, the disk consists of a gas\npressure dominated midplane and a magnetically dominated corona. The turbulent\nstrength increases with net flux, and angular momentum transport is dominated\nby turbulent fluctuations. The magnetic dynamo that leads to cyclic flips of\nlarge-scale fields still exists, but becomes more sporadic as net flux\nincreases. For beta_0<10^3, the entire disk becomes magnetic dominated. The\nturbulent strength saturates, and the magnetic dynamo is quenched. Stronger\nlarge-scale fields are generated with increasing net flux, which dominates\nangular momentum transport. A strong outflow is launched from the disk by the\nmagnetocentrifugal mechanism, and the mass flux increases linearly with net\nvertical flux and shows sign of saturation at beta_0=10^2. However, the outflow\nis unlikely to be directly connected to a global wind: for beta_0>10^3, the\nlarge-scale field has no permanent bending direction due to dynamo activities,\nwhile for beta_0<10^3, the outflows from the top and bottom sides of the disk\nbend towards opposite directions, inconsistent with a physical disk wind\ngeometry. Global simulations are needed to address the fate of the outflow. \n\n"}
{"id": "1210.6853", "contents": "Title: On solving large scale polynomial convex problems by randomized\n  first-order algorithms Abstract: One of the most attractive recent approaches to processing well-structured\nlarge-scale convex optimization problems is based on smooth convex-concave\nsaddle point reformu-lation of the problem of interest and solving the\nresulting problem by a fast First Order saddle point method utilizing\nsmoothness of the saddle point cost function. In this paper, we demonstrate\nthat when the saddle point cost function is polynomial, the precise gra-dients\nof the cost function required by deterministic First Order saddle point\nalgorithms and becoming prohibitively computationally expensive in the\nextremely large-scale case, can be replaced with incomparably cheaper\ncomputationally unbiased random estimates of the gradients. We show that for\nlarge-scale problems with favourable geometry, this randomization accelerates,\nprogressively as the sizes of the problem grow, the solution process. This\nextends significantly previous results on acceleration by randomization, which,\nto the best of our knowledge, dealt solely with bilinear saddle point problems.\nWe illustrate our theoretical findings by instructive and encouraging numerical\nexperiments. \n\n"}
{"id": "1210.7072", "contents": "Title: Production and propagation of heavy hadrons in air-shower simulators Abstract: Very energetic charm and bottom hadrons may be produced in the upper\natmosphere when a primary cosmic ray or the leading hadron in an extensive air\nshower collide with a nucleon. At $E\\approx 10^8$ GeV their decay length\nbecomes of the order of 10 km, implying that they tend to interact in the air\ninstead of decaying. Since the inelasticity in these collisions is much smaller\nthan the one in proton and pion collisions, there could be rare events where a\nheavy-hadron component transports a significant amount of energy deep into the\natmosphere. We have developed a module for the detailed simulation of these\nprocesses and have included it in a new version of the air shower simulator\nAIRES. We study the frequency, the energy distribution and the depth of charm\nand bottom production, as well as the depth and the energy distribution of\nthese quarks when they decay. As an illustration, we consider the production\nand decay of tau leptons (from $D_s$ decays) and the lepton flux at PeV\nenergies from a 30 EeV proton primary. The proper inclusion of charm and bottom\nhadrons in AIRES opens the possibility to search for air-shower observables\nthat are sensitive to heavy quark effects. \n\n"}
{"id": "1210.8014", "contents": "Title: Volume Rendering of AMR Simulations Abstract: High-resolution simulations often rely on the Adaptive Mesh Resolution (AMR)\ntechnique to optimize memory consumption versus attainable precision. While\nthis technique allows for dramatic improvements in terms of computing\nperformance, the analysis and visualization of its data outputs remain\nchallenging. The lack of effective volume renderers for the octree-based AMR\nused by the RAMSES simulation program has led to the development of the\nsolutions presented in this paper. Two custom algorithms are discussed, based\non the splatting and the ray-casting techniques. Their usage is illustrated in\nthe context of the visualization of a high-resolution, 6000-processor\nsimulation of a Milky Way-like galaxy. Performance obtained in terms of memory\nmanagement and parallelism speedup are presented. \n\n"}
{"id": "1210.8180", "contents": "Title: Galactic Streams of Cosmic-ray Electrons and Positrons Abstract: Isotropic diffusion is a key assumption in many models of cosmic-ray\nelectrons and positrons. We find that simulation results imply a critical\nenergy of ~10-1000~GeV above which electrons and positrons can spend their\nentire lives in streams threading magnetic fields, due to energy losses. This\nwould restrict the number of electron/positron sources contributing at Earth,\nlikely leading to smooth electron/positron spectra, as is observed. For\npositrons, this could be as few as one, with an enhanced flux that would ease\nenergetics concerns of a pulsar origin of the positron excess, or even zero,\nbringing dark matter to the fore. We conclude that ideas about\nelectron/positron propagation must be revised and discuss implications for\nrecent AMS-02 data. \n\n"}
{"id": "1210.8338", "contents": "Title: On the Power of Conditional Samples in Distribution Testing Abstract: In this paper we define and examine the power of the {\\em\nconditional-sampling} oracle in the context of distribution-property testing.\nThe conditional-sampling oracle for a discrete distribution $\\mu$ takes as\ninput a subset $S \\subset [n]$ of the domain, and outputs a random sample $i\n\\in S$ drawn according to $\\mu$, conditioned on $S$ (and independently of all\nprior samples). The conditional-sampling oracle is a natural generalization of\nthe ordinary sampling oracle in which $S$ always equals $[n]$.\n  We show that with the conditional-sampling oracle, testing uniformity,\ntesting identity to a known distribution, and testing any label-invariant\nproperty of distributions is easier than with the ordinary sampling oracle. On\nthe other hand, we also show that for some distribution properties the\nsample-complexity remains near-maximal even with conditional sampling. \n\n"}
{"id": "1211.2251", "contents": "Title: Independent subsets of powers of paths, and Fibonacci cubes Abstract: We provide a formula for the number of edges of the Hasse diagram of the\nindependent subsets of the h-th power of a path ordered by inclusion. For h=1\nsuch a value is the number of edges of a Fibonacci cube. We show that, in\ngeneral, the number of edges of the diagram is obtained by convolution of a\nFibonacci-like sequence with itself. \n\n"}
{"id": "1211.2664", "contents": "Title: Testing probability distributions using conditional samples Abstract: We study a new framework for property testing of probability distributions,\nby considering distribution testing algorithms that have access to a\nconditional sampling oracle.* This is an oracle that takes as input a subset $S\n\\subseteq [N]$ of the domain $[N]$ of the unknown probability distribution $D$\nand returns a draw from the conditional probability distribution $D$ restricted\nto $S$. This new model allows considerable flexibility in the design of\ndistribution testing algorithms; in particular, testing algorithms in this\nmodel can be adaptive.\n  We study a wide range of natural distribution testing problems in this new\nframework and some of its variants, giving both upper and lower bounds on query\ncomplexity. These problems include testing whether $D$ is the uniform\ndistribution $\\mathcal{U}$; testing whether $D = D^\\ast$ for an explicitly\nprovided $D^\\ast$; testing whether two unknown distributions $D_1$ and $D_2$\nare equivalent; and estimating the variation distance between $D$ and the\nuniform distribution. At a high level our main finding is that the new\n\"conditional sampling\" framework we consider is a powerful one: while all the\nproblems mentioned above have $\\Omega(\\sqrt{N})$ sample complexity in the\nstandard model (and in some cases the complexity must be almost linear in $N$),\nwe give $\\mathrm{poly}(\\log N, 1/\\varepsilon)$-query algorithms (and in some\ncases $\\mathrm{poly}(1/\\varepsilon)$-query algorithms independent of $N$) for\nall these problems in our conditional sampling setting.\n  *Independently from our work, Chakraborty et al. also considered this\nframework. We discuss their work in Subsection [1.4]. \n\n"}
{"id": "1211.3049", "contents": "Title: Reversible Christoffel factorizations Abstract: We define a family of natural decompositions of Sturmian words in Christoffel\nwords, called *reversible Christoffel* (RC) factorizations. They arise from the\nobservation that two Sturmian words with the same language have (almost always)\narbitrarily long Abelian equivalent prefixes. Using the three gap theorem, we\nprove that in each RC factorization, only 2 or 3 distinct Christoffel words may\noccur. We begin the study of such factorizations, considered as infinite words\nover 2 or 3 letters, and show that in the general case they are either Sturmian\nwords, or obtained by a three-interval exchange transformation. \n\n"}
{"id": "1211.3202", "contents": "Title: Stationary Scalar Clouds Around Rotating Black Holes Abstract: Motivated by novel results in the theory of wave dynamics in black-hole\nspacetimes, we analyze the dynamics of a massive scalar field surrounding a\nrapidly rotating Kerr black hole. In particular, we report on the existence of\nstationary (infinitely long-lived) regular field configurations in the\nbackground of maximally rotating black holes. The effective height of these\nscalar \"clouds\" above the central black hole is determined analytically. Our\nresults support the possible existence of stationary scalar field dark matter\ndistributions surrounding rapidly rotating black holes. \n\n"}
{"id": "1211.3627", "contents": "Title: Acceleration of cosmic rays by young core-collapse supernova remnants Abstract: Context. Supernova remnants (SNRs) are thought to be the primary candidates\nfor the sources of Galactic cosmic rays. According to the diffusive shock\nacceleration theory, SNR shocks produce a power-law spectrum with an index of s\n= 2, perhaps nonlinearly modified to harder spectra at high energy.\nObservations of SNRs often indicate particle spectra that are softer than that\nand show features not expected from classical theory. Known drawbacks of the\nstandard approach are the assumption that SNRs evolve in a uniform environment,\nand that the reverse shock does not accelerate particles. Relaxing these\nassumptions increases the complexity of the problem, because one needs reliable\nhydrodynamical data for the plasma flow as well as good estimates for the\nmagnetic field (MF) at the reverse shock. Aims. We show that these two factors\nare especially important when modeling young core-collapse SNRs that evolve in\na complicated circumstellar medium shaped by the winds of progenitor stars.\nMethods. We used high-resolution numerical simulations for the hydrodynamical\nevolution of the SNR. Instead of parametrizations of the MF profiles inside the\nSNR, we followed the advection of the frozen-in MF inside the SNR, and thus\nobtained the B-field value at all locations, in particular at the reverse\nshock. To model cosmic-ray acceleration we solved the cosmic-ray transport\nequation in test-particle approximation. Results. We find that the complex\nplasma-flow profiles of core-collapse SNRs significantly modify the particle\nspectra. Additionally, the reverse shock strongly affects the emission spectra\nand the surface brightness. \n\n"}
{"id": "1212.0027", "contents": "Title: Generalized Cayley Graphs and Cellular Automata over them Abstract: Cayley graphs have a number of useful features: the ability to graphically\nrepresent finitely generated group elements and their relations; to name all\nvertices relative to a point; and the fact that they have a well-defined notion\nof translation. We propose a notion of graph associated to a language, which\nconserves or generalizes these features. Whereas Cayley graphs are very\nregular; associated graphs are arbitrary, although of a bounded degree.\nMoreover, it is well-known that cellular automata can be characterized as the\nset of translation-invariant continuous functions for a distance on the set of\nconfigurations that makes it a compact metric space; this point of view makes\nit easy to extend their definition from grids to Cayley graphs. Similarly, we\nextend their definition to these arbitrary, bounded degree, time-varying\ngraphs. The obtained notion of Cellular Automata over generalized Cayley graphs\nis stable under composition and under inversion. KEYWORDS: Causal Graph\nDynamics, Curtis-Hedlund-Lyndon, Dynamical networks, Boolean networks,\nGenerative networks automata, Graph Automata, Graph rewriting automata,\nL-systems, parallel graph transformations, Amalgamated graph transformations,\nTime-varying graphs, Regge calculus, Local, No-signalling, Reversibility. \n\n"}
{"id": "1212.3471", "contents": "Title: Optimal Cuts and Partitions in Tree Metrics in Polynomial Time Abstract: We present a polynomial time dynamic programming algorithm for optimal\npartitions in the shortest path metric induced by a tree. This resolves, among\nother things, the exact complexity status of the optimal partition problems in\none dimensional geometric metric settings. Our method of solution could be also\nof independent interest in other applications. We discuss also an extension of\nour method to the class of metrics induced by the bounded treewidth graphs. \n\n"}
{"id": "1212.4756", "contents": "Title: One Tile to Rule Them All: Simulating Any Turing Machine, Tile Assembly\n  System, or Tiling System with a Single Puzzle Piece Abstract: In this paper we explore the power of tile self-assembly models that extend\nthe well-studied abstract Tile Assembly Model (aTAM) by permitting tiles of\nshapes beyond unit squares. Our main result shows the surprising fact that any\naTAM system, consisting of many different tile types, can be simulated by a\nsingle tile type of a general shape. As a consequence, we obtain a single\nuniversal tile type of a single (constant-size) shape that serves as a\n\"universal tile machine\": the single universal tile type can simulate any\ndesired aTAM system when given a single seed assembly that encodes the desired\naTAM system. We also show how to adapt this result to convert any of a variety\nof plane tiling systems (such as Wang tiles) into a \"nearly\" plane tiling\nsystem with a single tile (but with small gaps between the tiles). All of these\nresults rely on the ability to both rotate and translate tiles; by contrast, we\nshow that a single nonrotatable tile, of arbitrary shape, can produce\nassemblies which either grow infinitely or cannot grow at all, implying\ndrastically limited computational power.\n  On the positive side, we show how to simulate arbitrary cellular automata for\na limited number of steps using a single nonrotatable tile and a linear-size\nseed assembly. \n\n"}
{"id": "1212.6781", "contents": "Title: Lattice Sparsification and the Approximate Closest Vector Problem Abstract: We give a deterministic algorithm for solving the (1+eps)-approximate Closest\nVector Problem (CVP) on any n dimensional lattice and any norm in\n2^{O(n)}(1+1/eps)^n time and 2^n poly(n) space. Our algorithm builds on the\nlattice point enumeration techniques of Micciancio and Voulgaris (STOC 2010)\nand Dadush, Peikert and Vempala (FOCS 2011), and gives an elegant,\ndeterministic alternative to the \"AKS Sieve\" based algorithms for (1+eps)-CVP\n(Ajtai, Kumar, and Sivakumar; STOC 2001 and CCC 2002). Furthermore, assuming\nthe existence of a poly(n)-space and 2^{O(n)} time algorithm for exact CVP in\nthe l_2 norm, the space complexity of our algorithm can be reduced to\npolynomial.\n  Our main technical contribution is a method for \"sparsifying\" any input\nlattice while approximately maintaining its metric structure. To this end, we\nemploy the idea of random sublattice restrictions, which was first employed by\nKhot (FOCS 2003) for the purpose of proving hardness for Shortest Vector\nProblem (SVP) under l_p norms. \n\n"}
{"id": "1212.6848", "contents": "Title: Maximum Balanced Subgraph Problem Parameterized Above Lower Bound Abstract: We consider graphs without loops or parallel edges in which every edge is\nassigned + or -. Such a signed graph is balanced if its vertex set can be\npartitioned into parts $V_1$ and $V_2$ such that all edges between vertices in\nthe same part have sign + and all edges between vertices of different parts\nhave sign $-$ (one of the parts may be empty). It is well-known that every\nconnected signed graph with $n$ vertices and $m$ edges has a balanced subgraph\nwith at least $\\frac{m}{2} + \\frac{n-1}{4}$ edges and this bound is tight. We\nconsider the following parameterized problem: given a connected signed graph\n$G$ with $n$ vertices and $m$ edges, decide whether $G$ has a balanced subgraph\nwith at least $\\frac{m}{2} + \\frac{n-1}{4}+\\frac{k}{4}$ edges, where $k$ is the\nparameter.\n  We obtain an algorithm for the problem of runtime $8^k(kn)^{O(1)}$. We also\nprove that for each instance $(G,k)$ of the problem, in polynomial time, we can\neither solve $(G,k)$ or produce an equivalent instance $(G',k')$ such that\n$k'\\le k$ and $|V(G')|=O(k^3)$. Our first result generalizes a result of\nCrowston, Jones and Mnich (ICALP 2012) on the corresponding parameterization of\nMax Cut (when every edge of $G$ has sign $-$). Our second result generalizes\nand significantly improves the corresponding result of Crowston, Jones and\nMnich: they showed that $|V(G')|=O(k^5)$. \n\n"}
{"id": "1301.0291", "contents": "Title: On the Non-existence of a Sharp Cooling Break in GRB Afterglow Spectra Abstract: Although the widely-used analytical afterglow model of gamma-ray bursts\n(GRBs) predicts a sharp cooling break $\\nu_c$ in its afterglow spectrum, the\nGRB observations so far rarely show clear evidence for a cooling break in their\nspectra or its corresponding temporal break in their light curves. Employing a\nLagrangian description of the blast wave, we conduct a sophisticated\ncalculation of the afterglow emission. We precisely follow the cooling history\nof non-thermal electrons accelerated into each Lagrangian shell. We show that a\ndetailed calculation of afterglow spectra does not in fact give rise to a sharp\ncooling break at $\\nu_c$. Instead, it displays a very mild and smooth\ntransition, which occurs gradually over a few orders of magnitude in energy or\nfrequency. The main source of this slow transition is that different\nmini-shells have different evolution histories of the comoving magnetic field\nstrength $B$, so that deriving the current value of $\\nu_c$ of each mini-shell\nrequires an integration of its cooling rate over the time elapsed since its\ncreation. We present the time evolution of optical and X-ray spectral indices\nto demonstrate the slow transition of spectral regimes, and discuss the\nimplications of our result in interpreting GRB afterglow data. \n\n"}
{"id": "1301.1138", "contents": "Title: Search for muon signal from dark matter annihilations in the Sun with\n  the Baksan Underground Scintillator Telescope for 24.12 years Abstract: We present a new dataset analysis of the neutrino experiment at the Baksan\nUnderground Scintillator Telescope with muon energy threshold about 1 GeV for\nthe longest exposure time toward the Sun. In search for a signal from\nself-annihilations of dark matter particles in the center of the Sun we use an\nupdated sample of upward through-going muons for 24.12 years of live time. No\nobservable excess has been found in measured muons relative to expected\nbackground from neutrinos of atmospheric origin. We present an improved data\nanalysis procedure and describe it in detail. We set the 90\\% C.L. new upper\nlimits on expected neutrino and muon fluxes from dark matter annihilations in\nthe Sun, on the corresponding annihilation rates and cross sections of their\nelastic scattering off proton. \n\n"}
{"id": "1301.2438", "contents": "Title: Unified description of dense matter in neutron stars and magnetars Abstract: We have recently developed a set of equations of state based on the nuclear\nenergy density functional theory providing a unified description of the\ndifferent regions constituting the interior of neutron stars and magnetars. The\nnuclear functionals, which were constructed from generalized Skyrme effective\nnucleon-nucleon interactions, yield not only an excellent fit to essentially\nall experimental atomic mass data but were also constrained to reproduce the\nneutron-matter equation of state as obtained from realistic many-body\ncalculations. \n\n"}
{"id": "1301.2626", "contents": "Title: Active Self-Assembly of Algorithmic Shapes and Patterns in\n  Polylogarithmic Time Abstract: We describe a computational model for studying the complexity of\nself-assembled structures with active molecular components. Our model captures\nnotions of growth and movement ubiquitous in biological systems. The model is\ninspired by biology's fantastic ability to assemble biomolecules that form\nsystems with complicated structure and dynamics, from molecular motors that\nwalk on rigid tracks and proteins that dynamically alter the structure of the\ncell during mitosis, to embryonic development where large-scale complicated\norganisms efficiently grow from a single cell. Using this active self-assembly\nmodel, we show how to efficiently self-assemble shapes and patterns from simple\nmonomers. For example, we show how to grow a line of monomers in time and\nnumber of monomer states that is merely logarithmic in the length of the line.\n  Our main results show how to grow arbitrary connected two-dimensional\ngeometric shapes and patterns in expected time that is polylogarithmic in the\nsize of the shape, plus roughly the time required to run a Turing machine\ndeciding whether or not a given pixel is in the shape. We do this while keeping\nthe number of monomer types logarithmic in shape size, plus those monomers\nrequired by the Kolmogorov complexity of the shape or pattern. This work thus\nhighlights the efficiency advantages of active self-assembly over passive\nself-assembly and motivates experimental effort to construct general-purpose\nactive molecular self-assembly systems. \n\n"}
{"id": "1301.2801", "contents": "Title: Acceleration of particles by acceleration horizons Abstract: We consider collision of two particles in the vicinity of the extremal\nacceleration horizon (charged or rotating) that includes the Bertotti-Robinson\nspace-time and the geometry of the Kerr throat. It is shown that the energy in\nthe centre of mass frame E_{c.m.} can become indefinitely large if parameters\nof one of the particles are fine-tuned, so the Ba\\~nados-Silk-West (BSW) effect\nmanifests itself. There exists coordinate transformation which brings the\nmetric into the form free of the horizon. This leads to some paradox since (i)\nthe BSW effect exists due to the horizon, (ii) E_{c.m.} is a scalar and cannot\ndepend on the frame. Careful comparison of near-horizon trajectories in both\nframes enables us to resolve this paradox. Although globally the space-time\nstructure of the metrics with acceleration horizons and black holes are\ncompletely different, locally the vicinity of the extremal black hole horizon\ncan be approximated by the metric of the acceleration one. The energy of one\nparticle from the viewpoint of the Kruskal observer (or the one obtained from\nit by finite local boost) diverges although in the stationary frame energies of\nboth colliding particles are finite. This suggests a new explanation of the BSW\neffect for black holes given from the viewpoint of an observer who crosses the\nhorizon. It is complementary to the previously found explanation from the point\nof view of a static or stationary observer. \n\n"}
{"id": "1301.3378", "contents": "Title: Pulse phase and precession phase resolved spectroscopy of Her X-1:\n  studying a representative Main-On with RXTE Abstract: We performed a detailed pulse phase resolved spectroscopy of the accreting\nbinary X-ray pulsar Her X-1 in the energy range 3.5-75 keV and have established\npulse phase profiles for all spectral parameters. For the centroid of the\ncyclotron line, the photon index and the flux of the 6.4 keV iron line, we have\nstudied the variation as a function of 35 d phase. We analyzed RXTE\nobservations of the Main-On of November 2002. Four different time intervals of\nabout 1 d duration were selected to provide a good coverage of a complete\nMain-On. The intervals are centered at 35 d phase 0.03, 0.10, 0.15, and 0.20,\nrespectively. All spectral parameters show a strong modulation with pulse\nphase. While the centroid energy of the cyclotron line follows roughly the\nshape of the pulse profile, both the photon index and the iron line intensity\nexhibit distinct minima around the peak of the X-ray pulse. With respect to\nvariations of the observed profiles with 35 d phase, we find that there is a\nclear evolution of the shape of the pulse profiles (flux versus pulse phase), a\nmoderate increase of the maximum cyclotron line energy (found around pulse\nphase 0.7), but no significant evolution of the shape of the pulse phase\nprofiles of the cyclotron line energy, the spectral power law index or the iron\nline intensity. The variation of spectral parameters as a function of the pulse\nphase provides important information about the system: 1. the disappearance of\nthe Fe line flux near the highest continuum flux may be an indication of a\nhollow cone geometry of the accretion structure; ii. the apparent\nnon-dependence of the cyclotron line energy profiles on 35 d phase provides a\nnew possibility to test the model of free precession of the neutron star,\nproposed to be responsible for the systematic variations in the pulse profiles. \n\n"}
{"id": "1301.5055", "contents": "Title: Nested Recursions, Simultaneous Parameters and Tree Superpositions Abstract: We apply a tree-based methodology to solve new, very broadly defined families\nof nested recursions of the general form R(n)=sum_{i=1}^k R(n-a_i-sum_{j=1}^p\nR(n-b_{ij})), where a_i are integers, b_{ij} are natural numbers, and k,p are\nnatural numbers that we use to denote \"arity\" and \"order,\" respectively, and\nwith some specified initial conditions. The key idea of the tree-based solution\nmethod is to associate such recursions with infinite labelled trees in a\nnatural way so that the solution to the recursions solves a counting question\nrelating to the corresponding trees. We characterize certain recursion families\nwithin R(n) by introducing \"simultaneous parameters\" that appear both within\nthe recursion itself and that also specify structural properties of the\ncorresponding tree. First, we extend and unify recently discovered results\nconcerning two families of arity k=2, order p=1 recursions. Next, we\ninvestigate the solution of nested recursion families by taking linear\ncombinations of solution sequence frequencies for simpler nested recursions,\nwhich correspond to superpositions of the associated trees; this leads us to\nidentify and solve two new recursion families for arity k=2 and general order\np. Finally, we extend these results to general arity k>2. We conclude with\nseveral related open problems. \n\n"}
{"id": "1301.5189", "contents": "Title: Structure of neutron stars in R-squared gravity Abstract: The effects implied for the structure of compact objects by the modification\nof General Relativity produced by the generalization of the Lagrangian density\nto the form f(R)=R+\\alpha R^2, where R is the Ricci curvature scalar, have been\nrecently explored. It seems likely that this squared-gravity may allow heavier\nNeutron Stars (NSs) than GR. In addition, these objects can be useful to\nconstrain free parameters of modified-gravity theories. The differences between\nalternative gravity theories is enhanced in the strong gravitational regime. In\nthis regime, because of the complexity of the field equations, perturbative\nmethods become a good choice to treat the problem. Following previous works in\nthe field, we performed a numerical integration of the structure equations that\ndescribe NSs in f(R)-gravity, recovering their mass-radius relations, but\nfocusing on particular features that arise from this approach in the profiles\nof the NS interior.\n  We show that these profiles run in correlation with the second-order\nderivative of the analytic approximation to the Equation of State (EoS), which\nleads to regions where the enclosed mass decreases with the radius in a\ncounter-intuitive way. We reproduce all computations with a simple polytropic\nEoS to separate zeroth-order modified gravity effects. \n\n"}
{"id": "1301.5293", "contents": "Title: Approximately counting semismooth integers Abstract: An integer $n$ is $(y,z)$-semismooth if $n=pm$ where $m$ is an integer with\nall prime divisors $\\le y$ and $p$ is 1 or a prime $\\le z$. arge quantities of\nsemismooth integers are utilized in modern integer factoring algorithms, such\nas the number field sieve, that incorporate the so-called large prime variant.\nThus, it is useful for factoring practitioners to be able to estimate the value\nof $\\Psi(x,y,z)$, the number of $(y,z)$-semismooth integers up to $x$, so that\nthey can better set algorithm parameters and minimize running times, which\ncould be weeks or months on a cluster supercomputer. In this paper, we explore\nseveral algorithms to approximate $\\Psi(x,y,z)$ using a generalization of\nBuchstab's identity with numeric integration. \n\n"}
{"id": "1301.5359", "contents": "Title: Local Graph Coloring and Index Coding Abstract: We present a novel upper bound for the optimal index coding rate. Our bound\nuses a graph theoretic quantity called the local chromatic number. We show how\na good local coloring can be used to create a good index code. The local\ncoloring is used as an alignment guide to assign index coding vectors from a\ngeneral position MDS code. We further show that a natural LP relaxation yields\nan even stronger index code. Our bounds provably outperform the state of the\nart on index coding but at most by a constant factor. \n\n"}
{"id": "1302.0011", "contents": "Title: High redshift blazars Abstract: Blazars are sources whose jet is pointing to us. Since their jets are\nrelativistic, the flux is greatly amplified in the direction of motion, making\nblazars the most powerful persistent objects in the Universe. This is true at\nall frequencies, but especially where their spectrum peaks. Although the\nspectrum of moderate powerful sources peaks in the ~GeV range, extremely\npowerful sources at high redshifts peak in the ~MeV band. This implies that the\nhard X-ray band is the optimal one to find powerful blazars beyond a redshift\nof ~4. First indications strongly suggest that powerful high-z blazars harbor\nthe most massive and active early black holes, exceeding a billion solar\nmasses. Since for each detected blazars there must exist hundreds of similar,\nbut misaligned, sources, the search for high-z blazars is becoming competitive\nwith the search of early massive black holes using radio-quiet quasars. Finding\nhow the two populations of black holes (one in jetted sources, the other in\nradio-quiet objects) evolve in redshift will shed light on the growth of the\nmost massive black holes and possibly on the feedback between the central\nengine and the rest of the host galaxy. \n\n"}
{"id": "1302.2040", "contents": "Title: Supernova Detection in IceCube: Status and Future Abstract: The IceCube detector, located at the South Pole, is discussed as a detector\nfor core collapse supernovae. The large flux of $\\bar{\\nu}_{e}$ from a Galactic\nsupernova gives rise to Cherenkov light from positrons and electrons created in\nneutrino interactions which increase the overall count rate of the\nphotomultipliers significantly. We will give an overview of the standard, count\nrate based, method for supernova detection and present the development of a\nnovel technique. This technique uses coincident hits to extract additional\ninformation such as the average energy and spectral features. The potential of\nthis technique increases with a higher sensor density, such as foreseen in\nprojected extensions of IceCube/DeepCore. \n\n"}
{"id": "1302.2238", "contents": "Title: The missing link between ultraluminous X-ray sources and metallicity Abstract: The nature of ultraluminous X-ray sources (ULXs) is still debated. Recent\nstudies show that metal-poor massive stars can collapse into massive stellar\nblack holes (MSBHs), that is black holes with mass > 25 Msun. Such MSBHs are\nsufficiently massive to explain most ULXs without requiring substantial\nviolations of the Eddington limit. The recent finding of an anti-correlation\nbetween metallicity of the environment and number of ULXs per galaxy supports\nthis hypothesis. We present the results of recent N-body simulations, including\nmetallicity dependent stellar evolution, and we discuss the main pathways to\nproduce X-ray binaries powered by MSBHs. \n\n"}
{"id": "1302.2426", "contents": "Title: Coloring Hypergraphs Induced by Dynamic Point Sets and Bottomless\n  Rectangles Abstract: We consider a coloring problem on dynamic, one-dimensional point sets: points\nappearing and disappearing on a line at given times. We wish to color them with\nk colors so that at any time, any sequence of p(k) consecutive points, for some\nfunction p, contains at least one point of each color.\n  We prove that no such function p(k) exists in general. However, in the\nrestricted case in which points appear gradually, but never disappear, we give\na coloring algorithm guaranteeing the property at any time with p(k)=3k-2. This\ncan be interpreted as coloring point sets in R^2 with k colors such that any\nbottomless rectangle containing at least 3k-2 points contains at least one\npoint of each color. Here a bottomless rectangle is an axis-aligned rectangle\nwhose bottom edge is below the lowest point of the set. For this problem, we\nalso prove a lower bound p(k)>ck, where c>1.67. Hence for every k there exists\na point set, every k-coloring of which is such that there exists a bottomless\nrectangle containing ck points and missing at least one of the k colors.\n  Chen et al. (2009) proved that no such function $p(k)$ exists in the case of\ngeneral axis-aligned rectangles. Our result also complements recent results\nfrom Keszegh and Palvolgyi on cover-decomposability of octants (2011, 2012). \n\n"}
{"id": "1302.6516", "contents": "Title: First Search for Dark Matter Annihilation in the Sun Using the ANTARES\n  Neutrino Telescope Abstract: A search for high-energy neutrinos coming from the direction of the Sun has\nbeen performed using the data recorded by the ANTARES neutrino telescope during\n2007 and 2008. The neutrino selection criteria have been chosen to maximize the\nselection of possible signals produced by the self-annihilation of weakly\ninteracting massive particles accumulated in the centre of the Sun with respect\nto the atmospheric background. After data unblinding, the number of neutrinos\nobserved towards the Sun was found to be compatible with background\nexpectations. The $90\\%$ CL upper limits in terms of spin-dependent and\nspin-independent WIMP-proton cross-sections are derived and compared to\npredictions of two supersymmetric models, CMSSM and MSSM-7. The ANTARES limits\nare competitive with those obtained by other neutrino observatories and are\nmore stringent than those obtained by direct search experiments for the\nspin-dependent WIMP-proton cross-section. \n\n"}
{"id": "1302.6648", "contents": "Title: Model on pulsed GeV radiation from magnetars Abstract: We discuss a possible scenario for radiation mechanism of pulsed GeV\ngamma-rays from magnetars. The magnetars have shown frequent X-ray bursts,\nwhich would be triggered by crust fractures and could release the energy of\norder of ~10^{41-42}erg. If the location of the crust cracking of the magnetic\nfield is close to the magnetic pole, part of the released energy may excite the\nAlfevn wave that can propagate into outer magnetosphere. The oscillation of the\nmagnetic field induces the available potential drop ~10^{15}Volts, which can\naccelerate the electrons and/or positrons to the Lorentz factor ~10^{7} in the\nouter magnetosphere. The curvature radiation process at outer magnetosphere can\nproduce GeV gamma-rays. If the radiation process is occurred above r~5x 10^7cm\nfrom the stellar surface, the emitted GeV gamma-rays can escape from the\npair-creation process with the X-rays and/or the magnetic field. The expected\nluminosity of the GeV emissions is order of L_{\\gamma}< 10^{35} erg/s, and the\nradiation process will last for a temporal scale of years. The expected pulse\nprofiles have a broad shape with sometimes sharp peaks. We apply the model to\nAXP 1E~2259+586. \n\n"}
{"id": "1303.0540", "contents": "Title: The Space of Solutions of Coupled XORSAT Formulae Abstract: The XOR-satisfiability (XORSAT) problem deals with a system of $n$ Boolean\nvariables and $m$ clauses. Each clause is a linear Boolean equation (XOR) of a\nsubset of the variables. A $K$-clause is a clause involving $K$ distinct\nvariables. In the random $K$-XORSAT problem a formula is created by choosing\n$m$ $K$-clauses uniformly at random from the set of all possible clauses on $n$\nvariables. The set of solutions of a random formula exhibits various\ngeometrical transitions as the ratio $\\frac{m}{n}$ varies.\n  We consider a {\\em coupled} $K$-XORSAT ensemble, consisting of a chain of\nrandom XORSAT models that are spatially coupled across a finite window along\nthe chain direction. We observe that the threshold saturation phenomenon takes\nplace for this ensemble and we characterize various properties of the space of\nsolutions of such coupled formulae. \n\n"}
{"id": "1303.1026", "contents": "Title: Non-overlapping codes Abstract: We say that a $q$-ary length $n$ code is \\emph{non-overlapping} if the set of\nnon-trivial prefixes of codewords and the set of non-trivial suffices of\ncodewords are disjoint. These codes were first studied by Levenshtein in 1964,\nmotivated by applications in synchronisation. More recently these codes were\nindependently invented (under the name \\emph{cross-bifix-free} codes) by\nBaji\\'c and Stojanovi\\'c.\n  We provide a simple construction for a class of non-overlapping codes which\nhas optimal cardinality whenever $n$ divides $q$. Moreover, for all parameters\n$n$ and $q$ we show that a code from this class is close to optimal, in the\nsense that it has cardinality within a constant factor of an upper bound due to\nLevenshtein from 1970. Previous constructions have cardinality within a\nconstant factor of the upper bound only when $q$ is fixed.\n  Chee, Kiah, Purkayastha and Wang showed that a $q$-ary length $n$\nnon-overlapping code contains at most $q^n/(2n-1)$ codewords; this bound is\nweaker than the Levenshtein bound. Their proof appealed to the application in\nsynchronisation: we provide a direct combinatorial argument to establish the\nbound of Chee \\emph{et al}.\n  We also consider codes of short length, finding the leading term of the\nmaximal cardinality of a non-overlapping code when $n$ is fixed and\n$q\\rightarrow \\infty$. The largest cardinality of non-overlapping codes of\nlengths $3$ or less is determined exactly. \n\n"}
{"id": "1303.2612", "contents": "Title: Quasithermal Neutrinos from Rotating Protoneutron Stars Born during Core\n  Collapse of Massive Stars Abstract: Rotating and magnetized protoneutron stars (PNSs) may drive relativistic\nmagneto-centrifugally accelerated winds as they cool immediately after core\ncollapse. The wind fluid near the star is composed of neutrons and protons, and\nthe neutrons become relativistic while collisionally coupled with the ions.\nHere, we argue that the neutrons in the flow eventually undergo inelastic\ncollisions around the termination shock inside the stellar material, producing\n~0.1-1 GeV neutrinos, without relying on cosmic-ray acceleration mechanisms.\nEven higher-energy neutrinos may be produced via particle acceleration\nmechanisms. We show that PINGU and Hyper-Kamiokande can detect such neutrinos\nfrom nearby core-collapse supernovae, by reducing the atmospheric neutrino\nbackground via coincident detection of MeV neutrinos or gravitational waves and\noptical observations. Detection of these GeV and/or higher-energy neutrinos\nwould provide important clues to the physics of magnetic acceleration,\nnucleosynthesis, the relation between supernovae and gamma-ray bursts, and the\nproperties of newly born neutron stars. \n\n"}
{"id": "1303.2963", "contents": "Title: A Competitive Ratio Approximation Scheme for the k-Server Problem in\n  Fixed Finite Metrics Abstract: We show how to restrict the analysis of a class of online problems that\nincludes the $k$-server problem in finite metrics such that we only have to\nconsider finite sequences of request. When applying the restrictions, both the\noptimal offline solutions and the best possible deterministic or randomized\nonline solutions only differ by at most an arbitrarily small constant factor\nfrom the corresponding solutions without restrictions. Furthermore, we show how\nto obtain an algorithm with best possible deterministic or randomized\ncompetitive ratio for the restricted setup. Thus, for each fixed finite metrics\nour result qualifies as a competitive ratio approximation scheme as defined by\nG\\\"unther et al. \n\n"}
{"id": "1303.3428", "contents": "Title: Constraints on Scalar Spectral Index from Latest Observational\n  Measurements Abstract: Recently, the nine-year data release of the Wilkinson Microwave Anisotropy\nProbe (WMAP9) found that the inflationary models with the scalar spectral index\nn_s \\geq 1 are excluded at about 5\\sigma confidence level. In this paper, we\nset the new limits on the scalar spectral index in different cosmological\nmodels combining the WMAP9 data with the small-scale cosmic microwave\nbackground measurement from the South Pole Telescope, baryon acoustic\noscillation data, Hubble Telescope measurements of the Hubble constant, and\nsupernovae luminosity distance data. In most of extended cosmological models,\ne.g. with a dark energy equation of state, the constraints on n_s do not change\nsignificantly. The Harrison-Zel'dovich-Peebles (HZ) scale invariant spectrum is\nstill disfavored at more than 4\\sigma confidence level. However, when\nconsidering the model with a number of relativistic species N_{eff}, we obtain\nthe limit on the spectral index of n_s=0.980\\pm0.011 (1\\sigma), due to the\nstrong degeneracy between n_s and N_{eff}. The HZ spectrum now is still\nconsistent with the current data at 95% confidence level. \n\n"}
{"id": "1303.3585", "contents": "Title: Unveiling the nature of the unidentified gamma-ray sources II: radio,\n  infrared and optical counterparts of the gamma-ray blazar candidates Abstract: A significant fraction (~30%) of the high-energy gamma-ray sources listed in\nthe second Fermi LAT catalog (2FGL) are still of unknown origin, being not yet\nassociated with counterparts at low energies. We recently developed a new\nassociation method to identify if there is a gamma-ray blazar candidate within\nthe positional uncertainty region of a generic 2FGL source. This method is\nentirely based on the discovery that blazars have distinct infrared colors with\nrespect to other extragalactic sources found thanks, to the Wide-field Infrared\nSurvey Explorer (WISE) all-sky observations. Several improvements have been\nalso performed to increase the efficiency of our method in recognizing\ngamma-ray blazar candidates. In this paper we applied our method to two\ndifferent samples, the first constituted by the unidentified gamma-ray sources\n(UGSs) while the second by the active galaxies of uncertain type (AGUs), both\nlisted in the 2FGL. We present a catalog of IR counterparts for ~20% of the\nUGSs investigated. Then, we also compare our results on the associated sources\nwith those present in literature. In addition, we illustrate the extensive\narchival research carried out to identify the radio, infrared, optical and\nX-ray counterparts of the WISE selected, gamma-ray blazar candidates. Finally,\nwe discuss the future developments of our method based on ground-based\nfollow-up observations. \n\n"}
{"id": "1303.3793", "contents": "Title: The Distribution of Ramsey Numbers Abstract: We prove that the number of integers in the interval [0,x] that are\nnon-trivial Ramsey numbers r(k,n) (3 <= k <= n) has order of magnitude (x ln\nx)**(1/2). \n\n"}
{"id": "1303.4349", "contents": "Title: Finding all Convex Cuts of a Plane Graph in Polynomial Time Abstract: Convexity is a notion that has been defined for subsets of $\\RR^n$ and for\nsubsets of general graphs. A convex cut of a graph $G=(V, E)$ is a\n$2$-partition $V_1 \\dot{\\cup} V_2=V$ such that both $V_1$ and $V_2$ are convex,\n\\ie shortest paths between vertices in $V_i$ never leave $V_i$, $i \\in \\{1,\n2\\}$. Finding convex cuts is $\\mathcal{NP}$-hard for general graphs. To\ncharacterize convex cuts, we employ the Djokovic relation, a reflexive and\nsymmetric relation on the edges of a graph that is based on shortest paths\nbetween the edges' end vertices.\n  It is known for a long time that, if $G$ is bipartite and the Djokovic\nrelation is transitive on $G$, \\ie $G$ is a partial cube, then the cut-sets of\n$G$'s convex cuts are precisely the equivalence classes of the Djokovic\nrelation. In particular, any edge of $G$ is contained in the cut-set of exactly\none convex cut. We first characterize a class of plane graphs that we call {\\em\nwell-arranged}. These graphs are not necessarily partial cubes, but any edge of\na well-arranged graph is contained in the cut-set(s) of at least one convex\ncut. We also present an algorithm that uses the Djokovic relation for computing\nall convex cuts of a (not necessarily plane) bipartite graph in $\\bigO(|E|^3)$\ntime. Specifically, a cut-set is the cut-set of a convex cut if and only if the\nDjokovic relation holds for any pair of edges in the cut-set.\n  We then characterize the cut-sets of the convex cuts of a general graph $H$\nusing two binary relations on edges: (i) the Djokovic relation on the edges of\na subdivision of $H$, where any edge of $H$ is subdivided into exactly two\nedges and (ii) a relation on the edges of $H$ itself that is not the Djokovic\nrelation. Finally, we use this characterization to present the first algorithm\nfor finding all convex cuts of a plane graph in polynomial time. \n\n"}
{"id": "1304.0730", "contents": "Title: Representation, Approximation and Learning of Submodular Functions Using\n  Low-rank Decision Trees Abstract: We study the complexity of approximate representation and learning of\nsubmodular functions over the uniform distribution on the Boolean hypercube\n$\\{0,1\\}^n$. Our main result is the following structural theorem: any\nsubmodular function is $\\epsilon$-close in $\\ell_2$ to a real-valued decision\ntree (DT) of depth $O(1/\\epsilon^2)$. This immediately implies that any\nsubmodular function is $\\epsilon$-close to a function of at most\n$2^{O(1/\\epsilon^2)}$ variables and has a spectral $\\ell_1$ norm of\n$2^{O(1/\\epsilon^2)}$. It also implies the closest previous result that states\nthat submodular functions can be approximated by polynomials of degree\n$O(1/\\epsilon^2)$ (Cheraghchi et al., 2012). Our result is proved by\nconstructing an approximation of a submodular function by a DT of rank\n$4/\\epsilon^2$ and a proof that any rank-$r$ DT can be $\\epsilon$-approximated\nby a DT of depth $\\frac{5}{2}(r+\\log(1/\\epsilon))$.\n  We show that these structural results can be exploited to give an\nattribute-efficient PAC learning algorithm for submodular functions running in\ntime $\\tilde{O}(n^2) \\cdot 2^{O(1/\\epsilon^{4})}$. The best previous algorithm\nfor the problem requires $n^{O(1/\\epsilon^{2})}$ time and examples (Cheraghchi\net al., 2012) but works also in the agnostic setting. In addition, we give\nimproved learning algorithms for a number of related settings.\n  We also prove that our PAC and agnostic learning algorithms are essentially\noptimal via two lower bounds: (1) an information-theoretic lower bound of\n$2^{\\Omega(1/\\epsilon^{2/3})}$ on the complexity of learning monotone\nsubmodular functions in any reasonable model; (2) computational lower bound of\n$n^{\\Omega(1/\\epsilon^{2/3})}$ based on a reduction to learning of sparse\nparities with noise, widely-believed to be intractable. These are the first\nlower bounds for learning of submodular functions over the uniform\ndistribution. \n\n"}
{"id": "1304.0988", "contents": "Title: Average Case and Distributional Analysis of Dual-Pivot Quicksort Abstract: In 2009, Oracle replaced the long-serving sorting algorithm in its Java 7\nruntime library by a new dual-pivot Quicksort variant due to Vladimir\nYaroslavskiy. The decision was based on the strikingly good performance of\nYaroslavskiy's implementation in running time experiments. At that time, no\nprecise investigations of the algorithm were available to explain its superior\nperformance - on the contrary: Previous theoretical studies of other dual-pivot\nQuicksort variants even discouraged the use of two pivots. Only in 2012, two of\nthe authors gave an average case analysis of a simplified version of\nYaroslavskiy's algorithm, proving that savings in the number of comparisons are\npossible. However, Yaroslavskiy's algorithm needs more swaps, which renders the\nanalysis inconclusive.\n  To force the issue, we herein extend our analysis to the fully detailed style\nof Knuth: We determine the exact number of executed Java Bytecode instructions.\nSurprisingly, Yaroslavskiy's algorithm needs sightly more Bytecode instructions\nthan a simple implementation of classic Quicksort - contradicting observed\nrunning times. Like in Oracle's library implementation we incorporate the use\nof Insertionsort on small subproblems and show that it indeed speeds up\nYaroslavskiy's Quicksort in terms of Bytecodes; but even with optimal\nInsertionsort thresholds the new Quicksort variant needs slightly more Bytecode\ninstructions on average.\n  Finally, we show that the (suitably normalized) costs of Yaroslavskiy's\nalgorithm converge to a random variable whose distribution is characterized by\na fixed-point equation. From that, we compute variances of costs and show that\nfor large n, costs are concentrated around their mean. \n\n"}
{"id": "1304.1007", "contents": "Title: Linear-in-$\\Delta$ Lower Bounds in the LOCAL Model Abstract: By prior work, there is a distributed algorithm that finds a maximal\nfractional matching (maximal edge packing) in $O(\\Delta)$ rounds, where\n$\\Delta$ is the maximum degree of the graph. We show that this is optimal:\nthere is no distributed algorithm that finds a maximal fractional matching in\n$o(\\Delta)$ rounds.\n  Our work gives the first linear-in-$\\Delta$ lower bound for a natural graph\nproblem in the standard model of distributed computing---prior lower bounds for\na wide range of graph problems have been at best logarithmic in $\\Delta$. \n\n"}
{"id": "1304.3935", "contents": "Title: Bidirectional Collision Detection and Faster Deterministic Isomorphism\n  Testing Abstract: In this work, we introduce bidirectional collision detection --- a new\nalgorithmic tool that applies to the collision problems that arise in many\nisomorphism problems. For the group isomorphism problem, we show that\nbidirectional collision detection yields a deterministic n^((1 / 2) log n +\nO(1)) time algorithm whereas previously the n^(log n + O(1))\ngenerator-enumeration algorithm was the best result for several decades. For\nthe hard special case of solvable groups, we combine bidirectional collision\ndetection with methods from the author's previous work to obtain a\ndeterministic square-root speedup over the best previous algorithm. We also\nshow a deterministic square-root speedup over the best previous algorithm for\ntesting isomorphism of rings. We can even apply bidirectional collision\ndetection to the graph isomorphism problem to obtain a deterministic T^(1 /\nsqrt(2)) speedup over the best previous deterministic algorithm. Although the\nspace requirements for our algorithms are greater than those for previous\ndeterministic isomorphism tests, we show time-space tradeoffs that interpolate\nbetween the resource requirements of our algorithms and previous work. \n\n"}
{"id": "1304.4023", "contents": "Title: Discovery of high and very high-energy emission from the BL Lac object\n  SHBL J001355.9-185406 Abstract: The detection of the high-frequency peaked BL Lac object (HBL) SHBL\nJ001355.9-185406 ($z$=0.095) at high (HE; 100 MeV$<$E$<$300 GeV) and very\nhigh-energy (VHE; $E>100\\,{\\rm GeV}$) with the \\fer\\ Large Area Telescope (LAT)\nand the High Energy Stereoscopic System (H.E.S.S.) is reported. Dedicated\nobservations have been performed with the H.E.S.S. telescopes, leading to a\ndetection at the $5.5\\,\\sigma$ significance level. The measured flux above 310\nGeV is $(8.3 \\pm 1.7_{\\rm{stat}}\\pm 1.7_{\\rm{sys}})\\times 10^{-13}$ photons\n\\cms\\ (about 0.6% of that of the Crab Nebula), and the power law spectrum has a\nphoton index of \\indexHESS. Using 3.5 years of publicly available \\fla\\ data, a\nfaint counterpart has been detected in the LAT data at the $5.5\\,\\sigma$\nsignificance level, with an integrated flux above 300 MeV of $(9.3 \\pm 3.4_{\\rm\nstat} \\pm 0.8_{\\rm sys})\\times 10^{-10}$ photons \\cms\\ and a photon index of\n$\\Gamma = 1.96 \\pm 0.20_{\\rm stat} \\pm 0.08_{\\rm sys}$. X-ray observations with\n\\textit{Swift}-XRT allow the synchrotron peak energy in $\\nu F_\\nu$\nrepresentation to be located at $\\sim 1.0\\,{\\rm keV}$. The broadband spectral\nenergy distribution is modelled with a one-zone synchrotron self-Compton (SSC)\nmodel and the optical data by a black-body emission describing the thermal\nemission of the host galaxy. The derived parameters are typical for HBLs\ndetected at VHE, with a particle dominated jet. \n\n"}
{"id": "1304.4626", "contents": "Title: Efficient Computation of Representative Sets with Applications in\n  Parameterized and Exact Algorithms Abstract: We give two algorithms computing representative families of linear and\nuniform matroids and demonstrate how to use representative families for\ndesigning single-exponential parameterized and exact exponential time\nalgorithms. The applications of our approach include\n  - LONGEST DIRECTED CYCLE\n  - MINIMUM EQUIVALENT GRAPH (MEG)\n  - Algorithms on graphs of bounded treewidth\n  -k-PATH, k-TREE, and more generally, k-SUBGRAPH ISOMORPHISM, where the\nk-vertex pattern graph is of constant treewidth. \n\n"}
{"id": "1304.5715", "contents": "Title: The distribution of second degrees in the Buckley-Osthus random graph\n  model Abstract: In this paper we consider a well-known generalization of the Barab\\'asi and\nAlbert preferential attachment model - the Buckley-Osthus model. Buckley and\nOsthus proved that in this model the degree sequence has a power law\ndistribution. As a natural (and arguably more interesting) next step, we study\nthe second degrees of vertices. Roughly speaking, the second degree of a vertex\nis the number of vertices at distance two from this vertex. The distribution of\nsecond degrees is of interest because it is a good approximation of PageRank,\nwhere the importance of a vertex is measured by taking into account the\npopularity of its neighbors.\n  We prove that the second degrees also obey a power law. More precisely, we\nestimate the expectation of the number of vertices with the second degree\ngreater than or equal to k and prove the concentration of this random variable\naround its expectation using the now-famous Talagrand's concentration\ninequality over product spaces. As far as we know this is the only application\nof Talagrand's inequality to random web graphs, where the (preferential\nattachment) edges are not defined over a product distribution, making the\napplication nontrivial, and requiring certain novelty. \n\n"}
{"id": "1304.7577", "contents": "Title: Optimal amortized regret in every interval Abstract: Consider the classical problem of predicting the next bit in a sequence of\nbits. A standard performance measure is {\\em regret} (loss in payoff) with\nrespect to a set of experts. For example if we measure performance with respect\nto two constant experts one that always predicts 0's and another that always\npredicts 1's it is well known that one can get regret $O(\\sqrt T)$ with respect\nto the best expert by using, say, the weighted majority algorithm. But this\nalgorithm does not provide performance guarantee in any interval. There are\nother algorithms that ensure regret $O(\\sqrt {x \\log T})$ in any interval of\nlength $x$. In this paper we show a randomized algorithm that in an amortized\nsense gets a regret of $O(\\sqrt x)$ for any interval when the sequence is\npartitioned into intervals arbitrarily. We empirically estimated the constant\nin the $O()$ for $T$ upto 2000 and found it to be small -- around 2.1. We also\nexperimentally evaluate the efficacy of this algorithm in predicting high\nfrequency stock data. \n\n"}
{"id": "1305.0325", "contents": "Title: High velocity HI is not associated with TeV supernova remnant W51C Abstract: The recently-detected TeV gamma-ray source HESS J1923+141 coincides with\nSupernova Remnant (SNR) W51C and the star forming region W51B of the W51\ncomplex. We construct HI absorption spectra to SNR W51C, HII regions G49.2-0.35\nand G49.1-0.38 in W51B, and a nearby compact extragalactic source. Our study\ndetects high-velocity (HV) HI clouds (above 83 km/s) which coincide with W51B,\nbut finds that the clouds are behind W51B. Both W51C and G49.2-0.35 have have\nsimilar highest-velocity absorption features at ~70 km/s. The HII region\nG49.1-0.38 is behind the SNR because its HI absorption spectrum has a feature\nat 83 km/s. These new results argue against previous claims that the SNR has\nshocked the HV HI clouds. Therefore the TeV emission from the complex should\nnot be associated with the HV HI clouds. W51C has a distance of about 4.3 kpc,\nsmaller than the tangent point distance of 5.5 kpc in that direction, but still\nin the Sagittarius spiral arm. \n\n"}
{"id": "1305.0505", "contents": "Title: On k-visibility graphs Abstract: We examine several types of visibility graphs in which sightlines can pass\nthrough $k$ objects. For $k \\geq 1$ we bound the maximum thickness of semi-bar\n$k$-visibility graphs between $\\lceil \\frac{2}{3} (k + 1) \\rceil$ and $2k$. In\naddition we show that the maximum number of edges in arc and circle\n$k$-visibility graphs on $n$ vertices is at most $(k+1)(3n-k-2)$ for $n > 4k+4$\nand ${n \\choose 2}$ for $n \\leq 4k+4$, while the maximum chromatic number is at\nmost $6k+6$. In semi-arc $k$-visibility graphs on $n$ vertices, we show that\nthe maximum number of edges is ${n \\choose 2}$ for $n \\leq 3k+3$ and at most\n$(k+1)(2n-\\frac{k+2}{2})$ for $n > 3k+3$, while the maximum chromatic number is\nat most $4k+4$. \n\n"}
{"id": "1305.1805", "contents": "Title: The solution to the challenge in \"Time-Reversible Random Number\n  Generators\" by Wm. G. Hoover and Carol G. Hoover Abstract: I provide the algorithm that solves the challenge proposed by Wm. G. Hoover\nand Carol G. Hoover in their recent paper \"Time-Reversible Random Number\nGenerators\", arXiv:1305.0961, with an explanation on how to derive it\nanalytically. \n\n"}
{"id": "1305.3916", "contents": "Title: Timing and spectral study of the Be XRB IGR J11305-6256: Swift discovers\n  the orbital period and a soft X-ray excess Abstract: IGR J11305-6256 is one of the numerous sources discovered through the\nINTEGRAL scan of the Galactic Plane. Thanks to the Swift-BAT survey, that\nallows the frequent sampling of any sky region, we have discovered in the hard\nX-ray emission of this source a modulation with a period of ~120.83 d. The\nsignificance of this periodic modulation is ~4 standard deviations in Gaussian\nstatistics. We interpret it as the orbital period of the binary system. We\nderive an orbital separation between IGR J11305-6256 and its companion star of\n~286 R$_{\\odot}$ corresponding to ~19 times the radius of the companion star.\nThe broadband XRT-BAT (0.3-150 keV) spectrum is described either by the sum of\na black-body and a cut-off power-law or by a partially absorbed cut-off\npower-law. The temporal and spectral characteristics of the source indicate its\npossible association with the class of persistent, but faint, Be X-ray binary\nsystems. \n\n"}
{"id": "1305.4032", "contents": "Title: Radiative Mechanisms in GRB prompt emission Abstract: Motivated by the Fermi gamma-ray space telescope results, in recent years\nimmense efforts were given to understanding the mechanism that leads to the\nprompt emission observed. The failure of the optically thin emission models\n(synchrotron and synchrotron self Compton) increased interest in alternative\nmodels. Optically thick models, while having several advantages, also face\ndifficulty in capturing several key observables. Theoretical efforts are\nfocused in two main directions: (1) mechanisms that act to broaden the Planck\nspectrum; and (2) combining the optically thin and optically thick models to a\nhybrid model that could explain the key observables. \n\n"}
{"id": "1305.4237", "contents": "Title: Independent set in categorical products of cographs and splitgraphs Abstract: We show that there are polynomial-time algorithms to compute maximum\nindependent sets in the categorical products of two cographs and two\nsplitgraphs. We show that the ultimate categorical independence ratio is\ncomputable in polynomial time for cographs. \n\n"}
{"id": "1305.4306", "contents": "Title: Quasinormal ringing of Kerr black holes. II. Excitation by particles\n  falling radially with arbitrary energy Abstract: The analytical understanding of quasinormal mode ringing requires an accurate\nknowledge of the Green's function describing the response of the black hole to\nexternal perturbations. We carry out a comprehensive study of quasinormal mode\nexcitation for Kerr black holes. Relying on the formalism developed by Mano,\nSuzuki and Takasugi, we improve and extend previous calculations of the\nquasinormal mode residues in the complex frequency plane (\"excitation factors\"\nB_q). Using these results we compute the \"excitation coefficients\" C_q\n(essentially the mode amplitudes) in the special case where the source of the\nperturbations is a particle falling into the black hole along the symmetry\naxis. We compare this calculation with numerical integrations of the\nperturbation equations, and we show quantitatively how the addition of higher\novertones improves the agreement with the numerical waveforms. Our results\nshould find applications in models of the ringdown stage and in the\nconstruction of semianalytical template banks for gravitational-wave detectors,\nespecially for binaries with large mass ratios and/or fast-spinning black\nholes. \n\n"}
{"id": "1305.4308", "contents": "Title: Connected Domatic Packings in Node-capacitated Graphs Abstract: A set of vertices in a graph is a dominating set if every vertex outside the\nset has a neighbor in the set. A dominating set is connected if the subgraph\ninduced by its vertices is connected. The connected domatic partition problem\nasks for a partition of the nodes into connected dominating sets. The connected\ndomatic number of a graph is the size of a largest connected domatic partition\nand it is a well-studied graph parameter with applications in the design of\nwireless networks. In this note, we consider the fractional counterpart of the\nconnected domatic partition problem in \\emph{node-capacitated} graphs. Let $n$\nbe the number of nodes in the graph and let $k$ be the minimum capacity of a\nnode separator in $G$. Fractionally we can pack at most $k$ connected\ndominating sets subject to the capacities on the nodes, and our algorithms\nconstruct packings whose sizes are proportional to $k$. Some of our main\ncontributions are the following: \\begin{itemize} \\item An algorithm for\nconstructing a fractional connected domatic packing of size $\\Omega(k)$ for\nnode-capacitated planar and minor-closed families of graphs. \\item An algorithm\nfor constructing a fractional connected domatic packing of size $\\Omega(k /\n\\ln{n})$ for node-capacitated general graphs. \\end{itemize} \n\n"}
{"id": "1305.4696", "contents": "Title: Tight Bounds for Set Disjointness in the Message Passing Model Abstract: In a multiparty message-passing model of communication, there are $k$\nplayers. Each player has a private input, and they communicate by sending\nmessages to one another over private channels. While this model has been used\nextensively in distributed computing and in multiparty computation, lower\nbounds on communication complexity in this model and related models have been\nsomewhat scarce. In recent work \\cite{phillips12,woodruff12,woodruff13}, strong\nlower bounds of the form $\\Omega(n \\cdot k)$ were obtained for several\nfunctions in the message-passing model; however, a lower bound on the classical\nSet Disjointness problem remained elusive.\n  In this paper, we prove tight lower bounds of the form $\\Omega(n \\cdot k)$\nfor the Set Disjointness problem in the message passing model. Our bounds are\nobtained by developing information complexity tools in the message-passing\nmodel, and then proving an information complexity lower bound for Set\nDisjointness. As a corollary, we show a tight lower bound for the task\nallocation problem \\cite{DruckerKuhnOshman} via a reduction from Set\nDisjointness. \n\n"}
{"id": "1305.4710", "contents": "Title: Closing in on the Fermi Line with a New Observation Strategy Abstract: Evidence for a spectral line in the inner Galaxy has caused a great deal of\nexcitement over the last year, mainly because of its interpretation as a\npossible dark matter signal. The observation has raised important questions\nabout statistics and suspicions about systematics, especially in photons from\nthe Earth limb. With enough additional data, we can address these concerns. In\nthis white paper, we summarize the current observational situation and project\nfuture sensitivities, finding that the status quo is dangerously close to\nleaving the issue unresolved until 2015. We advocate a change in survey\nstrategy that more than doubles the data rate in the inner Galaxy, and is\nrelatively non-disruptive to other survey science. This strategy will clearly\nseparate the null hypothesis from the line signal hypothesis and provide ample\nlimb data for systematics checks by the end of 2014. The standard survey mode\nmay not. \n\n"}
{"id": "1305.5855", "contents": "Title: First Test of High Frequency Gravity Waves from Inflation using ADVANCED\n  LIGO Abstract: Inflation models ending in a first order phase transition produce\ngravitational waves (GW) via bubble collisions of the true vacuum phase. We\ndemonstrate that these bubble collisions can leave an observable signature in\nAdvanced LIGO, an upcoming ground-based GW experiment. These GW are dependent\non two parameters of the inflationary model: $\\varepsilon$ represents the\nenergy difference between the false vacuum and the true vacuum of the inflaton\npotential, and $\\chi$ measures how fast the phase transition ends ($\\chi \\sim$\nthe number of e-folds during the actual phase transition). Advanced LIGO will\nbe able to test the validity of single-phase transition models within the\nparameter space $10^7 \\rm{GeV}\\lesssim \\varepsilon^{1/4} \\lesssim 10^{10}\n\\rm{GeV}$ and $0.19 \\lesssim \\chi \\lesssim 1$. If inflation occurred through a\nfirst order phase transition, then Advanced LIGO could be the first to discover\nhigh frequency GW from inflation. \n\n"}
{"id": "1305.6079", "contents": "Title: The Energy Spectrum of Ultra-High-Energy Cosmic Rays Measured by the\n  Telescope Array FADC Fluorescence Detectors in Monocular Mode Abstract: We present a measurement of the energy spectrum of ultra-high-energy cosmic\nrays performed by the Telescope Array experiment using monocular observations\nfrom its two new FADC-based fluorescence detectors. After a short description\nof the experiment, we describe the data analysis and event reconstruction\nprocedures. Since the aperture of the experiment must be calculated by Monte\nCarlo simulation, we describe this calculation and the comparisons of simulated\nand real data used to verify the validity of the aperture calculation. Finally,\nwe present the energy spectrum calculated from the merged monocular data sets\nof the two FADC-based detectors, and also the combination of this merged\nspectrum with an independent, previously published monocular spectrum\nmeasurement performed by Telescope Array's third fluorescence detector\n(Abu-Zayyad {\\it et al.}, {Astropart. Phys.} 39 (2012), 109). This combined\nspectrum corroborates the recently published Telescope Array surface detector\nspectrum (Abu-Zayyad {\\it et al.}, {Astrophys. Journ.} 768 (2013), L1) with\nindependent systematic uncertainties. \n\n"}
{"id": "1305.7376", "contents": "Title: Polynomial Gap Extensions of the Erd\\H{o}s-P\\'osa Theorem Abstract: Given a graph $H$, we denote by ${\\cal M}(H)$ all graphs that can be\ncontracted to $H$. The following extension of the Erd\\H{o}s-P\\'osa Theorem\nholds: for every $h$-vertex planar graph $H$, there exists a function $f_{H}$\nsuch that every graph $G$, either contains $k$ disjoint copies of graphs in\n${\\cal M}(H)$, or contains a set of $f_{H}(k)$ vertices meeting every subgraph\nof $G$ that belongs in ${\\cal M}(H)$. In this paper we prove that this is the\ncase for every graph $H$ of pathwidth at most 2 and, in particular, that\n$f_{H}(k) = 2^{O(h^2)}\\cdot k^{2}\\cdot \\log k$. As a main ingredient of the\nproof of our result, we show that for every graph $H$ on $h$ vertices and\npathwidth at most 2, either $G$ contains $k$ disjoint copies of $H$ as a minor\nor the treewidth of $G$ is upper-bounded by $2^{O(h^2)}\\cdot k^{2}\\cdot \\log\nk$. We finally prove that the exponential dependence on $h$ in these bounds can\nbe avoided if $H=K_{2,r}$. In particular, we show that $f_{K_{2,r}}=O(r^2\\cdot\nk^2)$ \n\n"}
{"id": "1306.4380", "contents": "Title: Interaction of Ultra Relativistic e- e+ Fireball Beam with Plasma Abstract: Ab initio simulations of the propagation in a plasma of a soon to be\navailable relativistic electron-positron beam or fireball beam provide an\neffective mean for the study of microphysics relevant to astrophysical\nscenarios. We show that the current filamentation instability associated with\nsome of these scenarios reaches saturation after only 10 cm of propagation in a\ntypical laboratory plasma with a density 10^17/cc. The different regimes of the\ninstability, from the purely transverse to the mixed mode filamentation, can be\naccessed by varying the background plasma density. The instability generates\nlarge local plasma gradients, intense transverse magnetic fields, and enhanced\nemission of radiation. We suggest that these effects may be observed\nexperimentally for the first time. \n\n"}
{"id": "1306.6556", "contents": "Title: Escape, capture, and levitation of matter in Eddington outbursts Abstract: Context: An impulsive increase in luminosity by one half or more of the\nEddington value will lead to ejection of all optically thin plasma from\nKeplerian orbits around the radiating star, if gravity is Newtonian and the\nPoynting-Robertson drag is neglected. Radiation drag may bring some particles\ndown to the stellar surface. On the other hand, general relativistic\ncalculations show that gravity may be balanced by a sufficiently intense\nradiation field at a certain distance from the star.\n  Aims: We investigate the motion of test particles around highly luminous\nstars to determine conditions under which plasma may be ejected from the\nsystem.\n  Results: In Einstein's gravity, if the outburst is close to the Eddington\nluminosity, all test particles orbiting outside an \"escape sphere\" will be\nejected from the system, while all others will be captured from their orbits\nonto the surface of another sphere, which is well above the stellar surface,\nand may even be outside the escape sphere, depending on the value of\nluminosity. Radiation drag will bring all the captured particles to rest on\nthis \"Eddington capture sphere,\" where they will remain suspended in an\nequilibrium state as long as the local flux of radiation does not change and\nremains at the effective Eddington value. \n\n"}
{"id": "1307.1757", "contents": "Title: Accuracy of gravitational waveform models for observing\n  neutron-star--black-hole binaries in Advanced LIGO Abstract: Gravitational waves radiated by the coalescence of compact-object binaries\ncontaining a neutron star and a black hole are one of the most interesting\nsources for the ground-based gravitational-wave observatories Advanced LIGO and\nAdvanced Virgo. Advanced LIGO will be sensitive to the inspiral of a $1.4\\,\nM_\\odot$ neutron star into a $10\\,M_\\odot$ black hole to a maximum distance of\n$\\sim 900$ Mpc. Achieving this sensitivity and extracting the physics imprinted\nin observed signals requires accurate modeling of the binary to construct\ntemplate waveforms. In a NSBH binary, the black hole may have significant\nangular momentum (spin), which affects the phase evolution of the emitted\ngravitational waves. We investigate the ability of post-Newtonian (PN)\ntemplates to model the gravitational waves emitted during the inspiral phase of\nNSBH binaries. We restrict the black hole's spin to be aligned with the orbital\nangular momentum and compare several approximants. We examine restricted\namplitude waveforms that are accurate to 3.5PN order in the orbital dynamics\nand complete to 2.5PN order in the spin dynamics. We also consider PN waveforms\nwith the recently derived 3.5PN spin-orbit and 3PN spin-orbit tail corrections.\nWe compare these approximants to the effective-one-body model. For all these\nmodels, large disagreements start at low to moderate black hole spins,\nparticularly for binaries where the spin is anti-aligned with the orbital\nangular momentum. We show that this divergence begins in the early inspiral at\n$v \\sim 0.2$ for $\\chi_{BH} \\sim 0.4$. PN spin corrections beyond those\ncurrently known will be required for optimal detection searches and to measure\nthe parameters of neutron star--black hole binaries. While this complicates\nsearches, the strong dependence of the gravitational-wave signal on the spin\ndynamics will make it possible to extract significant astrophysical\ninformation. \n\n"}
{"id": "1307.2187", "contents": "Title: Everything you always wanted to know about the parameterized complexity\n  of Subgraph Isomorphism (but were afraid to ask) Abstract: Given two graphs $H$ and $G$, the Subgraph Isomorphism problem asks if $H$ is\nisomorphic to a subgraph of $G$. While NP-hard in general, algorithms exist for\nvarious parameterized versions of the problem: for example, the problem can be\nsolved (1) in time $2^{O(|V(H)|)}\\cdot n^{O(\\tw(H))}$ using the color-coding\ntechnique of Alon, Yuster, and Zwick; (2) in time $f(|V(H)|,\\tw(G))\\cdot n$\nusing Courcelle's Theorem; (3) in time $f(|V(H)|,\\genus(G))\\cdot n$ using a\nresult on first-order model checking by Frick and Grohe; or (4) in time\n$f(\\maxdeg(H))\\cdot n^{O(\\tw(G)})$ for connected $H$ using the algorithm of\nMatou\\v{s}ek and Thomas. Already this small sample of results shows that the\nway an algorithm can depend on the parameters is highly nontrivial and subtle.\n  We develop a framework involving 10 relevant parameters for each of $H$ and\n$G$ (such as treewidth, pathwidth, genus, maximum degree, number of vertices,\nnumber of components, etc.), and ask if an algorithm with running time \\[\nf_1(p_1,p_2,..., p_\\ell)\\cdot n^{f_2(p_{\\ell+1},..., p_k)} \\] exist, where each\nof $p_1,..., p_k$ is one of the 10 parameters depending only on $H$ or $G$. We\nshow that {\\em all} the questions arising in this framework are answered by a\nset of 11 maximal positive results (algorithms) and a set of 17 maximal\nnegative results (hardness proofs); some of these results already appear in the\nliterature, while others are new in this paper.\n  On the algorithmic side, our study reveals for example that an unexpected\ncombination of bounded degree, genus, and feedback vertex set number of $G$\ngives rise to a highly nontrivial algorithm for Subgraph Isomorphism. On the\nhardness side, we present W[1]-hardness proofs under extremely restricted\nconditions, such as when $H$ is a bounded-degree tree of constant pathwidth and\n$G$ is a planar graph of bounded pathwidth. \n\n"}
{"id": "1307.3301", "contents": "Title: Optimal Bounds on Approximation of Submodular and XOS Functions by\n  Juntas Abstract: We investigate the approximability of several classes of real-valued\nfunctions by functions of a small number of variables ({\\em juntas}). Our main\nresults are tight bounds on the number of variables required to approximate a\nfunction $f:\\{0,1\\}^n \\rightarrow [0,1]$ within $\\ell_2$-error $\\epsilon$ over\nthe uniform distribution: 1. If $f$ is submodular, then it is $\\epsilon$-close\nto a function of $O(\\frac{1}{\\epsilon^2} \\log \\frac{1}{\\epsilon})$ variables.\nThis is an exponential improvement over previously known results. We note that\n$\\Omega(\\frac{1}{\\epsilon^2})$ variables are necessary even for linear\nfunctions. 2. If $f$ is fractionally subadditive (XOS) it is $\\epsilon$-close\nto a function of $2^{O(1/\\epsilon^2)}$ variables. This result holds for all\nfunctions with low total $\\ell_1$-influence and is a real-valued analogue of\nFriedgut's theorem for boolean functions. We show that $2^{\\Omega(1/\\epsilon)}$\nvariables are necessary even for XOS functions.\n  As applications of these results, we provide learning algorithms over the\nuniform distribution. For XOS functions, we give a PAC learning algorithm that\nruns in time $2^{poly(1/\\epsilon)} poly(n)$. For submodular functions we give\nan algorithm in the more demanding PMAC learning model (Balcan and Harvey,\n2011) which requires a multiplicative $1+\\gamma$ factor approximation with\nprobability at least $1-\\epsilon$ over the target distribution. Our uniform\ndistribution algorithm runs in time $2^{poly(1/(\\gamma\\epsilon))} poly(n)$.\nThis is the first algorithm in the PMAC model that over the uniform\ndistribution can achieve a constant approximation factor arbitrarily close to 1\nfor all submodular functions. As follows from the lower bounds in (Feldman et\nal., 2013) both of these algorithms are close to optimal. We also give\napplications for proper learning, testing and agnostic learning with value\nqueries of these classes. \n\n"}
{"id": "1307.4401", "contents": "Title: The Afterglow of GRB 130427A from 1 to 10^16 GHz Abstract: We present multiwavelength observations of the afterglow of GRB 130427A, the\nbrightest (in total fluence) gamma-ray burst of the past 29 years. Optical\nspectroscopy from Gemini-North reveals the redshift of the GRB to be z=0.340,\nindicating that its unprecedented brightness is primarily the result of its\nrelatively close proximity to Earth; the intrinsic luminosities of both the GRB\nand its afterglow are not extreme in comparison to other bright GRBs. We\npresent a large suite of multiwavelength observations spanning from 300 s to\n130 d after the burst and demonstrate that the afterglow shows relatively\nsimple, smooth evolution at all frequencies with no significant late-time\nflaring or rebrightening activity. The entire dataset from 1 GHz to 10 GeV can\nbe modeled as synchrotron emission from a combination of reverse and forward\nshocks in good agreement with the standard afterglow model, providing strong\nsupport to the applicability of the underlying theory and clarifying the nature\nof the GeV emission observed to last for minutes to hours following other very\nbright GRBs. A tenuous, wind-stratified circumburst density profile is required\nby the observations, suggesting a massive-star progenitor with a low mass-loss\nrate, perhaps due to low metallicity. GRBs similar in nature to GRB 130427A,\ninhabiting low-density media and exhibiting strong reverse shocks, are probably\nnot uncommon but may have been difficult to recognize in the past due to their\nrelatively faint late-time radio emission; more such events should be found in\nabundance by the new generation of sensitive radio and millimeter instruments. \n\n"}
{"id": "1307.5296", "contents": "Title: First-Come-First-Served for Online Slot Allocation and Huffman Coding Abstract: Can one choose a good Huffman code on the fly, without knowing the underlying\ndistribution? Online Slot Allocation (OSA) models this and similar problems:\nThere are n slots, each with a known cost. There are n items. Requests for\nitems are drawn i.i.d. from a fixed but hidden probability distribution p.\nAfter each request, if the item, i, was not previously requested, then the\nalgorithm (knowing the slot costs and the requests so far, but not p) must\nplace the item in some vacant slot j(i). The goal is to minimize the sum, over\nthe items, of the probability of the item times the cost of its assigned slot.\n  The optimal offline algorithm is trivial: put the most probable item in the\ncheapest slot, the second most probable item in the second cheapest slot, etc.\nThe optimal online algorithm is First Come First Served (FCFS): put the first\nrequested item in the cheapest slot, the second (distinct) requested item in\nthe second cheapest slot, etc. The optimal competitive ratios for any online\nalgorithm are 1+H(n-1) ~ ln n for general costs and 2 for concave costs. For\nlogarithmic costs, the ratio is, asymptotically, 1: FCFS gives cost opt + O(log\nopt).\n  For Huffman coding, FCFS yields an online algorithm (one that allocates\ncodewords on demand, without knowing the underlying probability distribution)\nthat guarantees asymptotically optimal cost: at most opt + 2 log(1+opt) + 2. \n\n"}
{"id": "1307.6068", "contents": "Title: Constraints on axion-like particles with H.E.S.S. from observations of\n  PKS 2155-304 Abstract: Axion-like particles are hypothetical new light (sub-eV) bosons predicted in\nsome extensions of the Standard Model of particle physics. In astrophysical\nenvironments comprising high-energy gamma rays and turbulent magnetic fields,\nthe existence of axion-like particles can modify the energy spectrum of the\ngamma rays. This modification would take the form of an irregular behavior of\nthe energy spectrum in a limited energy range. Data from the H.E.S.S.\nobservations of the distant BL Lac PKS 2155-304 are used to derive conservative\nupper limits on the strength of the axion-like particle coupling to photons.\nThis study gives rise to the first exclusions on axion-like particles from\ngamma-ray astronomy. The derived constraints apply to both light pseudo-scalar\nand scalar bosons that couple to the electromagnetic field. \n\n"}
{"id": "1307.7083", "contents": "Title: Gamma-ray binaries and related systems Abstract: After initial claims and a long hiatus, it is now established that several\nbinary stars emit high (0.1-100 GeV) and very high energy (>100 GeV) gamma\nrays. A new class has emerged called 'gamma-ray binaries', since most of their\nradiated power is emitted beyond 1 MeV. Accreting X-ray binaries, novae and a\ncolliding wind binary (eta Car) have also been detected - 'related systems'\nthat confirm the ubiquity of particle acceleration in astrophysical sources. Do\nthese systems have anything in common ? What drives their high-energy emission\n? How do the processes involved compare to those in other sources of gamma\nrays: pulsars, active galactic nuclei, supernova remnants ? I review the wealth\nof observational and theoretical work that have followed these detections, with\nan emphasis on gamma-ray binaries. I present the current evidence that\ngamma-ray binaries are driven by rotation-powered pulsars. Binaries are\nlaboratories giving access to different vantage points or physical conditions\non a regular timescale as the components revolve on their orbit. I explain the\nbasic ingredients that models of gamma-ray binaries use, the challenges that\nthey currently face, and how they can bring insights into the physics of\npulsars. I discuss how gamma-ray emission from microquasars provides a window\ninto the connection between accretion--ejection and acceleration, while eta Car\nand novae raise new questions on the physics of these objects - or on the\ntheory of diffusive shock acceleration. Indeed, explaining the gamma-ray\nemission from binaries strains our theories of high-energy astrophysical\nprocesses, by testing them on scales and in environments that were generally\nnot foreseen, and this is how these detections are most valuable. \n\n"}
{"id": "1308.0180", "contents": "Title: Space complexity of list H-colouring: a dichotomy Abstract: The Dichotomy Conjecture for constraint satisfaction problems (CSPs) states\nthat every CSP is in P or is NP-complete (Feder-Vardi, 1993). It has been\nverified for conservative problems (also known as list homomorphism problems)\nby A. Bulatov (2003). We augment this result by showing that for digraph\ntemplates H, every conservative CSP, denoted LHOM(H), is solvable in logspace\nor is hard for NL. More precisely, we introduce a digraph structure we call a\ncircular N, and prove the following dichotomy: if H contains no circular N then\nLHOM(H) admits a logspace algorithm, and otherwise LHOM(H) is hard for NL. Our\nalgorithm operates by reducing the lists in a complex manner based on a novel\ndecomposition of an auxiliary digraph, combined with repeated applications of\nReingold's algorithm for undirected reachability (2005). We also prove an\nalgebraic version of this dichotomy: the digraphs without a circular N are\nprecisely those that admit a finite chain of polymorphisms satisfying the\nHagemann-Mitschke identities. This confirms a conjecture of Larose and Tesson\n(2007) for LHOM(H). Moreover, we show that the presence of a circular N can be\ndecided in time polynomial in the size of H. \n\n"}
{"id": "1308.0820", "contents": "Title: Anisotropy studies with the Pierre Auger Observatory Abstract: We report recent results from the Pierre Auger Observatory about the study of\nthe anisotropy in the arrival directions of ultra-high energy cosmic rays. We\npresent the results of the search for a dipolar anisotropy at the EeV energy\nscale. Measurements of the phase and the amplitude of the first harmonic\nmodulation in the right-ascension distribution are discussed. For cosmic rays\nwith energies above 55 EeV, we present an update of the search for correlations\nbetween their arrival directions and the positions of active galactic nuclei\nfrom the Veron-Cetty and Veron catalog. We also discuss the results of\ncorrelation analyses applied to other populations of extragalactic objects.\nFinally we present the search for anisotropies in the data without the usage of\nastronomical catalogues. \n\n"}
{"id": "1308.1001", "contents": "Title: The diversity of progenitors and emission mechanisms for ultra-long\n  bursts Abstract: GRB 111209A is the longest ever recorded burst. This burst was detected by\nSwift and Konus-Wind, and we obtained TOO time from XMM-Newton as well as\nprompt data from TAROT. We made a common reduction using data from these\ninstruments together with other ones. This allows for the first time a precise\nstudy at high signal-to-noise ratio of the prompt to afterglow transition. We\nshow that several mechanisms are responsible of this phase. In its prompt\nphase, we show that its duration is longer than 20 000 seconds. This, combined\nwith the fact that the burst fluence is among the top 5% of what is observed\nfor other events, makes this event extremely energetic. We discuss the possible\nprogenitors that could explain the extreme duration properties of this burst as\nwell as its spectral properties. We present evidences that this burst belong to\na new, previously unidentified, class of GRBs. The most probable progenitor of\nthis new class is a low metalicity blue super-giant star. We show that\nselection effects could prevent the detection of other bursts at larger\nredshift and conclude that this kind of event is intrinsically rare in the\nlocal Universe. The afterglow presents similar features to other normal long\nGRBs and a late rebrightening in the optical wavelengths, as observed in other\nlong GRBs. A broad band SED from radio to X-rays at late times does not show\nsignificant deviations from the expected standard fireball afterglow\nsynchrotron emission. \n\n"}
{"id": "1308.1009", "contents": "Title: Sign Stable Projections, Sign Cauchy Projections and Chi-Square Kernels Abstract: The method of stable random projections is popular for efficiently computing\nthe Lp distances in high dimension (where 0<p<=2), using small space. Because\nit adopts nonadaptive linear projections, this method is naturally suitable\nwhen the data are collected in a dynamic streaming fashion (i.e., turnstile\ndata streams). In this paper, we propose to use only the signs of the projected\ndata and analyze the probability of collision (i.e., when the two signs\ndiffer). We derive a bound of the collision probability which is exact when p=2\nand becomes less sharp when p moves away from 2. Interestingly, when p=1 (i.e.,\nCauchy random projections), we show that the probability of collision can be\naccurately approximated as functions of the chi-square similarity. For example,\nwhen the (un-normalized) data are binary, the maximum approximation error of\nthe collision probability is smaller than 0.0192. In text and vision\napplications, the chi-square similarity is a popular measure for nonnegative\ndata when the features are generated from histograms. Our experiments confirm\nthat the proposed method is promising for large-scale learning applications. \n\n"}
{"id": "1308.1933", "contents": "Title: Chandra Survey of Nearby Highly Inclined Disc Galaxies - II: Correlation\n  Analysis of Galactic Coronal Properties Abstract: X-ray observations provide a key tool for exploring the properties of\ngalactic coronae and their formation processes. In an earlier paper, we have\npresented a Chandra data analysis of the coronae of 53 nearby highly-inclined\ndisc galaxies. Here we study the correlation of the X-ray measurements with\nother galaxy properties and compare the results with those obtained for\nelliptical galaxies. A good correlation is present between the coronal\nluminosity Lx and the SFR. But we find a better correlation between Lx and the\ntotal SN mechanical energy input rate (ESN), including the expected\ncontribution from core collapsed (CC) and Ia SNe. The X-ray radiation\nefficiency (eta=Lx/ESN) has a mean value of ~0.4% with an rms of ~0.5dex. eta\nfurther correlates with MTF/M* (MTF is the baryon mass measured from the\nrotation velocity and the Tully-Fisher relation, M* is the stellar mass\nmeasured from the K-band luminosity) and the CC SN rate surface density (FSN,\nin units of SN/yr/kpc^2), which can be characterized as: eta=0.41%MTF/M* and\neta=1.4%FSN^-0.3. These correlations reflect the roles played by the\ngravitational mass and energetic feedback concentrations in determining eta.\nThe characteristic temperature of the corona shows little dependence on the\ntotal or specific SFR, the cold gas content, or Lx. The coronae of disc\ngalaxies tend to be more X-ray luminous, hotter, and lower in Fe/O abundance\nratio than those of elliptical ones of similar masses. Early-type non-starburst\ndisc galaxies tend to be more Fe-rich, while starburst ones have a roughly\nconstant abundance ratio of Fe/O~0.36solar. Our results are consistent with the\ncoronal gas being mainly provided by stellar feedback in a mass range of\nM*~10^{8.7-11}Msun. In addition, processes such as charge exchange at cool/hot\ngas interfaces, as well as various environmental effects, are also needed to\nexplain the diversity of coronal properties. \n\n"}
{"id": "1308.3665", "contents": "Title: On Sparsification for Computing Treewidth Abstract: We investigate whether an n-vertex instance (G,k) of Treewidth, asking\nwhether the graph G has treewidth at most k, can efficiently be made sparse\nwithout changing its answer. By giving a special form of OR-cross-composition,\nwe prove that this is unlikely: if there is an e > 0 and a polynomial-time\nalgorithm that reduces n-vertex Treewidth instances to equivalent instances, of\nan arbitrary problem, with O(n^{2-e}) bits, then NP is in coNP/poly and the\npolynomial hierarchy collapses to its third level.\n  Our sparsification lower bound has implications for structural\nparameterizations of Treewidth: parameterizations by measures that do not\nexceed the vertex count, cannot have kernels with O(k^{2-e}) bits for any e >\n0, unless NP is in coNP/poly. Motivated by the question of determining the\noptimal kernel size for Treewidth parameterized by vertex cover, we improve the\nO(k^3)-vertex kernel from Bodlaender et al. (STACS 2011) to a kernel with\nO(k^2) vertices. Our improved kernel is based on a novel form of\ntreewidth-invariant set. We use the q-expansion lemma of Fomin et al. (STACS\n2011) to find such sets efficiently in graphs whose vertex count is\nsuperquadratic in their vertex cover number. \n\n"}
{"id": "1308.3946", "contents": "Title: Optimal Algorithms for Testing Closeness of Discrete Distributions Abstract: We study the question of closeness testing for two discrete distributions.\nMore precisely, given samples from two distributions $p$ and $q$ over an\n$n$-element set, we wish to distinguish whether $p=q$ versus $p$ is at least\n$\\eps$-far from $q$, in either $\\ell_1$ or $\\ell_2$ distance. Batu et al. gave\nthe first sub-linear time algorithms for these problems, which matched the\nlower bounds of Valiant up to a logarithmic factor in $n$, and a polynomial\nfactor of $\\eps.$\n  In this work, we present simple (and new) testers for both the $\\ell_1$ and\n$\\ell_2$ settings, with sample complexity that is information-theoretically\noptimal, to constant factors, both in the dependence on $n$, and the dependence\non $\\eps$; for the $\\ell_1$ testing problem we establish that the sample\ncomplexity is $\\Theta(\\max\\{n^{2/3}/\\eps^{4/3}, n^{1/2}/\\eps^2 \\}).$ \n\n"}
{"id": "1308.4332", "contents": "Title: Broadband monitoring tracing the evolution of the jet and disk in the\n  black hole candidate X-ray binary MAXI J1659-152 Abstract: MAXI J1659-152 was discovered on 2010 September 25 as a new X-ray transient,\ninitially identified as a gamma-ray burst, but was later shown to be a new\nX-ray binary with a black hole as the most likely compact object. Dips in the\nX-ray light curves have revealed that MAXI J1659-152 is the shortest period\nblack hole candidate identified to date. Here we present the results of a large\nobserving campaign at radio, sub-millimeter, near-infrared (nIR), optical and\nultraviolet (UV) wavelengths. We have combined this very rich data set with the\navailable X-ray observations to compile a broadband picture of the evolution of\nthis outburst. We have performed broadband spectral modeling, demonstrating the\npresence of a spectral break at radio frequencies and a relationship between\nthe radio spectrum and X-ray states. Also, we have determined physical\nparameters of the accretion disk and put them into context with respect to the\nother parameters of the binary system. Finally, we have investigated the\nradio-X-ray and nIR/optical/UV-X-ray correlations up to ~3 years after the\noutburst onset to examine the link between the jet and the accretion disk, and\nfound that there is no significant jet contribution to the nIR emission when\nthe source is in the soft or intermediate X-ray spectral state, consistent with\nour detection of the jet break at radio frequencies during these states. \n\n"}
{"id": "1308.4574", "contents": "Title: The effects of thermodynamic stability on wind properties in different\n  low mass black hole binary states Abstract: We present a systematic theory-motivated study of the thermodynamic stability\ncondition as an explanation for the observed accretion disk wind signatures in\ndifferent states of low mass black hole binaries (BHB). The variability in\nobserved ions is conventionally explained either by variations in the driving\nmechanisms or the changes in the ionizing flux or due to density effects,\nwhilst thermodynamic stability considerations have been largely ignored. It\nwould appear that the observability of particular ions in different BHB states\ncan be accounted for through simple thermodynamic considerations in the static\nlimit. Our calculations predict that in the disk dominated soft thermal and\nintermediate states, the wind should be thermodynamically stable and hence\nobservable. On the other hand, in the powerlaw dominated spectrally hard state\nthe wind is found to be thermodynamically unstable for a certain range of 3.55\n<= log \\xi <= 4.20. In the spectrally hard state, a large number of the He-like\nand H-like ions (including e.g. Fe XXV, Ar XVIII and S XV) have peak ion\nfractions in the unstable ionization parameter (\\xi) range, making these ions\nundetectable. Our theoretical predictions have clear corroboration in the\nliterature reporting differences in wind ion observability as the BHBs\ntransition through the accretion states Lee et al. 2002; Miller et al. 2008;\nNeilsen & Lee 2009; Blum et al. 2010; Ponti et al. 2012; Neilsen & Homan 2012).\nWhile this effect may not be the only one responsible for the observed gradient\nin the wind properties as a function of the accretion state in BHBs, it is\nclear that its inclusion in the calculations is crucial to understanding the\nlink between the environment of the compact object and its accretion processes. \n\n"}
{"id": "1308.6281", "contents": "Title: The discovery of a population of gamma-ray novae Abstract: Novae have long been expected to be sources of emission at several MeV from\nthe decay of radioactive elements in the novae ejecta, however, they were not\nanticipated to be sources of continuum emission in the GeV energy domain. In\nMarch 2010 the Large Area Telescope (LAT) on-board the Fermi Gamma-ray Space\nTelescope discovered for the first time >100 MeV gamma-ray emission from a nova\nwithin our galaxy, V407 Cyg. The high-energy spectrum and light curve was\nexplained as a consequence of shock acceleration in the nova shell as it\ninteracts with the local ambient medium. While this was an exciting and\nimportant discovery it was suspected that the necessary conditions for\nhigh-energy emission from novae would be rare. In June 2012 the LAT detected\ntwo new transient sources that have been associated with classical novae\nobserved in the optical, Nova Sco 2012 and Nova Mon 2012. We report on the\nobservational properties of the population of gamma-ray novae, their\nsimilarities and differences and the emission processes that generate the high\nenergy radiation in these systems. \n\n"}
{"id": "1309.1764", "contents": "Title: Standard Model Explanation of the Ultra-high Energy Neutrino Events at\n  IceCube Abstract: The recent observation of two PeV events at IceCube, followed by an\nadditional 26 events between 30 - 300 TeV, has generated considerable\nspeculations on its origin, and many exotic New Physics explanations have been\ninvoked. For a reliable interpretation, it is however important to first\nscrutinize the Standard Model (SM) expectations carefully, including the\ntheoretical uncertainties, mainly due to the parton distribution functions.\nAssuming a new isotropic cosmic neutrino flux with a simple unbroken power-law\nspectrum, $\\Phi\\propto E^{-s}$ for the entire energy range of interest, we find\nthat with $s=1.5$ - 2, the SM neutrino-nucleon interactions are sufficient to\nexplain all the observed events so far, without the need for any beyond the SM\nexplanation. With more statistics, this powerful detector could provide a\nunique test of the SM up to the PeV scale, and lead to important clues of New\nPhysics. \n\n"}
{"id": "1309.2756", "contents": "Title: The Galactic Center Origin of a Subset of IceCube Neutrino Events Abstract: The center of the Milkyway is a host to energetic phenomena across many\nelectromagnetic wave-bands and now possibly of high-energy neutrinos. We show\nthat 5 out of 21 IceCube shower-like events, including a PeV event, likely\noriginated from the Galactic Center region. Hard spectrum and flux inferred\nfrom these events are inconsistent with atmospheric neutrinos. The flux of\nthese neutrinos is consistent with an extrapolation of the gamma-ray flux\nmeasured by Fermi-LAT from the inner Galactic region. This indicates a common\nhadronic origin of both, powered by supernovae. Three other shower-like events\nare spatially correlated with the Fermi bubbles, originating from the Galactic\nCenter activity, within the uncertainty of reconstructing their arrival\ndirections. Origin of the other neutrino events, including 7 track-like events,\nis still elusive. \n\n"}
{"id": "1309.5738", "contents": "Title: Probing the extragalactic background light with H.E.S.S Abstract: The imprint of cosmic backgrounds in the gamma ray spectra of blazars has\nrecently been detected by H.E.S.S. and Fermi-LAT, opening the way to studies of\ngamma-ray propagation on cosmological scales. This proceeding discusses the\ncurrent constraints on the extragalactic background light (EBL), the effort to\nincrease the collection of blazars detected at TeV energies, and a crucial part\nof the science case of next-generation instruments: gamma-ray cosmology. \n\n"}
{"id": "1309.6115", "contents": "Title: A Simple FPTAS for Counting Edge Covers Abstract: An edge cover of a graph is a set of edges such that every vertex has at\nleast an adjacent edge in it. Previously, approximation algorithm for counting\nedge covers is only known for 3 regular graphs and it is randomized. We design\na very simple deterministic fully polynomial-time approximation scheme (FPTAS)\nfor counting the number of edge covers for any graph. Our main technique is\ncorrelation decay, which is a powerful tool to design FPTAS for counting\nproblems. In order to get FPTAS for general graphs without degree bound, we\nmake use of a stronger notion called computationally efficient correlation\ndecay, which is introduced in [Li, Lu, Yin SODA 2012]. \n\n"}
{"id": "1310.0554", "contents": "Title: Discriminating hadronic and quark stars through gravitational waves of\n  fluid pulsation modes Abstract: We investigate non-radial oscillations of hadronic, hybrid and pure\nself-bound strange quark stars with maximum masses above the mass of the\nrecently observed massive pulsars PSR J1614-2230 and PSR J0348-0432 with $M\n\\approx 2 M_{\\odot}$. For the hadronic equation of state we employ different\nparametrizations of a relativistic mean-field model and for quark matter we use\nthe MIT bag model including the effect of strong interactions and color\nsuperconductivity. We find that the first pressure mode for strange quark stars\nhas a very different shape than for hadronic and hybrid stars. For strange\nquarks stars the frequency of the p1 mode is larger than 6 kHz and diverge at\nsmall stellar masses, but for hadronic and hybrid stars it is in the range 4-6\nkHz. This allows an observational identification of strange stars even if extra\ninformation such as the mass, the radius or the gravitational redshift of the\nobject is unavailable or uncertain. Also, we find as in previous works that the\nfrequency of the g-mode associated with the quark-hadron discontinuity in a\nhybrid star is in the range 0.4-1 kHz for all masses. Thus, compact objects\nemitting gravitational waves above 6 kHz should be interpreted as strange quark\nstars and those emitting a signal within 0.4-1 kHz should be interpreted as\nhybrid stars. \n\n"}
{"id": "1310.4408", "contents": "Title: Radio Observations of Supernova 1987A Abstract: Supernovae and their remnants are believed to be prodigious sources of\nGalactic cosmic rays and interstellar dust. Understanding the mechanisms behind\ntheir surprisingly high production rate is helped by the study of nearby young\nsupernova remnants. There has been none better in modern times than SN1987A,\nfor which radio observations have been made for over a quarter of a century. We\nreview extensive observations made with the Australia Telescope Compact Array\n(ATCA) at centimetre wavelengths. Emission at frequencies from 1 to 100 GHz is\ndominated by synchrotron radiation from an outer shock front which has been\ngrowing exponentially in strength from day 3000, and is currently sweeping\naround the circumstellar ring at about 4000 km s$^{-1}$. Three dimensional\nmodels of the propagation of the shock into the circumstellar medium are able\nto reproduce the main observational features of the remnant, and their\nevolution. We find that up to 4% of the electrons encountered by the shock are\naccelerated to relativistic energies. High-frequency ALMA observations will\nbreak new ground in the understanding of dust and molecule production. \n\n"}
{"id": "1310.6524", "contents": "Title: Some hard families of parameterised counting problems Abstract: We consider parameterised subgraph-counting problems of the following form:\ngiven a graph G, how many k-tuples of its vertices have a given property? A\nnumber of such problems are known to be #W[1]-complete; here we substantially\ngeneralise some of these existing results by proving hardness for two large\nfamilies of such problems. We demonstrate that it is #W[1]-hard to count the\nnumber of k-vertex subgraphs having any property where the number of distinct\nedge-densities of labelled subgraphs that satisfy the property is o(k^2). In\nthe special case that the property in question depends only on the number of\nedges in the subgraph, we give a strengthening of this result which leads to\nour second family of hard problems. \n\n"}
{"id": "1310.7701", "contents": "Title: The properties of the clumpy torus and BLR in the polar-scattered\n  Seyfert 1 galaxy ESO 323-G77 through X-ray absorption variability Abstract: We report results from multi-epoch X-ray observations of the polar-scattered\nSeyfert 1 galaxy ESO 323-G77. The source exhibits remarkable spectral\nvariability from months to years timescales. The observed spectral variability\nis entirely due to variations of the column density of a neutral absorber\ntowards the intrinsic nuclear continuum. The column density is generally\nCompton-thin ranging from a few times 10$^{22}$ cm$^{-2}$ to a few times\n10$^{23}$ cm$^{-2}$. However, one observation reveals a Compton-thick state\nwith column density of the order of 1.5 $\\times$ 10$^{24}$ cm$^{-2}$. The\nobserved variability offers a rare opportunity to study the properties of the\nX-ray absorber(s) in an active galaxy. We identify variable X-ray absorption\nfrom two different components, namely (i) a clumpy torus whose individual\nclumps have a density of $\\leq$ 1.7 $\\times$ 10$^8$ cm$^{-3}$ and an average\ncolumn density of $\\sim$ 4 $\\times$ 10$^{22}$ cm$^{-2}$, and (ii) the broad\nline region (BLR), comprising individual clouds with density of 0.1-8 $\\times$\n10$^9$ cm$^{-3}$ and column density of 10$^{23}$-10$^{24}$ cm$^{-2}$. The\nderived properties of the clumpy torus can also be used to estimate the torus\nhalf-opening angle, which is of the order of 47 $^\\circ$. We also confirm the\npreviously reported detection of two highly ionized warm absorbers with outflow\nvelocities of 1000-4000 km s$^{-1}$. The observed outflow velocities are\nconsistent with the Keplerian/escape velocity at the BLR. Hence, the warm\nabsorbers may be tentatively identified with the warm/hot inter-cloud medium\nwhich ensures that the BLR clouds are in pressure equilibrium with their\nsurroundings. The BLR line-emitting clouds may well be the cold, dense clumps\nof this outflow, whose warm/hot phase is likely more homogeneous, as suggested\nby the lack of strong variability of the warm absorber(s) properties during our\nmonitoring. \n\n"}
{"id": "1311.0031", "contents": "Title: Multiwavelength Observations of Swift J1753.5-0127 Abstract: We present contemporaneous X-ray, ultraviolet, optical and near-infrared\nobservations of the black hole binary system, Swift J1753.5-0127, acquired in\n2012 October. The UV observations, obtained with the Cosmic Origins\nSpectrograph on the Hubble Space Telescope, are the first UV spectra of this\nsystem. The dereddened UV spectrum is characterized by a smooth, blue continuum\nand broad emission lines of CIV and HeII. The system was stable in the UV to\n<10% during our observations. We estimated the interstellar reddening by\nfitting the 2175 A absorption feature and fit the interstellar absorption\nprofile of Ly$\\alpha$ to directly measure the neutral hydrogen column density\nalong the line of sight. By comparing the UV continuum flux to steady-state\nthin accretion disk models, we determined upper limits on the distance to the\nsystem as a function of black hole mass. The continuum is well fit with disk\nmodels dominated by viscous heating rather than irradiation. The broadband\nspectral energy distribution shows the system has declined at all wavelengths\nsince previous broadband observations in 2005 and 2007. If we assume that the\nUV emission is dominated by the accretion disk the inner radius of the disk\nmust be truncated at radii above the ISCO to be consistent with the X-ray flux,\nrequiring significant mass loss from outflows and/or energy loss via advection\ninto the black hole to maintain energy balance. \n\n"}
{"id": "1311.0139", "contents": "Title: Probing a dark matter density spike at the Galactic Center Abstract: The dark matter halo profile in the inner Galaxy is very uncertain. Yet its\nradial dependence toward the Galactic Center is of crucial importance for the\ndetermination of the gamma-ray and radio fluxes originating from dark matter\nannihilations. Here we use synchrotron emission to probe the dark matter energy\ndistribution in the inner Galaxy. We first solve the problem of the cosmic ray\ndiffusion on very small scales, typically smaller than 10^{-3} pc, by using a\nGreen's function approach and use this technique to quantify the effect of a\nspiky profile (rho(r) ~ r^{-7/3}) on the morphology and intensity of the\nsynchrotron emission expected from dark matter. We illustrate our results using\n10 and 800 GeV candidate weakly interacting dark matter particles annihilating\ndirectly into e+ e-. Our most critical assumptions are that the dark matter is\nheavier than a few GeV and directly produces a reasonable amount of electrons\nand positrons in the Galaxy. We conclude that dark matter indirect detection\ntechniques (including the Planck experiment) could be used to shed light on the\ndark matter halo profile on scales that lie beyond the capability of any\ncurrent numerical simulations. \n\n"}
{"id": "1311.0595", "contents": "Title: On the paradox of Hawking radiation in a maximally extended\n  Schwarzschild solution Abstract: This paper considers the effect of Hawking radiation on an eternal black hole\n- that is. a maximally extended Schwarzschild solution. Symmetry considerations\nthat hold independent of the details of the emission mechanism show there is an\ninconsistency in the claim that such a blackhole evaporates away in a finite\ntime. In essence: because the external domain is static, there is an infinite\ntime available for the process to take place, so whenever the evaporation\nprocess is claimed to come to completion, it should have happened earlier. The\nproblem is identified to lie in the claim that the locus of emission of Hawking\nradiation lies just outside the globally defined event horizon. Rather, the\nemission domain must be mainly located inside the event horizon, so most of the\nHawking radiation ends up at this singularity rather than at infinity and the\nblack hole never evaporates away. This result supports a previous claim\n[arXiv:1310.4771] that astrophysical black holes do not evaporate. \n\n"}
{"id": "1311.1714", "contents": "Title: KaHIP v3.00 -- Karlsruhe High Quality Partitioning -- User Guide Abstract: This paper severs as a user guide to the graph partitioning framework KaHIP\n(Karlsruhe High Quality Partitioning). We give a rough overview of the\ntechniques used within the framework and describe the user interface as well as\nthe file formats used. Moreover, we provide a short description of the current\nlibrary functions provided within the framework. Since version 3.00 we support\nmultilevel partitioning, memetic algorithms, distributed and shared-memory\nparallel algorithms, node separator and ordering algorithms, edge partitioning\nalgorithms as well as ILP solvers. \n\n"}
{"id": "1311.2466", "contents": "Title: Parameterized Approximation Schemes using Graph Widths Abstract: Combining the techniques of approximation algorithms and parameterized\ncomplexity has long been considered a promising research area, but relatively\nfew results are currently known. In this paper we study the parameterized\napproximability of a number of problems which are known to be hard to solve\nexactly when parameterized by treewidth or clique-width. Our main contribution\nis to present a natural randomized rounding technique that extends well-known\nideas and can be used for both of these widths. Applying this very generic\ntechnique we obtain approximation schemes for a number of problems, evading\nboth polynomial-time inapproximability and parameterized intractability bounds. \n\n"}
{"id": "1311.2654", "contents": "Title: Torsional oscillations of crystalline color-superconducting hybrid\n  stars: Possible sources for Advanced LIGO? Abstract: Deconfined quark matter may exist in a crystalline color-superconducting\nphase in the interiors of compact stars. In this paper, we study the torsional\noscillations of compact stars featuring a crystalline color-superconducting\nquark-matter core in general relativity. Depending on the size of the\ncrystalline core and the value of the gap parameter $\\Delta$, we find that the\nfrequencies of the torsional oscillation modes can range from a few hundred\nhertz to a few kilohertz for our canonical $1.4 M_\\odot$ compact star models.\nWe have also studied the prospect for detecting the gravitational-wave signals\nemitted from these modes in a pulsar glitch event. Assuming that at least\n$10\\%$ of the energy released in a Vela glitch can be channeled to the\noscillation modes, we find that the Einstein Telescope should be able to detect\nthese signals in quite general situations. Furthermore, if the size of the\ncrystalline core is comparable to the stellar radius and the gap parameter is\nrelatively small at $\\Delta \\sim 5$ MeV, the signal-to-noise ratio for Advanced\nLIGO could reach $\\sim$10 for a Vela glitch. Our optimistic results suggest\nthat we might already be able to probe the nature of crystalline\ncolor-superconducting quark matter with the second-generation\ngravitational-wave detectors when they come online in the next few years. \n\n"}
{"id": "1311.4742", "contents": "Title: Energy budget of the bifurcated component in the radio pulsar profile of\n  PSR J1012+5307 Abstract: The bifurcated emission component (BEC) in the radio profile of the\nmillisecond pulsar J1012+5307 can be interpreted as the signature of the\ncurvature radiation beam polarised orthogonally to the plane of electron\ntrajectory. Since the beam is intrinsically narrow (~1 deg), the associated\nemission region must be small for the observed BEC to avoid smearing out by\nspatial convolution. We estimate whether the energy available in the stream is\nsufficient to produce such a bright feature in the averaged profile. The energy\nconsiderations become complicated by the angular constraints imposed by the\nwidth of the microbeam, and by the specific spectrum of the BEC which is found\nto have the spectral index xi ~ -0.9 in comparison to the index of xi ~ -2 for\nthe total profile. For typical parameters, the luminosity of the BEC is\ndetermined to be 4 10^{25} erg/s, whereas the maximum-possible\nbeam-size-limited power of the stream is L_max ~ 2 10^{29} erg/s. This implies\nthe minimum energy-conversion efficiency of eta ~ 2 10^{-4}. The BEC's\nluminosity does not exceed any absolute limits of energetics, in particular, it\nis smaller than the power of primary electron and/or secondary plasma stream.\nHowever, the implied efficiency of energy transfer into the radio band is\nextreme if the coherently emitting charge-separated plasma density is limited\nto the Goldreich-Julian value. This suggests that the bifurcated shape of the\nBEC has macroscopic origin, however, several uncertainties (eg. the dipole\ninclination and spectral shape) make this conclusion not firm. \n\n"}
{"id": "1311.4821", "contents": "Title: On the Complexity of Random Satisfiability Problems with Planted\n  Solutions Abstract: The problem of identifying a planted assignment given a random $k$-SAT\nformula consistent with the assignment exhibits a large algorithmic gap: while\nthe planted solution becomes unique and can be identified given a formula with\n$O(n\\log n)$ clauses, there are distributions over clauses for which the best\nknown efficient algorithms require $n^{k/2}$ clauses. We propose and study a\nunified model for planted $k$-SAT, which captures well-known special cases. An\ninstance is described by a planted assignment $\\sigma$ and a distribution on\nclauses with $k$ literals. We define its distribution complexity as the largest\n$r$ for which the distribution is not $r$-wise independent ($1 \\le r \\le k$ for\nany distribution with a planted assignment).\n  Our main result is an unconditional lower bound, tight up to logarithmic\nfactors, for statistical (query) algorithms [Kearns 1998, Feldman et. al 2012],\nmatching known upper bounds, which, as we show, can be implemented using a\nstatistical algorithm. Since known approaches for problems over distributions\nhave statistical analogues (spectral, MCMC, gradient-based, convex optimization\netc.), this lower bound provides a rigorous explanation of the observed\nalgorithmic gap. The proof introduces a new general technique for the analysis\nof statistical query algorithms. It also points to a geometric paring\nphenomenon in the space of all planted assignments.\n  We describe consequences of our lower bounds to Feige's refutation hypothesis\n[Feige 2002] and to lower bounds on general convex programs that solve planted\n$k$-SAT. Our bounds also extend to other planted $k$-CSP models, and, in\nparticular, provide concrete evidence for the security of Goldreich's one-way\nfunction and the associated pseudorandom generator when used with a\nsufficiently hard predicate [Goldreich 2000]. \n\n"}
{"id": "1311.5022", "contents": "Title: Extended Formulations for Online Linear Bandit Optimization Abstract: On-line linear optimization on combinatorial action sets (d-dimensional\nactions) with bandit feedback, is known to have complexity in the order of the\ndimension of the problem. The exponential weighted strategy achieves the best\nknown regret bound that is of the order of $d^{2}\\sqrt{n}$ (where $d$ is the\ndimension of the problem, $n$ is the time horizon). However, such strategies\nare provably suboptimal or computationally inefficient. The complexity is\nattributed to the combinatorial structure of the action set and the dearth of\nefficient exploration strategies of the set. Mirror descent with entropic\nregularization function comes close to solving this problem by enforcing a\nmeticulous projection of weights with an inherent boundary condition. Entropic\nregularization in mirror descent is the only known way of achieving a\nlogarithmic dependence on the dimension. Here, we argue otherwise and recover\nthe original intuition of exponential weighting by borrowing a technique from\ndiscrete optimization and approximation algorithms called `extended\nformulation'. Such formulations appeal to the underlying geometry of the set\nwith a guaranteed logarithmic dependence on the dimension underpinned by an\ninformation theoretic entropic analysis. \n\n"}
{"id": "1311.6140", "contents": "Title: Propagation of extragalactic photons at ultra-high energy with the EleCa\n  code Abstract: Ultra-high energy (UHE) photons play an important role as an independent\nprobe of the photo-pion production mechanism by UHE cosmic rays. Their\nobservation, or non-observation, may constrain astrophysical scenarios for the\norigin of UHECRs and help to understand the nature of the flux suppression\nobserved by several experiments at energies above $10^{19.5}$ eV. Whereas the\ninteraction length of UHE photons above $10^{17}$ eV ranges from a few hundred\nkpc up to tenths of Mpc, photons can interact with the extragalactic background\nradiation initiating the development of electromagnetic cascades which affect\nthe fluxes of photons observed at Earth. The interpretation of the current\nexperimental results rely on the simulations of the UHE photon propagation. In\nthis paper, we present the novel Monte Carlo code EleCa to simulate the\n$Ele$ctromagnetic $Ca$scading initiated by high-energy photons and electrons.\n  We provide an estimation of the surviving probability for photons inducing\nelectromagnetic cascades as a function of their distance from the observer and\nwe calculate the distances within which we expect to observe UHE photons with\nenergy between $10^{17}$ and $10^{19}$ eV. Furthermore, the flux of GZK photons\nat Earth is investigated in several astrophysical scenarios where we vary both\ninjection spectrum and composition at the source and the intensity of the\nintervening extragalactic magnetic field. Although the photon propagation\ndepends on several astrophysical factors, our numerical predictions combined\nwith future experimental observations (or non-observations) of UHE photons --\nin the energy range between $10^{17.5}$ eV and $10^{20}$ eV -- can help to\nconstrain these scenarios. \n\n"}
{"id": "1311.6204", "contents": "Title: Approximating Hereditary Discrepancy via Small Width Ellipsoids Abstract: The Discrepancy of a hypergraph is the minimum attainable value, over\ntwo-colorings of its vertices, of the maximum absolute imbalance of any\nhyperedge. The Hereditary Discrepancy of a hypergraph, defined as the maximum\ndiscrepancy of a restriction of the hypergraph to a subset of its vertices, is\na measure of its complexity. Lovasz, Spencer and Vesztergombi (1986) related\nthe natural extension of this quantity to matrices to rounding algorithms for\nlinear programs, and gave a determinant based lower bound on the hereditary\ndiscrepancy. Matousek (2011) showed that this bound is tight up to a\npolylogarithmic factor, leaving open the question of actually computing this\nbound. Recent work by Nikolov, Talwar and Zhang (2013) showed a polynomial time\n$\\tilde{O}(\\log^3 n)$-approximation to hereditary discrepancy, as a by-product\nof their work in differential privacy. In this paper, we give a direct simple\n$O(\\log^{3/2} n)$-approximation algorithm for this problem. We show that up to\nthis approximation factor, the hereditary discrepancy of a matrix $A$ is\ncharacterized by the optimal value of simple geometric convex program that\nseeks to minimize the largest $\\ell_{\\infty}$ norm of any point in a ellipsoid\ncontaining the columns of $A$. This characterization promises to be a useful\ntool in discrepancy theory. \n\n"}
{"id": "1311.6615", "contents": "Title: Improved approximation algorithm for Fault-Tolerant Facility Placement Abstract: We consider the Fault-Tolerant Facility Placement problem ($FTFP$), which is\na generalization of the classical Uncapacitated Facility Location problem\n($UFL$). In the $FTFP$ problem we have a set of clients $C$ and a set of\nfacilities $F$. Each facility $i \\in F$ can be opened many times. For each\nopening of facility $i$ we pay $f_i \\geq 0$. Our goal is to connect each client\n$j \\in C$ with $r_j \\geq 1$ open facilities in a way that minimizes the total\ncost of open facilities and established connections.\n  In a series of recent papers $FTFP$ was essentially reduced to $FTFL$ and\nthen to $UFL$ showing it could be approximated with ratio $1.575$. In this\npaper we show that $FTFP$ can actually be approximated even better. We consider\napproximation ratio as a function of $r = min_{j \\in C} r_j$ (minimum\nrequirement of a client). With increasing $r$ the approximation ratio of our\nalgorithm $\\lambda_r$ converges to one. Furthermore, for $r > 1$ the value of\n$\\lambda_r$ is less than 1.463 (hardness of approximation of $UFL$). We also\nshow a lower bound of 1.278 for the approximability of the Fault-Tolerant\nFacility Location problem ($FTFL$) for arbitrary $r$. Already for $r > 3$ we\nobtain that $FTFP$ can be approximated with ratio 1.275, showing that under\nstandard complexity theoretic assumptions $FTFP$ is strictly better\napproximable than $FTFL$. \n\n"}
{"id": "1312.3752", "contents": "Title: Deep GMRT radio observations and a multi-wavelength study of the region\n  around HESS J1858+020 Abstract: There are a number of very high energy sources in the Galaxy that remain\nunidentified. Multi-wavelength and variability studies, and catalogue searches,\nare powerful tools to identify the physical counterpart, given the uncertainty\nin the source location and extension. This work carries out a thorough\nmulti-wavelength study of the unidentified, very high energy source HESS\nJ1858+020 and its environs. Giant Metrewave Radio Telescope observations at 610\nMHz and 1.4 GHz have been done to obtain a deep, low-frequency radio image of\nthe region surrounding HESS J1858+020. Archival radio, infrared, and X-ray data\nhave been analysed as well. This observational information, combined with\nmolecular data, catalogue sources, and a nearby Fermi gamma-ray detection of\nunidentified origin, are combined to explore possible counterparts to the very\nhigh energy source. We provide with a deep radio image of a supernova remnant\nthat might be related to the GeV and TeV emission in the region. We confirm the\npresence of an H II region next to the supernova remnant and coincident with\nmolecular emission. A potential region of star formation is also identified. We\nidentify several radio and X-ray sources in the surroundings. Some of these\nsources are known planetary nebulae, whereas others may be non-thermal extended\nemitters and embedded young stellar objects. Three old, background Galactic\npulsars also neighbour HESS J1858+020 along the line of sight. The region\nsurrounding HESS J1858+020 is rich in molecular structures and non-thermal\nobjects that may potentially be linked to this unidentified very high energy\nsource. In particular, a supernova remnant interacting with nearby molecular\nclouds may be a good candidate, but a star forming region, or a non-thermal\nradio source of yet unclear nature, may also be behind the gamma-ray source.\nFurther observational studies are needed. \n\n"}
{"id": "1312.3901", "contents": "Title: Radio Emission from the Bow Shock of G2 Abstract: The radio flux from the synchrotron emission of electrons accelerated in the\nforward bow shock of G2 is expected to have peaked when the forward shock\npasses close to the pericenter from the Galactic Center, around autumn of 2013.\nThis radio flux is model dependent. We find that if G2 were to be a\nmomentum-supported bow shock of a faint star with a strong wind, the radio\nsynchrotron flux from the forward-shock heated ISM is well below the quiescent\nradio flux of Sgr A*. By contrast, if G2 is a diffuse cloud, the radio flux is\npredicted to be much larger than the quiescent radio flux and therefore should\nhave already been detected or will be detected shortly. No such radiation has\nbeen observed to date. Radio measurements can reveal the nature of G2 well\nbefore G2 completes its periapsis passage. \n\n"}
{"id": "1312.3963", "contents": "Title: Optical selection of quasars: SDSS and LSST Abstract: Over the last decade, quasar sample sizes have increased from several\nthousand to several hundred thousand, thanks mostly to SDSS imaging and\nspectroscopic surveys. LSST, the next-generation optical imaging survey, will\nprovide hundreds of detections per object for a sample of more than ten million\nquasars with redshifts of up to about seven. We briefly review optical quasar\nselection techniques, with emphasis on methods based on colors, variability\nproperties and astrometric behavior. \n\n"}
{"id": "1312.4182", "contents": "Title: Adaptive Protocols for Interactive Communication Abstract: How much adversarial noise can protocols for interactive communication\ntolerate? This question was examined by Braverman and Rao (IEEE Trans. Inf.\nTheory, 2014) for the case of \"robust\" protocols, where each party sends\nmessages only in fixed and predetermined rounds. We consider a new class of\nnon-robust protocols for Interactive Communication, which we call adaptive\nprotocols. Such protocols adapt structurally to the noise induced by the\nchannel in the sense that both the order of speaking, and the length of the\nprotocol may vary depending on observed noise.\n  We define models that capture adaptive protocols and study upper and lower\nbounds on the permissible noise rate in these models. When the length of the\nprotocol may adaptively change according to the noise, we demonstrate a\nprotocol that tolerates noise rates up to $1/3$. When the order of speaking may\nadaptively change as well, we demonstrate a protocol that tolerates noise rates\nup to $2/3$. Hence, adaptivity circumvents an impossibility result of $1/4$ on\nthe fraction of tolerable noise (Braverman and Rao, 2014). \n\n"}
{"id": "1312.5067", "contents": "Title: Rainbow path and color degree in edge colored graphs Abstract: Let $G$ be an edge colored graph. A {\\it}{rainbow path} in $G$ is a path in\nwhich all the edges are colored with distinct colors. Let $d^c(v)$ be the color\ndegree of a vertex $v$ in $G$, i.e. the number of distinct colors present on\nthe edges incident on the vertex $v$. Let $t$ be the maximum length of a\nrainbow path in $G$. Chen and Li showed that if $d^c \\geq k$, for every vertex\n$v$ of $G$, then $t \\geq \\left \\lceil \\frac{3 k}{5}\\right \\rceil + 1$ (Long\nheterochromatic paths in edge-colored graphs, The Electronic Journal of\nCombinatorics 12 (2005), # R33, Pages:1-33.) Unfortunately, proof by Chen and\nLi is very long and comes to about 23 pages in the journal version. Chen and Li\nstates in their paper that it was conjectured by Akira Saito, that $t \\ge \\left\n\\lceil \\frac {2k} {3} \\right \\rceil$. They also states in their paper that they\nbelieve $t \\ge k - c$ for some constant $c$.\n  In this note, we give a short proof to show that $t \\ge \\left \\lceil \\frac{3\nk}{5}\\right \\rceil$, using an entirely different method. Our proof is only\nabout 2 pages long. The draw-back is that our bound is less by 1, than the\nbound given by Chen and Li. We hope that the new approach adopted in this paper\nwould eventually lead to the settlement of the conjectures by Saito and/or Chen\nand Li. \n\n"}
{"id": "1312.5620", "contents": "Title: Further results on strong edge-colourings in outerplanar graphs Abstract: An edge-colouring is {\\em strong} if every colour class is an induced\nmatching. In this work we give a formulae that determines either the optimal or\nthe optimal plus one strong chromatic index of bipartite outerplanar graphs.\nFurther, we give an improved upper bound for any outerplanar graph which is\nclose to optimal. All our proofs yield efficient algorithms to construct such\ncolourings. \n\n"}
{"id": "1312.6680", "contents": "Title: Faster all-pairs shortest paths via circuit complexity Abstract: We present a new randomized method for computing the min-plus product\n(a.k.a., tropical product) of two $n \\times n$ matrices, yielding a faster\nalgorithm for solving the all-pairs shortest path problem (APSP) in dense\n$n$-node directed graphs with arbitrary edge weights. On the real RAM, where\nadditions and comparisons of reals are unit cost (but all other operations have\ntypical logarithmic cost), the algorithm runs in time\n\\[\\frac{n^3}{2^{\\Omega(\\log n)^{1/2}}}\\] and is correct with high probability.\nOn the word RAM, the algorithm runs in $n^3/2^{\\Omega(\\log n)^{1/2}} +\nn^{2+o(1)}\\log M$ time for edge weights in $([0,M] \\cap {\\mathbb\nZ})\\cup\\{\\infty\\}$. Prior algorithms used either $n^3/(\\log^c n)$ time for\nvarious $c \\leq 2$, or $O(M^{\\alpha}n^{\\beta})$ time for various $\\alpha > 0$\nand $\\beta > 2$.\n  The new algorithm applies a tool from circuit complexity, namely the\nRazborov-Smolensky polynomials for approximately representing ${\\sf AC}^0[p]$\ncircuits, to efficiently reduce a matrix product over the $(\\min,+)$ algebra to\na relatively small number of rectangular matrix products over ${\\mathbb F}_2$,\neach of which are computable using a particularly efficient method due to\nCoppersmith. We also give a deterministic version of the algorithm running in\n$n^3/2^{\\log^{\\delta} n}$ time for some $\\delta > 0$, which utilizes the\nYao-Beigel-Tarui translation of ${\\sf AC}^0[m]$ circuits into \"nice\" depth-two\ncircuits. \n\n"}
{"id": "1312.7563", "contents": "Title: Weighted Well-Covered Claw-Free Graphs Abstract: A graph G is well-covered if all its maximal independent sets are of the same\ncardinality. Assume that a weight function w is defined on its vertices. Then G\nis w-well-covered if all maximal independent sets are of the same weight. For\nevery graph G, the set of weight functions w such that G is w-well-covered is a\nvector space. Given an input claw-free graph G, we present an O(n^6)algortihm,\nwhose input is a claw-free graph G, and output is the vector space of weight\nfunctions w, for which G is w-well-covered. A graph G is equimatchable if all\nits maximal matchings are of the same cardinality. Assume that a weight\nfunction w is defined on the edges of G. Then G is w-equimatchable if all its\nmaximal matchings are of the same weight. For every graph G, the set of weight\nfunctions w such that G is w-equimatchable is a vector space. We present an\nO(m*n^4 + n^5*log(n)) algorithm which receives an input graph G, and outputs\nthe vector space of weight functions w such that G is w-equimatchable. \n\n"}
{"id": "1401.0294", "contents": "Title: Complexity results for generating subgraphs Abstract: A graph G is well-covered if all its maximal independent sets are of the same\ncardinality. Assume that a weight function w is defined on its vertices. Then G\nis w-well-covered if all maximal independent sets are of the same weight.\n  For every graph G, the set of weight functions w such that G is\nw-well-covered is a vector space, denoted WCW(G). Let B be a complete bipartite\ninduced subgraph of G on vertex sets of bipartition B_X and B_Y. Then B is\ngenerating if there exists an independent set S such that S \\cup B_X and S \\cup\nB_Y are both maximal independent sets of G. A relating edge is a generating\nsubgraph in the restricted case that B = K_{1,1}.\n  Deciding whether an input graph G is well-covered is co-NP-complete.\nTherefore finding WCW(G) is co-NP-hard. Deciding whether an edge is relating is\nco-NP-complete. Therefore, deciding whether a subgraph is generating is\nco-NP-complete as well.\n  In this article we discuss the connections among these problems, provide\nproofs for NP-completeness for several restricted cases, and present polynomial\ncharacterizations for some other cases. \n\n"}
{"id": "1401.1829", "contents": "Title: The low or retrograde spin of the first extragalactic microquasar:\n  implications for Blandford-Znajek powering of jets Abstract: Transitions to high mass accretion rates in black hole X-ray binaries are\nassociated with the ejection of powerful, relativistically-moving jets. The\nmechanism powering such events is thought to be linked to tapping of the\nangular momentum (spin) of the black hole, the rate of accretion through the\ndisc or some combination of the two. We can attempt to discriminate between\nthese possibilities by comparing proxies for jet power with spin estimates. Due\nto the small number of sources reaching Eddington rates and have therefore been\nsuggested to act as 'standard candles', there has been much recent debate as to\nwhether a significant correlation exists between jet power and spin. We perform\ncontinuum fitting to the high-quality, disc-dominated XMM-Newton spectra of the\nextragalactic microquasar discovered in M31. Assuming prograde spin, we find\nthat, for sensible constraints the spin is always very low (a < 0.15 at\n3-sigma). When combined with a proxy for jet power derived from the maximum 5\nGHz radio luminosity during a bright flaring event, we find that the source\nsits well above the previously reported, rising correlation that would indicate\nthat spin tapping is the dominant mechanism for powering the jets. The notable\nexceptions require the inclination to be improbably small or the jet to be very\nfast. We investigate whether this could be a by-product of selecting\nprograde-only spin, finding that the data statistically favour a substantially\nretrograde spin for the same constraints (a < -0.17 at 3-sigma). Although\ntheoretically improbable, this remarkable finding could be confirmation that\nretrograde spin can power such jets via spin-tapping, as has been suggested for\ncertain radio quasars. In either case this work demonstrates the value of\nstudying local extragalactic microquasars as a means to better understand the\nphysics of jet launching. \n\n"}
{"id": "1401.2071", "contents": "Title: On the Nearest Neighbor Rule for the Metric Traveling Salesman Problem Abstract: We present a very simple family of traveling salesman instances with $n$\ncities where the nearest neighbor rule may produce a tour that is $\\Theta(\\log\nn)$ times longer than an optimum solution. Our family works for the graphic,\nthe euclidean, and the rectilinear traveling salesman problem at the same time.\nIt improves the so far best known lower bound in the euclidean case and proves\nfor the first time a lower bound in the rectilinear case. \n\n"}
{"id": "1401.5707", "contents": "Title: Relations between automata and the simple k-path problem Abstract: Let $G$ be a directed graph on $n$ vertices. Given an integer $k<=n$, the\nSIMPLE $k$-PATH problem asks whether there exists a simple $k$-path in $G$. In\ncase $G$ is weighted, the MIN-WT SIMPLE $k$-PATH problem asks for a simple\n$k$-path in $G$ of minimal weight. The fastest currently known deterministic\nalgorithm for MIN-WT SIMPLE $k$-PATH by Fomin, Lokshtanov and Saurabh runs in\ntime $O(2.851^k\\cdot n^{O(1)}\\cdot \\log W)$ for graphs with integer weights in\nthe range $[-W,W]$. This is also the best currently known deterministic\nalgorithm for SIMPLE k-PATH- where the running time is the same without the\n$\\log W$ factor. We define $L_k(n)\\subseteq [n]^k$ to be the set of words of\nlength $k$ whose symbols are all distinct. We show that an explicit\nconstruction of a non-deterministic automaton (NFA) of size $f(k)\\cdot\nn^{O(1)}$ for $L_k(n)$ implies an algorithm of running time $O(f(k)\\cdot\nn^{O(1)}\\cdot \\log W)$ for MIN-WT SIMPLE $k$-PATH when the weights are\nnon-negative or the constructed NFA is acyclic as a directed graph. We show\nthat the algorithm of Kneis et al. and its derandomization by Chen et al. for\nSIMPLE $k$-PATH can be used to construct an acylic NFA for $L_k(n)$ of size\n$O^*(4^{k+o(k)})$.\n  We show, on the other hand, that any NFA for $L_k(n)$ must be size at least\n$2^k$. We thus propose closing this gap and determining the smallest NFA for\n$L_k(n)$ as an interesting open problem that might lead to faster algorithms\nfor MIN-WT SIMPLE $k$-PATH.\n  We use a relation between SIMPLE $k$-PATH and non-deterministic xor automata\n(NXA) to give another direction for a deterministic algorithm with running time\n$O^*(2^k)$ for SIMPLE $k$-PATH. \n\n"}
{"id": "1401.5839", "contents": "Title: A way forward in the study of the symmetry energy: experiment, theory,\n  and observation Abstract: The symmetry energy describes how the energy of nuclear matter rises as one\ngoes away from equal numbers of neutrons and protons. This is very important to\ndescribe neutron rich matter in astrophysics. This article reviews our\nknowledge of the symmetry energy from theoretical calculations, nuclear\nstructure measurements, heavy ion collisions, and astronomical observations. We\nthen present a roadmap to make progress in areas of relevance to the symmetry\nenergy that promotes collaboration between the astrophysics and the nuclear\nphysics communities. \n\n"}
{"id": "1402.0054", "contents": "Title: Popular conjectures imply strong lower bounds for dynamic problems Abstract: We consider several well-studied problems in dynamic algorithms and prove\nthat sufficient progress on any of them would imply a breakthrough on one of\nfive major open problems in the theory of algorithms:\n  1. Is the 3SUM problem on $n$ numbers in $O(n^{2-\\epsilon})$ time for some\n$\\epsilon>0$?\n  2. Can one determine the satisfiability of a CNF formula on $n$ variables in\n$O((2-\\epsilon)^n poly n)$ time for some $\\epsilon>0$?\n  3. Is the All Pairs Shortest Paths problem for graphs on $n$ vertices in\n$O(n^{3-\\epsilon})$ time for some $\\epsilon>0$?\n  4. Is there a linear time algorithm that detects whether a given graph\ncontains a triangle?\n  5. Is there an $O(n^{3-\\epsilon})$ time combinatorial algorithm for $n\\times\nn$ Boolean matrix multiplication?\n  The problems we consider include dynamic versions of bipartite perfect\nmatching, bipartite maximum weight matching, single source reachability, single\nsource shortest paths, strong connectivity, subgraph connectivity, diameter\napproximation and some nongraph problems such as Pagh's problem defined in a\nrecent paper by Patrascu [STOC 2010]. \n\n"}
{"id": "1402.1475", "contents": "Title: The Transport of Cosmic Rays Across Magnetic Fieldlines Abstract: The long residence times and small anisotropies of cosmic rays suggest that\nthey are well confined and well scattered by the Galactic magnetic field. Due\nto the disklike shape of the confinement volume, transport in the vertical\ndirection, perpendicular to the mean Galactic magnetic field, is key to cosmic\nray escape. It has long been recognized that this vertical transport depends\nboth on the vertical component of the fieldlines themselves and on the extent\nto which the cosmic rays are tied to the fieldlines. In this paper we use\nmagnetic fields with very simple spatial and temporal structure to isolate some\nimportant features of cross fieldline transport. We show that even simple\nmagnetic nonuniformities combined with pitch angle scattering can enhance cross\nfieldline transport by several orders of magnitude, while pitch angle\nscattering is unnecessary for enhanced transport if the field is chaotic.\nNevertheless, perpendicular transport is much less than parallel transport in\nall the cases we study. We apply the results to confinement of cosmic rays in\nthe Fermi Bubbles. \n\n"}
{"id": "1402.2332", "contents": "Title: Flux upper limits for 47 AGN observed with H.E.S.S. in 2004-2011 Abstract: About 40% of the observation time of the High Energy Stereoscopic System\n(H.E.S.S.) is dedicated to studying active galactic nuclei (AGN), with the aim\nof increasing the sample of known extragalactic very-high-energy (VHE, E>100\nGeV) sources and constraining the physical processes at play in potential\nemitters. H.E.S.S. observations of AGN, spanning a period from April 2004 to\nDecember 2011, are investigated to constrain their gamma-ray fluxes. Only the\n47 sources without significant excess detected at the position of the targets\nare presented. Upper limits on VHE fluxes of the targets were computed and a\nsearch for variability was performed on the nightly time scale. For 41 objects,\nthe flux upper limits we derived are the most constraining reported to date.\nThese constraints at VHE are compared with the flux level expected from\nextrapolations of Fermi-LAT measurements in the two-year catalog of AGN. The\nH.E.S.S. upper limits are at least a factor of two lower than the extrapolated\nFermi-LAT fluxes for 11 objects. Taking into account the attenuation by the\nextragalactic background light reduces the tension for all but two of them,\nsuggesting intrinsic curvature in the high-energy spectra of these two AGN.\nCompilation efforts led by current VHE instruments are of critical importance\nfor target-selection strategies before the advent of the Cherenkov Telescope\nArray, CTA. \n\n"}
{"id": "1402.2589", "contents": "Title: Partitioning Perfect Graphs into Stars Abstract: The partition of graphs into \"nice\" subgraphs is a central algorithmic\nproblem with strong ties to matching theory. We study the partitioning of\nundirected graphs into same-size stars, a problem known to be NP-complete even\nfor the case of stars on three vertices. We perform a thorough computational\ncomplexity study of the problem on subclasses of perfect graphs and identify\nseveral polynomial-time solvable cases, for example, on interval graphs and\nbipartite permutation graphs, and also NP-complete cases, for example, on grid\ngraphs and chordal graphs. \n\n"}
{"id": "1402.4556", "contents": "Title: Spatial Mixing of Coloring Random Graphs Abstract: We study the strong spatial mixing (decay of correlation) property of proper\n$q$-colorings of random graph $G(n, d/n)$ with a fixed $d$. The strong spatial\nmixing of coloring and related models have been extensively studied on graphs\nwith bounded maximum degree. However, for typical classes of graphs with\nbounded average degree, such as $G(n, d/n)$, an easy counterexample shows that\ncolorings do not exhibit strong spatial mixing with high probability.\nNevertheless, we show that for $q\\ge\\alpha d+\\beta$ with $\\alpha>2$ and\nsufficiently large $\\beta=O(1)$, with high probability proper $q$-colorings of\nrandom graph $G(n, d/n)$ exhibit strong spatial mixing with respect to an\narbitrarily fixed vertex. This is the first strong spatial mixing result for\ncolorings of graphs with unbounded maximum degree. Our analysis of strong\nspatial mixing establishes a block-wise correlation decay instead of the\nstandard point-wise decay, which may be of interest by itself, especially for\ngraphs with unbounded degree. \n\n"}
{"id": "1402.6310", "contents": "Title: Approximating the Cubicity of Trees Abstract: Cubicity of a graph $G$ is the smallest dimension $d$, for which $G$ is a\nunit disc graph in ${\\mathbb{R}}^d$, under the $l^\\infty$ metric, i.e. $G$ can\nbe represented as an intersection graph of $d$-dimensional (axis-parallel) unit\nhypercubes. We call such an intersection representation a $d$-dimensional cube\nrepresentation of $G$. Computing cubicity is known to be inapproximable in\npolynomial time, within an $O(n^{1-\\epsilon})$ factor for any $\\epsilon >0$,\nunless NP=ZPP.\n  In this paper, we present a randomized algorithm that runs in polynomial time\nand computes cube representations of trees, of dimension within a constant\nfactor of the optimum. It is also shown that the cubicity of trees can be\napproximated within a constant factor in deterministic polynomial time, if the\ncube representation is not required to be computed. As far as we know, this is\nthe first constant factor approximation algorithm for computing the cubicity of\ntrees. It is not yet clear whether computing the cubicity of trees is NP-hard\nor not. \n\n"}
{"id": "1402.7224", "contents": "Title: On low treewidth graphs and supertrees Abstract: Compatibility of unrooted phylogenetic trees is a well studied problem in\nphylogenetics. It asks to determine whether for a set of k input trees there\nexists a larger tree (called a supertree) that contains the topologies of all k\ninput trees. When any such supertree exists we call the instance compatible and\notherwise incompatible. It is known that the problem is NP-hard and FPT,\nalthough a constructive FPT algorithm is not known. It has been shown that\nwhenever the treewidth of an auxiliary structure known as the display graph is\nstrictly larger than the number of input trees, the instance is incompatible.\nHere we show that whenever the treewidth of the display graph is at most 2, the\ninstance is compatible. Furthermore, we give a polynomial-time algorithm to\nconstruct a supertree in this case. Finally, we demonstrate both compatible and\nincompatible instances that have display graphs with treewidth 3, highlighting\nthat the treewidth of the display graph is (on its own) not sufficient to\ndetermine compatibility. \n\n"}
{"id": "1403.0252", "contents": "Title: A Back-to-Basics Empirical Study of Priority Queues Abstract: The theory community has proposed several new heap variants in the recent\npast which have remained largely untested experimentally. We take the field\nback to the drawing board, with straightforward implementations of both classic\nand novel structures using only standard, well-known optimizations. We study\nthe behavior of each structure on a variety of inputs, including artificial\nworkloads, workloads generated by running algorithms on real map data, and\nworkloads from a discrete event simulator used in recent systems networking\nresearch. We provide observations about which characteristics are most\ncorrelated to performance. For example, we find that the L1 cache miss rate\nappears to be strongly correlated with wallclock time. We also provide\nobservations about how the input sequence affects the relative performance of\nthe different heap variants. For example, we show (both theoretically and in\npractice) that certain random insertion-deletion sequences are degenerate and\ncan lead to misleading results. Overall, our findings suggest that while the\nconventional wisdom holds in some cases, it is sorely mistaken in others. \n\n"}
{"id": "1403.3881", "contents": "Title: Complexity of Equilibrium in Diffusion Games on Social Networks Abstract: In this paper, we consider the competitive diffusion game, and study the\nexistence of its pure-strategy Nash equilibrium when defined over general\nundirected networks. We first determine the set of pure-strategy Nash\nequilibria for two special but well-known classes of networks, namely the\nlattice and the hypercube. Characterizing the utility of the players in terms\nof graphical distances of their initial seed placements to other nodes in the\nnetwork, we show that in general networks the decision process on the existence\nof pure-strategy Nash equilibrium is an NP-hard problem. Following this, we\nprovide some necessary conditions for a given profile to be a Nash equilibrium.\nFurthermore, we study players' utilities in the competitive diffusion game over\nErdos-Renyi random graphs and show that as the size of the network grows, the\nutilities of the players are highly concentrated around their expectation, and\nare bounded below by some threshold based on the parameters of the network.\nFinally, we obtain a lower bound for the maximum social welfare of the game\nwith two players, and study sub-modularity of the players' utilities. \n\n"}
{"id": "1403.6775", "contents": "Title: Spin frequency distributions of binary millisecond pulsars Abstract: Rotation-powered millisecond radio pulsars have been spun up to their present\nspin period by a $10^8$ - $10^9$ yr long X-ray-bright phase of accretion of\nmatter and angular momentum in a low-to-intermediate mass binary system.\nRecently, the discovery of transitional pulsars that alternate cyclically\nbetween accretion and rotation-powered states on time scales of a few years or\nshorter, has demonstrated this evolutionary scenario. Here, we present a\nthorough statistical analysis of the spin distributions of the various classes\nof millisecond pulsars to assess the evolution of their spin period between the\ndifferent stages. Accreting sources that showed oscillations exclusively during\nthermonuclear type I X-ray bursts (nuclear-powered millisecond pulsars) are\nfound to be significantly faster than rotation-powered sources, while accreting\nsources that possess a magnetosphere and show coherent pulsations (accreting\nmillisecond pulsars) are not. On the other hand, if accreting millisecond\npulsars and eclipsing rotation-powered millisecond pulsars form a common class\nof transitional pulsars, these are shown to have a spin distribution\nintermediate between the faster nuclear-powered millisecond pulsars and the\nslower non-eclipsing rotation-powered millisecond pulsars. We interpret these\nfindings in terms of a spin-down due to the decreasing mass-accretion rate\nduring the latest stages of the accretion phase, and in terms of the different\norbital evolutionary channels mapped by the various classes of pulsars. We\nsummarize possible instrumental selection effects, showing that even if an\nunbiased sample of pulsars is still lacking, their influence on the results of\nthe presented analysis is reduced by recent improvements in instrumentation and\nsearching techniques. \n\n"}
{"id": "1404.0799", "contents": "Title: Threesomes, Degenerates, and Love Triangles Abstract: The 3SUM problem is to decide, given a set of $n$ real numbers, whether any\nthree sum to zero. It is widely conjectured that a trivial $O(n^2)$-time\nalgorithm is optimal and over the years the consequences of this conjecture\nhave been revealed. This 3SUM conjecture implies $\\Omega(n^2)$ lower bounds on\nnumerous problems in computational geometry and a variant of the conjecture\nimplies strong lower bounds on triangle enumeration, dynamic graph algorithms,\nand string matching data structures.\n  In this paper we refute the 3SUM conjecture. We prove that the decision tree\ncomplexity of 3SUM is $O(n^{3/2}\\sqrt{\\log n})$ and give two subquadratic 3SUM\nalgorithms, a deterministic one running in $O(n^2 / (\\log n/\\log\\log n)^{2/3})$\ntime and a randomized one running in $O(n^2 (\\log\\log n)^2 / \\log n)$ time with\nhigh probability. Our results lead directly to improved bounds for $k$-variate\nlinear degeneracy testing for all odd $k\\ge 3$. The problem is to decide, given\na linear function $f(x_1,\\ldots,x_k) = \\alpha_0 + \\sum_{1\\le i\\le k} \\alpha_i\nx_i$ and a set $A \\subset \\mathbb{R}$, whether $0\\in f(A^k)$. We show the\ndecision tree complexity of this problem is $O(n^{k/2}\\sqrt{\\log n})$.\n  Finally, we give a subcubic algorithm for a generalization of the\n$(\\min,+)$-product over real-valued matrices and apply it to the problem of\nfinding zero-weight triangles in weighted graphs. We give a\ndepth-$O(n^{5/2}\\sqrt{\\log n})$ decision tree for this problem, as well as an\nalgorithm running in time $O(n^3 (\\log\\log n)^2/\\log n)$. \n\n"}
{"id": "1404.1323", "contents": "Title: Lower bounds for testing digraph connectivity with one-pass streaming\n  algorithms Abstract: In this note, we show that three graph properties - strong connectivity,\nacyclicity, and reachability from a vertex $s$ to all vertices - each require a\nworking memory of $\\Omega (\\epsilon m)$ on a graph with $m$ edges to be\ndetermined correctly with probability greater than $(1+\\epsilon)/2$. \n\n"}
{"id": "1404.3918", "contents": "Title: A simple SVD algorithm for finding hidden partitions Abstract: Finding a hidden partition in a random environment is a general and important\nproblem, which contains as subproblems many famous questions, such as finding a\nhidden clique, finding a hidden coloring, finding a hidden bipartition etc.\n  In this paper, we provide a simple SVD algorithm for this purpose, answering\na question of McSherry. This algorithm is very easy to implement and works for\nsparse graphs with optimal density. \n\n"}
{"id": "1404.5236", "contents": "Title: Sum-of-squares proofs and the quest toward optimal algorithms Abstract: In order to obtain the best-known guarantees, algorithms are traditionally\ntailored to the particular problem we want to solve. Two recent developments,\nthe Unique Games Conjecture (UGC) and the Sum-of-Squares (SOS) method,\nsurprisingly suggest that this tailoring is not necessary and that a single\nefficient algorithm could achieve best possible guarantees for a wide range of\ndifferent problems.\n  The Unique Games Conjecture (UGC) is a tantalizing conjecture in\ncomputational complexity, which, if true, will shed light on the complexity of\na great many problems. In particular this conjecture predicts that a single\nconcrete algorithm provides optimal guarantees among all efficient algorithms\nfor a large class of computational problems.\n  The Sum-of-Squares (SOS) method is a general approach for solving systems of\npolynomial constraints. This approach is studied in several scientific\ndisciplines, including real algebraic geometry, proof complexity, control\ntheory, and mathematical programming, and has found applications in fields as\ndiverse as quantum information theory, formal verification, game theory and\nmany others.\n  We survey some connections that were recently uncovered between the Unique\nGames Conjecture and the Sum-of-Squares method. In particular, we discuss new\ntools to rigorously bound the running time of the SOS method for obtaining\napproximate solutions to hard optimization problems, and how these tools give\nthe potential for the sum-of-squares method to provide new guarantees for many\nproblems of interest, and possibly to even refute the UGC. \n\n"}
{"id": "1404.5316", "contents": "Title: Different X-ray spectral evolution for black hole X-ray binaries in dual\n  tracks of radio-X-ray correlation Abstract: Recently an `outliers' track of radio-X-ray correlation was found, which is\nmuch steeper than the former universal correlation, where dual tracks were\nspeculated to be triggered by different accretion processes. In this work, we\ntest this issue by exploring hard X-ray spectral evolution in four black-hole\nX-ray binaries (XRBs) with multiple, quasi-simultaneous radio and X-ray\nobservations. Firstly, we find that hard X-ray photon indices, $\\Gamma$, are\nanti- and positively correlated to X-ray fluxes when the X-ray flux, $F_{\\rm\n3-9keV}$, is below and above a critical flux, $F_{\\rm X,crit}$, which are\nconsistent with prediction of advection dominated accretion flow (ADAF) and\ndisk-corona model respectively. Secondly and most importantly, we find that the\nradio-X-ray correlations are also clearly different when the X-ray fluxes are\nhigher and lower than the critical flux that defined by X-ray spectral\nevolution. The data points with $F_{\\rm 3-9keV}\\gtrsim F_{\\rm X,crit}$ have a\nsteeper radio-X-ray correlation ($F_{\\rm X}\\propto F_{\\rm R}^{b}$ and $b\\sim\n1.1-1.4$), which roughly form the `outliers' track. However, the data points\nwith anti-correlation of $\\Gamma-F_{\\rm 3-9keV}$ either stay in the universal\ntrack with $b\\sim0.61$ or stay in transition track (from the universal to\n`outliers' tracks or vice versa). Therefore, our results support that the\nuniversal and `outliers' tracks of radio-X-ray correlations are regulated by\nradiatively inefficient and radiatively efficient accretion model respectively. \n\n"}
{"id": "1404.5432", "contents": "Title: Win-Win Kernelization for Degree Sequence Completion Problems Abstract: We study provably effective and efficient data reduction for a class of\nNP-hard graph modification problems based on vertex degree properties. We show\nfixed-parameter tractability for NP-hard graph completion (that is, edge\naddition) cases while we show that there is no hope to achieve analogous\nresults for the corresponding vertex or edge deletion versions. Our algorithms\nare based on transforming graph completion problems into efficiently solvable\nnumber problems and exploiting f-factor computations for translating the\nresults back into the graph setting. Our core observation is that we encounter\na win-win situation: either the number of edge additions is small or the\nproblem is polynomial-time solvable. This approach helps in answering an open\nquestion by Mathieson and Szeider [JCSS 2012] concerning the polynomial\nkernelizability of Degree Constraint Edge Addition and leads to a general\nmethod of approaching polynomial-time preprocessing for a wider class of degree\nsequence completion problems. \n\n"}
{"id": "1404.6962", "contents": "Title: Fast Synchronization of Random Automata Abstract: A synchronizing word for an automaton is a word that brings that automaton\ninto one and the same state, regardless of the starting position. Cerny\nconjectured in 1964 that if a n-state deterministic automaton has a\nsynchronizing word, then it has a synchronizing word of size at most (n-1)^2.\nBerlinkov recently made a breakthrough in the probabilistic analysis of\nsynchronization by proving that with high probability, an automaton has a\nsynchronizing word. In this article, we prove that with high probability an\nautomaton admits a synchronizing word of length smaller than n^(1+\\epsilon),\nand therefore that the Cerny conjecture holds with high probability. \n\n"}
{"id": "1404.7219", "contents": "Title: Sublinear separators, fragility and subexponential expansion Abstract: Let G be a subgraph-closed graph class with bounded maximum degree. We show\nthat if G has balanced separators whose size is smaller than linear by a\npolynomial factor, then G has subexponential expansion. This gives a partial\nconverse to a result of Ne\\v{s}et\\v{r}il and Ossona de Mendez. As an\nintermediate step, the proof uses a new kind of graph decompositions. \n\n"}
{"id": "1404.7758", "contents": "Title: Between Treewidth and Clique-width Abstract: Many hard graph problems can be solved efficiently when restricted to graphs\nof bounded treewidth, and more generally to graphs of bounded clique-width. But\nthere is a price to be paid for this generality, exemplified by the four\nproblems MaxCut, Graph Coloring, Hamiltonian Cycle and Edge Dominating Set that\nare all FPT parameterized by treewidth but none of which can be FPT\nparameterized by clique-width unless FPT = W[1], as shown by Fomin et al [7,\n8]. We therefore seek a structural graph parameter that shares some of the\ngenerality of clique-width without paying this price. Based on splits, branch\ndecompositions and the work of Vatshelle [18] on Maximum Matching-width, we\nconsider the graph parameter sm-width which lies between treewidth and\nclique-width. Some graph classes of unbounded treewidth, like\ndistance-hereditary graphs, have bounded sm-width. We show that MaxCut, Graph\nColoring, Hamiltonian Cycle and Edge Dominating Set are all FPT parameterized\nby sm-width. \n\n"}
{"id": "1405.0170", "contents": "Title: Un algorithme de test pour la connexit\\'e temporelle des graphes\n  dynamiques de faible densit\\'e Abstract: We address the problem of testing whether a dynamic graph is temporally\nconnected, i.e. a temporal path ({\\em journey}) exists between all pairs of\nvertices. We consider a discrete version of the problem, where the topology is\ngiven as an evolving graph $\\G=\\{G_1,G_2,...,G_{k}\\}$ in which only the set of\n(directed) edges varies. Two cases are studied, depending on whether a single\nedge or an unlimited number of edges can be crossed in a same $G_i$ (strict\njourneys {\\it vs} non-strict journeys). For strict journeys, two existing\nalgorithms designed for other problems can be adapted. However, we show that a\ndedicated approach achieves a better time complexity than one of these two\nalgorithms in all cases, and than the other one for those graphs whose density\nis low at any time (though arbitrary over time). The time complexity of our\nalgorithm is $O(k\\mu n)$, where $k=|\\G|$ is the number of time steps and\n$\\mu=max(|E_i|)$ is the maximum {\\em instant} density, to be contrasted with\n$m=|\\cup E_i|$, the {\\em cumulated} density. Indeed, it is not uncommon for a\nmobility scenario to satisfy, for instance, both $\\mu=o(n)$ and\n$m=\\Theta(n^2)$. We characterize the key values of $k, \\mu$ and $m$ for which\nour algorithm should be used. For non-strict journeys, for which no algorithm\nis known, we show that a similar strategy can be used to answer the question,\nstill in $O(k\\mu n)$ time. \n\n"}
{"id": "1405.0618", "contents": "Title: The Luminosity Function of Low Mass X-Ray Binaries in the Globular\n  Cluster System of NGC 1399 Abstract: We present a study of the faint-end of the X-ray Luminosity Function of Low\nMass X-ray binaries in the Globular Cluster system of the cD galaxy NGC 1399 by\nperforming a stacking experiment on 618 X-ray undetected GCs, in order to\nverify the presence of faint LMXBs and to constrain the faint-end slope of the\nGC-LMXBs XLF below the individual detection threshold of $8\\times10^{37}$ erg\ns$^{-1}$ in the $0.5-8$ keV band. We obtain a significant X-ray detection for\nthe whole GC sample, as well as for the red and blue GC subpopulations,\ncorresponding to an average luminosity per GC $<L_{X}>_{GC}$ of\n$(3.6\\pm1.0)\\times10^{36}\\ erg\\ s^{-1}$, $(6.9\\pm2.1)\\times10^{36}\\ erg\\\ns^{-1}$ and $(1.7\\pm0.9)\\times10^{36}\\ erg\\ s^{-1}$, respectively for all, red\nand blue GCs. If LMXBs in red and blue GCs have the same average intrinsic\nluminosity, we derive a red/blu ratio $\\simeq 3$ of GCs hosting LMXBs\n($2.5\\pm1.0$ or $4.1\\pm2.5$ depending on the surveyed region); alternatively,\nassuming the fractions observed for brighter sources, we measure an average\nX-ray luminosity of $L_{X}=(4.3\\pm1.3)\\times10^{37}\\ erg\\ s^{-1}$ and\n$L_{X}=(3.4\\pm1.7)\\times10^{37}\\ erg\\ s^{-1}$ per red and blue GC-LMXBs\nrespectively. In the assumption that the XLF follows a power-law distribution,\nwe find that a low-luminosity break is required at $L_{X}\\leq 8\\times10^{37}$\nerg s$^{-1}$ both in the whole, as well as in the color-selected (red and blue)\nsubsamples. Given the bright-end slopes measured above the X-ray completeness\nlimit, this result is significant at $>3\\sigma$ level. Our best estimates for\nthe faint end slope are $\\beta_{L}=-1.39/-1.38/-1.36$ for all/red/blue\nGC-LMXBs. We also find evidence that the luminosity function becomes steeper at\nluminosities $L_X\\gtrsim 3\\times 10^{39}$ erg s$^{-1}$, as observed in old\nellipticals. \n\n"}
{"id": "1405.4671", "contents": "Title: Asymmetry of the angular distribution of Cherenkov photons of extensive\n  air showers induced by the geomagnetic field Abstract: The angular distribution of Cherenkov light in an air shower is closely\nlinked to that of the shower electrons and positrons. As charged particles in\nextensive air showers are deflected by the magnetic field of the Earth, a\ndeformation of the angular distribution of the Cherenkov light, that would be\napproximately symmetric about the shower axis if no magnetic field were\npresent, is expected. In this work we study the variation of the Cherenkov\nlight distribution as a function of the azimuth angle in the plane\nperpendicular to shower axis. It is found that the asymmetry induced by the\ngeomagnetic field is most significant for early stages of shower evolution and\nfor showers arriving almost perpendicular to the vector of the local\ngeomagnetic field. Furthermore, it is shown that ignoring the azimuthal\nasymmetry of Cherenkov light might lead to a significant under- or\noverestimation of the Cherenkov light signal especially at sites where the\nlocal geomagnetic field is strong. Based on CORSIKA simulations, the azimuthal\ndistribution of Cherenkov light is parametrized in dependence on the magnetic\nfield component perpendicular to the shower axis and the local air density.\nThis parametrization provides an efficient approximation for estimating the\nasymmetry of the Cherenkov light distribution for shower simulation and\nreconstruction in cosmic ray and gamma-ray experiments in which the Cherenkov\nsignal of showers with energies above 10^14 eV is observed. \n\n"}
{"id": "1405.4840", "contents": "Title: Numerical Relativity and Astrophysics Abstract: Throughout the Universe many powerful events are driven by strong\ngravitational effects that require general relativity to fully describe them.\nThese include compact binary mergers, black hole accretion and stellar\ncollapse, where velocities can approach the speed of light, and extreme\ngravitational fields --$\\Phi_{\\rm Newt}/c^2 \\simeq 1$-- mediate the\ninteractions. Many of these processes trigger emission across a broad range of\nthe electromagnetic spectrum. Compact binaries further source strong\ngravitational wave emission that could directly be detected in the near future.\nThis feat will open up a gravitational wave window into our Universe and\nrevolutionize its understanding. Describing these phenomena requires general\nrelativity, and --where dynamical effects strongly modify gravitational\nfields-- the full Einstein equations coupled to matter sources. Numerical\nrelativity is a field within general relativity concerned with studying such\nscenarios that cannot be accurately modeled via perturbative or analytical\ncalculations. In this review, we examine results obtained within this\ndiscipline, with a focus on its impact in astrophysics. \n\n"}
{"id": "1405.5594", "contents": "Title: From Finite Automata to Regular Expressions and Back--A Summary on\n  Descriptional Complexity Abstract: The equivalence of finite automata and regular expressions dates back to the\nseminal paper of Kleene on events in nerve nets and finite automata from 1956.\nIn the present paper we tour a fragment of the literature and summarize results\non upper and lower bounds on the conversion of finite automata to regular\nexpressions and vice versa. We also briefly recall the known bounds for the\nremoval of spontaneous transitions (epsilon-transitions) on non-epsilon-free\nnondeterministic devices. Moreover, we report on recent results on the average\ncase descriptional complexity bounds for the conversion of regular expressions\nto finite automata and brand new developments on the state elimination\nalgorithm that converts finite automata to regular expressions. \n\n"}
{"id": "1405.5869", "contents": "Title: Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search\n  (MIPS) Abstract: We present the first provably sublinear time algorithm for approximate\n\\emph{Maximum Inner Product Search} (MIPS). Our proposal is also the first\nhashing algorithm for searching with (un-normalized) inner product as the\nunderlying similarity measure. Finding hashing schemes for MIPS was considered\nhard. We formally show that the existing Locality Sensitive Hashing (LSH)\nframework is insufficient for solving MIPS, and then we extend the existing LSH\nframework to allow asymmetric hashing schemes. Our proposal is based on an\ninteresting mathematical phenomenon in which inner products, after independent\nasymmetric transformations, can be converted into the problem of approximate\nnear neighbor search. This key observation makes efficient sublinear hashing\nscheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we\nprovide an explicit construction of provably fast hashing scheme for MIPS. The\nproposed construction and the extended LSH framework could be of independent\ntheoretical interest. Our proposed algorithm is simple and easy to implement.\nWe evaluate the method, for retrieving inner products, in the collaborative\nfiltering task of item recommendations on Netflix and Movielens datasets. \n\n"}
{"id": "1405.5949", "contents": "Title: Internal Energy Dissipation of Gamma-Ray Bursts Observed with Swift:\n  Precursors, Prompt Gamma-rays, Extended emission and Late X-ray Flares Abstract: We jointly analyze the gamma-ray burst (GRB) data observed with BAT and XRT\non board the Swift mission to present a global view on the internal energy\ndissipation processes in GRBs, including precursors, prompt gamma-ray emission,\nextended soft gamma-ray emission, and late X-ray flares. The Bayesian block\nmethod is utilized to analyze the BAT lightcurves to identify various emission\nepisodes. Our results suggest that these emission components likely share a\nsame physical origin, which is repeated activation of the GRB central engine.\nWhat we observe in the gamma-ray band may be the tip-of-iceberg of more\nextended underlying activities. The precursor emission, which is detected in\nabout 10% of {\\em Swift} GRBs, is preferably detected in those GRBs that have a\nmassive star core-collapse origin. The soft extended emission (EE) tail, on the\nother hand, is preferably detected in those GRBs that have a compact star\nmerger origin. Bright X-ray emission is detected during the BAT quiescent\nphases prior to subsequent gamma-ray peaks, implying that X-ray emission may be\ndetectable prior the BAT trigger time. Future GRB alert instruments with soft\nX-ray capability would be essential to reveal the early stage of GRB central\nengine activities, sheding light into jet composition and jet launching\nmechanism in GRBs. \n\n"}
{"id": "1405.5975", "contents": "Title: Solving Multi-choice Secretary Problem in Parallel: An Optimal\n  Observation-Selection Protocol Abstract: The classical secretary problem investigates the question of how to hire the\nbest secretary from $n$ candidates who come in a uniformly random order. In\nthis work we investigate a parallel generalizations of this problem introduced\nby Feldman and Tennenholtz [14]. We call it shared $Q$-queue $J$-choice\n$K$-best secretary problem. In this problem, $n$ candidates are evenly\ndistributed into $Q$ queues, and instead of hiring the best one, the employer\nwants to hire $J$ candidates among the best $K$ persons. The $J$ quotas are\nshared by all queues. This problem is a generalized version of $J$-choice\n$K$-best problem which has been extensively studied and it has more practical\nvalue as it characterizes the parallel situation.\n  Although a few of works have been done about this generalization, to the best\nof our knowledge, no optimal deterministic protocol was known with general $Q$\nqueues. In this paper, we provide an optimal deterministic protocol for this\nproblem. The protocol is in the same style of the $1\\over e$-solution for the\nclassical secretary problem, but with multiple phases and adaptive criteria.\nOur protocol is very simple and efficient, and we show that several\ngeneralizations, such as the fractional $J$-choice $K$-best secretary problem\nand exclusive $Q$-queue $J$-choice $K$-best secretary problem, can be solved\noptimally by this protocol with slight modification and the latter one solves\nan open problem of Feldman and Tennenholtz [14].\n  In addition, we provide theoretical analysis for two typical cases, including\nthe 1-queue 1-choice $K$-best problem and the shared 2-queue 2-choice 2-best\nproblem. For the former, we prove a lower bound $1-O(\\frac{\\ln^2K}{K^2})$ of\nthe competitive ratio. For the latter, we show the optimal competitive ratio is\n$\\approx0.372$ while previously the best known result is 0.356 [14]. \n\n"}
{"id": "1405.6750", "contents": "Title: Spectral and Timing Properties of the Black Hole X-ray Binary H 1743-322\n  in the Low/hard State Studied with Suzaku Abstract: We report on the results from Suzaku observations of the Galactic black hole\nX-ray binary H 1743-322 in the low/hard state during its outburst in 2012\nOctober. We appropriately take into account the effects of dust-scattering to\naccurately analyze the X-ray spectra. The time-averaged spectra in the 1-200\nkeV band are dominated by a hard power-law component of a photon index of\n\\approx 1.6 with a high-energy cutoff at \\approx 60 keV, which is well\ndescribed with Comptonization of the disk emission by hot corona. We estimate\nthe inner disk radius from the multi-color disk component, and find that it is\n1.3-2.3 times larger than the radius in the high/soft state. This suggests that\nthe standard disk was not extended to the innermost stable circular orbit. A\nreflection component from the disk is detected with R = \\Omega/2\\pi \\approx 0.6\n(\\Omega is the solid angle). We also successfully estimate the stable disk\ncomponent in a way independent of the time-averaged spectral modeling, by\nanalyzing short-term spectral variability on the \\sim 1-sec timescale. A weak\nlow-frequency quasi-periodic oscillation (LF QPO) at 0.1-0.2 Hz is detected,\nwhose frequency is found to correlate with the X-ray luminosity and photon\nindex. This result may be explained by the evolution of the disk truncation\nradius. \n\n"}
{"id": "1405.6802", "contents": "Title: On the growth rate of 1324-avoiding permutations Abstract: We give an improved algorithm for counting the number of $1324$-avoiding\npermutations, resulting in 5 further terms of the generating function. We\nanalyse the known coefficients and find compelling evidence that unlike other\nclassical length-4 pattern-avoiding permutations, the generating function in\nthis case does not have an algebraic singularity. Rather, the number of\n1324-avoiding permutations of length $n$ behaves as $$B\\cdot \\mu^n \\cdot\n\\mu_1^{n^{\\sigma}} \\cdot n^g.$$ We estimate $\\mu=11.60 \\pm 0.01,$ $\\sigma=1/2,$\n$\\mu_1 = 0.0398 \\pm 0.0010,$ $g = -1.1 \\pm 0.2$ and $B =9.5 \\pm 1.0.$ \n\n"}
{"id": "1405.6851", "contents": "Title: Exact Algorithms for 0-1 Integer Programs with Linear Equality\n  Constraints Abstract: In this paper, we show $O(1.415^n)$-time and $O(1.190^n)$-space exact\nalgorithms for 0-1 integer programs where constraints are linear equalities and\ncoefficients are arbitrary real numbers. Our algorithms are quadratically\nfaster than exhaustive search and almost quadratically faster than an algorithm\nfor an inequality version of the problem by Impagliazzo, Lovett, Paturi and\nSchneider (arXiv:1401.5512), which motivated our work. Rather than improving\nthe time and space complexity, we advance to a simple direction as inclusion of\nmany NP-hard problems in terms of exact exponential algorithms. Specifically,\nwe extend our algorithms to linear optimization problems. \n\n"}
{"id": "1405.6929", "contents": "Title: Directed cycle double covers and cut-obstacles Abstract: A directed cycle double cover of a graph G is a family of cycles of G, each\nprovided with an orientation, such that every edge of G is covered by exactly\ntwo oppositely directed cycles. Explicit obstacles to the existence of a\ndirected cycle double cover in a graph are bridges. Jaeger conjectured that\nbridges are actually the only obstacles. One of the difficulties in proving the\nJaeger's conjecture lies in discovering and avoiding obstructions to partial\nstrategies that, if successful, create directed cycle double covers. In this\nwork, we suggest a way to circumvent this difficulty. We formulate a conjecture\non graph connections, whose validity follows by the successful avoidance of one\ncut-type obstruction that we call cut-obstacles. The main result of this work\nclaims that our 'cut-obstacles avoidance conjecture' already implies Jaeger's\ndirected cycle double cover conjecture. \n\n"}
{"id": "1405.7112", "contents": "Title: Optimal query complexity for estimating the trace of a matrix Abstract: Given an implicit $n\\times n$ matrix $A$ with oracle access $x^TA x$ for any\n$x\\in \\mathbb{R}^n$, we study the query complexity of randomized algorithms for\nestimating the trace of the matrix. This problem has many applications in\nquantum physics, machine learning, and pattern matching. Two metrics are\ncommonly used for evaluating the estimators: i) variance; ii) a high\nprobability multiplicative-approximation guarantee. Almost all the known\nestimators are of the form $\\frac{1}{k}\\sum_{i=1}^k x_i^T A x_i$ for $x_i\\in\n\\mathbb{R}^n$ being i.i.d. for some special distribution.\n  Our main results are summarized as follows. We give an exact characterization\nof the minimum variance unbiased estimator in the broad class of linear\nnonadaptive estimators (which subsumes all the existing known estimators). We\nalso consider the query complexity lower bounds for any (possibly nonlinear and\nadaptive) estimators: (1) We show that any estimator requires\n$\\Omega(1/\\epsilon)$ queries to have a guarantee of variance at most\n$\\epsilon$. (2) We show that any estimator requires\n$\\Omega(\\frac{1}{\\epsilon^2}\\log \\frac{1}{\\delta})$ queries to achieve a\n$(1\\pm\\epsilon)$-multiplicative approximation guarantee with probability at\nleast $1 - \\delta$. Both above lower bounds are asymptotically tight.\n  As a corollary, we also resolve a conjecture in the seminal work of Avron and\nToledo (Journal of the ACM 2011) regarding the sample complexity of the\nGaussian Estimator. \n\n"}
{"id": "1406.0576", "contents": "Title: Welfare and Revenue Guarantees for Competitive Bundling Equilibrium Abstract: We study equilibria of markets with $m$ heterogeneous indivisible goods and\n$n$ consumers with combinatorial preferences. It is well known that a\ncompetitive equilibrium is not guaranteed to exist when valuations are not\ngross substitutes. Given the widespread use of bundling in real-life markets,\nwe study its role as a stabilizing and coordinating device by considering the\nnotion of \\emph{competitive bundling equilibrium}: a competitive equilibrium\nover the market induced by partitioning the goods for sale into fixed bundles.\nCompared to other equilibrium concepts involving bundles, this notion has the\nadvantage of simulatneous succinctness ($O(m)$ prices) and market clearance.\n  Our first set of results concern welfare guarantees. We show that in markets\nwhere consumers care only about the number of goods they receive (known as\nmulti-unit or homogeneous markets), even in the presence of complementarities,\nthere always exists a competitive bundling equilibrium that guarantees a\nlogarithmic fraction of the optimal welfare, and this guarantee is tight. We\nalso establish non-trivial welfare guarantees for general markets, two-consumer\nmarkets, and markets where the consumer valuations are additive up to a fixed\nbudget (budget-additive).\n  Our second set of results concern revenue guarantees. Motivated by the fact\nthat the revenue extracted in a standard competitive equilibrium may be zero\n(even with simple unit-demand consumers), we show that for natural subclasses\nof gross substitutes valuations, there always exists a competitive bundling\nequilibrium that extracts a logarithmic fraction of the optimal welfare, and\nthis guarantee is tight. The notion of competitive bundling equilibrium can\nthus be useful even in markets which possess a standard competitive\nequilibrium. \n\n"}
{"id": "1406.1158", "contents": "Title: Kernelization lower bound for Permutation Pattern Matching Abstract: A permutation $\\pi$ contains a permutation $\\sigma$ as a pattern if it\ncontains a subsequence of length $|\\sigma|$ whose elements are in the same\nrelative order as in the permutation $\\sigma$. This notion plays a major role\nin enumerative combinatorics. We prove that the problem does not have a\npolynomial kernel (under the widely believed complexity assumption $\\mbox{NP}\n\\not\\subseteq \\mbox{co-NP}/\\mbox{poly}$) by introducing a new polynomial\nreduction from the clique problem to permutation pattern matching. \n\n"}
{"id": "1406.1344", "contents": "Title: Is there room for highly magnetized pulsar wind nebulae among those\n  non-detected at TeV? Abstract: We make a time-dependent characterization of pulsar wind nebulae (PWNe)\nsurrounding some of the highest spin-down pulsars that have not yet been\ndetected at TeV. Our aim is assessing their possible level of magnetization. We\nanalyze the nebulae driven by J2022+3842 in G76.9+1.0, J0540-6919 in N158A (the\nCrab twin), J1400--6325 in G310.6--1.6, and J1124--5916 in G292.0+0.18, none of\nwhich have been found at TeV energies. For comparison we refer to published\nmodels of G54.1+0.3, the Crab nebula, and develop a model for N157B in the\nLarge Magellanic Cloud (LMC). We conclude that further observations of N158A\ncould lead to its detection at VHE. According to our model, a FIR energy\ndensity of 5 eV cm$^{-3}$ could already lead to a detection in H.E.S.S.\n(assuming no other IC target field) within 50 hours of exposure and just the\nCMB inverse Compton contribution would produce VHE photons at the CTA\nsensitivity. We also propose models for G76.9+1.0, G310.6--1.6 and G292.0+1.8\nwhich suggest their TeV detection in a moderate exposure for the latter two\nwith the current generation of Cherenkov telescopes. We analyze the possibility\nthat these PWNe are highly magnetized, where the low number of particles\nexplains the residual detection in X-rays and their lack of detection at TeV\nenergies. \n\n"}
{"id": "1406.4454", "contents": "Title: An Improved Approximation Algorithm for the Hard Uniform Capacitated\n  k-median Problem Abstract: In the $k$-median problem, given a set of locations, the goal is to select a\nsubset of at most $k$ centers so as to minimize the total cost of connecting\neach location to its nearest center. We study the uniform hard capacitated\nversion of the $k$-median problem, in which each selected center can only serve\na limited number of locations.\n  Inspired by the algorithm of Charikar, Guha, Tardos and Shmoys, we give a\n$(6+10\\alpha)$-approximation algorithm for this problem with increasing the\ncapacities by a factor of $2+\\frac{2}{\\alpha}, \\alpha\\geq 4$, which improves\nthe previous best $(32 l^2+28 l+7)$-approximation algorithm proposed by Byrka,\nFleszar, Rybicki and Spoerhase violating the capacities by factor\n$2+\\frac{3}{l-1}, l\\in \\{2,3,4,\\dots\\}$. \n\n"}
{"id": "1406.4718", "contents": "Title: Graph Isomorphism Parameterized by Elimination Distance to Bounded\n  Degree Abstract: A commonly studied means of parameterizing graph problems is the deletion\ndistance from triviality (Guo et al. 2004), which counts vertices that need to\nbe deleted from a graph to place it in some class for which efficient\nalgorithms are known. In the context of graph isomorphism, we define triviality\nto mean a graph with maximum degree bounded by a constant, as such graph\nclasses admit polynomial-time isomorphism tests. We generalise deletion\ndistance to a measure we call elimination distance to triviality, based on\nelimination trees or tree-depth decompositions. We establish that graph\ncanonisation, and thus graph isomorphism, is FPT when parameterized by\nelimination distance to bounded degree, extending results of Bouland et al.\n(2012). \n\n"}
{"id": "1406.5664", "contents": "Title: A magnetic reconnection model for explaining the multi-wavelength\n  emission of the microquasars Cyg X-1 and Cyg X-3 Abstract: Recent studies have indicated that cosmic ray acceleration by a first-order\nFermi process in magnetic reconnection current sheets can be efficient enough\nin the surrounds of compact sources. In this work, we discuss this acceleration\nmechanism operating in the core region of galactic black hole binaries (or\nmicroquasars) and show the conditions under which this can be more efficient\nthan shock acceleration. In addition, we compare the corresponding acceleration\nrate with the relevant radiative loss rates obtaining the possible energy\ncut-off of the accelerated particles and also compute the expected spectral\nenergy distribution (SED) for two sources of this class, namely Cygnus X-1 and\nCygnus X-3, considering both leptonic and hadronic processes. The derived SEDs\nare comparable to the observed ones in the low and high energy ranges. Our\nresults suggest that hadronic non-thermal emission due to photo-meson\nproduction may produce the very high energy gamma-rays in these microquasars. \n\n"}
{"id": "1406.5687", "contents": "Title: Parallel Algorithms for Counting Triangles in Networks with Large\n  Degrees Abstract: Finding the number of triangles in a network is an important problem in the\nanalysis of complex networks. The number of triangles also has important\napplications in data mining. Existing distributed memory parallel algorithms\nfor counting triangles are either Map-Reduce based or message passing interface\n(MPI) based and work with overlapping partitions of the given network. These\nalgorithms are designed for very sparse networks and do not work well when the\ndegrees of the nodes are relatively larger. For networks with larger degrees,\nMap-Reduce based algorithm generates prohibitively large intermediate data, and\nin MPI based algorithms with overlapping partitions, each partition can grow as\nlarge as the original network, wiping out the benefit of partitioning the\nnetwork.\n  In this paper, we present two efficient MPI-based parallel algorithms for\ncounting triangles in massive networks with large degrees. The first algorithm\nis a space-efficient algorithm for networks that do not fit in the main memory\nof a single compute node. This algorithm divides the network into\nnon-overlapping partitions. The second algorithm is for the case where the main\nmemory of each node is large enough to contain the entire network. We observe\nthat for such a case, computation load can be balanced dynamically and present\na dynamic load balancing scheme which improves the performance significantly.\nBoth of our algorithms scale well to large networks and to a large number of\nprocessors. \n\n"}
{"id": "1406.6576", "contents": "Title: Linear-Time Algorithm for Sliding Tokens on Trees Abstract: Suppose that we are given two independent sets $I_b$ and $I_r$ of a graph\nsuch that $|I_b|=|I_r|$, and imagine that a token is placed on each vertex in\n$I_b$. Then, the sliding token problem is to determine whether there exists a\nsequence of independent sets which transforms $I_b$ into $I_r$ so that each\nindependent set in the sequence results from the previous one by sliding\nexactly one token along an edge in the graph. This problem is known to be\nPSPACE-complete even for planar graphs, and also for bounded treewidth graphs.\nIn this paper, we thus study the problem restricted to trees, and give the\nfollowing three results: (1) the decision problem is solvable in linear time;\n(2) for a yes-instance, we can find in quadratic time an actual sequence of\nindependent sets between $I_b$ and $I_r$ whose length (i.e., the number of\ntoken-slides) is quadratic; and (3) there exists an infinite family of\ninstances on paths for which any sequence requires quadratic length. \n\n"}
{"id": "1407.0834", "contents": "Title: Shadow of five-dimensional rotating Myers-Perry black hole Abstract: A black hole casts a shadow as an optical appearance because of its strong\ngravitational field. We study the shadow cast by the five-dimensional\nMyers-Perry black hole with equal rotation parameters. We demonstrate that the\nnull geodesic equations can be integrated that allows us to investigate the\nshadow cast by a black hole. The shadow of a black hole is found to be a dark\nzone covered by deformed circle. Interestingly, the shapes of the black hole\nshadow are more distorted and size decreases for larger black hole spins.\nInterestingly, it turns out that, for fixed values of rotation parameter, the\nshadow is slightly smaller and less deformed than for its four-dimensional Kerr\nblack counterpart. Further, the shadow of the five-dimensional Kerr black hole\nis concentric deformed circles. The effect of rotation parameter on the shape\nand size of a naked singularity shadow is also analyzed. \n\n"}
{"id": "1407.0892", "contents": "Title: A Fully Polynomial-Time Approximation Scheme for Speed Scaling with\n  Sleep State Abstract: We study classical deadline-based preemptive scheduling of tasks in a\ncomputing environment equipped with both dynamic speed scaling and sleep state\ncapabilities: Each task is specified by a release time, a deadline and a\nprocessing volume, and has to be scheduled on a single, speed-scalable\nprocessor that is supplied with a sleep state. In the sleep state, the\nprocessor consumes no energy, but a constant wake-up cost is required to\ntransition back to the active state. In contrast to speed scaling alone, the\naddition of a sleep state makes it sometimes beneficial to accelerate the\nprocessing of tasks in order to transition the processor to the sleep state for\nlonger amounts of time and incur further energy savings. The goal is to output\na feasible schedule that minimizes the energy consumption. Since the\nintroduction of the problem by Irani et al. [16], its exact computational\ncomplexity has been repeatedly posed as an open question (see e.g. [2,8,15]).\nThe currently best known upper and lower bounds are a 4/3-approximation\nalgorithm and NP-hardness due to [2] and [2,17], respectively. We close the\naforementioned gap between the upper and lower bound on the computational\ncomplexity of speed scaling with sleep state by presenting a fully\npolynomial-time approximation scheme for the problem. The scheme is based on a\ntransformation to a non-preemptive variant of the problem, and a discretization\nthat exploits a carefully defined lexicographical ordering among schedules. \n\n"}
{"id": "1407.0950", "contents": "Title: On the Average-case Complexity of Pattern Matching with Wildcards Abstract: Pattern matching with wildcards is the problem of finding all factors of a\ntext $t$ of length $n$ that match a pattern $x$ of length $m$, where wildcards\n(characters that match everything) may be present. In this paper we present a\nnumber of fast average-case algorithms for pattern matching where wildcards are\nrestricted to either the pattern or the text, however, the results are easily\nadapted to the case where wildcards are allowed in both. We analyse the\n\\textit{average-case} complexity of these algorithms and show the first\nnon-trivial time bounds. These are the first results on the average-case\ncomplexity of pattern matching with wildcards which, as a by product, provide\nwith first provable separation in complexity between exact pattern matching and\npattern matching with wildcards in the word RAM model. \n\n"}
{"id": "1407.2407", "contents": "Title: $LCSk$++: Practical similarity metric for long strings Abstract: In this paper we present $LCSk$++: a new metric for measuring the similarity\nof long strings, and provide an algorithm for its efficient computation. With\never increasing size of strings occuring in practice, e.g. large genomes of\nplants and animals, classic algorithms such as Longest Common Subsequence (LCS)\nfail due to demanding computational complexity. Recently, Benson et al. defined\na similarity metric named $LCSk$. By relaxing the requirement that the\n$k$-length substrings should not overlap, we extend their definition into a new\nmetric. An efficient algorithm is presented which computes $LCSk$++ with\ncomplexity of $O((|X|+|Y|)\\log(|X|+|Y|))$ for strings $X$ and $Y$ under a\nrealistic random model. The algorithm has been designed with implementation\nsimplicity in mind. Additionally, we describe how it can be adjusted to compute\n$LCSk$ as well, which gives an improvement of the $O(|X|\\dot|Y|)$ algorithm\npresented in the original $LCSk$ paper. \n\n"}
{"id": "1407.2589", "contents": "Title: Gamma-ray burst supernovae as standardizable candles Abstract: A long-duration gamma-ray burst (GRB) marks the violent end of a massive\nstar. GRBs are rare in the universe, and their progenitor stars are thought to\npossess unique physical properties such as low metal content and rapid\nrotation, while the supernovae (SNe) that are associated with GRBs are expected\nto be highly aspherical. To date, it has been unclear whether GRB-SNe could be\nused as standardizable candles, with contrasting conclusions found by different\nteams. In this paper I present evidence that GRB-SNe have the potential to be\nused as standardizable candles, and show that a statistically significant\nrelation exists between the brightness and width of their decomposed light\ncurves relative to a template supernova. Every single nearby spectroscopically\nidentified GRB-SN, for which the rest-frame and host contributions have been\naccurately determined, follows this relation. Additionally, it is shown that\nnot only GRB-SNe, but perhaps all supernovae whose explosion is powered by a\ncentral engine, may eventually be used as a standardizable candle. Finally, I\nsuggest that the use of GRB-SNe as standardizable candles likely arises from\nfrom a combination of the viewing angle and similar explosion geometry in each\nevent, the latter which is influenced by the explosion mechanism of GRB-SNe. \n\n"}
{"id": "1407.4235", "contents": "Title: The List Coloring Reconfiguration Problem for Bounded Pathwidth Graphs Abstract: We study the problem of transforming one list (vertex) coloring of a graph\ninto another list coloring by changing only one vertex color assignment at a\ntime, while at all times maintaining a list coloring, given a list of allowed\ncolors for each vertex. This problem is known to be PSPACE-complete for\nbipartite planar graphs. In this paper, we first show that the problem remains\nPSPACE-complete even for bipartite series-parallel graphs, which form a proper\nsubclass of bipartite planar graphs. We note that our reduction indeed shows\nthe PSPACE-completeness for graphs with pathwidth two, and it can be extended\nfor threshold graphs. In contrast, we give a polynomial-time algorithm to solve\nthe problem for graphs with pathwidth one. Thus, this paper gives precise\nanalyses of the problem with respect to pathwidth. \n\n"}
{"id": "1407.4286", "contents": "Title: Constructing small tree grammars and small circuits for formulas Abstract: It is shown that every tree of size $n$ over a fixed set of $\\sigma$\ndifferent ranked symbols can be decomposed (in linear time as well as in\nlogspace) into $O\\big(\\frac{n}{\\log_\\sigma n}\\big) = O\\big(\\frac{n \\log\n\\sigma}{\\log n}\\big)$ many hierarchically defined pieces. Formally, such a\nhierarchical decomposition has the form of a straight-line linear context-free\ntree grammar of size $O\\big(\\frac{n}{\\log_\\sigma n}\\big)$, which can be used as\na compressed representation of the input tree. This generalizes an analogous\nresult for strings. Previous grammar-based tree compressors were not analyzed\nfor the worst-case size of the computed grammar, except for the top dag of\nBille et al., for which only the weaker upper bound of\n$O\\big(\\frac{n}{\\log_\\sigma^{0.19} n}\\big)$ (which was very recently improved\nto $O\\big(\\frac{n \\cdot \\log \\log_\\sigma n}{\\log_\\sigma n}\\big)$ by\nH\\\"ubschle-Schneider and Raman) for unranked and unlabelled trees has been\nderived. The main result is used to show that every arithmetical formula of\nsize $n$, in which only $m \\leq n$ different variables occur, can be\ntransformed (in linear time as well as in logspace) into an arithmetical\ncircuit of size $O\\big(\\frac{n \\cdot \\log m}{\\log n}\\big)$ and depth $O(\\log\nn)$. This refines a classical result of Brent from 1974, according to which an\narithmetical formula of size $n$ can be transformed into a logarithmic depth\ncircuit of size $O(n)$. \n\n"}
{"id": "1407.4640", "contents": "Title: A new algorithm for solving the rSUM problem Abstract: A determined algorithm is presented for solving the rSUM problem for any\nnatural r with a sub-quadratic assessment of time complexity in some cases. In\nterms of an amount of memory used the obtained algorithm is the nlog^3(n)\norder.\n  The idea of the obtained algorithm is based not considering integer numbers,\nbut rather k (is a natural) successive bits of these numbers in the binary\nnumeration system. It is shown that if a sum of integer numbers is equal to\nzero, then the sum of numbers presented by any k successive bits of these\nnumbers must be sufficiently \"close\" to zero. This makes it possible to discard\nthe numbers, which a fortiori, do not establish the solution. \n\n"}
{"id": "1407.6327", "contents": "Title: Compressed representation of Learning Spaces Abstract: Learning Spaces are certain set systems that are applied in the mathematical\nmodeling of education. We propose a suitable compression (without loss of\ninformation) of such set systems to facilitate their logical and statistical\nanalysis. Under certain circumstances compression is the prerequisite to\ncalculate the Learning Space in the first place. There are connections to the\ndual framework of Formal Concept Analysis and in particular to so called\nattribute exploration. \n\n"}
{"id": "1407.6790", "contents": "Title: FUSE Observations of a Full Orbit of Scorpius X-1 Abstract: We obtained UV spectra of X-ray binary Scorpius X-1 in the 900-1200 A range\nwith the Far Ultraviolet Spectroscopic Explorer over the full 0.79 day binary\norbit. The strongest emission lines are the doublet of O VI at 1032,1038 A and\nthe C III complex at 1175 A. The spectrum is affected by a multitude of narrow\ninterstellar absorption lines, both atomic and molecular. Examination of line\nvariability and Doppler tomograms suggests emission from both the neighborhood\nof the donor star and the accretion disk. Models of turbulence and Doppler\nbroadened Keplerian disk lines Doppler shifted with the orbit of the neutron\nstar added to narrow Gaussian emission lines with undetermined Doppler shift\nfit the data with consistent values of disk radius, inclination, and radial\nline brightness profile. The Doppler shift of the narrow component with the\norbit suggests an association with the donor star. We test our line models with\npreviously analyzed near UV spectra obtained with the Hubble Space Telescope\nGoddard High Resolution Spectrograph and archival spectra obtained with the HST\nCosmic Origins Spectrograph. \n\n"}
{"id": "1407.7216", "contents": "Title: PTAS for Minimax Approval Voting Abstract: We consider Approval Voting systems where each voter decides on a subset to\ncandidates he/she approves. We focus on the optimization problem of finding the\ncommittee of fixed size k minimizing the maximal Hamming distance from a vote.\nIn this paper we give a PTAS for this problem and hence resolve the open\nquestion raised by Carragianis et al. [AAAI'10]. The result is obtained by\nadapting the techniques developed by Li et al. [JACM'02] originally used for\nthe less constrained Closest String problem. The technique relies on extracting\ninformation and structural properties of constant size subsets of votes. \n\n"}
{"id": "1408.1157", "contents": "Title: Early inflation induced gravity waves can restrict Astro-Particle\n  physics Abstract: In this paper, we discuss limits on various astro-particle scenarios if the\nscale \\textit{and} the reheat temperature of the last relevant inflation were\nvery high. While the observed \"B\" like pattern of polarizations of the CMB\nsuggest a very high ($\\ge 10^{16}\\ GeV$) scale of a primordial (which motivated\nthis work initially) and may reflect effects of dust, we believe that\naddressing these issues is nonetheless very useful. We recall the potential\ndifficulties with various topological defects - monopoles, strings and domain\nwalls generated at the SSB (spontaneous symmetry breaking) of various gauge\nsymmetries. The main part of the paper is devoted to discussing difficulties\nwith long-lived heavy particles, which could be dark matter but cannot\nefficiently annihilate to the required residual density because of basic\nS-Matrix unitarity/analyticity limits. We indicate in simple terms yet in some\ndetail how the WIMP miracle occurs at $M(X)\\sim{TeV}$ and how the axiomatic\nupper bound presently updated to $M(X) \\le{110 TeV}$ was originally derived by\nGreist and Kamionokowski. We also argue that generically we expect the stronger\n$M(X)\\le{20\\ GeV}$ bound to hold. We then elaborate on the pure particle\nphysics approaches aiming to enhance the annihilation and evade the bounds. We\nfind that the only and in fact very satisfactory way of doing this requires\nendowing the particles with gauge interactions with a confinement scale lower\nthan $M(X)$. We also comment on models with light $O(KeV)$ dark matter, which\nwas supposed to be frozen in via out-of equilibrium processes so as to have the\nright relic densities pointing out that in many such cases \\textit{very} low\nreheat temperatures are indeed required and speculate on the large desert\nscenario of particle physics. Most of what we discuss is not new but was not\npresented in a coherent fashion. \n\n"}
{"id": "1408.1211", "contents": "Title: A Unifying Hierarchy of Valuations with Complements and Substitutes Abstract: We introduce a new hierarchy over monotone set functions, that we refer to as\n$\\mathcal{MPH}$ (Maximum over Positive Hypergraphs). Levels of the hierarchy\ncorrespond to the degree of complementarity in a given function. The highest\nlevel of the hierarchy, $\\mathcal{MPH}$-$m$ (where $m$ is the total number of\nitems) captures all monotone functions. The lowest level, $\\mathcal{MPH}$-$1$,\ncaptures all monotone submodular functions, and more generally, the class of\nfunctions known as $\\mathcal{XOS}$. Every monotone function that has a positive\nhypergraph representation of rank $k$ (in the sense defined by Abraham,\nBabaioff, Dughmi and Roughgarden [EC 2012]) is in $\\mathcal{MPH}$-$k$. Every\nmonotone function that has supermodular degree $k$ (in the sense defined by\nFeige and Izsak [ITCS 2013]) is in $\\mathcal{MPH}$-$(k+1)$. In both cases, the\nconverse direction does not hold, even in an approximate sense. We present\nadditional results that demonstrate the expressiveness power of\n$\\mathcal{MPH}$-$k$.\n  One can obtain good approximation ratios for some natural optimization\nproblems, provided that functions are required to lie in low levels of the\n$\\mathcal{MPH}$ hierarchy. We present two such applications. One shows that the\nmaximum welfare problem can be approximated within a ratio of $k+1$ if all\nplayers hold valuation functions in $\\mathcal{MPH}$-$k$. The other is an upper\nbound of $2k$ on the price of anarchy of simultaneous first price auctions.\n  Being in $\\mathcal{MPH}$-$k$ can be shown to involve two requirements -- one\nis monotonicity and the other is a certain requirement that we refer to as\n$\\mathcal{PLE}$ (Positive Lower Envelope). Removing the monotonicity\nrequirement, one obtains the $\\mathcal{PLE}$ hierarchy over all non-negative\nset functions (whether monotone or not), which can be fertile ground for\nfurther research. \n\n"}
{"id": "1408.3869", "contents": "Title: Treewidth of graphs with balanced separations Abstract: We prove that if every subgraph of a graph $G$ has a balanced separation of\norder at most $a$ then $G$ has treewidth at most $15a$. This establishes a\nlinear dependence between the treewidth and the separation number. \n\n"}
{"id": "1408.5129", "contents": "Title: An all-sky search for continuous gravitational waves in the Parkes\n  Pulsar Timing Array data set Abstract: We present results of an all-sky search in the Parkes Pulsar Timing Array\n(PPTA) Data Release 1 data set for continuous gravitational waves (GWs) in the\nfrequency range from $5\\times 10^{-9}$ to $2\\times 10^{-7}$ Hz. Such signals\ncould be produced by individual supermassive binary black hole systems in the\nearly stage of coalescence. We phase up the pulsar timing array data set to\nform, for each position on the sky, two data streams that correspond to the two\nGW polarizations and then carry out an optimal search for GW signals on these\ndata streams. Since no statistically significant GWs were detected, we place\nupper limits on the intrinsic GW strain amplitude $h_0$ for a range of GW\nfrequencies. For example, at $10^{-8}$ Hz our analysis has excluded with $95\\%$\nconfidence the presence of signals with $h_0\\geqslant 1.7\\times 10^{-14}$. Our\nnew limits are about a factor of four more stringent than those of Yardley et\nal. (2010) based on an earlier PPTA data set and a factor of two better than\nthose reported in the recent Arzoumanian et al. (2014) paper. We also present\nPPTA directional sensitivity curves and find that for the most sensitive region\non the sky, the current data set is sensitive to GWs from circular supermassive\nbinary black holes with chirp masses of $10^{9} M_{\\odot}$ out to a luminosity\ndistance of about 100 Mpc. Finally, we set an upper limit of $4 \\times 10^{-3}\n{\\rm{Mpc}}^{-3} {\\rm{Gyr}}^{-1}$ at $95\\%$ confidence on the coalescence rate\nof nearby ($z \\lesssim 0.1$) supermassive binary black holes in circular orbits\nwith chirp masses of $10^{10}M_{\\odot}$. \n\n"}
{"id": "1408.5412", "contents": "Title: On the Complexity of Role Colouring Planar Graphs, Trees and Cographs Abstract: We prove several results about the complexity of the role colouring problem.\nA role colouring of a graph $G$ is an assignment of colours to the vertices of\n$G$ such that two vertices of the same colour have identical sets of colours in\ntheir neighbourhoods. We show that the problem of finding a role colouring with\n$1< k <n$ colours is NP-hard for planar graphs. We show that restricting the\nproblem to trees yields a polynomially solvable case, as long as $k$ is either\nconstant or has a constant difference with $n$, the number of vertices in the\ntree. Finally, we prove that cographs are always $k$-role-colourable for\n$1<k\\leq n$ and construct such a colouring in polynomial time. \n\n"}
{"id": "1409.1348", "contents": "Title: Large induced forests in planar graphs with girth 4 or 5 Abstract: We give here some new lower bounds on the order of a largest induced forest\nin planar graphs with girth $4$ and $5$. In particular we prove that a\ntriangle-free planar graph of order $n$ admits an induced forest of order at\nleast $\\frac{6n+7}{11}$ , improving the lower bound of Salavatipour [M. R.\nSalavatipour, Large induced forests in triangle-free planar graphs, Graphs and\nCombinatorics, 22:113-126, 2006]. We also prove that a planar graph of order\n$n$ and girth at least $5$ admits an induced forest of order at least\n$\\frac{44n+50}{69}$. \n\n"}
{"id": "1409.2138", "contents": "Title: Streaming Lower Bounds for Approximating MAX-CUT Abstract: We consider the problem of estimating the value of max cut in a graph in the\nstreaming model of computation. At one extreme, there is a trivial\n$2$-approximation for this problem that uses only $O(\\log n)$ space, namely,\ncount the number of edges and output half of this value as the estimate for max\ncut value. On the other extreme, if one allows $\\tilde{O}(n)$ space, then a\nnear-optimal solution to the max cut value can be obtained by storing an\n$\\tilde{O}(n)$-size sparsifier that essentially preserves the max cut. An\nintriguing question is if poly-logarithmic space suffices to obtain a\nnon-trivial approximation to the max-cut value (that is, beating the factor\n$2$). It was recently shown that the problem of estimating the size of a\nmaximum matching in a graph admits a non-trivial approximation in\npoly-logarithmic space.\n  Our main result is that any streaming algorithm that breaks the\n$2$-approximation barrier requires $\\tilde{\\Omega}(\\sqrt{n})$ space even if the\nedges of the input graph are presented in random order. Our result is obtained\nby exhibiting a distribution over graphs which are either bipartite or\n$\\frac{1}{2}$-far from being bipartite, and establishing that\n$\\tilde{\\Omega}(\\sqrt{n})$ space is necessary to differentiate between these\ntwo cases. Thus as a direct corollary we obtain that $\\tilde{\\Omega}(\\sqrt{n})$\nspace is also necessary to test if a graph is bipartite or $\\frac{1}{2}$-far\nfrom being bipartite.\n  We also show that for any $\\epsilon > 0$, any streaming algorithm that\nobtains a $(1 + \\epsilon)$-approximation to the max cut value when edges arrive\nin adversarial order requires $n^{1 - O(\\epsilon)}$ space, implying that\n$\\Omega(n)$ space is necessary to obtain an arbitrarily good approximation to\nthe max cut value. \n\n"}
{"id": "1409.4828", "contents": "Title: Fast algorithmic self-assembly of simple shapes using random agitation Abstract: We study the power of uncontrolled random molecular movement in the nubot\nmodel of self-assembly. The nubot model is an asynchronous nondeterministic\ncellular automaton augmented with rigid-body movement rules (push/pull,\ndeterministically and programmatically applied to specific monomers) and random\nagitations (nondeterministically applied to every monomer and direction with\nequal probability all of the time). Previous work on the nubot model showed how\nto build simple shapes such as lines and squares quickly---in expected time\nthat is merely logarithmic of their size. These results crucially make use of\nthe programmable rigid-body movement rule: the ability for a single monomer to\ncontrol the movement of a large objects quickly, and only at a time and place\nof the programmers' choosing. However, in engineered molecular systems,\nmolecular motion is largely uncontrolled and fundamentally random. This raises\nthe question of whether similar results can be achieved in a more restrictive,\nand perhaps easier to justify, model where uncontrolled random movements, or\nagitations, are happening throughout the self-assembly process and are the only\nform of rigid-body movement. We show that this is indeed the case: we give a\npolylogarithmic expected time construction for squares using agitation, and a\nsublinear expected time construction to build a line. Such results are\nimpossible in an agitation-free (and movement-free) setting and thus show the\nbenefits of exploiting uncontrolled random movement. \n\n"}
{"id": "1409.5766", "contents": "Title: Implications of Fast Radio Burst Pulse Widths Abstract: The pulse widths, dispersion measures and dispersion indices of Fast Radio\nBursts (FRB) impose coupled constraints that all models must satisfy. We show\nthat if the dispersion measures resulted from propagation through the\nintergalactic medium at cosmological distances and the pulse widths were a\nconsequence of scattering by single thin screens, then the screens' electron\ndensities were $\\gtrsim 20$/cm$^3$, $10^8$ times the mean intergalactic\ndensity. This problem is resolved if the radiation scattered close to its\nsource, where high densities are possible. Observation of dispersion indices\nclose to their low density limit of $-2$ sets a model-independent upper bound\non the electron density and a lower bound on the size of the dispersive plasma\ncloud, excluding terrestrial or Solar System origin. The scattering and much of\nthe dispersion measures may be attributed to scattering regions about 1 AU from\nthe sources, with electron densities $\\sim 3 \\times 10^8$/cm$^3$. The inferred\nparameters are only marginally consistent; re-examination of the assumed\nrelation between dispersion measure and distance is warranted. Origin in an\nionized starburst or protogalaxy is suggested, but statistical arguments\nexclude compact young SNR in the Galactic neighborhood. An appendix applies\nthese arguments to PSR J1745-2900 at the Galactic Center. We suggest that its\npulse width and angular broadening may be reconciled if we are near a caustic\nor focal point produced by refraction, rather than by the classic thin sheet\nscattering model. \n\n"}
{"id": "1409.5816", "contents": "Title: 1-String CZ-Representation of Planar Graphs Abstract: In this paper, we prove that every planar 4-connected graph has a\nCZ-representation---a string representation using paths in a rectangular grid\nthat contain at most one vertical segment. Furthermore, two paths representing\nvertices $u,v$ intersect precisely once whenever there is an edge between $u$\nand $v$. The required size of the grid is $n \\times 2n$. \n\n"}
{"id": "1409.6365", "contents": "Title: Lift & Project Systems Performing on the Partial-Vertex-Cover Polytope Abstract: We study integrality gap (IG) lower bounds on strong LP and SDP relaxations\nderived by the Sherali-Adams (SA), Lovasz-Schrijver-SDP (LS+), and\nSherali-Adams-SDP (SA+) lift-and-project (L&P) systems for the\nt-Partial-Vertex-Cover (t-PVC) problem, a variation of the classic Vertex-Cover\nproblem in which only t edges need to be covered. t-PVC admits a\n2-approximation using various algorithmic techniques, all relying on a natural\nLP relaxation. Starting from this LP relaxation, our main results assert that\nfor every epsilon > 0, level-Theta(n) LPs or SDPs derived by all known L&P\nsystems that have been used for positive algorithmic results (but the Lasserre\nhierarchy) have IGs at least (1-epsilon)n/t, where n is the number of vertices\nof the input graph. Our lower bounds are nearly tight.\n  Our results show that restricted yet powerful models of computation derived\nby many L&P systems fail to witness c-approximate solutions to t-PVC for any\nconstant c, and for t = O(n). This is one of the very few known examples of an\nintractable combinatorial optimization problem for which LP-based algorithms\ninduce a constant approximation ratio, still lift-and-project LP and SDP\ntightenings of the same LP have unbounded IGs.\n  We also show that the SDP that has given the best algorithm known for t-PVC\nhas integrality gap n/t on instances that can be solved by the level-1 LP\nrelaxation derived by the LS system. This constitutes another rare phenomenon\nwhere (even in specific instances) a static LP outperforms an SDP that has been\nused for the best approximation guarantee for the problem at hand. Finally, one\nof our main contributions is that we make explicit of a new and simple\nmethodology of constructing solutions to LP relaxations that almost trivially\nsatisfy constraints derived by all SDP L&P systems known to be useful for\nalgorithmic positive results (except the La system). \n\n"}
{"id": "1409.6806", "contents": "Title: On spectral properties for graph matching and graph isomorphism problems Abstract: Problems related to graph matching and isomorphisms are very important both\nfrom a theoretical and practical perspective, with applications ranging from\nimage and video analysis to biological and biomedical problems. The graph\nmatching problem is challenging from a computational point of view, and\ntherefore different relaxations are commonly used. Although common relaxations\ntechniques tend to work well for matching perfectly isomorphic graphs, it is\nnot yet fully understood under which conditions the relaxed problem is\nguaranteed to obtain the correct answer.\n  In this paper we prove that the graph matching problem and its most common\nconvex relaxation, where the matching domain of permutation matrices is\nsubstituted with its convex hull of doubly-stochastic matrices, are equivalent\nfor a certain class of graphs, such equivalence being based on spectral\nproperties of the corresponding adjacency matrices. We also derive results\nabout the automorphism group of a graph, and provide fundamental spectral\nproperties of the adjacency matrix. \n\n"}
{"id": "1409.7651", "contents": "Title: Opacities and spectra of hydrogen atmospheres of moderately magnetized\n  neutron stars Abstract: There is observational evidence that central compact objects (CCOs) in\nsupernova remnants have moderately strong magnetic fields $B\\sim10^{11}$ G.\nMeanwhile, available models of partially ionized hydrogen atmospheres of\nneutron stars with strong magnetic fields are restricted to $B\\gtrsim10^{12}$\nG. We extend the equation of state and radiative opacities, presented in\nprevious papers for $10^{12}\\mbox{ G}\\lesssim B \\lesssim 10^{15}$ G, to weaker\nfields. An equation of state and radiative opacities for a partially ionized\nhydrogen plasma are obtained at magnetic fields $B$, temperatures $T$, and\ndensities $\\rho$ typical for atmospheres of CCOs and other isolated neutron\nstars with moderately strong magnetic fields. The first- and second-order\nthermodynamic functions, monochromatic radiative opacities, and Rosseland mean\nopacities are calculated and tabulated, taking account of partial ionization,\nfor $3\\times10^{10}\\mbox{ G}\\lesssim B\\lesssim 10^{12}$ G, $10^5$ K $\\lesssim\nT\\lesssim 10^7$ K, and a wide range of densities. Atmosphere models and spectra\nare calculated to verify the applicability of the results and to determine the\nrange of magnetic fields and effective temperatures where the incomplete\nionization of the hydrogen plasma is important. \n\n"}
{"id": "1409.7688", "contents": "Title: Irrelevant Components and Exact Computation of the Diameter Constrained\n  Reliability Abstract: Let $G=(V,E)$ be a simple graph with $|V|=n$ nodes and $|E|=m$ links, a\nsubset $K \\subseteq V$ of \\emph{terminals}, a vector $p=(p_1,...,p_m) \\in\n[0,1]^m$ and a positive integer $d$, called \\emph{diameter}. We assume nodes\nare perfect but links fail stochastically and independently, with probabilities\n$q_i=1-p_i$. The \\emph{diameter-constrained reliability} (DCR for short), is\nthe probability that the terminals of the resulting subgraph remain connected\nby paths composed by $d$ links, or less. This number is denoted by\n$R_{K,G}^{d}(p)$. The general computation of the parameter $R_{K,G}^{d}(p)$\nbelongs to the class of $\\mathcal{N}\\mathcal{P}$-Hard problems, since is\nsubsumes the complexity that a random graph is connected.\n  A discussion of the computational complexity for DCR-subproblems is provided\nin terms of the number of terminal nodes $k=|K|$ and diameter $d$. Either when\n$d=1$ or when $d=2$ and $k$ is fixed, the DCR is inside the class $\\mathcal{P}$\nof polynomial-time problems. The DCR turns $\\mathcal{N}\\mathcal{P}$-Hard even\nif $k \\geq 2$ and $d\\geq 3$ are fixed, or in an all-terminal scenario when\n$d=2$. The traditional approach is to design either exponential exact\nalgorithms or efficient solutions for particular graph classes.\n  The contributions of this paper are two-fold. First, a new recursive class of\ngraphs are shown to have efficient DCR computation. Second, we define a\nfactorization method in order to develop an exact DCR computation in general.\nThe approach is inspired in prior works related with the determination of\nirrelevant links and deletion-contraction formula. \n\n"}
{"id": "1409.8671", "contents": "Title: Light Echoes From Supernova 2014J in M82 Abstract: Type Ia SN 2014J exploded in the nearby starburst galaxy M82 = NGC 3032, and\nwas discovered at Earth about seven days later on 2014 January 21, reaching V\nmaximum light around 2014 February 5. SN 2014J is the closest SN Ia in at least\nfour decades and probably many more. Recent HST/WFC3 imaging (2014 September 5\nand 2015 February 2) of M82 around SN 2014J reveals a light echo at radii of\nabout 0.6 arcsec from the SN (corresponding to about 12 pc at the distance of\nM82). Likely additional light echoes reside at a smaller radii of about 0.4\narcsec. The major echo signal corresponds to echoing material about 330 pc in\nthe foreground of SN 2014J, and tends to be bright where pre-existing nebular\nstructure in M82 is also bright. The second, likely echo corresponds to\nforeground distances of 80 pc in front of the SN. Even one year after maximum\nlight, there are indications of further echo structures appearing at smalle\nradii, and future observations may show how extinction in these affect detected\necho farther from the SN, which will affect interpretion of details of the\nthree-dimensional structure of this gas and dust. Given enough data we might\neven use these considerations to constrain the near-SN material's shadowing on\ndistant echoing clouds, even without directly observing the foreground\nstructure. This is in addition to echoes in the near future might also reveal\ncircumstellar structure around SN 2014J's progenitor star from direct imaging\nobservations and other techniques. \n\n"}
{"id": "1410.0768", "contents": "Title: Space-Efficient Path-Reporting Approximate Distance Oracles Abstract: We consider approximate {\\em path-reporting} distance oracles, distance\nlabeling and labeled routing with extremely low space requirement, for general\nundirected graphs. For distance oracles, we show how to break the n\\log n space\nbound of Thorup and Zwick if approximate {\\em paths} rather than distances need\nto be reported. For approximate distance labeling and labeled routing, we break\nthe previously best known space bound of O(log n) words per vertex. The cost\nfor such space efficiency is an increased stretch. \n\n"}
{"id": "1410.2242", "contents": "Title: A Comprehensive Search for Dark Matter Annihilation in Dwarf Galaxies Abstract: We present a new formalism designed to discover dark matter annihilation\noccurring in the Milky Way's dwarf galaxies. The statistical framework extracts\nall available information in the data by simultaneously combining observations\nof all the dwarf galaxies and incorporating the impact of particle physics\nproperties, the distribution of dark matter in the dwarfs, and the detector\nresponse. The method performs maximally powerful frequentist searches and\nproduces confidence limits on particle physics parameters. Probability\ndistributions of test statistics under various hypotheses are constructed\nexactly, without relying on large sample approximations. The derived limits\nhave proper coverage by construction and claims of detection are not biased by\nimperfect background modeling. We implement this formalism using data from the\nFermi Gamma-ray Space Telescope to search for an annihilation signal in the\ncomplete sample of Milky Way dwarfs whose dark matter distributions can be\nreliably determined. We find that the observed data is consistent with\nbackground for each of the dwarf galaxies individually as well as in a joint\nanalysis. The strongest constraints are at small dark matter particle masses.\nTaking the median of the systematic uncertainty in dwarf density profiles, the\ncross section upper limits are below the pure s-wave weak scale relic abundance\nvalue (2.2 x 10^-26 cm^3/s) for dark matter masses below 26 GeV (for\nannihilation into b quarks), 29 GeV (tau leptons), 35 GeV (up, down, strange,\nand charm quarks and gluons), 6 GeV (electrons/positrons), and 114 GeV\n(two-photon final state). For dark matter particle masses less than 1 TeV,\nthese represent the strongest limits obtained to date using dwarf galaxies. \n\n"}
{"id": "1410.2628", "contents": "Title: Algorithm engineering for a quantum annealing platform Abstract: Recent advances bring within reach the viability of solving combinatorial\nproblems using a quantum annealing algorithm implemented on a purpose-built\nplatform that exploits quantum properties. However, the question of how to tune\nthe algorithm for most effective use in this framework is not well understood.\nIn this paper we describe some operational parameters that drive performance,\ndiscuss approaches for mitigating sources of error, and present experimental\nresults from a D-Wave Two quantum annealing processor. \n\n"}
{"id": "1410.2739", "contents": "Title: Revisiting coincidence rate between Gravitational Wave detection and\n  short Gamma-Ray Burst for the Advanced and third generation Abstract: We use realistic Monte-Carlo simulations including both gravitational-wave\nand short gamma-ray burst selection effects to revisit the coincident rate of\nbinary systems composed of two neutron stars or a neutron star and a black\nhole. We show that the fraction of GW triggers that can be observed in\ncoincidence with sGRBs is proportional to the beaming factor at $z=0$, but\nincreases with the distance, until it reaches 100 \\% at the GW detector horizon\ndistance. When this is taken into account the rate is improved by a factor of\n$~3$ compared to the simple beaming factor correction. We provide an estimate\nof the performance future GRB detectors should achieve in order to fully\nexploit the potentiality of the planned third generation GW antenna Einstein\nTelescope, and we propose a simple method to constrain the beaming angle of\nsGRBs. \n\n"}
{"id": "1410.3173", "contents": "Title: Characteristic Length and Clustering Abstract: We explore relations between various variational problems for graphs like\nEuler characteristic chi(G), characteristic length mu(G), mean clustering\nnu(G), inductive dimension iota(G), edge density epsilon(G), scale measure\nsigma(G), Hilbert action eta(G) and spectral complexity xi(G). A new insight in\nthis note is that the local cluster coefficient C(x) in a finite simple graph\ncan be written as a relative characteristic length L(x) of the unit sphere S(x)\nwithin the unit ball B(x) of a vertex. This relation L(x) = 2-C(x) will allow\nto study clustering in more general metric spaces like Riemannian manifolds or\nfractals. If eta is the average of scalar curvature s(x), a formula mu ~\n1+log(epsilon)/log(eta) of Newman, Watts and Strogatz relates mu with the edge\ndensity epsilon and average scalar curvature eta telling that large curvature\ncorrelates with small characteristic length. Experiments show that the\nstatistical relation mu ~ log(1/nu) holds for random or deterministic\nconstructed networks, indicating that small clustering is often associated to\nlarge characteristic lengths and lambda=mu/log(nu) can converge in some graph\nlimits of networks. Mean clustering nu, edge density epsilon and curvature\naverage eta therefore can relate with characteristic length mu on a statistical\nlevel. We also discovered experimentally that inductive dimension iota and\ncluster-length ratio lambda correlate strongly on Erdos-Renyi probability\nspaces. \n\n"}
{"id": "1410.3680", "contents": "Title: On the Origin of High-Energy Cosmic Neutrinos Abstract: Recently, the IceCube collaboration made a big announcement of the first\ndiscovery of high-energy cosmic neutrinos. Their origin is a new interesting\nmystery in astroparticle physics. The present multimessenger data may give us\nhints of connection to cosmic-ray and/or gamma-ray sources. We look over\npossible scenarios for the cosmic neutrino signal, and emphasize the importance\nof multimessenger approaches in identifying the PeV neutrino sources and\nobtaining crucial clues to the cosmic-ray origin. We also discuss some\npossibilities to study neutrino properties and probe new physics. \n\n"}
{"id": "1410.3747", "contents": "Title: Revisiting the SN1987A gamma-ray limit on ultralight axion-like\n  particles Abstract: We revise the bound from the supernova SN1987A on the coupling of ultralight\naxion-like particles (ALPs) to photons. In a core-collapse supernova, ALPs\nwould be emitted via the Primakoff process, and eventually convert into gamma\nrays in the magnetic field of the Milky Way. The lack of a gamma-ray signal in\nthe GRS instrument of the SMM satellite in coincidence with the observation of\nthe neutrinos emitted from SN1987A therefore provides a strong bound on their\ncoupling to photons. Due to the large uncertainty associated with the current\nbound, we revise this argument, based on state-of-the-art physical inputs both\nfor the supernova models and for the Milky-Way magnetic field. Furthermore, we\nprovide major amendments, such as the consistent treatment of\nnucleon-degeneracy effects and of the reduction of the nuclear masses in the\nhot and dense nuclear medium of the supernova. With these improvements, we\nobtain a new upper limit on the photon-ALP coupling: g_{a\\gamma} < 5.3 x\n10^{-12} GeV^{-1}, for m_a < 4.4 x 10^{-10} eV, and we also give its dependence\nat larger ALP masses. Moreover, we discuss how much the Fermi-LAT satellite\nexperiment could improve this bound, should a close-enough supernova explode in\nthe near future. \n\n"}
{"id": "1411.1445", "contents": "Title: The Gamma Ray Opacity of the Universe -- Indirect Measurements of the\n  Extragalactic Background Light Abstract: Indirect constraints on the intensity of the Extragalactic Background Light\n(EBL) were provided by recent studies of extragalactic sources emitting sub-TeV\nto multi-TeV photons. These constraints are provided thanks to the absorption\nof gamma rays by soft photons from the EBL (UV/optical/IR) via pair production\nby gamma - gamma interactions. This paper provides an overview of recent\nresults that have led to substantially reduced uncertainties on the EBL\nintensity over a wide range of wavelengths from 0.1 to 15 micron. \n\n"}
{"id": "1411.2577", "contents": "Title: Sketching and Embedding are Equivalent for Norms Abstract: An outstanding open question posed by Guha and Indyk in 2006 asks to\ncharacterize metric spaces in which distances can be estimated using efficient\nsketches. Specifically, we say that a sketching algorithm is efficient if it\nachieves constant approximation using constant sketch size. A well-known result\nof Indyk (J. ACM, 2006) implies that a metric that admits a constant-distortion\nembedding into $\\ell_p$ for $p\\in(0,2]$ also admits an efficient sketching\nscheme. But is the converse true, i.e., is embedding into $\\ell_p$ the only way\nto achieve efficient sketching?\n  We address these questions for the important special case of normed spaces,\nby providing an almost complete characterization of sketching in terms of\nembeddings. In particular, we prove that a finite-dimensional normed space\nallows efficient sketches if and only if it embeds (linearly) into\n$\\ell_{1-\\varepsilon}$ with constant distortion. We further prove that for\nnorms that are closed under sum-product, efficient sketching is equivalent to\nembedding into $\\ell_1$ with constant distortion. Examples of such norms\ninclude the Earth Mover's Distance (specifically its norm variant, called\nKantorovich-Rubinstein norm), and the trace norm (a.k.a. Schatten $1$-norm or\nthe nuclear norm). Using known non-embeddability theorems for these norms by\nNaor and Schechtman (SICOMP, 2007) and by Pisier (Compositio. Math., 1978), we\nthen conclude that these spaces do not admit efficient sketches either, making\nprogress towards answering another open question posed by Indyk in 2006.\n  Finally, we observe that resolving whether \"sketching is equivalent to\nembedding into $\\ell_1$ for general norms\" (i.e., without the above\nrestriction) is equivalent to resolving a well-known open problem in Functional\nAnalysis posed by Kwapien in 1969. \n\n"}
{"id": "1411.3047", "contents": "Title: Acyclic edge colourings of graphs with large girth Abstract: An edge colouring of a graph $G$ is called acyclic if it is proper and every\ncycle contains at least three colours. We show that for every $\\varepsilon>0$,\nthere exists a $g=g(\\varepsilon)$ such that if $G$ has girth at least $g$ then\n$G$ admits an acyclic edge colouring with at most $(1+\\varepsilon)\\Delta$\ncolours. \n\n"}
{"id": "1411.4267", "contents": "Title: Relativistic Gravothermal Instabilities Abstract: The thermodynamic instabilities of the self-gravitating, classical ideal gas\nare studied in the case of static, spherically symmetric configurations in\nGeneral Relativity taking into account the Tolman-Ehrenfest effect. One type of\ninstabilities is found at low energies, where thermal energy becomes too weak\nto halt gravity and another at high energies, where gravitational attraction of\nthermal pressure overcomes its stabilizing effect. These turning points of\nstability are found to depend on the total rest mass $\\mathcal{M}$ over the\nradius $R$. The low energy instability is the relativistic generalization of\nAntonov instability, which is recovered in the limit $G\\mathcal{M} \\ll R c^2$\nand low temperatures, while in the same limit and high temperatures, the high\nenergy instability recovers the instability of the radiation equation of state.\nIn the temperature versus energy diagram of series of equilibria, the two types\nof gravothermal instabilities make themselves evident as a double spiral! The\ntwo energy limits correspond also to radius limits. So that, stable static\nconfigurations exist only in between two marginal radii for any fixed energy\nwith negative thermal plus gravitational energy. Ultimate limits of rest mass,\nas well as total mass-energy, are reported. Applications to neutron cores are\ndiscussed. \n\n"}
{"id": "1411.4915", "contents": "Title: Gamma-ray flaring activity from the gravitationally lensed blazar PKS\n  1830-211 observed by Fermi LAT Abstract: The Large Area Telescope (LAT) on board the Fermi Gamma-ray Space Telescope\nroutinely detects the highly dust-absorbed, reddened, and MeV-peaked flat\nspectrum radio quasar PKS 1830-211 (z=2.507). Its apparent isotropic gamma-ray\nluminosity (E>100 MeV) averaged over $\\sim$ 3 years of observations and peaking\non 2010 October 14/15 at 2.9 X 10^{50} erg s^{-1}, makes it among the brightest\nhigh-redshift Fermi blazars. No published model with a single lens can account\nfor all of the observed characteristics of this complex system. Based on radio\nobservations, one expects time delayed variability to follow about 25 days\nafter a primary flare, with flux about a factor 1.5 less. Two large gamma-ray\nflares of PKS 1830-211 have been detected by the LAT in the considered period\nand no substantial evidence for such a delayed activity was found. This allows\nus to place a lower limit of about 6 on the gamma rays flux ratio between the\ntwo lensed images. Swift XRT observations from a dedicated Target of\nOpportunity program indicate a hard spectrum and with no significant\ncorrelation of X-ray flux with the gamma-ray variability. The spectral energy\ndistribution can be modeled with inverse Compton scattering of thermal photons\nfrom the dusty torus. The implications of the LAT data in terms of variability,\nthe lack of evident delayed flare events, and different radio and gamma-ray\nflux ratios are discussed. Microlensing effects, absorption, size and location\nof the emitting regions, the complex mass distribution of the system, an\nenergy-dependent inner structure of the source, and flux suppression by the\nlens galaxy for one image path may be considered as hypotheses for\nunderstanding our results. \n\n"}
{"id": "1412.1190", "contents": "Title: ASTRO-H White Paper - Broad-band Spectroscopy and Polarimetry Abstract: The broad energy range spanned by ASTRO-H instruments, from ~0.3 to 600 keV,\nwith its high spectral resolution calorimeter and sensitive hard X-ray imaging,\noffers unique opportunities to study black holes and their environments. The\nability to measure polarization is particularly novel, with potential sources\nincluding blazars, Galactic pulsars and X-ray binaries. In this White Paper, we\npresent an overview of the synergistic instrumental capabilities and the\nimprovements over prior missions. We also show how ASTRO-H fits into the\nmulti-wavelength landscape. We present in more detail examples and simulations\nof key science ASTRO-H can achieve in a typical 100 ksec observation when data\nfrom all four instruments are combined. Specifically, we consider observations\nof black-hole source (Cyg X-1 and GRS 1915+105), blazars (Mrk 421 and Mrk 501),\na quasar (3C 273), radio galaxies (Centaurus A and 3C 120), and active galaxies\nwith a strong starburst (Circinus and NGC 4945). We will also address possible\nnew discoveries expected from ASTRO-H. \n\n"}
{"id": "1412.3512", "contents": "Title: The equidistribution of some length three vincular patterns on\n  $S_n(132)$ Abstract: In 2012 B\\'ona showed the rather surprising fact that the cumulative number\nof occurrences of the classical patterns $231$ and $213$ are the same on the\nset of permutations avoiding $132$, beside the pattern based statistics $231$\nand $213$ do not have the same distribution on this set. Here we show that if\nit is required for the symbols playing the role of $1$ and $3$ in the\noccurrences of $231$ and $213$ to be adjacent, then the obtained statistics are\nequidistributed on the set of $132$-avoiding permutations. Actually, expressed\nin terms of vincular patterns, we prove the following more general results: the\nstatistics based on the patterns $b-ca$, $b-ac$ and $ba-c$, together with other\nstatistics, have the same joint distribution on $S_n(132)$, and so do the\npatterns $bc-a$ and $c-ab$; and up to trivial transformations, these statistics\nare the only based on length three proper (not classical nor adjacent) vincular\npatterns which are equidistributed on a set of permutations avoiding a\nclassical length three pattern. \n\n"}
{"id": "1412.5822", "contents": "Title: Bounding the Number of Hyperedges in Friendship $r$-Hypergraphs Abstract: For $r \\ge 2$, an $r$-uniform hypergraph is called a friendship\n$r$-hypergraph if every set $R$ of $r$ vertices has a unique 'friend' - that\nis, there exists a unique vertex $x \\notin R$ with the property that for each\nsubset $A \\subseteq R$ of size $r-1$, the set $A \\cup \\{x\\}$ is a hyperedge.\n  We show that for $r \\geq 3$, the number of hyperedges in a friendship\n$r$-hypergraph is at least $\\frac{r+1}{r} \\binom{n-1}{r-1}$, and we\ncharacterise those hypergraphs which achieve this bound. This generalises a\nresult given by Li and van Rees in the case when $r = 3$.\n  We also obtain a new upper bound on the number of hyperedges in a friendship\n$r$-hypergraph, which improves on a known bound given by Li, van Rees, Seo and\nSinghi when $r=3$. \n\n"}
{"id": "1412.6075", "contents": "Title: A Generalized Cheeger Inequality Abstract: The generalized conductance $\\phi(G,H)$ between two graphs $G$ and $H$ on the\nsame vertex set $V$ is defined as the ratio $$\n  \\phi(G,H) = \\min_{S\\subseteq V} \\frac{cap_G(S,\\bar{S})}{ cap_H(S,\\bar{S})},\n$$ where $cap_G(S,\\bar{S})$ is the total weight of the edges crossing from $S$\nto $\\bar{S}=V-S$. We show that the minimum generalized eigenvalue\n$\\lambda(L_G,L_H)$ of the pair of Laplacians $L_G$ and $L_H$ satisfies $$\n  \\lambda(L_G,L_H) \\geq \\phi(G,H) \\phi(G)/8, $$ where $\\phi(G)$ is the usual\nconductance of $G$. A generalized cut that meets this bound can be obtained\nfrom the generalized eigenvector corresponding to $\\lambda(L_G,L_H)$. The\ninequality complements a recent proof that $\\phi(G)$ cannot be replaced by\n$\\Theta(\\phi(G,H))$ in the above inequality, unless the Unique Games Conjecture\nis false. \n\n"}
{"id": "1412.6982", "contents": "Title: Production of 92Nb, 92Mo, and 146Sm in the gamma-process in SNIa Abstract: The knowledge of the production of extinct radioactivities like 92Nb and\n146Sm by photodisintegration processes in ccSN and SNIa models is essential for\ninterpreting abundances in meteoritic material and for Galactic Chemical\nEvolution (GCE). The 92Mo/92Nb and 146Sm/144Sm ratios provide constraints for\nGCE and production sites. We present results for SNIa with emphasis on nuclear\nuncertainties. \n\n"}
{"id": "1501.04594", "contents": "Title: Optical observations of PSR J2021+3651 in the Dragonfly Nebula with the\n  GTC Abstract: PSR J2021+3651 is a 17 kyr old rotation powered pulsar detected in the radio,\nX-rays, and $\\gamma$-rays. It powers a torus-like pulsar wind nebula with jets,\ndubbed the Dragonfly, which is very similar to that of the Vela pulsar. The\nDragonfly is likely associated with the extended TeV source VER J2019+368 and\nextended radio emission. We conducted first deep optical observations with the\nGTC in the Sloan $r'$ band to search for optical counterparts of the pulsar and\nits nebula. No counterparts were detected down to $r'\\gtrsim27.2$ and\n$\\gtrsim24.8$ for the point-like pulsar and the compact X-ray nebula,\nrespectively. We also reanalyzed Chandra archival X-ray data taking into\naccount an interstellar extinction--distance relation, constructed by us for\nthe Dragonfly line of sight using the red-clump stars as standard candles. This\nallowed us to constrain the distance to the pulsar, $D=1.8^{+1.7}_{-1.4}$ kpc\nat 90% confidence. It is much smaller than the dispersion measure distance of\n$\\sim$12 kpc but compatible with a $\\gamma$-ray \"pseudo-distance\" of 1 kpc.\nBased on that and the optical upper limits, we conclude that PSR J2021+3651,\nsimilar to the Vela pulsar, is a very inefficient nonthermal emitter in the\noptical and X-rays, while its $\\gamma$-ray efficiency is consistent with an\naverage efficiency for $\\gamma$-pulsars of similar age. Our optical flux upper\nlimit for the pulsar is consistent with the long-wavelength extrapolation of\nits X-ray spectrum while the nebula flux upper limit does not constrain the\nrespective extrapolation. \n\n"}
{"id": "1502.00551", "contents": "Title: High-Order Numerical-Relativity Simulations of Binary Neutron Stars Abstract: We report simulations of the inspiral and merger of binary neutron stars\nperformed with \\texttt{WhiskyTHC}, the first of a new generation of numerical\nrelativity codes employing higher than second-order methods for both the\nspacetime and the hydrodynamic evolution. We find that the use of higher-order\nschemes improves substantially the quality of the gravitational waveforms\nextracted from the simulations when compared to those computed using\ntraditional second-order schemes. The reduced de-phasing and the faster\nconvergence rate allow us to estimate the phase evolution of the gravitational\nwaves emitted, as well as the magnitude of finite-resolution effects, without\nthe need of phase- or time-alignments or rescalings of the waves, as sometimes\ndone in other works. Furthermore, by using an additional unpublished simulation\nat very high resolution, we confirm the robustness of our high convergence\norder of $3.2$. \n\n"}
{"id": "1502.00589", "contents": "Title: No trace of a single-degenerate companion in late spectra of SNe 2011fe\n  and 2014J Abstract: Left-over, ablated material from a possible non-degenerate companion can\nreveal itself after about one year in spectra of Type Ia SNe (SNe Ia). We have\nsearched for such material in spectra of SN 2011fe (at 294 days after the\nexplosion) and for SN 2014J (315 days past explosion). The observations are\ncompared with numerical models simulating the expected line emission. The\nspectral lines sought for are H-alpha, [O I] 6300 and [Ca II] 7291,7324, and\nthe expected width of these lines is about 1000 km/s. No signs of these lines\ncan be traced in any of the two supernovae. When systematic uncertainties are\nincluded, the limits on hydrogen-rich ablated gas in SNe 2011fe and 2014J are\n0.003 M_sun and 0.0085 M_sun, respectively, where the limit for SN 2014J is the\nsecond lowest ever, and the limit for SN 2011fe is a revision of a previous\nlimit. Limits are also put on helium-rich ablated gas. These limits are used,\nin conjunction with other data, to argue that these supernovae can stem from\ndouble-degenerate systems, or from single-degenerate systems with a spun\nup/spun down super-Chandrasekhar white dwarf. For SN 2011fe, other types of\nhydrogen-rich donors can likely be ruled out, whereas for SN 2014J a\nmain-sequence donor system with large intrinsic separation is still possible.\nHelium-rich donor systems cannot be ruled out for any of the two supernovae,\nbut the expected short delay time for such progenitors makes this possibility\nless likely, especially for SN 2011fe. The broad [Ni II] 7378 emission in SN\n2014J is redshifted by about +1300 km/s, as opposed to the known blueshift of\nroughly -1100 km/s for SN 2011fe. [Fe II] 7155 is also redshifted in SN 2014J.\nSN 2014J belongs to a minority of SNe Ia that both have a nebular redshift of\n[Fe II] 7155 and [Ni II] 7378, and a slow decline of the Si II 6355 absorption\ntrough just after B-band maximum. \n\n"}
{"id": "1502.01403", "contents": "Title: Distributed Estimation of Generalized Matrix Rank: Efficient Algorithms\n  and Lower Bounds Abstract: We study the following generalized matrix rank estimation problem: given an\n$n \\times n$ matrix and a constant $c \\geq 0$, estimate the number of\neigenvalues that are greater than $c$. In the distributed setting, the matrix\nof interest is the sum of $m$ matrices held by separate machines. We show that\nany deterministic algorithm solving this problem must communicate $\\Omega(n^2)$\nbits, which is order-equivalent to transmitting the whole matrix. In contrast,\nwe propose a randomized algorithm that communicates only $\\widetilde O(n)$\nbits. The upper bound is matched by an $\\Omega(n)$ lower bound on the\nrandomized communication complexity. We demonstrate the practical effectiveness\nof the proposed algorithm with some numerical experiments. \n\n"}
{"id": "1502.01755", "contents": "Title: Signals from dark atom formation in halos Abstract: We consider indirect detection signals of atomic dark matter, with a massive\ndark photon which mixes kinetically with hypercharge. In significant regions of\nparameter space, dark matter remains at least partially ionized today, and dark\natom formation can occur efficiently in dense regions, such as the centers of\ngalactic halos. The formation of dark atoms is accompanied by emission of a\ndark photon, which can subsequently decay into Standard Model particles. We\ndiscuss the expected signal strength and compare it to that of annihilating\ndark matter. As a case study, we explore the possibility that dark atom\nformation can account for the observed 511 keV line and outline the relevant\nparameter space. \n\n"}
{"id": "1502.03316", "contents": "Title: The Hardness of Approximation of Euclidean k-means Abstract: The Euclidean $k$-means problem is a classical problem that has been\nextensively studied in the theoretical computer science, machine learning and\nthe computational geometry communities. In this problem, we are given a set of\n$n$ points in Euclidean space $R^d$, and the goal is to choose $k$ centers in\n$R^d$ so that the sum of squared distances of each point to its nearest center\nis minimized. The best approximation algorithms for this problem include a\npolynomial time constant factor approximation for general $k$ and a\n$(1+\\epsilon)$-approximation which runs in time $poly(n) 2^{O(k/\\epsilon)}$. At\nthe other extreme, the only known computational complexity result for this\nproblem is NP-hardness [ADHP'09]. The main difficulty in obtaining hardness\nresults stems from the Euclidean nature of the problem, and the fact that any\npoint in $R^d$ can be a potential center. This gap in understanding left open\nthe intriguing possibility that the problem might admit a PTAS for all $k,d$.\n  In this paper we provide the first hardness of approximation for the\nEuclidean $k$-means problem. Concretely, we show that there exists a constant\n$\\epsilon > 0$ such that it is NP-hard to approximate the $k$-means objective\nto within a factor of $(1+\\epsilon)$. We show this via an efficient reduction\nfrom the vertex cover problem on triangle-free graphs: given a triangle-free\ngraph, the goal is to choose the fewest number of vertices which are incident\non all the edges. Additionally, we give a proof that the current best hardness\nresults for vertex cover can be carried over to triangle-free graphs. To show\nthis we transform $G$, a known hard vertex cover instance, by taking a graph\nproduct with a suitably chosen graph $H$, and showing that the size of the\n(normalized) maximum independent set is almost exactly preserved in the product\ngraph using a spectral analysis, which might be of independent interest. \n\n"}
{"id": "1502.03965", "contents": "Title: Uniform Kernelization Complexity of Hitting Forbidden Minors Abstract: The F-Minor-Free Deletion problem asks, for a fixed set F and an input\nconsisting of a graph G and integer k, whether k vertices can be removed from G\nsuch that the resulting graph does not contain any member of F as a minor. This\npaper analyzes to what extent provably effective and efficient preprocessing is\npossible for F-Minor-Free Deletion. Fomin et al. (FOCS 2012) showed that the\nspecial case Planar F-Deletion (when F contains at least one planar graph) has\na kernel of size f(F) * k^{g(F)} for some functions f and g. The degree g of\nthe polynomial grows very quickly; it is not even known to be computable. Fomin\net al. left open whether Planar F-Deletion has kernels whose size is uniformly\npolynomial, i.e., of the form f(F) * k^c for some universal constant c that\ndoes not depend on F. Our results in this paper are twofold. (1) We prove that\nsome Planar F-Deletion problems do not have uniformly polynomial kernels\n(unless NP is in coNP/poly). In particular, we prove that Treewidth-Eta\nDeletion does not have a kernel with O(k^{eta/4} - eps) vertices for any eps >\n0, unless NP is in coNP/poly. In fact, we even prove the kernelization lower\nbound for the larger parameter vertex cover number. This resolves an open\nproblem of Cygan et al. (IPEC 2011). It is a natural question whether further\nrestrictions on F lead to uniformly polynomial kernels. However, we prove that\neven when F contains a path, the degree of the polynomial must, in general,\ndepend on the set F. (2) A canonical F-Minor-Free Deletion problem when F\ncontains a path is Treedepth-eta Deletion: can k vertices be removed to obtain\na graph of treedepth at most eta? We prove that Treedepth-eta Deletion admits\nuniformly polynomial kernels with O(k^6) vertices for every fixed eta. In order\nto develop the kernelization we prove several new results about the structure\nof optimal treedepth-decompositions. \n\n"}
{"id": "1502.04644", "contents": "Title: Beyond the Runs Theorem Abstract: Recently, a short and elegant proof was presented showing that a binary word\nof length $n$ contains at most $n-3$ runs. Here we show, using the same\ntechnique and a computer search, that the number of runs in a binary word of\nlength $n$ is at most $\\frac{22}{23}n<0.957n$. \n\n"}
{"id": "1502.05222", "contents": "Title: Hierarchical Time-Dependent Oracles Abstract: We study networks obeying \\emph{time-dependent} min-cost path metrics, and\npresent novel oracles for them which \\emph{provably} achieve two unique\nfeatures: % (i) \\emph{subquadratic} preprocessing time and space,\n\\emph{independent} of the metric's amount of disconcavity; % (ii)\n\\emph{sublinear} query time, in either the network size or the actual\nDijkstra-Rank of the query at hand. \n\n"}
{"id": "1502.06116", "contents": "Title: The origin of the cosmic gamma-ray background in the MeV range Abstract: There has been much debate about the origin of the diffuse $\\gamma$--ray\nbackground in the MeV range. At lower energies, AGNs and Seyfert galaxies can\nexplain the background, but not above $\\simeq$0.3 MeV. Beyond $\\sim$10 MeV\nblazars appear to account for the flux observed. That leaves an unexplained gap\nfor which different candidates have been proposed, including annihilations of\nWIMPS. One candidate are Type Ia supernovae (SNe Ia). Early studies concluded\nthat they were able to account for the $\\gamma$--ray background in the gap,\nwhile later work attributed a significantly lower contribution to them.\n  All those estimates were based on SN Ia explosion models which did not\nreflect the full 3D hydrodynamics of SNe Ia explosions. In addition, new\nmeasurements obtained since 2010 have provided new, direct estimates of high-z\nSNe Ia rates beyond $z\\sim$2. We take into account these new advances to see\nthe predicted contribution to the gamma--ray background.\n  We use here a wide variety of explosion models and a plethora of new\nmeasurements of SNe Ia rates. SNe Ia still fall short of the observed\nbackground. Only for a fit, which would imply $\\sim$150\\% systematic error in\ndetecting SNe Ia events, do the theoretical predictions approach the observed\nfluxes. This fit is, however, at odds at the highest redshifts with recent SN\nIa rates estimates. Other astrophysical sources such as FSRQs do match the\nobserved flux levels in the MeV regime, while SNe Ia make up to 30--50\\% of the\nobserved flux. \n\n"}
{"id": "1502.06631", "contents": "Title: Polynomial Interpolation and Identity Testing from High Powers over\n  Finite Fields Abstract: We consider the problem of recovering (that is, interpolating) and identity\ntesting of a \"hidden\" monic polynomial $f$, given an oracle access to $f(x)^e$\nfor $x\\in{\\mathbb F_q}$ (extension fields access is not permitted). The naive\ninterpolation algorithm needs $O(e\\, \\mathrm{deg}\\, f)$ queries and thus\nrequires $e\\, \\mathrm{deg}\\, f<q$. We design algorithms that are asymptotically\nbetter in certain cases; requiring only $e^{o(1)}$ queries to the oracle. In\nthe randomized (and quantum) setting, we give a substantially better\ninterpolation algorithm, that requires only $O(\\mathrm{deg}\\, f \\log q)$\nqueries. Such results have been known before only for the special case of a\nlinear $f$, called the hidden shifted power problem.\n  We use techniques from algebra, such as effective versions of Hilbert's\nNullstellensatz, and analytic number theory, such as results on the\ndistribution of rational functions in subgroups and character sum estimates. \n\n"}
{"id": "1502.06948", "contents": "Title: Bounding the Clique-Width of $H$-free Chordal Graphs Abstract: A graph is $H$-free if it has no induced subgraph isomorphic to $H$.\nBrandst\\\"adt, Engelfriet, Le and Lozin proved that the class of chordal graphs\nwith independence number at most 3 has unbounded clique-width. Brandst\\\"adt, Le\nand Mosca erroneously claimed that the gem and the co-gem are the only two\n1-vertex $P_4$-extensions $H$ for which the class of $H$-free chordal graphs\nhas bounded clique-width. In fact we prove that bull-free chordal and\nco-chair-free chordal graphs have clique-width at most 3 and 4, respectively.\nIn particular, we find four new classes of $H$-free chordal graphs of bounded\nclique-width. Our main result, obtained by combining new and known results,\nprovides a classification of all but two stubborn cases, that is, with two\npotential exceptions we determine all graphs $H$ for which the class of\n$H$-free chordal graphs has bounded clique-width. We illustrate the usefulness\nof this classification for classifying other types of graph classes by proving\nthat the class of $(2P_1+P_3,K_4)$-free graphs has bounded clique-width via a\nreduction to $K_4$-free chordal graphs. Finally, we give a complete\nclassification of the (un)boundedness of clique-width of $H$-free weakly\nchordal graphs. \n\n"}
{"id": "1503.01120", "contents": "Title: The evolution of the X-ray luminosity functions of unabsorbed and\n  absorbed AGNs out to z~5 Abstract: We present new measurements of the evolution of the X-ray luminosity\nfunctions (XLFs) of unabsorbed and absorbed Active Galactic Nuclei (AGNs) out\nto z~5. We construct samples containing 2957 sources detected at hard (2-7 keV)\nX-ray energies and 4351 sources detected at soft (0.5-2 keV) energies from a\ncompilation of Chandra surveys supplemented by wide-area surveys from ASCA} and\nROSAT. We consider the hard and soft X-ray samples separately and find that the\nXLF based on either (initially neglecting absorption effects) is best described\nby a new flexible model parametrization where the break luminosity,\nnormalization and faint-end slope all evolve with redshift. We then incorporate\nabsorption effects, separately modelling the evolution of the XLFs of\nunabsorbed ($20<\\log N_\\mathrm{H}<22$) and absorbed ($22<\\log N_\\mathrm{H}<24$)\nAGNs, seeking a model that can reconcile both the hard- and soft-band samples.\nWe find that the absorbed AGN XLF has a lower break luminosity, a higher\nnormalization, and a steeper faint-end slope than the unabsorbed AGN XLF out to\nz~2. Hence, absorbed AGNs dominate at low luminosities, with the absorbed\nfraction falling rapidly as luminosity increases. Both XLFs undergo strong\nluminosity evolution which shifts the transition in the absorbed fraction to\nhigher luminosities at higher redshifts. The evolution in the shape of the\ntotal XLF is primarily driven by the changing mix of unabsorbed and absorbed\npopulations. \n\n"}
{"id": "1503.05698", "contents": "Title: The Lock-free $k$-LSM Relaxed Priority Queue Abstract: Priority queues are data structures which store keys in an ordered fashion to\nallow efficient access to the minimal (maximal) key. Priority queues are\nessential for many applications, e.g., Dijkstra's single-source shortest path\nalgorithm, branch-and-bound algorithms, and prioritized schedulers.\n  Efficient multiprocessor computing requires implementations of basic data\nstructures that can be used concurrently and scale to large numbers of threads\nand cores. Lock-free data structures promise superior scalability by avoiding\nblocking synchronization primitives, but the \\emph{delete-min} operation is an\ninherent scalability bottleneck in concurrent priority queues. Recent work has\nfocused on alleviating this obstacle either by batching operations, or by\nrelaxing the requirements to the \\emph{delete-min} operation.\n  We present a new, lock-free priority queue that relaxes the \\emph{delete-min}\noperation so that it is allowed to delete \\emph{any} of the $\\rho+1$ smallest\nkeys, where $\\rho$ is a runtime configurable parameter. Additionally, the\nbehavior is identical to a non-relaxed priority queue for items added and\nremoved by the same thread. The priority queue is built from a logarithmic\nnumber of sorted arrays in a way similar to log-structured merge-trees. We\nexperimentally compare our priority queue to recent state-of-the-art lock-free\npriority queues, both with relaxed and non-relaxed semantics, showing high\nperformance and good scalability of our approach. \n\n"}
{"id": "1503.05859", "contents": "Title: Evolutionary tracks of millisecond pulsars with low-mass companions Abstract: We consider the evolution of millisecond radio pulsars in binary systems with\na main-sequence or evolved stellar companion. Evolution of non-accreting binary\nsystems with \"eclipsing\" milisecond pulsars was described by Klu\\'zniak, Czerny\n& Ray (1992) who predicted that systems like the one containing the Terzan 5\nPSR 1744-24A will in the future become accreting low mass X-ray binaries\n(LMXBs), while PSR 1957+20 may evaporate its companion. The model presented in\nthe current paper gives similar results for these two objects and allows to\nobtain diverse evolutionary tracks of millisecond pulsars with low mass\ncompanions (black widows). Our results suggest that the properties of many\nblack widow systems can be explained by an ablation phase lasting a few hundred\nmillion years. Some of these sources may regain Roche lobe contact in a\ncomparable time, and become LMXBs. \n\n"}
{"id": "1503.06381", "contents": "Title: Balancing Communication for Multi-party Interactive Coding Abstract: We consider interactive coding in a setting where $n$ parties wish to compute\na joint function of their inputs via an interactive protocol over imperfect\nchannels. We assume that adversarial errors can comprise a\n$\\mathcal{O}(\\frac{1}{n})$ fraction of the total communication, occurring\nanywhere on the communication network. Our goal is to maintain a constant\nmultiplicative overhead in the total communication required, as compared to the\nerror-free setting, and also to balance the workload over the different\nparties. We build upon the prior protocol of Jain, Kalai, and Lewko, but while\nthat protocol relies on a single coordinator to shoulder a heavy burden\nthroughout the protocol, we design a mechanism to pass the coordination duties\nfrom party to party, resulting in a more even distribution of communication\nover the course of the computation. \n\n"}
{"id": "1503.07568", "contents": "Title: Router-level community structure of the Internet Autonomous Systems Abstract: The Internet is composed of routing devices connected between them and\norganized into independent administrative entities: the Autonomous Systems. The\nexistence of different types of Autonomous Systems (like large connectivity\nproviders, Internet Service Providers or universities) together with\ngeographical and economical constraints, turns the Internet into a complex\nmodular and hierarchical network. This organization is reflected in many\nproperties of the Internet topology, like its high degree of clustering and its\nrobustness.\n  In this work, we study the modular structure of the Internet router-level\ngraph in order to assess to what extent the Autonomous Systems satisfy some of\nthe known notions of community structure. We show that the modular structure of\nthe Internet is much richer than what can be captured by the current community\ndetection methods, which are severely affected by resolution limits and by the\nheterogeneity of the Autonomous Systems. Here we overcome this issue by using a\nmultiresolution detection algorithm combined with a small sample of nodes. We\nalso discuss recent work on community structure in the light of our results. \n\n"}
{"id": "1503.08147", "contents": "Title: Assessing the Observability of Hypernovae and Pair-Instability\n  Supernovae in the Early Universe Abstract: The era of the universe's first (Population III) stars is essentially\nunconstrained by observation. Ultra-luminous and massive stars from this time\naltered the chemistry of the cosmos, provided the radiative scaffolding to\nsupport the formation of the first protogalaxies, and facilitated the creation\nand growth of now-supermassive black holes. Unfortunately, because these stars\nlie literally at the edge of the observable universe, they will remain beyond\nthe reach of even the next generation of telescopes such as the James Webb\nSpace Telescope and the Thirty-Meter Telescope. In this paper, we provide a\nprimer to supernovae modeling and the first stars to make our discussion\naccessible to those new to or outside our field. We review recent work of the\nLos Alamos Supernova Light Curve Project and Brigham Young University to\nexplore the possibility of probing this era through observations of the\nspectacular deaths of the first stars. We find that many such brilliant\nsupernova explosions will be observable as far back as $\\sim 99$% of the\nuniverse's current age, tracing primordial star formation rates and the\nlocations of their protogalaxies on the sky. The observation of Population III\nsupernovae will be among the most spectacular discoveries in observational\nastronomy in the coming decade. \n\n"}
{"id": "1504.00681", "contents": "Title: Approximation of non-boolean 2CSP Abstract: We develop a polynomial time $\\Omega\\left ( \\frac 1R \\log R \\right)$\napproximate algorithm for Max 2CSP-$R$, the problem where we are given a\ncollection of constraints, each involving two variables, where each variable\nranges over a set of size $R$, and we want to find an assignment to the\nvariables that maximizes the number of satisfied constraints. Assuming the\nUnique Games Conjecture, this is the best possible approximation up to constant\nfactors.\n  Previously, a $1/R$-approximate algorithm was known, based on linear\nprogramming. Our algorithm is based on semidefinite programming (SDP) and on a\nnovel rounding technique. The SDP that we use has an almost-matching\nintegrality gap. \n\n"}
{"id": "1504.00954", "contents": "Title: Approximately Counting Triangles in Sublinear Time Abstract: We consider the problem of estimating the number of triangles in a graph.\nThis problem has been extensively studied in both theory and practice, but all\nexisting algorithms read the entire graph. In this work we design a {\\em\nsublinear-time\\/} algorithm for approximating the number of triangles in a\ngraph, where the algorithm is given query access to the graph. The allowed\nqueries are degree queries, vertex-pair queries and neighbor queries.\n  We show that for any given approximation parameter $0<\\epsilon<1$, the\nalgorithm provides an estimate $\\widehat{t}$ such that with high constant\nprobability, $(1-\\epsilon)\\cdot t< \\widehat{t}<(1+\\epsilon)\\cdot t$, where $t$\nis the number of triangles in the graph $G$. The expected query complexity of\nthe algorithm is $\\!\\left(\\frac{n}{t^{1/3}} + \\min\\left\\{m,\n\\frac{m^{3/2}}{t}\\right\\}\\right)\\cdot {\\rm poly}(\\log n, 1/\\epsilon)$, where\n$n$ is the number of vertices in the graph and $m$ is the number of edges, and\nthe expected running time is $\\!\\left(\\frac{n}{t^{1/3}} +\n\\frac{m^{3/2}}{t}\\right)\\cdot {\\rm poly}(\\log n, 1/\\epsilon)$. We also prove\nthat $\\Omega\\!\\left(\\frac{n}{t^{1/3}} + \\min\\left\\{m,\n\\frac{m^{3/2}}{t}\\right\\}\\right)$ queries are necessary, thus establishing that\nthe query complexity of this algorithm is optimal up to polylogarithmic factors\nin $n$ (and the dependence on $1/\\epsilon$). \n\n"}
{"id": "1504.01076", "contents": "Title: Nearly-optimal bounds for sparse recovery in generic norms, with\n  applications to $k$-median sketching Abstract: We initiate the study of trade-offs between sparsity and the number of\nmeasurements in sparse recovery schemes for generic norms. Specifically, for a\nnorm $\\|\\cdot\\|$, sparsity parameter $k$, approximation factor $K>0$, and\nprobability of failure $P>0$, we ask: what is the minimal value of $m$ so that\nthere is a distribution over $m \\times n$ matrices $A$ with the property that\nfor any $x$, given $Ax$, we can recover a $k$-sparse approximation to $x$ in\nthe given norm with probability at least $1-P$? We give a partial answer to\nthis problem, by showing that for norms that admit efficient linear sketches,\nthe optimal number of measurements $m$ is closely related to the doubling\ndimension of the metric induced by the norm $\\|\\cdot\\|$ on the set of all\n$k$-sparse vectors. By applying our result to specific norms, we cast known\nmeasurement bounds in our general framework (for the $\\ell_p$ norms, $p \\in\n[1,2]$) as well as provide new, measurement-efficient schemes (for the\nEarth-Mover Distance norm). The latter result directly implies more succinct\nlinear sketches for the well-studied planar $k$-median clustering problem.\nFinally, our lower bound for the doubling dimension of the EMD norm enables us\nto address the open question of [Frahling-Sohler, STOC'05] about the space\ncomplexity of clustering problems in the dynamic streaming model. \n\n"}
{"id": "1504.01130", "contents": "Title: Proving the Herman-Protocol Conjecture Abstract: Herman's self-stabilisation algorithm, introduced 25 years ago, is a\nwell-studied synchronous randomised protocol for enabling a ring of $N$\nprocesses collectively holding any odd number of tokens to reach a stable state\nin which a single token remains. Determining the worst-case expected time to\nstabilisation is the central outstanding open problem about this protocol. It\nis known that there is a constant $h$ such that any initial configuration has\nexpected stabilisation time at most $h N^2$. Ten years ago, McIver and Morgan\nestablished a lower bound of $4/27 \\approx 0.148$ for $h$, achieved with three\nequally-spaced tokens, and conjectured this to be the optimal value of $h$. A\nseries of papers over the last decade gradually reduced the upper bound on $h$,\nwith the present record (achieved in 2014) standing at approximately $0.156$.\nIn this paper, we prove McIver and Morgan's conjecture and establish that $h =\n4/27$ is indeed optimal. \n\n"}
{"id": "1504.01459", "contents": "Title: A Complete Worst-Case Analysis of Heapsort with Experimental\n  Verification of Its Results, A manuscript (MS) Abstract: A rigorous proof is presented that the number of comparisons of keys\nperformed in the worst case by ${\\tt Heapsort}$ on any array of size $N \\geq 2$\nis equal to: $ 2 (N-1)\\, ( \\, \\lg \\frac{N-1}{2} +\\varepsilon \\, ) - 2s_2(N) -\ne_2(N) + \\min (\\lfloor \\lg (N-1) \\rfloor, 2) + 6 + c, $ where $ \\varepsilon $,\ngiven by: $\\varepsilon = 1 + \\lceil \\lg \\, (N-1) \\rceil - \\lg \\, (N-1) -\n2^{\\lceil \\lg \\, (N-1) \\rceil - \\lg \\, (N-1)} ,$ is a function of $ N $ with\nthe minimum value 0 and and the supremum value $\\delta = 1 - \\lg e + \\lg \\lg e\n\\approx 0.0860713320559342$, $s_2(N)$ is the sum of all digits of the binary\nrepresentation of $N$, $e_2(N)$ is the exponent of $2$ in the prime\nfactorization of $N$, and $ c $ is a binary function on the set of integers\ndefined by: $c = 1$, if $N \\leq 2 ^{\\lceil \\lg N \\rceil} - 4$, and $c = 0$,\notherwise. An algorithm that generates worst-case input arrays of any size $ N\n\\geq 2 $ for ${\\tt Heapsort}$ is offered. The algorithm has been implemented in\nJava, runs in $O( N \\log N )$ time, and allows for precise experimental\nverification of the above formula. \n\n"}
{"id": "1504.01836", "contents": "Title: New Unconditional Hardness Results for Dynamic and Online Problems Abstract: There has been a resurgence of interest in lower bounds whose truth rests on\nthe conjectured hardness of well known computational problems. These\nconditional lower bounds have become important and popular due to the painfully\nslow progress on proving strong unconditional lower bounds. Nevertheless, the\nlong term goal is to replace these conditional bounds with unconditional ones.\nIn this paper we make progress in this direction by studying the cell probe\ncomplexity of two conjectured to be hard problems of particular importance:\nmatrix-vector multiplication and a version of dynamic set disjointness known as\nPatrascu's Multiphase Problem. We give improved unconditional lower bounds for\nthese problems as well as introducing new proof techniques of independent\ninterest. These include a technique capable of proving strong threshold lower\nbounds of the following form: If we insist on having a very fast query time,\nthen the update time has to be slow enough to compute a lookup table with the\nanswer to every possible query. This is the first time a lower bound of this\ntype has been proven. \n\n"}
{"id": "1504.02146", "contents": "Title: Discrete Stochastic Submodular Maximization: Adaptive vs. Non-Adaptive\n  vs. Offline Abstract: We consider the problem of stochastic monotone submodular function\nmaximization, subject to constraints. We give results on adaptivity gaps, and\non the gap between the optimal offline and online solutions. We present a\nprocedure that transforms a decision tree (adaptive algorithm) into a\nnon-adaptive chain. We prove that this chain achieves at least ${\\tau}$ times\nthe utility of the decision tree, over a product distribution and binary state\nspace, where ${\\tau} = \\min_{i,j} \\Pr[x_i=j]$. This proves an adaptivity gap of\n$1/{\\tau}$ (which is $2$ in the case of a uniform distribution) for the problem\nof stochastic monotone submodular maximization subject to state-independent\nconstraints. For a cardinality constraint, we prove that a simple adaptive\ngreedy algorithm achieves an approximation factor of $(1-1/e^{\\tau})$ with\nrespect to the optimal offline solution; previously, it has been proven that\nthe algorithm achieves an approximation factor of $(1-1/e)$ with respect to the\noptimal adaptive online solution. Finally, we show that there exists a\nnon-adaptive solution for the stochastic max coverage problem that is within a\nfactor $(1-1/e)$ of the optimal adaptive solution and within a factor of\n${\\tau}(1-1/e)$ of the optimal offline solution. \n\n"}
{"id": "1504.02605", "contents": "Title: Lempel Ziv Computation In Small Space (LZ-CISS) Abstract: For both the Lempel Ziv 77- and 78-factorization we propose algorithms\ngenerating the respective factorization using $(1+\\epsilon) n \\lg n + O(n)$\nbits (for any positive constant $\\epsilon \\le 1$) working space (including the\nspace for the output) for any text of size \\$n\\$ over an integer alphabet in\n$O(n / \\epsilon^{2})$ time. \n\n"}
{"id": "1504.04103", "contents": "Title: Faster Algorithms for Testing under Conditional Sampling Abstract: There has been considerable recent interest in distribution-tests whose\nrun-time and sample requirements are sublinear in the domain-size $k$. We study\ntwo of the most important tests under the conditional-sampling model where each\nquery specifies a subset $S$ of the domain, and the response is a sample drawn\nfrom $S$ according to the underlying distribution.\n  For identity testing, which asks whether the underlying distribution equals a\nspecific given distribution or $\\epsilon$-differs from it, we reduce the known\ntime and sample complexities from $\\tilde{\\mathcal{O}}(\\epsilon^{-4})$ to\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$, thereby matching the information\ntheoretic lower bound. For closeness testing, which asks whether two\ndistributions underlying observed data sets are equal or different, we reduce\nexisting complexity from $\\tilde{\\mathcal{O}}(\\epsilon^{-4} \\log^5 k)$ to an\neven sub-logarithmic $\\tilde{\\mathcal{O}}(\\epsilon^{-5} \\log \\log k)$ thus\nproviding a better bound to an open problem in Bertinoro Workshop on Sublinear\nAlgorithms [Fisher, 2004]. \n\n"}
{"id": "1504.05476", "contents": "Title: Optimal parameterized algorithms for planar facility location problems\n  using Voronoi diagrams Abstract: We study a general family of facility location problems defined on planar\ngraphs and on the 2-dimensional plane. In these problems, a subset of $k$\nobjects has to be selected, satisfying certain packing (disjointness) and\ncovering constraints. Our main result is showing that, for each of these\nproblems, the $n^{O(k)}$ time brute force algorithm of selecting $k$ objects\ncan be improved to $n^{O(\\sqrt{k})}$ time. The algorithm is based on an idea\nthat was introduced recently in the design of geometric QPTASs, but was not yet\nused for exact algorithms and for planar graphs. We focus on the Voronoi\ndiagram of a hypothetical solution of $k$ objects, guess a balanced separator\ncycle of this Voronoi diagram to obtain a set that separates the solution in a\nbalanced way, and then recurse on the resulting subproblems. We complement our\nstudy by giving evidence that packing problems have $n^{O(\\sqrt{k})}$ time\nalgorithms for a much more general class of objects than covering problems\nhave. \n\n"}
{"id": "1504.05515", "contents": "Title: Parameterized complexity dichotomy for $(r,\\ell)$-Vertex Deletion Abstract: For two integers $r, \\ell \\geq 0$, a graph $G = (V, E)$ is an\n$(r,\\ell)$-graph if $V$ can be partitioned into $r$ independent sets and $\\ell$\ncliques. In the parameterized $(r,\\ell)$-Vertex Deletion problem, given a graph\n$G$ and an integer $k$, one has to decide whether at most $k$ vertices can be\nremoved from $G$ to obtain an $(r,\\ell)$-graph. This problem is NP-hard if\n$r+\\ell \\geq 1$ and encompasses several relevant problems such as Vertex Cover\nand Odd Cycle Transversal. The parameterized complexity of $(r,\\ell)$-Vertex\nDeletion was known for all values of $(r,\\ell)$ except for $(2,1)$, $(1,2)$,\nand $(2,2)$. We prove that each of these three cases is FPT and, furthermore,\nsolvable in single-exponential time, which is asymptotically optimal in terms\nof $k$. We consider as well the version of $(r,\\ell)$-Vertex Deletion where the\nset of vertices to be removed has to induce an independent set, and provide\nalso a parameterized complexity dichotomy for this problem. \n\n"}
{"id": "1505.00024", "contents": "Title: Gamma Ray Burst as Sources of Exotic Particles Abstract: We consider the possible production of stable lightest first level KK\nparticle (LKP) in baryonic gamma ray bursts (GRB) out flows. We numerically\ncomputed the energy-dependent cross-sections of Kaluza-Klein (KK) excitations\nfor the Standard Model gauge bosons, photon and Z. Next, we determined the\nfeasibility of producing these KK excitations in gamma-ray emitting regions of\nGRBs. We found that a GRB fireball that accelerates baryons to energies greater\nthan 10^14 eV could produce KK excitations out to approximately 10^12 cm from\nthe central engine, indicating that GRBs may be a significant source of the\nLKP. Finally, we explore the potential observational consequences of our\nresults. \n\n"}
{"id": "1505.00612", "contents": "Title: On the Threshold of Intractability Abstract: We study the computational complexity of the graph modification problems\nThreshold Editing and Chain Editing, adding and deleting as few edges as\npossible to transform the input into a threshold (or chain) graph. In this\narticle, we show that both problems are NP-complete, resolving a conjecture by\nNatanzon, Shamir, and Sharan (Discrete Applied Mathematics, 113(1):109--128,\n2001). On the positive side, we show the problem admits a quadratic vertex\nkernel. Furthermore, we give a subexponential time parameterized algorithm\nsolving Threshold Editing in $2^{O(\\surd k \\log k)} + \\text{poly}(n)$ time,\nmaking it one of relatively few natural problems in this complexity class on\ngeneral graphs. These results are of broader interest to the field of social\nnetwork analysis, where recent work of Brandes (ISAAC, 2014) posits that the\nminimum edit distance to a threshold graph gives a good measure of consistency\nfor node centralities. Finally, we show that all our positive results extend to\nthe related problem of Chain Editing, as well as the completion and deletion\nvariants of both problems. \n\n"}
{"id": "1505.00619", "contents": "Title: Using higher-order Fourier analysis over general fields Abstract: Higher-order Fourier analysis, developed over prime fields, has been recently\nused in different areas of computer science, including list decoding,\nalgorithmic decomposition and testing. We extend the tools of higher-order\nFourier analysis to analyze functions over general fields. Using these new\ntools, we revisit the results in the above areas.\n  * For any fixed finite field $\\mathbb{K}$, we show that the list decoding\nradius of the generalized Reed Muller code over $\\mathbb{K}$ equals the minimum\ndistance of the code. Previously, this had been proved over prime fields [BL14]\nand for the case when $|\\mathbb{K}|-1$ divides the order of the code [GKZ08].\n  * For any fixed finite field $\\mathbb{K}$, we give a polynomial time\nalgorithm to decide whether a given polynomial $P: \\mathbb{K}^n \\to \\mathbb{K}$\ncan be decomposed as a particular composition of lesser degree polynomials.\nThis had been previously established over prime fields [Bha14, BHT15].\n  * For any fixed finite field $\\mathbb{K}$, we prove that all locally\ncharacterized affine-invariant properties of functions $f: \\mathbb{K}^n \\to\n\\mathbb{K}$ are testable with one-sided error. The same result was known when\n$\\mathbb{K}$ is prime [BFHHL13] and when the property is linear [KS08].\nMoreover, we show that for any fixed finite field $\\mathbb{F}$, an\naffine-invariant property of functions $f: \\mathbb{K}^n \\to \\mathbb{F}$, where\n$\\mathbb{K}$ is a growing field extension over $\\mathbb{F}$, is testable if it\nis locally characterized by constraints of bounded weight. \n\n"}
{"id": "1505.00875", "contents": "Title: Evaluating the Potential of a Dual Randomized Kaczmarz Solver for\n  Laplacian Linear Systems Abstract: A new method for solving Laplacian linear systems proposed by Kelner et al.\ninvolves the random sampling and update of fundamental cycles in a graph.\nKelner et al. proved asymptotic bounds on the complexity of this method but did\nnot report experimental results. We seek to both evaluate the performance of\nthis approach and to explore improvements to it in practice. We compare the\nperformance of this method to other Laplacian solvers on a variety of real\nworld graphs. We consider different ways to improve the performance of this\nmethod by exploring different ways of choosing the set of cycles and the\nsequence of updates, with the goal of providing more flexibility and potential\nparallelism. We propose a parallel model of the Kelner et al. method, for\nevaluating potential parallelism in terms of the span of edges updated at each\niteration. We provide experimental results comparing the potential parallelism\nof the fundamental cycle basis and our extended cycle set. Our preliminary\nexperiments show that choosing a non-fundamental set of cycles can save\nsignificant work compared to a fundamental cycle basis. \n\n"}
{"id": "1505.00933", "contents": "Title: Recollimation Shocks in Magnetized Relativistic Jets Abstract: We have performed two-dimensional special-relativistic magnetohydrodynamic\nsimulations of non-equilibrium over-pressured relativistic jets in cylindrical\ngeometry. Multiple stationary recollimation shock and rarefaction structures\nare produced along the jet by the nonlinear interaction of shocks and\nrarefaction waves excited at the interface between the jet and the surrounding\nambient medium. Although initially the jet is kinematically dominated, we have\nconsidered axial, toroidal and helical magnetic fields to investigate the\neffects of different magnetic-field topologies and strengths on the\nrecollimation structures. We find that an axial field introduces a larger\neffective gas-pressure and leads to stronger recollimation shocks and\nrarefactions, resulting in larger flow variations. The jet boost grows\nquadratically with the initial magnetic field. On the other hand, a toroidal\nfield leads to weaker recollimation shocks and rarefactions, modifying\nsignificantly the jet structure after the first recollimation rarefaction and\nshock. The jet boost decreases systematically. For a helical field, instead,\nthe behaviour depends on the magnetic pitch, with a phenomenology that ranges\nbetween the one seen for axial and toroidal magnetic fields, respectively. In\ngeneral, however, a helical magnetic field yields a more complex shock and\nrarefaction substructure close to the inlet that significantly modifies the jet\nstructure. The differences in shock structure resulting from different field\nconfigurations and strengths may have observable consequences for disturbances\npropagating through a stationary recollimation shock. \n\n"}
{"id": "1505.01281", "contents": "Title: Hydromagnetics of advective accretion flows around black holes: Removal\n  of angular momentum by large scale magnetic stresses Abstract: We show that the removal of angular momentum is possible in the presence of\nlarge scale magnetic stresses in geometrically thick, advective, sub-Keplerian\naccretion flows around black holes in steady-state, in the complete absence of\nalpha-viscosity. The efficiency of such an angular momentum transfer could be\nequivalent to that of alpha-viscosity with alpha=0.01-0.08. Nevertheless,\nrequired field is well below its equipartition value, leading to a magnetically\nstable disk flow. This is essentially important in order to describe the hard\nspectral state of the sources, when the flow is non/sub-Keplerian. We show in\nour simpler 1.5-dimensional, vertically averaged disk model that larger the\nvertical-gradient of azimuthal component of magnetic field, stronger the rate\nof angular momentum transfer is, which in turn may lead to a faster rate of\noutflowing matter. Finding efficient angular momentum transfer, in black hole\ndisks, via magnetic stresses alone is very interesting, when the generic origin\nof alpha-viscosity is still being explored. \n\n"}
{"id": "1505.02166", "contents": "Title: Nonthermally Dominated Electron Acceleration during Magnetic\n  Reconnection in a Low-beta Plasma Abstract: By means of fully kinetic simulations, we investigate electron acceleration\nduring magnetic reconnection in a nonrelativistic proton--electron plasma with\nconditions similar to solar corona and flares. We demonstrate that reconnection\nleads to a nonthermally dominated electron acceleration with a power-law energy\ndistribution in the nonrelativistic low-$\\beta$ regime but not in the\nhigh-$\\beta$ regime, where $\\beta$ is the ratio of the plasma thermal pressure\nand the magnetic pressure. The accelerated electrons contain most of the\ndissipated magnetic energy in the low-$\\beta$ regime. A guiding-center current\ndescription is used to reveal the role of electron drift motions during the\nbulk nonthermal energization. We find that the main acceleration mechanism is a\n\\textit{Fermi}-type acceleration accomplished by the particle curvature drift\nmotion along the electric field induced by the reconnection outflows. Although\nthe acceleration mechanism is similar for different plasma $\\beta$, low-$\\beta$\nreconnection drives fast acceleration on Alfv\\'enic timescales and develops\npower laws out of thermal distribution. The nonthermally dominated acceleration\nresulting from magnetic reconnection in low-$\\beta$ plasma may have strong\nimplications for the highly efficient electron acceleration in solar flares and\nother astrophysical systems. \n\n"}
{"id": "1505.03044", "contents": "Title: Duality between Temporal Networks and Signals: Extraction of the\n  Temporal Network Structures Abstract: We develop a framework to track the structure of temporal networks with a\nsignal processing approach. The method is based on the duality between networks\nand signals using a multidimensional scaling technique. This enables a study of\nthe network structure using frequency patterns of the corresponding signals. An\nextension is proposed for temporal networks, thereby enabling a tracking of the\nnetwork structure over time. A method to automatically extract the most\nsignificant frequency patterns and their activation coefficients over time is\nthen introduced, using nonnegative matrix factorization of the temporal\nspectra. The framework, inspired by audio decomposition, allows transforming\nback these frequency patterns into networks, to highlight the evolution of the\nunderlying structure of the network over time. The effectiveness of the method\nis first evidenced on a toy example, prior being used to study a temporal\nnetwork of face-to-face contacts. The extraction of sub-networks highlights\nsignificant structures decomposed on time intervals. \n\n"}
{"id": "1505.03570", "contents": "Title: Deep NuSTAR and Swift Monitoring Observations of the Magnetar 1E\n  1841-045 Abstract: We report on a 350-ks NuSTAR observation of the magnetar 1E 1841-045 taken in\n2013 September. During the observation, NuSTAR detected six bursts of short\nduration, with $T_{90}<1$ s. An elevated level of emission tail is detected\nafter the brightest burst, persisting for $\\sim$1 ks. The emission showed a\npower-law decay with a temporal index of 0.5 before returning to the persistent\nemission level. The long observation also provided detailed phase-resolved\nspectra of the persistent X-ray emission of the source. By comparing the\npersistent spectrum with that previously reported, we find that the source\nhard-band emission has been stable over approximately 10 years. The persistent\nhard X-ray emission is well fitted by a coronal outflow model, where $e^{+/-}$\npairs in the magnetosphere upscatter thermal X-rays. Our fit of phase-resolved\nspectra allowed us to estimate the angle between the rotational and magnetic\ndipole axes of the magnetar, $\\alpha_{mag}=0.25$, the twisted magnetic flux,\n$2.5\\times10^{26}\\rm \\ G\\ cm^2$, and the power released in the twisted\nmagnetosphere, $L_j=6\\times10^{36}\\rm \\ erg\\ s^{-1}$. Assuming this model for\nthe hard X-ray spectrum, the soft X-ray component is well fit by a\ntwo-blackbody model, with the hotter blackbody consistent with the footprint of\nthe twisted magnetic field lines on the star. We also report on the 3-year\nSwift monitoring observations obtained since 2011 July. The soft X-ray spectrum\nremained stable during this period, and the timing behavior was noisy, with\nlarge timing residuals. \n\n"}
{"id": "1505.05156", "contents": "Title: Statistics of Measuring Neutron Star Radii: Assessing A Frequentist and\n  A Bayesian Approach Abstract: Measuring neutron star radii with spectroscopic and timing techniques relies\non the combination of multiple observables to break the degeneracies between\nthe mass and radius introduced by general relativistic effects. Here, we\nexplore a previously used frequentist and a newly proposed Bayesian framework\nto obtain the most likely value and the uncertainty in such a measurement. We\nfind that, for the expected range of masses and radii and for realistic\nmeasurement errors, the frequentist approach suffers from biases that are\nlarger than the accuracy in the radius measurement required to distinguish\nbetween the different equations of state. In contrast, in the Bayesian\nframework, the inferred uncertainties are larger, but the most likely values do\nnot suffer from such biases. We also investigate ways of quantifying the degree\nof consistency between different spectroscopic measurements from a single\nsource. We show that a careful assessment of the systematic uncertainties in\nthe measurements eliminates the need for introducing ad hoc biases, which lead\nto artificially large inferred radii. \n\n"}
{"id": "1505.05837", "contents": "Title: On the detection of neutrinos from solar flares using pion-decay photons\n  to provide a time window template Abstract: Since the end of the eighties and in response to a reported increase in the\ntotal neutrino flux in the Homestake experiment in coincidence with solar\nflares, solar neutrino detectors have searched for solar flare signals. Even\nthough these detectors have used different solar flare samples and analyses,\nnone of them has been able to confirm the possible signal seen by Homestake.\nNeutrinos from the decay of mesons, which are themselves produced in collisions\nof accelerated ions with the solar atmosphere would provide a novel window on\nthe underlying physics of the hadronic acceleration and interaction processes\nduring solar flares. Solar flare neutrino flux measurements would indeed help\nto constrain current parameters such as the composition of the accelerated\nflux, the proton/ion spectral index and the high energy cutoff or the magnetic\nconfiguration in the interaction region. We describe here a new way to search\nfor these neutrinos by considering a specific solar flare sample and a data\ndriven time window template which will improve the likelihood of neutrino\ndetection. \n\n"}
{"id": "1505.06161", "contents": "Title: Dynamics of Lattice Triangulations on Thin Rectangles Abstract: We consider random lattice triangulations of $n\\times k$ rectangular regions\nwith weight $\\lambda^{|\\sigma|}$ where $\\lambda>0$ is a parameter and\n$|\\sigma|$ denotes the total edge length of the triangulation. When\n$\\lambda\\in(0,1)$ and $k$ is fixed, we prove a tight upper bound of order $n^2$\nfor the mixing time of the edge-flip Glauber dynamics. Combined with the\npreviously known lower bound of order $\\exp(\\Omega(n^2))$ for $\\lambda>1$ [3],\nthis establishes the existence of a dynamical phase transition for thin\nrectangles with critical point at $\\lambda=1$. \n\n"}
{"id": "1505.08162", "contents": "Title: Dimension and cut vertices: an application of Ramsey theory Abstract: Motivated by quite recent research involving the relationship between the\ndimension of a poset and graph-theoretic properties of its cover graph, we show\nthat for every $d\\geq 1$, if $P$ is a poset and the dimension of a subposet $B$\nof $P$ is at most $d$ whenever the cover graph of $B$ is a block of the cover\ngraph of $P$, then the dimension of $P$ is at most $d+2$. We also construct\nexamples which show that this inequality is best possible. We consider the\nproof of the upper bound to be fairly elegant and relatively compact. However,\nwe know of no simple proof for the lower bound, and our argument requires a\npowerful tool known as the Product Ramsey Theorem. As a consequence, our\nconstructions involve posets of enormous size. \n\n"}
{"id": "1506.01063", "contents": "Title: Very high energy neutrino emission from the core of low luminosity AGNs\n  triggered by magnetic reconnection acceleration Abstract: The detection of astrophysical very high energy (VHE) neutrinos in the range\nof TeV-PeV energies by the IceCube observatory has opened a new season in high\nenergy astrophysics. Energies ~PeV imply that the neutrinos are originated from\nsources where cosmic rays (CRs) can be accelerated up to ~ 10^{17}eV. Recently,\nwe have shown that the observed TeV gamma-rays from radio-galaxies may have a\nhadronic origin in their nuclear region and in such a case this could lead to\nneutrino production. In this paper we show that relativistic protons\naccelerated by magnetic reconnection in the core region of these sources may\nproduce VHE neutrinos via the decay of charged pions produced by photo-meson\nprocess. We have also calculated the diffuse flux of VHE neutrinos and found\nthat it can be associated to the IceCube data. \n\n"}
{"id": "1506.01222", "contents": "Title: On the Physical Nature of the Source of Ultraluminous X-ray Pulsations Abstract: To reconcile the observed unusual high luminosity of NuSTAR X-ray pulsations\nfrom M82X-2 with the most extreme violation of the Eddington limit, and in view\nthat the persistent X-ray radiation from M82X-2 almost precludes the\npossibility of common pulsars, we tackle the problem by the implications of\n{\\em microscopic theory of black hole} (MTBH). The preceding developments of\nMTBH are proved to be quite fruitful for the physics of ultra-high energy (UHE)\ncosmic-rays. Namely, replacing a central singularity by the infrastructures\ninside event horizon, subject to certain rules, MTBH explains the origin of\nZeV-neutrinos which are of vital interest for the source of UHE-particles. The\nM82X-2 is assumed to be a spinning intermediate mass black hole resided in\nfinal stage of growth. As a corollary, the thermal blackbody X-ray emission\narisen due to the rotational kinetic energy of black hole escapes from event\nhorizon through the vista to outside world that detected as ultraluminous X-ray\npulsations. The M82X-2 indeed releases $\\sim 99.6\\%$ of its pulsed radiative\nenergy predominantly in the X-ray bandpass $0.3-30$ keV. We derive a pulse\nprofile and give a quantitative account of energetics and orbital parameters of\nthe semi-detached X-ray binary containing a primary accretor M82X-2 of inferred\nmass $M\\simeq 138.5-226\\,M_{\\odot}$ and secondary massive, $M_{2}> 48.3-\n64.9\\,M_{\\odot}$, O/B-type donor star with radius of $R> 22.1-\n25.7\\,R_{\\odot}$, respectively. We compute the torque added to M82X-2 per unit\nmass of accreted matter which yields the measured spin-up rate. \n\n"}
{"id": "1506.01703", "contents": "Title: Searching for FUV line emission from $10^7$ K gas in massive elliptical\n  galaxies and galaxy clusters as a tracer of turbulent velocities Abstract: Non-thermal pressure from turbulence and bulk flows is a fundamental\ningredient in hot gaseous halos, and in the intracluster medium it will be\nmeasured through emission line kinematics with calorimeters on future X-ray\nspacecraft. In this paper we present a complementary method for measuring these\neffects, using forbidden FUV emission lines of highly ionized Iron which trace\n$10^7$ K gas. The brightest of these is [Fe XXI] $\\lambda$1354.1. We search for\nthese lines in archival HST-COS spectra from the well-known elliptical galaxies\nM87 and NGC4696, which harbor large reservoirs of $10^7$ K gas. We report a\n2.2$\\sigma$ feature which we attribute to [Fe XXI] from a filament in M87, and\npositive residuals in the nuclei of M87 and NGC4696, for which the 90\\% upper\nlimits on the line flux are close to the predicted fluxes based on X-ray\nobservations. In a newer reduction of the data from the Hubble Spectroscopic\nLegacy Archive, these limits become tighter and the [Fe XXI] feature reaches a\nformal significance of 5.3$\\sigma$, neglecting uncertainty in fitting the\ncontinuum. Using our constraints, we perform emission measure analysis,\nconstraining the characteristic path length and column density of the\n$\\sim10^7$ K gas. We also examine several sightlines towards filaments or\ncooling flows in other galaxy clusters, for which the fraction of gas at $10^7$\nK is unknown, and place upper limits on its emission measure in each case. A\nmedium-resolution HST-COS observation of the M87 filament for $\\sim$10 orbits\nwould confirm our detection of [Fe XXI] and measure its width. \n\n"}
{"id": "1506.02359", "contents": "Title: X-raying extended emission and rapid decay of short gamma-ray bursts Abstract: Extended emission is a mystery in short gamma-ray bursts (SGRBs). By making\ntime resolved spectral analyses of brightest nine events observed by ${\\it\nSwift}$ XRT, we obviously classify the early X-ray emission of SGRBs into two\ntypes. One is the extended emission with exponentially rapid decay, which shows\nsignificant spectral softening during hundreds seconds since the SGRB trigger\nand is also detected by ${\\it Swift}$-BAT. The other is a dim afterglow only\nshowing power-law decay over $10^4$ s. The correlations between the temporal\ndecay and spectral indices of the extended emissions are inconsistent with the\n$\\alpha$-$\\beta$ correlation expected for the high-latitude curvature emission\nfrom a uniform jet. The observed too-rapid decay suggests the emission from a\nphotosphere or a patchy surface, and manifests the stopping central engine via\nsuch as magnetic reconnection at the black hole. \n\n"}
{"id": "1506.02631", "contents": "Title: A Model of the Stochastic Gravitational-Wave Background due to Core\n  Collapse to Black Holes Abstract: Superposition of gravitational waves generated by astrophysical sources is\nexpected to give rise to the stochastic gravitational-wave background. We focus\non the background generated by the ring-down of black holes produced in the\nstellar core collapse events across the universe. We systematically study the\nparameter space in this model, including the most recent information about the\nstar formation rate and about the population of black holes as a function of\nredshift and of metallicity. We investigate the accessibility of this\ngravitational wave background to the upcoming gravitational-wave detectors,\nsuch as Advanced LIGO and Einstein Telescope. \n\n"}
{"id": "1506.04524", "contents": "Title: Simultaneous spectral and reverberation modelling of relativistic\n  reflection in Mrk 335 Abstract: We present an X-ray spectral and timing model to investigate the broad and\nvariable iron line seen in the high flux state of Mrk 335. The model consists\nof a variable X-ray source positioned along the rotation axis of the black hole\nthat illuminates the accretion disc producing a back-scattered, ionized\nreflection spectrum. We compute time lags including full dilution effects and\nperform simultaneous fitting of the 2-10 keV spectrum and the\nfrequency-dependent time lags of 2.5-4 vs. 4-6.5 keV bands. The best-fitting\nparameters are consistent with a black hole mass of approximately 1.3 x 10^7\nM_sun, disc inclination of 45 degrees and the photon index of the direct\ncontinuum of 2.4. The iron abundance is 0.5 and the ionization parameter is\n10^3 erg cm / s at the innermost part of the disc and decreases further out.\nThe X-ray source height is very small, approximately 2 r_g. Furthermore, we fit\nthe Fe L lags simultaneously with the 0.3-10 keV spectrum. The key parameters\nare comparable to those previously obtained. We also report the differences\nbelow 2 keV using the xillver and reflionx models which could affect the\ninterpretation of the soft excess. While simultaneously fitting spectroscopic\nand timing data can break the degeneracy between the source height and the\nblack hole mass, we find that the measurements of the source height and the\ncentral mass significantly depend on the ionization state of the disc and are\npossibly model-dependent. \n\n"}
{"id": "1506.04910", "contents": "Title: Sequential Consistency and Concurrent Data Structures Abstract: Linearizability, the de facto correctness condition for concurrent data\nstructure implementations, despite its intuitive appeal is known to lead to\npoor scalability. This disadvantage has led researchers to design scalable data\nstructures satisfying consistency conditions weaker than linearizability.\nDespite this recent trend, sequential consistency as a strictly weaker\nconsistency condition than linearizability has received no interest.\n  In this paper, we investigate the applicability of sequential consistency as\nan alternative correctness criterion for concurrent data structure\nimplementations. Our first finding formally justifies the reluctance in moving\ntowards sequentially consistent data structures: Implementations in which each\nthread modifies only its thread-local variables are sequentially consistent for\nvarious standard data structures such as pools, queues and stacks. We also show\nthat for almost all data structures, and the data structures we consider in\nthis paper, it is possible to have sequentially consistent behaviors in which a\ndesignated thread does not synchronize at all. As a potential remedy, we define\na hierarchy of quantitatively strengthened variants of sequential consistency\nsuch that the stronger the variant the more synchronization it enforces which\nat the limit is equal to that enforced by linearizability. \n\n"}
{"id": "1506.05299", "contents": "Title: Shining in the Dark: the Spectral Evolution of the First Black Holes Abstract: Massive Black Hole (MBH) seeds at redshift $z \\gtrsim 10$ are now thought to\nbe key ingredients to explain the presence of the super-massive ($10^{9-10} \\,\n\\mathrm{M_{\\odot}}$) black holes in place $ < 1 \\, \\mathrm{Gyr}$ after the Big\nBang. Once formed, massive seeds grow and emit copious amounts of radiation by\naccreting the left-over halo gas; their spectrum can then provide crucial\ninformation on their evolution. By combining radiation-hydrodynamic and\nspectral synthesis codes, we simulate the time-evolving spectrum emerging from\nthe host halo of a MBH seed with initial mass $10^5 \\, \\mathrm{M_{\\odot}}$,\nassuming both standard Eddington-limited accretion, or slim accretion disks,\nappropriate for super-Eddington flows. The emission occurs predominantly in the\nobserved infrared-submm ($1-1000 \\, \\mathrm{\\mu m}$) and X-ray ($0.1 - 100 \\,\n\\mathrm{keV}$) bands. Such signal should be easily detectable by JWST around\n$\\sim 1 \\, \\mathrm{\\mu m}$ up to $z \\sim 25$, and by ATHENA (between $0.1$ and\n$10 \\, \\mathrm{keV}$, up to $z \\sim 15$). Ultra-deep X-ray surveys like the\nChandra Deep Field South could have already detected these systems up to $z\n\\sim 15$. Based on this, we provide an upper limit for the $z \\gtrsim 6$ MBH\nmass density of $\\rho_{\\bullet} \\lesssim 2.5 \\times 10^{2} \\, \\mathrm{M_{\\odot}\n\\, Mpc^{-3}}$ assuming standard Eddington-limited accretion. If accretion\noccurs in the slim disk mode the limits are much weaker, $\\rho_{\\bullet}\n\\lesssim 7.6 \\times 10^{3} \\, \\mathrm{M_{\\odot} \\, Mpc^{-3}}$ in the most\nconstraining case. \n\n"}
{"id": "1506.05552", "contents": "Title: Equation of state for neutron stars with hyperons and quarks in\n  relativistic Hartree-Fock approximation Abstract: We construct the equation of state (EoS) for neutron stars explicitly\nincluding hyperons and quarks. Using the quark-meson coupling model with\nrelativistic Hartree-Fock approximation, the EoS for hadronic matter is derived\nby taking into account the strange ($\\sigma^{\\ast}$ and $\\phi$) mesons as well\nas the light non-strange ($\\sigma$, $\\omega$, $\\vec{\\pi}$ and $\\vec{\\rho}$)\nmesons. Relevant coupling constants are determined to reproduce the\nexperimental data of nuclear matter and hypernuclei in SU(3) flavor symmetry.\nFor quark matter, we employ the MIT bag model with one-gluon-exchange\ninteraction, and Gibbs criteria for chemical equilibrium in the phase\ntransition from hadrons to quarks. We find that the strange vector ($\\phi$)\nmeson and the Fock contribution make the hadronic EoS stiff, and that the\nmaximum mass of a neutron star can be consistent with the observed mass of\nheavy neutron stars even if the coexistence of hadrons and quarks takes place\nin the core. However, in the present calculation the transition to pure quark\nmatter does not occur in stable neutron stars. Furthermore, the lower bound of\nthe critical chemical potential of the quark-hadron transition at zero\ntemperature turns out to be around 1.5 GeV in order to be consistent with the\nrecent observed neutron star data. \n\n"}
{"id": "1506.06564", "contents": "Title: Filling the Complexity Gaps for Colouring Planar and Bounded Degree\n  Graphs Abstract: A colouring of a graph $G=(V,E)$ is a function $c: V\\rightarrow\\{1,2,\\ldots\n\\}$ such that $c(u)\\neq c(v)$ for every $uv\\in E$. A $k$-regular list\nassignment of $G$ is a function $L$ with domain $V$ such that for every $u\\in\nV$, $L(u)$ is a subset of $\\{1, 2, \\dots\\}$ of size $k$. A colouring $c$ of $G$\nrespects a $k$-regular list assignment $L$ of $G$ if $c(u)\\in L(u)$ for every\n$u\\in V$. A graph $G$ is $k$-choosable if for every $k$-regular list assignment\n$L$ of $G$, there exists a colouring of $G$ that respects $L$. We may also ask\nif for a given $k$-regular list assignment $L$ of a given graph $G$, there\nexists a colouring of $G$ that respects $L$. This yields the $k$-Regular List\nColouring problem. For $k\\in \\{3,4\\}$ we determine a family of classes ${\\cal\nG}$ of planar graphs, such that either $k$-Regular List Colouring is\nNP-complete for instances $(G,L)$ with $G\\in {\\cal G}$, or every $G\\in {\\cal\nG}$ is $k$-choosable. By using known examples of non-$3$-choosable and\nnon-$4$-choosable graphs, this enables us to classify the complexity of\n$k$-Regular List Colouring restricted to planar graphs, planar bipartite\ngraphs, planar triangle-free graphs and to planar graphs with no $4$-cycles and\nno $5$-cycles. We also classify the complexity of $k$-Regular List Colouring\nand a number of related colouring problems for graphs with bounded maximum\ndegree. \n\n"}
{"id": "1507.00349", "contents": "Title: Long-term quasi-periodicity of 4U 1636-536 resulting from accretion disc\n  instability Abstract: We present the results of a study of the low-mass X-ray binary 4U 1636-536.\nWe have performed temporal analysis of all available RXTE/ASM, Swift/BAT and\nMAXI data. We have confirmed the previously discovered quasi-periodicity of ~45\nd present during ~2004, however we found it continued to 2006. At other epochs,\nthe quasi-periodicity is only transient, and the quasi-period, if present,\ndrifts. We have then applied a time-dependent accretion disc model to the\ninterval with the significant X-ray quasi-periodicity. For our best model, the\nperiod and the amplitude of the theoretical light curve agree well with that\nobserved. The modelled quasi-periodicity is due to the hydrogen\nthermal-ionization instability occurring in outer regions of the accretion\ndisc. The model parameters are the average mass accretion rate (estimated from\nthe light curves), and the accretion disc viscosity parameters, for the hot and\ncold phases. Our best model gives relatively low values of viscosity parameter\nfor cold phase 0.01 and for hot phase 0.03. \n\n"}
{"id": "1507.01088", "contents": "Title: Generic properties of subgroups of free groups and finite presentations Abstract: Asymptotic properties of finitely generated subgroups of free groups, and of\nfinite group presentations, can be considered in several fashions, depending on\nthe way these objects are represented and on the distribution assumed on these\nrepresentations: here we assume that they are represented by tuples of reduced\nwords (generators of a subgroup) or of cyclically reduced words (relators).\nClassical models consider fixed size tuples of words (e.g. the few-generator\nmodel) or exponential size tuples (e.g. Gromov's density model), and they\nusually consider that equal length words are equally likely. We generalize both\nthe few-generator and the density models with probabilistic schemes that also\nallow variability in the size of tuples and non-uniform distributions on words\nof a given length.Our first results rely on a relatively mild prefix-heaviness\nhypothesis on the distributions, which states essentially that the probability\nof a word decreases exponentially fast as its length grows. Under this\nhypothesis, we generalize several classical results: exponentially generically\na randomly chosen tuple is a basis of the subgroup it generates, this subgroup\nis malnormal and the tuple satisfies a small cancellation property, even for\nexponential size tuples. In the special case of the uniform distribution on\nwords of a given length, we give a phase transition theorem for the central\ntree property, a combinatorial property closely linked to the fact that a tuple\nfreely generates a subgroup. We then further refine our results when the\ndistribution is specified by a Markovian scheme, and in particular we give a\nphase transition theorem which generalizes the classical results on the\ndensities up to which a tuple of cyclically reduced words chosen uniformly at\nrandom exponentially generically satisfies a small cancellation property, and\nbeyond which it presents a trivial group. \n\n"}
{"id": "1507.01917", "contents": "Title: Polynomial-time isomorphism test of groups that are tame extensions Abstract: We give new polynomial-time algorithms for testing isomorphism of a class of\ngroups given by multiplication tables (GpI). Two results (Cannon & Holt, J.\nSymb. Comput. 2003; Babai, Codenotti & Qiao, ICALP 2012) imply that GpI reduces\nto the following: given groups G, H with characteristic subgroups of the same\ntype and isomorphic to $\\mathbb{Z}_p^d$, and given the coset of isomorphisms\n$Iso(G/\\mathbb{Z}_p^d, H/\\mathbb{Z}_p^d)$, compute Iso(G, H) in time poly(|G|).\nBabai & Qiao (STACS 2012) solved this problem when a Sylow p-subgroup of\n$G/\\mathbb{Z}_p^d$ is trivial. In this paper, we solve the preceding problem in\nthe so-called \"tame\" case, i.e., when a Sylow p-subgroup of $G/\\mathbb{Z}_p^d$\nis cyclic, dihedral, semi-dihedral, or generalized quaternion. These cases\ncorrespond exactly to the group algebra\n$\\overline{\\mathbb{F}}_p[G/\\mathbb{Z}_p^d]$ being of tame type, as in the\ncelebrated tame-wild dichotomy in representation theory. We then solve new\ncases of GpI in polynomial time.\n  Our result relies crucially on the divide-and-conquer strategy proposed\nearlier by the authors (CCC 2014), which splits GpI into two problems, one on\ngroup actions (representations), and one on group cohomology. Based on this\nstrategy, we combine permutation group and representation algorithms with new\nmathematical results, including bounds on the number of indecomposable\nrepresentations of groups in the tame case, and on the size of their cohomology\ngroups.\n  Finally, we note that when a group extension is not tame, the preceding\nbounds do not hold. This suggests a precise sense in which the tame-wild\ndichotomy from representation theory may also be a dividing line between the\n(currently) easy and hard instances of GpI. \n\n"}
{"id": "1507.02184", "contents": "Title: The \"art of trellis decoding\" is fixed-parameter tractable Abstract: Given n subspaces of a finite-dimensional vector space over a fixed finite\nfield $\\mathbb F$, we wish to find a linear layout $V_1,V_2,\\ldots,V_n$ of the\nsubspaces such that $\\dim((V_1+V_2+\\cdots+V_i) \\cap (V_{i+1}+\\cdots+V_n))\\le k$\nfor all i, such a linear layout is said to have width at most k. When\nrestricted to 1-dimensional subspaces, this problem is equivalent to computing\nthe trellis-width (or minimum trellis state-complexity) of a linear code in\ncoding theory and computing the path-width of an $\\mathbb F$-represented\nmatroid in matroid theory.\n  We present a fixed-parameter tractable algorithm to construct a linear layout\nof width at most k, if it exists, for input subspaces of a finite-dimensional\nvector space over $\\mathbb F$. As corollaries, we obtain a fixed-parameter\ntractable algorithm to produce a path-decomposition of width at most k for an\ninput $\\mathbb F$-represented matroid of path-width at most k, and a\nfixed-parameter tractable algorithm to find a linear rank-decomposition of\nwidth at most k for an input graph of linear rank-width at most k. In both\ncorollaries, no such algorithms were known previously.\n  It was previously known that a fixed-parameter tractable algorithm exists for\nthe decision version of the problem for matroid path-width, a theorem by\nGeelen, Gerards, and Whittle~(2002) implies that for each fixed finite field\n$\\mathbb F$, there are finitely many forbidden $\\mathbb F$-representable minors\nfor the class of matroids of path-width at most k. An algorithm by\nHlin\\v{e}n\\'y (2006) can detect a minor in an input $\\mathbb F$-represented\nmatroid of bounded branch-width. However, this indirect approach would not\nproduce an actual path-decomposition. Our algorithm is the first one to\nconstruct such a path-decomposition and does not depend on the finiteness of\nforbidden minors. \n\n"}
{"id": "1507.02268", "contents": "Title: Optimal approximate matrix product in terms of stable rank Abstract: We prove, using the subspace embedding guarantee in a black box way, that one\ncan achieve the spectral norm guarantee for approximate matrix multiplication\nwith a dimensionality-reducing map having $m = O(\\tilde{r}/\\varepsilon^2)$\nrows. Here $\\tilde{r}$ is the maximum stable rank, i.e. squared ratio of\nFrobenius and operator norms, of the two matrices being multiplied. This is a\nquantitative improvement over previous work of [MZ11, KVZ14], and is also\noptimal for any oblivious dimensionality-reducing map. Furthermore, due to the\nblack box reliance on the subspace embedding property in our proofs, our\ntheorem can be applied to a much more general class of sketching matrices than\nwhat was known before, in addition to achieving better bounds. For example, one\ncan apply our theorem to efficient subspace embeddings such as the Subsampled\nRandomized Hadamard Transform or sparse subspace embeddings, or even with\nsubspace embedding constructions that may be developed in the future.\n  Our main theorem, via connections with spectral error matrix multiplication\nshown in prior work, implies quantitative improvements for approximate least\nsquares regression and low rank approximation. Our main result has also already\nbeen applied to improve dimensionality reduction guarantees for $k$-means\nclustering [CEMMP14], and implies new results for nonparametric regression\n[YPW15].\n  We also separately point out that the proof of the \"BSS\" deterministic\nrow-sampling result of [BSS12] can be modified to show that for any matrices\n$A, B$ of stable rank at most $\\tilde{r}$, one can achieve the spectral norm\nguarantee for approximate matrix multiplication of $A^T B$ by deterministically\nsampling $O(\\tilde{r}/\\varepsilon^2)$ rows that can be found in polynomial\ntime. The original result of [BSS12] was for rank instead of stable rank. Our\nobservation leads to a stronger version of a main theorem of [KMST10]. \n\n"}
{"id": "1507.03558", "contents": "Title: Testing Shape Restrictions of Discrete Distributions Abstract: We study the question of testing structured properties (classes) of discrete\ndistributions. Specifically, given sample access to an arbitrary distribution\n$D$ over $[n]$ and a property $\\mathcal{P}$, the goal is to distinguish between\n$D\\in\\mathcal{P}$ and $\\ell_1(D,\\mathcal{P})>\\varepsilon$. We develop a general\nalgorithm for this question, which applies to a large range of\n\"shape-constrained\" properties, including monotone, log-concave, $t$-modal,\npiecewise-polynomial, and Poisson Binomial distributions. Moreover, for all\ncases considered, our algorithm has near-optimal sample complexity with regard\nto the domain size and is computationally efficient. For most of these classes,\nwe provide the first non-trivial tester in the literature. In addition, we also\ndescribe a generic method to prove lower bounds for this problem, and use it to\nshow our upper bounds are nearly tight. Finally, we extend some of our\ntechniques to tolerant testing, deriving nearly-tight upper and lower bounds\nfor the corresponding questions. \n\n"}
{"id": "1507.04046", "contents": "Title: Distance labeling schemes for trees Abstract: We consider distance labeling schemes for trees: given a tree with $n$ nodes,\nlabel the nodes with binary strings such that, given the labels of any two\nnodes, one can determine, by looking only at the labels, the distance in the\ntree between the two nodes.\n  A lower bound by Gavoille et. al. (J. Alg. 2004) and an upper bound by Peleg\n(J. Graph Theory 2000) establish that labels must use $\\Theta(\\log^2 n)$\nbits\\footnote{Throughout this paper we use $\\log$ for $\\log_2$.}. Gavoille et.\nal. (ESA 2001) show that for very small approximate stretch, labels use\n$\\Theta(\\log n \\log \\log n)$ bits. Several other papers investigate various\nvariants such as, for example, small distances in trees (Alstrup et. al.,\nSODA'03).\n  We improve the known upper and lower bounds of exact distance labeling by\nshowing that $\\frac{1}{4} \\log^2 n$ bits are needed and that $\\frac{1}{2}\n\\log^2 n$ bits are sufficient. We also give ($1+\\epsilon$)-stretch labeling\nschemes using $\\Theta(\\log n)$ bits for constant $\\epsilon>0$.\n($1+\\epsilon$)-stretch labeling schemes with polylogarithmic label size have\npreviously been established for doubling dimension graphs by Talwar (STOC\n2004).\n  In addition, we present matching upper and lower bounds for distance labeling\nfor caterpillars, showing that labels must have size $2\\log n - \\Theta(\\log\\log\nn)$. For simple paths with $k$ nodes and edge weights in $[1,n]$, we show that\nlabels must have size $\\frac{k-1}{k}\\log n+\\Theta(\\log k)$. \n\n"}
{"id": "1507.04299", "contents": "Title: Tight Lower Bounds for Data-Dependent Locality-Sensitive Hashing Abstract: We prove a tight lower bound for the exponent $\\rho$ for data-dependent\nLocality-Sensitive Hashing schemes, recently used to design efficient solutions\nfor the $c$-approximate nearest neighbor search. In particular, our lower bound\nmatches the bound of $\\rho\\le \\frac{1}{2c-1}+o(1)$ for the $\\ell_1$ space,\nobtained via the recent algorithm from [Andoni-Razenshteyn, STOC'15].\n  In recent years it emerged that data-dependent hashing is strictly superior\nto the classical Locality-Sensitive Hashing, when the hash function is\ndata-independent. In the latter setting, the best exponent has been already\nknown: for the $\\ell_1$ space, the tight bound is $\\rho=1/c$, with the upper\nbound from [Indyk-Motwani, STOC'98] and the matching lower bound from\n[O'Donnell-Wu-Zhou, ITCS'11].\n  We prove that, even if the hashing is data-dependent, it must hold that\n$\\rho\\ge \\frac{1}{2c-1}-o(1)$. To prove the result, we need to formalize the\nexact notion of data-dependent hashing that also captures the complexity of the\nhash functions (in addition to their collision properties). Without restricting\nsuch complexity, we would allow for obviously infeasible solutions such as the\nVoronoi diagram of a dataset. To preclude such solutions, we require our hash\nfunctions to be succinct. This condition is satisfied by all the known\nalgorithmic results. \n\n"}
{"id": "1507.05136", "contents": "Title: Tight Lower Bounds for Planted Clique in the Degree-4 SOS Program Abstract: We give a lower bound of $\\tilde{\\Omega}(\\sqrt{n})$ for the degree-4\nSum-of-Squares SDP relaxation for the planted clique problem. Specifically, we\nshow that on an Erd\\\"os-R\\'enyi graph $G(n,\\tfrac{1}{2})$, with high\nprobability there is a feasible point for the degree-4 SOS relaxation of the\nclique problem with an objective value of $\\tilde{\\Omega}(\\sqrt{n})$, so that\nthe program cannot distinguish between a random graph and a random graph with a\nplanted clique of size $\\tilde{O}(\\sqrt{n})$. This bound is tight.\n  We build on the works of Deshpande and Montanari and Meka et al., who give\nlower bounds of $\\tilde{\\Omega}(n^{1/3})$ and $\\tilde{\\Omega}(n^{1/4})$\nrespectively. We improve on their results by making a perturbation to the SDP\nsolution proposed in their work, then showing that this perturbation remains\nPSD as the objective value approaches $\\tilde{\\Omega}(n^{1/2})$.\n  In an independent work, Hopkins, Kothari and Potechin [HKP15] have obtained a\nsimilar lower bound for the degree-$4$ SOS relaxation. \n\n"}
{"id": "1507.05944", "contents": "Title: Faster Worst Case Deterministic Dynamic Connectivity Abstract: We present a deterministic dynamic connectivity data structure for undirected\ngraphs with worst case update time $O\\left(\\sqrt{\\frac{n(\\log\\log n)^2}{\\log\nn}}\\right)$ and constant query time. This improves on the previous best\ndeterministic worst case algorithm of Frederickson (STOC 1983) and Eppstein\nGalil, Italiano, and Nissenzweig (J. ACM 1997), which had update time\n$O(\\sqrt{n})$. All other algorithms for dynamic connectivity are either\nrandomized (Monte Carlo) or have only amortized performance guarantees. \n\n"}
{"id": "1507.07080", "contents": "Title: Range Predecessor and Lempel-Ziv Parsing Abstract: The Lempel-Ziv parsing of a string (LZ77 for short) is one of the most\nimportant and widely-used algorithmic tools in data compression and string\nprocessing. We show that the Lempel-Ziv parsing of a string of length $n$ on an\nalphabet of size $\\sigma$ can be computed in $O(n\\log\\log\\sigma)$ time ($O(n)$\ntime if we allow randomization) using $O(n\\log\\sigma)$ bits of working space;\nthat is, using space proportional to that of the input string in bits. The\nprevious fastest algorithm using $O(n\\log\\sigma)$ space takes\n$O(n(\\log\\sigma+\\log\\log n))$ time. We also consider the important rightmost\nvariant of the problem, where the goal is to associate with each phrase of the\nparsing its most recent occurrence in the input string. We solve this problem\nin $O(n(1 + (\\log\\sigma/\\sqrt{\\log n}))$ time, using the same working space as\nabove. The previous best solution for rightmost parsing uses\n$O(n(1+\\log\\sigma/\\log\\log n))$ time and $O(n\\log n)$ space. As a bonus, in our\nsolution for rightmost parsing we provide a faster construction method for\nefficient 2D orthogonal range reporting, which is of independent interest. \n\n"}
{"id": "1507.07497", "contents": "Title: An Efficient Parallel Algorithm for Spectral Sparsification of Laplacian\n  and SDDM Matrix Polynomials Abstract: For \"large\" class $\\mathcal{C}$ of continuous probability density functions\n(p.d.f.), we demonstrate that for every $w\\in\\mathcal{C}$ there is mixture of\ndiscrete Binomial distributions (MDBD) with $T\\geq N\\sqrt{\\phi_{w}/\\delta}$\ndistinct Binomial distributions $B(\\cdot,N)$ that $\\delta$-approximates a\ndiscretized p.d.f. $\\widehat{w}(i/N)\\triangleq\nw(i/N)/[\\sum_{\\ell=0}^{N}w(\\ell/N)]$ for all $i\\in[3:N-3]$, where\n$\\phi_{w}\\geq\\max_{x\\in[0,1]}|w(x)|$. Also, we give two efficient parallel\nalgorithms to find such MDBD.\n  Moreover, we propose a sequential algorithm that on input MDBD with $N=2^k$\nfor $k\\in\\mathbb{N}_{+}$ that induces a discretized p.d.f. $\\beta$, $B=D-M$\nthat is either Laplacian or SDDM matrix and parameter $\\epsilon\\in(0,1)$,\noutputs in $\\widehat{O}(\\epsilon^{-2}m + \\epsilon^{-4}nT)$ time a spectral\nsparsifier $D-\\widehat{M}_{N} \\approx_{\\epsilon}\nD-D\\sum_{i=0}^{N}\\beta_{i}(D^{-1} M)^i$ of a matrix-polynomial, where\n$\\widehat{O}(\\cdot)$ notation hides $\\mathrm{poly}(\\log n,\\log N)$ factors.\nThis improves the Cheng et al.'s [CCLPT15] algorithm whose run time is\n$\\widehat{O}(\\epsilon^{-2} m N^2 + NT)$.\n  Furthermore, our algorithm is parallelizable and runs in work\n$\\widehat{O}(\\epsilon^{-2}m + \\epsilon^{-4}nT)$ and depth $O(\\log\nN\\cdot\\mathrm{poly}(\\log n)+\\log T)$. Our main algorithmic contribution is to\npropose the first efficient parallel algorithm that on input continuous p.d.f.\n$w\\in\\mathcal{C}$, matrix $B=D-M$ as above, outputs a spectral sparsifier of\nmatrix-polynomial whose coefficients approximate component-wise the discretized\np.d.f. $\\widehat{w}$.\n  Our results yield the first efficient and parallel algorithm that runs in\nnearly linear work and poly-logarithmic depth and analyzes the long term\nbehaviour of Markov chains in non-trivial settings. In addition, we strengthen\nthe Spielman and Peng's [PS14] parallel SDD solver. \n\n"}
{"id": "1507.07531", "contents": "Title: Good NEWS for GeV Dark Matter Searches Abstract: The proposed NEWS apparatus, a spherical detector with a small central\nelectrode sensor operating as a proportional counter, promises to explore new\nswaths of the direct detection parameter space in the GeV and sub-GeV Dark\nMatter particle mass range by employing very light nuclear targets, such as H\nand He, and by taking advantage of a very low (sub-keV) energy threshold. Here\nwe discuss and study two example classes of Dark Matter models that will be\ntested with NEWS: GeV-scale millicharged Dark Matter, and a GeV-Dirac Fermion\nDark Matter model with a light (MeV-GeV) scalar or vector mediator, and\nindicate the physical regions of parameter space the experiment can probe. \n\n"}
{"id": "1508.00690", "contents": "Title: Non-commutative Edmonds' problem and matrix semi-invariants Abstract: In 1967, Edmonds introduced the problem of computing the rank over the\nrational function field of an $n\\times n$ matrix $T$ with integral homogeneous\nlinear polynomials. In this paper, we consider the non-commutative version of\nEdmonds' problem: compute the rank of $T$ over the free skew field. It is known\nthat this problem relates to the ring of matrix semi-invariants. In particular,\nif the nullcone of matrix semi-invariants is defined by elements of degree\n$\\leq \\sigma$, then there follows a $\\mathrm{poly}(n, \\sigma)$-time randomized\nalgorithm to decide whether the non-commutative rank of $T$ is $<n$. To our\nknowledge, previously the best bound for $\\sigma$ was $O(n^2\\cdot 4^{n^2})$\nover algebraically closed fields of characteristic $0$ (Derksen, 2001).\n  In this article we prove the following results:\n  (1) We observe that by using an algorithm of Gurvits, and assuming the above\nbound $\\sigma$ for $R(n, m)$ over $\\mathbb{Q}$, deciding whether $T$ has\nnon-commutative rank $<n$ over $\\mathbb{Q}$ can be done deterministically in\ntime polynomial in the input size and $\\sigma$.\n  (2) When $\\mathbb{F}$ is large enough, we devise a deterministic algorithm\nfor non-commutative Edmonds' problem in time polynomial in $(n+1)!$, with the\nfollowing consequences.\n  (2.a) If the commutative rank and the non-commutative rank of $T$ differ by a\nconstant, then there exists a randomized efficient algorithm that computes the\nnon-commutative rank of $T$.\n  (2.b) We prove that $\\sigma\\leq (n+1)!$. This not only improves the bound\nobtained from Derksen's work over algebraically closed field of characteristic\n$0$ but, more importantly, also provides for the first time an explicit bound\non $\\sigma$ for matrix semi-invariants over fields of positive characteristics. \n\n"}
{"id": "1508.01753", "contents": "Title: Practical Algorithms for Finding Extremal Sets Abstract: The minimal sets within a collection of sets are defined as the ones which do\nnot have a proper subset within the collection, and the maximal sets are the\nones which do not have a proper superset within the collection. Identifying\nextremal sets is a fundamental problem with a wide-range of applications in SAT\nsolvers, data-mining and social network analysis. In this paper, we present two\nnovel improvements of the high-quality extremal set identification algorithm,\n\\textit{AMS-Lex}, described by Bayardo and Panda. The first technique uses\nmemoization to improve the execution time of the single-threaded variant of the\nAMS-Lex, whilst our second improvement uses parallel programming methods. In a\nsubset of the presented experiments our memoized algorithm executes more than\n$400$ times faster than the highly efficient publicly available implementation\nof AMS-Lex. Moreover, we show that our modified algorithm's speedup is not\nbounded above by a constant and that it increases as the length of the common\nprefixes in successive input \\textit{itemsets} increases. We provide\nexperimental results using both real-world and synthetic data sets, and show\nour multi-threaded variant algorithm out-performing AMS-Lex by $3$ to $6$\ntimes. We find that on synthetic input datasets when executed using $16$ CPU\ncores of a $32$-core machine, our multi-threaded program executes about as fast\nas the state of the art parallel GPU-based program using an NVIDIA GTX 580\ngraphics processing unit. \n\n"}
{"id": "1508.02571", "contents": "Title: Mass loss from advective accretion disc around rotating black holes Abstract: We examine the properties of the outflowing matter from an advective\naccretion disc around a spinning black hole. During accretion, rotating matter\nexperiences centrifugal pressure supported shock transition that effectively\nproduces a virtual barrier around the black hole in the form of post-shock\ncorona (hereafter, PSC). Due to shock compression, PSC becomes hot and dense\nthat eventually deflects a part of the inflowing matter as bipolar outflows\nbecause of the presence of extra thermal gradient force. In our approach, we\nstudy the outflow properties in terms of the inflow parameters, namely specific\nenergy (${\\mathcal E}$) and specific angular momentum ($\\lambda$) considering\nthe realistic outflow geometry around the rotating black holes. We find that\nspin of the black hole ($a_k$) plays an important role in deciding the outflow\nrate $R_{\\dot m}$ (ratio of mass flux of outflow and inflow), in particular,\n$R_{\\dot m}$ is directly correlated with $a_k$ for the same set of inflow\nparameters. It is found that a large range of the inflow parameters allows\nglobal accretion-ejection solutions and the effective area of the parameter\nspace (${\\mathcal E}$, $\\lambda$) with and without outflow decreases with black\nhole spin ($a_k$). We compute the maximum outflow rate ($R^{max}_{\\dot m}$) as\nfunction of black hole spin ($a_k$) and observe that $R^{max}_{\\dot m}$ weakly\ndepends on $a_k$ that lies in the range $\\sim 10\\%-18\\%$ of the inflow rate for\nthe adiabatic index $(\\gamma)$ with $1.5 \\ge \\gamma \\ge 4/3$. We present the\nobservational implication of our approach while studying the steady/persistent\nJet activities based on the accretion states of black holes. We discuss that\nour formalism seems to have the potential to explain the observed Jet kinetic\npower for several Galactic Black Hole sources (GBHs) and Active Galactic Nuclei\n(AGNs). \n\n"}
{"id": "1508.02773", "contents": "Title: Editing to a Planar Graph of Given Degrees Abstract: We consider the following graph modification problem. Let the input consist\nof a graph $G=(V,E)$, a weight function $w\\colon V\\cup E\\rightarrow\n\\mathbb{N}$, a cost function $c\\colon V\\cup E\\rightarrow \\mathbb{N}$ and a\ndegree function $\\delta\\colon V\\rightarrow \\mathbb{N}_0$, together with three\nintegers $k_v, k_e$ and $C$. The question is whether we can delete a set of\nvertices of total weight at most $k_v$ and a set of edges of total weight at\nmost $k_e$ so that the total cost of the deleted elements is at most $C$ and\nevery non-deleted vertex $v$ has degree $\\delta(v)$ in the resulting graph\n$G'$. We also consider the variant in which $G'$ must be connected. Both\nproblems are known to be NP-complete and W[1]-hard when parameterized by\n$k_v+k_e$. We prove that, when restricted to planar graphs, they stay\nNP-complete but have polynomial kernels when parameterized by $k_v+k_e$. \n\n"}
{"id": "1508.03608", "contents": "Title: Galaxy Strategy for LIGO-Virgo Gravitational Wave Counterpart Searches Abstract: In this work we continue a line of inquiry begun in Kanner et al. which\ndetailed a strategy for utilizing telescopes with narrow fields of view, such\nas the Swift X-ray Telescope (XRT), to localize gravity wave (GW) triggers from\nLIGO/Virgo. If one considers the brightest galaxies that produce ~50% of the\nlight, then the number of galaxies inside typical GW error boxes will be\nseveral tens. We have found that this result applies both in the early years of\nAdvanced LIGO when the range is small and the error boxes large, and in the\nlater years when the error boxes will be small and the range large. This\nstrategy has the beneficial property of reducing the number of telescope\npointings by a factor 10 to 100 compared with tiling the entire error box.\nAdditional galaxy count reduction will come from a GW rapid distance estimate\nwhich will restrict the radial slice in search volume. Combining the bright\ngalaxy strategy with a convolution based on anticipated GW localizations, we\nfind that the searches can be restricted to about 18+/-5 galaxies for 2015,\nabout 23+/-4 for 2017, and about 11+/-2 for 2020. This assumes a distance\nlocalization at or near the putative NS-NS merger range for each target year,\nand these totals are integrated out to the range. Integrating out to the\nhorizon would roughly double the totals. For nearer localizations the totals\nwould decrease. The galaxy strategy we present in this work will enable\nnumerous sensitive optical and X-ray telescopes with small fields of view to\nparticipate meaningfully in searches wherein the prospects for rapidly fading\nafterglow place a premium on a fast response time. \n\n"}
{"id": "1508.04775", "contents": "Title: Search for precursor eruptions among Type IIb supernovae Abstract: The progenitor stars of several Type IIb supernovae (SNe) show indications\nfor extended hydrogen envelopes. These envelopes might be the outcome of\nluminous energetic pre-explosion events, so-called precursor eruptions. We use\nthe Palomar Transient Factory (PTF) pre-explosion observations of a sample of\n27 nearby Type IIb SNe to look for such precursors during the final years prior\nto the SN explosion. No precursors are found when combining the observations in\n15-day bins, and we calculate the absolute-magnitude-dependent upper limit on\nthe precursor rate. At the 90% confidence level, Type IIb SNe have on average\n$<0.86$ precursors as bright as absolute $R$-band magnitude $-14$ in the final\n3.5 years before the explosion and $<0.56$ events over the final year. In\ncontrast, precursors among SNe IIn have a $\\gtrsim 5$ times higher rate. The\nkinetic energy required to unbind a low-mass stellar envelope is comparable to\nthe radiated energy of a few-weeks-long precursor which would be detectable for\nthe closest SNe in our sample. Therefore, mass ejections, if they are common in\nsuch SNe, are radiatively inefficient or have durations longer than months.\nIndeed, when using 60-day bins a faint precursor candidate is detected prior to\nSN 2012cs ($\\sim2$% false-alarm probability). We also report the detection of\nthe progenitor of SN 2011dh which does not show detectable variability over the\nfinal two years before the explosion. The suggested progenitor of SN 2012P is\nstill present, and hence is likely a compact star cluster, or an unrelated\nobject. \n\n"}
{"id": "1508.06019", "contents": "Title: Dense Subset Sum may be the hardest Abstract: The Subset Sum problem asks whether a given set of $n$ positive integers\ncontains a subset of elements that sum up to a given target $t$. It is an\noutstanding open question whether the $O^*(2^{n/2})$-time algorithm for Subset\nSum by Horowitz and Sahni [J. ACM 1974] can be beaten in the worst-case setting\nby a \"truly faster\", $O^*(2^{(0.5-\\delta)n})$-time algorithm, with some\nconstant $\\delta > 0$. Continuing an earlier work [STACS 2015], we study Subset\nSum parameterized by the maximum bin size $\\beta$, defined as the largest\nnumber of subsets of the $n$ input integers that yield the same sum. For every\n$\\epsilon > 0$ we give a truly faster algorithm for instances with $\\beta \\leq\n2^{(0.5-\\epsilon)n}$, as well as instances with $\\beta \\geq 2^{0.661n}$.\nConsequently, we also obtain a characterization in terms of the popular density\nparameter $n/\\log_2 t$: if all instances of density at least $1.003$ admit a\ntruly faster algorithm, then so does every instance. This goes against the\ncurrent intuition that instances of density 1 are the hardest, and therefore is\na step toward answering the open question in the affirmative. Our results stem\nfrom novel combinations of earlier algorithms for Subset Sum and a study of an\nextremal question in additive combinatorics connected to the problem of\nUniquely Decodable Code Pairs in information theory. \n\n"}
{"id": "1509.00840", "contents": "Title: An Ultrasoft X-ray Flare from 3XMM J152130.7+074916: a Tidal Disruption\n  Event Candidate Abstract: We report on the discovery of an ultrasoft X-ray transient source, 3XMM\nJ152130.7+074916. It was serendipitously detected in an XMM-Newton observation\non 2000 August 23, and its location is consistent with the center of the galaxy\nSDSS J152130.72+074916.5 (z=0.17901 and d_L=866 Mpc). The high-quality X-ray\nspectrum can be fitted with a thermal disk with an apparent inner disk\ntemperature of 0.17 keV and a rest-frame 0.24-11.8 keV unabsorbed luminosity of\n~5e43 erg/s, subject to a fast-moving warm absorber. Short-term variability was\nalso clearly observed, with the spectrum being softer at lower flux. The source\nwas covered but not detected in a Chandra observation on 2000 April 3, a Swift\nobservation on 2005 September 10, and a second XMM-Newton observation on 2014\nJanuary 19, implying a large variability (>260) of the X-ray flux. The optical\nspectrum of the candidate host galaxy, taken ~11 yrs after the XMM-Newton\ndetection, shows no sign of nuclear activity. This, combined with its transient\nand ultrasoft properties, leads us to explain the source as tidal disruption of\na star by the supermassive black hole in the galactic center. We attribute the\nfast-moving warm absorber detected in the first XMM-Newton observation to the\nsuper-Eddington outflow associated with the event and the short-term\nvariability to a disk instability that caused fast change of the inner disk\nradius at a constant mass accretion rate. \n\n"}
{"id": "1509.03976", "contents": "Title: Approximability of TSP on Power Law Graphs Abstract: In this paper we study the special case of Graphic TSP where the underlying\ngraph is a power law graph (PLG). We give a refined analysis of some of the\ncurrent best approximation algorithms and show that an improved approximation\nratio can be achieved for certain ranges of the power law exponent $\\beta$. For\nthe value of power law exponent $\\beta=1.5$ we obtain an approximation ratio of\n$1.34$ for Graphic TSP. Moreover we study the $(1,2)$-TSP with the underlying\ngraph of $1$-edges being a PLG. We show improved approximation ratios in the\ncase of underlying deterministic PLGs for $\\beta$ greater than $1.666$. For\nunderlying random PLGs we further improve the analysis and show even better\nexpected approximation ratio for the range of $\\beta$ between $1$ and $3.5$. On\nthe other hand we prove the first explicit inapproximability bounds for\n$(1,2)$-TSP for an underlying power law graph. \n\n"}
{"id": "1509.05572", "contents": "Title: Randomised enumeration of small witnesses using a decision oracle Abstract: Many combinatorial problems involve determining whether a universe of $n$\nelements contains a witness consisting of $k$ elements which have some\nspecified property. In this paper we investigate the relationship between the\ndecision and enumeration versions of such problems: efficient methods are known\nfor transforming a decision algorithm into a search procedure that finds a\nsingle witness, but even finding a second witness is not so straightforward in\ngeneral. We show that, if the decision version of the problem can be solved in\ntime $f(k) \\cdot poly(n)$, there is a randomised algorithm which enumerates all\nwitnesses in time $e^{k + o(k)} \\cdot f(k) \\cdot poly(n) \\cdot N$, where $N$ is\nthe total number of witnesses. If the decision version of the problem is solved\nby a randomised algorithm which may return false negatives, then the same\nmethod allows us to output a list of witnesses in which any given witness will\nbe included with high probability. The enumeration algorithm also gives rise to\nan efficient algorithm to count the total number of witnesses when this number\nis small. \n\n"}
{"id": "1509.06305", "contents": "Title: On a linearization technique for solving quadratic set covering problem\n  and variations Abstract: In this paper we identify some inaccuracies in the paper by R.R. Saxena and\nS.R. Arora, A Linearization technique for solving the Quadratic Set Covering\nProblem, Optimization, 39 (1997) 33-42. In particular, we observe that their\nalgorithm need not guarantee optimality, contrary to what is claimed.\nExperimental analysis with the algorithm has been carried out to evaluate its\nmerit as a heuristic and compared with CPLEX. The results disclose that for\nsome class of problems the algorithm is reasonably effective while for some\nother class, it's performance is very poor. we also discussion similar\ninaccuracies in another related paper. \n\n"}
{"id": "1509.06357", "contents": "Title: Using Contracted Solution Graphs for Solving Reconfiguration Problems Abstract: We introduce in a general setting a dynamic programming method for solving\nreconfiguration problems. Our method is based on contracted solution graphs,\nwhich are obtained from solution graphs by performing an appropriate series of\nedge contractions that decrease the graph size without losing any critical\ninformation needed to solve the reconfiguration problem under consideration.\nOur general framework captures the approach behind known reconfiguration\nresults of Bonsma (2012) and Hatanaka, Ito and Zhou (2014). As a third example,\nwe apply the method to the following problem: given two $k$-colorings $\\alpha$\nand $\\beta$ of a graph $G$, can $\\alpha$ be modified into $\\beta$ by recoloring\none vertex of $G$ at a time, while maintaining a $k$-coloring throughout? This\nproblem is known to be PSPACE-hard even for bipartite planar graphs and $k=4$.\nBy applying our method in combination with a thorough exploitation of the graph\nstructure we obtain a polynomial time algorithm for $(k-2)$-connected chordal\ngraphs. \n\n"}
{"id": "1509.06956", "contents": "Title: The $\\gamma$-ray pulsar J0633+0632 in X-rays Abstract: We analysed Chandra observations of the bright Fermi pulsar J0633+0632 and\nfound evidence of an absorption feature in its spectrum at $804^{+42}_{-26}$ eV\n(the errors here and below are at 90% confidence) with equivalent width of\n$63^{+47}_{-36}$ eV. In addition, we analysed in detail the X-ray spectral\ncontinuum taking into account correlations between the interstellar absorption\nand the distance to the source. We confirm early findings by Ray et al. (2011)\nthat the spectrum contains non-thermal and thermal components. The latter is\nequally well described by the blackbody and magnetised atmosphere models and\ncan be attributed to the emission from the bulk of the stellar surface in both\ncases. The distance to the pulsar is constrained in a range of 1--4 kpc from\nthe spectral fits. We infer the blackbody surface temperature of\n$108^{+22}_{-14}$ eV, while for the atmosphere model, the temperature, as seen\nby a distant observer, is $53^{+12}_{-7}$ eV. In the latter case J0633+0632 is\none of the coldest middle-aged isolated neutron stars with measured\ntemperatures. Finally, it powers an extended pulsar wind nebula whose shape\nsuggests a high pulsar proper motion. Looking backwards the direction of the\npresumed proper motion we found a likely birthplace of the pulsar -- the\nRosette nebula, a 50-Myr-old active star-forming region located at about\n1$.\\!\\!^\\circ$5 from the pulsar. If true, this constrains the distance to the\npulsar in the range of 1.2--1.8 kpc. \n\n"}
{"id": "1509.08251", "contents": "Title: Tight Lower and Upper Bounds for the Complexity of Canonical Colour\n  Refinement Abstract: An assignment of colours to the vertices of a graph is stable if any two\nvertices of the same colour have identically coloured neighbourhoods. The goal\nof colour refinement is to find a stable colouring that uses a minimum number\nof colours. This is a widely used subroutine for graph isomorphism testing\nalgorithms, since any automorphism needs to be colour preserving. We give an\n$O((m+n)\\log n)$ algorithm for finding a canonical version of such a stable\ncolouring, on graphs with $n$ vertices and $m$ edges. We show that no faster\nalgorithm is possible, under some modest assumptions about the type of\nalgorithm, which captures all known colour refinement algorithms. \n\n"}
{"id": "1510.00673", "contents": "Title: A gravitational wave afterglow in binary neutron star mergers Abstract: We study in detail the f-mode secular instability for rapidly rotating\nneutron stars, putting emphasis on supermassive models which do not have a\nstable nonrotating counterpart. Such neutron stars are thought to be the\ngeneric outcome of the merger of two standard mass neutron stars. In addition\nwe take into account the effects of strong magnetic field and r-mode\ninstability, that can drain a substantial amount of angular momentum. We find\nthat the gravitational wave signal emitted by supramassive neutron stars can\nreach above the Advance LIGO sensitivity at distance of about 20Mpc and the\ndetectability is substantially enhanced for the Einstein Telescope. The event\nrate will be of the same order as the merging rates, while the analysis of the\nsignal will carry information for the equation of state of the post-merging\nneutron stars and the strength of the magnetic fields. \n\n"}
{"id": "1510.01753", "contents": "Title: Doubled patterns are $3$-avoidable Abstract: In combinatorics on words, a word $w$ over an alphabet $\\Sigma$ is said to\navoid a pattern $p$ over an alphabet $\\Delta$ if there is no factor $f$ of $w$\nsuch that $f=h(p)$ where $h:\\Delta^*\\to\\Sigma^*$ is a non-erasing morphism. A\npattern $p$ is said to be $k$-avoidable if there exists an infinite word over a\n$k$-letter alphabet that avoids $p$. A pattern is said to be doubled if no\nvariable occurs only once. Doubled patterns with at most 3 variables and\npatterns with at least 6 variables are $3$-avoidable. We show that doubled\npatterns with 4 and 5 variables are also $3$-avoidable. \n\n"}
{"id": "1510.02882", "contents": "Title: Lempel-Ziv Computation In Compressed Space (LZ-CICS) Abstract: We show that both the Lempel Ziv 77- and the 78-factorization of a text of\nlength $n$ on an integer alphabet of size $\\sigma$ can be computed in $O(n \\lg\n\\lg \\sigma)$ time (linear time if we allow randomization) using $O(n \\lg\n\\sigma)$ bits of working space. Given that a compressed representation of the\nsuffix tree is loaded into RAM, we can compute both factorizations in $O(n)$\ntime using $z \\lg n + O(n)$ bits of space, where $z$ is the number of factors. \n\n"}
{"id": "1510.05008", "contents": "Title: Dark Matter Decay to a Photon and a Neutrino: the Double Monochromatic\n  Smoking Gun Scenario Abstract: In the energy range from few TeV to 25 TeV, upper bounds on the dark matter\ndecay rate into high energy monochromatic neutrinos have recently become\ncomparable to those on monochromatic gamma-ray lines. This implies clear\npossibilities of a future double \"smoking-gun\" evidence for the dark matter\nparticle, from the observation of both a gamma and a neutrino line at the same\nenergy. In particular, we show that a scenario where both lines are induced\nfrom the same dark matter particle decay leads to correlations that can already\nbe tested. We study this \"double monochromatic\" scenario by considering the\ncomplete list of lowest dimensional effective operators that could induce such\na decay. Furthermore, we argue that, on top of lines from decays into two-body\nfinal states, three-body final states can also be highly relevant. In addition\nto producing a distinct hard photon spectrum, three-body final states also\nproduce a line-like feature in the neutrino spectrum that can be searched for\nby neutrino telescopes. \n\n"}
{"id": "1510.05886", "contents": "Title: Approximation Algorithm for Minimum Weight Connected $m$-Fold Dominating\n  Set Abstract: Using connected dominating set (CDS) to serve as a virtual backbone in a\nwireless networks can save energy and reduce interference. Since nodes may fail\ndue to accidental damage or energy depletion, it is desirable that the virtual\nbackbone has some fault-tolerance. A $k$-connected $m$-fold dominating set\n($(k,m)$-CDS) of a graph $G$ is a node set $D$ such that every node in\n$V\\setminus D$ has at least $m$ neighbors in $D$ and the subgraph of $G$\ninduced by $D$ is $k$-connected. Using $(k,m)$-CDS can tolerate the failure of\n$\\min\\{k-1,m-1\\}$ nodes. In this paper, we study Minimum Weight $(1,m)$-CDS\nproblem ($(1,m)$-MWCDS), and present an\n$(H(\\delta+m)+2H(\\delta-1))$-approximation algorithm, where $\\delta$ is the\nmaximum degree of the graph and $H(\\cdot)$ is the Harmonic number. Notice that\nthere is a $1.35\\ln n$-approximation algorithm for the $(1,1)$-MWCDS problem,\nwhere $n$ is the number of nodes in the graph. Though our constant in $O(\\ln\n\\cdot)$ is larger than 1.35, $n$ is replaced by $\\delta$. Such a replacement\nenables us to obtain a $(6.67+\\varepsilon)$-approximation for the $(1,m)$-MWCDS\nproblem on unit disk graphs. \n\n"}
{"id": "1510.08865", "contents": "Title: Mixed Robust/Average Submodular Partitioning: Fast Algorithms,\n  Guarantees, and Applications to Parallel Machine Learning and Multi-Label\n  Image Segmentation Abstract: We study two mixed robust/average-case submodular partitioning problems that\nwe collectively call Submodular Partitioning. These problems generalize both\npurely robust instances of the problem (namely max-min submodular fair\nallocation (SFA) and min-max submodular load balancing (SLB) and also\ngeneralize average-case instances (that is the submodular welfare problem (SWP)\nand submodular multiway partition (SMP). While the robust versions have been\nstudied in the theory community, existing work has focused on tight\napproximation guarantees, and the resultant algorithms are not, in general,\nscalable to very large real-world applications. This is in contrast to the\naverage case, where most of the algorithms are scalable. In the present paper,\nwe bridge this gap, by proposing several new algorithms (including those based\non greedy, majorization-minimization, minorization-maximization, and relaxation\nalgorithms) that not only scale to large sizes but that also achieve\ntheoretical approximation guarantees close to the state-of-the-art, and in some\ncases achieve new tight bounds. We also provide new scalable algorithms that\napply to additive combinations of the robust and average-case extreme\nobjectives. We show that these problems have many applications in machine\nlearning (ML). This includes: 1) data partitioning and load balancing for\ndistributed machine algorithms on parallel machines; 2) data clustering; and 3)\nmulti-label image segmentation with (only) Boolean submodular functions via\npixel partitioning. We empirically demonstrate the efficacy of our algorithms\non real-world problems involving data partitioning for distributed optimization\nof standard machine learning objectives (including both convex and deep neural\nnetwork objectives), and also on purely unsupervised (i.e., no supervised or\nsemi-supervised learning, and no interactive segmentation) image segmentation. \n\n"}
{"id": "1510.09192", "contents": "Title: A note on coloring (even-hole,cap)-free graphs Abstract: A {\\em hole} is a chordless cycle of length at least four. A hole is {\\em\neven} (resp. {\\em odd}) if it contains an even (resp. odd) number of vertices.\nA \\emph{cap} is a graph induced by a hole with an additional vertex that is\nadjacent to exactly two adjacent vertices on the hole. In this note, we use a\ndecomposition theorem by Conforti et al. (1999) to show that if a graph $G$\ndoes not contain any even hole or cap as an induced subgraph, then $\\chi(G)\\le\n\\lfloor\\frac{3}{2}\\omega(G)\\rfloor$, where $\\chi(G)$ and $\\omega(G)$ are the\nchromatic number and the clique number of $G$, respectively. This bound is\nattained by odd holes and the Hajos graph. The proof leads to a polynomial-time\n$3/2$-approximation algorithm for coloring (even-hole,cap)-free graphs. \n\n"}
{"id": "1511.00434", "contents": "Title: Estimating the distribution of rest-frame timescales for blazar jets: a\n  statistical approach Abstract: In any flux-density limited sample of blazars, the distribution of the\ntimescale modulation factor $\\Delta t'/\\Delta t$, which quantifies the change\nin observed timescales compared to the rest-frame ones due to redshift and\nrelativistic compression follows an exponential distribution with a mean\ndepending on the flux limit of the sample. In this work we produce the\nmathematical formalism that allows us to use this information in order to\nuncover the underlining rest-frame probability density function of measurable\ntimescales of blazar jets. We extensively test our proposed methodology using a\nsimulated FSRQ population with a 1.5 Jy flux-density limit in the simple case\n(where all blazars share the same intrinsic timescale), in order to identify\nlimits of applicability and potential biases due to observational systematics\nand sample selection. We find that for monitoring with time intervals between\nobservations longer than $\\sim$30\\% of the intrinsic timescale under\ninvestigation the method loses its ability to produce robust results. For time\nintervals of $\\sim$3\\% of the intrinsic timescale the error of the method is as\nlow as 1\\% in recovering the intrinsic rest-frame timescale. We applied our\nmethod to rotations of the optical polarization angle of blazars observed by\nRoboPol. We found that the intrinsic timescales of the longest-duration\nrotation event in each blazar follows a narrow distribution, well-described by\na normal distribution with mean 87 days and standard deviation 5 days. We\ndiscuss possible interpretations of this result. \n\n"}
{"id": "1511.01379", "contents": "Title: Fully polynomial-time parameterized computations for graphs and matrices\n  of low treewidth Abstract: We investigate the complexity of several fundamental polynomial-time solvable\nproblems on graphs and on matrices, when the given instance has low treewidth;\nin the case of matrices, we consider the treewidth of the graph formed by\nnon-zero entries. In each of the considered cases, the best known algorithms\nworking on general graphs run in polynomial time, however the exponent of the\npolynomial is large. Therefore, our main goal is to construct algorithms with\nrunning time of the form $\\textrm{poly}(k)\\cdot n$ or $\\textrm{poly}(k)\\cdot\nn\\log n$, where $k$ is the width of the tree decomposition given on the input.\nSuch procedures would outperform the best known algorithms for the considered\nproblems already for moderate values of the treewidth, like $O(n^{1/c})$ for\nsome small constant $c$.\n  Our results include:\n  -- an algorithm for computing the determinant and the rank of an $n\\times n$\nmatrix using $O(k^3\\cdot n)$ time and arithmetic operations;\n  -- an algorithm for solving a system of linear equations using $O(k^3\\cdot\nn)$ time and arithmetic operations;\n  -- an $O(k^3\\cdot n\\log n)$-time randomized algorithm for finding the\ncardinality of a maximum matching in a graph;\n  -- an $O(k^4\\cdot n\\log^2 n)$-time randomized algorithm for constructing a\nmaximum matching in a graph;\n  -- an $O(k^2\\cdot n\\log n)$-time algorithm for finding a maximum vertex flow\nin a directed graph.\n  Moreover, we give an approximation algorithm for treewidth with time\ncomplexity suited to the running times as above. Namely, the algorithm, when\ngiven a graph $G$ and integer $k$, runs in time $O(k^7\\cdot n\\log n)$ and\neither correctly reports that the treewidth of $G$ is larger than $k$, or\nconstructs a tree decomposition of $G$ of width $O(k^2)$. \n\n"}
{"id": "1511.04139", "contents": "Title: Correcting for Interstellar Scattering Delay in High-precision Pulsar\n  Timing: Simulation Results Abstract: Light travel time changes due to gravitational waves may be detected within\nthe next decade through precision timing of millisecond pulsars. Removal of\nfrequency-dependent interstellar medium (ISM) delays due to dispersion and\nscattering is a key issue in the detection process. Current timing algorithms\nroutinely correct pulse times of arrival (TOAs) for time-variable delays due to\ncold plasma dispersion. However, none of the major pulsar timing groups correct\nfor delays due to scattering from multi-path propagation in the ISM. Scattering\nintroduces a frequency-dependent phase change in the signal that results in\npulse broadening and arrival time delays. Any method to correct the TOA for\ninterstellar propagation effects must be based on multi-frequency measurements\nthat can effectively separate dispersion and scattering delay terms from\nfrequency-independent perturbations such as those due to a gravitational wave.\nCyclic spectroscopy, first described in an astronomical context by Demorest\n(2011), is a potentially powerful tool to assist in this multi-frequency\ndecomposition. As a step toward a more comprehensive ISM propagation delay\ncorrection, we demonstrate through a simulation that we can accurately recover\nimpulse response functions (IRFs), such as those that would be introduced by\nmulti-path scattering, with a realistic signal-to-noise ratio. We demonstrate\nthat timing precision is improved when scatter-corrected TOAs are used, under\nthe assumptions of a high signal-to-noise and highly scattered signal. We also\nshow that the effect of pulse-to-pulse \"jitter\" is not a serious problem for\nIRF reconstruction, at least for jitter levels comparable to those observed in\nseveral bright pulsars. \n\n"}
{"id": "1511.04387", "contents": "Title: Combining Monte-Carlo and Hyper-heuristic methods for the Multi-mode\n  Resource-constrained Multi-project Scheduling Problem Abstract: Multi-mode resource and precedence-constrained project scheduling is a\nwell-known challenging real-world optimisation problem. An important variant of\nthe problem requires scheduling of activities for multiple projects considering\navailability of local and global resources while respecting a range of\nconstraints. A critical aspect of the benchmarks addressed in this paper is\nthat the primary objective is to minimise the sum of the project completion\ntimes, with the usual makespan minimisation as a secondary objective. We\nobserve that this leads to an expected different overall structure of good\nsolutions and discuss the effects this has on the algorithm design. This paper\npresents a carefully designed hybrid of Monte-Carlo tree search, novel\nneighbourhood moves, memetic algorithms, and hyper-heuristic methods. The\nimplementation is also engineered to increase the speed with which iterations\nare performed, and to exploit the computing power of multicore machines.\nEmpirical evaluation shows that the resulting information-sharing\nmulti-component algorithm significantly outperforms other solvers on a set of\n\"hidden\" instances, i.e. instances not available at the algorithm design phase. \n\n"}
{"id": "1511.05886", "contents": "Title: Randomization can be as helpful as a glimpse of the future in online\n  computation Abstract: We provide simple but surprisingly useful direct product theorems for proving\nlower bounds on online algorithms with a limited amount of advice about the\nfuture. As a consequence, we are able to translate decades of research on\nrandomized online algorithms to the advice complexity model. Doing so improves\nsignificantly on the previous best advice complexity lower bounds for many\nonline problems, or provides the first known lower bounds. For example, if $n$\nis the number of requests, we show that:\n  (1) A paging algorithm needs $\\Omega(n)$ bits of advice to achieve a\ncompetitive ratio better than $H_k=\\Omega(\\log k)$, where $k$ is the cache\nsize. Previously, it was only known that $\\Omega(n)$ bits of advice were\nnecessary to achieve a constant competitive ratio smaller than $5/4$.\n  (2) Every $O(n^{1-\\varepsilon})$-competitive vertex coloring algorithm must\nuse $\\Omega(n\\log n)$ bits of advice. Previously, it was only known that\n$\\Omega(n\\log n)$ bits of advice were necessary to be optimal.\n  For certain online problems, including the MTS, $k$-server, paging, list\nupdate, and dynamic binary search tree problem, our results imply that\nrandomization and sublinear advice are equally powerful (if the underlying\nmetric space or node set is finite). This means that several long-standing open\nquestions regarding randomized online algorithms can be equivalently stated as\nquestions regarding online algorithms with sublinear advice. For example, we\nshow that there exists a deterministic $O(\\log k)$-competitive $k$-server\nalgorithm with advice complexity $o(n)$ if and only if there exists a\nrandomized $O(\\log k)$-competitive $k$-server algorithm without advice.\n  Technically, our main direct product theorem is obtained by extending an\ninformation theoretical lower bound technique due to Emek, Fraigniaud, Korman,\nand Ros\\'en [ICALP'09]. \n\n"}
{"id": "1511.06099", "contents": "Title: On Sketching Quadratic Forms Abstract: We undertake a systematic study of sketching a quadratic form: given an $n\n\\times n$ matrix $A$, create a succinct sketch $\\textbf{sk}(A)$ which can\nproduce (without further access to $A$) a multiplicative\n$(1+\\epsilon)$-approximation to $x^T A x$ for any desired query $x \\in\n\\mathbb{R}^n$. While a general matrix does not admit non-trivial sketches,\npositive semi-definite (PSD) matrices admit sketches of size\n$\\Theta(\\epsilon^{-2} n)$, via the Johnson-Lindenstrauss lemma, achieving the\n\"for each\" guarantee, namely, for each query $x$, with a constant probability\nthe sketch succeeds. (For the stronger \"for all\" guarantee, where the sketch\nsucceeds for all $x$'s simultaneously, again there are no non-trivial\nsketches.)\n  We design significantly better sketches for the important subclass of graph\nLaplacian matrices, which we also extend to symmetric diagonally dominant\nmatrices. A sequence of work culminating in that of Batson, Spielman, and\nSrivastava (SIAM Review, 2014), shows that by choosing and reweighting\n$O(\\epsilon^{-2} n)$ edges in a graph, one achieves the \"for all\" guarantee.\nOur main results advance this front.\n  $\\bullet$ For the \"for all\" guarantee, we prove that Batson et al.'s bound is\noptimal even when we restrict to \"cut queries\" $x\\in \\{0,1\\}^n$.\n  In contrast, previous lower bounds showed the bound only for {\\em\nspectral-sparsifiers}.\n  $\\bullet$ For the \"for each\" guarantee, we design a sketch of size $\\tilde\nO(\\epsilon^{-1} n)$ bits for \"cut queries\" $x\\in \\{0,1\\}^n$. We prove a\nnearly-matching lower bound of $\\Omega(\\epsilon^{-1} n)$ bits. For general\nqueries $x \\in \\mathbb{R}^n$, we construct sketches of size\n$\\tilde{O}(\\epsilon^{-1.6} n)$ bits. \n\n"}
{"id": "1511.08205", "contents": "Title: Breaking Symmetries in Graph Search with Canonizing Sets Abstract: There are many complex combinatorial problems which involve searching for an\nundirected graph satisfying given constraints. Such problems are often highly\nchallenging because of the large number of isomorphic representations of their\nsolutions. This paper introduces effective and compact, complete symmetry\nbreaking constraints for small graph search. Enumerating with these symmetry\nbreaks generates all and only non-isomorphic solutions. For small search\nproblems, with up to $10$ vertices, we compute instance independent symmetry\nbreaking constraints. For small search problems with a larger number of\nvertices we demonstrate the computation of instance dependent constraints which\nare complete. We illustrate the application of complete symmetry breaking\nconstraints to extend two known sequences from the OEIS related to graph\nenumeration. We also demonstrate the application of a generalization of our\napproach to fully-interchangeable matrix search problems. \n\n"}
{"id": "1511.09229", "contents": "Title: Efficient Deterministic Single Round Document Exchange for Edit Distance Abstract: Suppose that we have two parties that possess each a binary string. Suppose\nthat the length of the first string (document) is $n$ and that the two strings\n(documents) have edit distance (minimal number of deletes, inserts and\nsubstitutions needed to transform one string into the other) at most $k$. The\nproblem we want to solve is to devise an efficient protocol in which the first\nparty sends a single message that allows the second party to guess the first\nparty's string. In this paper we show an efficient deterministic protocol for\nthis problem. The protocol runs in time $O(n\\cdot \\mathtt{polylog}(n))$ and has\nmessage size $O(k^2+k\\log^2n)$ bits. To the best of our knowledge, ours is the\nfirst efficient deterministic protocol for this problem, if efficiency is\nmeasured in both the message size and the running time. As an immediate\napplication of our new protocol, we show a new error correcting code that is\nefficient even for large numbers of (adversarial) edit errors. \n\n"}
{"id": "1511.09360", "contents": "Title: On the Complexity of Multi-Parameterized Cluster Editing Abstract: The Cluster Editing problem seeks a transformation of a given undirected\ngraph into a disjoint union of cliques via a minimum number of edge additions\nor deletions. A multi-parameterized version of the problem is studied,\nfeaturing a number of input parameters that bound the amount of both\nedge-additions and deletions per single vertex, as well as the size of a\nclique-cluster. We show that the problem remains NP-hard even when only one\nedge can be deleted and at most two edges can be added per vertex. However, the\nnew formulation allows us to solve Cluster Editing (exactly) in polynomial time\nwhen the number of edge-edit operations per vertex is smaller than half the\nminimum cluster size. In other words, Correlation Clustering can be solved\nefficiently when the number of false positives/negatives per single data\nelement is expected to be small compared to the minimum cluster size. As a\nbyproduct, we obtain a kernelization algorithm that delivers linear-size\nkernels when the two edge-edit bounds are small constants. \n\n"}
{"id": "1511.09389", "contents": "Title: The role of twins in computing planar supports of hypergraphs Abstract: A support or realization of a hypergraph $H$ is a graph $G$ on the same\nvertex as $H$ such that for each hyperedge of $H$ it holds that its vertices\ninduce a connected subgraph of $G$. The NP-hard problem of finding a planar\nsupport has applications in hypergraph drawing and network design. Previous\nalgorithms for the problem assume that twins -- pairs of vertices that are in\nprecisely the same hyperedges -- can safely be removed from the input\nhypergraph. We prove that this assumption is generally wrong, yet that the\nnumber of twins necessary for a hypergraph to have a planar support only\ndepends on its number of hyperedges. We give an explicit upper bound on the\nnumber of twins necessary for a hypergraph with $m$ hyperedges to have an\n$r$-outerplanar support, which depends only on $r$ and $m$. Since all\nadditional twins can be safely removed, we obtain a linear-time algorithm for\ncomputing $r$-outerplanar supports for hypergraphs with $m$ hyperedges if $m$\nand $r$ are constant; in other words, the problem is fixed-parameter\nlinear-time solvable with respect to the parameters $m$ and $r$. \n\n"}
{"id": "1511.09408", "contents": "Title: Search for correlations between the arrival directions of IceCube\n  neutrino events and ultrahigh-energy cosmic rays detected by the Pierre Auger\n  Observatory and the Telescope Array Abstract: This paper presents the results of different searches for correlations\nbetween very high-energy neutrino candidates detected by IceCube and the\nhighest-energy cosmic rays measured by the Pierre Auger Observatory and the\nTelescope Array. We first consider samples of cascade neutrino events and of\nhigh-energy neutrino-induced muon tracks, which provided evidence for a\nneutrino flux of astrophysical origin, and study their cross-correlation with\nthe ultrahigh-energy cosmic ray (UHECR) samples as a function of angular\nseparation. We also study their possible directional correlations using a\nlikelihood method stacking the neutrino arrival directions and adopting\ndifferent assumptions on the size of the UHECR magnetic deflections. Finally,\nwe perform another likelihood analysis stacking the UHECR directions and using\na sample of through-going muon tracks optimized for neutrino point-source\nsearches with sub-degree angular resolution. No indications of correlations at\ndiscovery level are obtained for any of the searches performed. The smallest of\nthe p-values comes from the search for correlation between UHECRs with IceCube\nhigh-energy cascades, a result that should continue to be monitored. \n\n"}
{"id": "1512.00529", "contents": "Title: Dense magnetized plasma associated with a fast radio burst Abstract: Fast Radio Bursts are bright, unresolved, non-repeating, broadband,\nmillisecond flashes, found primarily at high Galactic latitudes, with\ndispersion measures much larger than expected for a Galactic source. The\ninferred all-sky burst rate is comparable to the core-collapse supernova rate\nout to redshift 0.5. If the observed dispersion measures are assumed to be\ndominated by the intergalactic medium, the sources are at cosmological\ndistances with redshifts of 0.2 to 1. These parameters are consistent with a\nwide range of source models. One fast radio burst showed circular polarization\n[21(7)%] of the radio emission, but no linear polarization was detected, and\nhence no Faraday rotation measure could be determined. Here we report the\nexamination of archival data revealing Faraday rotation in a newly detected\nburst - FRB 110523. It has radio flux at least 0.6 Jy and dispersion measure\n623.30(5) pc cm$^{-3}$. Using Galactic contribution 45 pc cm$^{-3}$ and a model\nof intergalactic electron density, we place the source at a maximum redshift of\n0.5. The burst has rotation measure -186.1(1.4) rad m$^{-2}$, much higher than\nexpected for this line of sight through the Milky Way and the intergalactic\nmedium, indicating magnetization in the vicinity of the source itself or within\na host galaxy. The pulse was scattered by two distinct plasma screens during\npropagation, which requires either a dense nebula associated with the source or\na location within the central region of its host galaxy. Keeping in mind that\nthere may be more than one type of fast radio burst source, the detection in\nthis instance of source-local magnetization and scattering favours models\ninvolving young stellar populations such as magnetars over models involving the\nmergers of older neutron stars, which are more likely to be located in low\ndensity regions of the host galaxy. \n\n"}
{"id": "1512.00775", "contents": "Title: Uncertainty Principle and Sampling of Signals Defined on Graphs Abstract: In many applications, from sensor to social networks, gene regulatory\nnetworks or big data, observations can be represented as a signal defined over\nthe vertices of a graph. Building on the recently introduced Graph Fourier\nTransform, the first contribution of this paper is to provide an uncertainty\nprinciple for signals on graph. As a by-product of this theory, we show how to\nbuild a dictionary of maximally concentrated signals on vertex/frequency\ndomains. Then, we establish a direct relation between uncertainty principle and\nsampling, which forms the basis for a sampling theorem of signals defined on\ngraph. Based on this theory, we show that, besides sampling rate, the samples'\nlocation plays a key role in the performance of signal recovery algorithms.\nHence, we suggest a few alternative sampling strategies and compare them with\nrecently proposed methods. \n\n"}
{"id": "1512.01242", "contents": "Title: Discovery of a FR0 radio galaxy emitting at $\\gamma$-ray energies Abstract: We present supporting evidence for the first association of a Fermi source,\n3FGLJ1330.0-3818, with the FR0 radio galaxy Tol1326-379. FR0s represent the\nmajority of the local radio loud AGN population but their nature is still\nunclear. They share the same properties of FRIs from the point of view of the\nnuclear and host properties, but they show a large deficit of extended radio\nemission. Here we show that FR0s can emit photons at very high energies.\nTol1326-379 has a GeV luminosity of $L_{>1~{\\rm GeV}} \\sim 2\\times10^{42}$ erg\ns$^{-1}$, typical of FRIs, but with a steeper $\\gamma$-ray spectrum\n($\\Gamma=2.78\\pm 0.14$). This could be related to the intrinsic jet properties\nbut also to a different viewing angle. \n\n"}
{"id": "1512.01256", "contents": "Title: Reconstruction of depth-3, top fan-in two circuits over characteristic\n  zero fields Abstract: Reconstruction of arithmetic circuits has been heavily studied in the past\nfew years and has connections to proving lower bounds and deterministic\nidentity testing. In this paper we present a polynomial time randomized\nalgorithm for reconstructing $\\Sigma\\Pi\\Sigma(2)$ circuits over $\\mathbb{F}$\n($char(\\mathbb{F})=0$), i.e. depth$-3$ circuits with fan-in $2$ at the top\naddition gate and having coefficients from a field of characteristic $0$. The\nalgorithm needs only a blackbox query access to the polynomial $f \\in\n\\mathbb{F}[x_1,\\ldots, x_n]$ of degree $d$, computable by a\n$\\Sigma\\Pi\\Sigma(2)$ circuit $C$. In addition, we assume that \"simple rank\" of\nthis polynomial (essential number of variables after removing gcd of the two\nmultiplication gates) is bigger than a constant. Our algorithm runs in time\n$poly(n, d)$ and returns an equivalent $\\Sigma\\Pi\\Sigma(2)$ circuit(with high\nprobability). The problem of reconstructing $\\Sigma\\Pi\\Sigma(2)$ circuits over\nfinite fields was first proposed by Shpilka in [24]. The generalization to\n$\\Sigma\\Pi\\Sigma(k)$ circuits, $k = O(1)$ (over finite fields) was addressed by\nKarnin and Shpilka in [15]. The techniques in these previous involve iterating\nover all objects of certain kinds over the ambient field and thus running time\ndepends on size of the field $\\mathbb{F}$. Their reconstruction algorithm uses\nlower bounds on the lengths of Linear Locally Decodable Codes with $2$ queries.\nIn our settings, such ideas immediately pose a problem and we need new ideas to\nhandle the case of the characteristic $0$ field $\\mathbb{F}$. Our main\ntechniques are based on the use of Quantitative Syslvester Gallai Theorems from\nthe work of Barak et.al. [3] to find a small collection of subspaces to project\nonto. The heart of our paper lies in subtle applications of Quantitative\nSylvester Gallai theorems to prove why projections w.r.t. these subspaces can\nbe glued. \n\n"}
{"id": "1512.01301", "contents": "Title: Cross-correlation method for intermediate-duration gravitational wave\n  searches associated with gamma-ray bursts Abstract: Several models of gamma-ray burst progenitors suggest that the gamma-ray\nevent may be followed by gravitational wave signals of $10^3$-$10^4$ seconds\nduration (possibly accompanying the so-called X-ray afterglow \"plateaus\"). We\nterm these signals \"intermediate-duration\" because they are shorter than\ncontinuous wave signals but longer than signals traditionally considered as\ngravitational wave bursts, and are difficult to detect with most burst and\ncontinuous wave methods. The cross-correlation technique proposed by [S.\nDhurandhar et al., Phys. Rev. D 77, 082001 (2008)], which so far has been used\nonly on continuous wave signals, in principle unifies both burst and continuous\nwave (as well as matched filtering and stochastic background) methods, reducing\nthem to different choices of which data to correlate on which time scales. Here\nwe perform the first tuning of this cross-correlation technique to\nintermediate-duration signals. We derive theoretical estimates of sensitivity\nin Gaussian noise in different limits of the cross-correlation formalism, and\ncompare them to the performance of a prototype search code on simulated\nGaussian-noise data. We estimate that the code is likely able to detect\n\\emph{some} classes of intermediate-duration signals (such as the ones\ndescribed in [A. Corsi \\& P. M\\'esz\\'aros, Astrophys. J., 702, 1171 (2009)])\nfrom sources located at astrophysically-relevant distances of several tens of\nMpc. \n\n"}
{"id": "1512.02122", "contents": "Title: The impulsive phase of magnetar giant flares: assessing linear tearing\n  as the trigger mechanism Abstract: Giant $\\gamma$-ray flares comprise the most extreme radiation events observed\nfrom magnetars. Developing on (sub)millisecond timescales and generating vast\namounts of energy within a fraction of a second, the initial phase of these\nextraordinary bursts present a significant challenge for candidate trigger\nmechanisms. Here we assess and critically analyse the linear growth of the\nrelativistic tearing instability in a globally twisted magnetosphere as the\ntrigger mechanism for giant $\\gamma$-ray flares. Our main constraints are given\nby the observed emission timescales, the energy output of the giant flare\nspike, and inferred dipolar magnetic field strengths. We find that the minimum\ngrowth time of the linear mode is comparable to the $e$-folding rise time, i.e.\n$\\sim10^{-1}$ ms. With this result we constrain basic geometric parameters of\nthe current sheet. We also discuss the validity of the presumption that the\n$e$-folding emission timescale may be equated with the growth time of an MHD\ninstability. \n\n"}
{"id": "1512.02337", "contents": "Title: Fast spectral algorithms from sum-of-squares proofs: tensor\n  decomposition and planted sparse vectors Abstract: We consider two problems that arise in machine learning applications: the\nproblem of recovering a planted sparse vector in a random linear subspace and\nthe problem of decomposing a random low-rank overcomplete 3-tensor. For both\nproblems, the best known guarantees are based on the sum-of-squares method. We\ndevelop new algorithms inspired by analyses of the sum-of-squares method. Our\nalgorithms achieve the same or similar guarantees as sum-of-squares for these\nproblems but the running time is significantly faster.\n  For the planted sparse vector problem, we give an algorithm with running time\nnearly linear in the input size that approximately recovers a planted sparse\nvector with up to constant relative sparsity in a random subspace of $\\mathbb\nR^n$ of dimension up to $\\tilde \\Omega(\\sqrt n)$. These recovery guarantees\nmatch the best known ones of Barak, Kelner, and Steurer (STOC 2014) up to\nlogarithmic factors.\n  For tensor decomposition, we give an algorithm with running time close to\nlinear in the input size (with exponent $\\approx 1.086$) that approximately\nrecovers a component of a random 3-tensor over $\\mathbb R^n$ of rank up to\n$\\tilde \\Omega(n^{4/3})$. The best previous algorithm for this problem due to\nGe and Ma (RANDOM 2015) works up to rank $\\tilde \\Omega(n^{3/2})$ but requires\nquasipolynomial time. \n\n"}
{"id": "1512.03547", "contents": "Title: Graph Isomorphism in Quasipolynomial Time Abstract: We show that the Graph Isomorphism (GI) problem and the related problems of\nString Isomorphism (under group action) (SI) and Coset Intersection (CI) can be\nsolved in quasipolynomial ($\\exp((\\log n)^{O(1)})$) time. The best previous\nbound for GI was $\\exp(O(\\sqrt{n\\log n}))$, where $n$ is the number of vertices\n(Luks, 1983); for the other two problems, the bound was similar,\n$\\exp(\\tilde{O}(\\sqrt{n}))$, where $n$ is the size of the permutation domain\n(Babai, 1983).\n  The algorithm builds on Luks's SI framework and attacks the barrier\nconfigurations for Luks's algorithm by group theoretic \"local certificates\" and\ncombinatorial canonical partitioning techniques. We show that in a well-defined\nsense, Johnson graphs are the only obstructions to effective canonical\npartitioning.\n  Luks's barrier situation is characterized by a homomorphism {\\phi} that maps\na given permutation group $G$ onto $S_k$ or $A_k$, the symmetric or alternating\ngroup of degree $k$, where $k$ is not too small. We say that an element $x$ in\nthe permutation domain on which $G$ acts is affected by {\\phi} if the\n{\\phi}-image of the stabilizer of $x$ does not contain $A_k$. The\naffected/unaffected dichotomy underlies the core \"local certificates\" routine\nand is the central divide-and-conquer tool of the algorithm. \n\n"}
{"id": "1512.05279", "contents": "Title: Improved Bounds for 3SUM, $k$-SUM, and Linear Degeneracy Abstract: Given a set of $n$ real numbers, the 3SUM problem is to decide whether there\nare three of them that sum to zero. Until a recent breakthrough by Gr{\\o}nlund\nand Pettie [FOCS'14], a simple $\\Theta(n^2)$-time deterministic algorithm for\nthis problem was conjectured to be optimal. Over the years many algorithmic\nproblems have been shown to be reducible from the 3SUM problem or its variants,\nincluding the more generalized forms of the problem, such as $k$-SUM and\n$k$-variate linear degeneracy testing ($k$-LDT). The conjectured hardness of\nthese problems have become extremely popular for basing conditional lower\nbounds for numerous algorithmic problems in P.\n  In this paper, we show that the randomized $4$-linear decision tree\ncomplexity of 3SUM is $O(n^{3/2})$, and that the randomized $(2k-2)$-linear\ndecision tree complexity of $k$-SUM and $k$-LDT is $O(n^{k/2})$, for any odd\n$k\\ge 3$. These bounds improve (albeit randomized) the corresponding\n$O(n^{3/2}\\sqrt{\\log n})$ and $O(n^{k/2}\\sqrt{\\log n})$ decision tree bounds\nobtained by Gr{\\o}nlund and Pettie. Our technique includes a specialized\nrandomized variant of fractional cascading data structure. Additionally, we\ngive another deterministic algorithm for 3SUM that runs in $O(n^2 \\log\\log n /\n\\log n )$ time. The latter bound matches a recent independent bound by Freund\n[Algorithmica 2017], but our algorithm is somewhat simpler, due to a better use\nof word-RAM model. \n\n"}
{"id": "1512.05368", "contents": "Title: Observationally constraining gravitational wave emission from short\n  gamma-ray burst remnants Abstract: Observations of short gamma-ray bursts indicate ongoing energy injection\nfollowing the prompt emission, with the most likely candidate being the birth\nof a rapidly rotating, highly magnetised neutron star. We utilise X-ray\nobservations of the burst remnant to constrain properties of the nascent\nneutron star, including its magnetic field-induced ellipticity and the\nsaturation amplitude of various oscillation modes. Moreover, we derive strict\nupper limits on the gravitational wave emission from these objects by looking\nonly at the X-ray light curve, showing the burst remnants are unlikely to be\ndetected in the near future using ground-based gravitational wave\ninterferometers such as Advanced LIGO. \n\n"}
{"id": "1512.05453", "contents": "Title: Extremal Black Holes in Dynamical Chern-Simons Gravity Abstract: Rapidly rotating black hole solutions in theories beyond general relativity\nplay a key role in experimental gravity, as they allow us to compute\nobservables in extreme spacetimes that deviate from the predictions of general\nrelativity. Such solutions are often difficult to find in\nbeyond-general-relativity theories due to the inclusion of additional fields\nthat couple to the metric non-linearly and non-minimally. In this paper, we\nconsider rotating black hole solutions in one such theory, dynamical\nChern-Simons gravity, where the Einstein-Hilbert action is modified by the\nintroduction of a dynamical scalar field that couples to the metric through the\nPontryagin density. We treat dynamical Chern-Simons gravity as an effective\nfield theory and work in the decoupling limit, where corrections are treated as\nsmall perturbations from general relativity. We perturb about the\nmaximally-rotating Kerr solution, the so-called extremal limit, and develop\nmathematical insight into the analysis techniques needed to construct solutions\nfor generic spin. First we find closed-form, analytic expressions for the\nextremal scalar field, and then determine the trace of the metric perturbation,\ngiving both in terms of Legendre decompositions. Retaining only the first three\nand four modes in the Legendre representation of the scalar field and the\ntrace, respectively, suffices to ensure a fidelity of over 99% relative to full\nnumerical solutions. The leading-order mode in the Legendre expansion of the\ntrace of the metric perturbation contains a logarithmic divergence at the\nextremal Kerr horizon, which is likely to be unimportant as it occurs inside\nthe perturbed dynamical Chern-Simons horizon. The techniques employed here\nshould enable the construction of analytic, closed-form expressions for the\nscalar field and metric perturbations on a background with arbitrary rotation. \n\n"}
{"id": "1512.06678", "contents": "Title: Solving $k$-SUM using few linear queries Abstract: The $k$-SUM problem is given $n$ input real numbers to determine whether any\n$k$ of them sum to zero. The problem is of tremendous importance in the\nemerging field of complexity theory within $P$, and it is in particular open\nwhether it admits an algorithm of complexity $O(n^c)$ with $c<\\lceil\n\\frac{k}{2} \\rceil$. Inspired by an algorithm due to Meiser (1993), we show\nthat there exist linear decision trees and algebraic computation trees of depth\n$O(n^3\\log^3 n)$ solving $k$-SUM. Furthermore, we show that there exists a\nrandomized algorithm that runs in $\\tilde{O}(n^{\\lceil \\frac{k}{2} \\rceil+8})$\ntime, and performs $O(n^3\\log^3 n)$ linear queries on the input. Thus, we show\nthat it is possible to have an algorithm with a runtime almost identical (up to\nthe $+8$) to the best known algorithm but for the first time also with the\nnumber of queries on the input a polynomial that is independent of $k$. The\n$O(n^3\\log^3 n)$ bound on the number of linear queries is also a tighter bound\nthan any known algorithm solving $k$-SUM, even allowing unlimited total time\noutside of the queries. By simultaneously achieving few queries to the input\nwithout significantly sacrificing runtime vis-\\`{a}-vis known algorithms, we\ndeepen the understanding of this canonical problem which is a cornerstone of\ncomplexity-within-$P$.\n  We also consider a range of tradeoffs between the number of terms involved in\nthe queries and the depth of the decision tree. In particular, we prove that\nthere exist $o(n)$-linear decision trees of depth $o(n^4)$. \n\n"}
{"id": "1512.07217", "contents": "Title: Searching for decaying dark matter in deep XMM-Newton observation of the\n  Draco dwarf spheroidal Abstract: We present results of a search for the 3.5 keV emission line in our recent\nvery long (~ 1.4 Ms) XMM-Newton observation of the Draco dwarf spheroidal\ngalaxy. The astrophysical X-ray emission from such dark matter-dominated\ngalaxies is faint, thus they provide a test for the dark matter origin of the\n3.5 keV line previously detected in other massive, but X-ray bright objects,\nsuch as galaxies and galaxy clusters. We do not detect a statistically\nsignificant emission line from Draco; this constrains the lifetime of a\ndecaying dark matter particle to tau > (7-9) x 10^27 s at 95% CL (combining all\nthree XMM-Newton cameras; the interval corresponds to the uncertainty of the\ndark matter column density in the direction of Draco). The PN camera, which has\nthe highest sensitivity of the three, does show a positive spectral residual\n(above the carefully modeled continuum) at E = 3.54 +/- 0.06 keV with a 2.3\nsigma significance. The two MOS cameras show less-significant or no positive\ndeviations, consistently within 1 sigma with PN. Our Draco limit on tau is\nconsistent with previous detections in the stacked galaxy clusters, M31 and the\nGalactic Center within their 1-2 sigma uncertainties, but is inconsistent with\nthe high signal from the core of the Perseus cluster (which has itself been\ninconsistent with the rest of the detections). We conclude that this Draco\nobservation does not exclude the dark matter interpretation of the 3.5 keV line\nin those objects. \n\n"}
{"id": "1512.08757", "contents": "Title: General Cut-Generating Procedures for the Stable Set Polytope Abstract: We propose general separation procedures for generating cuts for the stable\nset polytope, inspired by a procedure by Rossi and Smriglio and applying a\nlifting method by Xavier and Camp\\^{e}lo. In contrast to existing\ncut-generating procedures, ours generate both rank and non-rank valid\ninequalities, hence they are of a more general nature than existing methods.\nThis is accomplished by iteratively solving a lifting problem, which consists\nof a maximum weighted stable set problem on a smaller graph. Computational\nexperience on DIMACS benchmark instances shows that the proposed approach may\nbe a useful tool for generating cuts for the stable set polytope. \n\n"}
{"id": "1512.09170", "contents": "Title: Statistical Query Algorithms for Mean Vector Estimation and Stochastic\n  Convex Optimization Abstract: Stochastic convex optimization, where the objective is the expectation of a\nrandom convex function, is an important and widely used method with numerous\napplications in machine learning, statistics, operations research and other\nareas. We study the complexity of stochastic convex optimization given only\nstatistical query (SQ) access to the objective function. We show that\nwell-known and popular first-order iterative methods can be implemented using\nonly statistical queries. For many cases of interest we derive nearly matching\nupper and lower bounds on the estimation (sample) complexity including linear\noptimization in the most general setting. We then present several consequences\nfor machine learning, differential privacy and proving concrete lower bounds on\nthe power of convex optimization based methods.\n  The key ingredient of our work is SQ algorithms and lower bounds for\nestimating the mean vector of a distribution over vectors supported on a convex\nbody in $\\mathbb{R}^d$. This natural problem has not been previously studied\nand we show that our solutions can be used to get substantially improved SQ\nversions of Perceptron and other online algorithms for learning halfspaces. \n\n"}
{"id": "1601.00271", "contents": "Title: Firefighting on Trees Beyond Integrality Gaps Abstract: The Firefighter problem and a variant of it, known as Resource Minimization\nfor Fire Containment (RMFC), are natural models for optimal inhibition of\nharmful spreading processes. Despite considerable progress on several fronts,\nthe approximability of these problems is still badly understood. This is the\ncase even when the underlying graph is a tree, which is one of the most-studied\ngraph structures in this context and the focus of this paper. In their simplest\nversion, a fire spreads from one fixed vertex step by step from burning to\nadjacent non-burning vertices, and at each time step, $B$ many non-burning\nvertices can be protected from catching fire. The Firefighter problem asks, for\na given $B$, to maximize the number of vertices that will not catch fire,\nwhereas RMFC (on a tree) asks to find the smallest $B$ that allows for saving\nall leaves of the tree. Prior to this work, the best known approximation ratios\nwere an $O(1)$-approximation for the Firefighter problem and an $O(\\log^*\nn)$-approximation for RMFC, both being LP-based and essentially matching the\nintegrality gaps of two natural LP relaxations.\n  We improve on both approximations by presenting a PTAS for the Firefighter\nproblem and an $O(1)$-approximation for RMFC, both qualitatively matching the\nknown hardness results. Our results are obtained through a combination of the\nknown LPs with several new techniques, which allow for efficiently enumerating\nsubsets of super-constant size of a good solution to obtain stronger LPs. \n\n"}
{"id": "1601.01384", "contents": "Title: Nonlinear Evolution and Final Fate of Charged Anti-de Sitter Black Hole\n  Superradiant Instability Abstract: We describe the full nonlinear development of the superradiant instability\nfor a charged massless scalar field, coupled to general relativity and\nelectromagnetism, in the vicinity of a Reissner-Nordstrom-AdS black hole. The\npresence of the negative cosmological constant provides a natural context for\nconsidering perfectly reflecting boundary conditions and studying the dynamics\nas the scalar field interacts repeatedly with the black hole. At early times,\nsmall superradiant perturbations grow as expected from linearized studies.\nBackreaction then causes the black hole to lose charge and mass until the\nperturbation becomes nonsuperradiant, with the final state described by a\nstable hairy black hole. For large gauge coupling, the instability extracts a\nlarge amount of charge per unit mass, resulting in greater entropy increase. We\ndiscuss the implications of the observed behavior for the general problem of\nsuperradiance in black hole spacetimes. \n\n"}
{"id": "1601.02171", "contents": "Title: I-Love-Q Relations: From Compact Stars to Black Holes Abstract: The relations between most observables associated with a compact star, such\nas the mass and radius of a neutron star or a quark star, typically depend\nstrongly on their unknown internal structure. The I-Love-Q relations (between\nthe moment of inertia, the tidal deformability and the quadrupole moment) are\nhowever approximately insensitive to this structure. These relations become\nexact for stationary black holes in General Relativity as shown by the no-hair\ntheorems. In this paper, we take the first steps toward studying how the\napproximate I-Love-Q relations become exact in the limit as compact stars\nbecome black holes. To do so, we consider a toy model, i.e. incompressible\nstars with anisotropic pressure, which allows us to model an equilibrium\nsequence of stars with their compactness approaching the black hole limit\narbitrarily closely. We extract the I-Love-Q trio by numerically constructing\nsuch a sequence in the slow-rotation and small-tide approximations. We find\nthat the I-Love-Q relations approach the black hole limit in a nontrivial way,\nwith the quadrupole moment and the tidal deformability changing sign as the\ncompactness and the amount of anisotropy are increased. Generalizing Maclaurin\nspheroids to anisotropic stars, we show that the multipole moments also change\nsign in the Newtonian limit as the amount of anisotropy is increased. We also\nprove analytically that the stellar moment of inertia reaches the black hole\nlimit as the compactness reaches the black hole value in the strongly\nanisotropic limit. Modeling the black hole limit through a sequence of\nanisotropic stars, however, fails when considering other theories of gravity.\nWe calculate the scalar dipole charge and the moment of inertia in a\nparity-violating modified theory and find that these quantities do not tend to\ntheir black hole counterparts as the anisotropic stellar sequence approaches\nthe black hole limit. \n\n"}
{"id": "1601.03030", "contents": "Title: Simulated Quantum Annealing Can Be Exponentially Faster than Classical\n  Simulated Annealing Abstract: Simulated Quantum Annealing (SQA) is a Markov Chain Monte-Carlo algorithm\nthat samples the equilibrium thermal state of a Quantum Annealing (QA)\nHamiltonian. In addition to simulating quantum systems, SQA has also been\nproposed as another physics-inspired classical algorithm for combinatorial\noptimization, alongside classical simulated annealing. However, in many cases\nit remains an open challenge to determine the performance of both QA and SQA.\nOne piece of evidence for the strength of QA over classical simulated annealing\ncomes from an example by Farhi, Goldstone and Gutmann . There a bit-symmetric\ncost function with a thin, high energy barrier was designed to show an\nexponential seperation between classical simulated annealing, for which thermal\nfluctuations take exponential time to climb the barrier, and quantum annealing\nwhich passes through the barrier and reaches the global minimum in poly time,\narguably by taking advantage of quantum tunneling. In this work we apply a\ncomparison method to rigorously show that the Markov chain underlying SQA\nefficiently samples the target distribution and finds the global minimum of\nthis spike cost function in polynomial time. Our work provides evidence for the\ngrowing consensus that SQA inherits at least some of the advantages of\ntunneling in QA, and so QA is unlikely to achieve exponential speedups over\nclassical computing solely by the use of quantum tunneling. Since we analyze\nonly a particular model this evidence is not decisive. However, techniques\napplied here---including warm starts from the adiabatic path and the use of the\nquantum ground state probability distribution to understand the stationary\ndistribution of SQA---may be valuable for future studies of the performance of\nSQA on cost functions for which QA is efficient. \n\n"}
{"id": "1601.03676", "contents": "Title: Arbitrary Overlap Constraints in Graph Packing Problems Abstract: In earlier versions of the community discovering problem, the overlap between\ncommunities was restricted by a simple count upper-bound [17,5,11,8]. In this\npaper, we introduce the $\\Pi$-Packing with $\\alpha()$-Overlap problem to allow\nfor more complex constraints in the overlap region than those previously\nstudied. Let $\\mathcal{V}^r$ be all possible subsets of vertices of $V(G)$ each\nof size at most $r$, and $\\alpha: \\mathcal{V}^r \\times \\mathcal{V}^r \\to\n\\{0,1\\}$ be a function. The $\\Pi$-Packing with $\\alpha()$-Overlap problem seeks\nat least $k$ induced subgraphs in a graph $G$ subject to: (i) each subgraph has\nat most $r$ vertices and obeys a property $\\Pi$, and (ii) for any pair\n$H_i,H_j$, with $i\\neq j$, $\\alpha(H_i, H_j) = 0$ (i.e., $H_i,H_j$ do not\nconflict). We also consider a variant that arises in clustering applications:\neach subgraph of a solution must contain a set of vertices from a given\ncollection of sets $\\mathcal{C}$, and no pair of subgraphs may share vertices\nfrom the sets of $\\mathcal{C}$. In addition, we propose similar formulations\nfor packing hypergraphs. We give an $O(r^{rk} k^{(r+1)k} n^{cr})$ algorithm for\nour problems where $k$ is the parameter and $c$ and $r$ are constants, provided\nthat: i) $\\Pi$ is computable in polynomial time in $n$ and ii) the function\n$\\alpha()$ satisfies specific conditions. Specifically, $\\alpha()$ is\nhereditary, applicable only to overlapping subgraphs, and computable in\npolynomial time in $n$. Motivated by practical applications we give several\nexamples of $\\alpha()$ functions which meet those conditions. \n\n"}
{"id": "1601.06781", "contents": "Title: Dark matter subhalos and unidentified sources in the Fermi 3FGL source\n  catalog Abstract: If dark matter consists of weakly interacting massive particles (WIMPs), dark\nmatter subhalos in the Milky Way could be detectable as gamma-ray point sources\ndue to WIMP annihilation. In this work, we perform an updated study of the\ndetectability of dark matter subhalos as gamma-ray sources with the Fermi Large\nArea Telescope (Fermi LAT). We use the results of the Via Lactea II simulation,\nscaled to the Planck 2015 cosmological parameters, to predict the local dark\nmatter subhalo distribution. Under optimistic assumptions for the WIMP\nparameters --- a 40 GeV particle annihilating to $b\\bar{b}$ with a thermal\ncross-section, as required to explain the Galactic center GeV excess --- we\npredict that at most $\\sim 10$ subhalos might be present in the third Fermi LAT\nsource catalog (3FGL). This is a smaller number than has been predicted by\nprior studies, and we discuss the origin of this difference. We also compare\nour predictions for the detectability of subhalos with the number of subhalo\ncandidate sources in 3FGL, and derive upper limits on the WIMP annihilation\ncross-section as a function of the particle mass. If a dark matter\ninterpretation could be excluded for all 3FGL sources, our constraints would be\ncompetitive with those found by indirect searches using other targets, such as\nknown Milky Way satellite galaxies. \n\n"}
{"id": "1601.07217", "contents": "Title: Possible confirmation of the existence of ergoregion by the Kerr\n  quasinormal mode in gravitational waves from Pop III massive black hole\n  binary Abstract: The existence of the ergoregion of the Kerr space-time has not been confirmed\nobservationally yet. We show that the confirmation would be possible by\nobserving the quasinormal mode in gravitational waves. As an example, using the\nrecent population synthesis results of Pop III binary black holes, we find that\nthe peak of the final merger mass ($M_f$) is about $50~\\rm M_{\\odot}$, while\nthe fraction of the final spin $q_f = a_f/M_f > 0.7$ needed for the\nconfirmation of a part of ergoregion is $\\sim 77\\%$. To confirm the frequency\nof the quasinormal mode, ${\\rm SNR} > 35$ is needed. The standard model of Pop\nIII population synthesis tells us that the event rate for the confirmation of\nmore than $50\\%$ of the ergoregion by the second generation gravitational wave\ndetectors is $\\sim 2.3$ ${\\rm events\\ yr^{-1}\\ (SFR_p/(10^{-2.5}\\ M_\\odot\nyr^{-1}\\ Mpc^{-3}))} \\cdot (\\rm [f_b/(1+f_b)]/0.33)$ where ${\\rm SFR_p}$ and\n${\\rm f_b}$ are the peak value of the Pop III star formation rate and the\nfraction of binaries, respectively. \n\n"}
{"id": "1601.08224", "contents": "Title: New classes of degree sequences with fast mixing swap Markov chain\n  sampling Abstract: In network modeling of complex systems one is often required to sample random\nrealizations of networks that obey a given set of constraints, usually in form\nof graph measures. A much studied class of problems targets uniform sampling of\nsimple graphs with given degree sequence or also with given degree correlations\nexpressed in the form of a joint degree matrix. One approach is to use Markov\nchains based on edge switches (swaps) that preserve the constraints, are\nirreducible (ergodic) and fast mixing. In 1999, Kannan, Tetali and Vempala\n(KTV) proposed a simple swap Markov chain for sampling graphs with given degree\nsequence and conjectured that it mixes rapidly (in poly-time) for arbitrary\ndegree sequences. While the conjecture is still open, it was proven for special\ndegree sequences, in particular, for those of undirected and directed regular\nsimple graphs, of half-regular bipartite graphs, and of graphs with certain\nbounded maximum degrees. Here we prove the fast mixing KTV conjecture for\nnovel, exponentially large classes of irregular degree sequences. Our method is\nbased on a canonical decomposition of degree sequences into split graph degree\nsequences, a structural theorem for the space of graph realizations and on a\nfactorization theorem for Markov chains. After introducing bipartite splitted\ndegree sequences, we also generalize the canonical split graph decomposition\nfor bipartite and directed graphs. \n\n"}
{"id": "1602.01295", "contents": "Title: How proofs are prepared at Camelot Abstract: We study a design framework for robust, independently verifiable, and\nworkload-balanced distributed algorithms working on a common input. An\nalgorithm based on the framework is essentially a distributed encoding\nprocedure for a Reed--Solomon code, which enables (a) robustness against\nbyzantine failures with intrinsic error-correction and identification of failed\nnodes, and (b) independent randomized verification to check the entire\ncomputation for correctness, which takes essentially no more resources than\neach node individually contributes to the computation. The framework builds on\nrecent Merlin--Arthur proofs of batch evaluation of Williams~[{\\em Electron.\\\nColloq.\\ Comput.\\ Complexity}, Report TR16-002, January 2016] with the\nobservation that {\\em Merlin's magic is not needed} for batch evaluation---mere\nKnights can prepare the proof, in parallel, and with intrinsic\nerror-correction.\n  The contribution of this paper is to show that in many cases the verifiable\nbatch evaluation framework admits algorithms that match in total resource\nconsumption the best known sequential algorithm for solving the problem. As our\nmain result, we show that the $k$-cliques in an $n$-vertex graph can be counted\n{\\em and} verified in per-node $O(n^{(\\omega+\\epsilon)k/6})$ time and space on\n$O(n^{(\\omega+\\epsilon)k/6})$ compute nodes, for any constant $\\epsilon>0$ and\npositive integer $k$ divisible by $6$, where $2\\leq\\omega<2.3728639$ is the\nexponent of matrix multiplication. This matches in total running time the best\nknown sequential algorithm, due to Ne{\\v{s}}et{\\v{r}}il and Poljak [{\\em\nComment.~Math.~Univ.~Carolin.}~26 (1985) 415--419], and considerably improves\nits space usage and parallelizability. Further results include novel algorithms\nfor counting triangles in sparse graphs, computing the chromatic polynomial of\na graph, and computing the Tutte polynomial of a graph. \n\n"}
{"id": "1602.02026", "contents": "Title: Graph parameters from symplectic group invariants Abstract: In this paper we introduce, and characterize, a class of graph parameters\nobtained from tensor invariants of the symplectic group. These parameters are\nsimilar to partition functions of vertex models, as introduced by de la Harpe\nand Jones, [P. de la Harpe, V.F.R. Jones, Graph invariants related to\nstatistical mechanical models: examples and problems, Journal of Combinatorial\nTheory, Series B 57 (1993) 207-227]. Yet they give a completely different class\nof graph invariants. We moreover show that certain evaluations of the cycle\npartition polynomial, as defined by Martin [P. Martin, Enum\\'erations\neul\\'eriennes dans les multigraphes et invariants de Tutte-Grothendieck, Diss.\nInstitut National Polytechnique de Grenoble-INPG; Universit\\'e\nJoseph-Fourier-Grenoble I, 1977], give examples of graph parameters that can be\nobtained this way. \n\n"}
{"id": "1602.05263", "contents": "Title: Scheduling MapReduce Jobs under Multi-Round Precedences Abstract: We consider non-preemptive scheduling of MapReduce jobs with multiple tasks\nin the practical scenario where each job requires several map-reduce rounds. We\nseek to minimize the average weighted completion time and consider scheduling\non identical and unrelated parallel processors. For identical processors, we\npresent LP-based O(1)-approximation algorithms. For unrelated processors, the\napproximation ratio naturally depends on the maximum number of rounds of any\njob. Since the number of rounds per job in typical MapReduce algorithms is a\nsmall constant, our scheduling algorithms achieve a small approximation ratio\nin practice. For the single-round case, we substantially improve on previously\nbest known approximation guarantees for both identical and unrelated\nprocessors. Moreover, we conduct an experimental analysis and compare the\nperformance of our algorithms against a fast heuristic and a lower bound on the\noptimal solution, thus demonstrating their promising practical performance. \n\n"}
{"id": "1602.05586", "contents": "Title: Carbon Shell or Core Ignitions in White Dwarfs Accreting from Helium\n  Stars Abstract: White dwarfs accreting from helium stars can stably burn at the accreted rate\nand avoid the challenge of mass loss associated with unstable Helium burning\nthat is a concern for many Type Ia supernovae scenarios. We study binaries with\nhelium stars of mass $1.25 M_\\odot\\le M_{\\rm{He}} \\le 1.8 M_\\odot$, which have\nlost their hydrogen rich envelopes in an earlier common envelope event and now\norbit with periods ($P_{\\rm orb}$) of several hours with non-rotating $0.84$\nand $1.0 M_\\odot$ C/O WDs. The helium stars fill their Roche lobes (RLs) after\nexhaustion of central helium and donate helium on their thermal timescales\n(${\\sim}10^5$yr). As shown by others, these mass transfer rates coincide with\nthe steady helium burning range for WDs, and grow the WD core up to near the\nChandrasekhar mass ($M_{\\rm Ch}$) and a core carbon ignition. We show here,\nhowever, that many of these scenarios lead to an ignition of hot carbon ashes\nnear the outer edge of the WD and an inward going carbon flame that does not\ncause an explosive outcome. For $P_{\\rm orb} = 3$ hours, $1.0 M_\\odot$ C/O WDs\nwith donor masses $M_{\\rm He}\\gtrsim1.8 M_\\odot$ experience a shell carbon\nignition, while $M_{\\rm He}\\lesssim1.3 M_\\odot$ will fall below the steady\nhelium burning range and undergo helium flashes before reaching core C\nignition. Those with $1.3 M_\\odot \\lesssim M_{\\rm He} \\lesssim 1.7 M_\\odot$\nwill experience a core C ignition. We also calculate the retention fraction of\naccreted helium when the accretion rate leads to recurrent weak helium flashes. \n\n"}
{"id": "1602.07194", "contents": "Title: Lens depth function and k-relative neighborhood graph: versatile tools\n  for ordinal data analysis Abstract: In recent years it has become popular to study machine learning problems in a\nsetting of ordinal distance information rather than numerical distance\nmeasurements. By ordinal distance information we refer to binary answers to\ndistance comparisons such as $d(A,B)<d(C,D)$. For many problems in machine\nlearning and statistics it is unclear how to solve them in such a scenario. Up\nto now, the main approach is to explicitly construct an ordinal embedding of\nthe data points in the Euclidean space, an approach that has a number of\ndrawbacks. In this paper, we propose algorithms for the problems of medoid\nestimation, outlier identification, classification, and clustering when given\nonly ordinal data. They are based on estimating the lens depth function and the\n$k$-relative neighborhood graph on a data set. Our algorithms are simple, are\nmuch faster than an ordinal embedding approach and avoid some of its drawbacks,\nand can easily be parallelized. \n\n"}
{"id": "1602.07876", "contents": "Title: On Satisfiability Problems with a Linear Structure Abstract: It was recently shown \\cite{STV} that satisfiability is polynomially solvable\nwhen the incidence graph is an interval bipartite graph (an interval graph\nturned into a bipartite graph by omitting all edges within each partite set).\nHere we relax this condition in several directions: First, we show that it\nholds for $k$-interval bigraphs, bipartite graphs which can be converted to\ninterval bipartite graphs by adding to each node of one side at most $k$ edges;\nthe same result holds for the counting and the weighted maximization version of\nsatisfiability. Second, given two linear orders, one for the variables and one\nfor the clauses, we show how to find, in polynomial time, the smallest $k$ such\nthat there is a $k$-interval bigraph compatible with these two orders. On the\nnegative side we prove that, barring complexity collapses, no such extensions\nare possible for CSPs more general than satisfiability. We also show\nNP-hardness of recognizing 1-interval bigraphs. \n\n"}
{"id": "1602.08369", "contents": "Title: Approximation Complexity of Max-Cut on Power Law Graphs Abstract: In this paper we study the MAX-CUT problem on power law graphs (PLGs) with\npower law exponent $\\beta$. We prove some new approximability results on that\nproblem. In particular we show that there exist polynomial time approximation\nschemes (PTAS) for MAX-CUT on PLGs for the power law exponent $\\beta$ in the\ninterval $(0,2)$. For $\\beta>2$ we show that for some $\\epsilon>0$, MAX-CUT is\nNP-hard to approximate within approximation ratio $1+\\epsilon$, ruling out the\nexistence of a PTAS in this case. Moreover we give an approximation algorithm\nwith improved constant approximation ratio for the case of $\\beta>2$. \n\n"}
{"id": "1602.08393", "contents": "Title: Exact Weighted Minwise Hashing in Constant Time Abstract: Weighted minwise hashing (WMH) is one of the fundamental subroutine, required\nby many celebrated approximation algorithms, commonly adopted in industrial\npractice for large scale-search and learning. The resource bottleneck of the\nalgorithms is the computation of multiple (typically a few hundreds to\nthousands) independent hashes of the data. The fastest hashing algorithm is by\nIoffe \\cite{Proc:Ioffe_ICDM10}, which requires one pass over the entire data\nvector, $O(d)$ ($d$ is the number of non-zeros), for computing one hash.\nHowever, the requirement of multiple hashes demands hundreds or thousands\npasses over the data. This is very costly for modern massive dataset.\n  In this work, we break this expensive barrier and show an expected constant\namortized time algorithm which computes $k$ independent and unbiased WMH in\ntime $O(k)$ instead of $O(dk)$ required by Ioffe's method. Moreover, our\nproposal only needs a few bits (5 - 9 bits) of storage per hash value compared\nto around $64$ bits required by the state-of-art-methodologies. Experimental\nevaluations, on real datasets, show that for computing 500 WMH, our proposal\ncan be 60000x faster than the Ioffe's method without losing any accuracy. Our\nmethod is also around 100x faster than approximate heuristics capitalizing on\nthe efficient \"densified\" one permutation hashing schemes\n\\cite{Proc:OneHashLSH_ICML14}. Given the simplicity of our approach and its\nsignificant advantages, we hope that it will replace existing implementations\nin practice. \n\n"}
{"id": "1603.02981", "contents": "Title: Ant-Inspired Density Estimation via Random Walks Abstract: Many ant species employ distributed population density estimation in\napplications ranging from quorum sensing [Pra05], to task allocation [Gor99],\nto appraisal of enemy colony strength [Ada90]. It has been shown that ants\nestimate density by tracking encounter rates -- the higher the population\ndensity, the more often the ants bump into each other [Pra05,GPT93].\n  We study distributed density estimation from a theoretical perspective. We\nprove that a group of anonymous agents randomly walking on a grid are able to\nestimate their density within a small multiplicative error in few steps by\nmeasuring their rates of encounter with other agents. Despite dependencies\ninherent in the fact that nearby agents may collide repeatedly (and, worse,\ncannot recognize when this happens), our bound nearly matches what would be\nrequired to estimate density by independently sampling grid locations.\n  From a biological perspective, our work helps shed light on how ants and\nother social insects can obtain relatively accurate density estimates via\nencounter rates. From a technical perspective, our analysis provides new tools\nfor understanding complex dependencies in the collision probabilities of\nmultiple random walks. We bound the strength of these dependencies using\n$local\\ mixing\\ properties$ of the underlying graph. Our results extend beyond\nthe grid to more general graphs and we discuss applications to size estimation\nfor social networks and density estimation for robot swarms. \n\n"}
{"id": "1603.03205", "contents": "Title: Hadwiger's Conjecture for squares of 2-Trees Abstract: Hadwiger's conjecture asserts that any graph contains a clique minor with\norder no less than the chromatic number of the graph. We prove that this\nwell-known conjecture is true for all graphs if and only if it is true for\nsquares of split graphs. This observation implies that Hadwiger's conjecture\nfor squares of chordal graphs is as difficult as the general case, since\nchordal graphs are a superclass of split graphs. Then we consider 2-trees which\nare a subclass of each of planar graphs, 2-degenerate graphs and chordal\ngraphs. We prove that Hadwiger's conjecture is true for squares of $2$-trees.\nWe achieve this by proving the following stronger result: for any $2$-tree $T$,\nits square $T^2$ has a clique minor of order $\\chi(T^2)$ for which each branch\nset induces a path, where $\\chi(T^2)$ is the chromatic number of $T^2$. \n\n"}
{"id": "1603.04451", "contents": "Title: The Quadratic Minimum Spanning Tree Problem and its Variations Abstract: The quadratic minimum spanning tree problem and its variations such as the\nquadratic bottleneck spanning tree problem, the minimum spanning tree problem\nwith conflict pair constraints, and the bottleneck spanning tree problem with\nconflict pair constraints are useful in modeling various real life\napplications. All these problems are known to be NP-hard. In this paper, we\ninvestigate these problems to obtain additional insights into the structure of\nthe problems and to identify possible demarcation between easy and hard special\ncases. New polynomially solvable cases have been identified, as well as NP-hard\ninstances on very simple graphs. As a byproduct, we have a recursive formula\nfor counting the number of spanning trees on a $(k,n)$-accordion and a\ncharacterization of matroids in the context of a quadratic objective function. \n\n"}
{"id": "1603.04876", "contents": "Title: Neutron stars in Horndeski gravity Abstract: Horndeski's theory of gravity is the most general scalar-tensor theory with a\nsingle scalar whose equations of motion contain at most second-order\nderivatives. A subsector of Horndeski's theory known as \"Fab Four\" gravity\nallows for dynamical self-tuning of the quantum vacuum energy, and therefore it\nhas received particular attention in cosmology as a possible alternative to the\n$\\Lambda$CDM model. Here we study compact stars in Fab Four gravity, which\nincludes as special cases general relativity (\"George\"),\nEinstein-dilaton-Gauss-Bonnet gravity (\"Ringo\"), theories with a nonminimal\ncoupling with the Einstein tensor (\"John\") and theories involving the\ndouble-dual of the Riemann tensor (\"Paul\"). We generalize and extend previous\nresults in theories of the John class and were not able to find realistic\ncompact stars in theories involving the Paul class. \n\n"}
{"id": "1603.06871", "contents": "Title: Cops, Robber and Medianwidth Parameters Abstract: In previous work, we introduced median decompositions, a generalisation of\ntree decompositions where a graph can be modelled after any median graph, along\nwith a hierarchy of $i$-medianwidth parameters $(mw_i)_{i\\geq 1}$ starting from\ntreewidth and converging to the clique number.\n  We introduce another graph parameter based on the concept of median\ndecompositions, to be called $i$-latticewidth and denoted by $lw_i$, for which\nwe restrict the modelling median graph of a decomposition to be isometrically\nembeddable into the Cartesian product of $i$ paths. The sequence $(lw_i)_{i\\geq\n1}$ gives rise to a hierarchy of parameters starting from pathwidth and\nconverging to the clique number. We characterise the $i$-latticewidth of a\ngraph in terms of maximal intersections of bags of $i$ path decompositions of\nthe graph.\n  We study a generalisation of the classical Cops and Robber game, where the\nrobber plays against not just one, but $i$ cop players. Depending on whether\nthe robber is visible or not, we show a direct connection to $i$-medianwidth or\n$i$-latticewidth, respectively. \n\n"}
{"id": "1603.07443", "contents": "Title: Non-Hermitian ${\\cal PT}$-symmetric relativistic quantum theory in an\n  intensive magnetic field Abstract: We develop relativistic non-Hermitian quantum theory and its application to\nneutrino physics in a strong magnetic field. It is well known, that one of the\nfundamental postulates of quantum theory is the requirement of Hermiticity of\nphysical parameters. This condition not only guarantees the reality of the\neigenvalues of Hamiltonian operators, but also implies the preservation of the\nprobabilities of the considered quantum processes. However as it was shown\nrelatively recently (Bender, Boettcher 1998), Hermiticity is a sufficient but\nit is not a necessary condition. It turned out that among non-Hermitian\nHamiltonians it is possible to allocate a number of such which have real energy\nspectra and can ensure the development of systems over time with preserving\nunitarity. This type of Hamiltonians includes so-called parity-time (${\\cal\nPT}$) symmetric models which is already used in various fields of modern\nphysics. The most developed in this respect are models, which used in the field\nof ${\\cal PT}$-symmetric optics, where for several years produced not only\ntheoretical but experimental studies. \n\n"}
{"id": "1604.04111", "contents": "Title: Lossy Kernelization Abstract: In this paper we propose a new framework for analyzing the performance of\npreprocessing algorithms. Our framework builds on the notion of kernelization\nfrom parameterized complexity. However, as opposed to the original notion of\nkernelization, our definitions combine well with approximation algorithms and\nheuristics. The key new definition is that of a polynomial size\n$\\alpha$-approximate kernel. Loosely speaking, a polynomial size\n$\\alpha$-approximate kernel is a polynomial time pre-processing algorithm that\ntakes as input an instance $(I,k)$ to a parameterized problem, and outputs\nanother instance $(I',k')$ to the same problem, such that $|I'|+k' \\leq\nk^{O(1)}$. Additionally, for every $c \\geq 1$, a $c$-approximate solution $s'$\nto the pre-processed instance $(I',k')$ can be turned in polynomial time into a\n$(c \\cdot \\alpha)$-approximate solution $s$ to the original instance $(I,k)$.\n  Our main technical contribution are $\\alpha$-approximate kernels of\npolynomial size for three problems, namely Connected Vertex Cover, Disjoint\nCycle Packing and Disjoint Factors. These problems are known not to admit any\npolynomial size kernels unless $NP \\subseteq coNP/poly$. Our approximate\nkernels simultaneously beat both the lower bounds on the (normal) kernel size,\nand the hardness of approximation lower bounds for all three problems. On the\nnegative side we prove that Longest Path parameterized by the length of the\npath and Set Cover parameterized by the universe size do not admit even an\n$\\alpha$-approximate kernel of polynomial size, for any $\\alpha \\geq 1$, unless\n$NP \\subseteq coNP/poly$. In order to prove this lower bound we need to combine\nin a non-trivial way the techniques used for showing kernelization lower bounds\nwith the methods for showing hardness of approximation \n\n"}
{"id": "1604.06707", "contents": "Title: Loopless Gray Code Enumeration and the Tower of Bucharest Abstract: We give new algorithms for generating all n-tuples over an alphabet of m\nletters, changing only one letter at a time (Gray codes). These algorithms are\nbased on the connection with variations of the Towers of Hanoi game. Our\nalgorithms are loopless, in the sense that the next change can be determined in\na constant number of steps, and they can be implemented in hardware. We also\ngive another family of loopless algorithms that is based on the idea of working\nahead and saving the work in a buffer. \n\n"}
{"id": "1605.00058", "contents": "Title: Strongly Refuting Random CSPs Below the Spectral Threshold Abstract: Random constraint satisfaction problems (CSPs) are known to exhibit threshold\nphenomena: given a uniformly random instance of a CSP with $n$ variables and\n$m$ clauses, there is a value of $m = \\Omega(n)$ beyond which the CSP will be\nunsatisfiable with high probability. Strong refutation is the problem of\ncertifying that no variable assignment satisfies more than a constant fraction\nof clauses; this is the natural algorithmic problem in the unsatisfiable regime\n(when $m/n = \\omega(1)$).\n  Intuitively, strong refutation should become easier as the clause density\n$m/n$ grows, because the contradictions introduced by the random clauses become\nmore locally apparent. For CSPs such as $k$-SAT and $k$-XOR, there is a\nlong-standing gap between the clause density at which efficient strong\nrefutation algorithms are known, $m/n \\ge \\widetilde O(n^{k/2-1})$, and the\nclause density at which instances become unsatisfiable with high probability,\n$m/n = \\omega (1)$.\n  In this paper, we give spectral and sum-of-squares algorithms for strongly\nrefuting random $k$-XOR instances with clause density $m/n \\ge \\widetilde\nO(n^{(k/2-1)(1-\\delta)})$ in time $\\exp(\\widetilde O(n^{\\delta}))$ or in\n$\\widetilde O(n^{\\delta})$ rounds of the sum-of-squares hierarchy, for any\n$\\delta \\in [0,1)$ and any integer $k \\ge 3$. Our algorithms provide a smooth\ntransition between the clause density at which polynomial-time algorithms are\nknown at $\\delta = 0$, and brute-force refutation at the satisfiability\nthreshold when $\\delta = 1$. We also leverage our $k$-XOR results to obtain\nstrong refutation algorithms for SAT (or any other Boolean CSP) at similar\nclause densities. Our algorithms match the known sum-of-squares lower bounds\ndue to Grigoriev and Schonebeck, up to logarithmic factors.\n  Additionally, we extend our techniques to give new results for certifying\nupper bounds on the injective tensor norm of random tensors. \n\n"}
{"id": "1605.03046", "contents": "Title: A half-normal distribution scheme for generating functions and the\n  unexpected behavior of Motzkin paths Abstract: We present an extension of a theorem by Michael Drmota and Mich\\`ele Soria\n[Images and Preimages in Random Mappings, 1997] that can be used to identify\nthe limiting distribution for a class of combinatorial schemata. This is\nachieved by determining analytical and algebraic properties of the associated\nbivariate generating function. We give sufficient conditions implying a\nhalf-normal limiting distribution, extending the known conditions leading to\neither a Rayleigh, a Gaussian, or a convolution of the last two distributions.\nWe conclude with three natural appearances of such a limiting distribution in\nthe domain of Motzkin paths. \n\n"}
{"id": "1605.03613", "contents": "Title: On the Lattice Distortion Problem Abstract: We introduce and study the \\emph{Lattice Distortion Problem} (LDP). LDP asks\nhow \"similar\" two lattices are. I.e., what is the minimal distortion of a\nlinear bijection between the two lattices? LDP generalizes the Lattice\nIsomorphism Problem (the lattice analogue of Graph Isomorphism), which simply\nasks whether the minimal distortion is one.\n  As our first contribution, we show that the distortion between any two\nlattices is approximated up to a $n^{O(\\log n)}$ factor by a simple function of\ntheir successive minima. Our methods are constructive, allowing us to compute\nlow-distortion mappings that are within a $2^{O(n \\log \\log n/\\log n)}$ factor\nof optimal in polynomial time and within a $n^{O(\\log n)}$ factor of optimal in\nsingly exponential time. Our algorithms rely on a notion of basis reduction\nintroduced by Seysen (Combinatorica 1993), which we show is intimately related\nto lattice distortion. Lastly, we show that LDP is NP-hard to approximate to\nwithin any constant factor (under randomized reductions), by a reduction from\nthe Shortest Vector Problem. \n\n"}
{"id": "1605.03797", "contents": "Title: Popular Conjectures as a Barrier for Dynamic Planar Graph Algorithms Abstract: The dynamic shortest paths problem on planar graphs asks us to preprocess a\nplanar graph $G$ such that we may support insertions and deletions of edges in\n$G$ as well as distance queries between any two nodes $u,v$ subject to the\nconstraint that the graph remains planar at all times. This problem has been\nextensively studied in both the theory and experimental communities over the\npast decades and gets solved millions of times every day by companies like\nGoogle, Microsoft, and Uber. The best known algorithm performs queries and\nupdates in $\\tilde{O}(n^{2/3})$ time, based on ideas of a seminal paper by\nFakcharoenphol and Rao [FOCS'01]. A $(1+\\varepsilon)$-approximation algorithm\nof Abraham et al. [STOC'12] performs updates and queries in\n$\\tilde{O}(\\sqrt{n})$ time. An algorithm with $O(polylog(n))$ runtime would be\na major breakthrough. However, such runtimes are only known for a\n$(1+\\varepsilon)$-approximation in a model where only restricted weight updates\nare allowed due to Abraham et al. [SODA'16], or for easier problems like\nconnectivity.\n  In this paper, we follow a recent and very active line of work on showing\nlower bounds for polynomial time problems based on popular conjectures,\nobtaining the first such results for natural problems in planar graphs. Such\nresults were previously out of reach due to the highly non-planar nature of\nknown reductions and the impossibility of \"planarizing gadgets\". We introduce a\nnew framework which is inspired by techniques from the literatures on distance\nlabelling schemes and on parameterized complexity.\n  Using our framework, we show that no algorithm for dynamic shortest paths or\nmaximum weight bipartite matching in planar graphs can support both updates and\nqueries in amortized $O(n^{\\frac{1}{2}-\\varepsilon})$ time, for\n$\\varepsilon>0$, unless the classical APSP problem can be solved in truly\nsubcubic time, [...] \n\n"}
{"id": "1605.05749", "contents": "Title: IceCube PeV Neutrinos and Leptophilic Dark Matter Abstract: We analyze the scenario where the IceCube high energy neutrino events are\nexplained in terms of an extraterrestrial flux due to two different components:\na contribution coming from know astrophysical sources for energies up to few\nhundreds TeV and a top-down contribution originated by the decay of heavy dark\nmatter particles with a mass of few PeV. Contrary to previous approaches, we\nconsider a leptophilic three-body decay that dominates at PeV energies due to\nthe absence of quarks in the final state. We find that the theoretical\npredictions of such a scenario are in a slightly better agreement with the\nIceCube data if the astrophysical component has a cut-off at about 100 TeV.\nThis interpretation of IceCube data can be easily tested in the near future\nsince the decaying dark matter scenario predicts a sharp cut-off at PeV energy\nscale and the observation of an anisotropy towards Galactic Center of our\nGalaxy in contrast with the isotropic astrophysical flux. \n\n"}
{"id": "1605.06066", "contents": "Title: Evidence for intermediate polars as the origin of the Galactic Center\n  hard X-ray emission Abstract: Recently, unresolved hard (20-40 keV) X-ray emission has been discovered\nwithin the central 10 pc of the Galaxy, possibly indicating a large population\nof intermediate polars (IPs). Chandra and XMM-Newton measurements in the\nsurrounding ~50 pc imply a much lighter population of IPs with $\\langle M_{\\rm\nWD} \\rangle \\approx 0.5 M_\\odot$. Here we use broad-band NuSTAR observations of\ntwo IPs: TV Columbae, which has a fairly typical but widely varying reported\nmass of $M_{\\rm WD} \\approx 0.5-1.0 M_\\odot$, and IGR J17303-0601, with a heavy\nreported mass of $M_{\\rm WD} \\approx 1.0-1.2 M_\\odot$. We investigate how\nvarying spectral models and observed energy ranges influence estimated white\ndwarf mass. Observations of the inner 10 pc can be accounted for by IPs with\n$\\langle M_{\\rm WD} \\rangle \\approx 0.9 M_\\odot$, consistent with that of the\nCV population in general, and the X-ray observed field IPs in particular. The\nlower mass derived by Chandra and XMM-Newton appears to be an artifact of\nnarrow energy band fitting. To explain the (unresolved) CHXE by IPs requires an\nX-ray (2-8 keV) luminosity function (XLF) extending down to at least\n$5\\times10^{31}$ erg/s. The CHXE XLF, if extended to the surrounding ~50 pc\nobserved by Chandra and XMM-Newton, requires at least ~20-40% of the $\\sim$9000\npoint sources are IPs. If the XLF extends just a factor of a few lower in\nluminosity, then the vast majority of these sources are IPs. This is in\ncontrast to recent observations of the Galactic ridge, where the bulk of the\n2-8 keV emission is ascribed to dwarf novae. \n\n"}
{"id": "1605.06950", "contents": "Title: A Sub-Quadratic Exact Medoid Algorithm Abstract: We present a new algorithm, trimed, for obtaining the medoid of a set, that\nis the element of the set which minimises the mean distance to all other\nelements. The algorithm is shown to have, under certain assumptions, expected\nrun time O(N^(3/2)) in R^d where N is the set size, making it the first\nsub-quadratic exact medoid algorithm for d>1. Experiments show that it performs\nvery well on spatial network data, frequently requiring two orders of magnitude\nfewer distance calculations than state-of-the-art approximate algorithms. As an\napplication, we show how trimed can be used as a component in an accelerated\nK-medoids algorithm, and then how it can be relaxed to obtain further\ncomputational gains with only a minor loss in cluster quality. \n\n"}
{"id": "1605.07245", "contents": "Title: The Intermediate Luminosity Optical Transient SN 2010da: The Progenitor,\n  Eruption and Aftermath of a Peculiar Supergiant High-mass X-ray Binary Abstract: We present optical spectroscopy, ultraviolet to infrared imaging and X-ray\nobservations of the intermediate luminosity optical transient (ILOT) SN 2010da\nin NGC 300 (d=1.86 Mpc) spanning from -6 to +6 years relative to the time of\noutburst in 2010. Based on the light curve and multi-epoch SEDs of SN 2010da,\nwe conclude that the progenitor of SN 2010da is a ~10-12 Msol yellow supergiant\npossibly transitioning into a blue loop phase. During outburst, SN 2010da had a\npeak absolute magnitude of M<-10.4 mag, dimmer than other ILOTs and supernova\nimpostors. We detect multi-component hydrogen Balmer, Paschen, and Ca II\nemission lines in our high-resolution spectra, which indicate a dusty and\ncomplex circumstellar environment. Since the 2010 eruption, the star has\nbrightened by a factor of ~5 and remains highly variable in the optical.\nFurthermore, we detect SN 2010da in archival Swift and Chandra observations as\nan ultraluminous X-ray source (L~6x10^{39} erg/s). We additionally attribute He\nII 4686 Angstrom and coronal Fe emission lines in addition to a steady X-ray\nluminosity of ~10^{37} erg/s to the presence of a compact companion. \n\n"}
{"id": "1605.07271", "contents": "Title: Constraining the Age and Distance of the Galactic Supernova Remnant\n  G156.2+5.7 by H-alpha Expansion Measurements Abstract: We present deep H-alpha images of portions of the X-ray bright but optically\nfaint Galactic supernova remnant G156.2+5.7, revealing numerous and delicately\nthin nonradiative filaments which mark the location of the remnant's forward\nshock. These new images show that these filaments have a complex structure not\nvisible on previous lower resolution optical images. By comparing H-alpha\nimages taken in 2004 at the McDonald Observatory and in 2015-2016 at the Kiso\nObservatory, we set a stringent 1-sigma upper limit of expansion to be 0.06\narcsec/yr. This proper motion, combined with a shock speed of 500 km/s inferred\nfrom X-ray spectral analyses, gives a distance of > 1.7 kpc. In addition, a\nsimple comparison of expansion indices of several SNRs allows us to infer the\nage of the remnant to be a few 10,000 yr old. These estimates are more\nstraightforward and reliable than any other previous studies, and clearly rule\nout a possibility that G156.2+5.7 is physically associated with part of the\nTaurus-Auriga cloud and dust complex at a distance of 200-300 pc. \n\n"}
{"id": "1605.07301", "contents": "Title: The Optical Variability of SDSS Quasars from Multi-epoch Spectroscopy.\n  III. A Sudden UV Cutoff in Quasar SDSS J2317+0005 Abstract: We have collected near-infrared to X-ray data of 20 multi-epoch heavily\nreddened SDSS quasars to investigate the physical mechanism of reddening. Of\nthese, J2317+0005 is found to be a UV cutoff quasar. Its continuum, which\nusually appears normal, decreases by a factor 3.5 at 3000{\\AA}, compared to its\nmore typical bright state during an interval of 23 days. During this sudden\ncontinuum cut-off, the broad emission line fluxes do not change, perhaps due to\nthe large size of the Broad Line Region (BLR), r > 23 / (1+z) days. The UV\ncontinuum may have suffered a dramatic drop out. However, there are some\ndifficulties with this explanation. Another possibility is that the intrinsic\ncontinuum did not change, but was temporarily blocked out, at least towards our\nline of sight. As indicated by X-ray observations, the continuum rapidly\nrecovers after 42 days. A comparison of the bright state and dim states would\nimply an eclipse by a dusty cloud with a reddening curve having a remarkably\nsharp rise shortward of 3500{\\AA}. Under the assumption of being eclipsed by a\nKeplerian dusty cloud, we characterized the cloud size with our observations,\nhowever, which is a little smaller than the 3000\\AA\\ continuum-emitting size\ninferred from accretion disk models. Therefore, we speculate this is due to a\nrapid outflow or inflow with a dusty cloud passing through our line-of-sight to\nthe center. \n\n"}
{"id": "1605.07503", "contents": "Title: A novel algorithm for solving the Decision Boolean Satisfiability\n  Problem without algebra Abstract: This paper depicts an algorithm for solving the Decision Boolean\nSatisfiability Problem using the binary numerical properties of a Special\nDecision Satisfiability Problem, parallel execution, object oriented, and short\ntermination. The two operations: expansion and simplification are used to\nexplains why using algebra grows the resolution steps. It is proved that its\ncomplexity has an upper bound of $2^{n-1}$ where $n$ is the number of logical\nvariables of the given problem. \n\n"}
{"id": "1605.08540", "contents": "Title: Induced Minor Free Graphs: Isomorphism and Clique-width Abstract: Given two graphs $G$ and $H$, we say that $G$ contains $H$ as an induced\nminor if a graph isomorphic to $H$ can be obtained from $G$ by a sequence of\nvertex deletions and edge contractions. We study the complexity of Graph\nIsomorphism on graphs that exclude a fixed graph as an induced minor. More\nprecisely, we determine for every graph $H$ that Graph Isomorphism is\npolynomial-time solvable on $H$-induced-minor-free graphs or that it is\nGI-complete. Additionally, we classify those graphs $H$ for which\n$H$-induced-minor-free graphs have bounded clique-width. These two results\ncomplement similar dichotomies for graphs that exclude a fixed graph as an\ninduced subgraph, minor, or subgraph. \n\n"}
{"id": "1605.08738", "contents": "Title: Parameterized Resiliency Problems via Integer Linear Programming Abstract: We introduce an extension of decision problems called resiliency problems. In\nresiliency problems, the goal is to decide whether an instance remains positive\nafter any (appropriately defined) perturbation has been applied to it. To\ntackle these kinds of problems, some of which might be of practical interest,\nwe introduce a notion of resiliency for Integer Linear Programs (ILP) and show\nhow to use a result of Eisenbrand and Shmonin (Math. Oper. Res., 2008) on\nParametric Linear Programming to prove that ILP Resiliency is fixed-parameter\ntractable (FPT) under a certain parameterization. To demonstrate the utility of\nour result, we consider natural resiliency versions of several concrete\nproblems, and prove that they are FPT under natural parameterizations. Our\nfirst results concern a four-variate problem which generalizes the Disjoint Set\nCover problem and which is of interest in access control. We obtain a complete\nparameterized complexity classification for every possible combination of the\nparameters. Then, we introduce and study a resiliency version of the Closest\nString problem, for which we extend an FPT result of Gramm et al.\n(Algorithmica, 2003). We also consider problems in the fields of scheduling and\nsocial choice. We believe that many other problems can be tackled by our\nframework. \n\n"}
{"id": "1605.09558", "contents": "Title: Dynamic index and LZ factorization in compressed space Abstract: In this paper, we propose a new \\emph{dynamic compressed index} of $O(w)$\nspace for a dynamic text $T$, where $w = O(\\min(z \\log N \\log^*M, N))$ is the\nsize of the signature encoding of $T$, $z$ is the size of the Lempel-Ziv77\n(LZ77) factorization of $T$, $N$ is the length of $T$, and $M \\geq 3N$ is an\ninteger that can be handled in constant time under word RAM model. Our index\nsupports searching for a pattern $P$ in $T$ in $O(|P| f_{\\mathcal{A}} + \\log w\n\\log |P| \\log^* M (\\log N + \\log |P| \\log^* M) + \\mathit{occ} \\log N)$ time and\ninsertion/deletion of a substring of length $y$ in $O((y+ \\log N\\log^* M)\\log w\n\\log N \\log^* M)$ time, where $f_{\\mathcal{A}} = O(\\min \\{ \\frac{\\log\\log M\n\\log\\log w}{\\log\\log\\log M}, \\sqrt{\\frac{\\log w}{\\log\\log w}} \\})$. Also, we\npropose a new space-efficient LZ77 factorization algorithm for a given text of\nlength $N$, which runs in $O(N f_{\\mathcal{A}} + z \\log w \\log^3 N (\\log^*\nN)^2)$ time with $O(w)$ working space. \n\n"}
{"id": "1606.00314", "contents": "Title: On the GBM event seen 0.4 sec after GW 150914 Abstract: In view of the recent report by Connaughton we analyse continuous TTE data of\nFermi-GBM around the time of the gravitational wave event GW 150914. We find\nthat after proper accounting for low count statistics, the GBM transient event\nat 0.4 s after GW 150914 is likely not due to an astrophysical source, but\nconsistent with a background fluctuation, removing the tension between the\nINTEGRAL/ACS non-detection and GBM. Additionally, reanalysis of other short\nGRBs shows that without proper statistical modeling the fluence of faint events\nis over-predicted, as verified for some joint GBM-ACS detections of short GRBs.\nWe detail the statistical procedure to correct these biases. As a result, faint\nshort GRBs, verified by ACS detections, with significances in the broad-band\nlight curve even smaller than that of the GBM-GW150914 event are recovered as\nproper non-zero source, while the GBM-GW150914 event is consistent with zero\nfluence. \n\n"}
{"id": "1606.03168", "contents": "Title: Finding Low-Rank Solutions via Non-Convex Matrix Factorization,\n  Efficiently and Provably Abstract: A rank-$r$ matrix $X \\in \\mathbb{R}^{m \\times n}$ can be written as a product\n$U V^\\top$, where $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n\n\\times r}$. One could exploit this observation in optimization: e.g., consider\nthe minimization of a convex function $f(X)$ over rank-$r$ matrices, where the\nset of rank-$r$ matrices is modeled via the factorization $UV^\\top$. Though\nsuch parameterization reduces the number of variables, and is more\ncomputationally efficient (of particular interest is the case $r \\ll \\min\\{m,\nn\\}$), it comes at a cost: $f(UV^\\top)$ becomes a non-convex function w.r.t.\n$U$ and $V$.\n  We study such parameterization for optimization of generic convex objectives\n$f$, and focus on first-order, gradient descent algorithmic solutions. We\npropose the Bi-Factored Gradient Descent (BFGD) algorithm, an efficient\nfirst-order method that operates on the $U, V$ factors. We show that when $f$\nis (restricted) smooth, BFGD has local sublinear convergence, and linear\nconvergence when $f$ is both (restricted) smooth and (restricted) strongly\nconvex. For several key applications, we provide simple and efficient\ninitialization schemes that provide approximate solutions good enough for the\nabove convergence results to hold. \n\n"}
{"id": "1606.05117", "contents": "Title: Recycled Pulsars: Spins, Masses and Ages Abstract: Recycled pulsars are mainly characterized by their spin periods, B-fields and\nmasses. All these quantities are affected by previous interactions with a\ncompanion star in a binary system. Therefore, we can use these quantities as\nfossil records and learn about binary evolution. Here, I briefly review the\ndistribution of these observed quantities and summarize our current\nunderstanding of the pulsar recycling process. \n\n"}
{"id": "1606.06399", "contents": "Title: Uniqueness Trees: A Possible Polynomial Approach to the Graph\n  Isomorphism Problem Abstract: This paper presents the novel `uniqueness tree' algorithm, as one possible\nmethod for determining whether two finite, undirected graphs are isomorphic. We\nprove that the algorithm has polynomial time complexity in the worst case, and\nthat it will always detect the presence of an isomorphism whenever one exists.\nWe also propose that the algorithm will equivalently discern the lack of an\nisomorphism whenever one does not exist, and some initial justifications are\ngiven for this proposition, although it cannot yet be rigorously proven.\nFinally, we present experimental evidence for both the effectiveness and\nefficiency of the uniqueness tree method, using data gathered from a practical\nimplementation of the algorithm. Some consequences and directions for further\nresearch are discussed. \n\n"}
{"id": "1606.08107", "contents": "Title: Consistent analytic approach to the efficiency of collisional Penrose\n  process Abstract: We propose a consistent analytic approach to the efficiency of collisional\nPenrose process in the vicinity of a maximally rotating Kerr black hole. We\nfocus on a collision with arbitrarily high center-of-mass energy, which occurs\nif either of the colliding particles has its angular momentum fine-tuned to the\ncritical value to enter the horizon. We show that if the fine-tuned particle is\ningoing on the collision, the upper limit of the efficiency is\n$(2+\\sqrt{3})(2-\\sqrt{2})\\simeq 2.186$, while if the fine-tuned particle is\nbounced back before the collision, the upper limit is $(2+\\sqrt{3})^{2}\\simeq\n13.93$. Despite earlier claims, the former can be attained for inverse Compton\nscattering if the fine-tuned particle is massive and starts at rest at\ninfinity, while the latter can be attained for various particle reactions, such\nas inverse Compton scattering and pair annihilation, if the fine-tuned particle\nis either massless or highly relativistic at infinity. We discuss the\ndifference between the present and earlier analyses. \n\n"}
{"id": "1606.08275", "contents": "Title: Near-Optimal Computation of Runs over General Alphabet via Non-Crossing\n  LCE Queries Abstract: Longest common extension queries (LCE queries) and runs are ubiquitous in\nalgorithmic stringology. Linear-time algorithms computing runs and\npreprocessing for constant-time LCE queries have been known for over a decade.\nHowever, these algorithms assume a linearly-sortable integer alphabet. A recent\nbreakthrough paper by Bannai et.\\ al.\\ (SODA 2015) showed a link between the\ntwo notions: all the runs in a string can be computed via a linear number of\nLCE queries. The first to consider these problems over a general ordered\nalphabet was Kosolobov (\\emph{Inf.\\ Process.\\ Lett.}, 2016), who presented an\n$O(n (\\log n)^{2/3})$-time algorithm for answering $O(n)$ LCE queries. This\nresult was improved by Gawrychowski et.\\ al.\\ (accepted to CPM 2016) to $O(n\n\\log \\log n)$ time. In this work we note a special \\emph{non-crossing} property\nof LCE queries asked in the runs computation. We show that any $n$ such\nnon-crossing queries can be answered on-line in $O(n \\alpha(n))$ time, which\nyields an $O(n \\alpha(n))$-time algorithm for computing runs. \n\n"}
{"id": "1606.09481", "contents": "Title: Generating massive complex networks with hyperbolic geometry faster in\n  practice Abstract: Generative network models play an important role in algorithm development,\nscaling studies, network analysis, and realistic system benchmarks for graph\ndata sets. The commonly used graph-based benchmark model R-MAT has some\ndrawbacks concerning realism and the scaling behavior of network properties. A\ncomplex network model gaining considerable popularity builds random hyperbolic\ngraphs, generated by distributing points within a disk in the hyperbolic plane\nand then adding edges between points whose hyperbolic distance is below a\nthreshold.\n  We present in this paper a fast generation algorithm for such graphs. Our\nexperiments show that our new generator achieves speedup factors of 3-60 over\nthe best previous implementation. One billion edges can now be generated in\nunder one minute on a shared-memory workstation. Furthermore, we present a\ndynamic extension to model gradual network change, while preserving at each\nstep the point position probabilities. \n\n"}
{"id": "1606.09536", "contents": "Title: Perspectives on Gamma-Ray Burst Physics and Cosmology with Next\n  Generation Facilities Abstract: High-redshift Gamma-Ray Bursts (GRBs) beyond redshift $\\sim6$ are potentially\npowerful tools to probe the distant early Universe. Their detections in large\nnumbers and at truly high redshifts call for the next generation of high-energy\nwide-field instruments with unprecedented sensitivity at least one order of\nmagnitude higher than the ones currently in orbit. On the other hand, follow-up\nobservations of the afterglows of high-redshift GRBs and identification of\ntheir host galaxies, which would be difficult for the currently operating\ntelescopes, require new, extremely large facilities of at multi-wavelengths.\nThis chapter describes future experiments that are expected to advance this\nexciting field, both being currently built and being proposed. The legacy of\nSwift will be continued by SVOM, which is equipped with a set of space-based\nmulti-wavelength instruments as well as and a ground segment including a wide\nangle camera and two follow-up telescopes. The established Lobster-eye X-ray\nfocusing optics provides a promising technology for the detection of faint GRBs\nat very large distances, based on which the {THESEUS}, {Einstein Probe} and\nother mission concepts have been proposed. Follow-up observations and\nexploration of the reionization era will be enabled by large facilities such as\n{SKA} in the radio, the 30m class telescopes in the optical/near-IR, and the\nspace-borne {WFIRST} and {JWST} in the optical/near-IR/mid-IR. In addition, the\nX-ray and $\\gamma$-ray polarization experiment POLAR is also introduced. \n\n"}
{"id": "1607.00651", "contents": "Title: Maximum efficiency of the collisional Penrose process Abstract: We consider collision of two particles that move in the equatorial plane near\na general stationary rotating axially symmetric extremal black hole. One of\nparticles is critical (with fine-tuned parameters) and moves in the outward\ndirection. The second particle (usual, not fine-tuned) comes from infinity. We\nexamine the efficiency $\\eta $ of the collisional Penrose process. There are\ntwo relevant cases here: (i) a particle falling into a black hole after\ncollision is heavy, (ii) it has a finite mass. We show that the maximum of\n$\\eta $ in case (ii) is less or equal to that in case (i). It is argued that\nfor superheavy particles, the bound applies to nonequatorial motion as well. As\nan example, we analyze collision in the Kerr-Newman background. When the bound\nis the same for processes (i) and (ii), $\\eta =3$ for this metric. For the Kerr\nblack hole, recent results in literature are reproduced. \n\n"}
{"id": "1607.01162", "contents": "Title: Unit Interval Vertex Deletion: Fewer Vertices are Relevant Abstract: The unit interval vertex deletion problem asks for a set of at most $k$\nvertices whose deletion from an $n$-vertex graph makes it a unit interval\ngraph. We develop an $O(k^4)$-vertex kernel for the problem, significantly\nimproving the $O(k^{53})$-vertex kernel of Fomin, Saurabh, and Villanger\n[ESA'12; SIAM J. Discrete Math 27(2013)]. We introduce a novel way of\norganizing cliques of a unit interval graph. Our constructive proof for the\ncorrectness of our algorithm, using interval models, greatly simplifies the\ndestructive proofs, based on forbidden induced subgraphs, for similar problems\nin literature. \n\n"}
{"id": "1607.01229", "contents": "Title: Improved Lower Bounds for Online Hypercube and Rectangle Packing Abstract: Packing a given sequence of items into as few bins as possible in an online\nfashion is a widely studied problem. We improve lower bounds for packing boxes\ninto bins in two or more dimensions, both for general algorithms for squares\nand rectangles (in two dimensions) and for an important subclass, so-called\nHarmonic-type algorithms for hypercubes (in two or more dimensions). Lastly, we\nshow that two adaptions of ideas from a one-dimensional packing algorithm to\nsquare packing do not help to break the barrier of 2. \n\n"}
{"id": "1607.02047", "contents": "Title: A no-go theorem for the dark matter interpretation of the positron\n  anomaly Abstract: The overabundance of high-energy cosmic positrons, observed by PAMELA and\nAMS-02, can be considered as the consequence of dark matter decays or\nannihilations. We show that recent FERMI/LAT measurements of the isotropic\ndiffuse gamma-ray background impose severe constraints on dark matter\nexplanations and make them practically inconsistent. \n\n"}
{"id": "1607.03069", "contents": "Title: SNR 1E 0102.2-7219 as an X-ray Calibration Standard in the 0.5-1.0 keV\n  Bandpass and Its Application to the CCD Instruments aboard Chandra, Suzaku,\n  Swift and XMM-Newton Abstract: We desire a simple comparison of the absolute effective areas of the current\ngeneration of CCD instruments onboard the following observatories: Chandra\nACIS-S3, XMM-Newton (EPIC-MOS and EPIC-pn), Suzaku XIS, and Swift XRT and a\nstraightforward comparison of the time-dependent response of these instruments\nacross their respective mission lifetimes. We have been using 1E 0102.2-7219,\nthe brightest supernova remnant in the Small Magellanic Cloud, to evaluate and\nmodify the response models of these instruments. 1E 0102.2-7219 has strong\nlines of O, Ne, and Mg below 1.5 keV and little or no Fe emission to complicate\nthe spectrum. As part of the activities of the International Astronomical\nConsortium for High Energy Calibration (IACHEC), we have developed a standard\nspectral model for 1E 0102.2-7219. The model is empirical in that it includes\nGaussians for the identified lines, an absorption component in the Galaxy,\nanother absorption component in the SMC, and two thermal continuum components.\nIn our fits, the model is highly constrained in that only the normalizations of\nthe four brightest lines/line complexes (the O vii He$\\alpha$ triplet, O viii\nLy$\\alpha$ line, the Ne ix He$\\alpha$ triplet, and the Ne x Ly$\\alpha$ line)\nand an overall normalization are allowed to vary. We have examined these\nmeasured line fluxes as a function of time for each instrument after applying\nthe most recent calibrations that account for the time-dependent response of\neach instrument. We perform our effective area comparison with representative,\nearly mission data when the radiation damage and contamination layers were at a\nminimum. We find that the measured fluxes of these lines generally agree to\nwithin +/-10% for all instruments, with 38 of our 48 fitted normalizations\nwithin +/-10% of the IACHEC model value. \n\n"}
{"id": "1607.03416", "contents": "Title: Dependence of X-Ray Burst Models on Nuclear Reaction Rates Abstract: X-ray bursts are thermonuclear flashes on the surface of accreting neutron\nstars and reliable burst models are needed to interpret observations in terms\nof properties of the neutron star and the binary system. We investigate the\ndependence of X-ray burst models on uncertainties in (p,$\\gamma$),\n($\\alpha$,$\\gamma$), and ($\\alpha$,p) nuclear reaction rates using fully\nself-consistent burst models that account for the feedbacks between changes in\nnuclear energy generation and changes in astrophysical conditions. A two-step\napproach first identified sensitive nuclear reaction rates in a single-zone\nmodel with ignition conditions chosen to match calculations with a\nstate-of-the-art 1D multi-zone model based on the {\\Kepler} stellar evolution\ncode. All relevant reaction rates on neutron deficient isotopes up to mass 106\nwere individually varied by a factor of 100 up and down. Calculations of the 84\nhighest impact reaction rate changes were then repeated in the 1D multi-zone\nmodel. We find a number of uncertain reaction rates that affect predictions of\nlight curves and burst ashes significantly. The results provide insights into\nthe nuclear processes that shape X-ray burst observables and guidance for\nfuture nuclear physics work to reduce nuclear uncertainties in X-ray burst\nmodels. \n\n"}
{"id": "1607.03600", "contents": "Title: The Elekes-Szab\\'o Theorem in four dimensions Abstract: Let $F\\in\\mathbb{C}[x,y,s,t]$ be an irreducible constant-degree polynomial,\nand let $A,B,C,D\\subset\\mathbb{C}$ be finite sets of size $n$. We show that $F$\nvanishes on at most $O(n^{8/3})$ points of the Cartesian product $A\\times\nB\\times C\\times D$, unless $F$ has a special group-related form. A similar\nstatement holds for $A,B,C,D$ of unequal sizes. This is a four-dimensional\nextension of our recent improved analysis of the original Elekes-Szab\\'o\ntheorem in three dimensions. We give three applications: an expansion bound for\nthree-variable real polynomials that do not have a special form, a bound on the\nnumber of coplanar quadruples on a space curve that is neither planar nor\nquartic, and a bound on the number of four-point circles on a plane curve that\nhas degree at least five. \n\n"}
{"id": "1607.03718", "contents": "Title: Streaming Algorithms For Computing Edit Distance Without Exploiting\n  Suffix Trees Abstract: The edit distance is a way of quantifying how similar two strings are to one\nanother by counting the minimum number of character insertions, deletions, and\nsubstitutions required to transform one string into the other.\n  In this paper we study the computational problem of computing the edit\ndistance between a pair of strings where their distance is bounded by a\nparameter $k\\ll n$. We present two streaming algorithms for computing edit\ndistance: One runs in time $O(n+k^2)$ and the other $n+O(k^3)$. By writing\n$n+O(k^3)$ we want to emphasize that the number of operations per an input\nsymbol is a small constant. In particular, the running time does not depend on\nthe alphabet size, and the algorithm should be easy to implement.\n  Previously a streaming algorithm with running time $O(n+k^4)$ was given in\nthe paper by the current authors (STOC'16). The best off-line algorithm runs in\ntime $O(n+k^2)$ (Landau et al., 1998) which is known to be optimal under the\nStrong Exponential Time Hypothesis. \n\n"}
{"id": "1607.03933", "contents": "Title: Luminosity dependence of the cyclotron line and evidence for the\n  accretion regime transition in V 0332+53 Abstract: We report on the analysis of NuSTAR observations of the Be-transient X-ray\npulsar V 0332+53 during the giant outburst in 2015 and another minor outburst\nin 2016. We confirm the cyclotron-line energy-luminosity correlation previously\nreported in the source and the line energy decrease during the giant outburst.\nBased on 2016 observations, we find that a year later the line energy has\nincreased again essentially reaching the pre-outburst values. We discuss this\nbehaviour and conclude that it is likely caused by a change of the emission\nregion geometry rather than previously suggested accretion-induced decay of the\nneutron stars magnetic field. At lower luminosities, we find for the first time\na hint of departure from the anticorrelation of line energy with flux, which we\ninterpret as a transition from super- to sub-critical accretion associated with\nthe disappearance of the accretion column. Finally, we confirm and briefly\ndiscuss the orbital modulation observed in the outburst light curve of the\nsource. \n\n"}
{"id": "1607.03961", "contents": "Title: Deleting and Testing Forbidden Patterns in Multi-Dimensional Arrays Abstract: Understanding the local behaviour of structured multi-dimensional data is a\nfundamental problem in various areas of computer science. As the amount of data\nis often huge, it is desirable to obtain sublinear time algorithms, and\nspecifically property testers, to understand local properties of the data.\n  We focus on the natural local problem of testing pattern freeness: given a\nlarge $d$-dimensional array $A$ and a fixed $d$-dimensional pattern $P$ over a\nfinite alphabet, we say that $A$ is $P$-free if it does not contain a copy of\nthe forbidden pattern $P$ as a consecutive subarray. The distance of $A$ to\n$P$-freeness is the fraction of entries of $A$ that need to be modified to make\nit $P$-free. For any $\\epsilon \\in [0,1]$ and any large enough pattern $P$ over\nany alphabet, other than a very small set of exceptional patterns, we design a\ntolerant tester that distinguishes between the case that the distance is at\nleast $\\epsilon$ and the case that it is at most $a_d \\epsilon$, with query\ncomplexity and running time $c_d \\epsilon^{-1}$, where $a_d < 1$ and $c_d$\ndepend only on $d$.\n  To analyze the testers we establish several combinatorial results, including\nthe following $d$-dimensional modification lemma, which might be of independent\ninterest: for any large enough pattern $P$ over any alphabet (excluding a small\nset of exceptional patterns for the binary case), and any array $A$ containing\na copy of $P$, one can delete this copy by modifying one of its locations\nwithout creating new $P$-copies in $A$.\n  Our results address an open question of Fischer and Newman, who asked whether\nthere exist efficient testers for properties related to tight substructures in\nmulti-dimensional structured data. They serve as a first step towards a general\nunderstanding of local properties of multi-dimensional arrays, as any such\nproperty can be characterized by a fixed family of forbidden patterns. \n\n"}
{"id": "1607.04277", "contents": "Title: The imprint of pulsar parameters on the morphology of Pulsar Wind\n  Nebulae Abstract: The morphology of young Pulsar Wind Nebulae (PWN) is largely determined by\nthe properties of the wind injected by the pulsar. We have used a recent\nparametrization of the wind obtained from Force Free Electrodynamics\nsimulations of pulsar magnetospheres to simulate nebulae for different sets of\npulsar parameters. We performed axisymmetric Relativistic Magnetohydrodynamics\nsimulations to test the morphology dependence of the nebula on the obliquity of\nthe pulsar and on the magnetization of the pulsar wind. We compare these\nsimulations to the morphology of the Vela and Crab PWN. We find that the\nmorphology of Vela can be reproduced qualitatively if the pulsar obliquity\nangle is alpha ~45deg and the magnetization of the wind is high (sigma_0 ~\n3.0). A morphology similar to the one of the Crab Nebula is only obtained for\nlow magnetization simulations with alpha >~ 45deg. Interestingly, we find that\nKelvin-Helmholtz instabilities produce small scale turbulences downstream of\nthe reverse shock of the pulsar wind. \n\n"}
{"id": "1607.04787", "contents": "Title: Robust algorithms with polynomial loss for near-unanimity CSPs Abstract: An instance of the Constraint Satisfaction Problem (CSP) is given by a family\nof constraints on overlapping sets of variables, and the goal is to assign\nvalues from a fixed domain to the variables so that all constraints are\nsatisfied. In the optimization version, the goal is to maximize the number of\nsatisfied constraints. An approximation algorithm for CSP is called robust if\nit outputs an assignment satisfying a $(1-g(\\varepsilon))$-fraction of\nconstraints on any $(1-\\varepsilon)$-satisfiable instance, where the loss\nfunction $g$ is such that $g(\\varepsilon)\\rightarrow 0$ as\n$\\varepsilon\\rightarrow 0$.\n  We study how the robust approximability of CSPs depends on the set of\nconstraint relations allowed in instances, the so-called constraint language.\nAll constraint languages admitting a robust polynomial-time algorithm (with\nsome $g$) have been characterised by Barto and Kozik, with the general bound on\nthe loss $g$ being doubly exponential, specifically\n$g(\\varepsilon)=O((\\log\\log(1/\\varepsilon))/\\log(1/\\varepsilon))$. It is\nnatural to ask when a better loss can be achieved: in particular, polynomial\nloss $g(\\varepsilon)=O(\\varepsilon^{1/k})$ for some constant $k$. In this\npaper, we consider CSPs with a constraint language having a near-unanimity\npolymorphism. We give two randomized robust algorithms with polynomial loss for\nsuch CSPs: one works for any near-unanimity polymorphism and the parameter $k$\nin the loss depends on the size of the domain and the arity of the relations in\n$\\Gamma$, while the other works for a special ternary near-unanimity operation\ncalled dual discriminator with $k=2$ for any domain size. In the latter case,\nthe CSP is a common generalisation of Unique Games with a fixed domain and\n2-SAT. In the former case, we use the algebraic approach to the CSP. Both cases\nuse the standard semidefinite programming relaxation for CSP. \n\n"}
{"id": "1607.05189", "contents": "Title: On the Sensitivity Conjecture for Disjunctive Normal Forms Abstract: The sensitivity conjecture of Nisan and Szegedy [CC '94] asks whether for any\nBoolean function $f$, the maximum sensitivity $s(f)$, is polynomially related\nto its block sensitivity $bs(f)$, and hence to other major complexity measures.\nDespite major advances in the analysis of Boolean functions over the last\ndecade, the problem remains widely open.\n  In this paper, we consider a restriction on the class of Boolean functions\nthrough a model of computation (DNF), and refer to the functions adhering to\nthis restriction as admitting the Normalized Block property. We prove that for\nany function $f$ admitting the Normalized Block property, $bs(f) \\leq 4s(f)^2$.\nWe note that (almost) all the functions mentioned in literature that achieve a\nquadratic separation between sensitivity and block sensitivity admit the\nNormalized Block property.\n  Recently, Gopalan et al. [ITCS '16] showed that every Boolean function $f$ is\nuniquely specified by its values on a Hamming ball of radius at most $2s(f)$.\nWe extend this result and also construct examples of Boolean functions which\nprovide the matching lower bounds. \n\n"}
{"id": "1607.05527", "contents": "Title: An Approximation Algorithm for the Art Gallery Problem Abstract: Given a simple polygon $\\mathcal{P}$ on $n$ vertices, two points $x,y$ in\n$\\mathcal{P}$ are said to be visible to each other if the line segment between\n$x$ and $y$ is contained in $\\mathcal{P}$. The Point Guard Art Gallery problem\nasks for a minimum set $S$ such that every point in $\\mathcal{P}$ is visible\nfrom a point in $S$. The set $S$ is referred to as guards. Assuming integer\ncoordinates and a specific general position assumption, we present the first\n$O(\\log \\text{OPT})$-approximation algorithm for the point guard problem for\nsimple polygons. This algorithm combines ideas of a paper of Efrat and\nHar-Peled [Inf. Process. Lett. 2006] and Deshpande et. al. [WADS 2007]. We also\npoint out a mistake in the latter. \n\n"}
{"id": "1607.05597", "contents": "Title: Distributed Construction of Purely Additive Spanners Abstract: This paper studies the complexity of distributed construction of purely\nadditive spanners in the CONGEST model. We describe algorithms for building\nsuch spanners in several cases. Because of the need to simultaneously make\ndecisions at far apart locations, the algorithms use additional mechanisms\ncompared to their sequential counterparts.\n  We complement our algorithms with a lower bound on the number of rounds\nrequired for computing pairwise spanners. The standard reductions from\nset-disjointness and equality seem unsuitable for this task because no specific\nedge needs to be removed from the graph. Instead, to obtain our lower bound, we\ndefine a new communication complexity problem that reduces to computing a\nsparse spanner, and prove a lower bound on its communication complexity using\ninformation theory. This technique significantly extends the current toolbox\nused for obtaining lower bounds for the CONGEST model, and we believe it may\nfind additional applications. \n\n"}
{"id": "1607.06141", "contents": "Title: Strong Hardness of Privacy from Weak Traitor Tracing Abstract: Despite much study, the computational complexity of differential privacy\nremains poorly understood. In this paper we consider the computational\ncomplexity of accurately answering a family $Q$ of statistical queries over a\ndata universe $X$ under differential privacy. A statistical query on a dataset\n$D \\in X^n$ asks \"what fraction of the elements of $D$ satisfy a given\npredicate $p$ on $X$?\" Dwork et al. (STOC'09) and Boneh and Zhandry (CRYPTO'14)\nshowed that if both $Q$ and $X$ are of polynomial size, then there is an\nefficient differentially private algorithm that accurately answers all the\nqueries, and if both $Q$ and $X$ are exponential size, then under a plausible\nassumption, no efficient algorithm exists.\n  We show that, under the same assumption, if either the number of queries or\nthe data universe is of exponential size, and the other has size at least\n$\\tilde{O}(n^7)$, then there is no differentially private algorithm that\nanswers all the queries. In both cases, the result is nearly quantitatively\ntight, since there is an efficient differentially private algorithm that\nanswers $\\tilde{\\Omega}(n^2)$ queries on an exponential size data universe, and\none that answers exponentially many queries on a data universe of size\n$\\tilde{\\Omega}(n^2)$.\n  Our proofs build on the connection between hardness results in differential\nprivacy and traitor-tracing schemes (Dwork et al., STOC'09; Ullman, STOC'13).\nWe prove our hardness result for a polynomial size query set (resp., data\nuniverse) by showing that they follow from the existence of a special type of\ntraitor-tracing scheme with very short ciphertexts (resp., secret keys), but\nvery weak security guarantees, and then constructing such a scheme. \n\n"}
{"id": "1607.06660", "contents": "Title: Fast Longest Common Extensions in Small Space Abstract: In this paper we address the longest common extension (LCE) problem: to\ncompute the length $\\ell$ of the longest common prefix between any two suffixes\nof $T\\in \\Sigma^n$ with $ \\Sigma = \\{0, \\ldots \\sigma-1\\} $. We present two\nfast and space-efficient solutions based on (Karp-Rabin)\n\\textit{fingerprinting} and \\textit{sampling}. Our first data structure\nexploits properties of Mersenne prime numbers when used as moduli of the\nKarp-Rabin hash function and takes $n\\lceil \\log_2\\sigma\\rceil$ bits of space.\nOur second structure works with any prime modulus and takes $n\\lceil\n\\log_2\\sigma\\rceil + n/w + w\\log_2 n$ bits of space ($ w $ memory-word size).\nBoth structures support $\\mathcal O\\left(m\\log\\sigma/w \\right)$-time extraction\nof any length-$m$ text substring, $\\mathcal O(\\log\\ell)$-time LCE queries with\nhigh probability, and can be built in optimal $\\mathcal O(n)$ time. In the\nfirst case, ours is the first result showing that it is possible to answer LCE\nqueries in $o(n)$ time while using only $\\mathcal O(1)$ words on top of the\nspace required to store the text. Our results improve the state of the art in\nspace usage, query times, and preprocessing times and are extremely practical:\nwe present a C++ implementation that is very fast and space-efficient in\npractice. \n\n"}
{"id": "1607.06669", "contents": "Title: Multiwavelength and parsec-scale properties of extragalactic jets Abstract: Extragalactic jets originating from the central supermassive black holes of\nactive galaxies are powerful, highly relativistic plasma outflows, emitting\nlight from the radio up to the gamma-ray regime. The details of their\nformation, composition and emission mechanisms are still not completely clear.\nThe combination of high-resolution observations using very long baseline\ninterferometry (VLBI) and multiwavelength monitoring provides the best insight\ninto these objects. Here, such a combined study of sources of the TANAMI sample\nis presented, investigating the parsec-scale and high-energy properties. The\nTANAMI program is a multiwavelength monitoring program of a sample of the radio\nand gamma-ray brightest extragalactic jets in the southern sky, below -30deg\ndeclination. We obtain the first-ever VLBI images for most of the sources,\nproviding crucial information on the jet kinematics and brightness distribution\nat milliarcsecond resolution. Two particular sources are discussed in detail:\nPMN J1603-4904, which can be classified either as an atypical blazar or a\ngamma-ray loud (young) radio galaxy, and Centaurus A, the nearest radio-loud\nactive galaxy. The VLBI kinematics of the innermost parsec of Centaurus A's jet\nresult in a consistent picture of an accelerated jet flow with a spine-sheath\nlike structure. \n\n"}
{"id": "1607.06757", "contents": "Title: Hereditary Graph Classes: When the Complexities of Colouring and Clique\n  Cover Coincide Abstract: A graph is $(H_1,H_2)$-free for a pair of graphs $H_1,H_2$ if it contains no\ninduced subgraph isomorphic to $H_1$ or $H_2$. In 2001, Kr\\'al',\nKratochv\\'{\\i}l, Tuza, and Woeginger initiated a study into the complexity of\nColouring for $(H_1,H_2)$-free graphs. Since then, others have tried to\ncomplete their study, but many cases remain open. We focus on those\n$(H_1,H_2)$-free graphs where $H_2$ is $\\overline{H_1}$, the complement of\n$H_1$. As these classes are closed under complementation, the computational\ncomplexities of Colouring and Clique Cover coincide. By combining new and known\nresults, we are able to classify the complexity of Colouring and Clique Cover\nfor $(H,\\overline{H})$-free graphs for all cases except when $H=sP_1+ P_3$ for\n$s\\geq 3$ or $H=sP_1+P_4$ for $s\\geq 2$. We also classify the complexity of\nColouring on graph classes characterized by forbidding a finite number of\nself-complementary induced subgraphs, and we initiate a study of $k$-Colouring\nfor $(P_r,\\overline{P_r})$-free graphs. \n\n"}
{"id": "1607.07497", "contents": "Title: A Hierarchy of Lower Bounds for Sublinear Additive Spanners Abstract: Spanners, emulators, and approximate distance oracles can be viewed as lossy\ncompression schemes that represent an unweighted graph metric in small space,\nsay $\\tilde{O}(n^{1+\\delta})$ bits. There is an inherent tradeoff between the\nsparsity parameter $\\delta$ and the stretch function $f$ of the compression\nscheme, but the qualitative nature of this tradeoff has remained a persistent\nopen problem.\n  In this paper we show that the recent additive spanner lower bound of Abboud\nand Bodwin is just the first step in a hierarchy of lower bounds that fully\ncharacterize the asymptotic behavior of the optimal stretch function $f$ as a\nfunction of $\\delta \\in (0,1/3)$. Specifically, for any integer $k\\ge 2$, any\ncompression scheme with size $O(n^{1+\\frac{1}{2^k-1} - \\epsilon})$ has a\nsublinear additive stretch function $f$: $$f(d) = d +\n\\Omega(d^{1-\\frac{1}{k}}).$$ This lower bound matches Thorup and Zwick's (2006)\nconstruction of sublinear additive emulators. It also shows that Elkin and\nPeleg's $(1+\\epsilon,\\beta)$-spanners have an essentially optimal tradeoff\nbetween $\\delta,\\epsilon,$ and $\\beta$, and that the sublinear additive\nspanners of Pettie (2009) and Chechik (2013) are not too far from optimal.\n  To complement these lower bounds we present a new construction of\n$(1+\\epsilon, O(k/\\epsilon)^{k-1})$-spanners with size $O((k/\\epsilon)^{h_k}\nkn^{1+\\frac{1}{2^{k+1}-1}})$, where $h_k < 3/4$. This size bound improves on\nthe spanners of Elkin and Peleg (2004), Thorup and Zwick (2006), and Pettie\n(2009). According to our lower bounds neither the size nor stretch function can\nbe substantially improved. \n\n"}
{"id": "1607.08192", "contents": "Title: Counting matchings with k unmatched vertices in planar graphs Abstract: We consider the problem of counting matchings in planar graphs. While perfect\nmatchings in planar graphs can be counted by a classical polynomial-time\nalgorithm, the problem of counting all matchings (possibly containing unmatched\nvertices, also known as defects) is known to be #P-complete on planar graphs.\nTo interpolate between the hard case of counting matchings and the easy case of\ncounting perfect matchings, we study the parameterized problem of counting\nmatchings with exactly k unmatched vertices in a planar graph G, on input G and\nk. This setting has a natural interpretation in statistical physics, and it is\na special case of counting perfect matchings in k-apex graphs (graphs that can\nbe turned planar by removing at most k vertices).\n  Starting from a recent #W[1]-hardness proof for counting perfect matchings on\nk-apex graphs, we obtain that counting matchings with k unmatched vertices in\nplanar graphs is #W[1]-hard. In contrast, given a plane graph G with s\ndistinguished faces, there is an $O(2^s \\cdot n^3)$ time algorithm for counting\nthose matchings with k unmatched vertices such that all unmatched vertices lie\non the distinguished faces. This implies an $f(k,s)\\cdot n^{O(1)}$ time\nalgorithm for counting perfect matchings in k-apex graphs whose apex\nneighborhood is covered by s faces. \n\n"}
{"id": "1607.08213", "contents": "Title: Global numerical simulations of the rise of vortex-mediated pulsar\n  glitches in full general relativity Abstract: In this paper, we study in detail the role of general relativity on the\nglobal dynamics of giant pulsar glitches as exemplified by Vela. For this\npurpose, we carry out numerical simulations of the spin up triggered by the\nsudden unpinning of superfluid vortices. In particular, we compute the exchange\nof angular momentum between the core neutron superfluid and the rest of the\nstar within a two-fluid model including both (non-dissipative) entrainment\neffects and (dissipative) mutual friction forces. Our simulations are based on\na quasi-stationary approach using realistic equations of state (EoSs). We show\nthat the evolution of the angular velocities of both fluids can be accurately\ndescribed by an exponential law. The associated characteristic rise time\n$\\tau_{\\text{r}}$, which can be precisely computed from stationary\nconfigurations only, has a form similar to that obtained in the Newtonian\nlimit. However, general relativity changes the structure of the star and leads\nto additional couplings between the fluids due to frame-dragging effects. As a\nconsequence, general relativity can have a large impact on the actual value of\n$\\tau_{\\text{r}}$: the errors incurred by using Newtonian gravity are thus\nfound to be as large as $\\sim 40 \\%$ for the models considered. Values of the\nrise time are calculated for Vela and compared with current observational\nlimits. Finally, we study the amount of gravitational waves emitted during a\nglitch. Simple expressions are obtained for the corresponding characteristic\namplitudes and frequencies. The detectability of glitches through gravitational\nwave observatories is briefly discussed. \n\n"}
{"id": "1608.01628", "contents": "Title: Binarisation for Valued Constraint Satisfaction Problems Abstract: We study methods for transforming valued constraint satisfaction problems\n(VCSPs) to binary VCSPs. First, we show that the standard dual encoding\npreserves many aspects of the algebraic properties that capture the\ncomputational complexity of VCSPs. Second, we extend the reduction of CSPs to\nbinary CSPs described by Bulin et al. [LMCS'15] to VCSPs. This reduction\nestablishes that VCSPs over a fixed valued constraint language are\npolynomial-time equivalent to Minimum-Cost Homomorphism Problems over a fixed\ndigraph. \n\n"}
{"id": "1608.01820", "contents": "Title: Bounded Clique-Width of ($S_{1,2,2}$,Triangle)-Free Graphs Abstract: If a graph has no induced subgraph isomorphic to $H_1$ or $H_2$ then it is\nsaid to be ($H_1,H_2$)-free. Dabrowski and Paulusma found 13 open cases for the\nquestion whether the clique-width of ($H_1,H_2$)-free graphs is bounded. One of\nthem is the class of ($S_{1,2,2}$,triangle)-free graphs. In this paper we show\nthat these graphs have bounded clique-width. Thus, also\n($P_1+2P_2$,triangle)-free graphs have bounded clique-width which solves\nanother open problem of Dabrowski and Paulusma. Meanwhile we were informed by\nPaulusma that in December 2015, Dabrowski, Dross and Paulusma showed that\n($S_{1,2,2}$,triangle)-free graphs (and some other graph classes) have bounded\nclique-width. \n\n"}
{"id": "1608.01998", "contents": "Title: Type Ibn Supernovae Show Photometric Homogeneity and Spectral Diversity\n  at Maximum Light Abstract: Type Ibn supernovae (SNe) are a small yet intriguing class of explosions\nwhose spectra are characterized by low-velocity helium emission lines with\nlittle to no evidence for hydrogen. The prevailing theory has been that these\nare the core-collapse explosions of very massive stars embedded in helium-rich\ncircumstellar material (CSM). We report optical observations of six new SNe\nIbn: PTF11rfh, PTF12ldy, iPTF14aki, iPTF15ul, SN 2015G, and iPTF15akq. This\nbrings the sample size of such objects in the literature to 22. We also report\nnew data, including a near-infrared spectrum, on the Type Ibn SN 2015U. In\norder to characterize the class as a whole, we analyze the photometric and\nspectroscopic properties of the full Type Ibn sample. We find that, despite the\nexpectation that CSM interaction would generate a heterogeneous set of light\ncurves, as seen in SNe IIn, most Type Ibn light curves are quite similar in\nshape, declining at rates around 0.1 mag/day during the first month after\nmaximum light, with a few significant exceptions. Early spectra of SNe Ibn come\nin at least two varieties, one that shows narrow P Cygni lines and another\ndominated by broader emission lines, both around maximum light, which may be an\nindication of differences in the state of the progenitor system at the time of\nexplosion. Alternatively, the spectral diversity could arise from viewing-angle\neffects or merely from a lack of early spectroscopic coverage. Together, the\nrelative light curve homogeneity and narrow spectral features suggest that the\nCSM consists of a spatially confined shell of helium surrounded by a less dense\nextended wind. \n\n"}
{"id": "1608.02282", "contents": "Title: Computing the Independence Polynomial: from the Tree Threshold down to\n  the Roots Abstract: We study an algorithm for approximating the multivariate independence\npolynomial $Z(\\mathbf{z})$, with negative and complex arguments, an object that\nhas strong connections to combinatorics and to statistical physics. In\nparticular, the independence polynomial with negative arguments,\n$Z(-\\mathbf{p})$, determines the Shearer region, the maximal region of\nprobabilities to which the Lovasz Local Lemma (LLL) can be extended (Shearer\n1985). In statistical physics, complex zeros of the independence polynomial\nrelate to existence of phase transitions.\n  Our main result is a deterministic algorithm to compute approximately the\nindependence polynomial in any root-free complex polydisc centered at the\norigin. Our algorithm is essentially the same as Weitz's algorithm for positive\nparameters up to the tree uniqueness threshold, and the core of our analysis is\na novel multivariate form of the correlation decay technique, which can handle\nnon-uniform complex parameters. In particular, in the univariate real setting\nour work implies that Weitz's algorithm works in an interval between two\ncritical points $(\\lambda'_c(d), \\lambda_c(d))$, and outside of this interval\nan approximation of $Z(\\mathbf{z})$ is known to be NP-hard.\n  As an application, we give a sub-exponential time algorithm for testing\napproximate membership in the Shearer region. We also give a new rounding based\ndeterministic algorithm for Shearer's lemma (an extension of the LLL), which,\nhowever, runs in sub-exponential time. On the hardness side, we prove that\nevaluating $Z(\\mathbf{z})$ at an arbitrary point in Shearer's region, and\ntesting membership in Shearer's region, are #P-hard problems. We also establish\nthe best possible dependence of the exponent of the run time of Weitz's\ncorrelation decay technique in the negative regime on the distance to the\nboundary of the Shearer region. \n\n"}
{"id": "1608.02709", "contents": "Title: Parameterized Algorithms for the Maximum Agreement Forest Problem on\n  Multiple Rooted Multifurcating Trees Abstract: The Maximum Agreement Forest problem has been extensively studied in\nphylogenetics. Most previous work is on two binary phylogenetic trees. In this\npaper, we study a generalized version of the problem: the Maximum Agreement\nForest problem on multiple rooted multifurcating phylogenetic trees, from the\nperspective of fixed-parameter algorithms. By taking advantage of a new\nbranch-and-bound strategy, two parameterized algorithms, with running times\n$O(2.42^k m^3 n^4)$ and $O(2.74^k m^3 n^5)$, respectively, are presented for\nthe hard version and the soft version of the problem, which correspond to two\ndifferent biological meanings to the polytomies in multifurcating phylogenetic\ntrees. \n\n"}
{"id": "1608.03165", "contents": "Title: Linear Programming based Converses for Finite Blocklength Lossy Joint\n  Source-Channel Coding Abstract: A linear programming (LP) based framework is presented for obtaining\nconverses for finite blocklength lossy joint source-channel coding problems.\nThe framework applies for any loss criterion, generalizes certain previously\nknown converses, and also extends to multi-terminal settings. The finite\nblocklength problem is posed equivalently as a nonconvex optimization problem\nand using a lift-and-project-like method, a close but tractable LP relaxation\nof this problem is derived. Lower bounds on the original problem are obtained\nby the construction of feasible points for the dual of the LP relaxation. A\nparticular application of this approach leads to new converses which recover\nand improve on the converses of Kostina and Verdu for finite blocklength lossy\njoint source-channel coding and lossy source coding. For finite blocklength\nchannel coding, the LP relaxation recovers the converse of Polyanskiy, Poor and\nVerdu and leads to a new improvement on the converse of Wolfowitz, showing\nthereby that our LP relaxation is asymptotically tight with increasing\nblocklengths for channel coding, lossless source coding and joint\nsource-channel coding with the excess distortion probability as the loss\ncriterion. Using a duality based argument, a new converse is derived for finite\nblocklength joint source-channel coding for a class of source-channel pairs.\nEmploying this converse, the LP relaxation is also shown to be tight for all\nblocklengths for the minimization of the expected average symbol-wise Hamming\ndistortion of a $q$-ary uniform source over a $q$-ary symmetric memoryless\nchannel for any $q \\in N$. The optimization formulation and the\nlift-and-project method are extended to networked settings and demonstrated by\nobtaining an improvement on a converse of Zhou et al. for the successive\nrefinement problem for successively refinable source-distortion measure\ntriplets. \n\n"}
{"id": "1608.03368", "contents": "Title: Bi-Arc Digraphs and Conservative Polymorphisms Abstract: In this paper we study the class of bi-arc digraphs, important from two\nseemingly unrelated perspectives. On the one hand, they are precisely the\ndigraphs that admit certain polymorphisms of interest in the study of\nconstraint satisfaction problems; on the other hand, they are a very broad\ngeneralization of interval graphs.\n  Bi-arc digraphs is the class of digraphs that admit conservative semilattice\npolymorphisms. There is much interest in understanding structures that admit\nparticular types of polymorphisms, and especially in their recognition\nalgorithms. (Such problems are referred to as metaproblems.) Surprisingly, the\nclass of bi-arc digraphs also describes the class of digraphs that admit\ncertain other kinds of conservative polymorphisms. Thus solving the recognition\nproblem for bi-arc digraphs solves the metaproblem for digraphs for several\ntypes of conservative polymorphisms. The complexity of the recognition problem\nfor digraphs with conservative semilattice polymorphisms was an open problem,\nwhile it was known to be NP-complete for certain more complex relational\nstructures. We complement our result by providing a complete dichotomy\nclassification of which general relational structures have polynomial or\nNP-complete recognition problems for the existence of conservative semilattice\npolymorphisms.\n  Bi-arc digraphs also generalizes the class of interval graphs; in fact it\nreduces to the class of interval graphs for symmetric and reflexive digraphs.\nIt is much broader than interval graphs and includes other generalizations of\ninterval graphs such as co-threshold tolerance graphs and adjusted interval\ndigraphs. Yet, it is still a reasonable extension of interval graphs, in the\nsense that it keeps much of the appeal of interval graphs.\n  Our main result is a forbidden obstruction characterization of, and a\npolynomial recognition for, the class of bi-arc digraphs. \n\n"}
{"id": "1608.03439", "contents": "Title: Finding Large Set Covers Faster via the Representation Method Abstract: The worst-case fastest known algorithm for the Set Cover problem on universes\nwith $n$ elements still essentially is the simple $O^*(2^n)$-time dynamic\nprogramming algorithm, and no non-trivial consequences of an $O^*(1.01^n)$-time\nalgorithm are known. Motivated by this chasm, we study the following natural\nquestion: Which instances of Set Cover can we solve faster than the simple\ndynamic programming algorithm? Specifically, we give a Monte Carlo algorithm\nthat determines the existence of a set cover of size $\\sigma n$ in\n$O^*(2^{(1-\\Omega(\\sigma^4))n})$ time. Our approach is also applicable to Set\nCover instances with exponentially many sets: By reducing the task of finding\nthe chromatic number $\\chi(G)$ of a given $n$-vertex graph $G$ to Set Cover in\nthe natural way, we show there is an $O^*(2^{(1-\\Omega(\\sigma^4))n})$-time\nrandomized algorithm that given integer $s=\\sigma n$, outputs NO if $\\chi(G) >\ns$ and YES with constant probability if $\\chi(G)\\leq s-1$.\n  On a high level, our results are inspired by the `representation method' of\nHowgrave-Graham and Joux~[EUROCRYPT'10] and obtained by only evaluating a\nrandomly sampled subset of the table entries of a dynamic programming\nalgorithm. \n\n"}
{"id": "1608.03580", "contents": "Title: Optimal Hashing-based Time-Space Trade-offs for Approximate Near\n  Neighbors Abstract: [See the paper for the full abstract.]\n  We show tight upper and lower bounds for time-space trade-offs for the\n$c$-Approximate Near Neighbor Search problem. For the $d$-dimensional Euclidean\nspace and $n$-point datasets, we develop a data structure with space $n^{1 +\n\\rho_u + o(1)} + O(dn)$ and query time $n^{\\rho_q + o(1)} + d n^{o(1)}$ for\nevery $\\rho_u, \\rho_q \\geq 0$ such that: \\begin{equation} c^2 \\sqrt{\\rho_q} +\n(c^2 - 1) \\sqrt{\\rho_u} = \\sqrt{2c^2 - 1}. \\end{equation}\n  This is the first data structure that achieves sublinear query time and\nnear-linear space for every approximation factor $c > 1$, improving upon\n[Kapralov, PODS 2015]. The data structure is a culmination of a long line of\nwork on the problem for all space regimes; it builds on Spherical\nLocality-Sensitive Filtering [Becker, Ducas, Gama, Laarhoven, SODA 2016] and\ndata-dependent hashing [Andoni, Indyk, Nguyen, Razenshteyn, SODA 2014] [Andoni,\nRazenshteyn, STOC 2015].\n  Our matching lower bounds are of two types: conditional and unconditional.\nFirst, we prove tightness of the whole above trade-off in a restricted model of\ncomputation, which captures all known hashing-based approaches. We then show\nunconditional cell-probe lower bounds for one and two probes that match the\nabove trade-off for $\\rho_q = 0$, improving upon the best known lower bounds\nfrom [Panigrahy, Talwar, Wieder, FOCS 2010]. In particular, this is the first\nspace lower bound (for any static data structure) for two probes which is not\npolynomially smaller than the one-probe bound. To show the result for two\nprobes, we establish and exploit a connection to locally-decodable codes. \n\n"}
{"id": "1608.06819", "contents": "Title: Pricing and Optimization in Shared Vehicle Systems: An Approximation\n  Framework Abstract: Optimizing shared vehicle systems (bike/scooter/car/ride-sharing) is more\nchallenging compared to traditional resource allocation settings due to the\npresence of \\emph{complex network externalities} -- changes in the\ndemand/supply at any location affect future supply throughout the system within\nshort timescales. These externalities are well captured by steady-state\nMarkovian models, which are therefore widely used to analyze such systems.\nHowever, using such models to design pricing and other control policies is\ncomputationally difficult since the resulting optimization problems are\nhigh-dimensional and non-convex.\n  To this end, we develop a \\emph{rigorous approximation framework} for shared\nvehicle systems, providing a unified approach for a wide range of controls\n(pricing, matching, rebalancing), objective functions (throughput, revenue,\nwelfare), and system constraints (travel-times, welfare benchmarks,\nposted-price constraints). Our approach is based on the analysis of natural\nconvex relaxations, and obtains as special cases existing approximate-optimal\npolicies for limited settings, asymptotic-optimality results, and heuristic\npolicies. The resulting guarantees are non-asymptotic and parametric, and\nprovide operational insights into the design of real-world systems. In\nparticular, for any shared vehicle system with $n$ stations and $m$ vehicles,\nour framework obtains an approximation ratio of $1+(n-1)/m$, which is\nparticularly meaningful when $m/n$, the average number of vehicles per station,\nis large, as is often the case in practice. \n\n"}
{"id": "1608.07505", "contents": "Title: A Note on the Practicality of Maximal Planar Subgraph Algorithms Abstract: Given a graph $G$, the NP-hard Maximum Planar Subgraph problem (MPS) asks for\na planar subgraph of $G$ with the maximum number of edges. There are several\nheuristic, approximative, and exact algorithms to tackle the problem, but---to\nthe best of our knowledge---they have never been compared competitively in\npractice. We report on an exploratory study on the relative merits of the\ndiverse approaches, focusing on practical runtime, solution quality, and\nimplementation complexity. Surprisingly, a seemingly only theoretically strong\napproximation forms the building block of the strongest choice. \n\n"}
{"id": "1608.08545", "contents": "Title: Dynamic Controllability of Conditional Simple Temporal Networks is\n  PSPACE-complete Abstract: Even after the proposal of various solution algorithms, the precise\ncomputational complexity of checking whether a Conditional Temporal Network is\nDynamically Controllable had still remained widely open. This issue gets\nsettled in this paper which provides constructions, algorithms, and bridging\nlemmas and arguments to formally prove that: (1) the problem is PSPACE-hard,\nand (2) the problem lies in PSPACE. \n\n"}
{"id": "1609.00265", "contents": "Title: Testing $k$-Monotonicity Abstract: A Boolean $k$-monotone function defined over a finite poset domain ${\\cal D}$\nalternates between the values $0$ and $1$ at most $k$ times on any ascending\nchain in ${\\cal D}$. Therefore, $k$-monotone functions are natural\ngeneralizations of the classical monotone functions, which are the $1$-monotone\nfunctions. Motivated by the recent interest in $k$-monotone functions in the\ncontext of circuit complexity and learning theory, and by the central role that\nmonotonicity testing plays in the context of property testing, we initiate a\nsystematic study of $k$-monotone functions, in the property testing model. In\nthis model, the goal is to distinguish functions that are $k$-monotone (or are\nclose to being $k$-monotone) from functions that are far from being\n$k$-monotone. Our results include the following:\n  - We demonstrate a separation between testing $k$-monotonicity and testing\nmonotonicity, on the hypercube domain $\\{0,1\\}^d$, for $k\\geq 3$;\n  - We demonstrate a separation between testing and learning on $\\{0,1\\}^d$,\nfor $k=\\omega(\\log d)$: testing $k$-monotonicity can be performed with\n$2^{O(\\sqrt d \\cdot \\log d\\cdot \\log{1/\\varepsilon})}$ queries, while learning\n$k$-monotone functions requires $2^{\\Omega(k\\cdot \\sqrt\nd\\cdot{1/\\varepsilon})}$ queries (Blais et al. (RANDOM 2015)).\n  - We present a tolerant test for functions $f\\colon[n]^d\\to \\{0,1\\}$ with\ncomplexity independent of $n$, which makes progress on a problem left open by\nBerman et al. (STOC 2014).\n  Our techniques exploit the testing-by-learning paradigm, use novel\napplications of Fourier analysis on the grid $[n]^d$, and draw connections to\ndistribution testing techniques. \n\n"}
{"id": "1609.01373", "contents": "Title: Improved Algorithms for Computing $k$-Sink on Dynamic Path Networks Abstract: We present a novel approach to finding the $k$-sink on dynamic path networks\nwith general edge capacities. Our first algorithm runs in $O(n \\log n + k^2\n\\log^4 n)$ time, where $n$ is the number of vertices on the given path, and our\nsecond algorithm runs in $O(n \\log^3 n)$ time. Together, they improve upon the\npreviously most efficient $O(kn \\log^2 n)$ time algorithm due to Arumugam et\nal. for all values of $k$. In the case where all the edges have the same\ncapacity, we again present two algorithms that run in $O(n + k^2 \\log^2n)$ time\nand $O(n \\log n)$ time, respectively, and they together improve upon the\npreviously best $O(kn)$ time algorithm due to Higashikawa et al. for all values\nof $k$. \n\n"}
{"id": "1609.03072", "contents": "Title: Prospects for Detecting Galactic Sources of Cosmic Neutrinos with\n  IceCube: An Update Abstract: Air-Cherenkov telescopes have mapped the Galactic plane at TeV energies. Here\nwe evaluate the prospects for detecting the neutrino emission from sources in\nthe Galactic plane assuming that the highest energy photons originate from the\ndecay of pions, which yields a straightforward prediction for the neutrino flux\nfrom the decay of the associated production of charged pions. Four promising\nsources are identified based on having a large flux and a flat spectrum. We\nsubsequently evaluate the probability of their identification above the\natmospheric neutrino background in IceCube data as a function of time. We show\nthat observing them over the twenty-year lifetime of the instrumentation is\nlikely, and that some should be observable at the $3\\,\\sigma$ level with six\nyears of data. In the absence of positive results, we derive constraints on the\nspectral index and cut-off energy of the sources, assuming a hadronic\nacceleration mechanism. \n\n"}
{"id": "1609.03737", "contents": "Title: Small Extended Formulation for Knapsack Cover Inequalities from Monotone\n  Circuits Abstract: Initially developed for the min-knapsack problem, the knapsack cover\ninequalities are used in the current best relaxations for numerous\ncombinatorial optimization problems of covering type. In spite of their\nwidespread use, these inequalities yield linear programming (LP) relaxations of\nexponential size, over which it is not known how to optimize exactly in\npolynomial time. In this paper we address this issue and obtain LP relaxations\nof quasi-polynomial size that are at least as strong as that given by the\nknapsack cover inequalities.\n  For the min-knapsack cover problem, our main result can be stated formally as\nfollows: for any $\\varepsilon >0$, there is a $(1/\\varepsilon)^{O(1)}n^{O(\\log\nn)}$-size LP relaxation with an integrality gap of at most $2+\\varepsilon$,\nwhere $n$ is the number of items. Prior to this work, there was no known\nrelaxation of subexponential size with a constant upper bound on the\nintegrality gap.\n  Our construction is inspired by a connection between extended formulations\nand monotone circuit complexity via Karchmer-Wigderson games. In particular,\nour LP is based on $O(\\log^2 n)$-depth monotone circuits with fan-in~$2$ for\nevaluating weighted threshold functions with $n$ inputs, as constructed by\nBeimel and Weinreb. We believe that a further understanding of this connection\nmay lead to more positive results complementing the numerous lower bounds\nrecently proved for extended formulations. \n\n"}
{"id": "1609.05277", "contents": "Title: Improved Lower Bounds on the Size of Balls over Permutations with the\n  Infinity Metric Abstract: We study the size (or volume) of balls in the metric space of permutations,\n$S_n$, under the infinity metric. We focus on the regime of balls with radius\n$r = \\rho \\cdot (n\\!-\\!1)$, $\\rho \\in [0,1]$, i.e., a radius that is a constant\nfraction of the maximum possible distance. We provide new lower bounds on the\nsize of such balls. These new lower bounds reduce the asymptotic gap to the\nknown upper bounds to at most $0.029$ bits per symbol. Additionally, they imply\nan improved ball-packing bound for error-correcting codes, and an improved\nupper bound on the size of optimal covering codes. \n\n"}
{"id": "1609.05573", "contents": "Title: Optimality and Sub-optimality of PCA for Spiked Random Matrices and\n  Synchronization Abstract: A central problem of random matrix theory is to understand the eigenvalues of\nspiked random matrix models, in which a prominent eigenvector is planted into a\nrandom matrix. These distributions form natural statistical models for\nprincipal component analysis (PCA) problems throughout the sciences. Baik, Ben\nArous and P\\'ech\\'e showed that the spiked Wishart ensemble exhibits a sharp\nphase transition asymptotically: when the signal strength is above a critical\nthreshold, it is possible to detect the presence of a spike based on the top\neigenvalue, and below the threshold the top eigenvalue provides no information.\nSuch results form the basis of our understanding of when PCA can detect a\nlow-rank signal in the presence of noise.\n  However, not all the information about the spike is necessarily contained in\nthe spectrum. We study the fundamental limitations of statistical methods,\nincluding non-spectral ones. Our results include:\n  I) For the Gaussian Wigner ensemble, we show that PCA achieves the optimal\ndetection threshold for a variety of benign priors for the spike. We extend\nprevious work on the spherically symmetric and i.i.d. Rademacher priors through\nan elementary, unified analysis.\n  II) For any non-Gaussian Wigner ensemble, we show that PCA is always\nsuboptimal for detection. However, a variant of PCA achieves the optimal\nthreshold (for benign priors) by pre-transforming the matrix entries according\nto a carefully designed function. This approach has been stated before, and we\ngive a rigorous and general analysis.\n  III) For both the Gaussian Wishart ensemble and various synchronization\nproblems over groups, we show that inefficient procedures can work below the\nthreshold where PCA succeeds, whereas no known efficient algorithm achieves\nthis. This conjectural gap between what is statistically possible and what can\nbe done efficiently remains open. \n\n"}
{"id": "1609.07056", "contents": "Title: Nash Social Welfare, Matrix Permanent, and Stable Polynomials Abstract: We study the problem of allocating $m$ items to $n$ agents subject to\nmaximizing the Nash social welfare (NSW) objective. We write a novel convex\nprogramming relaxation for this problem, and we show that a simple randomized\nrounding algorithm gives a $1/e$ approximation factor of the objective.\n  Our main technical contribution is an extension of Gurvits's lower bound on\nthe coefficient of the square-free monomial of a degree $m$-homogeneous stable\npolynomial on $m$ variables to all homogeneous polynomials. We use this\nextension to analyze the expected welfare of the allocation returned by our\nrandomized rounding algorithm. \n\n"}
{"id": "1609.08095", "contents": "Title: How much does a treedepth modulator help to obtain polynomial kernels\n  beyond sparse graphs? Abstract: In the last years, kernelization with structural parameters has been an\nactive area of research within the field of parameterized complexity. As a\nrelevant example, Gajarsk{\\`y} et al. [ESA 2013] proved that every graph\nproblem satisfying a property called finite integer index admits a linear\nkernel on graphs of bounded expansion and an almost linear kernel on nowhere\ndense graphs, parameterized by the size of a $c$-treedepth modulator, which is\na vertex set whose removal results in a graph of treedepth at most $c$, where\n$c \\geq 1$ is a fixed integer. The authors left as further research to\ninvestigate this parameter on general graphs, and in particular to find\nproblems that, while admitting polynomial kernels on sparse graphs, behave\ndifferently on general graphs.\n  In this article we answer this question by finding two very natural such\nproblems: we prove that Vertex Cover admits a polynomial kernel on general\ngraphs for any integer $c \\geq 1$, and that Dominating Set does not for any\ninteger $c \\geq 2$ even on degenerate graphs, unless $\\text{NP} \\subseteq\n\\text{coNP}/\\text{poly}$. For the positive result, we build on the techniques\nof Jansen and Bodlaender [STACS 2011], and for the negative result we use a\npolynomial parameter transformation for $c\\geq 3$ and an OR-cross-composition\nfor $c = 2$. As existing results imply that Dominating Set admits a polynomial\nkernel on degenerate graphs for $c = 1$, our result provides a dichotomy about\nthe existence of polynomial kernels for Dominating Set on degenerate graphs\nwith this parameter. \n\n"}
{"id": "1609.08403", "contents": "Title: Tight Hardness Results for Distance and Centrality Problems in Constant\n  Degree Graphs Abstract: Finding important nodes in a graph and measuring their importance is a\nfundamental problem in the analysis of social networks, transportation\nnetworks, biological systems, etc. Among popular such metrics are graph\ncentrality, betweenness centrality (BC), and reach centrality (RC). These\nmeasures are also very related to classic notions like diameter and radius.\nRoditty and Vassilevska Williams~[STOC'13] showed that no algorithm can compute\na (3/2-\\delta)-approximation of the diameter in sparse and unweighted graphs\nfaster that n^{2-o(1)} time unless the widely believed strong exponential time\nhypothesis (SETH) is false. Abboud et al.~[SODA'15] and [SODA'16] further\nanalyzed these problems under the recent line of research on hardness in P.\nThey showed that in sparse and unweighted graphs (weighted for BC) none of\nthese problems can be solved faster than n^{2-o(1)} unless some popular\nconjecture is false. Furthermore they ruled out a (2-\\delta)-approximation for\nRC, a (3/2-\\delta)-approximation for Radius and a (5/3-\\delta)-approximation\nfor computing all eccentricities of a graph for any \\delta > 0. We extend these\nresults to the case of unweighted graphs with constant maximum degree. Through\nnew graph constructions we are able to obtain the same approximation and time\nbounds as for sparse graphs even in unweighted bounded-degree graphs. We show\nthat no (3/2-\\delta) approximation of Radius or Diameter,\n(2-\\delta)-approximation of RC, (5/3-\\delta)-approximation of all\neccentricities or exact algorithm for BC exists in time n^{2-o(1)} for such\ngraphs and any \\delta > 0. This strengthens the result for BC of Abboud et\nal.~[SODA'16] by showing a hardness result for unweighted graphs, and follows\nin the footsteps of Abboud et al.~[SODA'16] and Abboud and Dahlgaard~[FOCS'16]\nin showing conditional lower bounds for restricted but realistic graph classes. \n\n"}
{"id": "1610.00353", "contents": "Title: $O(m^9)$ network flow LP model of the Assignment Problem polytope with\n  applications to hard combinatorial optimization problems Abstract: In this paper, we present a new, network flow LP model of the standard\nAssignment Problem (AP) polytope. The model is not meant to be competitive with\nexisting standard procedures for solving the AP, as its complexity order of\nsize is $O(m^9)$, where m is the number of assignments. However, it allows for\nhard combinatorial optimization problems (COPs) to be solved as Assignment\nProblems (APs), including, in particular, the Quadratic, Cubic, Quartic,\nQuintic, and Sextic Assignment Problems, as well as the Traveling Salesman\nProblem and many of its variations. Hence, in particular, the model re-affirms\n\"P = NP.\" Illustrations are provided for the Linear Assignment (LAP), Quadratic\nAssignment (QAP), and Traveling Salesman (TSP) problems. Issues pertaining to\nthe extended formulations \"barriers\" for the LP modeling of hard COPs are not\ndiscussed in this paper because the developments are focused on the Assignment\nProblem polytope only, and also the applicability/non-applicability of those\n\"barriers\" are thoroughly addressed in a separate paper* in which it is shown\nthat, in an optimization context, these \"barriers\" have no pertinence for a\nmodel which projects to the AP polytope, provided appropriate costs can be\nattached to the non-superfluous variables of the model. Hence, the issues of\nthe \"barriers\" are left out of this paper essentially for the sake of space.\n  *: Diaby, M., M. Karwan, and L. Sun [2024]. On modeling NP-Complete problems\nas polynomial-sized linear programs: Escaping/Side-stepping the \"barriers.\"\nAvailable at: arXiv:2304.07716 [cc.CC]. \n\n"}
{"id": "1610.00853", "contents": "Title: Constrained Hitting Set and Steiner Tree in $SC_k$ and $2K_2$-free\n  Graphs Abstract: \\emph{Strictly Chordality-$k$ graphs ($SC_k$)} are graphs which are either\ncycle-free or every induced cycle is of length exactly $k, k \\geq 3$. Strictly\nchordality-3 and strictly chordality-4 graphs are well known chordal and\nchordal bipartite graphs, respectively. For $k\\geq 5$, the study has been\nrecently initiated in \\cite{sadagopan} and various structural and algorithmic\nresults are reported. In this paper, we show that maximum independent set\n(MIS), minimum vertex cover, minimum dominating set, feedback vertex set (FVS),\nodd cycle transversal (OCT), even cycle transversal (ECT) and Steiner tree\nproblem are polynomial time solvable on $SC_k$ graphs, $k\\geq 5$. We next\nconsider $2K_2$-free graphs and show that FVS, OCT, ECT, Steiner tree problem\nare polynomial time solvable on subclasses of $2K_2$-free graphs. \n\n"}
{"id": "1610.02175", "contents": "Title: F-index and coindex of some derived graphs Abstract: In this study, the explicit expressions for F-index and coindex of derived\ngraphs such as a line graph, subdivision graph, vertex-semitotal graph,\nedge-semitotal graph, total graph and paraline graph (line graph of the\nsubdivision graph) are obtained. \n\n"}
{"id": "1610.03061", "contents": "Title: The ASAS-SN Bright Supernova Catalog $-$ II. 2015 Abstract: This manuscript presents information for all supernovae discovered by the\nAll-Sky Automated Survey for SuperNovae (ASAS-SN) during 2015, its second full\nyear of operations. The same information is presented for bright ($m_V\\leq17$),\nspectroscopically confirmed supernovae discovered by other sources in 2015. As\nwith the first ASAS-SN bright supernova catalog, we also present redshifts and\nnear-UV through IR magnitudes for all supernova host galaxies in both samples.\nCombined with our previous catalog, this work comprises a complete catalog of\n455 supernovae from multiple professional and amateur sources, allowing for\npopulation studies that were previously impossible. This is the second of a\nseries of yearly papers on bright supernovae and their hosts from the ASAS-SN\nteam. \n\n"}
{"id": "1610.03766", "contents": "Title: Independent Set Reconfiguration Thresholds of Hereditary Graph Classes Abstract: Traditionally, reconfiguration problems ask the question whether a given\nsolution of an optimization problem can be transformed to a target solution in\na sequence of small steps that preserve feasibility of the intermediate\nsolutions. In this paper, rather than asking this question from an algorithmic\nperspective, we analyze the combinatorial structure behind it. We consider the\nproblem of reconfiguring one independent set into another, using two different\nprocesses: (1) exchanging exactly $k$ vertices in each step, or (2) removing or\nadding one vertex in each step while ensuring the intermediate sets contain at\nmost $k$ fewer vertices than the initial solution. We are interested in\ndetermining the minimum value of $k$ for which this reconfiguration is\npossible, and bound these threshold values in terms of several structural graph\nparameters. For hereditary graph classes we identify structures that cause the\nreconfiguration threshold to be large. \n\n"}
{"id": "1610.04055", "contents": "Title: Approximating partition functions of bounded-degree Boolean counting\n  Constraint Satisfaction Problems Abstract: We study the complexity of approximate counting Constraint Satisfaction\nProblems (#CSPs) in a bounded degree setting. Specifically, given a Boolean\nconstraint language $\\Gamma$ and a degree bound $\\Delta$, we study the\ncomplexity of #CSP$_\\Delta(\\Gamma)$, which is the problem of counting\nsatisfying assignments to CSP instances with constraints from $\\Gamma$ and\nwhose variables can appear at most $\\Delta$ times. Our main result shows that:\n(i) if every function in $\\Gamma$ is affine, then #CSP$_\\Delta(\\Gamma)$ is in\nFP for all $\\Delta$, (ii) otherwise, if every function in $\\Gamma$ is in a\nclass called IM$_2$, then for all sufficiently large $\\Delta$,\n#CSP$_\\Delta(\\Gamma)$ is equivalent under approximation-preserving (AP)\nreductions to the counting problem #BIS (the problem of counting independent\nsets in bipartite graphs) (iii) otherwise, for all sufficiently large $\\Delta$,\nit is NP-hard to approximate the number of satisfying assignments of an\ninstance of #CSP$_\\Delta(\\Gamma)$, even within an exponential factor. Our\nresult extends previous results, which apply only in the so-called\n\"conservative\" case. \n\n"}
{"id": "1610.04317", "contents": "Title: Approximate Counting, the Lovasz Local Lemma and Inference in Graphical\n  Models Abstract: In this paper we introduce a new approach for approximately counting in\nbounded degree systems with higher-order constraints. Our main result is an\nalgorithm to approximately count the number of solutions to a CNF formula\n$\\Phi$ when the width is logarithmic in the maximum degree. This closes an\nexponential gap between the known upper and lower bounds.\n  Moreover our algorithm extends straightforwardly to approximate sampling,\nwhich shows that under Lov\\'asz Local Lemma-like conditions it is not only\npossible to find a satisfying assignment, it is also possible to generate one\napproximately uniformly at random from the set of all satisfying assignments.\nOur approach is a significant departure from earlier techniques in approximate\ncounting, and is based on a framework to bootstrap an oracle for computing\nmarginal probabilities on individual variables. Finally, we give an application\nof our results to show that it is algorithmically possible to sample from the\nposterior distribution in an interesting class of graphical models. \n\n"}
{"id": "1610.05115", "contents": "Title: Tropical Vertex-Disjoint Cycles of a Vertex-Colored Digraph: Barter\n  Exchange with Multiple Items Per Agent Abstract: In a barter exchange market, agents bring items and seek to exchange their\nitems with one another. Agents may agree to a k-way exchange involving a cycle\nof k agents. A barter exchange market can be represented by a digraph where the\nvertices represent items and the edges out of a vertex indicate the items that\nan agent is willing to accept in exchange for that item. It is known that the\nproblem of finding a set of vertex-disjoint cycles with the maximum total\nnumber of vertices (MAX-SIZE-EXCHANGE) can be solved in polynomial time. We\nconsider a barter exchange where each agent may bring multiple items, and items\nof the same agent are represented by vertices with the same color. A set of\ncycles is said to be tropical if for every color there is a cycle that contains\na vertex of that color. We show that the problem of determining whether there\nexists a tropical set of vertex-disjoint cycles in a digraph\n(TROPICAL-EXCHANGE) is NP-complete and APX-hard. This is equivalent to\ndetermining whether it is possible to arrange an exchange of items among agents\nsuch that every agent trades away at least one item. TROPICAL-MAX-SIZE-EXCHANGE\nis a similar problem, where the goal is to find a set of vertex-disjoint cycles\nthat contains the maximum number of vertices and also contains all of the\ncolors in the graph. We show that this problem is likewise NP-complete and\nAPX-hard. For the restricted case where there are at most two vertices of each\ncolor (corresponding to a restriction that each agent may bring at most two\nitems), both problems remain NP-hard but are in APX. Finally, we consider\nMAX-SIZE-TROPICAL-EXCHANGE, where the set of cycles must primarily include as\nmany colors as possible and secondarily include as many vertices as possible.\nWe show that this problem is NP-hard. \n\n"}
{"id": "1610.06539", "contents": "Title: Linear separation of connected dominating sets in graphs Abstract: A connected dominating set in a graph is a dominating set of vertices that\ninduces a connected subgraph. Following analogous studies in the literature\nrelated to independent sets, dominating sets, and total dominating sets, we\nstudy in this paper the class of graphs in which the connected dominating sets\ncan be separated from the other vertex subsets by a linear weight function.\nMore precisely, we say that a graph is connected-domishold if it admits\nnon-negative real weights associated to its vertices such that a set of\nvertices is a connected dominating set if and only if the sum of the\ncorresponding weights exceeds a certain threshold. We characterize the graphs\nin this non-hereditary class in terms of a property of the set of minimal\ncutsets of the graph. We give several characterizations for the hereditary\ncase, that is, when each connected induced subgraph is required to be\nconnected-domishold. The characterization by forbidden induced subgraphs\nimplies that the class properly generalizes two well known classes of chordal\ngraphs, the block graphs and the trivially perfect graphs. Finally, we study\ncertain algorithmic aspects of connected-domishold graphs. Building on\nconnections with minimal cutsets and properties of the derived hypergraphs and\nBoolean functions, we show that our approach leads to new polynomially solvable\ncases of the weighted connected dominating set problem. \n\n"}
{"id": "1610.06928", "contents": "Title: The second closest gamma-ray burst: sub-luminous GRB 111005A with no\n  supernova in a super-solar metallicity environment Abstract: We report the detection of the radio afterglow of a long gamma-ray burst\n(GRB) 111005A at 5-345 GHz, including the very long baseline interferometry\nobservations with the positional error of 0.2 mas. The afterglow position is\ncoincident with the disk of a galaxy ESO 580-49 at z= 0.01326 (~1\" from its\ncenter), which makes GRB 111005A the second closest GRB known to date, after\nGRB 980425. The radio afterglow of GRB 111005A was an order of magnitude less\nluminous than those of local low-luminosity GRBs, and obviously than those of\ncosmological GRBs. The radio flux was approximately constant and then\nexperienced an unusually rapid decay a month after the GRB explosion. Similarly\nto only two other GRBs, we did not find the associated supernovae (SN), despite\ndeep near- and mid-infrared observations 1-9 days after the GRB explosion,\nreaching ~20 times fainter than other SNe associated with GRBs. Moreover, we\nmeasured twice solar metallicity for the GRB location. The low gamma-ray and\nradio luminosities, rapid decay, lack of a SN, and super-solar metallicity\nsuggest that GRB 111005A represents a different rare class of GRBs than typical\ncore-collapse events. We modelled the spectral energy distribution of the GRB\n111005A host finding that it is a dwarf, moderately star-forming galaxy,\nsimilar to the host of GRB 980425. The existence of two local GRBs in such\ngalaxies is still consistent with the hypothesis that the GRB rate is\nproportional to the cosmic star formation rate (SFR) density, but suggests that\nthe GRB rate is biased towards low SFRs. Using the far-infrared detection of\nESO 580-49, we conclude that the hosts of both GRBs 111005A and 980425 exhibit\nlower dust content than what would be expected from their stellar masses and\noptical colours. \n\n"}
{"id": "1610.07766", "contents": "Title: Hardness of approximation for strip packing Abstract: Strip packing is a classical packing problem, where the goal is to pack a set\nof rectangular objects into a strip of a given width, while minimizing the\ntotal height of the packing. The problem has multiple applications, e.g. in\nscheduling and stock-cutting, and has been studied extensively.\n  When the dimensions of objects are allowed to be exponential in the total\ninput size, it is known that the problem cannot be approximated within a factor\nbetter than $3/2$, unless $\\mathrm{P}=\\mathrm{NP}$. However, there was no\ncorresponding lower bound for polynomially bounded input data. In fact,\nNadiradze and Wiese [SODA 2016] have recently proposed a $(1.4 + \\epsilon)$\napproximation algorithm for this variant, thus showing that strip packing with\npolynomially bounded data can be approximated better than when exponentially\nlarge values in the input data are allowed. Their result has subsequently been\nimproved to a $(4/3 + \\epsilon)$ approximation by two independent research\ngroups [FSTTCS 2016, arXiv:1610.04430]. This raises a question whether strip\npacking with polynomially bounded input data admits a quasi-polynomial time\napproximation scheme, as is the case for related two-dimensional packing\nproblems like maximum independent set of rectangles or two-dimensional\nknapsack.\n  In this paper we answer this question in negative by proving that it is\nNP-hard to approximate strip packing within a factor better than $12/11$, even\nwhen admitting only polynomially bounded input data. In particular, this shows\nthat the strip packing problem admits no quasi-polynomial time approximation\nscheme, unless $\\mathrm{NP} \\subseteq \\mathrm{DTIME}(2^{\\mathrm{polylog}(n)})$. \n\n"}
{"id": "1610.08586", "contents": "Title: Prospects for Neutrino Spin Coherence in Supernovae Abstract: We present neutrino bulb model simulations of Majorana neutrino coherent spin\ntransformation (i.e., neutrino-antineutrino transformation), coupled to\nneutrino flavor evolution, for conditions corresponding to the neutronization\nburst epoch of an Oxygen-Neon-Magnesium (O-Ne-Mg) core collapse supernova.\nSignificant neutrino spin transformation in, for example, the neutronization\nburst, could alter the fluences of neutrinos and antineutrinos in a way which\nis potentially detectable for a Galactic core collapse supernova. Our\ncalculations for the first time incorporate geometric dilution in the spin\nevolution of the neutrinos and combine two-flavor and three-flavor evolution\nwith spin mixing physics. We find that significant spin transformations can\noccur, but only with a large neutrino luminosity and an electron fraction\n($Y_e$) profile which facilitates adiabatic conditions for the spin-channel\nresonance. Using our adopted parameters of neutrino energy spectra, luminosity,\ndensity and $Y_e$ profiles, our calculations require an unrealistically large\nneutrino rest mass to sustain the spin transformation. It is an open question\nwhether examining different density profiles or incorporating other sources of\nnonlinear feedback, such as $Y_e$ feedback, could mitigate this need. We find\nthat spin transformations are not sensitive to the flavor structure of\nneutrinos, i.e., the spin transformations occur regardless of whether we\nsimulate two- or three-flavor transformations. In the two-flavor case, spin\ntransformations were insensitive to the choice of solar or atmospheric\nmass-squared splitting as well as the choice of the Majorana phase.\nImportantly, our three-flavor simulations, as well as our two-flavor\nsimulations done with the atmospheric mass-squared splitting, show that the\ninclusion of spin degrees of freedom can significantly and qualitatively alter\nneutrino flavor evolution. \n\n"}
{"id": "1610.08739", "contents": "Title: Finding Largest Common Substructures of Molecules in Quadratic Time Abstract: Finding the common structural features of two molecules is a fundamental task\nin cheminformatics. Most drugs are small molecules, which can naturally be\ninterpreted as graphs. Hence, the task is formalized as maximum common subgraph\nproblem. Albeit the vast majority of molecules yields outerplanar graphs this\nproblem remains NP-hard.\n  We consider a variation of the problem of high practical relevance, where the\nrings of molecules must not be broken, i.e., the block and bridge structure of\nthe input graphs must be retained by the common subgraph. We present an\nalgorithm for finding a maximum common connected induced subgraph of two given\nouterplanar graphs subject to this constraint. Our approach runs in time\n$\\mathcal{O}(\\Delta n^2)$ in outerplanar graphs on $n$ vertices with maximum\ndegree $\\Delta$. This leads to a quadratic time complexity in molecular graphs,\nwhich have bounded degree. The experimental comparison on synthetic and\nreal-world datasets shows that our approach is highly efficient in practice and\noutperforms comparable state-of-the-art algorithms. \n\n"}
{"id": "1610.09186", "contents": "Title: Emission knots and polarization swings of swinging jets Abstract: Knots (emission features in jets of active galactic nuclei) often show\nnon-ballistic dynamics and variable emission/polarization properties. We model\nthese features as emission pattern propagating in a jet that carries helical\nmagnetic field and is launched along a changing direction. The model can\nreproduce a wide range of phenomena observed in the motion of knots:\nnon-ballistic motion (both smooth and occasional sudden change of direction,\nand/or oscillatory behavior), variable brightness, confinement of knots' motion\nwithin an overlaying envelope. The model also reproduces smooth large\npolarization angle swings, and at the same time allows for the seemingly random\nbehavior of synchrotron fluxes, polarization fraction and occasional $\\pi/2$\npolarization jumps. \n\n"}
{"id": "1611.00898", "contents": "Title: Low Rank Approximation with Entrywise $\\ell_1$-Norm Error Abstract: We study the $\\ell_1$-low rank approximation problem, where for a given $n\n\\times d$ matrix $A$ and approximation factor $\\alpha \\geq 1$, the goal is to\noutput a rank-$k$ matrix $\\widehat{A}$ for which\n  $$\\|A-\\widehat{A}\\|_1 \\leq \\alpha \\cdot \\min_{\\textrm{rank-}k\\textrm{\nmatrices}~A'}\\|A-A'\\|_1,$$ where for an $n \\times d$ matrix $C$, we let\n$\\|C\\|_1 = \\sum_{i=1}^n \\sum_{j=1}^d |C_{i,j}|$. This error measure is known to\nbe more robust than the Frobenius norm in the presence of outliers and is\nindicated in models where Gaussian assumptions on the noise may not apply. The\nproblem was shown to be NP-hard by Gillis and Vavasis and a number of\nheuristics have been proposed. It was asked in multiple places if there are any\napproximation algorithms.\n  We give the first provable approximation algorithms for $\\ell_1$-low rank\napproximation, showing that it is possible to achieve approximation factor\n$\\alpha = (\\log d) \\cdot \\mathrm{poly}(k)$ in $\\mathrm{nnz}(A) + (n+d)\n\\mathrm{poly}(k)$ time, where $\\mathrm{nnz}(A)$ denotes the number of non-zero\nentries of $A$. If $k$ is constant, we further improve the approximation ratio\nto $O(1)$ with a $\\mathrm{poly}(nd)$-time algorithm. Under the Exponential Time\nHypothesis, we show there is no $\\mathrm{poly}(nd)$-time algorithm achieving a\n$(1+\\frac{1}{\\log^{1+\\gamma}(nd)})$-approximation, for $\\gamma > 0$ an\narbitrarily small constant, even when $k = 1$.\n  We give a number of additional results for $\\ell_1$-low rank approximation:\nnearly tight upper and lower bounds for column subset selection, CUR\ndecompositions, extensions to low rank approximation with respect to\n$\\ell_p$-norms for $1 \\leq p < 2$ and earthmover distance, low-communication\ndistributed protocols and low-memory streaming algorithms, algorithms with\nlimited randomness, and bicriteria algorithms. We also give a preliminary\nempirical evaluation. \n\n"}
{"id": "1611.01259", "contents": "Title: Generalized Topic Modeling Abstract: Recently there has been significant activity in developing algorithms with\nprovable guarantees for topic modeling. In standard topic models, a topic (such\nas sports, business, or politics) is viewed as a probability distribution $\\vec\na_i$ over words, and a document is generated by first selecting a mixture $\\vec\nw$ over topics, and then generating words i.i.d. from the associated mixture\n$A{\\vec w}$. Given a large collection of such documents, the goal is to recover\nthe topic vectors and then to correctly classify new documents according to\ntheir topic mixture.\n  In this work we consider a broad generalization of this framework in which\nwords are no longer assumed to be drawn i.i.d. and instead a topic is a complex\ndistribution over sequences of paragraphs. Since one could not hope to even\nrepresent such a distribution in general (even if paragraphs are given using\nsome natural feature representation), we aim instead to directly learn a\ndocument classifier. That is, we aim to learn a predictor that given a new\ndocument, accurately predicts its topic mixture, without learning the\ndistributions explicitly. We present several natural conditions under which one\ncan do this efficiently and discuss issues such as noise tolerance and sample\ncomplexity in this model. More generally, our model can be viewed as a\ngeneralization of the multi-view or co-training setting in machine learning. \n\n"}
{"id": "1611.01357", "contents": "Title: Neutron star properties and the equation of state for its core Abstract: Few unified equations of state for neutron star matter where core and crust\nare described using the same nuclear model are available. However the use of\nnon-unified equations of state with a simplified matching between the crust and\nthe core has been shown to introduce uncertainties in the radius determination\nwhich can be larger than the expected precision of the next generation of X-ray\nsatellites.\n  We aim at eliminating the dependence of the radius and mass of neutron staron\nthe detailed model for the crust and on the crust-core matching procedure. We\nsolve the approximate equations of the hydrostatic equilibrium for the crust of\nneutron stars obtaining a precise formula for the radius which depends only on\nthe core mass and radius, and on the baryon chemical potential at the\ncore-crust interface and on the crust surface. For a fully accreted crust one\nneeds additionally the value of the total deep crustal heating per one accreted\nnucleon.\n  For typical neutron star masses the approximate approach allows to determine\nthe neutron star radius with an error ~0.1% (~ 10 m, equivalent to a 1%\ninaccuracy in the crust thickness).\n  The formalism applies to neutron stars with a catalyzed or a fully accreted\ncrust. The difference in the neutron star radius between the two models is\nproportional to the total energy release due to deep crustal heating.\n  For a given model of dense matter describing the neutron star core, the\nradius of a neutron star can be accurately determined independently of the\ncrust model with a precision much better than the ~5% one expected from the\nnext generation of X-ray satellites. This allows to circumvent the problem of\nthe radius uncertainty which may arise when non-unified equations of state for\nthe crust and the core are used. \n\n"}
{"id": "1611.02278", "contents": "Title: The nova-like nebular optical spectrum of V404 Cygni at the beginning of\n  the 2015 outburst decay Abstract: We report on FORS2 optical spectroscopy of the black hole X-ray binary V404\nCygni, performed at the very beginning of its 2015 outburst decay, complemented\nby quasi-simultaneous $Swift$ X-ray and ultra-violet as well as REM\nnear-infrared observations. Its peculiar spectrum is dominated by a wealth of\nemission signatures of HI, HeI, and higher ionisation species, in particular\nFeII. The spectral features are divided between broad red-shifted and narrow\nstationary varieties, the latter being emitted in the outer regions. Continuum\nand line variability at short time scale is high and we find Baldwin\neffect-like anti-correlations between the full-widths at half-maximum and\nequivalent widths of the broad lines with their local continua. The Balmer\ndecrement H{\\alpha}/H{\\beta} is also abnormally large at $4.61\\pm0.62$. We\nargue that these properties hint at the broad lines being optically thick and\narising within a circumbinary component in which shocks between faster\noptically thick and slower optically thin regions may occur. We associate it to\na nova-like nebula formed by the cooling remnant of strong accretion disc winds\nthat turned off when the mass-accretion rate dropped following the last major\nflare. The FeII lines likely arise from the overlap region between this nebula\nand the companion star winds, whereas we favour the shocks within the nebula as\nresponsible for the optical continuum via self-absorbed optically thin\nbremsstrahlung. The presence of a near-infrared excess also points towards the\ncontribution of a strongly variable compact jet or a dusty component. \n\n"}
{"id": "1611.05732", "contents": "Title: Revisiting the Amati and Yonetoku Correlations with Swift GRBs Abstract: We use a sample of \\textit{Swift} gamma-ray bursts (GRBs) to analyze the\nAmati and Yonetoku correlations. The first relation is between $E_{p,i}$, the\nintrinsic peak energy of the prompt GRB emission, and $E_{iso}$, the equivalent\nisotropic energy. The second relation is between $E_{p,i}$ and $L_{iso}$, the\nisotropic peak luminosity. We select a sample of 71 \\textit{Swift} GRBs that\nhave a measured redshift and whose observed $E^{obs}_p$ is within the interval\nof energy 15-150 keV with a relative uncertainty of less than 70\\%. We seek to\nfind correlation relations for long-duration GRBs (LGRBs) with a peak photon\nflux $P_{ph}\\geq 2.6~ \\mathrm{ph/cm^{2}/s}$. Uncertainties (error bars) on the\nvalues of the calculated energy flux \\textit{P}, the energy $E_{iso}$, and the\npeak isotropic luminosity $L_{iso}$ are estimated using a Monte Carlo approach.\nWe find 27 \\textit{Swift} LGRBs that satisfy all our constraints. Results of\nour analyses of the sample of 71 GRBs and the selected subsample (27 GRBs) are\nin good agreement with published results. The plots of the two relations for\nall bursts show a large dispersion around the best straight lines in the sample\nof 71 LGRBs but not so much in the subsample of 27 GRBs. \n\n"}
{"id": "1611.07682", "contents": "Title: Special cases of the quadratic shortest path problem Abstract: The quadratic shortest path problem (QSPP) is \\textcolor{black}{the problem\nof finding a path with prespecified start vertex $s$ and end vertex $t$ in a\ndigraph} such that the sum of weights of arcs and the sum of interaction costs\nover all pairs of arcs on the path is minimized. We first consider a variant of\nthe QSPP known as the adjacent QSPP. It was recently proven that the adjacent\nQSPP on cyclic digraphs cannot be approximated unless P=NP. Here, we give a\nsimple proof for the same result.\n  We also show that if the quadratic cost matrix is a symmetric weak sum matrix\n\\textcolor{black}{ and all $s$-$t$ paths have the same length,} then an optimal\nsolution for the QSPP can be obtained by solving the corresponding instance of\nthe shortest path problem. Similarly, it is shown that the QSPP with a\nsymmetric product cost matrix is solvable in polynomial time.\n  Further, we provide sufficient and necessary conditions for a QSPP instance\non a complete symmetric digraph with four vertices to be linearizable. We also\ncharacterize linearizable QSPP instances on complete symmetric digraphs with\nmore than four vertices. Finally, we derive an algorithm that examines whether\na QSPP instance on the directed grid graph $G_{pq}$ ($p,q\\geq 2$) is\nlinearizable. The complexity of this algorithm is\n${\\mathcal{O}(p^{3}q^{2}+p^{2}q^{3})}$. \n\n"}
{"id": "1611.08898", "contents": "Title: On the Size of Lempel-Ziv and Lyndon Factorizations Abstract: Lyndon factorization and Lempel-Ziv (LZ) factorization are both important\ntools for analysing the structure and complexity of strings, but their\ncombinatorial structure is very different. In this paper, we establish the\nfirst direct connection between the two by showing that while the Lyndon\nfactorization can be bigger than the non-overlapping LZ factorization (which we\ndemonstrate by describing a new, non-trivial family of strings) it is never\nmore than twice the size. \n\n"}
{"id": "1612.00958", "contents": "Title: Reconfiguring Ordered Bases of a Matroid Abstract: For a matroid with an ordered (or \"labelled\") basis, a basis exchange step\nremoves one element with label $l$ and replaces it by a new element that\nresults in a new basis, and with the new element assigned label $l$. We prove\nthat one labelled basis can be reconfigured to another if and only if for every\nlabel, the initial and final elements with that label lie in the same connected\ncomponent of the matroid. Furthermore, we prove that when the reconfiguration\nis possible, the number of basis exchange steps required is $O(r^{1.5})$ for a\nrank $r$ matroid. For a graphic matroid we improve the bound to $O(r \\log r)$. \n\n"}
{"id": "1612.02788", "contents": "Title: Faster Space-Efficient Algorithms for Subset Sum, k-Sum and Related\n  Problems Abstract: We present space efficient Monte Carlo algorithms that solve Subset Sum and\nKnapsack instances with $n$ items using $O^*(2^{0.86n})$ time and polynomial\nspace, where the $O^*(\\cdot)$ notation suppresses factors polynomial in the\ninput size. Both algorithms assume random read-only access to random bits.\nModulo this mild assumption, this resolves a long-standing open problem in\nexact algorithms for NP-hard problems. These results can be extended to solve\nBinary Linear Programming on $n$ variables with few constraints in a similar\nrunning time. We also show that for any constant $k\\geq 2$, random instances of\n$k$-Sum can be solved using $O(n^{k-0.5}polylog(n))$ time and $O(\\log n)$\nspace, without the assumption of random access to random bits.\n  Underlying these results is an algorithm that determines whether two given\nlists of length $n$ with integers bounded by a polynomial in $n$ share a common\nvalue. Assuming random read-only access to random bits, we show that this\nproblem can be solved using $O(\\log n)$ space significantly faster than the\ntrivial $O(n^2)$ time algorithm if no value occurs too often in the same list. \n\n"}
{"id": "1612.03456", "contents": "Title: Baby-Step Giant-Step Algorithms for the Symmetric Group Abstract: We study discrete logarithms in the setting of group actions. Suppose that\n$G$ is a group that acts on a set $S$. When $r,s \\in S$, a solution $g \\in G$\nto $r^g = s$ can be thought of as a kind of logarithm. In this paper, we study\nthe case where $G = S_n$, and develop analogs to the Shanks baby-step /\ngiant-step procedure for ordinary discrete logarithms. Specifically, we compute\ntwo sets $A, B \\subseteq S_n$ such that every permutation of $S_n$ can be\nwritten as a product $ab$ of elements $a \\in A$ and $b \\in B$. Our\ndeterministic procedure is optimal up to constant factors, in the sense that\n$A$ and $B$ can be computed in optimal asymptotic complexity, and $|A|$ and\n$|B|$ are a small constant from $\\sqrt{n!}$ in size. We also analyze randomized\n\"collision\" algorithms for the same problem. \n\n"}
{"id": "1612.04383", "contents": "Title: R-parity as a residual gauge symmetry : probing a theory of cosmological\n  dark matter Abstract: We present a non-supersymmetric scenario in which the R-parity symmetry $R_P\n= (-1)^{3(B-L)+2s}$ arises as a result of spontaneous gauge symmetry breaking,\nleading to a viable Dirac fermion WIMP dark matter candidate. Direct detection\nin nuclear recoil experiments probes dark matter masses around $2-5$ TeV for\n$M_{Z^{\\prime}} \\sim 3-4$ TeV consistent with searches at the LHC, while lepton\nflavor violation rates and flavor changing neutral currents in neutral meson\nsystems lie within reach of upcoming experiments. \n\n"}
{"id": "1612.04473", "contents": "Title: Radio detection of air showers with the ARIANNA experiment on the Ross\n  Ice Shelf Abstract: The ARIANNA hexagonal radio array (HRA) is an experiment in its pilot phase\ndesigned to detect cosmogenic neutrinos of energies above 10^16 eV. The most\nneutrino-like background stems from the radio emission of air showers. This\narticle reports on dedicated efforts of simulating and detecting the signals of\ncosmic rays. A description of the fully radio self-triggered data-set, the\nproperties of the detected air shower signals in the frequency range of\n\\unit[100-500]{MHz} and the consequences for neutrino detection are given. 38\nair shower signals are identified by their distinct waveform characteristics,\nare in good agreement with simulations and their signals provide evidence that\nneutrino-induced radio signals will be distinguishable with high efficiency in\nARIANNA. The cosmic ray flux at a mean energy of\n$6.5^{+1.2}_{-1.0}\\times10^{17}$ eV is measured to be\n$1.1^{+1.0}_{-0.7}\\times10^{-16}$ eV$^{-1}$km$^{-2}$sr$^{-1}$yr$^{-1}$ and one\nfive-fold coincident event is used to illustrate the capabilities of the\nARIANNA detector to reconstruct arrival direction and energy of air showers. \n\n"}
{"id": "1612.05588", "contents": "Title: Collective excitations of a quantized vortex in $^3P_2$ superfluids in\n  neutron stars Abstract: We discuss collective excitations (both fundamental and solitonic\nexcitations) of quantized superfluid vortices in neutron $^3P_2$ superfluids,\nwhich likely exist in high density neutron matter such as neutron stars.\nBesides the well-known Kelvin modes (translational zero modes), we find a\ngapfull mode whose low-energy description takes the simple form of a double\nsine-Gordon model. The associated kink solution and its effects on spontaneous\nmagnetization inside the vortex core are analyzed in detail. \n\n"}
{"id": "1612.05665", "contents": "Title: PAM: Parallel Augmented Maps Abstract: Ordered (key-value) maps are an important and widely-used data type for\nlarge-scale data processing frameworks. Beyond simple search, insertion and\ndeletion, more advanced operations such as range extraction, filtering, and\nbulk updates form a critical part of these frameworks.\n  We describe an interface for ordered maps that is augmented to support fast\nrange queries and sums, and introduce a parallel and concurrent library called\nPAM (Parallel Augmented Maps) that implements the interface. The interface\nincludes a wide variety of functions on augmented maps ranging from basic\ninsertion and deletion to more interesting functions such as union,\nintersection, filtering, extracting ranges, splitting, and range-sums. We\ndescribe algorithms for these functions that are efficient both in theory and\npractice.\n  As examples of the use of the interface and the performance of PAM, we apply\nthe library to four applications: simple range sums, interval trees, 2D range\ntrees, and ranked word index searching. The interface greatly simplifies the\nimplementation of these data structures over direct implementations.\nSequentially the code achieves performance that matches or exceeds existing\nlibraries designed specially for a single application, and in parallel our\nimplementation gets speedups ranging from 40 to 90 on 72 cores with 2-way\nhyperthreading. \n\n"}
{"id": "1612.05697", "contents": "Title: Testing the axion-conversion hypothesis of 3.5 keV emission with\n  polarization Abstract: The recently measured 3.5 keV line in a number of galaxy clusters, the\nAndromeda galaxy (M31) and the Milky Way (MW) center can be well accounted for\nby a scenario in which dark matter decays to axion-like particles (ALPs) and\nsubsequently convert to 3.5 keV photons in magnetic fields of galaxy clusters\nor galaxies. We propose to test this hypothesis by performing X-ray\npolarization measurements. Since ALPs can only couple to photons with\npolarization orientation parallel to magnetic field, we can confirm or reject\nthis model by measuring the polarization of 3.5 keV line and comparing it to\nthe orientation of magnetic field. We discuss luminosity and polarization\nmeasurements for both galaxy cluster and spiral galaxy, and provide a general\nrelation between polarization and galaxy inclination angle. This effect is\nmarginally detectable with X-ray polarimetry detectors currently under\ndevelopment, such as the enhanced X-ray Timing and Polarization (eXTP), the\nImaging X-ray Polarimetry Explorer (IXPE) and the X-ray Imaging Polarimetry\nExplorer (XIPE). The sensitivity can be further improved in the future with\ndetectors of larger effective area or better energy resolutions. \n\n"}
{"id": "1612.05832", "contents": "Title: Implementations and the independent set polynomial below the Shearer\n  threshold Abstract: The independent set polynomial is important in many areas. For every integer\n$\\Delta\\geq 2$, the Shearer threshold is the value\n$\\lambda^*(\\Delta)=(\\Delta-1)^{\\Delta-1}/\\Delta^{\\Delta}$ . It is known that\nfor $\\lambda < - \\lambda^*(\\Delta)$, there are graphs~$G$ with maximum\ndegree~$\\Delta$ whose independent set polynomial, evaluated at~$\\lambda$, is at\nmost~$0$. Also, there are no such graphs for any $\\lambda >\n-\\lambda^*(\\Delta)$. This paper is motivated by the computational problem of\napproximating the independent set polynomial when $\\lambda < -\n\\lambda^*(\\Delta)$. The key issue in complexity bounds for this problem is\n\"implementation\". Informally, an implementation of a real number $\\lambda'$ is\na graph whose hard-core partition function, evaluated at~$\\lambda$, simulates a\nvertex-weight of~$\\lambda'$ in the sense that $\\lambda'$ is the ratio between\nthe contribution to the partition function from independent sets containing a\ncertain vertex and the contribution from independent sets that do not contain\nthat vertex. Implementations are the cornerstone of intractability results for\nthe problem of approximately evaluating the independent set polynomial. Our\nmain result is that, for any $\\lambda < - \\lambda^*(\\Delta)$, it is possible to\nimplement a set of values that is dense over the reals. The result is tight in\nthe sense that it is not possible to implement a set of values that is dense\nover the reals for any $\\lambda> \\lambda^*(\\Delta)$. Our result has already\nbeen used in a paper with \\bezakova{} (STOC 2018) to show that it is \\#P-hard\nto approximate the evaluation of the independent set polynomial on graphs of\ndegree at most~$\\Delta$ at any value $\\lambda<-\\lambda^*(\\Delta)$. In the\nappendix, we give an additional incomparable inapproximability result\n(strengthening the inapproximability bound to an exponential factor, but\nweakening the hardness to NP-hardness). \n\n"}
{"id": "1612.06057", "contents": "Title: Similarity preserving compressions of high dimensional sparse data Abstract: The rise of internet has resulted in an explosion of data consisting of\nmillions of articles, images, songs, and videos. Most of this data is high\ndimensional and sparse. The need to perform an efficient search for similar\nobjects in such high dimensional big datasets is becoming increasingly common.\nEven with the rapid growth in computing power, the brute-force search for such\na task is impractical and at times impossible. Therefore it is quite natural to\ninvestigate the techniques that compress the dimension of the data-set while\npreserving the similarity between data objects.\n  In this work, we propose an efficient compression scheme mapping binary\nvectors into binary vectors and simultaneously preserving Hamming distance and\nInner Product. The length of our compression depends only on the sparsity and\nis independent of the dimension of the data. Moreover our schemes provide\none-shot solution for Hamming distance and Inner Product, and work in the\nstreaming setting as well. In contrast with the \"local projection\" strategies\nused by most of the previous schemes, our scheme combines (using sparsity) the\nfollowing two strategies: $1.$ Partitioning the dimensions into several\nbuckets, $2.$ Then obtaining \"global linear summaries\" in each of these\nbuckets. We generalize our scheme for real-valued data and obtain compressions\nfor Euclidean distance, Inner Product, and $k$-way Inner Product. \n\n"}
{"id": "1612.07083", "contents": "Title: Search for transitions between states in redbacks and black widows using\n  seven years of Fermi-LAT observations Abstract: Considering about seven years of Fermi-Large Area Telescope (LAT) data, we\npresent a systematic search for variability possibly related to transitions\nbetween states in redbacks and black widow systems. Transitions are\ncharacterized by sudden and significant changes in the gamma-ray flux that\npersist on a timescale much larger than the orbital period. This phenomenology\nwas already detected in the case of two redback systems, PSR J1023+0038 and PSR\nJ1227-4853, for which we present here a dedicated study. We show the existence\nof only one transition for each of these systems over the past seven years. We\ndetermine their spectra, establishing high-energy cutoffs at a few GeV for the\nhigh gamma-ray state of PSR J1023+0038 and for both states of PSR J1227-4853.\nThe surveying capability of the Fermi-LAT allows studying whether similar\nphenomenology has occurred in other sources. Although we have not found any\nhint for a state transition for most of the studied pulsars, we note two\nblack-widow systems, PSR J2234+0944 and PSR J1446-4701, whose apparent\nvariability is reminiscent of the transitions in PSR J1023+0038 and PSR\nJ1227-4853. For the other systems we set limits on potential transitions in\ntheir measured gamma-ray light curves. \n\n"}
{"id": "1612.07162", "contents": "Title: Supercritical Space-Width Trade-offs for Resolution Abstract: We show that there are CNF formulas which can be refuted in resolution in\nboth small space and small width, but for which any small-width proof must have\nspace exceeding by far the linear worst-case upper bound. This significantly\nstrengthens the space-width trade-offs in [Ben-Sasson '09]}, and provides one\nmore example of trade-offs in the \"supercritical\" regime above worst case\nrecently identified by [Razborov '16]. We obtain our results by using\nRazborov's new hardness condensation technique and combining it with the space\nlower bounds in [Ben-Sasson and Nordstrom '08]. \n\n"}
{"id": "1612.07710", "contents": "Title: Set Similarity Search Beyond MinHash Abstract: We consider the problem of approximate set similarity search under\nBraun-Blanquet similarity $B(\\mathbf{x}, \\mathbf{y}) = |\\mathbf{x} \\cap\n\\mathbf{y}| / \\max(|\\mathbf{x}|, |\\mathbf{y}|)$. The $(b_2, b_2)$-approximate\nBraun-Blanquet similarity search problem is to preprocess a collection of sets\n$P$ such that, given a query set $\\mathbf{q}$, if there exists $\\mathbf{x} \\in\nP$ with $B(\\mathbf{q}, \\mathbf{x}) \\geq b_1$, then we can efficiently return\n$\\mathbf{x}' \\in P$ with $B(\\mathbf{q}, \\mathbf{x}') > b_2$.\n  We present a simple data structure that solves this problem with space usage\n$O(n^{1+\\rho}\\log n + \\sum_{\\mathbf{x} \\in P}|\\mathbf{x}|)$ and query time\n$O(|\\mathbf{q}|n^{\\rho} \\log n)$ where $n = |P|$ and $\\rho =\n\\log(1/b_1)/\\log(1/b_2)$. Making use of existing lower bounds for\nlocality-sensitive hashing by O'Donnell et al. (TOCT 2014) we show that this\nvalue of $\\rho$ is tight across the parameter space, i.e., for every choice of\nconstants $0 < b_2 < b_1 < 1$.\n  In the case where all sets have the same size our solution strictly improves\nupon the value of $\\rho$ that can be obtained through the use of\nstate-of-the-art data-independent techniques in the Indyk-Motwani\nlocality-sensitive hashing framework (STOC 1998) such as Broder's MinHash (CCS\n1997) for Jaccard similarity and Andoni et al.'s cross-polytope LSH (NIPS 2015)\nfor cosine similarity. Surprisingly, even though our solution is\ndata-independent, for a large part of the parameter space we outperform the\ncurrently best data-dependent method by Andoni and Razenshteyn (STOC 2015). \n\n"}
{"id": "1612.08447", "contents": "Title: Higher-order organization of complex networks Abstract: Networks are a fundamental tool for understanding and modeling complex\nsystems in physics, biology, neuroscience, engineering, and social science.\nMany networks are known to exhibit rich, lower-order connectivity patterns that\ncan be captured at the level of individual nodes and edges. However,\nhigher-order organization of complex networks---at the level of small network\nsubgraphs---remains largely unknown. Here we develop a generalized framework\nfor clustering networks based on higher-order connectivity patterns. This\nframework provides mathematical guarantees on the optimality of obtained\nclusters and scales to networks with billions of edges. The framework reveals\nhigher-order organization in a number of networks including information\npropagation units in neuronal networks and hub structure in transportation\nnetworks. Results show that networks exhibit rich higher-order organizational\nstructures that are exposed by clustering based on higher-order connectivity\npatterns. \n\n"}
{"id": "1701.00105", "contents": "Title: Electron Acceleration Mechanisms in Thunderstorms Abstract: Thunderstorms produce strong electric fields over regions on the order of\nkilometer. The corresponding electric potential differences are on the order of\n100 MV. Secondary cosmic rays reaching these regions may be significantly\naccelerated and even amplified in relativistic runaway avalanche processes.\nThese phenomena lead to enhancements of the high-energy background radiation\nobserved by detectors on the ground and on board aircraft. Moreover, intense\nsubmillisecond gamma-ray bursts named terrestrial gamma-ray flashes (TGFs)\nproduced in thunderstorms are detected from low Earth orbit satellites. When\npassing through the atmosphere, these gamma-rays are recognized to produce\nsecondary relativistic electrons and positrons rapidly trapped in the\ngeomagnetic field and injected into the near-Earth space environment. In the\npresent work, we attempt to give an overview of the current state of research\non high-energy phenomena associated with thunderstorms. \n\n"}
{"id": "1701.00948", "contents": "Title: Abelian-Square-Rich Words Abstract: An abelian square is the concatenation of two words that are anagrams of one\nanother. A word of length $n$ can contain at most $\\Theta(n^2)$ distinct\nfactors, and there exist words of length $n$ containing $\\Theta(n^2)$ distinct\nabelian-square factors, that is, distinct factors that are abelian squares.\nThis motivates us to study infinite words such that the number of distinct\nabelian-square factors of length $n$ grows quadratically with $n$. More\nprecisely, we say that an infinite word $w$ is {\\it abelian-square-rich} if,\nfor every $n$, every factor of $w$ of length $n$ contains, on average, a number\nof distinct abelian-square factors that is quadratic in $n$; and {\\it uniformly\nabelian-square-rich} if every factor of $w$ contains a number of distinct\nabelian-square factors that is proportional to the square of its length. Of\ncourse, if a word is uniformly abelian-square-rich, then it is\nabelian-square-rich, but we show that the converse is not true in general. We\nprove that the Thue-Morse word is uniformly abelian-square-rich and that the\nfunction counting the number of distinct abelian-square factors of length $2n$\nof the Thue-Morse word is $2$-regular. As for Sturmian words, we prove that a\nSturmian word $s_{\\alpha}$ of angle $\\alpha$ is uniformly abelian-square-rich\nif and only if the irrational $\\alpha$ has bounded partial quotients, that is,\nif and only if $s_{\\alpha}$ has bounded exponent. \n\n"}
{"id": "1701.02312", "contents": "Title: The chemical enrichment of long-GRB nurseries up to z=2 Abstract: We investigate the existence of a metallicity threshold for the production of\nlong gamma-ray bursts (LGRBs). We used the host galaxies of the Swift/BAT6\nsample of LGRBs. We considered the stellar mass, star formation rate (SFR), and\nmetallicity determined from the host galaxy photometry and spectroscopy up to z\n= 2 and used them to compare the distribution of host galaxies to that of field\ngalaxies in the mass-metallicity and fundamental metallicity relation plane. We\nfind that although LGRBs also form in galaxies with relatively large stellar\nmasses, the large majority of host galaxies have metallicities below\nlog(O=H)~8.6. The extension to z = 2 results in a good sampling of stellar\nmasses also above Log(Mstar/Msun)~9.5 and provides evidence that LGRB host\ngalaxies do not follow the fundamental metallicity relation. As shown by the\ncomparison with dedicated numerical simulations of LGRB host galaxy population,\nthese results are naturally explained by the existence of a mild (~0.7 Zsun)\nthreshold for the LGRB formation. The present statistics does not allow us to\ndiscriminate between different shapes of the metallicity cutoff, but the\nrelatively high metallicity threshold found in this work is somewhat in\ndisagreement to most of the standard single-star models for LGRB progenitors. \n\n"}
{"id": "1701.02370", "contents": "Title: Millisecond Magnetar Birth Connects FRB 121102 to Superluminous\n  Supernovae and Long Duration Gamma-ray Bursts Abstract: Sub-arcsecond localization of the repeating fast radio burst FRB 121102\nrevealed its coincidence with a dwarf host galaxy and a steady (`quiescent')\nnon-thermal radio source. We show that the properties of the host galaxy are\nconsistent with those of long-duration gamma-ray bursts (LGRB) and\nhydrogen-poor superluminous supernovae (SLSNe-I). Both LGRBs and SLSNe-I were\npreviously hypothesized to be powered by the electromagnetic spin-down of\nnewly-formed, strongly-magnetized neutron stars with millisecond birth rotation\nperiods (`millisecond magnetars'). This motivates considering a scenario\nwhereby the repeated bursts from FRB 121102 originate from a young magnetar\nremnant embedded within a young hydrogen-poor supernova remnant. Requirements\non the GHz free-free optical depth through the expanding supernova ejecta\n(accounting for photo-ionization by the rotationally-powered magnetar nebula),\nenergetic constraints on the bursts, and constraints on the size of the\nquiescent source all point to an age of less than a few decades to a century.\nThe quiescent radio source can be attributed to synchrotron emission from the\nshock interaction between the fast outer layer of the supernova ejecta with the\nsurrounding wind of the progenitor star, or from deeper within the magnetar\nwind nebula. Alternatively, the radio emission could be an orphan afterglow\nfrom an initially off-axis LGRB jet, though this might require the source to be\ntoo young. The young age of the source can be tested by searching for a time\nderivative of the dispersion measure and predicted fading of the quiescent\nradio source. We propose future tests of the SLSNe-I/LGRB/FRB connection, such\nas searches for FRBs from nearby SLSNe-I/LGRB on timescales of decades after\ntheir explosions. \n\n"}
{"id": "1701.02692", "contents": "Title: Von Neumann Regular Cellular Automata Abstract: For any group $G$ and any set $A$, a cellular automaton (CA) is a\ntransformation of the configuration space $A^G$ defined via a finite memory set\nand a local function. Let $\\text{CA}(G;A)$ be the monoid of all CA over $A^G$.\nIn this paper, we investigate a generalisation of the inverse of a CA from the\nsemigroup-theoretic perspective. An element $\\tau \\in \\text{CA}(G;A)$ is von\nNeumann regular (or simply regular) if there exists $\\sigma \\in \\text{CA}(G;A)$\nsuch that $\\tau \\circ \\sigma \\circ \\tau = \\tau$ and $\\sigma \\circ \\tau \\circ\n\\sigma = \\sigma$, where $\\circ$ is the composition of functions. Such an\nelement $\\sigma$ is called a generalised inverse of $\\tau$. The monoid\n$\\text{CA}(G;A)$ itself is regular if all its elements are regular. We\nestablish that $\\text{CA}(G;A)$ is regular if and only if $\\vert G \\vert = 1$\nor $\\vert A \\vert = 1$, and we characterise all regular elements in\n$\\text{CA}(G;A)$ when $G$ and $A$ are both finite. Furthermore, we study\nregular linear CA when $A= V$ is a vector space over a field $\\mathbb{F}$; in\nparticular, we show that every regular linear CA is invertible when $G$ is\ntorsion-free elementary amenable (e.g. when $G=\\mathbb{Z}^d, \\ d \\in\n\\mathbb{N}$) and $V=\\mathbb{F}$, and that every linear CA is regular when $V$\nis finite-dimensional and $G$ is locally finite with $\\text{Char}(\\mathbb{F})\n\\nmid o(g)$ for all $g \\in G$. \n\n"}
{"id": "1701.02752", "contents": "Title: Equation of State Effects on Gravitational Waves from Rotating Core\n  Collapse Abstract: Gravitational waves (GWs) generated by axisymmetric rotating collapse,\nbounce, and early postbounce phases of a galactic core-collapse supernova will\nbe detectable by current-generation gravitational wave observatories. Since\nthese GWs are emitted from the quadrupole-deformed nuclear-density core, they\nmay encode information on the uncertain nuclear equation of state (EOS). We\nexamine the effects of the nuclear EOS on GWs from rotating core collapse and\ncarry out 1824 axisymmetric general-relativistic hydrodynamic simulations that\ncover a parameter space of 98 different rotation profiles and 18 different EOS.\nWe show that the bounce GW signal is largely independent of the EOS and\nsensitive primarily to the ratio of rotational to gravitational energy, and at\nhigh rotation rates, to the degree of differential rotation. The GW frequency\nof postbounce core oscillations shows stronger EOS dependence that can be\nparameterized by the core's EOS-dependent dynamical frequency\n$\\sqrt{G\\bar{\\rho}_c}$. We find that the ratio of the peak frequency to the\ndynamical frequency follows a universal trend that is obeyed by all EOS and\nrotation profiles and that indicates that the nature of the core oscillations\nchanges when the rotation rate exceeds the dynamical frequency. We find that\ndifferences in the treatments of low-density nonuniform nuclear matter, of the\ntransition from nonuniform to uniform nuclear matter, and in the description of\nnuclear matter up to around twice saturation density can mildly affect the GW\nsignal. We find that approximations and uncertainties in electron capture rates\ncan lead to variations in the GW signal that are of comparable magnitude to\nthose due to different nuclear EOS. This emphasizes the need for reliable\nnuclear electron capture rates and for self-consistent multi-dimensional\nneutrino radiation-hydrodynamic simulations of rotating core collapse. \n\n"}
{"id": "1701.03414", "contents": "Title: On Efficient Domination for Some Classes of $H$-Free Chordal Graphs Abstract: A vertex set $D$ in a finite undirected graph $G$ is an efficient dominating\nset (e.d.s. for short) of $G$ if every vertex of $G$ is dominated by exactly\none vertex of $D$. The Efficient Domination (ED) problem, which asks for the\nexistence of an e.d.s.\\ in $G$, is known to be \\NP-complete even for very\nrestricted graph classes such as for $2P_3$-free chordal graphs while it is\nsolvable in polynomial time for $P_6$-free chordal graphs (and even for\n$P_6$-free graphs). A standard reduction from the \\NP-complete Exact Cover\nproblem shows that ED is \\NP-complete for a very special subclass of chordal\ngraphs generalizing split graphs. The reduction implies that ED is \\NP-complete\ne.g.\\ for double-gem-free chordal graphs while it is solvable in linear time\nfor gem-free chordal graphs (by various reasons such as bounded clique-width,\ndistance-hereditary graphs, chordal square etc.), and ED is \\NP-complete for\nbutterfly-free chordal graphs while it is solvable in linear time for\n$2P_2$-free graphs.\n  We show that (weighted) ED can be solved in polynomial time for $H$-free\nchordal graphs when $H$ is net, extended gem, or $S_{1,2,3}$. \n\n"}
{"id": "1701.03758", "contents": "Title: Counting triangles, tunable clustering and the small-world property in\n  random key graphs (Extended version) Abstract: Random key graphs were introduced to study various properties of the\nEschenauer-Gligor key predistribution scheme for wireless sensor networks\n(WSNs). Recently this class of random graphs has received much attention in\ncontexts as diverse as recommender systems, social network modeling, and\nclustering and classification analysis. This paper is devoted to analyzing\nvarious properties of random key graphs. In particular, we establish a zero-one\nlaw for the the existence of triangles in random key graphs, and identify the\ncorresponding critical scaling. This zero-one law exhibits significant\ndifferences with the corresponding result in Erdos-Renyi (ER) graphs. We also\ncompute the clustering coefficient of random key graphs, and compare it to that\nof ER graphs in the many node regime when their expected average degrees are\nasymptotically equivalent. For the parameter range of practical relevance in\nboth wireless sensor network and social network applications, random key graphs\nare shown to be much more clustered than the corresponding ER graphs. We also\nexplore the suitability of random key graphs as small world models in the sense\nof Watts and Strogatz. \n\n"}
{"id": "1701.04148", "contents": "Title: SF-sketch: A Two-stage Sketch for Data Streams Abstract: A sketch is a probabilistic data structure used to record frequencies of\nitems in a multi-set. Sketches are widely used in various fields, especially\nthose that involve processing and storing data streams. In streaming\napplications with high data rates, a sketch \"fills up\" very quickly. Thus, its\ncontents are periodically transferred to the remote collector, which is\nresponsible for answering queries. In this paper, we propose a new sketch,\ncalled Slim-Fat (SF) sketch, which has a significantly higher accuracy compared\nto prior art, a much smaller memory footprint, and at the same time achieves\nthe same speed as the best prior sketch. The key idea behind our proposed\nSF-sketch is to maintain two separate sketches: a small sketch called\nSlim-subsketch and a large sketch called Fat-subsketch. The Slim-subsketch is\nperiodically transferred to the remote collector for answering queries quickly\nand accurately. The Fat-subsketch, however, is not transferred to the remote\ncollector because it is used only to assist the Slim-subsketch during the\ninsertions and deletions and is not used to answer queries. We implemented and\nextensively evaluated SF-sketch along with several prior sketches and compared\nthem side by side. Our experimental results show that SF-sketch outperforms the\nmost widely used CM-sketch by up to 33.1 times in terms of accuracy. We have\nreleased the source codes of our proposed sketch as well as existing sketches\nat Github. The short version of this paper will appear in ICDE 2017. \n\n"}
{"id": "1701.04866", "contents": "Title: Cosmic Ray Antiprotons at High Energies Abstract: Cosmic ray antiprotons provide a powerful tool to probe dark matter\nannihilations in our galaxy. The sensitivity of this important channel is,\nhowever, diluted by sizable uncertainties in the secondary antiproton\nbackground. In this work, we improve the calculation of secondary antiproton\nproduction with a particular focus on the high energy regime. We employ the\nmost recent collider data and identify a substantial increase of antiproton\ncross sections with energy. This increase is driven by the violation of Feynman\nscaling as well as by an enhanced strange hyperon production. The updated\nantiproton production cross sections are made publicly available for\nindependent use in cosmic ray studies. In addition, we provide the correlation\nmatrix of cross section uncertainties for the AMS-02 experiment. At high\nenergies, the new cross sections improve the compatibility of the AMS-02 data\nwith a pure secondary origin of antiprotons in cosmic rays. \n\n"}
{"id": "1701.06985", "contents": "Title: Fine-Grained Parameterized Complexity Analysis of Graph Coloring\n  Problems Abstract: The $q$-Coloring problem asks whether the vertices of a graph can be properly\ncolored with $q$ colors. Lokshtanov et al. [SODA 2011] showed that $q$-Coloring\non graphs with a feedback vertex set of size $k$ cannot be solved in time\n$\\mathcal{O}^*((q-\\varepsilon)^k)$, for any $\\varepsilon > 0$, unless the\nStrong Exponential-Time Hypothesis (SETH) fails. In this paper we perform a\nfine-grained analysis of the complexity of $q$-Coloring with respect to a\nhierarchy of parameters. We show that even when parameterized by the vertex\ncover number, $q$ must appear in the base of the exponent: Unless ETH fails,\nthere is no universal constant $\\theta$ such that $q$-Coloring parameterized by\nvertex cover can be solved in time $\\mathcal{O}^*(\\theta^k)$ for all fixed $q$.\nWe apply a method due to Jansen and Kratsch [Inform. & Comput. 2013] to prove\nthat there are $\\mathcal{O}^*((q - \\varepsilon)^k)$ time algorithms where $k$\nis the vertex deletion distance to several graph classes $\\mathcal{F}$ for\nwhich $q$-Coloring is known to be solvable in polynomial time. We generalize\nearlier ad-hoc results by showing that if $\\mathcal{F}$ is a class of graphs\nwhose $(q+1)$-colorable members have bounded treedepth, then there exists some\n$\\varepsilon > 0$ such that $q$-Coloring can be solved in time\n$\\mathcal{O}^*((q-\\varepsilon)^k)$ when parameterized by the size of a given\nmodulator to $\\mathcal{F}$. In contrast, we prove that if $\\mathcal{F}$ is the\nclass of paths - some of the simplest graphs of unbounded treedepth - then no\nsuch algorithm can exist unless SETH fails. \n\n"}
{"id": "1701.07204", "contents": "Title: Fast Exact k-Means, k-Medians and Bregman Divergence Clustering in 1D Abstract: The $k$-Means clustering problem on $n$ points is NP-Hard for any dimension\n$d\\ge 2$, however, for the 1D case there exists exact polynomial time\nalgorithms. Previous literature reported an $O(kn^2)$ time dynamic programming\nalgorithm that uses $O(kn)$ space. It turns out that the problem has been\nconsidered under a different name more than twenty years ago. We present all\nthe existing work that had been overlooked and compare the various solutions\ntheoretically. Moreover, we show how to reduce the space usage for some of\nthem, as well as generalize them to data structures that can quickly report an\noptimal $k$-Means clustering for any $k$. Finally we also generalize all the\nalgorithms to work for the absolute distance and to work for any Bregman\nDivergence. We complement our theoretical contributions by experiments that\ncompare the practical performance of the various algorithms. \n\n"}
{"id": "1701.07289", "contents": "Title: Combined Magnetohydrodynamic- Monte Carlo Simulations of Proton\n  Acceleration in Colliding Wind Binaries Abstract: The interaction between the strong winds of the stars in colliding-wind\nbinary (CWB) systems produces two shock fronts, delimiting the wind collision\nregion (WCR). There, particles are expected to be accelerated mainly via\ndiffusive shock acceleration, and to produce $\\gamma$-rays, in processes\ninvolving relativistic electrons and/or protons. We investigate the injection\nand the acceleration of protons in typical CWB systems by means of Monte Carlo\nsimulations, with a test-particle approach. We use magnetohydrodynamic\nsimulations to determine the background conditions in the wind collision\nregion. This allows us to consider particle acceleration at both shocks, on\neither side of the WCR, with a self-consistently determined large-scale\nmagnetic field, which has an impact on the shape of the WCR, and the topology\nof which plays an important role in particle acceleration at collisionless\nshocks. Such studies may contribute to improve $\\gamma$-ray flux predictions\nfor CWB systems. \n\n"}
{"id": "1701.08162", "contents": "Title: Low-mass White Dwarfs with Hydrogen Envelopes as a Missing Link in the\n  Tidal Disruption Menu Abstract: We construct a menu of objects that can give rise to bright flares when\ndisrupted by massive black holes (BHs), ranging from planets to evolved stars.\nThrough their tidal disruption, main sequence and evolved stars can effectively\nprobe the existence of otherwise quiescent supermassive BHs and white dwarfs\ncan probe intermediate mass BHs. Many low-mass white dwarfs possess extended\nhydrogen envelopes, which allow for the production of prompt flares in\ndisruptive encounters with moderately massive BHs of $10^5$ to\n$10^7~M_\\odot$--masses that may constitute the majority of massive BHs by\nnumber. These objects are a missing link in two ways: (1) for probing\nmoderately massive BHs and (2) for understanding the hydrodynamics of the\ndisruption of objects with tenuous envelopes. A flare arising from the tidal\ndisruption of a $0.17~M_\\odot$ white dwarf by a $10^5~M_\\odot$ BH reaches a\nmaximum between 0.6 and 11 days, with a peak fallback rate that is usually\nsuper-Eddington and results in a flare that is likely brighter than a typical\ntidal disruption event. Encounters stripping only the envelope can provide\nhydrogen-only fallback, while encounters disrupting the core evolve from H- to\nHe-rich fallback. While most tidal disruption candidates observed thus far are\nconsistent with the disruptions of main sequence stars, the rapid timescales of\nnuclear transients such as Dougie and PTF10iya are naturally explained by the\ndisruption of low-mass white dwarfs. As the number of observed flares continues\nto increase, the menu presented here will be essential for characterizing\nnuclear BHs and their environments through tidal disruptions. \n\n"}
{"id": "1702.00948", "contents": "Title: Ranking vertices for active module recovery problem Abstract: Selecting a connected subnetwork enriched in individually important vertices\nis an approach commonly used in many areas of bioinformatics, including\nanalysis of gene expression data, mutations, metabolomic profiles and others.\nIt can be formulated as a recovery of an active module from which an\nexperimental signal is generated. Commonly, methods for solving this problem\nresult in a single subnetwork that is considered to be a good candidate.\nHowever, it is usually useful to consider not one but multiple candidate\nmodules at different significance threshold levels. Therefore, in this paper we\nsuggest to consider a problem of finding a vertex ranking instead of finding a\nsingle module. We also propose two algorithms for solving this problem: one\nthat we consider to be optimal but computationally expensive for real-world\nnetworks and one that works close to the optimal in practice and is also able\nto work with big networks. \n\n"}
{"id": "1702.01719", "contents": "Title: A 2-Approximation for the Height of Maximal Outerplanar Graph Drawings Abstract: In this paper, we study planar drawings of maximal outerplanar graphs with\nthe objective of achieving small height. A recent paper gave an algorithm for\nsuch drawings that is within a factor of 4 of the optimum height. In this\npaper, we substantially improve the approximation factor to become 2. The main\ningredient is to define a new parameter of outerplanar graphs (the so-called\numbrella depth, obtained by recursively splitting the graph into graphs called\numbrellas). We argue that the height of any poly-line drawing must be at least\nthe umbrella depth, and then devise an algorithm that achieves height at most\ntwice the umbrella depth. \n\n"}
{"id": "1702.03106", "contents": "Title: A Las Vegas approximation algorithm for metric $1$-median selection Abstract: Given an $n$-point metric space, consider the problem of finding a point with\nthe minimum sum of distances to all points. We show that this problem has a\nrandomized algorithm that {\\em always} outputs a $(2+\\epsilon)$-approximate\nsolution in an expected $O(n/\\epsilon^2)$ time for each constant $\\epsilon>0$.\nInheriting Indyk's algorithm, our algorithm outputs a\n$(1+\\epsilon)$-approximate $1$-median in $O(n/\\epsilon^2)$ time with\nprobability $\\Omega(1)$. \n\n"}
{"id": "1702.04536", "contents": "Title: A $(2+\\epsilon)$-Approximation for Maximum Weight Matching in the\n  Semi-Streaming Model Abstract: We present a simple deterministic single-pass $(2+\\epsilon)$-approximation\nalgorithm for the maximum weight matching problem in the semi-streaming model.\nThis improves upon the currently best known approximation ratio of\n$(4+\\epsilon)$.\n  Our algorithm uses $O(n\\log^2 n)$ bits of space for constant values of\n$\\epsilon$. It relies on a variation of the local-ratio theorem, which may be\nof use for other algorithms in the semi-streaming model as well. \n\n"}
{"id": "1702.04765", "contents": "Title: Hot magnetized nuclear matter: Thermodynamic and Saturation Properties Abstract: We have used a realistic nuclear potential, AV18, and a many body technique,\nthe lowest order constraint variational (LOCV) approach, to calculate the\nproperties of hot magnetized nuclear matter. By investigating the free energy,\nspin polarization parameter, and symmetry energy, we have studied the\ntemperature and magnetic field dependence of the saturation properties of\nmagnetized nuclear matter. In addition, we have calculated the equation of\nstate of magnetized nuclear matter at different temperatures and magnetic\nfields. It was found that the flashing temperature of nuclear matter decreases\nby increasing the magnetic field. In addition, we have studied the effect of\nthe magnetic field on liquid gas phase transition of nuclear matter. The liquid\ngas coexistence curves, the order parameter of the liquid gas phase transition,\nand the properties of critical point at different magnetic fields have been\ncalculated. \n\n"}
{"id": "1702.05224", "contents": "Title: Continuous Relaxations for the Traveling Salesman Problem Abstract: In this work, we aim to explore connections between dynamical systems\ntechniques and combinatorial optimization problems. In particular, we construct\nheuristic approaches for the traveling salesman problem (TSP) based on\nembedding the relaxed discrete optimization problem into appropriate manifolds.\nWe explore multiple embedding techniques -- namely, the construction of new\ndynamical systems on the manifold of orthogonal matrices and associated\nProcrustes approximations of the TSP cost function. Using these dynamical\nsystems, we analyze the local neighborhood around the optimal TSP solutions\n(which are equilibria) using computations to approximate the associated\n\\emph{stable manifolds}. We find that these flows frequently converge to\nundesirable equilibria. However, the solutions of the dynamical systems and the\nassociated Procrustes approximation provide an interesting biasing approach for\nthe popular Lin--Kernighan heuristic which yields fast convergence. The\nLin--Kernighan heuristic is typically based on the computation of edges that\nhave a `high probability' of being in the shortest tour, thereby effectively\npruning the search space. Our new approach, instead, relies on a natural\nrelaxation of the combinatorial optimization problem to the manifold of\northogonal matrices and the subsequent use of this solution to bias the\nLin--Kernighan heuristic. Although the initial cost of computing these edges\nusing the Procrustes solution is higher than existing methods, we find that the\nProcrustes solution, when coupled with a homotopy computation, contains\nvaluable information regarding the optimal edges. We explore the Procrustes\nbased approach on several TSP instances and find that our approach often\nrequires fewer $k$-opt moves than existing approaches. Broadly, we hope that\nthis work initiates more work in the intersection of dynamical systems theory\nand combinatorial optimization. \n\n"}
{"id": "1702.06110", "contents": "Title: Density Independent Algorithms for Sparsifying $k$-Step Random Walks Abstract: We give faster algorithms for producing sparse approximations of the\ntransition matrices of $k$-step random walks on undirected, weighted graphs.\nThese transition matrices also form graphs, and arise as intermediate objects\nin a variety of graph algorithms. Our improvements are based on a better\nunderstanding of processes that sample such walks, as well as tighter bounds on\nkey weights underlying these sampling processes. On a graph with $n$ vertices\nand $m$ edges, our algorithm produces a graph with about $n\\log{n}$ edges that\napproximates the $k$-step random walk graph in about $m + n \\log^4{n}$ time. In\norder to obtain this runtime bound, we also revisit \"density independent\"\nalgorithms for sparsifying graphs whose runtime overhead is expressed only in\nterms of the number of vertices. \n\n"}
{"id": "1702.08443", "contents": "Title: Elementary Yet Precise Worst-case Analysis of MergeSort, A short version\n  (SV) Abstract: This paper offers two elementary yet precise derivations of an exact formula\n  \\[ W(n) = \\sum_{i=1} ^{n} \\lceil \\lg i \\rceil = n \\lceil \\lg n \\rceil -\n2^{\\lceil \\lg n \\rceil} + 1 \\] for the maximum number $ W(n) $ of comparisons\nof keys performed by $ {\\tt MergeSort} $ on an $ n $-element array. The first\nof the two, due to its structural regularity, is well worth carefully studying\nin its own right.\n  Close smooth bounds on $ W(n) $ are derived. It seems interesting that $ W(n)\n$ is linear between the points $ n = 2^{\\lfloor \\lg n \\rfloor} $ and it\nlinearly interpolates its own lower bound $ n \\lg n - n + 1 $ between these\npoints. \n\n"}
{"id": "1702.08557", "contents": "Title: Multimodal Clustering for Community Detection Abstract: Multimodal clustering is an unsupervised technique for mining interesting\npatterns in $n$-adic binary relations or $n$-mode networks. Among different\ntypes of such generalized patterns one can find biclusters and formal concepts\n(maximal bicliques) for 2-mode case, triclusters and triconcepts for 3-mode\ncase, closed $n$-sets for $n$-mode case, etc. Object-attribute biclustering\n(OA-biclustering) for mining large binary datatables (formal contexts or 2-mode\nnetworks) arose by the end of the last decade due to intractability of\ncomputation problems related to formal concepts; this type of patterns was\nproposed as a meaningful and scalable approximation of formal concepts. In this\npaper, our aim is to present recent advance in OA-biclustering and its\nextensions to mining multi-mode communities in SNA setting. We also discuss\nconnection between clustering coefficients known in SNA community for 1-mode\nand 2-mode networks and OA-bicluster density, the main quality measure of an\nOA-bicluster. Our experiments with 2-, 3-, and 4-mode large real-world networks\nshow that this type of patterns is suitable for community detection in\nmulti-mode cases within reasonable time even though the number of corresponding\n$n$-cliques is still unknown due to computation difficulties. An interpretation\nof OA-biclusters for 1-mode networks is provided as well. \n\n"}
{"id": "1703.00066", "contents": "Title: On the Power of Learning from $k$-Wise Queries Abstract: Several well-studied models of access to data samples, including statistical\nqueries, local differential privacy and low-communication algorithms rely on\nqueries that provide information about a function of a single sample. (For\nexample, a statistical query (SQ) gives an estimate of $Ex_{x \\sim D}[q(x)]$\nfor any choice of the query function $q$ mapping $X$ to the reals, where $D$ is\nan unknown data distribution over $X$.) Yet some data analysis algorithms rely\non properties of functions that depend on multiple samples. Such algorithms\nwould be naturally implemented using $k$-wise queries each of which is\nspecified by a function $q$ mapping $X^k$ to the reals. Hence it is natural to\nask whether algorithms using $k$-wise queries can solve learning problems more\nefficiently and by how much.\n  Blum, Kalai and Wasserman (2003) showed that for any weak PAC learning\nproblem over a fixed distribution, the complexity of learning with $k$-wise SQs\nis smaller than the (unary) SQ complexity by a factor of at most $2^k$. We show\nthat for more general problems over distributions the picture is substantially\nricher. For every $k$, the complexity of distribution-independent PAC learning\nwith $k$-wise queries can be exponentially larger than learning with\n$(k+1)$-wise queries. We then give two approaches for simulating a $k$-wise\nquery using unary queries. The first approach exploits the structure of the\nproblem that needs to be solved. It generalizes and strengthens (exponentially)\nthe results of Blum et al.. It allows us to derive strong lower bounds for\nlearning DNF formulas and stochastic constraint satisfaction problems that hold\nagainst algorithms using $k$-wise queries. The second approach exploits the\n$k$-party communication complexity of the $k$-wise query function. \n\n"}
{"id": "1703.01054", "contents": "Title: When Hashes Met Wedges: A Distributed Algorithm for Finding High\n  Similarity Vectors Abstract: Finding similar user pairs is a fundamental task in social networks, with\nnumerous applications in ranking and personalization tasks such as link\nprediction and tie strength detection. A common manifestation of user\nsimilarity is based upon network structure: each user is represented by a\nvector that represents the user's network connections, where pairwise cosine\nsimilarity among these vectors defines user similarity. The predominant task\nfor user similarity applications is to discover all similar pairs that have a\npairwise cosine similarity value larger than a given threshold $\\tau$. In\ncontrast to previous work where $\\tau$ is assumed to be quite close to 1, we\nfocus on recommendation applications where $\\tau$ is small, but still\nmeaningful. The all pairs cosine similarity problem is computationally\nchallenging on networks with billions of edges, and especially so for settings\nwith small $\\tau$. To the best of our knowledge, there is no practical solution\nfor computing all user pairs with, say $\\tau = 0.2$ on large social networks,\neven using the power of distributed algorithms.\n  Our work directly addresses this challenge by introducing a new algorithm ---\nWHIMP --- that solves this problem efficiently in the MapReduce model. The key\ninsight in WHIMP is to combine the \"wedge-sampling\" approach of Cohen-Lewis for\napproximate matrix multiplication with the SimHash random projection techniques\nof Charikar. We provide a theoretical analysis of WHIMP, proving that it has\nnear optimal communication costs while maintaining computation cost comparable\nwith the state of the art. We also empirically demonstrate WHIMP's scalability\nby computing all highly similar pairs on four massive data sets, and show that\nit accurately finds high similarity pairs. In particular, we note that WHIMP\nsuccessfully processes the entire Twitter network, which has tens of billions\nof edges. \n\n"}
{"id": "1703.01686", "contents": "Title: Parameterized complexity of finding a spanning tree with minimum reload\n  cost diameter Abstract: We study the minimum diameter spanning tree problem under the reload cost\nmodel (DIAMETER-TREE for short) introduced by Wirth and Steffan (2001). In this\nproblem, given an undirected edge-colored graph $G$, reload costs on a path\narise at a node where the path uses consecutive edges of different colors. The\nobjective is to find a spanning tree of $G$ of minimum diameter with respect to\nthe reload costs. We initiate a systematic study of the parameterized\ncomplexity of the DIAMETER-TREE problem by considering the following\nparameters: the cost of a solution, and the treewidth and the maximum degree\n$\\Delta$ of the input graph. We prove that DIAMETER-TREE is para-NP-hard for\nany combination of two of these three parameters, and that it is FPT\nparameterized by the three of them. We also prove that the problem can be\nsolved in polynomial time on cactus graphs. This result is somehow surprising\nsince we prove DIAMETER-TREE to be NP-hard on graphs of treewidth two, which is\nbest possible as the problem can be trivially solved on forests. When the\nreload costs satisfy the triangle inequality, Wirth and Steffan (2001) proved\nthat the problem can be solved in polynomial time on graphs with $\\Delta = 3$,\nand Galbiati (2008) proved that it is NP-hard if $\\Delta = 4$. Our results\nshow, in particular, that without the requirement of the triangle inequality,\nthe problem is NP-hard if $\\Delta = 3$, which is also best possible. Finally,\nin the case where the reload costs are polynomially bounded by the size of the\ninput graph, we prove that DIAMETER-TREE is in XP and W[1]-hard parameterized\nby the treewidth plus $\\Delta$. \n\n"}
{"id": "1703.04226", "contents": "Title: Do FRB Mark Dark Core Collapse? Abstract: Are some neutron stars produced without a supernova, without ejecting mass in\na remnant? Theoretical calculations of core collapse in massive stars often\npredict this. The observation of the repeating FRB 121102, whose dispersion\nmeasure has not changed over several years, suggests that dark core collapses\nare not just failures of computer codes, but may be real. The existence of one\nrepeating FRB with unchanging dispersion measure is not conclusive, but within\na decade hundreds or thousands of FRB are expected to be discovered, likely\nincluding scores of repeaters, permitting useful statistical inferences. A\nna\\\"{\\i}ve supernova remnant model predicts observable decline in dispersion\nmeasure for 100 years after its formation. If an upper limit on the decline of\n2 pc/cm$^3$-y is set for five repeating FRB, then the na\\\"{\\i}ve model with\nnominal parameters is rejected at the 95\\% level of confidence. This may\nindicate dark neutron star formation without a supernova or supernova remnant.\nThis hypothesis may also be tested with LSST data that would show, if present,\na supernova at an interferometric FRB position if it occurred within the LSST\nepoch. \n\n"}
{"id": "1703.04686", "contents": "Title: The early B-type star Rho Oph A is an X-ray lighthouse Abstract: We present the results of a 140 ks XMM-Newton observation of the B2 star\n$\\rho$ Ophiuchi A. The star has exhibited strong X-ray variability: a\ncusp-shaped increase of rate, similar to that which we partially observed in\n2013, and a bright flare. These events are separated in time by about 104 ks,\nwhich likely corresponds to the rotational period of the star (1.2 days). Time\nresolved spectroscopy of the X-ray spectra shows that the first event is caused\nby an increase of the plasma emission measure, while the second increase of\nrate is a major flare with temperatures in excess of 60 MK ($kT\\sim5$ keV).\nFrom the analysis of its rise, we infer a magnetic field of $\\ge300$ G and a\nsize of the flaring region of $\\sim1.4-1.9\\times10^{11}$ cm, which corresponds\nto $\\sim25\\%-30\\%$ of the stellar radius. We speculate that either an intrinsic\nmagnetism that produces a hot spot on its surface or an unknown low mass\ncompanion are the source of such X-rays and variability. A hot spot of magnetic\norigin should be a stable structure over a time span of $\\ge$2.5 years, and\nsuggests an overall large scale dipolar magnetic field that produces an\nextended feature on the stellar surface. In the second scenario, a low mass\nunknown companion is the emitter of X-rays and it should orbit extremely close\nto the surface of the primary in a locked spin-orbit configuration, almost on\nthe verge of collapsing onto the primary. As such, the X-ray activity of the\nsecondary star would be enhanced by its young age, and the tight orbit as in RS\nCvn systems and $\\rho$ Ophiuchi would constitute an extreme system that is\nworthy of further investigation. \n\n"}
{"id": "1703.05045", "contents": "Title: Average whenever you meet: Opportunistic protocols for community\n  detection Abstract: Consider the following asynchronous, opportunistic communication model over a\ngraph $G$: in each round, one edge is activated uniformly and independently at\nrandom and (only) its two endpoints can exchange messages and perform local\ncomputations. Under this model, we study the following random process: The\nfirst time a vertex is an endpoint of an active edge, it chooses a random\nnumber, say $\\pm 1$ with probability $1/2$; then, in each round, the two\nendpoints of the currently active edge update their values to their average. We\nshow that, if $G$ exhibits a two-community structure (for example, two\nexpanders connected by a sparse cut), the values held by the nodes will\ncollectively reflect the underlying community structure over a suitable phase\nof the above process, allowing efficient and effective recovery in important\ncases.\n  In more detail, we first provide a first-moment analysis showing that, for a\nlarge class of almost-regular clustered graphs that includes the stochastic\nblock model, the expected values held by all but a negligible fraction of the\nnodes eventually reflect the underlying cut signal. We prove this property\nemerges after a mixing period of length $\\mathcal O(n\\log n)$. We further\nprovide a second-moment analysis for a more restricted class of regular\nclustered graphs that includes the regular stochastic block model. For this\ncase, we are able to show that most nodes can efficiently and locally identify\ntheir community of reference over a suitable time window. This results in the\nfirst opportunistic protocols that approximately recover community structure\nusing only polylogarithmic work per node. Even for the above class of regular\ngraphs, our second moment analysis requires new concentration bounds on the\nproduct of certain random matrices that are technically challenging and\npossibly of independent interest. \n\n"}
{"id": "1703.05199", "contents": "Title: Optimal Unateness Testers for Real-Valued Functions: Adaptivity Helps Abstract: We study the problem of testing unateness of functions $f:\\{0,1\\}^d \\to\n\\mathbb{R}.$ We give a $O(\\frac{d}{\\epsilon} \\cdot\n\\log\\frac{d}{\\epsilon})$-query nonadaptive tester and a\n$O(\\frac{d}{\\epsilon})$-query adaptive tester and show that both testers are\noptimal for a fixed distance parameter $\\epsilon$. Previously known unateness\ntesters worked only for Boolean functions, and their query complexity had worse\ndependence on the dimension both for the adaptive and the nonadaptive case.\nMoreover, no lower bounds for testing unateness were known. We also generalize\nour results to obtain optimal unateness testers for functions $f:[n]^d \\to\n\\mathbb{R}$.\n  Our results establish that adaptivity helps with testing unateness of\nreal-valued functions on domains of the form $\\{0,1\\}^d$ and, more generally,\n$[n]^d$. This stands in contrast to the situation for monotonicity testing\nwhere there is no adaptivity gap for functions $f:[n]^d \\to \\mathbb{R}$. \n\n"}
{"id": "1703.05796", "contents": "Title: Stellar binaries in galactic nuclei: tidally stimulated mergers followed\n  by tidal disruptions Abstract: We investigate interactions of stellar binaries in galactic nuclear clusters\nwith a massive black hole (MBH). We consider binaries on highly eccentric\norbits around the MBH that change due to random gravitational interactions with\nother stars in the nuclear stellar cluster. The pericenters of the orbits\nperform a random walk, and we consider cases where this random walk slowly\nbrings the binary to the Hills tidal separation radius (the so-called empty\nloss-cone regime). However, we find that in a majority of cases the expected\nseparation does not occur and instead the members of the binary merge together.\nThis happens because the binary's eccentricity is excited by tidal interactions\nwith the MBH, and the relative excursions of the internal eccentricity of the\nbinary far exceed those in its internal semimajor axis. This frequently reduces\nthe pericenter separation to values below typical stellar diameters, which\ninduces a significant fraction of such binaries to merge ($\\gtrsim 75\\%$ in our\nset of numerical experiments). Stellar tides do not appreciably change the\ntotal rate of mergers but circularise binaries, leading to a significant\nfraction of low-eccentricity, low-impact-velocity mergers. Some of the stellar\nmerger products will then be tidally disrupted by the MBH within $\\sim 10^6$\nyears. If the merger strongly enhances the magnetic field of the merger\nproduct, this process could explain observations of prompt relativistic jet\nformation in some tidal disruption events. \n\n"}
{"id": "1703.06048", "contents": "Title: An FPTAS for the Knapsack Problem with Parametric Weights Abstract: In this paper, we investigate the parametric weight knapsack problem, in\nwhich the item weights are affine functions of the form $w_i(\\lambda) = a_i +\n\\lambda \\cdot b_i$ for $i \\in \\{1,\\ldots,n\\}$ depending on a real-valued\nparameter $\\lambda$. The aim is to provide a solution for all values of the\nparameter. It is well-known that any exact algorithm for the problem may need\nto output an exponential number of knapsack solutions. We present the first\nfully polynomial-time approximation scheme (FPTAS) for the problem that, for\nany desired precision $\\varepsilon \\in (0,1)$, computes\n$(1-\\varepsilon)$-approximate solutions for all values of the parameter. Our\nFPTAS is based on two different approaches and achieves a running time of\n$\\mathcal{O}(n^3/\\varepsilon^2 \\cdot \\min\\{ \\log^2 P, n^2 \\} \\cdot \\min\\{\\log\nM, n \\log (n/\\varepsilon) / \\log(n \\log (n/\\varepsilon) )\\})$ where $P$ is an\nupper bound on the optimal profit and $M := \\max\\{W, n \\cdot \\max\\{a_i,b_i: i\n\\in \\{1,\\ldots,n\\}\\}\\}$ for a knapsack with capacity $W$. \n\n"}
{"id": "1703.07005", "contents": "Title: Super-Eddington accretion onto a magnetized neutron star Abstract: Most of ultraluminous X-ray sources are thought to be objects accreting above\ntheir Eddington limits. In the recently identified class of ultraluminous X-ray\npulsars, accretor is a neutron star and thus has a fairly small mass with a\nsmall Eddington limit. The accretion disc structure around such an object\naffects important observables such as equilibrium period, period derivative and\nthe size of the magnetosphere. We propose a model of a nearly-standard\naccretion disc interacting with the magnetosphere only in a thin layer near the\ninner disc rim. Our calculations show that the size of the magnetosphere may be\nrepresented as the classical Alfv\\'en radius times a dimensionless factor $\\xi$\nwhich depends on the disc thickness only. In the case of\nradiation-pressure-dominated disc, the size of the magnetosphere does not\ndepend on the mass accretion rate. In general, increasing the disc thickness\nleads to a larger magnetosphere size in units of the Alfv\\'en radius. For large\nenough mass accretion rates and magnetic moments, it is important to take into\naccount not only the pressure of the magnetic field and the radiation pressure\ninside the disc, but also the pressure of the radiation produced close to the\nsurface of the neutron star in accretion column. The magnetospheric size may\nincrease by up to factor of two as a result of the effects related to the disc\nthickness and the irradiation from the central source. Accounting for these\neffects reduces the estimate of the neutron star magnetic moment by a factor of\nseveral. \n\n"}
{"id": "1703.09083", "contents": "Title: The weighted stable matching problem Abstract: We study the stable matching problem in non-bipartite graphs with incomplete\nbut strict preference lists, where the edges have weights and the goal is to\ncompute a stable matching of minimum or maximum weight. This problem is known\nto be NP-hard in general. Our contribution is two fold: a polyhedral\ncharacterization and an approximation algorithm. Previously Chen et al. have\nshown that the stable matching polytope is integral if and only if the subgraph\nobtained after running phase one of Irving's algorithm is bipartite. We improve\nupon this result by showing that there are instances where this subgraph might\nnot be bipartite but one can further eliminate some edges and arrive at a\nbipartite subgraph. Our elimination procedure ensures that the set of stable\nmatchings remains the same, and thus the stable matching polytope of the final\nsubgraph contains the incidence vectors of all stable matchings of our original\ngraph. This allows us to characterize a larger class of instances for which the\nweighted stable matching problem is polynomial-time solvable. We also show that\nour edge elimination procedure is best possible, meaning that if the subgraph\nwe arrive at is not bipartite, then there is no bipartite subgraph that has the\nsame set of stable matchings as the original graph. We complement these results\nwith a $2$-approximation algorithm for the minimum weight stable matching\nproblem for instances where each agent has at most two possible partners in any\nstable matching. This is the first approximation result for any class of\ninstances with general weights. \n\n"}
{"id": "1703.10657", "contents": "Title: High-redshift blazars through nustar eyes Abstract: The most powerful sources among the blazar family are MeV blazars. Often\ndetected at $z>2$, they usually display high X- and \\gm-ray luminosities,\nlarger-than-average jet powers and black hole masses $\\gtrsim 10^9 M_{\\odot}$.\nIn the present work we perform a multiwavelength study of three high redshift\nblazars: 3FGL J0325.5+2223 ($z=2.06$), 3FGL J0449.0+1121 ($z= 2.15$), and 3FGL\nJ0453.2$-$2808 ($z=2.56$), analysing quasi simultaneous data from GROND,\n\\swift-UVOT and XRT, \\nustar, and \\fermi-LAT. Our main focus is on the hard\nX-ray band recently unveiled by \\nustar~(3$-$79 keV) where these objects show a\nhard spectrum which enables us to constrain the inverse Compton peak and the\njet power. We found that all three targets resemble the most powerful blazars,\nwith the synchrotron peak located in the sub-millimeter range and the inverse\nCompton peak in the MeV range, and therefore belong to the MeV blazar class.\nUsing a simple one zone leptonic emission model to reproduce the spectral\nenergy distributions, we conclude that a simple combination of synchrotron and\naccretion disk emission reproduces the infrared-optical spectra while the X-ray\nto \\gm-ray part is well reproduced by the inverse Compton scattering of low\nenergy photons supplied by the broad line region. The black hole masses for\neach of the three sources are calculated to be $\\gtrsim 4 \\times 10^{8}\nM_{\\odot}$. The three studied sources have jet power at the level of, or\nbeyond, the accretion luminosity. \n\n"}
{"id": "1704.00249", "contents": "Title: Complexity of short Presburger arithmetic Abstract: We study complexity of short sentences in Presburger arithmetic (Short-PA).\nHere by \"short\" we mean sentences with a bounded number of variables,\nquantifiers, inequalities and Boolean operations; the input consists only of\nthe integers involved in the inequalities. We prove that assuming Kannan's\npartition can be found in polynomial time, the satisfiability of Short-PA\nsentences can be decided in polynomial time. Furthermore, under the same\nassumption, we show that the numbers of satisfying assignments of short\nPresburger sentences can also be computed in polynomial time. \n\n"}
{"id": "1704.02367", "contents": "Title: Testing hereditary properties of ordered graphs and matrices Abstract: We consider properties of edge-colored vertex-ordered graphs, i.e., graphs\nwith a totally ordered vertex set and a finite set of possible edge colors. We\nshow that any hereditary property of such graphs is strongly testable, i.e.,\ntestable with a constant number of queries. We also explain how the proof can\nbe adapted to show that any hereditary property of $2$-dimensional matrices\nover a finite alphabet (where row and column order is not ignored) is strongly\ntestable. The first result generalizes the result of Alon and Shapira [FOCS'05,\nSICOMP'08], who showed that any hereditary graph property (without vertex\norder) is strongly testable. The second result answers and generalizes a\nconjecture of Alon, Fischer and Newman [SICOMP'07] concerning testing of matrix\nproperties.\n  The testability is proved by establishing a removal lemma for vertex-ordered\ngraphs. It states that for any finite or infinite family $\\mathcal{F}$ of\nforbidden vertex-ordered graphs, and any $\\epsilon > 0$, there exist $\\delta >\n0$ and $k$ so that any vertex-ordered graph which is $\\epsilon$-far from being\n$\\mathcal{F}$-free contains at least $\\delta n^{|F|}$ copies of some\n$F\\in\\mathcal{F}$ (with the correct vertex order) where $|F|\\leq k$. The proof\nbridges the gap between techniques related to the regularity lemma, used in the\nlong chain of papers investigating graph testing, and string testing\ntechniques. Along the way we develop a Ramsey-type lemma for $k$-partite graphs\nwith \"undesirable\" edges, stating that one can find a Ramsey-type structure in\nsuch a graph, in which the density of the undesirable edges is not much higher\nthan the density of those edges in the graph. \n\n"}
{"id": "1704.03486", "contents": "Title: Simply Exponential Approximation of the Permanent of Positive\n  Semidefinite Matrices Abstract: We design a deterministic polynomial time $c^n$ approximation algorithm for\nthe permanent of positive semidefinite matrices where $c=e^{\\gamma+1}\\simeq\n4.84$. We write a natural convex relaxation and show that its optimum solution\ngives a $c^n$ approximation of the permanent. We further show that this factor\nis asymptotically tight by constructing a family of positive semidefinite\nmatrices. \n\n"}
{"id": "1704.03864", "contents": "Title: A Matrix Expander Chernoff Bound Abstract: We prove a Chernoff-type bound for sums of matrix-valued random variables\nsampled via a random walk on an expander, confirming a conjecture due to\nWigderson and Xiao. Our proof is based on a new multi-matrix extension of the\nGolden-Thompson inequality which improves in some ways the inequality of\nSutter, Berta, and Tomamichel, and may be of independent interest, as well as\nan adaptation of an argument for the scalar case due to Healy. Secondarily, we\nalso provide a generic reduction showing that any concentration inequality for\nvector-valued martingales implies a concentration inequality for the\ncorresponding expander walk, with a weakening of parameters proportional to the\nsquared mixing time. \n\n"}
{"id": "1704.03866", "contents": "Title: Robustly Learning a Gaussian: Getting Optimal Error, Efficiently Abstract: We study the fundamental problem of learning the parameters of a\nhigh-dimensional Gaussian in the presence of noise -- where an\n$\\varepsilon$-fraction of our samples were chosen by an adversary. We give\nrobust estimators that achieve estimation error $O(\\varepsilon)$ in the total\nvariation distance, which is optimal up to a universal constant that is\nindependent of the dimension.\n  In the case where just the mean is unknown, our robustness guarantee is\noptimal up to a factor of $\\sqrt{2}$ and the running time is polynomial in $d$\nand $1/\\epsilon$. When both the mean and covariance are unknown, the running\ntime is polynomial in $d$ and quasipolynomial in $1/\\varepsilon$. Moreover all\nof our algorithms require only a polynomial number of samples. Our work shows\nthat the same sorts of error guarantees that were established over fifty years\nago in the one-dimensional setting can also be achieved by efficient algorithms\nin high-dimensional settings. \n\n"}
{"id": "1704.03910", "contents": "Title: The Fermi Galactic Center GeV Excess and Implications for Dark Matter Abstract: The region around the Galactic center (GC) is now well established to be\nbrighter at energies of a few GeV than expected from conventional models of\ndiffuse gamma-ray emission and catalogs of known gamma-ray sources. We study\nthe GeV excess using 6.5 years of data from the Fermi Large Area Telescope. We\ncharacterize the uncertainty of the GC excess spectrum and morphology due to\nuncertainties in cosmic-ray source distributions and propagation, uncertainties\nin the distribution of interstellar gas in the Milky Way, and uncertainties due\nto a potential contribution from the Fermi bubbles. We also evaluate\nuncertainties in the excess properties due to resolved point sources of gamma\nrays. The Galactic center is of particular interest as it would be expected to\nhave the brightest signal from annihilation of weakly interacting massive dark\nmatter particles. However, control regions along the Galactic plane, where a\ndark-matter signal is not expected, show excesses of similar amplitude relative\nto the local background. Based on the magnitude of the systematic\nuncertainties, we conservatively report upper limits for the annihilation cross\nsection as function of particle mass and annihilation channel. \n\n"}
{"id": "1704.04249", "contents": "Title: Parameterized Complexity and Approximability of Directed Odd Cycle\n  Transversal Abstract: A directed odd cycle transversal of a directed graph (digraph) $D$ is a\nvertex set $S$ that intersects every odd directed cycle of $D$. In the Directed\nOdd Cycle Transversal (DOCT) problem, the input consists of a digraph $D$ and\nan integer $k$. The objective is to determine whether there exists a directed\nodd cycle transversal of $D$ of size at most $k$.\n  In this paper, we settle the parameterized complexity of DOCT when\nparameterized by the solution size $k$ by showing that DOCT does not admit an\nalgorithm with running time $f(k)n^{O(1)}$ unless FPT = W[1]. On the positive\nside, we give a factor $2$ fixed parameter tractable (FPT) approximation\nalgorithm for the problem. More precisely, our algorithm takes as input $D$ and\n$k$, runs in time $2^{O(k^2)}n^{O(1)}$, and either concludes that $D$ does not\nhave a directed odd cycle transversal of size at most $k$, or produces a\nsolution of size at most $2k$. Finally, we provide evidence that there exists\n$\\epsilon > 0$ such that DOCT does not admit a factor $(1+\\epsilon)$\nFPT-approximation algorithm. \n\n"}
{"id": "1704.04546", "contents": "Title: SETH-Based Lower Bounds for Subset Sum and Bicriteria Path Abstract: Subset-Sum and k-SAT are two of the most extensively studied problems in\ncomputer science, and conjectures about their hardness are among the\ncornerstones of fine-grained complexity. One of the most intriguing open\nproblems in this area is to base the hardness of one of these problems on the\nother.\n  Our main result is a tight reduction from k-SAT to Subset-Sum on dense\ninstances, proving that Bellman's 1962 pseudo-polynomial $O^{*}(T)$-time\nalgorithm for Subset-Sum on $n$ numbers and target $T$ cannot be improved to\ntime $T^{1-\\varepsilon}\\cdot 2^{o(n)}$ for any $\\varepsilon>0$, unless the\nStrong Exponential Time Hypothesis (SETH) fails. This is one of the strongest\nknown connections between any two of the core problems of fine-grained\ncomplexity.\n  As a corollary, we prove a \"Direct-OR\" theorem for Subset-Sum under SETH,\noffering a new tool for proving conditional lower bounds: It is now possible to\nassume that deciding whether one out of $N$ given instances of Subset-Sum is a\nYES instance requires time $(N T)^{1-o(1)}$. As an application of this\ncorollary, we prove a tight SETH-based lower bound for the classical Bicriteria\ns,t-Path problem, which is extensively studied in Operations Research. We\nseparate its complexity from that of Subset-Sum: On graphs with $m$ edges and\nedge lengths bounded by $L$, we show that the $O(Lm)$ pseudo-polynomial time\nalgorithm by Joksch from 1966 cannot be improved to $\\tilde{O}(L+m)$, in\ncontrast to a recent improvement for Subset Sum (Bringmann, SODA 2017). \n\n"}
{"id": "1704.05040", "contents": "Title: One-particle reducible contribution to the one-loop spinor propagator in\n  a constant field Abstract: Extending work by Gies and Karbstein on the Euler-Heisenberg Lagrangian, it\nhas recently been shown that the one-loop propagator of a charged scalar\nparticle in a constant electromagnetic field has a one-particle reducible\ncontribution in addition to the well-studied irreducible one. Here we further\ngeneralize this result to the spinor case, and find the same relation between\nthe reducible term, the tree-level propagator and the one-loop Euler-Heisenberg\nLagrangian as in the scalar case. Our demonstration uses a novel worldline path\nintegral representation of the photon-dressed spinor propagator in a constant\nelectromagnetic field background. \n\n"}
{"id": "1704.06241", "contents": "Title: On monotone circuits with local oracles and clique lower bounds Abstract: We investigate monotone circuits with local oracles [K., 2016], i.e.,\ncircuits containing additional inputs $y_i = y_i(\\vec{x})$ that can perform\nunstructured computations on the input string $\\vec{x}$. Let $\\mu \\in [0,1]$ be\nthe locality of the circuit, a parameter that bounds the combined strength of\nthe oracle functions $y_i(\\vec{x})$, and $U_{n,k}, V_{n,k} \\subseteq \\{0,1\\}^m$\nbe the set of $k$-cliques and the set of complete $(k-1)$-partite graphs,\nrespectively (similarly to [Razborov, 1985]). Our results can be informally\nstated as follows.\n  1. For an appropriate extension of depth-$2$ monotone circuits with local\noracles, we show that the size of the smallest circuits separating $U_{n,3}$\n(triangles) and $V_{n,3}$ (complete bipartite graphs) undergoes two phase\ntransitions according to $\\mu$.\n  2. For $5 \\leq k(n) \\leq n^{1/4}$, arbitrary depth, and $\\mu \\leq 1/50$, we\nprove that the monotone circuit size complexity of separating the sets\n$U_{n,k}$ and $V_{n,k}$ is $n^{\\Theta(\\sqrt{k})}$, under a certain restrictive\nassumption on the local oracle gates.\n  The second result, which concerns monotone circuits with restricted oracles,\nextends and provides a matching upper bound for the exponential lower bounds on\nthe monotone circuit size complexity of $k$-clique obtained by Alon and Boppana\n(1987). \n\n"}
{"id": "1704.06622", "contents": "Title: Path-contractions, edge deletions and connectivity preservation Abstract: We study several problems related to graph modification problems under\nconnectivity constraints from the perspective of parameterized complexity: {\\sc\n(Weighted) Biconnectivity Deletion}, where we are tasked with deleting~$k$\nedges while preserving biconnectivity in an undirected graph, {\\sc\nVertex-deletion Preserving Strong Connectivity}, where we want to maintain\nstrong connectivity of a digraph while deleting exactly~$k$ vertices, and {\\sc\nPath-contraction Preserving Strong Connectivity}, in which the operation of\npath contraction on arcs is used instead. The parameterized tractability of\nthis last problem was posed by Bang-Jensen and Yeo [DAM 2008] as an open\nquestion and we answer it here in the negative: both variants of preserving\nstrong connectivity are $\\sf W[1]$-hard. Preserving biconnectivity, on the\nother hand, turns out to be fixed parameter tractable and we provide a\n$2^{O(k\\log k)} n^{O(1)}$-algorithm that solves {\\sc Weighted Biconnectivity\nDeletion}. Further, we show that the unweighted case even admits a randomized\npolynomial kernel. All our results provide further interesting data points for\nthe systematic study of connectivity-preservation constraints in the\nparameterized setting. \n\n"}
{"id": "1704.07110", "contents": "Title: The Physics of UHECRs: Spectra, Composition and the Transition\n  Galactic-Extragalactic Abstract: We review the experimental evidences about flux and mass composition of ultra\nhigh energy cosmic rays in connection with theoretical scenarios concerning\nastrophysical sources. In this context, we also address the discussion about\nthe expected transition between cosmic rays produced inside the Galaxy and\nthose coming from the intergalactic space. \n\n"}
{"id": "1704.07284", "contents": "Title: Hitting minors on bounded treewidth graphs. I. General upper bounds Abstract: For a finite collection of graphs ${\\cal F}$, the ${\\cal F}$-M-DELETION\nproblem consists in, given a graph $G$ and an integer $k$, deciding whether\nthere exists $S \\subseteq V(G)$ with $|S| \\leq k$ such that $G \\setminus S$\ndoes not contain any of the graphs in ${\\cal F}$ as a minor. We are interested\nin the parameterized complexity of ${\\cal F}$-M-DELETION when the parameter is\nthe treewidth of $G$, denoted by $tw$. Our objective is to determine, for a\nfixed ${\\cal F}$, the smallest function $f_{{\\cal F}}$ such that {${\\cal\nF}$-M-DELETION can be solved in time $f_{{\\cal F}}(tw) \\cdot n^{O(1)}$ on\n$n$-vertex graphs. We prove that $f_{{\\cal F}}(tw) = 2^{2^{O(tw \\cdot\\log\ntw)}}$ for every collection ${\\cal F}$, that $f_{{\\cal F}}(tw) = 2^{O(tw\n\\cdot\\log tw)}$ if ${\\cal F}$ contains a planar graph, and that $f_{{\\cal\nF}}(tw) = 2^{O(tw)}$ if in addition the input graph $G$ is planar or embedded\nin a surface. We also consider the version of the problem where the graphs in\n${\\cal F}$ are forbidden as topological minors, called ${\\cal F}$-TM-DELETION.\nWe prove similar results for this problem, except that in the last two\nalgorithms, instead of requiring ${\\cal F}$ to contain a planar graph, we need\nit to contain a subcubic planar graph. This is the first of a series of\narticles on this topic. \n\n"}
{"id": "1704.07291", "contents": "Title: Minimal Controllability of Conjunctive Boolean Networks is NP-Complete Abstract: Given a conjunctive Boolean network (CBN) with $n$ state-variables, we\nconsider the problem of finding a minimal set of state-variables to directly\naffect with an input so that the resulting conjunctive Boolean control network\n(CBCN) is controllable. We give a necessary and sufficient condition for\ncontrollability of a CBCN; an $O(n^2)$-time algorithm for testing\ncontrollability; and prove that nonetheless the minimal controllability problem\nfor CBNs is NP-hard. \n\n"}
{"id": "1704.07960", "contents": "Title: Comparisons of Jet Properties between GeV Radio Galaxies and Blazars Abstract: We compile a sample of spectral energy distribution (SED) of 12 GeV radio\ngalaxies (RGs), including eight FR I RGs and four FR II RGs. These SEDs can be\nrepresented with the one-zone leptonic model. No significant unification as\nexpected in the unification model is found for the derived jet parameters\nbetween FR I RGs and BL Lacertae objects (BL Lacs) and between FR II RGs and\nflat spectrum radio quasars (FSRQs). However, on average FR I RGs have the\nlarger gamma_b (break Lorentz factor of electrons) and lower B (magnetic field\nstrength) than FR II RGs, analogous to the differences between BL Lacs and\nFSRQs. The derived Doppler factors (delta) of RGs are on average smaller than\nthat of balzars, which is consistent with the unification model that RGs are\nthe misaligned parent populations of blazars with smaller delta. On the basis\nof jet parameters from SED fits, we calculate their jet powers and the powers\ncarried by each component, and compare their jet compositions and radiation\nefficiencies with blazars. Most of the RG jets may be dominated by particles,\nlike BL Lacs, not FSRQs. However, the jets of RGs with higher radiation\nefficiencies tend to have higher jet magnetization. A strong anticorrelation\nbetween synchrotron peak frequency and jet power is observed for the GeV RGs\nand blazars in both the observer and co-moving frames, indicating that the\n\"sequence\" behavior among blazars, together with the GeV RGs, may be dominated\nby the jet power intrinsically. \n\n"}
{"id": "1704.08246", "contents": "Title: Relative Error Tensor Low Rank Approximation Abstract: We consider relative error low rank approximation of $tensors$ with respect\nto the Frobenius norm: given an order-$q$ tensor $A \\in\n\\mathbb{R}^{\\prod_{i=1}^q n_i}$, output a rank-$k$ tensor $B$ for which\n$\\|A-B\\|_F^2 \\leq (1+\\epsilon)$OPT, where OPT $= \\inf_{\\textrm{rank-}k~A'}\n\\|A-A'\\|_F^2$. Despite the success on obtaining relative error low rank\napproximations for matrices, no such results were known for tensors. One\nstructural issue is that there may be no rank-$k$ tensor $A_k$ achieving the\nabove infinum. Another, computational issue, is that an efficient relative\nerror low rank approximation algorithm for tensors would allow one to compute\nthe rank of a tensor, which is NP-hard. We bypass these issues via (1)\nbicriteria and (2) parameterized complexity solutions:\n  (1) We give an algorithm which outputs a rank $k' = O((k/\\epsilon)^{q-1})$\ntensor $B$ for which $\\|A-B\\|_F^2 \\leq (1+\\epsilon)$OPT in $nnz(A) + n \\cdot\n\\textrm{poly}(k/\\epsilon)$ time in the real RAM model. Here $nnz(A)$ is the\nnumber of non-zero entries in $A$.\n  (2) We give an algorithm for any $\\delta >0$ which outputs a rank $k$ tensor\n$B$ for which $\\|A-B\\|_F^2 \\leq (1+\\epsilon)$OPT and runs in $ ( nnz(A) + n\n\\cdot \\textrm{poly}(k/\\epsilon) + \\exp(k^2/\\epsilon) ) \\cdot n^\\delta$ time in\nthe unit cost RAM model.\n  For outputting a rank-$k$ tensor, or even a bicriteria solution with\nrank-$Ck$ for a certain constant $C > 1$, we show a $2^{\\Omega(k^{1-o(1)})}$\ntime lower bound under the Exponential Time Hypothesis.\n  Our results give the first relative error low rank approximations for tensors\nfor a large number of robust error measures for which nothing was known, as\nwell as column row and tube subset selection. We also obtain new results for\nmatrices, such as $nnz(A)$-time CUR decompositions, improving previous\n$nnz(A)\\log n$-time algorithms, which may be of independent interest. \n\n"}
{"id": "1704.08462", "contents": "Title: Communication complexity of approximate maximum matching in the\n  message-passing model Abstract: We consider the communication complexity of finding an approximate maximum\nmatching in a graph in a multi-party message-passing communication model. The\nmaximum matching problem is one of the most fundamental graph combinatorial\nproblems, with a variety of applications.\n  The input to the problem is a graph $G$ that has $n$ vertices and the set of\nedges partitioned over $k$ sites, and an approximation ratio parameter\n$\\alpha$. The output is required to be a matching in $G$ that has to be\nreported by one of the sites, whose size is at least factor $\\alpha$ of the\nsize of a maximum matching in $G$.\n  We show that the communication complexity of this problem is $\\Omega(\\alpha^2\nk n)$ information bits. This bound is shown to be tight up to a $\\log n$\nfactor, by constructing an algorithm, establishing its correctness, and an\nupper bound on the communication cost. The lower bound also applies to other\ngraph combinatorial problems in the message-passing communication model,\nincluding max-flow and graph sparsification. \n\n"}
{"id": "1704.08529", "contents": "Title: A polynomial-time randomized reduction from tournament isomorphism to\n  tournament asymmetry Abstract: The paper develops a new technique to extract a characteristic subset from a\nrandom source that repeatedly samples from a set of elements. Here a\ncharacteristic subset is a set that when containing an element contains all\nelements that have the same probability. With this technique at hand the paper\nlooks at the special case of the tournament isomorphism problem that stands in\nthe way towards a polynomial-time algorithm for the graph isomorphism problem.\nNoting that there is a reduction from the automorphism (asymmetry) problem to\nthe isomorphism problem, a reduction in the other direction is nevertheless not\nknown and remains a thorny open problem. Applying the new technique, we develop\na randomized polynomial-time Turing-reduction from the tournament isomorphism\nproblem to the tournament automorphism problem. This is the first such\nreduction for any kind of combinatorial object not known to have a\npolynomial-time solvable isomorphism problem. \n\n"}
{"id": "1705.00712", "contents": "Title: Comparison of transport coefficients for weakly coupled multi-component\n  plasmas obtained with different formalisms Abstract: LANL Memorandum of potentially broad interest. A direct comparison is\nperformed between transport coefficients for weakly coupled plasmas obtained\nwith different formalisms to demonstrate that these formalisms give identical\npredictions. This is what one would expect since for weakly coupled plasmas all\nthe formalisms rely on the same physical assumptions and mathematical\napproximations. \n\n"}
{"id": "1705.01595", "contents": "Title: Homomorphisms Are a Good Basis for Counting Small Subgraphs Abstract: We introduce graph motif parameters, a class of graph parameters that depend\nonly on the frequencies of constant-size induced subgraphs. Classical works by\nLov\\'asz show that many interesting quantities have this form, including, for\nfixed graphs $H$, the number of $H$-copies (induced or not) in an input graph\n$G$, and the number of homomorphisms from $H$ to $G$.\n  Using the framework of graph motif parameters, we obtain faster algorithms\nfor counting subgraph copies of fixed graphs $H$ in host graphs $G$: For graphs\n$H$ on $k$ edges, we show how to count subgraph copies of $H$ in time\n$k^{O(k)}\\cdot n^{0.174k + o(k)}$ by a surprisingly simple algorithm. This\nimproves upon previously known running times, such as $O(n^{0.91k + c})$ time\nfor $k$-edge matchings or $O(n^{0.46k + c})$ time for $k$-cycles.\n  Furthermore, we prove a general complexity dichotomy for evaluating graph\nmotif parameters: Given a class $\\mathcal C$ of such parameters, we consider\nthe problem of evaluating $f\\in \\mathcal C$ on input graphs $G$, parameterized\nby the number of induced subgraphs that $f$ depends upon. For every recursively\nenumerable class $\\mathcal C$, we prove the above problem to be either FPT or\n#W[1]-hard, with an explicit dichotomy criterion. This allows us to recover\nknown dichotomies for counting subgraphs, induced subgraphs, and homomorphisms\nin a uniform and simplified way, together with improved lower bounds.\n  Finally, we extend graph motif parameters to colored subgraphs and prove a\ncomplexity trichotomy: For vertex-colored graphs $H$ and $G$, where $H$ is from\na fixed class $\\mathcal H$, we want to count color-preserving $H$-copies in\n$G$. We show that this problem is either polynomial-time solvable or FPT or\n#W[1]-hard, and that the FPT cases indeed need FPT time under reasonable\nassumptions. \n\n"}
{"id": "1705.02517", "contents": "Title: $\\mathcal{B}$-partitions, application to determinant and permanent of\n  graphs Abstract: Let $G$ be a graph(directed or undirected) having $k$ number of blocks. A\n$\\mathcal{B}$-partition of $G$ is a partition into $k$ vertex-disjoint subgraph\n$(\\hat{B_1},\\hat{B_1},\\hdots,\\hat{B_k})$ such that $\\hat{B}_i$ is induced\nsubgraph of $B_i$ for $i=1,2,\\hdots,k.$ The terms\n$\\prod_{i=1}^{k}\\det(\\hat{B}_i),\\ \\prod_{i=1}^{k}\\text{per}(\\hat{B}_i)$ are\ndet-summands and per-summands, respectively, corresponding to the\n$\\mathcal{B}$-partition. The determinant and permanent of a graph having no\nloops on its cut-vertices is equal to summation of det-summands and\nper-summands, respectively, corresponding to all possible\n$\\mathcal{B}$-partitions. Thus, in this paper we calculate determinant and\npermanent of some graphs, which include block graph with negatives cliques,\nsigned unicyclic graph, mix complete graph, negative mix complete graph, and\nstar mix block graphs. \n\n"}
{"id": "1705.04111", "contents": "Title: Critical Graphs for Minimum Vertex Cover Abstract: In the context of the chromatic-number problem, a critical graph is an\ninstance where the deletion of any element would decrease the graph's chromatic\nnumber. Such instances have shown to be interesting objects of study for deepen\nthe understanding of the optimization problem.\n  This work introduces critical graphs in context of Minimum Vertex Cover. We\ndemonstrate their potential for the generation of larger graphs with hidden a\npriori known solutions. Firstly, we propose a parametrized graph-generation\nprocess which preserves the knowledge of the minimum cover. Secondly, we\nconduct a systematic search for small critical graphs. Thirdly, we illustrate\nthe applicability for benchmarking purposes by reporting on a series of\nexperiments using the state-of-the-art heuristic solver NuMVC. \n\n"}
{"id": "1705.04761", "contents": "Title: On the production of heavy axion-like particles in the accretion disks\n  of gamma-ray bursts Abstract: Heavy axion-like particles have been introduced in several scenarios beyond\nthe Standard Model and their production in some astrophysical systems should be\npossible. In this work, we re-examine the possibility that these type of\nparticles can be generated in the accretion disks of gamma-ray bursts (GRB),\nthe most powerful events in the universe. If the produced axions decay into\nphotons or $e^+e-$ pairs at the correct distances, a fireball is generated. We\ncalculate the structure transient accretion disks in GRBs (density, temperature\nand thickness profiles) taking into account the effect of heavy axion emission\nas well as the rest of the relevant standard cooling processes. This allows us\nto obtain the values of the coupling constant g_{aN} in order for the axions\nnot to become trapped, and we can also compute the emitted heavy axion\nluminosity from the entire disk. We find that for the couplings within the\nranges found, then the mechanism for powering GRBs based on heavy axion\nproduction and decay becomes an alternative to the standard picture based upon\nmagnetohydrodynamic processes and neutrino-antineutrino annihilation.\nOtherwise, if heavy axions are produced in the disk but their decay to takes\nplace further away, the mechanism fails. Still, the decay products (gamma rays\nor electrons and positrons) should leave observable signatures which are not\nobserved for different ranges of values of the coupling constants, depending on\nthe mass of the heavy axion \n\n"}
{"id": "1705.05382", "contents": "Title: Rapid growth of black holes accompanied with hot or warm outflows\n  exposed to anisotropic super-Eddington radiation Abstract: We perform two-dimensional radiation hydrodynamical simulations of accretion\nflows onto a black hole (BH) with a mass of $10^3\\leq M_{\\rm BH}/M_{\\odot}\n\\lesssim 10^6$ in order to study rapid growth of BHs in the early Universe. For\nspherically symmetric flows, hyper-Eddington accretion onto the BH from outside\nthe Bondi radius can occur unimpeded by radiation feedback only when the BH\nmass is higher than $\\simeq 10^4~M_{\\odot}(n_\\infty/10^5~{\\rm\ncm}^{-3})^{-1}(T_\\infty/10^4~{\\rm K})^{3/2}$, where $n_\\infty$ and $T_\\infty$\nare the density and temperature of ambient gas. Here, we study the properties\nof accretion flows exposed to anisotropic radiation from a nuclear accretion\ndisk with a luminosity higher than the Eddington value ($L_{\\rm Edd}$) due to\ncollimation toward the bipolar directions. We find that, unlike the spherically\nsymmetric case, even less massive BHs with $M_{\\rm BH} < 10^4~M_{\\odot}$ can be\nfed by surrounding gas at high accretion rates of $\\gtrsim L_{\\rm Edd}/c^2$\nthrough the equatorial plane, while ionized regions expand to the polar\ndirections producing hot outflows with $T\\sim 10^5$K. For more massive BHs with\n$M_{\\rm BH}\\gtrsim 5\\times 10^5~M_{\\odot}$, neutral gas through the equatorial\nplane totally covers the central radiating region due to the non-radial gas\nmotions, and thus the emergent radiation in all directions is blocked. Because\nof efficient recombination by hydrogen, the entire flow results in neutral and\nwarm gas with $T \\simeq 8000~{\\rm K}$ . The central BH is fed through the\nequator at the averaged rate of $\\sim 5\\times 10^4~L_{\\rm Edd}/c^2$, which\ncorresponds to $\\sim 50~\\%$ of the inflow rate from the Bondi radius. Moreover,\nradiation momentum absorbed by neutral hydrogen produces warm outflows toward\nthe bipolar directions at $\\sim 30~\\%$ of the BH feeding rate and with a\ntypical velocity of $\\simeq 50~{\\rm km~s}^{-1}$. \n\n"}
{"id": "1705.05687", "contents": "Title: Can dark matter annihilations explain the AMS-02 positron data? Abstract: We show that no dark matter model with the conventional isotropic density\ndistribution can provide a satisfactory explanation of the cosmic positron\nexcess, while being consistent with Fermi-LAT data on diffuse gamma-ray\nbackground. \n\n"}
{"id": "1705.05735", "contents": "Title: Comparison-Based Choices Abstract: A broad range of on-line behaviors are mediated by interfaces in which people\nmake choices among sets of options. A rich and growing line of work in the\nbehavioral sciences indicate that human choices follow not only from the\nutility of alternatives, but also from the choice set in which alternatives are\npresented. In this work we study comparison-based choice functions, a simple\nbut surprisingly rich class of functions capable of exhibiting so-called\nchoice-set effects. Motivated by the challenge of predicting complex choices,\nwe study the query complexity of these functions in a variety of settings. We\nconsider settings that allow for active queries or passive observation of a\nstream of queries, and give analyses both at the granularity of individuals or\npopulations that might exhibit heterogeneous choice behavior. Our main result\nis that any comparison-based choice function in one dimension can be inferred\nas efficiently as a basic maximum or minimum choice function across many query\ncontexts, suggesting that choice-set effects need not entail any fundamental\nalgorithmic barriers to inference. We also introduce a class of choice\nfunctions we call distance-comparison-based functions, and briefly discuss the\nanalysis of such functions. The framework we outline provides intriguing\nconnections between human choice behavior and a range of questions in the\ntheory of sorting. \n\n"}
{"id": "1705.07551", "contents": "Title: Parameterized Complexity of the List Coloring Reconfiguration Problem\n  with Graph Parameters Abstract: Let $G$ be a graph such that each vertex has its list of available colors,\nand assume that each list is a subset of the common set consisting of $k$\ncolors. For two given list colorings of $G$, we study the problem of\ntransforming one into the other by changing only one vertex color assignment at\na time, while at all times maintaining a list coloring. This problem is known\nto be PSPACE-complete even for bounded bandwidth graphs and a fixed constant\n$k$. In this paper, we study the fixed-parameter tractability of the problem\nwhen parameterized by several graph parameters. We first give a fixed-parameter\nalgorithm for the problem when parameterized by $k$ and the modular-width of an\ninput graph. We next give a fixed-parameter algorithm for the shortest variant\nwhen parameterized by $k$ and the size of a minimum vertex cover of an input\ngraph. As corollaries, we show that the problem for cographs and the shortest\nvariant for split graphs are fixed-parameter tractable even when only $k$ is\ntaken as a parameter. On the other hand, we prove that the problem is W[1]-hard\nwhen parameterized only by the size of a minimum vertex cover of an input\ngraph. \n\n"}
{"id": "1705.07728", "contents": "Title: Improved method for finding optimal formulae for bilinear maps in a\n  finite field Abstract: In 2012, Barbulescu, Detrey, Estibals and Zimmermann proposed a new framework\nto exhaustively search for optimal formulae for evaluating bilinear maps, such\nas Strassen or Karatsuba formulae. The main contribution of this work is a new\ncriterion to aggressively prune useless branches in the exhaustive search, thus\nleading to the computation of new optimal formulae, in particular for the short\nproduct modulo X 5 and the circulant product modulo (X 5 -- 1). Moreover , we\nare able to prove that there is essentially only one optimal decomposition of\nthe product of 3 x 2 by 2 x 3 matrices up to the action of some group of\nautomorphisms. \n\n"}
{"id": "1705.08213", "contents": "Title: Parallel Accelerated Custom Correlation Coefficient Calculations for\n  Genomics Applications Abstract: The massive quantities of genomic data being made available through gene\nsequencing techniques are enabling breakthroughs in genomic science in many\nareas such as medical advances in the diagnosis and treatment of diseases.\nAnalyzing this data, however, is a computational challenge insofar as the\ncomputational costs of the relevant algorithms can grow with quadratic, cubic\nor higher complexity-leading to the need for leadership scale computing. In\nthis paper we describe a new approach to calculations of the Custom Correlation\nCoefficient (CCC) between Single Nucleotide Polymorphisms (SNPs) across a\npopulation, suitable for parallel systems equipped with graphics processing\nunits (GPUs) or Intel Xeon Phi processors. We describe the mapping of the\nalgorithms to accelerated processors, techniques used for eliminating redundant\ncalculations due to symmetries, and strategies for efficient mapping of the\ncalculations to many-node parallel systems. Results are presented demonstrating\nhigh per-node performance and near-ideal parallel scalability with rates of\nmore than nine quadrillion elementwise comparisons achieved per second with the\nlatest optimized code on the ORNL Titan system, this being orders of magnitude\nfaster than rates achieved using other codes and platforms as reported in the\nliterature. Also it is estimated that as many as 90 quadrillion comparisons per\nsecond may be achievable on the upcoming ORNL Summit system, an additional 10X\nperformance increase. In a companion paper we describe corresponding techniques\napplied to calculations of the Proportional Similarity metric for comparative\ngenomics applications. \n\n"}
{"id": "1705.08282", "contents": "Title: Algorithms and hardness results for happy coloring problems Abstract: In a vertex-colored graph, an edge is happy if its endpoints have the same\ncolor. Similarly, a vertex is happy if all its incident edges are happy.\nMotivated by the computation of homophily in social networks, we consider the\nalgorithmic aspects of the following Maximum Happy Edges (k-MHE) problem: given\na partially k-colored graph G, find an extended full k-coloring of G maximizing\nthe number of happy edges. When we want to maximize the number of happy\nvertices, the problem is known as Maximum Happy Vertices (k-MHV). We further\nstudy the complexity of the problems and their weighted variants. For instance,\nwe prove that for every k >= 3, both problems are NP-complete for bipartite\ngraphs and k-MHV remains hard for split graphs. In terms of exact algorithms,\nwe show both problems can be solved in time O*(2^n), and give an even faster\nO*(1.89^n)-time algorithm when k = 3. From a parameterized perspective, we give\na linear vertex kernel for Weighted k-MHE, where edges are weighted and the\ngoal is to obtain happy edges of at least a specified total weight. Finally, we\nprove both problems are solvable in polynomial-time when the graph has bounded\ntreewidth or bounded neighborhood diversity. \n\n"}
{"id": "1705.08350", "contents": "Title: Bounding Cache Miss Costs of Multithreaded Computations Under General\n  Schedulers Abstract: We analyze the caching overhead incurred by a class of multithreaded\nalgorithms when scheduled by an arbitrary scheduler. We obtain bounds that\nmatch or improve upon the well-known $O(Q+S \\cdot (M/B))$ caching cost for the\nrandomized work stealing (RWS) scheduler, where $S$ is the number of steals,\n$Q$ is the sequential caching cost, and $M$ and $B$ are the cache size and\nblock (or cache line) size respectively. \n\n"}
{"id": "1705.10065", "contents": "Title: Counting Subwords Occurrences in Base-b Expansions Abstract: We count the number of distinct (scattered) subwords occurring in the base-b\nexpansion of the non-negative integers. More precisely, we consider the\nsequence $(S_b(n))_{n\\ge 0}$ counting the number of positive entries on each\nrow of a generalization of the Pascal triangle to binomial coefficients of\nbase-$b$ expansions. By using a convenient tree structure, we provide\nrecurrence relations for $(S_b(n))_{n\\ge 0}$ leading to the $b$-regularity of\nthe latter sequence. Then we deduce the asymptotics of the summatory function\nof the sequence $(S_b(n))_{n\\ge 0}$. \n\n"}
{"id": "1705.10797", "contents": "Title: The cocoon emission - an electromagnetic counterpart to gravitational\n  waves from neutron star mergers Abstract: Short Gamma-Ray Bursts (SGRBs) are believed to arise from compact binary\nmergers (either neutron star-neutron star or black hole-neutron star). If so\ntheir jets must penetrate outflows that are ejected during the merger. As a jet\ncrosses the ejecta it dissipates its energy, producing a hot cocoon which\nsurrounds it. We present here 3D numerical simulations of jet propagation in\nmergers' outflows and we calculate the resulting emission. This emission\nconsists of two components: the cooling emission, the leakage of the thermal\nenergy of the hot cocoon, and the cocoon macronova that arises from the\nradioactive decay of the cocoon's material. This emission gives a brief (~ one\nhour) blue, wide angle signal. While the parameters of the outflow and jet are\nuncertain, for the configurations we have considered the signal is bright (~\n-14 $-$ -15 absolute magnitude) and outshines all other predicted UV-optical\nsignals. The signal is brighter when the jet breakout time is longer and its\npeak brightness does not depend strongly on the highly uncertain opacity. A\nrapid search for such a signal is a promising strategy to detect an\nelectromagnetic merger counterpart. A detected candidate could be then followed\nby deep IR searches for the longer but weaker macronova arising from the rest\nof the ejecta. \n\n"}
{"id": "1706.00295", "contents": "Title: Completing graphs to metric spaces Abstract: We prove that certain classes of metrically homogeneous graphs omitting\ntriangles of odd short perimeter as well as triangles of long perimeter have\nthe extension property for partial automorphisms and we describe their Ramsey\nexpansions. \n\n"}
{"id": "1706.00617", "contents": "Title: Exploring the complexity of layout parameters in tournaments and\n  semi-complete digraphs Abstract: A simple digraph is semi-complete if for any two of its vertices $u$ and $v$,\nat least one of the arcs $(u,v)$ and $(v,u)$ is present. We study the\ncomplexity of computing two layout parameters of semi-complete digraphs:\ncutwidth and optimal linear arrangement (OLA). We prove that: (1) Both\nparameters are $\\mathsf{NP}$-hard to compute and the known exact and\nparameterized algorithms for them have essentially optimal running times,\nassuming the Exponential Time Hypothesis; (2) The cutwidth parameter admits a\nquadratic Turing kernel, whereas it does not admit any polynomial kernel unless\n$\\mathsf{NP}\\subseteq \\mathsf{coNP}/\\textrm{poly}$. By contrast, OLA admits a\nlinear kernel. These results essentially complete the complexity analysis of\ncomputing cutwidth and OLA on semi-complete digraphs. Our techniques can be\nalso used to analyze the sizes of minimal obstructions for having small\ncutwidth under the induced subdigraph relation. \n\n"}
{"id": "1706.02356", "contents": "Title: Cover time for random walks on arbitrary complex networks Abstract: We present an analytical method for computing the mean cover time of a random\nwalk process on arbitrary, complex networks. The cover time is defined as the\ntime a random walker requires to visit every node in the network at least once.\nThis quantity is particularly important for random search processes and target\nlocalization in network topologies. Based on the global mean first passage time\nof target nodes we derive an estimate for the cumulative distribution function\nof the cover time based on first passage time statistics. We show that our\nresult can be applied to various model networks, including Erd\\H{o}s-R\\'enyi\nand Barab\\'asi-Albert networks, as well as various real-world networks. Our\nresults reveal an intimate link between first passage and cover time statistics\nin networks in which structurally induced temporal correlations decay quickly\nand offer a computationally efficient way for estimating cover times in network\nrelated applications. \n\n"}
{"id": "1706.03175", "contents": "Title: Recovery Guarantees for One-hidden-layer Neural Networks Abstract: In this paper, we consider regression problems with one-hidden-layer neural\nnetworks (1NNs). We distill some properties of activation functions that lead\nto $\\mathit{local~strong~convexity}$ in the neighborhood of the ground-truth\nparameters for the 1NN squared-loss objective. Most popular nonlinear\nactivation functions satisfy the distilled properties, including rectified\nlinear units (ReLUs), leaky ReLUs, squared ReLUs and sigmoids. For activation\nfunctions that are also smooth, we show $\\mathit{local~linear~convergence}$\nguarantees of gradient descent under a resampling rule. For homogeneous\nactivations, we show tensor methods are able to initialize the parameters to\nfall into the local strong convexity region. As a result, tensor initialization\nfollowed by gradient descent is guaranteed to recover the ground truth with\nsample complexity $ d \\cdot \\log(1/\\epsilon) \\cdot \\mathrm{poly}(k,\\lambda )$\nand computational complexity $n\\cdot d \\cdot \\mathrm{poly}(k,\\lambda) $ for\nsmooth homogeneous activations with high probability, where $d$ is the\ndimension of the input, $k$ ($k\\leq d$) is the number of hidden nodes,\n$\\lambda$ is a conditioning property of the ground-truth parameter matrix\nbetween the input layer and the hidden layer, $\\epsilon$ is the targeted\nprecision and $n$ is the number of samples. To the best of our knowledge, this\nis the first work that provides recovery guarantees for 1NNs with both sample\ncomplexity and computational complexity $\\mathit{linear}$ in the input\ndimension and $\\mathit{logarithmic}$ in the precision. \n\n"}
{"id": "1706.03592", "contents": "Title: Gamma-ray pulsars with Fermi Abstract: In 8 years of operation, the Large Area Telescope (LAT) on the Fermi\nsatellite has impacted our understanding of gamma-ray pulsars dramatically. The\nLAT now sees over two hundred pulsars: the largest class of GeV sources in the\nMilky Way. They are diverse -- radio loud versus quiet, young versus\nmillisecond, in evolving binary systems versus isolated, and so on. Relatively\nfew of the GeV pulsars have also been seen in soft gamma rays. After an\noverview, we present 10 new radio pulsars, six young and four recycled, for\nwhich we detect gamma-ray pulsations. \n\n"}
{"id": "1706.04889", "contents": "Title: Improved Set-based Symbolic Algorithms for Parity Games Abstract: Graph games with {\\omega}-regular winning conditions provide a mathematical\nframework to analyze a wide range of problems in the analysis of reactive\nsystems and programs (such as the synthesis of reactive systems, program\nrepair, and the verification of branching time properties). Parity conditions\nare canonical forms to specify {\\omega}-regular winning conditions. Graph games\nwith parity conditions are equivalent to {\\mu}-calculus model checking, and\nthus a very important algorithmic problem. Symbolic algorithms are of great\nsignificance because they provide scalable algorithms for the analysis of large\nfinite-state systems, as well as algorithms for the analysis of infinite-state\nsystems with finite quotient. A set-based symbolic algorithm uses the basic set\noperations and the one-step predecessor operators. We consider graph games with\n$n$ vertices and parity conditions with $c$ priorities. While many explicit\nalgorithms exist for graph games with parity conditions, for set-based symbolic\nalgorithms there are only two algorithms (notice that we use space to refer to\nthe number of sets stored by a symbolic algorithm): (a) the basic algorithm\nthat requires $O(n^c)$ symbolic operations and linear space; and (b) an\nimproved algorithm that requires $O(n^{c/2+1})$ symbolic operations but also\n$O(n^{c/2+1})$ space (i.e., exponential space). In this work we present two\nset-based symbolic algorithms for parity games: (a) our first algorithm\nrequires $O(n^{c/2+1})$ symbolic operations and only requires linear space; and\n(b) developing on our first algorithm, we present an algorithm that requires\n$O(n^{c/3+1})$ symbolic operations and only linear space. We also present the\nfirst linear space set-based symbolic algorithm for parity games that requires\nat most a sub-exponential number of symbolic operations. \n\n"}
{"id": "1706.05379", "contents": "Title: Stellar equilibrium in semiclassical gravity Abstract: The phenomenon of quantum vacuum polarization in the presence of a\ngravitational field is well understood and is expected to have a physical\nreality, but studies of its back-reaction on the dynamics of spacetime are\npractically non-existent outside the specific context of homogeneous\ncosmologies. Building on previous results of quantum field theory in curved\nspacetimes, in this letter we first derive the semiclassical equations of\nstellar equilibrium in the s-wave Polyakov approximation. It is highlighted\nthat incorporating the polarization of the quantum vacuum leads to a\ngeneralization of the classical Tolman-Oppenheimer-Volkoff equation. Despite\nthe complexity of the resulting field equations, it is possible to find exact\nsolutions. Aside from being the first known exact solutions that describe\nrelativistic stars including the non-perturbative backreaction of semiclassical\neffects, these are identified as a non-trivial combination of the black star\nand gravastar proposals. \n\n"}
{"id": "1706.05738", "contents": "Title: Fourier-Based Testing for Families of Distributions Abstract: We study the general problem of testing whether an unknown distribution\nbelongs to a specified family of distributions. More specifically, given a\ndistribution family $\\mathcal{P}$ and sample access to an unknown discrete\ndistribution $\\mathbf{P}$, we want to distinguish (with high probability)\nbetween the case that $\\mathbf{P} \\in \\mathcal{P}$ and the case that\n$\\mathbf{P}$ is $\\epsilon$-far, in total variation distance, from every\ndistribution in $\\mathcal{P}$. This is the prototypical hypothesis testing\nproblem that has received significant attention in statistics and, more\nrecently, in theoretical computer science.\n  The sample complexity of this general inference task depends on the\nunderlying family $\\mathcal{P}$. The gold standard in distribution property\ntesting is to design sample-optimal and computationally efficient algorithms\nfor this task. The main contribution of this work is a simple and general\ntesting technique that is applicable to all distribution families whose Fourier\nspectrum satisfies a certain approximate sparsity property. To the best of our\nknowledge, ours is the first use of the Fourier transform in the context of\ndistribution testing.\n  We apply our Fourier-based framework to obtain near sample-optimal and\ncomputationally efficient testers for the following fundamental distribution\nfamilies: Sums of Independent Integer Random Variables (SIIRVs), Poisson\nMultinomial Distributions (PMDs), and Discrete Log-Concave Distributions. For\nthe first two, ours are the first non-trivial testers in the literature, vastly\ngeneralizing previous work on testing Poisson Binomial Distributions. For the\nthird, our tester improves on prior work in both sample and time complexity. \n\n"}
{"id": "1706.05767", "contents": "Title: On the arithmetic of graphs Abstract: The Zykov ring of signed finite simple graphs with topological join as\naddition and compatible multiplication is an integral domain but not a unique\nfactorization domain. We know that because by taking graph complements, it\nbecomes isomorphic to the strong Sabidussi ring with disjoint union as\naddition. We prove that the Euler characteristic is a ring homomorphism from\nthe strong ring to the integers by demonstrating that the strong ring is\nhomotopic to a Stanley-Reisner Cartesian ring. More generally, the Kuenneth\nformula holds on the strong ring so that the Poincare polynomial is compatible\nwith the ring structure. The Zykov ring has the clique number as a ring\nhomomorphism. Furthermore, the Cartesian ring has the property that the functor\nwhich attaches to a graph the spectrum of its connection Laplacian is\nmultiplicative. The reason is that the connection Laplacians do tensor under\nmultiplication, similarly to what the adjacency matrix does for the weak ring.\nThe strong ring product of two graphs contains both the weak and direct product\ngraphs as subgraphs. The Zykov, Sabidussi or Stanley-Reisner rings are so\nmanifestations of a network arithmetic which has remarkable cohomological\nproperties, dimension and spectral compatibility but where arithmetic questions\nlike the complexity of detecting primes or factoring are not yet studied well.\nWe illustrate the Zykov arithmetic with examples, especially from the subring\ngenerated by point graphs which contains spheres, stars or complete bipartite\ngraphs. While things are formulated in the language of graph theory, all\nconstructions generalize to the larger category of finite abstract simplicial\ncomplexes. \n\n"}
{"id": "1706.06301", "contents": "Title: The impact of red giant/AGB winds on AGN jet propagation Abstract: Dense stellar winds may mass-load the jets of active galactic nuclei,\nalthough it is unclear what are the time and spatial scales in which the mixing\ntakes place. We study the first steps of the interaction between jets and\nstellar winds, and also the scales at which the stellar wind may mix with the\njet and mass-load it. We present a detailed two-dimensional simulation,\nincluding thermal cooling, of a bubble formed by the wind of a star. We also\nstudy the first interaction of the wind bubble with the jet using a\nthree-dimensional simulation in which the star enters the jet. Stability\nanalysis is carried out for the shocked wind structure, to evaluate the\ndistances over which the jet-dragged wind, which forms a tail, can propagate\nwithout mixing with the jet flow. The two-dimensional simulations point at\nquick wind bubble expansion and fragmentation after about one bubble shock\ncrossing time. Three-dimensional simulations and stability analysis point at\nlocal mixing in the case of strong perturbations and relatively small density\nratios between the jet and the jet dragged-wind, and to a possibly more stable\nshocked wind structure at the phase of maximum tail mass flux. Analytical\nestimates also indicate that very early stages of the star jet-penetration time\nmay be also relevant for mass loading. The combination of these and previous\nresults from the literature suggest highly unstable interaction structures and\nefficient wind-jet flow mixing on the scale of the jet interaction height,\npossibly producing strong inhomogeneities within the jet. In addition, the\ninitial wind bubble shocked by the jet leads to a transient, large interaction\nsurface. The interaction structure can be a source of significant non-thermal\nemission. \n\n"}
{"id": "1706.07708", "contents": "Title: Study Morphology of Minimum Spanning Tree Problem and Generalized\n  Algorithms Abstract: In this paper, we study the form over the minimum spanning tree problem (MST)\nfrom which we will derive an intuitively generalized model and new methods with\nthe upper bound of runtimes of logarithm. The new pattern we made has taken\nsuccessful to better equilibrium the benefits of local and global when we\nemploy the strategy of divide and conquer to optimize solutions on problem.\nUnder new model, we let the course of clustering become more transparent with\nmany details, so that the whole solution may be featured of much reasonable,\nflexibility, efficiency and approach to reveal or reflect the reality. There\nare some important methods and avenues as fruits derived from discussions or\ntrial which can be broad usefulness in the fields of graphic analysis, data\nmining, k-means clustering problem and so forth. \n\n"}
{"id": "1706.08841", "contents": "Title: An Efficient Algorithm for Matrix-Valued and Vector-Valued Optimal Mass\n  Transport Abstract: We present an efficient algorithm for recent generalizations of optimal mass\ntransport theory to matrix-valued and vector-valued densities. These\ngeneralizations lead to several applications including diffusion tensor\nimaging, color images processing, and multi-modality imaging. The algorithm is\nbased on sequential quadratic programming (SQP). By approximating the Hessian\nof the cost and solving each iteration in an inexact manner, we are able to\nsolve each iteration with relatively low cost while still maintaining a fast\nconvergent rate. The core of the algorithm is solving a weighted Poisson\nequation, where different efficient preconditioners may be employed. We utilize\nincomplete Cholesky factorization, which yields an efficient and\nstraightforward solver for our problem. Several illustrative examples are\npresented for both the matrix and vector-valued cases. \n\n"}
{"id": "1706.09066", "contents": "Title: On the complexity of finding internally vertex-disjoint long directed\n  paths Abstract: For two positive integers $k$ and $\\ell$, a $(k \\times \\ell)$-spindle is the\nunion of $k$ pairwise internally vertex-disjoint directed paths with $\\ell$\narcs between two vertices $u$ and $v$. We are interested in the (parameterized)\ncomplexity of several problems consisting in deciding whether a given digraph\ncontains a subdivision of a spindle, which generalize both the Maximum Flow and\nLongest Path problems. We obtain the following complexity dichotomy: for a\nfixed $\\ell \\geq 1$, finding the largest $k$ such that an input digraph $G$\ncontains a subdivision of a $(k \\times \\ell)$-spindle is polynomial-time\nsolvable if $\\ell \\leq 3$, and NP-hard otherwise. We place special emphasis on\nfinding spindles with exactly two paths and present FPT algorithms that are\nasymptotically optimal under the ETH. These algorithms are based on the\ntechnique of representative families in matroids, and use also color-coding as\na subroutine. Finally, we study the case where the input graph is acyclic, and\npresent several algorithmic and hardness results. \n\n"}
{"id": "1706.09339", "contents": "Title: Lossy Kernels for Connected Dominating Set on Sparse Graphs Abstract: For $\\alpha > 1$, an $\\alpha$-approximate (bi-)kernel is a polynomial-time\nalgorithm that takes as input an instance $(I, k)$ of a problem $\\mathcal{Q}$\nand outputs an instance $(I',k')$ (of a problem $\\mathcal{Q}'$) of size bounded\nby a function of $k$ such that, for every $c\\geq 1$, a $c$-approximate solution\nfor the new instance can be turned into a $(c\\cdot\\alpha)$-approximate solution\nof the original instance in polynomial time. This framework of lossy\nkernelization was recently introduced by Lokshtanov et al. We study Connected\nDominating Set (and its distance-$r$ variant) parameterized by solution size on\nsparse graph classes like biclique-free graphs, classes of bounded expansion,\nand nowhere dense classes. We prove that for every $\\alpha>1$, Connected\nDominating Set admits a polynomial-size $\\alpha$-approximate (bi-)kernel on all\nthe aforementioned classes. Our results are in sharp contrast to the\nkernelization complexity of Connected Dominating Set, which is known to not\nadmit a polynomial kernel even on $2$-degenerate graphs and graphs of bounded\nexpansion, unless $\\textsf{NP} \\subseteq \\textsf{coNP/poly}$. We complement our\nresults by the following conditional lower bound. We show that if a class\n$\\mathcal{C}$ is somewhere dense and closed under taking subgraphs, then for\nsome value of $r\\in\\mathbb{N}$ there cannot exist an $\\alpha$-approximate\nbi-kernel for the (Connected) Distance-$r$ Dominating Set problem on\n$\\mathcal{C}$ for any $\\alpha>1$ (assuming the Gap Exponential Time\nHypothesis). \n\n"}
{"id": "1706.09608", "contents": "Title: Token Jumping in minor-closed classes Abstract: Given two $k$-independent sets $I$ and $J$ of a graph $G$, one can ask if it\nis possible to transform the one into the other in such a way that, at any\nstep, we replace one vertex of the current independent set by another while\nkeeping the property of being independent. Deciding this problem, known as the\nToken Jumping (TJ) reconfiguration problem, is PSPACE-complete even on planar\ngraphs. Ito et al. proved in 2014 that the problem is FPT parameterized by $k$\nif the input graph is $K_{3,\\ell}$-free.\n  We prove that the result of Ito et al. can be extended to any\n$K_{\\ell,\\ell}$-free graphs. In other words, if $G$ is a $K_{\\ell,\\ell}$-free\ngraph, then it is possible to decide in FPT-time if $I$ can be transformed into\n$J$. As a by product, the TJ-reconfiguration problem is FPT in many well-known\nclasses of graphs such as any minor-free class. \n\n"}
{"id": "1707.00362", "contents": "Title: Dynamic Parameterized Problems and Algorithms Abstract: Fixed-parameter algorithms and kernelization are two powerful methods to\nsolve $\\mathsf{NP}$-hard problems. Yet, so far those algorithms have been\nlargely restricted to static inputs.\n  In this paper we provide fixed-parameter algorithms and kernelizations for\nfundamental $\\mathsf{NP}$-hard problems with dynamic inputs. We consider a\nvariety of parameterized graph and hitting set problems which are known to have\n$f(k)n^{1+o(1)}$ time algorithms on inputs of size $n$, and we consider the\nquestion of whether there is a data structure that supports small updates (such\nas edge/vertex/set/element insertions and deletions) with an update time of\n$g(k)n^{o(1)}$; such an update time would be essentially optimal. Update and\nquery times independent of $n$ are particularly desirable. Among many other\nresults, we show that Feedback Vertex Set and $k$-Path admit dynamic algorithms\nwith $f(k)\\log^{O(1)}n$ update and query times for some function $f$ depending\non the solution size $k$ only.\n  We complement our positive results by several conditional and unconditional\nlower bounds. For example, we show that unlike their undirected counterparts,\nDirected Feedback Vertex Set and Directed $k$-Path do not admit dynamic\nalgorithms with $n^{o(1)}$ update and query times even for constant solution\nsizes $k\\leq 3$, assuming popular hardness hypotheses. We also show that\nunconditionally, in the cell probe model, Directed Feedback Vertex Set cannot\nbe solved with update time that is purely a function of $k$. \n\n"}
{"id": "1707.01470", "contents": "Title: On Directed Feedback Vertex Set parameterized by treewidth Abstract: We study the Directed Feedback Vertex Set problem parameterized by the\ntreewidth of the input graph. We prove that unless the Exponential Time\nHypothesis fails, the problem cannot be solved in time $2^{o(t\\log t)}\\cdot\nn^{\\mathcal{O}(1)}$ on general directed graphs, where $t$ is the treewidth of\nthe underlying undirected graph. This is matched by a dynamic programming\nalgorithm with running time $2^{\\mathcal{O}(t\\log t)}\\cdot n^{\\mathcal{O}(1)}$.\nOn the other hand, we show that if the input digraph is planar, then the\nrunning time can be improved to $2^{\\mathcal{O}(t)}\\cdot n^{\\mathcal{O}(1)}$. \n\n"}
{"id": "1707.01797", "contents": "Title: Turing Kernelization for Finding Long Paths in Graph Classes Excluding a\n  Topological Minor Abstract: The notion of Turing kernelization investigates whether a polynomial-time\nalgorithm can solve an NP-hard problem, when it is aided by an oracle that can\nbe queried for the answers to bounded-size subproblems. One of the main open\nproblems in this direction is whether k-Path admits a polynomial Turing kernel:\ncan a polynomial-time algorithm determine whether an undirected graph has a\nsimple path of length k, using an oracle that answers queries of size poly(k)?\n  We show this can be done when the input graph avoids a fixed graph H as a\ntopological minor, thereby significantly generalizing an earlier result for\nbounded-degree and $K_{3,t}$-minor-free graphs. Moreover, we show that k-Path\neven admits a polynomial Turing kernel when the input graph is not\nH-topological-minor-free itself, but contains a known vertex modulator of size\nbounded polynomially in the parameter, whose deletion makes it so. To obtain\nour results, we build on the graph minors decomposition to show that any\nH-topological-minor-free graph that does not contain a k-path, has a separation\nthat can safely be reduced after communication with the oracle. \n\n"}
{"id": "1707.03556", "contents": "Title: Core forging and local limit theorems for the k-core of random graphs Abstract: We establish a multivariate local limit theorem for the order and size as\nwell as several other parameters of the k-core of the Erdos-Renyi graph. The\nproof is based on a novel approach to the k-core problem that replaces the\nmeticulous analysis of the peeling process by a generative model of graphs with\na core of a given order and size. The generative model, which is inspired by\nthe Warning Propagation message passing algorithm, facilitates the direct study\nof properties of the core and its connections with the mantle and should\ntherefore be of interest in its own right. \n\n"}
{"id": "1707.04310", "contents": "Title: Topological Sorting under Regular Constraints Abstract: We introduce the constrained topological sorting problem (CTS): given a\nregular language K and a directed acyclic graph G with labeled vertices,\ndetermine if G has a topological sort that forms a word in K. This natural\nproblem applies to several settings, e.g., scheduling with costs or verifying\nconcurrent programs. We consider the problem CTS[K] where the target language K\nis fixed, and study its complexity depending on K. We show that CTS[K] is\ntractable when K falls in several language families, e.g., unions of monomials,\nwhich can be used for pattern matching. However, we show that CTS[K] is NP-hard\nfor K = (ab)^* and introduce a shuffle reduction technique to show hardness for\nmore languages. We also study the special case of the constrained shuffle\nproblem (CSh), where the input graph is a disjoint union of strings, and show\nthat CSh[K] is additionally tractable when K is a group language or a union of\ndistrict group monomials. We conjecture that a dichotomy should hold on the\ncomplexity of CTS[K] or CSh[K] depending on K, and substantiate this by proving\na coarser dichotomy under a different problem phrasing which ensures that\ntractable languages are closed under common operators. \n\n"}
{"id": "1707.05784", "contents": "Title: A VLA Study of High-redshift GRBs II - The Complex Radio Afterglow of\n  GRB 140304A: Shell Collisions and Two Reverse Shocks Abstract: We present detailed multi-frequency, multi-epoch radio observations of GRB\n140304A at $z=5.283$ from 1 to 86 GHz and 0.45 d to 89 d. The radio and mm data\nexhibit unusual multiple spectral components, which cannot be simply explained\nby standard forward and reverse shock scenarios. Through detailed\nmulti-wavelength analysis spanning radio to X-rays, we constrain the forward\nshock parameters to $E_{\\rm K, iso}\\approx4.9\\times10^{54}\\,$erg, $A_* \\approx\n2.6\\times10^{-2}$, $\\epsilon_{\\rm e}\\approx2.5\\times10^{-2}$, $\\epsilon_{\\rm\nB}\\approx5.9\\times10^{-2}$, $p\\approx2.6$, and $\\theta_{\\rm\njet}\\approx1.1^{\\circ}$, yielding a beaming corrected $\\gamma$-ray and kinetic\nenergy, $E_{\\gamma}\\approx2.3\\times10^{49}\\,$erg and $E_{\\rm\nK}\\approx9.5\\times10^{50}\\,$erg, respectively. We model the excess radio\nemission as due to a combination of a late-time reverse shock (RS) launched by\na shell collision, which also produces a re-brightening in the X-rays at\n$\\approx0.26\\,$d, and either a standard RS or diffractive interstellar\nscintillation. Under the standard RS interpretation, we invoke consistency\narguments between the forward and reverse shocks to derive a deceleration time,\n$t_{\\rm dec}\\approx100\\,$s, the ejecta Lorentz factor, $\\Gamma(t_{\\rm\ndec})\\approx300$, and a low RS magnetization, $R_{\\rm B}\\approx0.6$. Our\nobservations highlight both the power of radio observations in capturing RS\nemission and thus constraining the properties of GRB ejecta and central\nengines, and the challenge presented by interstellar scintillation in\nconclusively identifying RS emission in GRB radio afterglows. \n\n"}
{"id": "1707.06499", "contents": "Title: Parameterized Approximation Algorithms for Bidirected Steiner Network\n  Problems Abstract: The Directed Steiner Network (DSN) problem takes as input a directed\nedge-weighted graph $G=(V,E)$ and a set $\\mathcal{D}\\subseteq V\\times V$ of $k$\ndemand pairs. The aim is to compute the cheapest network $N\\subseteq G$ for\nwhich there is an $s\\to t$ path for each $(s,t)\\in\\mathcal{D}$. It is known\nthat this problem is notoriously hard as there is no\n$k^{1/4-o(1)}$-approximation algorithm under Gap-ETH, even when parametrizing\nthe runtime by $k$ [Dinur & Manurangsi, ITCS 2018]. In light of this, we\nsystematically study several special cases of DSN and determine their\nparameterized approximability for the parameter $k$.\n  For the bi-DSN$_\\text{Planar}$ problem, the aim is to compute a solution\n$N\\subseteq G$ whose cost is at most that of an optimum planar solution in a\nbidirected graph $G$, i.e., for every edge $uv$ of $G$ the reverse edge $vu$\nexists and has the same weight. This problem is a generalization of several\nwell-studied special cases. Our main result is that this problem admits a\nparameterized approximation scheme (PAS) for $k$. We also prove that our result\nis tight in the sense that (a) the runtime of our PAS cannot be significantly\nimproved, and (b) it is unlikely that a PAS exists for any generalization of\nbi-DSN$_\\text{Planar}$, unless FPT=W[1].\n  One important special case of DSN is the Strongly Connected Steiner Subgraph\n(SCSS) problem, for which the solution network $N\\subseteq G$ needs to strongly\nconnect a given set of $k$ terminals. It has been observed before that for SCSS\na parameterized $2$-approximation exists when parameterized by $k$ [Chitnis et\nal., IPEC 2013]. We give a tight inapproximability result by showing that for\n$k$ no parameterized $(2-\\varepsilon)$-approximation algorithm exists under\nGap-ETH. Additionally we show that when restricting the input of SCSS to\nbidirected graphs, the problem remains NP-hard but becomes FPT for $k$. \n\n"}
{"id": "1707.06919", "contents": "Title: Secondary antinuclei from supernova remnants and background for dark\n  matter searches Abstract: We compute the spectra of cosmic-ray (CR) nuclei and antinuclei under a\nscenario where hadronic interaction processes inside supernova remnants (SNRs)\ncan produce a diffusively-shock-accelerated \"source component\" of secondary\nparticles. This scenario is able to explain the recent measurements reported by\nAMS on the antiproton/proton ratio, that is found to be remarkably constant at\n~60-450 GeV of kinetic energy. However, as we will show, this explanation is\nruled out by the new AMS data on the B/C ratio, which is found to decrease\nsteadily up to TeV/n energies. With the constraints provided by the two ratios,\nwe calculate conservative (B/C driven) and speculative (pbar/p driven)\nSNR-induced flux contribution for the spectra of antideuteron and antihelium in\nCRs, along with their standard secondary component expected from CR collisions\nin the interstellar gas. We found that the SNR component of anti-nuclei can be\nsignificantly large at high-energy, above a few ~10 GeV/n, but it is always\nsub-dominant at sub-GeV/n energies, that is, the energy region where\ndark-matter induced signals may exceed the standard astrophysical background.\nFurthermore, the total antinuclei flux from insterstellar spallation plus\nSNR-component is tightly bounded by the data, so that hadronic production in\nSNRs has a minor impact on the astrophysical background for dark matter\nsearches. \n\n"}
{"id": "1707.08473", "contents": "Title: Hard Cosmic Ray Sea in the Galactic Center: a consistent interpretation\n  of H.E.S.S. and Fermi-LAT $\\gamma$-ray data Abstract: We present a novel interpretation of the gamma-ray diffuse emission measured\nby H.E.S.S. in the Galactic Center (GC) region and the Galactic ridge. Our\nstarting base is an updated analysis of PASS8 Fermi-LAT data, which allows to\nextend down to few GeV the spectra measured by H.E.S.S. and to infer the\nprimary CR radial distribution above 100 GeV. We compare those results with a\nCR transport model assuming a harder scaling of the diffusion coefficient with\nrigidity in the inner Galaxy. Such a behavior reproduces the radial dependence\nof the CR spectral index recently inferred from Fermi-LAT measurements in the\ninner GP. We find that, in this scenario, the bulk of the Galactic ridge\nemission can be naturally explained by the interaction of the diffuse,\nsteady-state Galactic CR sea interacting with the gas present in the Central\nmolecular zone. The evidence of a GC PeVatron is significantly weaker than that\ninferred adopting a conventional (softer) CR sea. \n\n"}
{"id": "1708.00002", "contents": "Title: Which Distribution Distances are Sublinearly Testable? Abstract: Given samples from an unknown distribution $p$ and a description of a\ndistribution $q$, are $p$ and $q$ close or far? This question of \"identity\ntesting\" has received significant attention in the case of testing whether $p$\nand $q$ are equal or far in total variation distance. However, in recent work,\nthe following questions have been been critical to solving problems at the\nfrontiers of distribution testing:\n  -Alternative Distances: Can we test whether $p$ and $q$ are far in other\ndistances, say Hellinger?\n  -Tolerance: Can we test when $p$ and $q$ are close, rather than equal? And if\nso, close in which distances?\n  Motivated by these questions, we characterize the complexity of distribution\ntesting under a variety of distances, including total variation, $\\ell_2$,\nHellinger, Kullback-Leibler, and $\\chi^2$. For each pair of distances $d_1$ and\n$d_2$, we study the complexity of testing if $p$ and $q$ are close in $d_1$\nversus far in $d_2$, with a focus on identifying which problems allow strongly\nsublinear testers (i.e., those with complexity $O(n^{1 - \\gamma})$ for some\n$\\gamma > 0$ where $n$ is the size of the support of the distributions $p$ and\n$q$). We provide matching upper and lower bounds for each case. We also study\nthese questions in the case where we only have samples from $q$ (equivalence\ntesting), showing qualitative differences from identity testing in terms of\nwhen tolerance can be achieved. Our algorithms fall into the classical paradigm\nof $\\chi^2$-statistics, but require crucial changes to handle the challenges\nintroduced by each distance we consider. Finally, we survey other recent\nresults in an attempt to serve as a reference for the complexity of various\ndistribution testing problems. \n\n"}
{"id": "1708.01379", "contents": "Title: The Fluorescence detector Array of Single-pixel Telescopes:\n  Contributions to the 35th International Cosmic Ray Conference (ICRC 2017) Abstract: Contributions of the Fluorescence detector Array of Single-pixel Telescopes\n(FAST) to the 35th International Cosmic Ray Conference, 12-20 July 2017, Busan,\nKorea \n\n"}
{"id": "1708.03495", "contents": "Title: Algorithms based on *-algebras, and their applications to isomorphism of\n  polynomials with one secret, group isomorphism, and polynomial identity\n  testing Abstract: We consider two basic algorithmic problems concerning tuples of\n(skew-)symmetric matrices. The first problem asks to decide, given two tuples\nof (skew-)symmetric matrices $(B_1, \\dots, B_m)$ and $(C_1, \\dots, C_m)$,\nwhether there exists an invertible matrix $A$ such that for every $i\\in\\{1,\n\\dots, m\\}$, $A^tB_iA=C_i$. We show that this problem can be solved in\nrandomized polynomial time over finite fields of odd size, the real field, and\nthe complex field. The second problem asks to decide, given a tuple of square\nmatrices $(B_1, \\dots, B_m)$, whether there exist invertible matrices $A$ and\n$D$, such that for every $i\\in\\{1, \\dots, m\\}$, $AB_iD$ is (skew-)symmetric. We\nshow that this problem can be solved in deterministic polynomial time over\nfields of characteristic not $2$. For both problems we exploit the structure of\nthe underlying $*$-algebras, and utilize results and methods from the module\nisomorphism problem.\n  Applications of our results range from multivariate cryptography, group\nisomorphism, to polynomial identity testing. Specifically, these results imply\nefficient algorithms for the following problems. (1) Test isomorphism of\nquadratic forms with one secret over a finite field of odd size. This problem\nbelongs to a family of problems that serves as the security basis of certain\nauthentication schemes proposed by Patarin (Eurocrypto 1996). (2) Test\nisomorphism of $p$-groups of class 2 and exponent $p$ ($p$ odd) with order\n$p^k$ in time polynomial in the group order, when the commutator subgroup is of\norder $p^{O(\\sqrt{k})}$. (3) Deterministically reveal two families of\nsingularity witnesses caused by the skew-symmetric structure, which represents\na natural next step for the polynomial identity testing problem following the\ndirection set up by the recent resolution of the non-commutative rank problem\n(Garg et al., FOCS 2016; Ivanyos et al., ITCS 2017). \n\n"}
{"id": "1708.03515", "contents": "Title: New Tools and Connections for Exponential-time Approximation Abstract: In this paper, we develop new tools and connections for exponential time\napproximation. In this setting, we are given a problem instance and a parameter\n$\\alpha>1$, and the goal is to design an $\\alpha$-approximation algorithm with\nthe fastest possible running time. We show the following results:\n  - An $r$-approximation for maximum independent set in $O^*(\\exp(\\tilde O(n/r\n\\log^2 r+r\\log^2r)))$ time,\n  - An $r$-approximation for chromatic number in $O^*(\\exp(\\tilde{O}(n/r \\log\nr+r\\log^2r)))$ time,\n  - A $(2-1/r)$-approximation for minimum vertex cover in\n$O^*(\\exp(n/r^{\\Omega(r)}))$ time, and\n  - A $(k-1/r)$-approximation for minimum $k$-hypergraph vertex cover in\n$O^*(\\exp(n/(kr)^{\\Omega(kr)}))$ time.\n  (Throughout, $\\tilde O$ and $O^*$ omit $\\mathrm{polyloglog}(r)$ and factors\npolynomial in the input size, respectively.) The best known time bounds for all\nproblems were $O^*(2^{n/r})$ [Bourgeois et al. 2009, 2011 & Cygan et al. 2008].\nFor maximum independent set and chromatic number, these bounds were\ncomplemented by $\\exp(n^{1-o(1)}/r^{1+o(1)})$ lower bounds (under the\nExponential Time Hypothesis (ETH)) [Chalermsook et al., 2013 & Laekhanukit,\n2014 (Ph.D. Thesis)]. Our results show that the naturally-looking\n$O^*(2^{n/r})$ bounds are not tight for all these problems. The key to these\nalgorithmic results is a sparsification procedure, allowing the use of better\napproximation algorithms for bounded degree graphs. For obtaining the first two\nresults, we introduce a new randomized branching rule.\n  Finally, we show a connection between PCP parameters and exponential-time\napproximation algorithms. This connection together with our independent set\nalgorithm refute the possibility to overly reduce the size of Chan's PCP [Chan,\n2016]. It also implies that a (significant) improvement over our result will\nrefute the gap-ETH conjecture [Dinur 2016 & Manurangsi and Raghavendra, 2016]. \n\n"}
{"id": "1708.03903", "contents": "Title: Distributed Exact Weighted All-Pairs Shortest Paths in $\\tilde\n  O(n^{5/4})$ Rounds Abstract: We study computing {\\em all-pairs shortest paths} (APSP) on distributed\nnetworks (the CONGEST model). The goal is for every node in the (weighted)\nnetwork to know the distance from every other node using communication. The\nproblem admits $(1+o(1))$-approximation $\\tilde O(n)$-time algorithms\n~\\cite{LenzenP-podc15,Nanongkai-STOC14}, which are matched with $\\tilde\n\\Omega(n)$-time lower\nbounds~\\cite{Nanongkai-STOC14,LenzenP_stoc13,FrischknechtHW12}\\footnote{$\\tilde\n\\Theta$, $\\tilde O$ and $\\tilde \\Omega$ hide polylogarithmic factors. Note that\nthe lower bounds also hold even in the unweighted case and in the weighted case\nwith polynomial approximation ratios.}. No $\\omega(n)$ lower bound or $o(m)$\nupper bound were known for exact computation.\n  In this paper, we present an $\\tilde O(n^{5/4})$-time randomized (Las Vegas)\nalgorithm for exact weighted APSP; this provides the first improvement over the\nnaive $O(m)$-time algorithm when the network is not so sparse. Our result also\nholds for the case where edge weights are {\\em asymmetric} (a.k.a. the directed\ncase where communication is bidirectional). Our techniques also yield an\n$\\tilde O(n^{3/4}k^{1/2}+n)$-time algorithm for the {\\em $k$-source shortest\npaths} problem where we want every node to know distances from $k$ sources;\nthis improves Elkin's recent bound~\\cite{Elkin-STOC17} when $k=\\tilde\n\\omega(n^{1/4})$. \n\n"}
{"id": "1708.04629", "contents": "Title: Mary, a pipeline to aid discovery of optical transients Abstract: The ability to quickly detect transient sources in optical images and trigger\nmulti-wavelength follow up is key for the discovery of fast transients. These\ninclude events rare and difficult to detect such as kilonovae, supernova shock\nbreakout, and \"orphan\" Gamma-ray Burst afterglows. We present the Mary\npipeline, a (mostly) automated tool to discover transients during high-cadenced\nobservations with the Dark Energy Camera (DECam) at CTIO. The observations are\npart of the \"Deeper Wider Faster\" program, a multi-facility, multi-wavelength\nprogram designed to discover fast transients, including counterparts to Fast\nRadio Bursts and gravitational waves. Our tests of the Mary pipeline on DECam\nimages return a false positive rate of ~2.2% and a missed fraction of ~3.4%\nobtained in less than 2 minutes, which proves the pipeline to be suitable for\nrapid and high-quality transient searches. The pipeline can be adapted to\nsearch for transients in data obtained with imagers other than DECam. \n\n"}
{"id": "1708.05128", "contents": "Title: The Giant Radio Array for Neutrino Detection (GRAND): Present and\n  Perspectives Abstract: The Giant Radio Array for Neutrino Detection (GRAND) aims at detecting\nultra-high energy extraterrestrial neutrinos via the extensive air showers\ninduced by the decay of tau leptons created in the interaction of neutrinos\nunder the Earth's surface. Consisting of an array of $\\sim10^5$ radio antennas\ndeployed over $\\sim 2\\times10^5\\,\\rm {km}^2$, GRAND plans to reach, for the\nfirst time, an all-flavor sensitivity of $\\sim1.5\\times10^{-10} \\,\\rm GeV\\,\ncm^{-2} \\,s^{-1}\\, sr^{-1}$ above $5\\times10^{17}$ eV and a sub-degree angular\nresolution, beyond the reach of other planned detectors. We describe here\npreliminary designs and simulation results, plans for the ongoing, staged\napproach to the construction of GRAND, and the rich research program made\npossible by GRAND's design sensitivity and angular resolution. \n\n"}
{"id": "1708.05380", "contents": "Title: Catching a star before explosion: the luminous blue variable progenitor\n  of SN 2015bh Abstract: In this paper we analyse the pre-explosion spectrum of SN2015bh by performing\nradiative transfer simulations using the CMFGEN code. This object has attracted\nsignificant attention due to its remarkable similarity to SN2009ip in both its\npre- and post-explosion behaviour. They seem to belong to a class of events for\nwhich the fate as a genuine core-collapse supernova or a non-terminal explosion\nis still under debate. Our CMFGEN models suggest that the progenitor of\nSN2015bh had an effective temperature between 8700 and 10000 K, luminosity in\nthe range ~ 1.8-4.74e6 Lsun, contained at least 25% H in mass at the surface,\nand half-solar Fe abundances. The results also show that the progenitor of SN\n2015bh generated an extended wind with a mass-loss rate of ~ 6e-4 to 1.5e-3\nMsun/yr and a velocity of 1000 km/s. We determined that the wind extended to at\nleast 2.57e14 cm and lasted for at least 30 days prior to the observations,\nreleasing 5e-5 Msun into the circumstellar medium. In analogy to 2009ip, we\npropose that this is the material that the explosive ejecta could interact at\nlate epochs, perhaps producing observable signatures that can be probed with\nfuture observations. We conclude that the progenitor of SN 2015bh was most\nlikely a warm luminous blue variable of at least 35 Msun before the explosion.\nConsidering the high wind velocity, we cannot exclude the possibility that the\nprogenitor was a Wolf-Rayet star that inflated just before the 2013 eruption,\nsimilar to HD5980 during its 1994 episode. If the star survived, late-time\nspectroscopy may reveal either a similar LBV or a Wolf-Rayet star, depending on\nthe mass of the H envelope before the explosion. If the star exploded as a\ngenuine SN, 2015bh would be a remarkable case of a successful explosion after\nblack-hole formation in a star with a possible minimum mass 35 Msun at the\npre-SN stage. \n\n"}
{"id": "1708.07355", "contents": "Title: Properties of the redback millisecond pulsar binary 3FGL J0212.1+5320 Abstract: Linares et al. (2016) obtained quasi-simultaneous g', r' and i-band light\ncurves and an absorption line radial velocity curve of the secondary star in\nthe redback system 3FGL J0212.1+5320. The light curves showed two maxima and\nminima primarily due to the secondary star's ellipsoidal modulation, but with\nunequal maxima and minima. We fit these light curves and radial velocities with\nour X-ray binary model including either a dark solar-type star spot or a hot\nspot due to off-centre heating from an intrabinary shock, to account for the\nunequal maxima. Both models give a radial velocity semi-amplitude and\nrotational broadening that agree with the observations. The observed secondary\nstar's effective temperature is best matched with the value obtained using the\nhot spot model, which gives a neutron star and secondary star mass of $M_{\\rm\n1}$=1.85$^{+0.32}_{-0.26}$ $M_{\\odot}$and $M_{\\rm 2}$=0.50$^{+0.22}_{-0.19}$\n$M_{\\odot}$, respectively. \n\n"}
{"id": "1708.08213", "contents": "Title: Constraining the Origin of Local Positrons with HAWC TeV Gamma-Ray\n  Observations of Two Nearby Pulsar Wind Nebulae Abstract: The HAWC Gamma-Ray Observatory has reported the discovery of TeV gamma-ray\nemission extending several degrees around the positions of Geminga and B0656+14\npulsars. Assuming these gamma rays are produced by inverse Compton scattering\noff low-energy photons in electron halos around the pulsars, we determine the\ndiffusion of electrons and positrons in the local interstellar medium. We will\npresent the morphological and spectral studies of these two VHE gamma-ray\nsources and the derived positron spectrum at Earth. \n\n"}
{"id": "1708.09398", "contents": "Title: Designing Strassen's algorithm Abstract: In 1969, Strassen shocked the world by showing that two n x n matrices could\nbe multiplied in time asymptotically less than $O(n^3)$. While the recursive\nconstruction in his algorithm is very clear, the key gain was made by showing\nthat 2 x 2 matrix multiplication could be performed with only 7 multiplications\ninstead of 8. The latter construction was arrived at by a process of\nelimination and appears to come out of thin air. Here, we give the simplest and\nmost transparent proof of Strassen's algorithm that we are aware of, using only\na simple unitary 2-design and a few easy lines of calculation. Moreover, using\nbasic facts from the representation theory of finite groups, we use 2-designs\ncoming from group orbits to generalize our construction to all n (although the\nresulting algorithms aren't optimal for n at least 3). \n\n"}
{"id": "1708.09714", "contents": "Title: Decoherence effect in neutrinos produced in micro-quasar jets Abstract: We study the effect of decoherence upon the neutrino spectra produced in\nmicro-quasar jets. In order to analyse the precession of the polarization\nvector of neutrinos we have calculated its time evolution by solving the\ncorresponding equations of motion, and by assuming two different scenarios,\nnamely: (i) the mixing between two active neutrinos, and (ii) the mixing\nbetween one active and one sterile neutrino. We have found that for the case\nwith two active neutrinos and large values of the neutrino-neutrino\ninteractions the onset of decoherence is not manifest. For the active-sterile\nscheme decoherence becomes manifest if the strength of the neutrino-neutrino\ninteractions ($\\mu$) and the ratio between the square-mass difference and the\nenergy ($\\omega=\\frac{\\delta m^2}{2 E}$) satisfy the relation\n$\\frac{\\mu}{w_{E=E_{\\rm min}}}>0.1$. \n\n"}
{"id": "1709.02674", "contents": "Title: Uniform generation of random graphs with power-law degree sequences Abstract: We give a linear-time algorithm that approximately uniformly generates a\nrandom simple graph with a power-law degree sequence whose exponent is at least\n2.8811. While sampling graphs with power-law degree sequence of exponent at\nleast 3 is fairly easy, and many samplers work efficiently in this case, the\nproblem becomes dramatically more difficult when the exponent drops below 3;\nours is the first provably practicable sampler for this case. We also show that\nwith an appropriate rejection scheme, our algorithm can be tuned into an exact\nuniform sampler. The running time of the exact sampler is O(n^{2.107}) with\nhigh probability, and O(n^{4.081}) in expectation. \n\n"}
{"id": "1709.02850", "contents": "Title: Mixed Integer Programming with Convex/Concave Constraints:\n  Fixed-Parameter Tractability and Applications to Multicovering and Voting Abstract: A classic result of Lenstra [Math.~Oper.~Res.~1983] says that an integer\nlinear program can be solved in fixed-parameter tractable (FPT) time for the\nparameter being the number of variables. We extend this result by incorporating\nnon-decreasing piecewise linear convex or concave functions to our (mixed)\ninteger programs. This general technique allows us to establish parameterized\ncomplexity of a number of classic computational problems. In particular, we\nprove that Weighted Set Multicover is in FPT when parameterized by the number\nof elements to cover, and that there exists an FPT-time approximation scheme\nfor Multiset Multicover for the same parameter. Further, we use our general\ntechnique to prove that a number of problems from computational social choice\n(e.g., problems related to bribery and control in elections) are in FPT when\nparameterized by the number of candidates. For bribery, this resolves a nearly\n10-year old family of open problems, and for weighted electoral control of\nApproval voting, this improves some previously known XP-memberships to\nFPT-memberships. \n\n"}
{"id": "1709.03859", "contents": "Title: A neighborhood-preserving translation operator on graphs Abstract: In this paper, we introduce translation operators on graphs. Contrary to\nspectrally-defined translations in the framework of graph signal processing,\nour operators mimic neighborhood-preserving properties of translation operators\ndefined in Euclidean spaces directly in the vertex domain, and therefore do not\ndeform a signal as it is translated. We show that in the case of grid graphs\nbuilt on top of a metric space, these operators exactly match underlying\nEuclidean translations, suggesting that they completely leverage the underlying\nmetric. More generally, these translations are defined on any graph, and can\ntherefore be used to process signals on those graphs. We show that identifying\nproposed translations is in general an NP-Complete problem. To cope with this\nissue, we introduce relaxed versions of these operators, and illustrate\ntranslation of signals on random graphs. \n\n"}
{"id": "1709.04963", "contents": "Title: Numerical investigation of kinetic turbulence in relativistic pair\n  plasmas I: Turbulence statistics Abstract: We describe results from particle-in-cell simulations of driven turbulence in\ncollisionless, magnetized, relativistic pair plasma. This physical regime\nprovides a simple setting for investigating the basic properties of kinetic\nturbulence and is relevant for high-energy astrophysical systems such as pulsar\nwind nebulae and astrophysical jets. In this paper, we investigate the\nstatistics of turbulent fluctuations in simulations on lattices of up to\n$1024^3$ cells and containing up to $2 \\times 10^{11}$ particles. Due to the\nabsence of a cooling mechanism in our simulations, turbulent energy dissipation\nreduces the magnetization parameter to order unity within a few dynamical\ntimes, causing turbulent motions to become sub-relativistic. In the developed\nstage, our results agree with predictions from magnetohydrodynamic turbulence\nphenomenology at inertial-range scales, including a power-law magnetic energy\nspectrum with index near $-5/3$, scale-dependent anisotropy of fluctuations\ndescribed by critical balance, log-normal distributions for particle density\nand internal energy density (related by a $4/3$ adiabatic index, as predicted\nfor an ultra-relativistic ideal gas), and the presence of intermittency. We\nalso present possible signatures of a kinetic cascade by measuring power-law\nspectra for the magnetic, electric, and density fluctuations at sub-Larmor\nscales. \n\n"}
{"id": "1709.07308", "contents": "Title: Predicting Positive and Negative Links with Noisy Queries: Theory &\n  Practice Abstract: Social networks involve both positive and negative relationships, which can\nbe captured in signed graphs. The {\\em edge sign prediction problem} aims to\npredict whether an interaction between a pair of nodes will be positive or\nnegative. We provide theoretical results for this problem that motivate natural\nimprovements to recent heuristics.\n  The edge sign prediction problem is related to correlation clustering; a\npositive relationship means being in the same cluster. We consider the\nfollowing model for two clusters: we are allowed to query any pair of nodes\nwhether they belong to the same cluster or not, but the answer to the query is\ncorrupted with some probability $0<q<\\frac{1}{2}$. Let $\\delta=1-2q$ be the\nbias. We provide an algorithm that recovers all signs correctly with high\nprobability in the presence of noise with $O(\\frac{n\\log\nn}{\\delta^2}+\\frac{\\log^2 n}{\\delta^6})$ queries. This is the best known result\nfor this problem for all but tiny $\\delta$, improving on the recent work of\nMazumdar and Saha \\cite{mazumdar2017clustering}. We also provide an algorithm\nthat performs $O(\\frac{n\\log n}{\\delta^4})$ queries, and uses breadth first\nsearch as its main algorithmic primitive. While both the running time and the\nnumber of queries for this algorithm are sub-optimal, our result relies on\nnovel theoretical techniques, and naturally suggests the use of edge-disjoint\npaths as a feature for predicting signs in online social networks.\nCorrespondingly, we experiment with using edge disjoint $s-t$ paths of short\nlength as a feature for predicting the sign of edge $(s,t)$ in real-world\nsigned networks. Empirical findings suggest that the use of such paths improves\nthe classification accuracy, especially for pairs of nodes with no common\nneighbors. \n\n"}
{"id": "1709.07869", "contents": "Title: NC Algorithms for Weighted Planar Perfect Matching and Related Problems Abstract: Consider a planar graph $G=(V,E)$ with polynomially bounded edge weight\nfunction $w:E\\to [0, poly(n)]$. The main results of this paper are NC\nalgorithms for the following problems:\n  - minimum weight perfect matching in $G$,\n  - maximum cardinality and maximum weight matching in $G$ when $G$ is\nbipartite,\n  - maximum multiple-source multiple-sink flow in $G$ where $c:E\\to [1,\npoly(n)]$ is a polynomially bounded edge capacity function,\n  - minimum weight $f$-factor in $G$ where $f:V\\to [1, poly(n)]$,\n  - min-cost flow in $G$ where $c:E\\to [1, poly(n)]$ is a polynomially bounded\nedge capacity function and $b:V\\to [1, poly(n)]$ is a polynomially bounded\nvertex demand function.\n  There have been no known NC algorithms for any of these problems previously\n(Before this and independent paper by Anari and Vazirani). In order to solve\nthese problems we develop a new relatively simple but versatile framework that\nis combinatorial in spirit. It handles the combinatorial structure of matchings\ndirectly and needs to only know weights of appropriately defined matchings from\nalgebraic subroutines. \n\n"}
{"id": "1709.08562", "contents": "Title: Resolving Dark Matter Subhalos With Future Sub-GeV Gamma-Ray Telescopes Abstract: Annihilating dark matter particles in nearby subhalos could generate\npotentially observable fluxes of gamma rays, unaccompanied by emission at other\nwavelengths. Furthermore, this gamma-ray emission is expected to be spatially\nextended, providing us with a powerful way to discriminate dark matter subhalos\nfrom other astrophysical gamma-ray sources. Fermi has detected two dark matter\nsubhalo candidates which exhibit a statistically significant degree of spatial\nextension (3FGL J2212.5+0703 and 3FGL J1924.8-1034). It has been argued that\nthe most likely non-dark matter interpretation of these observations is that\nthey are each in fact multiple nearby point sources, too close to one another\non the sky to be individually resolved. In this study, we consider the ability\nof next generation gamma-ray telescopes to spatially resolve the gamma-ray\nemission from subhalo candidates, focusing on the proposed e-ASTROGAM mission.\nWe find that such an instrument could significantly clarify the nature of\nFermi's dark matter subhalo candidates, and provide an unprecedented level of\nsensitivity to the presence of annihilating dark matter in nearby subhalos. \n\n"}
{"id": "1709.10258", "contents": "Title: An improved algorithm for recognizing matroids Abstract: Let $M$ be a matroid defined on a finite set $E$ and $L\\subset E$. $L$ is\nlocked in $M$ if $M|L$ and $M^*|(E\\backslash L)$ are 2-connected, and\n$min\\{r(L), r^*(E\\backslash L)\\} \\geq 2$. Locked subsets characterize\nnontrivial facets of the bases polytope. In this paper, we give a new axiom\nsystem for matroids based on locked subsets. We deduce an algorithm for\nrecognizing matroids improving the running time complexity of the best known\ntill today. This algorithm induces a polynomial time algorithm for recognizing\nuniform matroids. This latter problem is intractable if we use an independence\noracle. \n\n"}
{"id": "1709.10462", "contents": "Title: Regular Intersecting Families Abstract: We call a family of sets intersecting, if any two sets in the family\nintersect. In this paper we investigate intersecting families $\\mathcal{F}$ of\n$k$-element subsets of $[n]:=\\{1,\\ldots, n\\},$ such that every element of $[n]$\nlies in the same (or approximately the same) number of members of\n$\\mathcal{F}$. In particular, we show that we can guarantee $|\\mathcal{F}| =\no({n-1\\choose k-1})$ if and only if $k=o(n)$. \n\n"}
{"id": "1710.00264", "contents": "Title: Bayesian estimation from few samples: community detection and related\n  problems Abstract: We propose an efficient meta-algorithm for Bayesian estimation problems that\nis based on low-degree polynomials, semidefinite programming, and tensor\ndecomposition. The algorithm is inspired by recent lower bound constructions\nfor sum-of-squares and related to the method of moments. Our focus is on sample\ncomplexity bounds that are as tight as possible (up to additive lower-order\nterms) and often achieve statistical thresholds or conjectured computational\nthresholds.\n  Our algorithm recovers the best known bounds for community detection in the\nsparse stochastic block model, a widely-studied class of estimation problems\nfor community detection in graphs. We obtain the first recovery guarantees for\nthe mixed-membership stochastic block model (Airoldi et el.) in constant\naverage degree graphs---up to what we conjecture to be the computational\nthreshold for this model. We show that our algorithm exhibits a sharp\ncomputational threshold for the stochastic block model with multiple\ncommunities beyond the Kesten--Stigum bound---giving evidence that this task\nmay require exponential time.\n  The basic strategy of our algorithm is strikingly simple: we compute the\nbest-possible low-degree approximation for the moments of the posterior\ndistribution of the parameters and use a robust tensor decomposition algorithm\nto recover the parameters from these approximate posterior moments. \n\n"}
{"id": "1710.00668", "contents": "Title: Parameterized Approximation Schemes for Steiner Trees with Small Number\n  of Steiner Vertices Abstract: We study the Steiner Tree problem, in which a set of terminal vertices needs\nto be connected in the cheapest possible way in an edge-weighted graph. This\nproblem has been extensively studied from the viewpoint of approximation and\nalso parametrization. In particular, on one hand Steiner Tree is known to be\nAPX-hard, and W[2]-hard on the other, if parameterized by the number of\nnon-terminals (Steiner vertices) in the optimum solution. In contrast to this\nwe give an efficient parameterized approximation scheme (EPAS), which\ncircumvents both hardness results. Moreover, our methods imply the existence of\na polynomial size approximate kernelization scheme (PSAKS) for the considered\nparameter.\n  We further study the parameterized approximability of other variants of\nSteiner Tree, such as Directed Steiner Tree and Steiner Forest. For neither of\nthese an EPAS is likely to exist for the studied parameter: for Steiner Forest\nan easy observation shows that the problem is APX-hard, even if the input graph\ncontains no Steiner vertices. For Directed Steiner Tree we prove that\napproximating within any function of the studied parameter is W[1]-hard.\nNevertheless, we show that an EPAS exists for Unweighted Directed Steiner Tree,\nbut a PSAKS does not. We also prove that there is an EPAS and a PSAKS for\nSteiner Forest if in addition to the number of Steiner vertices, the number of\nconnected components of an optimal solution is considered to be a parameter. \n\n"}
{"id": "1710.02058", "contents": "Title: Skyline Computation with Noisy Comparisons Abstract: Given a set of $n$ points in a $d$-dimensional space, we seek to compute the\nskyline, i.e., those points that are not strictly dominated by any other point,\nusing few comparisons between elements. We adopt the noisy comparison model\n[FRPU94] where comparisons fail with constant probability and confidence can be\nincreased through independent repetitions of a comparison. In this model\nmotivated by Crowdsourcing applications, Groz & Milo [GM15] show three bounds\non the query complexity for the skyline problem. We improve significantly on\nthat state of the art and provide two output-sensitive algorithms computing the\nskyline with respective query complexity $O(nd\\log (dk/\\delta))$ and $O(ndk\\log\n(k/\\delta))$ where $k$ is the size of the skyline and $\\delta$ the expected\nprobability that our algorithm fails to return the correct answer. These\nresults are tight for low dimensions. \n\n"}
{"id": "1710.02440", "contents": "Title: Structure and properties of large intersecting families Abstract: We say that a family of $k$-subsets of an $n$-element set is intersecting, if\nany two of its sets intersect. In this paper we study different extremal\nproperties of intersecting families, as well as the structure of large\nintersecting families. We also give some results on $k$-uniform families\nwithout $s$ pairwise disjoint sets, related to the Erd\\H{o}s Matching\nConjecture. We prove a conclusive version of Frankl's theorem on intersecting\nfamilies with bounded maximal degree. This theorem, along with its\ngeneralizations to cross-intersecting families, implies many results on the\ntopic, obtained by Frankl, Frankl and Tokushige, Kupavskii and Zakharov and\nothers. We study the structure of large intersecting families, obtaining some\ngeneral structural theorems which generalize the results of Han and Kohayakawa,\nas well as Kostochka and Mubayi. We give degree and subset degree version of\nthe Erd\\H{o}s--Ko--Rado and the Hilton--Milner theorems, extending the results\nof Huang and Zhao, and Frankl, Han, Huang and Zhao. We also extend the range in\nwhich the degree version of the Erd\\H{o}s Matching conjecture holds. \n\n"}
{"id": "1710.04376", "contents": "Title: On the Power of Tree-Depth for Fully Polynomial FPT Algorithms Abstract: There are many classical problems in P whose time complexities have not been\nimproved over the past decades. Recent studies of \"Hardness in P\" have revealed\nthat, for several of such problems, the current fastest algorithm is the best\npossible under some complexity assumptions. To bypass this difficulty, Fomin et\nal. (SODA 2017) introduced the concept of fully polynomial FPT algorithms. For\na problem with the current best time complexity $O(n^c)$, the goal is to design\nan algorithm running in $k^{O(1)}n^{c'}$ time for a parameter $k$ and a\nconstant $c'<c$. In this paper, we investigate the complexity of graph problems\nin P parameterized by tree-depth, a graph parameter related to tree-width. We\nshow that a simple divide-and-conquer method can solve many graph problems,\nincluding Weighted Matching, Negative Cycle Detection, Minimum Weight Cycle,\nReplacement Paths, and 2-hop Cover, in $O(\\mathrm{td}\\cdot m)$ time or\n$O(\\mathrm{td}\\cdot (m+n\\log n))$ time, where $\\mathrm{td}$ is the tree-depth\nof the input graph. Because any graph of tree-width $\\mathrm{tw}$ has\ntree-depth at most $(\\mathrm{tw}+1)\\log_2 n$, our algorithms also run in\n$O(\\mathrm{tw}\\cdot m\\log n)$ time or $O(\\mathrm{tw}\\cdot (m+n\\log n)\\log n)$\ntime. These results match or improve the previous best algorithms parameterized\nby tree-width. Especially, we solve an open problem of fully polynomial FPT\nalgorithm for Weighted Matching parameterized by tree-width posed by Fomin et\nal. \n\n"}
{"id": "1710.05017", "contents": "Title: The power of sum-of-squares for detecting hidden structures Abstract: We study planted problems---finding hidden structures in random noisy\ninputs---through the lens of the sum-of-squares semidefinite programming\nhierarchy (SoS). This family of powerful semidefinite programs has recently\nyielded many new algorithms for planted problems, often achieving the best\nknown polynomial-time guarantees in terms of accuracy of recovered solutions\nand robustness to noise. One theme in recent work is the design of spectral\nalgorithms which match the guarantees of SoS algorithms for planted problems.\nClassical spectral algorithms are often unable to accomplish this: the twist in\nthese new spectral algorithms is the use of spectral structure of matrices\nwhose entries are low-degree polynomials of the input variables. We prove that\nfor a wide class of planted problems, including refuting random constraint\nsatisfaction problems, tensor and sparse PCA, densest-k-subgraph, community\ndetection in stochastic block models, planted clique, and others, eigenvalues\nof degree-d matrix polynomials are as powerful as SoS semidefinite programs of\nroughly degree d. For such problems it is therefore always possible to match\nthe guarantees of SoS without solving a large semidefinite program. Using\nrelated ideas on SoS algorithms and low-degree matrix polynomials (and inspired\nby recent work on SoS and the planted clique problem by Barak et al.), we prove\nnew nearly-tight SoS lower bounds for the tensor and sparse principal component\nanalysis problems. Our lower bounds for sparse principal component analysis are\nthe first to suggest that going beyond existing algorithms for this problem may\nrequire sub-exponential time. \n\n"}
{"id": "1710.05849", "contents": "Title: The unpolarized macronova associated with the gravitational wave event\n  GW170817 Abstract: The merger of two dense stellar remnants including at least one neutron star\n(NS) is predicted to produce gravitational waves (GWs) and short duration gamma\nray bursts (GRBs). In the process, neutron-rich material is ejected from the\nsystem and heavy elements are synthesized by r-process nucleosynthesis. The\nradioactive decay of these heavy elements produces additional transient\nradiation termed \"kilonova\" or \"macronova\". We report the detection of linear\noptical polarization P = (0.50 +/- 0.07)% at 1.46 days after detection of the\nGWs from GW170817, a double neutron star merger associated with an optical\nmacronova counterpart and a short GRB. The optical emission from a macronova is\nexpected to be characterized by a blue, rapidly decaying, component and a red,\nmore slowly evolving, component due to material rich of heavy elements, the\nlanthanides. The polarization measurement was made when the macronova was still\nin its blue phase, during which there is an important contribution from a\nlanthanide-free outflow. The low degree of polarization is consistent with\nintrinsically unpolarized emission scattered by Galactic dust, suggesting a\nsymmetric geometry of the emitting region and low inclination of the merger\nsystem. Stringent upper limits to the polarization degree from 2.45 - 9.48 days\npost-burst are consistent with the lanthanides-rich macronova interpretation. \n\n"}
{"id": "1710.08436", "contents": "Title: HyperMinHash: MinHash in LogLog space Abstract: In this extended abstract, we describe and analyze a lossy compression of\nMinHash from buckets of size $O(\\log n)$ to buckets of size $O(\\log\\log n)$ by\nencoding using floating-point notation. This new compressed sketch, which we\ncall HyperMinHash, as we build off a HyperLogLog scaffold, can be used as a\ndrop-in replacement of MinHash. Unlike comparable Jaccard index fingerprinting\nalgorithms in sub-logarithmic space (such as b-bit MinHash), HyperMinHash\nretains MinHash's features of streaming updates, unions, and cardinality\nestimation. For a multiplicative approximation error $1+ \\epsilon$ on a Jaccard\nindex $ t $, given a random oracle, HyperMinHash needs $O\\left(\\epsilon^{-2}\n\\left( \\log\\log n + \\log \\frac{1}{ t \\epsilon} \\right)\\right)$ space.\nHyperMinHash allows estimating Jaccard indices of 0.01 for set cardinalities on\nthe order of $10^{19}$ with relative error of around 10\\% using 64KiB of\nmemory; MinHash can only estimate Jaccard indices for cardinalities of\n$10^{10}$ with the same memory consumption. \n\n"}
{"id": "1710.11115", "contents": "Title: On the Time Variation of Dust Extinction and Gas Absorption for Type~Ia\n  Supernovae Observed Through Non-uniform Interstellar Medium Abstract: For Type Ia supernovae (SNe Ia) observed through a non-uniform interstellar\nmedium (ISM) in its host galaxy, we investigate whether the non-uniformity can\ncause observable time variations in dust extinction and in gas absorption due\nto the expansion of the SN photosphere with time. We show that, owing to the\nsteep spectral index of the ISM density power spectrum, sizable density\nfluctuation amplitudes at the length scale of typical ISM structures ($\\gtrsim\n\\text{ 10 pc}$) will translate to much smaller fluctuations on the scales of a\nSN photosphere. Therefore the typical amplitude of time variation due to\nnon-uniform ISM, of absorption equivalent widths and of extinction, would be\nsmall. As a result, we conclude that non-uniform ISM density should not impact\ncosmology measurements based on SNe Ia. We apply our predictions based on the\nISM density power law power spectrum to the observations of two highly reddened\nSNe Ia, SN 2012cu and SN 2014J. \n\n"}
{"id": "1711.00963", "contents": "Title: The Complexity of Finding Small Separators in Temporal Graphs Abstract: Temporal graphs are graphs with time-stamped edges. We study the problem of\nfinding a small vertex set (the separator) with respect to two designated\nterminal vertices such that the removal of the set eliminates all temporal\npaths connecting one terminal to the other. Herein, we consider two models of\ntemporal paths: paths that pass through arbitrarily many edges per time step\n(non-strict) and paths that pass through at most one edge per time step\n(strict). Regarding the number of time steps of a temporal graph, we show a\ncomplexity dichotomy (NP-hardness versus polynomial-time solvability) for both\nproblem variants. Moreover we prove both problem variants to be NP-complete\neven on temporal graphs whose underlying graph is planar. We further show that,\non temporal graphs with planar underlying graph, if additionally the number of\ntime steps is constant, then the problem variant for strict paths is solvable\nin quasi-linear time. Finally, we introduce and motivate the notion of a\ntemporal core (vertices whose incident edges change over time). We prove that\nthe non-strict variant is fixed-parameter tractable when parameterized by the\nsize of the temporal core, while the strict variant remains NP-complete, even\nfor constant-size temporal cores. \n\n"}
{"id": "1711.01486", "contents": "Title: The ANTARES Collaboration: Contributions to ICRC 2017 Part II: The\n  multi-messenger program Abstract: Papers on the ANTARES multi-messenger program, prepared for the 35th\nInternational Cosmic Ray Conference (ICRC 2017, Busan, South Korea) by the\nANTARES Collaboration \n\n"}
{"id": "1711.01811", "contents": "Title: Computational Complexity Aspects of Point Visibility Graphs Abstract: A point visibility graph is a graph induced by a set of points in the plane\nwhere the vertices of the graph represent the points in the point set and two\nvertices are adjacent if and only if no other point from the point set lies on\nthe line segment between the two corresponding points. The set of all point\nvisibility graphs form a graph class which is examined from a computational\ncomplexity perspective in this paper. We show NP-hardness for several classic\ngraph problems on point visibility graphs such as Feedback Vertex Set, Longest\nInduced Path, Bisection and $\\mathcal{F}$-free Vertex Deletion (for certain\nsets $\\mathcal{F}$). Furthermore, we consider the complexity of the Dominating\nSet problem on point visibility graphs of points on a grid. \n\n"}
{"id": "1711.02449", "contents": "Title: A young contracting white dwarf in the peculiar binary HD 49798/RX\n  J0648.0--4418? Abstract: HD 49798/RX J0648.0--4418 is a peculiar X-ray binary with a hot subdwarf\n(sdO) mass donor. The nature of the accreting compact object is not known, but\nits spin period $P=13.2$~s and $\\dot P =-2.15 \\times 10^{-15}$s~s$^{-1}$, prove\nthat it can be only either a white dwarf or a neutron star. The spin-up has\nbeen very stable for more than 20 years. We demonstrate that the continuous\nstable spin-up of the compact companion of HD 49798 can be best explained by\ncontraction of a young white dwarf with an age $\\sim 2$~Myrs. This allows us to\ninterpret all the basic parameters of the system in the framework of an\naccreting white dwarf. We present examples of binary evolution which result in\nsuch systems. If correct, this is the first direct evidence for a white dwarf\ncontraction on early evolutionary stages. \n\n"}
{"id": "1711.04604", "contents": "Title: Smaller parameters for vertex cover kernelization Abstract: We revisit the topic of polynomial kernels for Vertex Cover relative to\nstructural parameters. Our starting point is a recent paper due to Fomin and\nStr{\\o}mme [WG 2016] who gave a kernel with $\\mathcal{O}(|X|^{12})$ vertices\nwhen $X$ is a vertex set such that each connected component of $G-X$ contains\nat most one cycle, i.e., $X$ is a modulator to a pseudoforest. We strongly\ngeneralize this result by using modulators to $d$-quasi-forests, i.e., graphs\nwhere each connected component has a feedback vertex set of size at most $d$,\nand obtain kernels with $\\mathcal{O}(|X|^{3d+9})$ vertices. Our result relies\non proving that minimal blocking sets in a $d$-quasi-forest have size at most\n$d+2$. This bound is tight and there is a related lower bound of\n$\\mathcal{O}(|X|^{d+2-\\epsilon})$ on the bit size of kernels.\n  In fact, we also get bounds for minimal blocking sets of more general graph\nclasses: For $d$-quasi-bipartite graphs, where each connected component can be\nmade bipartite by deleting at most $d$ vertices, we get the same tight bound of\n$d+2$ vertices. For graphs whose connected components each have a vertex cover\nof cost at most $d$ more than the best fractional vertex cover, which we call\n$d$-quasi-integral, we show that minimal blocking sets have size at most\n$2d+2$, which is also tight. Combined with existing randomized polynomial\nkernelizations this leads to randomized polynomial kernelizations for\nmodulators to $d$-quasi-bipartite and $d$-quasi-integral graphs. There are\nlower bounds of $\\mathcal{O}(|X|^{d+2-\\epsilon})$ and\n$\\mathcal{O}(|X|^{2d+2-\\epsilon})$ for the bit size of such kernels. \n\n"}
{"id": "1711.06997", "contents": "Title: Generation of circular polarization in CMB radiation via nonlinear\n  photon-photon interaction Abstract: Standard cosmological models do predict a measurable amount of anisotropies\nin the intensity and linear polarization of the Cosmic Microwave Background\nradiation (CMB) via Thomson scattering, even though these theoretical models do\nnot predict circular polarization for CMB radiation. In other hand, the\ncircular polarization of CMB has not been excluded in observational evidences.\nHere we estimate the circular polarization power spectrum $C_{l}^{V(S)}$ in CMB\nradiation due to Compton scattering and non-linear photon-photon forward\nscattering via Euler-Heisenberg Effective Lagrangian. We have estimated the\naverage value of circular power spectrum is $1(l+1)\\,C_{l}^{V(S)}/(2\\pi)\\sim\n10^{-4}\\mu\\,K^2$ for $l\\sim300$ at present time which is smaller than recently\nreported data (SPIDER collaboration) but in the range of the future achievable\nexperimental data. We also show that the generation of B-mode polarization for\nCMB photons in the presence of the primordial scalar perturbation via\nEuler-Heisenberg interaction is possible however this contribution for B-mode\npolarization is not remarkable. \n\n"}
{"id": "1711.07211", "contents": "Title: List-Decodable Robust Mean Estimation and Learning Mixtures of Spherical\n  Gaussians Abstract: We study the problem of list-decodable Gaussian mean estimation and the\nrelated problem of learning mixtures of separated spherical Gaussians. We\ndevelop a set of techniques that yield new efficient algorithms with\nsignificantly improved guarantees for these problems.\n  {\\bf List-Decodable Mean Estimation.} Fix any $d \\in \\mathbb{Z}_+$ and $0<\n\\alpha <1/2$. We design an algorithm with runtime $O\n(\\mathrm{poly}(n/\\alpha)^{d})$ that outputs a list of $O(1/\\alpha)$ many\ncandidate vectors such that with high probability one of the candidates is\nwithin $\\ell_2$-distance $O(\\alpha^{-1/(2d)})$ from the true mean. The only\nprevious algorithm for this problem achieved error $\\tilde O(\\alpha^{-1/2})$\nunder second moment conditions. For $d = O(1/\\epsilon)$, our algorithm runs in\npolynomial time and achieves error $O(\\alpha^{\\epsilon})$. We also give a\nStatistical Query lower bound suggesting that the complexity of our algorithm\nis qualitatively close to best possible.\n  {\\bf Learning Mixtures of Spherical Gaussians.} We give a learning algorithm\nfor mixtures of spherical Gaussians that succeeds under significantly weaker\nseparation assumptions compared to prior work. For the prototypical case of a\nuniform mixture of $k$ identity covariance Gaussians we obtain: For any\n$\\epsilon>0$, if the pairwise separation between the means is at least\n$\\Omega(k^{\\epsilon}+\\sqrt{\\log(1/\\delta)})$, our algorithm learns the unknown\nparameters within accuracy $\\delta$ with sample complexity and running time\n$\\mathrm{poly} (n, 1/\\delta, (k/\\epsilon)^{1/\\epsilon})$. The previously best\nknown polynomial time algorithm required separation at least $k^{1/4}\n\\mathrm{polylog}(k/\\delta)$.\n  Our main technical contribution is a new technique, using degree-$d$\nmultivariate polynomials, to remove outliers from high-dimensional datasets\nwhere the majority of the points are corrupted. \n\n"}
{"id": "1711.08020", "contents": "Title: First-order methods for constrained convex programming based on\n  linearized augmented Lagrangian function Abstract: First-order methods have been popularly used for solving large-scale\nproblems. However, many existing works only consider unconstrained problems or\nthose with simple constraint. In this paper, we develop two first-order methods\nfor constrained convex programs, for which the constraint set is represented by\naffine equations and smooth nonlinear inequalities. Both methods are based on\nthe classic augmented Lagrangian function. They update the multipliers in the\nsame way as the augmented Lagrangian method (ALM) but employ different primal\nvariable updates. The first method, at each iteration, performs a single\nproximal gradient step to the primal variable, and the second method is a block\nupdate version of the first one.\n  For the first method, we establish its global iterate convergence as well as\nglobal sublinear and local linear convergence, and for the second method, we\nshow a global sublinear convergence result in expectation. Numerical\nexperiments are carried out on the basis pursuit denoising and a convex\nquadratically constrained quadratic program to show the empirical performance\nof the proposed methods. Their numerical behaviors closely match the\nestablished theoretical results. \n\n"}
{"id": "1711.08628", "contents": "Title: Determining the nature of white dwarfs from low-frequency gravitational\n  waves Abstract: An extreme-mass-ratio system composed of a white dwarf (WD) and a massive\nblack hole can be observed by the low-frequency gravitational wave detectors,\nsuch as the Laser Interferometer Space Antenna (LISA). When the mass of the\nblack hole is around $10^4 \\sim 10^5 M_\\odot$, the WD will be disrupted by the\ntidal interaction at the final inspiraling stage. The event position and time\nof the tidal disruption of the WD can be accurately determined by the\ngravitational wave signals. Such position and time depend upon the mass of the\nblack hole and especially on the density of the WD. We present the theory by\nusing LISA-like gravitational wave detectors, the mass-radius relation and then\nthe equations of state of WDs could be strictly constrained (accuracy up to\n$0.1\\%$). We also point out that LISA can accurately predict the disruption\ntime of a WD, and forecast the electromagnetic follow-up of this tidal\ndisruption event. \n\n"}
{"id": "1711.09041", "contents": "Title: Evolutions of unequal mass, highly spinning black hole binaries Abstract: We evolve a binary black hole system bearing a mass ratio of $q=m_1/m_2=2/3$\nand individual spins of $S^z_1/m_1^2=0.95$ and $S^z_2/m_2^2=-0.95$ in a\nconfiguration where the large black hole has its spin antialigned with the\norbital angular momentum, $L^z$, and the small black hole has its spin aligned\nwith $L^z$. This configuration was chosen to measure the maximum recoil of the\nremnant black hole for nonprecessing binaries. We find that the remnant black\nhole recoils at 500km/s, the largest recorded value from numerical simulations\nfor aligned spin configurations. The remnant mass, spin, and gravitational\nwaveform peak luminosity and frequency also provide a valuable point in\nparameter space for source modeling. \n\n"}
{"id": "1711.11560", "contents": "Title: Testing Conditional Independence of Discrete Distributions Abstract: We study the problem of testing \\emph{conditional independence} for discrete\ndistributions. Specifically, given samples from a discrete random variable $(X,\nY, Z)$ on domain $[\\ell_1]\\times[\\ell_2] \\times [n]$, we want to distinguish,\nwith probability at least $2/3$, between the case that $X$ and $Y$ are\nconditionally independent given $Z$ from the case that $(X, Y, Z)$ is\n$\\epsilon$-far, in $\\ell_1$-distance, from every distribution that has this\nproperty. Conditional independence is a concept of central importance in\nprobability and statistics with a range of applications in various scientific\ndomains. As such, the statistical task of testing conditional independence has\nbeen extensively studied in various forms within the statistics and\neconometrics communities for nearly a century. Perhaps surprisingly, this\nproblem has not been previously considered in the framework of distribution\nproperty testing and in particular no tester with sublinear sample complexity\nis known, even for the important special case that the domains of $X$ and $Y$\nare binary.\n  The main algorithmic result of this work is the first conditional\nindependence tester with {\\em sublinear} sample complexity for discrete\ndistributions over $[\\ell_1]\\times[\\ell_2] \\times [n]$. To complement our upper\nbounds, we prove information-theoretic lower bounds establishing that the\nsample complexity of our algorithm is optimal, up to constant factors, for a\nnumber of settings. Specifically, for the prototypical setting when $\\ell_1,\n\\ell_2 = O(1)$, we show that the sample complexity of testing conditional\nindependence (upper bound and matching lower bound) is\n  \\[\n  \\Theta\\left({\\max\\left(n^{1/2}/\\epsilon^2,\\min\\left(n^{7/8}/\\epsilon,n^{6/7}/\\epsilon^{8/7}\\right)\\right)}\\right)\\,.\n  \\] \n\n"}
{"id": "1711.11579", "contents": "Title: TeV dark matter and the DAMPE electron excess Abstract: The recent high energy electron and positron flux observed by the DAMPE\nexperiment indicates possible excess events near 1.4 TeV. Such an excess may be\nevidence of dark matter annihilations or decays in a dark matter subhalo that\nis located close to the solar system. We give here an analysis of this excess\nfrom annihilations of Dirac fermion dark matter which is charged under a new\n$U(1)_X$ gauge symmetry. The interactions between dark matter and the standard\nmodel particles are mediated the $U(1)_X$ gauge boson. We show that dark matter\nannihilations from a local subhalo can explain the excess with the canonical\nthermal annihilation cross section. We further discuss the constraints from the\nrelic density, from the dark matter direct detection, from the dark matter\nindirect detection, from the cosmic microwave background, and from the particle\ncolliders. \n\n"}
{"id": "1712.02302", "contents": "Title: Which groups are amenable to proving exponent two for matrix\n  multiplication? Abstract: The Cohn-Umans group-theoretic approach to matrix multiplication suggests\nembedding matrix multiplication into group algebra multiplication, and bounding\n$\\omega$ in terms of the representation theory of the host group. This\nframework is general enough to capture the best known upper bounds on $\\omega$\nand is conjectured to be powerful enough to prove $\\omega = 2$, although\nfinding a suitable group and constructing such an embedding has remained\nelusive. Recently it was shown, by a generalization of the proof of the Cap Set\nConjecture, that abelian groups of bounded exponent cannot prove $\\omega = 2$\nin this framework, which ruled out a family of potential constructions in the\nliterature.\n  In this paper we study nonabelian groups as potential hosts for an embedding.\nWe prove two main results:\n  (1) We show that a large class of nonabelian groups---nilpotent groups of\nbounded exponent satisfying a mild additional condition---cannot prove $\\omega\n= 2$ in this framework. We do this by showing that the shrinkage rate of powers\nof the augmentation ideal is similar to the shrinkage rate of the number of\nfunctions over $(\\mathbb{Z}/p\\mathbb{Z})^n$ that are degree $d$ polynomials;\nour proof technique can be seen as a generalization of the polynomial method\nused to resolve the Cap Set Conjecture.\n  (2) We show that symmetric groups $S_n$ cannot prove nontrivial bounds on\n$\\omega$ when the embedding is via three Young subgroups---subgroups of the\nform $S_{k_1} \\times S_{k_2} \\times \\dotsb \\times S_{k_\\ell}$---which is a\nnatural strategy that includes all known constructions in $S_n$.\n  By developing techniques for negative results in this paper, we hope to\ncatalyze a fruitful interplay between the search for constructions proving\nbounds on $\\omega$ and methods for ruling them out. \n\n"}
{"id": "1712.03158", "contents": "Title: Graph-based time-space trade-offs for approximate near neighbors Abstract: We take a first step towards a rigorous asymptotic analysis of graph-based\napproaches for finding (approximate) nearest neighbors in high-dimensional\nspaces, by analyzing the complexity of (randomized) greedy walks on the\napproximate near neighbor graph. For random data sets of size $n = 2^{o(d)}$ on\nthe $d$-dimensional Euclidean unit sphere, using near neighbor graphs we can\nprovably solve the approximate nearest neighbor problem with approximation\nfactor $c > 1$ in query time $n^{\\rho_q + o(1)}$ and space $n^{1 + \\rho_s +\no(1)}$, for arbitrary $\\rho_q, \\rho_s \\geq 0$ satisfying \\begin{align} (2c^2 -\n1) \\rho_q + 2 c^2 (c^2 - 1) \\sqrt{\\rho_s (1 - \\rho_s)} \\geq c^4. \\end{align}\nGraph-based near neighbor searching is especially competitive with hash-based\nmethods for small $c$ and near-linear memory, and in this regime the asymptotic\nscaling of a greedy graph-based search matches the recent optimal hash-based\ntrade-offs of Andoni-Laarhoven-Razenshteyn-Waingarten [SODA'17]. We further\nstudy how the trade-offs scale when the data set is of size $n =\n2^{\\Theta(d)}$, and analyze asymptotic complexities when applying these results\nto lattice sieving. \n\n"}
{"id": "1712.04886", "contents": "Title: Optimal Construction of Compressed Indexes for Highly Repetitive Texts Abstract: We propose algorithms that, given the input string of length $n$ over integer\nalphabet of size $\\sigma$, construct the Burrows-Wheeler transform (BWT), the\npermuted longest-common-prefix (PLCP) array, and the LZ77 parsing in\n$O(n/\\log_{\\sigma}n+r\\,{\\rm polylog}\\,n)$ time and working space, where $r$ is\nthe number of runs in the BWT of the input. These are the essential components\nof many compressed indexes such as compressed suffix tree, FM-index, and\ngrammar and LZ77-based indexes, but also find numerous applications in sequence\nanalysis and data compression. The value of $r$ is a common measure of\nrepetitiveness that is significantly smaller than $n$ if the string is highly\nrepetitive. Since just accessing every symbol of the string requires\n$\\Omega(n/\\log_{\\sigma}n)$ time, the presented algorithms are time and space\noptimal for inputs satisfying the assumption $n/r\\in\\Omega({\\rm polylog}\\,n)$\non the repetitiveness. For such inputs our result improves upon the currently\nfastest general algorithms of Belazzougui (STOC 2014) and Munro et al. (SODA\n2017) which run in $O(n)$ time and use $O(n/\\log_{\\sigma} n)$ working space. We\nalso show how to use our techniques to obtain optimal solutions on highly\nrepetitive data for other fundamental string processing problems such as:\nLyndon factorization, construction of run-length compressed suffix arrays, and\nsome classical \"textbook\" problems such as computing the longest substring\noccurring at least some fixed number of times. \n\n"}
{"id": "1712.05451", "contents": "Title: Quasi-Periodic Behavior of Mini-Disks in Binary Black Holes Approaching\n  Merger Abstract: We present the first magnetohydrodynamic simulation in which a circumbinary\ndisk around a relativistic binary black hole feeds mass to individual accretion\ndisks (\"mini-disks\") around each black hole. Mass flow through the accretion\nstreams linking the circumbinary disk to the mini-disks is modulated\nquasi-periodically by the streams' interaction with a nonlinear $m=1$ density\nfeature, or \"lump\", at the inner edge of the circumbinary disk: the stream\nsupplying each mini-disk comes into phase with the lump at a frequency $0.74$\ntimes the binary orbital frequency. Because the binary is relativistic, the\ntidal truncation radii of the mini-disks are not much larger than their\ninnermost stable circular orbits; consequently, the mini-disks' inflow times\nare shorter than the conventional estimate and are comparable to the stream\nmodulation period. As a result, the mini-disks are always in inflow\ndisequilibrium, with their masses and spiral density wave structures responding\nto the stream's quasi-periodic modulation. The fluctuations in each mini-disk's\nmass are so large that as much as $75\\%$ of the total mini-disk mass can be\ncontained within a single mini-disk. Such quasi-periodic modulation of the\nmini-disk structure may introduce distinctive time-dependent features in the\nbinary's electromagnetic emission. \n\n"}
{"id": "1712.06040", "contents": "Title: Neutron stars: Observational diversity and evolution Abstract: Ever since the discovery of the Crab and Vela pulsars in their respective\nSupernova Remnants, our understanding of how neutron stars manifest themselves\nobservationally has been dramatically shaped by the surge of discoveries and\ndedicated studies across the electromagnetic spectrum, particularly in the\nhigh-energy band. The growing diversity of neutron stars includes the highly\nmagnetized neutron stars (magnetars) and the Central Compact Objects shining in\nX-rays and mostly lacking pulsar wind nebulae. These two subclasses of\nhigh-energy objects, however, seem to be characterized by anomalously high or\nanomalously low surface magnetic fields (thus dubbed as `magnetars' and\n`anti-magnetars', respectively), and have pulsar characteristic ages that are\noften much offset from their associated SNRs' ages. In addition, some neutron\nstars act `schizophrenic' in that they occasionally display properties that\nseem common to more than one of the defined subclasses. I review the growing\ndiversity of neutron stars from an observational perspective, then highlight\nrecent and on-going theoretical and observational work attempting to address\nthis diversity, particularly in light of their magnetic field evolution, energy\nloss mechanisms, and supernova progenitors' studies. \n\n"}
{"id": "1712.06668", "contents": "Title: Simultaneous multiwavelength observations of V404 Cygni during its 2015\n  June outburst decay strengthen the case for an extremely energetic jet-base Abstract: We present results of multiband optical photometry of the black hole X-ray\nbinary system V404 Cygni obtained using Wheaton College Observatory's 0.3m\ntelescope, along with strictly simultaneous INTEGRAL and Swift observations\nduring 2015 June 25.15--26.33 UT, and 2015 June 27.10--27.34 UT. These\nobservations were made during the 2015 June outburst of the source when it was\ngoing through an epoch of violent activity in all wavelengths ranging from\nradio to $\\gamma$-rays. The multiwavelength variability timescale favors a\ncompact emission region, most likely originating in a jet outflow, for both\nobserving epochs presented in this work. The simultaneous INTEGRAL/Imager on\nBoard the Integral Satellite (IBIS) 20--40 keV light curve obtained during the\nJune 27 observing run correlates very strongly with the optical light curve,\nwith no detectable delay between the optical bands as well as between the\noptical and hard X-rays. The average slope of the dereddened spectral energy\ndistribution was roughly flat between the $I_C$- and $V$-bands during the June\n27 run, even though the optical and X-ray flux varied by $>$25$\\times$ during\nthe run, ruling out an irradiation origin for the optical and suggesting that\nthe optically thick to optically thin jet synchrotron break during the\nobservations was at a frequency larger than that of $V$-band, which is quite\nextreme for X-ray binaries. These observations suggest that the optical\nemission originated very close to the base of the jet. A strong H$\\alpha$\nemission line, probably originating in a quasi-spherical nebula around the\nsource, also contributes significantly in the $R_C$-band. Our data, in\nconjunction with contemporaneous data at other wavelengths presented by other\ngroups, strongly suggest that the jet-base was extremely compact and energetic\nduring this phase of the outburst. \n\n"}
{"id": "1712.07405", "contents": "Title: Circumstellar interaction in supernovae in dense environments - an\n  observational perspective Abstract: In a supernova explosion, the ejecta interacting with the surrounding\ncircumstellar medium (CSM) give rise to variety of radiation. Since CSM is\ncreated from the mass lost from the progenitor star, it carries footprints of\nthe late time evolution of the star. This is one of the unique ways to get a\nhandle on the nature of the progenitor star system. Here, I will focus mainly\non the supernovae (SNe) exploding in dense environments, a.k.a. Type IIn SNe.\nRadio and X-ray emission from this class of SNe have revealed important\nmodifications in their radiation properties, due to the presence of high\ndensity CSM. Forward shock dominance of the X-ray emission, internal free-free\nabsorption of the radio emission, episodic or non-steady mass loss rate,\nasymmetry in the explosion seem to be common properties of this class of SNe. \n\n"}
{"id": "1712.07861", "contents": "Title: PHOEG Helps Obtaining Extremal Graphs Abstract: Extremal Graph Theory aims to determine bounds for graph invariants as well\nas the graphs attaining those bounds.\n  We are currently developping PHOEG, an ecosystem of tools designed to help\nresearchers in Extremal Graph Theory.\n  It uses a big relational database of undirected graphs and works with the\nconvex hull of the graphs as points in the invariants space in order to exactly\nobtain the extremal graphs and optimal bounds on the invariants for some fixed\nparameters. The results obtained on the restricted finite class of graphs can\nlater be used to infer conjectures. This database also allows us to make\nqueries on those graphs. Once the conjecture defined, PHOEG goes one step\nfurther by helping in the process of designing a proof guided by successive\napplications of transformations from any graph to an extremal graph. To this\naim, we use a second database based on a graph data model.\n  The paper presents ideas and techniques used in PHOEG to assist the study of\nExtremal Graph Theory. \n\n"}
{"id": "1712.08362", "contents": "Title: Connected Vertex Cover for $(sP_1+P_5)$-Free Graphs Abstract: The Connected Vertex Cover problem is to decide if a graph G has a vertex\ncover of size at most $k$ that induces a connected subgraph of $G$. This is a\nwell-studied problem, known to be NP-complete for restricted graph classes,\nand, in particular, for $H$-free graphs if $H$ is not a linear forest (a graph\nis $H$-free if it does not contain $H$ as an induced subgraph). It is easy to\nsee that Connected Vertex Cover is polynomial-time solvable for $P_4$-free\ngraphs. We continue the search for tractable graph classes: we prove that it is\nalso polynomial-time solvable for $(sP_1+P_5)$-free graphs for every integer\n$s\\geq 0$. \n\n"}
{"id": "1712.08749", "contents": "Title: Cartesian trees and Lyndon trees Abstract: The article describes the structural and algorithmic relations between\nCartesian trees and Lyndon Trees. This leads to a uniform presentation of the\nLyndon table of a word corresponding to the Next Nearest Smaller table of a\nsequence of numbers. It shows how to efficiently compute runs, that is, maximal\nperiodicities occurring in a word. \n\n"}
{"id": "1712.09754", "contents": "Title: Cumulative Neutrino and Gamma-Ray Backgrounds from Halo and Galaxy\n  Mergers Abstract: The merger of dark matter halos and the gaseous structures embedded in them,\nsuch as proto-galaxies, galaxies, and groups and clusters of galaxies, results\nin strong shocks that are capable of accelerating cosmic rays (CRs) to\n$\\sim10~\\rm PeV$. These shocks will produce high-energy neutrinos and\n$\\gamma$-rays through inelastic $pp$ collisions with ambient gaseous\nenvironments. In this work, we study the contributions of these halo mergers to\nthe diffuse neutrino flux measured in IceCube and to the non-blazar portion of\nthe extragalactic $\\gamma$-ray background measured by $Fermi$. In order to\ncalculate them, we formulate the redshift dependence of the shock velocity,\ngalactic radius, halo gas content and galactic/intergalactic magnetic fields\nover the dark matter halo distribution up to a redshift $z=10$. We find that\nhigh-redshift mergers contribute a significant amount of the cosmic-ray energy\nluminosity density, and the resulting neutrino spectra could explain a large\npart of the observed diffuse neutrino flux above 0.1 PeV up to several PeV. We\nalso show that our model can somewhat alleviate tensions with the extragalactic\n$\\gamma$-ray background. First, since a larger fraction of the CR energy\nluminosity density comes from high redshifts, the accompanying $\\gamma$-rays\nare more strongly suppressed through $\\gamma\\gamma$ annihilations with the\ncosmic microwave background (CMB) and the extragalactic background light (EBL).\nSecond, mildly radiative-cooled shocks may lead to a harder CR spectrum with\nspectral indices of $1.5\\lesssim s\\lesssim2.0$. Our study suggests that halo\nmergers, a fraction of which may also induce starbursts in the merged galaxies,\ncan be promising neutrino emitters without violating the existing $Fermi$\n$\\gamma$-ray constraints on the non-blazar component of the extragalactic\n$\\gamma$-ray background. \n\n"}
{"id": "1712.09810", "contents": "Title: Observational Properties of SNe Ia Progenitors Close to the Explosion Abstract: We determine the expected signal in various observational bands of Supernovae\nIa progenitors just before the explosion by assuming the rotating Double\nDegenerate scenario. Our results are valid also for all the evolutionary\nscenarios invoking rotation as the driving mechanism of the accretion process\nas well as the evolution up to the explosion. We find that the observational\nproperties depend mainly on the mass of the exploding object, even if the\nangular momentum evolution after the end of the mass accretion phase and before\nthe onset of C-burning plays a non-negligible role. Just before the explosion\nthe magnitude M_V ranges between 9 and 11 mag, while the colour (F225W-F555W)\nis about -1.64 mag. The photometric properties remain constant for a few\ndecades before the explosion. During the last few months the luminosity\ndecreases very rapidly. The corresponding decline in the optical bands varies\nfrom few hundredths up to one magnitude, the exact value depending on both the\nWD total mass and the braking efficiency at the end of the mass transfer. This\nfeature is related to the exponentially increasing energy production which\ndrives the formation of a convective core rapidly extending over a large part\nof the exploding object. Also a drop in the angular velocity occurs. We find\nthat observations in the soft X band (0.5 -2 keV) may be used to check if the\nSNe Ia progenitors evolution up to explosion is driven by rotation and, hence,\nto discriminate among different progenitor scenarios. \n\n"}
{"id": "1712.10197", "contents": "Title: Interesting Paths in the Mapper Abstract: The Mapper produces a compact summary of high dimensional data as a\nsimplicial complex. We study the problem of quantifying the interestingness of\nsubpopulations in a Mapper, which appear as long paths, flares, or loops.\nFirst, we create a weighted directed graph G using the 1-skeleton of the\nMapper. We use the average values at the vertices of a target function to\ndirect edges (from low to high). The difference between the average values at\nvertices (high-low) is set as the edge's weight. Covariation of the remaining h\nfunctions (independent variables) is captured by a h-bit binary signature\nassigned to the edge. An interesting path in G is a directed path whose edges\nall have the same signature. We define the interestingness score of such a path\nas a sum of its edge weights multiplied by a nonlinear function of their ranks\nin the path.\n  Second, we study three optimization problems on this graph G. In the problem\nMax-IP, we seek an interesting path in G with the maximum interestingness\nscore. We show that Max-IP is NP-complete. For the special case when G is a\ndirected acyclic graph (DAG), we show that Max-IP can be solved in polynomial\ntime - in O(mnd_i) where d_i is the maximum indegree of a vertex in G.\n  In the more general problem IP, the goal is to find a collection of\nedge-disjoint interesting paths such that the overall sum of their\ninterestingness scores is maximized. We also study a variant of IP termed k-IP,\nwhere the goal is to identify a collection of edge-disjoint interesting paths\neach with k edges, and their total interestingness score is maximized. While\nk-IP can be solved in polynomial time for k <= 2, we show k-IP is NP-complete\nfor k >= 3 even when G is a DAG. We develop polynomial time heuristics for IP\nand k-IP on DAGs. \n\n"}
{"id": "1801.01059", "contents": "Title: Slowing Down Top Trees for Better Worst-Case Bounds Abstract: We consider the top tree compression scheme introduced by Bille et al. [ICALP\n2013] and construct an infinite family of trees on $n$ nodes labeled from an\nalphabet of size $\\sigma$, for which the size of the top DAG is\n$\\Theta(\\frac{n}{\\log_\\sigma n}\\log\\log_\\sigma n)$. Our construction matches a\npreviously known upper bound and exhibits a weakness of this scheme, as the\ninformation-theoretic lower bound is $\\Omega(\\frac{n}{\\log_\\sigma n})$. This\nsettles an open problem stated by Lohrey et al. [arXiv 2017], who designed a\nmore involved version achieving the lower bound. We show that this can be also\nguaranteed by a very minor modification of the original scheme: informally, one\nonly needs to ensure that different parts of the tree are not compressed too\nquickly. Arguably, our version is more uniform, and in particular, the\ncompression procedure is oblivious to the value of $\\sigma$. \n\n"}
{"id": "1801.01141", "contents": "Title: Neutron Star Mergers as sites of r-process Nucleosynthesis and Short\n  Gamma-Ray Bursts Abstract: Neutron star mergers have been long considered as promising sites of heavy\n$r$-process nucleosynthesis. We overview observational evidence supporting this\nscenario including: the total amount of $r$-process elements in the Galaxy,\nextreme metal poor stars, geological radioactive elemental abundances, dwarf\ngalaxies, and short gamma-ray bursts (sGRBs). Recently, the advanced LIGO and\nVirgo observatories discovered a gravitational-wave signal of a neutron star\nmerger, GW170817, as well as accompanying multi-wavelength electromagnetic (EM)\ncounterparts. The ultra-violet, optical, and near infrared observations point\nto $r$-process elements that have been synthesized in the merger ejecta. The\nrate and ejected mass inferred from GW170817 and the EM counterparts are\nconsistent with other observations. We find however that, within simple one\nzone chemical evolution models (based on merger rates with reasonable delay\ntime distributions as expected from evolutionary models, or from observations\nof sGRBs), it is difficult to reconcile the current observations of the\neuropium abundance history of Galactic stars for [Fe/H] $\\gtrsim -1$. This\nimplies that to account for the role of mergers in the Galactic chemical\nevolution, we need a Galactic model with multiple populations that have\ndifferent spatial distributions and/or varying formation rates. \n\n"}
{"id": "1801.01165", "contents": "Title: Automorphism groups and Ramsey properties of sparse graphs Abstract: We study automorphism groups of sparse graphs from the viewpoint of\ntopological dynamics and the Kechris, Pestov, Todor\\v{c}evi\\'c correspondence.\nWe investigate amenable and extremely amenable subgroups of these groups using\nthe space of orientations of the graph and results from structural Ramsey\ntheory. Resolving one of the open questions in the area, we show that\nHrushovski's example of an $\\omega$-categorical sparse graph has no\n$\\omega$-categorical expansion with extremely amenable automorphism group. \n\n"}
{"id": "1801.03334", "contents": "Title: Applications of deep learning to relativistic hydrodynamics Abstract: Relativistic hydrodynamics is a powerful tool to simulate the evolution of\nthe quark gluon plasma (QGP) in relativistic heavy ion collisions. Using 10000\ninitial and final profiles generated from 2+1-d relativistic hydrodynamics\nVISH2+1 with MC-Glauber initial conditions, we train a deep neural network\nbased on stacked U-net, and use it to predict the final profiles associated\nwith various initial conditions, including MC-Glauber, MC-KLN and AMPT and\nTRENTo. A comparison with the VISH2+1 results shows that the network\npredictions can nicely capture the magnitude and inhomogeneous structures of\nthe final profiles, and nicely describe the related eccentricity distributions\n$P(\\varepsilon_n)$ (n=2, 3, 4). These results indicate that deep learning\ntechnique can capture the main features of the non-linear evolution of\nhydrodynamics, showing its potential to largely accelerate the event-by-event\nsimulations of relativistic hydrodynamics. \n\n"}
{"id": "1801.03879", "contents": "Title: Parameterized (Approximate) Defective Coloring Abstract: In Defective Coloring we are given a graph $G = (V, E)$ and two integers\n$\\chi_d, \\Delta^*$ and are asked if we can partition $V$ into $\\chi_d$ color\nclasses, so that each class induces a graph of maximum degree $\\Delta^*$. We\ninvestigate the complexity of this generalization of Coloring with respect to\nseveral well-studied graph parameters, and show that the problem is W-hard\nparameterized by treewidth, pathwidth, tree-depth, or feedback vertex set, if\n$\\chi_d = 2$. As expected, this hardness can be extended to larger values of\n$\\chi_d$ for most of these parameters, with one surprising exception: we show\nthat the problem is FPT parameterized by feedback vertex set for any $\\chi_d\n\\ge 2$, and hence 2-coloring is the only hard case for this parameter. In\naddition to the above, we give an ETH-based lower bound for treewidth and\npathwidth, showing that no algorithm can solve the problem in $n^{o(pw)}$,\nessentially matching the complexity of an algorithm obtained with standard\ntechniques.\n  We complement these results by considering the problem's approximability and\nshow that, with respect to $\\Delta^*$, the problem admits an algorithm which\nfor any $\\epsilon > 0$ runs in time $(tw/\\epsilon)^{O(tw)}$ and returns a\nsolution with exactly the desired number of colors that approximates the\noptimal $\\Delta^*$ within $(1 + \\epsilon)$. We also give a $(tw)^{O(tw)}$\nalgorithm which achieves the desired $\\Delta^*$ exactly while 2-approximating\nthe minimum value of $\\chi_d$. We show that this is close to optimal, by\nestablishing that no FPT algorithm can (under standard assumptions) achieve a\nbetter than $3/2$-approximation to $\\chi_d$, even when an extra constant\nadditive error is also allowed. \n\n"}
{"id": "1801.04641", "contents": "Title: Strategies for Stable Merge Sorting Abstract: We introduce new stable natural merge sort algorithms, called $2$-merge sort\nand $\\alpha$-merge sort. We prove upper and lower bounds for several merge sort\nalgorithms, including Timsort, Shivers' sort, $\\alpha$-stack sorts, and our new\n$2$-merge and $\\alpha$-merge sorts. The upper and lower bounds have the forms\n$c \\cdot n \\log m$ and $c \\cdot n \\log n$ for inputs of length~$n$ comprising\n$m$~monotone runs. For Timsort, we prove a lower bound of $(1.5 - o(1)) n \\log\nn$. For $2$-merge sort, we prove optimal upper and lower bounds of\napproximately $(1.089 \\pm o(1))n \\log m$. We prove similar asymptotically\nmatching upper and lower bounds for $\\alpha$-merge sort, when $\\varphi < \\alpha\n< 2$, where $\\varphi$ is the golden ratio.\n  Our bounds are in terms of merge cost; this upper bounds the number of\ncomparisons and accurately models runtime. The merge strategies can be used for\nany stable merge sort, not just natural merge sorts. The new $2$-merge and\n$\\alpha$-merge sorts have better worst-case merge cost upper bounds and are\nslightly simpler to implement than the widely-used Timsort; they also perform\nbetter in experiments. We report also experimental comparisons with algorithms\ndeveloped by Munro-Wild and Jug\\'e subsequently to the results of the present\npaper. \n\n"}
{"id": "1801.04736", "contents": "Title: The first continuous optical monitoring of the transitional millisecond\n  pulsar PSR J1023+0038 with Kepler Abstract: We report on the first continuous, 80 day optical monitoring of the\ntransitional millisecond pulsar PSR J1023+0038 carried out in mid-2017 with\nKepler in the K2 configuration, when an X-ray subluminous accretion disk was\npresent in the binary. Flares lasting from minutes to 14 hr were observed for\n15.6% of the time, which is a larger fraction than previously reported on the\nbasis of X-ray and past optical observations, and more frequently when the\ncompanion was at the superior conjunction of the orbit. A sinusoidal modulation\nat the binary orbital period was also present with an amplitude of ~16%, which\nvaried by a few percent over timescales of days, and with a maximum that took\nplace 890 +/- 85 s earlier than the superior conjunction of the donor. We\ninterpret these phenomena in terms of reprocessing of the X-ray emission by an\nasymmetrically heated companion star surface and/or a non-axisymmetric outflow\npossibly launched close to the inner Lagrangian point. Furthermore, the\nnon-flaring average emission varied by up to ~ 40% over a time scale of days in\nthe absence of correspondingly large variations of the irradiating X-ray flux.\nThe latter suggests that the observed changes in the average optical luminosity\nmight be due to variations of the geometry, size, and/or mass accretion rate in\nthe outer regions of the accretion disk. \n\n"}
{"id": "1801.05498", "contents": "Title: Graph-indexed random walks on special classes of graphs Abstract: We investigate the paramater of the average range of $M$-Lipschitz mapping of\na given graph. We focus on well-known classes such as paths, complete graphs,\ncomplete bipartite graphs and cycles and show closed formulas for computing\nthis parameter and also we conclude asymptotics of this parameter on these\naforementioned classes. \n\n"}
{"id": "1801.06164", "contents": "Title: The evolution of the X-ray afterglow emission of GW 170817 / GRB 170817A\n  in XMM-Newton observations Abstract: We report our observation of the short GRB 170817A, associated to the binary\nneutron star merger event GW 170817, perfomed in the X-ray band with XMM-Newton\n135 d after the event (on the 29th December 2017). We find evidence for a\nflattening of the X-ray light curve with respect to the previously observed\nbrightening. This is also supported by a nearly simultaneous optical Hubble\nSpace Telescope and successive X-ray Chandra and low-frequency radio\nobservations recently reported in the literature. Since the optical-to-X-ray\nspectral slope did not change with respect to previous observations, we exclude\nthat the change in the temporal evolution of the light curve is due to the\npassage of the cooling frequency: its origin must be geometric or dynamical. We\ninterpret all the existing afterglow data with two models: i) a structured jet\nand ii) a jet-less isotropic fireball with some stratification in its radial\nvelocity structure. Both models fit the data and predict that the radio flux\nmust decrease simultaneously with the optical and the X-ray one, making hard to\ndistinguish between them at the present stage. Polarimetric measures and the\nrate of short GRB-GW association in future LIGO/Virgo runs will be key to\ndisentangle these two geometrically different scenarios. \n\n"}
{"id": "1801.08657", "contents": "Title: Fully kinetic large scale simulations of the collisionless\n  Magnetorotational instability Abstract: We present two-dimensional particle-in-cell (PIC) simulations of the fully\nkinetic collisionless magnetorotational instability (MRI) in weakly magnetized\n(high $\\beta$) pair plasma. The central result of this numerical analysis is\nthe emergence of a self-induced turbulent regime in the saturation state of the\ncollisionless MRI, which can only be captured for large enough simulation\ndomains. One of the underlying mechanisms for the development of this turbulent\nstate is the drift-kink instability (DKI) of the current sheets resulting from\nthe nonlinear evolution of the channel modes. The onset of the DKI can only be\nobserved for simulation domain sizes exceeding several linear MRI wavelengths.\nThe DKI, together with ensuing magnetic reconnection, activate the turbulent\nmotion of the plasma in the late stage of the nonlinear evolution of the MRI.\nAt steady state, the magnetic energy has an MHD-like spectrum with a slope of\n$k^{-5/3}$ for $k\\rho<1$ and $k^{-3}$ for sub-Larmor scale ($k\\rho>1$). We also\nexamine the role of the collisionless MRI and associated magnetic reconnection\nin the development of pressure anisotropy. We study the stability of the system\ndue to this pressure anisotropy, observing the development of mirror\ninstability during the early-stage of the MRI. We further discuss the\nimportance of magnetic reconnection for particle acceleration during the\nturbulence regime. In particular, consistent with reconnection studies, we show\nthat at late times the kinetic energy presents a characteristic slope of\n$\\epsilon^{-2}$ in the high-energy region. \n\n"}
{"id": "1801.09798", "contents": "Title: Earthmover Resilience and Testing in Ordered Structures Abstract: One of the main challenges in property testing is to characterize those\nproperties that are testable with a constant number of queries. For unordered\nstructures such as graphs and hypergraphs this task has been mostly settled.\nHowever, for ordered structures such as strings, images, and ordered graphs,\nthe characterization problem seems very difficult in general.\n  In this paper, we identify a wide class of properties of ordered structures -\nthe earthmover resilient (ER) properties - and show that the \"good behavior\" of\nsuch properties allows us to obtain general testability results that are\nsimilar to (and more general than) those of unordered graphs. A property P is\nER if, roughly speaking, slight changes in the order of the elements in an\nobject satisfying P cannot make this object far from P. The class of ER\nproperties includes, e.g., all unordered graph properties, many natural visual\nproperties of images, such as convexity, and all hereditary properties of\nordered graphs and images.\n  A special case of our results implies, building on a recent result of Alon\nand the authors, that the distance of a given image or ordered graph from any\nhereditary property can be estimated (with good probability) up to a constant\nadditive error, using a constant number of queries. \n\n"}
{"id": "1801.10161", "contents": "Title: Searching for Dark Photon Dark Matter with Gravitational Wave Detectors Abstract: If dark matter stems from the background of a very light gauge boson, this\ngauge boson could exert forces on test masses in gravitational wave detectors,\nresulting in displacements with a characteristic frequency set by the gauge\nboson mass. We outline a novel search strategy to hunt for such dark matter,\nand show that both ground-based and future space-based gravitational wave\ndetectors have the capability to make a 5$\\sigma$ discovery in unexplored\nparameter regimes. \n\n"}
{"id": "1801.10416", "contents": "Title: Hardness, Approximability, and Fixed-Parameter Tractability of the\n  Clustered Shortest-Path Tree Problem Abstract: Given an $n$-vertex non-negatively real-weighted graph $G$, whose vertices\nare partitioned into a set of $k$ clusters, a \\emph{clustered network design\nproblem} on $G$ consists of solving a given network design optimization problem\non $G$, subject to some additional constraint on its clusters.\n  In particular, we focus on the classic problem of designing a\n\\emph{single-source shortest-path tree}, and we analyze its computational\nhardness when in a feasible solution each cluster is required to form a\nsubtree. We first study the \\emph{unweighted} case, and prove that the problem\nis \\np-hard. However, on the positive side, we show the existence of an\napproximation algorithm whose quality essentially depends on few parameters,\nbut which remarkably is an $O(1)$-approximation when the largest out of all the\n\\emph{diameters} of the clusters is either $O(1)$ or $\\Theta(n)$. Furthermore,\nwe also show that the problem is \\emph{fixed-parameter tractable} with respect\nto $k$ or to the number of vertices that belong to clusters of size at least 2.\nThen, we focus on the \\emph{weighted} case, and show that the problem can be\napproximated within a tight factor of $O(n)$, and that it is fixed-parameter\ntractable as well. Finally, we analyze the unweighted \\emph{single-pair\nshortest path problem}, and we show it is hard to approximate within a (tight)\nfactor of $n^{1-\\epsilon}$, for any $\\epsilon>0$. \n\n"}
{"id": "1802.01238", "contents": "Title: Listening to the cohomology of graphs Abstract: We prove that the spectrum of the Kirchhoff Laplacian H0 of a finite simple\nBarycentric refined graph and the spectrum of the connection Laplacian L of G\ndetermine each other: we prove that L-L^(-1) is similar to the Hodge Laplacian\nH of G which is in one dimensions the direct sum of the Kirchhoff Laplacian H0\nand its 1-form analog H1. The spectrum of a single choice of H0,H1 or H alone\ndetermines the Betti numbers b0,b1 of G as well as the spectrum of the other\nmatrices. It follows that b0 is the number of eigenvalues 1 of L and that b1 is\nthe number of eigenvalues -1 of L. For a general abstract finite simplicial\ncomplex G, we express the matrix entries g(x,y) = w(x) w(y) X( St(x) cap St(y)\n) of the inverse of L using stars St(x)= { z in G | x subset of z } of x and\nw(x)=(-1)^dim(x) and Euler characteristic X. One can see W+(x)=St(x) and\nW-(x)={ z in G | z subset x } as stable and unstable manifolds of a simplex x\nin G and g(x,y) =w(x) w(y) X(W+(x) cap W+(y)) as heteroclinic intersection\nnumbers or curvatures and the identity L g=1 as a collection of Gauss-Bonnet\nformulas. The homoclinic energy w(x)=X(W+(x) cap W-(x)) by definition adds up\nto X(G). The matrix M(x,y)=w(x) w(y) X(W-(x) cap W-(y)) is similar to\nL(x,y)=X(W-(x) cap W-(y)). The sum of the matrix entries of M is the definition\nof Wu characteristic. For dimension 2 and higher we don't know yet how to\nrecover the Betti numbers from the eigenvalues of the matrix H or from L. So\nfar, it can only be obtained from a collection of block matrices, via the Hodge\nrelations b_k = dim(H_k). A natural conjecture is that for a Barycentric\nrefinement of a complex G, the spectrum of L determines the Betti vector. We\nknow this now in one dimensions. \n\n"}
{"id": "1802.03160", "contents": "Title: Distributed Spanner Approximation Abstract: We address the fundamental network design problem of constructing approximate\nminimum spanners. Our contributions are for the distributed setting, providing\nboth algorithmic and hardness results.\n  Our main hardness result shows that an $\\alpha$-approximation for the minimum\ndirected $k$-spanner problem for $k \\geq 5$ requires $\\Omega(n\n/\\sqrt{\\alpha}\\log{n})$ rounds using deterministic algorithms or\n$\\Omega(\\sqrt{n }/\\sqrt{\\alpha}\\log{n})$ rounds using randomized ones, in the\nCONGEST model of distributed computing. Combined with the constant-round\n$O(n^{\\epsilon})$-approximation algorithm in the LOCAL model of [Barenboim,\nElkin and Gavoille, 2016], as well as a polylog-round\n$(1+\\epsilon)$-approximation algorithm in the LOCAL model that we show here,\nour lower bounds for the CONGEST model imply a strict separation between the\nLOCAL and CONGEST models. Notably, to the best of our knowledge, this is the\nfirst separation between these models for a local approximation problem.\n  Similarly, a separation between the directed and undirected cases is implied.\nWe also prove a nearly-linear lower bound for the minimum weighted $k$-spanner\nproblem for $k \\geq 4$, and we show lower bounds for the weighted 2-spanner\nproblem.\n  On the algorithmic side, apart from the aforementioned\n$(1+\\epsilon)$-approximation algorithm for minimum $k$-spanners, our main\ncontribution is a new distributed construction of minimum 2-spanners that uses\nonly polynomial local computations. Our algorithm has a guaranteed\napproximation ratio of $O(\\log(m/n))$ for a graph with $n$ vertices and $m$\nedges, which matches the best known ratio for polynomial time sequential\nalgorithms [Kortsarz and Peleg, 1994], and is tight if we restrict ourselves to\npolynomial local computations. Our approach allows us to extend our algorithm\nto work also for the directed, weighted, and client-server variants of the\nproblem. \n\n"}
{"id": "1802.04773", "contents": "Title: Future Ground-based Wide Field of View Air Shower Detectors Abstract: Extensive air shower (EAS) arrays directly sample the shower particles that\nreach the observation altitude. They are wide field of view (FoV) detectors\nable to view the whole sky simultaneously and continuously. In fact, EAS arrays\nhave an effective FoV of about 2 sr and operate with a duty cycle of\n$\\sim$100\\%. This capability makes them well suited to study extended sources,\nsuch as the Galactic diffuse emission and measure the spectra of Galactic\nsources at the highest energies (near or beyond 100 TeV). Their sensitivity in\nthe sub-TeV/TeV energy domain cannot compete with that of Cherenkov telescopes,\nbut the wide FoV is ideal to perform unbiased sky surveys, discover transients\nor explosive events (GRBs) and monitor variable or flaring sources such as\nActive Galactic Nuclei (AGN). An EAS array is able to detect at the same time\nevents induced by photons and charged cosmic rays, thus studying the connection\nbetween these two messengers of the non-thermal Universe. Therefore, these\ndetectors are, by definition, multi-messenger instruments.\n  Wide FoV telescopes are crucial for a multi-messenger study of the\nGravitational Wave events due to their capability to survey simultaneously all\nthe large sky regions identified by LIGO and VIRGO, looking for a possible\ncorrelated $\\gamma$-ray emission.\n  In this contribution we summarize the scientific motivations which push the\nconstruction of new wide FoV air shower detectors and introduce the future\ninstruments currently under installation. Finally, we emphasize the need of an\nEAS array in the Southern hemisphere to monitor the Inner Galaxy and face a\nnumber of important open problems. \n\n"}
{"id": "1802.05134", "contents": "Title: Quantum versus Classical Online Streaming Algorithms with Advice Abstract: We consider online algorithms with respect to the competitive ratio. Here, we\ninvestigate quantum and classical one-way automata with non-constant size of\nmemory (streaming algorithms) as a model for online algorithms. We construct\nproblems that can be solved by quantum online streaming algorithms better than\nby classical ones in a case of logarithmic or sublogarithmic size of memory,\neven if classical online algorithms get advice bits. Furthermore, we show that\na quantum online algorithm with a constant number of qubits can be better than\nany deterministic online algorithm with a constant number of advice bits and\nunlimited computational power. \n\n"}
{"id": "1802.06030", "contents": "Title: Improving the Florentine algorithms: recovering algorithms for Motzkin\n  and Schr\\\"oder paths Abstract: We present random sampling procedures for Motzkin and Schr\\\"oder paths,\nfollowing previous work on Dyck paths. Our algorithms follow the anticipated\nrejection method of the Florentine algorithms (Barcucci et al. 1994+), but\nintroduce a recovery idea to greatly reduce the probability of rejection. They\nuse an optimal amount of randomness and achieve a better time complexity than\nthe Florentine algorithms. \n\n"}
{"id": "1802.06204", "contents": "Title: Approximate Set Union Via Approximate Randomization Abstract: We develop an randomized approximation algorithm for the size of set union\nproblem $\\arrowvert A_1\\cup A_2\\cup...\\cup A_m\\arrowvert$, which given a list\nof sets $A_1,...,A_m$ with approximate set size $m_i$ for $A_i$ with $m_i\\in\n\\left((1-\\beta_L)|A_i|, (1+\\beta_R)|A_i|\\right)$, and biased random generators\nwith $Prob(x=\\randomElm(A_i))\\in \\left[{1-\\alpha_L\\over |A_i|},{1+\\alpha_R\\over\n|A_i|}\\right]$ for each input set $A_i$ and element $x\\in A_i,$ where $i=1, 2,\n..., m$. The approximation ratio for $\\arrowvert A_1\\cup A_2\\cup...\\cup\nA_m\\arrowvert$ is in the range $[(1-\\epsilon)(1-\\alpha_L)(1-\\beta_L),\n(1+\\epsilon)(1+\\alpha_R)(1+\\beta_R)]$ for any $\\epsilon\\in (0,1)$, where\n$\\alpha_L, \\alpha_R, \\beta_L,\\beta_R\\in (0,1)$. The complexity of the algorithm\nis measured by both time complexity, and round complexity. The algorithm is\nallowed to make multiple membership queries and get random elements from the\ninput sets in one round. Our algorithm makes adaptive accesses to input sets\nwith multiple rounds. Our algorithm gives an approximation scheme with\n$O(\\setCount\\cdot(\\log \\setCount)^{O(1)})$ running time and $O(\\log m)$ rounds,\nwhere $m$ is the number of sets. Our algorithm can handle input sets that can\ngenerate random elements with bias, and its approximation ratio depends on the\nbias. Our algorithm gives a flexible tradeoff with time complexity\n$O\\left(\\setCount^{1+\\xi}\\right)$ and round complexity $O\\left({1\\over\n\\xi}\\right)$ for any $\\xi\\in(0,1)$. \n\n"}
{"id": "1802.06361", "contents": "Title: On Finding Dense Common Subgraphs Abstract: We study the recently introduced problem of finding dense common subgraphs:\nGiven a sequence of graphs that share the same vertex set, the goal is to find\na subset of vertices $S$ that maximizes some aggregate measure of the density\nof the subgraphs induced by $S$ in each of the given graphs. Different choices\nfor the aggregation function give rise to variants of the problem that were\nstudied recently. We settle many of the questions left open by previous works,\nshowing NP-hardness, hardness of approximation, non-trivial approximation\nalgorithms, and an integrality gap for a natural relaxation. \n\n"}
{"id": "1802.06575", "contents": "Title: On the Decidability of Reachability in Linear Time-Invariant Systems Abstract: We consider the decidability of state-to-state reachability in linear\ntime-invariant control systems over discrete time. We analyse this problem with\nrespect to the allowable control sets, which in general are assumed to be\ndefined by boolean combinations of linear inequalities. Decidability of the\nversion of the reachability problem in which control sets are affine subspaces\nof $\\mathbb{R}^n$ is a fundamental result in control theory. Our first result\nis that reachability is undecidable if the set of controls is a finite union of\naffine subspaces. We also consider versions of the reachability problem in\nwhich (i)~the set of controls consists of a single affine subspace together\nwith the origin and (ii)~the set of controls is a convex polytope. In these two\ncases we respectively show that the reachability problem is as hard as Skolem's\nProblem and the Positivity Problem for linear recurrence sequences (whose\ndecidability has been open for several decades). Our main contribution is to\nshow decidability of a version of the reachability problem in which control\nsets are convex polytopes, under certain spectral assumptions on the transition\nmatrix. \n\n"}
{"id": "1802.06676", "contents": "Title: A Simple Parallel and Distributed Sampling Technique: Local Glauber\n  Dynamics Abstract: \\emph{Sampling} constitutes an important tool in a variety of areas: from\nmachine learning and combinatorial optimization to computational physics and\nbiology. A central class of sampling algorithms is the \\emph{Markov Chain Monte\nCarlo} method, based on the construction of a Markov chain with the desired\nsampling distribution as its stationary distribution. Many of the traditional\nMarkov chains, such as the \\emph{Glauber dynamics}, do not scale well with\nincreasing dimension. To address this shortcoming, we propose a simple local\nupdate rule based on the Glauber dynamics that leads to efficient parallel and\ndistributed algorithms for sampling from Gibbs distributions.\n  Concretely, we present a Markov chain that mixes in $O(\\log n)$ rounds when\nDobrushin's condition for the Gibbs distribution is satisfied. This improves\nover the \\emph{LubyGlauber} algorithm by Feng, Sun, and Yin [PODC'17], which\nneeds $O(\\Delta \\log n)$ rounds, and their \\emph{LocalMetropolis} algorithm,\nwhich converges in $O(\\log n)$ rounds but requires a considerably stronger\nmixing condition. Here, $n$ denotes the number of nodes in the graphical model\ninducing the Gibbs distribution, and $\\Delta$ its maximum degree. In\nparticular, our method can sample a uniform proper coloring with $\\alpha\n\\Delta$ colors in $O(\\log n)$ rounds for any $\\alpha>2$, which almost matches\nthe threshold of the sequential Glauber dynamics and improves on the $\\alpha>2\n+\\sqrt{2}$ threshold of Feng et al. \n\n"}
{"id": "1802.06905", "contents": "Title: Communication-Optimal Convolutional Neural Nets Abstract: Efficiently executing convolutional neural nets (CNNs) is important in many\nmachine-learning tasks. Since the cost of moving a word of data, either between\nlevels of a memory hierarchy or between processors over a network, is much\nhigher than the cost of an arithmetic operation, minimizing data movement is\ncritical to performance optimization. In this paper, we present both new lower\nbounds on data movement needed for CNNs, and optimal sequential algorithms that\nattain these lower bounds. In most common cases, our optimal algorithms can\nattain significantly more data reuse than matrix multiplication. \n\n"}
{"id": "1802.07632", "contents": "Title: Spanning Tree Congestion and Computation of Generalized\n  Gy\\H{o}ri-Lov\\'{a}sz Partition Abstract: We study a natural problem in graph sparsification, the Spanning Tree\nCongestion (\\STC) problem. Informally, the \\STC problem seeks a spanning tree\nwith no tree-edge \\emph{routing} too many of the original edges. The root of\nthis problem dates back to at least 30 years ago, motivated by applications in\nnetwork design, parallel computing and circuit design. Variants of the problem\nhave also seen algorithmic applications as a preprocessing step of several\nimportant graph algorithms.\n  For any general connected graph with $n$ vertices and $m$ edges, we show that\nits STC is at most $\\mathcal{O}(\\sqrt{mn})$, which is asymptotically optimal\nsince we also demonstrate graphs with STC at least $\\Omega(\\sqrt{mn})$. We\npresent a polynomial-time algorithm which computes a spanning tree with\ncongestion $\\mathcal{O}(\\sqrt{mn}\\cdot \\log n)$. We also present another\nalgorithm for computing a spanning tree with congestion\n$\\mathcal{O}(\\sqrt{mn})$; this algorithm runs in sub-exponential time when $m =\n\\omega(n \\log^2 n)$.\n  For achieving the above results, an important intermediate theorem is\n\\emph{generalized Gy\\H{o}ri-Lov\\'{a}sz theorem}, for which Chen et al. gave a\nnon-constructive proof. We give the first elementary and constructive proof by\nproviding a local search algorithm with running time $\\mathcal{O}^*\\left( 4^n\n\\right)$, which is a key ingredient of the above-mentioned sub-exponential time\nalgorithm. We discuss a few consequences of the theorem concerning graph\npartitioning, which might be of independent interest.\n  We also show that for any graph which satisfies certain \\emph{expanding\nproperties}, its STC is at most $\\mathcal{O}(n)$, and a corresponding spanning\ntree can be computed in polynomial time. We then use this to show that a random\ngraph has STC $\\Theta(n)$ with high probability. \n\n"}
{"id": "1802.08189", "contents": "Title: Complexity of the Steiner Network Problem with Respect to the Number of\n  Terminals Abstract: In the Directed Steiner Network problem we are given an arc-weighted digraph\n$G$, a set of terminals $T \\subseteq V(G)$, and an (unweighted) directed\nrequest graph $R$ with $V(R)=T$. Our task is to output a subgraph $G' \\subseteq\nG$ of the minimum cost such that there is a directed path from $s$ to $t$ in\n$G'$ for all $st \\in A(R)$.\n  It is known that the problem can be solved in time $|V(G)|^{O(|A(R)|)}$\n[Feldman&Ruhl, SIAM J. Comput. 2006] and cannot be solved in time\n$|V(G)|^{o(|A(R)|)}$ even if $G$ is planar, unless Exponential-Time Hypothesis\n(ETH) fails [Chitnis et al., SODA 2014]. However, as this reduction (and other\nreductions showing hardness of the problem) only shows that the problem cannot\nbe solved in time $|V(G)|^{o(|T|)}$ unless ETH fails, there is a significant\ngap in the complexity with respect to $|T|$ in the exponent.\n  We show that Directed Steiner Network is solvable in time $f(R)\\cdot\n|V(G)|^{O(c_g \\cdot |T|)}$, where $c_g$ is a constant depending solely on the\ngenus of $G$ and $f$ is a computable function. We complement this result by\nshowing that there is no $f(R)\\cdot |V(G)|^{o(|T|^2/ \\log |T|)}$ algorithm for\nany function $f$ for the problem on general graphs, unless ETH fails. \n\n"}
{"id": "1802.08227", "contents": "Title: Quantum linear systems algorithms: a primer Abstract: The Harrow-Hassidim-Lloyd (HHL) quantum algorithm for sampling from the\nsolution of a linear system provides an exponential speed-up over its classical\ncounterpart. The problem of solving a system of linear equations has a wide\nscope of applications, and thus HHL constitutes an important algorithmic\nprimitive. In these notes, we present the HHL algorithm and its improved\nversions in detail, including explanations of the constituent sub- routines.\nMore specifically, we discuss various quantum subroutines such as quantum phase\nestimation and amplitude amplification, as well as the important question of\nloading data into a quantum computer, via quantum RAM. The improvements to the\noriginal algorithm exploit variable-time amplitude amplification as well as a\nmethod for implementing linear combinations of unitary operations (LCUs) based\non a decomposition of the operators using Fourier and Chebyshev series.\nFinally, we discuss a linear solver based on the quantum singular value\nestimation (QSVE) subroutine. \n\n"}
{"id": "1802.08509", "contents": "Title: Graph Similarity and Approximate Isomorphism Abstract: The graph similarity problem, also known as approximate graph isomorphism or\ngraph matching problem, has been extensively studied in the machine learning\ncommunity, but has not received much attention in the algorithms community:\nGiven two graphs $G,H$ of the same order $n$ with adjacency matrices $A_G,A_H$,\na well-studied measure of similarity is the Frobenius distance \\[\n\\mathrm{dist}(G,H):=\\min_{\\pi}\\|A_G^\\pi-A_H\\|_F, \\] where $\\pi$ ranges over all\npermutations of the vertex set of $G$, where $A_G^\\pi$ denotes the matrix\nobtained from $A_G$ by permuting rows and columns according to $\\pi$, and where\n$\\|M\\|_F$ is the Frobenius norm of a matrix $M$. The (weighted) graph\nsimilarity problem, denoted by SIM (WSIM), is the problem of computing this\ndistance for two graphs of same order. This problem is closely related to the\nnotoriously hard quadratic assignment problem (QAP), which is known to be\nNP-hard even for severely restricted cases.\n  It is known that SIM (WSIM) is NP-hard; we strengthen this hardness result by\nshowing that the problem remains NP-hard even for the class of trees.\nIdentifying the boundary of tractability for WSIM is best done in the framework\nof linear algebra. We show that WSIM is NP-hard as long as one of the matrices\nhas unbounded rank or negative eigenvalues: hence, the realm of tractability is\nrestricted to positive semi-definite matrices of bounded rank. Our main result\nis a polynomial time algorithm for the special case where one of the matrices\nhas a bounded clustering number, a parameter arising from spectral graph\ndrawing techniques. \n\n"}
{"id": "1803.00599", "contents": "Title: Synchrotron radiation from the fast tail of dynamical ejecta of neutron\n  star mergers Abstract: We find, using high resolution numerical relativistic simulations, that the\ntail of the dynamical ejecta of neutron star mergers extends to mildly\nrelativistic velocities faster than $0.7c$. The kinetic energy of this fast\ntail is $\\sim 10^{47}$--$10^{49}$ erg, depending on the neutron star equation\nof state and on the binary masses. The synchrotron flare arising from the\ninteraction of this fast tail with the surrounding ISM can power the observed\nnon-thermal emission that followed GW170817, provided that the ISM density is\n$\\sim 10^{-2}\\,{\\rm cm^{-3}}$, the two neutron stars had roughly equal masses\nand the neutron star equation of state is soft (small neutron star radii). One\nof the generic predictions of this scenario is that the cooling frequency\ncrosses the X-ray band on a time scale of a few months to a year, leading to a\ncooling break in the X-ray light curve. If this dynamical ejecta scenario is\ncorrect, we expect that the synchrotron radio flare from the ejecta that have\nproduced the macronova/kilonova emission will be observable on time scales of\n$10^3$ to $10^5$ days. Further multi-frequency observations will confirm or\nrule out this dynamical ejecta scenario. \n\n"}
{"id": "1803.03833", "contents": "Title: Submodular Hypergraphs: p-Laplacians, Cheeger Inequalities and Spectral\n  Clustering Abstract: We introduce submodular hypergraphs, a family of hypergraphs that have\ndifferent submodular weights associated with different cuts of hyperedges.\nSubmodular hypergraphs arise in clustering applications in which higher-order\nstructures carry relevant information. For such hypergraphs, we define the\nnotion of p-Laplacians and derive corresponding nodal domain theorems and k-way\nCheeger inequalities. We conclude with the description of algorithms for\ncomputing the spectra of 1- and 2-Laplacians that constitute the basis of new\nspectral hypergraph clustering methods. \n\n"}
{"id": "1803.03839", "contents": "Title: Efficient Enumeration of Bipartite Subgraphs in Graphs Abstract: Subgraph enumeration problems ask to output all subgraphs of an input graph\nthat belongs to the specified graph class or satisfy the given constraint.\nThese problems have been widely studied in theoretical computer science. As\nfar, many efficient enumeration algorithms for the fundamental substructures\nsuch as spanning trees, cycles, and paths, have been developed. This paper\naddresses the enumeration problem of bipartite subgraphs. Even though bipartite\ngraphs are quite fundamental and have numerous applications in both theory and\napplication, its enumeration algorithms have not been intensively studied, to\nthe best of our knowledge. We propose the first non-trivial algorithms for\nenumerating all bipartite subgraphs in a given graph. As the main results, we\ndevelop two efficient algorithms: the one enumerates all bipartite induced\nsubgraphs of a graph with degeneracy $k$ in $O(k)$ time per solution. The other\nenumerates all bipartite subgraphs in $O(1)$ time per solution. \n\n"}
{"id": "1803.04388", "contents": "Title: Partitioning a graph into degenerate subgraphs Abstract: Let $G = (V, E)$ be a connected graph with maximum degree $k\\geq 3$ distinct\nfrom $K_{k+1}$. Given integers $s \\geq 2$ and $p_1,\\ldots,p_s\\geq 0$, $G$ is\nsaid to be $(p_1, \\dots, p_s)$-partitionable if there exists a partition of $V$\ninto sets~$V_1,\\ldots,V_s$ such that $G[V_i]$ is $p_i$-degenerate for\n$i\\in\\{1,\\ldots,s\\}$. In this paper, we prove that we can find a $(p_1, \\dots,\np_s)$-partition of $G$ in $O(|V| + |E|)$-time whenever $1\\geq p_1, \\dots, p_s\n\\geq 0$ and $p_1 + \\dots + p_s \\geq k - s$. This generalizes a result of Bonamy\net al. (MFCS, 2017) and can be viewed as an algorithmic extension of Brooks'\ntheorem and several results on vertex arboricity of graphs of bounded maximum\ndegree.\n  We also prove that deciding whether $G$ is $(p, q)$-partitionable is\n$\\mathbb{NP}$-complete for every $k \\geq 5$ and pairs of non-negative integers\n$(p, q)$ such that $(p, q) \\not = (1, 1)$ and $p + q = k - 3$. This resolves an\nopen problem of Bonamy et al. (manuscript, 2017). Combined with results of\nBorodin, Kostochka and Toft (\\emph{Discrete Mathematics}, 2000), Yang and Yuan\n(\\emph{Discrete Mathematics}, 2006) and Wu, Yuan and Zhao (\\emph{Journal of\nMathematical Study}, 1996), it also settles the complexity of deciding whether\na graph with bounded maximum degree can be partitioned into two subgraphs of\nprescribed degeneracy. \n\n"}
{"id": "1803.04822", "contents": "Title: A Deep X-ray Survey of the Globular Cluster Omega Centauri Abstract: We identify 233 X-ray sources, of which 95 are new, in a 222 ks exposure of\nOmega Centauri with the Chandra X-ray Observatory's ACIS-I detector. The\nlimiting unabsorbed flux in the core is $f_x$ (0.5$-$6.0 keV) $\\simeq$ 3\n$\\times$ 10$^{-16}$ erg s$^{-1}$ cm$^{-2}$ ($L_X$ $\\simeq$ 1 $\\times$ 10$^{30}$\nerg s$^{-1}$ at 5.2 kpc). We estimate that ~$60\\pm 20$ of these are cluster\nmembers, of which ~30 lie within the core ($r_c$ $=$ 155 arcsec), and another\n~30 between 1$-$2 core radii. We identify four new optical counterparts, for a\ntotal of 45 likely identifications. Probable cluster members include 18\ncataclysmic variables (CVs) and CV candidates, one quiescent low-mass X-ray\nbinary, four variable stars, and five stars that are either associated with w\nCen's anomalous red giant branch, or are sub-subgiants. We estimate that the\ncluster contains $40\\pm 10$ CVs with $L_X$ $>$ 10$^{31}$ erg s$^{-1}$,\nconfirming that CVs are underabundant in w Cen relative to the field. Intrinsic\nabsorption is required to fit X-ray spectra of six of the nine brightest CVs,\nsuggesting magnetic CVs, or high-inclination systems. Though no radio\nmillisecond pulsars (MSPs) are currently known in w Cen, more than 30\nunidentified sources have luminosities and X-ray colours like those of MSPs\nfound in other globular clusters; these could be responsible for the\nFermi-detected gamma-ray emission from the cluster. Finally, we identify a CH\nstar as the counterpart to the second-brightest X-ray source in the cluster and\nargue that it is a symbiotic star. This is the first such giant/white dwarf\nbinary to be identified in a globular cluster. \n\n"}
{"id": "1803.05539", "contents": "Title: Tutte Invariants for Alternating Dimaps Abstract: An alternating dimap is an orientably embedded Eulerian directed graph where\nthe edges incident with each vertex are directed inwards and outwards\nalternately. Three reduction operations for alternating dimaps were\ninvestigated by Farr. A minor of an alternating dimap can be obtained by\nreducing some of its edges using the reduction operations. Unlike classical\nminor operations, these reduction operations do not commute in general. A Tutte\ninvariant for alternating dimaps is a function $ P $ defined on every\nalternating dimap and taking values in a field such that $ P $ is invariant\nunder isomorphism and obeys a linear recurrence relation involving reduction\noperations. It is well known that if a graph $ G $ is planar, then the Tutte\npolynomial $ T $ satisfies $ T(G;x,y)=T(G^{*};y,x) $. We note an analogous\nrelation for the extended Tutte invariants for alternating dimaps introduced by\nFarr. We then characterise the Tutte invariant for alternating dimaps of genus\nzero under several conditions. As a result of the non-commutativity of the\nreduction operations, the recursions based on them cannot always be satisfied.\nWe investigate the properties of alternating dimaps of genus zero that are\nrequired in order to obtain a well defined Tutte invariant. Some excluded minor\ncharacterisations for these alternating dimaps are also given. \n\n"}
{"id": "1803.08038", "contents": "Title: On Non-localization of Eigenvectors of High Girth Graphs Abstract: We prove improved bounds on how localized an eigenvector of a high girth\nregular graph can be, and present examples showing that these bounds are close\nto sharp. This study was initiated by Brooks and Lindenstrauss (2009) who\nrelied on the observation that certain suitably normalized averaging operators\non high girth graphs are hyper-contractive and can be used to approximate\nprojectors onto the eigenspaces of such graphs. Informally, their\ndelocalization result in the contrapositive states that for any $\\varepsilon\n\\in (0,1)$ and positive integer $k,$ if a $(d+1)-$regular graph has an\neigenvector which supports $\\varepsilon$ fraction of the $\\ell_2^2$ mass on a\nsubset of $k$ vertices, then the graph must have a cycle of size\n$\\tilde{O}(\\log_{d}(k)/\\varepsilon^2)$, suppressing logarithmic terms in\n$1/\\varepsilon$. In this paper, we improve the upper bound to\n$\\tilde{O}(\\log_{d}(k)/\\varepsilon)$ and present a construction showing a lower\nbound of $\\Omega(\\log_d(k)/\\varepsilon)$. Our construction is probabilistic and\ninvolves gluing together a pair of trees while maintaining high girth as well\nas control on the eigenvectors and could be of independent interest. \n\n"}
{"id": "1803.08924", "contents": "Title: On the maximum pair multiplicity of pulsar cascades Abstract: We study electron-positron pair production in polar caps of energetic pulsars\nto determine the maximum multiplicity of pair plasma a pulsar can produce under\nthe most favorable conditions. This paper complements and updates our study of\npair cascades presented in Timokhin & Harding (2015) with more accurate\ntreatment of the effects of ultra strong B>3x10^{12}G magnetic fields and\nemission processes of primary and secondary particles. We include pairs\nproduced by curvature and synchrotron radiation photons as well as resonant\nCompton scattered photons. We develop a semi-analytical model of\nelectron-positrons cascades which can efficiently simulate pair cascades with\nan arbitrary number of microphysical processes and use it to explore cascade\nproperties for a wide range of pulsar parameters. We argue that the maximum\ncascade multiplicity can not exceed ~ a few x 10^5 and the multiplicity has a\nrather weak dependence on pulsar period on pulsar period. The highest\nmultiplicity is achieved in pulsars with magnetic field 4x10^{12}<B<10^{13}G\nand hot surfaces, with T>10^6K. We also derive analytical expressions for\nseveral physical quantities relevant for electromagnetic cascade in pulsars\nwhich may be useful in future works on pulsar cascades, including the upper\nlimit on cascade multiplicity and various approximations for the parameter\n\\chi, the exponential factor in the expression for photon attenuation in strong\nmagnetic field. \n\n"}
{"id": "1803.09370", "contents": "Title: Popular Matching in Roommates Setting is NP-hard Abstract: An input to the Popular Matching problem, in the roommates setting, consists\nof a graph $G$ and each vertex ranks its neighbors in strict order, known as\nits preference. In the Popular Matching problem the objective is to test\nwhether there exists a matching $M^\\star$ such that there is no matching $M$\nwhere more people are happier with $M$ than with $M^\\star$. In this paper we\nsettle the computational complexity of the Popular Matching problem in the\nroommates setting by showing that the problem is NP-complete. Thus, we resolve\nan open question that has been repeatedly, explicitly asked over the last\ndecade. \n\n"}
{"id": "1803.09483", "contents": "Title: Clustering to Given Connectivities Abstract: We define a general variant of the graph clustering problem where the\ncriterion of density for the clusters is (high) connectivity. In {\\sc\nClustering to Given Connectivities}, we are given an $n$-vertex graph $G$, an\ninteger $k$, and a sequence $\\Lambda=\\langle\n\\lambda_{1},\\ldots,\\lambda_{t}\\rangle$ of positive integers and we ask whether\nit is possible to remove at most $k$ edges from $G$ such that the resulting\nconnected components are {\\sl exactly} $t$ and their corresponding edge\nconnectivities are lower-bounded by the numbers in $\\Lambda$. We prove that\nthis problem, parameterized by $k$, is fixed parameter tractable i.e., can be\nsolved by an $f(k)\\cdot n^{O(1)}$-step algorithm, for some function $f$ that\ndepends only on the parameter $k$. Our algorithm uses the recursive\nunderstanding technique that is especially adapted so to deal with the fact\nthat, in out setting, we do not impose any restriction to the connectivity\ndemands in $\\Lambda$. \n\n"}
{"id": "1803.09675", "contents": "Title: Extra Space during Initialization of Succinct Data Structures and\n  Dynamical Initializable Arrays Abstract: Many succinct data structures on the word RAM require precomputed tables to\nstart operating. Usually, the tables can be constructed in sublinear time. In\nthis time, most of a data structure is not initialized, i.e., there is plenty\nof unused space allocated for the data structure. We present a general\nframework to store temporarily extra buffers between the real data so that the\ndata can be processed immediately, stored first in the buffers, and then moved\ninto the real data structure after finishing the tables. As an application, we\napply our framework to Dodis, Patrascu, and Thorup's data structure (STOC 2010)\nthat emulates c-ary memory and to Farzan and Munro's succinct encoding of\narbitrary graphs (TCS 2013). We also use our framework to present an in-place\ndynamical initializable array. \n\n"}
{"id": "1803.10976", "contents": "Title: Discovery of a 23.8h QPO in the SWIFT light curve of XMMU\n  J134736.6+173403 Abstract: XMMU J134736.6+173403 is an X-ray source discovered serendipitously by\nXMM-Newton which was found to be spatially coincident with a pair of galaxies,\nincluding a Seyfert 2 galaxy, but presented in 2003 a very sharp persistent\nflux drop of a factor 6.5 within 1h. From the analysis of a set of 29 Swift\nobservations conducted from the 6 February to the 23 May 2008, we discovered\ntwin-peak quasi-periodic oscillations (QPOs) with periods of 23.82+-0.07 h and\n71.44+-0.57 h. Using a Chandra observation of 2008, we evaluate more accurately\nthe position of the X-ray source and show that the new source coordinates\ncoincide with the position of the Seyfert 2 galaxy. We provide a detailed\nspectral energy distribution of the AGN counterpart using multi-wavelength\nobservations. The AGN is radio-loud and the broadband SED modelling indicates a\nblack hole with a mass of 9.8x10^6 Msun, that accretes at an Eddington ratio of\n0.047. QPOs for active galaxies have been reported so far in only few cases,\nthe most reliable one being from RE J1034+396 for which a 1 h periodicity has\nbeen discovered analysing a ~91 ks XMM-Newton observation. Twin peak QPOs with\nan observed frequency ratio of 3:1 have not been reported so far for any AGN.\nFrom resonance models of the epicyclic frequencies we evaluate the different\npossible mass-spin relations. It's still not clear what could have been the\norigin of the high flux and sharp drop only observed in 2003. \n\n"}
{"id": "1804.01076", "contents": "Title: Operator Scaling via Geodesically Convex Optimization, Invariant Theory\n  and Polynomial Identity Testing Abstract: We propose a new second-order method for geodesically convex optimization on\nthe natural hyperbolic metric over positive definite matrices. We apply it to\nsolve the operator scaling problem in time polynomial in the input size and\nlogarithmic in the error. This is an exponential improvement over previous\nalgorithms which were analyzed in the usual Euclidean, \"commutative\" metric\n(for which the above problem is not convex). Our method is general and\napplicable to other settings.\n  As a consequence, we solve the equivalence problem for the left-right group\naction underlying the operator scaling problem. This yields a deterministic\npolynomial-time algorithm for a new class of Polynomial Identity Testing (PIT)\nproblems, which was the original motivation for studying operator scaling. \n\n"}
{"id": "1804.02351", "contents": "Title: Bound on a diffuse flux of ultra-high energy neutrinos in the ADD model Abstract: The search for ultra-high energy downward-going and Earth-skimming cosmic\nneutrinos by the Surface Detector array of the Pierre Auger Observatory (PAO)\nis analyzed in the ADD model with n extra flat spatial dimensions. We assumed\nthat the diffuse neutrino flux dN_nu/dE_nu$ is equal to k E_nu^(-20) in the\nenergy range 10^(17) eV - 2.5 10^(19) eV. Taking into account that no neutrino\nevents where found by the PAO, we have estimated an upper bound on a value of\nk. It is shown that this bound can be stronger than the upper bound on k\nrecently obtained by the Pierre Auger Collaboration, depending on n and\n(n+4)-dimensional gravity scale M_D. \n\n"}
{"id": "1804.02785", "contents": "Title: Maximizing the Number of Spanning Trees in a Connected Graph Abstract: We study the problem of maximizing the number of spanning trees in a\nconnected graph by adding at most $k$ edges from a given candidate edge set. We\ngive both algorithmic and hardness results for this problem:\n  - We give a greedy algorithm that, using submodularity, obtains an\napproximation ratio of $(1 - 1/e - \\epsilon)$ in the exponent of the number of\nspanning trees for any $\\epsilon > 0$ in time $\\tilde{O}(m \\epsilon^{-1} + (n +\nq) \\epsilon^{-3})$, where $m$ and $q$ is the number of edges in the original\ngraph and the candidate edge set, respectively. Our running time is optimal\nwith respect to the input size up to logarithmic factors, and substantially\nimproves upon the $O(n^3)$ running time of the previous proposed greedy\nalgorithm with approximation ratio $(1 - 1/e)$ in the exponent. Notably, the\nindependence of our running time of $k$ is novel, comparing to conventional\ntop-$k$ selections on graphs that usually run in $\\Omega(mk)$ time. A key\ningredient of our greedy algorithm is a routine for maintaining effective\nresistances under edge additions in an online-offline hybrid setting.\n  - We show the exponential inapproximability of this problem by proving that\nthere exists a constant $c > 0$ such that it is NP-hard to approximate the\noptimum number of spanning trees in the exponent within $(1 - c)$. This\ninapproximability result follows from a reduction from the minimum path cover\nin undirected graphs, whose hardness again follows from the constant\ninapproximability of the Traveling Salesman Problem (TSP) with distances 1 and\n2. Thus, the approximation ratio of our algorithm is also optimal up to a\nconstant factor in the exponent. To our knowledge, this is the first hardness\nof approximation result for maximizing the number of spanning trees in a graph,\nor equivalently, by Kirchhoff's matrix-tree theorem, maximizing the determinant\nof an SDDM matrix. \n\n"}
{"id": "1804.02854", "contents": "Title: Tight Hardness Results for Consensus Problems on Circular Strings and\n  Time Series Abstract: Consensus problems for strings and sequences appear in numerous application\ncontexts, ranging from bioinformatics over data mining to machine learning.\nClosing some gaps in the literature, we show that several fundamental problems\nin this context are NP- and W[1]-hard, and that the known (partially\nbrute-force) algorithms are close to optimality assuming the Exponential Time\nHypothesis. Among our main contributions is to settle the complexity status of\ncomputing a mean in dynamic time warping spaces which, as pointed out by Brill\net al. [DMKD 2019], suffered from many unproven or false assumptions in the\nliterature. We prove this problem to be NP-hard and additionally show that a\nrecent dynamic programming algorithm is essentially optimal. In this context,\nwe study a broad family of circular string alignment problems. This family also\nserves as a key for our hardness reductions, and it is of independent\n(practical) interest in molecular biology. In particular, we show tight\nhardness and running time lower bounds for Circular Consensus String; notably,\nthe corresponding non-circular version is easily linear-time solvable. \n\n"}
{"id": "1804.02949", "contents": "Title: Personalized PageRank dimensionality and algorithmic implications Abstract: Many systems, including the Internet, social networks, and the power grid,\ncan be represented as graphs. When analyzing graphs, it is often useful to\ncompute scores describing the relative importance or distance between nodes.\nOne example is Personalized PageRank (PPR), which assigns to each node $v$ a\nvector whose $i$-th entry describes the importance of the $i$-th node from the\nperspective of $v$. PPR has proven useful in many applications, such as\nrecommending who users should follow on social networks (if this $i$-th entry\nis large, $v$ may be interested in following the $i$-th user). Unfortunately,\ncomputing $n$ such PPR vectors (where $n$ is the number of nodes) is infeasible\nfor many graphs of interest.\n  In this work, we argue that the situation is not so dire. Our main result\nshows that the dimensionality of the set of PPR vectors scales sublinearly in\n$n$ with high probability, for a certain class of random graphs and for a\nnotion of dimensionality similar to rank. Put differently, we argue that the\neffective dimension of this set is much less than $n$, despite the fact that\nthe matrix containing these vectors has rank $n$. Furthermore, we show this\ndimensionality measure relates closely to the complexity of a PPR estimation\nscheme that was proposed (but not analyzed) by Jeh and Widom. This allows us to\nargue that accurately estimating all $n$ PPR vectors amounts to computing a\nvanishing fraction of the $n^2$ vector elements (when the technical assumptions\nof our main result are satisfied). Finally, we demonstrate empirically that\nsimilar conclusions hold when considering real-world networks, despite the\nassumptions of our theory not holding. \n\n"}
{"id": "1804.03884", "contents": "Title: Weighted proper orientations of trees and graphs of bounded treewidth Abstract: Given a simple graph $G$, a weight function $w:E(G)\\rightarrow \\mathbb{N}\n\\setminus \\{0\\}$, and an orientation $D$ of $G$, we define $\\mu^-(D) = \\max_{v\n\\in V(G)} w_D^-(v)$, where $w^-_D(v) = \\sum_{u\\in N_D^{-}(v)}w(uv)$. We say\nthat $D$ is a weighted proper orientation of $G$ if $w^-_D(u) \\neq w^-_D(v)$\nwhenever $u$ and $v$ are adjacent. We introduce the parameter weighted proper\norientation number of $G$, denoted by $\\overrightarrow{\\chi}(G,w)$, which is\nthe minimum, over all weighted proper orientations $D$ of $G$, of $\\mu^-(D)$.\nWhen all the weights are equal to 1, this parameter is equal to the proper\norientation number of $G$, which has been object of recent studies and whose\ndetermination is NP-hard in general, but polynomial-time solvable on trees.\nHere, we prove that the equivalent decision problem of the weighted proper\norientation number (i.e., $\\overrightarrow{\\chi}(G,w) \\leq k$?) is (weakly)\nNP-complete on trees but can be solved by a pseudo-polynomial time algorithm\nwhose running time depends on $k$. Furthermore, we present a dynamic\nprogramming algorithm to determine whether a general graph $G$ on $n$ vertices\nand treewidth at most ${\\sf tw}$ satisfies $\\overrightarrow{\\chi}(G,w) \\leq k$,\nrunning in time $O(2^{{\\sf tw}^2}\\cdot k^{3{\\sf tw}}\\cdot {\\sf tw} \\cdot n)$,\nand we complement this result by showing that the problem is W[1]-hard on\ngeneral graphs parameterized by the treewidth of $G$, even if the weights are\npolynomial in $n$. \n\n"}
{"id": "1804.04025", "contents": "Title: Rapid mixing of Glauber dynamics for colorings below Vigoda's $11/6$\n  threshold Abstract: A well-known conjecture in computer science and statistical physics is that\nGlauber dynamics on the set of $k$-colorings of a graph $G$ on $n$ vertices\nwith maximum degree $\\Delta$ is rapidly mixing for $k \\geq \\Delta +2$. In FOCS\n1999, Vigoda showed rapid mixing of flip dynamics with certain flip parameters\non the set of proper $k$-colorings for $k > \\frac{11}{6}\\Delta$, implying rapid\nmixing for Glauber dynamics. In this paper, we obtain the first improvement\nbeyond the $\\frac{11}{6}\\Delta$ barrier for general graphs by showing rapid\nmixing for $k > (\\frac{11}{6} - \\eta)\\Delta$ for some positive constant $\\eta$.\nThe key to our proof is combining path coupling with a new kind of metric that\nincorporates a count of the extremal configurations of the chain. Additionally,\nour results extend to list coloring, a widely studied generalization of\ncoloring. Combined, these results answer two open questions from Frieze and\nVigoda's 2007 survey paper on Glauber dynamics for colorings. \n\n"}
{"id": "1804.04739", "contents": "Title: Efficient algorithms for tensor scaling, quantum marginals and moment\n  polytopes Abstract: We present a polynomial time algorithm to approximately scale tensors of any\nformat to arbitrary prescribed marginals (whenever possible). This unifies and\ngeneralizes a sequence of past works on matrix, operator and tensor scaling.\nOur algorithm provides an efficient weak membership oracle for the associated\nmoment polytopes, an important family of implicitly-defined convex polytopes\nwith exponentially many facets and a wide range of applications. These include\nthe entanglement polytopes from quantum information theory (in particular, we\nobtain an efficient solution to the notorious one-body quantum marginal\nproblem) and the Kronecker polytopes from representation theory (which capture\nthe asymptotic support of Kronecker coefficients). Our algorithm can be applied\nto succinct descriptions of the input tensor whenever the marginals can be\nefficiently computed, as in the important case of matrix product states or\ntensor-train decompositions, widely used in computational physics and numerical\nmathematics.\n  We strengthen and generalize the alternating minimization approach of\nprevious papers by introducing the theory of highest weight vectors from\nrepresentation theory into the numerical optimization framework. We show that\nhighest weight vectors are natural potential functions for scaling algorithms\nand prove new bounds on their evaluations to obtain polynomial-time\nconvergence. Our techniques are general and we believe that they will be\ninstrumental to obtain efficient algorithms for moment polytopes beyond the\nones consider here, and more broadly, for other optimization problems\npossessing natural symmetries. \n\n"}
{"id": "1804.05013", "contents": "Title: Connectivity in Random Annulus Graphs and the Geometric Block Model Abstract: We provide new connectivity results for {\\em vertex-random graphs} or {\\em\nrandom annulus graphs} which are significant generalizations of random\ngeometric graphs. Random geometric graphs (RGG) are one of the most basic\nmodels of random graphs for spatial networks proposed by Gilbert in 1961,\nshortly after the introduction of the Erd\\H{o}s-R\\'{en}yi random graphs. They\nresemble social networks in many ways (e.g. by spontaneously creating cluster\nof nodes with high modularity). The connectivity properties of RGG have been\nstudied since its introduction, and analyzing them has been significantly\nharder than their Erd\\H{o}s-R\\'{en}yi counterparts due to correlated edge\nformation.\n  Our next contribution is in using the connectivity of random annulus graphs\nto provide necessary and sufficient conditions for efficient recovery of\ncommunities for {\\em the geometric block model} (GBM). The GBM is a\nprobabilistic model for community detection defined over an RGG in a similar\nspirit as the popular {\\em stochastic block model}, which is defined over an\nErd\\H{o}s-R\\'{en}yi random graph. The geometric block model inherits the\ntransitivity properties of RGGs and thus models communities better than a\nstochastic block model. However, analyzing them requires fresh perspectives as\nall prior tools fail due to correlation in edge formation. We provide a simple\nand efficient algorithm that can recover communities in GBM exactly with high\nprobability in the regime of connectivity. \n\n"}
{"id": "1804.05097", "contents": "Title: Design and Implementation of Dynamic Memory Management in a Reversible\n  Object-Oriented Programming Language Abstract: The reversible object-oriented programming language (ROOPL) was presented in\nlate 2016 and proved that object-oriented programming paradigms works in the\nreversible setting. The language featured simple statically scoped objects\nwhich made non-trivial programs tedious, if not impossible to write using the\nlimited tools provided. We introduce an extension to ROOPL in form the new\nlanguage ROOPL++, featuring dynamic memory management and fixed-sized arrays\nfor increased language expressiveness. The language is a superset of ROOPL and\nhas formally been defined by its language semantics, type system and\ncomputational universality. Considerations for reversible memory manager\nlayouts are discussed and ultimately lead to the selection of the Buddy Memory\nlayout. Translations of the extensions added in ROOPL++ to the reversible\nassembly language PISA are presented to provide garbage-free computations. The\ndynamic memory management extension successfully increases the expressiveness\nof ROOPL and as a result, shows that non-trivial reversible data structures,\nsuch as binary trees and doubly-linked lists, are feasible and do not\ncontradict the reversible computing paradigm. \n\n"}
{"id": "1804.05230", "contents": "Title: The threshold for SDP-refutation of random regular NAE-3SAT Abstract: Unlike its cousin 3SAT, the NAE-3SAT (not-all-equal-3SAT) problem has the\nproperty that spectral/SDP algorithms can efficiently refute random instances\nwhen the constraint density is a large constant (with high probability). But do\nthese methods work immediately above the \"satisfiability threshold\", or is\nthere still a range of constraint densities for which random NAE-3SAT instances\nare unsatisfiable but hard to refute?\n  We show that the latter situation prevails, at least in the context of random\nregular instances and SDP-based refutation. More precisely, whereas a random\n$d$-regular instance of NAE-3SAT is easily shown to be unsatisfiable (whp) once\n$d \\geq 8$, we establish the following sharp threshold result regarding\nefficient refutation: If $d < 13.5$ then the basic SDP, even augmented with\ntriangle inequalities, fails to refute satisfiability (whp), if $d > 13.5$ then\neven the most basic spectral algorithm refutes satisfiability~(whp). \n\n"}
{"id": "1804.05345", "contents": "Title: Data-Dependent Coresets for Compressing Neural Networks with\n  Applications to Generalization Bounds Abstract: We present an efficient coresets-based neural network compression algorithm\nthat sparsifies the parameters of a trained fully-connected neural network in a\nmanner that provably approximates the network's output. Our approach is based\non an importance sampling scheme that judiciously defines a sampling\ndistribution over the neural network parameters, and as a result, retains\nparameters of high importance while discarding redundant ones. We leverage a\nnovel, empirical notion of sensitivity and extend traditional coreset\nconstructions to the application of compressing parameters. Our theoretical\nanalysis establishes guarantees on the size and accuracy of the resulting\ncompressed network and gives rise to generalization bounds that may provide new\ninsights into the generalization properties of neural networks. We demonstrate\nthe practical effectiveness of our algorithm on a variety of neural network\nconfigurations and real-world data sets. \n\n"}
{"id": "1804.05436", "contents": "Title: Hidden Hamiltonian Cycle Recovery via Linear Programming Abstract: We introduce the problem of hidden Hamiltonian cycle recovery, where there is\nan unknown Hamiltonian cycle in an $n$-vertex complete graph that needs to be\ninferred from noisy edge measurements. The measurements are independent and\ndistributed according to $\\calP_n$ for edges in the cycle and $\\calQ_n$\notherwise. This formulation is motivated by a problem in genome assembly, where\nthe goal is to order a set of contigs (genome subsequences) according to their\npositions on the genome using long-range linking measurements between the\ncontigs. Computing the maximum likelihood estimate in this model reduces to a\nTraveling Salesman Problem (TSP). Despite the NP-hardness of TSP, we show that\na simple linear programming (LP) relaxation, namely the fractional $2$-factor\n(F2F) LP, recovers the hidden Hamiltonian cycle with high probability as $n \\to\n\\infty$ provided that $\\alpha_n - \\log n \\to \\infty$, where $\\alpha_n\n\\triangleq -2 \\log \\int \\sqrt{d P_n d Q_n}$ is the R\\'enyi divergence of order\n$\\frac{1}{2}$. This condition is information-theoretically optimal in the sense\nthat, under mild distributional assumptions, $\\alpha_n \\geq (1+o(1)) \\log n$ is\nnecessary for any algorithm to succeed regardless of the computational cost.\n  Departing from the usual proof techniques based on dual witness construction,\nthe analysis relies on the combinatorial characterization (in particular, the\nhalf-integrality) of the extreme points of the F2F polytope. Represented as\nbicolored multi-graphs, these extreme points are further decomposed into\nsimpler \"blossom-type\" structures for the large deviation analysis and counting\narguments. Evaluation of the algorithm on real data shows improvements over\nexisting approaches. \n\n"}
{"id": "1804.08731", "contents": "Title: Longest Common Substring Made Fully Dynamic Abstract: In the longest common substring (LCS) problem, we are given two strings $S$\nand $T$, each of length at most $n$, and we are asked to find a longest string\noccurring as a fragment of both $S$ and $T$. This is a classical and\nwell-studied problem in computer science with a known $\\mathcal{O}(n)$-time\nsolution. In the fully dynamic version of the problem, edit operations are\nallowed in either of the two strings, and we are asked to report an LCS after\neach such operation. We present the first solution to this problem that\nrequires sublinear time per edit operation. In particular, we show how to\nreturn an LCS in $\\tilde{\\mathcal{O}}(n^{2/3})$ time (or\n$\\tilde{\\mathcal{O}}(\\sqrt{n})$ time if edits are allowed in only one of the\ntwo strings) after each operation using $\\tilde{\\mathcal{O}}(n)$ space.\n  This line of research was recently initiated by the authors [SPIRE 2017] in a\nsomewhat restricted dynamic variant. An $\\tilde{\\mathcal{O}}(n)$-sized data\nstructure that returns an LCS of the two strings after a single edit operation\n(that is reverted afterwards) in $\\tilde{\\mathcal{O}}(1)$ time was presented.\nAt CPM 2018, three papers studied analogously restricted dynamic variants of\nproblems on strings. We show that our techniques can be used to obtain fully\ndynamic algorithms for several classical problems on strings, namely, computing\nthe longest repeat, the longest palindrome and the longest Lyndon substring of\na string. The only previously known sublinear-time dynamic algorithms for\nproblems on strings were obtained for maintaining a dynamic collection of\nstrings for comparison queries and for pattern matching with the most recent\nadvances made by Gawrychowski et al. [SODA 2018] and by Clifford et al. [STACS\n2018]. \n\n"}
{"id": "1804.10173", "contents": "Title: Efficient and adaptive parameterized algorithms on modular\n  decompositions Abstract: We study the influence of a graph parameter called modular-width on the time\ncomplexity for optimally solving well-known polynomial problems such as Maximum\nMatching, Triangle Counting, and Maximum $s$-$t$ Vertex-Capacitated Flow. The\nmodular-width of a graph depends on its (unique) modular decomposition tree,\nand can be computed in linear time $O(n+m)$ for graphs with $n$ vertices and\n$m$ edges. Modular decompositions are an important tool for graph algorithms,\ne.g., for linear-time recognition of certain graph classes. Throughout, we\nobtain efficient parameterized algorithms of running times $O(f(mw)n+m)$,\n$O(n+f(mw)m)$ , or $O(f(mw)+n+m)$ for graphs of modular-width $mw$. Our\nalgorithm for Maximum Matching, running in time $O(mw^2\\log mw \\cdot n+m)$, is\nboth faster and simpler than the recent $O(mw^4n+m)$ time algorithm of Coudert\net al. (SODA 2018). For several other problems, e.g., Triangle Counting and\nMaximum $b$-Matching, we give adaptive algorithms, meaning that their running\ntimes match the best unparameterized algorithms for worst-case modular-width of\n$mw=\\Theta(n)$ and they outperform them already for $mw=o(n)$, until reaching\nlinear time for $mw=O(1)$. \n\n"}
{"id": "1805.03253", "contents": "Title: Efficient Shortest Paths in Scale-Free Networks with Underlying\n  Hyperbolic Geometry Abstract: A common way to accelerate shortest path algorithms on graphs is the use of a\nbidirectional search, which simultaneously explores the graph from the start\nand the destination. It has been observed recently that this strategy performs\nparticularly well on scale-free real-world networks. Such networks typically\nhave a heterogeneous degree distribution (e.g., a power-law distribution) and\nhigh clustering (i.e., vertices with a common neighbor are likely to be\nconnected themselves). These two properties can be obtained by assuming an\nunderlying hyperbolic geometry.\n  To explain the observed behavior of the bidirectional search, we analyze its\nrunning time on hyperbolic random graphs and prove that it is $\\mathcal {\\tilde\nO}(n^{2 - 1/\\alpha} + n^{1/(2\\alpha)} + \\delta_{\\max})$ with high probability,\nwhere $\\alpha \\in (0.5, 1)$ controls the power-law exponent of the degree\ndistribution, and $\\delta_{\\max}$ is the maximum degree. This bound is\nsublinear, improving the obvious worst-case linear bound. Although our analysis\ndepends on the underlying geometry, the algorithm itself is oblivious to it. \n\n"}
{"id": "1805.03437", "contents": "Title: Exact Lexicographic Scheduling and Approximate Rescheduling Abstract: In industrial resource allocation problems, an initial planning stage may\nsolve a nominal problem instance and a subsequent recovery stage may intervene\nto repair inefficiencies and infeasibilities due to uncertainty, e.g.\\ machine\nfailures and job processing time variations. In this context, we investigate\nthe minimum makespan scheduling problem, a.k.a.\\ $P||C_{\\max}$, under\nuncertainty. We propose a two-stage robust scheduling approach where\nfirst-stage decisions are computed with exact lexicographic scheduling and\nsecond-stage decisions are derived using approximate rescheduling. We explore\nrecovery strategies accounting for planning decisions and constrained by\nlimited permitted deviations from the original schedule. Our approach is\nsubstantiated analytically, with a price of robustness characterization\nparameterized by the degree of uncertainty, and numerically. This analysis is\nbased on optimal substructure imposed by lexicographic optimality. Thus,\nlexicographic optimization enables more efficient rescheduling. Further, we\nrevisit state-of-the-art exact lexicographic optimization methods and propose a\nlexicographic branch-and-bound algorithm whose performance is validated\ncomputationally. \n\n"}
{"id": "1805.06836", "contents": "Title: Deleting edges to restrict the size of an epidemic in temporal networks Abstract: Spreading processes on graphs are a natural model for a wide variety of\nreal-world phenomena, including information spread over social networks and\nbiological diseases spreading over contact networks. Often, the networks over\nwhich these processes spread are dynamic in nature, and can be modeled with\ntemporal graphs. Here, we study the problem of deleting edges from a given\ntemporal graph in order to reduce the number of vertices (temporally) reachable\nfrom a given starting point. This could be used to control the spread of a\ndisease, rumour, etc. in a temporal graph. In particular, our aim is to find a\ntemporal subgraph in which a process starting at any single vertex can be\ntransferred to only a limited number of other vertices using a\ntemporally-feasible path. We introduce a natural edge-deletion problem for\ntemporal graphs and provide positive and negative results on its computational\ncomplexity and approximability. \n\n"}
{"id": "1805.10086", "contents": "Title: On some tractable and hard instances for partial incentives and target\n  set selection Abstract: A widely studied model for influence diffusion in social networks are {\\it\ntarget sets}. For a graph $G$ and an integer-valued threshold function $\\tau$\non its vertex set, a {\\it target set} or {\\it dynamic monopoly} is a set of\nvertices of $G$ such that iteratively adding to it vertices $u$ of $G$ that\nhave at least $\\tau(u)$ neighbors in it eventually yields the entire vertex set\nof $G$. This notion is limited to the binary choice of including a vertex in\nthe target set or not, and Cordasco et al.~proposed {\\it partial incentives} as\na variant allowing for intermediate choices.\n  We show that finding optimal partial incentives is hard for chordal graphs\nand planar graphs but tractable for graphs of bounded treewidth and for\ninterval graphs with bounded thresholds. We also contribute some new results\nabout target set seletion on planar graphs by showing the hardness of this\nproblem, and by describing an efficient $O(\\sqrt{n})$-approximation algorithm\nas well as a PTAS for the dual problem of finding a maximum degenerate set. \n\n"}
{"id": "1805.10330", "contents": "Title: Size-varying reversible causal graph dynamics Abstract: Consider a network that evolves according to a reversible, nearest neighbours\ndynamics. Is the dynamics allowed to vary the size of the network? On the one\nhand it seems that, being the principal carriers of information, nodes cannot\nbe destroyed without jeopardising bijectivity. On the other hand, there are\nplenty of bijective functions from the set of graphs to the set of graphs that\nare non-vertex-preserving. The question has been settled negatively -- for\nthree different reasons. Yet, in this paper we do obtain reversible local node\ncreation/destruction -- in three relaxed settings, whose equivalence we prove\nfor robustness. We motivate our work both by theoretical computer science\nconsiderations (reversible computing, cellular automata extensions) and\ntheoretical physics concerns (basic formalisms towards discrete quantum\ngravity). \n\n"}
{"id": "1805.10885", "contents": "Title: High Probability Frequency Moment Sketches Abstract: We consider the problem of sketching the $p$-th frequency moment of a vector,\n$p>2$, with multiplicative error at most $1\\pm \\epsilon$ and \\emph{with high\nconfidence} $1-\\delta$. Despite the long sequence of work on this problem,\ntight bounds on this quantity are only known for constant $\\delta$. While one\ncan obtain an upper bound with error probability $\\delta$ by repeating a\nsketching algorithm with constant error probability $O(\\log(1/\\delta))$ times\nin parallel, and taking the median of the outputs, we show this is a suboptimal\nalgorithm! Namely, we show optimal upper and lower bounds of $\\Theta(n^{1-2/p}\n\\log(1/\\delta) + n^{1-2/p} \\log^{2/p} (1/\\delta) \\log n)$ on the sketching\ndimension, for any constant approximation. Our result should be contrasted with\nresults for estimating frequency moments for $1 \\leq p \\leq 2$, for which we\nshow the optimal algorithm for general $\\delta$ is obtained by repeating the\noptimal algorithm for constant error probability $O(\\log(1/\\delta))$ times and\ntaking the median output. We also obtain a matching lower bound for this\nproblem, up to constant factors. \n\n"}
{"id": "1805.12238", "contents": "Title: High-Quality Disjoint and Overlapping Community Structure in Large-Scale\n  Complex Networks Abstract: In this paper, we propose an improved version of an agglomerative\nhierarchical clustering algorithm that performs disjoint community detection in\nlarge-scale complex networks. The improved algorithm is achieved after\nreplacing the local structural similarity used in the original algorithm, with\nthe recently proposed Dynamic Structural Similarity. Additionally, the improved\nalgorithm is extended to detect fuzzy and crisp overlapping community\nstructure. The extended algorithm leverages the disjoint community structure\ngenerated by itself and the dynamic structural similarity measures, to compute\na proposed membership probability function that defines the fuzzy communities.\nMoreover, an experimental evaluation is performed on reference benchmark graphs\nin order to compare the proposed algorithms with the state-of-the-art. \n\n"}
{"id": "1806.00348", "contents": "Title: The Dusty Progenitor Star of the Type II Supernova 2017eaw Abstract: We present pre-explosion photometry of the likely progenitor star of the Type\nII supernova (SN II) 2017eaw in NGC 6946. We use a Hubble Space Telescope (HST)\nimage of SN 2017eaw to perform relative astrometry with HST and Spitzer Space\nTelescope (Spitzer) imaging, finding a single point source consistent with its\nposition. We detect the progenitor star in $>$40 epochs of HST and Spitzer\nimaging covering 12.9 years to 43 days before discovery. While the progenitor\nluminosity was roughly constant for most of this period, there was a $\\sim$20%\nincrease in its $4.5~\\mu$m luminosity over the final 3 years before explosion.\nWe interpret the bright mid-infrared emission as a signature of circumstellar\ndust around the progenitor system. Using the pre-explosion photometry and\nassuming some circumstellar dust, we find the progenitor is most likely a red\nsupergiant with $\\log(L/L_{\\odot}) = 4.9$ and $T = 3350$ K, obscured by a\n$>2\\times10^{-5}~M_{\\odot}$ dust shell with $R = 4000~R_{\\odot}$ and $T = 960$\nK. Comparing to single-star evolutionary tracks, we find that the progenitor\nstar had an initial mass of $13~M_{\\odot}$ and a mass-loss rate of\n$2\\times10^{-7}~M_{\\odot}~\\text{yr}^{-1}$, consistent with the population of SN\nII progenitor stars. \n\n"}
{"id": "1806.00534", "contents": "Title: Provably convergent acceleration in factored gradient descent with\n  applications in matrix sensing Abstract: We present theoretical results on the convergence of \\emph{non-convex}\naccelerated gradient descent in matrix factorization models with $\\ell_2$-norm\nloss. The purpose of this work is to study the effects of acceleration in\nnon-convex settings, where provable convergence with acceleration should not be\nconsidered a \\emph{de facto} property. The technique is applied to matrix\nsensing problems, for the estimation of a rank $r$ optimal solution $X^\\star\n\\in \\mathbb{R}^{n \\times n}$. Our contributions can be summarized as follows.\n$i)$ We show that acceleration in factored gradient descent converges at a\nlinear rate; this fact is novel for non-convex matrix factorization settings,\nunder common assumptions. $ii)$ Our proof technique requires the acceleration\nparameter to be carefully selected, based on the properties of the problem,\nsuch as the condition number of $X^\\star$ and the condition number of objective\nfunction. $iii)$ Currently, our proof leads to the same dependence on the\ncondition number(s) in the contraction parameter, similar to recent results on\nnon-accelerated algorithms. $iv)$ Acceleration is observed in practice, both in\nsynthetic examples and in two real applications: neuronal multi-unit activities\nrecovery from single electrode recordings, and quantum state tomography on\nquantum computing simulators. \n\n"}
{"id": "1806.01140", "contents": "Title: Percolation of Lipschitz surface and tight bounds on the spread of\n  information among mobile agents Abstract: We consider the problem of spread of information among mobile agents on the\ntorus. The agents are initially distributed as a Poisson point process on the\ntorus, and move as independent simple random walks. Two agents can share\ninformation whenever they are at the same vertex of the torus. We study the\nso-called flooding time: the amount of time it takes for information to be\nknown by all agents. We establish a tight upper bound on the flooding time, and\nintroduce a technique which we believe can be applicable to analyze other\nprocesses involving mobile agents. \n\n"}
{"id": "1806.01305", "contents": "Title: Towards the Practical Application of Near-Term Quantum Computers in\n  Quantum Chemistry Simulations: A Problem Decomposition Approach Abstract: With the aim of establishing a framework to efficiently perform the practical\napplication of quantum chemistry simulation on near-term quantum devices, we\nenvision a hybrid quantum--classical framework for leveraging problem\ndecomposition (PD) techniques in quantum chemistry. Specifically, we use PD\ntechniques to decompose a target molecular system into smaller subsystems\nrequiring fewer computational resources. In our framework, there are two levels\nof hybridization. At the first level, we use a classical algorithm to decompose\na target molecule into subsystems, and utilize a quantum algorithm to simulate\nthe quantum nature of the subsystems. The second level is in the quantum\nalgorithm. We consider the quantum--classical variational algorithm that\niterates between an expectation estimation using a quantum device and a\nparameter optimization using a classical device. We investigate three popular\nPD techniques for our hybrid approach: the fragment molecular-orbital (FMO)\nmethod, the divide-and-conquer (DC) technique, and the density matrix embedding\ntheory (DMET). We examine the efficacy of these techniques in correctly\ndifferentiating conformations of simple alkane molecules. In particular, we\nconsider the ratio between the number of qubits for PD and that of the full\nsystem; the mean absolute deviation; and the Pearson correlation coefficient\nand Spearman's rank correlation coefficient. Sampling error is introduced when\nexpectation values are measured on the quantum device. Therefore, we study how\nthis error affects the predictive performance of PD techniques. The present\nstudy is our first step to opening up the possibility of using quantum\nchemistry simulations at a scale close to the size of molecules relevant to\nindustry on near-term quantum hardware. \n\n"}
{"id": "1806.01848", "contents": "Title: X-ray Variability from the Ultraluminous Black Hole Candidate X-ray\n  Binary in the Globular Cluster RZ 2109 Abstract: We present the results of long-term monitoring of the X-ray emission from the\nultraluminous X-ray source XMMUJ122939.9+075333 in the extragalactic globular\ncluster RZ2109. The combination of the high X-ray luminosity, short term X-ray\nvariability, X-ray spectrum, and optical emission suggest that this system is\nlikely an accreting black hole in a globular cluster. To study the long-term\nbehavior of the X-ray emission from this source, we analyze both new and\narchival Chandra and XMM-Newton observations, covering 16 years from 2000 to\n2016. For all of these observations, we fit extracted spectra of RZ2109 with\nxspec models. The spectra are all dominated by a soft component, which is very\nsoft with typical fit temperatures of T $\\simeq$ 0.15 keV. The resulting X-ray\nfluxes show strong variability on short and long timescales. We also find that\nthe X-ray spectrum often shows no significant change even with luminosity\nchanges as large as a factor of five. \n\n"}
{"id": "1806.04163", "contents": "Title: Urca reactions during neutron star inspiral Abstract: We study the impact of nonlinear bulk viscosity due to Urca reactions driven\nby tidally-induced fluid motion during binary neutron star inspiral. Fluid\ncompression is computed for low radial order oscillation modes through an\nadiabatic, time-dependent solution of the mode amplitudes. Optically thin\nneutrino emission and heating rates are then computed from this adiabatic fluid\nmotion. Calculations use direct and modified Urca reactions operating in a\n$M=1.4\\, M_\\odot$ neutron star, which is constructed using the Skyrme Rs\nequation of state. We find that the energy pumped into low order oscillation\nmodes is not efficiently thermalized even by direct Urca reactions, with core\ntemperatures reaching only $T \\simeq 10^8\\, {\\rm K}$ during the inspiral.\nAlthough this is an order of magnitude larger than the heating due to shear\nviscosity considered by previous studies, it reinforces the result that the\nstars are quite cold at merger. Upon excitation of the lowest order g-mode, the\nchemical potential imbalance reaches $\\beta > 1\\, {\\rm MeV}$ at orbital\nfrequencies $\\nu_{\\rm orb} > 200\\, {\\rm Hz}$, implying significant\ncharged-current optical depths and Fermi blocking. To asses the importance of\nneutrino degeneracy effects, the neutrino transfer equation is solved in the\nstatic approximation for the three-dimensional density distribution, and the\nreaction rates are then computed including Fermi-blocking. We find that the\nheating rate is suppressed by factors of a few for $\\nu_{\\rm orb} > 200\\, {\\rm\nHz}$. The spectrum of emitted $\\nu_e$ and $\\bar{\\nu}_e$, including radiation\ntransfer effects, is presented for a range of orbital separations. \n\n"}
{"id": "1806.04310", "contents": "Title: MISSION: Ultra Large-Scale Feature Selection using Count-Sketches Abstract: Feature selection is an important challenge in machine learning. It plays a\ncrucial role in the explainability of machine-driven decisions that are rapidly\npermeating throughout modern society. Unfortunately, the explosion in the size\nand dimensionality of real-world datasets poses a severe challenge to standard\nfeature selection algorithms. Today, it is not uncommon for datasets to have\nbillions of dimensions. At such scale, even storing the feature vector is\nimpossible, causing most existing feature selection methods to fail.\nWorkarounds like feature hashing, a standard approach to large-scale machine\nlearning, helps with the computational feasibility, but at the cost of losing\nthe interpretability of features. In this paper, we present MISSION, a novel\nframework for ultra large-scale feature selection that performs stochastic\ngradient descent while maintaining an efficient representation of the features\nin memory using a Count-Sketch data structure. MISSION retains the simplicity\nof feature hashing without sacrificing the interpretability of the features\nwhile using only O(log^2(p)) working memory. We demonstrate that MISSION\naccurately and efficiently performs feature selection on real-world,\nlarge-scale datasets with billions of dimensions. \n\n"}
{"id": "1806.04478", "contents": "Title: On the $t$-adic Littlewood Conjecture Abstract: The $p$-adic Littlewood Conjecture due to De Mathan and Teuli\\'e asserts that\nfor any prime number $p$ and any real number $\\alpha$, the equation\n$$\\inf_{|m|\\ge 1} |m|\\cdot |m|_p\\cdot |\\langle m\\alpha \\rangle|\\, =\\, 0 $$\nholds. Here, $|m|$ is the usual absolute value of the integer $m$, $|m|_p$ its\n$p$-adic absolute value and $ |\\langle x\\rangle|$ denotes the distance from a\nreal number $x$ to the set of integers. This still open conjecture stands as a\nvariant of the well-known Littlewood Conjecture. In the same way as the latter,\nit admits a natural counterpart over the field of formal Laurent series\n$\\mathbb{K}\\left(\\left(t^{-1}\\right)\\right)$ of a ground field $\\mathbb{K}$.\nThis is the so-called \\emph{$t$-adic Littlewood Conjecture} ($t$-LC).\n  It is known that $t$--LC fails when the ground field $\\mathbb{K}$ is\ninfinite. This article is concerned with the much more difficult case when the\nlatter field is finite. More precisely, a \\emph{fully explicit} counterexample\nis provided to show that $t$-LC does not hold in the case that $\\mathbb{K}$ is\na finite field with characteristic 3. Generalizations to fields with\ncharacteristics different from 3 are also discussed.\n  The proof is computer assisted. It reduces to showing that an infinite matrix\nencoding Hankel determinants of the Paper-Folding sequence over $\\mathbb{F}_3$,\nthe so-called Number Wall of this sequence, can be obtained as a\ntwo-dimensional automatic tiling satisfying a finite number of suitable local\nconstraints. \n\n"}
{"id": "1806.05797", "contents": "Title: Polyhedra Circuits and Their Applications Abstract: We introduce polyhedra circuits. Each polyhedra circuit characterizes a\ngeometric region in $\\mathbb{R}^d$. They can be applied to represent a rich\nclass of geometric objects, which include all polyhedra and the union of a\nfinite number of polyhedra. They can be used to approximate a large class of\n$d$-dimensional manifolds in $\\mathbb{R}^d$. Barvinok developed polynomial time\nalgorithms to compute the volume of a rational polyhedra, and to count the\nnumber of lattice points in a rational polyhedra in a fixed dimensional space\n$\\mathbb{R}^d$ with a fix $d$. Define $T_V(d,\\, n)$ be the polynomial time in\n$n$ to compute the volume of one rational polyhedra, $T_L(d,\\, n)$ be the\npolynomial time in $n$ to count the number of lattice points in one rational\npolyhedra with $d$ be a fixed dimensional number, $T_I(d,\\, n)$ be the\npolynomial time in $n$ to solve integer linear programming time with $d$ be the\nfixed dimensional number, where $n$ is the total number of linear inequalities\nfrom input polyhedra. We develop algorithms to count the number of lattice\npoints in the geometric region determined by a polyhedra circuit in\n$O\\left(nd\\cdot r_d(n)\\cdot T_V(d,\\, n)\\right)$ time and to compute the volume\nof the geometric region determined by a polyhedra circuit in $O\\left(n\\cdot\nr_d(n)\\cdot T_I(d,\\, n)+r_d(n)T_L(d,\\, n)\\right)$ time, where $n$ is the number\nof input linear inequalities, $d$ is number of variables and $r_d(n)$ be the\nmaximal number of regions that $n$ linear inequalities with $d$ variables\npartition $\\mathbb{R}^d$. \n\n"}
{"id": "1806.06429", "contents": "Title: On Sketching the $q$ to $p$ norms Abstract: We initiate the study of data dimensionality reduction, or sketching, for the\n$q\\to p$ norms. Given an $n \\times d$ matrix $A$, the $q\\to p$ norm, denoted\n$\\|A\\|_{q \\to p} = \\sup_{x \\in \\mathbb{R}^d \\backslash \\vec{0}}\n\\frac{\\|Ax\\|_p}{\\|x\\|_q}$, is a natural generalization of several matrix and\nvector norms studied in the data stream and sketching models, with applications\nto datamining, hardness of approximation, and oblivious routing. We say a\ndistribution $S$ on random matrices $L \\in \\mathbb{R}^{nd} \\rightarrow\n\\mathbb{R}^k$ is a $(k,\\alpha)$-sketching family if from $L(A)$, one can\napproximate $\\|A\\|_{q \\to p}$ up to a factor $\\alpha$ with constant\nprobability. We provide upper and lower bounds on the sketching dimension $k$\nfor every $p, q \\in [1, \\infty]$, and in a number of cases our bounds are\ntight. While we mostly focus on constant $\\alpha$, we also consider large\napproximation factors $\\alpha$, as well as other variants of the problem such\nas when $A$ has low rank. \n\n"}
{"id": "1806.07107", "contents": "Title: Nivat's Conjecture and Pattern Complexity in Algebraic Subshifts Abstract: We study Nivat's conjecture on algebraic subshifts and prove that in some of\nthem every low complexity configuration is periodic. This is the case in the\nLedrappier subshift (the 3-dot system) and, more generally, in all\ntwo-dimensional algebraic subshifts over $\\mathbb{F}_p$ defined by a polynomial\nwithout line polynomial factors in more than one direction. We also find an\nalgebraic subshift that is defined by a product of two line polynomials that\nhas this property (the 4-dot system) and another one that does not. \n\n"}
{"id": "1806.08664", "contents": "Title: Acyclicity in finite groups and groupoids Abstract: We expound a concise construction of finite groups and groupoids whose Cayley\ngraphs satisfy graded acyclicity requirements. Our acyclicity criteria concern\ncyclic patterns formed by coset-like configurations w.r.t. subsets of the\ngenerator set rather than just by individual generators. The proposed\nconstructions correspondingly yield finite groups and groupoids whose Cayley\ngraphs satisfy much stronger acyclicity conditions than large girth. We thus\nobtain generic and canonical constructions of highly homogeneous graph\nstructures with strong acyclicity properties, which support known applications\nin finite graph and hypergraph coverings that locally unfold cyclic\nconfigurations. \n\n"}
{"id": "1806.09189", "contents": "Title: On Nondeterministic Derandomization of Freivalds' Algorithm:\n  Consequences, Avenues and Algorithmic Progress Abstract: Motivated by studying the power of randomness, certifying algorithms and\nbarriers for fine-grained reductions, we investigate the question whether the\nmultiplication of two $n\\times n$ matrices can be performed in near-optimal\nnondeterministic time $\\tilde{O}(n^2)$. Since a classic algorithm due to\nFreivalds verifies correctness of matrix products probabilistically in time\n$O(n^2)$, our question is a relaxation of the open problem of derandomizing\nFreivalds' algorithm.\n  We discuss consequences of a positive or negative resolution of this problem\nand provide potential avenues towards resolving it. Particularly, we show that\nsufficiently fast deterministic verifiers for 3SUM or univariate polynomial\nidentity testing yield faster deterministic verifiers for matrix\nmultiplication. Furthermore, we present the partial algorithmic progress that\ndistinguishing whether an integer matrix product is correct or contains between\n1 and $n$ erroneous entries can be performed in time $\\tilde{O}(n^2)$ --\ninterestingly, the difficult case of deterministic matrix product verification\nis not a problem of \"finding a needle in the haystack\", but rather cancellation\neffects in the presence of many errors.\n  Our main technical contribution is a deterministic algorithm that corrects an\ninteger matrix product containing at most $t$ errors in time\n$\\tilde{O}(\\sqrt{t} n^2 + t^2)$. To obtain this result, we show how to compute\nan integer matrix product with at most $t$ nonzeroes in the same running time.\nThis improves upon known deterministic output-sensitive integer matrix\nmultiplication algorithms for $t = \\Omega(n^{2/3})$ nonzeroes, which is of\nindependent interest. \n\n"}
{"id": "1806.09540", "contents": "Title: Parameterized algorithms and data reduction for the short secluded\n  $s$-$t$-path problem Abstract: Given a graph $G=(V,E)$, two vertices $s,t\\in V$, and two integers $k,\\ell$,\nthe Short Secluded Path problem is to find a simple $s$-$t$-path with at most\n$k$ vertices and $\\ell$ neighbors. We study the parameterized complexity of the\nproblem with respect to four structural graph parameters: the vertex cover\nnumber, treewidth, feedback vertex number, and feedback edge number. In\nparticular, we completely settle the question of the existence of problem\nkernels with size polynomial in these parameters and their combinations with\n$k$ and $\\ell$. We also obtain a $2^{O(w)}\\cdot \\ell^2\\cdot n$-time algorithm\nfor graphs of treewidth $w$, which yields subexponential-time algorithms in\nseveral graph classes. \n\n"}
{"id": "1806.11413", "contents": "Title: (k,p)-Planarity: A Relaxation of Hybrid Planarity Abstract: We present a new model for hybrid planarity that relaxes existing hybrid\nrepresentations. A graph $G = (V,E)$ is $(k,p)$-planar if $V$ can be\npartitioned into clusters of size at most $k$ such that $G$ admits a drawing\nwhere: (i) each cluster is associated with a closed, bounded planar region,\ncalled a cluster region; (ii) cluster regions are pairwise disjoint, (iii) each\nvertex $v \\in V$ is identified with at most $p$ distinct points, called\n\\emph{ports}, on the boundary of its cluster region; (iv) each inter-cluster\nedge $(u,v) \\in E$ is identified with a Jordan arc connecting a port of $u$ to\na port of $v$; (v) inter-cluster edges do not cross or intersect cluster\nregions except at their endpoints. We first tightly bound the number of edges\nin a $(k,p)$-planar graph with $p<k$. We then prove that $(4,1)$-planarity\ntesting and $(2,2)$-planarity testing are NP-complete problems. Finally, we\nprove that neither the class of $(2,2)$-planar graphs nor the class of\n$1$-planar graphs contains the other, indicating that the $(k,p)$-planar graphs\nare a large and novel class. \n\n"}
{"id": "1806.11542", "contents": "Title: High Dimensional Discrete Integration over the Hypergrid Abstract: Recently Ermon et al. (2013) pioneered a way to practically compute\napproximations to large scale counting or discrete integration problems by\nusing random hashes. The hashes are used to reduce the counting problem into\nmany separate discrete optimization problems. The optimization problems then\ncan be solved by an NP-oracle such as commercial SAT solvers or integer linear\nprogramming (ILP) solvers. In particular, Ermon et al. showed that if the\ndomain of integration is $\\{0,1\\}^n$ then it is possible to obtain a solution\nwithin a factor of $16$ of the optimal (a 16-approximation) by this technique.\n  In many crucial counting tasks, such as computation of partition function of\nferromagnetic Potts model, the domain of integration is naturally $\\{0,1,\\dots,\nq-1\\}^n, q>2$, the hypergrid. The straightforward extension of Ermon et al.'s\nmethod allows a $q^2$-approximation for this problem. For large values of $q$,\nthis is undesirable. In this paper, we show an improved technique to obtain an\napproximation factor of $4+O(1/q^2)$ to this problem. We are able to achieve\nthis by using an idea of optimization over multiple bins of the hash functions,\nthat can be easily implemented by inequality constraints, or even in\nunconstrained way. Also the burden on the NP-oracle is not increased by our\nmethod (an ILP solver can still be used). We provide experimental simulation\nresults to support the theoretical guarantees of our algorithms. \n\n"}
{"id": "1806.11548", "contents": "Title: Algorithmic Pirogov-Sinai theory Abstract: We develop an efficient algorithmic approach for approximate counting and\nsampling in the low-temperature regime of a broad class of statistical physics\nmodels on finite subsets of the lattice $\\mathbb Z^d$ and on the torus\n$(\\mathbb Z/n \\mathbb Z)^d$. Our approach is based on combining contour\nrepresentations from Pirogov-Sinai theory with Barvinok's approach to\napproximate counting using truncated Taylor series. Some consequences of our\nmain results include an FPTAS for approximating the partition function of the\nhard-core model at sufficiently high fugacity on subsets of $\\mathbb Z^d$ with\nappropriate boundary conditions and an efficient sampling algorithm for the\nferromagnetic Potts model on the discrete torus $(\\mathbb Z/n \\mathbb Z)^d$ at\nsufficiently low temperature. \n\n"}
{"id": "1807.01680", "contents": "Title: Tight bounds for popping algorithms Abstract: We sharpen run-time analysis for algorithms under the partial rejection\nsampling framework. Our method yields improved bounds for: the cluster-popping\nalgorithm for approximating all-terminal network reliability; the cycle-popping\nalgorithm for sampling rooted spanning trees; the sink-popping algorithm for\nsampling sink-free orientations. In all three applications, our bounds are not\nonly tight in order, but also optimal in constants. \n\n"}
{"id": "1807.04766", "contents": "Title: Polarization of the first-hour macronovae Abstract: Macronovae (or kilonovae) are the optical and NIR counterparts of binary\nneutron star mergers. While the macronova in GW170817 was detected about 10\nhours after the GW detection, future observations can possibly detect them\nwithin the first hour after the merger. Early- time macronovae are potentially\npowered by some mechanisms such as the beta-decay heating of the surviving free\nneutrons. In this paper, we propose that the polarimetric observation can be a\nuseful tool to study the early macronova emissions. If free nucleons remain in\nthe outermost layer of the ejecta, the electron scattering produces a larger\npolarization than that by the r-process element-rich ejecta. The degree of\npolarization can show a large value of $\\sim3\\%$ for the first $0.3-1$ hour for\nthe free nucleon mass of $10^{-5}-10^{-4}\\,M_{\\odot}$. Quick polarimetric\nobservations enable us to study not only the aspherical morphology of the\nejecta but also the amount of the free nucleons in the ejecta, which is helpful\nto discriminate the emission mechanisms of the early macronovae. \n\n"}
{"id": "1807.04936", "contents": "Title: Non-Gaussian Component Analysis using Entropy Methods Abstract: Non-Gaussian component analysis (NGCA) is a problem in multidimensional data\nanalysis which, since its formulation in 2006, has attracted considerable\nattention in statistics and machine learning. In this problem, we have a random\nvariable $X$ in $n$-dimensional Euclidean space. There is an unknown subspace\n$\\Gamma$ of the $n$-dimensional Euclidean space such that the orthogonal\nprojection of $X$ onto $\\Gamma$ is standard multidimensional Gaussian and the\northogonal projection of $X$ onto $\\Gamma^{\\perp}$, the orthogonal complement\nof $\\Gamma$, is non-Gaussian, in the sense that all its one-dimensional\nmarginals are different from the Gaussian in a certain metric defined in terms\nof moments. The NGCA problem is to approximate the non-Gaussian subspace\n$\\Gamma^{\\perp}$ given samples of $X$.\n  Vectors in $\\Gamma^{\\perp}$ correspond to `interesting' directions, whereas\nvectors in $\\Gamma$ correspond to the directions where data is very noisy. The\nmost interesting applications of the NGCA model is for the case when the\nmagnitude of the noise is comparable to that of the true signal, a setting in\nwhich traditional noise reduction techniques such as PCA don't apply directly.\nNGCA is also related to dimension reduction and to other data analysis problems\nsuch as ICA. NGCA-like problems have been studied in statistics for a long time\nusing techniques such as projection pursuit.\n  We give an algorithm that takes polynomial time in the dimension $n$ and has\nan inverse polynomial dependence on the error parameter measuring the angle\ndistance between the non-Gaussian subspace and the subspace output by the\nalgorithm. Our algorithm is based on relative entropy as the contrast function\nand fits under the projection pursuit framework. The techniques we develop for\nanalyzing our algorithm maybe of use for other related problems. \n\n"}
{"id": "1807.06101", "contents": "Title: A PTAS for $\\ell_p$-Low Rank Approximation Abstract: A number of recent works have studied algorithms for entrywise $\\ell_p$-low\nrank approximation, namely, algorithms which given an $n \\times d$ matrix $A$\n(with $n \\geq d$), output a rank-$k$ matrix $B$ minimizing\n$\\|A-B\\|_p^p=\\sum_{i,j}|A_{i,j}-B_{i,j}|^p$ when $p > 0$; and\n$\\|A-B\\|_0=\\sum_{i,j}[A_{i,j}\\neq B_{i,j}]$ for $p=0$.\n  On the algorithmic side, for $p \\in (0,2)$, we give the first\n$(1+\\epsilon)$-approximation algorithm running in time\n$n^{\\text{poly}(k/\\epsilon)}$. Further, for $p = 0$, we give the first\nalmost-linear time approximation scheme for what we call the Generalized Binary\n$\\ell_0$-Rank-$k$ problem. Our algorithm computes $(1+\\epsilon)$-approximation\nin time $(1/\\epsilon)^{2^{O(k)}/\\epsilon^{2}} \\cdot nd^{1+o(1)}$.\n  On the hardness of approximation side, for $p \\in (1,2)$, assuming the Small\nSet Expansion Hypothesis and the Exponential Time Hypothesis (ETH), we show\nthat there exists $\\delta := \\delta(\\alpha) > 0$ such that the entrywise\n$\\ell_p$-Rank-$k$ problem has no $\\alpha$-approximation algorithm running in\ntime $2^{k^{\\delta}}$. \n\n"}
{"id": "1807.06481", "contents": "Title: Dynamic Sampling from Graphical Models Abstract: In this paper, we study the problem of sampling from a graphical model when\nthe model itself is changing dynamically with time. This problem derives its\ninterest from a variety of inference, learning, and sampling settings in\nmachine learning, computer vision, statistical physics, and theoretical\ncomputer science. While the problem of sampling from a static graphical model\nhas received considerable attention, theoretical works for its dynamic variants\nhave been largely lacking. The main contribution of this paper is an algorithm\nthat can sample dynamically from a broad class of graphical models over\ndiscrete random variables. Our algorithm is parallel and Las Vegas: it knows\nwhen to stop and it outputs samples from the exact distribution. We also\nprovide sufficient conditions under which this algorithm runs in time\nproportional to the size of the update, on general graphical models as well as\nwell-studied specific spin systems. In particular we obtain, for the Ising\nmodel (ferromagnetic or anti-ferromagnetic) and for the hardcore model the\nfirst dynamic sampling algorithms that can handle both edge and vertex updates\n(addition, deletion, change of functions), both efficient within regimes that\nare close to the respective uniqueness regimes, beyond which, even for the\nstatic and approximate sampling, no local algorithms were known or the problem\nitself is intractable. Our dynamic sampling algorithm relies on a local\nresampling algorithm and a new \"equilibrium\" property that is shown to be\nsatisfied by our algorithm at each step, and enables us to prove its\ncorrectness. This equilibrium property is robust enough to guarantee the\ncorrectness of our algorithm, helps us improve bounds on fast convergence on\nspecific models, and should be of independent interest. \n\n"}
{"id": "1807.06577", "contents": "Title: Fisher zeros and correlation decay in the Ising model Abstract: We study the complex zeros of the partition function of the Ising model,\nviewed as a polynomial in the \"interaction parameter\"; these are known as\nFisher zeros in light of their introduction by Fisher in 1965. While the zeros\nof the partition function as a polynomial in the \"field\" parameter have been\nextensively studied since the classical work of Lee and Yang, comparatively\nlittle is known about Fisher zeros for general graphs. Our main result shows\nthat the zero-field Ising model has no Fisher zeros in a complex neighborhood\nof the entire region of parameters where the model exhibits correlation decay.\nIn addition to shedding light on Fisher zeros themselves, this result also\nestablishes a formal connection between two distinct notions of phase\ntransition for the Ising model: the absence of complex zeros (analyticity of\nthe free energy, or the logarithm of the partition function) and decay of\ncorrelations with distance. We also discuss the consequences of our result for\nefficient deterministic approximation of the partition function. Our proof\nrelies heavily on algorithmic techniques, notably Weitz's self-avoiding walk\ntree, and as such belongs to a growing body of work that uses algorithmic\nmethods to resolve classical questions in statistical physics. \n\n"}
{"id": "1807.06874", "contents": "Title: An Information-theoretic Framework for the Lossy Compression of Link\n  Streams Abstract: Graph compression is a data analysis technique that consists in the\nreplacement of parts of a graph by more general structural patterns in order to\nreduce its description length. It notably provides interesting exploration\ntools for the study of real, large-scale, and complex graphs which cannot be\ngrasped at first glance. This article proposes a framework for the compression\nof temporal graphs, that is for the compression of graphs that evolve with\ntime. This framework first builds on a simple and limited scheme, exploiting\nstructural equivalence for the lossless compression of static graphs, then\ngeneralises it to the lossy compression of link streams, a recent formalism for\nthe study of temporal graphs. Such generalisation relies on the natural\nextension of (bidimensional) relational data by the addition of a third\ntemporal dimension. Moreover, we introduce an information-theoretic measure to\nquantify and to control the information that is lost during compression, as\nwell as an algebraic characterisation of the space of possible compression\npatterns to enhance the expressiveness of the initial compression scheme. These\ncontributions lead to the definition of a combinatorial optimisation problem,\nthat is the Lossy Multistream Compression Problem, for which we provide an\nexact algorithm. \n\n"}
{"id": "1807.07067", "contents": "Title: A Fixed-Parameter Linear-Time Algorithm to Compute Principal Typings of\n  Planar Flow Networks Abstract: We present an alternative and simpler method for computing principal typings\nof flow networks. When limited to planar flow networks, the method can be made\nto run in fixed-parameter linear-time -- where the parameter not to be exceeded\nis what is called the edge-outerplanarity of the networks' underlying graphs. \n\n"}
{"id": "1807.07156", "contents": "Title: Approximation Schemes for Low-Rank Binary Matrix Approximation Problems Abstract: We provide a randomized linear time approximation scheme for a generic\nproblem about clustering of binary vectors subject to additional constrains.\nThe new constrained clustering problem encompasses a number of problems and by\nsolving it, we obtain the first linear time-approximation schemes for a number\nof well-studied fundamental problems concerning clustering of binary vectors\nand low-rank approximation of binary matrices. Among the problems solvable by\nour approach are \\textsc{Low GF(2)-Rank Approximation}, \\textsc{Low\nBoolean-Rank Approximation}, and various versions of \\textsc{Binary\nClustering}. For example, for \\textsc{Low GF(2)-Rank Approximation} problem,\nwhere for an $m\\times n$ binary matrix $A$ and integer $r>0$, we seek for a\nbinary matrix $B$ of $GF_2$ rank at most $r$ such that $\\ell_0$ norm of matrix\n$A-B$ is minimum, our algorithm, for any $\\epsilon>0$ in time $\nf(r,\\epsilon)\\cdot n\\cdot m$, where $f$ is some computable function, outputs a\n$(1+\\epsilon)$-approximate solution with probability at least\n$(1-\\frac{1}{e})$. Our approximation algorithms substantially improve the\nrunning times and approximation factors of previous works. We also give\n(deterministic) PTASes for these problems running in time\n$n^{f(r)\\frac{1}{\\epsilon^2}\\log \\frac{1}{\\epsilon}}$, where $f$ is some\nfunction depending on the problem. Our algorithm for the constrained clustering\nproblem is based on a novel sampling lemma, which is interesting in its own. \n\n"}
{"id": "1807.08678", "contents": "Title: Submodular Function Maximization in Parallel via the Multilinear\n  Relaxation Abstract: Balkanski and Singer [5] recently initiated the study of adaptivity (or\nparallelism) for constrained submodular function maximization, and studied the\nsetting of a cardinality constraint. Very recent improvements for this problem\nby Balkanski, Rubinstein, and Singer [6] and Ene and Nguyen [21] resulted in a\nnear-optimal $(1-1/e-\\epsilon)$-approximation in $O(\\log n/\\epsilon^2)$ rounds\nof adaptivity. Partly motivated by the goal of extending these results to more\ngeneral constraints, we describe parallel algorithms for approximately\nmaximizing the multilinear relaxation of a monotone submodular function subject\nto packing constraints. Formally our problem is to maximize $F(x)$ over $x \\in\n[0,1]^{n}$ subject to $Ax \\le 1$ where $F$ is the multilinear relaxation of a\nmonotone submodular function. Our algorithm achieves a near-optimal\n$(1-1/e-\\epsilon)$-approximation in $O(\\log^2 m \\log n/\\epsilon^4)$ rounds\nwhere $n$ is the cardinality of the ground set and $m$ is the number of packing\nconstraints. For many constraints of interest, the resulting fractional\nsolution can be rounded via known randomized rounding schemes that are\noblivious to the specific submodular function. We thus derive randomized\nalgorithms with poly-logarithmic adaptivity for a number of constraints\nincluding partition and laminar matroids, matchings, knapsack constraints, and\ntheir intersections. \n\n"}
{"id": "1807.11069", "contents": "Title: Variability and Optical Polarization Can Probe the Neutrino and\n  Electromagnetic Emission Mechanisms of TXS~0506+056 Abstract: The association of the high-energy neutrino event IceCube-170922A with the\nflaring blazar TXS~0506+056 indicates that hadronic processes may operate in a\nblazar jet. We perform semi-analytical spectral fitting of the multi-wavelength\nemission to obtain estimates of the jet physical parameters, and find that the\nmulti-wavelength emission can be explained by either a proton synchrotron\nscenario or an electron inverse Compton scattering scenario. In the proton\nsynchrotron scenario, a strong magnetic field of $10-100$~G is required,\nimplying that the particle acceleration is likely driven by magnetic energy\ndissipation such as magnetic reconnection events. The inverse Compton scenario\nimplies a magnetic field of $0.1-1$~G. Thus the particle acceleration is likely\ndriven by the kinetic energy dissipation such as shocks. We also discuss the\nneutrino production in the context of single-zone and multi-zone models based\non the above two scenarios. We demonstrate that the variability and optical\npolarization signatures can be used to distinguish the two scenarios due to\ntheir drastically different magnetic field. Specifically, the proton\nsynchrotron scenario may show orphan fast variability in the low-energy\nspectral component on top of the active state, with an optical polarization\ndegree $\\lesssim 10\\%$ throughout the active state. The inverse Compton\nscattering scenario instead predicts co-variability of the low- and high-energy\ncomponents on both short and long time scales, as well as a strongly variable\noptical polarization degree that can reach $\\gtrsim 20\\%$. Our results suggest\nthat optical polarization measurements and well-resolved multi-wavelength light\ncurves can be used to understand the electromagnetic and high-energy neutrino\nemissions by TXS~0506+056 and similar events in the future. \n\n"}
{"id": "1807.11869", "contents": "Title: On Exploring Temporal Graphs of Small Pathwidth Abstract: We show that the Temporal Graph Exploration Problem is NP-complete, even when\nthe underlying graph has pathwidth 2 and at each time step, the current graph\nis connected. \n\n"}
{"id": "1808.00863", "contents": "Title: A Menger-like property of tree-cut width Abstract: In 1990, Thomas proved that every graph admits a tree decomposition of\nminimum width that additionally satisfies a certain vertex-connectivity\ncondition called leanness [A Menger-like property of tree-width: The finite\ncase. Journal of Combinatorial Theory, Series B, 48(1):67-76, 1990]. This\nresult had many uses and has been extended to several other decompositions.\n  In this paper, we consider tree-cut decompositions, that have been introduced\nby Wollan as a possible edge-version of tree decompositions [The structure of\ngraphs not admitting a fixed immersion. Journal of Combinatorial Theory, Series\nB, 110:47-66, 2015]. We show that every graph admits a tree-cut decomposition\nof minimum width that additionally satisfies an edge-connectivity condition\nanalogous to Thomas' leanness. \n\n"}
{"id": "1808.02534", "contents": "Title: Extending Supernova Spectral Templates for Next-Generation Space\n  Telescope Observations Abstract: Empirical models of supernova (SN) spectral energy distributions (SEDs) are\nwidely used for SN survey simulations and photometric classifications. The\nexisting library of SED models has excellent optical templates but limited,\npoorly constrained coverage of ultraviolet (UV) and infrared (IR) wavelengths.\nHowever, both regimes are critical for the design and operation of future SN\nsurveys, particularly at IR wavelengths that will be accessible with the James\nWebb Space Telescope (JWST) and the Wide-Field Infrared Survey Telescope\n(WFIRST). We create a public repository of improved empirical SED templates\nusing a sampling of Type Ia and core-collapse (CC) photometric light curves to\nextend the Type Ia parameterized SALT2 model and a set of SN Ib, SN Ic, and SN\nII SED templates into the UV and near-IR. We apply this new repository of\nextrapolated SN SED models to examine how future surveys can discriminate\nbetween CC and Type Ia SNe at UV and IR wavelengths, and present an open-source\nsoftware package written in Python, SNSEDextend, that enables a user to\ngenerate their own extrapolated SEDs. \n\n"}
{"id": "1808.02675", "contents": "Title: Packing colouring of some classes of cubic graphs Abstract: The packing chromatic number $\\chi$ $\\rho$ (G) of a graph G is the smallest\ninteger k such that its set of vertices V (G) can be partitioned into k\ndisjoint subsets V 1 ,. .. , V k , in such a way that every two distinct\nvertices in V i are at distance greater than i in G for every i, 1 $\\le$ i\n$\\le$ k. Recently, Balogh, Kostochka and Liu proved that $\\chi$ $\\rho$ is not\nbounded in the class of subcubic graphs [Packing chromatic number of subcubic\ngraphs, Discrete Math. 341 (2018), 474483], thus answering a question\npreviously addressed in several papers. However, several subclasses of cubic or\nsubcubic graphs have bounded packing chromatic number. In this paper, we\ndetermine the exact value of, or upper and lower bounds on, the packing\nchromatic number of some classes of cubic graphs, namely circular ladders, and\nso-called H-graphs and generalised H-graphs. \n\n"}
{"id": "1808.03367", "contents": "Title: A note on partial rejection sampling for the hard disks model in the\n  plane Abstract: In this note, we slightly improve the guarantees obtained by Guo and Jerrum\nfor sampling from the hard disks model in the plane via partial rejection\nsampling. Our proof makes use of the fact that if one spreads apart a\ncollection of disks in the plane, the area of the union of the disks cannot\ndecrease. \n\n"}
{"id": "1808.03633", "contents": "Title: A New Algorithm for the Robust Semi-random Independent Set Problem Abstract: In this paper, we study a general semi-random version of the planted\nindependent set problem in a model initially proposed by Feige and Kilian,\nwhich has a large proportion of adversarial edges.\n  We give a new deterministic algorithm that finds a list of independent sets,\none of which, with high probability, is the planted one, provided that the\nplanted set has size $k=\\Omega(n^{2/3})$. This improves on Feige and Kilian's\noriginal randomized algorithm, which with high probability recovers an\nindependent set of size at least $k$ when $k=\\alpha n$ where $\\alpha$ is a\nconstant. \n\n"}
{"id": "1808.04123", "contents": "Title: The photocentre-AGN displacement: Is M87 actually harbouring a displaced\n  supermassive black hole? Abstract: M87 has been identified as a displaced supermassive black hole (SMBH)\ncandidate. We investigated this possibility by a temporal analysis of twelve\nAdaptive-Optics assisted-VLT and HST images spanning twenty years. We found\nthat the centre of the isophotal fitting to the nuclear region of M87 -assumed\nto mark the centre of mass of the galaxy- changes location depending on the\nimage and size of the image analysed. In an absolute frame of reference, the\nchange varies from 15 to 130 miliarcseconds (mas) with respect to the active\ngalactic nucleus (AGN), which remains stable within an uncertainty of $\\pm$15\nmas in both x and y axis. The temporal analysis of the results indicates that\nthe major displacements measured coincide with a powerful outburst that took\nplace between 2003 and 2007, where there was a flux increment in the nucleus\nand the first knot of the jet. After the outburst, the isophotal centre remains\nstable and is consistent with the AGN location. This suggests that the\ndisplacements are artificially caused by a flux variation in the galaxy and\nthat the SMBH actually resides in the equilibrium position. We caution about\nthe determination of the galaxy photocentre by isophotal fitting in cases of\nnuclear variability and/or presence of photometric irregularities, and advise a\nlong-term temporal analysis of the results to confirm possible displaced SMBHs. \n\n"}
{"id": "1808.04729", "contents": "Title: Red and Reddened: Ultraviolet through Near-Infrared Observations of Type\n  Ia Supernova 2017erp Abstract: We present space-based ultraviolet/optical photometry and spectroscopy with\nthe Swift Ultra-Violet/Optical Telescope and Hubble Space Telescope,\nrespectively, along with ground-based optical photometry and spectroscopy and\nnear-infrared spectroscopy of supernova SN2017erp. The optical light curves and\nspectra are consistent with a normal Type Ia supernova (SN Ia). Compared to\nprevious photometric samples in the near-ultraviolet (NUV), SN2017erp has\ncolors similar to the NUV-red category after correcting for Milky Way and host\ndust reddening. We find the difference between SN2017erp and the NUV-blue\nSN2011fe is not consistent with dust reddening alone but is similar to the SALT\ncolor law, derived from rest-frame UV photometry of higher redshift SNe Ia.\nThis chromatic difference is dominated by the intrinsic differences in the UV\nand only a small contribution from the expected dust reddening. Differentiating\nthe two can have important consequences for determining cosmological distances\nwith rest-frame UV photometry. This spectroscopic series is important for\nanalyzing SNe Ia with intrinsically redder NUV colors. We also show model\ncomparisons suggesting that metallicity could be the physical difference\nbetween NUV-blue and NUV-red SNe Ia, with emission peaks from reverse\nfluorescence near 3000 Angstroms implying a factor of ten higher metallicity in\nthe upper layers of SN2017erp compared to SN~2011fe. Metallicity estimates are\nvery model dependent however, and there are multiple effects in the UV. Further\nmodels and UV spectra of SNe Ia are needed to explore the diversity of SNe Ia\nwhich show seemingly independent differences in the near-UV peaks and mid-UV\nflux levels. \n\n"}
{"id": "1808.05318", "contents": "Title: Insight-HXMT observations of the New Black Hole Candidate MAXI\n  J1535-571: timing analysis Abstract: We present the X-ray timing results of the new black hole candidate (BHC)\nMAXI J1535-571 during its 2017 outburst from Hard X-ray Modulation Telescope\n(\\emph{Insight}-HXMT) observations taken from 2017 September 6 to 23. Following\nthe definitions given by \\citet{Belloni2010}, we find that the source exhibits\nstate transitions from Low/Hard state (LHS) to Hard Intermediate state (HIMS)\nand eventually to Soft Intermediate state (SIMS). Quasi-periodic oscillations\n(QPOs) are found in the intermediate states, which suggest different types of\nQPOs. With the large effective area of \\emph{Insight}-HXMT at high energies, we\nare able to present the energy dependence of the QPO amplitude and centroid\nfrequency up to 100 keV which is rarely explored by previous satellites. We\nalso find that the phase lag at the type-C QPOs centroid frequency is negative\n(soft lags) and strongly correlated with the centroid frequency. By assuming a\ngeometrical origin of type-C QPOs, the source is consistent with being a high\ninclination system. \n\n"}
{"id": "1808.05676", "contents": "Title: Why did the shape of your network change? (On detecting network\n  anomalies via non-local curvatures) Abstract: $Anomaly$ $detection$ problems (also called $change$-$point$ $detection$\nproblems) have been studied in data mining, statistics and computer science\nover the last several decades in applications such as medical condition\nmonitoring and weather change detection. In recent days, however, anomaly\ndetection problems have become increasing more relevant in the context of\n$network$ $science$ since useful insights for many complex systems in biology,\nfinance and social science are often obtained by representing them via\nnetworks. Notions of local and non-local curvatures of higher-dimensional\ngeometric shapes and topological spaces play a $fundamental$ role in physics\nand mathematics in characterizing anomalous behaviours of these higher\ndimensional entities. However, using curvature measures to detect anomalies in\nnetworks is not yet very common. To this end, a main goal in this paper to\nformulate and analyze curvature analysis methods to provide the foundations of\nsystematic approaches to find $critical$ $components$ and $detect$ $anomalies$\nin networks. For this purpose, we use two measures of network curvatures which\ndepend on non-trivial global properties, such as distributions of geodesics and\nhigher-order correlations among nodes, of the given network. Based on these\nmeasures, we precisely formulate several computational problems related to\nanomaly detection in static or dynamic networks, and provide non-trivial\ncomputational complexity results for these problems. This paper must $not$ be\nviewed as delivering the final word on appropriateness and suitability of\nspecific curvature measures. Instead, it is our hope that this paper will\nstimulate and motivate further theoretical or empirical research concerning the\nexciting interplay between notions of curvatures from network and non-network\ndomains, a $much$ desired goal in our opinion. \n\n"}
{"id": "1808.07400", "contents": "Title: Analytic solution of a magnetized tori with magnetic polarization around\n  Kerr black holes Abstract: We present the first family of magnetically polarized equilibrium tori around\na Kerr black hole. The models were obtained in the test fluid approximation by\nassuming that the tori is a linear media, making it is possible to characterize\nthe magnetic polarization of the fluid through the magnetic susceptibility\n$\\chi_{m}$. The magnetohydrodynamic (MHD) structure of the models was solved by\nfollowing the Komissarov approach, but with the aim of including the magnetic\npolarization of the fluid, the integrability condition for the magnetic\ncounterpart was modified. We build two kinds of magnetized tori depending on\nwhether the magnetic susceptibility is constant in space or not. In the models\nwith constant $\\chi_{m}$, we find that the paramagnetic tori ($\\chi_{m}>0$) are\nmore dense and less magnetized than the diamagnetic ones ($\\chi_{m}<0$) in the\nregion between the inner edge, $r_{in}$, and the center of the disk, $r_{c}$;\nhowever, we find the opposite behavior for $r>r_{c}$. Now, in the models with\nnon-constant $\\chi_{m}$, the tori become more magnetized than the Komissarov\nsolution in the region where $\\partial\\chi_{m}/\\partial r<0$, and less\nmagnetized when $\\partial\\chi_{m}/\\partial r>0$. Nevertheless, it is worth\nmentioning that in all solutions presented in this paper the magnetic pressure\nis greater than the hydrodynamic pressure. These new equilibrium tori can be\nuseful for studying the accretion of a magnetic media onto a rotating black\nhole. \n\n"}
{"id": "1808.09476", "contents": "Title: First ALMA Light Curve Constrains Refreshed Reverse Shocks and Jet\n  Magnetization in GRB 161219B Abstract: We present detailed multi-wavelength observations of GRB 161219B at\n$z=0.1475$, spanning the radio to X-ray regimes, and the first ALMA light curve\nof a GRB afterglow. The cm- and mm-band observations before $8.5$ d require\nemission in excess of that produced by the afterglow forward shock (FS). These\ndata are consistent with radiation from a refreshed reverse shock (RS) produced\nby the injection of energy into the FS, signatures of which are also present in\nthe X-ray and optical light curves. We infer a constant-density circumburst\nenvironment with an extremely low density, $n_0\\approx 3\\times10^{-4}$\ncm$^{-3}$ and show that this is a characteristic of all strong RS detections to\ndate. The VLA observations exhibit unexpected rapid variability on $\\sim$\nminute timescales, indicative of strong interstellar scintillation. The X-ray,\nALMA, and VLA observations together constrain the jet break time, $t_{\\rm\njet}\\approx32$ day, yielding a wide jet opening angle of $\\theta_{\\rm\njet}\\approx13^{\\circ}$, implying beaming corrected $\\gamma$-ray and kinetic\nenergies of $E_{\\gamma}\\approx4.9\\times10^{48}$ erg and $E_{\\rm\nK}\\approx1.3\\times10^{50}$ erg, respectively. Comparing the RS and FS emission,\nwe show that the ejecta are only weakly magnetized, with relative\nmagnetization, $R_{\\rm B}\\approx1$, compared to the FS. These direct,\nmulti-frequency measurements of a refreshed RS spanning the optical to radio\nbands highlight the impact of radio and millimeter data in probing the\nproduction and nature of GRB jets. \n\n"}
{"id": "1808.09669", "contents": "Title: Recent progress on scaling algorithms and applications Abstract: Scaling problems have a rich and diverse history, and thereby have found\nnumerous applications in several fields of science and engineering. For\ninstance, the matrix scaling problem has had applications ranging from\ntheoretical computer science to telephone forecasting, economics, statistics,\noptimization, among many other fields. Recently, a generalization of matrix\nscaling known as operator scaling has found applications in non-commutative\nalgebra, invariant theory, combinatorics and algebraic complexity; and a\nfurther generalization (tensor scaling) has found more applications in quantum\ninformation theory, geometric complexity theory and invariant theory. In this\nsurvey, we will describe in detail the scaling problems mentioned above,\nshowing how alternating minimization algorithms naturally arise in this\nsetting, and we shall present a general framework to rigorously analyze such\nalgorithms. These simple problems and algorithms are not just applicable to\ndiverse mathematical and CS areas, but also serve to bring out deep connections\nbetween them. As this framework makes extensive use of concepts from invariant\ntheory, we also provide a very gentle introduction to basic concepts of\ninvariant theory and how they are used to analyze alternating minimization\nalgorithms for the scaling problems. This survey is intended for a general\ncomputer science audience, and the only background required is basic knowledge\nof calculus and linear algebra, thereby making it accessible to graduate\nstudents and even to advanced undergraduates. \n\n"}
{"id": "1808.09969", "contents": "Title: A concordance picture of FRB 121102 as a flaring magnetar embedded in a\n  magnetized ion-electron wind nebula Abstract: The fast radio burst FRB 121102 has repeated multiple times, enabling the\nidentification of its host galaxy and of a spatially-coincident, compact,\nsteady (`quiescent') radio synchrotron source. It was proposed that FRB 121102\nis powered by a young flaring magnetar, embedded within a decades-old supernova\nremnant. Using a time-dependent one-zone model, we show that a single expanding\nmagnetized electron-ion nebula (powered by the same outbursts likely\nresponsible for the FRBs) can explain all the basic properties of the quiescent\nsource (size, flux, self-absorption constraints) and the large but decreasing\nrotation measure (RM) of the bursts. The quiescent emission is powered by\nrelativistic thermal electrons heated at the termination shock of the magnetar\nwind, while the RM originates from non-relativistic electrons injected earlier\nin the nebula's evolution and cooled through expansion and radiative losses.\nThe model contains few free parameters, which are tightly constrained by\nobservations: the total energy injected into the nebula over its history, $\\sim\n10^{50}-10^{51}$ erg, agrees with the magnetic energy of a millisecond\nmagnetar; the baryon loading of the magnetar outflow (driven by intermittent\nflares) is close to the neutron star escape speed; the predicted source age\n$\\sim 10-40$ years is consistent with other constraints on the nebula size. For\nan energy input rate $\\dot{E} \\propto t^{-\\alpha}$ following the onset of\nmagnetar activity, we predict secular decay of the RM and quiescent source\nflux, which approximately follow ${\\rm RM} \\propto t^{-(6+\\alpha)/2}$ and\n$F_{\\nu} \\propto t^{-(\\alpha^2+7\\alpha-2)/4}$, respectively. \n\n"}
{"id": "1808.10655", "contents": "Title: Accretion heated atmospheres of X-ray bursting neutron stars Abstract: Some thermonuclear (type I) X-ray bursts at the neutron star surfaces in\nlow-mass X-ray binaries take place during hard persistent states of the\nsystems. Spectral evolution of these bursts is well described by the atmosphere\nmodel of a passively cooling neutron star when the burst luminosity is high\nenough. The observed spectral evolution deviates from the model predictions\nwhen the burst luminosity drops below a critical value of 20-70% of the maximum\nluminosity. We suggest that these deviations are induced by the additional\nheating of the accreted particles. We present a method for computation of the\nneutron star atmosphere models heated by accreted particles assuming that their\nenergy is released via Coulomb interactions with electrons. We compute the\ntemperature structures and the emergent spectra of the atmospheres of various\nchemical compositions and investigate the dependence of the results on the\nother model parameters. We show that the heated atmosphere develops the hot\n(20--100 keV) corona-like surface layer cooled by Compton scattering, and the\ndeeper, almost isothermal optically thick region with a temperature of a few\nkeV. The emergent spectra deviate strongly from those of undisturbed neutron\nstar atmospheres, with the main differences being the presence of a high-energy\ntail and a strong excess in the low-energy part of the spectrum. They also lack\nthe iron absorption edge, which is visible in the spectra of undisturbed\nlow-luminosity atmospheres with solar chemical composition. Using the computed\nspectra, we obtained the dependences of the dilution and color-correction\nfactors as functions of relative luminosities for pure helium and solar\nabundance atmospheres. We show that the helium model atmosphere heated by\naccretion corresponding to 5% of the Eddington luminosity describes well the\nlate stages of the X-ray bursts in 4U 1820-30. \n\n"}
{"id": "1809.00394", "contents": "Title: Mining Frequent Patterns in Evolving Graphs Abstract: Given a labeled graph, the frequent-subgraph mining (FSM) problem asks to\nfind all the $k$-vertex subgraphs that appear with frequency greater than a\ngiven threshold. FSM has numerous applications ranging from biology to network\nscience, as it provides a compact summary of the characteristics of the graph.\nHowever, the task is challenging, even more so for evolving graphs due to the\nstreaming nature of the input and the exponential time complexity of the\nproblem.\n  In this paper, we initiate the study of the approximate FSM problem in both\nincremental and fully-dynamic streaming settings, where arbitrary edges can be\nadded or removed from the graph. For each streaming setting, we propose\nalgorithms that can extract a high-quality approximation of the frequent\n$k$-vertex subgraphs for a given threshold, at any given time instance, with\nhigh probability. In contrast to the existing state-of-the-art solutions that\nrequire iterating over the entire set of subgraphs for any update, our\nalgorithms operate by maintaining a uniform sample of $k$-vertex subgraphs with\noptimized neighborhood-exploration procedures local to the updates. We provide\ntheoretical analysis of the proposed algorithms and empirically demonstrate\nthat the proposed algorithms generate high-quality results compared to\nbaselines. \n\n"}
{"id": "1809.01207", "contents": "Title: SOS lower bounds with hard constraints: think global, act local Abstract: Many previous Sum-of-Squares (SOS) lower bounds for CSPs had two deficiencies\nrelated to global constraints. First, they were not able to support a\n\"cardinality constraint\", as in, say, the Min-Bisection problem. Second, while\nthe pseudoexpectation of the objective function was shown to have some value\n$\\beta$, it did not necessarily actually \"satisfy\" the constraint \"objective =\n$\\beta$\". In this paper we show how to remedy both deficiencies in the case of\nrandom CSPs, by translating \\emph{global} constraints into \\emph{local}\nconstraints. Using these ideas, we also show that degree-$\\Omega(\\sqrt{n})$ SOS\ndoes not provide a $(\\frac{4}{3} - \\epsilon)$-approximation for Min-Bisection,\nand degree-$\\Omega(n)$ SOS does not provide a $(\\frac{11}{12} +\n\\epsilon)$-approximation for Max-Bisection or a $(\\frac{5}{4} -\n\\epsilon)$-approximation for Min-Bisection. No prior SOS lower bounds for these\nproblems were known. \n\n"}
{"id": "1809.02481", "contents": "Title: ALMA CO Observations of Supernova Remnant N63A in the Large Magellanic\n  Cloud: Discovery of Dense Molecular Clouds Embedded within Shock-Ionized and\n  Photoionized Nebulae Abstract: We carried out new $^{12}$CO($J$ = 1-0, 3-2) observations of a N63A supernova\nremnant (SNR) from the LMC using ALMA and ASTE. We find three giant molecular\nclouds toward the northeast, east, and near the center of the SNR. Using the\nALMA data, we spatially resolved clumpy molecular clouds embedded within the\noptical nebulae in both the shock-ionized and photoionized lobes discovered by\nprevious H$\\alpha$ and [S II] observations. The total mass of the molecular\nclouds is $\\sim$$800$ $M_{\\odot}$ for the shock-ionized region and $\\sim$$1700$\n$M_{\\odot}$ for the photoionized region. Spatially resolved X-ray spectroscopy\nreveals that the absorbing column densities toward the molecular clouds are\n$\\sim$$1.5$-$6.0\\times10^{21}$ cm$^{-2}$, which are $\\sim$$1.5$-$15$ times less\nthan the averaged interstellar proton column densities for each region. This\nmeans that the X-rays are produced not only behind the molecular clouds, but\nalso in front of them. We conclude that the dense molecular clouds have been\ncompletely engulfed by the shock waves, but have still survived erosion owing\nto their high-density and short interacting time. The X-ray spectrum toward the\ngas clumps is well explained by an absorbed power-law or high-temperature\nplasma models in addition to the thermal plasma components, implying that the\nshock-cloud interaction is efficiently working for both the cases through the\nshock ionization and magnetic field amplification. If the hadronic gamma-ray is\ndominant in the GeV band, the total energy of cosmic-ray protons is calculated\nto be $\\sim$$0.3$-$1.4\\times10^{49}$ erg with the estimated ISM proton density\nof $\\sim$$190\\pm90$ cm$^{-3}$, containing both the shock-ionized gas and\nneutral atomic hydrogen. \n\n"}
{"id": "1809.02835", "contents": "Title: Multitasking Capacity: Hardness Results and Improved Constructions Abstract: We consider the problem of determining the maximal $\\alpha \\in (0,1]$ such\nthat every matching $M$ of size $k$ (or at most $k$) in a bipartite graph $G$\ncontains an induced matching of size at least $\\alpha |M|$. This measure was\nrecently introduced in Alon et al. (NIPS 2018) and is motivated by\nconnectionist models of cognition as well as modeling interference in wireless\nand communication networks.\n  We prove various hardness results for computing $\\alpha$ either exactly or\napproximately. En route to our results, we also consider the maximum connected\nmatching problem: determining the largest matching $N$ in a graph $G$ such that\nevery two edges in $N$ are connected by an edge. We prove a nearly optimal\n$n^{1-\\epsilon}$ hardness of approximation result (under randomized reductions)\nfor connected matching in bipartite graphs (with both sides of cardinality\n$n$). Towards this end we define bipartite half-covers: A new combinatorial\nobject that may be of independent interest. To the best of our knowledge, the\nbest previous hardness result for the connected matching problem was some\nconstant $\\beta>1$.\n  Finally, we demonstrate the existence of bipartite graphs with $n$ vertices\non each side of average degree $d$, that achieve $\\alpha=1/2-\\epsilon$ for\nmatchings of size sufficiently smaller than $n/poly(d)$. This nearly matches\nthe trivial upper bound of $1/2$ on $\\alpha$ which holds for any graph\ncontaining a path of length 3. \n\n"}
{"id": "1809.03546", "contents": "Title: A log-Sobolev inequality for the multislice, with applications Abstract: Let $\\kappa \\in \\mathbb{N}_+^\\ell$ satisfy $\\kappa_1 + \\dots + \\kappa_\\ell =\nn$ and let $\\mathcal{U}_\\kappa$ denote the \"multislice\" of all strings $u$ in\n$[\\ell]^n$ having exactly $\\kappa_i$ coordinates equal to $i$, for all $i \\in\n[\\ell]$. Consider the Markov chain on $\\mathcal{U}_\\kappa$, where a step is a\nrandom transposition of two coordinates of $u$. We show that the log-Sobolev\nconstant $\\rho_\\kappa$ for the chain satisfies $$(\\rho_\\kappa)^{-1} \\leq n\n\\sum_{i=1}^{\\ell} \\tfrac{1}{2} \\log_2(4n/\\kappa_i),$$ which is sharp up to\nconstants whenever $\\ell$ is constant. From this, we derive some consequences\nfor small-set expansion and isoperimetry in the multislice, including a KKL\nTheorem, a Kruskal--Katona Theorem for the multislice, a Friedgut Junta\nTheorem, and a Nisan--Szegedy Theorem. \n\n"}
{"id": "1809.04126", "contents": "Title: Binary black hole growth by gas accretion in stellar clusters Abstract: We show that binaries of stellar-mass black holes formed inside a young\nprotoglobular cluster, can grow rapidly inside the cluster's core by accretion\nof the intracluster gas, before the gas may be depleted from the core. A black\nhole with mass of the order of eight solar masses can grow to values of the\norder of thirty five solar masses in accordance with recent gravitational waves\nsignals observed by LIGO. Due to the black hole mass increase, a binary may\nalso harden. The growth of binary black holes in a dense protoglobular cluster\nthrough mass accretion indicates a potentially important formation and\nhardening channel. \n\n"}
{"id": "1809.04578", "contents": "Title: Simplicity Creates Inequity: Implications for Fairness, Stereotypes, and\n  Interpretability Abstract: Algorithms are increasingly used to aid, or in some cases supplant, human\ndecision-making, particularly for decisions that hinge on predictions. As a\nresult, two additional features in addition to prediction quality have\ngenerated interest: (i) to facilitate human interaction and understanding with\nthese algorithms, we desire prediction functions that are in some fashion\nsimple or interpretable; and (ii) because they influence consequential\ndecisions, we also want them to produce equitable allocations. We develop a\nformal model to explore the relationship between the demands of simplicity and\nequity. Although the two concepts appear to be motivated by qualitatively\ndistinct goals, we show a fundamental inconsistency between them. Specifically,\nwe formalize a general framework for producing simple prediction functions, and\nin this framework we establish two basic results. First, every simple\nprediction function is strictly improvable: there exists a more complex\nprediction function that is both strictly more efficient and also strictly more\nequitable. Put another way, using a simple prediction function both reduces\nutility for disadvantaged groups and reduces overall welfare relative to other\noptions. Second, we show that simple prediction functions necessarily create\nincentives to use information about individuals' membership in a disadvantaged\ngroup --- incentives that weren't present before simplification, and that work\nagainst these individuals. Thus, simplicity transforms disadvantage into bias\nagainst the disadvantaged group. Our results are not only about algorithms but\nabout any process that produces simple models, and as such they connect to the\npsychology of stereotypes and to an earlier economics literature on statistical\ndiscrimination. \n\n"}
{"id": "1809.05419", "contents": "Title: Approximate Query Processing over Static Sets and Sliding Windows Abstract: Indexing of static and dynamic sets is fundamental to a large set of\napplications such as information retrieval and caching. Denoting the\ncharacteristic vector of the set by B, we consider the problem of encoding sets\nand multisets to support approximate versions of the operations rank(i) (i.e.,\ncomputing sum_{j <= i}B[j]) and select(i) (i.e., finding min{p | rank(p) >= i})\nqueries. We study multiple types of approximations (allowing an error in the\nquery or the result) and present lower bounds and succinct data structures for\nseveral variants of the problem. We also extend our model to sliding windows,\nin which we process a stream of elements and compute suffix sums. This is a\ngeneralization of the window summation problem that allows the user to specify\nthe window size at query time. Here, we provide an algorithm that supports\nupdates and queries in constant time while requiring just (1+o(1)) factor more\nspace than the fixed-window summation algorithms. \n\n"}
{"id": "1809.06141", "contents": "Title: On the Reconstruction of Static and Dynamic Discrete Structures Abstract: We study inverse problems of reconstructing static and dynamic discrete\nstructures from tomographic data (with a special focus on the `classical' task\nof reconstructing finite point sets in $\\mathbb{R}^d$). The main emphasis is on\nrecent mathematical developments and new applications, which emerge in\nscientific areas such as physics and materials science, but also in inner\nmathematical fields such as number theory, optimization, and imaging. Along\nwith a concise introduction to the field of discrete tomography, we give\npointers to related aspects of computerized tomography in order to contrast the\nworlds of continuous and discrete inverse problems. \n\n"}
{"id": "1809.07568", "contents": "Title: The Gravothermal Instability at all scales: from Turnaround Radius to\n  Supernovae Abstract: The gravitational instability, responsible for the formation of the structure\nof the Universe, occurs below energy thresholds and above spatial scales of a\nself-gravitating expanding region, when thermal energy can no longer\ncounterbalance self-gravity. I argue that at sufficiently-large scales, dark\nenergy may restore thermal stability. This stability re-entrance of an\nisothermal sphere defines a turnaround radius, which dictates the maximum\nallowed size of any structure generated by gravitational instability. On the\nopposite limit of high energies and small scales, I will show that an ideal,\nquantum or classical, self-gravitating gas is subject to a high-energy\nrelativistic gravothermal instability. It occurs at sufficiently-high energy\nand small radii, when thermal energy cannot support its own gravitational\nattraction. Applications of the phenomenon include neutron stars and\ncore-collapse supernovae. I also extend the original Oppenheimer--Volkov\ncalculation of the maximum mass limit of ideal neutron cores to the non-zero\ntemperature regime, relevant to the whole cooling stage from a hot\nproto-neutron star down to the final cold state. \n\n"}
{"id": "1809.09615", "contents": "Title: The ANITA Anomalous Events as Signatures of a Beyond Standard Model\n  Particle, and Supporting Observations from IceCube Abstract: The ANITA collaboration have reported observation of two anomalous events\nthat appear to be $\\varepsilon_{\\rm cr} \\approx 0.6$ EeV cosmic ray showers\nemerging from the Earth with exit angles of $27^\\circ$ and $35^\\circ$,\nrespectively. While EeV-scale upgoing showers have been anticipated as a result\nof astrophysical tau neutrinos converting to tau leptons during Earth passage,\nthe observed exit angles are much steeper than expected in Standard Model (SM)\nscenarios. Indeed, under conservative extrapolations of the SM interactions,\nthere is no particle that can propagate through the Earth with probability $p >\n10^{-6}$ at these energies and exit angles. We explore here whether \"beyond the\nStandard Model\" (BSM) particles are required to explain the ANITA events, if\ncorrectly interpreted, and conclude that they are. Seeking confirmation or\nrefutation of the physical phenomenon of sub-EeV Earth-emergent cosmic rays in\ndata from other facilities, we find support for the reality of the ANITA\nevents, and three candidate analog events, among the Extremely High Energy\nNorthern Track neutrinos of the IceCube Neutrino Observatory. Properties of the\nimplied BSM particle are anticipated, at least in part, by those predicted for\nthe \"stau\" slepton ($\\tilde{\\tau}_R$) in some supersymmetric models of the\nfundamental interactions, wherein the stau manifests as the next-to-lowest mass\nsupersymmetric partner particle. \n\n"}
{"id": "1809.10469", "contents": "Title: Probabilistic Analysis of Edge Elimination for Euclidean TSP Abstract: One way to speed up the calculation of optimal TSP tours in practice is\neliminating edges that are certainly not in the optimal tour as a preprocessing\nstep. In order to do so several edge elimination approaches have been proposed\nin the past. In this work we investigate two of them in the scenario where the\ninput consists of $n$ independently distributed random points in the\n2-dimensional unit square with bounded density function from above and below by\narbitrary positive constants. We show that after the edge elimination procedure\nof Hougardy and Schroeder the expected number of remaining edges is\n$\\Theta(n)$, while after that the non-recursive part of Jonker and Volgenant\nthe expected number of remaining edges is $\\Theta(n^2)$. \n\n"}
{"id": "1809.10508", "contents": "Title: Distance and routing labeling schemes for cube-free median graphs Abstract: Distance labeling schemes are schemes that label the vertices of a graph with\nshort labels in such a way that the distance between any two vertices $u$ and\n$v$ can be determined efficiently by merely inspecting the labels of $u$ and\n$v$, without using any other information. Similarly, routing labeling schemes\nlabel the vertices of a graph in a such a way that given the labels of a source\nnode and a destination node, it is possible to compute efficiently the port\nnumber of the edge from the source that heads in the direction of the\ndestination. One of important problems is finding natural classes of graphs\nadmitting distance and/or routing labeling schemes with labels of\npolylogarithmic size. In this paper, we show that the class of cube-free median\ngraphs on $n$ nodes enjoys distance and routing labeling schemes with labels of\n$O(\\log^3 n)$ bits. \n\n"}
{"id": "1810.00190", "contents": "Title: Axion cooling of neutron stars. II. Beyond hadronic axions Abstract: We study the axion cooling of neutron stars within the\nDine-Fischler-Srednicki-Zhitnitsky (DFSZ) model, which allows for tree level\ncoupling of electrons to the axion {and locks the Peccei-Quinn charges of\nfermions via an angle parameter}. This extends our previous study [Phys. Rev. D\n93, 065044 (2016)] limited to hadronic models of axions. We explore the\ntwo-dimensional space of axion parameters within the DFSZ model by comparing\nthe theoretical cooling models with the surface temperatures of a few stars\nwith measured surface temperatures. It is found that axions masses $m_a\\ge\n0.06$ to 0.12 eV can be excluded by x-ray observations of thermal emission of\nneutron stars (in particular by those of Cas A), the precise limiting value\ndepending on the angle parameter of the DFSZ model. It is also found that axion\nemission by electron bremsstrahlung in neutron star crusts is negligible except\nfor the special case where neutron Peccei-Quinn charge is small enough, so that\nthe coupling of neutrons to axions can be neglected. \n\n"}
{"id": "1810.00528", "contents": "Title: Gravitational Wave from Phase Transition inside Neutron Stars Abstract: In this work, we propose a new source for gravitational wave (GW) radiation\nassociated with the quantum chromodynamics (QCD) phase transition in the inner\ncores of neutron stars. The mechanism is based on the bubble dynamics during\nthe first-order phase transition from nuclear matter to quark matter. We\nidentify the characteristic frequency to be of order $\\omega_c\\sim 10^6~{\\rm\nrad/s}$ for this kind of sources and the strain magnitude ($h\\sim 10^{-24}$ for\na neutron star at a distance of $0.1~{\\rm Mpc}$) reachable by future GW\ndetectors. The GW spectra are shown to be useful to check the transition nature\nat high baryon chemical potential as well as to constrain the radius and\ndensity of the inner cores, which are still indistinct up to now. \n\n"}
{"id": "1810.00580", "contents": "Title: Slaying Hydrae: Improved Bounds for Generalized k-Server in Uniform\n  Metrics Abstract: The generalized $k$-server problem is an extension of the weighted $k$-server\nproblem, which in turn extends the classic $k$-server problem. In the\ngeneralized $k$-server problem, each of $k$ servers $s_1, \\dots, s_k$ remains\nin its own metric space $M_i$. A request is a tuple $(r_1,\\dots,r_k)$, where\n$r_i \\in M_i$, and to service it, an algorithm needs to move at least one\nserver $s_i$ to the point $r_i$. The objective is to minimize the total\ndistance traveled by all servers.\n  In this paper, we focus on the generalized $k$-server problem for the case\nwhere all $M_i$ are uniform metrics. We show an $O(k^2 \\cdot \\log\nk)$-competitive randomized algorithm improving over a recent result by Bansal\net al. [SODA 2018], who gave an $O(k^3 \\cdot \\log k)$-competitive algorithm. To\nthis end, we define an abstract online problem, called Hydra game, and we show\nthat a randomized solution of low cost to this game implies a randomized\nalgorithm to the generalized $k$-server problem with low competitive ratio.\n  We also show that no randomized algorithm can achieve competitive ratio lower\nthan $\\Omega(k)$, thus improving the lower bound of $\\Omega(k / \\log^2 k)$ by\nBansal et al. \n\n"}
{"id": "1810.00789", "contents": "Title: Enumerating minimal dominating sets in $K_t$-free graphs and variants Abstract: It is a long-standing open problem whether the minimal dominating sets of a\ngraph can be enumerated in output-polynomial time. In this paper we investigate\nthis problem in graph classes defined by forbidding an induced subgraph. In\nparticular, we provide output-polynomial time algorithms for $K_t$-free graphs\nand variants. This answers a question of Kant\\'e et al. about enumeration in\nbipartite graphs. \n\n"}
{"id": "1810.00811", "contents": "Title: Caterpillars in Erd\\H{o}s-Hajnal Abstract: Let $T$ be a tree such that all its vertices of degree more than two lie on\none path, that is, $T$ is a caterpillar subdivision. We prove that there exists\n$\\epsilon>0$ such that for every graph $G$ with $|V(G)|\\ge 2$ not containing\n$T$ as an induced subgraph, either some vertex has at least $\\epsilon|V(G)|$\nneighbours, or there are two disjoint sets of vertices $A,B$, both of\ncardinality at least $\\epsilon|V(G)|$, where there is no edge joining $A$ and\n$B$.\n  A consequence is: for every caterpillar subdivision $T$, there exists $c>0$\nsuch that for every graph $G$ containing neither of $T$ and its complement as\nan induced subgraph, $G$ has a clique or stable set with at least $|V(G)|^c$\nvertices. This extends a theorem of Bousquet, Lagoutte and Thomass\\'e [JCTB\n2015], who proved the same when $T$ is a path, and a recent theorem of\nChoromanski, Falik, Liebenau, Patel and Pilipczuk [Electron. J. Combin. 2018],\nwho proved it when $T$ is a \"hook\". \n\n"}
{"id": "1810.01223", "contents": "Title: Near-Linear Approximation Algorithms for Scheduling Problems with Batch\n  Setup Times Abstract: We investigate the scheduling of $n$ jobs divided into $c$ classes on $m$\nidentical parallel machines. For every class there is a setup time which is\nrequired whenever a machine switches from the processing of one class to\nanother class. The objective is to find a schedule that minimizes the makespan.\nWe give near-linear approximation algorithms for the following problem\nvariants: the non-preemptive context where jobs may not be preempted, the\npreemptive context where jobs may be preempted but not parallelized, as well as\nthe splittable context where jobs may be preempted and parallelized.\n  We present the first algorithm improving the previously best approximation\nratio of $2$ to a better ratio of $3/2$ in the preemptive case. In more detail,\nfor all three flavors we present an approximation ratio $2$ with running time\n$\\mathcal{O}(n)$, ratio $3/2+\\varepsilon$ in time $\\mathcal{O}(n\\log\n1/\\varepsilon)$ as well as a ratio of $3/2$. The $(3/2)$-approximate algorithms\nhave different running times. In the non-preemptive case we get time\n$\\mathcal{O}(n\\log (n+\\Delta))$ where $\\Delta$ is the largest value of the\ninput. The splittable approximation runs in time $\\mathcal{O}(n+c\\log(c+m))$\nwhereas the preemptive algorithm has a running time $\\mathcal{O}(n \\log (c+m))\n\\leq \\mathcal{O}(n \\log n)$. So far, no PTAS is known for the preemptive\nproblem without restrictions, so we make progress towards that question.\nRecently Jansen et al. found an EPTAS for the splittable and non-preemptive\ncase but with impractical running times exponential in $1/\\varepsilon$. \n\n"}
{"id": "1810.02183", "contents": "Title: Revealing Network Structure, Confidentially: Improved Rates for\n  Node-Private Graphon Estimation Abstract: Motivated by growing concerns over ensuring privacy on social networks, we\ndevelop new algorithms and impossibility results for fitting complex\nstatistical models to network data subject to rigorous privacy guarantees. We\nconsider the so-called node-differentially private algorithms, which compute\ninformation about a graph or network while provably revealing almost no\ninformation about the presence or absence of a particular node in the graph.\n  We provide new algorithms for node-differentially private estimation for a\npopular and expressive family of network models: stochastic block models and\ntheir generalization, graphons. Our algorithms improve on prior work, reducing\ntheir error quadratically and matching, in many regimes, the optimal nonprivate\nalgorithm. We also show that for the simplest random graph models ($G(n,p)$ and\n$G(n,m)$), node-private algorithms can be qualitatively more accurate than for\nmore complex models---converging at a rate of $\\frac{1}{\\epsilon^2 n^{3}}$\ninstead of $\\frac{1}{\\epsilon^2 n^2}$. This result uses a new extension lemma\nfor differentially private algorithms that we hope will be broadly useful. \n\n"}
{"id": "1810.02304", "contents": "Title: Polynomial-time Recognition of 4-Steiner Powers Abstract: The $k^{th}$-power of a given graph $G=(V,E)$ is obtained from $G$ by adding\nan edge between every two distinct vertices at a distance at most $k$ in $G$.\nWe call $G$ a $k$-Steiner power if it is an induced subgraph of the\n$k^{th}$-power of some tree. Our main contribution is a polynomial-time\nrecognition algorithm of $4$-Steiner powers, thereby extending the\ndecade-year-old results of (Lin, Kearney and Jiang, ISAAC'00) for $k=1,2$ and\n(Chang and Ko, WG'07) for $k=3$.\n  A graph $G$ is termed $k$-leaf power if there is some tree $T$ such that: all\nvertices in $V(G)$ are leaf-nodes of $T$, and $G$ is an induced subgraph of the\n$k^{th}$-power of $T$. As a byproduct of our main result, we give the first\nknown polynomial-time recognition algorithm for $6$-leaf powers. \n\n"}
{"id": "1810.03395", "contents": "Title: 1-Safe Petri nets and special cube complexes: equivalence and\n  applications Abstract: Nielsen, Plotkin, and Winskel (1981) proved that every 1-safe Petri net $N$\nunfolds into an event structure $\\mathcal{E}_N$. By a result of Thiagarajan\n(1996 and 2002), these unfoldings are exactly the trace regular event\nstructures. Thiagarajan (1996 and 2002) conjectured that regular event\nstructures correspond exactly to trace regular event structures. In a recent\npaper (Chalopin and Chepoi, 2017, 2018), we disproved this conjecture, based on\nthe striking bijection between domains of event structures, median graphs, and\nCAT(0) cube complexes. On the other hand, in Chalopin and Chepoi (2018) we\nproved that Thiagarajan's conjecture is true for regular event structures whose\ndomains are principal filters of universal covers of (virtually) finite special\ncube complexes.\n  In the current paper, we prove the converse: to any finite 1-safe Petri net\n$N$ one can associate a finite special cube complex ${X}_N$ such that the\ndomain of the event structure $\\mathcal{E}_N$ (obtained as the unfolding of\n$N$) is a principal filter of the universal cover $\\widetilde{X}_N$ of $X_N$.\nThis establishes a bijection between 1-safe Petri nets and finite special cube\ncomplexes and provides a combinatorial characterization of trace regular event\nstructures.\n  Using this bijection and techniques from graph theory and geometry (MSO\ntheory of graphs, bounded treewidth, and bounded hyperbolicity) we disprove yet\nanother conjecture by Thiagarajan (from the paper with S. Yang from 2014) that\nthe monadic second order logic of a 1-safe Petri net is decidable if and only\nif its unfolding is grid-free.\n  Our counterexample is the trace regular event structure $\\mathcal{\\dot E}_Z$\nwhich arises from a virtually special square complex $\\dot Z$. The domain of\n$\\mathcal{\\dot E}_Z$ is grid-free (because it is hyperbolic), but the MSO\ntheory of the event structure $\\mathcal{\\dot E}_Z$ is undecidable. \n\n"}
{"id": "1810.04173", "contents": "Title: The Impact of Radio AGN Bubble Composition on the Dynamics and Thermal\n  Balance of the Intracluster Medium Abstract: Feeding and feedback of active galactic nuclei (AGN) are critical for\nunderstanding the dynamics and thermodynamics of the intracluster medium (ICM)\nwithin the cores of galaxy clusters. While radio bubbles inflated by AGN jets\ncould be dynamically supported by cosmic rays (CRs), the impact of CR-dominated\njets are not well understood. In this work, we perform three-dimensional\nsimulations of CR-jet feedback in an isolated cluster atmosphere; we find that\nCR jets impact the multiphase gas differently than jets dominated by kinetic\nenergy. In particular, CR bubbles can more efficiently uplift the cluster gas\nand cause an outward expansion of the hot ICM. Due to adiabatic cooling from\nthe expansion and less efficient heating from CR bubbles by direct mixing, the\nICM is more prone to local thermal instabilities, which will later enhance\nchaotic cold accretion onto the AGN. The amount of cold gas formed during the\nbubble formation and its late-time evolution sensitively depend on whether CR\ntransport processes are included or not. We also find that low-level, subsonic\ndriving of turbulence by AGN jets holds for both kinetic and CR jets;\nnevertheless, the kinematics is consistent with the Hitomi measurements.\nFinally, we carefully discuss the key observable signatures of each bubble\nmodel, focusing on gamma-ray emission (and related comparison with Fermi), as\nwell as thermal Sunyaev-Zel'dovich constraints. \n\n"}
{"id": "1810.04176", "contents": "Title: r-process enrichment of ultra-faint dwarf galaxies by fast merging\n  double neutron stars Abstract: The recent aLIGO/aVirgo discovery of gravitational waves from the neutron\nstar merger (NSM) GW170817 and the follow up kilonova observations have shown\nthat NSMs produce copious amount of r-process material. However, it is\ndifficult to reconcile the large natal kicks and long average merging times of\nDouble Neutron Stars (DNSs), with the levels of r-process enrichment seen in\nultra faint dwarf (UFD) galaxies such as Reticulum II and Tucana III. Assuming\nthat such dwarf systems have lost a significant fraction of their stellar mass\nthrough tidal stripping, we conclude that contrary to most current models, it\nis the DNSs with rather large natal kicks but very short merging timescales\nthat can enrich UFD-type galaxies. These binaries are either on highly\neccentric orbits, or form with very short separations due to an additional\nmass-transfer between the first-born neutron star and a naked helium star,\nprogenitor of the second-born neutron star. These DNSs are born with a\nfrequency that agrees with the statistics of the r-process UFDs, and merge well\nwithin the virial radius of their host halos, therefore contributing\nsignificantly to their r-process enrichment. \n\n"}
{"id": "1810.04620", "contents": "Title: Parameterized Complexity of Independent Set in H-Free Graphs Abstract: In this paper, we investigate the complexity of Maximum Independent Set (MIS)\nin the class of $H$-free graphs, that is, graphs excluding a fixed graph as an\ninduced subgraph. Given that the problem remains $NP$-hard for most graphs $H$,\nwe study its fixed-parameter tractability and make progress towards a dichotomy\nbetween $FPT$ and $W[1]$-hard cases. We first show that MIS remains $W[1]$-hard\nin graphs forbidding simultaneously $K_{1, 4}$, any finite set of cycles of\nlength at least $4$, and any finite set of trees with at least two branching\nvertices. In particular, this answers an open question of Dabrowski et al.\nconcerning $C_4$-free graphs. Then we extend the polynomial algorithm of\nAlekseev when $H$ is a disjoint union of edges to an $FPT$ algorithm when $H$\nis a disjoint union of cliques. We also provide a framework for solving several\nother cases, which is a generalization of the concept of \\emph{iterative\nexpansion} accompanied by the extraction of a particular structure using\nRamsey's theorem. Iterative expansion is a maximization version of the\nso-called \\emph{iterative compression}. We believe that our framework can be of\nindependent interest for solving other similar graph problems. Finally, we\npresent positive and negative results on the existence of polynomial (Turing)\nkernels for several graphs $H$. \n\n"}
{"id": "1810.06625", "contents": "Title: Parameterized Dynamic Cluster Editing Abstract: We introduce a dynamic version of the NP-hard graph problem Cluster Editing.\nThe essential point here is to take into account dynamically evolving input\ngraphs: Having a cluster graph (that is, a disjoint union of cliques) that\nrepresents a solution for the first input graph, can we cost-efficiently\ntransform it into a \"similar\" cluster graph that is a solution for the second\n(\"subsequent\") input graph? This model is motivated by several application\nscenarios, including incremental clustering, the search for compromise\nclusterings, or also local search in graph-based data clustering. We thoroughly\nstudy six problem variants (edge editing, edge deletion, edge insertion; each\ncombined with two distance measures between cluster graphs). We obtain both\nfixed-parameter tractability as well as (parameterized) hardness results, thus\n(except for three open questions) providing a fairly complete picture of the\nparameterized computational complexity landscape under the two perhaps most\nnatural parameterizations: the distance of the new \"similar\" cluster graph to\n(i) the second input graph and to (ii) the input cluster graph. \n\n"}
{"id": "1810.08498", "contents": "Title: Network Classification Based Structural Analysis of Real Networks and\n  their Model-Generated Counterparts Abstract: Data-driven analysis of complex networks has been in the focus of research\nfor decades. An important area of research is to study how well real networks\ncan be described with a small selection of metrics, furthermore how well\nnetwork models can capture the relations between graph metrics observed in real\nnetworks. In this paper, we apply machine learning techniques to investigate\nthe aforementioned problems. We study 500 real-world networks along with 2,000\nsynthetic networks generated by four frequently used network models with\npreviously calibrated parameters to make the generated graphs as similar to the\nreal networks as possible. This paper unifies several branches of data-driven\ncomplex network analysis, such as the study of graph metrics and their\npair-wise relationships, network similarity estimation, model calibration, and\ngraph classification. We find that the correlation profiles of the structural\nmeasures significantly differ across network domains and the domain can be\nefficiently determined using a small selection of graph metrics. The structural\nproperties of the network models with fixed parameters are robust enough to\nperform parameter calibration. The goodness-of-fit of the network models highly\ndepends on the network domain. By solving classification problems, we find that\nthe models lack the capability of generating a graph with a high clustering\ncoefficient and relatively large diameter simultaneously. On the other hand,\nmodels are able to capture exactly the degree-distribution-related metrics. \n\n"}
{"id": "1810.11051", "contents": "Title: A Hydrogen-Poor Superluminous Supernova with Enhanced Iron-Group\n  Absorption: A New Link Between SLSNe and Broad-Lined Type Ic SNe Abstract: We present optical observations of the Type I superluminous supernova\n(SLSN-I) SN2017dwh at $z\\!\\approx\\!0.13$, which reached $M_{i}\\!\\approx\\!-21$\nmag at peak. Spectra taken a few days after peak show an unusual and strong\nabsorption line centered near 3200\\AA\\ that we identify with Co II, suggesting\na high fraction of synthesized $^{56}$Ni in the ejecta. By $\\sim\\!1$ month\nafter peak, SN2017dwh became much redder than other SLSNe-I, instead strongly\nresembling broad-lined Type Ic supernovae (Ic-BL SNe) with clear suppression of\nthe flux redward of $\\sim\\!5000$ \\AA, providing further evidence for a large\nmass of Fe-group elements. Late-time upper limits indicate a $^{56}$Ni mass of\n$\\lesssim 0.6$ M$_\\odot$, leaving open the possibility that SN2017dwh produced\na $^{56}$Ni mass comparable to SN1998bw ($\\approx\\!0.4$ M$_\\odot$). Fitting the\nlight curve with a combined magnetar and $^{56}$Ni model using ${\\tt MOSFiT}$,\nwe find that the light curve can easily accommodate such masses without\naffecting the inferred magnetar parameters. We also find that SN2017dwh\noccurred in the least-luminous detected host galaxy to date for a SLSN-I, with\n$M_{B} = -13.5$ mag and an implied metallicity of $Z\\!\\sim\\!0.08$ $Z_\\odot$.\nThe spectral properties of SN2017dwh provide new evidence linking SLSNe-I with\nType Ic-BL SNe, and in particular the high Fe-group abundance may be due to\nenhanced $^{56}$Ni production or mixing due to asphericity. Finally, we find\nthat SN2017dwh represents the most extreme end of a correlation between\ncontinuum shape and Co II absorption strength in the near-peak spectra of\nSLSNe-I, indicating that Fe-group abundance likely accounts for some of the\nvariation in their spectral shapes. \n\n"}
{"id": "1810.11896", "contents": "Title: Smoothed Analysis of Discrete Tensor Decomposition and Assemblies of\n  Neurons Abstract: We analyze linear independence of rank one tensors produced by tensor powers\nof randomly perturbed vectors. This enables efficient decomposition of sums of\nhigh-order tensors. Our analysis builds upon [BCMV14] but allows for a wider\nrange of perturbation models, including discrete ones. We give an application\nto recovering assemblies of neurons.\n  Assemblies are large sets of neurons representing specific memories or\nconcepts. The size of the intersection of two assemblies has been shown in\nexperiments to represent the extent to which these memories co-occur or these\nconcepts are related; the phenomenon is called association of assemblies. This\nsuggests that an animal's memory is a complex web of associations, and poses\nthe problem of recovering this representation from cognitive data. Motivated by\nthis problem, we study the following more general question: Can we reconstruct\nthe Venn diagram of a family of sets, given the sizes of their $\\ell$-wise\nintersections? We show that as long as the family of sets is randomly\nperturbed, it is enough for the number of measurements to be polynomially\nlarger than the number of nonempty regions of the Venn diagram to fully\nreconstruct the diagram. \n\n"}
{"id": "1810.12980", "contents": "Title: Improved Bounds for Randomly Sampling Colorings via Linear Programming Abstract: A well-known conjecture in computer science and statistical physics is that\nGlauber dynamics on the set of $k$-colorings of a graph $G$ on $n$ vertices\nwith maximum degree $\\Delta$ is rapidly mixing for $k\\ge\\Delta+2$. In FOCS\n1999, Vigoda showed that the flip dynamics (and therefore also Glauber\ndynamics) is rapidly mixing for any $k>\\frac{11}{6}\\Delta$. It turns out that\nthere is a natural barrier at $\\frac{11}{6}$, below which there is no one-step\ncoupling that is contractive with respect to the Hamming metric, even for the\nflip dynamics.\n  We use linear programming and duality arguments to fully characterize the\nobstructions to going beyond $\\frac{11}{6}$. These extremal configurations turn\nout to be quite brittle, and in this paper we use this to give two proofs that\nthe Glauber dynamics is rapidly mixing for any $k\\ge\\left(\\frac{11}{6} -\n\\epsilon_0\\right)\\Delta$ for some absolute constant $\\epsilon_0>0$. This is the\nfirst improvement to Vigoda's result that holds for general graphs. Our first\napproach analyzes a variable-length coupling in which these configurations\nbreak apart with high probability before the coupling terminates, and our other\napproach analyzes a one-step path coupling with a new metric that counts the\nextremal configurations. Additionally, our results extend to list coloring, a\nwidely studied generalization of coloring, where the previously best known\nresults required $k > 2 \\Delta$. \n\n"}
{"id": "1810.13187", "contents": "Title: Non-Empty Bins with Simple Tabulation Hashing Abstract: We consider the hashing of a set $X\\subseteq U$ with $|X|=m$ using a simple\ntabulation hash function $h:U\\to [n]=\\{0,\\dots,n-1\\}$ and analyse the number of\nnon-empty bins, that is, the size of $h(X)$. We show that the expected size of\n$h(X)$ matches that with fully random hashing to within low-order terms. We\nalso provide concentration bounds. The number of non-empty bins is a\nfundamental measure in the balls and bins paradigm, and it is critical in\napplications such as Bloom filters and Filter hashing. For example, normally\nBloom filters are proportioned for a desired low false-positive probability\nassuming fully random hashing (see \\url{en.wikipedia.org/wiki/Bloom_filter}).\nOur results imply that if we implement the hashing with simple tabulation, we\nobtain the same low false-positive probability for any possible input. \n\n"}
{"id": "1811.01442", "contents": "Title: Towards a Zero-One Law for Column Subset Selection Abstract: There are a number of approximation algorithms for NP-hard versions of low\nrank approximation, such as finding a rank-$k$ matrix $B$ minimizing the sum of\nabsolute values of differences to a given $n$-by-$n$ matrix $A$,\n$\\min_{\\textrm{rank-}k~B}\\|A-B\\|_1$, or more generally finding a rank-$k$\nmatrix $B$ which minimizes the sum of $p$-th powers of absolute values of\ndifferences, $\\min_{\\textrm{rank-}k~B}\\|A-B\\|_p^p$. Many of these algorithms\nare linear time columns subset selection algorithms, returning a subset of\n$\\mathrm{poly}(k \\log n)$ columns whose cost is no more than a\n$\\mathrm{poly}(k)$ factor larger than the cost of the best rank-$k$ matrix. The\nabove error measures are special cases of the following general entrywise low\nrank approximation problem: given an arbitrary function $g:\\mathbb{R}\n\\rightarrow \\mathbb{R}_{\\geq 0}$, find a rank-$k$ matrix $B$ which minimizes\n$\\|A-B\\|_g = \\sum_{i,j}g(A_{i,j}-B_{i,j})$. A natural question is which\nfunctions $g$ admit efficient approximation algorithms? Indeed, this is a\ncentral question of recent work studying generalized low rank models. In this\nwork we give approximation algorithms for $\\textit{every}$ function $g$ which\nis approximately monotone and satisfies an approximate triangle inequality, and\nwe show both of these conditions are necessary. Further, our algorithm is\nefficient if the function $g$ admits an efficient approximate regression\nalgorithm. Our approximation algorithms handle functions which are not even\nscale-invariant, such as the Huber loss function, which we show have very\ndifferent structural properties than $\\ell_p$-norms, e.g., one can show the\nlack of scale-invariance causes any column subset selection algorithm to\nprovably require a $\\sqrt{\\log n}$ factor larger number of columns than\n$\\ell_p$-norms; nevertheless we design the first efficient column subset\nselection algorithms for such error measures. \n\n"}
{"id": "1811.01839", "contents": "Title: The inner engine of GeV-radiation-emitting gamma-ray bursts Abstract: We motivate how the most recent progress in the understanding the nature of\nthe GeV radiation in most energetic gamma-ray bursts (GRBs), the binary-driven\nhypernovae (BdHNe), has led to the solution of a forty years unsolved problem\nin relativistic astrophysics: how to extract the rotational energy from a Kerr\nblack hole for powering synchrotron emission and ultra high-energy cosmic rays.\nThe \"inner engine\" is identified in the proper use of a classical solution\nintroduced by Wald in 1974 duly extended to the most extreme conditions found\naround the newborn black hole in a BdHN. The energy extraction process occurs\nin a sequence impulsive processes each accelerating protons to $10^{21}$ eV in\na timescale of $10^{-6}$ s and in presence of an external magnetic field of\n$10^{14}$ G. Specific example is given for a black hole of initial angular\nmomentum $J=0.3\\,M^2$ and mass $M\\approx 3\\,M_\\odot$ leading to the GeV\nradiation of $10^{49}$ erg$\\cdot$s$^{-1}$. The process can energetically\ncontinue for thousands of years. \n\n"}
{"id": "1811.02023", "contents": "Title: Ordered Graph Limits and Their Applications Abstract: The emerging theory of graph limits exhibits an analytic perspective on\ngraphs, showing that many important concepts and tools in graph theory and its\napplications can be described more naturally (and sometimes proved more easily)\nin analytic language. We extend the theory of graph limits to the ordered\nsetting, presenting a limit object for dense vertex-ordered graphs, which we\ncall an orderon. As a special case, this yields limit objects for matrices\nwhose rows and columns are ordered, and for dynamic graphs that expand (via\nvertex insertions) over time. Along the way, we devise an ordered\nlocality-preserving variant of the cut distance between ordered graphs, showing\nthat two graphs are close with respect to this distance if and only if they are\nsimilar in terms of their ordered subgraph frequencies. We show that the space\nof orderons is compact with respect to this distance notion, which is key to a\nsuccessful analysis of combinatorial objects through their limits.\n  We derive several applications of the ordered limit theory in extremal\ncombinatorics, sampling, and property testing in ordered graphs. In particular,\nwe prove a new ordered analogue of the well-known result by Alon and Stav\n[RS\\&A'08] on the furthest graph from a hereditary property; this is the first\nknown result of this type in the ordered setting. Unlike the unordered regime,\nhere the random graph model $G(n, p)$ with an ordering over the vertices is not\nalways asymptotically the furthest from the property for some $p$. However,\nusing our ordered limit theory, we show that random graphs generated by a\nstochastic block model, where the blocks are consecutive in the vertex\nordering, are (approximately) the furthest. Additionally, we describe an\nalternative analytic proof of the ordered graph removal lemma [Alon et al.,\nFOCS'17]. \n\n"}
{"id": "1811.02842", "contents": "Title: X-ray binaries with neutron stars at different accretion stages Abstract: Different accretion regimes onto magnetized NSs in HMXBs are considered:\nwind-fed supersonic (Bondi) regime at high accretion rates $\\dot M\\gtrsim\n4\\times 10^{16}$~[g s$^{-1}$] , subsonic settling regime at lower $\\dot M$ and\nsupercritical disc accretion during Roche lobe overflow. In wind-fed stage, NSs\nin HMXBs reach equilibrium spin periods $P^*$ proportional to binary orbital\nperiod $P_b$. At supercritical accretion stage, the system may appear as a\npulsating ULX. Population synthesis of Galactic HMXBs using standard\nassumptions on the binary evolution and NS formation is presented. Comparison\nof the model $P^*$ -- $P_b$ (the Corbet diagram), $P^*$ -- $L_x$ and $P_b$ --\n$L_x$ distributions with those for the observed HMXBs (including Be X-ray\nbinaries) and pulsating ULXs suggests the importance of the reduction of $P^*$\nin non-circular orbits, explaining the location of Be X-ray binaries in the\nmodel Corbet diagram, and the universal parameters of pulsating ULXs depending\nonly on the NS magnetic fields. \n\n"}
{"id": "1811.03491", "contents": "Title: Degree-$d$ Chow Parameters Robustly Determine Degree-$d$ PTFs (and\n  Algorithmic Applications) Abstract: The degree-$d$ Chow parameters of a Boolean function $f: \\{-1,1\\}^n \\to\n\\mathbb{R}$ are its degree at most $d$ Fourier coefficients. It is well-known\nthat degree-$d$ Chow parameters uniquely characterize degree-$d$ polynomial\nthreshold functions (PTFs) within the space of all bounded functions. In this\npaper, we prove a robust version of this theorem: For $f$ any Boolean\ndegree-$d$ PTF and $g$ any bounded function, if the degree-$d$ Chow parameters\nof $f$ are close to the degree-$d$ Chow parameters of $g$ in $\\ell_2$-norm,\nthen $f$ is close to $g$ in $\\ell_1$-distance. Notably, our bound relating the\ntwo distances is completely independent of the dimension $n$. That is, we show\nthat Boolean degree-$d$ PTFs are {\\em robustly identifiable} from their\ndegree-$d$ Chow parameters. Results of this form had been shown for the $d=1$\ncase~\\cite{OS11:chow, DeDFS14}, but no non-trivial bound was previously known\nfor $d >1$.\n  Our robust identifiability result gives the following algorithmic\napplications: First, we show that Boolean degree-$d$ PTFs can be efficiently\napproximately reconstructed from approximations to their degree-$d$ Chow\nparameters. This immediately implies that degree-$d$ PTFs are efficiently\nlearnable in the uniform distribution $d$-RFA\nmodel~\\cite{BenDavidDichterman:98}. As a byproduct of our approach, we also\nobtain the first low integer-weight approximations of degree-$d$ PTFs, for\n$d>1$. As our second application, our robust identifiability result gives the\nfirst efficient algorithm, with dimension-independent error guarantees, for\nmalicious learning of Boolean degree-$d$ PTFs under the uniform distribution. \n\n"}
{"id": "1811.04460", "contents": "Title: Analysis vs Synthesis - An Investigation of (Co)sparse Signal Models on\n  Graphs Abstract: In this work, we present a theoretical study of signals with sparse\nrepresentations in the vertex domain of a graph, which is primarily motivated\nby the discrepancy arising from respectively adopting a synthesis and analysis\nview of the graph Laplacian matrix. Sparsity on graphs and, in particular, the\ncharacterization of the subspaces of signals which are sparse with respect to\nthe connectivity of the graph, as induced by analysis with a suitable graph\noperator, remains in general an opaque concept which we aim to elucidate. By\nleveraging the theory of cosparsity, we present a novel (co)sparse graph\nLaplacian-based signal model and characterize the underlying (structured)\n(co)sparsity, smoothness and localization of its solution subspaces on\nundirected graphs, while providing more refined statements for special cases\nsuch as circulant graphs. Ultimately, we substantiate fundamental discrepancies\nbetween the cosparse analysis and sparse synthesis models in this structured\nsetting, by demonstrating that the former constitutes a special, constrained\ninstance of the latter. \n\n"}
{"id": "1811.04722", "contents": "Title: On an Annihilation Number Conjecture Abstract: Let $\\alpha(G)$ denote the cardinality of a maximum independent set, while\n$\\mu(G)$ be the size of a maximum matching in the graph $G=\\left(V,E\\right) $.\nIf $\\alpha(G)+\\mu(G)=\\left\\vert V\\right\\vert $, then $G$ is a\nK\\\"onig-Egerv\\'ary graph. If $d_{1}\\leq d_{2}\\leq\\cdots\\leq d_{n}$ is the\ndegree sequence of $G$, then the annihilation number $h\\left(G\\right) $ of $G$\nis the largest integer $k$ such that $\\sum\\limits_{i=1}^{k}d_{i}\\leq\\left\\vert\nE\\right\\vert $ (Pepper 2004, Pepper 2009). A set $A\\subseteq V$ satisfying\n$\\sum \\limits_{a\\in A} deg(a)\\leq\\left\\vert E\\right\\vert $ is an annihilation\nset, if, in addition, $ deg\\left(v\\right) +\\sum\\limits_{a\\in A}\ndeg(a)>\\left\\vert E\\right\\vert $, for every vertex $v\\in V(G)-A$, then $A$ is a\nmaximal annihilation set in $G$.\n  In (Larson & Pepper 2011) it was conjectured that the following assertions\nare equivalent:\n  (i) $\\alpha\\left(G\\right) =h\\left(G\\right) $;\n  (ii) $G$ is a K\\\"onig-Egerv\\'ary graph and every maximum independent set is a\nmaximal annihilating set.\n  In this paper, we prove that the implication \"(i) $\\Longrightarrow$ (ii)\" is\ncorrect, while for the opposite direction we provide a series of generic\ncounterexamples.\n  Keywords: maximum independent set, matching, tree, bipartite graph,\nK\\\"onig-Egerv\\'ary graph, annihilation set, annihilation number. \n\n"}
{"id": "1811.06072", "contents": "Title: Communication-Optimal Distributed Dynamic Graph Clustering Abstract: We consider the problem of clustering graph nodes over large-scale dynamic\ngraphs, such as citation networks, images and web networks, when graph updates\nsuch as node/edge insertions/deletions are observed distributively. We propose\ncommunication-efficient algorithms for two well-established communication\nmodels namely the message passing and the blackboard models. Given a graph with\n$n$ nodes that is observed at $s$ remote sites over time $[1,t]$, the two\nproposed algorithms have communication costs $\\tilde{O}(ns)$ and\n$\\tilde{O}(n+s)$ ($\\tilde{O}$ hides a polylogarithmic factor), almost matching\ntheir lower bounds, $\\Omega(ns)$ and $\\Omega(n+s)$, respectively, in the\nmessage passing and the blackboard models. More importantly, we prove that at\neach time point in $[1,t]$ our algorithms generate clustering quality nearly as\ngood as that of centralizing all updates up to that time and then applying a\nstandard centralized clustering algorithm. We conducted extensive experiments\non both synthetic and real-life datasets which confirmed the communication\nefficiency of our approach over baseline algorithms while achieving comparable\nclustering results. \n\n"}
{"id": "1811.08420", "contents": "Title: The domino problem is undecidable on surface groups Abstract: We show that the domino problem is undecidable on orbit graphs of\nnon-deterministic substitutions which satisfy a technical property. As an\napplication, we prove that the domino problem is undecidable for the\nfundamental group of any closed orientable surface of genus at least 2. \n\n"}
{"id": "1811.09136", "contents": "Title: REPT: A Streaming Algorithm of Approximating Global and Local Triangle\n  Counts in Parallel Abstract: Recently, considerable efforts have been devoted to approximately computing\nthe global and local (i.e., incident to each node) triangle counts of a large\ngraph stream represented as a sequence of edges. Existing approximate triangle\ncounting algorithms rely on sampling techniques to reduce the computational\ncost. However, their estimation errors are significantly determined by the\ncovariance between sampled triangles. Moreover, little attention has been paid\nto developing parallel one-pass streaming algorithms that can be used to fast\nand approximately count triangles on a multi-core machine or a cluster of\nmachines. To solve these problems, we develop a novel parallel method REPT to\nsignificantly reduce the covariance (even completely eliminate the covariance\nfor some cases) between sampled triangles. We theoretically prove that REPT is\nmore accurate than parallelizing existing triangle count estimation algorithms\nin a direct manner. In addition, we also conduct extensive experiments on a\nvariety of real-world graphs, and the results demonstrate that our method REPT\nis several times more accurate than state-of-the-art methods. \n\n"}
{"id": "1811.10125", "contents": "Title: Cartan's Magic Formula for Simplicial Complexes Abstract: Cartan's magic formula L_X = i_X d + d i_X = (d+i_X)^2=D_X^2 relates the\nexterior derivative d, an interior derivative i_X and its Lie derivative L_X.\nWe use this formula to define a finite dimensional vector space of vector\nfields X on a finite abstract simplicial complex G. This space has a Lie\nalgebra structure satisfying L_[X,Y] = L_X L_Y - L_Y L_X as in the continuum.\nAny such vector field X defines a coordinate change on the finite dimensional\nvector space l^2(G) which play the role of translations along the vector field.\nIf i_X^2=0, the relation L_X=D_X^2 with D_X=i_X+d mirrors the Hodge\nfactorization L=D^2, where D=d+d^* we can see f_t = - L_X f defining the flow\nof X as the analogue of the heat equation f_t = - L f and view the Newton type\nequations f'' = -L_X f as the analogue of the wave equation f'' = -L f.\nSimilarly as the wave equation is solved by u(t)=exp(i Dt) u(0) with complex\nvalued u(t)=f(t)-i D^-1 f_t(t), also any second order differential equation f''\n= -L_X f is solved by u(t) = exp(i D_X t) u(0) in l^2(G,C}). If X is supported\non odd forms, the factorization property L_X = D_X^2 extends to the Lie algebra\nand i_[X,Y] remains an inner derivative. If the kernel of L_X on p-forms has\ndimension b_p(X), then the general Euler-Poincare formula holds for every\nparameter field X. Extreme cases are i_X=d^*, where b_k are the usual Betti\nnumbers and X=0, where b_k=f_k(G) are the components of the f-vector of the\nsimplicial complex G. We also note that the McKean-Singer super-symmetry\nextends from L to Lie derivatives. It also holds for L_X on Riemannian\nmanifolds. the non-zero spectrum of L_X on even forms is the same than the\nnon-zero spectrum of L_X on odd forms. We also can deform with D_X' = [B_X,D_X]\nof D_X=d+i_X + b_X, B_X=d_X-d_X^*+i b_X the exterior derivative d governed by\nthe vector field X. \n\n"}
{"id": "1811.11856", "contents": "Title: Efficient Measuring of Congruence on High Dimensional Time Series Abstract: A time series is a sequence of data items; typical examples are streams of\ntemperature measurements, stock ticker data, or gestures recorded with modern\nvirtual reality motion controllers. Quite some research has been devoted to\ncomparing and indexing time series. Especially, when the comparison should not\nbe affected by time warping, the ubiquitous Dynamic Time Warping distance\nfunction ($\\texttt{DTW}$) is one of the most analyzed time series distance\nfunctions. The Dog-Keeper distance ($\\texttt{DK}$) is another example for a\ndistance function on time series which is truely invariant under time warping.\n  For many application scenarios (e.$\\,$g. motion gesture recognition in\nvirtual reality), the invariance under isometric spatial transformations\n(i.$\\,$e. rotation, translation, and mirroring) is as important as the\ninvariance under time warping. Distance functions on time series which are\ninvariant under isometric transformations can be seen as measurements for the\ncongruency of two time series. The congruence distance ($\\texttt{CD}$) is an\nexample for such a distance function. However, it is very hard to compute and\nit is not invariant under time warpings.\n  In this work, we are taking one step towards developing a feasable distance\nfunction which is invariant under isometric spatial transformations and time\nwarping: We develop four approximations for $\\texttt{CD}$. Two of these even\nsatisfy the triangle inequality and can thus be used with metric indexing\nstructures. We show that all approximations serve as a lower bound to\n$\\texttt{CD}$. Our evaluation shows that they achieve remarkable tightness\nwhile providing a speedup of more than two orders of magnitude to the\ncongruence distance. \n\n"}
{"id": "1812.02248", "contents": "Title: Atmospheric Charm, QCD and Neutrino Astronomy Abstract: We present predictions for the prompt-neutrino flux arising from the decay of\ncharmed mesons and baryons produced by the interactions of high-energy cosmic\nrays in the Earth's atmosphere, making use of a QCD approach on the basis of\nthe general-mass variable-flavor-number scheme for the description of charm\nhadroproduction at next-to-leading order, complemented by a consistent set of\nfragmentation functions. This same scheme is used for the description of charm\nhadroproduction at both the Tevatron and the Large Hadron Collider. We compare\nthe theoretical predictions to those already obtained by our and other groups\nwith different theoretical approaches. We provide comparisons with the\nexperimental results obtained by the IceCube Collaboration and we discuss\nimplications for parton distribution functions. \n\n"}
{"id": "1812.02960", "contents": "Title: Proton acceleration in colliding stellar wind binaries Abstract: The interaction between the strong winds in stellar colliding-wind binary\n(CWB) systems produces two shock fronts, delimiting the wind collision region\n(WCR). There, particles are expected to be accelerated mainly via diffusive\nshock acceleration (DSA). We investigate the injection and the acceleration of\nprotons in typical CWB systems by means of Monte Carlo simulations, with both a\ntest-particle approach and a non-linear method modelling a shock locally\nmodified by the backreaction of the accelerated protons. We use\nmagnetohydrodynamic simulations to determine the background plasma in the WCR\nand its vicinity. This allows us to consider particle acceleration at both\nshocks, on either side of the WCR, with a realistic large-scale magnetic field.\nWe highlight the possible effects of particle acceleration on the local shock\nprofiles at the WCR. We include the effect of magnetic field amplification due\nto resonant streaming instability (RSI), and compare results without and with\nthe backreaction of the accelerated protons. In the latter case we find a lower\nflux of the non-thermal proton population, and a considerable magnetic field\namplification. This would significantly increase the synchrotron losses of\nrelativistic electrons accelerated in CWB systems, lowering the maximal energy\nthey can reach and strongly reducing the inverse Compton fluxes. As a result,\n$\\gamma$-rays from CWBs would be predominantly due to the decay of neutral\npions produced in nucleon-nucleon collisions. This might provide a way to\nexplain why, in the vast majority of cases, CWB systems have not been\nidentified as $\\gamma$-ray sources, while they emit synchrotron radiation. \n\n"}
{"id": "1812.03159", "contents": "Title: On completely regular codes of covering radius 1 in the halved\n  hypercubes Abstract: We consider constructions of covering-radius-1 completely regular codes, or,\nequivalently, equitable 2-partitions (regular 2-partitions, perfect\n2-colorings), of halved n-cubes. Keywords: completely regular code, equitable\npartition, regular partition, partition design, perfect coloring, halved\nhypercube. \n\n"}
{"id": "1812.05524", "contents": "Title: A Polynomial Time Algorithm for Maximum Likelihood Estimation of\n  Multivariate Log-concave Densities Abstract: We study the problem of computing the maximum likelihood estimator (MLE) of\nmultivariate log-concave densities. Our main result is the first\ncomputationally efficient algorithm for this problem. In more detail, we give\nan algorithm that, on input a set of $n$ points in $\\mathbb{R}^d$ and an\naccuracy parameter $\\epsilon>0$, it runs in time $\\text{poly}(n, d,\n1/\\epsilon)$, and outputs a log-concave density that with high probability\nmaximizes the log-likelihood up to an additive $\\epsilon$. Our approach relies\non a natural convex optimization formulation of the underlying problem that can\nbe efficiently solved by a projected stochastic subgradient method. The main\nchallenge lies in showing that a stochastic subgradient of our objective\nfunction can be efficiently approximated. To achieve this, we rely on\nstructural results on approximation of log-concave densities and leverage\nclassical algorithmic tools on volume approximation of convex bodies and\nuniform sampling from convex sets. \n\n"}
{"id": "1812.06889", "contents": "Title: Magnetized Current Filaments as a Source of Circularly Polarized Light Abstract: We show that the Weibel or currente filamentation instability can lead to the\nemission of circularly polarized radiation. Using particle-in-cell (PIC)\nsimulations and a radiation post-processing numerical algorithm, we demonstrate\nthat the level of circular polarization increases with the initial plasma\nmagnetization, saturating at ~13% when the magnetization, given by the ratio of\nmagnetic energy density to the electron kinetic energy density, is larger than\n0.05. Furthermore, we show that this effect requires an ion-electron mass ratio\ngreater than unity. These findings, which could also be tested in currently\navailable laboratory conditions, show that the recent observation of circular\npolarization in gamma ray burst afterglows could be attributed to the presence\nof magnetized current filaments driven by the Weibel or the current\nfilamentation instability. \n\n"}
{"id": "1812.08578", "contents": "Title: Observations of X-ray reverberation around black holes Abstract: The X-ray emission from accreting black hole (BH) systems displays strong\nvariability. Short reverberation lags are expected between the primary hard\nX-ray continuum and the reprocessed disc emission. These lags depend on\nlight-travel distances, thus offering the opportunity to map the geometry of\nthe innermost accretion flow. X-ray reverberation lags have been observed in\nseveral BH accreting systems. In radio quiet active galactic nuclei (AGN) these\nlags scale with BH mass and point to a reprocessing region located close to the\nComptonizing X-ray corona. On the other hand, reverberation lags detected in\nthe hard state of some BH X-ray binaries (BHXRB) suggest a different accretion\nflow geometry than in AGN, showing evidence of evolution as a function of\nluminosity. \n\n"}
{"id": "1812.08881", "contents": "Title: Using First Hitting Times to Find Sets that Maximize the Convergence\n  Rate to Consensus Abstract: In a model of communication in a social network described by a simple\nconsensus model, we pose the problem of finding a subset of nodes with given\ncardinality and fixed consensus values that enable the fastest convergence rate\nto equilibrium of the values of the remaining nodes. Given a network topology\nand a subset, called the stubborn nodes, the equilibrium exists and is a convex\nsum of the initial values of the stubborn nodes. The value at a non-stubborn\nnode converges to its consensus value exponentially with a rate constant\ndetermined by the expected first hitting time of a random walker starting at\nthe node and ending at the first stubborn node it visits. In this paper, we\nwill use the sum of the expected first hitting times to the stubborn nodes as\nan objective function for a minimization problem. Its solution is a set with\nthe fastest convergence rate. We present a polynomial time method for obtaining\napproximate solutions of the optimization problem for fixed cardinality less\nthan that of a reference vertex cover. Under the assumption that the transition\nmatrix for the random walk is irreducible and reversible, we also obtain an\nupper bound for the expected first hitting time and therefore an upper bound on\nthe rate of convergence to consensus, using results from the mixing theory of\nMarkov chains \n\n"}
{"id": "1812.09967", "contents": "Title: Sherali--Adams Strikes Back Abstract: Let $G$ be any $n$-vertex graph whose random walk matrix has its nontrivial\neigenvalues bounded in magnitude by $1/\\sqrt{\\Delta}$ (for example, a random\ngraph $G$ of average degree~$\\Theta(\\Delta)$ typically has this property). We\nshow that the $\\exp\\Big(c \\frac{\\log n}{\\log \\Delta}\\Big)$-round Sherali--Adams\nlinear programming hierarchy certifies that the maximum cut in such a~$G$ is at\nmost $50.1\\%$ (in fact, at most $\\tfrac12 + 2^{-\\Omega(c)}$). For example, in\nrandom graphs with $n^{1.01}$ edges, $O(1)$ rounds suffice; in random graphs\nwith $n \\cdot \\text{polylog}(n)$ edges, $n^{O(1/\\log \\log n)} = n^{o(1)}$\nrounds suffice.\n  Our results stand in contrast to the conventional beliefs that linear\nprogramming hierarchies perform poorly for \\maxcut and other CSPs, and that\neigenvalue/SDP methods are needed for effective refutation. Indeed, our results\nimply that constant-round Sherali--Adams can strongly refute random Boolean\n$k$-CSP instances with $n^{\\lceil k/2 \\rceil + \\delta}$ constraints; previously\nthis had only been done with spectral algorithms or the SOS SDP hierarchy. \n\n"}
{"id": "1812.10950", "contents": "Title: Fast Breadth-First Search in Still Less Space Abstract: It is shown that a breadth-first search in a directed or undirected graph\nwith $n$ vertices and $m$ edges can be carried out in $O(n+m)$ time with\n$n\\log_2 3+O((\\log n)^2)$ bits of working memory. \n\n"}
{"id": "1812.11889", "contents": "Title: Hybrid equation of state with pasta phases and third family of compact\n  stars Abstract: The effect of pasta phases on the quark-hadron phase transition is\ninvestigated for a set of relativistic mean-field equations of state for both\nhadron and quark matter. The results of the full numerical solution with pasta\nphases are compared with those of an interpolating construction used in\nprevious works, for which we demonstrate an adequate description of the\nnumerical results. A one-to-one mapping of the free parameter of the\nconstruction to the physical surface tension of the quark-hadron interface is\nobtained for which a fit formula is given. For each pair of quark and hadron\nmatter models the critical value of the surface tension is determined, above\nwhich the phase transition becomes close to the Maxwell construction. This\nresult agrees well with earlier theoretical estimates. The study is extended to\nneutron star matter in beta equilibrium with electrons and muons and is applied\nto investigate the effect of pasta phases on the structure of hybrid compact\nstars and the robustness of a possible third family solution. \n\n"}
{"id": "1901.00717", "contents": "Title: Almost Optimal Distribution-free Junta Testing Abstract: We consider the problem of testing whether an unknown $n$-variable Boolean\nfunction is a $k$-junta in the distribution-free property testing model, where\nthe distance between function is measured with respect to an arbitrary and\nunknown probability distribution over $\\{0,1\\}^n$. Chen, Liu, Servedio, Sheng\nand Xie showed that the distribution-free $k$-junta testing can be performed,\nwith one-sided error, by an adaptive algorithm that makes $\\tilde\nO(k^2)/\\epsilon$ queries. In this paper, we give a simple two-sided error\nadaptive algorithm that makes $\\tilde O(k/\\epsilon)$ queries. \n\n"}
{"id": "1901.00868", "contents": "Title: Observable Features of GW170817 Kilonova Afterglow Abstract: The neutron star merger, GW170817, was followed by an optical-infrared\ntransient (a kilonova) which indicated that a substantial ejection of mass at\ntrans-relativistic velocities occurred during the merger. Modeling of the\nkilonova is able to constrain the kinetic energy of the ejecta and its\ncharacteristic velocity, but not the high-velocity distribution of the ejecta.\nYet, this distribution contains crucial information on the merger dynamics. In\nthis work, we assume a power-law distribution of the form\n$E(>\\beta\\Gamma)\\propto(\\beta\\Gamma)^{-\\alpha}$ for the energy of the kilonova\nejecta and calculate the non-thermal signatures produced by the interaction of\nthe ejecta with the ambient gas. We find that ejecta with minimum velocity\n$\\beta_0\\simeq 0.3$ and energy $E\\sim 10^{51}$ erg, as inferred from kilonova\nmodeling, has a detectable radio, and possibly X-ray, afterglow for a broad\nrange of parameter space. This afterglow component is expected to dominate the\nobserved emission on a timescale of a few years post merger and peak around a\ndecade later. Its light curve can be used to determine properties of the\nkilonova ejecta and in particular the ejecta velocity distribution $\\alpha$,\nthe minimum velocity $\\beta_0$ and its total kinetic energy $E$. We also\npredict that an afterglow rebrightening, that is associated with the kilonova\ncomponent, will be accompanied by a shift of the centroid of the radio source\ntowards the initial position of the explosion. \n\n"}
{"id": "1901.01410", "contents": "Title: Origin of the Heaviest Elements: the Rapid Neutron-Capture Process Abstract: The production of about half of the heavy elements found in nature is\nassigned to a specific astrophysical nucleosynthesis process: the rapid neutron\ncapture process (r-process). Although this idea has been postulated more than\nsix decades ago, the full understanding faces two types of uncertainties/open\nquestions: (a) The nucleosynthesis path in the nuclear chart runs close to the\nneutron-drip line, where presently only limited experimental information is\navailable, and one has to rely strongly on theoretical predictions for nuclear\nproperties. (b) While for many years the occurrence of the r-process has been\nassociated with supernovae, more recent studies have cast substantial doubts on\nthis environment. Alternative scenarios include the mergers of neutron stars,\nneutron-star black hole mergers, but possibly also rare classes of supernovae\nas well as hypernovae/collapsars with polar jet ejecta and also accretion disk\noutflows related to the collapse of fast rotating massive stars with high\nmagnetic fields. Stellar r-process abundance observations, have provided\ninsights into, and constraints on the frequency of and conditions in the\nresponsible stellar production sites. One of them, neutron star mergers, was\njust identified and related to the Gravitational Wave event GW170817. High\nresolution observations, increasingly more precise due to improved experimental\natomic data, have been particularly important in defining the heavy element\nabundance patterns of the old halo stars, and thus determining the extent, and\nnature, of the earliest nucleosynthesis in our Galaxy. Combining new results\nand important breakthroughs in the related nuclear, atomic and astronomical\nfields of science, this review attempts to provide an answer to the question\n\"How Were the Elements from Iron to Uranium Made?\" (Abridged) \n\n"}
{"id": "1901.03627", "contents": "Title: Destroying Bicolored $P_3$s by Deleting Few Edges Abstract: We introduce and study the Bicolored $P_3$ Deletion problem defined as\nfollows. The input is a graph $G=(V,E)$ where the edge set $E$ is partitioned\ninto a set $E_r$ of red edges and a set $E_b$ of blue edges. The question is\nwhether we can delete at most $k$ edges such that $G$ does not contain a\nbicolored $P_3$ as an induced subgraph. Here, a bicolored $P_3$ is a path on\nthree vertices with one blue and one red edge. We show that Bicolored $P_3$\nDeletion is NP-hard and cannot be solved in $2^{o(|V|+|E|)}$ time on\nbounded-degree graphs if the ETH is true. Then, we show that Bicolored $P_3$\nDeletion is polynomial-time solvable when $G$ does not contain a bicolored\n$K_3$, that is, a triangle with edges of both colors. Moreover, we provide a\npolynomial-time algorithm for the case that $G$ contains no blue $P_3$, red\n$P_3$, blue $K_3$, and red $K_3$. Finally, we show that Bicolored $P_3$\nDeletion can be solved in $ O(1.84^k\\cdot |V| \\cdot |E|)$ time and that it\nadmits a kernel with $ O(k\\Delta\\min(k,\\Delta))$ vertices, where $\\Delta$ is\nthe maximum degree of $G$. \n\n"}
{"id": "1901.04426", "contents": "Title: Extending partial isometries of antipodal graphs Abstract: We prove EPPA (extension property for partial automorphisms) for all\nantipodal classes from Cherlin's list of metrically homogeneous graphs, thereby\nanswering a question of Aranda et al. This paper should be seen as the first\napplication of a new general method for proving EPPA which can bypass the lack\nof an automorphism-preserving completion. It is done by combining the recent\nstrengthening of the Herwig--Lascar theorem by Hubi\\v{c}ka, Ne\\v{s}et\\v{r}il\nand the author with the ideas of the proof of EPPA for two-graphs by Evans et\nal. \n\n"}
{"id": "1901.04583", "contents": "Title: Platoon Forming Algorithms for Intelligent Street Intersections Abstract: We study intersection access control for autonomous vehicles. Platoon forming\nalgorithms, which aim to organize individual vehicles in platoons, are very\npromising. To create those platoons, we slow down vehicles before the actual\narrival at the intersection in such a way that each vehicle can traverse the\nintersection at high speed. This increases the capacity of the intersection\nsignificantly, offering huge potential savings with respect to travel time\ncompared to nowadays traffic.\n  We propose several new platoon forming algorithms and provide an approximate\nmean delay analysis for our algorithms. A comparison between the current day\npractice at intersections (through a case study in SUMO) and our proposed\nalgorithms is provided. Simulation results for fairness are obtained as well,\nshowing that platoon forming algorithms with a low mean delay sometimes are\nrelatively unfair, indicating a potential need for balancing mean delay and\nfairness. \n\n"}
{"id": "1901.06385", "contents": "Title: The time step constraint in radiation hydrodynamics Abstract: Explicit radiation hydrodynamic simulations of the atmospheres of massive\nstars and of convection in accretion discs around white dwarfs suffer from\nprohibitively short time steps due to radiation. This constraint is related to\nthe cooling time rather than the radiative pressure, which also becomes\nimportant in hot stars and discs. We show that the radiative time step\nconstraint is governed by the minimum of the sum of the optically thick and\nthin contributions rather than the smaller one of the two. In simulations with\nthe Pencil Code, their weighting fractions are found empirically. In\nthree-dimensional convective accretion disc simulations, the Deardorff term is\nfound to be the main contributor to the enthalpy flux rather than the\nsuperadiabatic gradient. We conclude with a discussion of how the radiative\ntime step problem could be mitigated in certain types of investigations. \n\n"}
{"id": "1901.06482", "contents": "Title: On Efficient Optimal Transport: An Analysis of Greedy and Accelerated\n  Mirror Descent Algorithms Abstract: We provide theoretical analyses for two algorithms that solve the regularized\noptimal transport (OT) problem between two discrete probability measures with\nat most $n$ atoms. We show that a greedy variant of the classical Sinkhorn\nalgorithm, known as the \\emph{Greenkhorn algorithm}, can be improved to\n$\\widetilde{\\mathcal{O}}(n^2\\varepsilon^{-2})$, improving on the best known\ncomplexity bound of $\\widetilde{\\mathcal{O}}(n^2\\varepsilon^{-3})$. Notably,\nthis matches the best known complexity bound for the Sinkhorn algorithm and\nhelps explain why the Greenkhorn algorithm can outperform the Sinkhorn\nalgorithm in practice. Our proof technique, which is based on a primal-dual\nformulation and a novel upper bound for the dual solution, also leads to a new\nclass of algorithms that we refer to as \\emph{adaptive primal-dual accelerated\nmirror descent} (APDAMD) algorithms. We prove that the complexity of these\nalgorithms is $\\widetilde{\\mathcal{O}}(n^2\\sqrt{\\delta}\\varepsilon^{-1})$,\nwhere $\\delta > 0$ refers to the inverse of the strong convexity module of\nBregman divergence with respect to $\\|\\cdot\\|_\\infty$. This implies that the\nAPDAMD algorithm is faster than the Sinkhorn and Greenkhorn algorithms in terms\nof $\\varepsilon$. Experimental results on synthetic and real datasets\ndemonstrate the favorable performance of the Greenkhorn and APDAMD algorithms\nin practice. \n\n"}
{"id": "1901.06731", "contents": "Title: Four Deviations Suffice for Rank 1 Matrices Abstract: We prove a matrix discrepancy bound that strengthens the famous\nKadison-Singer result of Marcus, Spielman, and Srivastava. Consider any\nindependent scalar random variables $\\xi_1, \\ldots, \\xi_n$ with finite support,\ne.g.\n  $\\{ \\pm 1 \\}$ or $\\{ 0,1 \\}$-valued random variables, or some combination\nthereof. Let $u_1, \\dots, u_n \\in \\mathbb{C}^m$ and $$ \\sigma^2 = \\left\\|\n\\sum_{i=1}^n \\text{Var}[ \\xi_i ] (u_i u_i^{*})^2 \\right\\|. $$ Then there exists\na choice of outcomes $\\varepsilon_1,\\ldots,\\varepsilon_n$ in the support of\n$\\xi_1, \\ldots, \\xi_n$ s.t. $$ \\left \\|\\sum_{i=1}^n \\mathbb{E} [ \\xi_i] u_i\nu_i^* - \\sum_{i=1}^n \\varepsilon_i u_i u_i^* \\right \\| \\leq 4 \\sigma. $$ A\nsimple consequence of our result is an improvement of a Lyapunov-type theorem\nof Akemann and Weaver. \n\n"}
{"id": "1901.07361", "contents": "Title: Lower bounds for testing graphical models: colorings and\n  antiferromagnetic Ising models Abstract: We study the identity testing problem in the context of spin systems or\nundirected graphical models, where it takes the following form: given the\nparameter specification of the model $M$ and a sampling oracle for the\ndistribution $\\mu_{\\hat{M}}$ of an unknown model $\\hat{M}$, can we efficiently\ndetermine if the two models $M$ and $\\hat{M}$ are the same? We consider\nidentity testing for both soft-constraint and hard-constraint systems. In\nparticular, we prove hardness results in two prototypical cases, the Ising\nmodel and proper colorings, and explore whether identity testing is any easier\nthan structure learning.\n  For the ferromagnetic (attractive) Ising model, Daskalakis et al. (2018)\npresented a polynomial time algorithm for identity testing. We prove hardness\nresults in the antiferromagnetic (repulsive) setting in the same regime of\nparameters where structure learning is known to require a super-polynomial\nnumber of samples. In particular, for $n$-vertex graphs of maximum degree $d$,\nwe prove that if $|\\beta| d = \\omega(\\log{n})$ (where $\\beta$ is the inverse\ntemperature parameter), then there is no polynomial running time identity\ntesting algorithm unless $RP=NP$. We also establish computational lower bounds\nfor a broader set of parameters under the (randomized) exponential time\nhypothesis. Our proofs utilize insights into the design of gadgets using random\ngraphs in recent works concerning the hardness of approximate counting by Sly\n(2010). In the hard-constraint setting, we present hardness results for\nidentity testing for proper colorings. Our results are based on the presumed\nhardness of #BIS, the problem of (approximately) counting independent sets in\nbipartite graphs. In particular, we prove that identity testing is hard in the\nsame range of parameters where structure learning is known to be hard. \n\n"}
{"id": "1901.09044", "contents": "Title: GW170817$-$the first observed neutron star merger and its kilonova:\n  implications for the astrophysical site of the r-process Abstract: The first neutron star (NS) merger observed by advanced LIGO and Virgo,\nGW170817, and its fireworks of electromagnetic counterparts across the entire\nelectromagnetic spectrum marked the beginning of multi-messenger astronomy and\nastrophysics with gravitational waves. The ultraviolet, optical, and\nnear-infrared emission was consistent with being powered by the radioactive\ndecay of nuclei synthesized in the merger ejecta by the rapid neutron capture\nprocess (r-process). Starting from an outline of the inferred properties of\nthis 'kilonova' emission, I discuss possible astrophysical sites for r-process\nnucleosynthesis in NS mergers, arguing that the heaviest r-process elements\nsynthesized in this event most likely originated in outflows from a post-merger\naccretion disk. I compare the inferred properties of r-process element\nproduction in GW170817 to current observational constraints on galactic heavy\nr-process nucleosynthesis and discuss challenges merger-only models face in\nexplaining the r-process content of our galaxy. Based on the observational\nproperties of GW170817 and recent theoretical progress on r-process\nnucleosynthesis in collapsars, I then show how GW170817 points to collapsars as\nthe dominant source of r-process enrichment in the Milky Way. These rare\ncore-collapse events arguably better satisfy existing constraints and overcome\nproblems related to r-process enrichment in various environments that NS\nmergers face. Finally, I comment on the universality of the r-process and on\nhow variations in light r-process elements can be obtained both in NS mergers\nand collapsars. \n\n"}
{"id": "1901.10084", "contents": "Title: A Parallel Projection Method for Metric Constrained Optimization Abstract: Many clustering applications in machine learning and data mining rely on\nsolving metric-constrained optimization problems. These problems are\ncharacterized by $O(n^3)$ constraints that enforce triangle inequalities on\ndistance variables associated with $n$ objects in a large dataset. Despite its\nusefulness, metric-constrained optimization is challenging in practice due to\nthe cubic number of constraints and the high-memory requirements of standard\noptimization software. Recent work has shown that iterative projection methods\nare able to solve metric-constrained optimization problems on a much larger\nscale than was previously possible, thanks to their comparatively low memory\nrequirement. However, the major limitation of projection methods is their slow\nconvergence rate. In this paper we present a parallel projection method for\nmetric-constrained optimization which allows us to speed up the convergence\nrate in practice. The key to our approach is a new parallel execution schedule\nthat allows us to perform projections at multiple metric constraints\nsimultaneously without any conflicts or locking of variables. We illustrate the\neffectiveness of this execution schedule by implementing and testing a parallel\nprojection method for solving the metric-constrained linear programming\nrelaxation of correlation clustering. We show numerous experimental results on\nproblems involving up to 2.9 trillion constraints. \n\n"}
{"id": "1901.10387", "contents": "Title: Matching is as Easy as the Decision Problem, in the NC Model Abstract: Is matching in NC, i.e., is there a deterministic fast parallel algorithm for\nit? This has been an outstanding open question in TCS for over three decades,\never since the discovery of randomized NC matching algorithms [KUW85, MVV87].\nOver the last five years, the theoretical computer science community has\nlaunched a relentless attack on this question, leading to the discovery of\nseveral powerful ideas. We give what appears to be the culmination of this line\nof work: An NC algorithm for finding a minimum-weight perfect matching in a\ngeneral graph with polynomially bounded edge weights, provided it is given an\noracle for the decision problem. Consequently, for settling the main open\nproblem, it suffices to obtain an NC algorithm for the decision problem. We\nbelieve this new fact has qualitatively changed the nature of this open\nproblem.\n  All known efficient matching algorithms for general graphs follow one of two\napproaches: given by Edmonds [Edm65] and Lov\\'asz [Lov79]. Our oracle-based\nalgorithm follows a new approach and uses many of the ideas discovered in the\nlast five years.\n  The difficulty of obtaining an NC perfect matching algorithm led researchers\nto study matching vis-a-vis clever relaxations of the class NC. In this vein,\nrecently Goldwasser and Grossman [GG15] gave a pseudo-deterministic RNC\nalgorithm for finding a perfect matching in a bipartite graph, i.e., an RNC\nalgorithm with the additional requirement that on the same graph, it should\nreturn the same (i.e., unique) perfect matching for almost all choices of\nrandom bits. A corollary of our reduction is an analogous algorithm for general\ngraphs. \n\n"}
{"id": "cond-mat/0209112", "contents": "Title: Graph equivalence and characterization via a continuous evolution of a\n  physical analog Abstract: A general novel approach mapping discrete, combinatorial, graph-theoretic\nproblems onto ``physical'' models - namely $n$ simplexes in $n-1$ dimensions -\nis applied to the graph equivalence problem. It is shown to solve this long\nstanding problem in polynomial, short, time. \n\n"}
{"id": "cs/0004007", "contents": "Title: Deciding first-order properties of locally tree-decomposable structures Abstract: We introduce the concept of a class of graphs, or more generally, relational\nstructures, being locally tree-decomposable. There are numerous examples of\nlocally tree-decomposable classes, among them the class of planar graphs and\nall classes of bounded valence or of bounded tree-width. We also consider a\nslightly more general concept of a class of structures having bounded local\ntree-width.\n  We show that for each property P of structures that is definable in\nfirst-order logic and for each locally tree-decomposable class C of graphs,\nthere is a linear time algorithm deciding whether a given structure A in C has\nproperty P. For classes C of bounded local tree-width, we show that for every\nk\\ge 1 there is an algorithm that solves the same problem in time\nO(n^{1+(1/k)}) (where n is the cardinality of the input structure). \n\n"}
{"id": "cs/0308044", "contents": "Title: EqRank: A Self-Consistent Equivalence Relation on Graph Vertexes Abstract: A new method of hierarchical clustering of graph vertexes is suggested. In\nthe method, the graph partition is determined with an equivalence relation\nsatisfying a recursive definition stating that vertexes are equivalent if the\nvertexes they point to (or vertexes pointing to them) are equivalent. Iterative\napplication of the partitioning yields a hierarchical clustering of graph\nvertexes. The method is applied to the citation graph of hep-th. The outcome is\na two-level classification scheme for the subject field presented in hep-th,\nand indexing of the papers from hep-th in this scheme. A number of tests show\nthat the classification obtained is adequate. \n\n"}
{"id": "cs/0502041", "contents": "Title: Logarithmic Lower Bounds in the Cell-Probe Model Abstract: We develop a new technique for proving cell-probe lower bounds on dynamic\ndata structures. This technique enables us to prove an amortized randomized\nOmega(lg n) lower bound per operation for several data structural problems on n\nelements, including partial sums, dynamic connectivity among disjoint paths (or\na forest or a graph), and several other dynamic graph problems (by simple\nreductions). Such a lower bound breaks a long-standing barrier of Omega(lg n /\nlglg n) for any dynamic language membership problem. It also establishes the\noptimality of several existing data structures, such as Sleator and Tarjan's\ndynamic trees. We also prove the first Omega(log_B n) lower bound in the\nexternal-memory model without assumptions on the data structure (such as the\ncomparison model). Our lower bounds also give a query-update trade-off curve\nmatched, e.g., by several data structures for dynamic connectivity in graphs.\nWe also prove matching upper and lower bounds for partial sums when\nparameterized by the word size and the maximum additive change in an update. \n\n"}
{"id": "cs/0601048", "contents": "Title: Permutation Polynomial Interleavers: An Algebraic-Geometric Perspective Abstract: An interleaver is a critical component for the channel coding\n  performance of turbo codes. Algebraic constructions are\n  important because they admit analytical designs and\n  simple, practical hardware implementation. The spread factor of an\n  interleaver is a common measure for turbo coding\n  applications. Maximum-spread interleavers are interleavers whose\n  spread factors achieve the upper bound. An infinite sequence of\n  quadratic permutation polynomials over integer rings that generate\n  maximum-spread interleavers is presented. New properties of\n  permutation polynomial interleavers are investigated from an\n  algebraic-geometric perspective resulting in a new non-linearity metric\n  for interleavers. A new interleaver metric that is a function of both\n  the non-linearity metric and the spread factor is proposed.\n  It is numerically demonstrated that the spread factor has a\n  diminishing importance with the block length. A table of good\n  interleavers for a variety of interleaver lengths according to the\n  new metric is listed. Extensive computer simulation results with impressive\n  frame error rates confirm the efficacy of the new metric. Further,\n  when tail-biting constituent codes are used, the resulting turbo\n  codes are quasi-cyclic. \n\n"}
{"id": "cs/0612001", "contents": "Title: Polynomial Time Symmetry and Isomorphism Testing for Connected Graphs Abstract: We use the concept of a Kirchhoff resistor network (alternatively random walk\non a network) to probe connected graphs and produce symmetry revealing\ncanonical labelings of the graph(s) nodes and edges. \n\n"}
{"id": "cs/0612060", "contents": "Title: The Common Prefix Problem On Trees Abstract: We present a theoretical study of a problem arising in database query\noptimization, which we call as The Common Prefix Problem. We present a\n$(1-o(1))$ factor approximation algorithm for this problem, when the underlying\ngraph is a binary tree. We then use a result of Feige and Kogan to show that\neven on stars, the problem is hard to approximate. \n\n"}
{"id": "cs/0703145", "contents": "Title: The Simultaneous Triple Product Property and Group-theoretic Results for\n  the Exponent of Matrix Multiplication Abstract: We describe certain special consequences of certain elementary methods from\ngroup theory for studying the algebraic complexity of matrix multiplication, as\ndeveloped by H. Cohn, C. Umans et. al. in 2003 and 2005. The measure of\ncomplexity here is the exponent of matrix multiplication, a real parameter\nbetween 2 and 3, which has been conjectured to be 2. More specifically, a\nfinite group may simultaneously \"realize\" several independent matrix\nmultiplications via its regular algebra if it has a family of triples of\n\"index\" subsets which satisfy the so-called simultaneous triple product\nproperty (STPP), in which case the complexity of these several multiplications\ndoes not exceed the rank (complexity) of the algebra. This leads to bounds for\nthe exponent in terms of the size of the group and the sizes of its STPP\ntriples, as well as the dimensions of its distinct irreducible representations.\nWreath products of Abelian with symmetric groups appear especially important,\nin this regard, and we give an example of such a group which shows that the\nexponent is less than 2.84, and could be possibly be as small as 2.02 depending\non the number of simultaneous matrix multiplications it realizes. \n\n"}
{"id": "math/0509004", "contents": "Title: Counting unlabelled toroidal graphs with no K33-subdivisions Abstract: We provide a description of unlabelled enumeration techniques, with complete\nproofs, for graphs that can be canonically obtained by substituting 2-pole\nnetworks for the edges of core graphs. Using structure theorems for toroidal\nand projective-planar graphs containing no K33-subdivisions, we apply these\ntechniques to obtain their unlabelled enumeration. \n\n"}
{"id": "math/0703575", "contents": "Title: Convex Discrete Optimization Abstract: We develop an algorithmic theory of convex optimization over discrete sets.\nUsing a combination of algebraic and geometric tools we are able to provide\npolynomial time algorithms for solving broad classes of convex combinatorial\noptimization problems and convex integer programming problems in variable\ndimension. We discuss some of the many applications of this theory including to\nquadratic programming, matroids, bin packing and cutting-stock problems, vector\npartitioning and clustering, multiway transportation problems, and privacy and\nconfidential statistical data disclosure. Highlights of our work include a\nstrongly polynomial time algorithm for convex and linear combinatorial\noptimization over any family presented by a membership oracle when the\nunderlying polytope has few edge-directions; a new theory of so-termed n-fold\ninteger programming, yielding polynomial time solution of important and natural\nclasses of convex and linear integer programming problems in variable\ndimension; and a complete complexity classification of high dimensional\ntransportation problems, with practical applications to fundamental problems in\nprivacy and confidential statistical data disclosure. \n\n"}
{"id": "physics/0012026", "contents": "Title: Charged Brown Particle: The More Retardation is - the Lower is the\n  Effective Temperature Abstract: The Brownian motion of a charged particle with finite size (described by\nSommerfeld model) is considered. It is found out that due to radiation\nreaction: (1) the effective temperature of such particle is lower, and (2) the\nacceleration of the average velocity is smaller, than that for classical Brown\nparticle without electric charge. \n\n"}
{"id": "physics/0702015", "contents": "Title: Size reduction of complex networks preserving modularity Abstract: The ubiquity of modular structure in real-world complex networks is being the\nfocus of attention in many trials to understand the interplay between network\ntopology and functionality. The best approaches to the identification of\nmodular structure are based on the optimization of a quality function known as\nmodularity. However this optimization is a hard task provided that the\ncomputational complexity of the problem is in the NP-hard class. Here we\npropose an exact method for reducing the size of weighted (directed and\nundirected) complex networks while maintaining invariant its modularity. This\nsize reduction allows the heuristic algorithms that optimize modularity for a\nbetter exploration of the modularity landscape. We compare the modularity\nobtained in several real complex-networks by using the Extremal Optimization\nalgorithm, before and after the size reduction, showing the improvement\nobtained. We speculate that the proposed analytical size reduction could be\nextended to an exact coarse graining of the network in the scope of real-space\nrenormalization. \n\n"}
{"id": "quant-ph/0311001", "contents": "Title: Quantum walk algorithm for element distinctness Abstract: We use quantum walks to construct a new quantum algorithm for element\ndistinctness and its generalization. For element distinctness (the problem of\nfinding two equal items among N given items), we get an O(N^{2/3}) query\nquantum algorithm. This improves the previous O(N^{3/4}) query quantum\nalgorithm of Buhrman et.al. (quant-ph/0007016) and matches the lower bound by\nShi (quant-ph/0112086). The algorithm also solves the generalization of element\ndistinctness in which we have to find k equal items among N items. For this\nproblem, we get an O(N^{k/(k+1)}) query quantum algorithm. \n\n"}
