{"id": "0710.2848", "contents": "Title: Consistency of trace norm minimization Abstract: Regularization by the sum of singular values, also referred to as the trace\nnorm, is a popular technique for estimating low rank rectangular matrices. In\nthis paper, we extend some of the consistency results of the Lasso to provide\nnecessary and sufficient conditions for rank consistency of trace norm\nminimization with the square loss. We also provide an adaptive version that is\nrank consistent even when the necessary condition for the non adaptive version\nis not fulfilled. \n\n"}
{"id": "0809.1241", "contents": "Title: A New Framework of Multistage Estimation Abstract: In this paper, we have established a unified framework of multistage\nparameter estimation. We demonstrate that a wide variety of statistical\nproblems such as fixed-sample-size interval estimation, point estimation with\nerror control, bounded-width confidence intervals, interval estimation\nfollowing hypothesis testing, construction of confidence sequences, can be cast\ninto the general framework of constructing sequential random intervals with\nprescribed coverage probabilities. We have developed exact methods for the\nconstruction of such sequential random intervals in the context of multistage\nsampling. In particular, we have established inclusion principle and coverage\ntuning techniques to control and adjust the coverage probabilities of\nsequential random intervals. We have obtained concrete sampling schemes which\nare unprecedentedly efficient in terms of sampling effort as compared to\nexisting procedures. \n\n"}
{"id": "0809.3170", "contents": "Title: A New Framework of Multistage Hypothesis Tests Abstract: In this paper, we have established a general framework of multistage\nhypothesis tests which applies to arbitrarily many mutually exclusive and\nexhaustive composite hypotheses. Within the new framework, we have constructed\nspecific multistage tests which rigorously control the risk of committing\ndecision errors and are more efficient than previous tests in terms of average\nsample number and the number of sampling operations. Without truncation, the\nsample numbers of our testing plans are absolutely bounded. \n\n"}
{"id": "0903.5328", "contents": "Title: A Stochastic View of Optimal Regret through Minimax Duality Abstract: We study the regret of optimal strategies for online convex optimization\ngames. Using von Neumann's minimax theorem, we show that the optimal regret in\nthis adversarial setting is closely related to the behavior of the empirical\nminimization algorithm in a stochastic process setting: it is equal to the\nmaximum, over joint distributions of the adversary's action sequence, of the\ndifference between a sum of minimal expected losses and the minimal empirical\nloss. We show that the optimal regret has a natural geometric interpretation,\nsince it can be viewed as the gap in Jensen's inequality for a concave\nfunctional--the minimizer over the player's actions of expected loss--defined\non a set of probability distributions. We use this expression to obtain upper\nand lower bounds on the regret of an optimal strategy for a variety of online\nlearning problems. Our method provides upper bounds without the need to\nconstruct a learning algorithm; the lower bounds provide explicit optimal\nstrategies for the adversary. \n\n"}
{"id": "0912.5410", "contents": "Title: A survey of statistical network models Abstract: Networks are ubiquitous in science and have become a focal point for\ndiscussion in everyday life. Formal statistical models for the analysis of\nnetwork data have emerged as a major topic of interest in diverse areas of\nstudy, and most of these involve a form of graphical representation.\nProbability models on graphs date back to 1959. Along with empirical studies in\nsocial psychology and sociology from the 1960s, these early works generated an\nactive network community and a substantial literature in the 1970s. This effort\nmoved into the statistical literature in the late 1970s and 1980s, and the past\ndecade has seen a burgeoning network literature in statistical physics and\ncomputer science. The growth of the World Wide Web and the emergence of online\nnetworking communities such as Facebook, MySpace, and LinkedIn, and a host of\nmore specialized professional network communities has intensified interest in\nthe study of networks and network data. Our goal in this review is to provide\nthe reader with an entry point to this burgeoning literature. We begin with an\noverview of the historical development of statistical network modeling and then\nwe introduce a number of examples that have been studied in the network\nliterature. Our subsequent discussion focuses on a number of prominent static\nand dynamic network models and their interconnections. We emphasize formal\nmodel descriptions, and pay special attention to the interpretation of\nparameters and their estimation. We end with a description of some open\nproblems and challenges for machine learning and statistics. \n\n"}
{"id": "1005.2638", "contents": "Title: Hierarchical Clustering for Finding Symmetries and Other Patterns in\n  Massive, High Dimensional Datasets Abstract: Data analysis and data mining are concerned with unsupervised pattern finding\nand structure determination in data sets. \"Structure\" can be understood as\nsymmetry and a range of symmetries are expressed by hierarchy. Such symmetries\ndirectly point to invariants, that pinpoint intrinsic properties of the data\nand of the background empirical domain of interest. We review many aspects of\nhierarchy here, including ultrametric topology, generalized ultrametric,\nlinkages with lattices and other discrete algebraic structures and with p-adic\nnumber representations. By focusing on symmetries in data we have a powerful\nmeans of structuring and analyzing massive, high dimensional data stores. We\nillustrate the powerfulness of hierarchical clustering in case studies in\nchemistry and finance, and we provide pointers to other published case studies. \n\n"}
{"id": "1005.3579", "contents": "Title: Graph-Structured Multi-task Regression and an Efficient Optimization\n  Method for General Fused Lasso Abstract: We consider the problem of learning a structured multi-task regression, where\nthe output consists of multiple responses that are related by a graph and the\ncorrelated response variables are dependent on the common inputs in a sparse\nbut synergistic manner. Previous methods such as l1/l2-regularized multi-task\nregression assume that all of the output variables are equally related to the\ninputs, although in many real-world problems, outputs are related in a complex\nmanner. In this paper, we propose graph-guided fused lasso (GFlasso) for\nstructured multi-task regression that exploits the graph structure over the\noutput variables. We introduce a novel penalty function based on fusion penalty\nto encourage highly correlated outputs to share a common set of relevant\ninputs. In addition, we propose a simple yet efficient proximal-gradient method\nfor optimizing GFlasso that can also be applied to any optimization problems\nwith a convex smooth loss and the general class of fusion penalty defined on\narbitrary graph structures. By exploiting the structure of the non-smooth\n''fusion penalty'', our method achieves a faster convergence rate than the\nstandard first-order method, sub-gradient method, and is significantly more\nscalable than the widely adopted second-order cone-programming and\nquadratic-programming formulations. In addition, we provide an analysis of the\nconsistency property of the GFlasso model. Experimental results not only\ndemonstrate the superiority of GFlasso over the standard lasso but also show\nthe efficiency and scalability of our proximal-gradient method. \n\n"}
{"id": "1006.3033", "contents": "Title: Extension of Wirtinger's Calculus to Reproducing Kernel Hilbert Spaces\n  and the Complex Kernel LMS Abstract: Over the last decade, kernel methods for nonlinear processing have\nsuccessfully been used in the machine learning community. The primary\nmathematical tool employed in these methods is the notion of the Reproducing\nKernel Hilbert Space. However, so far, the emphasis has been on batch\ntechniques. It is only recently, that online techniques have been considered in\nthe context of adaptive signal processing tasks. Moreover, these efforts have\nonly been focussed on real valued data sequences. To the best of our knowledge,\nno adaptive kernel-based strategy has been developed, so far, for complex\nvalued signals. Furthermore, although the real reproducing kernels are used in\nan increasing number of machine learning problems, complex kernels have not,\nyet, been used, in spite of their potential interest in applications that deal\nwith complex signals, with Communications being a typical example. In this\npaper, we present a general framework to attack the problem of adaptive\nfiltering of complex signals, using either real reproducing kernels, taking\nadvantage of a technique called \\textit{complexification} of real RKHSs, or\ncomplex reproducing kernels, highlighting the use of the complex gaussian\nkernel. In order to derive gradients of operators that need to be defined on\nthe associated complex RKHSs, we employ the powerful tool of Wirtinger's\nCalculus, which has recently attracted attention in the signal processing\ncommunity. To this end, in this paper, the notion of Wirtinger's calculus is\nextended, for the first time, to include complex RKHSs and use it to derive\nseveral realizations of the Complex Kernel Least-Mean-Square (CKLMS) algorithm.\nExperiments verify that the CKLMS offers significant performance improvements\nover several linear and nonlinear algorithms, when dealing with nonlinearities. \n\n"}
{"id": "1009.3958", "contents": "Title: Approximate Inference and Stochastic Optimal Control Abstract: We propose a novel reformulation of the stochastic optimal control problem as\nan approximate inference problem, demonstrating, that such a interpretation\nleads to new practical methods for the original problem. In particular we\ncharacterise a novel class of iterative solutions to the stochastic optimal\ncontrol problem based on a natural relaxation of the exact dual formulation.\nThese theoretical insights are applied to the Reinforcement Learning problem\nwhere they lead to new model free, off policy methods for discrete and\ncontinuous problems. \n\n"}
{"id": "1010.3091", "contents": "Title: Near-Optimal Bayesian Active Learning with Noisy Observations Abstract: We tackle the fundamental problem of Bayesian active learning with noise,\nwhere we need to adaptively select from a number of expensive tests in order to\nidentify an unknown hypothesis sampled from a known prior distribution. In the\ncase of noise-free observations, a greedy algorithm called generalized binary\nsearch (GBS) is known to perform near-optimally. We show that if the\nobservations are noisy, perhaps surprisingly, GBS can perform very poorly. We\ndevelop EC2, a novel, greedy active learning algorithm and prove that it is\ncompetitive with the optimal policy, thus obtaining the first competitiveness\nguarantees for Bayesian active learning with noisy observations. Our bounds\nrely on a recently discovered diminishing returns property called adaptive\nsubmodularity, generalizing the classical notion of submodular set functions to\nadaptive policies. Our results hold even if the tests have non-uniform cost and\ntheir noise is correlated. We also propose EffECXtive, a particularly fast\napproximation of EC2, and evaluate it on a Bayesian experimental design problem\ninvolving human subjects, intended to tease apart competing economic theories\nof how people make decisions under uncertainty. \n\n"}
{"id": "1102.0059", "contents": "Title: Statistical methods for tissue array images - algorithmic scoring and\n  co-training Abstract: Recent advances in tissue microarray technology have allowed\nimmunohistochemistry to become a powerful medium-to-high throughput analysis\ntool, particularly for the validation of diagnostic and prognostic biomarkers.\nHowever, as study size grows, the manual evaluation of these assays becomes a\nprohibitive limitation; it vastly reduces throughput and greatly increases\nvariability and expense. We propose an algorithm - Tissue Array Co-Occurrence\nMatrix Analysis (TACOMA) - for quantifying cellular phenotypes based on\ntextural regularity summarized by local inter-pixel relationships. The\nalgorithm can be easily trained for any staining pattern, is absent of\nsensitive tuning parameters and has the ability to report salient pixels in an\nimage that contribute to its score. Pathologists' input via informative\ntraining patches is an important aspect of the algorithm that allows the\ntraining for any specific marker or cell type. With co-training, the error rate\nof TACOMA can be reduced substantially for a very small training sample (e.g.,\nwith size 30). We give theoretical insights into the success of co-training via\nthinning of the feature set in a high-dimensional setting when there is\n\"sufficient\" redundancy among the features. TACOMA is flexible, transparent and\nprovides a scoring process that can be evaluated with clarity and confidence.\nIn a study based on an estrogen receptor (ER) marker, we show that TACOMA is\ncomparable to, or outperforms, pathologists' performance in terms of accuracy\nand repeatability. \n\n"}
{"id": "1102.1027", "contents": "Title: Collective Classification of Textual Documents by Guided\n  Self-Organization in T-Cell Cross-Regulation Dynamics Abstract: We present and study an agent-based model of T-Cell cross-regulation in the\nadaptive immune system, which we apply to binary classification. Our method\nexpands an existing analytical model of T-cell cross-regulation (Carneiro et\nal. in Immunol Rev 216(1):48-68, 2007) that was used to study the\nself-organizing dynamics of a single population of T-Cells in interaction with\nan idealized antigen presenting cell capable of presenting a single antigen.\nWith agent-based modeling we are able to study the self-organizing dynamics of\nmultiple populations of distinct T-cells which interact via antigen presenting\ncells that present hundreds of distinct antigens. Moreover, we show that such\nself-organizing dynamics can be guided to produce an effective binary\nclassification of antigens, which is competitive with existing machine learning\nmethods when applied to biomedical text classification. More specifically, here\nwe test our model on a dataset of publicly available full-text biomedical\narticles provided by the BioCreative challenge (Krallinger in The biocreative\nii. 5 challenge overview, p 19, 2009). We study the robustness of our model's\nparameter configurations, and show that it leads to encouraging results\ncomparable to state-of-the-art classifiers. Our results help us understand both\nT-cell cross-regulation as a general principle of guided self-organization, as\nwell as its applicability to document classification. Therefore, we show that\nour bio-inspired algorithm is a promising novel method for biomedical article\nclassification and for binary document classification in general. \n\n"}
{"id": "1110.0718", "contents": "Title: Directed information and Pearl's causal calculus Abstract: Probabilistic graphical models are a fundamental tool in statistics, machine\nlearning, signal processing, and control. When such a model is defined on a\ndirected acyclic graph (DAG), one can assign a partial ordering to the events\noccurring in the corresponding stochastic system. Based on the work of Judea\nPearl and others, these DAG-based \"causal factorizations\" of joint probability\nmeasures have been used for characterization and inference of functional\ndependencies (causal links). This mostly expository paper focuses on several\nconnections between Pearl's formalism (and in particular his notion of\n\"intervention\") and information-theoretic notions of causality and feedback\n(such as causal conditioning, directed stochastic kernels, and directed\ninformation). As an application, we show how conditional directed information\ncan be used to develop an information-theoretic version of Pearl's \"back-door\"\ncriterion for identifiability of causal effects from passive observations. This\nsuggests that the back-door criterion can be thought of as a causal analog of\nstatistical sufficiency. \n\n"}
{"id": "1203.3783", "contents": "Title: Learning Feature Hierarchies with Centered Deep Boltzmann Machines Abstract: Deep Boltzmann machines are in principle powerful models for extracting the\nhierarchical structure of data. Unfortunately, attempts to train layers jointly\n(without greedy layer-wise pretraining) have been largely unsuccessful. We\npropose a modification of the learning algorithm that initially recenters the\noutput of the activation functions to zero. This modification leads to a better\nconditioned Hessian and thus makes learning easier. We test the algorithm on\nreal data and demonstrate that our suggestion, the centered deep Boltzmann\nmachine, learns a hierarchy of increasingly abstract representations and a\nbetter generative model of data. \n\n"}
{"id": "1203.4523", "contents": "Title: On the Equivalence between Herding and Conditional Gradient Algorithms Abstract: We show that the herding procedure of Welling (2009) takes exactly the form\nof a standard convex optimization algorithm--namely a conditional gradient\nalgorithm minimizing a quadratic moment discrepancy. This link enables us to\ninvoke convergence results from convex optimization and to consider faster\nalternatives for the task of approximating integrals in a reproducing kernel\nHilbert space. We study the behavior of the different variants through\nnumerical simulations. The experiments indicate that while we can improve over\nherding on the task of approximating integrals, the original herding algorithm\ntends to approach more often the maximum entropy distribution, shedding more\nlight on the learning bias behind herding. \n\n"}
{"id": "1204.0136", "contents": "Title: Near-Optimal Algorithms for Online Matrix Prediction Abstract: In several online prediction problems of recent interest the comparison class\nis composed of matrices with bounded entries. For example, in the online\nmax-cut problem, the comparison class is matrices which represent cuts of a\ngiven graph and in online gambling the comparison class is matrices which\nrepresent permutations over n teams. Another important example is online\ncollaborative filtering in which a widely used comparison class is the set of\nmatrices with a small trace norm. In this paper we isolate a property of\nmatrices, which we call (beta,tau)-decomposability, and derive an efficient\nonline learning algorithm, that enjoys a regret bound of O*(sqrt(beta tau T))\nfor all problems in which the comparison class is composed of\n(beta,tau)-decomposable matrices. By analyzing the decomposability of cut\nmatrices, triangular matrices, and low trace-norm matrices, we derive near\noptimal regret bounds for online max-cut, online gambling, and online\ncollaborative filtering. In particular, this resolves (in the affirmative) an\nopen problem posed by Abernethy (2010); Kleinberg et al (2010). Finally, we\nderive lower bounds for the three problems and show that our upper bounds are\noptimal up to logarithmic factors. In particular, our lower bound for the\nonline collaborative filtering problem resolves another open problem posed by\nShamir and Srebro (2011). \n\n"}
{"id": "1205.4349", "contents": "Title: From Exact Learning to Computing Boolean Functions and Back Again Abstract: The goal of the paper is to relate complexity measures associated with the\nevaluation of Boolean functions (certificate complexity, decision tree\ncomplexity) and learning dimensions used to characterize exact learning\n(teaching dimension, extended teaching dimension). The high level motivation is\nto discover non-trivial relations between exact learning of an unknown concept\nand testing whether an unknown concept is part of a concept class or not.\nConcretely, the goal is to provide lower and upper bounds of complexity\nmeasures for one problem type in terms of the other. \n\n"}
{"id": "1207.6430", "contents": "Title: Optimal Data Collection For Informative Rankings Expose Well-Connected\n  Graphs Abstract: Given a graph where vertices represent alternatives and arcs represent\npairwise comparison data, the statistical ranking problem is to find a\npotential function, defined on the vertices, such that the gradient of the\npotential function agrees with the pairwise comparisons. Our goal in this paper\nis to develop a method for collecting data for which the least squares\nestimator for the ranking problem has maximal Fisher information. Our approach,\nbased on experimental design, is to view data collection as a bi-level\noptimization problem where the inner problem is the ranking problem and the\nouter problem is to identify data which maximizes the informativeness of the\nranking. Under certain assumptions, the data collection problem decouples,\nreducing to a problem of finding multigraphs with large algebraic connectivity.\nThis reduction of the data collection problem to graph-theoretic questions is\none of the primary contributions of this work. As an application, we study the\nYahoo! Movie user rating dataset and demonstrate that the addition of a small\nnumber of well-chosen pairwise comparisons can significantly increase the\nFisher informativeness of the ranking. As another application, we study the\n2011-12 NCAA football schedule and propose schedules with the same number of\ngames which are significantly more informative. Using spectral clustering\nmethods to identify highly-connected communities within the division, we argue\nthat the NCAA could improve its notoriously poor rankings by simply scheduling\nmore out-of-conference games. \n\n"}
{"id": "1208.1056", "contents": "Title: Sequential Estimation Methods from Inclusion Principle Abstract: In this paper, we propose new sequential estimation methods based on\ninclusion principle. The main idea is to reformulate the estimation problems as\nconstructing sequential random intervals and use confidence sequences to\ncontrol the associated coverage probabilities. In contrast to existing\nasymptotic sequential methods, our estimation procedures rigorously guarantee\nthe pre-specified levels of confidence. \n\n"}
{"id": "1208.2873", "contents": "Title: Detecting Events and Patterns in Large-Scale User Generated Textual\n  Streams with Statistical Learning Methods Abstract: A vast amount of textual web streams is influenced by events or phenomena\nemerging in the real world. The social web forms an excellent modern paradigm,\nwhere unstructured user generated content is published on a regular basis and\nin most occasions is freely distributed. The present Ph.D. Thesis deals with\nthe problem of inferring information - or patterns in general - about events\nemerging in real life based on the contents of this textual stream. We show\nthat it is possible to extract valuable information about social phenomena,\nsuch as an epidemic or even rainfall rates, by automatic analysis of the\ncontent published in Social Media, and in particular Twitter, using Statistical\nMachine Learning methods. An important intermediate task regards the formation\nand identification of features which characterise a target event; we select and\nuse those textual features in several linear, non-linear and hybrid inference\napproaches achieving a significantly good performance in terms of the applied\nloss function. By examining further this rich data set, we also propose methods\nfor extracting various types of mood signals revealing how affective norms - at\nleast within the social web's population - evolve during the day and how\nsignificant events emerging in the real world are influencing them. Lastly, we\npresent some preliminary findings showing several spatiotemporal\ncharacteristics of this textual information as well as the potential of using\nit to tackle tasks such as the prediction of voting intentions. \n\n"}
{"id": "1208.3728", "contents": "Title: Online Learning with Predictable Sequences Abstract: We present methods for online linear optimization that take advantage of\nbenign (as opposed to worst-case) sequences. Specifically if the sequence\nencountered by the learner is described well by a known \"predictable process\",\nthe algorithms presented enjoy tighter bounds as compared to the typical worst\ncase bounds. Additionally, the methods achieve the usual worst-case regret\nbounds if the sequence is not benign. Our approach can be seen as a way of\nadding prior knowledge about the sequence within the paradigm of online\nlearning. The setting is shown to encompass partial and side information.\nVariance and path-length bounds can be seen as particular examples of online\nlearning with simple predictable sequences.\n  We further extend our methods and results to include competing with a set of\npossible predictable processes (models), that is \"learning\" the predictable\nprocess itself concurrently with using it to obtain better regret guarantees.\nWe show that such model selection is possible under various assumptions on the\navailable feedback. Our results suggest a promising direction of further\nresearch with potential applications to stock market and time series\nprediction. \n\n"}
{"id": "1208.5062", "contents": "Title: Changepoint detection for high-dimensional time series with missing data Abstract: This paper describes a novel approach to change-point detection when the\nobserved high-dimensional data may have missing elements. The performance of\nclassical methods for change-point detection typically scales poorly with the\ndimensionality of the data, so that a large number of observations are\ncollected after the true change-point before it can be reliably detected.\nFurthermore, missing components in the observed data handicap conventional\napproaches. The proposed method addresses these challenges by modeling the\ndynamic distribution underlying the data as lying close to a time-varying\nlow-dimensional submanifold embedded within the ambient observation space.\nSpecifically, streaming data is used to track a submanifold approximation,\nmeasure deviations from this approximation, and calculate a series of\nstatistics of the deviations for detecting when the underlying manifold has\nchanged in a sharp or unexpected manner. The approach described in this paper\nleverages several recent results in the field of high-dimensional data\nanalysis, including subspace tracking with missing data, multiscale analysis\ntechniques for point clouds, online optimization, and change-point detection\nperformance analysis. Simulations and experiments highlight the robustness and\nefficacy of the proposed approach in detecting an abrupt change in an otherwise\nslowly varying low-dimensional manifold. \n\n"}
{"id": "1211.7012", "contents": "Title: Learning-Assisted Automated Reasoning with Flyspeck Abstract: The considerable mathematical knowledge encoded by the Flyspeck project is\ncombined with external automated theorem provers (ATPs) and machine-learning\npremise selection methods trained on the proofs, producing an AI system capable\nof answering a wide range of mathematical queries automatically. The\nperformance of this architecture is evaluated in a bootstrapping scenario\nemulating the development of Flyspeck from axioms to the last theorem, each\ntime using only the previous theorems and proofs. It is shown that 39% of the\n14185 theorems could be proved in a push-button mode (without any high-level\nadvice and user interaction) in 30 seconds of real time on a fourteen-CPU\nworkstation. The necessary work involves: (i) an implementation of sound\ntranslations of the HOL Light logic to ATP formalisms: untyped first-order,\npolymorphic typed first-order, and typed higher-order, (ii) export of the\ndependency information from HOL Light and ATP proofs for the machine learners,\nand (iii) choice of suitable representations and methods for learning from\nprevious proofs, and their integration as advisors with HOL Light. This work is\ndescribed and discussed here, and an initial analysis of the body of proofs\nthat were found fully automatically is provided. \n\n"}
{"id": "1212.2287", "contents": "Title: Runtime Optimizations for Prediction with Tree-Based Models Abstract: Tree-based models have proven to be an effective solution for web ranking as\nwell as other problems in diverse domains. This paper focuses on optimizing the\nruntime performance of applying such models to make predictions, given an\nalready-trained model. Although exceedingly simple conceptually, most\nimplementations of tree-based models do not efficiently utilize modern\nsuperscalar processor architectures. By laying out data structures in memory in\na more cache-conscious fashion, removing branches from the execution flow using\na technique called predication, and micro-batching predictions using a\ntechnique called vectorization, we are able to better exploit modern processor\narchitectures and significantly improve the speed of tree-based models over\nhard-coded if-else blocks. Our work contributes to the exploration of\narchitecture-conscious runtime implementations of machine learning algorithms. \n\n"}
{"id": "1212.3276", "contents": "Title: Learning Sparse Low-Threshold Linear Classifiers Abstract: We consider the problem of learning a non-negative linear classifier with a\n$1$-norm of at most $k$, and a fixed threshold, under the hinge-loss. This\nproblem generalizes the problem of learning a $k$-monotone disjunction. We\nprove that we can learn efficiently in this setting, at a rate which is linear\nin both $k$ and the size of the threshold, and that this is the best possible\nrate. We provide an efficient online learning algorithm that achieves the\noptimal rate, and show that in the batch case, empirical risk minimization\nachieves this rate as well. The rates we show are tighter than the uniform\nconvergence rate, which grows with $k^2$. \n\n"}
{"id": "1212.5860", "contents": "Title: A short note on the tail bound of Wishart distribution Abstract: We study the tail bound of the emperical covariance of multivariate normal\ndistribution. Following the work of (Gittens & Tropp, 2011), we provide a tail\nbound with a small constant. \n\n"}
{"id": "1301.1299", "contents": "Title: Automated Variational Inference in Probabilistic Programming Abstract: We present a new algorithm for approximate inference in probabilistic\nprograms, based on a stochastic gradient for variational programs. This method\nis efficient without restrictions on the probabilistic program; it is\nparticularly practical for distributions which are not analytically tractable,\nincluding highly structured distributions that arise in probabilistic programs.\nWe show how to automatically derive mean-field probabilistic programs and\noptimize them, and demonstrate that our perspective improves inference\nefficiency over other algorithms. \n\n"}
{"id": "1301.2725", "contents": "Title: Robust High Dimensional Sparse Regression and Matching Pursuit Abstract: We consider high dimensional sparse regression, and develop strategies able\nto deal with arbitrary -- possibly, severe or coordinated -- errors in the\ncovariance matrix $X$. These may come from corrupted data, persistent\nexperimental errors, or malicious respondents in surveys/recommender systems,\netc. Such non-stochastic error-in-variables problems are notoriously difficult\nto treat, and as we demonstrate, the problem is particularly pronounced in\nhigh-dimensional settings where the primary goal is {\\em support recovery} of\nthe sparse regressor. We develop algorithms for support recovery in sparse\nregression, when some number $n_1$ out of $n+n_1$ total covariate/response\npairs are {\\it arbitrarily (possibly maliciously) corrupted}. We are interested\nin understanding how many outliers, $n_1$, we can tolerate, while identifying\nthe correct support. To the best of our knowledge, neither standard outlier\nrejection techniques, nor recently developed robust regression algorithms (that\nfocus only on corrupted response variables), nor recent algorithms for dealing\nwith stochastic noise or erasures, can provide guarantees on support recovery.\nPerhaps surprisingly, we also show that the natural brute force algorithm that\nsearches over all subsets of $n$ covariate/response pairs, and all subsets of\npossible support coordinates in order to minimize regression error, is\nremarkably poor, unable to correctly identify the support with even $n_1 =\nO(n/k)$ corrupted points, where $k$ is the sparsity. This is true even in the\nbasic setting we consider, where all authentic measurements and noise are\nindependent and sub-Gaussian. In this setting, we provide a simple algorithm --\nno more computationally taxing than OMP -- that gives stronger performance\nguarantees, recovering the support with up to $n_1 = O(n/(\\sqrt{k} \\log p))$\ncorrupted points, where $p$ is the dimension of the signal to be recovered. \n\n"}
{"id": "1302.3407", "contents": "Title: A consistent clustering-based approach to estimating the number of\n  change-points in highly dependent time-series Abstract: The problem of change-point estimation is considered under a general\nframework where the data are generated by unknown stationary ergodic process\ndistributions. In this context, the consistent estimation of the number of\nchange-points is provably impossible. However, it is shown that a consistent\nclustering method may be used to estimate the number of change points, under\nthe additional constraint that the correct number of process distributions that\ngenerate the data is provided. This additional parameter has a natural\ninterpretation in many real-world applications. An algorithm is proposed that\nestimates the number of change-points and locates the changes. The proposed\nalgorithm is shown to be asymptotically consistent; its empirical evaluations\nare provided. \n\n"}
{"id": "1303.5145", "contents": "Title: Node-Based Learning of Multiple Gaussian Graphical Models Abstract: We consider the problem of estimating high-dimensional Gaussian graphical\nmodels corresponding to a single set of variables under several distinct\nconditions. This problem is motivated by the task of recovering transcriptional\nregulatory networks on the basis of gene expression data {containing\nheterogeneous samples, such as different disease states, multiple species, or\ndifferent developmental stages}. We assume that most aspects of the conditional\ndependence networks are shared, but that there are some structured differences\nbetween them. Rather than assuming that similarities and differences between\nnetworks are driven by individual edges, we take a node-based approach, which\nin many cases provides a more intuitive interpretation of the network\ndifferences. We consider estimation under two distinct assumptions: (1)\ndifferences between the K networks are due to individual nodes that are\nperturbed across conditions, or (2) similarities among the K networks are due\nto the presence of common hub nodes that are shared across all K networks.\nUsing a row-column overlap norm penalty function, we formulate two convex\noptimization problems that correspond to these two assumptions. We solve these\nproblems using an alternating direction method of multipliers algorithm, and we\nderive a set of necessary and sufficient conditions that allows us to decompose\nthe problem into independent subproblems so that our algorithm can be scaled to\nhigh-dimensional settings. Our proposal is illustrated on synthetic data, a\nwebpage data set, and a brain cancer gene expression data set. \n\n"}
{"id": "1303.5613", "contents": "Title: Network Detection Theory and Performance Abstract: Network detection is an important capability in many areas of applied\nresearch in which data can be represented as a graph of entities and\nrelationships. Oftentimes the object of interest is a relatively small subgraph\nin an enormous, potentially uninteresting background. This aspect characterizes\nnetwork detection as a \"big data\" problem. Graph partitioning and network\ndiscovery have been major research areas over the last ten years, driven by\ninterest in internet search, cyber security, social networks, and criminal or\nterrorist activities. The specific problem of network discovery is addressed as\na special case of graph partitioning in which membership in a small subgraph of\ninterest must be determined. Algebraic graph theory is used as the basis to\nanalyze and compare different network detection methods. A new Bayesian network\ndetection framework is introduced that partitions the graph based on prior\ninformation and direct observations. The new approach, called space-time threat\npropagation, is proved to maximize the probability of detection and is\ntherefore optimum in the Neyman-Pearson sense. This optimality criterion is\ncompared to spectral community detection approaches which divide the global\ngraph into subsets or communities with optimal connectivity properties. We also\nexplore a new generative stochastic model for covert networks and analyze using\nreceiver operating characteristics the detection performance of both classes of\noptimal detection techniques. \n\n"}
{"id": "1304.0682", "contents": "Title: Sparse Signal Processing with Linear and Nonlinear Observations: A\n  Unified Shannon-Theoretic Approach Abstract: We derive fundamental sample complexity bounds for recovering sparse and\nstructured signals for linear and nonlinear observation models including sparse\nregression, group testing, multivariate regression and problems with missing\nfeatures. In general, sparse signal processing problems can be characterized in\nterms of the following Markovian property. We are given a set of $N$ variables\n$X_1,X_2,\\ldots,X_N$, and there is an unknown subset of variables $S \\subset\n\\{1,\\ldots,N\\}$ that are relevant for predicting outcomes $Y$. More\nspecifically, when $Y$ is conditioned on $\\{X_n\\}_{n\\in S}$ it is conditionally\nindependent of the other variables, $\\{X_n\\}_{n \\not \\in S}$. Our goal is to\nidentify the set $S$ from samples of the variables $X$ and the associated\noutcomes $Y$. We characterize this problem as a version of the noisy channel\ncoding problem. Using asymptotic information theoretic analyses, we establish\nmutual information formulas that provide sufficient and necessary conditions on\nthe number of samples required to successfully recover the salient variables.\nThese mutual information expressions unify conditions for both linear and\nnonlinear observations. We then compute sample complexity bounds for the\naforementioned models, based on the mutual information expressions in order to\ndemonstrate the applicability and flexibility of our results in general sparse\nsignal processing models. \n\n"}
{"id": "1304.5583", "contents": "Title: Distributed Low-rank Subspace Segmentation Abstract: Vision problems ranging from image clustering to motion segmentation to\nsemi-supervised learning can naturally be framed as subspace segmentation\nproblems, in which one aims to recover multiple low-dimensional subspaces from\nnoisy and corrupted input data. Low-Rank Representation (LRR), a convex\nformulation of the subspace segmentation problem, is provably and empirically\naccurate on small problems but does not scale to the massive sizes of modern\nvision datasets. Moreover, past work aimed at scaling up low-rank matrix\nfactorization is not applicable to LRR given its non-decomposable constraints.\nIn this work, we propose a novel divide-and-conquer algorithm for large-scale\nsubspace segmentation that can cope with LRR's non-decomposable constraints and\nmaintains LRR's strong recovery guarantees. This has immediate implications for\nthe scalability of subspace segmentation, which we demonstrate on a benchmark\nface recognition dataset and in simulations. We then introduce novel\napplications of LRR-based subspace segmentation to large-scale semi-supervised\nlearning for multimedia event detection, concept detection, and image tagging.\nIn each case, we obtain state-of-the-art results and order-of-magnitude speed\nups. \n\n"}
{"id": "1305.2452", "contents": "Title: Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet\n  Allocation Abstract: In the internet era there has been an explosion in the amount of digital text\ninformation available, leading to difficulties of scale for traditional\ninference algorithms for topic models. Recent advances in stochastic\nvariational inference algorithms for latent Dirichlet allocation (LDA) have\nmade it feasible to learn topic models on large-scale corpora, but these\nmethods do not currently take full advantage of the collapsed representation of\nthe model. We propose a stochastic algorithm for collapsed variational Bayesian\ninference for LDA, which is simpler and more efficient than the state of the\nart method. We show connections between collapsed variational Bayesian\ninference and MAP estimation for LDA, and leverage these connections to prove\nconvergence properties of the proposed algorithm. In experiments on large-scale\ntext corpora, the algorithm was found to converge faster and often to a better\nsolution than the previous method. Human-subject experiments also demonstrated\nthat the method can learn coherent topics in seconds on small corpora,\nfacilitating the use of topic models in interactive document analysis software. \n\n"}
{"id": "1306.5362", "contents": "Title: A Statistical Perspective on Algorithmic Leveraging Abstract: One popular method for dealing with large-scale data sets is sampling. For\nexample, by using the empirical statistical leverage scores as an importance\nsampling distribution, the method of algorithmic leveraging samples and\nrescales rows/columns of data matrices to reduce the data size before\nperforming computations on the subproblem. This method has been successful in\nimproving computational efficiency of algorithms for matrix problems such as\nleast-squares approximation, least absolute deviations approximation, and\nlow-rank matrix approximation. Existing work has focused on algorithmic issues\nsuch as worst-case running times and numerical issues associated with providing\nhigh-quality implementations, but none of it addresses statistical aspects of\nthis method.\n  In this paper, we provide a simple yet effective framework to evaluate the\nstatistical properties of algorithmic leveraging in the context of estimating\nparameters in a linear regression model with a fixed number of predictors. We\nshow that from the statistical perspective of bias and variance, neither\nleverage-based sampling nor uniform sampling dominates the other. This result\nis particularly striking, given the well-known result that, from the\nalgorithmic perspective of worst-case analysis, leverage-based sampling\nprovides uniformly superior worst-case algorithmic results, when compared with\nuniform sampling. Based on these theoretical results, we propose and analyze\ntwo new leveraging algorithms. A detailed empirical evaluation of existing\nleverage-based methods as well as these two new methods is carried out on both\nsynthetic and real data sets. The empirical results indicate that our theory is\na good predictor of practical performance of existing and new leverage-based\nalgorithms and that the new algorithms achieve improved performance. \n\n"}
{"id": "1307.0471", "contents": "Title: Quantum support vector machine for big data classification Abstract: Supervised machine learning is the classification of new data based on\nalready classified training examples. In this work, we show that the support\nvector machine, an optimized binary classifier, can be implemented on a quantum\ncomputer, with complexity logarithmic in the size of the vectors and the number\nof training examples. In cases when classical sampling algorithms require\npolynomial time, an exponential speed-up is obtained. At the core of this\nquantum big data algorithm is a non-sparse matrix exponentiation technique for\nefficiently performing a matrix inversion of the training data inner-product\n(kernel) matrix. \n\n"}
{"id": "1309.0787", "contents": "Title: Online Tensor Methods for Learning Latent Variable Models Abstract: We introduce an online tensor decomposition based approach for two latent\nvariable modeling problems namely, (1) community detection, in which we learn\nthe latent communities that the social actors in social networks belong to, and\n(2) topic modeling, in which we infer hidden topics of text articles. We\nconsider decomposition of moment tensors using stochastic gradient descent. We\nconduct optimization of multilinear operations in SGD and avoid directly\nforming the tensors, to save computational and storage costs. We present\noptimized algorithm in two platforms. Our GPU-based implementation exploits the\nparallelism of SIMD architectures to allow for maximum speed-up by a careful\noptimization of storage and data transfer, whereas our CPU-based implementation\nuses efficient sparse matrix computations and is suitable for large sparse\ndatasets. For the community detection problem, we demonstrate accuracy and\ncomputational efficiency on Facebook, Yelp and DBLP datasets, and for the topic\nmodeling problem, we also demonstrate good performance on the New York Times\ndataset. We compare our results to the state-of-the-art algorithms such as the\nvariational method, and report a gain of accuracy and a gain of several orders\nof magnitude in the execution time. \n\n"}
{"id": "1309.1392", "contents": "Title: Bayesian Structural Inference for Hidden Processes Abstract: We introduce a Bayesian approach to discovering patterns in structurally\ncomplex processes. The proposed method of Bayesian Structural Inference (BSI)\nrelies on a set of candidate unifilar HMM (uHMM) topologies for inference of\nprocess structure from a data series. We employ a recently developed exact\nenumeration of topological epsilon-machines. (A sequel then removes the\ntopological restriction.) This subset of the uHMM topologies has the added\nbenefit that inferred models are guaranteed to be epsilon-machines,\nirrespective of estimated transition probabilities. Properties of\nepsilon-machines and uHMMs allow for the derivation of analytic expressions for\nestimating transition probabilities, inferring start states, and comparing the\nposterior probability of candidate model topologies, despite process internal\nstructure being only indirectly present in data. We demonstrate BSI's\neffectiveness in estimating a process's randomness, as reflected by the Shannon\nentropy rate, and its structure, as quantified by the statistical complexity.\nWe also compare using the posterior distribution over candidate models and the\nsingle, maximum a posteriori model for point estimation and show that the\nformer more accurately reflects uncertainty in estimated values. We apply BSI\nto in-class examples of finite- and infinite-order Markov processes, as well to\nan out-of-class, infinite-state hidden process. \n\n"}
{"id": "1311.4296", "contents": "Title: Reflection methods for user-friendly submodular optimization Abstract: Recently, it has become evident that submodularity naturally captures widely\noccurring concepts in machine learning, signal processing and computer vision.\nConsequently, there is need for efficient optimization procedures for\nsubmodular functions, especially for minimization problems. While general\nsubmodular minimization is challenging, we propose a new method that exploits\nexisting decomposability of submodular functions. In contrast to previous\napproaches, our method is neither approximate, nor impractical, nor does it\nneed any cumbersome parameter tuning. Moreover, it is easy to implement and\nparallelize. A key component of our method is a formulation of the discrete\nsubmodular minimization problem as a continuous best approximation problem that\nis solved through a sequence of reflections, and its solution can be easily\nthresholded to obtain an optimal discrete solution. This method solves both the\ncontinuous and discrete formulations of the problem, and therefore has\napplications in learning, inference, and reconstruction. In our experiments, we\nillustrate the benefits of our method on two image segmentation tasks. \n\n"}
{"id": "1311.6041", "contents": "Title: No Free Lunch Theorem and Bayesian probability theory: two sides of the\n  same coin. Some implications for black-box optimization and metaheuristics Abstract: Challenging optimization problems, which elude acceptable solution via\nconventional calculus methods, arise commonly in different areas of industrial\ndesign and practice. Hard optimization problems are those who manifest the\nfollowing behavior: a) high number of independent input variables; b) very\ncomplex or irregular multi-modal fitness; c) computational expensive fitness\nevaluation. This paper will focus on some theoretical issues that have strong\nimplications for practice. I will stress how an interpretation of the No Free\nLunch theorem leads naturally to a general Bayesian optimization framework. The\nchoice of a prior over the space of functions is a critical and inevitable step\nin every black-box optimization. \n\n"}
{"id": "1312.0232", "contents": "Title: Stochastic continuum armed bandit problem of few linear parameters in\n  high dimensions Abstract: We consider a stochastic continuum armed bandit problem where the arms are\nindexed by the $\\ell_2$ ball $B_{d}(1+\\nu)$ of radius $1+\\nu$ in\n$\\mathbb{R}^d$. The reward functions $r :B_{d}(1+\\nu) \\rightarrow \\mathbb{R}$\nare considered to intrinsically depend on $k \\ll d$ unknown linear parameters\nso that $r(\\mathbf{x}) = g(\\mathbf{A} \\mathbf{x})$ where $\\mathbf{A}$ is a full\nrank $k \\times d$ matrix. Assuming the mean reward function to be smooth we\nmake use of results from low-rank matrix recovery literature and derive an\nefficient randomized algorithm which achieves a regret bound of $O(C(k,d)\nn^{\\frac{1+k}{2+k}} (\\log n)^{\\frac{1}{2+k}})$ with high probability. Here\n$C(k,d)$ is at most polynomial in $d$ and $k$ and $n$ is the number of rounds\nor the sampling budget which is assumed to be known beforehand. \n\n"}
{"id": "1312.2164", "contents": "Title: Budgeted Influence Maximization for Multiple Products Abstract: The typical algorithmic problem in viral marketing aims to identify a set of\ninfluential users in a social network, who, when convinced to adopt a product,\nshall influence other users in the network and trigger a large cascade of\nadoptions. However, the host (the owner of an online social platform) often\nfaces more constraints than a single product, endless user attentions,\nunlimited budget and unbounded time; in reality, multiple products need to be\nadvertised, each user can tolerate only a small number of recommendations,\ninfluencing user has a cost and advertisers have only limited budgets, and the\nadoptions need to be maximized within a short time window.\n  Given theses myriads of user, monetary, and timing constraints, it is\nextremely challenging for the host to design principled and efficient viral\nmarket algorithms with provable guarantees. In this paper, we provide a novel\nsolution by formulating the problem as a submodular maximization in a\ncontinuous-time diffusion model under an intersection of a matroid and multiple\nknapsack constraints. We also propose an adaptive threshold greedy algorithm\nwhich can be faster than the traditional greedy algorithm with lazy evaluation,\nand scalable to networks with million of nodes. Furthermore, our mathematical\nformulation allows us to prove that the algorithm can achieve an approximation\nfactor of $k_a/(2+2 k)$ when $k_a$ out of the $k$ knapsack constraints are\nactive, which also improves over previous guarantees from combinatorial\noptimization literature. In the case when influencing each user has uniform\ncost, the approximation becomes even better to a factor of $1/3$. Extensive\nsynthetic and real world experiments demonstrate that our budgeted influence\nmaximization algorithm achieves the-state-of-the-art in terms of both\neffectiveness and scalability, often beating the next best by significant\nmargins. \n\n"}
{"id": "1312.4314", "contents": "Title: Learning Factored Representations in a Deep Mixture of Experts Abstract: Mixtures of Experts combine the outputs of several \"expert\" networks, each of\nwhich specializes in a different part of the input space. This is achieved by\ntraining a \"gating\" network that maps each input to a distribution over the\nexperts. Such models show promise for building larger networks that are still\ncheap to compute at test time, and more parallelizable at training time. In\nthis this work, we extend the Mixture of Experts to a stacked model, the Deep\nMixture of Experts, with multiple sets of gating and experts. This\nexponentially increases the number of effective experts by associating each\ninput with a combination of experts at each layer, yet maintains a modest model\nsize. On a randomly translated version of the MNIST dataset, we find that the\nDeep Mixture of Experts automatically learns to develop location-dependent\n(\"where\") experts at the first layer, and class-specific (\"what\") experts at\nthe second layer. In addition, we see that the different combinations are in\nuse when the model is applied to a dataset of speech monophones. These\ndemonstrate effective use of all expert combinations. \n\n"}
{"id": "1312.4461", "contents": "Title: Low-Rank Approximations for Conditional Feedforward Computation in Deep\n  Neural Networks Abstract: Scalability properties of deep neural networks raise key research questions,\nparticularly as the problems considered become larger and more challenging.\nThis paper expands on the idea of conditional computation introduced by Bengio,\net. al., where the nodes of a deep network are augmented by a set of gating\nunits that determine when a node should be calculated. By factorizing the\nweight matrix into a low-rank approximation, an estimation of the sign of the\npre-nonlinearity activation can be efficiently obtained. For networks using\nrectified-linear hidden units, this implies that the computation of a hidden\nunit with an estimated negative pre-nonlinearity can be ommitted altogether, as\nits value will become zero when nonlinearity is applied. For sparse neural\nnetworks, this can result in considerable speed gains. Experimental results\nusing the MNIST and SVHN data sets with a fully-connected deep neural network\ndemonstrate the performance robustness of the proposed scheme with respect to\nthe error introduced by the conditional computation process. \n\n"}
{"id": "1312.5465", "contents": "Title: Learning rates of $l^q$ coefficient regularization learning with\n  Gaussian kernel Abstract: Regularization is a well recognized powerful strategy to improve the\nperformance of a learning machine and $l^q$ regularization schemes with\n$0<q<\\infty$ are central in use. It is known that different $q$ leads to\ndifferent properties of the deduced estimators, say, $l^2$ regularization leads\nto smooth estimators while $l^1$ regularization leads to sparse estimators.\nThen, how does the generalization capabilities of $l^q$ regularization learning\nvary with $q$? In this paper, we study this problem in the framework of\nstatistical learning theory and show that implementing $l^q$ coefficient\nregularization schemes in the sample dependent hypothesis space associated with\nGaussian kernel can attain the same almost optimal learning rates for all\n$0<q<\\infty$. That is, the upper and lower bounds of learning rates for $l^q$\nregularization learning are asymptotically identical for all $0<q<\\infty$. Our\nfinding tentatively reveals that, in some modeling contexts, the choice of $q$\nmight not have a strong impact with respect to the generalization capability.\nFrom this perspective, $q$ can be arbitrarily specified, or specified merely by\nother no generalization criteria like smoothness, computational complexity,\nsparsity, etc.. \n\n"}
{"id": "1312.5578", "contents": "Title: Multimodal Transitions for Generative Stochastic Networks Abstract: Generative Stochastic Networks (GSNs) have been recently introduced as an\nalternative to traditional probabilistic modeling: instead of parametrizing the\ndata distribution directly, one parametrizes a transition operator for a Markov\nchain whose stationary distribution is an estimator of the data generating\ndistribution. The result of training is therefore a machine that generates\nsamples through this Markov chain. However, the previously introduced GSN\nconsistency theorems suggest that in order to capture a wide class of\ndistributions, the transition operator in general should be multimodal,\nsomething that has not been done before this paper. We introduce for the first\ntime multimodal transition distributions for GSNs, in particular using models\nin the NADE family (Neural Autoregressive Density Estimator) as output\ndistributions of the transition operator. A NADE model is related to an RBM\n(and can thus model multimodal distributions) but its likelihood (and\nlikelihood gradient) can be computed easily. The parameters of the NADE are\nobtained as a learned function of the previous state of the learned Markov\nchain. Experiments clearly illustrate the advantage of such multimodal\ntransition distributions over unimodal GSNs. \n\n"}
{"id": "1312.5604", "contents": "Title: Learning Transformations for Classification Forests Abstract: This work introduces a transformation-based learner model for classification\nforests. The weak learner at each split node plays a crucial role in a\nclassification tree. We propose to optimize the splitting objective by learning\na linear transformation on subspaces using nuclear norm as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same class, and, at the same time, maximizes the separation\nbetween different classes, thereby improving the performance of the split\nfunction. Theoretical and experimental results support the proposed framework. \n\n"}
{"id": "1312.5921", "contents": "Title: Group-sparse Embeddings in Collective Matrix Factorization Abstract: CMF is a technique for simultaneously learning low-rank representations based\non a collection of matrices with shared entities. A typical example is the\njoint modeling of user-item, item-property, and user-feature matrices in a\nrecommender system. The key idea in CMF is that the embeddings are shared\nacross the matrices, which enables transferring information between them. The\nexisting solutions, however, break down when the individual matrices have\nlow-rank structure not shared with others. In this work we present a novel CMF\nsolution that allows each of the matrices to have a separate low-rank structure\nthat is independent of the other matrices, as well as structures that are\nshared only by a subset of them. We compare MAP and variational Bayesian\nsolutions based on alternating optimization algorithms and show that the model\nautomatically infers the nature of each factor using group-wise sparsity. Our\napproach supports in a principled way continuous, binary and count observations\nand is efficient for sparse matrices involving missing data. We illustrate the\nsolution on a number of examples, focusing in particular on an interesting\nuse-case of augmented multi-view learning. \n\n"}
{"id": "1312.6114", "contents": "Title: Auto-Encoding Variational Bayes Abstract: How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable\nposterior distributions, and large datasets? We introduce a stochastic\nvariational inference and learning algorithm that scales to large datasets and,\nunder some mild differentiability conditions, even works in the intractable\ncase. Our contributions are two-fold. First, we show that a reparameterization\nof the variational lower bound yields a lower bound estimator that can be\nstraightforwardly optimized using standard stochastic gradient methods. Second,\nwe show that for i.i.d. datasets with continuous latent variables per\ndatapoint, posterior inference can be made especially efficient by fitting an\napproximate inference model (also called a recognition model) to the\nintractable posterior using the proposed lower bound estimator. Theoretical\nadvantages are reflected in experimental results. \n\n"}
{"id": "1312.6607", "contents": "Title: Using Latent Binary Variables for Online Reconstruction of Large Scale\n  Systems Abstract: We propose a probabilistic graphical model realizing a minimal encoding of\nreal variables dependencies based on possibly incomplete observation and an\nempirical cumulative distribution function per variable. The target application\nis a large scale partially observed system, like e.g. a traffic network, where\na small proportion of real valued variables are observed, and the other\nvariables have to be predicted. Our design objective is therefore to have good\nscalability in a real-time setting. Instead of attempting to encode the\ndependencies of the system directly in the description space, we propose a way\nto encode them in a latent space of binary variables, reflecting a rough\nperception of the observable (congested/non-congested for a traffic road). The\nmethod relies in part on message passing algorithms, i.e. belief propagation,\nbut the core of the work concerns the definition of meaningful latent variables\nassociated to the variables of interest and their pairwise dependencies.\nNumerical experiments demonstrate the applicability of the method in practice. \n\n"}
{"id": "1312.7606", "contents": "Title: Distributed Policy Evaluation Under Multiple Behavior Strategies Abstract: We apply diffusion strategies to develop a fully-distributed cooperative\nreinforcement learning algorithm in which agents in a network communicate only\nwith their immediate neighbors to improve predictions about their environment.\nThe algorithm can also be applied to off-policy learning, meaning that the\nagents can predict the response to a behavior different from the actual\npolicies they are following. The proposed distributed strategy is efficient,\nwith linear complexity in both computation time and memory footprint. We\nprovide a mean-square-error performance analysis and establish convergence\nunder constant step-size updates, which endow the network with continuous\nlearning capabilities. The results show a clear gain from cooperation: when the\nindividual agents can estimate the solution, cooperation increases stability\nand reduces bias and variance of the prediction error; but, more importantly,\nthe network is able to approach the optimal solution even when none of the\nindividual agents can (e.g., when the individual behavior policies restrict\neach agent to sample a small portion of the state space). \n\n"}
{"id": "1401.0767", "contents": "Title: From Kernel Machines to Ensemble Learning Abstract: Ensemble methods such as boosting combine multiple learners to obtain better\nprediction than could be obtained from any individual learner. Here we propose\na principled framework for directly constructing ensemble learning methods from\nkernel methods. Unlike previous studies showing the equivalence between\nboosting and support vector machines (SVMs), which needs a translation\nprocedure, we show that it is possible to design boosting-like procedure to\nsolve the SVM optimization problems.\n  In other words, it is possible to design ensemble methods directly from SVM\nwithout any middle procedure.\n  This finding not only enables us to design new ensemble learning methods\ndirectly from kernel methods, but also makes it possible to take advantage of\nthose highly-optimized fast linear SVM solvers for ensemble learning.\n  We exemplify this framework for designing binary ensemble learning as well as\na new multi-class ensemble learning methods.\n  Experimental results demonstrate the flexibility and usefulness of the\nproposed framework. \n\n"}
{"id": "1401.1880", "contents": "Title: DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation Abstract: In recent years, there has been growing focus on the study of automated\nrecommender systems. Music recommendation systems serve as a prominent domain\nfor such works, both from an academic and a commercial perspective. A\nfundamental aspect of music perception is that music is experienced in temporal\ncontext and in sequence. In this work we present DJ-MC, a novel\nreinforcement-learning framework for music recommendation that does not\nrecommend songs individually but rather song sequences, or playlists, based on\na model of preferences for both songs and song transitions. The model is\nlearned online and is uniquely adapted for each listener. To reduce exploration\ntime, DJ-MC exploits user feedback to initialize a model, which it subsequently\nupdates by reinforcement. We evaluate our framework with human participants\nusing both real song and playlist data. Our results indicate that DJ-MC's\nability to recommend sequences of songs provides a significant improvement over\nmore straightforward approaches, which do not take transitions into account. \n\n"}
{"id": "1402.3511", "contents": "Title: A Clockwork RNN Abstract: Sequence prediction and classification are ubiquitous and challenging\nproblems in machine learning that can require identifying complex dependencies\nbetween temporally distant inputs. Recurrent Neural Networks (RNNs) have the\nability, in theory, to cope with these temporal dependencies by virtue of the\nshort-term memory implemented by their recurrent (feedback) connections.\nHowever, in practice they are difficult to train successfully when the\nlong-term memory is required. This paper introduces a simple, yet powerful\nmodification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in\nwhich the hidden layer is partitioned into separate modules, each processing\ninputs at its own temporal granularity, making computations only at its\nprescribed clock rate. Rather than making the standard RNN models more complex,\nCW-RNN reduces the number of RNN parameters, improves the performance\nsignificantly in the tasks tested, and speeds up the network evaluation. The\nnetwork is demonstrated in preliminary experiments involving two tasks: audio\nsignal generation and TIMIT spoken word classification, where it outperforms\nboth RNN and LSTM networks. \n\n"}
{"id": "1404.3184", "contents": "Title: Decreasing Weighted Sorted $\\ell_1$ Regularization Abstract: We consider a new family of regularizers, termed {\\it weighted sorted\n$\\ell_1$ norms} (WSL1), which generalizes the recently introduced {\\it\noctagonal shrinkage and clustering algorithm for regression} (OSCAR) and also\ncontains the $\\ell_1$ and $\\ell_{\\infty}$ norms as particular instances. We\nfocus on a special case of the WSL1, the {\\sl decreasing WSL1} (DWSL1), where\nthe elements of the argument vector are sorted in non-increasing order and the\nweights are also non-increasing. In this paper, after showing that the DWSL1 is\nindeed a norm, we derive two key tools for its use as a regularizer: the dual\nnorm and the Moreau proximity operator. \n\n"}
{"id": "1405.2262", "contents": "Title: Training Deep Fourier Neural Networks To Fit Time-Series Data Abstract: We present a method for training a deep neural network containing sinusoidal\nactivation functions to fit to time-series data. Weights are initialized using\na fast Fourier transform, then trained with regularization to improve\ngeneralization. A simple dynamic parameter tuning method is employed to adjust\nboth the learning rate and regularization term, such that stability and\nefficient training are both achieved. We show how deeper layers can be utilized\nto model the observed sequence using a sparser set of sinusoid units, and how\nnon-uniform regularization can improve generalization by promoting the shifting\nof weight toward simpler units. The method is demonstrated with time-series\nproblems to show that it leads to effective extrapolation of nonlinear trends. \n\n"}
{"id": "1405.3536", "contents": "Title: Improving offline evaluation of contextual bandit algorithms via\n  bootstrapping techniques Abstract: In many recommendation applications such as news recommendation, the items\nthat can be rec- ommended come and go at a very fast pace. This is a challenge\nfor recommender systems (RS) to face this setting. Online learning algorithms\nseem to be the most straight forward solution. The contextual bandit framework\nwas introduced for that very purpose. In general the evaluation of a RS is a\ncritical issue. Live evaluation is of- ten avoided due to the potential loss of\nrevenue, hence the need for offline evaluation methods. Two options are\navailable. Model based meth- ods are biased by nature and are thus difficult to\ntrust when used alone. Data driven methods are therefore what we consider here.\nEvaluat- ing online learning algorithms with past data is not simple but some\nmethods exist in the litera- ture. Nonetheless their accuracy is not satisfac-\ntory mainly due to their mechanism of data re- jection that only allow the\nexploitation of a small fraction of the data. We precisely address this issue\nin this paper. After highlighting the limita- tions of the previous methods, we\npresent a new method, based on bootstrapping techniques. This new method comes\nwith two important improve- ments: it is much more accurate and it provides a\nmeasure of quality of its estimation. The latter is a highly desirable property\nin order to minimize the risks entailed by putting online a RS for the first\ntime. We provide both theoretical and ex- perimental proofs of its superiority\ncompared to state-of-the-art methods, as well as an analysis of the convergence\nof the measure of quality. \n\n"}
{"id": "1405.3726", "contents": "Title: Topic words analysis based on LDA model Abstract: Social network analysis (SNA), which is a research field describing and\nmodeling the social connection of a certain group of people, is popular among\nnetwork services. Our topic words analysis project is a SNA method to visualize\nthe topic words among emails from Obama.com to accounts registered in Columbus,\nOhio. Based on Latent Dirichlet Allocation (LDA) model, a popular topic model\nof SNA, our project characterizes the preference of senders for target group of\nreceptors. Gibbs sampling is used to estimate topic and word distribution. Our\ntraining and testing data are emails from the carbon-free server\nDatagreening.com. We use parallel computing tool BashReduce for word processing\nand generate related words under each latent topic to discovers typical\ninformation of political news sending specially to local Columbus receptors.\nRunning on two instances using paralleling tool BashReduce, our project\ncontributes almost 30% speedup processing the raw contents, comparing with\nprocessing contents on one instance locally. Also, the experimental result\nshows that the LDA model applied in our project provides precision rate 53.96%\nhigher than TF-IDF model finding target words, on the condition that\nappropriate size of topic words list is selected. \n\n"}
{"id": "1406.3884", "contents": "Title: Learning An Invariant Speech Representation Abstract: Recognition of speech, and in particular the ability to generalize and learn\nfrom small sets of labelled examples like humans do, depends on an appropriate\nrepresentation of the acoustic input. We formulate the problem of finding\nrobust speech features for supervised learning with small sample complexity as\na problem of learning representations of the signal that are maximally\ninvariant to intraclass transformations and deformations. We propose an\nextension of a theory for unsupervised learning of invariant visual\nrepresentations to the auditory domain and empirically evaluate its validity\nfor voiced speech sound classification. Our version of the theory requires the\nmemory-based, unsupervised storage of acoustic templates -- such as specific\nphones or words -- together with all the transformations of each that normally\noccur. A quasi-invariant representation for a speech segment can be obtained by\nprojecting it to each template orbit, i.e., the set of transformed signals, and\ncomputing the associated one-dimensional empirical probability distributions.\nThe computations can be performed by modules of filtering and pooling, and\nextended to hierarchical architectures. In this paper, we apply a single-layer,\nmulticomponent representation for phonemes and demonstrate improved accuracy\nand decreased sample complexity for vowel classification compared to standard\nspectral, cepstral and perceptual features. \n\n"}
{"id": "1406.3896", "contents": "Title: Freeze-Thaw Bayesian Optimization Abstract: In this paper we develop a dynamic form of Bayesian optimization for machine\nlearning models with the goal of rapidly finding good hyperparameter settings.\nOur method uses the partial information gained during the training of a machine\nlearning model in order to decide whether to pause training and start a new\nmodel, or resume the training of a previously-considered model. We specifically\ntailor our method to machine learning problems by developing a novel\npositive-definite covariance kernel to capture a variety of training curves.\nFurthermore, we develop a Gaussian process prior that scales gracefully with\nadditional temporal observations. Finally, we provide an information-theoretic\nframework to automate the decision process. Experiments on several common\nmachine learning models show that our approach is extremely effective in\npractice. \n\n"}
{"id": "1407.0749", "contents": "Title: Projecting Ising Model Parameters for Fast Mixing Abstract: Inference in general Ising models is difficult, due to high treewidth making\ntree-based algorithms intractable. Moreover, when interactions are strong,\nGibbs sampling may take exponential time to converge to the stationary\ndistribution. We present an algorithm to project Ising model parameters onto a\nparameter set that is guaranteed to be fast mixing, under several divergences.\nWe find that Gibbs sampling using the projected parameters is more accurate\nthan with the original parameters when interaction strengths are strong and\nwhen limited time is available for sampling. \n\n"}
{"id": "1407.0753", "contents": "Title: Global convergence of splitting methods for nonconvex composite\n  optimization Abstract: We consider the problem of minimizing the sum of a smooth function $h$ with a\nbounded Hessian, and a nonsmooth function. We assume that the latter function\nis a composition of a proper closed function $P$ and a surjective linear map\n$\\cal M$, with the proximal mappings of $\\tau P$, $\\tau > 0$, simple to\ncompute. This problem is nonconvex in general and encompasses many important\napplications in engineering and machine learning. In this paper, we examined\ntwo types of splitting methods for solving this nonconvex optimization problem:\nalternating direction method of multipliers and proximal gradient algorithm.\nFor the direct adaptation of the alternating direction method of multipliers,\nwe show that, if the penalty parameter is chosen sufficiently large and the\nsequence generated has a cluster point, then it gives a stationary point of the\nnonconvex problem. We also establish convergence of the whole sequence under an\nadditional assumption that the functions $h$ and $P$ are semi-algebraic.\nFurthermore, we give simple sufficient conditions to guarantee boundedness of\nthe sequence generated. These conditions can be satisfied for a wide range of\napplications including the least squares problem with the $\\ell_{1/2}$\nregularization. Finally, when $\\cal M$ is the identity so that the proximal\ngradient algorithm can be efficiently applied, we show that any cluster point\nis stationary under a slightly more flexible constant step-size rule than what\nis known in the literature for a nonconvex $h$. \n\n"}
{"id": "1407.1640", "contents": "Title: WordRep: A Benchmark for Research on Learning Word Representations Abstract: WordRep is a benchmark collection for the research on learning distributed\nword representations (or word embeddings), released by Microsoft Research. In\nthis paper, we describe the details of the WordRep collection and show how to\nuse it in different types of machine learning research related to word\nembedding. Specifically, we describe how the evaluation tasks in WordRep are\nselected, how the data are sampled, and how the evaluation tool is built. We\nthen compare several state-of-the-art word representations on WordRep, report\ntheir evaluation performance, and make discussions on the results. After that,\nwe discuss new potential research topics that can be supported by WordRep, in\naddition to algorithm comparison. We hope that this paper can help people gain\ndeeper understanding of WordRep, and enable more interesting research on\nlearning distributed word representations and related topics. \n\n"}
{"id": "1407.1687", "contents": "Title: KNET: A General Framework for Learning Word Embedding using\n  Morphological Knowledge Abstract: Neural network techniques are widely applied to obtain high-quality\ndistributed representations of words, i.e., word embeddings, to address text\nmining, information retrieval, and natural language processing tasks. Recently,\nefficient methods have been proposed to learn word embeddings from context that\ncaptures both semantic and syntactic relationships between words. However, it\nis challenging to handle unseen words or rare words with insufficient context.\nIn this paper, inspired by the study on word recognition process in cognitive\npsychology, we propose to take advantage of seemingly less obvious but\nessentially important morphological knowledge to address these challenges. In\nparticular, we introduce a novel neural network architecture called KNET that\nleverages both contextual information and morphological word similarity built\nbased on morphological knowledge to learn word embeddings. Meanwhile, the\nlearning architecture is also able to refine the pre-defined morphological\nknowledge and obtain more accurate word similarity. Experiments on an\nanalogical reasoning task and a word similarity task both demonstrate that the\nproposed KNET framework can greatly enhance the effectiveness of word\nembeddings. \n\n"}
{"id": "1407.5155", "contents": "Title: Sparse and spurious: dictionary learning with noise and outliers Abstract: A popular approach within the signal processing and machine learning\ncommunities consists in modelling signals as sparse linear combinations of\natoms selected from a learned dictionary. While this paradigm has led to\nnumerous empirical successes in various fields ranging from image to audio\nprocessing, there have only been a few theoretical arguments supporting these\nevidences. In particular, sparse coding, or sparse dictionary learning, relies\non a non-convex procedure whose local minima have not been fully analyzed yet.\nIn this paper, we consider a probabilistic model of sparse signals, and show\nthat, with high probability, sparse coding admits a local minimum around the\nreference dictionary generating the signals. Our study takes into account the\ncase of over-complete dictionaries, noisy signals, and possible outliers, thus\nextending previous work limited to noiseless settings and/or under-complete\ndictionaries. The analysis we conduct is non-asymptotic and makes it possible\nto understand how the key quantities of the problem, such as the coherence or\nthe level of noise, can scale with respect to the dimension of the signals, the\nnumber of atoms, the sparsity and the number of observations. \n\n"}
{"id": "1407.7159", "contents": "Title: Pairwise Correlations in Layered Close-Packed Structures Abstract: Given a description of the stacking statistics of layered close-packed\nstructures in the form of a hidden Markov model, we develop analytical\nexpressions for the pairwise correlation functions between the layers. These\nmay be calculated analytically as explicit functions of model parameters or the\nexpressions may be used as a fast, accurate, and efficient way to obtain\nnumerical values. We present several examples, finding agreement with previous\nwork as well as deriving new relations. \n\n"}
{"id": "1407.8339", "contents": "Title: Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically\n  Triggered Arms Abstract: We define a general framework for a large class of combinatorial multi-armed\nbandit (CMAB) problems, where subsets of base arms with unknown distributions\nform super arms. In each round, a super arm is played and the base arms\ncontained in the super arm are played and their outcomes are observed. We\nfurther consider the extension in which more based arms could be\nprobabilistically triggered based on the outcomes of already triggered arms.\nThe reward of the super arm depends on the outcomes of all played arms, and it\nonly needs to satisfy two mild assumptions, which allow a large class of\nnonlinear reward instances. We assume the availability of an offline\n(\\alpha,\\beta)-approximation oracle that takes the means of the outcome\ndistributions of arms and outputs a super arm that with probability {\\beta}\ngenerates an {\\alpha} fraction of the optimal expected reward. The objective of\nan online learning algorithm for CMAB is to minimize\n(\\alpha,\\beta)-approximation regret, which is the difference between the\n\\alpha{\\beta} fraction of the expected reward when always playing the optimal\nsuper arm, and the expected reward of playing super arms according to the\nalgorithm. We provide CUCB algorithm that achieves O(log n)\ndistribution-dependent regret, where n is the number of rounds played, and we\nfurther provide distribution-independent bounds for a large class of reward\nfunctions. Our regret analysis is tight in that it matches the bound of UCB1\nalgorithm (up to a constant factor) for the classical MAB problem, and it\nsignificantly improves the regret bound in a earlier paper on combinatorial\nbandits with linear rewards. We apply our CMAB framework to two new\napplications, probabilistic maximum coverage and social influence maximization,\nboth having nonlinear reward structures. In particular, application to social\ninfluence maximization requires our extension on probabilistically triggered\narms. \n\n"}
{"id": "1408.2327", "contents": "Title: On the Consistency of Ordinal Regression Methods Abstract: Many of the ordinal regression models that have been proposed in the\nliterature can be seen as methods that minimize a convex surrogate of the\nzero-one, absolute, or squared loss functions. A key property that allows to\nstudy the statistical implications of such approximations is that of Fisher\nconsistency. Fisher consistency is a desirable property for surrogate loss\nfunctions and implies that in the population setting, i.e., if the probability\ndistribution that generates the data were available, then optimization of the\nsurrogate would yield the best possible model. In this paper we will\ncharacterize the Fisher consistency of a rich family of surrogate loss\nfunctions used in the context of ordinal regression, including support vector\nordinal regression, ORBoosting and least absolute deviation. We will see that,\nfor a family of surrogate loss functions that subsumes support vector ordinal\nregression and ORBoosting, consistency can be fully characterized by the\nderivative of a real-valued function at zero, as happens for convex\nmargin-based surrogates in binary classification. We also derive excess risk\nbounds for a surrogate of the absolute error that generalize existing risk\nbounds for binary classification. Finally, our analysis suggests a novel\nsurrogate of the squared error loss. We compare this novel surrogate with\ncompeting approaches on 9 different datasets. Our method shows to be highly\ncompetitive in practice, outperforming the least squares loss on 7 out of 9\ndatasets. \n\n"}
{"id": "1408.3467", "contents": "Title: Evaluating Visual Properties via Robust HodgeRank Abstract: Nowadays, how to effectively evaluate visual properties has become a popular\ntopic for fine-grained visual comprehension. In this paper we study the problem\nof how to estimate such visual properties from a ranking perspective with the\nhelp of the annotators from online crowdsourcing platforms. The main challenges\nof our task are two-fold. On one hand, the annotations often contain\ncontaminated information, where a small fraction of label flips might ruin the\nglobal ranking of the whole dataset. On the other hand, considering the large\ndata capacity, the annotations are often far from being complete. What is\nworse, there might even exist imbalanced annotations where a small subset of\nsamples are frequently annotated. Facing such challenges, we propose a robust\nranking framework based on the principle of Hodge decomposition of imbalanced\nand incomplete ranking data. According to the HodgeRank theory, we find that\nthe major source of the contamination comes from the cyclic ranking component\nof the Hodge decomposition. This leads us to an outlier detection formulation\nas sparse approximations of the cyclic ranking projection. Taking a step\nfurther, it facilitates a novel outlier detection model as Huber's LASSO in\nrobust statistics. Moreover, simple yet scalable algorithms are developed based\non Linearized Bregman Iteration to achieve an even less biased estimator.\nStatistical consistency of outlier detection is established in both cases under\nnearly the same conditions. Our studies are supported by experiments with both\nsimulated examples and real-world data. The proposed framework provides us a\npromising tool for robust ranking with large scale crowdsourcing data arising\nfrom computer vision. \n\n"}
{"id": "1408.5823", "contents": "Title: Improved Distributed Principal Component Analysis Abstract: We study the distributed computing setting in which there are multiple\nservers, each holding a set of points, who wish to compute functions on the\nunion of their point sets. A key task in this setting is Principal Component\nAnalysis (PCA), in which the servers would like to compute a low dimensional\nsubspace capturing as much of the variance of the union of their point sets as\npossible. Given a procedure for approximate PCA, one can use it to\napproximately solve $\\ell_2$-error fitting problems such as $k$-means\nclustering and subspace clustering. The essential properties of an approximate\ndistributed PCA algorithm are its communication cost and computational\nefficiency for a given desired accuracy in downstream applications. We give new\nalgorithms and analyses for distributed PCA which lead to improved\ncommunication and computational costs for $k$-means clustering and related\nproblems. Our empirical study on real world data shows a speedup of orders of\nmagnitude, preserving communication with only a negligible degradation in\nsolution quality. Some of these techniques we develop, such as a general\ntransformation from a constant success probability subspace embedding to a high\nsuccess probability subspace embedding with a dimension and sparsity\nindependent of the success probability, may be of independent interest. \n\n"}
{"id": "1409.0272", "contents": "Title: Multi-task Sparse Structure Learning Abstract: Multi-task learning (MTL) aims to improve generalization performance by\nlearning multiple related tasks simultaneously. While sometimes the underlying\ntask relationship structure is known, often the structure needs to be estimated\nfrom data at hand. In this paper, we present a novel family of models for MTL,\napplicable to regression and classification problems, capable of learning the\nstructure of task relationships. In particular, we consider a joint estimation\nproblem of the task relationship structure and the individual task parameters,\nwhich is solved using alternating minimization. The task relationship structure\nlearning component builds on recent advances in structure learning of Gaussian\ngraphical models based on sparse estimators of the precision (inverse\ncovariance) matrix. We illustrate the effectiveness of the proposed model on a\nvariety of synthetic and benchmark datasets for regression and classification.\nWe also consider the problem of combining climate model outputs for better\nprojections of future climate, with focus on temperature in South America, and\nshow that the proposed model outperforms several existing methods for the\nproblem. \n\n"}
{"id": "1409.5209", "contents": "Title: Pedestrian Detection with Spatially Pooled Features and Structured\n  Ensemble Learning Abstract: Many typical applications of object detection operate within a prescribed\nfalse-positive range. In this situation the performance of a detector should be\nassessed on the basis of the area under the ROC curve over that range, rather\nthan over the full curve, as the performance outside the range is irrelevant.\nThis measure is labelled as the partial area under the ROC curve (pAUC). We\npropose a novel ensemble learning method which achieves a maximal detection\nrate at a user-defined range of false positive rates by directly optimizing the\npartial AUC using structured learning.\n  In order to achieve a high object detection performance, we propose a new\napproach to extract low-level visual features based on spatial pooling.\nIncorporating spatial pooling improves the translational invariance and thus\nthe robustness of the detection process. Experimental results on both synthetic\nand real-world data sets demonstrate the effectiveness of our approach, and we\nshow that it is possible to train state-of-the-art pedestrian detectors using\nthe proposed structured ensemble learning method with spatially pooled\nfeatures. The result is the current best reported performance on the\nCaltech-USA pedestrian detection dataset. \n\n"}
{"id": "1410.0736", "contents": "Title: HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale\n  Visual Recognition Abstract: In image classification, visual separability between different object\ncategories is highly uneven, and some categories are more difficult to\ndistinguish than others. Such difficult categories demand more dedicated\nclassifiers. However, existing deep convolutional neural networks (CNN) are\ntrained as flat N-way classifiers, and few efforts have been made to leverage\nthe hierarchical structure of categories. In this paper, we introduce\nhierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category\nhierarchy. An HD-CNN separates easy classes using a coarse category classifier\nwhile distinguishing difficult classes using fine category classifiers. During\nHD-CNN training, component-wise pretraining is followed by global finetuning\nwith a multinomial logistic loss regularized by a coarse category consistency\nterm. In addition, conditional executions of fine category classifiers and\nlayer parameter compression make HD-CNNs scalable for large-scale visual\nrecognition. We achieve state-of-the-art results on both CIFAR100 and\nlarge-scale ImageNet 1000-class benchmark datasets. In our experiments, we\nbuild up three different HD-CNNs and they lower the top-1 error of the standard\nCNNs by 2.65%, 3.1% and 1.1%, respectively. \n\n"}
{"id": "1410.1141", "contents": "Title: On the Computational Efficiency of Training Neural Networks Abstract: It is well-known that neural networks are computationally hard to train. On\nthe other hand, in practice, modern day neural networks are trained efficiently\nusing SGD and a variety of tricks that include different activation functions\n(e.g. ReLU), over-specification (i.e., train networks which are larger than\nneeded), and regularization. In this paper we revisit the computational\ncomplexity of training neural networks from a modern perspective. We provide\nboth positive and negative results, some of them yield new provably efficient\nand practical algorithms for training certain types of neural networks. \n\n"}
{"id": "1410.2838", "contents": "Title: Approximate False Positive Rate Control in Selection Frequency for\n  Random Forest Abstract: Random Forest has become one of the most popular tools for feature selection.\nIts ability to deal with high-dimensional data makes this algorithm especially\nuseful for studies in neuroimaging and bioinformatics. Despite its popularity\nand wide use, feature selection in Random Forest still lacks a crucial\ningredient: false positive rate control. To date there is no efficient,\nprincipled and computationally light-weight solution to this shortcoming. As a\nresult, researchers using Random Forest for feature selection have to resort to\nusing heuristically set thresholds on feature rankings. This article builds an\napproximate probabilistic model for the feature selection process in random\nforest training, which allows us to compute an estimated false positive rate\nfor a given threshold on selection frequency. Hence, it presents a principled\nway to determine thresholds for the selection of relevant features without any\nadditional computational load. Experimental analysis with synthetic data\ndemonstrates that the proposed approach can limit false positive rates on the\norder of the desired values and keep false negative rates low. Results show\nthat this holds even in the presence of a complex correlation structure between\nfeatures. Its good statistical properties and light-weight computational needs\nmake this approach widely applicable to feature selection for a wide-range of\napplications. \n\n"}
{"id": "1410.5392", "contents": "Title: Scalable Parallel Factorizations of SDD Matrices and Efficient Sampling\n  for Gaussian Graphical Models Abstract: Motivated by a sampling problem basic to computational statistical inference,\nwe develop a nearly optimal algorithm for a fundamental problem in spectral\ngraph theory and numerical analysis. Given an $n\\times n$ SDDM matrix ${\\bf\n\\mathbf{M}}$, and a constant $-1 \\leq p \\leq 1$, our algorithm gives efficient\naccess to a sparse $n\\times n$ linear operator $\\tilde{\\mathbf{C}}$ such that\n$${\\mathbf{M}}^{p} \\approx \\tilde{\\mathbf{C}} \\tilde{\\mathbf{C}}^\\top.$$ The\nsolution is based on factoring ${\\bf \\mathbf{M}}$ into a product of simple and\nsparse matrices using squaring and spectral sparsification. For ${\\mathbf{M}}$\nwith $m$ non-zero entries, our algorithm takes work nearly-linear in $m$, and\npolylogarithmic depth on a parallel machine with $m$ processors. This gives the\nfirst sampling algorithm that only requires nearly linear work and $n$ i.i.d.\nrandom univariate Gaussian samples to generate i.i.d. random samples for\n$n$-dimensional Gaussian random fields with SDDM precision matrices. For\nsampling this natural subclass of Gaussian random fields, it is optimal in the\nrandomness and nearly optimal in the work and parallel complexity. In addition,\nour sampling algorithm can be directly extended to Gaussian random fields with\nSDD precision matrices. \n\n"}
{"id": "1410.8750", "contents": "Title: Learning Mixtures of Ranking Models Abstract: This work concerns learning probabilistic models for ranking data in a\nheterogeneous population. The specific problem we study is learning the\nparameters of a Mallows Mixture Model. Despite being widely studied, current\nheuristics for this problem do not have theoretical guarantees and can get\nstuck in bad local optima. We present the first polynomial time algorithm which\nprovably learns the parameters of a mixture of two Mallows models. A key\ncomponent of our algorithm is a novel use of tensor decomposition techniques to\nlearn the top-k prefix in both the rankings. Before this work, even the\nquestion of identifiability in the case of a mixture of two Mallows models was\nunresolved. \n\n"}
{"id": "1411.0728", "contents": "Title: Approachability in Stackelberg Stochastic Games with Vector Costs Abstract: The notion of approachability was introduced by Blackwell [1] in the context\nof vector-valued repeated games. The famous Blackwell's approachability theorem\nprescribes a strategy for approachability, i.e., for `steering' the average\ncost of a given agent towards a given target set, irrespective of the\nstrategies of the other agents. In this paper, motivated by the multi-objective\noptimization/decision making problems in dynamically changing environments, we\naddress the approachability problem in Stackelberg stochastic games with vector\nvalued cost functions. We make two main contributions. Firstly, we give a\nsimple and computationally tractable strategy for approachability for\nStackelberg stochastic games along the lines of Blackwell's. Secondly, we give\na reinforcement learning algorithm for learning the approachable strategy when\nthe transition kernel is unknown. We also recover as a by-product Blackwell's\nnecessary and sufficient condition for approachability for convex sets in this\nset up and thus a complete characterization. We also give sufficient conditions\nfor non-convex sets. \n\n"}
{"id": "1411.1784", "contents": "Title: Conditional Generative Adversarial Nets Abstract: Generative Adversarial Nets [8] were recently introduced as a novel way to\ntrain generative models. In this work we introduce the conditional version of\ngenerative adversarial nets, which can be constructed by simply feeding the\ndata, y, we wish to condition on to both the generator and discriminator. We\nshow that this model can generate MNIST digits conditioned on class labels. We\nalso illustrate how this model could be used to learn a multi-modal model, and\nprovide preliminary examples of an application to image tagging in which we\ndemonstrate how this approach can generate descriptive tags which are not part\nof training labels. \n\n"}
{"id": "1411.2337", "contents": "Title: Multi-Task Metric Learning on Network Data Abstract: Multi-task learning (MTL) improves prediction performance in different\ncontexts by learning models jointly on multiple different, but related tasks.\nNetwork data, which are a priori data with a rich relational structure, provide\nan important context for applying MTL. In particular, the explicit relational\nstructure implies that network data is not i.i.d. data. Network data also often\ncomes with significant metadata (i.e., attributes) associated with each entity\n(node). Moreover, due to the diversity and variation in network data (e.g.,\nmulti-relational links or multi-category entities), various tasks can be\nperformed and often a rich correlation exists between them. Learning algorithms\nshould exploit all of these additional sources of information for better\nperformance. In this work we take a metric-learning point of view for the MTL\nproblem in the network context. Our approach builds on structure preserving\nmetric learning (SPML). In particular SPML learns a Mahalanobis distance metric\nfor node attributes using network structure as supervision, so that the learned\ndistance function encodes the structure and can be used to predict link\npatterns from attributes. SPML is described for single-task learning on single\nnetwork. Herein, we propose a multi-task version of SPML, abbreviated as\nMT-SPML, which is able to learn across multiple related tasks on multiple\nnetworks via shared intermediate parametrization. MT-SPML learns a specific\nmetric for each task and a common metric for all tasks. The task correlation is\ncarried through the common metric and the individual metrics encode task\nspecific information. When combined together, they are structure-preserving\nwith respect to individual tasks. MT-SPML works on general networks, thus is\nsuitable for a wide variety of problems. In experiments, we challenge MT-SPML\non two real-word problems, where MT-SPML achieves significant improvement. \n\n"}
{"id": "1411.3315", "contents": "Title: Statistically Significant Detection of Linguistic Change Abstract: We propose a new computational approach for tracking and detecting\nstatistically significant linguistic shifts in the meaning and usage of words.\nSuch linguistic shifts are especially prevalent on the Internet, where the\nrapid exchange of ideas can quickly change a word's meaning. Our meta-analysis\napproach constructs property time series of word usage, and then uses\nstatistically sound change point detection algorithms to identify significant\nlinguistic shifts.\n  We consider and analyze three approaches of increasing complexity to generate\nsuch linguistic property time series, the culmination of which uses\ndistributional characteristics inferred from word co-occurrences. Using\nrecently proposed deep neural language models, we first train vector\nrepresentations of words for each time period. Second, we warp the vector\nspaces into one unified coordinate system. Finally, we construct a\ndistance-based distributional time series for each word to track it's\nlinguistic displacement over time.\n  We demonstrate that our approach is scalable by tracking linguistic change\nacross years of micro-blogging using Twitter, a decade of product reviews using\na corpus of movie reviews from Amazon, and a century of written books using the\nGoogle Book-ngrams. Our analysis reveals interesting patterns of language usage\nchange commensurate with each medium. \n\n"}
{"id": "1411.3553", "contents": "Title: Greedy metrics in orthogonal greedy learning Abstract: Orthogonal greedy learning (OGL) is a stepwise learning scheme that adds a\nnew atom from a dictionary via the steepest gradient descent and build the\nestimator via orthogonal projecting the target function to the space spanned by\nthe selected atoms in each greedy step. Here, \"greed\" means choosing a new atom\naccording to the steepest gradient descent principle. OGL then avoids the\noverfitting/underfitting by selecting an appropriate iteration number. In this\npaper, we point out that the overfitting/underfitting can also be avoided via\nredefining \"greed\" in OGL. To this end, we introduce a new greedy metric,\ncalled $\\delta$-greedy thresholds, to refine \"greed\" and theoretically verifies\nits feasibility. Furthermore, we reveals that such a greedy metric can bring an\nadaptive termination rule on the premise of maintaining the prominent learning\nperformance of OGL. Our results show that the steepest gradient descent is not\nthe unique greedy metric of OGL and some other more suitable metric may lessen\nthe hassle of model-selection of OGL. \n\n"}
{"id": "1411.4070", "contents": "Title: A unified view of generative models for networks: models, methods,\n  opportunities, and challenges Abstract: Research on probabilistic models of networks now spans a wide variety of\nfields, including physics, sociology, biology, statistics, and machine\nlearning. These efforts have produced a diverse ecology of models and methods.\nDespite this diversity, many of these models share a common underlying\nstructure: pairwise interactions (edges) are generated with probability\nconditional on latent vertex attributes. Differences between models generally\nstem from different philosophical choices about how to learn from data or\ndifferent empirically-motivated goals. The highly interdisciplinary nature of\nwork on these generative models, however, has inhibited the development of a\nunified view of their similarities and differences. For instance, novel\ntheoretical models and optimization techniques developed in machine learning\nare largely unknown within the social and biological sciences, which have\ninstead emphasized model interpretability. Here, we describe a unified view of\ngenerative models for networks that draws together many of these disparate\nthreads and highlights the fundamental similarities and differences that span\nthese fields. We then describe a number of opportunities and challenges for\nfuture work that are revealed by this view. \n\n"}
{"id": "1411.4491", "contents": "Title: Joint cross-domain classification and subspace learning for unsupervised\n  adaptation Abstract: Domain adaptation aims at adapting the knowledge acquired on a source domain\nto a new different but related target domain. Several approaches have\nbeenproposed for classification tasks in the unsupervised scenario, where no\nlabeled target data are available. Most of the attention has been dedicated to\nsearching a new domain-invariant representation, leaving the definition of the\nprediction function to a second stage. Here we propose to learn both jointly.\nSpecifically we learn the source subspace that best matches the target subspace\nwhile at the same time minimizing a regularized misclassification loss. We\nprovide an alternating optimization technique based on stochastic sub-gradient\ndescent to solve the learning problem and we demonstrate its performance on\nseveral domain adaptation tasks. \n\n"}
{"id": "1412.5949", "contents": "Title: Large Scale Distributed Distance Metric Learning Abstract: In large scale machine learning and data mining problems with high feature\ndimensionality, the Euclidean distance between data points can be\nuninformative, and Distance Metric Learning (DML) is often desired to learn a\nproper similarity measure (using side information such as example data pairs\nbeing similar or dissimilar). However, high dimensionality and large volume of\npairwise constraints in modern big data can lead to prohibitive computational\ncost for both the original DML formulation in Xing et al. (2002) and later\nextensions. In this paper, we present a distributed algorithm for DML, and a\nlarge-scale implementation on a parameter server architecture. Our approach\nbuilds on a parallelizable reformulation of Xing et al. (2002), and an\nasynchronous stochastic gradient descent optimization procedure. To our\nknowledge, this is the first distributed solution to DML, and we show that, on\na system with 256 CPU cores, our program is able to complete a DML task on a\ndataset with 1 million data points, 22-thousand features, and 200 million\nlabeled data pairs, in 15 hours; and the learned metric shows great\neffectiveness in properly measuring distances. \n\n"}
{"id": "1412.6558", "contents": "Title: Random Walk Initialization for Training Very Deep Feedforward Networks Abstract: Training very deep networks is an important open problem in machine learning.\nOne of many difficulties is that the norm of the back-propagated error gradient\ncan grow or decay exponentially. Here we show that training very deep\nfeed-forward networks (FFNs) is not as difficult as previously thought. Unlike\nwhen back-propagation is applied to a recurrent network, application to an FFN\namounts to multiplying the error gradient by a different random matrix at each\nlayer. We show that the successive application of correctly scaled random\nmatrices to an initial vector results in a random walk of the log of the norm\nof the resulting vectors, and we compute the scaling that makes this walk\nunbiased. The variance of the random walk grows only linearly with network\ndepth and is inversely proportional to the size of each layer. Practically,\nthis implies a gradient whose log-norm scales with the square root of the\nnetwork depth and shows that the vanishing gradient problem can be mitigated by\nincreasing the width of the layers. Mathematical analyses and experimental\nresults using stochastic gradient descent to optimize tasks related to the\nMNIST and TIMIT datasets are provided to support these claims. Equations for\nthe optimal matrix scaling are provided for the linear and ReLU cases. \n\n"}
{"id": "1412.7004", "contents": "Title: Tailoring Word Embeddings for Bilexical Predictions: An Experimental\n  Comparison Abstract: We investigate the problem of inducing word embeddings that are tailored for\na particular bilexical relation. Our learning algorithm takes an existing\nlexical vector space and compresses it such that the resulting word embeddings\nare good predictors for a target bilexical relation. In experiments we show\nthat task-specific embeddings can benefit both the quality and efficiency in\nlexical prediction tasks. \n\n"}
{"id": "1412.7006", "contents": "Title: Multi-modal Sensor Registration for Vehicle Perception via Deep Neural\n  Networks Abstract: The ability to simultaneously leverage multiple modes of sensor information\nis critical for perception of an automated vehicle's physical surroundings.\nSpatio-temporal alignment of registration of the incoming information is often\na prerequisite to analyzing the fused data. The persistence and reliability of\nmulti-modal registration is therefore the key to the stability of decision\nsupport systems ingesting the fused information. LiDAR-video systems like on\nthose many driverless cars are a common example of where keeping the LiDAR and\nvideo channels registered to common physical features is important. We develop\na deep learning method that takes multiple channels of heterogeneous data, to\ndetect the misalignment of the LiDAR-video inputs. A number of variations were\ntested on the Ford LiDAR-video driving test data set and will be discussed. To\nthe best of our knowledge the use of multi-modal deep convolutional neural\nnetworks for dynamic real-time LiDAR-video registration has not been presented. \n\n"}
{"id": "1412.7024", "contents": "Title: Training deep neural networks with low precision multiplications Abstract: Multipliers are the most space and power-hungry arithmetic operators of the\ndigital implementation of deep neural networks. We train a set of\nstate-of-the-art neural networks (Maxout networks) on three benchmark datasets:\nMNIST, CIFAR-10 and SVHN. They are trained with three distinct formats:\nfloating point, fixed point and dynamic fixed point. For each of those datasets\nand for each of those formats, we assess the impact of the precision of the\nmultiplications on the final error after training. We find that very low\nprecision is sufficient not just for running trained networks but also for\ntraining them. For example, it is possible to train Maxout networks with 10\nbits multiplications. \n\n"}
{"id": "1412.7026", "contents": "Title: Language Recognition using Random Indexing Abstract: Random Indexing is a simple implementation of Random Projections with a wide\nrange of applications. It can solve a variety of problems with good accuracy\nwithout introducing much complexity. Here we use it for identifying the\nlanguage of text samples. We present a novel method of generating language\nrepresentation vectors using letter blocks. Further, we show that the method is\neasily implemented and requires little computational power and space.\nExperiments on a number of model parameters illustrate certain properties about\nhigh dimensional sparse vector representations of data. Proof of statistically\nrelevant language vectors are shown through the extremely high success of\nvarious language recognition tasks. On a difficult data set of 21,000 short\nsentences from 21 different languages, our model performs a language\nrecognition task and achieves 97.8% accuracy, comparable to state-of-the-art\nmethods. \n\n"}
{"id": "1412.7725", "contents": "Title: Automatic Photo Adjustment Using Deep Neural Networks Abstract: Photo retouching enables photographers to invoke dramatic visual impressions\nby artistically enhancing their photos through stylistic color and tone\nadjustments. However, it is also a time-consuming and challenging task that\nrequires advanced skills beyond the abilities of casual photographers. Using an\nautomated algorithm is an appealing alternative to manual work but such an\nalgorithm faces many hurdles. Many photographic styles rely on subtle\nadjustments that depend on the image content and even its semantics. Further,\nthese adjustments are often spatially varying. Because of these\ncharacteristics, existing automatic algorithms are still limited and cover only\na subset of these challenges. Recently, deep machine learning has shown unique\nabilities to address hard problems that resisted machine algorithms for long.\nThis motivated us to explore the use of deep learning in the context of photo\nediting. In this paper, we explain how to formulate the automatic photo\nadjustment problem in a way suitable for this approach. We also introduce an\nimage descriptor that accounts for the local semantics of an image. Our\nexperiments demonstrate that our deep learning formulation applied using these\ndescriptors successfully capture sophisticated photographic styles. In\nparticular and unlike previous techniques, it can model local adjustments that\ndepend on the image semantics. We show on several examples that this yields\nresults that are qualitatively and quantitatively better than previous work. \n\n"}
{"id": "1412.8534", "contents": "Title: Disjunctive Normal Networks Abstract: Artificial neural networks are powerful pattern classifiers; however, they\nhave been surpassed in accuracy by methods such as support vector machines and\nrandom forests that are also easier to use and faster to train.\nBackpropagation, which is used to train artificial neural networks, suffers\nfrom the herd effect problem which leads to long training times and limit\nclassification accuracy. We use the disjunctive normal form and approximate the\nboolean conjunction operations with products to construct a novel network\narchitecture. The proposed model can be trained by minimizing an error function\nand it allows an effective and intuitive initialization which solves the\nherd-effect problem associated with backpropagation. This leads to state-of-the\nart classification accuracy and fast training times. In addition, our model can\nbe jointly optimized with convolutional features in an unified structure\nleading to state-of-the-art results on computer vision problems with fast\nconvergence rates. A GPU implementation of LDNN with optional convolutional\nfeatures is also available \n\n"}
{"id": "1501.06095", "contents": "Title: Between Pure and Approximate Differential Privacy Abstract: We show a new lower bound on the sample complexity of $(\\varepsilon,\n\\delta)$-differentially private algorithms that accurately answer statistical\nqueries on high-dimensional databases. The novelty of our bound is that it\ndepends optimally on the parameter $\\delta$, which loosely corresponds to the\nprobability that the algorithm fails to be private, and is the first to\nsmoothly interpolate between approximate differential privacy ($\\delta > 0$)\nand pure differential privacy ($\\delta = 0$).\n  Specifically, we consider a database $D \\in \\{\\pm1\\}^{n \\times d}$ and its\n\\emph{one-way marginals}, which are the $d$ queries of the form \"What fraction\nof individual records have the $i$-th bit set to $+1$?\" We show that in order\nto answer all of these queries to within error $\\pm \\alpha$ (on average) while\nsatisfying $(\\varepsilon, \\delta)$-differential privacy, it is necessary that\n$$ n \\geq \\Omega\\left( \\frac{\\sqrt{d \\log(1/\\delta)}}{\\alpha \\varepsilon}\n\\right), $$ which is optimal up to constant factors. To prove our lower bound,\nwe build on the connection between \\emph{fingerprinting codes} and lower bounds\nin differential privacy (Bun, Ullman, and Vadhan, STOC'14).\n  In addition to our lower bound, we give new purely and approximately\ndifferentially private algorithms for answering arbitrary statistical queries\nthat improve on the sample complexity of the standard Laplace and Gaussian\nmechanisms for achieving worst-case accuracy guarantees by a logarithmic\nfactor. \n\n"}
{"id": "1501.06195", "contents": "Title: Randomized sketches for kernels: Fast and optimal non-parametric\n  regression Abstract: Kernel ridge regression (KRR) is a standard method for performing\nnon-parametric regression over reproducing kernel Hilbert spaces. Given $n$\nsamples, the time and space complexity of computing the KRR estimate scale as\n$\\mathcal{O}(n^3)$ and $\\mathcal{O}(n^2)$ respectively, and so is prohibitive\nin many cases. We propose approximations of KRR based on $m$-dimensional\nrandomized sketches of the kernel matrix, and study how small the projection\ndimension $m$ can be chosen while still preserving minimax optimality of the\napproximate KRR estimate. For various classes of randomized sketches, including\nthose based on Gaussian and randomized Hadamard matrices, we prove that it\nsuffices to choose the sketch dimension $m$ proportional to the statistical\ndimension (modulo logarithmic factors). Thus, we obtain fast and minimax\noptimal approximations to the KRR estimate for non-parametric regression. \n\n"}
{"id": "1502.00060", "contents": "Title: A Random Matrix Theoretical Approach to Early Event Detection in Smart\n  Grid Abstract: Power systems are developing very fast nowadays, both in size and in\ncomplexity; this situation is a challenge for Early Event Detection (EED). This\npaper proposes a data- driven unsupervised learning method to handle this\nchallenge. Specifically, the random matrix theories (RMTs) are introduced as\nthe statistical foundations for random matrix models (RMMs); based on the RMMs,\nlinear eigenvalue statistics (LESs) are defined via the test functions as the\nsystem indicators. By comparing the values of the LES between the experimental\nand the theoretical ones, the anomaly detection is conducted. Furthermore, we\ndevelop 3D power-map to visualize the LES; it provides a robust auxiliary\ndecision-making mechanism to the operators. In this sense, the proposed method\nconducts EED with a pure statistical procedure, requiring no knowledge of\nsystem topologies, unit operation/control models, etc. The LES, as a key\ningredient during this procedure, is a high dimensional indictor derived\ndirectly from raw data. As an unsupervised learning indicator, the LES is much\nmore sensitive than the low dimensional indictors obtained from supervised\nlearning. With the statistical procedure, the proposed method is universal and\nfast; moreover, it is robust against traditional EED challenges (such as error\naccumulations, spurious correlations, and even bad data in core area). Case\nstudies, with both simulated data and real ones, validate the proposed method.\nTo manage large-scale distributed systems, data fusion is mentioned as another\ndata processing ingredient. \n\n"}
{"id": "1502.03167", "contents": "Title: Batch Normalization: Accelerating Deep Network Training by Reducing\n  Internal Covariate Shift Abstract: Training Deep Neural Networks is complicated by the fact that the\ndistribution of each layer's inputs changes during training, as the parameters\nof the previous layers change. This slows down the training by requiring lower\nlearning rates and careful parameter initialization, and makes it notoriously\nhard to train models with saturating nonlinearities. We refer to this\nphenomenon as internal covariate shift, and address the problem by normalizing\nlayer inputs. Our method draws its strength from making normalization a part of\nthe model architecture and performing the normalization for each training\nmini-batch. Batch Normalization allows us to use much higher learning rates and\nbe less careful about initialization. It also acts as a regularizer, in some\ncases eliminating the need for Dropout. Applied to a state-of-the-art image\nclassification model, Batch Normalization achieves the same accuracy with 14\ntimes fewer training steps, and beats the original model by a significant\nmargin. Using an ensemble of batch-normalized networks, we improve upon the\nbest published result on ImageNet classification: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the accuracy of human raters. \n\n"}
{"id": "1502.03475", "contents": "Title: Combinatorial Bandits Revisited Abstract: This paper investigates stochastic and adversarial combinatorial multi-armed\nbandit problems. In the stochastic setting under semi-bandit feedback, we\nderive a problem-specific regret lower bound, and discuss its scaling with the\ndimension of the decision space. We propose ESCB, an algorithm that efficiently\nexploits the structure of the problem and provide a finite-time analysis of its\nregret. ESCB has better performance guarantees than existing algorithms, and\nsignificantly outperforms these algorithms in practice. In the adversarial\nsetting under bandit feedback, we propose \\textsc{CombEXP}, an algorithm with\nthe same regret scaling as state-of-the-art algorithms, but with lower\ncomputational complexity for some combinatorial problems. \n\n"}
{"id": "1502.05556", "contents": "Title: Just Sort It! A Simple and Effective Approach to Active Preference\n  Learning Abstract: We address the problem of learning a ranking by using adaptively chosen\npairwise comparisons. Our goal is to recover the ranking accurately but to\nsample the comparisons sparingly. If all comparison outcomes are consistent\nwith the ranking, the optimal solution is to use an efficient sorting\nalgorithm, such as Quicksort. But how do sorting algorithms behave if some\ncomparison outcomes are inconsistent with the ranking? We give favorable\nguarantees for Quicksort for the popular Bradley-Terry model, under natural\nassumptions on the parameters. Furthermore, we empirically demonstrate that\nsorting algorithms lead to a very simple and effective active learning\nstrategy: repeatedly sort the items. This strategy performs as well as\nstate-of-the-art methods (and much better than random sampling) at a minuscule\nfraction of the computational cost. \n\n"}
{"id": "1502.06922", "contents": "Title: Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis\n  and Application to Information Retrieval Abstract: This paper develops a model that addresses sentence embedding, a hot topic in\ncurrent natural language processing research, using recurrent neural networks\nwith Long Short-Term Memory (LSTM) cells. Due to its ability to capture long\nterm memory, the LSTM-RNN accumulates increasingly richer information as it\ngoes through the sentence, and when it reaches the last word, the hidden layer\nof the network provides a semantic representation of the whole sentence. In\nthis paper, the LSTM-RNN is trained in a weakly supervised manner on user\nclick-through data logged by a commercial web search engine. Visualization and\nanalysis are performed to understand how the embedding process works. The model\nis found to automatically attenuate the unimportant words and detects the\nsalient keywords in the sentence. Furthermore, these detected keywords are\nfound to automatically activate different cells of the LSTM-RNN, where words\nbelonging to a similar topic activate the same cell. As a semantic\nrepresentation of the sentence, the embedding vector can be used in many\ndifferent applications. These automatic keyword detection and topic allocation\nabilities enabled by the LSTM-RNN allow the network to perform document\nretrieval, a difficult language processing task, where the similarity between\nthe query and documents can be measured by the distance between their\ncorresponding sentence embedding vectors computed by the LSTM-RNN. On a web\nsearch task, the LSTM-RNN embedding is shown to significantly outperform\nseveral existing state of the art methods. We emphasize that the proposed model\ngenerates sentence embedding vectors that are specially useful for web document\nretrieval tasks. A comparison with a well known general sentence embedding\nmethod, the Paragraph Vector, is performed. The results show that the proposed\nmethod in this paper significantly outperforms it for web document retrieval\ntask. \n\n"}
{"id": "1503.00038", "contents": "Title: Sequential Feature Explanations for Anomaly Detection Abstract: In many applications, an anomaly detection system presents the most anomalous\ndata instance to a human analyst, who then must determine whether the instance\nis truly of interest (e.g. a threat in a security setting). Unfortunately, most\nanomaly detectors provide no explanation about why an instance was considered\nanomalous, leaving the analyst with no guidance about where to begin the\ninvestigation. To address this issue, we study the problems of computing and\nevaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE\nof an anomaly is a sequence of features, which are presented to the analyst one\nat a time (in order) until the information contained in the highlighted\nfeatures is enough for the analyst to make a confident judgement about the\nanomaly. Since analyst effort is related to the amount of information that they\nconsider in an investigation, an explanation's quality is related to the number\nof features that must be revealed to attain confidence. One of our main\ncontributions is to present a novel framework for large scale quantitative\nevaluations of SFEs, where the quality measure is based on analyst effort. To\ndo this we construct anomaly detection benchmarks from real data sets along\nwith artificial experts that can be simulated for evaluation. Our second\ncontribution is to evaluate several novel explanation approaches within the\nframework and on traditional anomaly detection benchmarks, offering several\ninsights into the approaches. \n\n"}
{"id": "1503.02531", "contents": "Title: Distilling the Knowledge in a Neural Network Abstract: A very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to\naverage their predictions. Unfortunately, making predictions using a whole\nensemble of models is cumbersome and may be too computationally expensive to\nallow deployment to a large number of users, especially if the individual\nmodels are large neural nets. Caruana and his collaborators have shown that it\nis possible to compress the knowledge in an ensemble into a single model which\nis much easier to deploy and we develop this approach further using a different\ncompression technique. We achieve some surprising results on MNIST and we show\nthat we can significantly improve the acoustic model of a heavily used\ncommercial system by distilling the knowledge in an ensemble of models into a\nsingle model. We also introduce a new type of ensemble composed of one or more\nfull models and many specialist models which learn to distinguish fine-grained\nclasses that the full models confuse. Unlike a mixture of experts, these\nspecialist models can be trained rapidly and in parallel. \n\n"}
{"id": "1503.02946", "contents": "Title: apsis - Framework for Automated Optimization of Machine Learning Hyper\n  Parameters Abstract: The apsis toolkit presented in this paper provides a flexible framework for\nhyperparameter optimization and includes both random search and a bayesian\noptimizer. It is implemented in Python and its architecture features\nadaptability to any desired machine learning code. It can easily be used with\ncommon Python ML frameworks such as scikit-learn. Published under the MIT\nLicense other researchers are heavily encouraged to check out the code,\ncontribute or raise any suggestions. The code can be found at\ngithub.com/FrederikDiehl/apsis. \n\n"}
{"id": "1503.03613", "contents": "Title: On the Impossibility of Learning the Missing Mass Abstract: This paper shows that one cannot learn the probability of rare events without\nimposing further structural assumptions. The event of interest is that of\nobtaining an outcome outside the coverage of an i.i.d. sample from a discrete\ndistribution. The probability of this event is referred to as the \"missing\nmass\". The impossibility result can then be stated as: the missing mass is not\ndistribution-free PAC-learnable in relative error. The proof is\nsemi-constructive and relies on a coupling argument using a dithered geometric\ndistribution. This result formalizes the folklore that in order to predict rare\nevents, one necessarily needs distributions with \"heavy tails\". \n\n"}
{"id": "1503.03712", "contents": "Title: On Graduated Optimization for Stochastic Non-Convex Problems Abstract: The graduated optimization approach, also known as the continuation method,\nis a popular heuristic to solving non-convex problems that has received renewed\ninterest over the last decade. Despite its popularity, very little is known in\nterms of theoretical convergence analysis. In this paper we describe a new\nfirst-order algorithm based on graduated optimiza- tion and analyze its\nperformance. We characterize a parameterized family of non- convex functions\nfor which this algorithm provably converges to a global optimum. In particular,\nwe prove that the algorithm converges to an {\\epsilon}-approximate solution\nwithin O(1/\\epsilon^2) gradient-based steps. We extend our algorithm and\nanalysis to the setting of stochastic non-convex optimization with noisy\ngradient feedback, attaining the same convergence rate. Additionally, we\ndiscuss the setting of zero-order optimization, and devise a a variant of our\nalgorithm which converges at rate of O(d^2/\\epsilon^4). \n\n"}
{"id": "1503.06833", "contents": "Title: On Lower and Upper Bounds for Smooth and Strongly Convex Optimization\n  Problems Abstract: We develop a novel framework to study smooth and strongly convex optimization\nalgorithms, both deterministic and stochastic. Focusing on quadratic functions\nwe are able to examine optimization algorithms as a recursive application of\nlinear operators. This, in turn, reveals a powerful connection between a class\nof optimization algorithms and the analytic theory of polynomials whereby new\nlower and upper bounds are derived. Whereas existing lower bounds for this\nsetting are only valid when the dimensionality scales with the number of\niterations, our lower bound holds in the natural regime where the\ndimensionality is fixed. Lastly, expressing it as an optimal solution for the\ncorresponding optimization problem over polynomials, as formulated by our\nframework, we present a novel systematic derivation of Nesterov's well-known\nAccelerated Gradient Descent method. This rather natural interpretation of AGD\ncontrasts with earlier ones which lacked a simple, yet solid, motivation. \n\n"}
{"id": "1503.08650", "contents": "Title: Comparison of Bayesian predictive methods for model selection Abstract: The goal of this paper is to compare several widely used Bayesian model\nselection methods in practical model selection problems, highlight their\ndifferences and give recommendations about the preferred approaches. We focus\non the variable subset selection for regression and classification and perform\nseveral numerical experiments using both simulated and real world data. The\nresults show that the optimization of a utility estimate such as the\ncross-validation (CV) score is liable to finding overfitted models due to\nrelatively high variance in the utility estimates when the data is scarce. This\ncan also lead to substantial selection induced bias and optimism in the\nperformance evaluation for the selected model. From a predictive viewpoint,\nbest results are obtained by accounting for model uncertainty by forming the\nfull encompassing model, such as the Bayesian model averaging solution over the\ncandidate models. If the encompassing model is too complex, it can be robustly\nsimplified by the projection method, in which the information of the full model\nis projected onto the submodels. This approach is substantially less prone to\noverfitting than selection based on CV-score. Overall, the projection method\nappears to outperform also the maximum a posteriori model and the selection of\nthe most probable variables. The study also demonstrates that the model\nselection can greatly benefit from using cross-validation outside the searching\nprocess both for guiding the model size selection and assessing the predictive\nperformance of the finally selected model. \n\n"}
{"id": "1504.04686", "contents": "Title: Local, Private, Efficient Protocols for Succinct Histograms Abstract: We give efficient protocols and matching accuracy lower bounds for frequency\nestimation in the local model for differential privacy. In this model,\nindividual users randomize their data themselves, sending differentially\nprivate reports to an untrusted server that aggregates them.\n  We study protocols that produce a succinct histogram representation of the\ndata. A succinct histogram is a list of the most frequent items in the data\n(often called \"heavy hitters\") along with estimates of their frequencies; the\nfrequency of all other items is implicitly estimated as 0.\n  If there are $n$ users whose items come from a universe of size $d$, our\nprotocols run in time polynomial in $n$ and $\\log(d)$. With high probability,\nthey estimate the accuracy of every item up to error\n$O\\left(\\sqrt{\\log(d)/(\\epsilon^2n)}\\right)$ where $\\epsilon$ is the privacy\nparameter. Moreover, we show that this much error is necessary, regardless of\ncomputational efficiency, and even for the simple setting where only one item\nappears with significant frequency in the data set.\n  Previous protocols (Mishra and Sandler, 2006; Hsu, Khanna and Roth, 2012) for\nthis task either ran in time $\\Omega(d)$ or had much worse error (about\n$\\sqrt[6]{\\log(d)/(\\epsilon^2n)}$), and the only known lower bound on error was\n$\\Omega(1/\\sqrt{n})$.\n  We also adapt a result of McGregor et al (2010) to the local setting. In a\nmodel with public coins, we show that each user need only send 1 bit to the\nserver. For all known local protocols (including ours), the transformation\npreserves computational efficiency. \n\n"}
{"id": "1504.08022", "contents": "Title: A Deep Learning Model for Structured Outputs with High-order Interaction Abstract: Many real-world applications are associated with structured data, where not\nonly input but also output has interplay. However, typical classification and\nregression models often lack the ability of simultaneously exploring high-order\ninteraction within input and that within output. In this paper, we present a\ndeep learning model aiming to generate a powerful nonlinear functional mapping\nfrom structured input to structured output. More specifically, we propose to\nintegrate high-order hidden units, guided discriminative pretraining, and\nhigh-order auto-encoders for this purpose. We evaluate the model with three\ndatasets, and obtain state-of-the-art performances among competitive methods.\nOur current work focuses on structured output regression, which is a less\nexplored area, although the model can be extended to handle structured label\nclassification. \n\n"}
{"id": "1505.02214", "contents": "Title: An Empirical Study of Leading Measures of Dependence Abstract: In exploratory data analysis, we are often interested in identifying\npromising pairwise associations for further analysis while filtering out\nweaker, less interesting ones. This can be accomplished by computing a measure\nof dependence on all variable pairs and examining the highest-scoring pairs,\nprovided the measure of dependence used assigns similar scores to equally noisy\nrelationships of different types. This property, called equitability, is\nformalized in Reshef et al. [2015b]. In addition to equitability, measures of\ndependence can also be assessed by the power of their corresponding\nindependence tests as well as their runtime.\n  Here we present extensive empirical evaluation of the equitability, power\nagainst independence, and runtime of several leading measures of dependence.\nThese include two statistics introduced in Reshef et al. [2015a]: MICe, which\nhas equitability as its primary goal, and TICe, which has power against\nindependence as its goal. Regarding equitability, our analysis finds that MICe\nis the most equitable method on functional relationships in most of the\nsettings we considered, although mutual information estimation proves the most\nequitable at large sample sizes in some specific settings. Regarding power\nagainst independence, we find that TICe, along with Heller and Gorfine's S^DDP,\nis the state of the art on the relationships we tested. Our analyses also show\na trade-off between power against independence and equitability consistent with\nthe theory in Reshef et al. [2015b]. In terms of runtime, MICe and TICe are\nsignificantly faster than many other measures of dependence tested, and\ncomputing either one makes computing the other trivial. This suggests that a\nfast and useful strategy for achieving a combination of power against\nindependence and equitability may be to filter relationships by TICe and then\nto examine the MICe of only the significant ones. \n\n"}
{"id": "1505.03410", "contents": "Title: Mind the duality gap: safer rules for the Lasso Abstract: Screening rules allow to early discard irrelevant variables from the\noptimization in Lasso problems, or its derivatives, making solvers faster. In\nthis paper, we propose new versions of the so-called $\\textit{safe rules}$ for\nthe Lasso. Based on duality gap considerations, our new rules create safe test\nregions whose diameters converge to zero, provided that one relies on a\nconverging solver. This property helps screening out more variables, for a\nwider range of regularization parameter values. In addition to faster\nconvergence, we prove that we correctly identify the active sets (supports) of\nthe solutions in finite time. While our proposed strategy can cope with any\nsolver, its performance is demonstrated using a coordinate descent algorithm\nparticularly adapted to machine learning use cases. Significant computing time\nreductions are obtained with respect to previous safe rules. \n\n"}
{"id": "1505.04215", "contents": "Title: An Analysis of Active Learning With Uniform Feature Noise Abstract: In active learning, the user sequentially chooses values for feature $X$ and\nan oracle returns the corresponding label $Y$. In this paper, we consider the\neffect of feature noise in active learning, which could arise either because\n$X$ itself is being measured, or it is corrupted in transmission to the oracle,\nor the oracle returns the label of a noisy version of the query point. In\nstatistics, feature noise is known as \"errors in variables\" and has been\nstudied extensively in non-active settings. However, the effect of feature\nnoise in active learning has not been studied before. We consider the\nwell-known Berkson errors-in-variables model with additive uniform noise of\nwidth $\\sigma$.\n  Our simple but revealing setting is that of one-dimensional binary\nclassification setting where the goal is to learn a threshold (point where the\nprobability of a $+$ label crosses half). We deal with regression functions\nthat are antisymmetric in a region of size $\\sigma$ around the threshold and\nalso satisfy Tsybakov's margin condition around the threshold. We prove minimax\nlower and upper bounds which demonstrate that when $\\sigma$ is smaller than the\nminimiax active/passive noiseless error derived in \\cite{CN07}, then noise has\nno effect on the rates and one achieves the same noiseless rates. For larger\n$\\sigma$, the \\textit{unflattening} of the regression function on convolution\nwith uniform noise, along with its local antisymmetry around the threshold,\ntogether yield a behaviour where noise \\textit{appears} to be beneficial. Our\nkey result is that active learning can buy significant improvement over a\npassive strategy even in the presence of feature noise. \n\n"}
{"id": "1505.04771", "contents": "Title: DopeLearning: A Computational Approach to Rap Lyrics Generation Abstract: Writing rap lyrics requires both creativity to construct a meaningful,\ninteresting story and lyrical skills to produce complex rhyme patterns, which\nform the cornerstone of good flow. We present a rap lyrics generation method\nthat captures both of these aspects. First, we develop a prediction model to\nidentify the next line of existing lyrics from a set of candidate next lines.\nThis model is based on two machine-learning techniques: the RankSVM algorithm\nand a deep neural network model with a novel structure. Results show that the\nprediction model can identify the true next line among 299 randomly selected\nlines with an accuracy of 17%, i.e., over 50 times more likely than by random.\nSecond, we employ the prediction model to combine lines from existing songs,\nproducing lyrics with rhyme and a meaning. An evaluation of the produced lyrics\nshows that in terms of quantitative rhyme density, the method outperforms the\nbest human rappers by 21%. The rap lyrics generator has been deployed as an\nonline tool called DeepBeat, and the performance of the tool has been assessed\nby analyzing its usage logs. This analysis shows that machine-learned rankings\ncorrelate with user preferences. \n\n"}
{"id": "1505.05215", "contents": "Title: Learning with a Drifting Target Concept Abstract: We study the problem of learning in the presence of a drifting target\nconcept. Specifically, we provide bounds on the error rate at a given time,\ngiven a learner with access to a history of independent samples labeled\naccording to a target concept that can change on each round. One of our main\ncontributions is a refinement of the best previous results for polynomial-time\nalgorithms for the space of linear separators under a uniform distribution. We\nalso provide general results for an algorithm capable of adapting to a variable\nrate of drift of the target concept. Some of the results also describe an\nactive learning variant of this setting, and provide bounds on the number of\nqueries for the labels of points in the sequence sufficient to obtain the\nstated bounds on the error rates. \n\n"}
{"id": "1505.05770", "contents": "Title: Variational Inference with Normalizing Flows Abstract: The choice of approximate posterior distribution is one of the core problems\nin variational inference. Most applications of variational inference employ\nsimple families of posterior approximations in order to allow for efficient\ninference, focusing on mean-field or other simple structured approximations.\nThis restriction has a significant impact on the quality of inferences made\nusing variational methods. We introduce a new approach for specifying flexible,\narbitrarily complex and scalable approximate posterior distributions. Our\napproximations are distributions constructed through a normalizing flow,\nwhereby a simple initial density is transformed into a more complex one by\napplying a sequence of invertible transformations until a desired level of\ncomplexity is attained. We use this view of normalizing flows to develop\ncategories of finite and infinitesimal flows and provide a unified view of\napproaches for constructing rich posterior approximations. We demonstrate that\nthe theoretical advantages of having posteriors that better match the true\nposterior, combined with the scalability of amortized variational approaches,\nprovides a clear improvement in performance and applicability of variational\ninference. \n\n"}
{"id": "1505.06770", "contents": "Title: Sketching for Sequential Change-Point Detection Abstract: We study sequential change-point detection procedures based on linear\nsketches of high-dimensional signal vectors using generalized likelihood ratio\n(GLR) statistics. The GLR statistics allow for an unknown post-change mean that\nrepresents an anomaly or novelty. We consider both fixed and time-varying\nprojections, derive theoretical approximations to two fundamental performance\nmetrics: the average run length (ARL) and the expected detection delay (EDD);\nthese approximations are shown to be highly accurate by numerical simulations.\nWe further characterize the relative performance measure of the sketching\nprocedure compared to that without sketching and show that there can be little\nperformance loss when the signal strength is sufficiently large, and enough\nnumber of sketches are used. Finally, we demonstrate the good performance of\nsketching procedures using simulation and real-data examples on solar flare\ndetection and failure detection in power networks. \n\n"}
{"id": "1505.06915", "contents": "Title: Large-scale Machine Learning for Metagenomics Sequence Classification Abstract: Metagenomics characterizes the taxonomic diversity of microbial communities\nby sequencing DNA directly from an environmental sample. One of the main\nchallenges in metagenomics data analysis is the binning step, where each\nsequenced read is assigned to a taxonomic clade. Due to the large volume of\nmetagenomics datasets, binning methods need fast and accurate algorithms that\ncan operate with reasonable computing requirements. While standard\nalignment-based methods provide state-of-the-art performance, compositional\napproaches that assign a taxonomic class to a DNA read based on the k-mers it\ncontains have the potential to provide faster solutions. In this work, we\ninvestigate the potential of modern, large-scale machine learning\nimplementations for taxonomic affectation of next-generation sequencing reads\nbased on their k-mers profile. We show that machine learning-based\ncompositional approaches benefit from increasing the number of fragments\nsampled from reference genome to tune their parameters, up to a coverage of\nabout 10, and from increasing the k-mer size to about 12. Tuning these models\ninvolves training a machine learning model on about 10 8 samples in 10 7\ndimensions, which is out of reach of standard soft-wares but can be done\nefficiently with modern implementations for large-scale machine learning. The\nresulting models are competitive in terms of accuracy with well-established\nalignment tools for problems involving a small to moderate number of candidate\nspecies, and for reasonable amounts of sequencing errors. We show, however,\nthat compositional approaches are still limited in their ability to deal with\nproblems involving a greater number of species, and more sensitive to\nsequencing errors. We finally confirm that compositional approach achieve\nfaster prediction times, with a gain of 3 to 15 times with respect to the\nBWA-MEM short read mapper, depending on the number of candidate species and the\nlevel of sequencing noise. \n\n"}
{"id": "1506.00619", "contents": "Title: Blocks and Fuel: Frameworks for deep learning Abstract: We introduce two Python frameworks to train neural networks on large\ndatasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler\nwith CUDA-support. It facilitates the training of complex neural network models\nby providing parametrized Theano operations, attaching metadata to Theano's\nsymbolic computational graph, and providing an extensive set of utilities to\nassist training the networks, e.g. training algorithms, logging, monitoring,\nvisualization, and serialization. Fuel provides a standard format for machine\nlearning datasets. It allows the user to easily iterate over large datasets,\nperforming many types of pre-processing on the fly. \n\n"}
{"id": "1506.01911", "contents": "Title: Beyond Temporal Pooling: Recurrence and Temporal Convolutions for\n  Gesture Recognition in Video Abstract: Recent studies have demonstrated the power of recurrent neural networks for\nmachine translation, image captioning and speech recognition. For the task of\ncapturing temporal structure in video, however, there still remain numerous\nopen research questions. Current research suggests using a simple temporal\nfeature pooling strategy to take into account the temporal aspect of video. We\ndemonstrate that this method is not sufficient for gesture recognition, where\ntemporal information is more discriminative compared to general video\nclassification tasks. We explore deep architectures for gesture recognition in\nvideo and propose a new end-to-end trainable neural network architecture\nincorporating temporal convolutions and bidirectional recurrence. Our main\ncontributions are twofold; first, we show that recurrence is crucial for this\ntask; second, we show that adding temporal convolutions leads to significant\nimprovements. We evaluate the different approaches on the Montalbano gesture\nrecognition dataset, where we achieve state-of-the-art results. \n\n"}
{"id": "1506.02516", "contents": "Title: Learning to Transduce with Unbounded Memory Abstract: Recently, strong results have been demonstrated by Deep Recurrent Neural\nNetworks on natural language transduction problems. In this paper we explore\nthe representational power of these models using synthetic grammars designed to\nexhibit phenomena similar to those found in real transduction problems such as\nmachine translation. These experiments lead us to propose new memory-based\nrecurrent networks that implement continuously differentiable analogues of\ntraditional data structures such as Stacks, Queues, and DeQues. We show that\nthese architectures exhibit superior generalisation performance to Deep RNNs\nand are often able to learn the underlying generating algorithms in our\ntransduction experiments. \n\n"}
{"id": "1506.02582", "contents": "Title: On Convergence of Emphatic Temporal-Difference Learning Abstract: We consider emphatic temporal-difference learning algorithms for policy\nevaluation in discounted Markov decision processes with finite spaces. Such\nalgorithms were recently proposed by Sutton, Mahmood, and White (2015) as an\nimproved solution to the problem of divergence of off-policy\ntemporal-difference learning with linear function approximation. We present in\nthis paper the first convergence proofs for two emphatic algorithms,\nETD($\\lambda$) and ELSTD($\\lambda$). We prove, under general off-policy\nconditions, the convergence in $L^1$ for ELSTD($\\lambda$) iterates, and the\nalmost sure convergence of the approximate value functions calculated by both\nalgorithms using a single infinitely long trajectory. Our analysis involves new\ntechniques with applications beyond emphatic algorithms leading, for example,\nto the first proof that standard TD($\\lambda$) also converges under off-policy\ntraining for $\\lambda$ sufficiently large. \n\n"}
{"id": "1506.03509", "contents": "Title: Convolutional Dictionary Learning through Tensor Factorization Abstract: Tensor methods have emerged as a powerful paradigm for consistent learning of\nmany latent variable models such as topic models, independent component\nanalysis and dictionary learning. Model parameters are estimated via CP\ndecomposition of the observed higher order input moments. However, in many\ndomains, additional invariances such as shift invariances exist, enforced via\nmodels such as convolutional dictionary learning. In this paper, we develop\nnovel tensor decomposition algorithms for parameter estimation of convolutional\nmodels. Our algorithm is based on the popular alternating least squares method,\nbut with efficient projections onto the space of stacked circulant matrices.\nOur method is embarrassingly parallel and consists of simple operations such as\nfast Fourier transforms and matrix multiplications. Our algorithm converges to\nthe dictionary much faster and more accurately compared to the alternating\nminimization over filters and activation maps. \n\n"}
{"id": "1506.05232", "contents": "Title: On the Depth of Deep Neural Networks: A Theoretical View Abstract: People believe that depth plays an important role in success of deep neural\nnetworks (DNN). However, this belief lacks solid theoretical justifications as\nfar as we know. We investigate role of depth from perspective of margin bound.\nIn margin bound, expected error is upper bounded by empirical margin error plus\nRademacher Average (RA) based capacity term. First, we derive an upper bound\nfor RA of DNN, and show that it increases with increasing depth. This indicates\nnegative impact of depth on test performance. Second, we show that deeper\nnetworks tend to have larger representation power (measured by Betti numbers\nbased complexity) than shallower networks in multi-class setting, and thus can\nlead to smaller empirical margin error. This implies positive impact of depth.\nThe combination of these two results shows that for DNN with restricted number\nof hidden units, increasing depth is not always good since there is a tradeoff\nbetween positive and negative impacts. These results inspire us to seek\nalternative ways to achieve positive impact of depth, e.g., imposing\nmargin-based penalty terms to cross entropy loss so as to reduce empirical\nmargin error without increasing depth. Our experiments show that in this way,\nwe achieve significantly better test performance. \n\n"}
{"id": "1506.05439", "contents": "Title: Learning with a Wasserstein Loss Abstract: Learning to predict multi-label outputs is challenging, but in many problems\nthere is a natural metric on the outputs that can be used to improve\npredictions. In this paper we develop a loss function for multi-label learning,\nbased on the Wasserstein distance. The Wasserstein distance provides a natural\nnotion of dissimilarity for probability measures. Although optimizing with\nrespect to the exact Wasserstein distance is costly, recent work has described\na regularized approximation that is efficiently computed. We describe an\nefficient learning algorithm based on this regularization, as well as a novel\nextension of the Wasserstein distance from probability measures to unnormalized\nmeasures. We also describe a statistical learning bound for the loss. The\nWasserstein loss can encourage smoothness of the predictions with respect to a\nchosen metric on the output space. We demonstrate this property on a real-data\ntag prediction problem, using the Yahoo Flickr Creative Commons dataset,\noutperforming a baseline that doesn't use the metric. \n\n"}
{"id": "1506.07503", "contents": "Title: Attention-Based Models for Speech Recognition Abstract: Recurrent sequence generators conditioned on input data through an attention\nmechanism have recently shown very good performance on a range of tasks in-\ncluding machine translation, handwriting synthesis and image caption gen-\neration. We extend the attention-mechanism with features needed for speech\nrecognition. We show that while an adaptation of the model used for machine\ntranslation in reaches a competitive 18.7% phoneme error rate (PER) on the\nTIMIT phoneme recognition task, it can only be applied to utterances which are\nroughly as long as the ones it was trained on. We offer a qualitative\nexplanation of this failure and propose a novel and generic method of adding\nlocation-awareness to the attention mechanism to alleviate this issue. The new\nmethod yields a model that is robust to long inputs and achieves 18% PER in\nsingle utterances and 20% in 10-times longer (repeated) utterances. Finally, we\npropose a change to the at- tention mechanism that prevents it from\nconcentrating too much on single frames, which further reduces PER to 17.6%\nlevel. \n\n"}
{"id": "1506.07704", "contents": "Title: AttentionNet: Aggregating Weak Directions for Accurate Object Detection Abstract: We present a novel detection method using a deep convolutional neural network\n(CNN), named AttentionNet. We cast an object detection problem as an iterative\nclassification problem, which is the most suitable form of a CNN. AttentionNet\nprovides quantized weak directions pointing a target object and the ensemble of\niterative predictions from AttentionNet converges to an accurate object\nboundary box. Since AttentionNet is a unified network for object detection, it\ndetects objects without any separated models from the object proposal to the\npost bounding-box regression. We evaluate AttentionNet by a human detection\ntask and achieve the state-of-the-art performance of 65% (AP) on PASCAL VOC\n2007/2012 with an 8-layered architecture only. \n\n"}
{"id": "1507.01476", "contents": "Title: Semi-proximal Mirror-Prox for Nonsmooth Composite Minimization Abstract: We propose a new first-order optimisation algorithm to solve high-dimensional\nnon-smooth composite minimisation problems. Typical examples of such problems\nhave an objective that decomposes into a non-smooth empirical risk part and a\nnon-smooth regularisation penalty. The proposed algorithm, called Semi-Proximal\nMirror-Prox, leverages the Fenchel-type representation of one part of the\nobjective while handling the other part of the objective via linear\nminimization over the domain. The algorithm stands in contrast with more\nclassical proximal gradient algorithms with smoothing, which require the\ncomputation of proximal operators at each iteration and can therefore be\nimpractical for high-dimensional problems. We establish the theoretical\nconvergence rate of Semi-Proximal Mirror-Prox, which exhibits the optimal\ncomplexity bounds, i.e. $O(1/\\epsilon^2)$, for the number of calls to linear\nminimization oracle. We present promising experimental results showing the\ninterest of the approach in comparison to competing methods. \n\n"}
{"id": "1507.02268", "contents": "Title: Optimal approximate matrix product in terms of stable rank Abstract: We prove, using the subspace embedding guarantee in a black box way, that one\ncan achieve the spectral norm guarantee for approximate matrix multiplication\nwith a dimensionality-reducing map having $m = O(\\tilde{r}/\\varepsilon^2)$\nrows. Here $\\tilde{r}$ is the maximum stable rank, i.e. squared ratio of\nFrobenius and operator norms, of the two matrices being multiplied. This is a\nquantitative improvement over previous work of [MZ11, KVZ14], and is also\noptimal for any oblivious dimensionality-reducing map. Furthermore, due to the\nblack box reliance on the subspace embedding property in our proofs, our\ntheorem can be applied to a much more general class of sketching matrices than\nwhat was known before, in addition to achieving better bounds. For example, one\ncan apply our theorem to efficient subspace embeddings such as the Subsampled\nRandomized Hadamard Transform or sparse subspace embeddings, or even with\nsubspace embedding constructions that may be developed in the future.\n  Our main theorem, via connections with spectral error matrix multiplication\nshown in prior work, implies quantitative improvements for approximate least\nsquares regression and low rank approximation. Our main result has also already\nbeen applied to improve dimensionality reduction guarantees for $k$-means\nclustering [CEMMP14], and implies new results for nonparametric regression\n[YPW15].\n  We also separately point out that the proof of the \"BSS\" deterministic\nrow-sampling result of [BSS12] can be modified to show that for any matrices\n$A, B$ of stable rank at most $\\tilde{r}$, one can achieve the spectral norm\nguarantee for approximate matrix multiplication of $A^T B$ by deterministically\nsampling $O(\\tilde{r}/\\varepsilon^2)$ rows that can be found in polynomial\ntime. The original result of [BSS12] was for rank instead of stable rank. Our\nobservation leads to a stronger version of a main theorem of [KMST10]. \n\n"}
{"id": "1507.02387", "contents": "Title: Decentralized Joint-Sparse Signal Recovery: A Sparse Bayesian Learning\n  Approach Abstract: This work proposes a decentralized, iterative, Bayesian algorithm called\nCB-DSBL for in-network estimation of multiple jointly sparse vectors by a\nnetwork of nodes, using noisy and underdetermined linear measurements. The\nproposed algorithm exploits the network wide joint sparsity of the un- known\nsparse vectors to recover them from significantly fewer number of local\nmeasurements compared to standalone sparse signal recovery schemes. To reduce\nthe amount of inter-node communication and the associated overheads, the nodes\nexchange messages with only a small subset of their single hop neighbors. Under\nthis communication scheme, we separately analyze the convergence of the\nunderlying Alternating Directions Method of Multipliers (ADMM) iterations used\nin our proposed algorithm and establish its linear convergence rate. The\nfindings from the convergence analysis of decentralized ADMM are used to\naccelerate the convergence of the proposed CB-DSBL algorithm. Using Monte Carlo\nsimulations, we demonstrate the superior signal reconstruction as well as\nsupport recovery performance of our proposed algorithm compared to existing\ndecentralized algorithms: DRL-1, DCOMP and DCSP. \n\n"}
{"id": "1507.02528", "contents": "Title: Faster Convex Optimization: Simulated Annealing with an Efficient\n  Universal Barrier Abstract: This paper explores a surprising equivalence between two seemingly-distinct\nconvex optimization methods. We show that simulated annealing, a well-studied\nrandom walk algorithms, is directly equivalent, in a certain sense, to the\ncentral path interior point algorithm for the the entropic universal barrier\nfunction. This connection exhibits several benefits. First, we are able improve\nthe state of the art time complexity for convex optimization under the\nmembership oracle model. We improve the analysis of the randomized algorithm of\nKalai and Vempala by utilizing tools developed by Nesterov and Nemirovskii that\nunderly the central path following interior point algorithm. We are able to\ntighten the temperature schedule for simulated annealing which gives an\nimproved running time, reducing by square root of the dimension in certain\ninstances. Second, we get an efficient randomized interior point method with an\nefficiently computable universal barrier for any convex set described by a\nmembership oracle. Previously, efficiently computable barriers were known only\nfor particular convex sets. \n\n"}
{"id": "1507.05775", "contents": "Title: Compression of Fully-Connected Layer in Neural Network by Kronecker\n  Product Abstract: In this paper we propose and study a technique to reduce the number of\nparameters and computation time in fully-connected layers of neural networks\nusing Kronecker product, at a mild cost of the prediction quality. The\ntechnique proceeds by replacing Fully-Connected layers with so-called Kronecker\nFully-Connected layers, where the weight matrices of the FC layers are\napproximated by linear combinations of multiple Kronecker products of smaller\nmatrices. In particular, given a model trained on SVHN dataset, we are able to\nconstruct a new KFC model with 73\\% reduction in total number of parameters,\nwhile the error only rises mildly. In contrast, using low-rank method can only\nachieve 35\\% reduction in total number of parameters given similar quality\ndegradation allowance. If we only compare the KFC layer with its counterpart\nfully-connected layer, the reduction in the number of parameters exceeds 99\\%.\nThe amount of computation is also reduced as we replace matrix product of the\nlarge matrices in FC layers with matrix products of a few smaller matrices in\nKFC layers. Further experiments on MNIST, SVHN and some Chinese Character\nrecognition models also demonstrate effectiveness of our technique. \n\n"}
{"id": "1508.00451", "contents": "Title: Integrated Inference and Learning of Neural Factors in Structural\n  Support Vector Machines Abstract: Tackling pattern recognition problems in areas such as computer vision,\nbioinformatics, speech or text recognition is often done best by taking into\naccount task-specific statistical relations between output variables. In\nstructured prediction, this internal structure is used to predict multiple\noutputs simultaneously, leading to more accurate and coherent predictions.\nStructural support vector machines (SSVMs) are nonprobabilistic models that\noptimize a joint input-output function through margin-based learning. Because\nSSVMs generally disregard the interplay between unary and interaction factors\nduring the training phase, final parameters are suboptimal. Moreover, its\nfactors are often restricted to linear combinations of input features, limiting\nits generalization power. To improve prediction accuracy, this paper proposes:\n(i) Joint inference and learning by integration of back-propagation and\nloss-augmented inference in SSVM subgradient descent; (ii) Extending SSVM\nfactors to neural networks that form highly nonlinear functions of input\nfeatures. Image segmentation benchmark results demonstrate improvements over\nconventional SSVM training methods in terms of accuracy, highlighting the\nfeasibility of end-to-end SSVM training with neural factors. \n\n"}
{"id": "1508.02593", "contents": "Title: Type-Constrained Representation Learning in Knowledge Graphs Abstract: Large knowledge graphs increasingly add value to various applications that\nrequire machines to recognize and understand queries and their semantics, as in\nsearch or question answering systems. Latent variable models have increasingly\ngained attention for the statistical modeling of knowledge graphs, showing\npromising results in tasks related to knowledge graph completion and cleaning.\nBesides storing facts about the world, schema-based knowledge graphs are backed\nby rich semantic descriptions of entities and relation-types that allow\nmachines to understand the notion of things and their semantic relationships.\nIn this work, we study how type-constraints can generally support the\nstatistical modeling with latent variable models. More precisely, we integrated\nprior knowledge in form of type-constraints in various state of the art latent\nvariable approaches. Our experimental results show that prior knowledge on\nrelation-types significantly improves these models up to 77% in link-prediction\ntasks. The achieved improvements are especially prominent when a low model\ncomplexity is enforced, a crucial requirement when these models are applied to\nvery large datasets. Unfortunately, type-constraints are neither always\navailable nor always complete e.g., they can become fuzzy when entities lack\nproper typing. We show that in these cases, it can be beneficial to apply a\nlocal closed-world assumption that approximates the semantics of relation-types\nbased on observations made in the data. \n\n"}
{"id": "1508.06585", "contents": "Title: Towards universal neural nets: Gibbs machines and ACE Abstract: We study from a physics viewpoint a class of generative neural nets, Gibbs\nmachines, designed for gradual learning. While including variational\nauto-encoders, they offer a broader universal platform for incrementally adding\nnewly learned features, including physical symmetries. Their direct connection\nto statistical physics and information geometry is established. A variational\nPythagorean theorem justifies invoking the exponential/Gibbs class of\nprobabilities for creating brand new objects. Combining these nets with\nclassifiers, gives rise to a brand of universal generative neural nets -\nstochastic auto-classifier-encoders (ACE). ACE have state-of-the-art\nperformance in their class, both for classification and density estimation for\nthe MNIST data set. \n\n"}
{"id": "1509.00114", "contents": "Title: Multi-Sensor Slope Change Detection Abstract: We develop a mixture procedure for multi-sensor systems to monitor data\nstreams for a change-point that causes a gradual degradation to a subset of the\nstreams. Observations are assumed to be initially normal random variables with\nknown constant means and variances. After the change-point, observations in the\nsubset will have increasing or decreasing means. The subset and the\nrate-of-changes are unknown. Our procedure uses a mixture statistics, which\nassumes that each sensor is affected by the change-point with probability\n$p_0$. Analytic expressions are obtained for the average run length (ARL) and\nthe expected detection delay (EDD) of the mixture procedure, which are\ndemonstrated to be quite accurate numerically. We establish the asymptotic\noptimality of the mixture procedure. Numerical examples demonstrate the good\nperformance of the proposed procedure. We also discuss an adaptive mixture\nprocedure using empirical Bayes. This paper extends our earlier work on\ndetecting an abrupt change-point that causes a mean-shift, by tackling the\nchallenges posed by the non-stationarity of the slope-change problem. \n\n"}
{"id": "1509.00519", "contents": "Title: Importance Weighted Autoencoders Abstract: The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently\nproposed generative model pairing a top-down generative network with a\nbottom-up recognition network which approximates posterior inference. It\ntypically makes strong assumptions about posterior inference, for instance that\nthe posterior distribution is approximately factorial, and that its parameters\ncan be approximated with nonlinear regression from the observations. As we show\nempirically, the VAE objective can lead to overly simplified representations\nwhich fail to use the network's entire modeling capacity. We present the\nimportance weighted autoencoder (IWAE), a generative model with the same\narchitecture as the VAE, but which uses a strictly tighter log-likelihood lower\nbound derived from importance weighting. In the IWAE, the recognition network\nuses multiple samples to approximate the posterior, giving it increased\nflexibility to model complex posteriors which do not fit the VAE modeling\nassumptions. We show empirically that IWAEs learn richer latent space\nrepresentations than VAEs, leading to improved test log-likelihood on density\nestimation benchmarks. \n\n"}
{"id": "1509.01469", "contents": "Title: Quantization based Fast Inner Product Search Abstract: We propose a quantization based approach for fast approximate Maximum Inner\nProduct Search (MIPS). Each database vector is quantized in multiple subspaces\nvia a set of codebooks, learned directly by minimizing the inner product\nquantization error. Then, the inner product of a query to a database vector is\napproximated as the sum of inner products with the subspace quantizers.\nDifferent from recently proposed LSH approaches to MIPS, the database vectors\nand queries do not need to be augmented in a higher dimensional feature space.\nWe also provide a theoretical analysis of the proposed approach, consisting of\nthe concentration results under mild assumptions. Furthermore, if a small\nsample of example queries is given at the training time, we propose a modified\ncodebook learning procedure which further improves the accuracy. Experimental\nresults on a variety of datasets including those arising from deep neural\nnetworks show that the proposed approach significantly outperforms the existing\nstate-of-the-art. \n\n"}
{"id": "1509.03475", "contents": "Title: Hessian-free Optimization for Learning Deep Multidimensional Recurrent\n  Neural Networks Abstract: Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable\nperformance in the area of speech and handwriting recognition. The performance\nof an MDRNN is improved by further increasing its depth, and the difficulty of\nlearning the deeper network is overcome by using Hessian-free (HF)\noptimization. Given that connectionist temporal classification (CTC) is\nutilized as an objective of learning an MDRNN for sequence labeling, the\nnon-convexity of CTC poses a problem when applying HF to the network. As a\nsolution, a convex approximation of CTC is formulated and its relationship with\nthe EM algorithm and the Fisher information matrix is discussed. An MDRNN up to\na depth of 15 layers is successfully trained using HF, resulting in an improved\nperformance for sequence labeling. \n\n"}
{"id": "1509.04640", "contents": "Title: Dynamic Poisson Factorization Abstract: Models for recommender systems use latent factors to explain the preferences\nand behaviors of users with respect to a set of items (e.g., movies, books,\nacademic papers). Typically, the latent factors are assumed to be static and,\ngiven these factors, the observed preferences and behaviors of users are\nassumed to be generated without order. These assumptions limit the explorative\nand predictive capabilities of such models, since users' interests and item\npopularity may evolve over time. To address this, we propose dPF, a dynamic\nmatrix factorization model based on the recent Poisson factorization model for\nrecommendations. dPF models the time evolving latent factors with a Kalman\nfilter and the actions with Poisson distributions. We derive a scalable\nvariational inference algorithm to infer the latent factors. Finally, we\ndemonstrate dPF on 10 years of user click data from arXiv.org, one of the\nlargest repository of scientific papers and a formidable source of information\nabout the behavior of scientists. Empirically we show performance improvement\nover both static and, more recently proposed, dynamic recommendation models. We\nalso provide a thorough exploration of the inferred posteriors over the latent\nvariables. \n\n"}
{"id": "1509.06461", "contents": "Title: Deep Reinforcement Learning with Double Q-learning Abstract: The popular Q-learning algorithm is known to overestimate action values under\ncertain conditions. It was not previously known whether, in practice, such\noverestimations are common, whether they harm performance, and whether they can\ngenerally be prevented. In this paper, we answer all these questions\naffirmatively. In particular, we first show that the recent DQN algorithm,\nwhich combines Q-learning with a deep neural network, suffers from substantial\noverestimations in some games in the Atari 2600 domain. We then show that the\nidea behind the Double Q-learning algorithm, which was introduced in a tabular\nsetting, can be generalized to work with large-scale function approximation. We\npropose a specific adaptation to the DQN algorithm and show that the resulting\nalgorithm not only reduces the observed overestimations, as hypothesized, but\nthat this also leads to much better performance on several games. \n\n"}
{"id": "1509.07577", "contents": "Title: A Review of Feature Selection Methods Based on Mutual Information Abstract: In this work we present a review of the state of the art of information\ntheoretic feature selection methods. The concepts of feature relevance,\nredundance and complementarity (synergy) are clearly defined, as well as Markov\nblanket. The problem of optimal feature selection is defined. A unifying\ntheoretical framework is described, which can retrofit successful heuristic\ncriteria, indicating the approximations made by each method. A number of open\nproblems in the field are presented. \n\n"}
{"id": "1509.08990", "contents": "Title: Learning without Recall: A Case for Log-Linear Learning Abstract: We analyze a model of learning and belief formation in networks in which\nagents follow Bayes rule yet they do not recall their history of past\nobservations and cannot reason about how other agents' beliefs are formed. They\ndo so by making rational inferences about their observations which include a\nsequence of independent and identically distributed private signals as well as\nthe beliefs of their neighboring agents at each time. Fully rational agents\nwould successively apply Bayes rule to the entire history of observations. This\nleads to forebodingly complex inferences due to lack of knowledge about the\nglobal network structure that causes those observations. To address these\ncomplexities, we consider a Learning without Recall model, which in addition to\nproviding a tractable framework for analyzing the behavior of rational agents\nin social networks, can also provide a behavioral foundation for the variety of\nnon-Bayesian update rules in the literature. We present the implications of\nvarious choices for time-varying priors of such agents and how this choice\naffects learning and its rate. \n\n"}
{"id": "1509.09308", "contents": "Title: Fast Algorithms for Convolutional Neural Networks Abstract: Deep convolutional neural networks take GPU days of compute time to train on\nlarge data sets. Pedestrian detection for self driving cars requires very low\nlatency. Image recognition for mobile phones is constrained by limited\nprocessing resources. The success of convolutional neural networks in these\nsituations is limited by how fast we can compute them. Conventional FFT based\nconvolution is fast for large filters, but state of the art convolutional\nneural networks use small, 3x3 filters. We introduce a new class of fast\nalgorithms for convolutional neural networks using Winograd's minimal filtering\nalgorithms. The algorithms compute minimal complexity convolution over small\ntiles, which makes them fast with small filters and small batch sizes. We\nbenchmark a GPU implementation of our algorithm with the VGG network and show\nstate of the art throughput at batch sizes from 1 to 64. \n\n"}
{"id": "1510.01171", "contents": "Title: On the Online Frank-Wolfe Algorithms for Convex and Non-convex\n  Optimizations Abstract: In this paper, the online variants of the classical Frank-Wolfe algorithm are\nconsidered. We consider minimizing the regret with a stochastic cost. The\nonline algorithms only require simple iterative updates and a non-adaptive step\nsize rule, in contrast to the hybrid schemes commonly considered in the\nliterature. Several new results are derived for convex and non-convex losses.\nWith a strongly convex stochastic cost and when the optimal solution lies in\nthe interior of the constraint set or the constraint set is a polytope, the\nregret bound and anytime optimality are shown to be ${\\cal O}( \\log^3 T / T )$\nand ${\\cal O}( \\log^2 T / T)$, respectively, where $T$ is the number of rounds\nplayed. These results are based on an improved analysis on the stochastic\nFrank-Wolfe algorithms. Moreover, the online algorithms are shown to converge\neven when the loss is non-convex, i.e., the algorithms find a stationary point\nto the time-varying/stochastic loss at a rate of ${\\cal O}(\\sqrt{1/T})$.\nNumerical experiments on realistic data sets are presented to support our\ntheoretical claims. \n\n"}
{"id": "1510.02676", "contents": "Title: Some Theory For Practical Classifier Validation Abstract: We compare and contrast two approaches to validating a trained classifier\nwhile using all in-sample data for training. One is simultaneous validation\nover an organized set of hypotheses (SVOOSH), the well-known method that began\nwith VC theory. The other is withhold and gap (WAG). WAG withholds a validation\nset, trains a holdout classifier on the remaining data, uses the validation\ndata to validate that classifier, then adds the rate of disagreement between\nthe holdout classifier and one trained using all in-sample data, which is an\nupper bound on the difference in error rates. We show that complex hypothesis\nclasses and limited training data can make WAG a favorable alternative. \n\n"}
{"id": "1510.03349", "contents": "Title: Toward a Better Understanding of Leaderboard Abstract: The leaderboard in machine learning competitions is a tool to show the\nperformance of various participants and to compare them. However, the\nleaderboard quickly becomes no longer accurate, due to hack or overfitting.\nThis article gives two pieces of advice to prevent easy hack or overfitting. By\nfollowing these advice, we reach the conclusion that something like the Ladder\nleaderboard introduced in [blum2015ladder] is inevitable. With this\nunderstanding, we naturally simplify Ladder by eliminating its redundant\ncomputation and explain how to choose the parameter and interpret it. We also\nprove that the sample complexity is cubic to the desired precision of the\nleaderboard. \n\n"}
{"id": "1510.04609", "contents": "Title: Layer-Specific Adaptive Learning Rates for Deep Networks Abstract: The increasing complexity of deep learning architectures is resulting in\ntraining time requiring weeks or even months. This slow training is due in part\nto vanishing gradients, in which the gradients used by back-propagation are\nextremely large for weights connecting deep layers (layers near the output\nlayer), and extremely small for shallow layers (near the input layer); this\nresults in slow learning in the shallow layers. Additionally, it has also been\nshown that in highly non-convex problems, such as deep neural networks, there\nis a proliferation of high-error low curvature saddle points, which slows down\nlearning dramatically. In this paper, we attempt to overcome the two above\nproblems by proposing an optimization method for training deep neural networks\nwhich uses learning rates which are both specific to each layer in the network\nand adaptive to the curvature of the function, increasing the learning rate at\nlow curvature points. This enables us to speed up learning in the shallow\nlayers of the network and quickly escape high-error low curvature saddle\npoints. We test our method on standard image classification datasets such as\nMNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy\nas well as reduces the required training time over standard algorithms. \n\n"}
{"id": "1510.04935", "contents": "Title: Holographic Embeddings of Knowledge Graphs Abstract: Learning embeddings of entities and relations is an efficient and versatile\nmethod to perform machine learning on relational data such as knowledge graphs.\nIn this work, we propose holographic embeddings (HolE) to learn compositional\nvector space representations of entire knowledge graphs. The proposed method is\nrelated to holographic models of associative memory in that it employs circular\ncorrelation to create compositional representations. By using correlation as\nthe compositional operator HolE can capture rich interactions but\nsimultaneously remains efficient to compute, easy to train, and scalable to\nvery large datasets. In extensive experiments we show that holographic\nembeddings are able to outperform state-of-the-art methods for link prediction\nin knowledge graphs and relational learning benchmark datasets. \n\n"}
{"id": "1510.05491", "contents": "Title: AdaCluster : Adaptive Clustering for Heterogeneous Data Abstract: Clustering algorithms start with a fixed divergence, which captures the\npossibly asymmetric distance between a sample and a centroid. In the mixture\nmodel setting, the sample distribution plays the same role. When all attributes\nhave the same topology and dispersion, the data are said to be homogeneous. If\nthe prior knowledge of the distribution is inaccurate or the set of plausible\ndistributions is large, an adaptive approach is essential. The motivation is\nmore compelling for heterogeneous data, where the dispersion or the topology\ndiffers among attributes. We propose an adaptive approach to clustering using\nclasses of parametrized Bregman divergences. We first show that the density of\na steep exponential dispersion model (EDM) can be represented with a Bregman\ndivergence. We then propose AdaCluster, an expectation-maximization (EM)\nalgorithm to cluster heterogeneous data using classes of steep EDMs. We compare\nAdaCluster with EM for a Gaussian mixture model on synthetic data and nine UCI\ndata sets. We also propose an adaptive hard clustering algorithm based on\nGeneralized Method of Moments. We compare the hard clustering algorithm with\nk-means on the UCI data sets. We empirically verified that adaptively learning\nthe underlying topology yields better clustering of heterogeneous data. \n\n"}
{"id": "1510.07211", "contents": "Title: On End-to-End Program Generation from User Intention by Deep Neural\n  Networks Abstract: This paper envisions an end-to-end program generation scenario using\nrecurrent neural networks (RNNs): Users can express their intention in natural\nlanguage; an RNN then automatically generates corresponding code in a\ncharacterby-by-character fashion. We demonstrate its feasibility through a case\nstudy and empirical analysis. To fully make such technique useful in practice,\nwe also point out several cross-disciplinary challenges, including modeling\nuser intention, providing datasets, improving model architectures, etc.\nAlthough much long-term research shall be addressed in this new field, we\nbelieve end-to-end program generation would become a reality in future decades,\nand we are looking forward to its practice. \n\n"}
{"id": "1511.00830", "contents": "Title: The Variational Fair Autoencoder Abstract: We investigate the problem of learning representations that are invariant to\ncertain nuisance or sensitive factors of variation in the data while retaining\nas much of the remaining information as possible. Our model is based on a\nvariational autoencoding architecture with priors that encourage independence\nbetween sensitive and latent factors of variation. Any subsequent processing,\nsuch as classification, can then be performed on this purged latent\nrepresentation. To remove any remaining dependencies we incorporate an\nadditional penalty term based on the \"Maximum Mean Discrepancy\" (MMD) measure.\nWe discuss how these architectures can be efficiently trained on data and show\nin experiments that this method is more effective than previous work in\nremoving unwanted sources of variation while maintaining informative latent\nrepresentations. \n\n"}
{"id": "1511.02024", "contents": "Title: Towards a Better Understanding of Predict and Count Models Abstract: In a recent paper, Levy and Goldberg pointed out an interesting connection\nbetween prediction-based word embedding models and count models based on\npointwise mutual information. Under certain conditions, they showed that both\nmodels end up optimizing equivalent objective functions. This paper explores\nthis connection in more detail and lays out the factors leading to differences\nbetween these models. We find that the most relevant differences from an\noptimization perspective are (i) predict models work in a low dimensional space\nwhere embedding vectors can interact heavily; (ii) since predict models have\nfewer parameters, they are less prone to overfitting.\n  Motivated by the insight of our analysis, we show how count models can be\nregularized in a principled manner and provide closed-form solutions for L1 and\nL2 regularization. Finally, we propose a new embedding model with a convex\nobjective and the additional benefit of being intelligible. \n\n"}
{"id": "1511.02222", "contents": "Title: Deep Kernel Learning Abstract: We introduce scalable deep kernels, which combine the structural properties\nof deep learning architectures with the non-parametric flexibility of kernel\nmethods. Specifically, we transform the inputs of a spectral mixture base\nkernel with a deep architecture, using local kernel interpolation, inducing\npoints, and structure exploiting (Kronecker and Toeplitz) algebra for a\nscalable kernel representation. These closed-form kernels can be used as\ndrop-in replacements for standard kernels, with benefits in expressive power\nand scalability. We jointly learn the properties of these kernels through the\nmarginal likelihood of a Gaussian process. Inference and learning cost $O(n)$\nfor $n$ training points, and predictions cost $O(1)$ per test point. On a large\nand diverse collection of applications, including a dataset with 2 million\nexamples, we show improved performance over scalable Gaussian processes with\nflexible kernel learning models, and stand-alone deep architectures. \n\n"}
{"id": "1511.02386", "contents": "Title: Hierarchical Variational Models Abstract: Black box variational inference allows researchers to easily prototype and\nevaluate an array of models. Recent advances allow such algorithms to scale to\nhigh dimensions. However, a central question remains: How to specify an\nexpressive variational distribution that maintains efficient computation? To\naddress this, we develop hierarchical variational models (HVMs). HVMs augment a\nvariational approximation with a prior on its parameters, which allows it to\ncapture complex structure for both discrete and continuous latent variables.\nThe algorithm we develop is black box, can be used for any HVM, and has the\nsame computational efficiency as the original approximation. We study HVMs on a\nvariety of deep discrete latent variable models. HVMs generalize other\nexpressive variational distributions and maintains higher fidelity to the\nposterior. \n\n"}
{"id": "1511.02543", "contents": "Title: Sandwiching the marginal likelihood using bidirectional Monte Carlo Abstract: Computing the marginal likelihood (ML) of a model requires marginalizing out\nall of the parameters and latent variables, a difficult high-dimensional\nsummation or integration problem. To make matters worse, it is often hard to\nmeasure the accuracy of one's ML estimates. We present bidirectional Monte\nCarlo, a technique for obtaining accurate log-ML estimates on data simulated\nfrom a model. This method obtains stochastic lower bounds on the log-ML using\nannealed importance sampling or sequential Monte Carlo, and obtains stochastic\nupper bounds by running these same algorithms in reverse starting from an exact\nposterior sample. The true value can be sandwiched between these two stochastic\nbounds with high probability. Using the ground truth log-ML estimates obtained\nfrom our method, we quantitatively evaluate a wide variety of existing ML\nestimators on several latent variable models: clustering, a low rank\napproximation, and a binary attributes model. These experiments yield insights\ninto how to accurately estimate marginal likelihoods. \n\n"}
{"id": "1511.02900", "contents": "Title: Neighbourhood NILM: A Big-data Approach to Household Energy\n  Disaggregation Abstract: In this paper, we investigate whether \"big-data\" is more valuable than\n\"precise\" data for the problem of energy disaggregation: the process of\nbreaking down aggregate energy usage on a per-appliance basis. Existing\ntechniques for disaggregation rely on energy metering at a resolution of 1\nminute or higher, but most power meters today only provide a reading once per\nmonth, and at most once every 15 minutes. In this paper, we propose a new\ntechnique called Neighbourhood NILM that leverages data from 'neighbouring'\nhomes to disaggregate energy given only a single energy reading per month. The\nkey intuition behind our approach is that 'similar' homes have 'similar' energy\nconsumption on a per-appliance basis. Neighbourhood NILM matches every home\nwith a set of 'neighbours' that have direct submetering infrastructure, i.e.\npower meters on individual circuits or loads. Many such homes already exist.\nThen, it estimates the appliance-level energy consumption of the target home to\nbe the average of its K neighbours. We evaluate this approach using 25 homes\nand results show that our approach gives comparable or better disaggregation in\ncomparison to state-of-the-art accuracy reported in the literature that depend\non manual model training, high frequency power metering, or both. Results show\nthat Neighbourhood NILM can achieve 83% and 79% accuracy disaggregating fridge\nand heating/cooling loads, compared to 74% and 73% for a technique called FHMM.\nFurthermore, it achieves up to 64% accuracy on washing machine, dryer,\ndishwasher, and lighting loads, which is higher than previously reported\nresults. Many existing techniques are not able to disaggregate these loads at\nall. These results indicate a potentially substantial advantage to installing\nsubmetering infrastructure in a select few homes rather than installing new\nhigh-frequency smart metering infrastructure in all homes. \n\n"}
{"id": "1511.05493", "contents": "Title: Gated Graph Sequence Neural Networks Abstract: Graph-structured data appears frequently in domains including chemistry,\nnatural language semantics, social networks, and knowledge bases. In this work,\nwe study feature learning techniques for graph-structured inputs. Our starting\npoint is previous work on Graph Neural Networks (Scarselli et al., 2009), which\nwe modify to use gated recurrent units and modern optimization techniques and\nthen extend to output sequences. The result is a flexible and broadly useful\nclass of neural network models that has favorable inductive biases relative to\npurely sequence-based models (e.g., LSTMs) when the problem is\ngraph-structured. We demonstrate the capabilities on some simple AI (bAbI) and\ngraph algorithm learning tasks. We then show it achieves state-of-the-art\nperformance on a problem from program verification, in which subgraphs need to\nbe matched to abstract data structures. \n\n"}
{"id": "1511.05607", "contents": "Title: Identifying the Absorption Bump with Deep Learning Abstract: The pervasive interstellar dust grains provide significant insights to\nunderstand the formation and evolution of the stars, planetary systems, and the\ngalaxies, and may harbor the building blocks of life. One of the most effective\nway to analyze the dust is via their interaction with the light from background\nsources. The observed extinction curves and spectral features carry the size\nand composition information of dust. The broad absorption bump at 2175 Angstrom\nis the most prominent feature in the extinction curves. Traditionally,\nstatistical methods are applied to detect the existence of the absorption bump.\nThese methods require heavy preprocessing and the co-existence of other\nreference features to alleviate the influence from the noises. In this paper,\nwe apply Deep Learning techniques to detect the broad absorption bump. We\ndemonstrate the key steps for training the selected models and their results.\nThe success of Deep Learning based method inspires us to generalize a common\nmethodology for broader science discovery problems. We present our on-going\nwork to build the DeepDis system for such kind of applications. \n\n"}
{"id": "1511.05644", "contents": "Title: Adversarial Autoencoders Abstract: In this paper, we propose the \"adversarial autoencoder\" (AAE), which is a\nprobabilistic autoencoder that uses the recently proposed generative\nadversarial networks (GAN) to perform variational inference by matching the\naggregated posterior of the hidden code vector of the autoencoder with an\narbitrary prior distribution. Matching the aggregated posterior to the prior\nensures that generating from any part of prior space results in meaningful\nsamples. As a result, the decoder of the adversarial autoencoder learns a deep\ngenerative model that maps the imposed prior to the data distribution. We show\nhow the adversarial autoencoder can be used in applications such as\nsemi-supervised classification, disentangling style and content of images,\nunsupervised clustering, dimensionality reduction and data visualization. We\nperformed experiments on MNIST, Street View House Numbers and Toronto Face\ndatasets and show that adversarial autoencoders achieve competitive results in\ngenerative modeling and semi-supervised classification tasks. \n\n"}
{"id": "1511.05952", "contents": "Title: Prioritized Experience Replay Abstract: Experience replay lets online reinforcement learning agents remember and\nreuse experiences from the past. In prior work, experience transitions were\nuniformly sampled from a replay memory. However, this approach simply replays\ntransitions at the same frequency that they were originally experienced,\nregardless of their significance. In this paper we develop a framework for\nprioritizing experience, so as to replay important transitions more frequently,\nand therefore learn more efficiently. We use prioritized experience replay in\nDeep Q-Networks (DQN), a reinforcement learning algorithm that achieved\nhuman-level performance across many Atari games. DQN with prioritized\nexperience replay achieves a new state-of-the-art, outperforming DQN with\nuniform replay on 41 out of 49 games. \n\n"}
{"id": "1511.06103", "contents": "Title: Principled Parallel Mean-Field Inference for Discrete Random Fields Abstract: Mean-field variational inference is one of the most popular approaches to\ninference in discrete random fields. Standard mean-field optimization is based\non coordinate descent and in many situations can be impractical. Thus, in\npractice, various parallel techniques are used, which either rely on ad-hoc\nsmoothing with heuristically set parameters, or put strong constraints on the\ntype of models. In this paper, we propose a novel proximal gradient-based\napproach to optimizing the variational objective. It is naturally\nparallelizable and easy to implement. We prove its convergence, and then\ndemonstrate that, in practice, it yields faster convergence and often finds\nbetter optima than more traditional mean-field optimization techniques.\nMoreover, our method is less sensitive to the choice of parameters. \n\n"}
{"id": "1511.06241", "contents": "Title: Convolutional Clustering for Unsupervised Learning Abstract: The task of labeling data for training deep neural networks is daunting and\ntedious, requiring millions of labels to achieve the current state-of-the-art\nresults. Such reliance on large amounts of labeled data can be relaxed by\nexploiting hierarchical features via unsupervised learning techniques. In this\nwork, we propose to train a deep convolutional network based on an enhanced\nversion of the k-means clustering algorithm, which reduces the number of\ncorrelated parameters in the form of similar filters, and thus increases test\ncategorization accuracy. We call our algorithm convolutional k-means\nclustering. We further show that learning the connection between the layers of\na deep convolutional neural network improves its ability to be trained on a\nsmaller amount of labeled data. Our experiments show that the proposed\nalgorithm outperforms other techniques that learn filters unsupervised.\nSpecifically, we obtained a test accuracy of 74.1% on STL-10 and a test error\nof 0.5% on MNIST. \n\n"}
{"id": "1511.06448", "contents": "Title: Learning Representations from EEG with Deep Recurrent-Convolutional\n  Neural Networks Abstract: One of the challenges in modeling cognitive events from electroencephalogram\n(EEG) data is finding representations that are invariant to inter- and\nintra-subject differences, as well as to inherent noise associated with such\ndata. Herein, we propose a novel approach for learning such representations\nfrom multi-channel EEG time-series, and demonstrate its advantages in the\ncontext of mental load classification task. First, we transform EEG activities\ninto a sequence of topology-preserving multi-spectral images, as opposed to\nstandard EEG analysis techniques that ignore such spatial information. Next, we\ntrain a deep recurrent-convolutional network inspired by state-of-the-art video\nclassification to learn robust representations from the sequence of images. The\nproposed approach is designed to preserve the spatial, spectral, and temporal\nstructure of EEG which leads to finding features that are less sensitive to\nvariations and distortions within each dimension. Empirical evaluation on the\ncognitive load classification task demonstrated significant improvements in\nclassification accuracy over current state-of-the-art approaches in this field. \n\n"}
{"id": "1511.06881", "contents": "Title: Zoom Better to See Clearer: Human and Object Parsing with Hierarchical\n  Auto-Zoom Net Abstract: Parsing articulated objects, e.g. humans and animals, into semantic parts\n(e.g. body, head and arms, etc.) from natural images is a challenging and\nfundamental problem for computer vision. A big difficulty is the large\nvariability of scale and location for objects and their corresponding parts.\nEven limited mistakes in estimating scale and location will degrade the parsing\noutput and cause errors in boundary details. To tackle these difficulties, we\npropose a \"Hierarchical Auto-Zoom Net\" (HAZN) for object part parsing which\nadapts to the local scales of objects and parts. HAZN is a sequence of two\n\"Auto-Zoom Net\" (AZNs), each employing fully convolutional networks that\nperform two tasks: (1) predict the locations and scales of object instances\n(the first AZN) or their parts (the second AZN); (2) estimate the part scores\nfor predicted object instance or part regions. Our model can adaptively \"zoom\"\n(resize) predicted image regions into their proper scales to refine the\nparsing.\n  We conduct extensive experiments over the PASCAL part datasets on humans,\nhorses, and cows. For humans, our approach significantly outperforms the\nstate-of-the-arts by 5% mIOU and is especially better at segmenting small\ninstances and small parts. We obtain similar improvements for parsing cows and\nhorses over alternative methods. In summary, our strategy of first zooming into\nobjects and then zooming into parts is very effective. It also enables us to\nprocess different regions of the image at different scales adaptively so that,\nfor example, we do not need to waste computational resources scaling the entire\nimage. \n\n"}
{"id": "1511.07953", "contents": "Title: Exploring Correlation between Labels to improve Multi-Label\n  Classification Abstract: This paper attempts multi-label classification by extending the idea of\nindependent binary classification models for each output label, and exploring\nhow the inherent correlation between output labels can be used to improve\npredictions. Logistic Regression, Naive Bayes, Random Forest, and SVM models\nwere constructed, with SVM giving the best results: an improvement of 12.9\\%\nover binary models was achieved for hold out cross validation by augmenting\nwith pairwise correlation probabilities of the labels. \n\n"}
{"id": "1511.08099", "contents": "Title: Strategic Dialogue Management via Deep Reinforcement Learning Abstract: Artificially intelligent agents equipped with strategic skills that can\nnegotiate during their interactions with other natural or artificial agents are\nstill underdeveloped. This paper describes a successful application of Deep\nReinforcement Learning (DRL) for training intelligent agents with strategic\nconversational skills, in a situated dialogue setting. Previous studies have\nmodelled the behaviour of strategic agents using supervised learning and\ntraditional reinforcement learning techniques, the latter using tabular\nrepresentations or learning with linear function approximation. In this study,\nwe apply DRL with a high-dimensional state space to the strategic board game of\nSettlers of Catan---where players can offer resources in exchange for others\nand they can also reply to offers made by other players. Our experimental\nresults report that the DRL-based learnt policies significantly outperformed\nseveral baselines including random, rule-based, and supervised-based\nbehaviours. The DRL-based policy has a 53% win rate versus 3 automated players\n(`bots'), whereas a supervised player trained on a dialogue corpus in this\nsetting achieved only 27%, versus the same 3 bots. This result supports the\nclaim that DRL is a promising framework for training dialogue systems, and\nstrategic agents with negotiation abilities. \n\n"}
{"id": "1512.01708", "contents": "Title: Variance Reduction for Distributed Stochastic Gradient Descent Abstract: Variance reduction (VR) methods boost the performance of stochastic gradient\ndescent (SGD) by enabling the use of larger, constant stepsizes and preserving\nlinear convergence rates. However, current variance reduced SGD methods require\neither high memory usage or an exact gradient computation (using the entire\ndataset) at the end of each epoch. This limits the use of VR methods in\npractical distributed settings. In this paper, we propose a variance reduction\nmethod, called VR-lite, that does not require full gradient computations or\nextra storage. We explore distributed synchronous and asynchronous variants\nthat are scalable and remain stable with low communication frequency. We\nempirically compare both the sequential and distributed algorithms to\nstate-of-the-art stochastic optimization methods, and find that our proposed\nalgorithms perform favorably to other stochastic methods. \n\n"}
{"id": "1512.01712", "contents": "Title: Generating News Headlines with Recurrent Neural Networks Abstract: We describe an application of an encoder-decoder recurrent neural network\nwith LSTM units and attention to generating headlines from the text of news\narticles. We find that the model is quite effective at concisely paraphrasing\nnews articles. Furthermore, we study how the neural network decides which input\nwords to pay attention to, and specifically we identify the function of the\ndifferent neurons in a simplified attention mechanism. Interestingly, our\nsimplified attention mechanism performs better that the more complex attention\nmechanism on a held out set of articles. \n\n"}
{"id": "1512.04829", "contents": "Title: Feature-Level Domain Adaptation Abstract: Domain adaptation is the supervised learning setting in which the training\nand test data are sampled from different distributions: training data is\nsampled from a source domain, whilst test data is sampled from a target domain.\nThis paper proposes and studies an approach, called feature-level domain\nadaptation (FLDA), that models the dependence between the two domains by means\nof a feature-level transfer model that is trained to describe the transfer from\nsource to target domain. Subsequently, we train a domain-adapted classifier by\nminimizing the expected loss under the resulting transfer model. For linear\nclassifiers and a large family of loss functions and transfer models, this\nexpected loss can be computed or approximated analytically, and minimized\nefficiently. Our empirical evaluation of FLDA focuses on problems comprising\nbinary and count data in which the transfer can be naturally modeled via a\ndropout distribution, which allows the classifier to adapt to differences in\nthe marginal probability of features in the source and the target domain. Our\nexperiments on several real-world problems show that FLDA performs on par with\nstate-of-the-art domain-adaptation techniques. \n\n"}
{"id": "1512.06388", "contents": "Title: Revisiting Differentially Private Regression: Lessons From Learning\n  Theory and their Consequences Abstract: Private regression has received attention from both database and security\ncommunities. Recent work by Fredrikson et al. (USENIX Security 2014) analyzed\nthe functional mechanism (Zhang et al. VLDB 2012) for training linear\nregression models over medical data. Unfortunately, they found that model\naccuracy is already unacceptable with differential privacy when $\\varepsilon =\n5$. We address this issue, presenting an explicit connection between\ndifferential privacy and stable learning theory through which a substantially\nbetter privacy/utility tradeoff can be obtained. Perhaps more importantly, our\ntheory reveals that the most basic mechanism in differential privacy, output\nperturbation, can be used to obtain a better tradeoff for all\nconvex-Lipschitz-bounded learning tasks. Since output perturbation is simple to\nimplement, it means that our approach is potentially widely applicable in\npractice. We go on to apply it on the same medical data as used by Fredrikson\net al. Encouragingly, we achieve accurate models even for $\\varepsilon = 0.1$.\nIn the last part of this paper, we study the impact of our improved\ndifferentially private mechanisms on model inversion attacks, a privacy attack\nintroduced by Fredrikson et al. We observe that the improved tradeoff makes the\nresulting differentially private model more susceptible to inversion attacks.\nWe analyze this phenomenon formally. \n\n"}
{"id": "1512.06612", "contents": "Title: Backward and Forward Language Modeling for Constrained Sentence\n  Generation Abstract: Recent language models, especially those based on recurrent neural networks\n(RNNs), make it possible to generate natural language from a learned\nprobability. Language generation has wide applications including machine\ntranslation, summarization, question answering, conversation systems, etc.\nExisting methods typically learn a joint probability of words conditioned on\nadditional information, which is (either statically or dynamically) fed to\nRNN's hidden layer. In many applications, we are likely to impose hard\nconstraints on the generated texts, i.e., a particular word must appear in the\nsentence. Unfortunately, existing approaches could not solve this problem. In\nthis paper, we propose a novel backward and forward language model. Provided a\nspecific word, we use RNNs to generate previous words and future words, either\nsimultaneously or asynchronously, resulting in two model variants. In this way,\nthe given word could appear at any position in the sentence. Experimental\nresults show that the generated texts are comparable to sequential LMs in\nquality. \n\n"}
{"id": "1512.07638", "contents": "Title: Satisficing in multi-armed bandit problems Abstract: Satisficing is a relaxation of maximizing and allows for less risky decision\nmaking in the face of uncertainty. We propose two sets of satisficing\nobjectives for the multi-armed bandit problem, where the objective is to\nachieve reward-based decision-making performance above a given threshold. We\nshow that these new problems are equivalent to various standard multi-armed\nbandit problems with maximizing objectives and use the equivalence to find\nbounds on performance. The different objectives can result in qualitatively\ndifferent behavior; for example, agents explore their options continually in\none case and only a finite number of times in another. For the case of Gaussian\nrewards we show an additional equivalence between the two sets of satisficing\nobjectives that allows algorithms developed for one set to be applied to the\nother. We then develop variants of the Upper Credible Limit (UCL) algorithm\nthat solve the problems with satisficing objectives and show that these\nmodified UCL algorithms achieve efficient satisficing performance. \n\n"}
{"id": "1512.08425", "contents": "Title: Convexified Modularity Maximization for Degree-corrected Stochastic\n  Block Models Abstract: The stochastic block model (SBM) is a popular framework for studying\ncommunity detection in networks. This model is limited by the assumption that\nall nodes in the same community are statistically equivalent and have equal\nexpected degrees. The degree-corrected stochastic block model (DCSBM) is a\nnatural extension of SBM that allows for degree heterogeneity within\ncommunities. This paper proposes a convexified modularity maximization approach\nfor estimating the hidden communities under DCSBM. Our approach is based on a\nconvex programming relaxation of the classical (generalized) modularity\nmaximization formulation, followed by a novel doubly-weighted $ \\ell_1 $-norm $\nk $-median procedure. We establish non-asymptotic theoretical guarantees for\nboth approximate clustering and perfect clustering. Our approximate clustering\nresults are insensitive to the minimum degree, and hold even in sparse regime\nwith bounded average degrees. In the special case of SBM, these theoretical\nresults match the best-known performance guarantees of computationally feasible\nalgorithms. Numerically, we provide an efficient implementation of our\nalgorithm, which is applied to both synthetic and real-world networks.\nExperiment results show that our method enjoys competitive performance compared\nto the state of the art in the literature. \n\n"}
{"id": "1512.09300", "contents": "Title: Autoencoding beyond pixels using a learned similarity metric Abstract: We present an autoencoder that leverages learned representations to better\nmeasure similarities in data space. By combining a variational autoencoder with\na generative adversarial network we can use learned feature representations in\nthe GAN discriminator as basis for the VAE reconstruction objective. Thereby,\nwe replace element-wise errors with feature-wise errors to better capture the\ndata distribution while offering invariance towards e.g. translation. We apply\nour method to images of faces and show that it outperforms VAEs with\nelement-wise similarity measures in terms of visual fidelity. Moreover, we show\nthat the method learns an embedding in which high-level abstract visual\nfeatures (e.g. wearing glasses) can be modified using simple arithmetic. \n\n"}
{"id": "1512.09327", "contents": "Title: Distributed Bayesian Learning with Stochastic Natural-gradient\n  Expectation Propagation and the Posterior Server Abstract: This paper makes two contributions to Bayesian machine learning algorithms.\nFirstly, we propose stochastic natural gradient expectation propagation (SNEP),\na novel alternative to expectation propagation (EP), a popular variational\ninference algorithm. SNEP is a black box variational algorithm, in that it does\nnot require any simplifying assumptions on the distribution of interest, beyond\nthe existence of some Monte Carlo sampler for estimating the moments of the EP\ntilted distributions. Further, as opposed to EP which has no guarantee of\nconvergence, SNEP can be shown to be convergent, even when using Monte Carlo\nmoment estimates. Secondly, we propose a novel architecture for distributed\nBayesian learning which we call the posterior server. The posterior server\nallows scalable and robust Bayesian learning in cases where a data set is\nstored in a distributed manner across a cluster, with each compute node\ncontaining a disjoint subset of data. An independent Monte Carlo sampler is run\non each compute node, with direct access only to the local data subset, but\nwhich targets an approximation to the global posterior distribution given all\ndata across the whole cluster. This is achieved by using a distributed\nasynchronous implementation of SNEP to pass messages across the cluster. We\ndemonstrate SNEP and the posterior server on distributed Bayesian learning of\nlogistic regression and neural networks.\n  Keywords: Distributed Learning, Large Scale Learning, Deep Learning, Bayesian\nLearn- ing, Variational Inference, Expectation Propagation, Stochastic\nApproximation, Natural Gradient, Markov chain Monte Carlo, Parameter Server,\nPosterior Server. \n\n"}
{"id": "1512.09328", "contents": "Title: Homology Computation of Large Point Clouds using Quantum Annealing Abstract: Homology is a tool in topological data analysis which measures the shape of\nthe data. In many cases, these measurements translate into new insights which\nare not available by other means. To compute homology, we rely on mathematical\nconstructions which scale exponentially with the size of the data. Therefore,\nfor large point clouds, the computation is infeasible using classical\ncomputers. In this paper, we present a quantum annealing pipeline for\ncomputation of homology of large point clouds. The pipeline takes as input a\ngraph approximating the given point cloud. It uses quantum annealing to compute\na clique covering of the graph and then uses this cover to construct a\nMayer-Vietoris complex. The pipeline terminates by performing a simplified\nhomology computation of the Mayer-Vietoris complex. We have introduced three\ndifferent clique coverings and their quantum annealing formulation. Our\npipeline scales polynomially in the size of the data, once the covering step is\nsolved. To prove correctness of our algorithm, we have also included tests\nusing D-Wave 2X quantum processor. \n\n"}
{"id": "1601.04033", "contents": "Title: Faster Asynchronous SGD Abstract: Asynchronous distributed stochastic gradient descent methods have trouble\nconverging because of stale gradients. A gradient update sent to a parameter\nserver by a client is stale if the parameters used to calculate that gradient\nhave since been updated on the server. Approaches have been proposed to\ncircumvent this problem that quantify staleness in terms of the number of\nelapsed updates. In this work, we propose a novel method that quantifies\nstaleness in terms of moving averages of gradient statistics. We show that this\nmethod outperforms previous methods with respect to convergence speed and\nscalability to many clients. We also discuss how an extension to this method\ncan be used to dramatically reduce bandwidth costs in a distributed training\ncontext. In particular, our method allows reduction of total bandwidth usage by\na factor of 5 with little impact on cost convergence. We also describe (and\nlink to) a software library that we have used to simulate these algorithms\ndeterministically on a single machine. \n\n"}
{"id": "1601.05900", "contents": "Title: When is Clustering Perturbation Robust? Abstract: Clustering is a fundamental data mining tool that aims to divide data into\ngroups of similar items. Generally, intuition about clustering reflects the\nideal case -- exact data sets endowed with flawless dissimilarity between\nindividual instances.\n  In practice however, these cases are in the minority, and clustering\napplications are typically characterized by noisy data sets with approximate\npairwise dissimilarities. As such, the efficacy of clustering methods in\npractical applications necessitates robustness to perturbations.\n  In this paper, we perform a formal analysis of perturbation robustness,\nrevealing that the extent to which algorithms can exhibit this desirable\ncharacteristic is inherently limited, and identifying the types of structures\nthat allow popular clustering paradigms to discover meaningful clusters in\nspite of faulty data. \n\n"}
{"id": "1601.06239", "contents": "Title: Divide and Conquer Local Average Regression Abstract: The divide and conquer strategy, which breaks a massive data set into a se-\nries of manageable data blocks, and then combines the independent results of\ndata blocks to obtain a final decision, has been recognized as a\nstate-of-the-art method to overcome challenges of massive data analysis. In\nthis paper, we merge the divide and conquer strategy with local average\nregression methods to infer the regressive relationship of input-output pairs\nfrom a massive data set. After theoretically analyzing the pros and cons, we\nfind that although the divide and conquer local average regression can reach\nthe optimal learning rate, the restric- tion to the number of data blocks is a\nbit strong, which makes it only feasible for small number of data blocks. We\nthen propose two variants to lessen (or remove) this restriction. Our results\nshow that these variants can achieve the optimal learning rate with much milder\nrestriction (or without such restriction). Extensive experimental studies are\ncarried out to verify our theoretical assertions. \n\n"}
{"id": "1601.06759", "contents": "Title: Pixel Recurrent Neural Networks Abstract: Modeling the distribution of natural images is a landmark problem in\nunsupervised learning. This task requires an image model that is at once\nexpressive, tractable and scalable. We present a deep neural network that\nsequentially predicts the pixels in an image along the two spatial dimensions.\nOur method models the discrete probability of the raw pixel values and encodes\nthe complete set of dependencies in the image. Architectural novelties include\nfast two-dimensional recurrent layers and an effective use of residual\nconnections in deep recurrent networks. We achieve log-likelihood scores on\nnatural images that are considerably better than the previous state of the art.\nOur main results also provide benchmarks on the diverse ImageNet dataset.\nSamples generated from the model appear crisp, varied and globally coherent. \n\n"}
{"id": "1602.00357", "contents": "Title: DeepCare: A Deep Dynamic Memory Model for Predictive Medicine Abstract: Personalized predictive medicine necessitates the modeling of patient illness\nand care processes, which inherently have long-term temporal dependencies.\nHealthcare observations, recorded in electronic medical records, are episodic\nand irregular in time. We introduce DeepCare, an end-to-end deep dynamic neural\nnetwork that reads medical records, stores previous illness history, infers\ncurrent illness states and predicts future medical outcomes. At the data level,\nDeepCare represents care episodes as vectors in space, models patient health\nstate trajectories through explicit memory of historical records. Built on Long\nShort-Term Memory (LSTM), DeepCare introduces time parameterizations to handle\nirregular timed events by moderating the forgetting and consolidation of memory\ncells. DeepCare also incorporates medical interventions that change the course\nof illness and shape future medical risk. Moving up to the health state level,\nhistorical and present health states are then aggregated through multiscale\ntemporal pooling, before passing through a neural network that estimates future\noutcomes. We demonstrate the efficacy of DeepCare for disease progression\nmodeling, intervention recommendation, and future risk prediction. On two\nimportant cohorts with heavy social and economic burden -- diabetes and mental\nhealth -- the results show improved modeling and risk prediction accuracy. \n\n"}
{"id": "1602.00426", "contents": "Title: An Iterative Deep Learning Framework for Unsupervised Discovery of\n  Speech Features and Linguistic Units with Applications on Spoken Term\n  Detection Abstract: In this work we aim to discover high quality speech features and linguistic\nunits directly from unlabeled speech data in a zero resource scenario. The\nresults are evaluated using the metrics and corpora proposed in the Zero\nResource Speech Challenge organized at Interspeech 2015. A Multi-layered\nAcoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets\nof acoustic tokens from the given corpus. Each acoustic token set is specified\nby a set of hyperparameters that describe the model configuration. These sets\nof acoustic tokens carry different characteristics fof the given corpus and the\nlanguage behind, thus can be mutually reinforced. The multiple sets of token\nlabels are then used as the targets of a Multi-target Deep Neural Network\n(MDNN) trained on low-level acoustic features. Bottleneck features extracted\nfrom the MDNN are then used as the feedback input to the MAT and the MDNN\nitself in the next iteration. We call this iterative deep learning framework\nthe Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), which\ngenerates both high quality speech features for the Track 1 of the Challenge\nand acoustic tokens for the Track 2 of the Challenge. In addition, we performed\nextra experiments on the same corpora on the application of query-by-example\nspoken term detection. The experimental results showed the iterative deep\nlearning framework of MAT-DNN improved the detection performance due to better\nunderlying speech features and acoustic tokens. \n\n"}
{"id": "1602.01024", "contents": "Title: On Deep Multi-View Representation Learning: Objectives and Optimization Abstract: We consider learning representations (features) in the setting in which we\nhave access to multiple unlabeled views of the data for learning while only one\nview is available for downstream tasks. Previous work on this problem has\nproposed several techniques based on deep neural networks, typically involving\neither autoencoder-like networks with a reconstruction objective or paired\nfeedforward networks with a batch-style correlation-based objective. We analyze\nseveral techniques based on prior work, as well as new variants, and compare\nthem empirically on image, speech, and text tasks. We find an advantage for\ncorrelation-based representation learning, while the best results on most tasks\nare obtained with our new variant, deep canonically correlated autoencoders\n(DCCAE). We also explore a stochastic optimization procedure for minibatch\ncorrelation-based objectives and discuss the time/performance trade-offs for\nkernel-based and neural network-based implementations. \n\n"}
{"id": "1602.01164", "contents": "Title: Single-Solution Hypervolume Maximization and its use for Improving\n  Generalization of Neural Networks Abstract: This paper introduces the hypervolume maximization with a single solution as\nan alternative to the mean loss minimization. The relationship between the two\nproblems is proved through bounds on the cost function when an optimal solution\nto one of the problems is evaluated on the other, with a hyperparameter to\ncontrol the similarity between the two problems. This same hyperparameter\nallows higher weight to be placed on samples with higher loss when computing\nthe hypervolume's gradient, whose normalized version can range from the mean\nloss to the max loss. An experiment on MNIST with a neural network is used to\nvalidate the theory developed, showing that the hypervolume maximization can\nbehave similarly to the mean loss minimization and can also provide better\nperformance, resulting on a 20% reduction of the classification error on the\ntest set. \n\n"}
{"id": "1602.02210", "contents": "Title: Classification accuracy as a proxy for two sample testing Abstract: When data analysts train a classifier and check if its accuracy is\nsignificantly different from chance, they are implicitly performing a\ntwo-sample test. We investigate the statistical properties of this flexible\napproach in the high-dimensional setting. We prove two results that hold for\nall classifiers in any dimensions: if its true error remains $\\epsilon$-better\nthan chance for some $\\epsilon>0$ as $d,n \\to \\infty$, then (a) the\npermutation-based test is consistent (has power approaching to one), (b) a\ncomputationally efficient test based on a Gaussian approximation of the null\ndistribution is also consistent. To get a finer understanding of the rates of\nconsistency, we study a specialized setting of distinguishing Gaussians with\nmean-difference $\\delta$ and common (known or unknown) covariance $\\Sigma$,\nwhen $d/n \\to c \\in (0,\\infty)$. We study variants of Fisher's linear\ndiscriminant analysis (LDA) such as \"naive Bayes\" in a nontrivial regime when\n$\\epsilon \\to 0$ (the Bayes classifier has true accuracy approaching 1/2), and\ncontrast their power with corresponding variants of Hotelling's test.\nSurprisingly, the expressions for their power match exactly in terms of\n$n,d,\\delta,\\Sigma$, and the LDA approach is only worse by a constant factor,\nachieving an asymptotic relative efficiency (ARE) of $1/\\sqrt{\\pi}$ for\nbalanced samples. We also extend our results to high-dimensional elliptical\ndistributions with finite kurtosis. Other results of independent interest\ninclude minimax lower bounds, and the optimality of Hotelling's test when\n$d=o(n)$. Simulation results validate our theory, and we present practical\ntakeaway messages along with natural open problems. \n\n"}
{"id": "1602.02262", "contents": "Title: Recovery guarantee of weighted low-rank approximation via alternating\n  minimization Abstract: Many applications require recovering a ground truth low-rank matrix from\nnoisy observations of the entries, which in practice is typically formulated as\na weighted low-rank approximation problem and solved by non-convex optimization\nheuristics such as alternating minimization. In this paper, we provide provable\nrecovery guarantee of weighted low-rank via a simple alternating minimization\nalgorithm. In particular, for a natural class of matrices and weights and\nwithout any assumption on the noise, we bound the spectral norm of the\ndifference between the recovered matrix and the ground truth, by the spectral\nnorm of the weighted noise plus an additive error that decreases exponentially\nwith the number of rounds of alternating minimization, from either\ninitialization by SVD or, more importantly, random initialization. These\nprovide the first theoretical results for weighted low-rank via alternating\nminimization with non-binary deterministic weights, significantly generalizing\nthose for matrix completion, the special case with binary weights, since our\nassumptions are similar or weaker than those made in existing works.\nFurthermore, this is achieved by a very simple algorithm that improves the\nvanilla alternating minimization with a simple clipping step.\n  The key technical challenge is that under non-binary deterministic weights,\nna\\\"ive alternating steps will destroy the incoherence and spectral properties\nof the intermediate solutions, which are needed for making progress towards the\nground truth. We show that the properties only need to hold in an average sense\nand can be achieved by the clipping step.\n  We further provide an alternating algorithm that uses a whitening step that\nkeeps the properties via SDP and Rademacher rounding and thus requires weaker\nassumptions. This technique can potentially be applied in some other\napplications and is of independent interest. \n\n"}
{"id": "1602.02383", "contents": "Title: Disentangled Representations in Neural Models Abstract: Representation learning is the foundation for the recent success of neural\nnetwork models. However, the distributed representations generated by neural\nnetworks are far from ideal. Due to their highly entangled nature, they are di\ncult to reuse and interpret, and they do a poor job of capturing the sparsity\nwhich is present in real- world transformations. In this paper, I describe\nmethods for learning disentangled representations in the two domains of\ngraphics and computation. These methods allow neural methods to learn\nrepresentations which are easy to interpret and reuse, yet they incur little or\nno penalty to performance. In the Graphics section, I demonstrate the ability\nof these methods to infer the generating parameters of images and rerender\nthose images under novel conditions. In the Computation section, I describe a\nmodel which is able to factorize a multitask learning problem into subtasks and\nwhich experiences no catastrophic forgetting. Together these techniques provide\nthe tools to design a wide range of models that learn disentangled\nrepresentations and better model the factors of variation in the real world. \n\n"}
{"id": "1602.02666", "contents": "Title: A Variational Analysis of Stochastic Gradient Algorithms Abstract: Stochastic Gradient Descent (SGD) is an important algorithm in machine\nlearning. With constant learning rates, it is a stochastic process that, after\nan initial phase of convergence, generates samples from a stationary\ndistribution. We show that SGD with constant rates can be effectively used as\nan approximate posterior inference algorithm for probabilistic modeling.\nSpecifically, we show how to adjust the tuning parameters of SGD such as to\nmatch the resulting stationary distribution to the posterior. This analysis\nrests on interpreting SGD as a continuous-time stochastic process and then\nminimizing the Kullback-Leibler divergence between its stationary distribution\nand the target posterior. (This is in the spirit of variational inference.) In\nmore detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then\nuse properties of this process to derive the optimal parameters. This\ntheoretical framework also connects SGD to modern scalable inference\nalgorithms; we analyze the recently proposed stochastic gradient Fisher scoring\nunder this perspective. We demonstrate that SGD with properly chosen constant\nrates gives a new way to optimize hyperparameters in probabilistic models. \n\n"}
{"id": "1602.02685", "contents": "Title: Predicting Clinical Events by Combining Static and Dynamic Information\n  Using Recurrent Neural Networks Abstract: In clinical data sets we often find static information (e.g. patient gender,\nblood type, etc.) combined with sequences of data that are recorded during\nmultiple hospital visits (e.g. medications prescribed, tests performed, etc.).\nRecurrent Neural Networks (RNNs) have proven to be very successful for\nmodelling sequences of data in many areas of Machine Learning. In this work we\npresent an approach based on RNNs, specifically designed for the clinical\ndomain, that combines static and dynamic information in order to predict future\nevents. We work with a database collected in the Charit\\'{e} Hospital in Berlin\nthat contains complete information concerning patients that underwent a kidney\ntransplantation. After the transplantation three main endpoints can occur:\nrejection of the kidney, loss of the kidney and death of the patient. Our goal\nis to predict, based on information recorded in the Electronic Health Record of\neach patient, whether any of those endpoints will occur within the next six or\ntwelve months after each visit to the clinic. We compared different types of\nRNNs that we developed for this work, with a model based on a Feedforward\nNeural Network and a Logistic Regression model. We found that the RNN that we\ndeveloped based on Gated Recurrent Units provides the best performance for this\ntask. We also used the same models for a second task, i.e., next event\nprediction, and found that here the model based on a Feedforward Neural Network\noutperformed the other models. Our hypothesis is that long-term dependencies\nare not as relevant in this task. \n\n"}
{"id": "1602.02865", "contents": "Title: The Role of Typicality in Object Classification: Improving The\n  Generalization Capacity of Convolutional Neural Networks Abstract: Deep artificial neural networks have made remarkable progress in different\ntasks in the field of computer vision. However, the empirical analysis of these\nmodels and investigation of their failure cases has received attention\nrecently. In this work, we show that deep learning models cannot generalize to\natypical images that are substantially different from training images. This is\nin contrast to the superior generalization ability of the visual system in the\nhuman brain. We focus on Convolutional Neural Networks (CNN) as the\nstate-of-the-art models in object recognition and classification; investigate\nthis problem in more detail, and hypothesize that training CNN models suffer\nfrom unstructured loss minimization. We propose computational models to improve\nthe generalization capacity of CNNs by considering how typical a training image\nlooks like. By conducting an extensive set of experiments we show that\ninvolving a typicality measure can improve the classification results on a new\nset of images by a large margin. More importantly, this significant improvement\nis achieved without fine-tuning the CNN model on the target image set. \n\n"}
{"id": "1602.03609", "contents": "Title: Attentive Pooling Networks Abstract: In this work, we propose Attentive Pooling (AP), a two-way attention\nmechanism for discriminative model training. In the context of pair-wise\nranking or classification with neural networks, AP enables the pooling layer to\nbe aware of the current input pair, in a way that information from the two\ninput items can directly influence the computation of each other's\nrepresentations. Along with such representations of the paired inputs, AP\njointly learns a similarity measure over projected segments (e.g. trigrams) of\nthe pair, and subsequently, derives the corresponding attention vector for each\ninput to guide the pooling. Our two-way attention mechanism is a general\nframework independent of the underlying representation learning, and it has\nbeen applied to both convolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs) in our studies. The empirical results, from three very\ndifferent benchmark tasks of question answering/answer selection, demonstrate\nthat our proposed models outperform a variety of strong baselines and achieve\nstate-of-the-art performance in all the benchmarks. \n\n"}
{"id": "1602.03943", "contents": "Title: Second-Order Stochastic Optimization for Machine Learning in Linear Time Abstract: First-order stochastic methods are the state-of-the-art in large-scale\nmachine learning optimization owing to efficient per-iteration complexity.\nSecond-order methods, while able to provide faster convergence, have been much\nless explored due to the high cost of computing the second-order information.\nIn this paper we develop second-order stochastic methods for optimization\nproblems in machine learning that match the per-iteration cost of gradient\nbased methods, and in certain settings improve upon the overall running time\nover popular first-order methods. Furthermore, our algorithm has the desirable\nproperty of being implementable in time linear in the sparsity of the input\ndata. \n\n"}
{"id": "1602.04579", "contents": "Title: Secure Approximation Guarantee for Cryptographically Private Empirical\n  Risk Minimization Abstract: Privacy concern has been increasingly important in many machine learning (ML)\nproblems. We study empirical risk minimization (ERM) problems under secure\nmulti-party computation (MPC) frameworks. Main technical tools for MPC have\nbeen developed based on cryptography. One of limitations in current\ncryptographically private ML is that it is computationally intractable to\nevaluate non-linear functions such as logarithmic functions or exponential\nfunctions. Therefore, for a class of ERM problems such as logistic regression\nin which non-linear function evaluations are required, one can only obtain\napproximate solutions. In this paper, we introduce a novel cryptographically\nprivate tool called secure approximation guarantee (SAG) method. The key\nproperty of SAG method is that, given an arbitrary approximate solution, it can\nprovide a non-probabilistic assumption-free bound on the approximation quality\nunder cryptographically secure computation framework. We demonstrate the\nbenefit of the SAG method by applying it to several problems including a\npractical privacy-preserving data analysis task on genomic and clinical\ninformation. \n\n"}
{"id": "1602.05110", "contents": "Title: Generating images with recurrent adversarial networks Abstract: Gatys et al. (2015) showed that optimizing pixels to match features in a\nconvolutional network with respect reference image features is a way to render\nimages of high visual quality. We show that unrolling this gradient-based\noptimization yields a recurrent computation that creates images by\nincrementally adding onto a visual \"canvas\". We propose a recurrent generative\nmodel inspired by this view, and show that it can be trained using adversarial\ntraining to generate very good image samples. We also propose a way to\nquantitatively compare adversarial networks by having the generators and\ndiscriminators of these networks compete against each other. \n\n"}
{"id": "1602.05179", "contents": "Title: Equilibrium Propagation: Bridging the Gap Between Energy-Based Models\n  and Backpropagation Abstract: We introduce Equilibrium Propagation, a learning framework for energy-based\nmodels. It involves only one kind of neural computation, performed in both the\nfirst phase (when the prediction is made) and the second phase of training\n(after the target or prediction error is revealed). Although this algorithm\ncomputes the gradient of an objective function just like Backpropagation, it\ndoes not need a special computation or circuit for the second phase, where\nerrors are implicitly propagated. Equilibrium Propagation shares similarities\nwith Contrastive Hebbian Learning and Contrastive Divergence while solving the\ntheoretical issues of both algorithms: our algorithm computes the gradient of a\nwell defined objective function. Because the objective function is defined in\nterms of local perturbations, the second phase of Equilibrium Propagation\ncorresponds to only nudging the prediction (fixed point, or stationary\ndistribution) towards a configuration that reduces prediction error. In the\ncase of a recurrent multi-layer supervised network, the output units are\nslightly nudged towards their target in the second phase, and the perturbation\nintroduced at the output layer propagates backward in the hidden layers. We\nshow that the signal 'back-propagated' during this second phase corresponds to\nthe propagation of error derivatives and encodes the gradient of the objective\nfunction, when the synaptic update corresponds to a standard form of\nspike-timing dependent plasticity. This work makes it more plausible that a\nmechanism similar to Backpropagation could be implemented by brains, since\nleaky integrator neural computation performs both inference and error\nback-propagation in our model. The only local difference between the two phases\nis whether synaptic changes are allowed or not. \n\n"}
{"id": "1602.07383", "contents": "Title: Automatic Moth Detection from Trap Images for Pest Management Abstract: Monitoring the number of insect pests is a crucial component in\npheromone-based pest management systems. In this paper, we propose an automatic\ndetection pipeline based on deep learning for identifying and counting pests in\nimages taken inside field traps. Applied to a commercial codling moth dataset,\nour method shows promising performance both qualitatively and quantitatively.\nCompared to previous attempts at pest detection, our approach uses no\npest-specific engineering which enables it to adapt to other species and\nenvironments with minimal human effort. It is amenable to implementation on\nparallel hardware and therefore capable of deployment in settings where\nreal-time performance is required. \n\n"}
{"id": "1602.07415", "contents": "Title: Ensuring Rapid Mixing and Low Bias for Asynchronous Gibbs Sampling Abstract: Gibbs sampling is a Markov chain Monte Carlo technique commonly used for\nestimating marginal distributions. To speed up Gibbs sampling, there has\nrecently been interest in parallelizing it by executing asynchronously. While\nempirical results suggest that many models can be efficiently sampled\nasynchronously, traditional Markov chain analysis does not apply to the\nasynchronous case, and thus asynchronous Gibbs sampling is poorly understood.\nIn this paper, we derive a better understanding of the two main challenges of\nasynchronous Gibbs: bias and mixing time. We show experimentally that our\ntheoretical results match practical outcomes. \n\n"}
{"id": "1602.08671", "contents": "Title: Lie Access Neural Turing Machine Abstract: Following the recent trend in explicit neural memory structures, we present a\nnew design of an external memory, wherein memories are stored in an Euclidean\nkey space $\\mathbb R^n$. An LSTM controller performs read and write via\nspecialized read and write heads. It can move a head by either providing a new\naddress in the key space (aka random access) or moving from its previous\nposition via a Lie group action (aka Lie access). In this way, the \"L\" and \"R\"\ninstructions of a traditional Turing Machine are generalized to arbitrary\nelements of a fixed Lie group action. For this reason, we name this new model\nthe Lie Access Neural Turing Machine, or LANTM.\n  We tested two different configurations of LANTM against an LSTM baseline in\nseveral basic experiments. We found the right configuration of LANTM to\noutperform the baseline in all of our experiments. In particular, we trained\nLANTM on addition of $k$-digit numbers for $2 \\le k \\le 16$, but it was able to\ngeneralize almost perfectly to $17 \\le k \\le 32$, all with the number of\nparameters 2 orders of magnitude below the LSTM baseline. \n\n"}
{"id": "1603.06829", "contents": "Title: Multi-velocity neural networks for gesture recognition in videos Abstract: We present a new action recognition deep neural network which adaptively\nlearns the best action velocities in addition to the classification. While deep\nneural networks have reached maturity for image understanding tasks, we are\nstill exploring network topologies and features to handle the richer\nenvironment of video clips. Here, we tackle the problem of multiple velocities\nin action recognition, and provide state-of-the-art results for gesture\nrecognition, on known and new collected datasets. We further provide the\ntraining steps for our semi-supervised network, suited to learn from huge\nunlabeled datasets with only a fraction of labeled examples. \n\n"}
{"id": "1603.07044", "contents": "Title: Recurrent Neural Network Encoder with Attention for Community Question\n  Answering Abstract: We apply a general recurrent neural network (RNN) encoder framework to\ncommunity question answering (cQA) tasks. Our approach does not rely on any\nlinguistic processing, and can be applied to different languages or domains.\nFurther improvements are observed when we extend the RNN encoders with a neural\nattention mechanism that encourages reasoning over entire sequences. To deal\nwith practical issues such as data sparsity and imbalanced labels, we apply\nvarious techniques such as transfer learning and multitask learning. Our\nexperiments on the SemEval-2016 cQA task show 10% improvement on a MAP score\ncompared to an information retrieval-based approach, and achieve comparable\nperformance to a strong handcrafted feature-based method. \n\n"}
{"id": "1603.08035", "contents": "Title: On kernel methods for covariates that are rankings Abstract: Permutation-valued features arise in a variety of applications, either in a\ndirect way when preferences are elicited over a collection of items, or an\nindirect way in which numerical ratings are converted to a ranking. To date,\nthere has been relatively limited study of regression, classification, and\ntesting problems based on permutation-valued features, as opposed to\npermutation-valued responses. This paper studies the use of reproducing kernel\nHilbert space methods for learning from permutation-valued features. These\nmethods embed the rankings into an implicitly defined function space, and allow\nfor efficient estimation of regression and test functions in this richer space.\nOur first contribution is to characterize both the feature spaces and spectral\nproperties associated with two kernels for rankings, the Kendall and Mallows\nkernels. Using tools from representation theory, we explain the limited\nexpressive power of the Kendall kernel by characterizing its degenerate\nspectrum, and in sharp contrast, we prove that Mallows' kernel is universal and\ncharacteristic. We also introduce families of polynomial kernels that\ninterpolate between the Kendall (degree one) and Mallows' (infinite degree)\nkernels. We show the practical effectiveness of our methods via applications to\nEurobarometer survey data as well as a Movielens ratings dataset. \n\n"}
{"id": "1603.08233", "contents": "Title: Evolution of active categorical image classification via saccadic eye\n  movement Abstract: Pattern recognition and classification is a central concern for modern\ninformation processing systems. In particular, one key challenge to image and\nvideo classification has been that the computational cost of image processing\nscales linearly with the number of pixels in the image or video. Here we\npresent an intelligent machine (the \"active categorical classifier,\" or ACC)\nthat is inspired by the saccadic movements of the eye, and is capable of\nclassifying images by selectively scanning only a portion of the image. We\nharness evolutionary computation to optimize the ACC on the MNIST hand-written\ndigit classification task, and provide a proof-of-concept that the ACC works on\nnoisy multi-class data. We further analyze the ACC and demonstrate its ability\nto classify images after viewing only a fraction of the pixels, and provide\ninsight on future research paths to further improve upon the ACC presented\nhere. \n\n"}
{"id": "1603.08561", "contents": "Title: Shuffle and Learn: Unsupervised Learning using Temporal Order\n  Verification Abstract: In this paper, we present an approach for learning a visual representation\nfrom the raw spatiotemporal signals in videos. Our representation is learned\nwithout supervision from semantic labels. We formulate our method as an\nunsupervised sequential verification task, i.e., we determine whether a\nsequence of frames from a video is in the correct temporal order. With this\nsimple task and no semantic labels, we learn a powerful visual representation\nusing a Convolutional Neural Network (CNN). The representation contains\ncomplementary information to that learned from supervised image datasets like\nImageNet. Qualitative results show that our method captures information that is\ntemporally varying, such as human pose. When used as pre-training for action\nrecognition, our method gives significant gains over learning without external\ndata on benchmark datasets like UCF101 and HMDB51. To demonstrate its\nsensitivity to human pose, we show results for pose estimation on the FLIC and\nMPII datasets that are competitive, or better than approaches using\nsignificantly more supervision. Our method can be combined with supervised\nrepresentations to provide an additional boost in accuracy. \n\n"}
{"id": "1603.08861", "contents": "Title: Revisiting Semi-Supervised Learning with Graph Embeddings Abstract: We present a semi-supervised learning framework based on graph embeddings.\nGiven a graph between instances, we train an embedding for each instance to\njointly predict the class label and the neighborhood context in the graph. We\ndevelop both transductive and inductive variants of our method. In the\ntransductive variant of our method, the class labels are determined by both the\nlearned embeddings and input feature vectors, while in the inductive variant,\nthe embeddings are defined as a parametric function of the feature vectors, so\npredictions can be made on instances not seen during training. On a large and\ndiverse set of benchmark tasks, including text classification, distantly\nsupervised entity extraction, and entity classification, we show improved\nperformance over many of the existing models. \n\n"}
{"id": "1604.01475", "contents": "Title: Learning A Deep $\\ell_\\infty$ Encoder for Hashing Abstract: We investigate the $\\ell_\\infty$-constrained representation which\ndemonstrates robustness to quantization errors, utilizing the tool of deep\nlearning. Based on the Alternating Direction Method of Multipliers (ADMM), we\nformulate the original convex minimization problem as a feed-forward neural\nnetwork, named \\textit{Deep $\\ell_\\infty$ Encoder}, by introducing the novel\nBounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as\nnetwork biases. Such a structural prior acts as an effective network\nregularization, and facilitates the model initialization. We then investigate\nthe effective use of the proposed model in the application of hashing, by\ncoupling the proposed encoders under a supervised pairwise loss, to develop a\n\\textit{Deep Siamese $\\ell_\\infty$ Network}, which can be optimized from end to\nend. Extensive experiments demonstrate the impressive performances of the\nproposed model. We also provide an in-depth analysis of its behaviors against\nthe competitors. \n\n"}
{"id": "1604.08642", "contents": "Title: On the representation and embedding of knowledge bases beyond binary\n  relations Abstract: The models developed to date for knowledge base embedding are all based on\nthe assumption that the relations contained in knowledge bases are binary. For\nthe training and testing of these embedding models, multi-fold (or n-ary)\nrelational data are converted to triples (e.g., in FB15K dataset) and\ninterpreted as instances of binary relations. This paper presents a canonical\nrepresentation of knowledge bases containing multi-fold relations. We show that\nthe existing embedding models on the popular FB15K datasets correspond to a\nsub-optimal modelling framework, resulting in a loss of structural information.\nWe advocate a novel modelling framework, which models multi-fold relations\ndirectly using this canonical representation. Using this framework, the\nexisting TransH model is generalized to a new model, m-TransH. We demonstrate\nexperimentally that m-TransH outperforms TransH by a large margin, thereby\nestablishing a new state of the art. \n\n"}
{"id": "1605.00017", "contents": "Title: deepMiRGene: Deep Neural Network based Precursor microRNA Prediction Abstract: Since microRNAs (miRNAs) play a crucial role in post-transcriptional gene\nregulation, miRNA identification is one of the most essential problems in\ncomputational biology. miRNAs are usually short in length ranging between 20\nand 23 base pairs. It is thus often difficult to distinguish miRNA-encoding\nsequences from other non-coding RNAs and pseudo miRNAs that have a similar\nlength, and most previous studies have recommended using precursor miRNAs\ninstead of mature miRNAs for robust detection. A great number of conventional\nmachine-learning-based classification methods have been proposed, but they\noften have the serious disadvantage of requiring manual feature engineering,\nand their performance is limited as well. In this paper, we propose a novel\nmiRNA precursor prediction algorithm, deepMiRGene, based on recurrent neural\nnetworks, specifically long short-term memory networks. deepMiRGene\nautomatically learns suitable features from the data themselves without manual\nfeature engineering and constructs a model that can successfully reflect\nstructural characteristics of precursor miRNAs. For the performance evaluation\nof our approach, we have employed several widely used evaluation metrics on\nthree recent benchmark datasets and verified that deepMiRGene delivered\ncomparable performance among the current state-of-the-art tools. \n\n"}
{"id": "1605.01451", "contents": "Title: Boltzmann meets Nash: Energy-efficient routing in optical networks under\n  uncertainty Abstract: Motivated by the massive deployment of power-hungry data centers for service\nprovisioning, we examine the problem of routing in optical networks with the\naim of minimizing traffic-driven power consumption. To tackle this issue,\nrouting must take into account energy efficiency as well as capacity\nconsiderations; moreover, in rapidly-varying network environments, this must be\naccomplished in a real-time, distributed manner that remains robust in the\npresence of random disturbances and noise. In view of this, we derive a pricing\nscheme whose Nash equilibria coincide with the network's socially optimum\nstates, and we propose a distributed learning method based on the Boltzmann\ndistribution of statistical mechanics. Using tools from stochastic calculus, we\nshow that the resulting Boltzmann routing scheme exhibits remarkable\nconvergence properties under uncertainty: specifically, the long-term average\nof the network's power consumption converges within $\\varepsilon$ of its\nminimum value in time which is at most $\\tilde O(1/\\varepsilon^2)$,\nirrespective of the fluctuations' magnitude; additionally, if the network\nadmits a strict, non-mixing optimum state, the algorithm converges to it -\nagain, no matter the noise level. Our analysis is supplemented by extensive\nnumerical simulations which show that Boltzmann routing can lead to a\nsignificant decrease in power consumption over basic, shortest-path routing\nschemes in realistic network conditions. \n\n"}
{"id": "1605.02832", "contents": "Title: Transport Analysis of Infinitely Deep Neural Network Abstract: We investigated the feature map inside deep neural networks (DNNs) by\ntracking the transport map. We are interested in the role of depth (why do DNNs\nperform better than shallow models?) and the interpretation of DNNs (what do\nintermediate layers do?) Despite the rapid development in their application,\nDNNs remain analytically unexplained because the hidden layers are nested and\nthe parameters are not faithful. Inspired by the integral representation of\nshallow NNs, which is the continuum limit of the width, or the hidden unit\nnumber, we developed the flow representation and transport analysis of DNNs.\nThe flow representation is the continuum limit of the depth or the hidden layer\nnumber, and it is specified by an ordinary differential equation with a vector\nfield. We interpret an ordinary DNN as a transport map or a Euler broken line\napproximation of the flow. Technically speaking, a dynamical system is a\nnatural model for the nested feature maps. In addition, it opens a new way to\nthe coordinate-free treatment of DNNs by avoiding the redundant parametrization\nof DNNs. Following Wasserstein geometry, we analyze a flow in three aspects:\ndynamical system, continuity equation, and Wasserstein gradient flow. A key\nfinding is that we specified a series of transport maps of the denoising\nautoencoder (DAE). Starting from the shallow DAE, this paper develops three\ntopics: the transport map of the deep DAE, the equivalence between the stacked\nDAE and the composition of DAEs, and the development of the double continuum\nlimit or the integral representation of the flow representation. As partial\nanswers to the research questions, we found that deeper DAEs converge faster\nand the extracted features are better; in addition, a deep Gaussian DAE\ntransports mass to decrease the Shannon entropy of the data distribution. \n\n"}
{"id": "1605.04638", "contents": "Title: Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online\n  Learning with True and Noisy Gradient Abstract: This work focuses on dynamic regret of online convex optimization that\ncompares the performance of online learning to a clairvoyant who knows the\nsequence of loss functions in advance and hence selects the minimizer of the\nloss function at each step. By assuming that the clairvoyant moves slowly\n(i.e., the minimizers change slowly), we present several improved\nvariation-based upper bounds of the dynamic regret under the true and noisy\ngradient feedback, which are {\\it optimal} in light of the presented lower\nbounds. The key to our analysis is to explore a regularity metric that measures\nthe temporal changes in the clairvoyant's minimizers, to which we refer as {\\it\npath variation}. Firstly, we present a general lower bound in terms of the path\nvariation, and then show that under full information or gradient feedback we\nare able to achieve an optimal dynamic regret. Secondly, we present a lower\nbound with noisy gradient feedback and then show that we can achieve optimal\ndynamic regrets under a stochastic gradient feedback and two-point bandit\nfeedback. Moreover, for a sequence of smooth loss functions that admit a small\nvariation in the gradients, our dynamic regret under the two-point bandit\nfeedback matches what is achieved with full information. \n\n"}
{"id": "1605.05368", "contents": "Title: Deep Action Sequence Learning for Causal Shape Transformation Abstract: Deep learning became the method of choice in recent year for solving a wide\nvariety of predictive analytics tasks. For sequence prediction, recurrent\nneural networks (RNN) are often the go-to architecture for exploiting\nsequential information where the output is dependent on previous computation.\nHowever, the dependencies of the computation lie in the latent domain which may\nnot be suitable for certain applications involving the prediction of a\nstep-wise transformation sequence that is dependent on the previous computation\nonly in the visible domain. We propose that a hybrid architecture of\nconvolution neural networks (CNN) and stacked autoencoders (SAE) is sufficient\nto learn a sequence of actions that nonlinearly transforms an input shape or\ndistribution into a target shape or distribution with the same support. While\nsuch a framework can be useful in a variety of problems such as robotic path\nplanning, sequential decision-making in games, and identifying material\nprocessing pathways to achieve desired microstructures, the application of the\nframework is exemplified by the control of fluid deformations in a microfluidic\nchannel by deliberately placing a sequence of pillars. Learning of a multistep\ntopological transform has significant implications for rapid advances in\nmaterial science and biomedical applications. \n\n"}
{"id": "1605.07051", "contents": "Title: Convergence Analysis for Rectangular Matrix Completion Using\n  Burer-Monteiro Factorization and Gradient Descent Abstract: We address the rectangular matrix completion problem by lifting the unknown\nmatrix to a positive semidefinite matrix in higher dimension, and optimizing a\nnonconvex objective over the semidefinite factor using a simple gradient\ndescent scheme. With $O( \\mu r^2 \\kappa^2 n \\max(\\mu, \\log n))$ random\nobservations of a $n_1 \\times n_2$ $\\mu$-incoherent matrix of rank $r$ and\ncondition number $\\kappa$, where $n = \\max(n_1, n_2)$, the algorithm linearly\nconverges to the global optimum with high probability. \n\n"}
{"id": "1605.07066", "contents": "Title: A Unifying Framework for Gaussian Process Pseudo-Point Approximations\n  using Power Expectation Propagation Abstract: Gaussian processes (GPs) are flexible distributions over functions that\nenable high-level assumptions about unknown functions to be encoded in a\nparsimonious, flexible and general way. Although elegant, the application of\nGPs is limited by computational and analytical intractabilities that arise when\ndata are sufficiently numerous or when employing non-Gaussian models.\nConsequently, a wealth of GP approximation schemes have been developed over the\nlast 15 years to address these key limitations. Many of these schemes employ a\nsmall set of pseudo data points to summarise the actual data. In this paper, we\ndevelop a new pseudo-point approximation framework using Power Expectation\nPropagation (Power EP) that unifies a large number of these pseudo-point\napproximations. Unlike much of the previous venerable work in this area, the\nnew framework is built on standard methods for approximate inference\n(variational free-energy, EP and Power EP methods) rather than employing\napproximations to the probabilistic generative model itself. In this way, all\nof approximation is performed at `inference time' rather than at `modelling\ntime' resolving awkward philosophical and empirical questions that trouble\nprevious approaches. Crucially, we demonstrate that the new framework includes\nnew pseudo-point approximation methods that outperform current approaches on\nregression and classification tasks. \n\n"}
{"id": "1605.07246", "contents": "Title: Adaptive ADMM with Spectral Penalty Parameter Selection Abstract: The alternating direction method of multipliers (ADMM) is a versatile tool\nfor solving a wide range of constrained optimization problems, with\ndifferentiable or non-differentiable objective functions. Unfortunately, its\nperformance is highly sensitive to a penalty parameter, which makes ADMM often\nunreliable and hard to automate for a non-expert user. We tackle this weakness\nof ADMM by proposing a method to adaptively tune the penalty parameters to\nachieve fast convergence. The resulting adaptive ADMM (AADMM) algorithm,\ninspired by the successful Barzilai-Borwein spectral method for gradient\ndescent, yields fast convergence and relative insensitivity to the initial\nstepsize and problem scaling. \n\n"}
{"id": "1605.07422", "contents": "Title: Computing Web-scale Topic Models using an Asynchronous Parameter Server Abstract: Topic models such as Latent Dirichlet Allocation (LDA) have been widely used\nin information retrieval for tasks ranging from smoothing and feedback methods\nto tools for exploratory search and discovery. However, classical methods for\ninferring topic models do not scale up to the massive size of today's publicly\navailable Web-scale data sets. The state-of-the-art approaches rely on custom\nstrategies, implementations and hardware to facilitate their asynchronous,\ncommunication-intensive workloads.\n  We present APS-LDA, which integrates state-of-the-art topic modeling with\ncluster computing frameworks such as Spark using a novel asynchronous parameter\nserver. Advantages of this integration include convenient usage of existing\ndata processing pipelines and eliminating the need for disk writes as data can\nbe kept in memory from start to finish. Our goal is not to outperform highly\ncustomized implementations, but to propose a general high-performance topic\nmodeling framework that can easily be used in today's data processing\npipelines. We compare APS-LDA to the existing Spark LDA implementations and\nshow that our system can, on a 480-core cluster, process up to 135 times more\ndata and 10 times more topics without sacrificing model quality. \n\n"}
{"id": "1605.07583", "contents": "Title: Recursive Sampling for the Nystr\\\"om Method Abstract: We give the first algorithm for kernel Nystr\\\"om approximation that runs in\n*linear time in the number of training points* and is provably accurate for all\nkernel matrices, without dependence on regularity or incoherence conditions.\nThe algorithm projects the kernel onto a set of $s$ landmark points sampled by\ntheir *ridge leverage scores*, requiring just $O(ns)$ kernel evaluations and\n$O(ns^2)$ additional runtime. While leverage score sampling has long been known\nto give strong theoretical guarantees for Nystr\\\"om approximation, by employing\na fast recursive sampling scheme, our algorithm is the first to make the\napproach scalable. Empirically we show that it finds more accurate, lower rank\nkernel approximations in less time than popular techniques such as uniformly\nsampled Nystr\\\"om approximation and the random Fourier features method. \n\n"}
{"id": "1605.07811", "contents": "Title: Probabilistic Numerical Methods for Partial Differential Equations and\n  Bayesian Inverse Problems Abstract: This paper develops a probabilistic numerical method for solution of partial\ndifferential equations (PDEs) and studies application of that method to\nPDE-constrained inverse problems. This approach enables the solution of\nchallenging inverse problems whilst accounting, in a statistically principled\nway, for the impact of discretisation error due to numerical solution of the\nPDE. In particular, the approach confers robustness to failure of the numerical\nPDE solver, with statistical inferences driven to be more conservative in the\npresence of substantial discretisation error. Going further, the problem of\nchoosing a PDE solver is cast as a problem in the Bayesian design of\nexperiments, where the aim is to minimise the impact of solver error on\nstatistical inferences; here the challenge of non-linear PDEs is also\nconsidered. The method is applied to parameter inference problems in which\ndiscretisation error in non-negligible and must be accounted for in order to\nreach conclusions that are statistically valid. \n\n"}
{"id": "1605.07824", "contents": "Title: Action Classification via Concepts and Attributes Abstract: Classes in natural images tend to follow long tail distributions. This is\nproblematic when there are insufficient training examples for rare classes.\nThis effect is emphasized in compound classes, involving the conjunction of\nseveral concepts, such as those appearing in action-recognition datasets. In\nthis paper, we propose to address this issue by learning how to utilize common\nvisual concepts which are readily available. We detect the presence of\nprominent concepts in images and use them to infer the target labels instead of\nusing visual features directly, combining tools from vision and\nnatural-language processing. We validate our method on the recently introduced\nHICO dataset reaching a mAP of 31.54\\% and on the Stanford-40 Actions dataset,\nwhere the proposed method outperforms that obtained by direct visual features,\nobtaining an accuracy 83.12\\%. Moreover, the method provides for each class a\nsemantically meaningful list of keywords and relevant image regions relating it\nto its constituent concepts. \n\n"}
{"id": "1605.08108", "contents": "Title: FLAG n' FLARE: Fast Linearly-Coupled Adaptive Gradient Methods Abstract: We consider first order gradient methods for effectively optimizing a\ncomposite objective in the form of a sum of smooth and, potentially, non-smooth\nfunctions. We present accelerated and adaptive gradient methods, called FLAG\nand FLARE, which can offer the best of both worlds. They can achieve the\noptimal convergence rate by attaining the optimal first-order oracle complexity\nfor smooth convex optimization. Additionally, they can adaptively and\nnon-uniformly re-scale the gradient direction to adapt to the limited curvature\navailable and conform to the geometry of the domain. We show theoretically and\nempirically that, through the compounding effects of acceleration and\nadaptivity, FLAG and FLARE can be highly effective for many data fitting and\nmachine learning applications. \n\n"}
{"id": "1605.08325", "contents": "Title: Theano-MPI: a Theano-based Distributed Training Framework Abstract: We develop a scalable and extendable training framework that can utilize GPUs\nacross nodes in a cluster and accelerate the training of deep learning models\nbased on data parallelism. Both synchronous and asynchronous training are\nimplemented in our framework, where parameter exchange among GPUs is based on\nCUDA-aware MPI. In this report, we analyze the convergence and capability of\nthe framework to reduce training time when scaling the synchronous training of\nAlexNet and GoogLeNet from 2 GPUs to 8 GPUs. In addition, we explore novel ways\nto reduce the communication overhead caused by exchanging parameters. Finally,\nwe release the framework as open-source for further research on distributed\ndeep learning \n\n"}
{"id": "1605.08803", "contents": "Title: Density estimation using Real NVP Abstract: Unsupervised learning of probabilistic models is a central yet challenging\nproblem in machine learning. Specifically, designing models with tractable\nlearning, sampling, inference and evaluation is crucial in solving this task.\nWe extend the space of such models using real-valued non-volume preserving\n(real NVP) transformations, a set of powerful invertible and learnable\ntransformations, resulting in an unsupervised learning algorithm with exact\nlog-likelihood computation, exact sampling, exact inference of latent\nvariables, and an interpretable latent space. We demonstrate its ability to\nmodel natural images on four datasets through sampling, log-likelihood\nevaluation and latent variable manipulations. \n\n"}
{"id": "1605.09046", "contents": "Title: TripleSpin - a generic compact paradigm for fast machine learning\n  computations Abstract: We present a generic compact computational framework relying on structured\nrandom matrices that can be applied to speed up several machine learning\nalgorithms with almost no loss of accuracy. The applications include new fast\nLSH-based algorithms, efficient kernel computations via random feature maps,\nconvex optimization algorithms, quantization techniques and many more. Certain\nmodels of the presented paradigm are even more compressible since they apply\nonly bit matrices. This makes them suitable for deploying on mobile devices.\nAll our findings come with strong theoretical guarantees. In particular, as a\nbyproduct of the presented techniques and by using relatively new\nBerry-Esseen-type CLT for random vectors, we give the first theoretical\nguarantees for one of the most efficient existing LSH algorithms based on the\n$\\textbf{HD}_{3}\\textbf{HD}_{2}\\textbf{HD}_{1}$ structured matrix (\"Practical\nand Optimal LSH for Angular Distance\"). These guarantees as well as theoretical\nresults for other aforementioned applications follow from the same general\ntheoretical principle that we present in the paper. Our structured family\ncontains as special cases all previously considered structured schemes,\nincluding the recently introduced $P$-model. Experimental evaluation confirms\nthe accuracy and efficiency of TripleSpin matrices. \n\n"}
{"id": "1606.00704", "contents": "Title: Adversarially Learned Inference Abstract: We introduce the adversarially learned inference (ALI) model, which jointly\nlearns a generation network and an inference network using an adversarial\nprocess. The generation network maps samples from stochastic latent variables\nto the data space while the inference network maps training examples in data\nspace to the space of latent variables. An adversarial game is cast between\nthese two networks and a discriminative network is trained to distinguish\nbetween joint latent/data-space samples from the generative network and joint\nsamples from the inference network. We illustrate the ability of the model to\nlearn mutually coherent inference and generation networks through the\ninspections of model samples and reconstructions and confirm the usefulness of\nthe learned representations by obtaining a performance competitive with\nstate-of-the-art on the semi-supervised SVHN and CIFAR10 tasks. \n\n"}
{"id": "1606.01280", "contents": "Title: Dependency Parsing as Head Selection Abstract: Conventional graph-based dependency parsers guarantee a tree structure both\nduring training and inference. Instead, we formalize dependency parsing as the\nproblem of independently selecting the head of each word in a sentence. Our\nmodel which we call \\textsc{DeNSe} (as shorthand for {\\bf De}pendency {\\bf\nN}eural {\\bf Se}lection) produces a distribution over possible heads for each\nword using features obtained from a bidirectional recurrent neural network.\nWithout enforcing structural constraints during training, \\textsc{DeNSe}\ngenerates (at inference time) trees for the overwhelming majority of sentences,\nwhile non-tree outputs can be adjusted with a maximum spanning tree algorithm.\nWe evaluate \\textsc{DeNSe} on four languages (English, Chinese, Czech, and\nGerman) with varying degrees of non-projectivity. Despite the simplicity of the\napproach, our parsers are on par with the state of the art. \n\n"}
{"id": "1606.01412", "contents": "Title: Distance Metric Ensemble Learning and the Andrews-Curtis Conjecture Abstract: Motivated by the search for a counterexample to the Poincar\\'e conjecture in\nthree and four dimensions, the Andrews-Curtis conjecture was proposed in 1965.\nIt is now generally suspected that the Andrews-Curtis conjecture is false, but\nsmall potential counterexamples are not so numerous, and previous work has\nattempted to eliminate some via combinatorial search. Progress has however been\nlimited, with the most successful approach (breadth-first-search using\nsecondary storage) being neither scalable nor heuristically-informed. A\nprevious empirical analysis of problem structure examined several heuristic\nmeasures of search progress and determined that none of them provided any\nuseful guidance for search. In this article, we induce new quality measures\ndirectly from the problem structure and combine them to produce a more\neffective search driver via ensemble machine learning. By this means, we\neliminate 19 potential counterexamples, the status of which had been unknown\nfor some years. \n\n"}
{"id": "1606.01587", "contents": "Title: A Deep-Learning Approach for Operation of an Automated Realtime Flare\n  Forecast Abstract: Automated forecasts serve important role in space weather science, by\nproviding statistical insights to flare-trigger mechanisms, and by enabling\ntailor-made forecasts and high-frequency forecasts. Only by realtime forecast\nwe can experimentally measure the performance of flare-forecasting methods\nwhile confidently avoiding overlearning.\n  We have been operating unmanned flare forecast service since August, 2015\nthat provides 24-hour-ahead forecast of solar flares, every 12 minutes. We\nreport the method and prediction results of the system. \n\n"}
{"id": "1606.02346", "contents": "Title: How is a data-driven approach better than random choice in label space\n  division for multi-label classification? Abstract: We propose using five data-driven community detection approaches from social\nnetworks to partition the label space for the task of multi-label\nclassification as an alternative to random partitioning into equal subsets as\nperformed by RAkELd: modularity-maximizing fastgreedy and leading eigenvector,\ninfomap, walktrap and label propagation algorithms. We construct a label\nco-occurence graph (both weighted an unweighted versions) based on training\ndata and perform community detection to partition the label set. We include\nBinary Relevance and Label Powerset classification methods for comparison. We\nuse gini-index based Decision Trees as the base classifier. We compare educated\napproaches to label space divisions against random baselines on 12 benchmark\ndata sets over five evaluation measures. We show that in almost all cases seven\neducated guess approaches are more likely to outperform RAkELd than otherwise\nin all measures, but Hamming Loss. We show that fastgreedy and walktrap\ncommunity detection methods on weighted label co-occurence graphs are 85-92%\nmore likely to yield better F1 scores than random partitioning. Infomap on the\nunweighted label co-occurence graphs is on average 90% of the times better than\nrandom paritioning in terms of Subset Accuracy and 89% when it comes to Jaccard\nsimilarity. Weighted fastgreedy is better on average than RAkELd when it comes\nto Hamming Loss. \n\n"}
{"id": "1606.02404", "contents": "Title: Clustering with Same-Cluster Queries Abstract: We propose a framework for Semi-Supervised Active Clustering framework\n(SSAC), where the learner is allowed to interact with a domain expert, asking\nwhether two given instances belong to the same cluster or not. We study the\nquery and computational complexity of clustering in this framework. We consider\na setting where the expert conforms to a center-based clustering with a notion\nof margin. We show that there is a trade off between computational complexity\nand query complexity; We prove that for the case of $k$-means clustering (i.e.,\nwhen the expert conforms to a solution of $k$-means), having access to\nrelatively few such queries allows efficient solutions to otherwise NP hard\nproblems.\n  In particular, we provide a probabilistic polynomial-time (BPP) algorithm for\nclustering in this setting that asks $O\\big(k^2\\log k + k\\log n)$ same-cluster\nqueries and runs with time complexity $O\\big(kn\\log n)$ (where $k$ is the\nnumber of clusters and $n$ is the number of instances). The algorithm succeeds\nwith high probability for data satisfying margin conditions under which,\nwithout queries, we show that the problem is NP hard. We also prove a lower\nbound on the number of queries needed to have a computationally efficient\nclustering algorithm in this setting. \n\n"}
{"id": "1606.03196", "contents": "Title: Phase Retrieval via Incremental Truncated Wirtinger Flow Abstract: In the phase retrieval problem, an unknown vector is to be recovered given\nquadratic measurements. This problem has received considerable attention in\nrecent times. In this paper, we present an algorithm to solve a nonconvex\nformulation of the phase retrieval problem, that we call $\\textit{Incremental\nTruncated Wirtinger Flow}$. Given random Gaussian sensing vectors, we prove\nthat it converges linearly to the solution, with an optimal sample complexity.\nWe also provide stability guarantees of the algorithm under noisy measurements.\nPerformance and comparisons with existing algorithms are illustrated via\nnumerical experiments on simulated and real data, with both random and\nstructured sensing vectors. \n\n"}
{"id": "1606.04160", "contents": "Title: The Crossover Process: Learnability and Data Protection from Inference\n  Attacks Abstract: It is usual to consider data protection and learnability as conflicting\nobjectives. This is not always the case: we show how to jointly control\ninference --- seen as the attack --- and learnability by a noise-free process\nthat mixes training examples, the Crossover Process (cp). One key point is that\nthe cp~is typically able to alter joint distributions without touching on\nmarginals, nor altering the sufficient statistic for the class. In other words,\nit saves (and sometimes improves) generalization for supervised learning, but\ncan alter the relationship between covariates --- and therefore fool measures\nof nonlinear independence and causal inference into misleading ad-hoc\nconclusions. For example, a cp~can increase / decrease odds ratios, bring\nfairness or break fairness, tamper with disparate impact, strengthen, weaken or\nreverse causal directions, change observed statistical measures of dependence.\nFor each of these, we quantify changes brought by a cp, as well as its\nstatistical impact on generalization abilities via a new complexity measure\nthat we call the Rademacher cp~complexity. Experiments on a dozen readily\navailable domains validate the theory. \n\n"}
{"id": "1606.04722", "contents": "Title: Bolt-on Differential Privacy for Scalable Stochastic Gradient\n  Descent-based Analytics Abstract: While significant progress has been made separately on analytics systems for\nscalable stochastic gradient descent (SGD) and private SGD, none of the major\nscalable analytics frameworks have incorporated differentially private SGD.\nThere are two inter-related issues for this disconnect between research and\npractice: (1) low model accuracy due to added noise to guarantee privacy, and\n(2) high development and runtime overhead of the private algorithms. This paper\ntakes a first step to remedy this disconnect and proposes a private SGD\nalgorithm to address \\emph{both} issues in an integrated manner. In contrast to\nthe white-box approach adopted by previous work, we revisit and use the\nclassical technique of {\\em output perturbation} to devise a novel \"bolt-on\"\napproach to private SGD. While our approach trivially addresses (2), it makes\n(1) even more challenging. We address this challenge by providing a novel\nanalysis of the $L_2$-sensitivity of SGD, which allows, under the same privacy\nguarantees, better convergence of SGD when only a constant number of passes can\nbe made over the data. We integrate our algorithm, as well as other\nstate-of-the-art differentially private SGD, into Bismarck, a popular scalable\nSGD-based analytics system on top of an RDBMS. Extensive experiments show that\nour algorithm can be easily integrated, incurs virtually no overhead, scales\nwell, and most importantly, yields substantially better (up to 4X) test\naccuracy than the state-of-the-art algorithms on many real datasets. \n\n"}
{"id": "1606.05685", "contents": "Title: Using Visual Analytics to Interpret Predictive Machine Learning Models Abstract: It is commonly believed that increasing the interpretability of a machine\nlearning model may decrease its predictive power. However, inspecting\ninput-output relationships of those models using visual analytics, while\ntreating them as black-box, can help to understand the reasoning behind\noutcomes without sacrificing predictive quality. We identify a space of\npossible solutions and provide two examples of where such techniques have been\nsuccessfully used in practice. \n\n"}
{"id": "1606.06160", "contents": "Title: DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low\n  Bitwidth Gradients Abstract: We propose DoReFa-Net, a method to train convolutional neural networks that\nhave low bitwidth weights and activations using low bitwidth parameter\ngradients. In particular, during backward pass, parameter gradients are\nstochastically quantized to low bitwidth numbers before being propagated to\nconvolutional layers. As convolutions during forward/backward passes can now\noperate on low bitwidth weights and activations/gradients respectively,\nDoReFa-Net can use bit convolution kernels to accelerate both training and\ninference. Moreover, as bit convolutions can be efficiently implemented on CPU,\nFPGA, ASIC and GPU, DoReFa-Net opens the way to accelerate training of low\nbitwidth neural network on these hardware. Our experiments on SVHN and ImageNet\ndatasets prove that DoReFa-Net can achieve comparable prediction accuracy as\n32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has\n1-bit weights, 2-bit activations, can be trained from scratch using 6-bit\ngradients to get 46.1\\% top-1 accuracy on ImageNet validation set. The\nDoReFa-Net AlexNet model is released publicly. \n\n"}
{"id": "1606.06357", "contents": "Title: Complex Embeddings for Simple Link Prediction Abstract: In statistical relational learning, the link prediction problem is key to\nautomatically understand the structure of large knowledge bases. As in previous\nstudies, we propose to solve this problem through latent factorization.\nHowever, here we make use of complex valued embeddings. The composition of\ncomplex embeddings can handle a large variety of binary relations, among them\nsymmetric and antisymmetric relations. Compared to state-of-the-art models such\nas Neural Tensor Network and Holographic Embeddings, our approach based on\ncomplex embeddings is arguably simpler, as it only uses the Hermitian dot\nproduct, the complex counterpart of the standard dot product between real\nvectors. Our approach is scalable to large datasets as it remains linear in\nboth space and time, while consistently outperforming alternative approaches on\nstandard link prediction benchmarks. \n\n"}
{"id": "1606.06653", "contents": "Title: Tracking Time-Vertex Propagation using Dynamic Graph Wavelets Abstract: Graph Signal Processing generalizes classical signal processing to signal or\ndata indexed by the vertices of a weighted graph. So far, the research efforts\nhave been focused on static graph signals. However numerous applications\ninvolve graph signals evolving in time, such as spreading or propagation of\nwaves on a network. The analysis of this type of data requires a new set of\nmethods that fully takes into account the time and graph dimensions. We propose\na novel class of wavelet frames named Dynamic Graph Wavelets, whose time-vertex\nevolution follows a dynamic process. We demonstrate that this set of functions\ncan be combined with sparsity based approaches such as compressive sensing to\nreveal information on the dynamic processes occurring on a graph. Experiments\non real seismological data show the efficiency of the technique, allowing to\nestimate the epicenter of earthquake events recorded by a seismic network. \n\n"}
{"id": "1606.08561", "contents": "Title: Estimating the class prior and posterior from noisy positives and\n  unlabeled data Abstract: We develop a classification algorithm for estimating posterior distributions\nfrom positive-unlabeled data, that is robust to noise in the positive labels\nand effective for high-dimensional data. In recent years, several algorithms\nhave been proposed to learn from positive-unlabeled data; however, many of\nthese contributions remain theoretical, performing poorly on real\nhigh-dimensional data that is typically contaminated with noise. We build on\nthis previous work to develop two practical classification algorithms that\nexplicitly model the noise in the positive labels and utilize univariate\ntransforms built on discriminative classifiers. We prove that these univariate\ntransforms preserve the class prior, enabling estimation in the univariate\nspace and avoiding kernel density estimation for high-dimensional data. The\ntheoretical development and both parametric and nonparametric algorithms\nproposed here constitutes an important step towards wide-spread use of robust\nclassification algorithms for positive-unlabeled data. \n\n"}
{"id": "1607.00474", "contents": "Title: Adaptive Neighborhood Graph Construction for Inference in\n  Multi-Relational Networks Abstract: A neighborhood graph, which represents the instances as vertices and their\nrelations as weighted edges, is the basis of many semi-supervised and\nrelational models for node labeling and link prediction. Most methods employ a\nsequential process to construct the neighborhood graph. This process often\nconsists of generating a candidate graph, pruning the candidate graph to make a\nneighborhood graph, and then performing inference on the variables (i.e.,\nnodes) in the neighborhood graph. In this paper, we propose a framework that\ncan dynamically adapt the neighborhood graph based on the states of variables\nfrom intermediate inference results, as well as structural properties of the\nrelations connecting them. A key strength of our framework is its ability to\nhandle multi-relational data and employ varying amounts of relations for each\ninstance based on the intermediate inference results. We formulate the link\nprediction task as inference on neighborhood graphs, and include preliminary\nresults illustrating the effects of different strategies in our proposed\nframework. \n\n"}
{"id": "1607.02533", "contents": "Title: Adversarial examples in the physical world Abstract: Most existing machine learning classifiers are highly vulnerable to\nadversarial examples. An adversarial example is a sample of input data which\nhas been modified very slightly in a way that is intended to cause a machine\nlearning classifier to misclassify it. In many cases, these modifications can\nbe so subtle that a human observer does not even notice the modification at\nall, yet the classifier still makes a mistake. Adversarial examples pose\nsecurity concerns because they could be used to perform an attack on machine\nlearning systems, even if the adversary has no access to the underlying model.\nUp to now, all previous work have assumed a threat model in which the adversary\ncan feed data directly into the machine learning classifier. This is not always\nthe case for systems operating in the physical world, for example those which\nare using signals from cameras and other sensors as an input. This paper shows\nthat even in such physical world scenarios, machine learning systems are\nvulnerable to adversarial examples. We demonstrate this by feeding adversarial\nimages obtained from cell-phone camera to an ImageNet Inception classifier and\nmeasuring the classification accuracy of the system. We find that a large\nfraction of adversarial examples are classified incorrectly even when perceived\nthrough the camera. \n\n"}
{"id": "1607.03597", "contents": "Title: Accelerating Eulerian Fluid Simulation With Convolutional Networks Abstract: Efficient simulation of the Navier-Stokes equations for fluid flow is a long\nstanding problem in applied mathematics, for which state-of-the-art methods\nrequire large compute resources. In this work, we propose a data-driven\napproach that leverages the approximation power of deep-learning with the\nprecision of standard solvers to obtain fast and highly realistic simulations.\nOur method solves the incompressible Euler equations using the standard\noperator splitting method, in which a large sparse linear system with many free\nparameters must be solved. We use a Convolutional Network with a highly\ntailored architecture, trained using a novel unsupervised learning framework to\nsolve the linear system. We present real-time 2D and 3D simulations that\noutperform recently proposed data-driven methods; the obtained results are\nrealistic and show good generalization properties. \n\n"}
{"id": "1607.07939", "contents": "Title: A Sensorimotor Reinforcement Learning Framework for Physical Human-Robot\n  Interaction Abstract: Modeling of physical human-robot collaborations is generally a challenging\nproblem due to the unpredictive nature of human behavior. To address this\nissue, we present a data-efficient reinforcement learning framework which\nenables a robot to learn how to collaborate with a human partner. The robot\nlearns the task from its own sensorimotor experiences in an unsupervised\nmanner. The uncertainty of the human actions is modeled using Gaussian\nprocesses (GP) to implement action-value functions. Optimal action selection\ngiven the uncertain GP model is ensured by Bayesian optimization. We apply the\nframework to a scenario in which a human and a PR2 robot jointly control the\nball position on a plank based on vision and force/torque data. Our\nexperimental results show the suitability of the proposed method in terms of\nfast and data-efficient model learning, optimal action selection under\nuncertainties and equal role sharing between the partners. \n\n"}
{"id": "1607.08012", "contents": "Title: Learning of Generalized Low-Rank Models: A Greedy Approach Abstract: Learning of low-rank matrices is fundamental to many machine learning\napplications. A state-of-the-art algorithm is the rank-one matrix pursuit\n(R1MP). However, it can only be used in matrix completion problems with the\nsquare loss. In this paper, we develop a more flexible greedy algorithm for\ngeneralized low-rank models whose optimization objective can be smooth or\nnonsmooth, general convex or strongly convex. The proposed algorithm has low\nper-iteration time complexity and fast convergence rate. Experimental results\nshow that it is much faster than the state-of-the-art, with comparable or even\nbetter prediction performance. \n\n"}
{"id": "1607.08863", "contents": "Title: Exponentially fast convergence to (strict) equilibrium via hedging Abstract: Motivated by applications to data networks where fast convergence is\nessential, we analyze the problem of learning in generic N-person games that\nadmit a Nash equilibrium in pure strategies. Specifically, we consider a\nscenario where players interact repeatedly and try to learn from past\nexperience by small adjustments based on local - and possibly imperfect -\npayoff information. For concreteness, we focus on the so-called \"hedge\" variant\nof the exponential weights algorithm where players select an action with\nprobability proportional to the exponential of the action's cumulative payoff\nover time. When players have perfect information on their mixed payoffs, the\nalgorithm converges locally to a strict equilibrium and the rate of convergence\nis exponentially fast - of the order of\n$\\mathcal{O}(\\exp(-a\\sum_{j=1}^{t}\\gamma_{j}))$ where $a>0$ is a constant and\n$\\gamma_{j}$ is the algorithm's step-size. In the presence of uncertainty,\nconvergence requires a more conservative step-size policy, but with high\nprobability, the algorithm remains locally convergent and achieves an\nexponential convergence rate. \n\n"}
{"id": "1608.00466", "contents": "Title: Learning Semantically Coherent and Reusable Kernels in Convolution\n  Neural Nets for Sentence Classification Abstract: The state-of-the-art CNN models give good performance on sentence\nclassification tasks. The purpose of this work is to empirically study\ndesirable properties such as semantic coherence, attention mechanism and\nreusability of CNNs in these tasks. Semantically coherent kernels are\npreferable as they are a lot more interpretable for explaining the decision of\nthe learned CNN model. We observe that the learned kernels do not have semantic\ncoherence. Motivated by this observation, we propose to learn kernels with\nsemantic coherence using clustering scheme combined with Word2Vec\nrepresentation and domain knowledge such as SentiWordNet. We suggest a\ntechnique to visualize attention mechanism of CNNs for decision explanation\npurpose. Reusable property enables kernels learned on one problem to be used in\nanother problem. This helps in efficient learning as only a few additional\ndomain specific filters may have to be learned. We demonstrate the efficacy of\nour core ideas of learning semantically coherent kernels and leveraging\nreusable kernels for efficient learning on several benchmark datasets.\nExperimental results show the usefulness of our approach by achieving\nperformance close to the state-of-the-art methods but with semantic and\nreusable properties. \n\n"}
{"id": "1608.00627", "contents": "Title: Learning Transferable Policies for Monocular Reactive MAV Control Abstract: The ability to transfer knowledge gained in previous tasks into new contexts\nis one of the most important mechanisms of human learning. Despite this,\nadapting autonomous behavior to be reused in partially similar settings is\nstill an open problem in current robotics research. In this paper, we take a\nsmall step in this direction and propose a generic framework for learning\ntransferable motion policies. Our goal is to solve a learning problem in a\ntarget domain by utilizing the training data in a different but related source\ndomain. We present this in the context of an autonomous MAV flight using\nmonocular reactive control, and demonstrate the efficacy of our proposed\napproach through extensive real-world flight experiments in outdoor cluttered\nenvironments. \n\n"}
{"id": "1608.03287", "contents": "Title: Deep vs. shallow networks : An approximation theory perspective Abstract: The paper briefy reviews several recent results on hierarchical architectures\nfor learning from examples, that may formally explain the conditions under\nwhich Deep Convolutional Neural Networks perform much better in function\napproximation problems than shallow, one-hidden layer architectures. The paper\nannounces new results for a non-smooth activation function - the ReLU function\n- used in present-day neural networks, as well as for the Gaussian networks. We\npropose a new definition of relative dimension to encapsulate different notions\nof sparsity of a function class that can possibly be exploited by deep networks\nbut not by shallow ones to drastically reduce the complexity required for\napproximation and learning. \n\n"}
{"id": "1608.03339", "contents": "Title: Distributed learning with regularized least squares Abstract: We study distributed learning with the least squares regularization scheme in\na reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach,\nthe algorithm partitions a data set into disjoint data subsets, applies the\nleast squares regularization scheme to each data subset to produce an output\nfunction, and then takes an average of the individual output functions as a\nfinal global estimator or predictor. We show with error bounds in expectation\nin both the $L^2$-metric and RKHS-metric that the global output function of\nthis distributed learning is a good approximation to the algorithm processing\nthe whole data in one single machine. Our error bounds are sharp and stated in\na general setting without any eigenfunction assumption. The analysis is\nachieved by a novel second order decomposition of operator differences in our\nintegral operator approach. Even for the classical least squares regularization\nscheme in the RKHS associated with a general kernel, we give the best learning\nrate in the literature. \n\n"}
{"id": "1608.04636", "contents": "Title: Linear Convergence of Gradient and Proximal-Gradient Methods Under the\n  Polyak-\\L{}ojasiewicz Condition Abstract: In 1963, Polyak proposed a simple condition that is sufficient to show a\nglobal linear convergence rate for gradient descent. This condition is a\nspecial case of the \\L{}ojasiewicz inequality proposed in the same year, and it\ndoes not require strong convexity (or even convexity). In this work, we show\nthat this much-older Polyak-\\L{}ojasiewicz (PL) inequality is actually weaker\nthan the main conditions that have been explored to show linear convergence\nrates without strong convexity over the last 25 years. We also use the PL\ninequality to give new analyses of randomized and greedy coordinate descent\nmethods, sign-based gradient descent methods, and stochastic gradient methods\nin the classic setting (with decreasing or constant step-sizes) as well as the\nvariance-reduced setting. We further propose a generalization that applies to\nproximal-gradient methods for non-smooth optimization, leading to simple proofs\nof linear convergence of these methods. Along the way, we give simple\nconvergence results for a wide variety of problems in machine learning: least\nsquares, logistic regression, boosting, resilient backpropagation,\nL1-regularization, support vector machines, stochastic dual coordinate ascent,\nand stochastic variance-reduced gradient methods. \n\n"}
{"id": "1608.05258", "contents": "Title: Parameter Learning for Log-supermodular Distributions Abstract: We consider log-supermodular models on binary variables, which are\nprobabilistic models with negative log-densities which are submodular. These\nmodels provide probabilistic interpretations of common combinatorial\noptimization tasks such as image segmentation. In this paper, we focus\nprimarily on parameter estimation in the models from known upper-bounds on the\nintractable log-partition function. We show that the bound based on separable\noptimization on the base polytope of the submodular function is always inferior\nto a bound based on \"perturb-and-MAP\" ideas. Then, to learn parameters, given\nthat our approximation of the log-partition function is an expectation (over\nour own randomization), we use a stochastic subgradient technique to maximize a\nlower-bound on the log-likelihood. This can also be extended to conditional\nmaximum likelihood. We illustrate our new results in a set of experiments in\nbinary image denoising, where we highlight the flexibility of a probabilistic\nmodel to learn with missing data. \n\n"}
{"id": "1608.06049", "contents": "Title: Local Binary Convolutional Neural Networks Abstract: We propose local binary convolution (LBC), an efficient alternative to\nconvolutional layers in standard convolutional neural networks (CNN). The\ndesign principles of LBC are motivated by local binary patterns (LBP). The LBC\nlayer comprises of a set of fixed sparse pre-defined binary convolutional\nfilters that are not updated during the training process, a non-linear\nactivation function and a set of learnable linear weights. The linear weights\ncombine the activated filter responses to approximate the corresponding\nactivated filter responses of a standard convolutional layer. The LBC layer\naffords significant parameter savings, 9x to 169x in the number of learnable\nparameters compared to a standard convolutional layer. Furthermore, the sparse\nand binary nature of the weights also results in up to 9x to 169x savings in\nmodel size compared to a standard convolutional layer. We demonstrate both\ntheoretically and experimentally that our local binary convolution layer is a\ngood approximation of a standard convolutional layer. Empirically, CNNs with\nLBC layers, called local binary convolutional neural networks (LBCNN), achieves\nperformance parity with regular CNNs on a range of visual datasets (MNIST,\nSVHN, CIFAR-10, and ImageNet) while enjoying significant computational savings. \n\n"}
{"id": "1608.06807", "contents": "Title: Efficient Training for Positive Unlabeled Learning Abstract: Positive unlabeled (PU) learning is useful in various practical situations,\nwhere there is a need to learn a classifier for a class of interest from an\nunlabeled data set, which may contain anomalies as well as samples from unknown\nclasses. The learning task can be formulated as an optimization problem under\nthe framework of statistical learning theory. Recent studies have theoretically\nanalyzed its properties and generalization performance, nevertheless, little\neffort has been made to consider the problem of scalability, especially when\nlarge sets of unlabeled data are available. In this work we propose a novel\nscalable PU learning algorithm that is theoretically proven to provide the\noptimal solution, while showing superior computational and memory performance.\nExperimental evaluation confirms the theoretical evidence and shows that the\nproposed method can be successfully applied to a large variety of real-world\nproblems involving PU learning. \n\n"}
{"id": "1608.06993", "contents": "Title: Densely Connected Convolutional Networks Abstract: Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, strengthen feature\npropagation, encourage feature reuse, and substantially reduce the number of\nparameters. We evaluate our proposed architecture on four highly competitive\nobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high performance. Code and\npre-trained models are available at https://github.com/liuzhuang13/DenseNet . \n\n"}
{"id": "1608.07739", "contents": "Title: Bayesian selection for the l2-Potts model regularization parameter: 1D\n  piecewise constant signal denoising Abstract: Piecewise constant denoising can be solved either by deterministic\noptimization approaches, based on the Potts model, or by stochastic Bayesian\nprocedures. The former lead to low computational time but require the selection\nof a regularization parameter, whose value significantly impacts the achieved\nsolution, and whose automated selection remains an involved and challenging\nproblem. Conversely, fully Bayesian formalisms encapsulate the regularization\nparameter selection into hierarchical models, at the price of high\ncomputational costs. This contribution proposes an operational strategy that\ncombines hierarchical Bayesian and Potts model formulations, with the double\naim of automatically tuning the regularization parameter and of maintaining\ncomputational effciency. The proposed procedure relies on formally connecting a\nBayesian framework to a l2-Potts functional. Behaviors and performance for the\nproposed piecewise constant denoising and regularization parameter tuning\ntechniques are studied qualitatively and assessed quantitatively, and shown to\ncompare favorably against those of a fully Bayesian hierarchical procedure,\nboth in accuracy and in computational load. \n\n"}
{"id": "1609.00150", "contents": "Title: Reward Augmented Maximum Likelihood for Neural Structured Prediction Abstract: A key problem in structured output prediction is direct optimization of the\ntask reward function that matters for test evaluation. This paper presents a\nsimple and computationally efficient approach to incorporate task reward into a\nmaximum likelihood framework. By establishing a link between the log-likelihood\nand expected reward objectives, we show that an optimal regularized expected\nreward is achieved when the conditional distribution of the outputs given the\ninputs is proportional to their exponentiated scaled rewards. Accordingly, we\npresent a framework to smooth the predictive probability of the outputs using\ntheir corresponding rewards. We optimize the conditional log-probability of\naugmented outputs that are sampled proportionally to their exponentiated scaled\nrewards. Experiments on neural sequence to sequence models for speech\nrecognition and machine translation show notable improvements over a maximum\nlikelihood baseline by using reward augmented maximum likelihood (RAML), where\nthe rewards are defined as the negative edit distance between the outputs and\nthe ground truth labels. \n\n"}
{"id": "1609.02542", "contents": "Title: Quantum-Assisted Learning of Hardware-Embedded Probabilistic Graphical\n  Models Abstract: Mainstream machine-learning techniques such as deep learning and\nprobabilistic programming rely heavily on sampling from generally intractable\nprobability distributions. There is increasing interest in the potential\nadvantages of using quantum computing technologies as sampling engines to speed\nup these tasks or to make them more effective. However, some pressing\nchallenges in state-of-the-art quantum annealers have to be overcome before we\ncan assess their actual performance. The sparse connectivity, resulting from\nthe local interaction between quantum bits in physical hardware\nimplementations, is considered the most severe limitation to the quality of\nconstructing powerful generative unsupervised machine-learning models. Here we\nuse embedding techniques to add redundancy to data sets, allowing us to\nincrease the modeling capacity of quantum annealers. We illustrate our findings\nby training hardware-embedded graphical models on a binarized data set of\nhandwritten digits and two synthetic data sets in experiments with up to 940\nquantum bits. Our model can be trained in quantum hardware without full\nknowledge of the effective parameters specifying the corresponding quantum\nGibbs-like distribution; therefore, this approach avoids the need to infer the\neffective temperature at each iteration, speeding up learning; it also\nmitigates the effect of noise in the control parameters, making it robust to\ndeviations from the reference Gibbs distribution. Our approach demonstrates the\nfeasibility of using quantum annealers for implementing generative models, and\nit provides a suitable framework for benchmarking these quantum technologies on\nmachine-learning-related tasks. \n\n"}
{"id": "1609.02845", "contents": "Title: Distributed Online Optimization in Dynamic Environments Using Mirror\n  Descent Abstract: This work addresses decentralized online optimization in non-stationary\nenvironments. A network of agents aim to track the minimizer of a global\ntime-varying convex function. The minimizer evolves according to a known\ndynamics corrupted by an unknown, unstructured noise. At each time, the global\nfunction can be cast as a sum of a finite number of local functions, each of\nwhich is assigned to one agent in the network. Moreover, the local functions\nbecome available to agents sequentially, and agents do not have a prior\nknowledge of the future cost functions. Therefore, agents must communicate with\neach other to build an online approximation of the global function. We propose\na decentralized variation of the celebrated Mirror Descent, developed by\nNemirovksi and Yudin. Using the notion of Bregman divergence in lieu of\nEuclidean distance for projection, Mirror Descent has been shown to be a\npowerful tool in large-scale optimization. Our algorithm builds on Mirror\nDescent, while ensuring that agents perform a consensus step to follow the\nglobal function and take into account the dynamics of the global minimizer. To\nmeasure the performance of the proposed online algorithm, we compare it to its\noffline counterpart, where the global functions are available a priori. The gap\nbetween the two is called dynamic regret. We establish a regret bound that\nscales inversely in the spectral gap of the network, and more notably it\nrepresents the deviation of minimizer sequence with respect to the given\ndynamics. We then show that our results subsume a number of results in\ndistributed optimization. We demonstrate the application of our method to\ndecentralized tracking of dynamic parameters and verify the results via\nnumerical experiments. \n\n"}
{"id": "1609.03544", "contents": "Title: Online Data Thinning via Multi-Subspace Tracking Abstract: In an era of ubiquitous large-scale streaming data, the availability of data\nfar exceeds the capacity of expert human analysts. In many settings, such data\nis either discarded or stored unprocessed in datacenters. This paper proposes a\nmethod of online data thinning, in which large-scale streaming datasets are\nwinnowed to preserve unique, anomalous, or salient elements for timely expert\nanalysis. At the heart of this proposed approach is an online anomaly detection\nmethod based on dynamic, low-rank Gaussian mixture models. Specifically, the\nhigh-dimensional covariances matrices associated with the Gaussian components\nare associated with low-rank models. According to this model, most observations\nlie near a union of subspaces. The low-rank modeling mitigates the curse of\ndimensionality associated with anomaly detection for high-dimensional data, and\nrecent advances in subspace clustering and subspace tracking allow the proposed\nmethod to adapt to dynamic environments. Furthermore, the proposed method\nallows subsampling, is robust to missing data, and uses a mini-batch online\noptimization approach. The resulting algorithms are scalable, efficient, and\nare capable of operating in real time. Experiments on wide-area motion imagery\nand e-mail databases illustrate the efficacy of the proposed approach. \n\n"}
{"id": "1609.03958", "contents": "Title: Noisy Inductive Matrix Completion Under Sparse Factor Models Abstract: Inductive Matrix Completion (IMC) is an important class of matrix completion\nproblems that allows direct inclusion of available features to enhance\nestimation capabilities. These models have found applications in personalized\nrecommendation systems, multilabel learning, dictionary learning, etc. This\npaper examines a general class of noisy matrix completion tasks where the\nunderlying matrix is following an IMC model i.e., it is formed by a mixing\nmatrix (a priori unknown) sandwiched between two known feature matrices. The\nmixing matrix here is assumed to be well approximated by the product of two\nsparse matrices---referred here to as \"sparse factor models.\" We leverage the\nmain theorem of Soni:2016:NMC and extend it to provide theoretical error bounds\nfor the sparsity-regularized maximum likelihood estimators for the class of\nproblems discussed in this paper. The main result is general in the sense that\nit can be used to derive error bounds for various noise models. In this paper,\nwe instantiate our main result for the case of Gaussian noise and provide\ncorresponding error bounds in terms of squared loss. \n\n"}
{"id": "1609.04167", "contents": "Title: Proceedings of the third \"international Traveling Workshop on\n  Interactions between Sparse models and Technology\" (iTWIST'16) Abstract: The third edition of the \"international - Traveling Workshop on Interactions\nbetween Sparse models and Technology\" (iTWIST) took place in Aalborg, the 4th\nlargest city in Denmark situated beautifully in the northern part of the\ncountry, from the 24th to 26th of August 2016. The workshop venue was at the\nAalborg University campus. One implicit objective of this biennial workshop is\nto foster collaboration between international scientific teams by disseminating\nideas through both specific oral/poster presentations and free discussions. For\nthis third edition, iTWIST'16 gathered about 50 international participants and\nfeatures 8 invited talks, 12 oral presentations, and 12 posters on the\nfollowing themes, all related to the theory, application and generalization of\nthe \"sparsity paradigm\": Sparsity-driven data sensing and processing (e.g.,\noptics, computer vision, genomics, biomedical, digital communication, channel\nestimation, astronomy); Application of sparse models in non-convex/non-linear\ninverse problems (e.g., phase retrieval, blind deconvolution, self\ncalibration); Approximate probabilistic inference for sparse problems; Sparse\nmachine learning and inference; \"Blind\" inverse problems and dictionary\nlearning; Optimization for sparse modelling; Information theory, geometry and\nrandomness; Sparsity? What's next? (Discrete-valued signals; Union of\nlow-dimensional spaces, Cosparsity, mixed/group norm, model-based,\nlow-complexity models, ...); Matrix/manifold sensing/processing (graph,\nlow-rank approximation, ...); Complexity/accuracy tradeoffs in numerical\nmethods/optimization; Electronic/optical compressive sensors (hardware). \n\n"}
{"id": "1609.05521", "contents": "Title: Playing FPS Games with Deep Reinforcement Learning Abstract: Advances in deep reinforcement learning have allowed autonomous agents to\nperform well on Atari games, often outperforming humans, using only raw pixels\nto make their decisions. However, most of these games take place in 2D\nenvironments that are fully observable to the agent. In this paper, we present\nthe first architecture to tackle 3D environments in first-person shooter games,\nthat involve partially observable states. Typically, deep reinforcement\nlearning methods only utilize visual input for training. We present a method to\naugment these models to exploit game feature information such as the presence\nof enemies or items, during the training phase. Our model is trained to\nsimultaneously learn these features along with minimizing a Q-learning\nobjective, which is shown to dramatically improve the training speed and\nperformance of our agent. Our architecture is also modularized to allow\ndifferent models to be independently trained for different phases of the game.\nWe show that the proposed architecture substantially outperforms built-in AI\nagents of the game as well as humans in deathmatch scenarios. \n\n"}
{"id": "1609.05539", "contents": "Title: On Randomized Distributed Coordinate Descent with Quantized Updates Abstract: In this paper, we study the randomized distributed coordinate descent\nalgorithm with quantized updates. In the literature, the iteration complexity\nof the randomized distributed coordinate descent algorithm has been\ncharacterized under the assumption that machines can exchange updates with an\ninfinite precision. We consider a practical scenario in which the messages\nexchange occurs over channels with finite capacity, and hence the updates have\nto be quantized. We derive sufficient conditions on the quantization error such\nthat the algorithm with quantized update still converge. We further verify our\ntheoretical results by running an experiment, where we apply the algorithm with\nquantized updates to solve a linear regression problem. \n\n"}
{"id": "1609.05566", "contents": "Title: Label-Free Supervision of Neural Networks with Physics and Domain\n  Knowledge Abstract: In many machine learning applications, labeled data is scarce and obtaining\nmore labels is expensive. We introduce a new approach to supervising neural\nnetworks by specifying constraints that should hold over the output space,\nrather than direct examples of input-output pairs. These constraints are\nderived from prior domain knowledge, e.g., from known laws of physics. We\ndemonstrate the effectiveness of this approach on real world and simulated\ncomputer vision tasks. We are able to train a convolutional neural network to\ndetect and track objects without any labeled examples. Our approach can\nsignificantly reduce the need for labeled training data, but introduces new\nchallenges for encoding prior knowledge into appropriate loss functions. \n\n"}
{"id": "1609.05807", "contents": "Title: Inherent Trade-Offs in the Fair Determination of Risk Scores Abstract: Recent discussion in the public sphere about algorithmic classification has\ninvolved tension between competing notions of what it means for a probabilistic\nclassification to be fair to different groups. We formalize three fairness\nconditions that lie at the heart of these debates, and we prove that except in\nhighly constrained special cases, there is no method that can satisfy these\nthree conditions simultaneously. Moreover, even satisfying all three conditions\napproximately requires that the data lie in an approximate version of one of\nthe constrained special cases identified by our theorem. These results suggest\nsome of the ways in which key notions of fairness are incompatible with each\nother, and hence provide a framework for thinking about the trade-offs between\nthem. \n\n"}
{"id": "1609.07537", "contents": "Title: A Tutorial on Distributed (Non-Bayesian) Learning: Problem, Algorithms\n  and Results Abstract: We overview some results on distributed learning with focus on a family of\nrecently proposed algorithms known as non-Bayesian social learning. We consider\ndifferent approaches to the distributed learning problem and its algorithmic\nsolutions for the case of finitely many hypotheses. The original centralized\nproblem is discussed at first, and then followed by a generalization to the\ndistributed setting. The results on convergence and convergence rate are\npresented for both asymptotic and finite time regimes. Various extensions are\ndiscussed such as those dealing with directed time-varying networks, Nesterov's\nacceleration technique and a continuum sets of hypothesis. \n\n"}
{"id": "1609.08435", "contents": "Title: Asynchronous Stochastic Proximal Optimization Algorithms with Variance\n  Reduction Abstract: Regularized empirical risk minimization (R-ERM) is an important branch of\nmachine learning, since it constrains the capacity of the hypothesis space and\nguarantees the generalization ability of the learning algorithm. Two classic\nproximal optimization algorithms, i.e., proximal stochastic gradient descent\n(ProxSGD) and proximal stochastic coordinate descent (ProxSCD) have been widely\nused to solve the R-ERM problem. Recently, variance reduction technique was\nproposed to improve ProxSGD and ProxSCD, and the corresponding ProxSVRG and\nProxSVRCD have better convergence rate. These proximal algorithms with variance\nreduction technique have also achieved great success in applications at small\nand moderate scales. However, in order to solve large-scale R-ERM problems and\nmake more practical impacts, the parallel version of these algorithms are\nsorely needed. In this paper, we propose asynchronous ProxSVRG (Async-ProxSVRG)\nand asynchronous ProxSVRCD (Async-ProxSVRCD) algorithms, and prove that\nAsync-ProxSVRG can achieve near linear speedup when the training data is\nsparse, while Async-ProxSVRCD can achieve near linear speedup regardless of the\nsparse condition, as long as the number of block partitions are appropriately\nset. We have conducted experiments on a regularized logistic regression task.\nThe results verified our theoretical findings and demonstrated the practical\nefficiency of the asynchronous stochastic proximal algorithms with variance\nreduction. \n\n"}
{"id": "1609.09525", "contents": "Title: Multi-dimensional signal approximation with sparse structured priors\n  using split Bregman iterations Abstract: This paper addresses the structurally-constrained sparse decomposition of\nmulti-dimensional signals onto overcomplete families of vectors, called\ndictionaries. The contribution of the paper is threefold. Firstly, a generic\nspatio-temporal regularization term is designed and used together with the\nstandard $\\ell_1$ regularization term to enforce a sparse decomposition\npreserving the spatio-temporal structure of the signal. Secondly, an\noptimization algorithm based on the split Bregman approach is proposed to\nhandle the associated optimization problem, and its convergence is analyzed.\nOur well-founded approach yields same accuracy as the other algorithms at the\nstate-of-the-art, with significant gains in terms of convergence speed.\nThirdly, the empirical validation of the approach on artificial and real-world\nproblems demonstrates the generality and effectiveness of the method. On\nartificial problems, the proposed regularization subsumes the Total Variation\nminimization and recovers the expected decomposition. On the real-world problem\nof electro-encephalography brainwave decomposition, the approach outperforms\nsimilar approaches in terms of P300 evoked potentials detection, using\nstructured spatial priors to guide the decomposition. \n\n"}
{"id": "1610.00087", "contents": "Title: Very Deep Convolutional Neural Networks for Raw Waveforms Abstract: Learning acoustic models directly from the raw waveform data with minimal\nprocessing is challenging. Current waveform-based models have generally used\nvery few (~2) convolutional layers, which might be insufficient for building\nhigh-level discriminative features. In this work, we propose very deep\nconvolutional neural networks (CNNs) that directly use time-domain waveforms as\ninputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over\nvery long sequences (e.g., vector of size 32000), necessary for processing\nacoustic waveforms. This is achieved through batch normalization, residual\nlearning, and a careful design of down-sampling in the initial layers. Our\nnetworks are fully convolutional, without the use of fully connected layers and\ndropout, to maximize representation learning. We use a large receptive field in\nthe first convolutional layer to mimic bandpass filters, but very small\nreceptive fields subsequently to control the model capacity. We demonstrate the\nperformance gains with the deeper models. Our evaluation shows that the CNN\nwith 18 weight layers outperform the CNN with 3 weight layers by over 15% in\nabsolute accuracy for an environmental sound recognition task and matches the\nperformance of models using log-mel features. \n\n"}
{"id": "1610.03164", "contents": "Title: Navigational Instruction Generation as Inverse Reinforcement Learning\n  with Neural Machine Translation Abstract: Modern robotics applications that involve human-robot interaction require\nrobots to be able to communicate with humans seamlessly and effectively.\nNatural language provides a flexible and efficient medium through which robots\ncan exchange information with their human partners. Significant advancements\nhave been made in developing robots capable of interpreting free-form\ninstructions, but less attention has been devoted to endowing robots with the\nability to generate natural language. We propose a navigational guide model\nthat enables robots to generate natural language instructions that allow humans\nto navigate a priori unknown environments. We first decide which information to\nshare with the user according to their preferences, using a policy trained from\nhuman demonstrations via inverse reinforcement learning. We then \"translate\"\nthis information into a natural language instruction using a neural\nsequence-to-sequence model that learns to generate free-form instructions from\nnatural language corpora. We evaluate our method on a benchmark route\ninstruction dataset and achieve a BLEU score of 72.18% when compared to\nhuman-generated reference instructions. We additionally conduct navigation\nexperiments with human participants that demonstrate that our method generates\ninstructions that people follow as accurately and easily as those produced by\nhumans. \n\n"}
{"id": "1610.03628", "contents": "Title: RetiNet: Automatic AMD identification in OCT volumetric data Abstract: Optical Coherence Tomography (OCT) provides a unique ability to image the eye\nretina in 3D at micrometer resolution and gives ophthalmologist the ability to\nvisualize retinal diseases such as Age-Related Macular Degeneration (AMD).\nWhile visual inspection of OCT volumes remains the main method for AMD\nidentification, doing so is time consuming as each cross-section within the\nvolume must be inspected individually by the clinician. In much the same way,\nacquiring ground truth information for each cross-section is expensive and time\nconsuming. This fact heavily limits the ability to acquire large amounts of\nground truth, which subsequently impacts the performance of learning-based\nmethods geared at automatic pathology identification. To avoid this burden, we\npropose a novel strategy for automatic analysis of OCT volumes where only\nvolume labels are needed. That is, we train a classifier in a semi-supervised\nmanner to conduct this task. Our approach uses a novel Convolutional Neural\nNetwork (CNN) architecture, that only needs volume-level labels to be trained\nto automatically asses whether an OCT volume is healthy or contains AMD. Our\narchitecture involves first learning a cross-section pathology classifier using\npseudo-labels that could be corrupted and then leverage these towards a more\naccurate volume-level classification. We then show that our approach provides\nexcellent performances on a publicly available dataset and outperforms a number\nof existing automatic techniques. \n\n"}
{"id": "1610.04317", "contents": "Title: Approximate Counting, the Lovasz Local Lemma and Inference in Graphical\n  Models Abstract: In this paper we introduce a new approach for approximately counting in\nbounded degree systems with higher-order constraints. Our main result is an\nalgorithm to approximately count the number of solutions to a CNF formula\n$\\Phi$ when the width is logarithmic in the maximum degree. This closes an\nexponential gap between the known upper and lower bounds.\n  Moreover our algorithm extends straightforwardly to approximate sampling,\nwhich shows that under Lov\\'asz Local Lemma-like conditions it is not only\npossible to find a satisfying assignment, it is also possible to generate one\napproximately uniformly at random from the set of all satisfying assignments.\nOur approach is a significant departure from earlier techniques in approximate\ncounting, and is based on a framework to bootstrap an oracle for computing\nmarginal probabilities on individual variables. Finally, we give an application\nof our results to show that it is algorithmically possible to sample from the\nposterior distribution in an interesting class of graphical models. \n\n"}
{"id": "1610.05792", "contents": "Title: Big Batch SGD: Automated Inference using Adaptive Batch Sizes Abstract: Classical stochastic gradient methods for optimization rely on noisy gradient\napproximations that become progressively less accurate as iterates approach a\nsolution. The large noise and small signal in the resulting gradients makes it\ndifficult to use them for adaptive stepsize selection and automatic stopping.\nWe propose alternative \"big batch\" SGD schemes that adaptively grow the batch\nsize over time to maintain a nearly constant signal-to-noise ratio in the\ngradient approximation. The resulting methods have similar convergence rates to\nclassical SGD, and do not require convexity of the objective. The high fidelity\ngradients enable automated learning rate selection and do not require stepsize\ndecay. Big batch methods are thus easily automated and can run with little or\nno oversight. \n\n"}
{"id": "1610.06209", "contents": "Title: Structured adaptive and random spinners for fast machine learning\n  computations Abstract: We consider an efficient computational framework for speeding up several\nmachine learning algorithms with almost no loss of accuracy. The proposed\nframework relies on projections via structured matrices that we call Structured\nSpinners, which are formed as products of three structured matrix-blocks that\nincorporate rotations. The approach is highly generic, i.e. i) structured\nmatrices under consideration can either be fully-randomized or learned, ii) our\nstructured family contains as special cases all previously considered\nstructured schemes, iii) the setting extends to the non-linear case where the\nprojections are followed by non-linear functions, and iv) the method finds\nnumerous applications including kernel approximations via random feature maps,\ndimensionality reduction algorithms, new fast cross-polytope LSH techniques,\ndeep learning, convex optimization algorithms via Newton sketches, quantization\nwith random projection trees, and more. The proposed framework comes with\ntheoretical guarantees characterizing the capacity of the structured model in\nreference to its unstructured counterpart and is based on a general theoretical\nprinciple that we describe in the paper. As a consequence of our theoretical\nanalysis, we provide the first theoretical guarantees for one of the most\nefficient existing LSH algorithms based on the HD3HD2HD1 structured matrix\n[Andoni et al., 2015]. The exhaustive experimental evaluation confirms the\naccuracy and efficiency of structured spinners for a variety of different\napplications. \n\n"}
{"id": "1610.06920", "contents": "Title: Bit-pragmatic Deep Neural Network Computing Abstract: We quantify a source of ineffectual computations when processing the\nmultiplications of the convolutional layers in Deep Neural Networks (DNNs) and\npropose Pragmatic (PRA), an architecture that exploits it improving performance\nand energy efficiency. The source of these ineffectual computations is best\nunderstood in the context of conventional multipliers which generate internally\nmultiple terms, that is, products of the multiplicand and powers of two, which\nadded together produce the final product [1]. At runtime, many of these terms\nare zero as they are generated when the multiplicand is combined with the\nzero-bits of the multiplicator. While conventional bit-parallel multipliers\ncalculate all terms in parallel to reduce individual product latency, PRA\ncalculates only the non-zero terms using a) on-the-fly conversion of the\nmultiplicator representation into an explicit list of powers of two, and b)\nhybrid bit-parallel multplicand/bit-serial multiplicator processing units. PRA\nexploits two sources of ineffectual computations: 1) the aforementioned zero\nproduct terms which are the result of the lack of explicitness in the\nmultiplicator representation, and 2) the excess in the representation precision\nused for both multiplicants and multiplicators, e.g., [2]. Measurements\ndemonstrate that for the convolutional layers, a straightforward variant of PRA\nimproves performance by 2.6x over the DaDiaNao (DaDN) accelerator [3] and by\n1.4x over STR [4]. Similarly, PRA improves energy efficiency by 28% and 10% on\naverage compared to DaDN and STR. An improved cross lane synchronication scheme\nboosts performance improvements to 3.1x over DaDN. Finally, Pragmatic benefits\npersist even with an 8-bit quantized representation [5]. \n\n"}
{"id": "1610.08611", "contents": "Title: Causal Network Learning from Multiple Interventions of Unknown\n  Manipulated Targets Abstract: In this paper, we discuss structure learning of causal networks from multiple\ndata sets obtained by external intervention experiments where we do not know\nwhat variables are manipulated. For example, the conditions in these\nexperiments are changed by changing temperature or using drugs, but we do not\nknow what target variables are manipulated by the external interventions. From\nsuch data sets, the structure learning becomes more difficult. For this case,\nwe first discuss the identifiability of causal structures. Next we present a\ngraph-merging method for learning causal networks for the case that the sample\nsizes are large for these interventions. Then for the case that the sample\nsizes of these interventions are relatively small, we propose a data-pooling\nmethod for learning causal networks in which we pool all data sets of these\ninterventions together for the learning. Further we propose a re-sampling\napproach to evaluate the edges of the causal network learned by the\ndata-pooling method. Finally we illustrate the proposed learning methods by\nsimulations. \n\n"}
{"id": "1610.08749", "contents": "Title: Differentially Private Variational Inference for Non-conjugate Models Abstract: Many machine learning applications are based on data collected from people,\nsuch as their tastes and behaviour as well as biological traits and genetic\ndata. Regardless of how important the application might be, one has to make\nsure individuals' identities or the privacy of the data are not compromised in\nthe analysis. Differential privacy constitutes a powerful framework that\nprevents breaching of data subject privacy from the output of a computation.\nDifferentially private versions of many important Bayesian inference methods\nhave been proposed, but there is a lack of an efficient unified approach\napplicable to arbitrary models. In this contribution, we propose a\ndifferentially private variational inference method with a very wide\napplicability. It is built on top of doubly stochastic variational inference, a\nrecent advance which provides a variational solution to a large class of\nmodels. We add differential privacy into doubly stochastic variational\ninference by clipping and perturbing the gradients. The algorithm is made more\nefficient through privacy amplification from subsampling. We demonstrate the\nmethod can reach an accuracy close to non-private level under reasonably strong\nprivacy guarantees, clearly improving over previous sampling-based alternatives\nespecially in the strong privacy regime. \n\n"}
{"id": "1610.09072", "contents": "Title: Orthogonal Random Features Abstract: We present an intriguing discovery related to Random Fourier Features: in\nGaussian kernel approximation, replacing the random Gaussian matrix by a\nproperly scaled random orthogonal matrix significantly decreases kernel\napproximation error. We call this technique Orthogonal Random Features (ORF),\nand provide theoretical and empirical justification for this behavior.\nMotivated by this discovery, we further propose Structured Orthogonal Random\nFeatures (SORF), which uses a class of structured discrete orthogonal matrices\nto speed up the computation. The method reduces the time cost from\n$\\mathcal{O}(d^2)$ to $\\mathcal{O}(d \\log d)$, where $d$ is the data\ndimensionality, with almost no compromise in kernel approximation quality\ncompared to ORF. Experiments on several datasets verify the effectiveness of\nORF and SORF over the existing methods. We also provide discussions on using\nthe same type of discrete orthogonal structure for a broader range of\napplications. \n\n"}
{"id": "1610.09201", "contents": "Title: A Conceptual Development of Quench Prediction App build on LSTM and ELQA\n  framework Abstract: This article presents a development of web application for quench prediction\nin \\gls{te-mpe-ee} at CERN. The authors describe an ELectrical Quality\nAssurance (ELQA) framework, a platform which was designed for rapid development\nof web integrated data analysis applications for different analysis needed\nduring the hardware commissioning of the Large Hadron Collider (LHC). In second\npart the article describes a research carried out with the data collected from\nQuench Detection System by means of using an LSTM recurrent neural network. The\narticle discusses and presents a conceptual work of implementing quench\nprediction application for \\gls{te-mpe-ee} based on the ELQA and quench\nprediction algorithm. \n\n"}
{"id": "1611.00255", "contents": "Title: Stationary time-vertex signal processing Abstract: This paper considers regression tasks involving high-dimensional multivariate\nprocesses whose structure is dependent on some {known} graph topology. We put\nforth a new definition of time-vertex wide-sense stationarity, or joint\nstationarity for short, that goes beyond product graphs. Joint stationarity\nhelps by reducing the estimation variance and recovery complexity. In\nparticular, for any jointly stationary process (a) one reliably learns the\ncovariance structure from as little as a single realization of the process, and\n(b) solves MMSE recovery problems, such as interpolation and denoising, in\ncomputational time nearly linear on the number of edges and timesteps.\nExperiments with three datasets suggest that joint stationarity can yield\naccuracy improvements in the recovery of high-dimensional processes evolving\nover a graph, even when the latter is only approximately known, or the process\nis not strictly stationary. \n\n"}
{"id": "1611.01190", "contents": "Title: Conspiracies between Learning Algorithms, Circuit Lower Bounds and\n  Pseudorandomness Abstract: We prove several results giving new and stronger connections between\nlearning, circuit lower bounds and pseudorandomness. Among other results, we\nshow a generic learning speedup lemma, equivalences between various learning\nmodels in the exponential time and subexponential time regimes, a dichotomy\nbetween learning and pseudorandomness, consequences of non-trivial learning for\ncircuit lower bounds, Karp-Lipton theorems for probabilistic exponential time,\nand NC$^1$-hardness for the Minimum Circuit Size Problem. \n\n"}
{"id": "1611.02683", "contents": "Title: Unsupervised Pretraining for Sequence to Sequence Learning Abstract: This work presents a general unsupervised learning method to improve the\naccuracy of sequence to sequence (seq2seq) models. In our method, the weights\nof the encoder and decoder of a seq2seq model are initialized with the\npretrained weights of two language models and then fine-tuned with labeled\ndata. We apply this method to challenging benchmarks in machine translation and\nabstractive summarization and find that it significantly improves the\nsubsequent supervised models. Our main result is that pretraining improves the\ngeneralization of seq2seq models. We achieve state-of-the art results on the\nWMT English$\\rightarrow$German task, surpassing a range of methods using both\nphrase-based machine translation and neural machine translation. Our method\nachieves a significant improvement of 1.3 BLEU from the previous best models on\nboth WMT'14 and WMT'15 English$\\rightarrow$German. We also conduct human\nevaluations on abstractive summarization and find that our method outperforms a\npurely supervised learning baseline in a statistically significant manner. \n\n"}
{"id": "1611.02960", "contents": "Title: A Unified Maximum Likelihood Approach for Optimal Distribution Property\n  Estimation Abstract: The advent of data science has spurred interest in estimating properties of\ndistributions over large alphabets. Fundamental symmetric properties such as\nsupport size, support coverage, entropy, and proximity to uniformity, received\nmost attention, with each property estimated using a different technique and\noften intricate analysis tools.\n  We prove that for all these properties, a single, simple, plug-in\nestimator---profile maximum likelihood (PML)---performs as well as the best\nspecialized techniques. This raises the possibility that PML may optimally\nestimate many other symmetric properties. \n\n"}
{"id": "1611.03218", "contents": "Title: Learning to Play Guess Who? and Inventing a Grounded Language as a\n  Consequence Abstract: Acquiring your first language is an incredible feat and not easily\nduplicated. Learning to communicate using nothing but a few pictureless books,\na corpus, would likely be impossible even for humans. Nevertheless, this is the\ndominating approach in most natural language processing today. As an\nalternative, we propose the use of situated interactions between agents as a\ndriving force for communication, and the framework of Deep Recurrent Q-Networks\nfor evolving a shared language grounded in the provided environment. We task\nthe agents with interactive image search in the form of the game Guess Who?.\nThe images from the game provide a non trivial environment for the agents to\ndiscuss and a natural grounding for the concepts they decide to encode in their\ncommunication. Our experiments show that the agents learn not only to encode\nphysical concepts in their words, i.e. grounding, but also that the agents\nlearn to hold a multi-step dialogue remembering the state of the dialogue from\nstep to step. \n\n"}
{"id": "1611.03530", "contents": "Title: Understanding deep learning requires rethinking generalization Abstract: Despite their massive size, successful deep artificial neural networks can\nexhibit a remarkably small difference between training and test performance.\nConventional wisdom attributes small generalization error either to properties\nof the model family, or to the regularization techniques used during training.\n  Through extensive systematic experiments, we show how these traditional\napproaches fail to explain why large neural networks generalize well in\npractice. Specifically, our experiments establish that state-of-the-art\nconvolutional networks for image classification trained with stochastic\ngradient methods easily fit a random labeling of the training data. This\nphenomenon is qualitatively unaffected by explicit regularization, and occurs\neven if we replace the true images by completely unstructured random noise. We\ncorroborate these experimental findings with a theoretical construction showing\nthat simple depth two neural networks already have perfect finite sample\nexpressivity as soon as the number of parameters exceeds the number of data\npoints as it usually does in practice.\n  We interpret our experimental findings by comparison with traditional models. \n\n"}
{"id": "1611.03579", "contents": "Title: Collision-based Testers are Optimal for Uniformity and Closeness Abstract: We study the fundamental problems of (i) uniformity testing of a discrete\ndistribution, and (ii) closeness testing between two discrete distributions\nwith bounded $\\ell_2$-norm. These problems have been extensively studied in\ndistribution testing and sample-optimal estimators are known for\nthem~\\cite{Paninski:08, CDVV14, VV14, DKN:15}.\n  In this work, we show that the original collision-based testers proposed for\nthese problems ~\\cite{GRdist:00, BFR+:00} are sample-optimal, up to constant\nfactors. Previous analyses showed sample complexity upper bounds for these\ntesters that are optimal as a function of the domain size $n$, but suboptimal\nby polynomial factors in the error parameter $\\epsilon$. Our main contribution\nis a new tight analysis establishing that these collision-based testers are\ninformation-theoretically optimal, up to constant factors, both in the\ndependence on $n$ and in the dependence on $\\epsilon$. \n\n"}
{"id": "1611.04578", "contents": "Title: Earliness-Aware Deep Convolutional Networks for Early Time Series\n  Classification Abstract: We present Earliness-Aware Deep Convolutional Networks (EA-ConvNets), an\nend-to-end deep learning framework, for early classification of time series\ndata. Unlike most existing methods for early classification of time series\ndata, that are designed to solve this problem under the assumption of the\navailability of a good set of pre-defined (often hand-crafted) features, our\nframework can jointly perform feature learning (by learning a deep hierarchy of\n\\emph{shapelets} capturing the salient characteristics in each time series),\nalong with a dynamic truncation model to help our deep feature learning\narchitecture focus on the early parts of each time series. Consequently, our\nframework is able to make highly reliable early predictions, outperforming\nvarious state-of-the-art methods for early time series classification, while\nalso being competitive when compared to the state-of-the-art time series\nclassification algorithms that work with \\emph{fully observed} time series\ndata. To the best of our knowledge, the proposed framework is the first to\nperform data-driven (deep) feature learning in the context of early\nclassification of time series data. We perform a comprehensive set of\nexperiments, on several benchmark data sets, which demonstrate that our method\nyields significantly better predictions than various state-of-the-art methods\ndesigned for early time series classification. In addition to obtaining high\naccuracies, our experiments also show that the learned deep shapelets based\nfeatures are also highly interpretable and can help gain better understanding\nof the underlying characteristics of time series data. \n\n"}
{"id": "1611.04967", "contents": "Title: Iterative Orthogonal Feature Projection for Diagnosing Bias in Black-Box\n  Models Abstract: Predictive models are increasingly deployed for the purpose of determining\naccess to services such as credit, insurance, and employment. Despite potential\ngains in productivity and efficiency, several potential problems have yet to be\naddressed, particularly the potential for unintentional discrimination. We\npresent an iterative procedure, based on orthogonal projection of input\nattributes, for enabling interpretability of black-box predictive models.\nThrough our iterative procedure, one can quantify the relative dependence of a\nblack-box model on its input attributes.The relative significance of the inputs\nto a predictive model can then be used to assess the fairness (or\ndiscriminatory extent) of such a model. \n\n"}
{"id": "1611.05475", "contents": "Title: The Bayesian Formulation and Well-Posedness of Fractional Elliptic\n  Inverse Problems Abstract: We study the inverse problem of recovering the order and the diffusion\ncoefficient of an elliptic fractional partial differential equation from a\nfinite number of noisy observations of the solution. We work in a Bayesian\nframework and show conditions under which the posterior distribution is given\nby a change of measure from the prior. Moreover, we show well-posedness of the\ninverse problem, in the sense that small perturbations of the observed solution\nlead to small Hellinger perturbations of the associated posterior measures. We\nthus provide a mathematical foundation to the Bayesian learning of the order\n---and other inputs--- of fractional models. \n\n"}
{"id": "1611.05990", "contents": "Title: Monte Carlo Tableau Proof Search Abstract: We study Monte Carlo Tree Search to guide proof search in tableau calculi.\nThis includes proposing a number of proof-state evaluation heuristics, some of\nwhich are learnt from previous proofs. We present an implementation based on\nthe leanCoP prover. The system is trained and evaluated on a large suite of\nrelated problems coming from the Mizar proof assistant, showing that it is\ncapable to find new and different proofs. \n\n"}
{"id": "1611.07476", "contents": "Title: Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond Abstract: We look at the eigenvalues of the Hessian of a loss function before and after\ntraining. The eigenvalue distribution is seen to be composed of two parts, the\nbulk which is concentrated around zero, and the edges which are scattered away\nfrom zero. We present empirical evidence for the bulk indicating how\nover-parametrized the system is, and for the edges that depend on the input\ndata. \n\n"}
{"id": "1611.07492", "contents": "Title: Inducing Interpretable Representations with Variational Autoencoders Abstract: We develop a framework for incorporating structured graphical models in the\n\\emph{encoders} of variational autoencoders (VAEs) that allows us to induce\ninterpretable representations through approximate variational inference. This\nallows us to both perform reasoning (e.g. classification) under the structural\nconstraints of a given graphical model, and use deep generative models to deal\nwith messy, high-dimensional domains where it is often difficult to model all\nthe variation. Learning in this framework is carried out end-to-end with a\nvariational objective, applying to both unsupervised and semi-supervised\nschemes. \n\n"}
{"id": "1611.07507", "contents": "Title: Variational Intrinsic Control Abstract: In this paper we introduce a new unsupervised reinforcement learning method\nfor discovering the set of intrinsic options available to an agent. This set is\nlearned by maximizing the number of different states an agent can reliably\nreach, as measured by the mutual information between the set of options and\noption termination states. To this end, we instantiate two policy gradient\nbased algorithms, one that creates an explicit embedding space of options and\none that represents options implicitly. The algorithms also provide an explicit\nmeasure of empowerment in a given state that can be used by an empowerment\nmaximizing agent. The algorithm scales well with function approximation and we\ndemonstrate the applicability of the algorithm on a range of tasks. \n\n"}
{"id": "1611.10258", "contents": "Title: Reliably Learning the ReLU in Polynomial Time Abstract: We give the first dimension-efficient algorithms for learning Rectified\nLinear Units (ReLUs), which are functions of the form $\\mathbf{x} \\mapsto\n\\max(0, \\mathbf{w} \\cdot \\mathbf{x})$ with $\\mathbf{w} \\in \\mathbb{S}^{n-1}$.\nOur algorithm works in the challenging Reliable Agnostic learning model of\nKalai, Kanade, and Mansour (2009) where the learner is given access to a\ndistribution $\\cal{D}$ on labeled examples but the labeling may be arbitrary.\nWe construct a hypothesis that simultaneously minimizes the false-positive rate\nand the loss on inputs given positive labels by $\\cal{D}$, for any convex,\nbounded, and Lipschitz loss function.\n  The algorithm runs in polynomial-time (in $n$) with respect to any\ndistribution on $\\mathbb{S}^{n-1}$ (the unit sphere in $n$ dimensions) and for\nany error parameter $\\epsilon = \\Omega(1/\\log n)$ (this yields a PTAS for a\nquestion raised by F. Bach on the complexity of maximizing ReLUs). These\nresults are in contrast to known efficient algorithms for reliably learning\nlinear threshold functions, where $\\epsilon$ must be $\\Omega(1)$ and strong\nassumptions are required on the marginal distribution. We can compose our\nresults to obtain the first set of efficient algorithms for learning\nconstant-depth networks of ReLUs.\n  Our techniques combine kernel methods and polynomial approximations with a\n\"dual-loss\" approach to convex programming. As a byproduct we obtain a number\nof applications including the first set of efficient algorithms for \"convex\npiecewise-linear fitting\" and the first efficient algorithms for noisy\npolynomial reconstruction of low-weight polynomials on the unit sphere. \n\n"}
{"id": "1612.00835", "contents": "Title: Scribbler: Controlling Deep Image Synthesis with Sketch and Color Abstract: Recently, there have been several promising methods to generate realistic\nimagery from deep convolutional networks. These methods sidestep the\ntraditional computer graphics rendering pipeline and instead generate imagery\nat the pixel level by learning from large collections of photos (e.g. faces or\nbedrooms). However, these methods are of limited utility because it is\ndifficult for a user to control what the network produces. In this paper, we\npropose a deep adversarial image synthesis architecture that is conditioned on\nsketched boundaries and sparse color strokes to generate realistic cars,\nbedrooms, or faces. We demonstrate a sketch based image synthesis system which\nallows users to 'scribble' over the sketch to indicate preferred color for\nobjects. Our network can then generate convincing images that satisfy both the\ncolor and the sketch constraints of user. The network is feed-forward which\nallows users to see the effect of their edits in real time. We compare to\nrecent work on sketch to image synthesis and show that our approach can\ngenerate more realistic, more diverse, and more controllable outputs. The\narchitecture is also effective at user-guided colorization of grayscale images. \n\n"}
{"id": "1612.01197", "contents": "Title: Neural Symbolic Machines: Learning Semantic Parsers on Freebase with\n  Weak Supervision (Short Version) Abstract: Extending the success of deep neural networks to natural language\nunderstanding and symbolic reasoning requires complex operations and external\nmemory. Recent neural program induction approaches have attempted to address\nthis problem, but are typically limited to differentiable memory, and\nconsequently cannot scale beyond small synthetic tasks. In this work, we\npropose the Manager-Programmer-Computer framework, which integrates neural\nnetworks with non-differentiable memory to support abstract, scalable and\nprecise operations through a friendly neural computer interface. Specifically,\nwe introduce a Neural Symbolic Machine, which contains a sequence-to-sequence\nneural \"programmer\", and a non-differentiable \"computer\" that is a Lisp\ninterpreter with code assist. To successfully apply REINFORCE for training, we\naugment it with approximate gold programs found by an iterative maximum\nlikelihood training process. NSM is able to learn a semantic parser from weak\nsupervision over a large knowledge base. It achieves new state-of-the-art\nperformance on WebQuestionsSP, a challenging semantic parsing dataset, with\nweak supervision. Compared to previous approaches, NSM is end-to-end, therefore\ndoes not rely on feature engineering or domain specific knowledge. \n\n"}
{"id": "1612.01367", "contents": "Title: An Asymptotically Optimal Contextual Bandit Algorithm Using Hierarchical\n  Structures Abstract: We propose online algorithms for sequential learning in the contextual\nmulti-armed bandit setting. Our approach is to partition the context space and\nthen optimally combine all of the possible mappings between the partition\nregions and the set of bandit arms in a data driven manner. We show that in our\napproach, the best mapping is able to approximate the best arm selection policy\nto any desired degree under mild Lipschitz conditions. Therefore, we design our\nalgorithms based on the optimal adaptive combination and asymptotically achieve\nthe performance of the best mapping as well as the best arm selection policy.\nThis optimality is also guaranteed to hold even in adversarial environments\nsince we do not rely on any statistical assumptions regarding the contexts or\nthe loss of the bandit arms. Moreover, we design efficient implementations for\nour algorithms in various hierarchical partitioning structures such as\nlexicographical or arbitrary position splitting and binary trees (and several\nother partitioning examples). For instance, in the case of binary tree\npartitioning, the computational complexity is only log-linear in the number of\nregions in the finest partition. In conclusion, we provide significant\nperformance improvements by introducing upper bounds (w.r.t. the best arm\nselection policy) that are mathematically proven to vanish in the average loss\nper round sense at a faster rate compared to the state-of-the-art. Our\nexperimental work extensively covers various scenarios ranging from bandit\nsettings to multi-class classification with real and synthetic data. In these\nexperiments, we show that our algorithms are highly superior over the\nstate-of-the-art techniques while maintaining the introduced mathematical\nguarantees and a computationally decent scalability. \n\n"}
{"id": "1612.01597", "contents": "Title: Deterministic and Probabilistic Conditions for Finite Completability of\n  Low-Tucker-Rank Tensor Abstract: We investigate the fundamental conditions on the sampling pattern, i.e.,\nlocations of the sampled entries, for finite completability of a low-rank\ntensor given some components of its Tucker rank. In order to find the\ndeterministic necessary and sufficient conditions, we propose an algebraic\ngeometric analysis on the Tucker manifold, which allows us to incorporate\nmultiple rank components in the proposed analysis in contrast with the\nconventional geometric approaches on the Grassmannian manifold. This analysis\ncharacterizes the algebraic independence of a set of polynomials defined based\non the sampling pattern, which is closely related to finite completion.\nProbabilistic conditions are then studied and a lower bound on the sampling\nprobability is given, which guarantees that the proposed deterministic\nconditions on the sampling patterns for finite completability hold with high\nprobability. Furthermore, using the proposed geometric approach for finite\ncompletability, we propose a sufficient condition on the sampling pattern that\nensures there exists exactly one completion for the sampled tensor. \n\n"}
{"id": "1612.01936", "contents": "Title: A Probabilistic Framework for Deep Learning Abstract: We develop a probabilistic framework for deep learning based on the Deep\nRendering Mixture Model (DRMM), a new generative probabilistic model that\nexplicitly capture variations in data due to latent task nuisance variables. We\ndemonstrate that max-sum inference in the DRMM yields an algorithm that exactly\nreproduces the operations in deep convolutional neural networks (DCNs),\nproviding a first principles derivation. Our framework provides new insights\ninto the successes and shortcomings of DCNs as well as a principled route to\ntheir improvement. DRMM training via the Expectation-Maximization (EM)\nalgorithm is a powerful alternative to DCN back-propagation, and initial\ntraining results are promising. Classification based on the DRMM and other\nvariants outperforms DCNs in supervised digit classification, training 2-3x\nfaster while achieving similar accuracy. Moreover, the DRMM is applicable to\nsemi-supervised and unsupervised learning tasks, achieving results that are\nstate-of-the-art in several categories on the MNIST benchmark and comparable to\nstate of the art on the CIFAR10 benchmark. \n\n"}
{"id": "1612.02222", "contents": "Title: A Communication-Efficient Parallel Method for Group-Lasso Abstract: Group-Lasso (gLasso) identifies important explanatory factors in predicting\nthe response variable by considering the grouping structure over input\nvariables. However, most existing algorithms for gLasso are not scalable to\ndeal with large-scale datasets, which are becoming a norm in many applications.\nIn this paper, we present a divide-and-conquer based parallel algorithm\n(DC-gLasso) to scale up gLasso in the tasks of regression with grouping\nstructures. DC-gLasso only needs two iterations to collect and aggregate the\nlocal estimates on subsets of the data, and is provably correct to recover the\ntrue model under certain conditions. We further extend it to deal with\noverlappings between groups. Empirical results on a wide range of synthetic and\nreal-world datasets show that DC-gLasso can significantly improve the time\nefficiency without sacrificing regression accuracy. \n\n"}
{"id": "1612.03079", "contents": "Title: Clipper: A Low-Latency Online Prediction Serving System Abstract: Machine learning is being deployed in a growing number of applications which\ndemand real-time, accurate, and robust predictions under heavy query load.\nHowever, most machine learning frameworks and systems only address model\ntraining and not deployment.\n  In this paper, we introduce Clipper, a general-purpose low-latency prediction\nserving system. Interposing between end-user applications and a wide range of\nmachine learning frameworks, Clipper introduces a modular architecture to\nsimplify model deployment across frameworks and applications. Furthermore, by\nintroducing caching, batching, and adaptive model selection techniques, Clipper\nreduces prediction latency and improves prediction throughput, accuracy, and\nrobustness without modifying the underlying machine learning frameworks. We\nevaluate Clipper on four common machine learning benchmark datasets and\ndemonstrate its ability to meet the latency, accuracy, and throughput demands\nof online serving applications. Finally, we compare Clipper to the TensorFlow\nServing system and demonstrate that we are able to achieve comparable\nthroughput and latency while enabling model composition and online learning to\nimprove accuracy and render more robust predictions. \n\n"}
{"id": "1612.04262", "contents": "Title: An equation-of-state-meter of QCD transition from deep learning Abstract: Supervised learning with a deep convolutional neural network is used to\nidentify the QCD equation of state (EoS) employed in relativistic hydrodynamic\nsimulations of heavy-ion collisions from the simulated final-state particle\nspectra $\\rho(p_T,\\Phi)$. High-level correlations of $\\rho(p_T,\\Phi)$ learned\nby the neural network act as an effective \"EoS-meter\" in detecting the nature\nof the QCD transition. The EoS-meter is model independent and insensitive to\nother simulation inputs, especially the initial conditions. Thus it provides a\npowerful direct-connection of heavy-ion collision observables with the bulk\nproperties of QCD. \n\n"}
{"id": "1612.05086", "contents": "Title: Coupling Adaptive Batch Sizes with Learning Rates Abstract: Mini-batch stochastic gradient descent and variants thereof have become\nstandard for large-scale empirical risk minimization like the training of\nneural networks. These methods are usually used with a constant batch size\nchosen by simple empirical inspection. The batch size significantly influences\nthe behavior of the stochastic optimization algorithm, though, since it\ndetermines the variance of the gradient estimates. This variance also changes\nover the optimization process; when using a constant batch size, stability and\nconvergence is thus often enforced by means of a (manually tuned) decreasing\nlearning rate schedule.\n  We propose a practical method for dynamic batch size adaptation. It estimates\nthe variance of the stochastic gradients and adapts the batch size to decrease\nthe variance proportionally to the value of the objective function, removing\nthe need for the aforementioned learning rate decrease. In contrast to recent\nrelated work, our algorithm couples the batch size to the learning rate,\ndirectly reflecting the known relationship between the two. On popular image\nclassification benchmarks, our batch size adaptation yields faster optimization\nconvergence, while simultaneously simplifying learning rate tuning. A\nTensorFlow implementation is available. \n\n"}
{"id": "1612.05974", "contents": "Title: An IoT Endpoint System-on-Chip for Secure and Energy-Efficient\n  Near-Sensor Analytics Abstract: Near-sensor data analytics is a promising direction for IoT endpoints, as it\nminimizes energy spent on communication and reduces network load - but it also\nposes security concerns, as valuable data is stored or sent over the network at\nvarious stages of the analytics pipeline. Using encryption to protect sensitive\ndata at the boundary of the on-chip analytics engine is a way to address data\nsecurity issues. To cope with the combined workload of analytics and encryption\nin a tight power envelope, we propose Fulmine, a System-on-Chip based on a\ntightly-coupled multi-core cluster augmented with specialized blocks for\ncompute-intensive data processing and encryption functions, supporting software\nprogrammability for regular computing tasks. The Fulmine SoC, fabricated in\n65nm technology, consumes less than 20mW on average at 0.8V achieving an\nefficiency of up to 70pJ/B in encryption, 50pJ/px in convolution, or up to\n25MIPS/mW in software. As a strong argument for real-life flexible application\nof our platform, we show experimental results for three secure analytics use\ncases: secure autonomous aerial surveillance with a state-of-the-art deep CNN\nconsuming 3.16pJ per equivalent RISC op; local CNN-based face detection with\nsecured remote recognition in 5.74pJ/op; and seizure detection with encrypted\ndata collection from EEG within 12.7pJ/op. \n\n"}
{"id": "1701.01722", "contents": "Title: Follow the Compressed Leader: Faster Online Learning of Eigenvectors and\n  Faster MMWU Abstract: The online problem of computing the top eigenvector is fundamental to machine\nlearning. In both adversarial and stochastic settings, previous results (such\nas matrix multiplicative weight update, follow the regularized leader, follow\nthe compressed leader, block power method) either achieve optimal regret but\nrun slow, or run fast at the expense of loosing a $\\sqrt{d}$ factor in total\nregret where $d$ is the matrix dimension.\n  We propose a $\\textit{follow-the-compressed-leader (FTCL)}$ framework which\nachieves optimal regret without sacrificing the running time. Our idea is to\n\"compress\" the matrix strategy to dimension 3 in the adversarial setting, or\ndimension 1 in the stochastic setting. These respectively resolve two open\nquestions regarding the design of optimal and efficient algorithms for the\nonline eigenvector problem. \n\n"}
{"id": "1701.03577", "contents": "Title: Kernel Approximation Methods for Speech Recognition Abstract: We study large-scale kernel methods for acoustic modeling in speech\nrecognition and compare their performance to deep neural networks (DNNs). We\nperform experiments on four speech recognition datasets, including the TIMIT\nand Broadcast News benchmark tasks, and compare these two types of models on\nframe-level performance metrics (accuracy, cross-entropy), as well as on\nrecognition metrics (word/character error rate). In order to scale kernel\nmethods to these large datasets, we use the random Fourier feature method of\nRahimi and Recht (2007). We propose two novel techniques for improving the\nperformance of kernel acoustic models. First, in order to reduce the number of\nrandom features required by kernel models, we propose a simple but effective\nmethod for feature selection. The method is able to explore a large number of\nnon-linear features while maintaining a compact model more efficiently than\nexisting approaches. Second, we present a number of frame-level metrics which\ncorrelate very strongly with recognition performance when computed on the\nheldout set; we take advantage of these correlations by monitoring these\nmetrics during training in order to decide when to stop learning. This\ntechnique can noticeably improve the recognition performance of both DNN and\nkernel models, while narrowing the gap between them. Additionally, we show that\nthe linear bottleneck method of Sainath et al. (2013) improves the performance\nof our kernel models significantly, in addition to speeding up training and\nmaking the models more compact. Together, these three methods dramatically\nimprove the performance of kernel acoustic models, making their performance\ncomparable to DNNs on the tasks we explored. \n\n"}
{"id": "1701.04099", "contents": "Title: Field-aware Factorization Machines in a Real-world Online Advertising\n  System Abstract: Predicting user response is one of the core machine learning tasks in\ncomputational advertising. Field-aware Factorization Machines (FFM) have\nrecently been established as a state-of-the-art method for that problem and in\nparticular won two Kaggle challenges. This paper presents some results from\nimplementing this method in a production system that predicts click-through and\nconversion rates for display advertising and shows that this method it is not\nonly effective to win challenges but is also valuable in a real-world\nprediction system. We also discuss some specific challenges and solutions to\nreduce the training time, namely the use of an innovative seeding algorithm and\na distributed learning mechanism. \n\n"}
{"id": "1701.04113", "contents": "Title: Near Optimal Behavior via Approximate State Abstraction Abstract: The combinatorial explosion that plagues planning and reinforcement learning\n(RL) algorithms can be moderated using state abstraction. Prohibitively large\ntask representations can be condensed such that essential information is\npreserved, and consequently, solutions are tractably computable. However, exact\nabstractions, which treat only fully-identical situations as equivalent, fail\nto present opportunities for abstraction in environments where no two\nsituations are exactly alike. In this work, we investigate approximate state\nabstractions, which treat nearly-identical situations as equivalent. We present\ntheoretical guarantees of the quality of behaviors derived from four types of\napproximate abstractions. Additionally, we empirically demonstrate that\napproximate abstractions lead to reduction in task complexity and bounded loss\nof optimality of behavior in a variety of environments. \n\n"}
{"id": "1701.04722", "contents": "Title: Adversarial Variational Bayes: Unifying Variational Autoencoders and\n  Generative Adversarial Networks Abstract: Variational Autoencoders (VAEs) are expressive latent variable models that\ncan be used to learn complex probability distributions from training data.\nHowever, the quality of the resulting model crucially relies on the\nexpressiveness of the inference model. We introduce Adversarial Variational\nBayes (AVB), a technique for training Variational Autoencoders with arbitrarily\nexpressive inference models. We achieve this by introducing an auxiliary\ndiscriminative network that allows to rephrase the maximum-likelihood-problem\nas a two-player game, hence establishing a principled connection between VAEs\nand Generative Adversarial Networks (GANs). We show that in the nonparametric\nlimit our method yields an exact maximum-likelihood assignment for the\nparameters of the generative model, as well as the exact posterior distribution\nover the latent variables given an observation. Contrary to competing\napproaches which combine VAEs with GANs, our approach has a clear theoretical\njustification, retains most advantages of standard Variational Autoencoders and\nis easy to implement. \n\n"}
{"id": "1701.04722", "contents": "Title: Adversarial Variational Bayes: Unifying Variational Autoencoders and\n  Generative Adversarial Networks Abstract: Variational Autoencoders (VAEs) are expressive latent variable models that\ncan be used to learn complex probability distributions from training data.\nHowever, the quality of the resulting model crucially relies on the\nexpressiveness of the inference model. We introduce Adversarial Variational\nBayes (AVB), a technique for training Variational Autoencoders with arbitrarily\nexpressive inference models. We achieve this by introducing an auxiliary\ndiscriminative network that allows to rephrase the maximum-likelihood-problem\nas a two-player game, hence establishing a principled connection between VAEs\nand Generative Adversarial Networks (GANs). We show that in the nonparametric\nlimit our method yields an exact maximum-likelihood assignment for the\nparameters of the generative model, as well as the exact posterior distribution\nover the latent variables given an observation. Contrary to competing\napproaches which combine VAEs with GANs, our approach has a clear theoretical\njustification, retains most advantages of standard Variational Autoencoders and\nis easy to implement. \n\n"}
{"id": "1701.04862", "contents": "Title: Towards Principled Methods for Training Generative Adversarial Networks Abstract: The goal of this paper is not to introduce a single algorithm or method, but\nto make theoretical steps towards fully understanding the training dynamics of\ngenerative adversarial networks. In order to substantiate our theoretical\nanalysis, we perform targeted experiments to verify our assumptions, illustrate\nour claims, and quantify the phenomena. This paper is divided into three\nsections. The first section introduces the problem at hand. The second section\nis dedicated to studying and proving rigorously the problems including\ninstability and saturation that arize when training generative adversarial\nnetworks. The third section examines a practical and theoretically grounded\ndirection towards solving these problems, while introducing new tools to study\nthem. \n\n"}
{"id": "1701.07179", "contents": "Title: Malicious URL Detection using Machine Learning: A Survey Abstract: Malicious URL, a.k.a. malicious website, is a common and serious threat to\ncybersecurity. Malicious URLs host unsolicited content (spam, phishing,\ndrive-by exploits, etc.) and lure unsuspecting users to become victims of scams\n(monetary loss, theft of private information, and malware installation), and\ncause losses of billions of dollars every year. It is imperative to detect and\nact on such threats in a timely manner. Traditionally, this detection is done\nmostly through the usage of blacklists. However, blacklists cannot be\nexhaustive, and lack the ability to detect newly generated malicious URLs. To\nimprove the generality of malicious URL detectors, machine learning techniques\nhave been explored with increasing attention in recent years. This article aims\nto provide a comprehensive survey and a structural understanding of Malicious\nURL Detection techniques using machine learning. We present the formal\nformulation of Malicious URL Detection as a machine learning task, and\ncategorize and review the contributions of literature studies that addresses\ndifferent dimensions of this problem (feature representation, algorithm design,\netc.). Further, this article provides a timely and comprehensive survey for a\nrange of different audiences, not only for machine learning researchers and\nengineers in academia, but also for professionals and practitioners in\ncybersecurity industry, to help them understand the state of the art and\nfacilitate their own research and practical applications. We also discuss\npractical issues in system design, open research challenges, and point out some\nimportant directions for future research. \n\n"}
{"id": "1701.07767", "contents": "Title: Riemannian-geometry-based modeling and clustering of network-wide\n  non-stationary time series: The brain-network case Abstract: This paper advocates Riemannian multi-manifold modeling in the context of\nnetwork-wide non-stationary time-series analysis. Time-series data, collected\nsequentially over time and across a network, yield features which are viewed as\npoints in or close to a union of multiple submanifolds of a Riemannian\nmanifold, and distinguishing disparate time series amounts to clustering\nmultiple Riemannian submanifolds. To support the claim that exploiting the\nlatent Riemannian geometry behind many statistical features of time series is\nbeneficial to learning from network data, this paper focuses on brain networks\nand puts forth two feature-generation schemes for network-wide dynamic time\nseries. The first is motivated by Granger-causality arguments and uses an\nauto-regressive moving average model to map low-rank linear vector subspaces,\nspanned by column vectors of appropriately defined observability matrices, to\npoints into the Grassmann manifold. The second utilizes (non-linear)\ndependencies among network nodes by introducing kernel-based partial\ncorrelations to generate points in the manifold of positive-definite matrices.\nCapitilizing on recently developed research on clustering Riemannian\nsubmanifolds, an algorithm is provided for distinguishing time series based on\ntheir geometrical properties, revealed within Riemannian feature spaces.\nExtensive numerical tests demonstrate that the proposed framework outperforms\nclassical and state-of-the-art techniques in clustering brain-network\nstates/structures hidden beneath synthetic fMRI time series and brain-activity\nsignals generated from real brain-network structural connectivity matrices. \n\n"}
{"id": "1701.07875", "contents": "Title: Wasserstein GAN Abstract: We introduce a new algorithm named WGAN, an alternative to traditional GAN\ntraining. In this new model, we show that we can improve the stability of\nlearning, get rid of problems like mode collapse, and provide meaningful\nlearning curves useful for debugging and hyperparameter searches. Furthermore,\nwe show that the corresponding optimization problem is sound, and provide\nextensive theoretical work highlighting the deep connections to other distances\nbetween distributions. \n\n"}
{"id": "1701.08106", "contents": "Title: Faster Discovery of Faster System Configurations with Spectral Learning Abstract: Despite the huge spread and economical importance of configurable software\nsystems, there is unsatisfactory support in utilizing the full potential of\nthese systems with respect to finding performance-optimal configurations. Prior\nwork on predicting the performance of software configurations suffered from\neither (a) requiring far too many sample configurations or (b) large variances\nin their predictions. Both these problems can be avoided using the WHAT\nspectral learner. WHAT's innovation is the use of the spectrum (eigenvalues) of\nthe distance matrix between the configurations of a configurable software\nsystem, to perform dimensionality reduction. Within that reduced configuration\nspace, many closely associated configurations can be studied by executing only\na few sample configurations. For the subject systems studied here, a few dozen\nsamples yield accurate and stable predictors - less than 10% prediction error,\nwith a standard deviation of less than 2%. When compared to the state of the\nart, WHAT (a) requires 2 to 10 times fewer samples to achieve similar\nprediction accuracies, and (b) its predictions are more stable (i.e., have\nlower standard deviation). Furthermore, we demonstrate that predictive models\ngenerated by WHAT can be used by optimizers to discover system configurations\nthat closely approach the optimal performance. \n\n"}
{"id": "1701.08528", "contents": "Title: Self-Adaptation of Activity Recognition Systems to New Sensors Abstract: Traditional activity recognition systems work on the basis of training,\ntaking a fixed set of sensors into account. In this article, we focus on the\nquestion how pattern recognition can leverage new information sources without\nany, or with minimal user input. Thus, we present an approach for opportunistic\nactivity recognition, where ubiquitous sensors lead to dynamically changing\ninput spaces. Our method is a variation of well-established principles of\nmachine learning, relying on unsupervised clustering to discover structure in\ndata and inferring cluster labels from a small number of labeled dates in a\nsemi-supervised manner. Elaborating the challenges, evaluations of over 3000\nsensor combinations from three multi-user experiments are presented in detail\nand show the potential benefit of our approach. \n\n"}
{"id": "1702.03644", "contents": "Title: Coresets for Kernel Regression Abstract: Kernel regression is an essential and ubiquitous tool for non-parametric data\nanalysis, particularly popular among time series and spatial data. However, the\ncentral operation which is performed many times, evaluating a kernel on the\ndata set, takes linear time. This is impractical for modern large data sets.\n  In this paper we describe coresets for kernel regression: compressed data\nsets which can be used as proxy for the original data and have provably bounded\nworst case error. The size of the coresets are independent of the raw number of\ndata points, rather they only depend on the error guarantee, and in some cases\nthe size of domain and amount of smoothing. We evaluate our methods on very\nlarge time series and spatial data, and demonstrate that they incur negligible\nerror, can be constructed extremely efficiently, and allow for great\ncomputational gains. \n\n"}
{"id": "1702.03673", "contents": "Title: Bayesian Probabilistic Numerical Methods Abstract: The emergent field of probabilistic numerics has thus far lacked clear\nstatistical principals. This paper establishes Bayesian probabilistic numerical\nmethods as those which can be cast as solutions to certain inverse problems\nwithin the Bayesian framework. This allows us to establish general conditions\nunder which Bayesian probabilistic numerical methods are well-defined,\nencompassing both non-linear and non-Gaussian models. For general computation,\na numerical approximation scheme is proposed and its asymptotic convergence\nestablished. The theoretical development is then extended to pipelines of\ncomputation, wherein probabilistic numerical methods are composed to solve more\nchallenging numerical tasks. The contribution highlights an important research\nfrontier at the interface of numerical analysis and uncertainty quantification,\nwith a challenging industrial application presented. \n\n"}
{"id": "1702.05181", "contents": "Title: RIPML: A Restricted Isometry Property based Approach to Multilabel\n  Learning Abstract: The multilabel learning problem with large number of labels, features, and\ndata-points has generated a tremendous interest recently. A recurring theme of\nthese problems is that only a few labels are active in any given datapoint as\ncompared to the total number of labels. However, only a small number of\nexisting work take direct advantage of this inherent extreme sparsity in the\nlabel space. By the virtue of Restricted Isometry Property (RIP), satisfied by\nmany random ensembles, we propose a novel procedure for multilabel learning\nknown as RIPML. During the training phase, in RIPML, labels are projected onto\na random low-dimensional subspace followed by solving a least-square problem in\nthis subspace. Inference is done by a k-nearest neighbor (kNN) based approach.\nWe demonstrate the effectiveness of RIPML by conducting extensive simulations\nand comparing results with the state-of-the-art linear dimensionality reduction\nbased approaches. \n\n"}
{"id": "1702.05815", "contents": "Title: Compressive Embedding and Visualization using Graphs Abstract: Visualizing high-dimensional data has been a focus in data analysis\ncommunities for decades, which has led to the design of many algorithms, some\nof which are now considered references (such as t-SNE for example). In our era\nof overwhelming data volumes, the scalability of such methods have become more\nand more important. In this work, we present a method which allows to apply any\nvisualization or embedding algorithm on very large datasets by considering only\na fraction of the data as input and then extending the information to all data\npoints using a graph encoding its global similarity. We show that in most\ncases, using only $\\mathcal{O}(\\log(N))$ samples is sufficient to diffuse the\ninformation to all $N$ data points. In addition, we propose quantitative\nmethods to measure the quality of embeddings and demonstrate the validity of\nour technique on both synthetic and real-world datasets. \n\n"}
{"id": "1702.07319", "contents": "Title: A Goal-Based Movement Model for Continuous Multi-Agent Tasks Abstract: Despite increasing attention paid to the need for fast, scalable methods to\nanalyze next-generation neuroscience data, comparatively little attention has\nbeen paid to the development of similar methods for behavioral analysis. Just\nas the volume and complexity of brain data have grown, behavioral paradigms in\nsystems neuroscience have likewise become more naturalistic and less\nconstrained, necessitating an increase in the flexibility and scalability of\nthe models used to study them. In particular, key assumptions made in the\nanalysis of typical decision paradigms --- optimality; analytic tractability;\ndiscrete, low-dimensional action spaces --- may be untenable in richer tasks.\nHere, using the case of a two-player, real-time, continuous strategic game as\nan example, we show how the use of modern machine learning methods allows us to\nrelax each of these assumptions. Following an inverse reinforcement learning\napproach, we are able to succinctly characterize the joint distribution over\nplayers' actions via a generative model that allows us to simulate realistic\ngame play. We compare simulated play from a number of generative time series\nmodels and show that ours successfully resists mode collapse while generating\ntrajectories with the rich variability of real behavior. Together, these\nmethods offer a rich class of models for the analysis of continuous action\ntasks at the single-trial level. \n\n"}
{"id": "1702.08138", "contents": "Title: Deceiving Google's Perspective API Built for Detecting Toxic Comments Abstract: Social media platforms provide an environment where people can freely engage\nin discussions. Unfortunately, they also enable several problems, such as\nonline harassment. Recently, Google and Jigsaw started a project called\nPerspective, which uses machine learning to automatically detect toxic\nlanguage. A demonstration website has been also launched, which allows anyone\nto type a phrase in the interface and instantaneously see the toxicity score\n[1]. In this paper, we propose an attack on the Perspective toxic detection\nsystem based on the adversarial examples. We show that an adversary can subtly\nmodify a highly toxic phrase in a way that the system assigns significantly\nlower toxicity score to it. We apply the attack on the sample phrases provided\nin the Perspective website and show that we can consistently reduce the\ntoxicity scores to the level of the non-toxic phrases. The existence of such\nadversarial examples is very harmful for toxic detection systems and seriously\nundermines their usability. \n\n"}
{"id": "1702.08169", "contents": "Title: Communication-efficient Algorithms for Distributed Stochastic Principal\n  Component Analysis Abstract: We study the fundamental problem of Principal Component Analysis in a\nstatistical distributed setting in which each machine out of $m$ stores a\nsample of $n$ points sampled i.i.d. from a single unknown distribution. We\nstudy algorithms for estimating the leading principal component of the\npopulation covariance matrix that are both communication-efficient and achieve\nestimation error of the order of the centralized ERM solution that uses all\n$mn$ samples. On the negative side, we show that in contrast to results\nobtained for distributed estimation under convexity assumptions, for the PCA\nobjective, simply averaging the local ERM solutions cannot guarantee error that\nis consistent with the centralized ERM. We show that this unfortunate phenomena\ncan be remedied by performing a simple correction step which correlates between\nthe individual solutions, and provides an estimator that is consistent with the\ncentralized ERM for sufficiently-large $n$. We also introduce an iterative\ndistributed algorithm that is applicable in any regime of $n$, which is based\non distributed matrix-vector products. The algorithm gives significant\nacceleration in terms of communication rounds over previous distributed\nalgorithms, in a wide regime of parameters. \n\n"}
{"id": "1703.00564", "contents": "Title: MoleculeNet: A Benchmark for Molecular Machine Learning Abstract: Molecular machine learning has been maturing rapidly over the last few years.\nImproved methods and the presence of larger datasets have enabled machine\nlearning algorithms to make increasingly accurate predictions about molecular\nproperties. However, algorithmic progress has been limited due to the lack of a\nstandard benchmark to compare the efficacy of proposed methods; most new\nalgorithms are benchmarked on different datasets making it challenging to gauge\nthe quality of proposed methods. This work introduces MoleculeNet, a large\nscale benchmark for molecular machine learning. MoleculeNet curates multiple\npublic datasets, establishes metrics for evaluation, and offers high quality\nopen-source implementations of multiple previously proposed molecular\nfeaturization and learning algorithms (released as part of the DeepChem open\nsource library). MoleculeNet benchmarks demonstrate that learnable\nrepresentations are powerful tools for molecular machine learning and broadly\noffer the best performance. However, this result comes with caveats. Learnable\nrepresentations still struggle to deal with complex tasks under data scarcity\nand highly imbalanced classification. For quantum mechanical and biophysical\ndatasets, the use of physics-aware featurizations can be more important than\nchoice of particular learning algorithm. \n\n"}
{"id": "1703.00593", "contents": "Title: Positive-Unlabeled Learning with Non-Negative Risk Estimator Abstract: From only positive (P) and unlabeled (U) data, a binary classifier could be\ntrained with PU learning, in which the state of the art is unbiased PU\nlearning. However, if its model is very flexible, empirical risks on training\ndata will go negative, and we will suffer from serious overfitting. In this\npaper, we propose a non-negative risk estimator for PU learning: when getting\nminimized, it is more robust against overfitting, and thus we are able to use\nvery flexible models (such as deep neural networks) given limited P data.\nMoreover, we analyze the bias, consistency, and mean-squared-error reduction of\nthe proposed risk estimator, and bound the estimation error of the resulting\nempirical risk minimizer. Experiments demonstrate that our risk estimator fixes\nthe overfitting problem of its unbiased counterparts. \n\n"}
{"id": "1703.02317", "contents": "Title: Convolutional Recurrent Neural Networks for Bird Audio Detection Abstract: Bird sounds possess distinctive spectral structure which may exhibit small\nshifts in spectrum depending on the bird species and environmental conditions.\nIn this paper, we propose using convolutional recurrent neural networks on the\ntask of automated bird audio detection in real-life environments. In the\nproposed method, convolutional layers extract high dimensional, local frequency\nshift invariant features, while recurrent layers capture longer term\ndependencies between the features extracted from short time frames. This method\nachieves 88.5% Area Under ROC Curve (AUC) score on the unseen evaluation data\nand obtains the second place in the Bird Audio Detection challenge. \n\n"}
{"id": "1703.02910", "contents": "Title: Deep Bayesian Active Learning with Image Data Abstract: Even though active learning forms an important pillar of machine learning,\ndeep learning tools are not prevalent within it. Deep learning poses several\ndifficulties when used in an active learning setting. First, active learning\n(AL) methods generally rely on being able to learn and update models from small\namounts of data. Recent advances in deep learning, on the other hand, are\nnotorious for their dependence on large amounts of data. Second, many AL\nacquisition functions rely on model uncertainty, yet deep learning methods\nrarely represent such model uncertainty. In this paper we combine recent\nadvances in Bayesian deep learning into the active learning framework in a\npractical way. We develop an active learning framework for high dimensional\ndata, a task which has been extremely challenging so far, with very sparse\nexisting literature. Taking advantage of specialised models such as Bayesian\nconvolutional neural networks, we demonstrate our active learning techniques\nwith image data, obtaining a significant improvement on existing active\nlearning approaches. We demonstrate this on both the MNIST dataset, as well as\nfor skin cancer diagnosis from lesion images (ISIC2016 task). \n\n"}
{"id": "1703.03044", "contents": "Title: A GAMP Based Low Complexity Sparse Bayesian Learning Algorithm Abstract: In this paper, we present an algorithm for the sparse signal recovery problem\nthat incorporates damped Gaussian generalized approximate message passing\n(GGAMP) into Expectation-Maximization (EM)-based sparse Bayesian learning\n(SBL). In particular, GGAMP is used to implement the E-step in SBL in place of\nmatrix inversion, leveraging the fact that GGAMP is guaranteed to converge with\nappropriate damping. The resulting GGAMP-SBL algorithm is much more robust to\narbitrary measurement matrix $\\boldsymbol{A}$ than the standard damped GAMP\nalgorithm while being much lower complexity than the standard SBL algorithm. We\nthen extend the approach from the single measurement vector (SMV) case to the\ntemporally correlated multiple measurement vector (MMV) case, leading to the\nGGAMP-TSBL algorithm. We verify the robustness and computational advantages of\nthe proposed algorithms through numerical experiments. \n\n"}
{"id": "1703.03400", "contents": "Title: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks Abstract: We propose an algorithm for meta-learning that is model-agnostic, in the\nsense that it is compatible with any model trained with gradient descent and\napplicable to a variety of different learning problems, including\nclassification, regression, and reinforcement learning. The goal of\nmeta-learning is to train a model on a variety of learning tasks, such that it\ncan solve new learning tasks using only a small number of training samples. In\nour approach, the parameters of the model are explicitly trained such that a\nsmall number of gradient steps with a small amount of training data from a new\ntask will produce good generalization performance on that task. In effect, our\nmethod trains the model to be easy to fine-tune. We demonstrate that this\napproach leads to state-of-the-art performance on two few-shot image\nclassification benchmarks, produces good results on few-shot regression, and\naccelerates fine-tuning for policy gradient reinforcement learning with neural\nnetwork policies. \n\n"}
{"id": "1703.04219", "contents": "Title: SPARTan: Scalable PARAFAC2 for Large & Sparse Data Abstract: In exploratory tensor mining, a common problem is how to analyze a set of\nvariables across a set of subjects whose observations do not align naturally.\nFor example, when modeling medical features across a set of patients, the\nnumber and duration of treatments may vary widely in time, meaning there is no\nmeaningful way to align their clinical records across time points for analysis\npurposes. To handle such data, the state-of-the-art tensor model is the\nso-called PARAFAC2, which yields interpretable and robust output and can\nnaturally handle sparse data. However, its main limitation up to now has been\nthe lack of efficient algorithms that can handle large-scale datasets.\n  In this work, we fill this gap by developing a scalable method to compute the\nPARAFAC2 decomposition of large and sparse datasets, called SPARTan. Our method\nexploits special structure within PARAFAC2, leading to a novel algorithmic\nreformulation that is both fast (in absolute time) and more memory-efficient\nthan prior work. We evaluate SPARTan on both synthetic and real datasets,\nshowing 22X performance gains over the best previous implementation and also\nhandling larger problem instances for which the baseline fails. Furthermore, we\nare able to apply SPARTan to the mining of temporally-evolving phenotypes on\ndata taken from real and medically complex pediatric patients. The clinical\nmeaningfulness of the phenotypes identified in this process, as well as their\ntemporal evolution over time for several patients, have been endorsed by\nclinical experts. \n\n"}
{"id": "1703.05051", "contents": "Title: Deep learning with convolutional neural networks for EEG decoding and\n  visualization Abstract: PLEASE READ AND CITE THE REVISED VERSION at Human Brain Mapping:\nhttp://onlinelibrary.wiley.com/doi/10.1002/hbm.23730/full\n  Code available here: https://github.com/robintibor/braindecode \n\n"}
{"id": "1703.05537", "contents": "Title: Shift Aggregate Extract Networks Abstract: We introduce an architecture based on deep hierarchical decompositions to\nlearn effective representations of large graphs. Our framework extends classic\nR-decompositions used in kernel methods, enabling nested part-of-part\nrelations. Unlike recursive neural networks, which unroll a template on input\ngraphs directly, we unroll a neural network template over the decomposition\nhierarchy, allowing us to deal with the high degree variability that typically\ncharacterize social network graphs. Deep hierarchical decompositions are also\namenable to domain compression, a technique that reduces both space and time\ncomplexity by exploiting symmetries. We show empirically that our approach is\nable to outperform current state-of-the-art graph classification methods on\nlarge social network datasets, while at the same time being competitive on\nsmall chemobiological benchmark datasets. \n\n"}
{"id": "1703.05880", "contents": "Title: Empirical Evaluation of Parallel Training Algorithms on Acoustic\n  Modeling Abstract: Deep learning models (DLMs) are state-of-the-art techniques in speech\nrecognition. However, training good DLMs can be time consuming especially for\nproduction-size models and corpora. Although several parallel training\nalgorithms have been proposed to improve training efficiency, there is no clear\nguidance on which one to choose for the task in hand due to lack of systematic\nand fair comparison among them. In this paper we aim at filling this gap by\ncomparing four popular parallel training algorithms in speech recognition,\nnamely asynchronous stochastic gradient descent (ASGD), blockwise model-update\nfiltering (BMUF), bulk synchronous parallel (BSP) and elastic averaging\nstochastic gradient descent (EASGD), on 1000-hour LibriSpeech corpora using\nfeed-forward deep neural networks (DNNs) and convolutional, long short-term\nmemory, DNNs (CLDNNs). Based on our experiments, we recommend using BMUF as the\ntop choice to train acoustic models since it is most stable, scales well with\nnumber of GPUs, can achieve reproducible results, and in many cases even\noutperforms single-GPU SGD. ASGD can be used as a substitute in some cases. \n\n"}
{"id": "1703.06749", "contents": "Title: Efficient variational Bayesian neural network ensembles for outlier\n  detection Abstract: In this work we perform outlier detection using ensembles of neural networks\nobtained by variational approximation of the posterior in a Bayesian neural\nnetwork setting. The variational parameters are obtained by sampling from the\ntrue posterior by gradient descent. We show our outlier detection results are\ncomparable to those obtained using other efficient ensembling methods. \n\n"}
{"id": "1703.06925", "contents": "Title: Black-Box Optimization in Machine Learning with Trust Region Based\n  Derivative Free Algorithm Abstract: In this work, we utilize a Trust Region based Derivative Free Optimization\n(DFO-TR) method to directly maximize the Area Under Receiver Operating\nCharacteristic Curve (AUC), which is a nonsmooth, noisy function. We show that\nAUC is a smooth function, in expectation, if the distributions of the positive\nand negative data points obey a jointly normal distribution. The practical\nperformance of this algorithm is compared to three prominent Bayesian\noptimization methods and random search. The presented numerical results show\nthat DFO-TR surpasses Bayesian optimization and random search on various\nblack-box optimization problem, such as maximizing AUC and hyperparameter\ntuning. \n\n"}
{"id": "1703.06959", "contents": "Title: CSI: A Hybrid Deep Model for Fake News Detection Abstract: The topic of fake news has drawn attention both from the public and the\nacademic communities. Such misinformation has the potential of affecting public\nopinion, providing an opportunity for malicious parties to manipulate the\noutcomes of public events such as elections. Because such high stakes are at\nplay, automatically detecting fake news is an important, yet challenging\nproblem that is not yet well understood. Nevertheless, there are three\ngenerally agreed upon characteristics of fake news: the text of an article, the\nuser response it receives, and the source users promoting it. Existing work has\nlargely focused on tailoring solutions to one particular characteristic which\nhas limited their success and generality. In this work, we propose a model that\ncombines all three characteristics for a more accurate and automated\nprediction. Specifically, we incorporate the behavior of both parties, users\nand articles, and the group behavior of users who propagate fake news.\nMotivated by the three characteristics, we propose a model called CSI which is\ncomposed of three modules: Capture, Score, and Integrate. The first module is\nbased on the response and text; it uses a Recurrent Neural Network to capture\nthe temporal pattern of user activity on a given article. The second module\nlearns the source characteristic based on the behavior of users, and the two\nare integrated with the third module to classify an article as fake or not.\nExperimental analysis on real-world data demonstrates that CSI achieves higher\naccuracy than existing models, and extracts meaningful latent representations\nof both users and articles. \n\n"}
{"id": "1703.07370", "contents": "Title: REBAR: Low-variance, unbiased gradient estimates for discrete latent\n  variable models Abstract: Learning in models with discrete latent variables is challenging due to high\nvariance gradient estimators. Generally, approaches have relied on control\nvariates to reduce the variance of the REINFORCE estimator. Recent work (Jang\net al. 2016, Maddison et al. 2016) has taken a different approach, introducing\na continuous relaxation of discrete variables to produce low-variance, but\nbiased, gradient estimates. In this work, we combine the two approaches through\na novel control variate that produces low-variance, \\emph{unbiased} gradient\nestimates. Then, we introduce a modification to the continuous relaxation and\nshow that the tightness of the relaxation can be adapted online, removing it as\na hyperparameter. We show state-of-the-art variance reduction on several\nbenchmark generative modeling tasks, generally leading to faster convergence to\na better final log-likelihood. \n\n"}
{"id": "1703.07830", "contents": "Title: Randomized Kernel Methods for Least-Squares Support Vector Machines Abstract: The least-squares support vector machine is a frequently used kernel method\nfor non-linear regression and classification tasks. Here we discuss several\napproximation algorithms for the least-squares support vector machine\nclassifier. The proposed methods are based on randomized block kernel matrices,\nand we show that they provide good accuracy and reliable scaling for\nmulti-class classification problems with relatively large data sets. Also, we\npresent several numerical experiments that illustrate the practical\napplicability of the proposed methods. \n\n"}
{"id": "1703.08245", "contents": "Title: On the Robustness of Convolutional Neural Networks to Internal\n  Architecture and Weight Perturbations Abstract: Deep convolutional neural networks are generally regarded as robust function\napproximators. So far, this intuition is based on perturbations to external\nstimuli such as the images to be classified. Here we explore the robustness of\nconvolutional neural networks to perturbations to the internal weights and\narchitecture of the network itself. We show that convolutional networks are\nsurprisingly robust to a number of internal perturbations in the higher\nconvolutional layers but the bottom convolutional layers are much more fragile.\nFor instance, Alexnet shows less than a 30% decrease in classification\nperformance when randomly removing over 70% of weight connections in the top\nconvolutional or dense layers but performance is almost at chance with the same\nperturbation in the first convolutional layer. Finally, we suggest further\ninvestigations which could continue to inform the robustness of convolutional\nnetworks to internal perturbations. \n\n"}
{"id": "1703.08524", "contents": "Title: Joint Modeling of Event Sequence and Time Series with Attentional Twin\n  Recurrent Neural Networks Abstract: A variety of real-world processes (over networks) produce sequences of data\nwhose complex temporal dynamics need to be studied. More especially, the event\ntimestamps can carry important information about the underlying network\ndynamics, which otherwise are not available from the time-series evenly sampled\nfrom continuous signals. Moreover, in most complex processes, event sequences\nand evenly-sampled times series data can interact with each other, which\nrenders joint modeling of those two sources of data necessary. To tackle the\nabove problems, in this paper, we utilize the rich framework of (temporal)\npoint processes to model event data and timely update its intensity function by\nthe synergic twin Recurrent Neural Networks (RNNs). In the proposed\narchitecture, the intensity function is synergistically modulated by one RNN\nwith asynchronous events as input and another RNN with time series as input.\nFurthermore, to enhance the interpretability of the model, the attention\nmechanism for the neural point process is introduced. The whole model with\nevent type and timestamp prediction output layers can be trained end-to-end and\nallows a black-box treatment for modeling the intensity. We substantiate the\nsuperiority of our model in synthetic data and three real-world benchmark\ndatasets. \n\n"}
{"id": "1703.08581", "contents": "Title: Sequence-to-Sequence Models Can Directly Translate Foreign Speech Abstract: We present a recurrent encoder-decoder deep neural network architecture that\ndirectly translates speech in one language into text in another. The model does\nnot explicitly transcribe the speech into text in the source language, nor does\nit require supervision from the ground truth source language transcription\nduring training. We apply a slightly modified sequence-to-sequence with\nattention architecture that has previously been used for speech recognition and\nshow that it can be repurposed for this more complex task, illustrating the\npower of attention-based models. A single model trained end-to-end obtains\nstate-of-the-art performance on the Fisher Callhome Spanish-English speech\ntranslation task, outperforming a cascade of independently trained\nsequence-to-sequence speech recognition and machine translation models by 1.8\nBLEU points on the Fisher test set. In addition, we find that making use of the\ntraining data in both languages by multi-task training sequence-to-sequence\nspeech translation and recognition models with a shared encoder network can\nimprove performance by a further 1.4 BLEU points. \n\n"}
{"id": "1703.08667", "contents": "Title: Exploration--Exploitation in MDPs with Options Abstract: While a large body of empirical results show that temporally-extended actions\nand options may significantly affect the learning performance of an agent, the\ntheoretical understanding of how and when options can be beneficial in online\nreinforcement learning is relatively limited. In this paper, we derive an upper\nand lower bound on the regret of a variant of UCRL using options. While we\nfirst analyze the algorithm in the general case of semi-Markov decision\nprocesses (SMDPs), we show how these results can be translated to the specific\ncase of MDPs with options and we illustrate simple scenarios in which the\nregret of learning with options can be \\textit{provably} much smaller than the\nregret suffered when learning with primitive actions. \n\n"}
{"id": "1703.09197", "contents": "Title: Deep Architectures for Modulation Recognition Abstract: We survey the latest advances in machine learning with deep neural networks\nby applying them to the task of radio modulation recognition. Results show that\nradio modulation recognition is not limited by network depth and further work\nshould focus on improving learned synchronization and equalization. Advances in\nthese areas will likely come from novel architectures designed for these tasks\nor through novel training methods. \n\n"}
{"id": "1703.09891", "contents": "Title: LabelBank: Revisiting Global Perspectives for Semantic Segmentation Abstract: Semantic segmentation requires a detailed labeling of image pixels by object\ncategory. Information derived from local image patches is necessary to describe\nthe detailed shape of individual objects. However, this information is\nambiguous and can result in noisy labels. Global inference of image content can\ninstead capture the general semantic concepts present. We advocate that\nholistic inference of image concepts provides valuable information for detailed\npixel labeling. We propose a generic framework to leverage holistic information\nin the form of a LabelBank for pixel-level segmentation.\n  We show the ability of our framework to improve semantic segmentation\nperformance in a variety of settings. We learn models for extracting a holistic\nLabelBank from visual cues, attributes, and/or textual descriptions. We\ndemonstrate improvements in semantic segmentation accuracy on standard datasets\nacross a range of state-of-the-art segmentation architectures and holistic\ninference approaches. \n\n"}
{"id": "1703.10669", "contents": "Title: QoS-Aware Multi-Armed Bandits Abstract: Motivated by runtime verification of QoS requirements in self-adaptive and\nself-organizing systems that are able to reconfigure their structure and\nbehavior in response to runtime data, we propose a QoS-aware variant of\nThompson sampling for multi-armed bandits. It is applicable in settings where\nQoS satisfaction of an arm has to be ensured with high confidence efficiently,\nrather than finding the optimal arm while minimizing regret. Preliminary\nexperimental results encourage further research in the field of QoS-aware\ndecision making. \n\n"}
{"id": "1704.00454", "contents": "Title: Clustering in Hilbert simplex geometry Abstract: Clustering categorical distributions in the finite-dimensional probability\nsimplex is a fundamental task met in many applications dealing with normalized\nhistograms. Traditionally, the differential-geometric structures of the\nprobability simplex have been used either by (i) setting the Riemannian metric\ntensor to the Fisher information matrix of the categorical distributions, or\n(ii) defining the dualistic information-geometric structure induced by a smooth\ndissimilarity measure, the Kullback-Leibler divergence. In this work, we\nintroduce for clustering tasks a novel computationally-friendly framework for\nmodeling geometrically the probability simplex: The {\\em Hilbert simplex\ngeometry}. In the Hilbert simplex geometry, the distance is the non-separable\nHilbert's metric distance which satisfies the property of information\nmonotonicity with distance level set functions described by polytope\nboundaries. We show that both the Aitchison and Hilbert simplex distances are\nnorm distances on normalized logarithmic representations with respect to the\n$\\ell_2$ and variation norms, respectively. We discuss the pros and cons of\nthose different statistical modelings, and benchmark experimentally these\ndifferent kind of geometries for center-based $k$-means and $k$-center\nclustering. Furthermore, since a canonical Hilbert distance can be defined on\nany bounded convex subset of the Euclidean space, we also consider Hilbert's\ngeometry of the elliptope of correlation matrices and study its clustering\nperformances compared to Fr\\\"obenius and log-det divergences. \n\n"}
{"id": "1704.00642", "contents": "Title: Local nearest neighbour classification with applications to\n  semi-supervised learning Abstract: We derive a new asymptotic expansion for the global excess risk of a\nlocal-$k$-nearest neighbour classifier, where the choice of $k$ may depend upon\nthe test point. This expansion elucidates conditions under which the dominant\ncontribution to the excess risk comes from the decision boundary of the optimal\nBayes classifier, but we also show that if these conditions are not satisfied,\nthen the dominant contribution may arise from the tails of the marginal\ndistribution of the features. Moreover, we prove that, provided the\n$d$-dimensional marginal distribution of the features has a finite $\\rho$th\nmoment for some $\\rho > 4$ (as well as other regularity conditions), a local\nchoice of $k$ can yield a rate of convergence of the excess risk of\n$O(n^{-4/(d+4)})$, where $n$ is the sample size, whereas for the standard\n$k$-nearest neighbour classifier, our theory would require $d \\geq 5$ and $\\rho\n> 4d/(d-4)$ finite moments to achieve this rate. These results motivate a new\n$k$-nearest neighbour classifier for semi-supervised learning problems, where\nthe unlabelled data are used to obtain an estimate of the marginal feature\ndensity, and fewer neighbours are used for classification when this density\nestimate is small. Our worst-case rates are complemented by a minimax lower\nbound, which reveals that the local, semi-supervised $k$-nearest neighbour\nclassifier attains the minimax optimal rate over our classes for the excess\nrisk, up to a subpolynomial factor in $n$. These theoretical improvements over\nthe standard $k$-nearest neighbour classifier are also illustrated through a\nsimulation study. \n\n"}
{"id": "1704.01444", "contents": "Title: Learning to Generate Reviews and Discovering Sentiment Abstract: We explore the properties of byte-level recurrent language models. When given\nsufficient amounts of capacity, training data, and compute time, the\nrepresentations learned by these models include disentangled features\ncorresponding to high-level concepts. Specifically, we find a single unit which\nperforms sentiment analysis. These representations, learned in an unsupervised\nmanner, achieve state of the art on the binary subset of the Stanford Sentiment\nTreebank. They are also very data efficient. When using only a handful of\nlabeled examples, our approach matches the performance of strong baselines\ntrained on full datasets. We also demonstrate the sentiment unit has a direct\ninfluence on the generative process of the model. Simply fixing its value to be\npositive or negative generates samples with the corresponding positive or\nnegative sentiment. \n\n"}
{"id": "1704.01701", "contents": "Title: Learning Certifiably Optimal Rule Lists for Categorical Data Abstract: We present the design and implementation of a custom discrete optimization\ntechnique for building rule lists over a categorical feature space. Our\nalgorithm produces rule lists with optimal training performance, according to\nthe regularized empirical risk, with a certificate of optimality. By leveraging\nalgorithmic bounds, efficient data structures, and computational reuse, we\nachieve several orders of magnitude speedup in time and a massive reduction of\nmemory consumption. We demonstrate that our approach produces optimal rule\nlists on practical problems in seconds. Our results indicate that it is\npossible to construct optimal sparse rule lists that are approximately as\naccurate as the COMPAS proprietary risk prediction tool on data from Broward\nCounty, Florida, but that are completely interpretable. This framework is a\nnovel alternative to CART and other decision tree methods for interpretable\nmodeling. \n\n"}
{"id": "1704.03144", "contents": "Title: Parametric Gaussian Process Regression for Big Data Abstract: This work introduces the concept of parametric Gaussian processes (PGPs),\nwhich is built upon the seemingly self-contradictory idea of making Gaussian\nprocesses parametric. Parametric Gaussian processes, by construction, are\ndesigned to operate in \"big data\" regimes where one is interested in\nquantifying the uncertainty associated with noisy data. The proposed\nmethodology circumvents the well-established need for stochastic variational\ninference, a scalable algorithm for approximating posterior distributions. The\neffectiveness of the proposed approach is demonstrated using an illustrative\nexample with simulated data and a benchmark dataset in the airline industry\nwith approximately 6 million records. \n\n"}
{"id": "1704.04110", "contents": "Title: DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks Abstract: Probabilistic forecasting, i.e. estimating the probability distribution of a\ntime series' future given its past, is a key enabler for optimizing business\nprocesses. In retail businesses, for example, forecasting demand is crucial for\nhaving the right inventory available at the right time at the right place. In\nthis paper we propose DeepAR, a methodology for producing accurate\nprobabilistic forecasts, based on training an auto regressive recurrent network\nmodel on a large number of related time series. We demonstrate how by applying\ndeep learning techniques to forecasting, one can overcome many of the\nchallenges faced by widely-used classical approaches to the problem. We show\nthrough extensive empirical evaluation on several real-world forecasting data\nsets accuracy improvements of around 15% compared to state-of-the-art methods. \n\n"}
{"id": "1704.04866", "contents": "Title: Effective Warm Start for the Online Actor-Critic Reinforcement Learning\n  based mHealth Intervention Abstract: Online reinforcement learning (RL) is increasingly popular for the\npersonalized mobile health (mHealth) intervention. It is able to personalize\nthe type and dose of interventions according to user's ongoing statuses and\nchanging needs. However, at the beginning of online learning, there are usually\ntoo few samples to support the RL updating, which leads to poor performances. A\ndelay in good performance of the online learning algorithms can be especially\ndetrimental in the mHealth, where users tend to quickly disengage with the\nmHealth app. To address this problem, we propose a new online RL methodology\nthat focuses on an effective warm start. The main idea is to make full use of\nthe data accumulated and the decision rule achieved in a former study. As a\nresult, we can greatly enrich the data size at the beginning of online learning\nin our method. Such case accelerates the online learning process for new users\nto achieve good performances not only at the beginning of online learning but\nalso through the whole online learning process. Besides, we use the decision\nrules achieved in a previous study to initialize the parameter in our online RL\nmodel for new users. It provides a good initialization for the proposed online\nRL algorithm. Experiment results show that promising improvements have been\nachieved by our method compared with the state-of-the-art method. \n\n"}
{"id": "1704.07287", "contents": "Title: Parsing Speech: A Neural Approach to Integrating Lexical and\n  Acoustic-Prosodic Information Abstract: In conversational speech, the acoustic signal provides cues that help\nlisteners disambiguate difficult parses. For automatically parsing spoken\nutterances, we introduce a model that integrates transcribed text and\nacoustic-prosodic features using a convolutional neural network over energy and\npitch trajectories coupled with an attention-based recurrent neural network\nthat accepts text and prosodic features. We find that different types of\nacoustic-prosodic features are individually helpful, and together give\nstatistically significant improvements in parse and disfluency detection F1\nscores over a strong text-only baseline. For this study with known sentence\nboundaries, error analyses show that the main benefit of acoustic-prosodic\nfeatures is in sentences with disfluencies, attachment decisions are most\nimproved, and transcription errors obscure gains from prosody. \n\n"}
{"id": "1704.07816", "contents": "Title: Introspective Classification with Convolutional Nets Abstract: We propose introspective convolutional networks (ICN) that emphasize the\nimportance of having convolutional neural networks empowered with generative\ncapabilities. We employ a reclassification-by-synthesis algorithm to perform\ntraining using a formulation stemmed from the Bayes theory. Our ICN tries to\niteratively: (1) synthesize pseudo-negative samples; and (2) enhance itself by\nimproving the classification. The single CNN classifier learned is at the same\ntime generative --- being able to directly synthesize new samples within its\nown discriminative model. We conduct experiments on benchmark datasets\nincluding MNIST, CIFAR-10, and SVHN using state-of-the-art CNN architectures,\nand observe improved classification results. \n\n"}
{"id": "1704.08803", "contents": "Title: Neural Ranking Models with Weak Supervision Abstract: Despite the impressive improvements achieved by unsupervised deep neural\nnetworks in computer vision and NLP tasks, such improvements have not yet been\nobserved in ranking for information retrieval. The reason may be the complexity\nof the ranking problem, as it is not obvious how to learn from queries and\ndocuments when no supervised signal is available. Hence, in this paper, we\npropose to train a neural ranking model using weak supervision, where labels\nare obtained automatically without human annotators or any external resources\n(e.g., click data). To this aim, we use the output of an unsupervised ranking\nmodel, such as BM25, as a weak supervision signal. We further train a set of\nsimple yet effective ranking models based on feed-forward neural networks. We\nstudy their effectiveness under various learning scenarios (point-wise and\npair-wise models) and using different input representations (i.e., from\nencoding query-document pairs into dense/sparse vectors to using word embedding\nrepresentation). We train our networks using tens of millions of training\ninstances and evaluate it on two standard collections: a homogeneous news\ncollection(Robust) and a heterogeneous large-scale web collection (ClueWeb).\nOur experiments indicate that employing proper objective functions and letting\nthe networks to learn the input representation based on weakly supervised data\nleads to impressive performance, with over 13% and 35% MAP improvements over\nthe BM25 model on the Robust and the ClueWeb collections. Our findings also\nsuggest that supervised neural ranking models can greatly benefit from\npre-training on large amounts of weakly labeled data that can be easily\nobtained from unsupervised IR models. \n\n"}
{"id": "1705.00930", "contents": "Title: Show, Adapt and Tell: Adversarial Training of Cross-domain Image\n  Captioner Abstract: Impressive image captioning results are achieved in domains with plenty of\ntraining image and sentence pairs (e.g., MSCOCO). However, transferring to a\ntarget domain with significant domain shifts but no paired training data\n(referred to as cross-domain image captioning) remains largely unexplored. We\npropose a novel adversarial training procedure to leverage unpaired data in the\ntarget domain. Two critic networks are introduced to guide the captioner,\nnamely domain critic and multi-modal critic. The domain critic assesses whether\nthe generated sentences are indistinguishable from sentences in the target\ndomain. The multi-modal critic assesses whether an image and its generated\nsentence are a valid pair. During training, the critics and captioner act as\nadversaries -- captioner aims to generate indistinguishable sentences, whereas\ncritics aim at distinguishing them. The assessment improves the captioner\nthrough policy gradient updates. During inference, we further propose a novel\ncritic-based planning method to select high-quality sentences without\nadditional supervision (e.g., tags). To evaluate, we use MSCOCO as the source\ndomain and four other datasets (CUB-200-2011, Oxford-102, TGIF, and Flickr30k)\nas the target domains. Our method consistently performs well on all datasets.\nIn particular, on CUB-200-2011, we achieve 21.8% CIDEr-D improvement after\nadaptation. Utilizing critics during inference further gives another 4.5%\nboost. \n\n"}
{"id": "1705.02210", "contents": "Title: SLDR-DL: A Framework for SLD-Resolution with Deep Learning Abstract: This paper introduces an SLD-resolution technique based on deep learning.\nThis technique enables neural networks to learn from old and successful\nresolution processes and to use learnt experiences to guide new resolution\nprocesses. An implementation of this technique is named SLDR-DL. It includes a\nProlog library of deep feedforward neural networks and some essential functions\nof resolution. In the SLDR-DL framework, users can define logical rules in the\nform of definite clauses and teach neural networks to use the rules in\nreasoning processes. \n\n"}
{"id": "1705.04374", "contents": "Title: Optimal fidelity multi-level Monte Carlo for quantification of\n  uncertainty in simulations of cloud cavitation collapse Abstract: We quantify uncertainties in the location and magnitude of extreme pressure\nspots revealed from large scale multi-phase flow simulations of cloud\ncavitation collapse. We examine clouds containing 500 cavities and quantify\nuncertainties related to their initial spatial arrangement. The resulting\n2000-dimensional space is sampled using a non-intrusive and computationally\nefficient Multi-Level Monte Carlo (MLMC) methodology. We introduce novel\noptimal control variate coefficients to enhance the variance reduction in MLMC.\nThe proposed optimal fidelity MLMC leads to more than two orders of magnitude\nspeedup when compared to standard Monte Carlo methods. We identify large\nuncertainties in the location and magnitude of the peak pressure pulse and\npresent its statistical correlations and joint probability density functions\nwith the geometrical characteristics of the cloud. Characteristic properties of\nspatial cloud structure are identified as potential causes of significant\nuncertainties in exerted collapse pressures. \n\n"}
{"id": "1705.04379", "contents": "Title: The Network Nullspace Property for Compressed Sensing of Big Data over\n  Networks Abstract: We present a novel condition, which we term the net- work nullspace property,\nwhich ensures accurate recovery of graph signals representing massive\nnetwork-structured datasets from few signal values. The network nullspace\nproperty couples the cluster structure of the underlying network-structure with\nthe geometry of the sampling set. Our results can be used to design efficient\nsampling strategies based on the network topology. \n\n"}
{"id": "1705.05690", "contents": "Title: A Long Short-Term Memory Recurrent Neural Network Framework for Network\n  Traffic Matrix Prediction Abstract: Network Traffic Matrix (TM) prediction is defined as the problem of\nestimating future network traffic from the previous and achieved network\ntraffic data. It is widely used in network planning, resource management and\nnetwork security. Long Short-Term Memory (LSTM) is a specific recurrent neural\nnetwork (RNN) architecture that is well-suited to learn from experience to\nclassify, process and predict time series with time lags of unknown size. LSTMs\nhave been shown to model temporal sequences and their long-range dependencies\nmore accurately than conventional RNNs. In this paper, we propose a LSTM RNN\nframework for predicting short and long term Traffic Matrix (TM) in large\nnetworks. By validating our framework on real-world data from GEANT network, we\nshow that our LSTM models converge quickly and give state of the art TM\nprediction performance for relatively small sized models. \n\n"}
{"id": "1705.05782", "contents": "Title: Hierarchical Temporal Representation in Linear Reservoir Computing Abstract: Recently, studies on deep Reservoir Computing (RC) highlighted the role of\nlayering in deep recurrent neural networks (RNNs). In this paper, the use of\nlinear recurrent units allows us to bring more evidence on the intrinsic\nhierarchical temporal representation in deep RNNs through frequency analysis\napplied to the state signals. The potentiality of our approach is assessed on\nthe class of Multiple Superimposed Oscillator tasks. Furthermore, our\ninvestigation provides useful insights to open a discussion on the main aspects\nthat characterize the deep learning framework in the temporal domain. \n\n"}
{"id": "1705.06224", "contents": "Title: Practical Processing of Mobile Sensor Data for Continual Deep Learning\n  Predictions Abstract: We present a practical approach for processing mobile sensor time series data\nfor continual deep learning predictions. The approach comprises data cleaning,\nnormalization, capping, time-based compression, and finally classification with\na recurrent neural network. We demonstrate the effectiveness of the approach in\na case study with 279 participants. On the basis of sparse sensor events, the\nnetwork continually predicts whether the participants would attend to a\nnotification within 10 minutes. Compared to a random baseline, the classifier\nachieves a 40% performance increase (AUC of 0.702) on a withheld test set. This\napproach allows to forgo resource-intensive, domain-specific, error-prone\nfeature engineering, which may drastically increase the applicability of\nmachine learning to mobile phone sensor data. \n\n"}
{"id": "1705.06452", "contents": "Title: Delving into adversarial attacks on deep policies Abstract: Adversarial examples have been shown to exist for a variety of deep learning\narchitectures. Deep reinforcement learning has shown promising results on\ntraining agent policies directly on raw inputs such as image pixels. In this\npaper we present a novel study into adversarial attacks on deep reinforcement\nlearning polices. We compare the effectiveness of the attacks using adversarial\nexamples vs. random noise. We present a novel method for reducing the number of\ntimes adversarial examples need to be injected for a successful attack, based\non the value function. We further explore how re-training on random noise and\nFGSM perturbations affects the resilience against adversarial examples. \n\n"}
{"id": "1705.07157", "contents": "Title: Clustering under Local Stability: Bridging the Gap between Worst-Case\n  and Beyond Worst-Case Analysis Abstract: Recently, there has been substantial interest in clustering research that\ntakes a beyond worst-case approach to the analysis of algorithms. The typical\nidea is to design a clustering algorithm that outputs a near-optimal solution,\nprovided the data satisfy a natural stability notion. For example, Bilu and\nLinial (2010) and Awasthi et al. (2012) presented algorithms that output\nnear-optimal solutions, assuming the optimal solution is preserved under small\nperturbations to the input distances. A drawback to this approach is that the\nalgorithms are often explicitly built according to the stability assumption and\ngive no guarantees in the worst case; indeed, several recent algorithms output\narbitrarily bad solutions even when just a small section of the data does not\nsatisfy the given stability notion.\n  In this work, we address this concern in two ways. First, we provide\nalgorithms that inherit the worst-case guarantees of clustering approximation\nalgorithms, while simultaneously guaranteeing near-optimal solutions when the\ndata is stable. Our algorithms are natural modifications to existing\nstate-of-the-art approximation algorithms. Second, we initiate the study of\nlocal stability, which is a property of a single optimal cluster rather than an\nentire optimal solution. We show our algorithms output all optimal clusters\nwhich satisfy stability locally. Specifically, we achieve strong positive\nresults in our local framework under recent stability notions including metric\nperturbation resilience (Angelidakis et al. 2017) and robust perturbation\nresilience (Balcan and Liang 2012) for the $k$-median, $k$-means, and\nsymmetric/asymmetric $k$-center objectives. \n\n"}
{"id": "1705.07461", "contents": "Title: Shallow Updates for Deep Reinforcement Learning Abstract: Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN)\nhave achieved state-of-the-art results in a variety of challenging,\nhigh-dimensional domains. This success is mainly attributed to the power of\ndeep neural networks to learn rich domain representations for approximating the\nvalue function or policy. Batch reinforcement learning methods with linear\nrepresentations, on the other hand, are more stable and require less hyper\nparameter tuning. Yet, substantial feature engineering is necessary to achieve\ngood results. In this work we propose a hybrid approach -- the Least Squares\nDeep Q-Network (LS-DQN), which combines rich feature representations learned by\na DRL algorithm with the stability of a linear least squares method. We do this\nby periodically re-training the last hidden layer of a DRL network with a batch\nleast squares update. Key to our approach is a Bayesian regularization term for\nthe least squares update, which prevents over-fitting to the more recent data.\nWe tested LS-DQN on five Atari games and demonstrate significant improvement\nover vanilla DQN and Double-DQN. We also investigated the reasons for the\nsuperior performance of our method. Interestingly, we found that the\nperformance improvement can be attributed to the large batch size used by the\nLS method when optimizing the last layer. \n\n"}
{"id": "1705.07576", "contents": "Title: Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk Abstract: We examine the theoretical properties of enforcing priors provided by\ngenerative deep neural networks via empirical risk minimization. In particular\nwe consider two models, one in which the task is to invert a generative neural\nnetwork given access to its last layer and another in which the task is to\ninvert a generative neural network given only compressive linear observations\nof its last layer. We establish that in both cases, in suitable regimes of\nnetwork layer sizes and a randomness assumption on the network weights, that\nthe non-convex objective function given by empirical risk minimization does not\nhave any spurious stationary points. That is, we establish that with high\nprobability, at any point away from small neighborhoods around two scalar\nmultiples of the desired solution, there is a descent direction. Hence, there\nare no local minima, saddle points, or other stationary points outside these\nneighborhoods. These results constitute the first theoretical guarantees which\nestablish the favorable global geometry of these non-convex optimization\nproblems, and they bridge the gap between the empirical success of enforcing\ndeep generative priors and a rigorous understanding of non-linear inverse\nproblems. \n\n"}
{"id": "1705.07664", "contents": "Title: CayleyNets: Graph Convolutional Neural Networks with Complex Rational\n  Spectral Filters Abstract: The rise of graph-structured data such as social networks, regulatory\nnetworks, citation graphs, and functional brain networks, in combination with\nresounding success of deep learning in various applications, has brought the\ninterest in generalizing deep learning models to non-Euclidean domains. In this\npaper, we introduce a new spectral domain convolutional architecture for deep\nlearning on graphs. The core ingredient of our model is a new class of\nparametric rational complex functions (Cayley polynomials) allowing to\nefficiently compute spectral filters on graphs that specialize on frequency\nbands of interest. Our model generates rich spectral filters that are localized\nin space, scales linearly with the size of the input data for\nsparsely-connected graphs, and can handle different constructions of Laplacian\noperators. Extensive experimental results show the superior performance of our\napproach, in comparison to other spectral domain convolutional architectures,\non spectral image classification, community detection, vertex classification\nand matrix completion tasks. \n\n"}
{"id": "1705.07831", "contents": "Title: Stabilizing GAN Training with Multiple Random Projections Abstract: Training generative adversarial networks is unstable in high-dimensions as\nthe true data distribution tends to be concentrated in a small fraction of the\nambient space. The discriminator is then quickly able to classify nearly all\ngenerated samples as fake, leaving the generator without meaningful gradients\nand causing it to deteriorate after a point in training. In this work, we\npropose training a single generator simultaneously against an array of\ndiscriminators, each of which looks at a different random low-dimensional\nprojection of the data. Individual discriminators, now provided with restricted\nviews of the input, are unable to reject generated samples perfectly and\ncontinue to provide meaningful gradients to the generator throughout training.\nMeanwhile, the generator learns to produce samples consistent with the full\ndata distribution to satisfy all discriminators simultaneously. We demonstrate\nthe practical utility of this approach experimentally, and show that it is able\nto produce image samples with higher quality than traditional training with a\nsingle discriminator. \n\n"}
{"id": "1705.08292", "contents": "Title: The Marginal Value of Adaptive Gradient Methods in Machine Learning Abstract: Adaptive optimization methods, which perform local optimization with a metric\nconstructed from the history of iterates, are becoming increasingly popular for\ntraining deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We\nshow that for simple overparameterized problems, adaptive methods often find\ndrastically different solutions than gradient descent (GD) or stochastic\ngradient descent (SGD). We construct an illustrative binary classification\nproblem where the data is linearly separable, GD and SGD achieve zero test\nerror, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to\nhalf. We additionally study the empirical generalization capability of adaptive\nmethods on several state-of-the-art deep learning models. We observe that the\nsolutions found by adaptive methods generalize worse (often significantly\nworse) than SGD, even when these solutions have better training performance.\nThese results suggest that practitioners should reconsider the use of adaptive\nmethods to train neural networks. \n\n"}
{"id": "1705.08386", "contents": "Title: Better Text Understanding Through Image-To-Text Transfer Abstract: Generic text embeddings are successfully used in a variety of tasks. However,\nthey are often learnt by capturing the co-occurrence structure from pure text\ncorpora, resulting in limitations of their ability to generalize. In this\npaper, we explore models that incorporate visual information into the text\nrepresentation. Based on comprehensive ablation studies, we propose a\nconceptually simple, yet well performing architecture. It outperforms previous\nmultimodal approaches on a set of well established benchmarks. We also improve\nthe state-of-the-art results for image-related text datasets, using orders of\nmagnitude less data. \n\n"}
{"id": "1705.08722", "contents": "Title: Open-Category Classification by Adversarial Sample Generation Abstract: In real-world classification tasks, it is difficult to collect training\nsamples from all possible categories of the environment. Therefore, when an\ninstance of an unseen class appears in the prediction stage, a robust\nclassifier should be able to tell that it is from an unseen class, instead of\nclassifying it to be any known category. In this paper, adopting the idea of\nadversarial learning, we propose the ASG framework for open-category\nclassification. ASG generates positive and negative samples of seen categories\nin the unsupervised manner via an adversarial learning strategy. With the\ngenerated samples, ASG then learns to tell seen from unseen in the supervised\nmanner. Experiments performed on several datasets show the effectiveness of\nASG. \n\n"}
{"id": "1705.09011", "contents": "Title: Principled Hybrids of Generative and Discriminative Domain Adaptation Abstract: We propose a probabilistic framework for domain adaptation that blends both\ngenerative and discriminative modeling in a principled way. Under this\nframework, generative and discriminative models correspond to specific choices\nof the prior over parameters. This provides us a very general way to\ninterpolate between generative and discriminative extremes through different\nchoices of priors. By maximizing both the marginal and the conditional\nlog-likelihoods, models derived from this framework can use both labeled\ninstances from the source domain as well as unlabeled instances from both\nsource and target domains. Under this framework, we show that the popular\nreconstruction loss of autoencoder corresponds to an upper bound of the\nnegative marginal log-likelihoods of unlabeled instances, where marginal\ndistributions are given by proper kernel density estimations. This provides a\nway to interpret the empirical success of autoencoders in domain adaptation and\nsemi-supervised learning. We instantiate our framework using neural networks,\nand build a concrete model, DAuto. Empirically, we demonstrate the\neffectiveness of DAuto on text, image and speech datasets, showing that it\noutperforms related competitors when domain adaptation is possible. \n\n"}
{"id": "1705.09021", "contents": "Title: Learning to Pour Abstract: Pouring is a simple task people perform daily. It is the second most\nfrequently executed motion in cooking scenarios, after pick-and-place. We\npresent a pouring trajectory generation approach, which uses force feedback\nfrom the cup to determine the future velocity of pouring. The approach uses\nrecurrent neural networks as its building blocks. We collected the pouring\ndemonstrations which we used for training. To test our approach in simulation,\nwe also created and trained a force estimation system. The simulated\nexperiments show that the system is able to generalize to single unseen element\nof the pouring characteristics. \n\n"}
{"id": "1705.10770", "contents": "Title: Forward-Backward Selection with Early Dropping Abstract: Forward-backward selection is one of the most basic and commonly-used feature\nselection algorithms available. It is also general and conceptually applicable\nto many different types of data. In this paper, we propose a heuristic that\nsignificantly improves its running time, while preserving predictive accuracy.\nThe idea is to temporarily discard the variables that are conditionally\nindependent with the outcome given the selected variable set. Depending on how\nthose variables are reconsidered and reintroduced, this heuristic gives rise to\na family of algorithms with increasingly stronger theoretical guarantees. In\ndistributions that can be faithfully represented by Bayesian networks or\nmaximal ancestral graphs, members of this algorithmic family are able to\ncorrectly identify the Markov blanket in the sample limit. In experiments we\nshow that the proposed heuristic increases computational efficiency by about\ntwo orders of magnitude in high-dimensional problems, while selecting fewer\nvariables and retaining predictive performance. Furthermore, we show that the\nproposed algorithm and feature selection with LASSO perform similarly when\nrestricted to select the same number of variables, making the proposed\nalgorithm an attractive alternative for problems where no (efficient) algorithm\nfor LASSO exists. \n\n"}
{"id": "1706.00476", "contents": "Title: The Mixing method: low-rank coordinate descent for semidefinite\n  programming with diagonal constraints Abstract: In this paper, we propose a low-rank coordinate descent approach to\nstructured semidefinite programming with diagonal constraints. The approach,\nwhich we call the Mixing method, is extremely simple to implement, has no free\nparameters, and typically attains an order of magnitude or better improvement\nin optimization performance over the current state of the art. We show that the\nmethod is strictly decreasing, converges to a critical point, and further that\nfor sufficient rank all non-optimal critical points are unstable. Moreover, we\nprove that with a step size, the Mixing method converges to the global optimum\nof the semidefinite program almost surely in a locally linear rate under random\ninitialization. This is the first low-rank semidefinite programming method that\nhas been shown to achieve a global optimum on the spherical manifold without\nassumption. We apply our algorithm to two related domains: solving the maximum\ncut semidefinite relaxation, and solving a maximum satisfiability relaxation\n(we also briefly consider additional applications such as learning word\nembeddings). In all settings, we demonstrate substantial improvement over the\nexisting state of the art along various dimensions, and in total, this work\nexpands the scope and scale of problems that can be solved using semidefinite\nprogramming methods. \n\n"}
{"id": "1706.00885", "contents": "Title: IDK Cascades: Fast Deep Learning by Learning not to Overthink Abstract: Advances in deep learning have led to substantial increases in prediction\naccuracy but have been accompanied by increases in the cost of rendering\npredictions. We conjecture that fora majority of real-world inputs, the recent\nadvances in deep learning have created models that effectively \"overthink\" on\nsimple inputs. In this paper, we revisit the classic question of building model\ncascades that primarily leverage class asymmetry to reduce cost. We introduce\nthe \"I Don't Know\"(IDK) prediction cascades framework, a general framework to\nsystematically compose a set of pre-trained models to accelerate inference\nwithout a loss in prediction accuracy. We propose two search based methods for\nconstructing cascades as well as a new cost-aware objective within this\nframework. The proposed IDK cascade framework can be easily adopted in the\nexisting model serving systems without additional model re-training. We\nevaluate the proposed techniques on a range of benchmarks to demonstrate the\neffectiveness of the proposed framework. \n\n"}
{"id": "1706.01014", "contents": "Title: Nonconvex penalties with analytical solutions for one-bit compressive\n  sensing Abstract: One-bit measurements widely exist in the real world, and they can be used to\nrecover sparse signals. This task is known as the problem of learning\nhalfspaces in learning theory and one-bit compressive sensing (1bit-CS) in\nsignal processing. In this paper, we propose novel algorithms based on both\nconvex and nonconvex sparsity-inducing penalties for robust 1bit-CS. We provide\na sufficient condition to verify whether a solution is globally optimal or not.\nThen we show that the globally optimal solution for positive homogeneous\npenalties can be obtained in two steps: a proximal operator and a normalization\nstep. For several nonconvex penalties, including minimax concave penalty (MCP),\n$\\ell_0$ norm, and sorted $\\ell_1$ penalty, we provide fast algorithms for\nfinding the analytical solutions by solving the dual problem. Specifically, our\nalgorithm is more than $200$ times faster than the existing algorithm for MCP.\nIts efficiency is comparable to the algorithm for the $\\ell_1$ penalty in time,\nwhile its performance is much better. Among these penalties, the sorted\n$\\ell_1$ penalty is most robust to noise in different settings. \n\n"}
{"id": "1706.01606", "contents": "Title: DeepKey: An EEG and Gait Based Dual-Authentication System Abstract: Biometric authentication involves various technologies to identify\nindividuals by exploiting their unique, measurable physiological and behavioral\ncharacteristics. However, traditional biometric authentication systems (e.g.,\nface recognition, iris, retina, voice, and fingerprint) are facing an\nincreasing risk of being tricked by biometric tools such as anti-surveillance\nmasks, contact lenses, vocoder, or fingerprint films. In this paper, we design\na multimodal biometric authentication system named Deepkey, which uses both\nElectroencephalography (EEG) and gait signals to better protect against such\nrisk. Deepkey consists of two key components: an Invalid ID Filter Model to\nblock unauthorized subjects and an identification model based on\nattention-based Recurrent Neural Network (RNN) to identify a subject`s EEG IDs\nand gait IDs in parallel. The subject can only be granted access while all the\ncomponents produce consistent evidence to match the user`s proclaimed identity.\nWe implement Deepkey with a live deployment in our university and conduct\nextensive empirical experiments to study its technical feasibility in practice.\nDeepKey achieves the False Acceptance Rate (FAR) and the False Rejection Rate\n(FRR) of 0 and 1.0%, respectively. The preliminary results demonstrate that\nDeepkey is feasible, show consistent superior performance compared to a set of\nmethods, and has the potential to be applied to the authentication deployment\nin real world settings. \n\n"}
{"id": "1706.01807", "contents": "Title: GAN and VAE from an Optimal Transport Point of View Abstract: This short article revisits some of the ideas introduced in arXiv:1701.07875\nand arXiv:1705.07642 in a simple setup. This sheds some lights on the\nconnexions between Variational Autoencoders (VAE), Generative Adversarial\nNetworks (GAN) and Minimum Kantorovitch Estimators (MKE). \n\n"}
{"id": "1706.02216", "contents": "Title: Inductive Representation Learning on Large Graphs Abstract: Low-dimensional embeddings of nodes in large graphs have proved extremely\nuseful in a variety of prediction tasks, from content recommendation to\nidentifying protein functions. However, most existing approaches require that\nall nodes in the graph are present during training of the embeddings; these\nprevious approaches are inherently transductive and do not naturally generalize\nto unseen nodes. Here we present GraphSAGE, a general, inductive framework that\nleverages node feature information (e.g., text attributes) to efficiently\ngenerate node embeddings for previously unseen data. Instead of training\nindividual embeddings for each node, we learn a function that generates\nembeddings by sampling and aggregating features from a node's local\nneighborhood. Our algorithm outperforms strong baselines on three inductive\nnode-classification benchmarks: we classify the category of unseen nodes in\nevolving information graphs based on citation and Reddit post data, and we show\nthat our algorithm generalizes to completely unseen graphs using a multi-graph\ndataset of protein-protein interactions. \n\n"}
{"id": "1706.02262", "contents": "Title: InfoVAE: Information Maximizing Variational Autoencoders Abstract: A key advance in learning generative models is the use of amortized inference\ndistributions that are jointly trained with the models. We find that existing\ntraining objectives for variational autoencoders can lead to inaccurate\namortized inference distributions and, in some cases, improving the objective\nprovably degrades the inference quality. In addition, it has been observed that\nvariational autoencoders tend to ignore the latent variables when combined with\na decoding distribution that is too flexible. We again identify the cause in\nexisting training criteria and propose a new class of objectives (InfoVAE) that\nmitigate these problems. We show that our model can significantly improve the\nquality of the variational posterior and can make effective use of the latent\nfeatures regardless of the flexibility of the decoding distribution. Through\nextensive qualitative and quantitative analyses, we demonstrate that our models\noutperform competing approaches on multiple performance metrics. \n\n"}
{"id": "1706.02815", "contents": "Title: From Bayesian Sparsity to Gated Recurrent Nets Abstract: The iterations of many first-order algorithms, when applied to minimizing\ncommon regularized regression functions, often resemble neural network layers\nwith pre-specified weights. This observation has prompted the development of\nlearning-based approaches that purport to replace these iterations with\nenhanced surrogates forged as DNN models from available training data. For\nexample, important NP-hard sparse estimation problems have recently benefitted\nfrom this genre of upgrade, with simple feedforward or recurrent networks\nousting proximal gradient-based iterations. Analogously, this paper\ndemonstrates that more powerful Bayesian algorithms for promoting sparsity,\nwhich rely on complex multi-loop majorization-minimization techniques, mirror\nthe structure of more sophisticated long short-term memory (LSTM) networks, or\nalternative gated feedback networks previously designed for sequence\nprediction. As part of this development, we examine the parallels between\nlatent variable trajectories operating across multiple time-scales during\noptimization, and the activations within deep network structures designed to\nadaptively model such characteristic sequences. The resulting insights lead to\na novel sparse estimation system that, when granted training data, can estimate\noptimal solutions efficiently in regimes where other algorithms fail, including\npractical direction-of-arrival (DOA) and 3D geometry recovery problems. The\nunderlying principles we expose are also suggestive of a learning process for a\nricher class of multi-loop algorithms in other domains. \n\n"}
{"id": "1706.03581", "contents": "Title: Enriched Deep Recurrent Visual Attention Model for Multiple Object\n  Recognition Abstract: We design an Enriched Deep Recurrent Visual Attention Model (EDRAM) - an\nimproved attention-based architecture for multiple object recognition. The\nproposed model is a fully differentiable unit that can be optimized end-to-end\nby using Stochastic Gradient Descent (SGD). The Spatial Transformer (ST) was\nemployed as visual attention mechanism which allows to learn the geometric\ntransformation of objects within images. With the combination of the Spatial\nTransformer and the powerful recurrent architecture, the proposed EDRAM can\nlocalize and recognize objects simultaneously. EDRAM has been evaluated on two\npublicly available datasets including MNIST Cluttered (with 70K cluttered\ndigits) and SVHN (with up to 250k real world images of house numbers).\nExperiments show that it obtains superior performance as compared with the\nstate-of-the-art models. \n\n"}
{"id": "1706.03850", "contents": "Title: Adversarial Feature Matching for Text Generation Abstract: The Generative Adversarial Network (GAN) has achieved great success in\ngenerating realistic (real-valued) synthetic data. However, convergence issues\nand difficulties dealing with discrete data hinder the applicability of GAN to\ntext. We propose a framework for generating realistic text via adversarial\ntraining. We employ a long short-term memory network as generator, and a\nconvolutional network as discriminator. Instead of using the standard objective\nof GAN, we propose matching the high-dimensional latent feature distributions\nof real and synthetic sentences, via a kernelized discrepancy metric. This\neases adversarial training by alleviating the mode-collapsing problem. Our\nexperiments show superior performance in quantitative evaluation, and\ndemonstrate that our model can generate realistic-looking sentences. \n\n"}
{"id": "1706.04371", "contents": "Title: A survey of dimensionality reduction techniques based on random\n  projection Abstract: Dimensionality reduction techniques play important roles in the analysis of\nbig data. Traditional dimensionality reduction approaches, such as principal\ncomponent analysis (PCA) and linear discriminant analysis (LDA), have been\nstudied extensively in the past few decades. However, as the dimensionality of\ndata increases, the computational cost of traditional dimensionality reduction\nmethods grows exponentially, and the computation becomes prohibitively\nintractable. These drawbacks have triggered the development of random\nprojection (RP) techniques, which map high-dimensional data onto a\nlow-dimensional subspace with extremely reduced time cost. However, the RP\ntransformation matrix is generated without considering the intrinsic structure\nof the original data and usually leads to relatively high distortion.\nTherefore, in recent years, methods based on RP have been proposed to address\nthis problem. In this paper, we summarize the methods used in different\nsituations to help practitioners to employ the proper techniques for their\nspecific applications. Meanwhile, we enumerate the benefits and limitations of\nthe various methods and provide further references for researchers to develop\nnovel RP-based approaches. \n\n"}
{"id": "1706.04599", "contents": "Title: On Calibration of Modern Neural Networks Abstract: Confidence calibration -- the problem of predicting probability estimates\nrepresentative of the true correctness likelihood -- is important for\nclassification models in many applications. We discover that modern neural\nnetworks, unlike those from a decade ago, are poorly calibrated. Through\nextensive experiments, we observe that depth, width, weight decay, and Batch\nNormalization are important factors influencing calibration. We evaluate the\nperformance of various post-processing calibration methods on state-of-the-art\narchitectures with image and document classification datasets. Our analysis and\nexperiments not only offer insights into neural network learning, but also\nprovide a simple and straightforward recipe for practical settings: on most\ndatasets, temperature scaling -- a single-parameter variant of Platt Scaling --\nis surprisingly effective at calibrating predictions. \n\n"}
{"id": "1706.04983", "contents": "Title: FreezeOut: Accelerate Training by Progressively Freezing Layers Abstract: The early layers of a deep neural net have the fewest parameters, but take up\nthe most computation. In this extended abstract, we propose to only train the\nhidden layers for a set portion of the training run, freezing them out\none-by-one and excluding them from the backward pass. Through experiments on\nCIFAR, we empirically demonstrate that FreezeOut yields savings of up to 20%\nwall-clock time during training with 3% loss in accuracy for DenseNets, a 20%\nspeedup without loss of accuracy for ResNets, and no improvement for VGG\nnetworks. Our code is publicly available at\nhttps://github.com/ajbrock/FreezeOut \n\n"}
{"id": "1706.05378", "contents": "Title: A framework for Multi-A(rmed)/B(andit) testing with online FDR control Abstract: We propose an alternative framework to existing setups for controlling false\nalarms when multiple A/B tests are run over time. This setup arises in many\npractical applications, e.g. when pharmaceutical companies test new treatment\noptions against control pills for different diseases, or when internet\ncompanies test their default webpages versus various alternatives over time.\nOur framework proposes to replace a sequence of A/B tests by a sequence of\nbest-arm MAB instances, which can be continuously monitored by the data\nscientist. When interleaving the MAB tests with an an online false discovery\nrate (FDR) algorithm, we can obtain the best of both worlds: low sample\ncomplexity and any time online FDR control. Our main contributions are: (i) to\npropose reasonable definitions of a null hypothesis for MAB instances; (ii) to\ndemonstrate how one can derive an always-valid sequential p-value that allows\ncontinuous monitoring of each MAB test; and (iii) to show that using rejection\nthresholds of online-FDR algorithms as the confidence levels for the MAB\nalgorithms results in both sample-optimality, high power and low FDR at any\npoint in time. We run extensive simulations to verify our claims, and also\nreport results on real data collected from the New Yorker Cartoon Caption\ncontest. \n\n"}
{"id": "1706.05598", "contents": "Title: On the Optimization Landscape of Tensor Decompositions Abstract: Non-convex optimization with local search heuristics has been widely used in\nmachine learning, achieving many state-of-art results. It becomes increasingly\nimportant to understand why they can work for these NP-hard problems on typical\ndata. The landscape of many objective functions in learning has been\nconjectured to have the geometric property that \"all local optima are\n(approximately) global optima\", and thus they can be solved efficiently by\nlocal search algorithms. However, establishing such property can be very\ndifficult.\n  In this paper, we analyze the optimization landscape of the random\nover-complete tensor decomposition problem, which has many applications in\nunsupervised learning, especially in learning latent variable models. In\npractice, it can be efficiently solved by gradient ascent on a non-convex\nobjective. We show that for any small constant $\\epsilon > 0$, among the set of\npoints with function values $(1+\\epsilon)$-factor larger than the expectation\nof the function, all the local maxima are approximate global maxima.\nPreviously, the best-known result only characterizes the geometry in small\nneighborhoods around the true components. Our result implies that even with an\ninitialization that is barely better than the random guess, the gradient ascent\nalgorithm is guaranteed to solve this problem.\n  Our main technique uses Kac-Rice formula and random matrix theory. To our\nbest knowledge, this is the first time when Kac-Rice formula is successfully\napplied to counting the number of local minima of a highly-structured random\npolynomial with dependent coefficients. \n\n"}
{"id": "1706.06066", "contents": "Title: On Quadratic Convergence of DC Proximal Newton Algorithm for Nonconvex\n  Sparse Learning in High Dimensions Abstract: We propose a DC proximal Newton algorithm for solving nonconvex regularized\nsparse learning problems in high dimensions. Our proposed algorithm integrates\nthe proximal Newton algorithm with multi-stage convex relaxation based on the\ndifference of convex (DC) programming, and enjoys both strong computational and\nstatistical guarantees. Specifically, by leveraging a sophisticated\ncharacterization of sparse modeling structures/assumptions (i.e., local\nrestricted strong convexity and Hessian smoothness), we prove that within each\nstage of convex relaxation, our proposed algorithm achieves (local) quadratic\nconvergence, and eventually obtains a sparse approximate local optimum with\noptimal statistical properties after only a few convex relaxations. Numerical\nexperiments are provided to support our theory. \n\n"}
{"id": "1706.07001", "contents": "Title: Improved Optimization of Finite Sums with Minibatch Stochastic Variance\n  Reduced Proximal Iterations Abstract: We present novel minibatch stochastic optimization methods for empirical risk\nminimization problems, the methods efficiently leverage variance reduced\nfirst-order and sub-sampled higher-order information to accelerate the\nconvergence speed. For quadratic objectives, we prove improved iteration\ncomplexity over state-of-the-art under reasonable assumptions. We also provide\nempirical evidence of the advantages of our method compared to existing\napproaches in the literature. \n\n"}
{"id": "1706.07853", "contents": "Title: Loom: Exploiting Weight and Activation Precisions to Accelerate\n  Convolutional Neural Networks Abstract: Loom (LM), a hardware inference accelerator for Convolutional Neural Networks\n(CNNs) is presented. In LM every bit of data precision that can be saved\ntranslates to proportional performance gains. Specifically, for convolutional\nlayers LM's execution time scales inversely proportionally with the precisions\nof both weights and activations. For fully-connected layers LM's performance\nscales inversely proportionally with the precision of the weights. LM targets\narea- and bandwidth-constrained System-on-a-Chip designs such as those found on\nmobile devices that cannot afford the multi-megabyte buffers that would be\nneeded to store each layer on-chip. Accordingly, given a data bandwidth budget,\nLM boosts energy efficiency and performance over an equivalent bit-parallel\naccelerator. For both weights and activations LM can exploit profile-derived\nperlayer precisions. However, at runtime LM further trims activation precisions\nat a much smaller than a layer granularity. Moreover, it can naturally exploit\nweight precision variability at a smaller granularity than a layer. On average,\nacross several image classification CNNs and for a configuration that can\nperform the equivalent of 128 16b x 16b multiply-accumulate operations per\ncycle LM outperforms a state-of-the-art bit-parallel accelerator [1] by 4.38x\nwithout any loss in accuracy while being 3.54x more energy efficient. LM can\ntrade-off accuracy for additional improvements in execution performance and\nenergy efficiency and compares favorably to an accelerator that targeted only\nactivation precisions. We also study 2- and 4-bit LM variants and find the the\n2-bit per cycle variant is the most energy efficient. \n\n"}
{"id": "1706.09186", "contents": "Title: Stochastic Bandit Models for Delayed Conversions Abstract: Online advertising and product recommendation are important domains of\napplications for multi-armed bandit methods. In these fields, the reward that\nis immediately available is most often only a proxy for the actual outcome of\ninterest, which we refer to as a conversion. For instance, in web advertising,\nclicks can be observed within a few seconds after an ad display but the\ncorresponding sale --if any-- will take hours, if not days to happen. This\npaper proposes and investigates a new stochas-tic multi-armed bandit model in\nthe framework proposed by Chapelle (2014) --based on empirical studies in the\nfield of web advertising-- in which each action may trigger a future reward\nthat will then happen with a stochas-tic delay. We assume that the probability\nof conversion associated with each action is unknown while the distribution of\nthe conversion delay is known, distinguishing between the (idealized) case\nwhere the conversion events may be observed whatever their delay and the more\nrealistic setting in which late conversions are censored. We provide\nperformance lower bounds as well as two simple but efficient algorithms based\non the UCB and KLUCB frameworks. The latter algorithm, which is preferable when\nconversion rates are low, is based on a Poissonization argument, of independent\ninterest in other settings where aggregation of Bernoulli observations with\ndifferent success probabilities is required. \n\n"}
{"id": "1707.00075", "contents": "Title: Data Decisions and Theoretical Implications when Adversarially Learning\n  Fair Representations Abstract: How can we learn a classifier that is \"fair\" for a protected or sensitive\ngroup, when we do not know if the input to the classifier belongs to the\nprotected group? How can we train such a classifier when data on the protected\ngroup is difficult to attain? In many settings, finding out the sensitive input\nattribute can be prohibitively expensive even during model training, and\nsometimes impossible during model serving. For example, in recommender systems,\nif we want to predict if a user will click on a given recommendation, we often\ndo not know many attributes of the user, e.g., race or age, and many attributes\nof the content are hard to determine, e.g., the language or topic. Thus, it is\nnot feasible to use a different classifier calibrated based on knowledge of the\nsensitive attribute.\n  Here, we use an adversarial training procedure to remove information about\nthe sensitive attribute from the latent representation learned by a neural\nnetwork. In particular, we study how the choice of data for the adversarial\ntraining effects the resulting fairness properties. We find two interesting\nresults: a small amount of data is needed to train these adversarial models,\nand the data distribution empirically drives the adversary's notion of\nfairness. \n\n"}
{"id": "1707.00683", "contents": "Title: Modulating early visual processing by language Abstract: It is commonly assumed that language refers to high-level visual concepts\nwhile leaving low-level visual processing unaffected. This view dominates the\ncurrent literature in computational models for language-vision tasks, where\nvisual and linguistic input are mostly processed independently before being\nfused into a single representation. In this paper, we deviate from this classic\npipeline and propose to modulate the \\emph{entire visual processing} by\nlinguistic input. Specifically, we condition the batch normalization parameters\nof a pretrained residual network (ResNet) on a language embedding. This\napproach, which we call MOdulated RESnet (\\MRN), significantly improves strong\nbaselines on two visual question answering tasks. Our ablation study shows that\nmodulating from the early stages of the visual processing is beneficial. \n\n"}
{"id": "1707.01250", "contents": "Title: Graph Based Recommendations: From Data Representation to Feature\n  Extraction and Application Abstract: Modeling users for the purpose of identifying their preferences and then\npersonalizing services on the basis of these models is a complex task,\nprimarily due to the need to take into consideration various explicit and\nimplicit signals, missing or uncertain information, contextual aspects, and\nmore. In this study, a novel generic approach for uncovering latent preference\npatterns from user data is proposed and evaluated. The approach relies on\nrepresenting the data using graphs, and then systematically extracting\ngraph-based features and using them to enrich the original user models. The\nextracted features encapsulate complex relationships between users, items, and\nmetadata. The enhanced user models can then serve as an input to any\nrecommendation algorithm. The proposed approach is domain-independent\n(demonstrated on data from movies, music, and business recommender systems),\nand is evaluated using several state-of-the-art machine learning methods, on\ndifferent recommendation tasks, and using different evaluation metrics. The\nresults show a unanimous improvement in the recommendation accuracy across\ntasks and domains. In addition, the evaluation provides a deeper analysis\nregarding the performance of the approach in special scenarios, including high\nsparsity and variability of ratings. \n\n"}
{"id": "1707.02392", "contents": "Title: Learning Representations and Generative Models for 3D Point Clouds Abstract: Three-dimensional geometric data offer an excellent domain for studying\nrepresentation learning and generative modeling. In this paper, we look at\ngeometric data represented as point clouds. We introduce a deep AutoEncoder\n(AE) network with state-of-the-art reconstruction quality and generalization\nability. The learned representations outperform existing methods on 3D\nrecognition tasks and enable shape editing via simple algebraic manipulations,\nsuch as semantic part editing, shape analogies and shape interpolation, as well\nas shape completion. We perform a thorough study of different generative models\nincluding GANs operating on the raw point clouds, significantly improved GANs\ntrained in the fixed latent space of our AEs, and Gaussian Mixture Models\n(GMMs). To quantitatively evaluate generative models we introduce measures of\nsample fidelity and diversity based on matchings between sets of point clouds.\nInterestingly, our evaluation of generalization, fidelity and diversity reveals\nthat GMMs trained in the latent space of our AEs yield the best results\noverall. \n\n"}
{"id": "1707.02515", "contents": "Title: A Fast Integrated Planning and Control Framework for Autonomous Driving\n  via Imitation Learning Abstract: For safe and efficient planning and control in autonomous driving, we need a\ndriving policy which can achieve desirable driving quality in long-term horizon\nwith guaranteed safety and feasibility. Optimization-based approaches, such as\nModel Predictive Control (MPC), can provide such optimal policies, but their\ncomputational complexity is generally unacceptable for real-time\nimplementation. To address this problem, we propose a fast integrated planning\nand control framework that combines learning- and optimization-based approaches\nin a two-layer hierarchical structure. The first layer, defined as the \"policy\nlayer\", is established by a neural network which learns the long-term optimal\ndriving policy generated by MPC. The second layer, called the \"execution\nlayer\", is a short-term optimization-based controller that tracks the reference\ntrajecotries given by the \"policy layer\" with guaranteed short-term safety and\nfeasibility. Moreover, with efficient and highly-representative features, a\nsmall-size neural network is sufficient in the \"policy layer\" to handle many\ncomplicated driving scenarios. This renders online imitation learning with\nDataset Aggregation (DAgger) so that the performance of the \"policy layer\" can\nbe improved rapidly and continuously online. Several exampled driving scenarios\nare demonstrated to verify the effectiveness and efficiency of the proposed\nframework. \n\n"}
{"id": "1707.02670", "contents": "Title: Accelerated Stochastic Power Iteration Abstract: Principal component analysis (PCA) is one of the most powerful tools in\nmachine learning. The simplest method for PCA, the power iteration, requires\n$\\mathcal O(1/\\Delta)$ full-data passes to recover the principal component of a\nmatrix with eigen-gap $\\Delta$. Lanczos, a significantly more complex method,\nachieves an accelerated rate of $\\mathcal O(1/\\sqrt{\\Delta})$ passes. Modern\napplications, however, motivate methods that only ingest a subset of available\ndata, known as the stochastic setting. In the online stochastic setting, simple\nalgorithms like Oja's iteration achieve the optimal sample complexity $\\mathcal\nO(\\sigma^2/\\Delta^2)$. Unfortunately, they are fully sequential, and also\nrequire $\\mathcal O(\\sigma^2/\\Delta^2)$ iterations, far from the $\\mathcal\nO(1/\\sqrt{\\Delta})$ rate of Lanczos. We propose a simple variant of the power\niteration with an added momentum term, that achieves both the optimal sample\nand iteration complexity. In the full-pass setting, standard analysis shows\nthat momentum achieves the accelerated rate, $\\mathcal O(1/\\sqrt{\\Delta})$. We\ndemonstrate empirically that naively applying momentum to a stochastic method,\ndoes not result in acceleration. We perform a novel, tight variance analysis\nthat reveals the \"breaking-point variance\" beyond which this acceleration does\nnot occur. By combining this insight with modern variance reduction techniques,\nwe construct stochastic PCA algorithms, for the online and offline setting,\nthat achieve an accelerated iteration complexity $\\mathcal O(1/\\sqrt{\\Delta})$.\nDue to the embarassingly parallel nature of our methods, this acceleration\ntranslates directly to wall-clock time if deployed in a parallel environment.\nOur approach is very general, and applies to many non-convex optimization\nproblems that can now be accelerated using the same technique. \n\n"}
{"id": "1707.03311", "contents": "Title: Similarity Search Over Graphs Using Localized Spectral Analysis Abstract: This paper provides a new similarity detection algorithm. Given an input set\nof multi-dimensional data points, where each data point is assumed to be\nmulti-dimensional, and an additional reference data point for similarity\nfinding, the algorithm uses kernel method that embeds the data points into a\nlow dimensional manifold. Unlike other kernel methods, which consider the\nentire data for the embedding, our method selects a specific set of kernel\neigenvectors. The eigenvectors are chosen to separate between the data points\nand the reference data point so that similar data points can be easily\nidentified as being distinct from most of the members in the dataset. \n\n"}
{"id": "1707.03351", "contents": "Title: Solving parametric PDE problems with artificial neural networks Abstract: The curse of dimensionality is commonly encountered in numerical partial\ndifferential equations (PDE), especially when uncertainties have to be modeled\ninto the equations as random coefficients. However, very often the variability\nof physical quantities derived from a PDE can be captured by a few features on\nthe space of the coefficient fields. Based on such an observation, we propose\nusing a neural-network (NN) based method to parameterize the physical quantity\nof interest as a function of input coefficients. The representability of such\nquantity using a neural-network can be justified by viewing the neural-network\nas performing time evolution to find the solutions to the PDE. We further\ndemonstrate the simplicity and accuracy of the approach through notable\nexamples of PDEs in engineering and physics. \n\n"}
{"id": "1707.04585", "contents": "Title: The Reversible Residual Network: Backpropagation Without Storing\n  Activations Abstract: Deep residual networks (ResNets) have significantly pushed forward the\nstate-of-the-art on image classification, increasing in performance as networks\ngrow both deeper and wider. However, memory consumption becomes a bottleneck,\nas one needs to store the activations in order to calculate gradients using\nbackpropagation. We present the Reversible Residual Network (RevNet), a variant\nof ResNets where each layer's activations can be reconstructed exactly from the\nnext layer's. Therefore, the activations for most layers need not be stored in\nmemory during backpropagation. We demonstrate the effectiveness of RevNets on\nCIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification\naccuracy to equally-sized ResNets, even though the activation storage\nrequirements are independent of depth. \n\n"}
{"id": "1707.06231", "contents": "Title: From Bach to the Beatles: The simulation of human tonal expectation\n  using ecologically-trained predictive models Abstract: Tonal structure is in part conveyed by statistical regularities between\nmusical events, and research has shown that computational models reflect tonal\nstructure in music by capturing these regularities in schematic constructs like\npitch histograms. Of the few studies that model the acquisition of perceptual\nlearning from musical data, most have employed self-organizing models that\nlearn a topology of static descriptions of musical contexts. Also, the stimuli\nused to train these models are often symbolic rather than acoustically faithful\nrepresentations of musical material. In this work we investigate whether\nsequential predictive models of musical memory (specifically, recurrent neural\nnetworks), trained on audio from commercial CD recordings, induce tonal\nknowledge in a similar manner to listeners (as shown in behavioral studies in\nmusic perception). Our experiments indicate that various types of recurrent\nneural networks produce musical expectations that clearly convey tonal\nstructure. Furthermore, the results imply that although implicit knowledge of\ntonal structure is a necessary condition for accurate musical expectation, the\nmost accurate predictive models also use other cues beyond the tonal structure\nof the musical context. \n\n"}
{"id": "1707.07785", "contents": "Title: Comparing Aggregators for Relational Probabilistic Models Abstract: Relational probabilistic models have the challenge of aggregation, where one\nvariable depends on a population of other variables. Consider the problem of\npredicting gender from movie ratings; this is challenging because the number of\nmovies per user and users per movie can vary greatly. Surprisingly, aggregation\nis not well understood. In this paper, we show that existing relational models\n(implicitly or explicitly) either use simple numerical aggregators that lose\ngreat amounts of information, or correspond to naive Bayes, logistic\nregression, or noisy-OR that suffer from overconfidence. We propose new simple\naggregators and simple modifications of existing models that empirically\noutperform the existing ones. The intuition we provide on different (existing\nor new) models and their shortcomings plus our empirical findings promise to\nform the foundation for future representations. \n\n"}
{"id": "1707.08475", "contents": "Title: DARLA: Improving Zero-Shot Transfer in Reinforcement Learning Abstract: Domain adaptation is an important open problem in deep reinforcement learning\n(RL). In many scenarios of interest data is hard to obtain, so agents may learn\na source policy in a setting where data is readily available, with the hope\nthat it generalises well to the target domain. We propose a new multi-stage RL\nagent, DARLA (DisentAngled Representation Learning Agent), which learns to see\nbefore learning to act. DARLA's vision is based on learning a disentangled\nrepresentation of the observed environment. Once DARLA can see, it is able to\nacquire source policies that are robust to many domain shifts - even with no\naccess to the target domain. DARLA significantly outperforms conventional\nbaselines in zero-shot domain adaptation scenarios, an effect that holds across\na variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms\n(DQN, A3C and EC). \n\n"}
{"id": "1708.00308", "contents": "Title: SenGen: Sentence Generating Neural Variational Topic Model Abstract: We present a new topic model that generates documents by sampling a topic for\none whole sentence at a time, and generating the words in the sentence using an\nRNN decoder that is conditioned on the topic of the sentence. We argue that\nthis novel formalism will help us not only visualize and model the topical\ndiscourse structure in a document better, but also potentially lead to more\ninterpretable topics since we can now illustrate topics by sampling\nrepresentative sentences instead of bag of words or phrases. We present a\nvariational auto-encoder approach for learning in which we use a factorized\nvariational encoder that independently models the posterior over topical\nmixture vectors of documents using a feed-forward network, and the posterior\nover topic assignments to sentences using an RNN. Our preliminary experiments\non two different datasets indicate early promise, but also expose many\nchallenges that remain to be addressed. \n\n"}
{"id": "1708.01529", "contents": "Title: Learning Model Reparametrizations: Implicit Variational Inference by\n  Fitting MCMC distributions Abstract: We introduce a new algorithm for approximate inference that combines\nreparametrization, Markov chain Monte Carlo and variational methods. We\nconstruct a very flexible implicit variational distribution synthesized by an\narbitrary Markov chain Monte Carlo operation and a deterministic transformation\nthat can be optimized using the reparametrization trick. Unlike current methods\nfor implicit variational inference, our method avoids the computation of log\ndensity ratios and therefore it is easily applicable to arbitrary continuous\nand differentiable models. We demonstrate the proposed algorithm for fitting\nbanana-shaped distributions and for training variational autoencoders. \n\n"}
{"id": "1708.02582", "contents": "Title: Cascade Adversarial Machine Learning Regularized with a Unified\n  Embedding Abstract: Injecting adversarial examples during training, known as adversarial\ntraining, can improve robustness against one-step attacks, but not for unknown\niterative attacks. To address this challenge, we first show iteratively\ngenerated adversarial images easily transfer between networks trained with the\nsame strategy. Inspired by this observation, we propose cascade adversarial\ntraining, which transfers the knowledge of the end results of adversarial\ntraining. We train a network from scratch by injecting iteratively generated\nadversarial images crafted from already defended networks in addition to\none-step adversarial images from the network being trained. We also propose to\nutilize embedding space for both classification and low-level (pixel-level)\nsimilarity learning to ignore unknown pixel level perturbation. During\ntraining, we inject adversarial images without replacing their corresponding\nclean images and penalize the distance between the two embeddings (clean and\nadversarial). Experimental results show that cascade adversarial training\ntogether with our proposed low-level similarity learning efficiently enhances\nthe robustness against iterative attacks, but at the expense of decreased\nrobustness against one-step attacks. We show that combining those two\ntechniques can also improve robustness under the worst case black box attack\nscenario. \n\n"}
{"id": "1708.02977", "contents": "Title: Hierarchically-Attentive RNN for Album Summarization and Storytelling Abstract: We address the problem of end-to-end visual storytelling. Given a photo\nalbum, our model first selects the most representative (summary) photos, and\nthen composes a natural language story for the album. For this task, we make\nuse of the Visual Storytelling dataset and a model composed of three\nhierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album\nphotos, select representative (summary) photos, and compose the story.\nAutomatic and human evaluations show our model achieves better performance on\nselection, generation, and retrieval than baselines. \n\n"}
{"id": "1708.03940", "contents": "Title: Leveraging Sparse and Dense Feature Combinations for Sentiment\n  Classification Abstract: Neural networks are one of the most popular approaches for many natural\nlanguage processing tasks such as sentiment analysis. They often outperform\ntraditional machine learning models and achieve the state-of-art results on\nmost tasks. However, many existing deep learning models are complex, difficult\nto train and provide a limited improvement over simpler methods. We propose a\nsimple, robust and powerful model for sentiment classification. This model\noutperforms many deep learning models and achieves comparable results to other\ndeep learning models with complex architectures on sentiment analysis datasets.\nWe publish the code online. \n\n"}
{"id": "1708.05344", "contents": "Title: SMASH: One-Shot Model Architecture Search through HyperNetworks Abstract: Designing architectures for deep neural networks requires expert knowledge\nand substantial computation time. We propose a technique to accelerate\narchitecture selection by learning an auxiliary HyperNet that generates the\nweights of a main model conditioned on that model's architecture. By comparing\nthe relative validation performance of networks with HyperNet-generated\nweights, we can effectively search over a wide range of architectures at the\ncost of a single training run. To facilitate this search, we develop a flexible\nmechanism based on memory read-writes that allows us to define a wide range of\nnetwork connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as\nspecial cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100,\nSTL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with\nsimilarly-sized hand-designed networks. Our code is available at\nhttps://github.com/ajbrock/SMASH \n\n"}
{"id": "1708.05866", "contents": "Title: A Brief Survey of Deep Reinforcement Learning Abstract: Deep reinforcement learning is poised to revolutionise the field of AI and\nrepresents a step towards building autonomous systems with a higher level\nunderstanding of the visual world. Currently, deep learning is enabling\nreinforcement learning to scale to problems that were previously intractable,\nsuch as learning to play video games directly from pixels. Deep reinforcement\nlearning algorithms are also applied to robotics, allowing control policies for\nrobots to be learned directly from camera inputs in the real world. In this\nsurvey, we begin with an introduction to the general field of reinforcement\nlearning, then progress to the main streams of value-based and policy-based\nmethods. Our survey will cover central algorithms in deep reinforcement\nlearning, including the deep $Q$-network, trust region policy optimisation, and\nasynchronous advantage actor-critic. In parallel, we highlight the unique\nadvantages of deep neural networks, focusing on visual understanding via\nreinforcement learning. To conclude, we describe several current areas of\nresearch within the field. \n\n"}
{"id": "1708.06019", "contents": "Title: A Capacity Scaling Law for Artificial Neural Networks Abstract: We derive the calculation of two critical numbers predicting the behavior of\nperceptron networks. First, we derive the calculation of what we call the\nlossless memory (LM) dimension. The LM dimension is a generalization of the\nVapnik--Chervonenkis (VC) dimension that avoids structured data and therefore\nprovides an upper bound for perfectly fitting almost any training data. Second,\nwe derive what we call the MacKay (MK) dimension. This limit indicates a 50%\nchance of not being able to train a given function. Our derivations are\nperformed by embedding a neural network into Shannon's communication model\nwhich allows to interpret the two points as capacities measured in bits. We\npresent a proof and practical experiments that validate our upper bounds with\nrepeatable experiments using different network configurations, diverse\nimplementations, varying activation functions, and several learning algorithms.\nThe bottom line is that the two capacity points scale strictly linear with the\nnumber of weights. Among other practical applications, our result allows to\ncompare and benchmark different neural network implementations independent of a\nconcrete learning task. Our results provide insight into the capabilities and\nlimits of neural networks and generate valuable know how for experimental\ndesign decisions. \n\n"}
{"id": "1708.06257", "contents": "Title: A Flow Model of Neural Networks Abstract: Based on a natural connection between ResNet and transport equation or its\ncharacteristic equation, we propose a continuous flow model for both ResNet and\nplain net. Through this continuous model, a ResNet can be explicitly\nconstructed as a refinement of a plain net. The flow model provides an\nalternative perspective to understand phenomena in deep neural networks, such\nas why it is necessary and sufficient to use 2-layer blocks in ResNets, why\ndeeper is better, and why ResNets are even deeper, and so on. It also opens a\ngate to bring in more tools from the huge area of differential equations. \n\n"}
{"id": "1708.06850", "contents": "Title: Learning Deep Neural Network Representations for Koopman Operators of\n  Nonlinear Dynamical Systems Abstract: The Koopman operator has recently garnered much attention for its value in\ndynamical systems analysis and data-driven model discovery. However, its\napplication has been hindered by the computational complexity of extended\ndynamic mode decomposition; this requires a combinatorially large basis set to\nadequately describe many nonlinear systems of interest, e.g. cyber-physical\ninfrastructure systems, biological networks, social systems, and fluid\ndynamics. Often the dictionaries generated for these problems are manually\ncurated, requiring domain-specific knowledge and painstaking tuning. In this\npaper we introduce a deep learning framework for learning Koopman operators of\nnonlinear dynamical systems. We show that this novel method automatically\nselects efficient deep dictionaries, outperforming state-of-the-art methods. We\nbenchmark this method on partially observed nonlinear systems, including the\nglycolytic oscillator and show it is able to predict quantitatively 100 steps\ninto the future, using only a single timepoint, and qualitative oscillatory\nbehavior 400 steps into the future. \n\n"}
{"id": "1708.08559", "contents": "Title: DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous\n  Cars Abstract: Recent advances in Deep Neural Networks (DNNs) have led to the development of\nDNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can\ndrive without any human intervention. Most major manufacturers including Tesla,\nGM, Ford, BMW, and Waymo/Google are working on building and testing different\ntypes of autonomous vehicles. The lawmakers of several US states including\nCalifornia, Texas, and New York have passed new legislation to fast-track the\nprocess of testing and deployment of autonomous vehicles on their roads.\n  However, despite their spectacular progress, DNNs, just like traditional\nsoftware, often demonstrate incorrect or unexpected corner case behaviors that\ncan lead to potentially fatal collisions. Several such real-world accidents\ninvolving autonomous cars have already happened including one which resulted in\na fatality. Most existing testing techniques for DNN-driven vehicles are\nheavily dependent on the manual collection of test data under different driving\nconditions which become prohibitively expensive as the number of test\nconditions increases.\n  In this paper, we design, implement and evaluate DeepTest, a systematic\ntesting tool for automatically detecting erroneous behaviors of DNN-driven\nvehicles that can potentially lead to fatal crashes. First, our tool is\ndesigned to automatically generated test cases leveraging real-world changes in\ndriving conditions like rain, fog, lighting conditions, etc. DeepTest\nsystematically explores different parts of the DNN logic by generating test\ninputs that maximize the numbers of activated neurons. DeepTest found thousands\nof erroneous behaviors under different realistic driving conditions (e.g.,\nblurring, rain, fog, etc.) many of which lead to potentially fatal crashes in\nthree top performing DNNs in the Udacity self-driving car challenge. \n\n"}
{"id": "1708.08755", "contents": "Title: Multi-task Neural Networks for Personalized Pain Recognition from\n  Physiological Signals Abstract: Pain is a complex and subjective experience that poses a number of\nmeasurement challenges. While self-report by the patient is viewed as the gold\nstandard of pain assessment, this approach fails when patients cannot verbally\ncommunicate pain intensity or lack normal mental abilities. Here, we present a\npain intensity measurement method based on physiological signals. Specifically,\nwe implement a multi-task learning approach based on neural networks that\naccounts for individual differences in pain responses while still leveraging\ndata from across the population. We test our method in a dataset containing\nmulti-modal physiological responses to nociceptive pain. \n\n"}
{"id": "1709.00149", "contents": "Title: Learning what to read: Focused machine reading Abstract: Recent efforts in bioinformatics have achieved tremendous progress in the\nmachine reading of biomedical literature, and the assembly of the extracted\nbiochemical interactions into large-scale models such as protein signaling\npathways. However, batch machine reading of literature at today's scale (PubMed\nalone indexes over 1 million papers per year) is unfeasible due to both cost\nand processing overhead. In this work, we introduce a focused reading approach\nto guide the machine reading of biomedical literature towards what literature\nshould be read to answer a biomedical query as efficiently as possible. We\nintroduce a family of algorithms for focused reading, including an intuitive,\nstrong baseline, and a second approach which uses a reinforcement learning (RL)\nframework that learns when to explore (widen the search) or exploit (narrow\nit). We demonstrate that the RL approach is capable of answering more queries\nthan the baseline, while being more efficient, i.e., reading fewer documents. \n\n"}
{"id": "1709.00911", "contents": "Title: Neural Networks for Safety-Critical Applications - Challenges,\n  Experiments and Perspectives Abstract: We propose a methodology for designing dependable Artificial Neural Networks\n(ANN) by extending the concepts of understandability, correctness, and validity\nthat are crucial ingredients in existing certification standards. We apply the\nconcept in a concrete case study in designing a high-way ANN-based motion\npredictor to guarantee safety properties such as impossibility for the ego\nvehicle to suggest moving to the right lane if there exists another vehicle on\nits right. \n\n"}
{"id": "1709.01572", "contents": "Title: Sequence Prediction with Neural Segmental Models Abstract: Segments that span contiguous parts of inputs, such as phonemes in speech,\nnamed entities in sentences, actions in videos, occur frequently in sequence\nprediction problems. Segmental models, a class of models that explicitly\nhypothesizes segments, have allowed the exploration of rich segment features\nfor sequence prediction. However, segmental models suffer from slow decoding,\nhampering the use of computationally expensive features.\n  In this thesis, we introduce discriminative segmental cascades, a multi-pass\ninference framework that allows us to improve accuracy by adding higher-order\nfeatures and neural segmental features while maintaining efficiency. We also\nshow that instead of including more features to obtain better accuracy,\nsegmental cascades can be used to speed up training and decoding.\n  Segmental models, similarly to conventional speech recognizers, are typically\ntrained in multiple stages. In the first stage, a frame classifier is trained\nwith manual alignments, and then in the second stage, segmental models are\ntrained with manual alignments and the out- puts of the frame classifier.\nHowever, obtaining manual alignments are time-consuming and expensive. We\nexplore end-to-end training for segmental models with various loss functions,\nand show how end-to-end training with marginal log loss can eliminate the need\nfor detailed manual alignments.\n  We draw the connections between the marginal log loss and a popular\nend-to-end training approach called connectionist temporal classification. We\npresent a unifying framework for various end-to-end graph search-based models,\nsuch as hidden Markov models, connectionist temporal classification, and\nsegmental models. Finally, we discuss possible extensions of segmental models\nto large-vocabulary sequence prediction tasks. \n\n"}
{"id": "1709.01779", "contents": "Title: Deep learning from crowds Abstract: Over the last few years, deep learning has revolutionized the field of\nmachine learning by dramatically improving the state-of-the-art in various\ndomains. However, as the size of supervised artificial neural networks grows,\ntypically so does the need for larger labeled datasets. Recently, crowdsourcing\nhas established itself as an efficient and cost-effective solution for labeling\nlarge sets of data in a scalable manner, but it often requires aggregating\nlabels from multiple noisy contributors with different levels of expertise. In\nthis paper, we address the problem of learning deep neural networks from\ncrowds. We begin by describing an EM algorithm for jointly learning the\nparameters of the network and the reliabilities of the annotators. Then, a\nnovel general-purpose crowd layer is proposed, which allows us to train deep\nneural networks end-to-end, directly from the noisy labels of multiple\nannotators, using only backpropagation. We empirically show that the proposed\napproach is able to internally capture the reliability and biases of different\nannotators and achieve new state-of-the-art results for various crowdsourced\ndatasets across different settings, namely classification, regression and\nsequence labeling. \n\n"}
{"id": "1709.01846", "contents": "Title: Symmetric Variational Autoencoder and Connections to Adversarial\n  Learning Abstract: A new form of the variational autoencoder (VAE) is proposed, based on the\nsymmetric Kullback-Leibler divergence. It is demonstrated that learning of the\nresulting symmetric VAE (sVAE) has close connections to previously developed\nadversarial-learning methods. This relationship helps unify the previously\ndistinct techniques of VAE and adversarially learning, and provides insights\nthat allow us to ameliorate shortcomings with some previously developed\nadversarial methods. In addition to an analysis that motivates and explains the\nsVAE, an extensive set of experiments validate the utility of the approach. \n\n"}
{"id": "1709.03942", "contents": "Title: Deep Reinforcement Learning with Surrogate Agent-Environment Interface Abstract: In this paper, we propose surrogate agent-environment interface (SAEI) in\nreinforcement learning. We also state that learning based on probability\nsurrogate agent-environment interface provides optimal policy of task\nagent-environment interface. We introduce surrogate probability action and\ndevelop the probability surrogate action deterministic policy gradient (PSADPG)\nalgorithm based on SAEI. This algorithm enables continuous control of discrete\naction. The experiments show PSADPG achieves the performance of DQN in certain\ntasks with the stochastic optimal policy nature in the initial training stage. \n\n"}
{"id": "1709.04136", "contents": "Title: Recursive Exponential Weighting for Online Non-convex Optimization Abstract: In this paper, we investigate the online non-convex optimization problem\nwhich generalizes the classic {online convex optimization problem by relaxing\nthe convexity assumption on the cost function.\n  For this type of problem, the classic exponential weighting online algorithm\nhas recently been shown to attain a sub-linear regret of $O(\\sqrt{T\\log T})$.\n  In this paper, we introduce a novel recursive structure to the online\nalgorithm to define a recursive exponential weighting algorithm that attains a\nregret of $O(\\sqrt{T})$, matching the well-known regret lower bound.\n  To the best of our knowledge, this is the first online algorithm with\nprovable $O(\\sqrt{T})$ regret for the online non-convex optimization problem. \n\n"}
{"id": "1709.05666", "contents": "Title: On Inductive Abilities of Latent Factor Models for Relational Learning Abstract: Latent factor models are increasingly popular for modeling multi-relational\nknowledge graphs. By their vectorial nature, it is not only hard to interpret\nwhy this class of models works so well, but also to understand where they fail\nand how they might be improved. We conduct an experimental survey of\nstate-of-the-art models, not towards a purely comparative end, but as a means\nto get insight about their inductive abilities. To assess the strengths and\nweaknesses of each model, we create simple tasks that exhibit first, atomic\nproperties of binary relations, and then, common inter-relational inference\nthrough synthetic genealogies. Based on these experimental results, we propose\nnew research directions to improve on existing models. \n\n"}
{"id": "1709.05804", "contents": "Title: Minimal Effort Back Propagation for Convolutional Neural Networks Abstract: As traditional neural network consumes a significant amount of computing\nresources during back propagation, \\citet{Sun2017mePropSB} propose a simple yet\neffective technique to alleviate this problem. In this technique, only a small\nsubset of the full gradients are computed to update the model parameters. In\nthis paper we extend this technique into the Convolutional Neural Network(CNN)\nto reduce calculation in back propagation, and the surprising results verify\nits validity in CNN: only 5\\% of the gradients are passed back but the model\nstill achieves the same effect as the traditional CNN, or even better. We also\nshow that the top-$k$ selection of gradients leads to a sparse calculation in\nback propagation, which may bring significant computational benefits for high\ncomputational complexity of convolution operation in CNN. \n\n"}
{"id": "1709.05963", "contents": "Title: Machine learning approximation algorithms for high-dimensional fully\n  nonlinear partial differential equations and second-order backward stochastic\n  differential equations Abstract: High-dimensional partial differential equations (PDE) appear in a number of\nmodels from the financial industry, such as in derivative pricing models,\ncredit valuation adjustment (CVA) models, or portfolio optimization models. The\nPDEs in such applications are high-dimensional as the dimension corresponds to\nthe number of financial assets in a portfolio. Moreover, such PDEs are often\nfully nonlinear due to the need to incorporate certain nonlinear phenomena in\nthe model such as default risks, transaction costs, volatility uncertainty\n(Knightian uncertainty), or trading constraints in the model. Such\nhigh-dimensional fully nonlinear PDEs are exceedingly difficult to solve as the\ncomputational effort for standard approximation methods grows exponentially\nwith the dimension. In this work we propose a new method for solving\nhigh-dimensional fully nonlinear second-order PDEs. Our method can in\nparticular be used to sample from high-dimensional nonlinear expectations. The\nmethod is based on (i) a connection between fully nonlinear second-order PDEs\nand second-order backward stochastic differential equations (2BSDEs), (ii) a\nmerged formulation of the PDE and the 2BSDE problem, (iii) a temporal forward\ndiscretization of the 2BSDE and a spatial approximation via deep neural nets,\nand (iv) a stochastic gradient descent-type optimization procedure. Numerical\nresults obtained using ${\\rm T{\\small ENSOR}F{\\small LOW}}$ in ${\\rm P{\\small\nYTHON}}$ illustrate the efficiency and the accuracy of the method in the cases\nof a $100$-dimensional Black-Scholes-Barenblatt equation, a $100$-dimensional\nHamilton-Jacobi-Bellman equation, and a nonlinear expectation of a $ 100\n$-dimensional $ G $-Brownian motion. \n\n"}
{"id": "1709.07200", "contents": "Title: Temporal Multimodal Fusion for Video Emotion Classification in the Wild Abstract: This paper addresses the question of emotion classification. The task\nconsists in predicting emotion labels (taken among a set of possible labels)\nbest describing the emotions contained in short video clips. Building on a\nstandard framework -- lying in describing videos by audio and visual features\nused by a supervised classifier to infer the labels -- this paper investigates\nseveral novel directions. First of all, improved face descriptors based on 2D\nand 3D Convo-lutional Neural Networks are proposed. Second, the paper explores\nseveral fusion methods, temporal and multimodal, including a novel hierarchical\nmethod combining features and scores. In addition, we carefully reviewed the\ndifferent stages of the pipeline and designed a CNN architecture adapted to the\ntask; this is important as the size of the training set is small compared to\nthe difficulty of the problem, making generalization difficult. The so-obtained\nmodel ranked 4th at the 2017 Emotion in the Wild challenge with the accuracy of\n58.8 %. \n\n"}
{"id": "1709.07772", "contents": "Title: Probabilistic Synchronous Parallel Abstract: Most machine learning and deep neural network algorithms rely on certain\niterative algorithms to optimise their utility/cost functions, e.g. Stochastic\nGradient Descent. In distributed learning, the networked nodes have to work\ncollaboratively to update the model parameters, and the way how they proceed is\nreferred to as synchronous parallel design (or barrier control). Synchronous\nparallel protocol is the building block of any distributed learning framework,\nand its design has direct impact on the performance and scalability of the\nsystem.\n  In this paper, we propose a new barrier control technique - Probabilistic\nSynchronous Parallel (PSP). Com- paring to the previous Bulk Synchronous\nParallel (BSP), Stale Synchronous Parallel (SSP), and (Asynchronous Parallel)\nASP, the proposed solution e ectively improves both the convergence speed and\nthe scalability of the SGD algorithm by introducing a sampling primitive into\nthe system. Moreover, we also show that the sampling primitive can be applied\natop of the existing barrier control mechanisms to derive fully distributed\nPSP-based synchronous parallel.\n  We not only provide a thorough theoretical analysis1 on the convergence of\nPSP-based SGD algorithm, but also implement a full-featured distributed\nlearning framework called Actor and perform intensive evaluation atop of it. \n\n"}
{"id": "1709.07796", "contents": "Title: On overfitting and asymptotic bias in batch reinforcement learning with\n  partial observability Abstract: This paper provides an analysis of the tradeoff between asymptotic bias\n(suboptimality with unlimited data) and overfitting (additional suboptimality\ndue to limited data) in the context of reinforcement learning with partial\nobservability. Our theoretical analysis formally characterizes that while\npotentially increasing the asymptotic bias, a smaller state representation\ndecreases the risk of overfitting. This analysis relies on expressing the\nquality of a state representation by bounding L1 error terms of the associated\nbelief states. Theoretical results are empirically illustrated when the state\nrepresentation is a truncated history of observations, both on synthetic POMDPs\nand on a large-scale POMDP in the context of smartgrids, with real-world data.\nFinally, similarly to known results in the fully observable setting, we also\nbriefly discuss and empirically illustrate how using function approximators and\nadapting the discount factor may enhance the tradeoff between asymptotic bias\nand overfitting in the partially observable context. \n\n"}
{"id": "1709.08520", "contents": "Title: Predictive-State Decoders: Encoding the Future into Recurrent Networks Abstract: Recurrent neural networks (RNNs) are a vital modeling technique that rely on\ninternal states learned indirectly by optimization of a supervised,\nunsupervised, or reinforcement training loss. RNNs are used to model dynamic\nprocesses that are characterized by underlying latent states whose form is\noften unknown, precluding its analytic representation inside an RNN. In the\nPredictive-State Representation (PSR) literature, latent state processes are\nmodeled by an internal state representation that directly models the\ndistribution of future observations, and most recent work in this area has\nrelied on explicitly representing and targeting sufficient statistics of this\nprobability distribution. We seek to combine the advantages of RNNs and PSRs by\naugmenting existing state-of-the-art recurrent neural networks with\nPredictive-State Decoders (PSDs), which add supervision to the network's\ninternal state representation to target predicting future observations.\nPredictive-State Decoders are simple to implement and easily incorporated into\nexisting training pipelines via additional loss regularization. We demonstrate\nthe effectiveness of PSDs with experimental results in three different domains:\nprobabilistic filtering, Imitation Learning, and Reinforcement Learning. In\neach, our method improves statistical performance of state-of-the-art recurrent\nbaselines and does so with fewer iterations and less data. \n\n"}
{"id": "1710.00032", "contents": "Title: Learning the Exact Topology of Undirected Consensus Networks Abstract: In this article, we present a method to learn the interaction topology of a\nnetwork of agents undergoing linear consensus updates in a non invasive manner.\nOur approach is based on multivariate Wiener filtering, which is known to\nrecover spurious edges apart from the true edges in the topology. The main\ncontribution of this work is to show that in the case of undirected consensus\nnetworks, all spurious links obtained using Wiener filtering can be identified\nusing frequency response of the Wiener filters. Thus, the exact interaction\ntopology of the agents is unveiled. The method presented requires time series\nmeasurements of the state of the agents and does not require any knowledge of\nlink weights. To the best of our knowledge this is the first approach that\nprovably reconstructs the structure of undirected consensus networks with\ncorrelated noise. We illustrate the effectiveness of the method developed\nthrough numerical simulations as well as experiments on a five node network of\nRaspberry Pis. \n\n"}
{"id": "1710.00085", "contents": "Title: Language-depedent I-Vectors for LRE15 Abstract: A standard recipe for spoken language recognition is to apply a Gaussian\nback-end to i-vectors. This ignores the uncertainty in the i-vector extraction,\nwhich could be important especially for short utterances. A recent paper by\nCumani, Plchot and Fer proposes a solution to propagate that uncertainty into\nthe backend. We propose an alternative method of propagating the uncertainty. \n\n"}
{"id": "1710.02836", "contents": "Title: RUM: network Representation learning throUgh Multi-level structural\n  information preservation Abstract: We have witnessed the discovery of many techniques for network representation\nlearning in recent years, ranging from encoding the context in random walks to\nembedding the lower order connections, to finding latent space representations\nwith auto-encoders. However, existing techniques are looking mostly into the\nlocal structures in a network, while higher-level properties such as global\ncommunity structures are often neglected. We propose a novel network\nrepresentations learning model framework called RUM (network Representation\nlearning throUgh Multi-level structural information preservation). In RUM, we\nincorporate three essential aspects of a node that capture a network's\ncharacteristics in multiple levels: a node's affiliated local triads, its\nneighborhood relationships, and its global community affiliations. Therefore\nthe framework explicitly and comprehensively preserves the structural\ninformation of a network, extending the encoding process both to the local end\nof the structural information spectrum and to the global end. The framework is\nalso flexible enough to take various community discovery algorithms as its\npreprocessor. Empirical results show that the representations learned by RUM\nhave demonstrated substantial performance advantages in real-life tasks. \n\n"}
{"id": "1710.04340", "contents": "Title: Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition Abstract: Spectral decomposition of the Koopman operator is attracting attention as a\ntool for the analysis of nonlinear dynamical systems. Dynamic mode\ndecomposition is a popular numerical algorithm for Koopman spectral analysis;\nhowever, we often need to prepare nonlinear observables manually according to\nthe underlying dynamics, which is not always possible since we may not have any\na priori knowledge about them. In this paper, we propose a fully data-driven\nmethod for Koopman spectral analysis based on the principle of learning Koopman\ninvariant subspaces from observed data. To this end, we propose minimization of\nthe residual sum of squares of linear least-squares regression to estimate a\nset of functions that transforms data into a form in which the linear\nregression fits well. We introduce an implementation with neural networks and\nevaluate performance empirically using nonlinear dynamical systems and\napplications. \n\n"}
{"id": "1710.05888", "contents": "Title: Discriminative Learning of Prediction Intervals Abstract: In this work we consider the task of constructing prediction intervals in an\ninductive batch setting. We present a discriminative learning framework which\noptimizes the expected error rate under a budget constraint on the interval\nsizes. Most current methods for constructing prediction intervals offer\nguarantees for a single new test point. Applying these methods to multiple test\npoints can result in a high computational overhead and degraded statistical\nguarantees. By focusing on expected errors, our method allows for variability\nin the per-example conditional error rates. As we demonstrate both analytically\nand empirically, this flexibility can increase the overall accuracy, or\nalternatively, reduce the average interval size.\n  While the problem we consider is of a regressive flavor, the loss we use is\ncombinatorial. This allows us to provide PAC-style, finite-sample guarantees.\nComputationally, we show that our original objective is NP-hard, and suggest a\ntractable convex surrogate. We conclude with a series of experimental\nevaluations. \n\n"}
{"id": "1710.05941", "contents": "Title: Searching for Activation Functions Abstract: The choice of activation functions in deep networks has a significant effect\non the training dynamics and task performance. Currently, the most successful\nand widely-used activation function is the Rectified Linear Unit (ReLU).\nAlthough various hand-designed alternatives to ReLU have been proposed, none\nhave managed to replace it due to inconsistent gains. In this work, we propose\nto leverage automatic search techniques to discover new activation functions.\nUsing a combination of exhaustive and reinforcement learning-based search, we\ndiscover multiple novel activation functions. We verify the effectiveness of\nthe searches by conducting an empirical evaluation with the best discovered\nactivation function. Our experiments show that the best discovered activation\nfunction, $f(x) = x \\cdot \\text{sigmoid}(\\beta x)$, which we name Swish, tends\nto work better than ReLU on deeper models across a number of challenging\ndatasets. For example, simply replacing ReLUs with Swish units improves top-1\nclassification accuracy on ImageNet by 0.9\\% for Mobile NASNet-A and 0.6\\% for\nInception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it\neasy for practitioners to replace ReLUs with Swish units in any neural network. \n\n"}
{"id": "1710.06382", "contents": "Title: Convergence diagnostics for stochastic gradient descent with constant\n  step size Abstract: Many iterative procedures in stochastic optimization exhibit a transient\nphase followed by a stationary phase. During the transient phase the procedure\nconverges towards a region of interest, and during the stationary phase the\nprocedure oscillates in that region, commonly around a single point. In this\npaper, we develop a statistical diagnostic test to detect such phase transition\nin the context of stochastic gradient descent with constant learning rate. We\npresent theory and experiments suggesting that the region where the proposed\ndiagnostic is activated coincides with the convergence region. For a class of\nloss functions, we derive a closed-form solution describing such region.\nFinally, we suggest an application to speed up convergence of stochastic\ngradient descent by halving the learning rate each time stationarity is\ndetected. This leads to a new variant of stochastic gradient descent, which in\nmany settings is comparable to state-of-art. \n\n"}
{"id": "1710.06943", "contents": "Title: First-Person Perceptual Guidance Behavior Decomposition using Active\n  Constraint Classification Abstract: Humans exhibit a wide range of adaptive and robust dynamic motion behavior\nthat is yet unmatched by autonomous control systems. These capabilities are\nessential for real-time behavior generation in cluttered environments. Recent\nwork suggests that human capabilities rely on task structure learning and\nembedded or ecological cognition in the form of perceptual guidance. This paper\ndescribes the experimental investigation of the functional elements of human\nmotion guidance, focusing on the control and perceptual mechanisms. The motion,\ncontrol, and perceptual data from first-person guidance experiments is\ndecomposed into elemental segments based on invariants. These elements are then\nanalyzed to determine their functional characteristics. The resulting model\nexplains the structure of the agent-environment interaction and provides lawful\ndescriptions of specific perceptual guidance and control mechanisms. \n\n"}
{"id": "1710.07742", "contents": "Title: Towards Black-box Iterative Machine Teaching Abstract: In this paper, we make an important step towards the black-box machine\nteaching by considering the cross-space machine teaching, where the teacher and\nthe learner use different feature representations and the teacher can not fully\nobserve the learner's model. In such scenario, we study how the teacher is\nstill able to teach the learner to achieve faster convergence rate than the\ntraditional passive learning. We propose an active teacher model that can\nactively query the learner (i.e., make the learner take exams) for estimating\nthe learner's status and provably guide the learner to achieve faster\nconvergence. The sample complexities for both teaching and query are provided.\nIn the experiments, we compare the proposed active teacher with the omniscient\nteacher and verify the effectiveness of the active teacher model. \n\n"}
{"id": "1710.08177", "contents": "Title: Progressive Learning for Systematic Design of Large Neural Networks Abstract: We develop an algorithm for systematic design of a large artificial neural\nnetwork using a progression property. We find that some non-linear functions,\nsuch as the rectifier linear unit and its derivatives, hold the property. The\nsystematic design addresses the choice of network size and regularization of\nparameters. The number of nodes and layers in network increases in progression\nwith the objective of consistently reducing an appropriate cost. Each layer is\noptimized at a time, where appropriate parameters are learned using convex\noptimization. Regularization parameters for convex optimization do not need a\nsignificant manual effort for tuning. We also use random instances for some\nweight matrices, and that helps to reduce the number of parameters we learn.\nThe developed network is expected to show good generalization power due to\nappropriate regularization and use of random weights in the layers. This\nexpectation is verified by extensive experiments for classification and\nregression problems, using standard databases. \n\n"}
{"id": "1710.08893", "contents": "Title: Fast Model Identification via Physics Engines for Data-Efficient Policy\n  Search Abstract: This paper presents a method for identifying mechanical parameters of robots\nor objects, such as their mass and friction coefficients. Key features are the\nuse of off-the-shelf physics engines and the adaptation of a Bayesian\noptimization technique towards minimizing the number of real-world experiments\nneeded for model-based reinforcement learning. The proposed framework\nreproduces in a physics engine experiments performed on a real robot and\noptimizes the model's mechanical parameters so as to match real-world\ntrajectories. The optimized model is then used for learning a policy in\nsimulation, before real-world deployment. It is well understood, however, that\nit is hard to exactly reproduce real trajectories in simulation. Moreover, a\nnear-optimal policy can be frequently found with an imperfect model. Therefore,\nthis work proposes a strategy for identifying a model that is just good enough\nto approximate the value of a locally optimal policy with a certain confidence,\ninstead of wasting effort on identifying the most accurate model. Evaluations,\nperformed both in simulation and on a real robotic manipulation task, indicate\nthat the proposed strategy results in an overall time-efficient, integrated\nmodel identification and learning solution, which significantly improves the\ndata-efficiency of existing policy search algorithms. \n\n"}
{"id": "1710.09668", "contents": "Title: PDE-Net: Learning PDEs from Data Abstract: In this paper, we present an initial attempt to learn evolution PDEs from\ndata. Inspired by the latest development of neural network designs in deep\nlearning, we propose a new feed-forward deep network, called PDE-Net, to\nfulfill two objectives at the same time: to accurately predict dynamics of\ncomplex systems and to uncover the underlying hidden PDE models. The basic idea\nof the proposed PDE-Net is to learn differential operators by learning\nconvolution kernels (filters), and apply neural networks or other machine\nlearning methods to approximate the unknown nonlinear responses. Comparing with\nexisting approaches, which either assume the form of the nonlinear response is\nknown or fix certain finite difference approximations of differential\noperators, our approach has the most flexibility by learning both differential\noperators and the nonlinear responses. A special feature of the proposed\nPDE-Net is that all filters are properly constrained, which enables us to\neasily identify the governing PDE models while still maintaining the expressive\nand predictive power of the network. These constrains are carefully designed by\nfully exploiting the relation between the orders of differential operators and\nthe orders of sum rules of filters (an important concept originated from\nwavelet theory). We also discuss relations of the PDE-Net with some existing\nnetworks in computer vision such as Network-In-Network (NIN) and Residual\nNeural Network (ResNet). Numerical experiments show that the PDE-Net has the\npotential to uncover the hidden PDE of the observed dynamics, and predict the\ndynamical behavior for a relatively long time, even in a noisy environment. \n\n"}
{"id": "1710.09988", "contents": "Title: Variance Reduced Value Iteration and Faster Algorithms for Solving\n  Markov Decision Processes Abstract: In this paper we provide faster algorithms for approximately solving\ndiscounted Markov Decision Processes in multiple parameter regimes. Given a\ndiscounted Markov Decision Process (DMDP) with $|S|$ states, $|A|$ actions,\ndiscount factor $\\gamma\\in(0,1)$, and rewards in the range $[-M, M]$, we show\nhow to compute an $\\epsilon$-optimal policy, with probability $1 - \\delta$ in\ntime \\[ \\tilde{O}\\left( \\left(|S|^2 |A| + \\frac{|S| |A|}{(1 - \\gamma)^3}\n\\right)\n  \\log\\left( \\frac{M}{\\epsilon} \\right) \\log\\left( \\frac{1}{\\delta} \\right)\n\\right) ~ . \\] This contribution reflects the first nearly linear time, nearly\nlinearly convergent algorithm for solving DMDPs for intermediate values of\n$\\gamma$.\n  We also show how to obtain improved sublinear time algorithms provided we can\nsample from the transition function in $O(1)$ time. Under this assumption we\nprovide an algorithm which computes an $\\epsilon$-optimal policy with\nprobability $1 - \\delta$ in time \\[ \\tilde{O} \\left(\\frac{|S| |A| M^2}{(1 -\n\\gamma)^4 \\epsilon^2} \\log \\left(\\frac{1}{\\delta}\\right) \\right) ~. \\]\n  Lastly, we extend both these algorithms to solve finite horizon MDPs. Our\nalgorithms improve upon the previous best for approximately computing optimal\npolicies for fixed-horizon MDPs in multiple parameter regimes.\n  Interestingly, we obtain our results by a careful modification of approximate\nvalue iteration. We show how to combine classic approximate value iteration\nanalysis with new techniques in variance reduction. Our fastest algorithms\nleverage further insights to ensure that our algorithms make monotonic progress\ntowards the optimal value. This paper is one of few instances in using sampling\nto obtain a linearly convergent linear programming algorithm and we hope that\nthe analysis may be useful more broadly. \n\n"}
{"id": "1710.10776", "contents": "Title: Transfer Learning to Learn with Multitask Neural Model Search Abstract: Deep learning models require extensive architecture design exploration and\nhyperparameter optimization to perform well on a given task. The exploration of\nthe model design space is often made by a human expert, and optimized using a\ncombination of grid search and search heuristics over a large space of possible\nchoices. Neural Architecture Search (NAS) is a Reinforcement Learning approach\nthat has been proposed to automate architecture design. NAS has been\nsuccessfully applied to generate Neural Networks that rival the best\nhuman-designed architectures. However, NAS requires sampling, constructing, and\ntraining hundreds to thousands of models to achieve well-performing\narchitectures. This procedure needs to be executed from scratch for each new\ntask. The application of NAS to a wide set of tasks currently lacks a way to\ntransfer generalizable knowledge across tasks. In this paper, we present the\nMultitask Neural Model Search (MNMS) controller. Our goal is to learn a\ngeneralizable framework that can condition model construction on successful\nmodel searches for previously seen tasks, thus significantly speeding up the\nsearch for new tasks. We demonstrate that MNMS can conduct an automated\narchitecture search for multiple tasks simultaneously while still learning\nwell-performing, specialized models for each task. We then show that\npre-trained MNMS controllers can transfer learning to new tasks. By leveraging\nknowledge from previous searches, we find that pre-trained MNMS models start\nfrom a better location in the search space and reduce search time on unseen\ntasks, while still discovering models that outperform published human-designed\nmodels. \n\n"}
{"id": "1710.11248", "contents": "Title: Learning Robust Rewards with Adversarial Inverse Reinforcement Learning Abstract: Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the\nneed for extensive feature and reward engineering. Deep reinforcement learning\nmethods can remove the need for explicit engineering of policy or value\nfeatures, but still require a manually specified reward function. Inverse\nreinforcement learning holds the promise of automatic reward acquisition, but\nhas proven exceptionally difficult to apply to large, high-dimensional problems\nwith unknown dynamics. In this work, we propose adverserial inverse\nreinforcement learning (AIRL), a practical and scalable inverse reinforcement\nlearning algorithm based on an adversarial reward learning formulation. We\ndemonstrate that AIRL is able to recover reward functions that are robust to\nchanges in dynamics, enabling us to learn policies even under significant\nvariation in the environment seen during training. Our experiments show that\nAIRL greatly outperforms prior methods in these transfer settings. \n\n"}
{"id": "1710.11345", "contents": "Title: Tensor Regression Meets Gaussian Processes Abstract: Low-rank tensor regression, a new model class that learns high-order\ncorrelation from data, has recently received considerable attention. At the\nsame time, Gaussian processes (GP) are well-studied machine learning models for\nstructure learning. In this paper, we demonstrate interesting connections\nbetween the two, especially for multi-way data analysis. We show that low-rank\ntensor regression is essentially learning a multi-linear kernel in Gaussian\nprocesses, and the low-rank assumption translates to the constrained Bayesian\ninference problem. We prove the oracle inequality and derive the average case\nlearning curve for the equivalent GP model. Our finding implies that low-rank\ntensor regression, though empirically successful, is highly dependent on the\neigenvalues of covariance functions as well as variable correlations. \n\n"}
{"id": "1711.00002", "contents": "Title: Log-DenseNet: How to Sparsify a DenseNet Abstract: Skip connections are increasingly utilized by deep neural networks to improve\naccuracy and cost-efficiency. In particular, the recent DenseNet is efficient\nin computation and parameters, and achieves state-of-the-art predictions by\ndirectly connecting each feature layer to all previous ones. However,\nDenseNet's extreme connectivity pattern may hinder its scalability to high\ndepths, and in applications like fully convolutional networks, full DenseNet\nconnections are prohibitively expensive. This work first experimentally shows\nthat one key advantage of skip connections is to have short distances among\nfeature layers during backpropagation. Specifically, using a fixed number of\nskip connections, the connection patterns with shorter backpropagation distance\namong layers have more accurate predictions. Following this insight, we propose\na connection template, Log-DenseNet, which, in comparison to DenseNet, only\nslightly increases the backpropagation distances among layers from 1 to ($1 +\n\\log_2 L$), but uses only $L\\log_2 L$ total connections instead of $O(L^2)$.\nHence, Log-DenseNets are easier than DenseNets to implement and to scale. We\ndemonstrate the effectiveness of our design principle by showing better\nperformance than DenseNets on tabula rasa semantic segmentation, and\ncompetitive results on visual recognition. \n\n"}
{"id": "1711.00048", "contents": "Title: Adversarial Semi-Supervised Audio Source Separation applied to Singing\n  Voice Extraction Abstract: The state of the art in music source separation employs neural networks\ntrained in a supervised fashion on multi-track databases to estimate the\nsources from a given mixture. With only few datasets available, often extensive\ndata augmentation is used to combat overfitting. Mixing random tracks, however,\ncan even reduce separation performance as instruments in real music are\nstrongly correlated. The key concept in our approach is that source estimates\nof an optimal separator should be indistinguishable from real source signals.\nBased on this idea, we drive the separator towards outputs deemed as realistic\nby discriminator networks that are trained to tell apart real from separator\nsamples. This way, we can also use unpaired source and mixture recordings\nwithout the drawbacks of creating unrealistic music mixtures. Our framework is\nwidely applicable as it does not assume a specific network architecture or\nnumber of sources. To our knowledge, this is the first adoption of adversarial\ntraining for music source separation. In a prototype experiment for singing\nvoice separation, separation performance increases with our approach compared\nto purely supervised training. \n\n"}
{"id": "1711.00464", "contents": "Title: Fixing a Broken ELBO Abstract: Recent work in unsupervised representation learning has focused on learning\ndeep directed latent-variable models. Fitting these models by maximizing the\nmarginal likelihood or evidence is typically intractable, thus a common\napproximation is to maximize the evidence lower bound (ELBO) instead. However,\nmaximum likelihood training (whether exact or approximate) does not necessarily\nresult in a good latent representation, as we demonstrate both theoretically\nand empirically. In particular, we derive variational lower and upper bounds on\nthe mutual information between the input and the latent variable, and use these\nbounds to derive a rate-distortion curve that characterizes the tradeoff\nbetween compression and reconstruction accuracy. Using this framework, we\ndemonstrate that there is a family of models with identical ELBO, but different\nquantitative and qualitative characteristics. Our framework also suggests a\nsimple new method to ensure that latent variable models with powerful\nstochastic decoders do not ignore their latent code. \n\n"}
{"id": "1711.00695", "contents": "Title: A Universal Marginalizer for Amortized Inference in Generative Models Abstract: We consider the problem of inference in a causal generative model where the\nset of available observations differs between data instances. We show how\ncombining samples drawn from the graphical model with an appropriate masking\nfunction makes it possible to train a single neural network to approximate all\nthe corresponding conditional marginal distributions and thus amortize the cost\nof inference. We further demonstrate that the efficiency of importance sampling\nmay be improved by basing proposals on the output of the neural network. We\nalso outline how the same network can be used to generate samples from an\napproximate joint posterior via a chain decomposition of the graph. \n\n"}
{"id": "1711.01239", "contents": "Title: Routing Networks: Adaptive Selection of Non-linear Functions for\n  Multi-Task Learning Abstract: Multi-task learning (MTL) with neural networks leverages commonalities in\ntasks to improve performance, but often suffers from task interference which\nreduces the benefits of transfer. To address this issue we introduce the\nrouting network paradigm, a novel neural network and training algorithm. A\nrouting network is a kind of self-organizing neural network consisting of two\ncomponents: a router and a set of one or more function blocks. A function block\nmay be any neural network - for example a fully-connected or a convolutional\nlayer. Given an input the router makes a routing decision, choosing a function\nblock to apply and passing the output back to the router recursively,\nterminating when a fixed recursion depth is reached. In this way the routing\nnetwork dynamically composes different function blocks for each input. We\nemploy a collaborative multi-agent reinforcement learning (MARL) approach to\njointly train the router and function blocks. We evaluate our model against\ncross-stitch networks and shared-layer baselines on multi-task settings of the\nMNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a\nsignificant improvement in accuracy, with sharper convergence. In addition,\nrouting networks have nearly constant per-task training cost while cross-stitch\nnetworks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we\nobtain cross-stitch performance levels with an 85% reduction in training time. \n\n"}
{"id": "1711.01558", "contents": "Title: Wasserstein Auto-Encoders Abstract: We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building\na generative model of the data distribution. WAE minimizes a penalized form of\nthe Wasserstein distance between the model distribution and the target\ndistribution, which leads to a different regularizer than the one used by the\nVariational Auto-Encoder (VAE). This regularizer encourages the encoded\ntraining distribution to match the prior. We compare our algorithm with several\nother techniques and show that it is a generalization of adversarial\nauto-encoders (AAE). Our experiments show that WAE shares many of the\nproperties of VAEs (stable training, encoder-decoder architecture, nice latent\nmanifold structure) while generating samples of better quality, as measured by\nthe FID score. \n\n"}
{"id": "1711.01558", "contents": "Title: Wasserstein Auto-Encoders Abstract: We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building\na generative model of the data distribution. WAE minimizes a penalized form of\nthe Wasserstein distance between the model distribution and the target\ndistribution, which leads to a different regularizer than the one used by the\nVariational Auto-Encoder (VAE). This regularizer encourages the encoded\ntraining distribution to match the prior. We compare our algorithm with several\nother techniques and show that it is a generalization of adversarial\nauto-encoders (AAE). Our experiments show that WAE shares many of the\nproperties of VAEs (stable training, encoder-decoder architecture, nice latent\nmanifold structure) while generating samples of better quality, as measured by\nthe FID score. \n\n"}
{"id": "1711.01846", "contents": "Title: Fast amortized inference of neural activity from calcium imaging data\n  with variational autoencoders Abstract: Calcium imaging permits optical measurement of neural activity. Since\nintracellular calcium concentration is an indirect measurement of neural\nactivity, computational tools are necessary to infer the true underlying\nspiking activity from fluorescence measurements. Bayesian model inversion can\nbe used to solve this problem, but typically requires either computationally\nexpensive MCMC sampling, or faster but approximate maximum-a-posteriori\noptimization. Here, we introduce a flexible algorithmic framework for fast,\nefficient and accurate extraction of neural spikes from imaging data. Using the\nframework of variational autoencoders, we propose to amortize inference by\ntraining a deep neural network to perform model inversion efficiently. The\nrecognition network is trained to produce samples from the posterior\ndistribution over spike trains. Once trained, performing inference amounts to a\nfast single forward pass through the network, without the need for iterative\noptimization or sampling. We show that amortization can be applied flexibly to\na wide range of nonlinear generative models and significantly improves upon the\nstate of the art in computation time, while achieving competitive accuracy. Our\nframework is also able to represent posterior distributions over spike-trains.\nWe demonstrate the generality of our method by proposing the first\nprobabilistic approach for separating backpropagating action potentials from\nputative synaptic inputs in calcium imaging of dendritic spines. \n\n"}
{"id": "1711.02475", "contents": "Title: Bayesian model and dimension reduction for uncertainty propagation:\n  applications in random media Abstract: Well-established methods for the solution of stochastic partial differential\nequations (SPDEs) typically struggle in problems with high-dimensional\ninputs/outputs. Such difficulties are only amplified in large-scale\napplications where even a few tens of full-order model runs are impracticable.\nWhile dimensionality reduction can alleviate some of these issues, it is not\nknown which and how many features of the (high-dimensional) input are actually\npredictive of the (high-dimensional) output. In this paper, we advocate a\nBayesian formulation that is capable of performing simultaneous dimension and\nmodel-order reduction. It consists of a component that encodes the\nhigh-dimensional input into a low-dimensional set of feature functions by\nemploying sparsity-enforcing priors and a decoding component that makes use of\nthe solution of a coarse-grained model in order to reconstruct that of the\nfull-order model. Both components are represented with latent variables in a\nprobabilistic graphical model and are simultaneously trained using Stochastic\nVariational Inference methods. The model is capable of quantifying the\npredictive uncertainty due to the information loss that unavoidably takes place\nin any model-order/dimension reduction as well as the uncertainty arising from\nfinite-sized training datasets. We demonstrate its capabilities in the context\nof random media where fine-scale fluctuations can give rise to random inputs\nwith tens of thousands of variables. With a few tens of full-order model\nsimulations, the proposed model is capable of identifying salient physical\nfeatures and produce sharp predictions under different boundary conditions of\nthe full output which itself consists of thousands of components. \n\n"}
{"id": "1711.02487", "contents": "Title: Deep density networks and uncertainty in recommender systems Abstract: Building robust online content recommendation systems requires learning\ncomplex interactions between user preferences and content features. The field\nhas evolved rapidly in recent years from traditional multi-arm bandit and\ncollaborative filtering techniques, with new methods employing Deep Learning\nmodels to capture non-linearities. Despite progress, the dynamic nature of\nonline recommendations still poses great challenges, such as finding the\ndelicate balance between exploration and exploitation. In this paper we show\nhow uncertainty estimations can be incorporated by employing them in an\noptimistic exploitation/exploration strategy for more efficient exploration of\nnew recommendations. We provide a novel hybrid deep neural network model, Deep\nDensity Networks (DDN), which integrates content-based deep learning models\nwith a collaborative scheme that is able to robustly model and estimate\nuncertainty. Finally, we present online and offline results after incorporating\nDNN into a real world content recommendation system that serves billions of\nrecommendations per day, and show the benefit of using DDN in practice. \n\n"}
{"id": "1711.04679", "contents": "Title: Attention-based Information Fusion using Multi-Encoder-Decoder Recurrent\n  Neural Networks Abstract: With the rising number of interconnected devices and sensors, modeling\ndistributed sensor networks is of increasing interest. Recurrent neural\nnetworks (RNN) are considered particularly well suited for modeling sensory and\nstreaming data. When predicting future behavior, incorporating information from\nneighboring sensor stations is often beneficial. We propose a new RNN based\narchitecture for context specific information fusion across multiple spatially\ndistributed sensor stations. Hereby, latent representations of multiple local\nmodels, each modeling one sensor station, are jointed and weighted, according\nto their importance for the prediction. The particular importance is assessed\ndepending on the current context using a separate attention function. We\ndemonstrate the effectiveness of our model on three different real-world sensor\nnetwork datasets. \n\n"}
{"id": "1711.04887", "contents": "Title: STARK: Structured Dictionary Learning Through Rank-one Tensor Recovery Abstract: In recent years, a class of dictionaries have been proposed for\nmultidimensional (tensor) data representation that exploit the structure of\ntensor data by imposing a Kronecker structure on the dictionary underlying the\ndata. In this work, a novel algorithm called \"STARK\" is provided to learn\nKronecker structured dictionaries that can represent tensors of any order. By\nestablishing that the Kronecker product of any number of matrices can be\nrearranged to form a rank-1 tensor, we show that Kronecker structure can be\nenforced on the dictionary by solving a rank-1 tensor recovery problem. Because\nrank-1 tensor recovery is a challenging nonconvex problem, we resort to solving\na convex relaxation of this problem. Empirical experiments on synthetic and\nreal data show promising results for our proposed algorithm. \n\n"}
{"id": "1711.05225", "contents": "Title: CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep\n  Learning Abstract: We develop an algorithm that can detect pneumonia from chest X-rays at a\nlevel exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer\nconvolutional neural network trained on ChestX-ray14, currently the largest\npublicly available chest X-ray dataset, containing over 100,000 frontal-view\nX-ray images with 14 diseases. Four practicing academic radiologists annotate a\ntest set, on which we compare the performance of CheXNet to that of\nradiologists. We find that CheXNet exceeds average radiologist performance on\nthe F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and\nachieve state of the art results on all 14 diseases. \n\n"}
{"id": "1711.05323", "contents": "Title: On Optimal Generalizability in Parametric Learning Abstract: We consider the parametric learning problem, where the objective of the\nlearner is determined by a parametric loss function. Employing empirical risk\nminimization with possibly regularization, the inferred parameter vector will\nbe biased toward the training samples. Such bias is measured by the cross\nvalidation procedure in practice where the data set is partitioned into a\ntraining set used for training and a validation set, which is not used in\ntraining and is left to measure the out-of-sample performance. A classical\ncross validation strategy is the leave-one-out cross validation (LOOCV) where\none sample is left out for validation and training is done on the rest of the\nsamples that are presented to the learner, and this process is repeated on all\nof the samples. LOOCV is rarely used in practice due to the high computational\ncomplexity. In this paper, we first develop a computationally efficient\napproximate LOOCV (ALOOCV) and provide theoretical guarantees for its\nperformance. Then we use ALOOCV to provide an optimization algorithm for\nfinding the regularizer in the empirical risk minimization framework. In our\nnumerical experiments, we illustrate the accuracy and efficiency of ALOOCV as\nwell as our proposed framework for the optimization of the regularizer. \n\n"}
{"id": "1711.05374", "contents": "Title: Optimizing Kernel Machines using Deep Learning Abstract: Building highly non-linear and non-parametric models is central to several\nstate-of-the-art machine learning systems. Kernel methods form an important\nclass of techniques that induce a reproducing kernel Hilbert space (RKHS) for\ninferring non-linear models through the construction of similarity functions\nfrom data. These methods are particularly preferred in cases where the training\ndata sizes are limited and when prior knowledge of the data similarities is\navailable. Despite their usefulness, they are limited by the computational\ncomplexity and their inability to support end-to-end learning with a\ntask-specific objective. On the other hand, deep neural networks have become\nthe de facto solution for end-to-end inference in several learning paradigms.\nIn this article, we explore the idea of using deep architectures to perform\nkernel machine optimization, for both computational efficiency and end-to-end\ninferencing. To this end, we develop the DKMO (Deep Kernel Machine\nOptimization) framework, that creates an ensemble of dense embeddings using\nNystrom kernel approximations and utilizes deep learning to generate\ntask-specific representations through the fusion of the embeddings.\nIntuitively, the filters of the network are trained to fuse information from an\nensemble of linear subspaces in the RKHS. Furthermore, we introduce the kernel\ndropout regularization to enable improved training convergence. Finally, we\nextend this framework to the multiple kernel case, by coupling a global fusion\nlayer with pre-trained deep kernel machines for each of the constituent\nkernels. Using case studies with limited training data, and lack of explicit\nfeature sources, we demonstrate the effectiveness of our framework over\nconventional model inferencing techniques. \n\n"}
{"id": "1711.05519", "contents": "Title: Accelerated Alternating Projections for Robust Principal Component\n  Analysis Abstract: We study robust PCA for the fully observed setting, which is about separating\na low rank matrix $\\boldsymbol{L}$ and a sparse matrix $\\boldsymbol{S}$ from\ntheir sum $\\boldsymbol{D}=\\boldsymbol{L}+\\boldsymbol{S}$. In this paper, a new\nalgorithm, dubbed accelerated alternating projections, is introduced for robust\nPCA which significantly improves the computational efficiency of the existing\nalternating projections proposed in [Netrapalli, Praneeth, et al., 2014] when\nupdating the low rank factor. The acceleration is achieved by first projecting\na matrix onto some low dimensional subspace before obtaining a new estimate of\nthe low rank matrix via truncated SVD. Exact recovery guarantee has been\nestablished which shows linear convergence of the proposed algorithm. Empirical\nperformance evaluations establish the advantage of our algorithm over other\nstate-of-the-art algorithms for robust PCA. \n\n"}
{"id": "1711.05627", "contents": "Title: Exploiting Layerwise Convexity of Rectifier Networks with Sign\n  Constrained Weights Abstract: By introducing sign constraints on the weights, this paper proposes sign\nconstrained rectifier networks (SCRNs), whose training can be solved\nefficiently by the well known majorization-minimization (MM) algorithms. We\nprove that the proposed two-hidden-layer SCRNs, which exhibit negative weights\nin the second hidden layer and negative weights in the output layer, are\ncapable of separating any two (or more) disjoint pattern sets. Furthermore, the\nproposed two-hidden-layer SCRNs can decompose the patterns of each class into\nseveral clusters so that each cluster is convexly separable from all the\npatterns from the other classes. This provides a means to learn the pattern\nstructures and analyse the discriminant factors between different classes of\npatterns. \n\n"}
{"id": "1711.05859", "contents": "Title: Hybrid Approach of Relation Network and Localized Graph Convolutional\n  Filtering for Breast Cancer Subtype Classification Abstract: Network biology has been successfully used to help reveal complex mechanisms\nof disease, especially cancer. On the other hand, network biology requires\nin-depth knowledge to construct disease-specific networks, but our current\nknowledge is very limited even with the recent advances in human cancer\nbiology. Deep learning has shown a great potential to address the difficult\nsituation like this. However, deep learning technologies conventionally use\ngrid-like structured data, thus application of deep learning technologies to\nthe classification of human disease subtypes is yet to be explored. Recently,\ngraph based deep learning techniques have emerged, which becomes an opportunity\nto leverage analyses in network biology. In this paper, we proposed a hybrid\nmodel, which integrates two key components 1) graph convolution neural network\n(graph CNN) and 2) relation network (RN). We utilize graph CNN as a component\nto learn expression patterns of cooperative gene community, and RN as a\ncomponent to learn associations between learned patterns. The proposed model is\napplied to the PAM50 breast cancer subtype classification task, the standard\nbreast cancer subtype classification of clinical utility. In experiments of\nboth subtype classification and patient survival analysis, our proposed method\nachieved significantly better performances than existing methods. We believe\nthat this work is an important starting point to realize the upcoming\npersonalized medicine. \n\n"}
{"id": "1711.06195", "contents": "Title: Neurology-as-a-Service for the Developing World Abstract: Electroencephalography (EEG) is an extensively-used and well-studied\ntechnique in the field of medical diagnostics and treatment for brain\ndisorders, including epilepsy, migraines, and tumors. The analysis and\ninterpretation of EEGs require physicians to have specialized training, which\nis not common even among most doctors in the developed world, let alone the\ndeveloping world where physician shortages plague society. This problem can be\naddressed by teleEEG that uses remote EEG analysis by experts or by local\ncomputer processing of EEGs. However, both of these options are prohibitively\nexpensive and the second option requires abundant computing resources and\ninfrastructure, which is another concern in developing countries where there\nare resource constraints on capital and computing infrastructure. In this work,\nwe present a cloud-based deep neural network approach to provide decision\nsupport for non-specialist physicians in EEG analysis and interpretation. Named\n`neurology-as-a-service,' the approach requires almost no manual intervention\nin feature engineering and in the selection of an optimal architecture and\nhyperparameters of the neural network. In this study, we deploy a pipeline that\nincludes moving EEG data to the cloud and getting optimal models for various\nclassification tasks. Our initial prototype has been tested only in developed\nworld environments to-date, but our intention is to test it in developing world\nenvironments in future work. We demonstrate the performance of our proposed\napproach using the BCI2000 EEG MMI dataset, on which our service attains 63.4%\naccuracy for the task of classifying real vs. imaginary activity performed by\nthe subject, which is significantly higher than what is obtained with a shallow\napproach such as support vector machines. \n\n"}
{"id": "1711.06583", "contents": "Title: Learning to Play Othello with Deep Neural Networks Abstract: Achieving superhuman playing level by AlphaGo corroborated the capabilities\nof convolutional neural architectures (CNNs) for capturing complex spatial\npatterns. This result was to a great extent due to several analogies between Go\nboard states and 2D images CNNs have been designed for, in particular\ntranslational invariance and a relatively large board. In this paper, we verify\nwhether CNN-based move predictors prove effective for Othello, a game with\nsignificantly different characteristics, including a much smaller board size\nand complete lack of translational invariance. We compare several CNN\narchitectures and board encodings, augment them with state-of-the-art\nextensions, train on an extensive database of experts' moves, and examine them\nwith respect to move prediction accuracy and playing strength. The empirical\nevaluation confirms high capabilities of neural move predictors and suggests a\nstrong correlation between prediction accuracy and playing strength. The best\nCNNs not only surpass all other 1-ply Othello players proposed to date but\ndefeat (2-ply) Edax, the best open-source Othello player. \n\n"}
{"id": "1711.07414", "contents": "Title: The Promise and Peril of Human Evaluation for Model Interpretability Abstract: Transparency, user trust, and human comprehension are popular ethical\nmotivations for interpretable machine learning. In support of these goals,\nresearchers evaluate model explanation performance using humans and real world\napplications. This alone presents a challenge in many areas of artificial\nintelligence. In this position paper, we propose a distinction between\ndescriptive and persuasive explanations. We discuss reasoning suggesting that\nfunctional interpretability may be correlated with cognitive function and user\npreferences. If this is indeed the case, evaluation and optimization using\nfunctional metrics could perpetuate implicit cognitive bias in explanations\nthat threaten transparency. Finally, we propose two potential research\ndirections to disambiguate cognitive function and explanation models, retaining\ncontrol over the tradeoff between accuracy and interpretability. \n\n"}
{"id": "1711.07682", "contents": "Title: JamBot: Music Theory Aware Chord Based Generation of Polyphonic Music\n  with LSTMs Abstract: We propose a novel approach for the generation of polyphonic music based on\nLSTMs. We generate music in two steps. First, a chord LSTM predicts a chord\nprogression based on a chord embedding. A second LSTM then generates polyphonic\nmusic from the predicted chord progression. The generated music sounds pleasing\nand harmonic, with only few dissonant notes. It has clear long-term structure\nthat is similar to what a musician would play during a jam session. We show\nthat our approach is sensible from a music theory perspective by evaluating the\nlearned chord embeddings. Surprisingly, our simple model managed to extract the\ncircle of fifths, an important tool in music theory, from the dataset. \n\n"}
{"id": "1711.07979", "contents": "Title: Posterior Sampling for Large Scale Reinforcement Learning Abstract: We propose a practical non-episodic PSRL algorithm that unlike recent\nstate-of-the-art PSRL algorithms uses a deterministic, model-independent\nepisode switching schedule. Our algorithm termed deterministic schedule PSRL\n(DS-PSRL) is efficient in terms of time, sample, and space complexity. We prove\na Bayesian regret bound under mild assumptions. Our result is more generally\napplicable to multiple parameters and continuous state action problems. We\ncompare our algorithm with state-of-the-art PSRL algorithms on standard\ndiscrete and continuous problems from the literature. Finally, we show how the\nassumptions of our algorithm satisfy a sensible parametrization for a large\nclass of problems in sequential recommendations. \n\n"}
{"id": "1711.08054", "contents": "Title: Generative Adversarial Positive-Unlabelled Learning Abstract: In this work, we consider the task of classifying binary positive-unlabeled\n(PU) data. The existing discriminative learning based PU models attempt to seek\nan optimal reweighting strategy for U data, so that a decent decision boundary\ncan be found. However, given limited P data, the conventional PU models tend to\nsuffer from overfitting when adapted to very flexible deep neural networks. In\ncontrast, we are the first to innovate a totally new paradigm to attack the\nbinary PU task, from perspective of generative learning by leveraging the\npowerful generative adversarial networks (GAN). Our generative\npositive-unlabeled (GenPU) framework incorporates an array of discriminators\nand generators that are endowed with different roles in simultaneously\nproducing positive and negative realistic samples. We provide theoretical\nanalysis to justify that, at equilibrium, GenPU is capable of recovering both\npositive and negative data distributions. Moreover, we show GenPU is\ngeneralizable and closely related to the semi-supervised classification. Given\nrather limited P data, experiments on both synthetic and real-world dataset\ndemonstrate the effectiveness of our proposed framework. With infinite\nrealistic and diverse sample streams generated from GenPU, a very flexible\nclassifier can then be trained using deep neural networks. \n\n"}
{"id": "1711.08095", "contents": "Title: SNeCT: Scalable network constrained Tucker decomposition for integrative\n  multi-platform data analysis Abstract: Motivation: How do we integratively analyze large-scale multi-platform\ngenomic data that are high dimensional and sparse? Furthermore, how can we\nincorporate prior knowledge, such as the association between genes, in the\nanalysis systematically? Method: To solve this problem, we propose a Scalable\nNetwork Constrained Tucker decomposition method we call SNeCT. SNeCT adopts\nparallel stochastic gradient descent approach on the proposed parallelizable\nnetwork constrained optimization function. SNeCT decomposition is applied to\ntensor constructed from large scale multi-platform multi-cohort cancer data,\nPanCan12, constrained on a network built from PathwayCommons database. Results:\nThe decomposed factor matrices are applied to stratify cancers, to search for\ntop-k similar patients, and to illustrate how the matrices can be used for\npersonalized interpretation. In the stratification test, combined twelve-cohort\ndata is clustered to form thirteen subclasses. The thirteen subclasses have a\nhigh correlation to tissue of origin in addition to other interesting\nobservations, such as clear separation of OV cancers to two groups, and high\nclinical correlation within subclusters formed in cohorts BRCA and UCEC. In the\ntop-k search, a new patient's genomic profile is generated and searched against\nexisting patients based on the factor matrices. The similarity of the top-k\npatient to the query is high for 23 clinical features, including\nestrogen/progesterone receptor statuses of BRCA patients with average precision\nvalue ranges from 0.72 to 0.86 and from 0.68 to 0.86, respectively. We also\nprovide an illustration of how the factor matrices can be used for\ninterpretable personalized analysis of each patient. \n\n"}
{"id": "1711.08244", "contents": "Title: Adversarial Phenomenon in the Eyes of Bayesian Deep Learning Abstract: Deep Learning models are vulnerable to adversarial examples, i.e.\\ images\nobtained via deliberate imperceptible perturbations, such that the model\nmisclassifies them with high confidence. However, class confidence by itself is\nan incomplete picture of uncertainty. We therefore use principled Bayesian\nmethods to capture model uncertainty in prediction for observing adversarial\nmisclassification. We provide an extensive study with different Bayesian neural\nnetworks attacked in both white-box and black-box setups. The behaviour of the\nnetworks for noise, attacks and clean test data is compared. We observe that\nBayesian neural networks are uncertain in their predictions for adversarial\nperturbations, a behaviour similar to the one observed for random Gaussian\nperturbations. Thus, we conclude that Bayesian neural networks can be\nconsidered for detecting adversarial examples. \n\n"}
{"id": "1711.08277", "contents": "Title: Few-shot Learning by Exploiting Visual Concepts within CNNs Abstract: Convolutional neural networks (CNNs) are one of the driving forces for the\nadvancement of computer vision. Despite their promising performances on many\ntasks, CNNs still face major obstacles on the road to achieving ideal machine\nintelligence. One is that CNNs are complex and hard to interpret. Another is\nthat standard CNNs require large amounts of annotated data, which is sometimes\nhard to obtain, and it is desirable to learn to recognize objects from few\nexamples. In this work, we address these limitations of CNNs by developing\nnovel, flexible, and interpretable models for few-shot learning. Our models are\nbased on the idea of encoding objects in terms of visual concepts (VCs), which\nare interpretable visual cues represented by the feature vectors within CNNs.\nWe first adapt the learning of VCs to the few-shot setting, and then uncover\ntwo key properties of feature encoding using VCs, which we call category\nsensitivity and spatial pattern. Motivated by these properties, we present two\nintuitive models for the problem of few-shot learning. Experiments show that\nour models achieve competitive performances, while being more flexible and\ninterpretable than alternative state-of-the-art few-shot learning methods. We\nconclude that using VCs helps expose the natural capability of CNNs for\nfew-shot learning. \n\n"}
{"id": "1711.08797", "contents": "Title: Practical Hash Functions for Similarity Estimation and Dimensionality\n  Reduction Abstract: Hashing is a basic tool for dimensionality reduction employed in several\naspects of machine learning. However, the perfomance analysis is often carried\nout under the abstract assumption that a truly random unit cost hash function\nis used, without concern for which concrete hash function is employed. The\nconcrete hash function may work fine on sufficiently random input. The question\nis if it can be trusted in the real world when faced with more structured\ninput.\n  In this paper we focus on two prominent applications of hashing, namely\nsimilarity estimation with the one permutation hashing (OPH) scheme of Li et\nal. [NIPS'12] and feature hashing (FH) of Weinberger et al. [ICML'09], both of\nwhich have found numerous applications, i.e. in approximate near-neighbour\nsearch with LSH and large-scale classification with SVM.\n  We consider mixed tabulation hashing of Dahlgaard et al.[FOCS'15] which was\nproved to perform like a truly random hash function in many applications,\nincluding OPH. Here we first show improved concentration bounds for FH with\ntruly random hashing and then argue that mixed tabulation performs similar for\nsparse input. Our main contribution, however, is an experimental comparison of\ndifferent hashing schemes when used inside FH, OPH, and LSH.\n  We find that mixed tabulation hashing is almost as fast as the\nmultiply-mod-prime scheme ax+b mod p. Mutiply-mod-prime is guaranteed to work\nwell on sufficiently random data, but we demonstrate that in the above\napplications, it can lead to bias and poor concentration on both real-world and\nsynthetic data. We also compare with the popular MurmurHash3, which has no\nproven guarantees. Mixed tabulation and MurmurHash3 both perform similar to\ntruly random hashing in our experiments. However, mixed tabulation is 40%\nfaster than MurmurHash3, and it has the proven guarantee of good performance on\nall possible input. \n\n"}
{"id": "1711.08946", "contents": "Title: Action Branching Architectures for Deep Reinforcement Learning Abstract: Discrete-action algorithms have been central to numerous recent successes of\ndeep reinforcement learning. However, applying these algorithms to\nhigh-dimensional action tasks requires tackling the combinatorial increase of\nthe number of possible actions with the number of action dimensions. This\nproblem is further exacerbated for continuous-action tasks that require fine\ncontrol of actions via discretization. In this paper, we propose a novel neural\narchitecture featuring a shared decision module followed by several network\nbranches, one for each action dimension. This approach achieves a linear\nincrease of the number of network outputs with the number of degrees of freedom\nby allowing a level of independence for each individual action dimension. To\nillustrate the approach, we present a novel agent, called Branching Dueling\nQ-Network (BDQ), as a branching variant of the Dueling Double Deep Q-Network\n(Dueling DDQN). We evaluate the performance of our agent on a set of\nchallenging continuous control tasks. The empirical results show that the\nproposed agent scales gracefully to environments with increasing action\ndimensionality and indicate the significance of the shared decision module in\ncoordination of the distributed action branches. Furthermore, we show that the\nproposed agent performs competitively against a state-of-the-art continuous\ncontrol algorithm, Deep Deterministic Policy Gradient (DDPG). \n\n"}
{"id": "1711.09219", "contents": "Title: Stacked Kernel Network Abstract: Kernel methods are powerful tools to capture nonlinear patterns behind data.\nThey implicitly learn high (even infinite) dimensional nonlinear features in\nthe Reproducing Kernel Hilbert Space (RKHS) while making the computation\ntractable by leveraging the kernel trick. Classic kernel methods learn a single\nlayer of nonlinear features, whose representational power may be limited.\nMotivated by recent success of deep neural networks (DNNs) that learn\nmulti-layer hierarchical representations, we propose a Stacked Kernel Network\n(SKN) that learns a hierarchy of RKHS-based nonlinear features. SKN interleaves\nseveral layers of nonlinear transformations (from a linear space to a RKHS) and\nlinear transformations (from a RKHS to a linear space). Similar to DNNs, a SKN\nis composed of multiple layers of hidden units, but each parameterized by a\nRKHS function rather than a finite-dimensional vector. We propose three ways to\nrepresent the RKHS functions in SKN: (1)nonparametric representation,\n(2)parametric representation and (3)random Fourier feature representation.\nFurthermore, we expand SKN into CNN architecture called Stacked Kernel\nConvolutional Network (SKCN). SKCN learning a hierarchy of RKHS-based nonlinear\nfeatures by convolutional operation with each filter also parameterized by a\nRKHS function rather than a finite-dimensional matrix in CNN, which is suitable\nfor image inputs. Experiments on various datasets demonstrate the effectiveness\nof SKN and SKCN, which outperform the competitive methods. \n\n"}
{"id": "1711.10284", "contents": "Title: Between-class Learning for Image Classification Abstract: In this paper, we propose a novel learning method for image classification\ncalled Between-Class learning (BC learning). We generate between-class images\nby mixing two images belonging to different classes with a random ratio. We\nthen input the mixed image to the model and train the model to output the\nmixing ratio. BC learning has the ability to impose constraints on the shape of\nthe feature distributions, and thus the generalization ability is improved. BC\nlearning is originally a method developed for sounds, which can be digitally\nmixed. Mixing two image data does not appear to make sense; however, we argue\nthat because convolutional neural networks have an aspect of treating input\ndata as waveforms, what works on sounds must also work on images. First, we\npropose a simple mixing method using internal divisions, which surprisingly\nproves to significantly improve performance. Second, we propose a mixing method\nthat treats the images as waveforms, which leads to a further improvement in\nperformance. As a result, we achieved 19.4% and 2.26% top-1 errors on\nImageNet-1K and CIFAR-10, respectively. \n\n"}
{"id": "1711.10433", "contents": "Title: Parallel WaveNet: Fast High-Fidelity Speech Synthesis Abstract: The recently-developed WaveNet architecture is the current state of the art\nin realistic speech synthesis, consistently rated as more natural sounding for\nmany different languages than any previous system. However, because WaveNet\nrelies on sequential generation of one audio sample at a time, it is poorly\nsuited to today's massively parallel computers, and therefore hard to deploy in\na real-time production setting. This paper introduces Probability Density\nDistillation, a new method for training a parallel feed-forward network from a\ntrained WaveNet with no significant difference in quality. The resulting system\nis capable of generating high-fidelity speech samples at more than 20 times\nfaster than real-time, and is deployed online by Google Assistant, including\nserving multiple English and Japanese voices. \n\n"}
{"id": "1711.10589", "contents": "Title: Contextual Outlier Interpretation Abstract: Outlier detection plays an essential role in many data-driven applications to\nidentify isolated instances that are different from the majority. While many\nstatistical learning and data mining techniques have been used for developing\nmore effective outlier detection algorithms, the interpretation of detected\noutliers does not receive much attention. Interpretation is becoming\nincreasingly important to help people trust and evaluate the developed models\nthrough providing intrinsic reasons why the certain outliers are chosen. It is\ndifficult, if not impossible, to simply apply feature selection for explaining\noutliers due to the distinct characteristics of various detection models,\ncomplicated structures of data in certain applications, and imbalanced\ndistribution of outliers and normal instances. In addition, the role of\ncontrastive contexts where outliers locate, as well as the relation between\noutliers and contexts, are usually overlooked in interpretation. To tackle the\nissues above, in this paper, we propose a novel Contextual Outlier\nINterpretation (COIN) method to explain the abnormality of existing outliers\nspotted by detectors. The interpretability for an outlier is achieved from\nthree aspects: outlierness score, attributes that contribute to the\nabnormality, and contextual description of its neighborhoods. Experimental\nresults on various types of datasets demonstrate the flexibility and\neffectiveness of the proposed framework compared with existing interpretation\napproaches. \n\n"}
{"id": "1711.10791", "contents": "Title: Reinforcement Learning To Adapt Speech Enhancement to Instantaneous\n  Input Signal Quality Abstract: Today, the optimal performance of existing noise-suppression algorithms, both\ndata-driven and those based on classic statistical methods, is range bound to\nspecific levels of instantaneous input signal-to-noise ratios. In this paper,\nwe present a new approach to improve the adaptivity of such algorithms enabling\nthem to perform robustly across a wide range of input signal and noise types.\nOur methodology is based on the dynamic control of algorithmic parameters via\nreinforcement learning. Specifically, we model the noise-suppression module as\na black box, requiring no knowledge of the algorithmic mechanics except a\nsimple feedback from the output. We utilize this feedback as the reward signal\nfor a reinforcement-learning agent that learns a policy to adapt the\nalgorithmic parameters for every incoming audio frame (16 ms of data). Our\npreliminary results show that such a control mechanism can substantially\nincrease the overall performance of the underlying noise-suppression algorithm;\n42% and 16% improvements in output SNR and MSE, respectively, when compared to\nno adaptivity. \n\n"}
{"id": "1711.10925", "contents": "Title: Deep Image Prior Abstract: Deep convolutional networks have become a popular tool for image generation\nand restoration. Generally, their excellent performance is imputed to their\nability to learn realistic image priors from a large number of example images.\nIn this paper, we show that, on the contrary, the structure of a generator\nnetwork is sufficient to capture a great deal of low-level image statistics\nprior to any learning. In order to do so, we show that a randomly-initialized\nneural network can be used as a handcrafted prior with excellent results in\nstandard inverse problems such as denoising, super-resolution, and inpainting.\nFurthermore, the same prior can be used to invert deep neural representations\nto diagnose them, and to restore images based on flash-no flash input pairs.\n  Apart from its diverse applications, our approach highlights the inductive\nbias captured by standard generator network architectures. It also bridges the\ngap between two very popular families of image restoration methods:\nlearning-based methods using deep convolutional networks and learning-free\nmethods based on handcrafted image priors such as self-similarity. Code and\nsupplementary material are available at\nhttps://dmitryulyanov.github.io/deep_image_prior . \n\n"}
{"id": "1712.00003", "contents": "Title: Modeling Information Flow Through Deep Neural Networks Abstract: This paper proposes a principled information theoretic analysis of\nclassification for deep neural network structures, e.g. convolutional neural\nnetworks (CNN). The output of convolutional filters is modeled as a random\nvariable Y conditioned on the object class C and network filter bank F. The\nconditional entropy (CENT) H(Y |C,F) is shown in theory and experiments to be a\nhighly compact and class-informative code, that can be computed from the filter\noutputs throughout an existing CNN and used to obtain higher classification\nresults than the original CNN itself. Experiments demonstrate the effectiveness\nof CENT feature analysis in two separate CNN classification contexts. 1) In the\nclassification of neurodegeneration due to Alzheimer's disease (AD) and natural\naging from 3D magnetic resonance image (MRI) volumes, 3 CENT features result in\nan AUC=94.6% for whole-brain AD classification, the highest reported accuracy\non the public OASIS dataset used and 12% higher than the softmax output of the\noriginal CNN trained for the task. 2) In the context of visual object\nclassification from 2D photographs, transfer learning based on a small set of\nCENT features identified throughout an existing CNN leads to AUC values\ncomparable to the 1000-feature softmax output of the original network when\nclassifying previously unseen object categories. The general information\ntheoretical analysis explains various recent CNN design successes, e.g. densely\nconnected CNN architectures, and provides insights for future research\ndirections in deep learning. \n\n"}
{"id": "1712.00004", "contents": "Title: Learnings Options End-to-End for Continuous Action Tasks Abstract: We present new results on learning temporally extended actions for\ncontinuoustasks, using the options framework (Suttonet al.[1999b], Precup\n[2000]). In orderto achieve this goal we work with the option-critic\narchitecture (Baconet al.[2017])using a deliberation cost and train it with\nproximal policy optimization (Schulmanet al.[2017]) instead of vanilla policy\ngradient. Results on Mujoco domains arepromising, but lead to interesting\nquestions aboutwhena given option should beused, an issue directly connected to\nthe use of initiation sets. \n\n"}
{"id": "1712.00481", "contents": "Title: Intelligent EHRs: Predicting Procedure Codes From Diagnosis Codes Abstract: In order to submit a claim to insurance companies, a doctor needs to code a\npatient encounter with both the diagnosis (ICDs) and procedures performed\n(CPTs) in an Electronic Health Record (EHR). Identifying and applying relevant\nprocedures code is a cumbersome and time-consuming task as a doctor has to\nchoose from around 13,000 procedure codes with no predefined one-to-one\nmapping. In this paper, we propose a state-of-the-art deep learning method for\nautomatic and intelligent coding of procedures (CPTs) from the diagnosis codes\n(ICDs) entered by the doctor. Precisely, we cast the learning problem as a\nmulti-label classification problem and use distributed representation to learn\nthe input mapping of high-dimensional sparse ICDs codes. Our final model\ntrained on 2.3 million claims is able to outperform existing rule-based\nprobabilistic and association-rule mining based methods and has a recall of\n90@3. \n\n"}
{"id": "1712.00656", "contents": "Title: An Asymptotically Optimal Algorithm for Communicating Multiplayer\n  Multi-Armed Bandit Problems Abstract: We consider a decentralized stochastic multi-armed bandit problem with\nmultiple players. Each player aims to maximize his/her own reward by pulling an\narm. The arms give rewards based on i.i.d. stochastic Bernoulli distributions.\nPlayers are not aware about the probability distributions of the arms. At the\nend of each turn, the players inform their neighbors about the arm he/she\npulled and the reward he/she got. Neighbors of players are determined according\nto an Erd{\\H{o}}s-R{\\'e}nyi graph with connectivity $\\alpha$. This graph is\nreproduced in the beginning of every turn with the same connectivity. When more\nthan one player choose the same arm in a turn, we assume that only one of the\nplayers who is randomly chosen gets the reward where the others get nothing. We\nfirst start by assuming players are not aware of the collision model and offer\nan asymptotically optimal algorithm for $\\alpha = 1$ case. Then, we extend our\nprior work and offer an asymptotically optimal algorithm for any connectivity\nbut zero, assuming players aware of the collision model. We also study the\neffect of $\\alpha$, the degree of communication between players, empirically on\nthe cumulative regret by comparing them with traditional multi-armed bandit\nalgorithms. \n\n"}
{"id": "1712.01106", "contents": "Title: Transferring Autonomous Driving Knowledge on Simulated and Real\n  Intersections Abstract: We view intersection handling on autonomous vehicles as a reinforcement\nlearning problem, and study its behavior in a transfer learning setting. We\nshow that a network trained on one type of intersection generally is not able\nto generalize to other intersections. However, a network that is pre-trained on\none intersection and fine-tuned on another performs better on the new task\ncompared to training in isolation. This network also retains knowledge of the\nprior task, even though some forgetting occurs. Finally, we show that the\nbenefits of fine-tuning hold when transferring simulated intersection handling\nknowledge to a real autonomous vehicle. \n\n"}
{"id": "1712.01935", "contents": "Title: How to Learn a Model Checker Abstract: We show how machine-learning techniques, particularly neural networks, offer\na very effective and highly efficient solution to the approximate\nmodel-checking problem for continuous and hybrid systems, a solution where the\ngeneral-purpose model checker is replaced by a model-specific classifier\ntrained by sampling model trajectories. To the best of our knowledge, we are\nthe first to establish this link from machine learning to model checking. Our\nmethod comprises a pipeline of analysis techniques for estimating and obtaining\nstatistical guarantees on the classifier's prediction performance, as well as\ntuning techniques to improve such performance. Our experimental evaluation\nconsiders the time-bounded reachability problem for three well-established\nbenchmarks in the hybrid systems community. On these examples, we achieve an\naccuracy of 99.82% to 100% and a false-negative rate (incorrectly predicting\nthat unsafe states are not reachable from a given state) of 0.0007 to 0. We\nbelieve that this level of accuracy is acceptable in many practical\napplications and we show how the approximate model checker can be made more\nconservative by tuning the classifier through further training and selection of\nthe classification threshold. \n\n"}
{"id": "1712.02505", "contents": "Title: Semi-Supervised Learning with IPM-based GANs: an Empirical Study Abstract: We present an empirical investigation of a recent class of Generative\nAdversarial Networks (GANs) using Integral Probability Metrics (IPM) and their\nperformance for semi-supervised learning. IPM-based GANs like Wasserstein GAN,\nFisher GAN and Sobolev GAN have desirable properties in terms of theoretical\nunderstanding, training stability, and a meaningful loss. In this work we\ninvestigate how the design of the critic (or discriminator) influences the\nperformance in semi-supervised learning. We distill three key take-aways which\nare important for good SSL performance: (1) the K+1 formulation, (2) avoiding\nbatch normalization in the critic and (3) avoiding gradient penalty constraints\non the classification layer. \n\n"}
{"id": "1712.06214", "contents": "Title: Predicting Individual Physiologically Acceptable States for Discharge\n  from a Pediatric Intensive Care Unit Abstract: Objective: Predict patient-specific vitals deemed medically acceptable for\ndischarge from a pediatric intensive care unit (ICU). Design: The means of each\npatient's hr, sbp and dbp measurements between their medical and physical\ndischarge from the ICU were computed as a proxy for their physiologically\nacceptable state space (PASS) for successful ICU discharge. These individual\nPASS values were compared via root mean squared error (rMSE) to population\nage-normal vitals, a polynomial regression through the PASS values of a\nPediatric ICU (PICU) population and predictions from two recurrent neural\nnetwork models designed to predict personalized PASS within the first twelve\nhours following ICU admission. Setting: PICU at Children's Hospital Los Angeles\n(CHLA). Patients: 6,899 PICU episodes (5,464 patients) collected between 2009\nand 2016. Interventions: None. Measurements: Each episode data contained 375\nvariables representing vitals, labs, interventions, and drugs. They also\nincluded a time indicator for PICU medical discharge and physical discharge.\nMain Results: The rMSEs between individual PASS values and population\nage-normals (hr: 25.9 bpm, sbp: 13.4 mmHg, dbp: 13.0 mmHg) were larger than the\nrMSEs corresponding to the polynomial regression (hr: 19.1 bpm, sbp: 12.3 mmHg,\ndbp: 10.8 mmHg). The rMSEs from the best performing RNN model were the lowest\n(hr: 16.4 bpm; sbp: 9.9 mmHg, dbp: 9.0 mmHg). Conclusion: PICU patients are a\nunique subset of the general population, and general age-normal vitals may not\nbe suitable as target values indicating physiologic stability at discharge.\nAge-normal vitals that were specifically derived from the medical-to-physical\ndischarge window of ICU patients may be more appropriate targets for\n'acceptable' physiologic state for critical care patients. Going beyond simple\nage bins, an RNN model can provide more personalized target values. \n\n"}
{"id": "1712.07113", "contents": "Title: Query-Efficient Black-box Adversarial Examples (superceded) Abstract: Note that this paper is superceded by \"Black-Box Adversarial Attacks with\nLimited Queries and Information.\"\n  Current neural network-based image classifiers are susceptible to adversarial\nexamples, even in the black-box setting, where the attacker is limited to query\naccess without access to gradients. Previous methods --- substitute networks\nand coordinate-based finite-difference methods --- are either unreliable or\nquery-inefficient, making these methods impractical for certain problems.\n  We introduce a new method for reliably generating adversarial examples under\nmore restricted, practical black-box threat models. First, we apply natural\nevolution strategies to perform black-box attacks using two to three orders of\nmagnitude fewer queries than previous methods. Second, we introduce a new\nalgorithm to perform targeted adversarial attacks in the partial-information\nsetting, where the attacker only has access to a limited number of target\nclasses. Using these techniques, we successfully perform the first targeted\nadversarial attack against a commercially deployed machine learning system, the\nGoogle Cloud Vision API, in the partial information setting. \n\n"}
{"id": "1712.07424", "contents": "Title: ADINE: An Adaptive Momentum Method for Stochastic Gradient Descent Abstract: Two major momentum-based techniques that have achieved tremendous success in\noptimization are Polyak's heavy ball method and Nesterov's accelerated\ngradient. A crucial step in all momentum-based methods is the choice of the\nmomentum parameter $m$ which is always suggested to be set to less than $1$.\nAlthough the choice of $m < 1$ is justified only under very strong theoretical\nassumptions, it works well in practice even when the assumptions do not\nnecessarily hold. In this paper, we propose a new momentum based method\n$\\textit{ADINE}$, which relaxes the constraint of $m < 1$ and allows the\nlearning algorithm to use adaptive higher momentum. We motivate our hypothesis\non $m$ by experimentally verifying that a higher momentum ($\\ge 1$) can help\nescape saddles much faster. Using this motivation, we propose our method\n$\\textit{ADINE}$ that helps weigh the previous updates more (by setting the\nmomentum parameter $> 1$), evaluate our proposed algorithm on deep neural\nnetworks and show that $\\textit{ADINE}$ helps the learning algorithm to\nconverge much faster without compromising on the generalization error. \n\n"}
{"id": "1712.08577", "contents": "Title: Adaptive Stochastic Dual Coordinate Ascent for Conditional Random Fields Abstract: This work investigates the training of conditional random fields (CRFs) via\nthe stochastic dual coordinate ascent (SDCA) algorithm of Shalev-Shwartz and\nZhang (2016). SDCA enjoys a linear convergence rate and a strong empirical\nperformance for binary classification problems. However, it has never been used\nto train CRFs. Yet it benefits from an `exact' line search with a single\nmarginalization oracle call, unlike previous approaches. In this paper, we\nadapt SDCA to train CRFs, and we enhance it with an adaptive non-uniform\nsampling strategy based on block duality gaps. We perform experiments on four\nstandard sequence prediction tasks. SDCA demonstrates performances on par with\nthe state of the art, and improves over it on three of the four datasets, which\nhave in common the use of sparse features. \n\n"}
{"id": "1712.09657", "contents": "Title: The information bottleneck and geometric clustering Abstract: The information bottleneck (IB) approach to clustering takes a joint\ndistribution $P\\!\\left(X,Y\\right)$ and maps the data $X$ to cluster labels $T$\nwhich retain maximal information about $Y$ (Tishby et al., 1999). This\nobjective results in an algorithm that clusters data points based upon the\nsimilarity of their conditional distributions $P\\!\\left(Y\\mid X\\right)$. This\nis in contrast to classic \"geometric clustering'' algorithms such as $k$-means\nand gaussian mixture models (GMMs) which take a set of observed data points\n$\\left\\{ \\mathbf{x}_{i}\\right\\} _{i=1:N}$ and cluster them based upon their\ngeometric (typically Euclidean) distance from one another. Here, we show how to\nuse the deterministic information bottleneck (DIB) (Strouse and Schwab, 2017),\na variant of IB, to perform geometric clustering, by choosing cluster labels\nthat preserve information about data point location on a smoothed dataset. We\nalso introduce a novel method to choose the number of clusters, based on\nidentifying solutions where the tradeoff between number of clusters used and\nspatial information preserved is strongest. We apply this approach to a variety\nof simple clustering problems, showing that DIB with our model selection\nprocedure recovers the generative cluster labels. We also show that, in\nparticular limits of our model parameters, clustering with DIB and IB is\nequivalent to $k$-means and EM fitting of a GMM with hard and soft assignments,\nrespectively. Thus, clustering with (D)IB generalizes and provides an\ninformation-theoretic perspective on these classic algorithms. \n\n"}
{"id": "1712.09707", "contents": "Title: Deep learning for universal linear embeddings of nonlinear dynamics Abstract: Identifying coordinate transformations that make strongly nonlinear dynamics\napproximately linear is a central challenge in modern dynamical systems. These\ntransformations have the potential to enable prediction, estimation, and\ncontrol of nonlinear systems using standard linear theory. The Koopman operator\nhas emerged as a leading data-driven embedding, as eigenfunctions of this\noperator provide intrinsic coordinates that globally linearize the dynamics.\nHowever, identifying and representing these eigenfunctions has proven to be\nmathematically and computationally challenging. This work leverages the power\nof deep learning to discover representations of Koopman eigenfunctions from\ntrajectory data of dynamical systems. Our network is parsimonious and\ninterpretable by construction, embedding the dynamics on a low-dimensional\nmanifold that is of the intrinsic rank of the dynamics and parameterized by the\nKoopman eigenfunctions. In particular, we identify nonlinear coordinates on\nwhich the dynamics are globally linear using a modified auto-encoder. We also\ngeneralize Koopman representations to include a ubiquitous class of systems\nthat exhibit continuous spectra, ranging from the simple pendulum to nonlinear\noptics and broadband turbulence. Our framework parametrizes the continuous\nfrequency using an auxiliary network, enabling a compact and efficient\nembedding at the intrinsic rank, while connecting our models to half a century\nof asymptotics. In this way, we benefit from the power and generality of deep\nlearning, while retaining the physical interpretability of Koopman embeddings. \n\n"}
{"id": "1801.00101", "contents": "Title: Parameter-free online learning via model selection Abstract: We introduce an efficient algorithmic framework for model selection in online\nlearning, also known as parameter-free online learning. Departing from previous\nwork, which has focused on highly structured function classes such as nested\nballs in Hilbert space, we propose a generic meta-algorithm framework that\nachieves online model selection oracle inequalities under minimal structural\nassumptions. We give the first computationally efficient parameter-free\nalgorithms that work in arbitrary Banach spaces under mild smoothness\nassumptions; previous results applied only to Hilbert spaces. We further derive\nnew oracle inequalities for matrix classes, non-nested convex sets, and\n$\\mathbb{R}^{d}$ with generic regularizers. Finally, we generalize these\nresults by providing oracle inequalities for arbitrary non-linear classes in\nthe online supervised learning model. These results are all derived through a\nunified meta-algorithm scheme using a novel \"multi-scale\" algorithm for\nprediction with expert advice based on random playout, which may be of\nindependent interest. \n\n"}
{"id": "1801.00318", "contents": "Title: Towards Building an Intelligent Anti-Malware System: A Deep Learning\n  Approach using Support Vector Machine (SVM) for Malware Classification Abstract: Effective and efficient mitigation of malware is a long-time endeavor in the\ninformation security community. The development of an anti-malware system that\ncan counteract an unknown malware is a prolific activity that may benefit\nseveral sectors. We envision an intelligent anti-malware system that utilizes\nthe power of deep learning (DL) models. Using such models would enable the\ndetection of newly-released malware through mathematical generalization. That\nis, finding the relationship between a given malware $x$ and its corresponding\nmalware family $y$, $f: x \\mapsto y$. To accomplish this feat, we used the\nMalimg dataset (Nataraj et al., 2011) which consists of malware images that\nwere processed from malware binaries, and then we trained the following DL\nmodels 1 to classify each malware family: CNN-SVM (Tang, 2013), GRU-SVM\n(Agarap, 2017), and MLP-SVM. Empirical evidence has shown that the GRU-SVM\nstands out among the DL models with a predictive accuracy of ~84.92%. This\nstands to reason for the mentioned model had the relatively most sophisticated\narchitecture design among the presented models. The exploration of an even more\noptimal DL-SVM model is the next stage towards the engineering of an\nintelligent anti-malware system. \n\n"}
{"id": "1801.01236", "contents": "Title: Multistep Neural Networks for Data-driven Discovery of Nonlinear\n  Dynamical Systems Abstract: The process of transforming observed data into predictive mathematical models\nof the physical world has always been paramount in science and engineering.\nAlthough data is currently being collected at an ever-increasing pace, devising\nmeaningful models out of such observations in an automated fashion still\nremains an open problem. In this work, we put forth a machine learning approach\nfor identifying nonlinear dynamical systems from data. Specifically, we blend\nclassical tools from numerical analysis, namely the multi-step time-stepping\nschemes, with powerful nonlinear function approximators, namely deep neural\nnetworks, to distill the mechanisms that govern the evolution of a given\ndata-set. We test the effectiveness of our approach for several benchmark\nproblems involving the identification of complex, nonlinear and chaotic\ndynamics, and we demonstrate how this allows us to accurately learn the\ndynamics, forecast future states, and identify basins of attraction. In\nparticular, we study the Lorenz system, the fluid flow behind a cylinder, the\nHopf bifurcation, and the Glycoltic oscillator model as an example of\ncomplicated nonlinear dynamics typical of biological systems. \n\n"}
{"id": "1801.01236", "contents": "Title: Multistep Neural Networks for Data-driven Discovery of Nonlinear\n  Dynamical Systems Abstract: The process of transforming observed data into predictive mathematical models\nof the physical world has always been paramount in science and engineering.\nAlthough data is currently being collected at an ever-increasing pace, devising\nmeaningful models out of such observations in an automated fashion still\nremains an open problem. In this work, we put forth a machine learning approach\nfor identifying nonlinear dynamical systems from data. Specifically, we blend\nclassical tools from numerical analysis, namely the multi-step time-stepping\nschemes, with powerful nonlinear function approximators, namely deep neural\nnetworks, to distill the mechanisms that govern the evolution of a given\ndata-set. We test the effectiveness of our approach for several benchmark\nproblems involving the identification of complex, nonlinear and chaotic\ndynamics, and we demonstrate how this allows us to accurately learn the\ndynamics, forecast future states, and identify basins of attraction. In\nparticular, we study the Lorenz system, the fluid flow behind a cylinder, the\nHopf bifurcation, and the Glycoltic oscillator model as an example of\ncomplicated nonlinear dynamics typical of biological systems. \n\n"}
{"id": "1801.01594", "contents": "Title: Differentially Private Releasing via Deep Generative Model (Technical\n  Report) Abstract: Privacy-preserving releasing of complex data (e.g., image, text, audio)\nrepresents a long-standing challenge for the data mining research community.\nDue to rich semantics of the data and lack of a priori knowledge about the\nanalysis task, excessive sanitization is often necessary to ensure privacy,\nleading to significant loss of the data utility. In this paper, we present\ndp-GAN, a general private releasing framework for semantic-rich data. Instead\nof sanitizing and then releasing the data, the data curator publishes a deep\ngenerative model which is trained using the original data in a differentially\nprivate manner; with the generative model, the analyst is able to produce an\nunlimited amount of synthetic data for arbitrary analysis tasks. In contrast of\nalternative solutions, dp-GAN highlights a set of key features: (i) it provides\ntheoretical privacy guarantee via enforcing the differential privacy principle;\n(ii) it retains desirable utility in the released model, enabling a variety of\notherwise impossible analyses; and (iii) most importantly, it achieves\npractical training scalability and stability by employing multi-fold\noptimization strategies. Through extensive empirical evaluation on benchmark\ndatasets and analyses, we validate the efficacy of dp-GAN. \n\n"}
{"id": "1801.04883", "contents": "Title: Unsupervised Cipher Cracking Using Discrete GANs Abstract: This work details CipherGAN, an architecture inspired by CycleGAN used for\ninferring the underlying cipher mapping given banks of unpaired ciphertext and\nplaintext. We demonstrate that CipherGAN is capable of cracking language data\nenciphered using shift and Vigenere ciphers to a high degree of fidelity and\nfor vocabularies much larger than previously achieved. We present how CycleGAN\ncan be made compatible with discrete data and train in a stable way. We then\nprove that the technique used in CipherGAN avoids the common problem of\nuninformative discrimination associated with GANs applied to discrete data. \n\n"}
{"id": "1801.04987", "contents": "Title: On the Complexity of the Weighted Fused Lasso Abstract: The solution path of the 1D fused lasso for an $n$-dimensional input is\npiecewise linear with $\\mathcal{O}(n)$ segments (Hoefling et al. 2010 and\nTibshirani et al 2011). However, existing proofs of this bound do not hold for\nthe weighted fused lasso. At the same time, results for the generalized lasso,\nof which the weighted fused lasso is a special case, allow $\\Omega(3^n)$\nsegments (Mairal et al. 2012). In this paper, we prove that the number of\nsegments in the solution path of the weighted fused lasso is\n$\\mathcal{O}(n^2)$, and that, for some instances, it is $\\Omega(n^2)$. We also\ngive a new, very simple, proof of the $\\mathcal{O}(n)$ bound for the fused\nlasso. \n\n"}
{"id": "1801.05039", "contents": "Title: Global Convergence of Policy Gradient Methods for the Linear Quadratic\n  Regulator Abstract: Direct policy gradient methods for reinforcement learning and continuous\ncontrol problems are a popular approach for a variety of reasons: 1) they are\neasy to implement without explicit knowledge of the underlying model 2) they\nare an \"end-to-end\" approach, directly optimizing the performance metric of\ninterest 3) they inherently allow for richly parameterized policies. A notable\ndrawback is that even in the most basic continuous control problem (that of\nlinear quadratic regulators), these methods must solve a non-convex\noptimization problem, where little is understood about their efficiency from\nboth computational and statistical perspectives. In contrast, system\nidentification and model based planning in optimal control theory have a much\nmore solid theoretical footing, where much is known with regards to their\ncomputational and statistical properties. This work bridges this gap showing\nthat (model free) policy gradient methods globally converge to the optimal\nsolution and are efficient (polynomially so in relevant problem dependent\nquantities) with regards to their sample and computational complexities. \n\n"}
{"id": "1801.05407", "contents": "Title: Deep Canonically Correlated LSTMs Abstract: We examine Deep Canonically Correlated LSTMs as a way to learn nonlinear\ntransformations of variable length sequences and embed them into a correlated,\nfixed dimensional space. We use LSTMs to transform multi-view time-series data\nnon-linearly while learning temporal relationships within the data. We then\nperform correlation analysis on the outputs of these neural networks to find a\ncorrelated subspace through which we get our final representation via\nprojection. This work follows from previous work done on Deep Canonical\nCorrelation (DCCA), in which deep feed-forward neural networks were used to\nlearn nonlinear transformations of data while maximizing correlation. \n\n"}
{"id": "1801.06637", "contents": "Title: Deep Hidden Physics Models: Deep Learning of Nonlinear Partial\n  Differential Equations Abstract: A long-standing problem at the interface of artificial intelligence and\napplied mathematics is to devise an algorithm capable of achieving human level\nor even superhuman proficiency in transforming observed data into predictive\nmathematical models of the physical world. In the current era of abundance of\ndata and advanced machine learning capabilities, the natural question arises:\nHow can we automatically uncover the underlying laws of physics from\nhigh-dimensional data generated from experiments? In this work, we put forth a\ndeep learning approach for discovering nonlinear partial differential equations\nfrom scattered and potentially noisy observations in space and time.\nSpecifically, we approximate the unknown solution as well as the nonlinear\ndynamics by two deep neural networks. The first network acts as a prior on the\nunknown solution and essentially enables us to avoid numerical differentiations\nwhich are inherently ill-conditioned and unstable. The second network\nrepresents the nonlinear dynamics and helps us distill the mechanisms that\ngovern the evolution of a given spatiotemporal data-set. We test the\neffectiveness of our approach for several benchmark problems spanning a number\nof scientific domains and demonstrate how the proposed framework can help us\naccurately learn the underlying dynamics and forecast future states of the\nsystem. In particular, we study the Burgers', Korteweg-de Vries (KdV),\nKuramoto-Sivashinsky, nonlinear Schr\\\"{o}dinger, and Navier-Stokes equations. \n\n"}
{"id": "1801.07194", "contents": "Title: Optimizing Prediction Intervals by Tuning Random Forest via\n  Meta-Validation Abstract: Recent studies have shown that tuning prediction models increases prediction\naccuracy and that Random Forest can be used to construct prediction intervals.\nHowever, to our best knowledge, no study has investigated the need to, and the\nmanner in which one can, tune Random Forest for optimizing prediction intervals\n{ this paper aims to fill this gap. We explore a tuning approach that combines\nan effectively exhaustive search with a validation technique on a single Random\nForest parameter. This paper investigates which, out of eight validation\ntechniques, are beneficial for tuning, i.e., which automatically choose a\nRandom Forest configuration constructing prediction intervals that are reliable\nand with a smaller width than the default configuration. Additionally, we\npresent and validate three meta-validation techniques to determine which are\nbeneficial, i.e., those which automatically chose a beneficial validation\ntechnique. This study uses data from our industrial partner (Keymind Inc.) and\nthe Tukutuku Research Project, related to post-release defect prediction and\nWeb application effort estimation, respectively. Results from our study\nindicate that: i) the default configuration is frequently unreliable, ii) most\nof the validation techniques, including previously successfully adopted ones\nsuch as 50/50 holdout and bootstrap, are counterproductive in most of the\ncases, and iii) the 75/25 holdout meta-validation technique is always\nbeneficial; i.e., it avoids the likely counterproductive effects of validation\ntechniques. \n\n"}
{"id": "1801.08214", "contents": "Title: Active Neural Localization Abstract: Localization is the problem of estimating the location of an autonomous agent\nfrom an observation and a map of the environment. Traditional methods of\nlocalization, which filter the belief based on the observations, are\nsub-optimal in the number of steps required, as they do not decide the actions\ntaken by the agent. We propose \"Active Neural Localizer\", a fully\ndifferentiable neural network that learns to localize accurately and\nefficiently. The proposed model incorporates ideas of traditional\nfiltering-based localization methods, by using a structured belief of the state\nwith multiplicative interactions to propagate belief, and combines it with a\npolicy model to localize accurately while minimizing the number of steps\nrequired for localization. Active Neural Localizer is trained end-to-end with\nreinforcement learning. We use a variety of simulation environments for our\nexperiments which include random 2D mazes, random mazes in the Doom game engine\nand a photo-realistic environment in the Unreal game engine. The results on the\n2D environments show the effectiveness of the learned policy in an idealistic\nsetting while results on the 3D environments demonstrate the model's capability\nof learning the policy and perceptual model jointly from raw-pixel based RGB\nobservations. We also show that a model trained on random textures in the Doom\nenvironment generalizes well to a photo-realistic office space environment in\nthe Unreal engine. \n\n"}
{"id": "1801.09710", "contents": "Title: tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution\n  Fluid Flow Abstract: We propose a temporally coherent generative model addressing the\nsuper-resolution problem for fluid flows. Our work represents a first approach\nto synthesize four-dimensional physics fields with neural networks. Based on a\nconditional generative adversarial network that is designed for the inference\nof three-dimensional volumetric data, our model generates consistent and\ndetailed results by using a novel temporal discriminator, in addition to the\ncommonly used spatial one. Our experiments show that the generator is able to\ninfer more realistic high-resolution details by using additional physical\nquantities, such as low-resolution velocities or vorticities. Besides\nimprovements in the training process and in the generated outputs, these inputs\noffer means for artistic control as well. We additionally employ a\nphysics-aware data augmentation step, which is crucial to avoid overfitting and\nto reduce memory requirements. In this way, our network learns to generate\nadvected quantities with highly detailed, realistic, and temporally coherent\nfeatures. Our method works instantaneously, using only a single time-step of\nlow-resolution fluid data. We demonstrate the abilities of our method using a\nvariety of complex inputs and applications in two and three dimensions. \n\n"}
{"id": "1801.09767", "contents": "Title: What Is the Fractional Laplacian? Abstract: The fractional Laplacian in R^d has multiple equivalent characterizations.\nMoreover, in bounded domains, boundary conditions must be incorporated in these\ncharacterizations in mathematically distinct ways, and there is currently no\nconsensus in the literature as to which definition of the fractional Laplacian\nin bounded domains is most appropriate for a given application. The Riesz (or\nintegral) definition, for example, admits a nonlocal boundary condition, where\nthe value of a function u(x) must be prescribed on the entire exterior of the\ndomain in order to compute its fractional Laplacian. In contrast, the spectral\ndefinition requires only the standard local boundary condition. These\ndifferences, among others, lead us to ask the question: \"What is the fractional\nLaplacian?\" We compare several commonly used definitions of the fractional\nLaplacian (the Riesz, spectral, directional, and horizon-based nonlocal\ndefinitions), and we use a joint theoretical and computational approach to\nexamining their different characteristics by studying solutions of related\nfractional Poisson equations formulated on bounded domains.\n  In this work, we provide new numerical methods as well as a self-contained\ndiscussion of state-of-the-art methods for discretizing the fractional\nLaplacian, and we present new results on the differences in features,\nregularity, and boundary behaviors of solutions to equations posed with these\ndifferent definitions. We present stochastic interpretations and demonstrate\nthe equivalence between some recent formulations. Through our efforts, we aim\nto further engage the research community in open problems and assist\npractitioners in identifying the most appropriate definition and computational\napproach to use for their mathematical models in addressing anomalous transport\nin diverse applications. \n\n"}
{"id": "1801.10119", "contents": "Title: An Incremental Path-Following Splitting Method for Linearly Constrained\n  Nonconvex Nonsmooth Programs Abstract: The stationary point of Problem 2 is NOT the stationary point of Problem 1.\nWe are sorry and we are working on fixing this error. \n\n"}
{"id": "1802.00030", "contents": "Title: Fusarium Damaged Kernels Detection Using Transfer Learning on Deep\n  Neural Network Architecture Abstract: The present work shows the application of transfer learning for a pre-trained\ndeep neural network (DNN), using a small image dataset ($\\approx$ 12,000) on a\nsingle workstation with enabled NVIDIA GPU card that takes up to 1 hour to\ncomplete the training task and archive an overall average accuracy of $94.7\\%$.\nThe DNN presents a $20\\%$ score of misclassification for an external test\ndataset. The accuracy of the proposed methodology is equivalent to ones using\nHSI methodology $(81\\%-91\\%)$ used for the same task, but with the advantage of\nbeing independent on special equipment to classify wheat kernel for FHB\nsymptoms. \n\n"}
{"id": "1802.00844", "contents": "Title: Intriguing Properties of Randomly Weighted Networks: Generalizing While\n  Learning Next to Nothing Abstract: Training deep neural networks results in strong learned representations that\nshow good generalization capabilities. In most cases, training involves\niterative modification of all weights inside the network via back-propagation.\nIn Extreme Learning Machines, it has been suggested to set the first layer of a\nnetwork to fixed random values instead of learning it. In this paper, we\npropose to take this approach a step further and fix almost all layers of a\ndeep convolutional neural network, allowing only a small portion of the weights\nto be learned. As our experiments show, fixing even the majority of the\nparameters of the network often results in performance which is on par with the\nperformance of learning all of them. The implications of this intriguing\nproperty of deep neural networks are discussed and we suggest ways to harness\nit to create more robust representations. \n\n"}
{"id": "1802.02207", "contents": "Title: Automated dataset generation for image recognition using the example of\n  taxonomy Abstract: This master thesis addresses the subject of automatically generating a\ndataset for image recognition, which takes a lot of time when being done\nmanually. As the thesis was written with motivation from the context of the\nbiodiversity workgroup at the City University of Applied Sciences Bremen, the\nclassification of taxonomic entries was chosen as an exemplary use case. In\norder to automate the dataset creation, a prototype was conceptualized and\nimplemented after working out knowledge basics and analyzing requirements for\nit. It makes use of an pre-trained abstract artificial intelligence which is\nable to sort out images that do not contain the desired content. Subsequent to\nthe implementation and the automated dataset creation resulting from it, an\nevaluation was performed. Other, manually collected datasets were compared to\nthe one the prototype produced in means of specifications and accuracy. The\nresults were more than satisfactory and showed that automatically generating a\ndataset for image recognition is not only possible, but also might be a decent\nalternative to spending time and money in doing this task manually. At the very\nend of this work, an idea of how to use the principle of employing abstract\nartificial intelligences for step-by-step classification of deeper taxonomic\nlayers in a productive system is presented and discussed. \n\n"}
{"id": "1802.02212", "contents": "Title: Classification and Disease Localization in Histopathology Using Only\n  Global Labels: A Weakly-Supervised Approach Abstract: Analysis of histopathology slides is a critical step for many diagnoses, and\nin particular in oncology where it defines the gold standard. In the case of\ndigital histopathological analysis, highly trained pathologists must review\nvast whole-slide-images of extreme digital resolution ($100,000^2$ pixels)\nacross multiple zoom levels in order to locate abnormal regions of cells, or in\nsome cases single cells, out of millions. The application of deep learning to\nthis problem is hampered not only by small sample sizes, as typical datasets\ncontain only a few hundred samples, but also by the generation of ground-truth\nlocalized annotations for training interpretable classification and\nsegmentation models. We propose a method for disease localization in the\ncontext of weakly supervised learning, where only image-level labels are\navailable during training. Even without pixel-level annotations, we are able to\ndemonstrate performance comparable with models trained with strong annotations\non the Camelyon-16 lymph node metastases detection challenge. We accomplish\nthis through the use of pre-trained deep convolutional networks, feature\nembedding, as well as learning via top instances and negative evidence, a\nmultiple instance learning technique from the field of semantic segmentation\nand object detection. \n\n"}
{"id": "1802.02511", "contents": "Title: DeepHeart: Semi-Supervised Sequence Learning for Cardiovascular Risk\n  Prediction Abstract: We train and validate a semi-supervised, multi-task LSTM on 57,675\nperson-weeks of data from off-the-shelf wearable heart rate sensors, showing\nhigh accuracy at detecting multiple medical conditions, including diabetes\n(0.8451), high cholesterol (0.7441), high blood pressure (0.8086), and sleep\napnea (0.8298). We compare two semi-supervised train- ing methods,\nsemi-supervised sequence learning and heuristic pretraining, and show they\noutperform hand-engineered biomarkers from the medical literature. We believe\nour work suggests a new approach to patient risk stratification based on\ncardiovascular risk scores derived from popular wearables such as Fitbit, Apple\nWatch, or Android Wear. \n\n"}
{"id": "1802.02840", "contents": "Title: Neural Network Renormalization Group Abstract: We present a variational renormalization group (RG) approach using a deep\ngenerative model based on normalizing flows. The model performs hierarchical\nchange-of-variables transformations from the physical space to a latent space\nwith reduced mutual information. Conversely, the neural net directly maps\nindependent Gaussian noises to physical configurations following the inverse RG\nflow. The model has an exact and tractable likelihood, which allows unbiased\ntraining and direct access to the renormalized energy function of the latent\nvariables. To train the model, we employ probability density distillation for\nthe bare energy function of the physical problem, in which the training loss\nprovides a variational upper bound of the physical free energy. We demonstrate\npractical usage of the approach by identifying mutually independent collective\nvariables of the Ising model and performing accelerated hybrid Monte Carlo\nsampling in the latent space. Lastly, we comment on the connection of the\npresent approach to the wavelet formulation of RG and the modern pursuit of\ninformation preserving RG. \n\n"}
{"id": "1802.03334", "contents": "Title: Learning Localized Spatio-Temporal Models From Streaming Data Abstract: We address the problem of predicting spatio-temporal processes with temporal\npatterns that vary across spatial regions, when data is obtained as a stream.\nThat is, when the training dataset is augmented sequentially. Specifically, we\ndevelop a localized spatio-temporal covariance model of the process that can\ncapture spatially varying temporal periodicities in the data. We then apply a\ncovariance-fitting methodology to learn the model parameters which yields a\npredictor that can be updated sequentially with each new data point. The\nproposed method is evaluated using both synthetic and real climate data which\ndemonstrate its ability to accurately predict data missing in spatial regions\nover time. \n\n"}
{"id": "1802.03480", "contents": "Title: GraphVAE: Towards Generation of Small Graphs Using Variational\n  Autoencoders Abstract: Deep learning on graphs has become a popular research topic with many\napplications. However, past work has concentrated on learning graph embedding\ntasks, which is in contrast with advances in generative models for images and\ntext. Is it possible to transfer this progress to the domain of graphs? We\npropose to sidestep hurdles associated with linearization of such discrete\nstructures by having a decoder output a probabilistic fully-connected graph of\na predefined maximum size directly at once. Our method is formulated as a\nvariational autoencoder. We evaluate on the challenging task of molecule\ngeneration. \n\n"}
{"id": "1802.03486", "contents": "Title: An LSTM Recurrent Network for Step Counting Abstract: Smartphones with sensors such as accelerometer and gyroscope can be used as\npedometers and navigators. In this paper, we propose to use an LSTM recurrent\nnetwork for counting the number of steps taken by both blind and sighted users,\nbased on an annotated smartphone sensor dataset, WeAllWork. The models were\ntrained separately for sighted people, blind people with a long cane or a guide\ndog for Leave-One-Out training modality. It achieved 5% overcount and\nundercount rate. \n\n"}
{"id": "1802.03532", "contents": "Title: Bayesian Optimization Using Monotonicity Information and Its Application\n  in Machine Learning Hyperparameter Abstract: We propose an algorithm for a family of optimization problems where the\nobjective can be decomposed as a sum of functions with monotonicity properties.\nThe motivating problem is optimization of hyperparameters of machine learning\nalgorithms, where we argue that the objective, validation error, can be\ndecomposed as monotonic functions of the hyperparameters. Our proposed\nalgorithm adapts Bayesian optimization methods to incorporate the monotonicity\nconstraints. We illustrate the advantages of exploiting monotonicity using\nillustrative examples and demonstrate the improvements in optimization\nefficiency for some machine learning hyperparameter tuning applications. \n\n"}
{"id": "1802.03628", "contents": "Title: Learning Correlation Space for Time Series Abstract: We propose an approximation algorithm for efficient correlation search in\ntime series data. In our method, we use Fourier transform and neural network to\nembed time series into a low-dimensional Euclidean space. The given space is\nlearned such that time series correlation can be effectively approximated from\nEuclidean distance between corresponding embedded vectors. Therefore, search\nfor correlated time series can be done using an index in the embedding space\nfor efficient nearest neighbor search. Our theoretical analysis illustrates\nthat our method's accuracy can be guaranteed under certain regularity\nconditions. We further conduct experiments on real-world datasets and the\nresults show that our method indeed outperforms the baseline solution. In\nparticular, for approximation of correlation, our method reduces the\napproximation loss by a half in most test cases compared to the baseline\nsolution. For top-$k$ highest correlation search, our method improves the\nprecision from 5\\% to 20\\% while the query time is similar to the baseline\napproach query time. \n\n"}
{"id": "1802.03699", "contents": "Title: PCA-Based Missing Information Imputation for Real-Time Crash Likelihood\n  Prediction Under Imbalanced Data Abstract: The real-time crash likelihood prediction has been an important research\ntopic. Various classifiers, such as support vector machine (SVM) and tree-based\nboosting algorithms, have been proposed in traffic safety studies. However, few\nresearch focuses on the missing data imputation in real-time crash likelihood\nprediction, although missing values are commonly observed due to breakdown of\nsensors or external interference. Besides, classifying imbalanced data is also\na difficult problem in real-time crash likelihood prediction, since it is hard\nto distinguish crash-prone cases from non-crash cases which compose the\nmajority of the observed samples. In this paper, principal component analysis\n(PCA) based approaches, including LS-PCA, PPCA, and VBPCA, are employed for\nimputing missing values, while two kinds of solutions are developed to solve\nthe problem in imbalanced data. The results show that PPCA and VBPCA not only\noutperform LS-PCA and other imputation methods (including mean imputation and\nk-means clustering imputation), in terms of the root mean square error (RMSE),\nbut also help the classifiers achieve better predictive performance. The two\nsolutions, i.e., cost-sensitive learning and synthetic minority oversampling\ntechnique (SMOTE), help improve the sensitivity by adjusting the classifiers to\npay more attention to the minority class. \n\n"}
{"id": "1802.03796", "contents": "Title: Curriculum Learning by Transfer Learning: Theory and Experiments with\n  Deep Networks Abstract: We provide theoretical investigation of curriculum learning in the context of\nstochastic gradient descent when optimizing the convex linear regression loss.\nWe prove that the rate of convergence of an ideal curriculum learning method is\nmonotonically increasing with the difficulty of the examples. Moreover, among\nall equally difficult points, convergence is faster when using points which\nincur higher loss with respect to the current hypothesis. We then analyze\ncurriculum learning in the context of training a CNN. We describe a method\nwhich infers the curriculum by way of transfer learning from another network,\npre-trained on a different task. While this approach can only approximate the\nideal curriculum, we observe empirically similar behavior to the one predicted\nby the theory, namely, a significant boost in convergence speed at the\nbeginning of training. When the task is made more difficult, improvement in\ngeneralization performance is also observed. Finally, curriculum learning\nexhibits robustness against unfavorable conditions such as excessive\nregularization. \n\n"}
{"id": "1802.03806", "contents": "Title: ThUnderVolt: Enabling Aggressive Voltage Underscaling and Timing Error\n  Resilience for Energy Efficient Deep Neural Network Accelerators Abstract: Hardware accelerators are being increasingly deployed to boost the\nperformance and energy efficiency of deep neural network (DNN) inference. In\nthis paper we propose Thundervolt, a new framework that enables aggressive\nvoltage underscaling of high-performance DNN accelerators without compromising\nclassification accuracy even in the presence of high timing error rates. Using\npost-synthesis timing simulations of a DNN accelerator modeled on the Google\nTPU, we show that Thundervolt enables between 34%-57% energy savings on\nstate-of-the-art speech and image recognition benchmarks with less than 1% loss\nin classification accuracy and no performance loss. Further, we show that\nThundervolt is synergistic with and can further increase the energy efficiency\nof commonly used run-time DNN pruning techniques like Zero-Skip. \n\n"}
{"id": "1802.03875", "contents": "Title: Pseudo-Recursal: Solving the Catastrophic Forgetting Problem in Deep\n  Neural Networks Abstract: In general, neural networks are not currently capable of learning tasks in a\nsequential fashion. When a novel, unrelated task is learnt by a neural network,\nit substantially forgets how to solve previously learnt tasks. One of the\noriginal solutions to this problem is pseudo-rehearsal, which involves learning\nthe new task while rehearsing generated items representative of the previous\ntask/s. This is very effective for simple tasks. However, pseudo-rehearsal has\nnot yet been successfully applied to very complex tasks because in these tasks\nit is difficult to generate representative items. We accomplish\npseudo-rehearsal by using a Generative Adversarial Network to generate items so\nthat our deep network can learn to sequentially classify the CIFAR-10, SVHN and\nMNIST datasets. After training on all tasks, our network loses only 1.67%\nabsolute accuracy on CIFAR-10 and gains 0.24% absolute accuracy on SVHN. Our\nmodel's performance is a substantial improvement compared to the current state\nof the art solution. \n\n"}
{"id": "1802.04063", "contents": "Title: Taking gradients through experiments: LSTMs and memory proximal policy\n  optimization for black-box quantum control Abstract: In this work we introduce the application of black-box quantum control as an\ninteresting rein- forcement learning problem to the machine learning community.\nWe analyze the structure of the reinforcement learning problems arising in\nquantum physics and argue that agents parameterized by long short-term memory\n(LSTM) networks trained via stochastic policy gradients yield a general method\nto solving them. In this context we introduce a variant of the proximal policy\noptimization (PPO) algorithm called the memory proximal policy optimization\n(MPPO) which is based on this analysis. We then show how it can be applied to\nspecific learning tasks and present results of nu- merical experiments showing\nthat our method achieves state-of-the-art results for several learning tasks in\nquantum control with discrete and continouous control parameters. \n\n"}
{"id": "1802.04623", "contents": "Title: Logarithmic Regret for Online Gradient Descent Beyond Strong Convexity Abstract: Hoffman's classical result gives a bound on the distance of a point from a\nconvex and compact polytope in terms of the magnitude of violation of the\nconstraints. Recently, several results showed that Hoffman's bound can be used\nto derive strongly-convex-like rates for first-order methods for\n\\textit{offline} convex optimization of curved, though not strongly convex,\nfunctions, over polyhedral sets. In this work, we use this classical result for\nthe first time to obtain faster rates for \\textit{online convex optimization}\nover polyhedral sets with curved convex, though not strongly convex, loss\nfunctions. We show that under several reasonable assumptions on the data, the\nstandard \\textit{Online Gradient Descent} algorithm guarantees logarithmic\nregret. To the best of our knowledge, the only previous algorithm to achieve\nlogarithmic regret in the considered settings is the \\textit{Online Newton\nStep} algorithm which requires quadratic (in the dimension) memory and at least\nquadratic runtime per iteration, which greatly limits its applicability to\nlarge-scale problems. In particular, our results hold for\n\\textit{semi-adversarial} settings in which the data is a combination of an\narbitrary (adversarial) sequence and a stochastic sequence, which might provide\nreasonable approximation for many real-world sequences, or under a natural\nassumption that the data is low-rank. We demonstrate via experiments that the\nregret of OGD is indeed comparable to that of ONS (and even far better) on\ncurved though not strongly-convex losses. \n\n"}
{"id": "1802.04796", "contents": "Title: Stochastic Variance-Reduced Cubic Regularized Newton Method Abstract: We propose a stochastic variance-reduced cubic regularized Newton method for\nnon-convex optimization. At the core of our algorithm is a novel\nsemi-stochastic gradient along with a semi-stochastic Hessian, which are\nspecifically designed for cubic regularization method. We show that our\nalgorithm is guaranteed to converge to an\n$(\\epsilon,\\sqrt{\\epsilon})$-approximately local minimum within\n$\\tilde{O}(n^{4/5}/\\epsilon^{3/2})$ second-order oracle calls, which\noutperforms the state-of-the-art cubic regularization algorithms including\nsubsampled cubic regularization. Our work also sheds light on the application\nof variance reduction technique to high-order non-convex optimization methods.\nThorough experiments on various non-convex optimization problems support our\ntheory. \n\n"}
{"id": "1802.04865", "contents": "Title: Learning Confidence for Out-of-Distribution Detection in Neural Networks Abstract: Modern neural networks are very powerful predictive models, but they are\noften incapable of recognizing when their predictions may be wrong. Closely\nrelated to this is the task of out-of-distribution detection, where a network\nmust determine whether or not an input is outside of the set on which it is\nexpected to safely perform. To jointly address these issues, we propose a\nmethod of learning confidence estimates for neural networks that is simple to\nimplement and produces intuitively interpretable outputs. We demonstrate that\non the task of out-of-distribution detection, our technique surpasses recently\nproposed techniques which construct confidence based on the network's output\ndistribution, without requiring any additional labels or access to\nout-of-distribution examples. Additionally, we address the problem of\ncalibrating out-of-distribution detectors, where we demonstrate that\nmisclassified in-distribution examples can be used as a proxy for\nout-of-distribution examples. \n\n"}
{"id": "1802.04942", "contents": "Title: Isolating Sources of Disentanglement in Variational Autoencoders Abstract: We decompose the evidence lower bound to show the existence of a term\nmeasuring the total correlation between latent variables. We use this to\nmotivate our $\\beta$-TCVAE (Total Correlation Variational Autoencoder), a\nrefinement of the state-of-the-art $\\beta$-VAE objective for learning\ndisentangled representations, requiring no additional hyperparameters during\ntraining. We further propose a principled classifier-free measure of\ndisentanglement called the mutual information gap (MIG). We perform extensive\nquantitative and qualitative experiments, in both restricted and non-restricted\nsettings, and show a strong relation between total correlation and\ndisentanglement, when the latent variables model is trained using our\nframework. \n\n"}
{"id": "1802.04947", "contents": "Title: Attack RMSE Leaderboard: An Introduction and Case Study Abstract: In this manuscript, we briefly introduce several tricks to climb the\nleaderboards which use RMSE for evaluation without exploiting any training\ndata. \n\n"}
{"id": "1802.05027", "contents": "Title: Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical\n  Care Abstract: Patients in the intensive care unit (ICU) require constant and close\nsupervision. To assist clinical staff in this task, hospitals use monitoring\nsystems that trigger audiovisual alarms if their algorithms indicate that a\npatient's condition may be worsening. However, current monitoring systems are\nextremely sensitive to movement artefacts and technical errors. As a result,\nthey typically trigger hundreds to thousands of false alarms per patient per\nday - drowning the important alarms in noise and adding to the exhaustion of\nclinical staff. In this setting, data is abundantly available, but obtaining\ntrustworthy annotations by experts is laborious and expensive. We frame the\nproblem of false alarm reduction from multivariate time series as a\nmachine-learning task and address it with a novel multitask network\narchitecture that utilises distant supervision through multiple related\nauxiliary tasks in order to reduce the number of expensive labels required for\ntraining. We show that our approach leads to significant improvements over\nseveral state-of-the-art baselines on real-world ICU data and provide new\ninsights on the importance of task selection and architectural choices in\ndistantly supervised multitask learning. \n\n"}
{"id": "1802.05193", "contents": "Title: Security Analysis and Enhancement of Model Compressed Deep Learning\n  Systems under Adversarial Attacks Abstract: DNN is presenting human-level performance for many complex intelligent tasks\nin real-world applications. However, it also introduces ever-increasing\nsecurity concerns. For example, the emerging adversarial attacks indicate that\neven very small and often imperceptible adversarial input perturbations can\neasily mislead the cognitive function of deep learning systems (DLS). Existing\nDNN adversarial studies are narrowly performed on the ideal software-level DNN\nmodels with a focus on single uncertainty factor, i.e. input perturbations,\nhowever, the impact of DNN model reshaping on adversarial attacks, which is\nintroduced by various hardware-favorable techniques such as hash-based weight\ncompression during modern DNN hardware implementation, has never been\ndiscussed. In this work, we for the first time investigate the multi-factor\nadversarial attack problem in practical model optimized deep learning systems\nby jointly considering the DNN model-reshaping (e.g. HashNet based deep\ncompression) and the input perturbations. We first augment adversarial example\ngenerating method dedicated to the compressed DNN models by incorporating the\nsoftware-based approaches and mathematical modeled DNN reshaping. We then\nconduct a comprehensive robustness and vulnerability analysis of deep\ncompressed DNN models under derived adversarial attacks. A defense technique\nnamed \"gradient inhibition\" is further developed to ease the generating of\nadversarial examples thus to effectively mitigate adversarial attacks towards\nboth software and hardware-oriented DNNs. Simulation results show that\n\"gradient inhibition\" can decrease the average success rate of adversarial\nattacks from 87.99% to 4.77% (from 86.74% to 4.64%) on MNIST (CIFAR-10)\nbenchmark with marginal accuracy degradation across various DNNs. \n\n"}
{"id": "1802.05380", "contents": "Title: Active Feature Acquisition with Supervised Matrix Completion Abstract: Feature missing is a serious problem in many applications, which may lead to\nlow quality of training data and further significantly degrade the learning\nperformance. While feature acquisition usually involves special devices or\ncomplex process, it is expensive to acquire all feature values for the whole\ndataset. On the other hand, features may be correlated with each other, and\nsome values may be recovered from the others. It is thus important to decide\nwhich features are most informative for recovering the other features as well\nas improving the learning performance. In this paper, we try to train an\neffective classification model with least acquisition cost by jointly\nperforming active feature querying and supervised matrix completion. When\ncompleting the feature matrix, a novel target function is proposed to\nsimultaneously minimize the reconstruction error on observed entries and the\nsupervised loss on training data. When querying the feature value, the most\nuncertain entry is actively selected based on the variance of previous\niterations. In addition, a bi-objective optimization method is presented for\ncost-aware active selection when features bear different acquisition costs. The\neffectiveness of the proposed approach is well validated by both theoretical\nanalysis and experimental study. \n\n"}
{"id": "1802.05405", "contents": "Title: Putting a bug in ML: The moth olfactory network learns to read MNIST Abstract: We seek to (i) characterize the learning architectures exploited in\nbiological neural networks for training on very few samples, and (ii) port\nthese algorithmic structures to a machine learning context. The Moth Olfactory\nNetwork is among the simplest biological neural systems that can learn, and its\narchitecture includes key structural elements and mechanisms widespread in\nbiological neural nets, such as cascaded networks, competitive inhibition, high\nintrinsic noise, sparsity, reward mechanisms, and Hebbian plasticity. These\nstructural biological elements, in combination, enable rapid learning.\n  MothNet is a computational model of the Moth Olfactory Network, closely\naligned with the moth's known biophysics and with in vivo electrode data\ncollected from moths learning new odors. We assign this model the task of\nlearning to read the MNIST digits. We show that MothNet successfully learns to\nread given very few training samples (1 to 10 samples per class). In this\nfew-samples regime, it outperforms standard machine learning methods such as\nnearest-neighbors, support-vector machines, and neural networks (NNs), and\nmatches specialized one-shot transfer-learning methods but without the need for\npre-training. The MothNet architecture illustrates how algorithmic structures\nderived from biological brains can be used to build alternative NNs that may\navoid some of the learning rate limitations of current engineered NNs. \n\n"}
{"id": "1802.05411", "contents": "Title: Selecting the Best in GANs Family: a Post Selection Inference Framework Abstract: \"Which Generative Adversarial Networks (GANs) generates the most plausible\nimages?\" has been a frequently asked question among researchers. To address\nthis problem, we first propose an \\emph{incomplete} U-statistics estimate of\nmaximum mean discrepancy $\\mathrm{MMD}_{inc}$ to measure the distribution\ndiscrepancy between generated and real images. $\\mathrm{MMD}_{inc}$ enjoys the\nadvantages of asymptotic normality, computation efficiency, and model\nagnosticity. We then propose a GANs analysis framework to select and test the\n\"best\" member in GANs family using the Post Selection Inference (PSI) with\n$\\mathrm{MMD}_{inc}$. In the experiments, we adopt the proposed framework on 7\nGANs variants and compare their $\\mathrm{MMD}_{inc}$ scores. \n\n"}
{"id": "1802.06355", "contents": "Title: Stochastic Chebyshev Gradient Descent for Spectral Optimization Abstract: A large class of machine learning techniques requires the solution of\noptimization problems involving spectral functions of parametric matrices, e.g.\nlog-determinant and nuclear norm. Unfortunately, computing the gradient of a\nspectral function is generally of cubic complexity, as such gradient descent\nmethods are rather expensive for optimizing objectives involving the spectral\nfunction. Thus, one naturally turns to stochastic gradient methods in hope that\nthey will provide a way to reduce or altogether avoid the computation of full\ngradients. However, here a new challenge appears: there is no straightforward\nway to compute unbiased stochastic gradients for spectral functions. In this\npaper, we develop unbiased stochastic gradients for spectral-sums, an important\nsubclass of spectral functions. Our unbiased stochastic gradients are based on\ncombining randomized trace estimators with stochastic truncation of the\nChebyshev expansions. A careful design of the truncation distribution allows us\nto offer distributions that are variance-optimal, which is crucial for fast and\nstable convergence of stochastic gradient methods. We further leverage our\nproposed stochastic gradients to devise stochastic methods for objective\nfunctions involving spectral-sums, and rigorously analyze their convergence\nrate. The utility of our methods is demonstrated in numerical experiments. \n\n"}
{"id": "1802.06367", "contents": "Title: Efficient Sparse-Winograd Convolutional Neural Networks Abstract: Convolutional Neural Networks (CNNs) are computationally intensive, which\nlimits their application on mobile devices. Their energy is dominated by the\nnumber of multiplies needed to perform the convolutions. Winograd's minimal\nfiltering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can\nreduce the operation count, but these two methods cannot be directly combined\n$-$ applying the Winograd transform fills in the sparsity in both the weights\nand the activations. We propose two modifications to Winograd-based CNNs to\nenable these methods to exploit sparsity. First, we move the ReLU operation\ninto the Winograd domain to increase the sparsity of the transformed\nactivations. Second, we prune the weights in the Winograd domain to exploit\nstatic weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet\ndatasets, our method reduces the number of multiplications by $10.4\\times$,\n$6.8\\times$ and $10.8\\times$ respectively with loss of accuracy less than\n$0.1\\%$, outperforming previous baselines by $2.0\\times$-$3.0\\times$. We also\nshow that moving ReLU to the Winograd domain allows more aggressive pruning. \n\n"}
{"id": "1802.06677", "contents": "Title: Degeneration in VAE: in the Light of Fisher Information Loss Abstract: While enormous progress has been made to Variational Autoencoder (VAE) in\nrecent years, similar to other deep networks, VAE with deep networks suffers\nfrom the problem of degeneration, which seriously weakens the correlation\nbetween the input and the corresponding latent codes, deviating from the goal\nof the representation learning. To investigate how degeneration affects VAE\nfrom a theoretical perspective, we illustrate the information transmission in\nVAE and analyze the intermediate layers of the encoders/decoders. Specifically,\nwe propose a Fisher Information measure for the layer-wise analysis. With such\nmeasure, we demonstrate that information loss is ineluctable in feed-forward\nnetworks and causes the degeneration in VAE. We show that skip connections in\nVAE enable the preservation of information without changing the model\narchitecture. We call this class of VAE equipped with skip connections as SCVAE\nand perform a range of experiments to show its advantages in information\npreservation and degeneration mitigation. \n\n"}
{"id": "1802.06847", "contents": "Title: Distribution Matching in Variational Inference Abstract: With the increasingly widespread deployment of generative models, there is a\nmounting need for a deeper understanding of their behaviors and limitations. In\nthis paper, we expose the limitations of Variational Autoencoders (VAEs), which\nconsistently fail to learn marginal distributions in both latent and visible\nspaces. We show this to be a consequence of learning by matching conditional\ndistributions, and the limitations of explicit model and posterior\ndistributions. It is popular to consider Generative Adversarial Networks (GANs)\nas a means of overcoming these limitations, leading to hybrids of VAEs and\nGANs. We perform a large-scale evaluation of several VAE-GAN hybrids and\nanalyze the implications of class probability estimation for learning\ndistributions. While promising, we conclude that at present, VAE-GAN hybrids\nhave limited applicability: they are harder to scale, evaluate, and use for\ninference compared to VAEs; and they do not improve over the generation quality\nof GANs. \n\n"}
{"id": "1802.07384", "contents": "Title: Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic\n  Corrections Abstract: We present a new algorithm to generate minimal, stable, and symbolic\ncorrections to an input that will cause a neural network with ReLU activations\nto change its output. We argue that such a correction is a useful way to\nprovide feedback to a user when the network's output is different from a\ndesired output. Our algorithm generates such a correction by solving a series\nof linear constraint satisfaction problems. The technique is evaluated on three\nneural network models: one predicting whether an applicant will pay a mortgage,\none predicting whether a first-order theorem can be proved efficiently by a\nsolver using certain heuristics, and the final one judging whether a drawing is\nan accurate rendition of a canonical drawing of a cat. \n\n"}
{"id": "1802.07714", "contents": "Title: Detecting Learning vs Memorization in Deep Neural Networks using Shared\n  Structure Validation Sets Abstract: The roles played by learning and memorization represent an important topic in\ndeep learning research. Recent work on this subject has shown that the\noptimization behavior of DNNs trained on shuffled labels is qualitatively\ndifferent from DNNs trained with real labels. Here, we propose a novel\npermutation approach that can differentiate memorization from learning in deep\nneural networks (DNNs) trained as usual (i.e., using the real labels to guide\nthe learning, rather than shuffled labels). The evaluation of weather the DNN\nhas learned and/or memorized, happens in a separate step where we compare the\npredictive performance of a shallow classifier trained with the features\nlearned by the DNN, against multiple instances of the same classifier, trained\non the same input, but using shuffled labels as outputs. By evaluating these\nshallow classifiers in validation sets that share structure with the training\nset, we are able to tell apart learning from memorization. Application of our\npermutation approach to multi-layer perceptrons and convolutional neural\nnetworks trained on image data corroborated many findings from other groups.\nMost importantly, our illustrations also uncovered interesting dynamic patterns\nabout how DNNs memorize over increasing numbers of training epochs, and support\nthe surprising result that DNNs are still able to learn, rather than only\nmemorize, when trained with pure Gaussian noise as input. \n\n"}
{"id": "1802.08054", "contents": "Title: VBALD - Variational Bayesian Approximation of Log Determinants Abstract: Evaluating the log determinant of a positive definite matrix is ubiquitous in\nmachine learning. Applications thereof range from Gaussian processes,\nminimum-volume ellipsoids, metric learning, kernel learning, Bayesian neural\nnetworks, Determinental Point Processes, Markov random fields to partition\nfunctions of discrete graphical models. In order to avoid the canonical, yet\nprohibitive, Cholesky $\\mathcal{O}(n^{3})$ computational cost, we propose a\nnovel approach, with complexity $\\mathcal{O}(n^{2})$, based on a constrained\nvariational Bayes algorithm. We compare our method to Taylor, Chebyshev and\nLanczos approaches and show state of the art performance on both synthetic and\nreal-world datasets. \n\n"}
{"id": "1802.08665", "contents": "Title: Learning Latent Permutations with Gumbel-Sinkhorn Networks Abstract: Permutations and matchings are core building blocks in a variety of latent\nvariable models, as they allow us to align, canonicalize, and sort data.\nLearning in such models is difficult, however, because exact marginalization\nover these combinatorial objects is intractable. In response, this paper\nintroduces a collection of new methods for end-to-end learning in such models\nthat approximate discrete maximum-weight matching using the continuous Sinkhorn\noperator. Sinkhorn iteration is attractive because it functions as a simple,\neasy-to-implement analog of the softmax operator. With this, we can define the\nGumbel-Sinkhorn method, an extension of the Gumbel-Softmax method (Jang et al.\n2016, Maddison2016 et al. 2016) to distributions over latent matchings. We\ndemonstrate the effectiveness of our method by outperforming competitive\nbaselines on a range of qualitatively different tasks: sorting numbers, solving\njigsaw puzzles, and identifying neural signals in worms. \n\n"}
{"id": "1802.09850", "contents": "Title: Solving Inverse Computational Imaging Problems using Deep Pixel-level\n  Prior Abstract: Signal reconstruction is a challenging aspect of computational imaging as it\noften involves solving ill-posed inverse problems. Recently, deep feed-forward\nneural networks have led to state-of-the-art results in solving various inverse\nimaging problems. However, being task specific, these networks have to be\nlearned for each inverse problem. On the other hand, a more flexible approach\nwould be to learn a deep generative model once and then use it as a signal\nprior for solving various inverse problems. We show that among the various\nstate of the art deep generative models, autoregressive models are especially\nsuitable for our purpose for the following reasons. First, they explicitly\nmodel the pixel level dependencies and hence are capable of reconstructing\nlow-level details such as texture patterns and edges better. Second, they\nprovide an explicit expression for the image prior which can then be used for\nMAP based inference along with the forward model. Third, they can model long\nrange dependencies in images which make them ideal for handling global\nmultiplexing as encountered in various compressive imaging systems. We\ndemonstrate the efficacy of our proposed approach in solving three\ncomputational imaging problems: Single Pixel Camera (SPC), LiSens and FlatCam.\nFor both real and simulated cases, we obtain better reconstructions than the\nstate-of-the-art methods in terms of perceptual and quantitative metrics. \n\n"}
{"id": "1802.10217", "contents": "Title: Investigating Human Priors for Playing Video Games Abstract: What makes humans so good at solving seemingly complex video games? Unlike\ncomputers, humans bring in a great deal of prior knowledge about the world,\nenabling efficient decision making. This paper investigates the role of human\npriors for solving video games. Given a sample game, we conduct a series of\nablation studies to quantify the importance of various priors on human\nperformance. We do this by modifying the video game environment to\nsystematically mask different types of visual information that could be used by\nhumans as priors. We find that removal of some prior knowledge causes a drastic\ndegradation in the speed with which human players solve the game, e.g. from 2\nminutes to over 20 minutes. Furthermore, our results indicate that general\npriors, such as the importance of objects and visual consistency, are critical\nfor efficient game-play. Videos and the game manipulations are available at\nhttps://rach0012.github.io/humanRL_website/ \n\n"}
{"id": "1802.10235", "contents": "Title: Parametrized Accelerated Methods Free of Condition Number Abstract: Analyses of accelerated (momentum-based) gradient descent usually assume\nbounded condition number to obtain exponential convergence rates. However, in\nmany real problems, e.g., kernel methods or deep neural networks, the condition\nnumber, even locally, can be unbounded, unknown or mis-estimated. This poses\nproblems in both implementing and analyzing accelerated algorithms. In this\npaper, we address this issue by proposing parametrized accelerated methods by\nconsidering the condition number as a free parameter. We provide spectral-level\nanalysis for several important accelerated algorithms, obtain explicit\nexpressions and improve worst case convergence rates. Moreover, we show that\nthose algorithm converge exponentially even when the condition number is\nunknown or mis-estimated. \n\n"}
{"id": "1802.10353", "contents": "Title: Relational Neural Expectation Maximization: Unsupervised Discovery of\n  Objects and their Interactions Abstract: Common-sense physical reasoning is an essential ingredient for any\nintelligent agent operating in the real-world. For example, it can be used to\nsimulate the environment, or to infer the state of parts of the world that are\ncurrently unobserved. In order to match real-world conditions this causal\nknowledge must be learned without access to supervised data. To address this\nproblem we present a novel method that learns to discover objects and model\ntheir physical interactions from raw visual images in a purely\n\\emph{unsupervised} fashion. It incorporates prior knowledge about the\ncompositional nature of human perception to factor interactions between\nobject-pairs and learn efficiently. On videos of bouncing balls we show the\nsuperior modelling capabilities of our method compared to other unsupervised\nneural approaches that do not incorporate such prior knowledge. We demonstrate\nits ability to handle occlusion and show that it can extrapolate learned\nknowledge to scenes with different numbers of objects. \n\n"}
{"id": "1802.10542", "contents": "Title: Memory-based Parameter Adaptation Abstract: Deep neural networks have excelled on a wide range of problems, from vision\nto language and game playing. Neural networks very gradually incorporate\ninformation into weights as they process data, requiring very low learning\nrates. If the training distribution shifts, the network is slow to adapt, and\nwhen it does adapt, it typically performs badly on the training distribution\nbefore the shift. Our method, Memory-based Parameter Adaptation, stores\nexamples in memory and then uses a context-based lookup to directly modify the\nweights of a neural network. Much higher learning rates can be used for this\nlocal adaptation, reneging the need for many iterations over similar data\nbefore good predictions can be made. As our method is memory-based, it\nalleviates several shortcomings of neural networks, such as catastrophic\nforgetting, fast, stable acquisition of new knowledge, learning with an\nimbalanced class labels, and fast learning during evaluation. We demonstrate\nthis on a range of supervised tasks: large-scale image classification and\nlanguage modelling. \n\n"}
{"id": "1803.00116", "contents": "Title: Separators and Adjustment Sets in Causal Graphs: Complete Criteria and\n  an Algorithmic Framework Abstract: Principled reasoning about the identifiability of causal effects from\nnon-experimental data is an important application of graphical causal models.\nThis paper focuses on effects that are identifiable by covariate adjustment, a\ncommonly used estimation approach. We present an algorithmic framework for\nefficiently testing, constructing, and enumerating $m$-separators in ancestral\ngraphs (AGs), a class of graphical causal models that can represent uncertainty\nabout the presence of latent confounders. Furthermore, we prove a reduction\nfrom causal effect identification by covariate adjustment to $m$-separation in\na subgraph for directed acyclic graphs (DAGs) and maximal ancestral graphs\n(MAGs). Jointly, these results yield constructive criteria that characterize\nall adjustment sets as well as all minimal and minimum adjustment sets for\nidentification of a desired causal effect with multivariate exposures and\noutcomes in the presence of latent confounding. Our results extend several\nexisting solutions for special cases of these problems. Our efficient\nalgorithms allowed us to empirically quantify the identifiability gap between\ncovariate adjustment and the do-calculus in random DAGs and MAGs, covering a\nwide range of scenarios. Implementations of our algorithms are provided in the\nR package dagitty. \n\n"}
{"id": "1803.00250", "contents": "Title: Distance Measure Machines Abstract: This paper presents a distance-based discriminative framework for learning\nwith probability distributions. Instead of using kernel mean embeddings or\ngeneralized radial basis kernels, we introduce embeddings based on\ndissimilarity of distributions to some reference distributions denoted as\ntemplates. Our framework extends the theory of similarity of Balcan et al.\n(2008) to the population distribution case and we show that, for some learning\nproblems, some dissimilarity on distribution achieves low-error linear decision\nfunctions with high probability. Our key result is to prove that the theory\nalso holds for empirical distributions. Algorithmically, the proposed approach\nconsists in computing a mapping based on pairwise dissimilarity where learning\na linear decision function is amenable. Our experimental results show that the\nWasserstein distance embedding performs better than kernel mean embeddings and\ncomputing Wasserstein distance is far more tractable than estimating pairwise\nKullback-Leibler divergence of empirical distributions. \n\n"}
{"id": "1803.00651", "contents": "Title: Static and Dynamic Robust PCA and Matrix Completion: A Review Abstract: Principal Components Analysis (PCA) is one of the most widely used dimension\nreduction techniques. Robust PCA (RPCA) refers to the problem of PCA when the\ndata may be corrupted by outliers. Recent work by Cand{\\`e}s, Wright, Li, and\nMa defined RPCA as a problem of decomposing a given data matrix into the sum of\na low-rank matrix (true data) and a sparse matrix (outliers). The column space\nof the low-rank matrix then gives the PCA solution. This simple definition has\nlead to a large amount of interesting new work on provably correct, fast, and\npractical solutions to RPCA. More recently, the dynamic (time-varying) version\nof the RPCA problem has been studied and a series of provably correct, fast,\nand memory efficient tracking solutions have been proposed. Dynamic RPCA (or\nrobust subspace tracking) is the problem of tracking data lying in a (slowly)\nchanging subspace while being robust to sparse outliers. This article provides\nan exhaustive review of the last decade of literature on RPCA and its dynamic\ncounterpart (robust subspace tracking), along with describing their theoretical\nguarantees, discussing the pros and cons of various approaches, and providing\nempirical comparisons of performance and speed.\n  A brief overview of the (low-rank) matrix completion literature is also\nprovided (the focus is on works not discussed in other recent reviews). This\nrefers to the problem of completing a low-rank matrix when only a subset of its\nentries are observed. It can be interpreted as a simpler special case of RPCA\nin which the indices of the outlier corrupted entries are known. \n\n"}
{"id": "1803.00684", "contents": "Title: Autostacker: A Compositional Evolutionary Learning System Abstract: We introduce an automatic machine learning (AutoML) modeling architecture\ncalled Autostacker, which combines an innovative hierarchical stacking\narchitecture and an Evolutionary Algorithm (EA) to perform efficient parameter\nsearch. Neither prior domain knowledge about the data nor feature preprocessing\nis needed. Using EA, Autostacker quickly evolves candidate pipelines with high\npredictive accuracy. These pipelines can be used as is or as a starting point\nfor human experts to build on. Autostacker finds innovative combinations and\nstructures of machine learning models, rather than selecting a single model and\noptimizing its hyperparameters. Compared with other AutoML systems on fifteen\ndatasets, Autostacker achieves state-of-art or competitive performance both in\nterms of test accuracy and time cost. \n\n"}
{"id": "1803.01442", "contents": "Title: Stochastic Activation Pruning for Robust Adversarial Defense Abstract: Neural networks are known to be vulnerable to adversarial examples. Carefully\nchosen perturbations to real images, while imperceptible to humans, induce\nmisclassification and threaten the reliability of deep learning systems in the\nwild. To guard against adversarial examples, we take inspiration from game\ntheory and cast the problem as a minimax zero-sum game between the adversary\nand the model. In general, for such games, the optimal strategy for both\nplayers requires a stochastic policy, also known as a mixed strategy. In this\nlight, we propose Stochastic Activation Pruning (SAP), a mixed strategy for\nadversarial defense. SAP prunes a random subset of activations (preferentially\npruning those with smaller magnitude) and scales up the survivors to\ncompensate. We can apply SAP to pretrained networks, including adversarially\ntrained models, without fine-tuning, providing robustness against adversarial\nexamples. Experiments demonstrate that SAP confers robustness against attacks,\nincreasing accuracy and preserving calibration. \n\n"}
{"id": "1803.01498", "contents": "Title: Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates Abstract: In large-scale distributed learning, security issues have become increasingly\nimportant. Particularly in a decentralized environment, some computing units\nmay behave abnormally, or even exhibit Byzantine failures -- arbitrary and\npotentially adversarial behavior. In this paper, we develop distributed\nlearning algorithms that are provably robust against such failures, with a\nfocus on achieving optimal statistical performance. A main result of this work\nis a sharp analysis of two robust distributed gradient descent algorithms based\non median and trimmed mean operations, respectively. We prove statistical error\nrates for three kinds of population loss functions: strongly convex,\nnon-strongly convex, and smooth non-convex. In particular, these algorithms are\nshown to achieve order-optimal statistical error rates for strongly convex\nlosses. To achieve better communication efficiency, we further propose a\nmedian-based distributed algorithm that is provably robust, and uses only one\ncommunication round. For strongly convex quadratic loss, we show that this\nalgorithm achieves the same optimal error rate as the robust distributed\ngradient descent algorithms. \n\n"}
{"id": "1803.01526", "contents": "Title: Blind Channel Equalization using Variational Autoencoders Abstract: A new maximum likelihood estimation approach for blind channel equalization,\nusing variational autoencoders (VAEs), is introduced. Significant and\nconsistent improvements in the error rate of the reconstructed symbols,\ncompared to constant modulus equalizers, are demonstrated. In fact, for the\nchannels that were examined, the performance of the new VAE blind channel\nequalizer was close to the performance of a nonblind adaptive linear minimum\nmean square error equalizer. The new equalization method enables a\nsignificantly lower latency channel acquisition compared to the constant\nmodulus algorithm (CMA). The VAE uses a convolutional neural network with two\nlayers and a very small number of free parameters. Although the computational\ncomplexity of the new equalizer is higher compared to CMA, it is still\nreasonable, and the number of free parameters to estimate is small. \n\n"}
{"id": "1803.02021", "contents": "Title: Understanding Short-Horizon Bias in Stochastic Meta-Optimization Abstract: Careful tuning of the learning rate, or even schedules thereof, can be\ncrucial to effective neural net training. There has been much recent interest\nin gradient-based meta-optimization, where one tunes hyperparameters, or even\nlearns an optimizer, in order to minimize the expected loss when the training\nprocedure is unrolled. But because the training procedure must be unrolled\nthousands of times, the meta-objective must be defined with an\norders-of-magnitude shorter time horizon than is typical for neural net\ntraining. We show that such short-horizon meta-objectives cause a serious bias\ntowards small step sizes, an effect we term short-horizon bias. We introduce a\ntoy problem, a noisy quadratic cost function, on which we analyze short-horizon\nbias by deriving and comparing the optimal schedules for short and long time\nhorizons. We then run meta-optimization experiments (both offline and online)\non standard benchmark datasets, showing that meta-optimization chooses too\nsmall a learning rate by multiple orders of magnitude, even when run with a\nmoderately long time horizon (100 steps) typical of work in the area. We\nbelieve short-horizon bias is a fundamental problem that needs to be addressed\nif meta-optimization is to scale to practical neural net training regimes. \n\n"}
{"id": "1803.02042", "contents": "Title: Accelerated Gradient Boosting Abstract: Gradient tree boosting is a prediction algorithm that sequentially produces a\nmodel in the form of linear combinations of decision trees, by solving an\ninfinite-dimensional optimization problem. We combine gradient boosting and\nNesterov's accelerated descent to design a new algorithm, which we call AGB\n(for Accelerated Gradient Boosting). Substantial numerical evidence is provided\non both synthetic and real-life data sets to assess the excellent performance\nof the method in a large variety of prediction problems. It is empirically\nshown that AGB is much less sensitive to the shrinkage parameter and outputs\npredictors that are considerably more sparse in the number of trees, while\nretaining the exceptional performance of gradient boosting. \n\n"}
{"id": "1803.02194", "contents": "Title: Bidding Machine: Learning to Bid for Directly Optimizing Profits in\n  Display Advertising Abstract: Real-time bidding (RTB) based display advertising has become one of the key\ntechnological advances in computational advertising. RTB enables advertisers to\nbuy individual ad impressions via an auction in real-time and facilitates the\nevaluation and the bidding of individual impressions across multiple\nadvertisers. In RTB, the advertisers face three main challenges when optimizing\ntheir bidding strategies, namely (i) estimating the utility (e.g., conversions,\nclicks) of the ad impression, (ii) forecasting the market value (thus the cost)\nof the given ad impression, and (iii) deciding the optimal bid for the given\nauction based on the first two. Previous solutions assume the first two are\nsolved before addressing the bid optimization problem. However, these\nchallenges are strongly correlated and dealing with any individual problem\nindependently may not be globally optimal. In this paper, we propose Bidding\nMachine, a comprehensive learning to bid framework, which consists of three\noptimizers dealing with each challenge above, and as a whole, jointly optimizes\nthese three parts. We show that such a joint optimization would largely\nincrease the campaign effectiveness and the profit. From the learning\nperspective, we show that the bidding machine can be updated smoothly with both\noffline periodical batch or online sequential training schemes. Our extensive\noffline empirical study and online A/B testing verify the high effectiveness of\nthe proposed bidding machine. \n\n"}
{"id": "1803.02312", "contents": "Title: Dimensionality Reduction for Stationary Time Series via Stochastic\n  Nonconvex Optimization Abstract: Stochastic optimization naturally arises in machine learning. Efficient\nalgorithms with provable guarantees, however, are still largely missing, when\nthe objective function is nonconvex and the data points are dependent. This\npaper studies this fundamental challenge through a streaming PCA problem for\nstationary time series data. Specifically, our goal is to estimate the\nprinciple component of time series data with respect to the covariance matrix\nof the stationary distribution. Computationally, we propose a variant of Oja's\nalgorithm combined with downsampling to control the bias of the stochastic\ngradient caused by the data dependency. Theoretically, we quantify the\nuncertainty of our proposed stochastic algorithm based on diffusion\napproximations. This allows us to prove the asymptotic rate of convergence and\nfurther implies near optimal asymptotic sample complexity. Numerical\nexperiments are provided to support our analysis. \n\n"}
{"id": "1803.02782", "contents": "Title: A bag-to-class divergence approach to multiple-instance learning Abstract: In multi-instance (MI) learning, each object (bag) consists of multiple\nfeature vectors (instances), and is most commonly regarded as a set of points\nin a multidimensional space. A different viewpoint is that the instances are\nrealisations of random vectors with corresponding probability distribution, and\nthat a bag is the distribution, not the realisations. In MI classification,\neach bag in the training set has a class label, but the instances are\nunlabelled. By introducing the probability distribution space to bag-level\nclassification problems, dissimilarities between probability distributions\n(divergences) can be applied. The bag-to-bag Kullback-Leibler information is\nasymptotically the best classifier, but the typical sparseness of MI training\nsets is an obstacle. We introduce bag-to-class divergence to MI learning,\nemphasising the hierarchical nature of the random vectors that makes bags from\nthe same class different. We propose two properties for bag-to-class\ndivergences, and an additional property for sparse training sets. \n\n"}
{"id": "1803.02855", "contents": "Title: Satisficing in Time-Sensitive Bandit Learning Abstract: Much of the recent literature on bandit learning focuses on algorithms that\naim to converge on an optimal action. One shortcoming is that this orientation\ndoes not account for time sensitivity, which can play a crucial role when\nlearning an optimal action requires much more information than near-optimal\nones. Indeed, popular approaches such as upper-confidence-bound methods and\nThompson sampling can fare poorly in such situations. We consider instead\nlearning a satisficing action, which is near-optimal while requiring less\ninformation, and propose satisficing Thompson sampling, an algorithm that\nserves this purpose. We establish a general bound on expected discounted regret\nand study the application of satisficing Thompson sampling to linear and\ninfinite-armed bandits, demonstrating arbitrarily large benefits over Thompson\nsampling. We also discuss the relation between the notion of satisficing and\nthe theory of rate distortion, which offers guidance on the selection of\nsatisficing actions. \n\n"}
{"id": "1803.02879", "contents": "Title: Deep Models of Interactions Across Sets Abstract: We use deep learning to model interactions across two or more sets of\nobjects, such as user-movie ratings, protein-drug bindings, or ternary\nuser-item-tag interactions. The canonical representation of such interactions\nis a matrix (or a higher-dimensional tensor) with an exchangeability property:\nthe encoding's meaning is not changed by permuting rows or columns. We argue\nthat models should hence be Permutation Equivariant (PE): constrained to make\nthe same predictions across such permutations. We present a parameter-sharing\nscheme and prove that it could not be made any more expressive without\nviolating PE. This scheme yields three benefits. First, we demonstrate\nstate-of-the-art performance on multiple matrix completion benchmarks. Second,\nour models require a number of parameters independent of the numbers of\nobjects, and thus scale well to large datasets. Third, models can be queried\nabout new objects that were not available at training time, but for which\ninteractions have since been observed. In experiments, our models achieved\nsurprisingly good generalization performance on this matrix extrapolation task,\nboth within domains (e.g., new users and new movies drawn from the same\ndistribution used for training) and even across domains (e.g., predicting music\nratings after training on movies). \n\n"}
{"id": "1803.03148", "contents": "Title: Generating Artificial Data for Private Deep Learning Abstract: In this paper, we propose generating artificial data that retain statistical\nproperties of real data as the means of providing privacy with respect to the\noriginal dataset. We use generative adversarial network to draw\nprivacy-preserving artificial data samples and derive an empirical method to\nassess the risk of information disclosure in a differential-privacy-like way.\nOur experiments show that we are able to generate artificial data of high\nquality and successfully train and validate machine learning models on this\ndata while limiting potential privacy loss. \n\n"}
{"id": "1803.03772", "contents": "Title: Generalization and Expressivity for Deep Nets Abstract: Along with the rapid development of deep learning in practice, the\ntheoretical explanations for its success become urgent. Generalization and\nexpressivity are two widely used measurements to quantify theoretical behaviors\nof deep learning. The expressivity focuses on finding functions expressible by\ndeep nets but cannot be approximated by shallow nets with the similar number of\nneurons. It usually implies the large capacity. The generalization aims at\nderiving fast learning rate for deep nets. It usually requires small capacity\nto reduce the variance. Different from previous studies on deep learning,\npursuing either expressivity or generalization, we take both factors into\naccount to explore the theoretical advantages of deep nets. For this purpose,\nwe construct a deep net with two hidden layers possessing excellent\nexpressivity in terms of localized and sparse approximation. Then, utilizing\nthe well known covering number to measure the capacity, we find that deep nets\npossess excellent expressive power (measured by localized and sparse\napproximation) without enlarging the capacity of shallow nets. As a\nconsequence, we derive near optimal learning rates for implementing empirical\nrisk minimization (ERM) on the constructed deep nets. These results\ntheoretically exhibit the advantage of deep nets from learning theory\nviewpoints. \n\n"}
{"id": "1803.04008", "contents": "Title: Multi-Armed Bandits for Correlated Markovian Environments with Smoothed\n  Reward Feedback Abstract: We study a multi-armed bandit problem in a dynamic environment where arm\nrewards evolve in a correlated fashion according to a Markov chain. Different\nthan much of the work on related problems, in our formulation a learning\nalgorithm does not have access to either a priori information or observations\nof the state of the Markov chain and only observes smoothed reward feedback\nfollowing time intervals we refer to as epochs. We demonstrate that existing\nmethods such as UCB and $\\varepsilon$-greedy can suffer linear regret in such\nan environment. Employing mixing-time bounds on Markov chains, we develop\nalgorithms called EpochUCB and EpochGreedy that draw inspiration from the\naforementioned methods, yet which admit sublinear regret guarantees for the\nproblem formulation. Our proposed algorithms proceed in epochs in which an arm\nis played repeatedly for a number of iterations that grows linearly as a\nfunction of the number of times an arm has been played in the past. We analyze\nthese algorithms under two types of smoothed reward feedback at the end of each\nepoch: a reward that is the discount-average of the discounted rewards within\nan epoch, and a reward that is the time-average of the rewards within an epoch. \n\n"}
{"id": "1803.04209", "contents": "Title: High Throughput Synchronous Distributed Stochastic Gradient Descent Abstract: We introduce a new, high-throughput, synchronous, distributed, data-parallel,\nstochastic-gradient-descent learning algorithm. This algorithm uses amortized\ninference in a compute-cluster-specific, deep, generative, dynamical model to\nperform joint posterior predictive inference of the mini-batch gradient\ncomputation times of all worker-nodes in a parallel computing cluster. We show\nthat a synchronous parameter server can, by utilizing such a model, choose an\noptimal cutoff time beyond which mini-batch gradient messages from slow workers\nare ignored that maximizes overall mini-batch gradient computations per second.\nIn keeping with earlier findings we observe that, under realistic conditions,\neagerly discarding the mini-batch gradient computations of stragglers not only\nincreases throughput but actually increases the overall rate of convergence as\na function of wall-clock time by virtue of eliminating idleness. The principal\nnovel contribution and finding of this work goes beyond this by demonstrating\nthat using the predicted run-times from a generative model of cluster worker\nperformance to dynamically adjust the cutoff improves substantially over the\nstatic-cutoff prior art, leading to, among other things, significantly reduced\ndeep neural net training times on large computer clusters. \n\n"}
{"id": "1803.04300", "contents": "Title: Neural Conditional Gradients Abstract: The move from hand-designed to learned optimizers in machine learning has\nbeen quite successful for gradient-based and -free optimizers. When facing a\nconstrained problem, however, maintaining feasibility typically requires a\nprojection step, which might be computationally expensive and not\ndifferentiable. We show how the design of projection-free convex optimization\nalgorithms can be cast as a learning problem based on Frank-Wolfe Networks:\nrecurrent networks implementing the Frank-Wolfe algorithm aka. conditional\ngradients. This allows them to learn to exploit structure when, e.g.,\noptimizing over rank-1 matrices. Our LSTM-learned optimizers outperform\nhand-designed as well learned but unconstrained ones. We demonstrate this for\ntraining support vector machines and softmax classifiers. \n\n"}
{"id": "1803.05045", "contents": "Title: Analysis of Nonautonomous Adversarial Systems Abstract: Generative adversarial networks are used to generate images but still their\nconvergence properties are not well understood. There have been a few studies\nwho intended to investigate the stability properties of GANs as a dynamical\nsystem. This short writing can be seen in that direction. Among the proposed\nmethods for stabilizing training of GANs, {\\ss}-GAN was the first who proposed\na complete annealing strategy to change high-level conditions of the GAN\nobjective. In this note, we show by a simple example how annealing strategy\nworks in GANs. The theoretical analysis is supported by simple simulations. \n\n"}
{"id": "1803.05591", "contents": "Title: On the insufficiency of existing momentum schemes for Stochastic\n  Optimization Abstract: Momentum based stochastic gradient methods such as heavy ball (HB) and\nNesterov's accelerated gradient descent (NAG) method are widely used in\npractice for training deep networks and other supervised learning models, as\nthey often provide significant improvements over stochastic gradient descent\n(SGD). Rigorously speaking, \"fast gradient\" methods have provable improvements\nover gradient descent only for the deterministic case, where the gradients are\nexact. In the stochastic case, the popular explanations for their wide\napplicability is that when these fast gradient methods are applied in the\nstochastic case, they partially mimic their exact gradient counterparts,\nresulting in some practical gain. This work provides a counterpoint to this\nbelief by proving that there exist simple problem instances where these methods\ncannot outperform SGD despite the best setting of its parameters. These\nnegative problem instances are, in an informal sense, generic; they do not look\nlike carefully constructed pathological instances. These results suggest (along\nwith empirical evidence) that HB or NAG's practical performance gains are a\nby-product of mini-batching.\n  Furthermore, this work provides a viable (and provable) alternative, which,\non the same set of problem instances, significantly improves over HB, NAG, and\nSGD's performance. This algorithm, referred to as Accelerated Stochastic\nGradient Descent (ASGD), is a simple to implement stochastic algorithm, based\non a relatively less popular variant of Nesterov's Acceleration. Extensive\nempirical results in this paper show that ASGD has performance gains over HB,\nNAG, and SGD. \n\n"}
{"id": "1803.07164", "contents": "Title: Adversarial Generalized Method of Moments Abstract: We provide an approach for learning deep neural net representations of models\ndescribed via conditional moment restrictions. Conditional moment restrictions\nare widely used, as they are the language by which social scientists describe\nthe assumptions they make to enable causal inference. We formulate the problem\nof estimating the underling model as a zero-sum game between a modeler and an\nadversary and apply adversarial training. Our approach is similar in nature to\nGenerative Adversarial Networks (GAN), though here the modeler is learning a\nrepresentation of a function that satisfies a continuum of moment conditions\nand the adversary is identifying violating moments. We outline ways of\nconstructing effective adversaries in practice, including kernels centered by\nk-means clustering, and random forests. We examine the practical performance of\nour approach in the setting of non-parametric instrumental variable regression. \n\n"}
{"id": "1803.07624", "contents": "Title: Dynamic Filtering with Large Sampling Field for ConvNets Abstract: We propose a dynamic filtering strategy with large sampling field for\nConvNets (LS-DFN), where the position-specific kernels learn from not only the\nidentical position but also multiple sampled neighbor regions. During sampling,\nresidual learning is introduced to ease training and an attention mechanism is\napplied to fuse features from different samples. Such multiple samples enlarge\nthe kernels' receptive fields significantly without requiring more parameters.\nWhile LS-DFN inherits the advantages of DFN, namely avoiding feature map\nblurring by position-wise kernels while keeping translation invariance, it also\nefficiently alleviates the overfitting issue caused by much more parameters\nthan normal CNNs. Our model is efficient and can be trained end-to-end via\nstandard back-propagation. We demonstrate the merits of our LS-DFN on both\nsparse and dense prediction tasks involving object detection, semantic\nsegmentation, and flow estimation. Our results show LS-DFN enjoys stronger\nrecognition abilities in object detection and semantic segmentation tasks on\nVOC benchmark and sharper responses in flow estimation on FlyingChairs dataset\ncompared to strong baselines. \n\n"}
{"id": "1803.07679", "contents": "Title: Product Characterisation towards Personalisation: Learning Attributes\n  from Unstructured Data to Recommend Fashion Products Abstract: In this paper, we describe a solution to tackle a common set of challenges in\ne-commerce, which arise from the fact that new products are continually being\nadded to the catalogue. The challenges involve properly personalising the\ncustomer experience, forecasting demand and planning the product range. We\nargue that the foundational piece to solve all of these problems is having\nconsistent and detailed information about each product, information that is\nrarely available or consistent given the multitude of suppliers and types of\nproducts. We describe in detail the architecture and methodology implemented at\nASOS, one of the world's largest fashion e-commerce retailers, to tackle this\nproblem. We then show how this quantitative understanding of the products can\nbe leveraged to improve recommendations in a hybrid recommender system\napproach. \n\n"}
{"id": "1803.07859", "contents": "Title: Efficient Sampling and Structure Learning of Bayesian Networks Abstract: Bayesian networks are probabilistic graphical models widely employed to\nunderstand dependencies in high dimensional data, and even to facilitate causal\ndiscovery. Learning the underlying network structure, which is encoded as a\ndirected acyclic graph (DAG) is highly challenging mainly due to the vast\nnumber of possible networks in combination with the acyclicity constraint.\nEfforts have focussed on two fronts: constraint-based methods that perform\nconditional independence tests to exclude edges and score and search approaches\nwhich explore the DAG space with greedy or MCMC schemes. Here we synthesise\nthese two fields in a novel hybrid method which reduces the complexity of MCMC\napproaches to that of a constraint-based method. Individual steps in the MCMC\nscheme only require simple table lookups so that very long chains can be\nefficiently obtained. Furthermore, the scheme includes an iterative procedure\nto correct for errors from the conditional independence tests. The algorithm\noffers markedly superior performance to alternatives, particularly because DAGs\ncan also be sampled from the posterior distribution, enabling full Bayesian\nmodel averaging for much larger Bayesian networks. \n\n"}
{"id": "1803.07868", "contents": "Title: Scalable Generalized Dynamic Topic Models Abstract: Dynamic topic models (DTMs) model the evolution of prevalent themes in\nliterature, online media, and other forms of text over time. DTMs assume that\nword co-occurrence statistics change continuously and therefore impose\ncontinuous stochastic process priors on their model parameters. These dynamical\npriors make inference much harder than in regular topic models, and also limit\nscalability. In this paper, we present several new results around DTMs. First,\nwe extend the class of tractable priors from Wiener processes to the generic\nclass of Gaussian processes (GPs). This allows us to explore topics that\ndevelop smoothly over time, that have a long-term memory or are temporally\nconcentrated (for event detection). Second, we show how to perform scalable\napproximate inference in these models based on ideas around stochastic\nvariational inference and sparse Gaussian processes. This way we can train a\nrich family of DTMs to massive data. Our experiments on several large-scale\ndatasets show that our generalized model allows us to find interesting patterns\nthat were not accessible by previous approaches. \n\n"}
{"id": "1803.07879", "contents": "Title: An Unsupervised Multivariate Time Series Kernel Approach for Identifying\n  Patients with Surgical Site Infection from Blood Samples Abstract: A large fraction of the electronic health records consists of clinical\nmeasurements collected over time, such as blood tests, which provide important\ninformation about the health status of a patient. These sequences of clinical\nmeasurements are naturally represented as time series, characterized by\nmultiple variables and the presence of missing data, which complicate analysis.\nIn this work, we propose a surgical site infection detection framework for\npatients undergoing colorectal cancer surgery that is completely unsupervised,\nhence alleviating the problem of getting access to labelled training data. The\nframework is based on powerful kernels for multivariate time series that\naccount for missing data when computing similarities. Our approach show\nsuperior performance compared to baselines that have to resort to imputation\ntechniques and performs comparable to a supervised classification baseline. \n\n"}
{"id": "1803.08276", "contents": "Title: Speaker Clustering With Neural Networks And Audio Processing Abstract: Speaker clustering is the task of differentiating speakers in a recording. In\na way, the aim is to answer \"who spoke when\" in audio recordings. A common\nmethod used in industry is feature extraction directly from the recording\nthanks to MFCC features, and by using well-known techniques such as Gaussian\nMixture Models (GMM) and Hidden Markov Models (HMM). In this paper, we studied\nneural networks (especially CNN) followed by clustering and audio processing in\nthe quest to reach similar accuracy to state-of-the-art methods. \n\n"}
{"id": "1803.08591", "contents": "Title: End-to-End Learning for the Deep Multivariate Probit Model Abstract: The multivariate probit model (MVP) is a popular classic model for studying\nbinary responses of multiple entities. Nevertheless, the computational\nchallenge of learning the MVP model, given that its likelihood involves\nintegrating over a multidimensional constrained space of latent variables,\nsignificantly limits its application in practice. We propose a flexible deep\ngeneralization of the classic MVP, the Deep Multivariate Probit Model (DMVP),\nwhich is an end-to-end learning scheme that uses an efficient parallel sampling\nprocess of the multivariate probit model to exploit GPU-boosted deep neural\nnetworks. We present both theoretical and empirical analysis of the convergence\nbehavior of DMVP's sampling process with respect to the resolution of the\ncorrelation structure. We provide convergence guarantees for DMVP and our\nempirical analysis demonstrates the advantages of DMVP's sampling compared with\nstandard MCMC-based methods. We also show that when applied to multi-entity\nmodelling problems, which are natural DMVP applications, DMVP trains faster\nthan classical MVP, by at least an order of magnitude, captures rich\ncorrelations among entities, and further improves the joint likelihood of\nentities compared with several competitive models. \n\n"}
{"id": "1803.09153", "contents": "Title: Fast variational Bayes for heavy-tailed PLDA applied to i-vectors and\n  x-vectors Abstract: The standard state-of-the-art backend for text-independent speaker\nrecognizers that use i-vectors or x-vectors, is Gaussian PLDA (G-PLDA),\nassisted by a Gaussianization step involving length normalization. G-PLDA can\nbe trained with both generative or discriminative methods. It has long been\nknown that heavy-tailed PLDA (HT-PLDA), applied without length normalization,\ngives similar accuracy, but at considerable extra computational cost. We have\nrecently introduced a fast scoring algorithm for a discriminatively trained\nHT-PLDA backend. This paper extends that work by introducing a fast,\nvariational Bayes, generative training algorithm. We compare old and new\nbackends, with and without length-normalization, with i-vectors and x-vectors,\non SRE'10, SRE'16 and SITW. \n\n"}
{"id": "1803.09160", "contents": "Title: Handling Adversarial Concept Drift in Streaming Data Abstract: Classifiers operating in a dynamic, real world environment, are vulnerable to\nadversarial activity, which causes the data distribution to change over time.\nThese changes are traditionally referred to as concept drift, and several\napproaches have been developed in literature to deal with the problem of drift\nhandling and detection. However, most concept drift handling techniques,\napproach it as a domain independent task, to make them applicable to a wide\ngamut of reactive systems. These techniques were developed from an adversarial\nagnostic perspective, where they are naive and assume that drift is a benign\nchange, which can be fixed by updating the model. However, this is not the case\nwhen an active adversary is trying to evade the deployed classification system.\nIn such an environment, the properties of concept drift are unique, as the\ndrift is intended to degrade the system and at the same time designed to avoid\ndetection by traditional concept drift detection techniques. This special\ncategory of drift is termed as adversarial drift, and this paper analyzes its\ncharacteristics and impact, in a streaming environment. A novel framework for\ndealing with adversarial concept drift is proposed, called the Predict-Detect\nstreaming framework. Experimental evaluation of the framework, on generated\nadversarial drifting data streams, demonstrates that this framework is able to\nprovide reliable unsupervised indication of drift, and is able to recover from\ndrifts swiftly. While traditional partially labeled concept drift detection\nmethodologies fail to detect adversarial drifts, the proposed framework is able\nto detect such drifts and operates with <6% labeled data, on average. Also, the\nframework provides benefits for active learning over imbalanced data streams,\nby innately providing for feature space honeypots, where minority class\nadversarial samples may be captured. \n\n"}
{"id": "1803.09319", "contents": "Title: SUNLayer: Stable denoising with generative networks Abstract: It has been experimentally established that deep neural networks can be used\nto produce good generative models for real world data. It has also been\nestablished that such generative models can be exploited to solve classical\ninverse problems like compressed sensing and super resolution. In this work we\nfocus on the classical signal processing problem of image denoising. We propose\na theoretical setting that uses spherical harmonics to identify what\nmathematical properties of the activation functions will allow signal denoising\nwith local methods. \n\n"}
{"id": "1803.09468", "contents": "Title: Clipping free attacks against artificial neural networks Abstract: During the last years, a remarkable breakthrough has been made in AI domain\nthanks to artificial deep neural networks that achieved a great success in many\nmachine learning tasks in computer vision, natural language processing, speech\nrecognition, malware detection and so on. However, they are highly vulnerable\nto easily crafted adversarial examples. Many investigations have pointed out\nthis fact and different approaches have been proposed to generate attacks while\nadding a limited perturbation to the original data. The most robust known\nmethod so far is the so called C&W attack [1]. Nonetheless, a countermeasure\nknown as feature squeezing coupled with ensemble defense showed that most of\nthese attacks can be destroyed [6]. In this paper, we present a new method we\ncall Centered Initial Attack (CIA) whose advantage is twofold : first, it\ninsures by construction the maximum perturbation to be smaller than a threshold\nfixed beforehand, without the clipping process that degrades the quality of\nattacks. Second, it is robust against recently introduced defenses such as\nfeature squeezing, JPEG encoding and even against a voting ensemble of\ndefenses. While its application is not limited to images, we illustrate this\nusing five of the current best classifiers on ImageNet dataset among which two\nare adversarialy retrained on purpose to be robust against attacks. With a\nfixed maximum perturbation of only 1.5% on any pixel, around 80% of attacks\n(targeted) fool the voting ensemble defense and nearly 100% when the\nperturbation is only 6%. While this shows how it is difficult to defend against\nCIA attacks, the last section of the paper gives some guidelines to limit their\nimpact. \n\n"}
{"id": "1803.10769", "contents": "Title: Network Traffic Anomaly Detection Using Recurrent Neural Networks Abstract: We show that a recurrent neural network is able to learn a model to represent\nsequences of communications between computers on a network and can be used to\nidentify outlier network traffic. Defending computer networks is a challenging\nproblem and is typically addressed by manually identifying known malicious\nactor behavior and then specifying rules to recognize such behavior in network\ncommunications. However, these rule-based approaches often generalize poorly\nand identify only those patterns that are already known to researchers. An\nalternative approach that does not rely on known malicious behavior patterns\ncan potentially also detect previously unseen patterns. We tokenize and\ncompress netflow into sequences of \"words\" that form \"sentences\" representative\nof a conversation between computers. These sentences are then used to generate\na model that learns the semantic and syntactic grammar of the newly generated\nlanguage. We use Long-Short-Term Memory (LSTM) cell Recurrent Neural Networks\n(RNN) to capture the complex relationships and nuances of this language. The\nlanguage model is then used predict the communications between two IPs and the\nprediction error is used as a measurement of how typical or atyptical the\nobserved communication are. By learning a model that is specific to each\nnetwork, yet generalized to typical computer-to-computer traffic within and\noutside the network, a language model is able to identify sequences of network\nactivity that are outliers with respect to the model. We demonstrate positive\nunsupervised attack identification performance (AUC 0.84) on the ISCX IDS\ndataset which contains seven days of network activity with normal traffic and\nfour distinct attack patterns. \n\n"}
{"id": "1803.11347", "contents": "Title: Learning to Adapt in Dynamic, Real-World Environments Through\n  Meta-Reinforcement Learning Abstract: Although reinforcement learning methods can achieve impressive results in\nsimulation, the real world presents two major challenges: generating samples is\nexceedingly expensive, and unexpected perturbations or unseen situations cause\nproficient but specialized policies to fail at test time. Given that it is\nimpractical to train separate policies to accommodate all situations the agent\nmay see in the real world, this work proposes to learn how to quickly and\neffectively adapt online to new tasks. To enable sample-efficient learning, we\nconsider learning online adaptation in the context of model-based reinforcement\nlearning. Our approach uses meta-learning to train a dynamics model prior such\nthat, when combined with recent data, this prior can be rapidly adapted to the\nlocal context. Our experiments demonstrate online adaptation for continuous\ncontrol tasks on both simulated and real-world agents. We first show simulated\nagents adapting their behavior online to novel terrains, crippled body parts,\nand highly-dynamic environments. We also illustrate the importance of\nincorporating online adaptation into autonomous agents that operate in the real\nworld by applying our method to a real dynamic legged millirobot. We\ndemonstrate the agent's learned ability to quickly adapt online to a missing\nleg, adjust to novel terrains and slopes, account for miscalibration or errors\nin pose estimation, and compensate for pulling payloads. \n\n"}
{"id": "1804.00218", "contents": "Title: HOUDINI: Lifelong Learning as Program Synthesis Abstract: We present a neurosymbolic framework for the lifelong learning of algorithmic\ntasks that mix perception and procedural reasoning. Reusing high-level concepts\nacross domains and learning complex procedures are key challenges in lifelong\nlearning. We show that a program synthesis approach that combines gradient\ndescent with combinatorial search over programs can be a more effective\nresponse to these challenges than purely neural methods. Our framework, called\nHOUDINI, represents neural networks as strongly typed, differentiable\nfunctional programs that use symbolic higher-order combinators to compose a\nlibrary of neural functions. Our learning algorithm consists of: (1) a symbolic\nprogram synthesizer that performs a type-directed search over parameterized\nprograms, and decides on the library functions to reuse, and the architectures\nto combine them, while learning a sequence of tasks; and (2) a neural module\nthat trains these programs using stochastic gradient descent. We evaluate\nHOUDINI on three benchmarks that combine perception with the algorithmic tasks\nof counting, summing, and shortest-path computation. Our experiments show that\nHOUDINI transfers high-level concepts more effectively than traditional\ntransfer learning and progressive neural networks, and that the typed\nrepresentation of networks significantly accelerates the search. \n\n"}
{"id": "1804.02808", "contents": "Title: Latent Space Policies for Hierarchical Reinforcement Learning Abstract: We address the problem of learning hierarchical deep neural network policies\nfor reinforcement learning. In contrast to methods that explicitly restrict or\ncripple lower layers of a hierarchy to force them to use higher-level\nmodulating signals, each layer in our framework is trained to directly solve\nthe task, but acquires a range of diverse strategies via a maximum entropy\nreinforcement learning objective. Each layer is also augmented with latent\nrandom variables, which are sampled from a prior distribution during the\ntraining of that layer. The maximum entropy objective causes these latent\nvariables to be incorporated into the layer's policy, and the higher level\nlayer can directly control the behavior of the lower layer through this latent\nspace. Furthermore, by constraining the mapping from latent variables to\nactions to be invertible, higher layers retain full expressivity: neither the\nhigher layers nor the lower layers are constrained in their behavior. Our\nexperimental evaluation demonstrates that we can improve on the performance of\nsingle-layer policies on standard benchmark tasks simply by adding additional\nlayers, and that our method can solve more complex sparse-reward tasks by\nlearning higher-level policies on top of high-entropy skills optimized for\nsimple low-level objectives. \n\n"}
{"id": "1804.03280", "contents": "Title: A Deep Active Survival Analysis Approach for Precision Treatment\n  Recommendations: Application of Prostate Cancer Abstract: Survival analysis has been developed and applied in the number of areas\nincluding manufacturing, finance, economics and healthcare. In healthcare\ndomain, usually clinical data are high-dimensional, sparse and complex and\nsometimes there exists few amount of time-to-event (labeled) instances.\nTherefore building an accurate survival model from electronic health records is\nchallenging. With this motivation, we address this issue and provide a new\nsurvival analysis framework using deep learning and active learning with a\nnovel sampling strategy. First, our approach provides better representation\nwith lower dimensions from clinical features using labeled (time-to-event) and\nunlabeled (censored) instances and then actively trains the survival model by\nlabeling the censored data using an oracle. As a clinical assistive tool, we\nintroduce a simple effective treatment recommendation approach based on our\nsurvival model. In the experimental study, we apply our approach on\nSEER-Medicare data related to prostate cancer among African-Americans and white\npatients. The results indicate that our approach outperforms significantly than\nbaseline models. \n\n"}
{"id": "1804.04212", "contents": "Title: Word2Vec applied to Recommendation: Hyperparameters Matter Abstract: Skip-gram with negative sampling, a popular variant of Word2vec originally\ndesigned and tuned to create word embeddings for Natural Language Processing,\nhas been used to create item embeddings with successful applications in\nrecommendation. While these fields do not share the same type of data, neither\nevaluate on the same tasks, recommendation applications tend to use the same\nalready tuned hyperparameters values, even if optimal hyperparameters values\nare often known to be data and task dependent. We thus investigate the marginal\nimportance of each hyperparameter in a recommendation setting through large\nhyperparameter grid searches on various datasets. Results reveal that\noptimizing neglected hyperparameters, namely negative sampling distribution,\nnumber of epochs, subsampling parameter and window-size, significantly improves\nperformance on a recommendation task, and can increase it by an order of\nmagnitude. Importantly, we find that optimal hyperparameters configurations for\nNatural Language Processing tasks and Recommendation tasks are noticeably\ndifferent. \n\n"}
{"id": "1804.04656", "contents": "Title: 3D G-CNNs for Pulmonary Nodule Detection Abstract: Convolutional Neural Networks (CNNs) require a large amount of annotated data\nto learn from, which is often difficult to obtain in the medical domain. In\nthis paper we show that the sample complexity of CNNs can be significantly\nimproved by using 3D roto-translation group convolutions (G-Convs) instead of\nthe more conventional translational convolutions. These 3D G-CNNs were applied\nto the problem of false positive reduction for pulmonary nodule detection, and\nproved to be substantially more effective in terms of performance, sensitivity\nto malignant nodules, and speed of convergence compared to a strong and\ncomparable baseline architecture with regular convolutions, data augmentation\nand a similar number of parameters. For every dataset size tested, the G-CNN\nachieved a FROC score close to the CNN trained on ten times more data. \n\n"}
{"id": "1804.05018", "contents": "Title: Comparatives, Quantifiers, Proportions: A Multi-Task Model for the\n  Learning of Quantities from Vision Abstract: The present work investigates whether different quantification mechanisms\n(set comparison, vague quantification, and proportional estimation) can be\njointly learned from visual scenes by a multi-task computational model. The\nmotivation is that, in humans, these processes underlie the same cognitive,\nnon-symbolic ability, which allows an automatic estimation and comparison of\nset magnitudes. We show that when information about lower-complexity tasks is\navailable, the higher-level proportional task becomes more accurate than when\nperformed in isolation. Moreover, the multi-task model is able to generalize to\nunseen combinations of target/non-target objects. Consistently with behavioral\nevidence showing the interference of absolute number in the proportional task,\nthe multi-task model no longer works when asked to provide the number of target\nobjects in the scene. \n\n"}
{"id": "1804.05214", "contents": "Title: Fast Optimal Bandwidth Selection for RBF Kernel using Reproducing Kernel\n  Hilbert Space Operators for Kernel Based Classifiers Abstract: Kernel based methods have shown effective performance in many remote sensing\nclassification tasks. However their performance significantly depend on its\nhyper-parameters. The conventional technique to estimate the parameter comes\nwith high computational complexity. Thus, the objective of this letter is to\npropose an fast and efficient method to select the bandwidth parameter of the\nGaussian kernel in the kernel based classification methods. The proposed method\nis developed based on the operators in the reproducing kernel Hilbert space and\nit is evaluated on Support vector machines and PerTurbo classification method.\nExperiments conducted with hyperspectral datasets show that our proposed method\noutperforms the state-of-art method in terms in computational time and\nclassification performance. \n\n"}
{"id": "1804.05251", "contents": "Title: An interpretable LSTM neural network for autoregressive exogenous model Abstract: In this paper, we propose an interpretable LSTM recurrent neural network,\ni.e., multi-variable LSTM for time series with exogenous variables. Currently,\nwidely used attention mechanism in recurrent neural networks mostly focuses on\nthe temporal aspect of data and falls short of characterizing variable\nimportance. To this end, our multi-variable LSTM equipped with tensorized\nhidden states is developed to learn variable specific representations, which\ngive rise to both temporal and variable level attention. Preliminary\nexperiments demonstrate comparable prediction performance of multi-variable\nLSTM w.r.t. encoder-decoder based baselines. More interestingly, variable\nimportance in real datasets characterized by the variable attention is highly\nin line with that determined by statistical Granger causality test, which\nexhibits the prospect of multi-variable LSTM as a simple and uniform end-to-end\nframework for both forecasting and knowledge discovery. \n\n"}
{"id": "1804.05816", "contents": "Title: Models for Capturing Temporal Smoothness in Evolving Networks for\n  Learning Latent Representation of Nodes Abstract: In a dynamic network, the neighborhood of the vertices evolve across\ndifferent temporal snapshots of the network. Accurate modeling of this temporal\nevolution can help solve complex tasks involving real-life social and\ninteraction networks. However, existing models for learning latent\nrepresentation are inadequate for obtaining the representation vectors of the\nvertices for different time-stamps of a dynamic network in a meaningful way. In\nthis paper, we propose latent representation learning models for dynamic\nnetworks which overcome the above limitation by considering two different kinds\nof temporal smoothness: (i) retrofitted, and (ii) linear transformation. The\nretrofitted model tracks the representation vector of a vertex over time,\nfacilitating vertex-based temporal analysis of a network. On the other hand,\nlinear transformation based model provides a smooth transition operator which\nmaps the representation vectors of all vertices from one temporal snapshot to\nthe next (unobserved) snapshot-this facilitates prediction of the state of a\nnetwork in a future time-stamp. We validate the performance of our proposed\nmodels by employing them for solving the temporal link prediction task.\nExperiments on 9 real-life networks from various domains validate that the\nproposed models are significantly better than the existing models for\npredicting the dynamics of an evolving network. \n\n"}
{"id": "1804.06218", "contents": "Title: Hierarchical correlation reconstruction with missing data, for example\n  for biology-inspired neuron Abstract: Machine learning often needs to model density from a multidimensional data\nsample, including correlations between coordinates. Additionally, we often have\nmissing data case: that data points can miss values for some of coordinates.\nThis article adapts rapid parametric density estimation approach for this\npurpose: modelling density as a linear combination of orthonormal functions,\nfor which $L^2$ optimization says that (independently) estimated coefficient\nfor a given function is just average over the sample of value of this function.\nHierarchical correlation reconstruction first models probability density for\neach separate coordinate using all its appearances in data sample, then adds\ncorrections from independently modelled pairwise correlations using all samples\nhaving both coordinates, and so on independently adding correlations for\ngrowing numbers of variables using often decreasing evidence in data sample. A\nbasic application of such modelled multidimensional density can be imputation\nof missing coordinates: by inserting known coordinates to the density, and\ntaking expected values for the missing coordinates, or even their entire joint\nprobability distribution. Presented method can be compared with cascade\ncorrelations approach, offering several advantages in flexibility and accuracy.\nIt can be also used as artificial neuron: maximizing prediction capabilities\nfor only local behavior - modelling and predicting local connections. \n\n"}
{"id": "1804.06451", "contents": "Title: Multi-Reward Reinforced Summarization with Saliency and Entailment Abstract: Abstractive text summarization is the task of compressing and rewriting a\nlong document into a short summary while maintaining saliency, directed logical\nentailment, and non-redundancy. In this work, we address these three important\naspects of a good summary via a reinforcement learning approach with two novel\nreward functions: ROUGESal and Entail, on top of a coverage-based baseline. The\nROUGESal reward modifies the ROUGE metric by up-weighting the salient\nphrases/words detected via a keyphrase classifier. The Entail reward gives high\n(length-normalized) scores to logically-entailed summaries using an entailment\nclassifier. Further, we show superior performance improvement when these\nrewards are combined with traditional metric (ROUGE) based rewards, via our\nnovel and effective multi-reward approach of optimizing multiple rewards\nsimultaneously in alternate mini-batches. Our method achieves the new\nstate-of-the-art results (including human evaluation) on the CNN/Daily Mail\ndataset as well as strong improvements in a test-only transfer setup on\nDUC-2002. \n\n"}
{"id": "1804.06498", "contents": "Title: Deep Multimodal Subspace Clustering Networks Abstract: We present convolutional neural network (CNN) based approaches for\nunsupervised multimodal subspace clustering. The proposed framework consists of\nthree main stages - multimodal encoder, self-expressive layer, and multimodal\ndecoder. The encoder takes multimodal data as input and fuses them to a latent\nspace representation. The self-expressive layer is responsible for enforcing\nthe self-expressiveness property and acquiring an affinity matrix corresponding\nto the data points. The decoder reconstructs the original input data. The\nnetwork uses the distance between the decoder's reconstruction and the original\ninput in its training. We investigate early, late and intermediate fusion\ntechniques and propose three different encoders corresponding to them for\nspatial fusion. The self-expressive layers and multimodal decoders are\nessentially the same for different spatial fusion-based approaches. In addition\nto various spatial fusion-based methods, an affinity fusion-based network is\nalso proposed in which the self-expressive layer corresponding to different\nmodalities is enforced to be the same. Extensive experiments on three datasets\nshow that the proposed methods significantly outperform the state-of-the-art\nmultimodal subspace clustering methods. \n\n"}
{"id": "1804.06769", "contents": "Title: CoNet: Collaborative Cross Networks for Cross-Domain Recommendation Abstract: The cross-domain recommendation technique is an effective way of alleviating\nthe data sparse issue in recommender systems by leveraging the knowledge from\nrelevant domains. Transfer learning is a class of algorithms underlying these\ntechniques. In this paper, we propose a novel transfer learning approach for\ncross-domain recommendation by using neural networks as the base model. In\ncontrast to the matrix factorization based cross-domain techniques, our method\nis deep transfer learning, which can learn complex user-item interaction\nrelationships. We assume that hidden layers in two base networks are connected\nby cross mappings, leading to the collaborative cross networks (CoNet). CoNet\nenables dual knowledge transfer across domains by introducing cross connections\nfrom one base network to another and vice versa. CoNet is achieved in\nmulti-layer feedforward networks by adding dual connections and joint loss\nfunctions, which can be trained efficiently by back-propagation. The proposed\nmodel is thoroughly evaluated on two large real-world datasets. It outperforms\nbaselines by relative improvements of 7.84\\% in NDCG. We demonstrate the\nnecessity of adaptively selecting representations to transfer. Our model can\nreduce tens of thousands training examples comparing with non-transfer methods\nand still has the competitive performance with them. \n\n"}
{"id": "1804.07010", "contents": "Title: Forward-Backward Stochastic Neural Networks: Deep Learning of\n  High-dimensional Partial Differential Equations Abstract: Classical numerical methods for solving partial differential equations suffer\nfrom the curse dimensionality mainly due to their reliance on meticulously\ngenerated spatio-temporal grids. Inspired by modern deep learning based\ntechniques for solving forward and inverse problems associated with partial\ndifferential equations, we circumvent the tyranny of numerical discretization\nby devising an algorithm that is scalable to high-dimensions. In particular, we\napproximate the unknown solution by a deep neural network which essentially\nenables us to benefit from the merits of automatic differentiation. To train\nthe aforementioned neural network we leverage the well-known connection between\nhigh-dimensional partial differential equations and forward-backward stochastic\ndifferential equations. In fact, independent realizations of a standard\nBrownian motion will act as training data. We test the effectiveness of our\napproach for a couple of benchmark problems spanning a number of scientific\ndomains including Black-Scholes-Barenblatt and Hamilton-Jacobi-Bellman\nequations, both in 100-dimensions. \n\n"}
{"id": "1804.07347", "contents": "Title: Randomized ICA and LDA Dimensionality Reduction Methods for\n  Hyperspectral Image Classification Abstract: Dimensionality reduction is an important step in processing the hyperspectral\nimages (HSI) to overcome the curse of dimensionality problem. Linear\ndimensionality reduction methods such as Independent component analysis (ICA)\nand Linear discriminant analysis (LDA) are commonly employed to reduce the\ndimensionality of HSI. These methods fail to capture non-linear dependency in\nthe HSI data, as data lies in the nonlinear manifold. To handle this, nonlinear\ntransformation techniques based on kernel methods were introduced for\ndimensionality reduction of HSI. However, the kernel methods involve cubic\ncomputational complexity while computing the kernel matrix, and thus its\npotential cannot be explored when the number of pixels (samples) are large. In\nliterature a fewer number of pixels are randomly selected to partial to\novercome this issue, however this sub-optimal strategy might neglect important\ninformation in the HSI. In this paper, we propose randomized solutions to the\nICA and LDA dimensionality reduction methods using Random Fourier features, and\nwe label them as RFFICA and RFFLDA. Our proposed method overcomes the\nscalability issue and to handle the non-linearities present in the data more\nefficiently. Experiments conducted with two real-world hyperspectral datasets\ndemonstrates that our proposed randomized methods outperform the conventional\nkernel ICA and kernel LDA in terms overall, per-class accuracies and\ncomputational time. \n\n"}
{"id": "1804.07612", "contents": "Title: Revisiting Small Batch Training for Deep Neural Networks Abstract: Modern deep neural network training is typically based on mini-batch\nstochastic gradient optimization. While the use of large mini-batches increases\nthe available computational parallelism, small batch training has been shown to\nprovide improved generalization performance and allows a significantly smaller\nmemory footprint, which might also be exploited to improve machine throughput.\n  In this paper, we review common assumptions on learning rate scaling and\ntraining duration, as a basis for an experimental comparison of test\nperformance for different mini-batch sizes. We adopt a learning rate that\ncorresponds to a constant average weight update per gradient calculation (i.e.,\nper unit cost of computation), and point out that this results in a variance of\nthe weight updates that increases linearly with the mini-batch size $m$.\n  The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet\ndatasets show that increasing the mini-batch size progressively reduces the\nrange of learning rates that provide stable convergence and acceptable test\nperformance. On the other hand, small mini-batch sizes provide more up-to-date\ngradient calculations, which yields more stable and reliable training. The best\nperformance has been consistently obtained for mini-batch sizes between $m = 2$\nand $m = 32$, which contrasts with recent work advocating the use of mini-batch\nsizes in the thousands. \n\n"}
{"id": "1804.09699", "contents": "Title: Towards Fast Computation of Certified Robustness for ReLU Networks Abstract: Verifying the robustness property of a general Rectified Linear Unit (ReLU)\nnetwork is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer\nCAV17]. Although finding the exact minimum adversarial distortion is hard,\ngiving a certified lower bound of the minimum distortion is possible. Current\navailable methods of computing such a bound are either time-consuming or\ndelivering low quality bounds that are too loose to be useful. In this paper,\nwe exploit the special structure of ReLU networks and provide two\ncomputationally efficient algorithms Fast-Lin and Fast-Lip that are able to\ncertify non-trivial lower bounds of minimum distortions, by bounding the ReLU\nunits with appropriate linear functions Fast-Lin, or by bounding the local\nLipschitz constant Fast-Lip. Experiments show that (1) our proposed methods\ndeliver bounds close to (the gap is 2-3X) exact minimum distortion found by\nReluplex in small MNIST networks while our algorithms are more than 10,000\ntimes faster; (2) our methods deliver similar quality of bounds (the gap is\nwithin 35% and usually around 10%; sometimes our bounds are even better) for\nlarger networks compared to the methods based on solving linear programming\nproblems but our algorithms are 33-14,000 times faster; (3) our method is\ncapable of solving large MNIST and CIFAR networks up to 7 layers with more than\n10,000 neurons within tens of seconds on a single CPU core.\n  In addition, we show that, in fact, there is no polynomial time algorithm\nthat can approximately find the minimum $\\ell_1$ adversarial distortion of a\nReLU network with a $0.99\\ln n$ approximation ratio unless\n$\\mathsf{NP}$=$\\mathsf{P}$, where $n$ is the number of neurons in the network. \n\n"}
{"id": "1804.10266", "contents": "Title: Tensor Methods for Nonlinear Matrix Completion Abstract: In the low-rank matrix completion (LRMC) problem, the low-rank assumption\nmeans that the columns (or rows) of the matrix to be completed are points on a\nlow-dimensional linear algebraic variety. This paper extends this thinking to\ncases where the columns are points on a low-dimensional nonlinear algebraic\nvariety, a problem we call Low Algebraic Dimension Matrix Completion (LADMC).\nMatrices whose columns belong to a union of subspaces are an important special\ncase. We propose a LADMC algorithm that leverages existing LRMC methods on a\ntensorized representation of the data. For example, a second-order tensorized\nrepresentation is formed by taking the Kronecker product of each column with\nitself, and we consider higher order tensorizations as well. This approach will\nsucceed in many cases where traditional LRMC is guaranteed to fail because the\ndata are low-rank in the tensorized representation but not in the original\nrepresentation. We provide a formal mathematical justification for the success\nof our method. In particular, we give bounds of the rank of these data in the\ntensorized representation, and we prove sampling requirements to guarantee\nuniqueness of the solution. We also provide experimental results showing that\nthe new approach outperforms existing state-of-the-art methods for matrix\ncompletion under a union of subspaces model. \n\n"}
{"id": "1804.11237", "contents": "Title: Deep learning improved by biological activation functions Abstract: `Biologically inspired' activation functions, such as the logistic sigmoid,\nhave been instrumental in the historical advancement of machine learning.\nHowever in the field of deep learning, they have been largely displaced by\nrectified linear units (ReLU) or similar functions, such as its exponential\nlinear unit (ELU) variant, to mitigate the effects of vanishing gradients\nassociated with error back-propagation. The logistic sigmoid however does not\nrepresent the true input-output relation in neuronal cells under physiological\nconditions. Here, bionodal root unit (BRU) activation functions are introduced,\nexhibiting input-output non-linearities that are substantially more\nbiologically plausible since their functional form is based on known\nbiophysical properties of neuronal cells.\n  In order to evaluate the learning performance of BRU activations, deep\nnetworks are constructed with identical architectures except differing in their\ntransfer functions (ReLU, ELU, and BRU). Multilayer perceptrons, stacked\nauto-encoders, and convolutional networks are used to test supervised and\nunsupervised learning based on the MNIST and CIFAR-10/100 datasets. Comparisons\nof learning performance, quantified using loss and error measurements,\ndemonstrate that bionodal networks both train faster than their ReLU and ELU\ncounterparts and result in the best generalised models even in the absence of\nformal regularisation. These results therefore suggest that revisiting the\ndetailed properties of biological neurones and their circuitry might prove\ninvaluable in the field of deep learning for the future. \n\n"}
{"id": "1805.00915", "contents": "Title: Trainability and Accuracy of Neural Networks: An Interacting Particle\n  System Approach Abstract: Neural networks, a central tool in machine learning, have demonstrated\nremarkable, high fidelity performance on image recognition and classification\ntasks. These successes evince an ability to accurately represent high\ndimensional functions, but rigorous results about the approximation error of\nneural networks after training are few. Here we establish conditions for global\nconvergence of the standard optimization algorithm used in machine learning\napplications, stochastic gradient descent (SGD), and quantify the scaling of\nits error with the size of the network. This is done by reinterpreting SGD as\nthe evolution of a particle system with interactions governed by a potential\nrelated to the objective or \"loss\" function used to train the network. We show\nthat, when the number $n$ of units is large, the empirical distribution of the\nparticles descends on a convex landscape towards the global minimum at a rate\nindependent of $n$, with a resulting approximation error that universally\nscales as $O(n^{-1})$. These properties are established in the form of a Law of\nLarge Numbers and a Central Limit Theorem for the empirical distribution. Our\nanalysis also quantifies the scale and nature of the noise introduced by SGD\nand provides guidelines for the step size and batch size to use when training a\nneural network. We illustrate our findings on examples in which we train neural\nnetworks to learn the energy function of the continuous 3-spin model on the\nsphere. The approximation error scales as our analysis predicts in as high a\ndimension as $d=25$. \n\n"}
{"id": "1805.01357", "contents": "Title: Boosting Noise Robustness of Acoustic Model via Deep Adversarial\n  Training Abstract: In realistic environments, speech is usually interfered by various noise and\nreverberation, which dramatically degrades the performance of automatic speech\nrecognition (ASR) systems. To alleviate this issue, the commonest way is to use\na well-designed speech enhancement approach as the front-end of ASR. However,\nmore complex pipelines, more computations and even higher hardware costs\n(microphone array) are additionally consumed for this kind of methods. In\naddition, speech enhancement would result in speech distortions and mismatches\nto training. In this paper, we propose an adversarial training method to\ndirectly boost noise robustness of acoustic model. Specifically, a jointly\ncompositional scheme of generative adversarial net (GAN) and neural\nnetwork-based acoustic model (AM) is used in the training phase. GAN is used to\ngenerate clean feature representations from noisy features by the guidance of a\ndiscriminator that tries to distinguish between the true clean signals and\ngenerated signals. The joint optimization of generator, discriminator and AM\nconcentrates the strengths of both GAN and AM for speech recognition.\nSystematic experiments on CHiME-4 show that the proposed method significantly\nimproves the noise robustness of AM and achieves the average relative error\nrate reduction of 23.38% and 11.54% on the development and test set,\nrespectively. \n\n"}
{"id": "1805.01532", "contents": "Title: Lifted Neural Networks Abstract: We describe a novel family of models of multi- layer feedforward neural\nnetworks in which the activation functions are encoded via penalties in the\ntraining problem. Our approach is based on representing a non-decreasing\nactivation function as the argmin of an appropriate convex optimiza- tion\nproblem. The new framework allows for algo- rithms such as block-coordinate\ndescent methods to be applied, in which each step is composed of a simple (no\nhidden layer) supervised learning problem that is parallelizable across data\npoints and/or layers. Experiments indicate that the pro- posed models provide\nexcellent initial guesses for weights for standard neural networks. In addi-\ntion, the model provides avenues for interesting extensions, such as robustness\nagainst noisy in- puts and optimizing over parameters in activation functions. \n\n"}
{"id": "1805.02269", "contents": "Title: Incorporating Privileged Information to Unsupervised Anomaly Detection Abstract: We introduce a new unsupervised anomaly detection ensemble called SPI which\ncan harness privileged information - data available only for training examples\nbut not for (future) test examples. Our ideas build on the Learning Using\nPrivileged Information (LUPI) paradigm pioneered by Vapnik et al. [19,17],\nwhich we extend to unsupervised learning and in particular to anomaly\ndetection. SPI (for Spotting anomalies with Privileged Information) constructs\na number of frames/fragments of knowledge (i.e., density estimates) in the\nprivileged space and transfers them to the anomaly scoring space through\n\"imitation\" functions that use only the partial information available for test\nexamples. Our generalization of the LUPI paradigm to unsupervised anomaly\ndetection shepherds the field in several key directions, including (i) domain\nknowledge-augmented detection using expert annotations as PI, (ii) fast\ndetection using computationally-demanding data as PI, and (iii) early detection\nusing \"historical future\" data as PI. Through extensive experiments on\nsimulated and real datasets, we show that augmenting privileged information to\nanomaly detection significantly improves detection performance. We also\ndemonstrate the promise of SPI under all three settings (i-iii); with PI\ncapturing expert knowledge, computationally expensive features, and future data\non three real world detection tasks. \n\n"}
{"id": "1805.02716", "contents": "Title: Real-time regression analysis with deep convolutional neural networks Abstract: We discuss the development of novel deep learning algorithms to enable\nreal-time regression analysis for time series data. We showcase the application\nof this new method with a timely case study, and then discuss the applicability\nof this approach to tackle similar challenges across science domains. \n\n"}
{"id": "1805.02830", "contents": "Title: Several Tunable GMM Kernels Abstract: While tree methods have been popular in practice, researchers and\npractitioners are also looking for simple algorithms which can reach similar\naccuracy of trees. In 2010, (Ping Li UAI'10) developed the method of\n\"abc-robust-logitboost\" and compared it with other supervised learning methods\non datasets used by the deep learning literature. In this study, we propose a\nseries of \"tunable GMM kernels\" which are simple and perform largely comparably\nto tree methods on the same datasets. Note that \"abc-robust-logitboost\"\nsubstantially improved the original \"GDBT\" in that (a) it developed a\ntree-split formula based on second-order information of the derivatives of the\nloss function; (b) it developed a new set of derivatives for multi-class\nclassification formulation.\n  In the prior study in 2017, the \"generalized min-max\" (GMM) kernel was shown\nto have good performance compared to the \"radial-basis function\" (RBF) kernel.\nHowever, as demonstrated in this paper, the original GMM kernel is often not as\ncompetitive as tree methods on the datasets used in the deep learning\nliterature. Since the original GMM kernel has no parameters, we propose tunable\nGMM kernels by adding tuning parameters in various ways. Three basic (i.e.,\nwith only one parameter) GMM kernels are the \"$e$GMM kernel\", \"$p$GMM kernel\",\nand \"$\\gamma$GMM kernel\", respectively. Extensive experiments show that they\nare able to produce good results for a large number of classification tasks.\nFurthermore, the basic kernels can be combined to boost the performance. \n\n"}
{"id": "1805.03504", "contents": "Title: Diffusion Based Network Embedding Abstract: In network embedding, random walks play a fundamental role in preserving\nnetwork structures. However, random walk based embedding methods have two\nlimitations. First, random walk methods are fragile when the sampling frequency\nor the number of node sequences changes. Second, in disequilibrium networks\nsuch as highly biases networks, random walk methods often perform poorly due to\nthe lack of global network information. In order to solve the limitations, we\npropose in this paper a network diffusion based embedding method. To solve the\nfirst limitation, our method employs a diffusion driven process to capture both\ndepth information and breadth information. The time dimension is also attached\nto node sequences that can strengthen information preserving. To solve the\nsecond limitation, our method uses the network inference technique based on\ncascades to capture the global network information. To verify the performance,\nwe conduct experiments on node classification tasks using the learned\nrepresentations. Results show that compared with random walk based methods,\ndiffusion based models are more robust when samplings under each node is rare.\nWe also conduct experiments on a highly imbalanced network. Results shows that\nthe proposed model are more robust under the biased network structure. \n\n"}
{"id": "1805.03591", "contents": "Title: Secure Mobile Edge Computing in IoT via Collaborative Online Learning Abstract: To accommodate heterogeneous tasks in Internet of Things (IoT), a new\ncommunication and computing paradigm termed mobile edge computing emerges that\nextends computing services from the cloud to edge, but at the same time exposes\nnew challenges on security. The present paper studies online security-aware\nedge computing under jamming attacks. Leveraging online learning tools, novel\nalgorithms abbreviated as SAVE-S and SAVE-A are developed to cope with the\nstochastic and adversarial forms of jamming, respectively. Without utilizing\nextra resources such as spectrum and transmission power to evade jamming\nattacks, SAVE-S and SAVE-A can select the most reliable server to offload\ncomputing tasks with minimal privacy and security concerns. It is analytically\nestablished that without any prior information on future jamming and server\nsecurity risks, the proposed schemes can achieve ${\\cal O}\\big(\\sqrt{T}\\big)$\nregret. Information sharing among devices can accelerate the security-aware\ncomputing tasks. Incorporating the information shared by other devices, SAVE-S\nand SAVE-A offer impressive improvements on the sublinear regret, which is\nguaranteed by what is termed \"value of cooperation.\" Effectiveness of the\nproposed schemes is tested on both synthetic and real datasets. \n\n"}
{"id": "1805.03714", "contents": "Title: Foundations of Sequence-to-Sequence Modeling for Time Series Abstract: The availability of large amounts of time series data, paired with the\nperformance of deep-learning algorithms on a broad class of problems, has\nrecently led to significant interest in the use of sequence-to-sequence models\nfor time series forecasting. We provide the first theoretical analysis of this\ntime series forecasting framework. We include a comparison of\nsequence-to-sequence modeling to classical time series models, and as such our\ntheory can serve as a quantitative guide for practitioners choosing between\ndifferent modeling methodologies. \n\n"}
{"id": "1805.04272", "contents": "Title: An $O(N)$ Sorting Algorithm: Machine Learning Sort Abstract: We propose an $O(N\\cdot M)$ sorting algorithm by Machine Learning method,\nwhich shows a huge potential sorting big data. This sorting algorithm can be\napplied to parallel sorting and is suitable for GPU or TPU acceleration.\nFurthermore, we discuss the application of this algorithm to sparse hash table. \n\n"}
{"id": "1805.04513", "contents": "Title: Laconic Deep Learning Computing Abstract: We motivate a method for transparently identifying ineffectual computations\nin unmodified Deep Learning models and without affecting accuracy.\nSpecifically, we show that if we decompose multiplications down to the bit\nlevel the amount of work performed during inference for image classification\nmodels can be consistently reduced by two orders of magnitude. In the best case\nstudied of a sparse variant of AlexNet, this approach can ideally reduce\ncomputation work by more than 500x. We present Laconic a hardware accelerator\nthat implements this approach to improve execution time, and energy efficiency\nfor inference with Deep Learning Networks. Laconic judiciously gives up some of\nthe work reduction potential to yield a low-cost, simple, and energy efficient\ndesign that outperforms other state-of-the-art accelerators. For example, a\nLaconic configuration that uses a weight memory interface with just 128 wires\noutperforms a conventional accelerator with a 2K-wire weight memory interface\nby 2.3x on average while being 2.13x more energy efficient on average. A\nLaconic configuration that uses a 1K-wire weight memory interface, outperforms\nthe 2K-wire conventional accelerator by 15.4x and is 1.95x more energy\nefficient. Laconic does not require but rewards advances in model design such\nas a reduction in precision, the use of alternate numeric representations that\nreduce the number of bits that are \"1\", or an increase in weight or activation\nsparsity. \n\n"}
{"id": "1805.05036", "contents": "Title: A Deep Learning Approach with an Attention Mechanism for Automatic Sleep\n  Stage Classification Abstract: Automatic sleep staging is a challenging problem and state-of-the-art\nalgorithms have not yet reached satisfactory performance to be used instead of\nmanual scoring by a sleep technician. Much research has been done to find good\nfeature representations that extract the useful information to correctly\nclassify each epoch into the correct sleep stage. While many useful features\nhave been discovered, the amount of features have grown to an extent that a\nfeature reduction step is necessary in order to avoid the curse of\ndimensionality. One reason for the need of such a large feature set is that\nmany features are good for discriminating only one of the sleep stages and are\nless informative during other stages. This paper explores how a second feature\nrepresentation over a large set of pre-defined features can be learned using an\nauto-encoder with a selective attention for the current sleep stage in the\ntraining batch. This selective attention allows the model to learn feature\nrepresentations that focuses on the more relevant inputs without having to\nperform any dimensionality reduction of the input data. The performance of the\nproposed algorithm is evaluated on a large data set of polysomnography (PSG)\nnight recordings of patients with sleep-disordered breathing. The performance\nof the auto-encoder with selective attention is compared with a regular\nauto-encoder and previous works using a deep belief network (DBN). \n\n"}
{"id": "1805.05751", "contents": "Title: Local Saddle Point Optimization: A Curvature Exploitation Approach Abstract: Gradient-based optimization methods are the most popular choice for finding\nlocal optima for classical minimization and saddle point problems. Here, we\nhighlight a systemic issue of gradient dynamics that arise for saddle point\nproblems, namely the presence of undesired stable stationary points that are no\nlocal optima. We propose a novel optimization approach that exploits curvature\ninformation in order to escape from these undesired stationary points. We prove\nthat different optimization methods, including gradient method and Adagrad,\nequipped with curvature exploitation can escape non-optimal stationary points.\nWe also provide empirical results on common saddle point problems which confirm\nthe advantage of using curvature exploitation. \n\n"}
{"id": "1805.05809", "contents": "Title: Efficient end-to-end learning for quantizable representations Abstract: Embedding representation learning via neural networks is at the core\nfoundation of modern similarity based search. While much effort has been put in\ndeveloping algorithms for learning binary hamming code representations for\nsearch efficiency, this still requires a linear scan of the entire dataset per\neach query and trades off the search accuracy through binarization. To this\nend, we consider the problem of directly learning a quantizable embedding\nrepresentation and the sparse binary hash code end-to-end which can be used to\nconstruct an efficient hash table not only providing significant search\nreduction in the number of data but also achieving the state of the art search\naccuracy outperforming previous state of the art deep metric learning methods.\nWe also show that finding the optimal sparse binary hash code in a mini-batch\ncan be computed exactly in polynomial time by solving a minimum cost flow\nproblem. Our results on Cifar-100 and on ImageNet datasets show the state of\nthe art search accuracy in precision@k and NMI metrics while providing up to\n98X and 478X search speedup respectively over exhaustive linear search. The\nsource code is available at\nhttps://github.com/maestrojeong/Deep-Hash-Table-ICML18 \n\n"}
{"id": "1805.05827", "contents": "Title: Graph Signal Sampling via Reinforcement Learning Abstract: We formulate the problem of sampling and recovering clustered graph signal as\na multi-armed bandit (MAB) problem. This formulation lends naturally to\nlearning sampling strategies using the well-known gradient MAB algorithm. In\nparticular, the sampling strategy is represented as a probability distribution\nover the individual arms of the MAB and optimized using gradient ascent. Some\nillustrative numerical experiments indicate that the sampling strategies based\non the gradient MAB algorithm outperform existing sampling methods. \n\n"}
{"id": "1805.06061", "contents": "Title: SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines Abstract: Recurrent and convolutional neural networks comprise two distinct families of\nmodels that have proven to be useful for encoding natural language utterances.\nIn this paper we present SoPa, a new model that aims to bridge these two\napproaches. SoPa combines neural representation learning with weighted\nfinite-state automata (WFSAs) to learn a soft version of traditional surface\npatterns. We show that SoPa is an extension of a one-layer CNN, and that such\nCNNs are equivalent to a restricted version of SoPa, and accordingly, to a\nrestricted form of WFSA. Empirically, on three text classification tasks, SoPa\nis comparable or better than both a BiLSTM (RNN) baseline and a CNN baseline,\nand is particularly useful in small data settings. \n\n"}
{"id": "1805.06546", "contents": "Title: Joint Classification and Prediction CNN Framework for Automatic Sleep\n  Stage Classification Abstract: Correctly identifying sleep stages is important in diagnosing and treating\nsleep disorders. This work proposes a joint classification-and-prediction\nframework based on CNNs for automatic sleep staging, and, subsequently,\nintroduces a simple yet efficient CNN architecture to power the framework.\nGiven a single input epoch, the novel framework jointly determines its label\n(classification) and its neighboring epochs' labels (prediction) in the\ncontextual output. While the proposed framework is orthogonal to the widely\nadopted classification schemes, which take one or multiple epochs as contextual\ninputs and produce a single classification decision on the target epoch, we\ndemonstrate its advantages in several ways. First, it leverages the dependency\namong consecutive sleep epochs while surpassing the problems experienced with\nthe common classification schemes. Second, even with a single model, the\nframework has the capacity to produce multiple decisions, which are essential\nin obtaining a good performance as in ensemble-of-models methods, with very\nlittle induced computational overhead. Probabilistic aggregation techniques are\nthen proposed to leverage the availability of multiple decisions. We conducted\nexperiments on two public datasets: Sleep-EDF Expanded with 20 subjects, and\nMontreal Archive of Sleep Studies dataset with 200 subjects. The proposed\nframework yields an overall classification accuracy of 82.3% and 83.6%,\nrespectively. We also show that the proposed framework not only is superior to\nthe baselines based on the common classification schemes but also outperforms\nexisting deep-learning approaches. To our knowledge, this is the first work\ngoing beyond the standard single-output classification to consider multitask\nneural networks for automatic sleep staging. This framework provides avenues\nfor further studies of different neural-network architectures for automatic\nsleep staging. \n\n"}
{"id": "1805.06962", "contents": "Title: Counterexample-Guided Data Augmentation Abstract: We present a novel framework for augmenting data sets for machine learning\nbased on counterexamples. Counterexamples are misclassified examples that have\nimportant properties for retraining and improving the model. Key components of\nour framework include a counterexample generator, which produces data items\nthat are misclassified by the model and error tables, a novel data structure\nthat stores information pertaining to misclassifications. Error tables can be\nused to explain the model's vulnerabilities and are used to efficiently\ngenerate counterexamples for augmentation. We show the efficacy of the proposed\nframework by comparing it to classical augmentation techniques on a case study\nof object detection in autonomous driving based on deep neural networks. \n\n"}
{"id": "1805.07113", "contents": "Title: Change Point Methods on a Sequence of Graphs Abstract: Given a finite sequence of graphs, e.g., coming from technological,\nbiological, and social networks, the paper proposes a methodology to identify\npossible changes in stationarity in the stochastic process generating the\ngraphs. In order to cover a large class of applications, we consider the\ngeneral family of attributed graphs where both topology (number of vertexes and\nedge configuration) and related attributes are allowed to change also in the\nstationary case. Novel Change Point Methods (CPMs) are proposed, that (i) map\ngraphs into a vector domain; (ii) apply a suitable statistical test in the\nvector space; (iii) detect the change --if any-- according to a confidence\nlevel and provide an estimate for its time occurrence. Two specific\nmultivariate CPMs have been designed: one that detects shifts in the\ndistribution mean, the other addressing generic changes affecting the\ndistribution. We ground our proposal with theoretical results showing how to\nrelate the inference attained in the numerical vector space to the graph\ndomain, and vice versa. We also show how to extend the methodology for handling\nmultiple change points in the same sequence. Finally, the proposed CPMs have\nbeen validated on real data sets coming from epileptic-seizure detection\nproblems and on labeled data sets for graph classification. Results show the\neffectiveness of what proposed in relevant application scenarios. \n\n"}
{"id": "1805.07226", "contents": "Title: Sequential Neural Likelihood: Fast Likelihood-free Inference with\n  Autoregressive Flows Abstract: We present Sequential Neural Likelihood (SNL), a new method for Bayesian\ninference in simulator models, where the likelihood is intractable but\nsimulating data from the model is possible. SNL trains an autoregressive flow\non simulated data in order to learn a model of the likelihood in the region of\nhigh posterior density. A sequential training procedure guides simulations and\nreduces simulation cost by orders of magnitude. We show that SNL is more\nrobust, more accurate and requires less tuning than related neural-based\nmethods, and we discuss diagnostics for assessing calibration, convergence and\ngoodness-of-fit. \n\n"}
{"id": "1805.07242", "contents": "Title: Siamese Capsule Networks Abstract: Capsule Networks have shown encouraging results on \\textit{defacto} benchmark\ncomputer vision datasets such as MNIST, CIFAR and smallNORB. Although, they are\nyet to be tested on tasks where (1) the entities detected inherently have more\ncomplex internal representations and (2) there are very few instances per class\nto learn from and (3) where point-wise classification is not suitable. Hence,\nthis paper carries out experiments on face verification in both controlled and\nuncontrolled settings that together address these points. In doing so we\nintroduce \\textit{Siamese Capsule Networks}, a new variant that can be used for\npairwise learning tasks. The model is trained using contrastive loss with\n$\\ell_2$-normalized capsule encoded pose features. We find that \\textit{Siamese\nCapsule Networks} perform well against strong baselines on both pairwise\nlearning datasets, yielding best results in the few-shot learning setting where\nimage pairs in the test set contain unseen subjects. \n\n"}
{"id": "1805.07563", "contents": "Title: Reinforcement Learning of Theorem Proving Abstract: We introduce a theorem proving algorithm that uses practically no domain\nheuristics for guiding its connection-style proof search. Instead, it runs many\nMonte-Carlo simulations guided by reinforcement learning from previous proof\nattempts. We produce several versions of the prover, parameterized by different\nlearning and guiding algorithms. The strongest version of the system is trained\non a large corpus of mathematical problems and evaluated on previously unseen\nproblems. The trained system solves within the same number of inferences over\n40% more problems than a baseline prover, which is an unusually high\nimprovement in this hard AI domain. To our knowledge this is the first time\nreinforcement learning has been convincingly applied to solving general\nmathematical problems on a large scale. \n\n"}
{"id": "1805.07594", "contents": "Title: Generalizing Point Embeddings using the Wasserstein Space of Elliptical\n  Distributions Abstract: Embedding complex objects as vectors in low dimensional spaces is a\nlongstanding problem in machine learning. We propose in this work an extension\nof that approach, which consists in embedding objects as elliptical probability\ndistributions, namely distributions whose densities have elliptical level sets.\nWe endow these measures with the 2-Wasserstein metric, with two important\nbenefits: (i) For such measures, the squared 2-Wasserstein metric has a closed\nform, equal to a weighted sum of the squared Euclidean distance between means\nand the squared Bures metric between covariance matrices. The latter is a\nRiemannian metric between positive semi-definite matrices, which turns out to\nbe Euclidean on a suitable factor representation of such matrices, which is\nvalid on the entire geodesic between these matrices. (ii) The 2-Wasserstein\ndistance boils down to the usual Euclidean metric when comparing Diracs, and\ntherefore provides a natural framework to extend point embeddings. We show that\nfor these reasons Wasserstein elliptical embeddings are more intuitive and\nyield tools that are better behaved numerically than the alternative choice of\nGaussian embeddings with the Kullback-Leibler divergence. In particular, and\nunlike previous work based on the KL geometry, we learn elliptical\ndistributions that are not necessarily diagonal. We demonstrate the advantages\nof elliptical embeddings by using them for visualization, to compute embeddings\nof words, and to reflect entailment or hypernymy. \n\n"}
{"id": "1805.07782", "contents": "Title: Model Aggregation via Good-Enough Model Spaces Abstract: In many applications, the training data for a machine learning task is\npartitioned across multiple nodes, and aggregating this data may be infeasible\ndue to communication, privacy, or storage constraints. Existing distributed\noptimization methods for learning global models in these settings typically\naggregate local updates from each node in an iterative fashion. However, these\napproaches require many rounds of communication between nodes, and assume that\nupdates can be synchronously shared across a connected network. In this work,\nwe present Good-Enough Model Spaces (GEMS), a novel framework for learning a\nglobal model by carefully intersecting the sets of \"good-enough\" models across\neach node. Our approach utilizes minimal communication and does not require\nsharing of data between nodes. We present methods for learning both convex\nmodels and neural networks within this framework and discuss how small samples\nof held-out data can be used for post-learning fine-tuning. In experiments on\nimage and medical datasets, our approach on average improves upon other\nbaseline aggregation techniques such as ensembling or model averaging by as\nmuch as 15 points (accuracy). \n\n"}
{"id": "1805.07833", "contents": "Title: Wasserstein regularization for sparse multi-task regression Abstract: We focus in this paper on high-dimensional regression problems where each\nregressor can be associated to a location in a physical space, or more\ngenerally a generic geometric space. Such problems often employ sparse priors,\nwhich promote models using a small subset of regressors. To increase\nstatistical power, the so-called multi-task techniques were proposed, which\nconsist in the simultaneous estimation of several related models. Combined with\nsparsity assumptions, it lead to models enforcing the active regressors to be\nshared across models, thanks to, for instance L1 / Lq norms. We argue in this\npaper that these techniques fail to leverage the spatial information associated\nto regressors. Indeed, while sparse priors enforce that only a small subset of\nvariables is used, the assumption that these regressors overlap across all\ntasks is overly simplistic given the spatial variability observed in real data.\nIn this paper, we propose a convex regularizer for multi-task regression that\nencodes a more flexible geometry. Our regularizer is based on unbalanced\noptimal transport (OT) theory, and can take into account a prior geometric\nknowledge on the regressor variables, without necessarily requiring overlapping\nsupports. We derive an efficient algorithm based on a regularized formulation\nof OT, which iterates through applications of Sinkhorn's algorithm along with\ncoordinate descent iterations. The performance of our model is demonstrated on\nregular grids with both synthetic and real datasets as well as complex\ntriangulated geometries of the cortex with an application in neuroimaging. \n\n"}
{"id": "1805.07914", "contents": "Title: Imitating Latent Policies from Observation Abstract: In this paper, we describe a novel approach to imitation learning that infers\nlatent policies directly from state observations. We introduce a method that\ncharacterizes the causal effects of latent actions on observations while\nsimultaneously predicting their likelihood. We then outline an action alignment\nprocedure that leverages a small amount of environment interactions to\ndetermine a mapping between the latent and real-world actions. We show that\nthis corrected labeling can be used for imitating the observed behavior, even\nthough no expert actions are given. We evaluate our approach within classic\ncontrol environments and a platform game and demonstrate that it performs\nbetter than standard approaches. Code for this work is available at\nhttps://github.com/ashedwards/ILPO. \n\n"}
{"id": "1805.08006", "contents": "Title: Bidirectional Learning for Robust Neural Networks Abstract: A multilayer perceptron can behave as a generative classifier by applying\nbidirectional learning (BL). It consists of training an undirected neural\nnetwork to map input to output and vice-versa; therefore it can produce a\nclassifier in one direction, and a generator in the opposite direction for the\nsame data. The learning process of BL tries to reproduce the neuroplasticity\nstated in Hebbian theory using only backward propagation of errors. In this\npaper, two novel learning techniques are introduced which use BL for improving\nrobustness to white noise static and adversarial examples. The first method is\nbidirectional propagation of errors, which the error propagation occurs in\nbackward and forward directions. Motivated by the fact that its generative\nmodel receives as input a constant vector per class, we introduce as a second\nmethod the hybrid adversarial networks (HAN). Its generative model receives a\nrandom vector as input and its training is based on generative adversarial\nnetworks (GAN). To assess the performance of BL, we perform experiments using\nseveral architectures with fully and convolutional layers, with and without\nbias. Experimental results show that both methods improve robustness to white\nnoise static and adversarial examples, and even increase accuracy, but have\ndifferent behavior depending on the architecture and task, being more\nbeneficial to use the one or the other. Nevertheless, HAN using a convolutional\narchitecture with batch normalization presents outstanding robustness, reaching\nstate-of-the-art accuracy on adversarial examples of hand-written digits. \n\n"}
{"id": "1805.08206", "contents": "Title: Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers Abstract: We consider the problem of uncertainty estimation in the context of\n(non-Bayesian) deep neural classification. In this context, all known methods\nare based on extracting uncertainty signals from a trained network optimized to\nsolve the classification problem at hand. We demonstrate that such techniques\ntend to introduce biased estimates for instances whose predictions are supposed\nto be highly confident. We argue that this deficiency is an artifact of the\ndynamics of training with SGD-like optimizers, and it has some properties\nsimilar to overfitting. Based on this observation, we develop an uncertainty\nestimation algorithm that selectively estimates the uncertainty of highly\nconfident points, using earlier snapshots of the trained model, before their\nestimates are jittered (and way before they are ready for actual\nclassification). We present extensive experiments indicating that the proposed\nalgorithm provides uncertainty estimates that are consistently better than all\nknown methods. \n\n"}
{"id": "1805.08349", "contents": "Title: A Solvable High-Dimensional Model of GAN Abstract: We present a theoretical analysis of the training process for a single-layer\nGAN fed by high-dimensional input data. The training dynamics of the proposed\nmodel at both microscopic and macroscopic scales can be exactly analyzed in the\nhigh-dimensional limit. In particular, we prove that the macroscopic quantities\nmeasuring the quality of the training process converge to a deterministic\nprocess characterized by an ordinary differential equation (ODE), whereas the\nmicroscopic states containing all the detailed weights remain stochastic, whose\ndynamics can be described by a stochastic differential equation (SDE). This\nanalysis provides a new perspective different from recent analyses in the limit\nof small learning rate, where the microscopic state is always considered\ndeterministic, and the contribution of noise is ignored. From our analysis, we\nshow that the level of the background noise is essential to the convergence of\nthe training process: setting the noise level too strong leads to failure of\nfeature recovery, whereas setting the noise too weak causes oscillation.\nAlthough this work focuses on a simple copy model of GAN, we believe the\nanalysis methods and insights developed here would prove useful in the\ntheoretical understanding of other variants of GANs with more advanced training\nalgorithms. \n\n"}
{"id": "1805.08916", "contents": "Title: Distribution Aware Active Learning Abstract: Discriminative learning machines often need a large set of labeled samples\nfor training. Active learning (AL) settings assume that the learner has the\nfreedom to ask an oracle to label its desired samples. Traditional AL\nalgorithms heuristically choose query samples about which the current learner\nis uncertain. This strategy does not make good use of the structure of the\ndataset at hand and is prone to be misguided by outliers. To alleviate this\nproblem, we propose to distill the structural information into a probabilistic\ngenerative model which acts as a \\emph{teacher} in our model. The active\n\\emph{learner} uses this information effectively at each cycle of active\nlearning. The proposed method is generic and does not depend on the type of\nlearner and teacher. We then suggest a query criterion for active learning that\nis aware of distribution of data and is more robust against outliers. Our\nmethod can be combined readily with several other query criteria for active\nlearning. We provide the formulation and empirically show our idea via toy and\nreal examples. \n\n"}
{"id": "1805.09484", "contents": "Title: Multi-Level Deep Cascade Trees for Conversion Rate Prediction in\n  Recommendation System Abstract: Developing effective and efficient recommendation methods is very challenging\nfor modern e-commerce platforms. Generally speaking, two essential modules\nnamed \"Click-Through Rate Prediction\" (\\textit{CTR}) and \"Conversion Rate\nPrediction\" (\\textit{CVR}) are included, where \\textit{CVR} module is a crucial\nfactor that affects the final purchasing volume directly. However, it is indeed\nvery challenging due to its sparseness nature. In this paper, we tackle this\nproblem by proposing multi-Level Deep Cascade Trees (\\textit{ldcTree}), which\nis a novel decision tree ensemble approach. It leverages deep cascade\nstructures by stacking Gradient Boosting Decision Trees (\\textit{GBDT}) to\neffectively learn feature representation. In addition, we propose to utilize\nthe cross-entropy in each tree of the preceding \\textit{GBDT} as the input\nfeature representation for next level \\textit{GBDT}, which has a clear\nexplanation, i.e., a traversal from root to leaf nodes in the next level\n\\textit{GBDT} corresponds to the combination of certain traversals in the\npreceding \\textit{GBDT}. The deep cascade structure and the combination rule\nenable the proposed \\textit{ldcTree} to have a stronger distributed feature\nrepresentation ability. Moreover, inspired by ensemble learning, we propose an\nEnsemble \\textit{ldcTree} (\\textit{E-ldcTree}) to encourage the model's\ndiversity and enhance the representation ability further. Finally, we propose\nan improved Feature learning method based on \\textit{EldcTree}\n(\\textit{F-EldcTree}) for taking adequate use of weak and strong correlation\nfeatures identified by pre-trained \\textit{GBDT} models. Experimental results\non off-line data set and online deployment demonstrate the effectiveness of the\nproposed methods. \n\n"}
{"id": "1805.09909", "contents": "Title: Structure Learning from Time Series with False Discovery Control Abstract: We consider the Granger causal structure learning problem from time series\ndata. Granger causal algorithms predict a 'Granger causal effect' between two\nvariables by testing if prediction error of one decreases significantly in the\nabsence of the other variable among the predictor covariates. Almost all\nexisting Granger causal algorithms condition on a large number of variables\n(all but two variables) to test for effects between a pair of variables. We\npropose a new structure learning algorithm called MMPC-p inspired by the well\nknown MMHC algorithm for non-time series data. We show that under some\nassumptions, the algorithm provides false discovery rate control. The algorithm\nis sound and complete when given access to perfect directed information testing\noracles. We also outline a novel tester for the linear Gaussian case. We show\nthrough our extensive experiments that the MMPC-p algorithm scales to larger\nproblems and has improved statistical power compared to existing state of the\nart for large sparse graphs. We also apply our algorithm on a global\ndevelopment dataset and validate our findings with subject matter experts. \n\n"}
{"id": "1805.10054", "contents": "Title: Stochastic algorithms with descent guarantees for ICA Abstract: Independent component analysis (ICA) is a widespread data exploration\ntechnique, where observed signals are modeled as linear mixtures of independent\ncomponents. From a machine learning point of view, it amounts to a matrix\nfactorization problem with a statistical independence criterion. Infomax is one\nof the most used ICA algorithms. It is based on a loss function which is a\nnon-convex log-likelihood. We develop a new majorization-minimization framework\nadapted to this loss function. We derive an online algorithm for the streaming\nsetting, and an incremental algorithm for the finite sum setting, with the\nfollowing benefits. First, unlike most algorithms found in the literature, the\nproposed methods do not rely on any critical hyper-parameter like a step size,\nnor do they require a line-search technique. Second, the algorithm for the\nfinite sum setting, although stochastic, guarantees a decrease of the loss\nfunction at each iteration. Experiments demonstrate progress on the\nstate-of-the-art for large scale datasets, without the necessity for any manual\nparameter tuning. \n\n"}
{"id": "1805.10123", "contents": "Title: TADAM: Task dependent adaptive metric for improved few-shot learning Abstract: Few-shot learning has become essential for producing models that generalize\nfrom few examples. In this work, we identify that metric scaling and metric\ntask conditioning are important to improve the performance of few-shot\nalgorithms. Our analysis reveals that simple metric scaling completely changes\nthe nature of few-shot algorithm parameter updates. Metric scaling provides\nimprovements up to 14% in accuracy for certain metrics on the mini-Imagenet\n5-way 5-shot classification task. We further propose a simple and effective way\nof conditioning a learner on the task sample set, resulting in learning a\ntask-dependent metric space. Moreover, we propose and empirically test a\npractical end-to-end optimization procedure based on auxiliary task co-training\nto learn a task-dependent metric space. The resulting few-shot learning model\nbased on the task-dependent scaled metric achieves state of the art on\nmini-Imagenet. We confirm these results on another few-shot dataset that we\nintroduce in this paper based on CIFAR100. Our code is publicly available at\nhttps://github.com/ElementAI/TADAM. \n\n"}
{"id": "1805.10369", "contents": "Title: Stable Recurrent Models Abstract: Stability is a fundamental property of dynamical systems, yet to this date it\nhas had little bearing on the practice of recurrent neural networks. In this\nwork, we conduct a thorough investigation of stable recurrent models.\nTheoretically, we prove stable recurrent neural networks are well approximated\nby feed-forward networks for the purpose of both inference and training by\ngradient descent. Empirically, we demonstrate stable recurrent models often\nperform as well as their unstable counterparts on benchmark sequence tasks.\nTaken together, these findings shed light on the effective power of recurrent\nnetworks and suggest much of sequence learning happens, or can be made to\nhappen, in the stable regime. Moreover, our results help to explain why in many\ncases practitioners succeed in replacing recurrent models by feed-forward\nmodels. \n\n"}
{"id": "1805.10503", "contents": "Title: Deep Learning Topological Invariants of Band Insulators Abstract: In this work we design and train deep neural networks to predict topological\ninvariants for one-dimensional four-band insulators in AIII class whose\ntopological invariant is the winding number, and two-dimensional two-band\ninsulators in A class whose topological invariant is the Chern number. Given\nHamiltonians in the momentum space as the input, neural networks can predict\ntopological invariants for both classes with accuracy close to or higher than\n90%, even for Hamiltonians whose invariants are beyond the training data set.\nDespite the complexity of the neural network, we find that the output of\ncertain intermediate hidden layers resembles either the winding angle for\nmodels in AIII class or the solid angle (Berry curvature) for models in A\nclass, indicating that neural networks essentially capture the mathematical\nformula of topological invariants. Our work demonstrates the ability of neural\nnetworks to predict topological invariants for complicated models with local\nHamiltonians as the only input, and offers an example that even a deep neural\nnetwork is understandable. \n\n"}
{"id": "1805.10896", "contents": "Title: Adaptive Network Sparsification with Dependent Variational\n  Beta-Bernoulli Dropout Abstract: While variational dropout approaches have been shown to be effective for\nnetwork sparsification, they are still suboptimal in the sense that they set\nthe dropout rate for each neuron without consideration of the input data. With\nsuch input-independent dropout, each neuron is evolved to be generic across\ninputs, which makes it difficult to sparsify networks without accuracy loss. To\novercome this limitation, we propose adaptive variational dropout whose\nprobabilities are drawn from sparsity-inducing beta Bernoulli prior. It allows\neach neuron to be evolved either to be generic or specific for certain inputs,\nor dropped altogether. Such input-adaptive sparsity-inducing dropout allows the\nresulting network to tolerate larger degree of sparsity without losing its\nexpressive power by removing redundancies among features. We validate our\ndependent variational beta-Bernoulli dropout on multiple public datasets, on\nwhich it obtains significantly more compact networks than baseline methods,\nwith consistent accuracy improvements over the base networks. \n\n"}
{"id": "1805.11155", "contents": "Title: Unsupervised Learning of Artistic Styles with Archetypal Style Analysis Abstract: In this paper, we introduce an unsupervised learning approach to\nautomatically discover, summarize, and manipulate artistic styles from large\ncollections of paintings. Our method is based on archetypal analysis, which is\nan unsupervised learning technique akin to sparse coding with a geometric\ninterpretation. When applied to deep image representations from a collection of\nartworks, it learns a dictionary of archetypal styles, which can be easily\nvisualized. After training the model, the style of a new image, which is\ncharacterized by local statistics of deep visual features, is approximated by a\nsparse convex combination of archetypes. This enables us to interpret which\narchetypal styles are present in the input image, and in which proportion.\nFinally, our approach allows us to manipulate the coefficients of the latent\narchetypal decomposition, and achieve various special effects such as style\nenhancement, transfer, and interpolation between multiple archetypes. \n\n"}
{"id": "1805.11614", "contents": "Title: Deep Learning under Privileged Information Using Heteroscedastic Dropout Abstract: Unlike machines, humans learn through rapid, abstract model-building. The\nrole of a teacher is not simply to hammer home right or wrong answers, but\nrather to provide intuitive comments, comparisons, and explanations to a pupil.\nThis is what the Learning Under Privileged Information (LUPI) paradigm\nendeavors to model by utilizing extra knowledge only available during training.\nWe propose a new LUPI algorithm specifically designed for Convolutional Neural\nNetworks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use a\nheteroscedastic dropout (i.e. dropout with a varying variance) and make the\nvariance of the dropout a function of privileged information. Intuitively, this\ncorresponds to using the privileged information to control the uncertainty of\nthe model output. We perform experiments using CNNs and RNNs for the tasks of\nimage classification and machine translation. Our method significantly\nincreases the sample efficiency during learning, resulting in higher accuracy\nwith a large margin when the number of training examples is limited. We also\ntheoretically justify the gains in sample efficiency by providing a\ngeneralization error bound decreasing with $O(\\frac{1}{n})$, where $n$ is the\nnumber of training examples, in an oracle case. \n\n"}
{"id": "1805.11640", "contents": "Title: K-Beam Minimax: Efficient Optimization for Deep Adversarial Learning Abstract: Minimax optimization plays a key role in adversarial training of machine\nlearning algorithms, such as learning generative models, domain adaptation,\nprivacy preservation, and robust learning. In this paper, we demonstrate the\nfailure of alternating gradient descent in minimax optimization problems due to\nthe discontinuity of solutions of the inner maximization. To address this, we\npropose a new epsilon-subgradient descent algorithm that addresses this problem\nby simultaneously tracking K candidate solutions. Practically, the algorithm\ncan find solutions that previous saddle-point algorithms cannot find, with only\na sublinear increase of complexity in K. We analyze the conditions under which\nthe algorithm converges to the true solution in detail. A significant\nimprovement in stability and convergence speed of the algorithm is observed in\nsimple representative problems, GAN training, and domain-adaptation problems. \n\n"}
{"id": "1805.11730", "contents": "Title: Learn to Combine Modalities in Multimodal Deep Learning Abstract: Combining complementary information from multiple modalities is intuitively\nappealing for improving the performance of learning-based approaches. However,\nit is challenging to fully leverage different modalities due to practical\nchallenges such as varying levels of noise and conflicts between modalities.\nExisting methods do not adopt a joint approach to capturing synergies between\nthe modalities while simultaneously filtering noise and resolving conflicts on\na per sample basis. In this work we propose a novel deep neural network based\ntechnique that multiplicatively combines information from different source\nmodalities. Thus the model training process automatically focuses on\ninformation from more reliable modalities while reducing emphasis on the less\nreliable modalities. Furthermore, we propose an extension that multiplicatively\ncombines not only the single-source modalities, but a set of mixtured source\nmodalities to better capture cross-modal signal correlations. We demonstrate\nthe effectiveness of our proposed technique by presenting empirical results on\nthree multimodal classification tasks from different domains. The results show\nconsistent accuracy improvements on all three tasks. \n\n"}
{"id": "1805.11837", "contents": "Title: Learning multiple non-mutually-exclusive tasks for improved\n  classification of inherently ordered labels Abstract: Medical image classification involves thresholding of labels that represent\nmalignancy risk levels. Usually, a task defines a single threshold, and when\ndeveloping computer-aided diagnosis tools, a single network is trained per such\nthreshold, e.g. as screening out healthy (very low risk) patients to leave\npossibly sick ones for further analysis (low threshold), or trying to find\nmalignant cases among those marked as non-risk by the radiologist (\"second\nreading\", high threshold). We propose a way to rephrase the classification\nproblem in a manner that yields several problems (corresponding to different\nthresholds) to be solved simultaneously. This allows the use of Multiple Task\nLearning (MTL) methods, significantly improving the performance of the original\nclassifier, by facilitating effective extraction of information from existing\ndata. \n\n"}
{"id": "1805.11956", "contents": "Title: Short-term Load Forecasting with Deep Residual Networks Abstract: We present in this paper a model for forecasting short-term power loads based\non deep residual networks. The proposed model is able to integrate domain\nknowledge and researchers' understanding of the task by virtue of different\nneural network building blocks. Specifically, a modified deep residual network\nis formulated to improve the forecast results. Further, a two-stage ensemble\nstrategy is used to enhance the generalization capability of the proposed\nmodel. We also apply the proposed model to probabilistic load forecasting using\nMonte Carlo dropout. Three public datasets are used to prove the effectiveness\nof the proposed model. Multiple test cases and comparison with existing models\nshow that the proposed model is able to provide accurate load forecasting\nresults and has high generalization capability. \n\n"}
{"id": "1805.12062", "contents": "Title: Sobolev Descent Abstract: We study a simplification of GAN training: the problem of transporting\nparticles from a source to a target distribution. Starting from the Sobolev GAN\ncritic, part of the gradient regularized GAN family, we show a strong relation\nwith Optimal Transport (OT). Specifically with the less popular dynamic\nformulation of OT that finds a path of distributions from source to target\nminimizing a ``kinetic energy''. We introduce Sobolev descent that constructs\nsimilar paths by following gradient flows of a critic function in a kernel\nspace or parametrized by a neural network. In the kernel version, we show\nconvergence to the target distribution in the MMD sense. We show in theory and\nexperiments that regularization has an important role in favoring smooth\ntransitions between distributions, avoiding large gradients from the critic.\nThis analysis in a simplified particle setting provides insight in paths to\nequilibrium in GANs. \n\n"}
{"id": "1805.12164", "contents": "Title: What the Vec? Towards Probabilistically Grounded Embeddings Abstract: Word2Vec (W2V) and GloVe are popular, fast and efficient word embedding\nalgorithms. Their embeddings are widely used and perform well on a variety of\nnatural language processing tasks. Moreover, W2V has recently been adopted in\nthe field of graph embedding, where it underpins several leading algorithms.\nHowever, despite their ubiquity and relatively simple model architecture, a\ntheoretical understanding of what the embedding parameters of W2V and GloVe\nlearn and why that is useful in downstream tasks has been lacking. We show that\ndifferent interactions between PMI vectors reflect semantic word relationships,\nsuch as similarity and paraphrasing, that are encoded in low dimensional word\nembeddings under a suitable projection, theoretically explaining why embeddings\nof W2V and GloVe work. As a consequence, we also reveal an interesting\nmathematical interconnection between the considered semantic relationships\nthemselves. \n\n"}
{"id": "1805.12168", "contents": "Title: A Flexible Framework for Multi-Objective Bayesian Optimization using\n  Random Scalarizations Abstract: Many real world applications can be framed as multi-objective optimization\nproblems, where we wish to simultaneously optimize for multiple criteria.\nBayesian optimization techniques for the multi-objective setting are pertinent\nwhen the evaluation of the functions in question are expensive. Traditional\nmethods for multi-objective optimization, both Bayesian and otherwise, are\naimed at recovering the Pareto front of these objectives. However, in certain\ncases a practitioner might desire to identify Pareto optimal points only in a\nsubset of the Pareto front due to external considerations. In this work, we\npropose a strategy based on random scalarizations of the objectives that\naddresses this problem. Our approach is able to flexibly sample from desired\nregions of the Pareto front and, computationally, is considerably cheaper than\nmost approaches for MOO. We also study a notion of regret in the\nmulti-objective setting and show that our strategy achieves sublinear regret.\nWe experiment with both synthetic and real-life problems, and demonstrate\nsuperior performance of our proposed algorithm in terms of the flexibility and\nregret. \n\n"}
{"id": "1805.12462", "contents": "Title: On GANs and GMMs Abstract: A longstanding problem in machine learning is to find unsupervised methods\nthat can learn the statistical structure of high dimensional signals. In recent\nyears, GANs have gained much attention as a possible solution to the problem,\nand in particular have shown the ability to generate remarkably realistic high\nresolution sampled images. At the same time, many authors have pointed out that\nGANs may fail to model the full distribution (\"mode collapse\") and that using\nthe learned models for anything other than generating samples may be very\ndifficult. In this paper, we examine the utility of GANs in learning\nstatistical models of images by comparing them to perhaps the simplest\nstatistical model, the Gaussian Mixture Model. First, we present a simple\nmethod to evaluate generative models based on relative proportions of samples\nthat fall into predetermined bins. Unlike previous automatic methods for\nevaluating models, our method does not rely on an additional neural network nor\ndoes it require approximating intractable computations. Second, we compare the\nperformance of GANs to GMMs trained on the same datasets. While GMMs have\npreviously been shown to be successful in modeling small patches of images, we\nshow how to train them on full sized images despite the high dimensionality.\nOur results show that GMMs can generate realistic samples (although less sharp\nthan those of GANs) but also capture the full distribution, which GANs fail to\ndo. Furthermore, GMMs allow efficient inference and explicit representation of\nthe underlying statistical structure. Finally, we discuss how GMMs can be used\nto generate sharp images. \n\n"}
{"id": "1805.12514", "contents": "Title: Scaling provable adversarial defenses Abstract: Recent work has developed methods for learning deep network classifiers that\nare provably robust to norm-bounded adversarial perturbation; however, these\nmethods are currently only possible for relatively small feedforward networks.\nIn this paper, in an effort to scale these approaches to substantially larger\nmodels, we extend previous work in three main directions. First, we present a\ntechnique for extending these training procedures to much more general\nnetworks, with skip connections (such as ResNets) and general nonlinearities;\nthe approach is fully modular, and can be implemented automatically (analogous\nto automatic differentiation). Second, in the specific case of $\\ell_\\infty$\nadversarial perturbations and networks with ReLU nonlinearities, we adopt a\nnonlinear random projection for training, which scales linearly in the number\nof hidden units (previous approaches scaled quadratically). Third, we show how\nto further improve robust error through cascade models. On both MNIST and CIFAR\ndata sets, we train classifiers that improve substantially on the state of the\nart in provable robust adversarial error bounds: from 5.8% to 3.1% on MNIST\n(with $\\ell_\\infty$ perturbations of $\\epsilon=0.1$), and from 80% to 36.4% on\nCIFAR (with $\\ell_\\infty$ perturbations of $\\epsilon=2/255$). Code for all\nexperiments in the paper is available at\nhttps://github.com/locuslab/convex_adversarial/. \n\n"}
{"id": "1806.00273", "contents": "Title: Sparse Pursuit and Dictionary Learning for Blind Source Separation in\n  Polyphonic Music Recordings Abstract: We propose an algorithm for the blind separation of single-channel audio\nsignals. It is based on a parametric model that describes the spectral\nproperties of the sounds of musical instruments independently of pitch. We\ndevelop a novel sparse pursuit algorithm that can match the discrete frequency\nspectra from the recorded signal with the continuous spectra delivered by the\nmodel. We first use this algorithm to convert an STFT spectrogram from the\nrecording into a novel form of log-frequency spectrogram whose resolution\nexceeds that of the mel spectrogram. We then make use of the pitch-invariant\nproperties of that representation in order to identify the sounds of the\ninstruments via the same sparse pursuit method. As the model parameters which\ncharacterize the musical instruments are not known beforehand, we train a\ndictionary that contains them, using a modified version of Adam. Applying the\nalgorithm on various audio samples, we find that it is capable of producing\nhigh-quality separation results when the model assumptions are satisfied and\nthe instruments are clearly distinguishable, but combinations of instruments\nwith similar spectral characteristics pose a conceptual difficulty. While a key\nfeature of the model is that it explicitly models inharmonicity, its presence\ncan also still impede performance of the sparse pursuit algorithm. In general,\ndue to its pitch-invariance, our method is especially suitable for dealing with\nspectra from acoustic instruments, requiring only a minimal number of\nhyperparameters to be preset. Additionally, we demonstrate that the dictionary\nthat is constructed for one recording can be applied to a different recording\nwith similar instruments without additional training. \n\n"}
{"id": "1806.00458", "contents": "Title: Improved Sample Complexity for Stochastic Compositional Variance Reduced\n  Gradient Abstract: Convex composition optimization is an emerging topic that covers a wide range\nof applications arising from stochastic optimal control, reinforcement learning\nand multi-stage stochastic programming. Existing algorithms suffer from\nunsatisfactory sample complexity and practical issues since they ignore the\nconvexity structure in the algorithmic design. In this paper, we develop a new\nstochastic compositional variance-reduced gradient algorithm with the sample\ncomplexity of $O((m+n)\\log(1/\\epsilon)+1/\\epsilon^3)$ where $m+n$ is the total\nnumber of samples. Our algorithm is near-optimal as the dependence on $m+n$ is\noptimal up to a logarithmic factor. Experimental results on real-world datasets\ndemonstrate the effectiveness and efficiency of the new algorithm. \n\n"}
{"id": "1806.00572", "contents": "Title: Autoencoders Learn Generative Linear Models Abstract: We provide a series of results for unsupervised learning with autoencoders.\nSpecifically, we study shallow two-layer autoencoder architectures with shared\nweights. We focus on three generative models for data that are common in\nstatistical machine learning: (i) the mixture-of-gaussians model, (ii) the\nsparse coding model, and (iii) the sparsity model with non-negative\ncoefficients. For each of these models, we prove that under suitable choices of\nhyperparameters, architectures, and initialization, autoencoders learned by\ngradient descent can successfully recover the parameters of the corresponding\nmodel. To our knowledge, this is the first result that rigorously studies the\ndynamics of gradient descent for weight-sharing autoencoders. Our analysis can\nbe viewed as theoretical evidence that shallow autoencoder modules indeed can\nbe used as feature learning mechanisms for a variety of data models, and may\nshed insight on how to train larger stacked architectures with autoencoders as\nbasic building blocks. \n\n"}
{"id": "1806.00730", "contents": "Title: Minnorm training: an algorithm for training over-parameterized deep\n  neural networks Abstract: In this work, we propose a new training method for finding minimum weight\nnorm solutions in over-parameterized neural networks (NNs). This method seeks\nto improve training speed and generalization performance by framing NN training\nas a constrained optimization problem wherein the sum of the norm of the\nweights in each layer of the network is minimized, under the constraint of\nexactly fitting training data. It draws inspiration from support vector\nmachines (SVMs), which are able to generalize well, despite often having an\ninfinite number of free parameters in their primal form, and from recent\ntheoretical generalization bounds on NNs which suggest that lower norm\nsolutions generalize better. To solve this constrained optimization problem,\nour method employs Lagrange multipliers that act as integrators of error over\ntraining and identify `support vector'-like examples. The method can be\nimplemented as a wrapper around gradient based methods and uses standard\nback-propagation of gradients from the NN for both regression and\nclassification versions of the algorithm. We provide theoretical justifications\nfor the effectiveness of this algorithm in comparison to early stopping and\n$L_2$-regularization using simple, analytically tractable settings. In\nparticular, we show faster convergence to the max-margin hyperplane in a\nshallow network (compared to vanilla gradient descent); faster convergence to\nthe minimum-norm solution in a linear chain (compared to $L_2$-regularization);\nand initialization-independent generalization performance in a deep linear\nnetwork. Finally, using the MNIST dataset, we demonstrate that this algorithm\ncan boost test accuracy and identify difficult examples in real-world datasets. \n\n"}
{"id": "1806.00732", "contents": "Title: Data-driven identification of parametric partial differential equations Abstract: In this work we present a data-driven method for the discovery of parametric\npartial differential equations (PDEs), thus allowing one to disambiguate\nbetween the underlying evolution equations and their parametric dependencies.\nGroup sparsity is used to ensure parsimonious representations of observed\ndynamics in the form of a parametric PDE, while also allowing the coefficients\nto have arbitrary time series, or spatial dependence. This work builds on\nprevious methods for the identification of constant coefficient PDEs, expanding\nthe field to include a new class of equations which until now have eluded\nmachine learning based identification methods. We show that group sequentially\nthresholded ridge regression outperforms group LASSO in identifying the fewest\nterms in the PDE along with their parametric dependency. The method is\ndemonstrated on four canonical models with and without the introduction of\nnoise. \n\n"}
{"id": "1806.01603", "contents": "Title: Layer rotation: a surprisingly powerful indicator of generalization in\n  deep networks? Abstract: Our work presents extensive empirical evidence that layer rotation, i.e. the\nevolution across training of the cosine distance between each layer's weight\nvector and its initialization, constitutes an impressively consistent indicator\nof generalization performance. In particular, larger cosine distances between\nfinal and initial weights of each layer consistently translate into better\ngeneralization performance of the final model. Interestingly, this relation\nadmits a network independent optimum: training procedures during which all\nlayers' weights reach a cosine distance of 1 from their initialization\nconsistently outperform other configurations -by up to 30% test accuracy.\nMoreover, we show that layer rotations are easily monitored and controlled\n(helpful for hyperparameter tuning) and potentially provide a unified framework\nto explain the impact of learning rate tuning, weight decay, learning rate\nwarmups and adaptive gradient methods on generalization and training speed. In\nan attempt to explain the surprising properties of layer rotation, we show on a\n1-layer MLP trained on MNIST that layer rotation correlates with the degree to\nwhich features of intermediate layers have been trained. \n\n"}
{"id": "1806.01655", "contents": "Title: Deep Gaussian Processes with Convolutional Kernels Abstract: Deep Gaussian processes (DGPs) provide a Bayesian non-parametric alternative\nto standard parametric deep learning models. A DGP is formed by stacking\nmultiple GPs resulting in a well-regularized composition of functions. The\nBayesian framework that equips the model with attractive properties, such as\nimplicit capacity control and predictive uncertainty, makes it at the same time\nchallenging to combine with a convolutional structure. This has hindered the\napplication of DGPs in computer vision tasks, an area where deep parametric\nmodels (i.e. CNNs) have made breakthroughs. Standard kernels used in DGPs such\nas radial basis functions (RBFs) are insufficient for handling pixel\nvariability in raw images. In this paper, we build on the recent convolutional\nGP to develop Convolutional DGP (CDGP) models which effectively capture image\nlevel features through the use of convolution kernels, therefore opening up the\nway for applying DGPs to computer vision tasks. Our model learns local spatial\ninfluence and outperforms strong GP based baselines on multi-class image\nclassification. We also consider various constructions of convolution kernel\nover the image patches, analyze the computational trade-offs and provide an\nefficient framework for convolutional DGP models. The experimental results on\nimage data such as MNIST, rectangles-image, CIFAR10 and Caltech101 demonstrate\nthe effectiveness of the proposed approaches. \n\n"}
{"id": "1806.01678", "contents": "Title: A Projection Method for Metric-Constrained Optimization Abstract: We outline a new approach for solving optimization problems which enforce\ntriangle inequalities on output variables. We refer to this as\nmetric-constrained optimization, and give several examples where problems of\nthis form arise in machine learning applications and theoretical approximation\nalgorithms for graph clustering. Although these problem are interesting from a\ntheoretical perspective, they are challenging to solve in practice due to the\nhigh memory requirement of black-box solvers. In order to address this\nchallenge we first prove that the metric-constrained linear program relaxation\nof correlation clustering is equivalent to a special case of the metric\nnearness problem. We then developed a general solver for metric-constrained\nlinear and quadratic programs by generalizing and improving a simple projection\nalgorithm originally developed for metric nearness. We give several novel\napproximation guarantees for using our framework to find lower bounds for\noptimal solutions to several challenging graph clustering problems. We also\ndemonstrate the power of our framework by solving optimizing problems involving\nup to 10^{8} variables and 10^{11} constraints. \n\n"}
{"id": "1806.02071", "contents": "Title: Deep Fluids: A Generative Network for Parameterized Fluid Simulations Abstract: This paper presents a novel generative model to synthesize fluid simulations\nfrom a set of reduced parameters. A convolutional neural network is trained on\na collection of discrete, parameterizable fluid simulation velocity fields. Due\nto the capability of deep learning architectures to learn representative\nfeatures of the data, our generative model is able to accurately approximate\nthe training data set, while providing plausible interpolated in-betweens. The\nproposed generative model is optimized for fluids by a novel loss function that\nguarantees divergence-free velocity fields at all times. In addition, we\ndemonstrate that we can handle complex parameterizations in reduced spaces, and\nadvance simulations in time by integrating in the latent space with a second\nnetwork. Our method models a wide variety of fluid behaviors, thus enabling\napplications such as fast construction of simulations, interpolation of fluids\nwith different parameters, time re-sampling, latent space simulations, and\ncompression of fluid simulation data. Reconstructed velocity fields are\ngenerated up to 700x faster than re-simulating the data with the underlying CPU\nsolver, while achieving compression rates of up to 1300x. \n\n"}
{"id": "1806.02146", "contents": "Title: Adversarial Auto-encoders for Speech Based Emotion Recognition Abstract: Recently, generative adversarial networks and adversarial autoencoders have\ngained a lot of attention in machine learning community due to their\nexceptional performance in tasks such as digit classification and face\nrecognition. They map the autoencoder's bottleneck layer output (termed as code\nvectors) to different noise Probability Distribution Functions (PDFs), that can\nbe further regularized to cluster based on class information. In addition, they\nalso allow a generation of synthetic samples by sampling the code vectors from\nthe mapped PDFs. Inspired by these properties, we investigate the application\nof adversarial autoencoders to the domain of emotion recognition. Specifically,\nwe conduct experiments on the following two aspects: (i) their ability to\nencode high dimensional feature vector representations for emotional utterances\ninto a compressed space (with a minimal loss of emotion class discriminability\nin the compressed space), and (ii) their ability to regenerate synthetic\nsamples in the original feature space, to be later used for purposes such as\ntraining emotion recognition classifiers. We demonstrate the promise of\nadversarial autoencoders with regards to these aspects on the Interactive\nEmotional Dyadic Motion Capture (IEMOCAP) corpus and present our analysis. \n\n"}
{"id": "1806.02390", "contents": "Title: Variational Implicit Processes Abstract: We introduce the implicit processes (IPs), a stochastic process that places\nimplicitly defined multivariate distributions over any finite collections of\nrandom variables. IPs are therefore highly flexible implicit priors over\nfunctions, with examples including data simulators, Bayesian neural networks\nand non-linear transformations of stochastic processes. A novel and efficient\napproximate inference algorithm for IPs, namely the variational implicit\nprocesses (VIPs), is derived using generalised wake-sleep updates. This method\nreturns simple update equations and allows scalable hyper-parameter learning\nwith stochastic optimization. Experiments show that VIPs return better\nuncertainty estimates and lower errors over existing inference methods for\nchallenging models such as Bayesian neural networks, and Gaussian processes. \n\n"}
{"id": "1806.02426", "contents": "Title: Deep Variational Reinforcement Learning for POMDPs Abstract: Many real-world sequential decision making problems are partially observable\nby nature, and the environment model is typically unknown. Consequently, there\nis great need for reinforcement learning methods that can tackle such problems\ngiven only a stream of incomplete and noisy observations. In this paper, we\npropose deep variational reinforcement learning (DVRL), which introduces an\ninductive bias that allows an agent to learn a generative model of the\nenvironment and perform inference in that model to effectively aggregate the\navailable information. We develop an n-step approximation to the evidence lower\nbound (ELBO), allowing the model to be trained jointly with the policy. This\nensures that the latent state representation is suitable for the control task.\nIn experiments on Mountain Hike and flickering Atari we show that our method\noutperforms previous approaches relying on recurrent neural networks to encode\nthe past. \n\n"}
{"id": "1806.02654", "contents": "Title: New Hybrid Neuro-Evolutionary Algorithms for Renewable Energy and\n  Facilities Management Problems Abstract: This Ph.D. thesis deals with the optimization of several renewable energy\nresources development as well as the improvement of facilities management in\noceanic engineering and airports, using computational hybrid methods belonging\nto AI to this end. Energy is essential to our society in order to ensure a good\nquality of life. This means that predictions over the characteristics on which\nrenewable energies depend are necessary, in order to know the amount of energy\nthat will be obtained at any time. The second topic tackled in this thesis is\nrelated to the basic parameters that influence in different marine activities\nand airports, whose knowledge is necessary to develop a proper facilities\nmanagement in these environments. Within this work, a study of the\nstate-of-the-art Machine Learning have been performed to solve the problems\nassociated with the topics above-mentioned, and several contributions have been\nproposed: One of the pillars of this work is focused on the estimation of the\nmost important parameters in the exploitation of renewable resources. The\nsecond contribution of this thesis is related to feature selection problems.\nThe proposed methodologies are applied to multiple problems: the prediction of\n$H_s$, relevant for marine energy applications and marine activities, the\nestimation of WPREs, undesirable variations in the electric power produced by a\nwind farm, the prediction of global solar radiation in areas from Spain and\nAustralia, really important in terms of solar energy, and the prediction of\nlow-visibility events at airports. All of these practical issues are developed\nwith the consequent previous data analysis, normally, in terms of\nmeteorological variables. \n\n"}
{"id": "1806.02855", "contents": "Title: Scalable Natural Gradient Langevin Dynamics in Practice Abstract: Stochastic Gradient Langevin Dynamics (SGLD) is a sampling scheme for\nBayesian modeling adapted to large datasets and models. SGLD relies on the\ninjection of Gaussian Noise at each step of a Stochastic Gradient Descent (SGD)\nupdate. In this scheme, every component in the noise vector is independent and\nhas the same scale, whereas the parameters we seek to estimate exhibit strong\nvariations in scale and significant correlation structures, leading to poor\nconvergence and mixing times. We compare different preconditioning approaches\nto the normalization of the noise vector and benchmark these approaches on the\nfollowing criteria: 1) mixing times of the multivariate parameter vector, 2)\nregularizing effect on small dataset where it is easy to overfit, 3) covariate\nshift detection and 4) resistance to adversarial examples. \n\n"}
{"id": "1806.02957", "contents": "Title: A Deep Neural Network Surrogate for High-Dimensional Random Partial\n  Differential Equations Abstract: Developing efficient numerical algorithms for the solution of high\ndimensional random Partial Differential Equations (PDEs) has been a challenging\ntask due to the well-known curse of dimensionality. We present a new solution\nframework for these problems based on a deep learning approach. Specifically,\nthe random PDE is approximated by a feed-forward fully-connected deep residual\nnetwork, with either strong or weak enforcement of initial and boundary\nconstraints. The framework is mesh-free, and can handle irregular computational\ndomains. Parameters of the approximating deep neural network are determined\niteratively using variants of the Stochastic Gradient Descent (SGD) algorithm.\nThe satisfactory accuracy of the proposed frameworks is numerically\ndemonstrated on diffusion and heat conduction problems, in comparison with the\nconverged Monte Carlo-based finite element results. \n\n"}
{"id": "1806.03085", "contents": "Title: A Stein variational Newton method Abstract: Stein variational gradient descent (SVGD) was recently proposed as a general\npurpose nonparametric variational inference algorithm [Liu & Wang, NIPS 2016]:\nit minimizes the Kullback-Leibler divergence between the target distribution\nand its approximation by implementing a form of functional gradient descent on\na reproducing kernel Hilbert space. In this paper, we accelerate and generalize\nthe SVGD algorithm by including second-order information, thereby approximating\na Newton-like iteration in function space. We also show how second-order\ninformation can lead to more effective choices of kernel. We observe\nsignificant computational gains over the original SVGD algorithm in multiple\ntest cases. \n\n"}
{"id": "1806.03125", "contents": "Title: Text Classification based on Word Subspace with Term-Frequency Abstract: Text classification has become indispensable due to the rapid increase of\ntext in digital form. Over the past three decades, efforts have been made to\napproach this task using various learning algorithms and statistical models\nbased on bag-of-words (BOW) features. Despite its simple implementation, BOW\nfeatures lack semantic meaning representation. To solve this problem, neural\nnetworks started to be employed to learn word vectors, such as the word2vec.\nWord2vec embeds word semantic structure into vectors, where the angle between\nvectors indicates the meaningful similarity between words. To measure the\nsimilarity between texts, we propose the novel concept of word subspace, which\ncan represent the intrinsic variability of features in a set of word vectors.\nThrough this concept, it is possible to model text from word vectors while\nholding semantic information. To incorporate the word frequency directly in the\nsubspace model, we further extend the word subspace to the term-frequency (TF)\nweighted word subspace. Based on these new concepts, text classification can be\nperformed under the mutual subspace method (MSM) framework. The validity of our\nmodeling is shown through experiments on the Reuters text database, comparing\nthe results to various state-of-art algorithms. \n\n"}
{"id": "1806.04509", "contents": "Title: A review on distance based time series classification Abstract: Time series classification is an increasing research topic due to the vast\namount of time series data that are being created over a wide variety of\nfields. The particularity of the data makes it a challenging task and different\napproaches have been taken, including the distance based approach. 1-NN has\nbeen a widely used method within distance based time series classification due\nto it simplicity but still good performance. However, its supremacy may be\nattributed to being able to use specific distances for time series within the\nclassification process and not to the classifier itself. With the aim of\nexploiting these distances within more complex classifiers, new approaches have\narisen in the past few years that are competitive or which outperform the 1-NN\nbased approaches. In some cases, these new methods use the distance measure to\ntransform the series into feature vectors, bridging the gap between time series\nand traditional classifiers. In other cases, the distances are employed to\nobtain a time series kernel and enable the use of kernel methods for time\nseries classification. One of the main challenges is that a kernel function\nmust be positive semi-definite, a matter that is also addressed within this\nreview. The presented review includes a taxonomy of all those methods that aim\nto classify time series using a distance based approach, as well as a\ndiscussion of the strengths and weaknesses of each method. \n\n"}
{"id": "1806.04562", "contents": "Title: Multi-Agent Deep Reinforcement Learning with Human Strategies Abstract: Deep learning has enabled traditional reinforcement learning methods to deal\nwith high-dimensional problems. However, one of the disadvantages of deep\nreinforcement learning methods is the limited exploration capacity of learning\nagents. In this paper, we introduce an approach that integrates human\nstrategies to increase the exploration capacity of multiple deep reinforcement\nlearning agents. We also report the development of our own multi-agent\nenvironment called Multiple Tank Defence to simulate the proposed approach. The\nresults show the significant performance improvement of multiple agents that\nhave learned cooperatively with human strategies. This implies that there is a\ncritical need for human intellect teamed with machines to solve complex\nproblems. In addition, the success of this simulation indicates that our\nmulti-agent environment can be used as a testbed platform to develop and\nvalidate other multi-agent control algorithms. \n\n"}
{"id": "1806.04830", "contents": "Title: Deep Multiscale Model Learning Abstract: The objective of this paper is to design novel multi-layer neural network\narchitectures for multiscale simulations of flows taking into account the\nobserved data and physical modeling concepts. Our approaches use deep learning\nconcepts combined with local multiscale model reduction methodologies to\npredict flow dynamics. Using reduced-order model concepts is important for\nconstructing robust deep learning architectures since the reduced-order models\nprovide fewer degrees of freedom. Flow dynamics can be thought of as\nmulti-layer networks. More precisely, the solution (e.g., pressures and\nsaturations) at the time instant $n+1$ depends on the solution at the time\ninstant $n$ and input parameters, such as permeability fields, forcing terms,\nand initial conditions. One can regard the solution as a multi-layer network,\nwhere each layer, in general, is a nonlinear forward map and the number of\nlayers relates to the internal time steps. We will rely on rigorous model\nreduction concepts to define unknowns and connections for each layer. In each\nlayer, our reduced-order models will provide a forward map, which will be\nmodified (\"trained\") using available data. It is critical to use reduced-order\nmodels for this purpose, which will identify the regions of influence and the\nappropriate number of variables. Because of the lack of available data, the\ntraining will be supplemented with computational data as needed and the\ninterpolation between data-rich and data-deficient models. We will also use\ndeep learning algorithms to train the elements of the reduced model discrete\nsystem. We will present main ingredients of our approach and numerical results.\nNumerical results show that using deep learning and multiscale models, we can\nimprove the forward models, which are conditioned to the available data. \n\n"}
{"id": "1806.05780", "contents": "Title: Surprising Negative Results for Generative Adversarial Tree Search Abstract: While many recent advances in deep reinforcement learning (RL) rely on\nmodel-free methods, model-based approaches remain an alluring prospect for\ntheir potential to exploit unsupervised data to learn environment model. In\nthis work, we provide an extensive study on the design of deep generative\nmodels for RL environments and propose a sample efficient and robust method to\nlearn the model of Atari environments. We deploy this model and propose\ngenerative adversarial tree search (GATS) a deep RL algorithm that learns the\nenvironment model and implements Monte Carlo tree search (MCTS) on the learned\nmodel for planning. While MCTS on the learned model is computationally\nexpensive, similar to AlphaGo, GATS follows depth limited MCTS. GATS employs\ndeep Q network (DQN) and learns a Q-function to assign values to the leaves of\nthe tree in MCTS. We theoretical analyze GATS vis-a-vis the bias-variance\ntrade-off and show GATS is able to mitigate the worst-case error in the\nQ-estimate. While we were expecting GATS to enjoy a better sample complexity\nand faster converges to better policies, surprisingly, GATS fails to outperform\nDQN. We provide a study on which we show why depth limited MCTS fails to\nperform desirably. \n\n"}
{"id": "1806.05978", "contents": "Title: Uncertainty Estimations by Softplus normalization in Bayesian\n  Convolutional Neural Networks with Variational Inference Abstract: We introduce a novel uncertainty estimation for classification tasks for\nBayesian convolutional neural networks with variational inference. By\nnormalizing the output of a Softplus function in the final layer, we estimate\naleatoric and epistemic uncertainty in a coherent manner. The intractable\nposterior probability distributions over weights are inferred by Bayes by\nBackprop. Firstly, we demonstrate how this reliable variational inference\nmethod can serve as a fundamental construct for various network architectures.\nOn multiple datasets in supervised learning settings (MNIST, CIFAR-10,\nCIFAR-100), this variational inference method achieves performances equivalent\nto frequentist inference in identical architectures, while the two desiderata,\na measure for uncertainty and regularization are incorporated naturally.\nSecondly, we examine how our proposed measure for aleatoric and epistemic\nuncertainties is derived and validate it on the aforementioned datasets. \n\n"}
{"id": "1806.06373", "contents": "Title: Geodesic Convex Optimization: Differentiation on Manifolds, Geodesics,\n  and Convexity Abstract: Convex optimization is a vibrant and successful area due to the existence of\na variety of efficient algorithms that leverage the rich structure provided by\nconvexity. Convexity of a smooth set or a function in a Euclidean space is\ndefined by how it interacts with the standard differential structure in this\nspace -- the Hessian of a convex function has to be positive semi-definite\neverywhere. However, in recent years, there is a growing demand to understand\nnon-convexity and develop computational methods to optimize non-convex\nfunctions. Intriguingly, there is a type of non-convexity that disappears once\none introduces a suitable differentiable structure and redefines convexity with\nrespect to the straight lines, or {\\em geodesics}, with respect to this\nstructure. Such convexity is referred to as {\\em geodesic convexity}. Interest\nin studying it arises due to recent reformulations of some non-convex problems\nas geodesically convex optimization problems over geodesically convex sets.\nGeodesics on manifolds have been extensively studied in various branches of\nMathematics and Physics. However, unlike convex optimization, understanding\ngeodesics and geodesic convexity from a computational point of view largely\nremains a mystery. The goal of this exposition is to introduce the first part\nof geodesic convex optimization -- geodesic convexity -- in a self-contained\nmanner. We first present a variety of notions from differential and Riemannian\ngeometry such as differentiation on manifolds, geodesics, and then introduce\ngeodesic convexity. We conclude by showing that certain non-convex optimization\nproblems such as computing the Brascamp-Lieb constant and the operator scaling\nproblem have geodesically convex formulations. \n\n"}
{"id": "1806.06384", "contents": "Title: Multi-variable LSTM neural network for autoregressive exogenous model Abstract: In this paper, we propose multi-variable LSTM capable of accurate forecasting\nand variable importance interpretation for time series with exogenous\nvariables. Current attention mechanism in recurrent neural networks mostly\nfocuses on the temporal aspect of data and falls short of characterizing\nvariable importance. To this end, the multi-variable LSTM equipped with\ntensorized hidden states is developed to learn hidden states for individual\nvariables, which give rise to our mixture temporal and variable attention.\nBased on such attention mechanism, we infer and quantify variable importance.\nExtensive experiments using real datasets with Granger-causality test and the\nsynthetic dataset with ground truth demonstrate the prediction performance and\ninterpretability of multi-variable LSTM in comparison to a variety of\nbaselines. It exhibits the prospect of multi-variable LSTM as an end-to-end\nframework for both forecasting and knowledge discovery. \n\n"}
{"id": "1806.06457", "contents": "Title: Fast Convex Pruning of Deep Neural Networks Abstract: We develop a fast, tractable technique called Net-Trim for simplifying a\ntrained neural network. The method is a convex post-processing module, which\nprunes (sparsifies) a trained network layer by layer, while preserving the\ninternal responses. We present a comprehensive analysis of Net-Trim from both\nthe algorithmic and sample complexity standpoints, centered on a fast, scalable\nconvex optimization program. Our analysis includes consistency results between\nthe initial and retrained models before and after Net-Trim application and\nguarantees on the number of training samples needed to discover a network that\ncan be expressed using a certain number of nonzero terms. Specifically, if\nthere is a set of weights that uses at most $s$ terms that can re-create the\nlayer outputs from the layer inputs, we can find these weights from\n$\\mathcal{O}(s\\log N/s)$ samples, where $N$ is the input size. These\ntheoretical results are similar to those for sparse regression using the Lasso,\nand our analysis uses some of the same recently-developed tools (namely recent\nresults on the concentration of measure and convex analysis). Finally, we\npropose an algorithmic framework based on the alternating direction method of\nmultipliers (ADMM), which allows a fast and simple implementation of Net-Trim\nfor network pruning and compression. \n\n"}
{"id": "1806.06720", "contents": "Title: An Online Prediction Algorithm for Reinforcement Learning with Linear\n  Function Approximation using Cross Entropy Method Abstract: In this paper, we provide two new stable online algorithms for the problem of\nprediction in reinforcement learning, \\emph{i.e.}, estimating the value\nfunction of a model-free Markov reward process using the linear function\napproximation architecture and with memory and computation costs scaling\nquadratically in the size of the feature set. The algorithms employ the\nmulti-timescale stochastic approximation variant of the very popular cross\nentropy (CE) optimization method which is a model based search method to find\nthe global optimum of a real-valued function. A proof of convergence of the\nalgorithms using the ODE method is provided. We supplement our theoretical\nresults with experimental comparisons. The algorithms achieve good performance\nfairly consistently on many RL benchmark problems with regards to computational\nefficiency, accuracy and stability. \n\n"}
{"id": "1806.06765", "contents": "Title: Modularity Matters: Learning Invariant Relational Reasoning Tasks Abstract: We focus on two supervised visual reasoning tasks whose labels encode a\nsemantic relational rule between two or more objects in an image: the MNIST\nParity task and the colorized Pentomino task. The objects in the images undergo\nrandom translation, scaling, rotation and coloring transformations. Thus these\ntasks involve invariant relational reasoning. We report uneven performance of\nvarious deep CNN models on these two tasks. For the MNIST Parity task, we\nreport that the VGG19 model soundly outperforms a family of ResNet models.\nMoreover, the family of ResNet models exhibits a general sensitivity to random\ninitialization for the MNIST Parity task. For the colorized Pentomino task, now\nboth the VGG19 and ResNet models exhibit sluggish optimization and very poor\ntest generalization, hovering around 30% test error. The CNN we tested all\nlearn hierarchies of fully distributed features and thus encode the distributed\nrepresentation prior. We are motivated by a hypothesis from cognitive\nneuroscience which posits that the human visual cortex is modularized, and this\nallows the visual cortex to learn higher order invariances. To this end, we\nconsider a modularized variant of the ResNet model, referred to as a Residual\nMixture Network (ResMixNet) which employs a mixture-of-experts architecture to\ninterleave distributed representations with more specialized, modular\nrepresentations. We show that very shallow ResMixNets are capable of learning\neach of the two tasks well, attaining less than 2% and 1% test error on the\nMNIST Parity and the colorized Pentomino tasks respectively. Most importantly,\nthe ResMixNet models are extremely parameter efficient: generalizing better\nthan various non-modular CNNs that have over 10x the number of parameters.\nThese experimental results support the hypothesis that modularity is a robust\nprior for learning invariant relational reasoning. \n\n"}
{"id": "1806.07185", "contents": "Title: Mixed batches and symmetric discriminators for GAN training Abstract: Generative adversarial networks (GANs) are pow- erful generative models based\non providing feed- back to a generative network via a discriminator network.\nHowever, the discriminator usually as- sesses individual samples. This prevents\nthe dis- criminator from accessing global distributional statistics of\ngenerated samples, and often leads to mode dropping: the generator models only\npart of the target distribution. We propose to feed the discriminator with\nmixed batches of true and fake samples, and train it to predict the ratio of\ntrue samples in the batch. The latter score does not depend on the order of\nsamples in a batch. Rather than learning this invariance, we introduce a\ngeneric permutation-invariant discriminator ar- chitecture. This architecture\nis provably a uni- versal approximator of all symmetric functions.\nExperimentally, our approach reduces mode col- lapse in GANs on two synthetic\ndatasets, and obtains good results on the CIFAR10 and CelebA datasets, both\nqualitatively and quantitatively. \n\n"}
{"id": "1806.07297", "contents": "Title: Canonical Tensor Decomposition for Knowledge Base Completion Abstract: The problem of Knowledge Base Completion can be framed as a 3rd-order binary\ntensor completion problem. In this light, the Canonical Tensor Decomposition\n(CP) (Hitchcock, 1927) seems like a natural solution; however, current\nimplementations of CP on standard Knowledge Base Completion benchmarks are\nlagging behind their competitors. In this work, we attempt to understand the\nlimits of CP for knowledge base completion. First, we motivate and test a novel\nregularizer, based on tensor nuclear $p$-norms. Then, we present a\nreformulation of the problem that makes it invariant to arbitrary choices in\nthe inclusion of predicates or their reciprocals in the dataset. These two\nmethods combined allow us to beat the current state of the art on several\ndatasets with a CP decomposition, and obtain even better results using the more\nadvanced ComplEx model. \n\n"}
{"id": "1806.07572", "contents": "Title: Neural Tangent Kernel: Convergence and Generalization in Neural Networks Abstract: At initialization, artificial neural networks (ANNs) are equivalent to\nGaussian processes in the infinite-width limit, thus connecting them to kernel\nmethods. We prove that the evolution of an ANN during training can also be\ndescribed by a kernel: during gradient descent on the parameters of an ANN, the\nnetwork function $f_\\theta$ (which maps input vectors to output vectors)\nfollows the kernel gradient of the functional cost (which is convex, in\ncontrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel\n(NTK). This kernel is central to describe the generalization features of ANNs.\nWhile the NTK is random at initialization and varies during training, in the\ninfinite-width limit it converges to an explicit limiting kernel and it stays\nconstant during training. This makes it possible to study the training of ANNs\nin function space instead of parameter space. Convergence of the training can\nthen be related to the positive-definiteness of the limiting NTK. We prove the\npositive-definiteness of the limiting NTK when the data is supported on the\nsphere and the non-linearity is non-polynomial. We then focus on the setting of\nleast-squares regression and show that in the infinite-width limit, the network\nfunction $f_\\theta$ follows a linear differential equation during training. The\nconvergence is fastest along the largest kernel principal components of the\ninput data with respect to the NTK, hence suggesting a theoretical motivation\nfor early stopping. Finally we study the NTK numerically, observe its behavior\nfor wide networks, and compare it to the infinite-width limit. \n\n"}
{"id": "1806.07703", "contents": "Title: Multi-View Multi-Graph Embedding for Brain Network Clustering Analysis Abstract: Network analysis of human brain connectivity is critically important for\nunderstanding brain function and disease states. Embedding a brain network as a\nwhole graph instance into a meaningful low-dimensional representation can be\nused to investigate disease mechanisms and inform therapeutic interventions.\nMoreover, by exploiting information from multiple neuroimaging modalities or\nviews, we are able to obtain an embedding that is more useful than the\nembedding learned from an individual view. Therefore, multi-view multi-graph\nembedding becomes a crucial task. Currently, only a few studies have been\ndevoted to this topic, and most of them focus on the vector-based strategy\nwhich will cause structural information contained in the original graphs lost.\nAs a novel attempt to tackle this problem, we propose Multi-view Multi-graph\nEmbedding (M2E) by stacking multi-graphs into multiple partially-symmetric\ntensors and using tensor techniques to simultaneously leverage the dependencies\nand correlations among multi-view and multi-graph brain networks. Extensive\nexperiments on real HIV and bipolar disorder brain network datasets demonstrate\nthe superior performance of M2E on clustering brain networks by leveraging the\nmulti-view multi-graph interactions. \n\n"}
{"id": "1806.07751", "contents": "Title: Versatile Auxiliary Classifier with Generative Adversarial Network\n  (VAC+GAN), Multi Class Scenarios Abstract: Conditional generators learn the data distribution for each class in a\nmulti-class scenario and generate samples for a specific class given the right\ninput from the latent space. In this work, a method known as \"Versatile\nAuxiliary Classifier with Generative Adversarial Network\" for multi-class\nscenarios is presented. In this technique, the Generative Adversarial Networks\n(GAN)'s generator is turned into a conditional generator by placing a\nmulti-class classifier in parallel with the discriminator network and\nbackpropagate the classification error through the generator. This technique is\nversatile enough to be applied to any GAN implementation. The results on two\ndatabases and comparisons with other method are provided as well. \n\n"}
{"id": "1806.07772", "contents": "Title: Accurate and Diverse Sampling of Sequences based on a \"Best of Many\"\n  Sample Objective Abstract: For autonomous agents to successfully operate in the real world, anticipation\nof future events and states of their environment is a key competence. This\nproblem has been formalized as a sequence extrapolation problem, where a number\nof observations are used to predict the sequence into the future. Real-world\nscenarios demand a model of uncertainty of such predictions, as predictions\nbecome increasingly uncertain -- in particular on long time horizons. While\nimpressive results have been shown on point estimates, scenarios that induce\nmulti-modal distributions over future sequences remain challenging. Our work\naddresses these challenges in a Gaussian Latent Variable model for sequence\nprediction. Our core contribution is a \"Best of Many\" sample objective that\nleads to more accurate and more diverse predictions that better capture the\ntrue variations in real-world sequence data. Beyond our analysis of improved\nmodel fit, our models also empirically outperform prior work on three diverse\ntasks ranging from traffic scenes to weather data. \n\n"}
{"id": "1806.08541", "contents": "Title: Visualizing and Understanding Deep Neural Networks in CTR Prediction Abstract: Although deep learning techniques have been successfully applied to many\ntasks, interpreting deep neural network models is still a big challenge to us.\nRecently, many works have been done on visualizing and analyzing the mechanism\nof deep neural networks in the areas of image processing and natural language\nprocessing. In this paper, we present our approaches to visualize and\nunderstand deep neural networks for a very important commercial task--CTR\n(Click-through rate) prediction. We conduct experiments on the productive data\nfrom our online advertising system with daily varying distribution. To\nunderstand the mechanism and the performance of the model, we inspect the\nmodel's inner status at neuron level. Also, a probe approach is implemented to\nmeasure the layer-wise performance of the model. Moreover, to measure the\ninfluence from the input features, we calculate saliency scores based on the\nback-propagated gradients. Practical applications are also discussed, for\nexample, in understanding, monitoring, diagnosing and refining models and\nalgorithms. \n\n"}
{"id": "1806.09192", "contents": "Title: On The Differential Privacy of Thompson Sampling With Gaussian Prior Abstract: We show that Thompson Sampling with Gaussian Prior as detailed by Algorithm 2\nin (Agrawal & Goyal, 2013) is already differentially private. Theorem 1 show\nthat it enjoys a very competitive privacy loss of only $\\mathcal{O}(\\ln^2 T)$\nafter T rounds. Finally, Theorem 2 show that one can control the privacy loss\nto any desirable $\\epsilon$ level by appropriately increasing the variance of\nthe samples from the Gaussian posterior. And this increases the regret only by\na term of $\\mathcal{O}(\\frac{\\ln^2 T}{\\epsilon})$. This compares favorably to\nthe previous result for Thompson Sampling in the literature ((Mishra &\nThakurta, 2015)) which adds a term of $\\mathcal{O}(\\frac{K \\ln^3\nT}{\\epsilon^2})$ to the regret in order to achieve the same privacy level.\nFurthermore, our result use the basic Thompson Sampling with few modifications\nwhereas the result of (Mishra & Thakurta, 2015) required sophisticated\nconstructions. \n\n"}
{"id": "1806.09602", "contents": "Title: A Machine-learning framework for automatic reference-free quality\n  assessment in MRI Abstract: Magnetic resonance (MR) imaging offers a wide variety of imaging techniques.\nA large amount of data is created per examination which needs to be checked for\nsufficient quality in order to derive a meaningful diagnosis. This is a manual\nprocess and therefore time- and cost-intensive. Any imaging artifacts\noriginating from scanner hardware, signal processing or induced by the patient\nmay reduce the image quality and complicate the diagnosis or any image\npost-processing. Therefore, the assessment or the ensurance of sufficient image\nquality in an automated manner is of high interest. Usually no reference image\nis available or difficult to define. Therefore, classical reference-based\napproaches are not applicable. Model observers mimicking the human observers\n(HO) can assist in this task. Thus, we propose a new machine-learning-based\nreference-free MR image quality assessment framework which is trained on\nHO-derived labels to assess MR image quality immediately after each\nacquisition. We include the concept of active learning and present an efficient\nblinded reading platform to reduce the effort in the HO labeling procedure.\nDerived image features and the applied classifiers (support-vector-machine,\ndeep neural network) are investigated for a cohort of 250 patients. The MR\nimage quality assessment framework can achieve a high test accuracy of 93.7$\\%$\nfor estimating quality classes on a 5-point Likert-scale. The proposed MR image\nquality assessment framework is able to provide an accurate and efficient\nquality estimation which can be used as a prospective quality assurance\nincluding automatic acquisition adaptation or guided MR scanner operation,\nand/or as a retrospective quality assessment including support of diagnostic\ndecisions or quality control in cohort studies. \n\n"}
{"id": "1806.09614", "contents": "Title: Accuracy-based Curriculum Learning in Deep Reinforcement Learning Abstract: In this paper, we investigate a new form of automated curriculum learning\nbased on adaptive selection of accuracy requirements, called accuracy-based\ncurriculum learning. Using a reinforcement learning agent based on the Deep\nDeterministic Policy Gradient algorithm and addressing the Reacher environment,\nwe first show that an agent trained with various accuracy requirements sampled\nrandomly learns more efficiently than when asked to be very accurate at all\ntimes. Then we show that adaptive selection of accuracy requirements, based on\na local measure of competence progress, automatically generates a curriculum\nwhere difficulty progressively increases, resulting in a better learning\nefficiency than sampling randomly. \n\n"}
{"id": "1806.09919", "contents": "Title: Tangent-Space Regularization for Neural-Network Models of Dynamical\n  Systems Abstract: This work introduces the concept of tangent space regularization for\nneural-network models of dynamical systems. The tangent space to the dynamics\nfunction of many physical systems of interest in control applications exhibits\nuseful properties, e.g., smoothness, motivating regularization of the model\nJacobian along system trajectories using assumptions on the tangent space of\nthe dynamics. Without assumptions, large amounts of training data are required\nfor a neural network to learn the full non-linear dynamics without overfitting.\nWe compare different network architectures on one-step prediction and\nsimulation performance and investigate the propensity of different\narchitectures to learn models with correct input-output Jacobian. Furthermore,\nthe influence of $L_2$ weight regularization on the learned Jacobian eigenvalue\nspectrum, and hence system stability, is investigated. \n\n"}
{"id": "1806.10293", "contents": "Title: QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic\n  Manipulation Abstract: In this paper, we study the problem of learning vision-based dynamic\nmanipulation skills using a scalable reinforcement learning approach. We study\nthis problem in the context of grasping, a longstanding challenge in robotic\nmanipulation. In contrast to static learning behaviors that choose a grasp\npoint and then execute the desired grasp, our method enables closed-loop\nvision-based control, whereby the robot continuously updates its grasp strategy\nbased on the most recent observations to optimize long-horizon grasp success.\nTo that end, we introduce QT-Opt, a scalable self-supervised vision-based\nreinforcement learning framework that can leverage over 580k real-world grasp\nattempts to train a deep neural network Q-function with over 1.2M parameters to\nperform closed-loop, real-world grasping that generalizes to 96% grasp success\non unseen objects. Aside from attaining a very high success rate, our method\nexhibits behaviors that are quite distinct from more standard grasping systems:\nusing only RGB vision-based perception from an over-the-shoulder camera, our\nmethod automatically learns regrasping strategies, probes objects to find the\nmost effective grasps, learns to reposition objects and perform other\nnon-prehensile pre-grasp manipulations, and responds dynamically to\ndisturbances and perturbations. \n\n"}
{"id": "1806.10586", "contents": "Title: Approximability of Discriminators Implies Diversity in GANs Abstract: While Generative Adversarial Networks (GANs) have empirically produced\nimpressive results on learning complex real-world distributions, recent works\nhave shown that they suffer from lack of diversity or mode collapse. The\ntheoretical work of Arora et al. suggests a dilemma about GANs' statistical\nproperties: powerful discriminators cause overfitting, whereas weak\ndiscriminators cannot detect mode collapse.\n  By contrast, we show in this paper that GANs can in principle learn\ndistributions in Wasserstein distance (or KL-divergence in many cases) with\npolynomial sample complexity, if the discriminator class has strong\ndistinguishing power against the particular generator class (instead of against\nall possible generators). For various generator classes such as mixture of\nGaussians, exponential families, and invertible and injective neural networks\ngenerators, we design corresponding discriminators (which are often neural nets\nof specific architectures) such that the Integral Probability Metric (IPM)\ninduced by the discriminators can provably approximate the Wasserstein distance\nand/or KL-divergence. This implies that if the training is successful, then the\nlearned distribution is close to the true distribution in Wasserstein distance\nor KL divergence, and thus cannot drop modes. Our preliminary experiments show\nthat on synthetic datasets the test IPM is well correlated with KL divergence\nor the Wasserstein distance, indicating that the lack of diversity in GANs may\nbe caused by the sub-optimality in optimization instead of statistical\ninefficiency. \n\n"}
{"id": "1806.11248", "contents": "Title: XGBoost: Scalable GPU Accelerated Learning Abstract: We describe the multi-GPU gradient boosting algorithm implemented in the\nXGBoost library (https://github.com/dmlc/xgboost). Our algorithm allows fast,\nscalable training on multi-GPU systems with all of the features of the XGBoost\nlibrary. We employ data compression techniques to minimise the usage of scarce\nGPU memory while still allowing highly efficient implementation. Using our\nalgorithm we show that it is possible to process 115 million training instances\nin under three minutes on a publicly available cloud computing instance. The\nalgorithm is implemented using end-to-end GPU parallelism, with prediction,\ngradient calculation, feature quantisation, decision tree construction and\nevaluation phases all computed on device. \n\n"}
{"id": "1806.11302", "contents": "Title: Generate the corresponding Image from Text Description using Modified\n  GAN-CLS Algorithm Abstract: Synthesizing images or texts automatically is a useful research area in the\nartificial intelligence nowadays. Generative adversarial networks (GANs), which\nare proposed by Goodfellow in 2014, make this task to be done more efficiently\nby using deep neural networks. We consider generating corresponding images from\nan input text description using a GAN. In this paper, we analyze the GAN-CLS\nalgorithm, which is a kind of advanced method of GAN proposed by Scott Reed in\n2016. First, we find the problem with this algorithm through inference. Then we\ncorrect the GAN-CLS algorithm according to the inference by modifying the\nobjective function of the model. Finally, we do the experiments on the\nOxford-102 dataset and the CUB dataset. As a result, our modified algorithm can\ngenerate images which are more plausible than the GAN-CLS algorithm in some\ncases. Also, some of the generated images match the input texts better. \n\n"}
{"id": "1806.11332", "contents": "Title: Knowledge-Based Distant Regularization in Learning Probabilistic Models Abstract: Exploiting the appropriate inductive bias based on the knowledge of data is\nessential for achieving good performance in statistical machine learning. In\npractice, however, the domain knowledge of interest often provides information\non the relationship of data attributes only distantly, which hinders direct\nutilization of such domain knowledge in popular regularization methods. In this\npaper, we propose the knowledge-based distant regularization framework, in\nwhich we utilize the distant information encoded in a knowledge graph for\nregularization of probabilistic model estimation. In particular, we propose to\nimpose prior distributions on model parameters specified by knowledge graph\nembeddings. As an instance of the proposed framework, we present the factor\nanalysis model with the knowledge-based distant regularization. We show the\nresults of preliminary experiments on the improvement of the generalization\ncapability of such model. \n\n"}
{"id": "1807.00263", "contents": "Title: Accurate Uncertainties for Deep Learning Using Calibrated Regression Abstract: Methods for reasoning under uncertainty are a key building block of accurate\nand reliable machine learning systems. Bayesian methods provide a general\nframework to quantify uncertainty. However, because of model misspecification\nand the use of approximate inference, Bayesian uncertainty estimates are often\ninaccurate -- for example, a 90% credible interval may not contain the true\noutcome 90% of the time. Here, we propose a simple procedure for calibrating\nany regression algorithm; when applied to Bayesian and probabilistic models, it\nis guaranteed to produce calibrated uncertainty estimates given enough data.\nOur procedure is inspired by Platt scaling and extends previous work on\nclassification. We evaluate this approach on Bayesian linear regression,\nfeedforward, and recurrent neural networks, and find that it consistently\noutputs well-calibrated credible intervals while improving performance on time\nseries forecasting and model-based reinforcement learning tasks. \n\n"}
{"id": "1807.00482", "contents": "Title: Tap-based User Authentication for Smartwatches Abstract: This paper presents TapMeIn, an eyes-free, two-factor authentication method\nfor smartwatches. It allows users to tap a memorable melody (tap-password) of\ntheir choice anywhere on the touchscreen to unlock their watch. A user is\nverified based on the tap-password as well as her physiological and behavioral\ncharacteristics when tapping. Results from preliminary experiments with 41\nparticipants show that TapMeIn could achieve an accuracy of 98.7% with a False\nPositive Rate of only 0.98%. In addition, TapMeIn retains its performance in\ndifferent conditions such as sitting and walking. In terms of speed, TapMeIn\nhas an average authentication time of 2 seconds. A user study with the System\nUsability Scale (SUS) tool suggests that TapMeIn has a high usability score. \n\n"}
{"id": "1807.00558", "contents": "Title: Relational Constraints for Metric Learning on Relational Data Abstract: Most of metric learning approaches are dedicated to be applied on data\ndescribed by feature vectors, with some notable exceptions such as times\nseries, trees or graphs. The objective of this paper is to propose a metric\nlearning algorithm that specifically considers relational data. The proposed\napproach can take benefit from both the topological structure of the data and\nsupervised labels. For selecting relative constraints representing the\nrelational information, we introduce a link-strength function that measures the\nstrength of relationship links between entities by the side-information of\ntheir common parents. We show the performance of the proposed method with two\ndifferent classical metric learning algorithms, which are ITML (Information\nTheoretic Metric Learning) and LSML (Least Squares Metric Learning), and test\non several real-world datasets. Experimental results show that using relational\ninformation improves the quality of the learned metric. \n\n"}
{"id": "1807.00882", "contents": "Title: Deep convolutional encoder-decoder networks for uncertainty\n  quantification of dynamic multiphase flow in heterogeneous media Abstract: Surrogate strategies are used widely for uncertainty quantification of\ngroundwater models in order to improve computational efficiency. However, their\napplication to dynamic multiphase flow problems is hindered by the curse of\ndimensionality, the saturation discontinuity due to capillarity effects, and\nthe time-dependence of the multi-output responses. In this paper, we propose a\ndeep convolutional encoder-decoder neural network methodology to tackle these\nissues. The surrogate modeling task is transformed to an image-to-image\nregression strategy. This approach extracts high-level coarse features from the\nhigh-dimensional input permeability images using an encoder, and then refines\nthe coarse features to provide the output pressure/saturation images through a\ndecoder. A training strategy combining a regression loss and a segmentation\nloss is proposed in order to better approximate the discontinuous saturation\nfield. To characterize the high-dimensional time-dependent outputs of the\ndynamic system, time is treated as an additional input to the network that is\ntrained using pairs of input realizations and of the corresponding system\noutputs at a limited number of time instances. The proposed method is evaluated\nusing a geological carbon storage process-based multiphase flow model with a\n2500-dimensional stochastic permeability field. With a relatively small number\nof training data, the surrogate model is capable of accurately characterizing\nthe spatio-temporal evolution of the pressure and discontinuous CO2 saturation\nfields and can be used efficiently to compute the statistics of the system\nresponses. \n\n"}
{"id": "1807.01001", "contents": "Title: Modular Vehicle Control for Transferring Semantic Information Between\n  Weather Conditions Using GANs Abstract: Even though end-to-end supervised learning has shown promising results for\nsensorimotor control of self-driving cars, its performance is greatly affected\nby the weather conditions under which it was trained, showing poor\ngeneralization to unseen conditions. In this paper, we show how knowledge can\nbe transferred using semantic maps to new weather conditions without the need\nto obtain new ground truth data. To this end, we propose to divide the task of\nvehicle control into two independent modules: a control module which is only\ntrained on one weather condition for which labeled steering data is available,\nand a perception module which is used as an interface between new weather\nconditions and the fixed control module. To generate the semantic data needed\nto train the perception module, we propose to use a generative adversarial\nnetwork (GAN)-based model to retrieve the semantic information for the new\nconditions in an unsupervised manner. We introduce a master-servant\narchitecture, where the master model (semantic labels available) trains the\nservant model (semantic labels not available). We show that our proposed method\ntrained with ground truth data for a single weather condition is capable of\nachieving similar results on the task of steering angle prediction as an\nend-to-end model trained with ground truth data of 15 different weather\nconditions. \n\n"}
{"id": "1807.01134", "contents": "Title: Welfare and Distributional Impacts of Fair Classification Abstract: Current methodologies in machine learning analyze the effects of various\nstatistical parity notions of fairness primarily in light of their impacts on\npredictive accuracy and vendor utility loss. In this paper, we propose a new\nframework for interpreting the effects of fairness criteria by converting the\nconstrained loss minimization problem into a social welfare maximization\nproblem. This translation moves a classifier and its output into utility space\nwhere individuals, groups, and society at-large experience different welfare\nchanges due to classification assignments. Under this characterization,\npredictions and fairness constraints are seen as shaping societal welfare and\ndistribution and revealing individuals' implied welfare weights in\nsociety--weights that may then be interpreted through a fairness lens. The\nsocial welfare formulation of the fairness problem brings to the fore concerns\nof distributive justice that have always had a central albeit more implicit\nrole in standard algorithmic fairness approaches. \n\n"}
{"id": "1807.01395", "contents": "Title: Patient representation learning and interpretable evaluation using\n  clinical notes Abstract: We have three contributions in this work: 1. We explore the utility of a\nstacked denoising autoencoder and a paragraph vector model to learn\ntask-independent dense patient representations directly from clinical notes. To\nanalyze if these representations are transferable across tasks, we evaluate\nthem in multiple supervised setups to predict patient mortality, primary\ndiagnostic and procedural category, and gender. We compare their performance\nwith sparse representations obtained from a bag-of-words model. We observe that\nthe learned generalized representations significantly outperform the sparse\nrepresentations when we have few positive instances to learn from, and there is\nan absence of strong lexical features. 2. We compare the model performance of\nthe feature set constructed from a bag of words to that obtained from medical\nconcepts. In the latter case, concepts represent problems, treatments, and\ntests. We find that concept identification does not improve the classification\nperformance. 3. We propose novel techniques to facilitate model\ninterpretability. To understand and interpret the representations, we explore\nthe best encoded features within the patient representations obtained from the\nautoencoder model. Further, we calculate feature sensitivity across two\nnetworks to identify the most significant input features for different\nclassification tasks when we use these pretrained representations as the\nsupervised input. We successfully extract the most influential features for the\npipeline using this technique. \n\n"}
{"id": "1807.01759", "contents": "Title: Learning Personalized Representation for Inverse Problems in Medical\n  Imaging Using Deep Neural Network Abstract: Recently deep neural networks have been widely and successfully applied in\ncomputer vision tasks and attracted growing interests in medical imaging. One\nbarrier for the application of deep neural networks to medical imaging is the\nneed of large amounts of prior training pairs, which is not always feasible in\nclinical practice. In this work we propose a personalized representation\nlearning framework where no prior training pairs are needed, but only the\npatient's own prior images. The representation is expressed using a deep neural\nnetwork with the patient's prior images as network input. We then applied this\nnovel image representation to inverse problems in medical imaging in which the\noriginal inverse problem was formulated as a constraint optimization problem\nand solved using the alternating direction method of multipliers (ADMM)\nalgorithm. Anatomically guided brain positron emission tomography (PET) image\nreconstruction and image denoising were employed as examples to demonstrate the\neffectiveness of the proposed framework. Quantification results based on\nsimulation and real datasets show that the proposed personalized representation\nframework outperform other widely adopted methods. \n\n"}
{"id": "1807.01798", "contents": "Title: Regularizing Autoencoder-Based Matrix Completion Models via Manifold\n  Learning Abstract: Autoencoders are popular among neural-network-based matrix completion models\ndue to their ability to retrieve potential latent factors from the partially\nobserved matrices. Nevertheless, when training data is scarce their performance\nis significantly degraded due to overfitting. In this paper, we mit- igate\noverfitting with a data-dependent regularization technique that relies on the\nprinciples of multi-task learning. Specifically, we propose an\nautoencoder-based matrix completion model that performs prediction of the\nunknown matrix values as a main task, and manifold learning as an auxiliary\ntask. The latter acts as an inductive bias, leading to solutions that\ngeneralize better. The proposed model outperforms the existing\nautoencoder-based models designed for matrix completion, achieving high\nreconstruction accuracy in well-known datasets. \n\n"}
{"id": "1807.01883", "contents": "Title: A multiscale neural network based on hierarchical matrices Abstract: In this work we introduce a new multiscale artificial neural network based on\nthe structure of $\\mathcal{H}$-matrices. This network generalizes the latter to\nthe nonlinear case by introducing a local deep neural network at each spatial\nscale. Numerical results indicate that the network is able to efficiently\napproximate discrete nonlinear maps obtained from discretized nonlinear partial\ndifferential equations, such as those arising from nonlinear Schr\\\"odinger\nequations and the Kohn-Sham density functional theory. \n\n"}
{"id": "1807.02128", "contents": "Title: Adaptive Path-Integral Autoencoder: Representation Learning and Planning\n  for Dynamical Systems Abstract: We present a representation learning algorithm that learns a low-dimensional\nlatent dynamical system from high-dimensional \\textit{sequential} raw data,\ne.g., video. The framework builds upon recent advances in amortized inference\nmethods that use both an inference network and a refinement procedure to output\nsamples from a variational distribution given an observation sequence, and\ntakes advantage of the duality between control and inference to approximately\nsolve the intractable inference problem using the path integral control\napproach. The learned dynamical model can be used to predict and plan the\nfuture states; we also present the efficient planning method that exploits the\nlearned low-dimensional latent dynamics. Numerical experiments show that the\nproposed path-integral control based variational inference method leads to\ntighter lower bounds in statistical model learning of sequential data. The\nsupplementary video: https://youtu.be/xCp35crUoLQ \n\n"}
{"id": "1807.02901", "contents": "Title: Quantifying model form uncertainty in Reynolds-averaged turbulence\n  models with Bayesian deep neural networks Abstract: Data-driven methods for improving turbulence modeling in Reynolds-Averaged\nNavier-Stokes (RANS) simulations have gained significant interest in the\ncomputational fluid dynamics community. Modern machine learning algorithms have\nopened up a new area of black-box turbulence models allowing for the tuning of\nRANS simulations to increase their predictive accuracy. While several\ndata-driven turbulence models have been reported, the quantification of the\nuncertainties introduced has mostly been neglected. Uncertainty quantification\nfor such data-driven models is essential since their predictive capability\nrapidly declines as they are tested for flow physics that deviate from that in\nthe training data. In this work, we propose a novel data-driven framework that\nnot only improves RANS predictions but also provides probabilistic bounds for\nfluid quantities such as velocity and pressure. The uncertainties capture both\nmodel form uncertainty as well as epistemic uncertainty induced by the limited\ntraining data. An invariant Bayesian deep neural network is used to predict the\nanisotropic tensor component of the Reynolds stress. This model is trained\nusing Stein variational gradient decent algorithm. The computed uncertainty on\nthe Reynolds stress is propagated to the quantities of interest by vanilla\nMonte Carlo simulation. Results are presented for two test cases that differ\ngeometrically from the training flows at several different Reynolds numbers.\nThe prediction enhancement of the data-driven model is discussed as well as the\nassociated probabilistic bounds for flow properties of interest. Ultimately\nthis framework allows for a quantitative measurement of model confidence and\nuncertainty quantification for flows in which no high-fidelity observations or\nprior knowledge is available. \n\n"}
{"id": "1807.02910", "contents": "Title: Model Agnostic Supervised Local Explanations Abstract: Model interpretability is an increasingly important component of practical\nmachine learning. Some of the most common forms of interpretability systems are\nexample-based, local, and global explanations. One of the main challenges in\ninterpretability is designing explanation systems that can capture aspects of\neach of these explanation types, in order to develop a more thorough\nunderstanding of the model. We address this challenge in a novel model called\nMAPLE that uses local linear modeling techniques along with a dual\ninterpretation of random forests (both as a supervised neighborhood approach\nand as a feature selection method). MAPLE has two fundamental advantages over\nexisting interpretability systems. First, while it is effective as a black-box\nexplanation system, MAPLE itself is a highly accurate predictive model that\nprovides faithful self explanations, and thus sidesteps the typical\naccuracy-interpretability trade-off. Specifically, we demonstrate, on several\nUCI datasets, that MAPLE is at least as accurate as random forests and that it\nproduces more faithful local explanations than LIME, a popular interpretability\nsystem. Second, MAPLE provides both example-based and local explanations and\ncan detect global patterns, which allows it to diagnose limitations in its\nlocal explanations. \n\n"}
{"id": "1807.03039", "contents": "Title: Glow: Generative Flow with Invertible 1x1 Convolutions Abstract: Flow-based generative models (Dinh et al., 2014) are conceptually attractive\ndue to tractability of the exact log-likelihood, tractability of exact\nlatent-variable inference, and parallelizability of both training and\nsynthesis. In this paper we propose Glow, a simple type of generative flow\nusing an invertible 1x1 convolution. Using our method we demonstrate a\nsignificant improvement in log-likelihood on standard benchmarks. Perhaps most\nstrikingly, we demonstrate that a generative model optimized towards the plain\nlog-likelihood objective is capable of efficient realistic-looking synthesis\nand manipulation of large images. The code for our model is available at\nhttps://github.com/openai/glow \n\n"}
{"id": "1807.03210", "contents": "Title: Efficient Decentralized Deep Learning by Dynamic Model Averaging Abstract: We propose an efficient protocol for decentralized training of deep neural\nnetworks from distributed data sources. The proposed protocol allows to handle\ndifferent phases of model training equally well and to quickly adapt to concept\ndrifts. This leads to a reduction of communication by an order of magnitude\ncompared to periodically communicating state-of-the-art approaches. Moreover,\nwe derive a communication bound that scales well with the hardness of the\nserialized learning problem. The reduction in communication comes at almost no\ncost, as the predictive performance remains virtually unchanged. Indeed, the\nproposed protocol retains loss bounds of periodically averaging schemes. An\nextensive empirical evaluation validates major improvement of the trade-off\nbetween model performance and communication which could be beneficial for\nnumerous decentralized learning applications, such as autonomous driving, or\nvoice recognition and image classification on mobile phones. \n\n"}
{"id": "1807.03326", "contents": "Title: Adaptive Adversarial Attack on Scene Text Recognition Abstract: Recent studies have shown that state-of-the-art deep learning models are\nvulnerable to the inputs with small perturbations (adversarial examples). We\nobserve two critical obstacles in adversarial examples: (i) Strong adversarial\nattacks (e.g., C&W attack) require manually tuning hyper-parameters and take a\nlong time to construct an adversarial example, making it impractical to attack\nreal-time systems; (ii) Most of the studies focus on non-sequential tasks, such\nas image classification, yet only a few consider sequential tasks. In this\nwork, we speed up adversarial attacks, especially on sequential learning tasks.\nBy leveraging the uncertainty of each task, we directly learn the adaptive\nmulti-task weightings, without manually searching hyper-parameters. A unified\narchitecture is developed and evaluated for both non-sequential tasks and\nsequential ones. To validate the effectiveness, we take the scene text\nrecognition task as a case study. To our best knowledge, our proposed method is\nthe first attempt to adversarial attack for scene text recognition. Adaptive\nAttack achieves over 99.9\\% success rate with 3-6X speedup compared to\nstate-of-the-art adversarial attacks. \n\n"}
{"id": "1807.04065", "contents": "Title: Recurrent Neural Networks with Flexible Gates using Kernel Activation\n  Functions Abstract: Gated recurrent neural networks have achieved remarkable results in the\nanalysis of sequential data. Inside these networks, gates are used to control\nthe flow of information, allowing to model even very long-term dependencies in\nthe data. In this paper, we investigate whether the original gate equation (a\nlinear projection followed by an element-wise sigmoid) can be improved. In\nparticular, we design a more flexible architecture, with a small number of\nadaptable parameters, which is able to model a wider range of gating functions\nthan the classical one. To this end, we replace the sigmoid function in the\nstandard gate with a non-parametric formulation extending the recently proposed\nkernel activation function (KAF), with the addition of a residual\nskip-connection. A set of experiments on sequential variants of the MNIST\ndataset shows that the adoption of this novel gate allows to improve accuracy\nwith a negligible cost in terms of computational power and with a large\nspeed-up in the number of training iterations. \n\n"}
{"id": "1807.04271", "contents": "Title: A quantum-inspired classical algorithm for recommendation systems Abstract: We give a classical analogue to Kerenidis and Prakash's quantum\nrecommendation system, previously believed to be one of the strongest\ncandidates for provably exponential speedups in quantum machine learning. Our\nmain result is an algorithm that, given an $m \\times n$ matrix in a data\nstructure supporting certain $\\ell^2$-norm sampling operations, outputs an\n$\\ell^2$-norm sample from a rank-$k$ approximation of that matrix in time\n$O(\\text{poly}(k)\\log(mn))$, only polynomially slower than the quantum\nalgorithm. As a consequence, Kerenidis and Prakash's algorithm does not in fact\ngive an exponential speedup over classical algorithms. Further, under strong\ninput assumptions, the classical recommendation system resulting from our\nalgorithm produces recommendations exponentially faster than previous classical\nsystems, which run in time linear in $m$ and $n$.\n  The main insight of this work is the use of simple routines to manipulate\n$\\ell^2$-norm sampling distributions, which play the role of quantum\nsuperpositions in the classical setting. This correspondence indicates a\npotentially fruitful framework for formally comparing quantum machine learning\nalgorithms to classical machine learning algorithms. \n\n"}
{"id": "1807.04302", "contents": "Title: Structured Bayesian Gaussian process latent variable model: applications\n  to data-driven dimensionality reduction and high-dimensional inversion Abstract: We introduce a methodology for nonlinear inverse problems using a variational\nBayesian approach where the unknown quantity is a spatial field. A structured\nBayesian Gaussian process latent variable model is used both to construct a\nlow-dimensional generative model of the sample-based stochastic prior as well\nas a surrogate for the forward evaluation. Its Bayesian formulation captures\nepistemic uncertainty introduced by the limited number of input and output\nexamples, automatically selects an appropriate dimensionality for the learned\nlatent representation of the data, and rigorously propagates the uncertainty of\nthe data-driven dimensionality reduction of the stochastic space through the\nforward model surrogate. The structured Gaussian process model explicitly\nleverages spatial information for an informative generative prior to improve\nsample efficiency while achieving computational tractability through Kronecker\nproduct decompositions of the relevant kernel matrices. Importantly, the\nBayesian inversion is carried out by solving a variational optimization\nproblem, replacing traditional computationally-expensive Monte Carlo sampling.\nThe methodology is demonstrated on an elliptic PDE and is shown to return\nwell-calibrated posteriors and is tractable with latent spaces with over 100\ndimensions. \n\n"}
{"id": "1807.04585", "contents": "Title: Deep Learning for Imbalance Data Classification using Class Expert\n  Generative Adversarial Network Abstract: Without any specific way for imbalance data classification, artificial\nintelligence algorithm cannot recognize data from minority classes easily. In\ngeneral, modifying the existing algorithm by assuming that the training data is\nimbalanced, is the only way to handle imbalance data. However, for a normal\ndata handling, this way mostly produces a deficient result. In this research,\nwe propose a class expert generative adversarial network (CE-GAN) as the\nsolution for imbalance data classification. CE-GAN is a modification in deep\nlearning algorithm architecture that does not have an assumption that the\ntraining data is imbalance data. Moreover, CE-GAN is designed to identify more\ndetail about the character of each class before classification step. CE-GAN has\nbeen proved in this research to give a good performance for imbalance data\nclassification. \n\n"}
{"id": "1807.06481", "contents": "Title: Dynamic Sampling from Graphical Models Abstract: In this paper, we study the problem of sampling from a graphical model when\nthe model itself is changing dynamically with time. This problem derives its\ninterest from a variety of inference, learning, and sampling settings in\nmachine learning, computer vision, statistical physics, and theoretical\ncomputer science. While the problem of sampling from a static graphical model\nhas received considerable attention, theoretical works for its dynamic variants\nhave been largely lacking. The main contribution of this paper is an algorithm\nthat can sample dynamically from a broad class of graphical models over\ndiscrete random variables. Our algorithm is parallel and Las Vegas: it knows\nwhen to stop and it outputs samples from the exact distribution. We also\nprovide sufficient conditions under which this algorithm runs in time\nproportional to the size of the update, on general graphical models as well as\nwell-studied specific spin systems. In particular we obtain, for the Ising\nmodel (ferromagnetic or anti-ferromagnetic) and for the hardcore model the\nfirst dynamic sampling algorithms that can handle both edge and vertex updates\n(addition, deletion, change of functions), both efficient within regimes that\nare close to the respective uniqueness regimes, beyond which, even for the\nstatic and approximate sampling, no local algorithms were known or the problem\nitself is intractable. Our dynamic sampling algorithm relies on a local\nresampling algorithm and a new \"equilibrium\" property that is shown to be\nsatisfied by our algorithm at each step, and enables us to prove its\ncorrectness. This equilibrium property is robust enough to guarantee the\ncorrectness of our algorithm, helps us improve bounds on fast convergence on\nspecific models, and should be of independent interest. \n\n"}
{"id": "1807.07215", "contents": "Title: Machine Learning Classifiers Do Not Improve the Prediction of Academic\n  Risk: Evidence from Australia Abstract: Machine learning methods tend to outperform traditional statistical models at\nprediction. In the prediction of academic achievement, ML models have not shown\nsubstantial improvement over logistic regression. So far, these results have\nalmost entirely focused on college achievement, due to the availability of\nadministrative datasets, and have contained relatively small sample sizes by ML\nstandards. In this article we apply popular machine learning models to a large\ndataset ($n=1.2$ million) containing primary and middle school performance on a\nstandardized test given annually to Australian students. We show that machine\nlearning models do not outperform logistic regression for detecting students\nwho will perform in the `below standard' band of achievement upon sitting their\nnext test, even in a large-$n$ setting. \n\n"}
{"id": "1807.07282", "contents": "Title: Anomaly Detection for Water Treatment System based on Neural Network\n  with Automatic Architecture Optimization Abstract: We continue to develop our neural network (NN) based forecasting approach to\nanomaly detection (AD) using the Secure Water Treatment (SWaT) industrial\ncontrol system (ICS) testbed dataset. We propose genetic algorithms (GA) to\nfind the best NN architecture for a given dataset, using the NAB metric to\nassess the quality of different architectures. The drawbacks of the F1-metric\nare analyzed. Several techniques are proposed to improve the quality of AD:\nexponentially weighted smoothing, mean p-powered error measure, individual\nerror weight for each variable, disjoint prediction windows. Based on the\ntechniques used, an approach to anomaly interpretation is introduced. \n\n"}
{"id": "1807.07543", "contents": "Title: Understanding and Improving Interpolation in Autoencoders via an\n  Adversarial Regularizer Abstract: Autoencoders provide a powerful framework for learning compressed\nrepresentations by encoding all of the information needed to reconstruct a data\npoint in a latent code. In some cases, autoencoders can \"interpolate\": By\ndecoding the convex combination of the latent codes for two datapoints, the\nautoencoder can produce an output which semantically mixes characteristics from\nthe datapoints. In this paper, we propose a regularization procedure which\nencourages interpolated outputs to appear more realistic by fooling a critic\nnetwork which has been trained to recover the mixing coefficient from\ninterpolated data. We then develop a simple benchmark task where we can\nquantitatively measure the extent to which various autoencoders can interpolate\nand show that our regularizer dramatically improves interpolation in this\nsetting. We also demonstrate empirically that our regularizer produces latent\ncodes which are more effective on downstream tasks, suggesting a possible link\nbetween interpolation abilities and learning useful representations. \n\n"}
{"id": "1807.08091", "contents": "Title: Streaming Methods for Restricted Strongly Convex Functions with\n  Applications to Prototype Selection Abstract: In this paper, we show that if the optimization function is\nrestricted-strongly-convex (RSC) and restricted-smooth (RSM) -- a rich subclass\nof weakly submodular functions -- then a streaming algorithm with constant\nfactor approximation guarantee is possible. More generally, our results are\napplicable to any monotone weakly submodular function with submodularity ratio\nbounded from above. This (positive) result which provides a sufficient\ncondition for having a constant factor streaming guarantee for weakly\nsubmodular functions may be of special interest given the recent negative\nresult (Elenberg et al., 2017) for the general class of weakly submodular\nfunctions. We apply our streaming algorithms for creating compact synopsis of\nlarge complex datasets, by selecting $m$ representative elements, by optimizing\na suitable RSC and RSM objective function. Above results hold even with\nadditional constraints such as learning non-negative weights, for\ninterpretability, for each selected element indicative of its importance. We\nempirically evaluate our algorithms on two real datasets: MNIST- a handwritten\ndigits dataset and Letters- a UCI dataset containing the alphabet written in\ndifferent fonts and styles. We observe that our algorithms are orders of\nmagnitude faster than the state-of-the-art streaming algorithm for weakly\nsubmodular functions and with our main algorithm still providing equally good\nsolutions in practice. \n\n"}
{"id": "1807.08518", "contents": "Title: Implementing Neural Turing Machines Abstract: Neural Turing Machines (NTMs) are an instance of Memory Augmented Neural\nNetworks, a new class of recurrent neural networks which decouple computation\nfrom memory by introducing an external memory unit. NTMs have demonstrated\nsuperior performance over Long Short-Term Memory Cells in several sequence\nlearning tasks. A number of open source implementations of NTMs exist but are\nunstable during training and/or fail to replicate the reported performance of\nNTMs. This paper presents the details of our successful implementation of a\nNTM. Our implementation learns to solve three sequential learning tasks from\nthe original NTM paper. We find that the choice of memory contents\ninitialization scheme is crucial in successfully implementing a NTM. Networks\nwith memory contents initialized to small constant values converge on average 2\ntimes faster than the next best memory contents initialization scheme. \n\n"}
{"id": "1807.08792", "contents": "Title: PCNNA: A Photonic Convolutional Neural Network Accelerator Abstract: Convolutional Neural Networks (CNN) have been the centerpiece of many\napplications including but not limited to computer vision, speech processing,\nand Natural Language Processing (NLP). However, the computationally expensive\nconvolution operations impose many challenges to the performance and\nscalability of CNNs. In parallel, photonic systems, which are traditionally\nemployed for data communication, have enjoyed recent popularity for data\nprocessing due to their high bandwidth, low power consumption, and\nreconfigurability. Here we propose a Photonic Convolutional Neural Network\nAccelerator (PCNNA) as a proof of concept design to speedup the convolution\noperation for CNNs. Our design is based on the recently introduced silicon\nphotonic microring weight banks, which use broadcast-and-weight protocol to\nperform Multiply And Accumulate (MAC) operation and move data through layers of\na neural network. Here, we aim to exploit the synergy between the inherent\nparallelism of photonics in the form of Wavelength Division Multiplexing (WDM)\nand sparsity of connections between input feature maps and kernels in CNNs.\nWhile our full system design offers up to more than 3 orders of magnitude\nspeedup in execution time, its optical core potentially offers more than 5\norder of magnitude speedup compared to state-of-the-art electronic\ncounterparts. \n\n"}
{"id": "1807.08844", "contents": "Title: Lesion segmentation using U-Net network Abstract: This paper explains the method used in the segmentation challenge (Task 1) in\nthe International Skin Imaging Collaboration's (ISIC) Skin Lesion Analysis\nTowards Melanoma Detection challenge held in 2018. We have trained a U-Net\nnetwork to perform the segmentation. The key elements for the training were\nfirst to adjust the loss function to incorporate unbalanced proportion of\nbackground and second to perform post-processing operation to adjust the\ncontour of the prediction. \n\n"}
{"id": "1807.08887", "contents": "Title: Supporting Very Large Models using Automatic Dataflow Graph Partitioning Abstract: This paper presents Tofu, a system that partitions very large DNN models\nacross multiple GPU devices to reduce per-GPU memory footprint. Tofu is\ndesigned to partition a dataflow graph of fine-grained tensor operators in\norder to work transparently with a general-purpose deep learning platform like\nMXNet. In order to automatically partition each operator, we propose to\ndescribe the semantics of an operator in a simple language which represents\ntensors as lambda functions mapping from tensor coordinates to values. To\noptimally partition different operators in a dataflow graph, Tofu uses a\nrecursive search algorithm that minimizes the total communication cost. Our\nexperiments on an 8-GPU machine show that Tofu enables the training of very\nlarge CNN and RNN models. It also achieves 25% - 400% speedup over alternative\napproaches to train very large models. \n\n"}
{"id": "1807.09289", "contents": "Title: Noise Contrastive Priors for Functional Uncertainty Abstract: Obtaining reliable uncertainty estimates of neural network predictions is a\nlong standing challenge. Bayesian neural networks have been proposed as a\nsolution, but it remains open how to specify their prior. In particular, the\ncommon practice of an independent normal prior in weight space imposes\nrelatively weak constraints on the function posterior, allowing it to\ngeneralize in unforeseen ways on inputs outside of the training distribution.\nWe propose noise contrastive priors (NCPs) to obtain reliable uncertainty\nestimates. The key idea is to train the model to output high uncertainty for\ndata points outside of the training distribution. NCPs do so using an input\nprior, which adds noise to the inputs of the current mini batch, and an output\nprior, which is a wide distribution given these inputs. NCPs are compatible\nwith any model that can output uncertainty estimates, are easy to scale, and\nyield reliable uncertainty estimates throughout training. Empirically, we show\nthat NCPs prevent overfitting outside of the training distribution and result\nin uncertainty estimates that are useful for active learning. We demonstrate\nthe scalability of our method on the flight delays data set, where we\nsignificantly improve upon previously published results. \n\n"}
{"id": "1807.09571", "contents": "Title: Deep Learning Detection Networks in MIMO Decode-Forward Relay Channels Abstract: In this paper, we consider signal detection algorithms in a multiple-input\nmultiple-output (MIMO) decode-forward (DF) relay channel with one source, one\nrelay, and one destination. The existing suboptimal near maximum likelihood\n(NML) detector and the NML with two-level pair-wise error probability\n(NMLw2PEP) detector achieve excellent performance with instantaneous channel\nstate information (CSI) of the source-relay (SR) link and with statistical CSI\nof the SR link, respectively. However, the NML detectors require an\nexponentially increasing complexity as the number of transmit antennas\nincreases. Using deep learning algorithms, NML-based detection networks\n(NMLDNs) are proposed with and without the CSI of the SR link at the\ndestination. The NMLDNs detect signals in changing channels after a single\ntraining using a large number of randomly distributed channels. The detection\nnetworks require much lower detection complexity than the exhaustive search NML\ndetectors while exhibiting good performance. To evaluate the performance, we\nintroduce semidefinite relaxation detectors with polynomial complexity based on\nthe NML detectors. Additionally, new linear detectors based on the zero\ngradient of the NML metrics are proposed. Applying various detection algorithms\nat the relay (DetR) and detection algorithms at the destination (DetD), we\npresent some DetR-DetD methods in MIMO DF relay channels. An appropriate\nDetR-DetD method can be employed according to the required error probability\nand detection complexity. The complexity analysis and simulation results\nvalidate the arguments of this paper. \n\n"}
{"id": "1807.09596", "contents": "Title: Contextual Stochastic Block Models Abstract: We provide the first information theoretic tight analysis for inference of\nlatent community structure given a sparse graph along with high dimensional\nnode covariates, correlated with the same latent communities. Our work bridges\nrecent theoretical breakthroughs in the detection of latent community structure\nwithout nodes covariates and a large body of empirical work using diverse\nheuristics for combining node covariates with graphs for inference. The\ntightness of our analysis implies in particular, the information theoretical\nnecessity of combining the different sources of information. Our analysis holds\nfor networks of large degrees as well as for a Gaussian version of the model. \n\n"}
{"id": "1807.09751", "contents": "Title: Multi-Perspective Neural Architecture for Recommendation System Abstract: Currently, there starts a research trend to leverage neural architecture for\nrecommendation systems. Though several deep recommender models are proposed,\nmost methods are too simple to characterize users' complex preference. In this\npaper, for a fine-grain analysis, users' ratings are explained from multiple\nperspectives, based on which, we propose our neural architecture. Specifically,\nour model employs several sequential stages to encode the user and item into\nhidden representations. In one stage, the user and item are represented from\nmultiple perspectives and in each perspective, the representations of user and\nitem put attentions to each other. Last, we metric the output representations\nof final stage to approach the users' rating. Extensive experiments demonstrate\nthat our method achieves substantial improvements against baselines. \n\n"}
{"id": "1807.09834", "contents": "Title: Applying Domain Randomization to Synthetic Data for Object Category\n  Detection Abstract: Recent advances in deep learning-based object detection techniques have\nrevolutionized their applicability in several fields. However, since these\nmethods rely on unwieldy and large amounts of data, a common practice is to\ndownload models pre-trained on standard datasets and fine-tune them for\nspecific application domains with a small set of domain relevant images. In\nthis work, we show that using synthetic datasets that are not necessarily\nphoto-realistic can be a better alternative to simply fine-tune pre-trained\nnetworks. Specifically, our results show an impressive 25% improvement in the\nmAP metric over a fine-tuning baseline when only about 200 labelled images are\navailable to train. Finally, an ablation study of our results is presented to\ndelineate the individual contribution of different components in the\nrandomization pipeline. \n\n"}
{"id": "1807.09844", "contents": "Title: Modular Mechanistic Networks: On Bridging Mechanistic and\n  Phenomenological Models with Deep Neural Networks in Natural Language\n  Processing Abstract: Natural language processing (NLP) can be done using either top-down (theory\ndriven) and bottom-up (data driven) approaches, which we call mechanistic and\nphenomenological respectively. The approaches are frequently considered to\nstand in opposition to each other. Examining some recent approaches in deep\nlearning we argue that deep neural networks incorporate both perspectives and,\nfurthermore, that leveraging this aspect of deep learning may help in solving\ncomplex problems within language technology, such as modelling language and\nperception in the domain of spatial cognition. \n\n"}
{"id": "1807.09875", "contents": "Title: Differentiable Perturb-and-Parse: Semi-Supervised Parsing with a\n  Structured Variational Autoencoder Abstract: Human annotation for syntactic parsing is expensive, and large resources are\navailable only for a fraction of languages. A question we ask is whether one\ncan leverage abundant unlabeled texts to improve syntactic parsers, beyond just\nusing the texts to obtain more generalisable lexical features (i.e. beyond word\nembeddings). To this end, we propose a novel latent-variable generative model\nfor semi-supervised syntactic dependency parsing. As exact inference is\nintractable, we introduce a differentiable relaxation to obtain approximate\nsamples and compute gradients with respect to the parser parameters. Our method\n(Differentiable Perturb-and-Parse) relies on differentiable dynamic programming\nover stochastically perturbed edge scores. We demonstrate effectiveness of our\napproach with experiments on English, French and Swedish. \n\n"}
{"id": "1807.10251", "contents": "Title: Aggregated Learning: A Deep Learning Framework Based on\n  Information-Bottleneck Vector Quantization Abstract: Based on the notion of information bottleneck (IB), we formulate a\nquantization problem called \"IB quantization\". We show that IB quantization is\nequivalent to learning based on the IB principle. Under this equivalence, the\nstandard neural network models can be viewed as scalar (single sample) IB\nquantizers. It is known, from conventional rate-distortion theory, that scalar\nquantizers are inferior to vector (multi-sample) quantizers. Such a deficiency\nthen inspires us to develop a novel learning framework, AgrLearn, that\ncorresponds to vector IB quantizers for learning with neural networks. Unlike\nstandard networks, AgrLearn simultaneously optimizes against multiple data\nsamples. We experimentally verify that AgrLearn can result in significant\nimprovements when applied to several current deep learning architectures for\nimage recognition and text classification. We also empirically show that\nAgrLearn can reduce up to 80% of the training samples needed for ResNet\ntraining. \n\n"}
{"id": "1807.10752", "contents": "Title: Dictionary Learning in Fourier Transform Scanning Tunneling Spectroscopy Abstract: Modern high-resolution microscopes, such as the scanning tunneling\nmicroscope, are commonly used to study specimens that have dense and aperiodic\nspatial structure. Extracting meaningful information from images obtained from\nsuch microscopes remains a formidable challenge. Fourier analysis is commonly\nused to analyze the underlying structure of fundamental motifs present in an\nimage. However, the Fourier transform fundamentally suffers from severe phase\nnoise when applied to aperiodic images. Here, we report the development of a\nnew algorithm based on nonconvex optimization, applicable to any microscopy\nmodality, that directly uncovers the fundamental motifs present in a real-space\nimage. Apart from being quantitatively superior to traditional Fourier\nanalysis, we show that this novel algorithm also uncovers phase sensitive\ninformation about the underlying motif structure. We demonstrate its usefulness\nby studying scanning tunneling microscopy images of a Co-doped iron arsenide\nsuperconductor and prove that the application of the algorithm allows for the\ncomplete recovery of quasiparticle interference in this material. Our phase\nsensitive quasiparticle interference imaging results indicate that the pairing\nsymmetry in optimally doped NaFeAs is consistent with a sign-changing s+- order\nparameter. \n\n"}
{"id": "1807.10875", "contents": "Title: TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing Abstract: Machine learning models are notoriously difficult to interpret and debug.\nThis is particularly true of neural networks. In this work, we introduce\nautomated software testing techniques for neural networks that are well-suited\nto discovering errors which occur only for rare inputs. Specifically, we\ndevelop coverage-guided fuzzing (CGF) methods for neural networks. In CGF,\nrandom mutations of inputs to a neural network are guided by a coverage metric\ntoward the goal of satisfying user-specified constraints. We describe how fast\napproximate nearest neighbor algorithms can provide this coverage metric. We\nthen discuss the application of CGF to the following goals: finding numerical\nerrors in trained neural networks, generating disagreements between neural\nnetworks and quantized versions of those networks, and surfacing undesirable\nbehavior in character level language models. Finally, we release an open source\nlibrary called TensorFuzz that implements the described techniques. \n\n"}
{"id": "1807.11694", "contents": "Title: Spectrum concentration in deep residual learning: a free probability\n  approach Abstract: We revisit the initialization of deep residual networks (ResNets) by\nintroducing a novel analytical tool in free probability to the community of\ndeep learning. This tool deals with non-Hermitian random matrices, rather than\ntheir conventional Hermitian counterparts in the literature. As a consequence,\nthis new tool enables us to evaluate the singular value spectrum of the\ninput-output Jacobian of a fully-connected deep ResNet for both linear and\nnonlinear cases. With the powerful tool of free probability, we conduct an\nasymptotic analysis of the spectrum on the single-layer case, and then extend\nthis analysis to the multi-layer case of an arbitrary number of layers. In\nparticular, we propose to rescale the classical random initialization by the\nnumber of residual units, so that the spectrum has the order of $O(1)$, when\ncompared with the large width and depth of the network. We empirically\ndemonstrate that the proposed initialization scheme learns at a speed of orders\nof magnitudes faster than the classical ones, and thus attests a strong\npractical relevance of this investigation. \n\n"}
{"id": "1808.00191", "contents": "Title: Graph R-CNN for Scene Graph Generation Abstract: We propose a novel scene graph generation model called Graph R-CNN, that is\nboth effective and efficient at detecting objects and their relations in\nimages. Our model contains a Relation Proposal Network (RePN) that efficiently\ndeals with the quadratic number of potential relations between objects in an\nimage. We also propose an attentional Graph Convolutional Network (aGCN) that\neffectively captures contextual information between objects and relations.\nFinally, we introduce a new evaluation metric that is more holistic and\nrealistic than existing metrics. We report state-of-the-art performance on\nscene graph generation as evaluated using both existing and our proposed\nmetrics. \n\n"}
{"id": "1808.00200", "contents": "Title: Anomaly Detection via Minimum Likelihood Generative Adversarial Networks Abstract: Anomaly detection aims to detect abnormal events by a model of normality. It\nplays an important role in many domains such as network intrusion detection,\ncriminal activity identity and so on. With the rapidly growing size of\naccessible training data and high computation capacities, deep learning based\nanomaly detection has become more and more popular. In this paper, a new\ndomain-based anomaly detection method based on generative adversarial networks\n(GAN) is proposed. Minimum likelihood regularization is proposed to make the\ngenerator produce more anomalies and prevent it from converging to normal data\ndistribution. Proper ensemble of anomaly scores is shown to improve the\nstability of discriminator effectively. The proposed method has achieved\nsignificant improvement than other anomaly detection methods on Cifar10 and UCI\ndatasets. \n\n"}
{"id": "1808.00408", "contents": "Title: Geometry of energy landscapes and the optimizability of deep neural\n  networks Abstract: Deep neural networks are workhorse models in machine learning with multiple\nlayers of non-linear functions composed in series. Their loss function is\nhighly non-convex, yet empirically even gradient descent minimisation is\nsufficient to arrive at accurate and predictive models. It is hitherto unknown\nwhy are deep neural networks easily optimizable. We analyze the energy\nlandscape of a spin glass model of deep neural networks using random matrix\ntheory and algebraic geometry. We analytically show that the multilayered\nstructure holds the key to optimizability: Fixing the number of parameters and\nincreasing network depth, the number of stationary points in the loss function\ndecreases, minima become more clustered in parameter space, and the tradeoff\nbetween the depth and width of minima becomes less severe. Our analytical\nresults are numerically verified through comparison with neural networks\ntrained on a set of classical benchmark datasets. Our model uncovers generic\ndesign principles of machine learning models. \n\n"}
{"id": "1808.00601", "contents": "Title: Classification of Building Information Model (BIM) Structures with Deep\n  Learning Abstract: In this work we study an application of machine learning to the construction\nindustry and we use classical and modern machine learning methods to categorize\nimages of building designs into three classes: Apartment building, Industrial\nbuilding or Other. No real images are used, but only images extracted from\nBuilding Information Model (BIM) software, as these are used by the\nconstruction industry to store building designs. For this task, we compared\nfour different methods: the first is based on classical machine learning, where\nHistogram of Oriented Gradients (HOG) was used for feature extraction and a\nSupport Vector Machine (SVM) for classification; the other three methods are\nbased on deep learning, covering common pre-trained networks as well as ones\ndesigned from scratch. To validate the accuracy of the models, a database of\n240 images was used. The accuracy achieved is 57% for the HOG + SVM model, and\nabove 89% for the neural networks. \n\n"}
{"id": "1808.00911", "contents": "Title: Detector monitoring with artificial neural networks at the CMS\n  experiment at the CERN Large Hadron Collider Abstract: Reliable data quality monitoring is a key asset in delivering collision data\nsuitable for physics analysis in any modern large-scale High Energy Physics\nexperiment. This paper focuses on the use of artificial neural networks for\nsupervised and semi-supervised problems related to the identification of\nanomalies in the data collected by the CMS muon detectors. We use deep neural\nnetworks to analyze LHC collision data, represented as images organized\ngeographically. We train a classifier capable of detecting the known anomalous\nbehaviors with unprecedented efficiency and explore the usage of convolutional\nautoencoders to extend anomaly detection capabilities to unforeseen failure\nmodes. A generalization of this strategy could pave the way to the automation\nof the data quality assessment process for present and future high-energy\nphysics experiments. \n\n"}
{"id": "1808.01357", "contents": "Title: A recurrent multi-scale approach to RBG-D Object Recognition Abstract: Technological development aims to produce generations of increasingly\nefficient robots able to perform complex tasks. This requires considerable\nefforts, from the scientific community, to find new algorithms that solve\ncomputer vision problems, such as object recognition. The diffusion of RGB-D\ncameras directed the study towards the research of new architectures able to\nexploit the RGB and Depth information. The project that is developed in this\nthesis concerns the realization of a new end-to-end architecture for the\nrecognition of RGB-D objects called RCFusion. Our method generates compact and\nhighly discriminative multi-modal features by combining complementary RGB and\ndepth information representing different levels of abstraction. We evaluate our\nmethod on standard object recognition datasets, RGB-D Object Dataset and\nJHUIT-50. The experiments performed show that our method outperforms the\nexisting approaches and establishes new state-of-the-art results for both\ndatasets. \n\n"}
{"id": "1808.02169", "contents": "Title: Fast Variance Reduction Method with Stochastic Batch Size Abstract: In this paper we study a family of variance reduction methods with randomized\nbatch size---at each step, the algorithm first randomly chooses the batch size\nand then selects a batch of samples to conduct a variance-reduced stochastic\nupdate. We give the linear convergence rate for this framework for composite\nfunctions, and show that the optimal strategy to achieve the optimal\nconvergence rate per data access is to always choose batch size of 1, which is\nequivalent to the SAGA algorithm. However, due to the presence of cache/disk IO\neffect in computer architecture, the number of data access cannot reflect the\nrunning time because of 1) random memory access is much slower than sequential\naccess, 2) when data is too big to fit into memory, disk seeking takes even\nlonger time. After taking these into account, choosing batch size of $1$ is no\nlonger optimal, so we propose a new algorithm called SAGA++ and show how to\ncalculate the optimal average batch size theoretically. Our algorithm\noutperforms SAGA and other existing batched and stochastic solvers on real\ndatasets. In addition, we also conduct a precise analysis to compare different\nupdate rules for variance reduction methods, showing that SAGA++ converges\nfaster than SVRG in theory. \n\n"}
{"id": "1808.03233", "contents": "Title: OBOE: Collaborative Filtering for AutoML Model Selection Abstract: Algorithm selection and hyperparameter tuning remain two of the most\nchallenging tasks in machine learning. Automated machine learning (AutoML)\nseeks to automate these tasks to enable widespread use of machine learning by\nnon-experts. This paper introduces OBOE, a collaborative filtering method for\ntime-constrained model selection and hyperparameter tuning. OBOE forms a matrix\nof the cross-validated errors of a large number of supervised learning models\n(algorithms together with hyperparameters) on a large number of datasets, and\nfits a low rank model to learn the low-dimensional feature vectors for the\nmodels and datasets that best predict the cross-validated errors. To find\npromising models for a new dataset, OBOE runs a set of fast but informative\nalgorithms on the new dataset and uses their cross-validated errors to infer\nthe feature vector for the new dataset. OBOE can find good models under\nconstraints on the number of models fit or the total time budget. To this end,\nthis paper develops a new heuristic for active learning in time-constrained\nmatrix completion based on optimal experiment design. Our experiments\ndemonstrate that OBOE delivers state-of-the-art performance faster than\ncompeting approaches on a test bed of supervised learning problems. Moreover,\nthe success of the bilinear model used by OBOE suggests that AutoML may be\nsimpler than was previously understood. \n\n"}
{"id": "1808.03398", "contents": "Title: Learning Parameters and Constitutive Relationships with Physics Informed\n  Deep Neural Networks Abstract: We present a physics informed deep neural network (DNN) method for estimating\nparameters and unknown physics (constitutive relationships) in partial\ndifferential equation (PDE) models. We use PDEs in addition to measurements to\ntrain DNNs to approximate unknown parameters and constitutive relationships as\nwell as states. The proposed approach increases the accuracy of DNN\napproximations of partially known functions when a limited number of\nmeasurements is available and allows for training DNNs when no direct\nmeasurements of the functions of interest are available. We employ physics\ninformed DNNs to estimate the unknown space-dependent diffusion coefficient in\na linear diffusion equation and an unknown constitutive relationship in a\nnon-linear diffusion equation. For the parameter estimation problem, we assume\nthat partial measurements of the coefficient and states are available and\ndemonstrate that under these conditions, the proposed method is more accurate\nthan state-of-the-art methods. For the non-linear diffusion PDE model with a\nfully unknown constitutive relationship (i.e., no measurements of constitutive\nrelationship are available), the physics informed DNN method can accurately\nestimate the non-linear constitutive relationship based on state measurements\nonly. Finally, we demonstrate that the proposed method remains accurate in the\npresence of measurement noise. \n\n"}
{"id": "1808.04685", "contents": "Title: Learning ReLU Networks on Linearly Separable Data: Algorithm,\n  Optimality, and Generalization Abstract: Neural networks with REctified Linear Unit (ReLU) activation functions\n(a.k.a. ReLU networks) have achieved great empirical success in various\ndomains. Nonetheless, existing results for learning ReLU networks either pose\nassumptions on the underlying data distribution being e.g. Gaussian, or require\nthe network size and/or training size to be sufficiently large. In this\ncontext, the problem of learning a two-layer ReLU network is approached in a\nbinary classification setting, where the data are linearly separable and a\nhinge loss criterion is adopted. Leveraging the power of random noise\nperturbation, this paper presents a novel stochastic gradient descent (SGD)\nalgorithm, which can \\emph{provably} train any single-hidden-layer ReLU network\nto attain global optimality, despite the presence of infinitely many bad local\nminima, maxima, and saddle points in general. This result is the first of its\nkind, requiring no assumptions on the data distribution, training/network size,\nor initialization. Convergence of the resultant iterative algorithm to a global\nminimum is analyzed by establishing both an upper bound and a lower bound on\nthe number of non-zero updates to be performed. Moreover, generalization\nguarantees are developed for ReLU networks trained with the novel SGD\nleveraging classic compression bounds. These guarantees highlight a key\ndifference (at least in the worst case) between reliably learning a ReLU\nnetwork as well as a leaky ReLU network in terms of sample complexity.\nNumerical tests using both synthetic data and real images validate the\neffectiveness of the algorithm and the practical merits of the theory. \n\n"}
{"id": "1808.05054", "contents": "Title: Shedding Light on Black Box Machine Learning Algorithms: Development of\n  an Axiomatic Framework to Assess the Quality of Methods that Explain\n  Individual Predictions Abstract: From self-driving vehicles and back-flipping robots to virtual assistants who\nbook our next appointment at the hair salon or at that restaurant for dinner -\nmachine learning systems are becoming increasingly ubiquitous. The main reason\nfor this is that these methods boast remarkable predictive capabilities.\nHowever, most of these models remain black boxes, meaning that it is very\nchallenging for humans to follow and understand their intricate inner workings.\nConsequently, interpretability has suffered under this ever-increasing\ncomplexity of machine learning models. Especially with regards to new\nregulations, such as the General Data Protection Regulation (GDPR), the\nnecessity for plausibility and verifiability of predictions made by these black\nboxes is indispensable. Driven by the needs of industry and practice, the\nresearch community has recognised this interpretability problem and focussed on\ndeveloping a growing number of so-called explanation methods over the past few\nyears. These methods explain individual predictions made by black box machine\nlearning models and help to recover some of the lost interpretability. With the\nproliferation of these explanation methods, it is, however, often unclear,\nwhich explanation method offers a higher explanation quality, or is generally\nbetter-suited for the situation at hand. In this thesis, we thus propose an\naxiomatic framework, which allows comparing the quality of different\nexplanation methods amongst each other. Through experimental validation, we\nfind that the developed framework is useful to assess the explanation quality\nof different explanation methods and reach conclusions that are consistent with\nindependent research. \n\n"}
{"id": "1808.05240", "contents": "Title: Blended Coarse Gradient Descent for Full Quantization of Deep Neural\n  Networks Abstract: Quantized deep neural networks (QDNNs) are attractive due to their much lower\nmemory storage and faster inference speed than their regular full precision\ncounterparts. To maintain the same performance level especially at low\nbit-widths, QDNNs must be retrained. Their training involves piecewise constant\nactivation functions and discrete weights, hence mathematical challenges arise.\nWe introduce the notion of coarse gradient and propose the blended coarse\ngradient descent (BCGD) algorithm, for training fully quantized neural\nnetworks. Coarse gradient is generally not a gradient of any function but an\nartificial ascent direction. The weight update of BCGD goes by coarse gradient\ncorrection of a weighted average of the full precision weights and their\nquantization (the so-called blending), which yields sufficient descent in the\nobjective value and thus accelerates the training. Our experiments demonstrate\nthat this simple blending technique is very effective for quantization at\nextremely low bit-width such as binarization. In full quantization of ResNet-18\nfor ImageNet classification task, BCGD gives 64.36\\% top-1 accuracy with binary\nweights across all layers and 4-bit adaptive activation. If the weights in the\nfirst and last layers are kept in full precision, this number increases to\n65.46\\%. As theoretical justification, we show convergence analysis of coarse\ngradient descent for a two-linear-layer neural network model with Gaussian\ninput data, and prove that the expected coarse gradient correlates positively\nwith the underlying true gradient. \n\n"}
{"id": "1808.05577", "contents": "Title: Deeper Image Quality Transfer: Training Low-Memory Neural Networks for\n  3D Images Abstract: In this paper we address the memory demands that come with the processing of\n3-dimensional, high-resolution, multi-channeled medical images in deep\nlearning. We exploit memory-efficient backpropagation techniques, to reduce the\nmemory complexity of network training from being linear in the network's depth,\nto being roughly constant $ - $ permitting us to elongate deep architectures\nwith negligible memory increase. We evaluate our methodology in the paradigm of\nImage Quality Transfer, whilst noting its potential application to various\ntasks that use deep learning. We study the impact of depth on accuracy and show\nthat deeper models have more predictive power, which may exploit larger\ntraining sets. We obtain substantially better results than the previous\nstate-of-the-art model with a slight memory increase, reducing the\nroot-mean-squared-error by $ 13\\% $. Our code is publicly available. \n\n"}
{"id": "1808.05587", "contents": "Title: Deep Convolutional Networks as shallow Gaussian Processes Abstract: We show that the output of a (residual) convolutional neural network (CNN)\nwith an appropriate prior over the weights and biases is a Gaussian process\n(GP) in the limit of infinitely many convolutional filters, extending similar\nresults for dense networks. For a CNN, the equivalent kernel can be computed\nexactly and, unlike \"deep kernels\", has very few parameters: only the\nhyperparameters of the original CNN. Further, we show that this kernel has two\nproperties that allow it to be computed efficiently; the cost of evaluating the\nkernel for a pair of images is similar to a single forward pass through the\noriginal CNN with only one filter per layer. The kernel equivalent to a\n32-layer ResNet obtains 0.84% classification error on MNIST, a new record for\nGPs with a comparable number of parameters. \n\n"}
{"id": "1808.05933", "contents": "Title: Decentralized Dictionary Learning Over Time-Varying Digraphs Abstract: This paper studies Dictionary Learning problems wherein the learning task is\ndistributed over a multi-agent network, modeled as a time-varying directed\ngraph. This formulation is relevant, for instance, in Big Data scenarios where\nmassive amounts of data are collected/stored in different locations (e.g.,\nsensors, clouds) and aggregating and/or processing all data in a fusion center\nmight be inefficient or unfeasible, due to resource limitations, communication\noverheads or privacy issues. We develop a unified decentralized algorithmic\nframework for this class of nonconvex problems, which is proved to converge to\nstationary solutions at a sublinear rate. The new method hinges on Successive\nConvex Approximation techniques, coupled with a decentralized tracking\nmechanism aiming at locally estimating the gradient of the smooth part of the\nsum-utility. To the best of our knowledge, this is the first provably\nconvergent decentralized algorithm for Dictionary Learning and, more generally,\nbi-convex problems over (time-varying) (di)graphs. \n\n"}
{"id": "1808.06394", "contents": "Title: Faster Support Vector Machines Abstract: The time complexity of support vector machines (SVMs) prohibits training on\nhuge data sets with millions of data points. Recently, multilevel approaches to\ntrain SVMs have been developed to allow for time-efficient training on huge\ndata sets. While regular SVMs perform the entire training in one -- time\nconsuming -- optimization step, multilevel SVMs first build a hierarchy of\nproblems decreasing in size that resemble the original problem and then train\nan SVM model for each hierarchy level, benefiting from the solved models of\nprevious levels. We present a faster multilevel support vector machine that\nuses a label propagation algorithm to construct the problem hierarchy.\nExtensive experiments indicate that our approach is up to orders of magnitude\nfaster than the previous fastest algorithm while having comparable\nclassification quality. For example, already one of our sequential solvers is\non average a factor 15 faster than the parallel ThunderSVM algorithm, while\nhaving similar classification quality. \n\n"}
{"id": "1808.06797", "contents": "Title: zoNNscan : a boundary-entropy index for zone inspection of neural models Abstract: The training of deep neural network classifiers results in decision\nboundaries which geometry is still not well understood. This is in direct\nrelation with classification problems such as so called adversarial examples.\nWe introduce zoNNscan, an index that is intended to inform on the boundary\nuncertainty (in terms of the presence of other classes) around one given input\ndatapoint. It is based on confidence entropy, and is implemented through\nsampling in the multidimensional ball surrounding that input. We detail the\nzoNNscan index, give an algorithm for approximating it, and finally illustrate\nits benefits on four applications, including two important problems for the\nadoption of deep networks in critical systems: adversarial examples and corner\ncase inputs. We highlight that zoNNscan exhibits significantly higher values\nthan for standard inputs in those two problem classes. \n\n"}
{"id": "1808.07217", "contents": "Title: Don't Use Large Mini-Batches, Use Local SGD Abstract: Mini-batch stochastic gradient methods (SGD) are state of the art for\ndistributed training of deep neural networks. Drastic increases in the\nmini-batch sizes have lead to key efficiency and scalability gains in recent\nyears. However, progress faces a major roadblock, as models trained with large\nbatches often do not generalize well, i.e. they do not show good accuracy on\nnew data. As a remedy, we propose a \\emph{post-local} SGD and show that it\nsignificantly improves the generalization performance compared to large-batch\ntraining on standard benchmarks while enjoying the same efficiency\n(time-to-accuracy) and scalability. We further provide an extensive study of\nthe communication efficiency vs. performance trade-offs associated with a host\nof \\emph{local SGD} variants. \n\n"}
{"id": "1808.07632", "contents": "Title: DOPING: Generative Data Augmentation for Unsupervised Anomaly Detection\n  with GAN Abstract: Recently, the introduction of the generative adversarial network (GAN) and\nits variants has enabled the generation of realistic synthetic samples, which\nhas been used for enlarging training sets. Previous work primarily focused on\ndata augmentation for semi-supervised and supervised tasks. In this paper, we\ninstead focus on unsupervised anomaly detection and propose a novel generative\ndata augmentation framework optimized for this task. In particular, we propose\nto oversample infrequent normal samples - normal samples that occur with small\nprobability, e.g., rare normal events. We show that these samples are\nresponsible for false positives in anomaly detection. However, oversampling of\ninfrequent normal samples is challenging for real-world high-dimensional data\nwith multimodal distributions. To address this challenge, we propose to use a\nGAN variant known as the adversarial autoencoder (AAE) to transform the\nhigh-dimensional multimodal data distributions into low-dimensional unimodal\nlatent distributions with well-defined tail probability. Then, we\nsystematically oversample at the `edge' of the latent distributions to increase\nthe density of infrequent normal samples. We show that our oversampling\npipeline is a unified one: it is generally applicable to datasets with\ndifferent complex data distributions. To the best of our knowledge, our method\nis the first data augmentation technique focused on improving performance in\nunsupervised anomaly detection. We validate our method by demonstrating\nconsistent improvements across several real-world datasets. \n\n"}
{"id": "1808.07784", "contents": "Title: Time-Agnostic Prediction: Predicting Predictable Video Frames Abstract: Prediction is arguably one of the most basic functions of an intelligent\nsystem. In general, the problem of predicting events in the future or between\ntwo waypoints is exceedingly difficult. However, most phenomena naturally pass\nthrough relatively predictable bottlenecks---while we cannot predict the\nprecise trajectory of a robot arm between being at rest and holding an object\nup, we can be certain that it must have picked the object up. To exploit this,\nwe decouple visual prediction from a rigid notion of time. While conventional\napproaches predict frames at regularly spaced temporal intervals, our\ntime-agnostic predictors (TAP) are not tied to specific times so that they may\ninstead discover predictable \"bottleneck\" frames no matter when they occur. We\nevaluate our approach for future and intermediate frame prediction across three\nrobotic manipulation tasks. Our predictions are not only of higher visual\nquality, but also correspond to coherent semantic subgoals in temporally\nextended tasks. \n\n"}
{"id": "1808.08268", "contents": "Title: Learning Models for Shared Control of Human-Machine Systems with Unknown\n  Dynamics Abstract: We present a novel approach to shared control of human-machine systems. Our\nmethod assumes no a priori knowledge of the system dynamics. Instead, we learn\nboth the dynamics and information about the user's interaction from observation\nthrough the use of the Koopman operator. Using the learned model, we define an\noptimization problem to compute the optimal policy for a given task, and\ncompare the user input to the optimal input. We demonstrate the efficacy of our\napproach with a user study. We also analyze the individual nature of the\nlearned models by comparing the effectiveness of our approach when the\ndemonstration data comes from a user's own interactions, from the interactions\nof a group of users and from a domain expert. Positive results include\nstatistically significant improvements on task metrics when comparing a\nuser-only control paradigm with our shared control paradigm. Surprising results\ninclude findings that suggest that individualizing the model based on a user's\nown data does not effect the ability to learn a useful dynamic system. We\nexplore this tension as it relates to developing human-in-the-loop systems\nfurther in the discussion. \n\n"}
{"id": "1808.08280", "contents": "Title: Deep multiscale convolutional feature learning for weakly supervised\n  localization of chest pathologies in X-ray images Abstract: Localization of chest pathologies in chest X-ray images is a challenging task\nbecause of their varying sizes and appearances. We propose a novel weakly\nsupervised method to localize chest pathologies using class aware deep\nmultiscale feature learning. Our method leverages intermediate feature maps\nfrom CNN layers at different stages of a deep network during the training of a\nclassification model using image level annotations of pathologies. During the\ntraining phase, a set of \\emph{layer relevance weights} are learned for each\npathology class and the CNN is optimized to perform pathology classification by\nconvex combination of feature maps from both shallow and deep layers using the\nlearned weights. During the test phase, to localize the predicted pathology,\nthe multiscale attention map is obtained by convex combination of class\nactivation maps from each stage using the \\emph{layer relevance weights}\nlearned during the training phase. We have validated our method using 112000\nX-ray images and compared with the state-of-the-art localization methods. We\nexperimentally demonstrate that the proposed weakly supervised method can\nimprove the localization performance of small pathologies such as nodule and\nmass while giving comparable performance for bigger pathologies e.g.,\nCardiomegaly \n\n"}
{"id": "1808.08627", "contents": "Title: Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation Abstract: As opposed to manual feature engineering which is tedious and difficult to\nscale, network representation learning has attracted a surge of research\ninterests as it automates the process of feature learning on graphs. The\nlearned low-dimensional node vector representation is generalizable and eases\nthe knowledge discovery process on graphs by enabling various off-the-shelf\nmachine learning tools to be directly applied. Recent research has shown that\nthe past decade of network embedding approaches either explicitly factorize a\ncarefully designed matrix to obtain the low-dimensional node vector\nrepresentation or are closely related to implicit matrix factorization, with\nthe fundamental assumption that the factorized node connectivity matrix is\nlow-rank. Nonetheless, the global low-rank assumption does not necessarily hold\nespecially when the factorized matrix encodes complex node interactions, and\nthe resultant single low-rank embedding matrix is insufficient to capture all\nthe observed connectivity patterns. In this regard, we propose a novel\nmulti-level network embedding framework BoostNE, which can learn multiple\nnetwork embedding representations of different granularity from coarse to fine\nwithout imposing the prevalent global low-rank assumption. The proposed BoostNE\nmethod is also in line with the successful gradient boosting method in ensemble\nlearning as multiple weak embeddings lead to a stronger and more effective one.\nWe assess the effectiveness of the proposed BoostNE framework by comparing it\nwith existing state-of-the-art network embedding methods on various datasets,\nand the experimental results corroborate the superiority of the proposed\nBoostNE network embedding framework. \n\n"}
{"id": "1808.08952", "contents": "Title: Deep Learning of Vortex Induced Vibrations Abstract: Vortex induced vibrations of bluff bodies occur when the vortex shedding\nfrequency is close to the natural frequency of the structure. Of interest is\nthe prediction of the lift and drag forces on the structure given some limited\nand scattered information on the velocity field. This is an inverse problem\nthat is not straightforward to solve using standard computational fluid\ndynamics (CFD) methods, especially since no information is provided for the\npressure. An even greater challenge is to infer the lift and drag forces given\nsome dye or smoke visualizations of the flow field. Here we employ deep neural\nnetworks that are extended to encode the incompressible Navier-Stokes equations\ncoupled with the structure's dynamic motion equation. In the first case, given\nscattered data in space-time on the velocity field and the structure's motion,\nwe use four coupled deep neural networks to infer very accurately the\nstructural parameters, the entire time-dependent pressure field (with no prior\ntraining data), and reconstruct the velocity vector field and the structure's\ndynamic motion. In the second case, given scattered data in space-time on a\nconcentration field only, we use five coupled deep neural networks to infer\nvery accurately the vector velocity field and all other quantities of interest\nas before. This new paradigm of inference in fluid mechanics for coupled\nmulti-physics problems enables velocity and pressure quantification from flow\nsnapshots in small subdomains and can be exploited for flow control\napplications and also for system identification. \n\n"}
{"id": "1808.09334", "contents": "Title: A Discriminative Latent-Variable Model for Bilingual Lexicon Induction Abstract: We introduce a novel discriminative latent variable model for bilingual\nlexicon induction. Our model combines the bipartite matching dictionary prior\nof Haghighi et al. (2008) with a representation-based approach (Artetxe et al.,\n2017). To train the model, we derive an efficient Viterbi EM algorithm. We\nprovide empirical results on six language pairs under two metrics and show that\nthe prior improves the induced bilingual lexicons. We also demonstrate how\nprevious work may be viewed as a similarly fashioned latent-variable model,\nalbeit with a different prior. \n\n"}
{"id": "1808.09942", "contents": "Title: Neural Compositional Denotational Semantics for Question Answering Abstract: Answering compositional questions requiring multi-step reasoning is\nchallenging. We introduce an end-to-end differentiable model for interpreting\nquestions about a knowledge graph (KG), which is inspired by formal approaches\nto semantics. Each span of text is represented by a denotation in a KG and a\nvector that captures ungrounded aspects of meaning. Learned composition modules\nrecursively combine constituent spans, culminating in a grounding for the\ncomplete sentence which answers the question. For example, to interpret \"not\ngreen\", the model represents \"green\" as a set of KG entities and \"not\" as a\ntrainable ungrounded vector---and then uses this vector to parameterize a\ncomposition function that performs a complement operation. For each sentence,\nwe build a parse chart subsuming all possible parses, allowing the model to\njointly learn both the composition operators and output structure by gradient\ndescent from end-task supervision. The model learns a variety of challenging\nsemantic operators, such as quantifiers, disjunctions and composed relations,\nand infers latent syntactic structure. It also generalizes well to longer\nquestions than seen in its training data, in contrast to RNN, its tree-based\nvariants, and semantic parsing baselines. \n\n"}
{"id": "1808.10120", "contents": "Title: ExIt-OOS: Towards Learning from Planning in Imperfect Information Games Abstract: The current state of the art in playing many important perfect information\ngames, including Chess and Go, combines planning and deep reinforcement\nlearning with self-play. We extend this approach to imperfect information games\nand present ExIt-OOS, a novel approach to playing imperfect information games\nwithin the Expert Iteration framework and inspired by AlphaZero. We use Online\nOutcome Sampling, an online search algorithm for imperfect information games in\nplace of MCTS. While training online, our neural strategy is used to improve\nthe accuracy of playouts in OOS, allowing a learning and planning feedback loop\nfor imperfect information games. \n\n"}
{"id": "1808.10393", "contents": "Title: Learning End-to-end Autonomous Driving using Guided Auxiliary\n  Supervision Abstract: Learning to drive faithfully in highly stochastic urban settings remains an\nopen problem. To that end, we propose a Multi-task Learning from Demonstration\n(MT-LfD) framework which uses supervised auxiliary task prediction to guide the\nmain task of predicting the driving commands. Our framework involves an\nend-to-end trainable network for imitating the expert demonstrator's driving\ncommands. The network intermediately predicts visual affordances and action\nprimitives through direct supervision which provide the aforementioned\nauxiliary supervised guidance. We demonstrate that such joint learning and\nsupervised guidance facilitates hierarchical task decomposition, assisting the\nagent to learn faster, achieve better driving performance and increases\ntransparency of the otherwise black-box end-to-end network. We run our\nexperiments to validate the MT-LfD framework in CARLA, an open-source urban\ndriving simulator. We introduce multiple non-player agents in CARLA and induce\ntemporal noise in them for realistic stochasticity. \n\n"}
{"id": "1808.10678", "contents": "Title: Self-Attention Linguistic-Acoustic Decoder Abstract: The conversion from text to speech relies on the accurate mapping from\nlinguistic to acoustic symbol sequences, for which current practice employs\nrecurrent statistical models like recurrent neural networks. Despite the good\nperformance of such models (in terms of low distortion in the generated\nspeech), their recursive structure tends to make them slow to train and to\nsample from. In this work, we try to overcome the limitations of recursive\nstructure by using a module based on the transformer decoder network, designed\nwithout recurrent connections but emulating them with attention and positioning\ncodes. Our results show that the proposed decoder network is competitive in\nterms of distortion when compared to a recurrent baseline, whilst being\nsignificantly faster in terms of CPU inference time. On average, it increases\nMel cepstral distortion between 0.1 and 0.3 dB, but it is over an order of\nmagnitude faster on average. Fast inference is important for the deployment of\nspeech synthesis systems on devices with restricted resources, like mobile\nphones or embedded systems, where speaking virtual assistants are gaining\nimportance. \n\n"}
{"id": "1809.00082", "contents": "Title: NEU: A Meta-Algorithm for Universal UAP-Invariant Feature Representation Abstract: Effective feature representation is key to the predictive performance of any\nalgorithm. This paper introduces a meta-procedure, called Non-Euclidean\nUpgrading (NEU), which learns feature maps that are expressive enough to embed\nthe universal approximation property (UAP) into most model classes while only\noutputting feature maps that preserve any model class's UAP. We show that NEU\ncan learn any feature map with these two properties if that feature map is\nasymptotically deformable into the identity. We also find that the\nfeature-representations learned by NEU are always submanifolds of the feature\nspace. NEU's properties are derived from a new deep neural model that is\nuniversal amongst all orientation-preserving homeomorphisms on the input space.\nWe derive qualitative and quantitative approximation guarantees for this\narchitecture. We quantify the number of parameters required for this new\narchitecture to memorize any set of input-output pairs while simultaneously\nfixing every point of the input space lying outside some compact set, and we\nquantify the size of this set as a function of our model's depth. Moreover, we\nshow that no deep feed-forward network with commonly used activation function\nhas all these properties. NEU's performance is evaluated against competing\nmachine learning methods on various regression and dimension reduction tasks\nboth with financial and simulated data. \n\n"}
{"id": "1809.00101", "contents": "Title: Attentive Crowd Flow Machines Abstract: Traffic flow prediction is crucial for urban traffic management and public\nsafety. Its key challenges lie in how to adaptively integrate the various\nfactors that affect the flow changes. In this paper, we propose a unified\nneural network module to address this problem, called Attentive Crowd Flow\nMachine~(ACFM), which is able to infer the evolution of the crowd flow by\nlearning dynamic representations of temporally-varying data with an attention\nmechanism. Specifically, the ACFM is composed of two progressive ConvLSTM units\nconnected with a convolutional layer for spatial weight prediction. The first\nLSTM takes the sequential flow density representation as input and generates a\nhidden state at each time-step for attention map inference, while the second\nLSTM aims at learning the effective spatial-temporal feature expression from\nattentionally weighted crowd flow features. Based on the ACFM, we further build\na deep architecture with the application to citywide crowd flow prediction,\nwhich naturally incorporates the sequential and periodic data as well as other\nexternal influences. Extensive experiments on two standard benchmarks (i.e.,\ncrowd flow in Beijing and New York City) show that the proposed method achieves\nsignificant improvements over the state-of-the-art methods. \n\n"}
{"id": "1809.00973", "contents": "Title: Equivalence of approximation by convolutional neural networks and\n  fully-connected networks Abstract: Convolutional neural networks are the most widely used type of neural\nnetworks in applications. In mathematical analysis, however, mostly\nfully-connected networks are studied. In this paper, we establish a connection\nbetween both network architectures. Using this connection, we show that all\nupper and lower bounds concerning approximation rates of {fully-connected}\nneural networks for functions $f \\in \\mathcal{C}$ -- for an arbitrary function\nclass $\\mathcal{C}$ -- translate to essentially the same bounds concerning\napproximation rates of convolutional neural networks for functions $f \\in\n{\\mathcal{C}^{equi}}$, with the class ${\\mathcal{C}^{equi}}$ consisting of all\ntranslation equivariant functions whose first coordinate belongs to\n$\\mathcal{C}$. All presented results consider exclusively the case of\nconvolutional neural networks without any pooling operation and with circular\nconvolutions, i.e., not based on zero-padding. \n\n"}
{"id": "1809.01266", "contents": "Title: DeepHunter: Hunting Deep Neural Network Defects via Coverage-Guided\n  Fuzzing Abstract: In company with the data explosion over the past decade, deep neural network\n(DNN) based software has experienced unprecedented leap and is becoming the key\ndriving force of many novel industrial applications, including many\nsafety-critical scenarios such as autonomous driving. Despite great success\nachieved in various human intelligence tasks, similar to traditional software,\nDNNs could also exhibit incorrect behaviors caused by hidden defects causing\nsevere accidents and losses. In this paper, we propose DeepHunter, an automated\nfuzz testing framework for hunting potential defects of general-purpose DNNs.\nDeepHunter performs metamorphic mutation to generate new semantically preserved\ntests, and leverages multiple plugable coverage criteria as feedback to guide\nthe test generation from different perspectives. To be scalable towards\npractical-sized DNNs, DeepHunter maintains multiple tests in a batch, and\nprioritizes the tests selection based on active feedback. The effectiveness of\nDeepHunter is extensively investigated on 3 popular datasets (MNIST, CIFAR-10,\nImageNet) and 7 DNNs with diverse complexities, under a large set of 6 coverage\ncriteria as feedback. The large-scale experiments demonstrate that DeepHunter\ncan (1) significantly boost the coverage with guidance; (2) generate useful\ntests to detect erroneous behaviors and facilitate the DNN model quality\nevaluation; (3) accurately capture potential defects during DNN quantization\nfor platform migration. \n\n"}
{"id": "1809.01479", "contents": "Title: UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification Abstract: The Fact Extraction and VERification (FEVER) shared task was launched to\nsupport the development of systems able to verify claims by extracting\nsupporting or refuting facts from raw text. The shared task organizers provide\na large-scale dataset for the consecutive steps involved in claim verification,\nin particular, document retrieval, fact extraction, and claim classification.\nIn this paper, we present our claim verification pipeline approach, which,\naccording to the preliminary results, scored third in the shared task, out of\n23 competing systems. For the document retrieval, we implemented a new entity\nlinking approach. In order to be able to rank candidate facts and classify a\nclaim on the basis of several selected facts, we introduce two extensions to\nthe Enhanced LSTM (ESIM). \n\n"}
{"id": "1809.01499", "contents": "Title: Extractive Adversarial Networks: High-Recall Explanations for\n  Identifying Personal Attacks in Social Media Posts Abstract: We introduce an adversarial method for producing high-recall explanations of\nneural text classifier decisions. Building on an existing architecture for\nextractive explanations via hard attention, we add an adversarial layer which\nscans the residual of the attention for remaining predictive signal. Motivated\nby the important domain of detecting personal attacks in social media comments,\nwe additionally demonstrate the importance of manually setting a semantically\nappropriate `default' behavior for the model by explicitly manipulating its\nbias term. We develop a validation set of human-annotated personal attacks to\nevaluate the impact of these changes. \n\n"}
{"id": "1809.01534", "contents": "Title: Utilizing Character and Word Embeddings for Text Normalization with\n  Sequence-to-Sequence Models Abstract: Text normalization is an important enabling technology for several NLP tasks.\nRecently, neural-network-based approaches have outperformed well-established\nmodels in this task. However, in languages other than English, there has been\nlittle exploration in this direction. Both the scarcity of annotated data and\nthe complexity of the language increase the difficulty of the problem. To\naddress these challenges, we use a sequence-to-sequence model with\ncharacter-based attention, which in addition to its self-learned character\nembeddings, uses word embeddings pre-trained with an approach that also models\nsubword information. This provides the neural model with access to more\nlinguistic information especially suitable for text normalization, without\nlarge parallel corpora. We show that providing the model with word-level\nfeatures bridges the gap for the neural network approach to achieve a\nstate-of-the-art F1 score on a standard Arabic language correction shared task\ndataset. \n\n"}
{"id": "1809.01628", "contents": "Title: Online local pool generation for dynamic classifier selection: an\n  extended version Abstract: Dynamic Classifier Selection (DCS) techniques have difficulty in selecting\nthe most competent classifier in a pool, even when its presence is assured.\nSince the DCS techniques rely only on local data to estimate a classifier's\ncompetence, the manner in which the pool is generated could affect the choice\nof the best classifier for a given sample. That is, the global perspective in\nwhich pools are generated may not help the DCS techniques in selecting a\ncompetent classifier for samples that are likely to be mislabelled. Thus, we\npropose in this work an online pool generation method that produces a locally\naccurate pool for test samples in difficult regions of the feature space. The\ndifficulty of a given area is determined by the classification difficulty of\nthe samples in it. That way, by using classifiers that were generated in a\nlocal scope, it could be easier for the DCS techniques to select the best one\nfor the difficult samples. For the query samples in easy regions, a simple\nnearest neighbors rule is used. In the extended version of this work, a deep\nanalysis on the correlation between instance hardness and the performance of\nDCS techniques is presented. An instance hardness measure that conveys the\ndegree of local class overlap is then used to decide when the local pool is\nused in the proposed scheme. The proposed method yielded significantly greater\nrecognition rates in comparison to a Bagging-generated pool and two other\nglobal pool generation schemes for all DCS techniques evaluated. The proposed\nscheme's performance was also significantly superior to three state-of-the-art\nclassification models and statistically equivalent to five of them. Moreover,\nan extended analysis on the computational complexity of the proposed method and\nof several DS techniques is presented in this version. We also provide the\nimplementation of the proposed technique using the DESLib library on GitHub. \n\n"}
{"id": "1809.01921", "contents": "Title: RDPD: Rich Data Helps Poor Data via Imitation Abstract: In many situations, we need to build and deploy separate models in related\nenvironments with different data qualities. For example, an environment with\nstrong observation equipments (e.g., intensive care units) often provides\nhigh-quality multi-modal data, which are acquired from multiple sensory devices\nand have rich-feature representations. On the other hand, an environment with\npoor observation equipment (e.g., at home) only provides low-quality, uni-modal\ndata with poor-feature representations. To deploy a competitive model in a\npoor-data environment without requiring direct access to multi-modal data\nacquired from a rich-data environment, this paper develops and presents a\nknowledge distillation (KD) method (RDPD) to enhance a predictive model trained\non poor data using knowledge distilled from a high-complexity model trained on\nrich, private data. We evaluated RDPD on three real-world datasets and shown\nthat its distilled model consistently outperformed all baselines across all\ndatasets, especially achieving the greatest performance improvement over a\nmodel trained only on low-quality data by 24.56% on PR-AUC and 12.21% on\nROC-AUC, and over that of a state-of-the-art KD model by 5.91% on PR-AUC and\n4.44% on ROC-AUC. \n\n"}
{"id": "1809.02070", "contents": "Title: ARCHER: Aggressive Rewards to Counter bias in Hindsight Experience\n  Replay Abstract: Experience replay is an important technique for addressing\nsample-inefficiency in deep reinforcement learning (RL), but faces difficulty\nin learning from binary and sparse rewards due to disproportionately few\nsuccessful experiences in the replay buffer. Hindsight experience replay (HER)\nwas recently proposed to tackle this difficulty by manipulating unsuccessful\ntransitions, but in doing so, HER introduces a significant bias in the replay\nbuffer experiences and therefore achieves a suboptimal improvement in\nsample-efficiency. In this paper, we present an analysis on the source of bias\nin HER, and propose a simple and effective method to counter the bias, to most\neffectively harness the sample-efficiency provided by HER. Our method,\nmotivated by counter-factual reasoning and called ARCHER, extends HER with a\ntrade-off to make rewards calculated for hindsight experiences numerically\ngreater than real rewards. We validate our algorithm on two continuous control\nenvironments from DeepMind Control Suite - Reacher and Finger, which simulate\nmanipulation tasks with a robotic arm - in combination with various reward\nfunctions, task complexities and goal sampling strategies. Our experiments\nconsistently demonstrate that countering bias using more aggressive hindsight\nrewards increases sample efficiency, thus establishing the greater benefit of\nARCHER in RL applications with limited computing budget. \n\n"}
{"id": "1809.02362", "contents": "Title: A proof that artificial neural networks overcome the curse of\n  dimensionality in the numerical approximation of Black-Scholes partial\n  differential equations Abstract: Artificial neural networks (ANNs) have very successfully been used in\nnumerical simulations for a series of computational problems ranging from image\nclassification/image recognition, speech recognition, time series analysis,\ngame intelligence, and computational advertising to numerical approximations of\npartial differential equations (PDEs). Such numerical simulations suggest that\nANNs have the capacity to very efficiently approximate high-dimensional\nfunctions and, especially, indicate that ANNs seem to admit the fundamental\npower to overcome the curse of dimensionality when approximating the\nhigh-dimensional functions appearing in the above named computational problems.\nThere are a series of rigorous mathematical approximation results for ANNs in\nthe scientific literature. Some of them prove convergence without convergence\nrates and some even rigorously establish convergence rates but there are only a\nfew special cases where mathematical results can rigorously explain the\nempirical success of ANNs when approximating high-dimensional functions. The\nkey contribution of this article is to disclose that ANNs can efficiently\napproximate high-dimensional functions in the case of numerical approximations\nof Black-Scholes PDEs. More precisely, this work reveals that the number of\nrequired parameters of an ANN to approximate the solution of the Black-Scholes\nPDE grows at most polynomially in both the reciprocal of the prescribed\napproximation accuracy $\\varepsilon > 0$ and the PDE dimension $d \\in\n\\mathbb{N}$. We thereby prove, for the first time, that ANNs do indeed overcome\nthe curse of dimensionality in the numerical approximation of Black-Scholes\nPDEs. \n\n"}
{"id": "1809.02657", "contents": "Title: dyngraph2vec: Capturing Network Dynamics using Dynamic Graph\n  Representation Learning Abstract: Learning graph representations is a fundamental task aimed at capturing\nvarious properties of graphs in vector space. The most recent methods learn\nsuch representations for static networks. However, real world networks evolve\nover time and have varying dynamics. Capturing such evolution is key to\npredicting the properties of unseen networks. To understand how the network\ndynamics affect the prediction performance, we propose an embedding approach\nwhich learns the structure of evolution in dynamic graphs and can predict\nunseen links with higher precision. Our model, dyngraph2vec, learns the\ntemporal transitions in the network using a deep architecture composed of dense\nand recurrent layers. We motivate the need of capturing dynamics for prediction\non a toy data set created using stochastic block models. We then demonstrate\nthe efficacy of dyngraph2vec over existing state-of-the-art methods on two real\nworld data sets. We observe that learning dynamics can improve the quality of\nembedding and yield better performance in link prediction. \n\n"}
{"id": "1809.03323", "contents": "Title: Deriving Enhanced Geographical Representations via Similarity-based\n  Spectral Analysis: Predicting Colorectal Cancer Survival Curves in Iowa Abstract: Neural networks are capable of learning rich, nonlinear feature\nrepresentations shown to be beneficial in many predictive tasks. In this work,\nwe use such models to explore different geographical feature representations in\nthe context of predicting colorectal cancer survival curves for patients in the\nstate of Iowa, spanning the years 1989 to 2013. Specifically, we compare model\nperformance using \"area between the curves\" (ABC) to assess (a) whether\nsurvival curves can be reasonably predicted for colorectal cancer patients in\nthe state of Iowa, (b) whether geographical features improve predictive\nperformance, (c) whether a simple binary representation, or a richer, spectral\nanalysis-elicited representation perform better, and (d) whether spectral\nanalysis-based representations can be improved upon by leveraging\ngeographically-descriptive features. In exploring (d), we devise a\nsimilarity-based spectral analysis procedure, which allows for the combination\nof geographically relational and geographically descriptive features. Our\nfindings suggest that survival curves can be reasonably estimated on average,\nwith predictive performance deviating at the five-year survival mark among all\nmodels. We also find that geographical features improve predictive performance,\nand that better performance is obtained using richer, spectral\nanalysis-elicited features. Furthermore, we find that similarity-based spectral\nanalysis-elicited representations improve upon the original spectral analysis\nresults by approximately 40%. \n\n"}
{"id": "1809.03363", "contents": "Title: Torchbearer: A Model Fitting Library for PyTorch Abstract: We introduce torchbearer, a model fitting library for pytorch aimed at\nresearchers working on deep learning or differentiable programming. The\ntorchbearer library provides a high level metric and callback API that can be\nused for a wide range of applications. We also include a series of built in\ncallbacks that can be used for: model persistence, learning rate decay,\nlogging, data visualization and more. The extensive documentation includes an\nexample library for deep learning and dynamic programming problems and can be\nfound at http://torchbearer.readthedocs.io. The code is licensed under the MIT\nLicense and available at https://github.com/ecs-vlc/torchbearer. \n\n"}
{"id": "1809.03548", "contents": "Title: VPE: Variational Policy Embedding for Transfer Reinforcement Learning Abstract: Reinforcement Learning methods are capable of solving complex problems, but\nresulting policies might perform poorly in environments that are even slightly\ndifferent. In robotics especially, training and deployment conditions often\nvary and data collection is expensive, making retraining undesirable.\nSimulation training allows for feasible training times, but on the other hand\nsuffers from a reality-gap when applied in real-world settings. This raises the\nneed of efficient adaptation of policies acting in new environments. We\nconsider this as a problem of transferring knowledge within a family of similar\nMarkov decision processes.\n  For this purpose we assume that Q-functions are generated by some\nlow-dimensional latent variable. Given such a Q-function, we can find a master\npolicy that can adapt given different values of this latent variable. Our\nmethod learns both the generative mapping and an approximate posterior of the\nlatent variables, enabling identification of policies for new tasks by\nsearching only in the latent space, rather than the space of all policies. The\nlow-dimensional space, and master policy found by our method enables policies\nto quickly adapt to new environments. We demonstrate the method on both a\npendulum swing-up task in simulation, and for simulation-to-real transfer on a\npushing task. \n\n"}
{"id": "1809.04445", "contents": "Title: Structured and Unstructured Outlier Identification for Robust PCA: A Non\n  iterative, Parameter free Algorithm Abstract: Robust PCA, the problem of PCA in the presence of outliers has been\nextensively investigated in the last few years. Here we focus on Robust PCA in\nthe outlier model where each column of the data matrix is either an inlier or\nan outlier. Most of the existing methods for this model assumes either the\nknowledge of the dimension of the lower dimensional subspace or the fraction of\noutliers in the system. However in many applications knowledge of these\nparameters is not available. Motivated by this we propose a parameter free\noutlier identification method for robust PCA which a) does not require the\nknowledge of outlier fraction, b) does not require the knowledge of the\ndimension of the underlying subspace, c) is computationally simple and fast d)\ncan handle structured and unstructured outliers. Further, analytical guarantees\nare derived for outlier identification and the performance of the algorithm is\ncompared with the existing state of the art methods in both real and synthetic\ndata for various outlier structures. \n\n"}
{"id": "1809.04705", "contents": "Title: Distilled Wasserstein Learning for Word Embedding and Topic Modeling Abstract: We propose a novel Wasserstein method with a distillation mechanism, yielding\njoint learning of word embeddings and topics. The proposed method is based on\nthe fact that the Euclidean distance between word embeddings may be employed as\nthe underlying distance in the Wasserstein topic model. The word distributions\nof topics, their optimal transports to the word distributions of documents, and\nthe embeddings of words are learned in a unified framework. When learning the\ntopic model, we leverage a distilled underlying distance matrix to update the\ntopic distributions and smoothly calculate the corresponding optimal\ntransports. Such a strategy provides the updating of word embeddings with\nrobust guidance, improving the algorithmic convergence. As an application, we\nfocus on patient admission records, in which the proposed method embeds the\ncodes of diseases and procedures and learns the topics of admissions, obtaining\nsuperior performance on clinically-meaningful disease network construction,\nmortality prediction as a function of admission codes, and procedure\nrecommendation. \n\n"}
{"id": "1809.04790", "contents": "Title: Adversarial Examples: Opportunities and Challenges Abstract: Deep neural networks (DNNs) have shown huge superiority over humans in image\nrecognition, speech processing, autonomous vehicles and medical diagnosis.\nHowever, recent studies indicate that DNNs are vulnerable to adversarial\nexamples (AEs), which are designed by attackers to fool deep learning models.\nDifferent from real examples, AEs can mislead the model to predict incorrect\noutputs while hardly be distinguished by human eyes, therefore threaten\nsecurity-critical deep-learning applications. In recent years, the generation\nand defense of AEs have become a research hotspot in the field of artificial\nintelligence (AI) security. This article reviews the latest research progress\nof AEs. First, we introduce the concept, cause, characteristics and evaluation\nmetrics of AEs, then give a survey on the state-of-the-art AE generation\nmethods with the discussion of advantages and disadvantages. After that, we\nreview the existing defenses and discuss their limitations. Finally, future\nresearch opportunities and challenges on AEs are prospected. \n\n"}
{"id": "1809.05050", "contents": "Title: Learning to Group and Label Fine-Grained Shape Components Abstract: A majority of stock 3D models in modern shape repositories are assembled with\nmany fine-grained components. The main cause of such data form is the\ncomponent-wise modeling process widely practiced by human modelers. These\nmodeling components thus inherently reflect some function-based shape\ndecomposition the artist had in mind during modeling. On the other hand,\nmodeling components represent an over-segmentation since a functional part is\nusually modeled as a multi-component assembly. Based on these observations, we\nadvocate that labeled segmentation of stock 3D models should not overlook the\nmodeling components and propose a learning solution to grouping and labeling of\nthe fine-grained components. However, directly characterizing the shape of\nindividual components for the purpose of labeling is unreliable, since they can\nbe arbitrarily tiny and semantically meaningless. We propose to generate part\nhypotheses from the components based on a hierarchical grouping strategy, and\nperform labeling on those part groups instead of directly on the components.\nPart hypotheses are mid-level elements which are more probable to carry\nsemantic information. A multiscale 3D convolutional neural network is trained\nto extract context-aware features for the hypotheses. To accomplish a labeled\nsegmentation of the whole shape, we formulate higher-order conditional random\nfields (CRFs) to infer an optimal label assignment for all components.\nExtensive experiments demonstrate that our method achieves significantly robust\nlabeling results on raw 3D models from public shape repositories. Our work also\ncontributes the first benchmark for component-wise labeling. \n\n"}
{"id": "1809.05053", "contents": "Title: XNLI: Evaluating Cross-lingual Sentence Representations Abstract: State-of-the-art natural language processing systems rely on supervision in\nthe form of annotated data to learn competent models. These models are\ngenerally trained on data in a single language (usually English), and cannot be\ndirectly used beyond that language. Since collecting data in every language is\nnot realistic, there has been a growing interest in cross-lingual language\nunderstanding (XLU) and low-resource cross-language transfer. In this work, we\nconstruct an evaluation set for XLU by extending the development and test sets\nof the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 15\nlanguages, including low-resource languages such as Swahili and Urdu. We hope\nthat our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence\nunderstanding by providing an informative standard evaluation task. In\naddition, we provide several baselines for multilingual sentence understanding,\nincluding two based on machine translation systems, and two that use parallel\ndata to train aligned multilingual bag-of-words and LSTM encoders. We find that\nXNLI represents a practical and challenging evaluation suite, and that directly\ntranslating the test data yields the best performance among available\nbaselines. \n\n"}
{"id": "1809.05483", "contents": "Title: A Multi-Stage Algorithm for Acoustic Physical Model Parameters\n  Estimation Abstract: One of the challenges in computational acoustics is the identification of\nmodels that can simulate and predict the physical behavior of a system\ngenerating an acoustic signal. Whenever such models are used for commercial\napplications an additional constraint is the time-to-market, making automation\nof the sound design process desirable. In previous works, a computational sound\ndesign approach has been proposed for the parameter estimation problem\ninvolving timbre matching by deep learning, which was applied to the synthesis\nof pipe organ tones. In this work we refine previous results by introducing the\nformer approach in a multi-stage algorithm that also adds heuristics and a\nstochastic optimization method operating on objective cost functions based on\npsychoacoustics. The optimization method shows to be able to refine the first\nestimate given by the deep learning approach and substantially improve the\nobjective metrics, with the additional benefit of reducing the sound design\nprocess time. Subjective listening tests are also conducted to gather\nadditional insights on the results. \n\n"}
{"id": "1809.05550", "contents": "Title: Efficient Structured Surrogate Loss and Regularization in Structured\n  Prediction Abstract: In this dissertation, we focus on several important problems in structured\nprediction. In structured prediction, the label has a rich intrinsic\nsubstructure, and the loss varies with respect to the predicted label and the\ntrue label pair. Structured SVM is an extension of binary SVM to adapt to such\nstructured tasks.\n  In the first part of the dissertation, we study the surrogate losses and its\nefficient methods. To minimize the empirical risk, a surrogate loss which upper\nbounds the loss, is used as a proxy to minimize the actual loss. Since the\nobjective function is written in terms of the surrogate loss, the choice of the\nsurrogate loss is important, and the performance depends on it. Another issue\nregarding the surrogate loss is the efficiency of the argmax label inference\nfor the surrogate loss. Efficient inference is necessary for the optimization\nsince it is often the most time-consuming step. We present a new class of\nsurrogate losses named bi-criteria surrogate loss, which is a generalization of\nthe popular surrogate losses. We first investigate an efficient method for a\nslack rescaling formulation as a starting point utilizing decomposability of\nthe model. Then, we extend the algorithm to the bi-criteria surrogate loss,\nwhich is very efficient and also shows performance improvements.\n  In the second part of the dissertation, another important issue of\nregularization is studied. Specifically, we investigate a problem of\nregularization in hierarchical classification when a structural imbalance\nexists in the label structure. We present a method to normalize the structure,\nas well as a new norm, namely shared Frobenius norm. It is suitable for\nhierarchical classification that adapts to the data in addition to the label\nstructure. \n\n"}
{"id": "1809.05650", "contents": "Title: Detecting and Explaining Drifts in Yearly Grant Applications Abstract: During the lifetime of a Business Process changes can be made to the\nworkflow, the required resources, required documents, . . . . Different traces\nfrom the same Business Process within a single log file can thus differ\nsubstantially due to these changes. We propose a method that is able to detect\nconcept drift in multivariate log files with a dozen attributes. We test our\napproach on the BPI Challenge 2018 data con- sisting of applications for EU\ndirect payment from farmers in Germany where we use it to detect Concept Drift.\nIn contrast to other methods our algorithm does not require the manual\nselection of the features used to detect drift. Our method first creates a\nmodel that captures the re- lations between attributes and between events of\ndifferent time steps. This model is then used to score every event and trace.\nThese scores can be used to detect outlying cases and concept drift. Thanks to\nthe decomposability of the score we are able to perform detailed root-cause\nanalysis. \n\n"}
{"id": "1809.05662", "contents": "Title: Wasserstein Autoencoders for Collaborative Filtering Abstract: The recommender systems have long been investigated in the literature.\nRecently, users' implicit feedback like `click' or `browse' are considered to\nbe able to enhance the recommendation performance. Therefore, a number of\nattempts have been made to resolve this issue. Among them, the variational\nautoencoders (VAE) approach already achieves a superior performance. However,\nthe distributions of the encoded latent variables overlap a lot which may\nrestrict its recommendation ability. To cope with this challenge, this paper\ntries to extend the Wasserstein autoencoders (WAE) for collaborative filtering.\nParticularly, the loss function of the adapted WAE is re-designed by\nintroducing two additional loss terms: (1) the mutual information loss between\nthe distribution of latent variables and the assumed ground truth distribution,\nand (2) the L1 regularization loss introduced to restrict the encoded latent\nvariables to be sparse. Two different cost functions are designed for measuring\nthe distance between the implicit feedback data and its re-generated version of\ndata. Experiments are valuated on three widely adopted data sets, i.e., ML-20M,\nNetflix and LASTFM. Both the baseline and the state-of-the-art approaches are\nchosen for the performance comparison which are Mult-DAE, Mult-VAE, CDAE and\nSlim. The performance of the proposed approach outperforms the compared methods\nwith respect to evaluation criteria Recall@1, Recall@5 and NDCG@10, and this\ndemonstrates the efficacy of the proposed approach. \n\n"}
{"id": "1809.06569", "contents": "Title: MBS: Macroblock Scaling for CNN Model Reduction Abstract: In this paper we propose the macroblock scaling (MBS) algorithm, which can be\napplied to various CNN architectures to reduce their model size. MBS adaptively\nreduces each CNN macroblock depending on its information redundancy measured by\nour proposed effective flops. Empirical studies conducted with ImageNet and\nCIFAR-10 attest that MBS can reduce the model size of some already compact CNN\nmodels, e.g., MobileNetV2 (25.03% further reduction) and ShuffleNet (20.74%),\nand even ultra-deep ones such as ResNet-101 (51.67%) and ResNet-1202 (72.71%)\nwith negligible accuracy degradation. MBS also performs better reduction at a\nmuch lower cost than the state-of-the-art optimization-based methods do. MBS's\nsimplicity and efficiency, its flexibility to work with any CNN model, and its\nscalability to work with models of any depth make it an attractive choice for\nCNN model size reduction. \n\n"}
{"id": "1809.06570", "contents": "Title: Switching Isotropic and Directional Exploration with Parameter Space\n  Noise in Deep Reinforcement Learning Abstract: This paper proposes an exploration method for deep reinforcement learning\nbased on parameter space noise. Recent studies have experimentally shown that\nparameter space noise results in better exploration than the commonly used\naction space noise. Previous methods devised a way to update the diagonal\ncovariance matrix of a noise distribution and did not consider the direction of\nthe noise vector and its correlation. In addition, fast updates of the noise\ndistribution are required to facilitate policy learning. We propose a method\nthat deforms the noise distribution according to the accumulated returns and\nthe noises that have led to the returns. Moreover, this method switches\nisotropic exploration and directional exploration in parameter space with\nregard to obtained rewards. We validate our exploration strategy in the OpenAI\nGym continuous environments and modified environments with sparse rewards. The\nproposed method achieves results that are competitive with a previous method at\nbaseline tasks. Moreover, our approach exhibits better performance in sparse\nreward environments by exploration with the switching strategy. \n\n"}
{"id": "1809.06686", "contents": "Title: Domain Adaptation for Real-Time Student Performance Prediction Abstract: Increasingly fast development and update cycle of online course contents, and\ndiverse demographics of students in each online classroom, make student\nperformance prediction in real-time (before the course finishes) and/or on\ncurriculum without specific historical performance data available interesting\ntopics for both industrial research and practical needs. In this research, we\ntackle the problem of real-time student performance prediction with on-going\ncourses in a domain adaptation framework, which is a system trained on\nstudents' labeled outcome from one set of previous coursework but is meant to\nbe deployed on another. In particular, we first introduce recently-developed\nGritNet architecture which is the current state of the art for student\nperformance prediction problem, and develop a new \\emph{unsupervised} domain\nadaptation method to transfer a GritNet trained on a past course to a new\ncourse without any (students' outcome) label. Our results for real Udacity\nstudents' graduation predictions show that the GritNet not only\n\\emph{generalizes} well from one course to another across different Nanodegree\nprograms, but enhances real-time predictions explicitly in the first few weeks\nwhen accurate predictions are most challenging. \n\n"}
{"id": "1809.06719", "contents": "Title: Improvements on Hindsight Learning Abstract: Sparse reward problems are one of the biggest challenges in Reinforcement\nLearning. Goal-directed tasks are one such sparse reward problems where a\nreward signal is received only when the goal is reached. One promising way to\ntrain an agent to perform goal-directed tasks is to use Hindsight Learning\napproaches. In these approaches, even when an agent fails to reach the desired\ngoal, the agent learns to reach the goal it achieved instead. Doing this over\nmultiple trajectories while generalizing the policy learned from the achieved\ngoals, the agent learns a goal conditioned policy to reach any goal. One such\napproach is Hindsight Experience replay which uses an off-policy Reinforcement\nLearning algorithm to learn a goal conditioned policy. In this approach, a\nreplay of the past transitions happens in a uniformly random fashion. Another\napproach is to use a Hindsight version of the policy gradients to directly\nlearn a policy. In this work, we discuss different ways to replay past\ntransitions to improve learning in hindsight experience replay focusing on\nprioritized variants in particular. Also, we implement the Hindsight Policy\ngradient methods to robotic tasks. \n\n"}
{"id": "1809.06750", "contents": "Title: Multiobjective Reinforcement Learning for Reconfigurable Adaptive\n  Optimal Control of Manufacturing Processes Abstract: In industrial applications of adaptive optimal control often multiple\ncontrary objectives have to be considered. The weights (relative importance) of\nthe objectives are often not known during the design of the control and can\nchange with changing production conditions and requirements. In this work a\nnovel model-free multiobjective reinforcement learning approach for adaptive\noptimal control of manufacturing processes is proposed. The approach enables\nsample-efficient learning in sequences of control configurations, given by\nparticular objective weights. \n\n"}
{"id": "1809.06858", "contents": "Title: FRAGE: Frequency-Agnostic Word Representation Abstract: Continuous word representation (aka word embedding) is a basic building block\nin many neural network-based models used in natural language processing tasks.\nAlthough it is widely accepted that words with similar semantics should be\nclose to each other in the embedding space, we find that word embeddings\nlearned in several tasks are biased towards word frequency: the embeddings of\nhigh-frequency and low-frequency words lie in different subregions of the\nembedding space, and the embedding of a rare word and a popular word can be far\nfrom each other even if they are semantically similar. This makes learned word\nembeddings ineffective, especially for rare words, and consequently limits the\nperformance of these neural network models. In this paper, we develop a neat,\nsimple yet effective way to learn \\emph{FRequency-AGnostic word Embedding}\n(FRAGE) using adversarial training. We conducted comprehensive studies on ten\ndatasets across four natural language processing tasks, including word\nsimilarity, language modeling, machine translation and text classification.\nResults show that with FRAGE, we achieve higher performance than the baselines\nin all tasks. \n\n"}
{"id": "1809.06970", "contents": "Title: FastDeepIoT: Towards Understanding and Optimizing Neural Network\n  Execution Time on Mobile and Embedded Devices Abstract: Deep neural networks show great potential as solutions to many sensing\napplication problems, but their excessive resource demand slows down execution\ntime, pausing a serious impediment to deployment on low-end devices. To address\nthis challenge, recent literature focused on compressing neural network size to\nimprove performance. We show that changing neural network size does not\nproportionally affect performance attributes of interest, such as execution\ntime. Rather, extreme run-time nonlinearities exist over the network\nconfiguration space. Hence, we propose a novel framework, called FastDeepIoT,\nthat uncovers the non-linear relation between neural network structure and\nexecution time, then exploits that understanding to find network configurations\nthat significantly improve the trade-off between execution time and accuracy on\nmobile and embedded devices. FastDeepIoT makes two key contributions. First,\nFastDeepIoT automatically learns an accurate and highly interpretable execution\ntime model for deep neural networks on the target device. This is done without\nprior knowledge of either the hardware specifications or the detailed\nimplementation of the used deep learning library. Second, FastDeepIoT informs a\ncompression algorithm how to minimize execution time on the profiled device\nwithout impacting accuracy. We evaluate FastDeepIoT using three different\nsensing-related tasks on two mobile devices: Nexus 5 and Galaxy Nexus.\nFastDeepIoT further reduces the neural network execution time by $48\\%$ to\n$78\\%$ and energy consumption by $37\\%$ to $69\\%$ compared with the\nstate-of-the-art compression algorithms. \n\n"}
{"id": "1809.08458", "contents": "Title: Shift-based Primitives for Efficient Convolutional Neural Networks Abstract: We propose a collection of three shift-based primitives for building\nefficient compact CNN-based networks. These three primitives (channel shift,\naddress shift, shortcut shift) can reduce the inference time on GPU while\nmaintains the prediction accuracy. These shift-based primitives only moves the\npointer but avoids memory copy, thus very fast. For example, the channel shift\noperation is 12.7x faster compared to channel shuffle in ShuffleNet but\nachieves the same accuracy. The address shift and channel shift can be merged\ninto the point-wise group convolution and invokes only a single kernel call,\ntaking little time to perform spatial convolution and channel shift. Shortcut\nshift requires no time to realize residual connection through allocating space\nin advance. We blend these shift-based primitives with point-wise group\nconvolution and built two inference-efficient CNN architectures named\nAddressNet and Enhanced AddressNet. Experiments on CIFAR100 and ImageNet\ndatasets show that our models are faster and achieve comparable or better\naccuracy. \n\n"}
{"id": "1809.09260", "contents": "Title: Low Precision Policy Distillation with Application to Low-Power,\n  Real-time Sensation-Cognition-Action Loop with Neuromorphic Computing Abstract: Low precision networks in the reinforcement learning (RL) setting are\nrelatively unexplored because of the limitations of binary activations for\nfunction approximation. Here, in the discrete action ATARI domain, we\ndemonstrate, for the first time, that low precision policy distillation from a\nhigh precision network provides a principled, practical way to train an RL\nagent. As an application, on 10 different ATARI games, we demonstrate real-time\nend-to-end game playing on low-power neuromorphic hardware by converting a\nsequence of game frames into discrete actions. \n\n"}
{"id": "1809.09582", "contents": "Title: Contextual Bandits with Cross-learning Abstract: In the classical contextual bandits problem, in each round $t$, a learner\nobserves some context $c$, chooses some action $i$ to perform, and receives\nsome reward $r_{i,t}(c)$. We consider the variant of this problem where in\naddition to receiving the reward $r_{i,t}(c)$, the learner also learns the\nvalues of $r_{i,t}(c')$ for some other contexts $c'$ in set $\\mathcal{O}_i(c)$;\ni.e., the rewards that would have been achieved by performing that action under\ndifferent contexts $c'\\in \\mathcal{O}_i(c)$. This variant arises in several\nstrategic settings, such as learning how to bid in non-truthful repeated\nauctions, which has gained a lot of attention lately as many platforms have\nswitched to running first-price auctions. We call this problem the contextual\nbandits problem with cross-learning. The best algorithms for the classical\ncontextual bandits problem achieve $\\tilde{O}(\\sqrt{CKT})$ regret against all\nstationary policies, where $C$ is the number of contexts, $K$ the number of\nactions, and $T$ the number of rounds. We design and analyze new algorithms for\nthe contextual bandits problem with cross-learning and show that their regret\nhas better dependence on the number of contexts. Under complete cross-learning\nwhere the rewards for all contexts are learned when choosing an action, i.e.,\nset $\\mathcal{O}_i(c)$ contains all contexts, we show that our algorithms\nachieve regret $\\tilde{O}(\\sqrt{KT})$, removing the dependence on $C$. For any\nother cases, i.e., under partial cross-learning where $|\\mathcal{O}_i(c)|< C$\nfor some context-action pair of $(i,c)$, the regret bounds depend on how the\nsets $\\mathcal O_i(c)$ impact the degree to which cross-learning between\ncontexts is possible. We simulate our algorithms on real auction data from an\nad exchange running first-price auctions and show that they outperform\ntraditional contextual bandit algorithms. \n\n"}
{"id": "1809.10271", "contents": "Title: Batch-normalized Recurrent Highway Networks Abstract: Gradient control plays an important role in feed-forward networks applied to\nvarious computer vision tasks. Previous work has shown that Recurrent Highway\nNetworks minimize the problem of vanishing or exploding gradients. They achieve\nthis by setting the eigenvalues of the temporal Jacobian to 1 across the time\nsteps. In this work, batch normalized recurrent highway networks are proposed\nto control the gradient flow in an improved way for network convergence.\nSpecifically, the introduced model can be formed by batch normalizing the\ninputs at each recurrence loop. The proposed model is tested on an image\ncaptioning task using MSCOCO dataset. Experimental results indicate that the\nbatch normalized recurrent highway networks converge faster and performs better\ncompared with the traditional LSTM and RHN based models. \n\n"}
{"id": "1809.10606", "contents": "Title: Solving Statistical Mechanics Using Variational Autoregressive Networks Abstract: We propose a general framework for solving statistical mechanics of systems\nwith finite size. The approach extends the celebrated variational mean-field\napproaches using autoregressive neural networks, which support direct sampling\nand exact calculation of normalized probability of configurations. It computes\nvariational free energy, estimates physical quantities such as entropy,\nmagnetizations and correlations, and generates uncorrelated samples all at\nonce. Training of the network employs the policy gradient approach in\nreinforcement learning, which unbiasedly estimates the gradient of variational\nparameters. We apply our approach to several classic systems, including 2D\nIsing models, the Hopfield model, the Sherrington-Kirkpatrick model, and the\ninverse Ising model, for demonstrating its advantages over existing variational\nmean-field methods. Our approach sheds light on solving statistical physics\nproblems using modern deep generative neural networks. \n\n"}
{"id": "1809.10717", "contents": "Title: Dataset: Rare Event Classification in Multivariate Time Series Abstract: A real-world dataset is provided from a pulp-and-paper manufacturing\nindustry. The dataset comes from a multivariate time series process. The data\ncontains a rare event of paper break that commonly occurs in the industry. The\ndata contains sensor readings at regular time-intervals (x's) and the event\nlabel (y). The primary purpose of the data is thought to be building a\nclassification model for early prediction of the rare event. However, it can\nalso be used for multivariate time series data exploration and building other\nsupervised and unsupervised models. \n\n"}
{"id": "1810.00045", "contents": "Title: Adversarial Domain Adaptation for Stable Brain-Machine Interfaces Abstract: Brain-Machine Interfaces (BMIs) have recently emerged as a clinically viable\noption to restore voluntary movements after paralysis. These devices are based\non the ability to extract information about movement intent from neural signals\nrecorded using multi-electrode arrays chronically implanted in the motor\ncortices of the brain. However, the inherent loss and turnover of recorded\nneurons requires repeated recalibrations of the interface, which can\npotentially alter the day-to-day user experience. The resulting need for\ncontinued user adaptation interferes with the natural, subconscious use of the\nBMI. Here, we introduce a new computational approach that decodes movement\nintent from a low-dimensional latent representation of the neural data. We\nimplement various domain adaptation methods to stabilize the interface over\nsignificantly long times. This includes Canonical Correlation Analysis used to\nalign the latent variables across days; this method requires prior\npoint-to-point correspondence of the time series across domains. Alternatively,\nwe match the empirical probability distributions of the latent variables across\ndays through the minimization of their Kullback-Leibler divergence. These two\nmethods provide a significant and comparable improvement in the performance of\nthe interface. However, implementation of an Adversarial Domain Adaptation\nNetwork trained to match the empirical probability distribution of the\nresiduals of the reconstructed neural signals outperforms the two methods based\non latent variables, while requiring remarkably few data points to solve the\ndomain adaptation problem. \n\n"}
{"id": "1810.00322", "contents": "Title: A Deep Learning Framework for Single-Sided Sound Speed Inversion in\n  Medical Ultrasound Abstract: Objective: Ultrasound elastography is gaining traction as an accessible and\nuseful diagnostic tool for such things as cancer detection and differentiation\nand thyroid disease diagnostics. Unfortunately, state of the art shear wave\nimaging techniques, essential to promote this goal, are limited to high-end\nultrasound hardware due to high power requirements; are extremely sensitive to\npatient and sonographer motion, and generally, suffer from low frame rates.\nMotivated by research and theory showing that longitudinal wave sound speed\ncarries similar diagnostic abilities to shear wave imaging, we present an\nalternative approach using single sided pressure-wave sound speed measurements\nfrom channel data.\n  Methods: In this paper, we present a single-sided sound speed inversion\nsolution using a fully convolutional deep neural network. We use simulations\nfor training, allowing the generation of limitless ground truth data.\n  Results: We show that it is possible to invert for longitudinal sound speed\nin soft tissue at high frame rates. We validate the method on simulated data.\nWe present highly encouraging results on limited real data.\n  Conclusion: Sound speed inversion on channel data has significant potential,\nmade possible in real time with deep learning technologies.\n  Significance: Specialized shear wave ultrasound systems remain inaccessible\nin many locations. longitudinal sound speed and deep learning technologies\nenable an alternative approach to diagnosis based on tissue elasticity. High\nframe rates are possible. \n\n"}
{"id": "1810.00518", "contents": "Title: Layer-compensated Pruning for Resource-constrained Convolutional Neural\n  Networks Abstract: Resource-efficient convolution neural networks enable not only the\nintelligence on edge devices but also opportunities in system-level\noptimization such as scheduling. In this work, we aim to improve the\nperformance of resource-constrained filter pruning by merging two sub-problems\ncommonly considered, i.e., (i) how many filters to prune for each layer and\n(ii) which filters to prune given a per-layer pruning budget, into a global\nfilter ranking problem. Our framework entails a novel algorithm, dubbed\nlayer-compensated pruning, where meta-learning is involved to determine better\nsolutions. We show empirically that the proposed algorithm is superior to prior\nart in both effectiveness and efficiency. Specifically, we reduce the accuracy\ngap between the pruned and original networks from 0.9% to 0.7% with 8x\nreduction in time needed for meta-learning, i.e., from 1 hour down to 7\nminutes. To this end, we demonstrate the effectiveness of our algorithm using\nResNet and MobileNetV2 networks under CIFAR-10, ImageNet, and Bird-200\ndatasets. \n\n"}
{"id": "1810.00867", "contents": "Title: Domain-Adversarial Multi-Task Framework for Novel Therapeutic Property\n  Prediction of Compounds Abstract: With the rapid development of high-throughput technologies, parallel\nacquisition of large-scale drug-informatics data provides huge opportunities to\nimprove pharmaceutical research and development. One significant application is\nthe purpose prediction of small molecule compounds, aiming to specify\ntherapeutic properties of extensive purpose-unknown compounds and to repurpose\nnovel therapeutic properties of FDA-approved drugs. Such problem is very\nchallenging since compound attributes contain heterogeneous data with various\nfeature patterns such as drug fingerprint, drug physicochemical property, drug\nperturbation gene expression. Moreover, there is complex nonlinear dependency\namong heterogeneous data. In this paper, we propose a novel domain-adversarial\nmulti-task framework for integrating shared knowledge from multiple domains.\nThe framework utilizes the adversarial strategy to effectively learn target\nrepresentations and models their nonlinear dependency. Experiments on two\nreal-world datasets illustrate that the performance of our approach obtains an\nobvious improvement over competitive baselines. The novel therapeutic\nproperties of purpose-unknown compounds we predicted are mostly reported or\nbrought to the clinics. Furthermore, our framework can integrate various\nattributes beyond the three domains examined here and can be applied in the\nindustry for screening the purpose of huge amounts of as yet unidentified\ncompounds. Source codes of this paper are available on Github. \n\n"}
{"id": "1810.00924", "contents": "Title: Joint On-line Learning of a Zero-shot Spoken Semantic Parser and a\n  Reinforcement Learning Dialogue Manager Abstract: Despite many recent advances for the design of dialogue systems, a true\nbottleneck remains the acquisition of data required to train its components.\nUnlike many other language processing applications, dialogue systems require\ninteractions with users, therefore it is complex to develop them with\npre-recorded data. Building on previous works, on-line learning is pursued here\nas a most convenient way to address the issue. Data collection, annotation and\nuse in learning algorithms are performed in a single process. The main\ndifficulties are then: to bootstrap an initial basic system, and to control the\nlevel of additional cost on the user side. Considering that well-performing\nsolutions can be used directly off the shelf for speech recognition and\nsynthesis, the study is focused on learning the spoken language understanding\nand dialogue management modules only. Several variants of joint learning are\ninvestigated and tested with user trials to confirm that the overall on-line\nlearning can be obtained after only a few hundred training dialogues and can\noverstep an expert-based system. \n\n"}
{"id": "1810.01018", "contents": "Title: Simultaneously Optimizing Weight and Quantizer of Ternary Neural Network\n  using Truncated Gaussian Approximation Abstract: In the past years, Deep convolution neural network has achieved great success\nin many artificial intelligence applications. However, its enormous model size\nand massive computation cost have become the main obstacle for deployment of\nsuch powerful algorithm in the low power and resource-limited mobile systems.\nAs the countermeasure to this problem, deep neural networks with ternarized\nweights (i.e. -1, 0, +1) have been widely explored to greatly reduce the model\nsize and computational cost, with limited accuracy degradation. In this work,\nwe propose a novel ternarized neural network training method which\nsimultaneously optimizes both weights and quantizer during training,\ndifferentiating from prior works. Instead of fixed and uniform weight\nternarization, we are the first to incorporate the thresholds of weight\nternarization into a closed-form representation using the truncated Gaussian\napproximation, enabling simultaneous optimization of weights and quantizer\nthrough back-propagation training. With both of the first and last layer\nternarized, the experiments on the ImageNet classification task show that our\nternarized ResNet-18/34/50 only has 3.9/2.52/2.16% accuracy degradation in\ncomparison to the full-precision counterparts. \n\n"}
{"id": "1810.01075", "contents": "Title: Implicit Self-Regularization in Deep Neural Networks: Evidence from\n  Random Matrix Theory and Implications for Learning Abstract: Random Matrix Theory (RMT) is applied to analyze weight matrices of Deep\nNeural Networks (DNNs), including both production quality, pre-trained models\nsuch as AlexNet and Inception, and smaller models trained from scratch, such as\nLeNet5 and a miniature-AlexNet. Empirical and theoretical results clearly\nindicate that the DNN training process itself implicitly implements a form of\nSelf-Regularization. The empirical spectral density (ESD) of DNN layer matrices\ndisplays signatures of traditionally-regularized statistical models, even in\nthe absence of exogenously specifying traditional forms of explicit\nregularization. Building on relatively recent results in RMT, most notably its\nextension to Universality classes of Heavy-Tailed matrices, we develop a theory\nto identify 5+1 Phases of Training, corresponding to increasing amounts of\nImplicit Self-Regularization. These phases can be observed during the training\nprocess as well as in the final learned DNNs. For smaller and/or older DNNs,\nthis Implicit Self-Regularization is like traditional Tikhonov regularization,\nin that there is a \"size scale\" separating signal from noise. For\nstate-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed\nSelf-Regularization, similar to the self-organization seen in the statistical\nphysics of disordered systems. This results from correlations arising at all\nsize scales, which arises implicitly due to the training process itself. This\nimplicit Self-Regularization can depend strongly on the many knobs of the\ntraining process. By exploiting the generalization gap phenomena, we\ndemonstrate that we can cause a small model to exhibit all 5+1 phases of\ntraining simply by changing the batch size. This demonstrates that---all else\nbeing equal---DNN optimization with larger batch sizes leads to less-well\nimplicitly-regularized models, and it provides an explanation for the\ngeneralization gap phenomena. \n\n"}
{"id": "1810.01222", "contents": "Title: CEM-RL: Combining evolutionary and gradient-based methods for policy\n  search Abstract: Deep neuroevolution and deep reinforcement learning (deep RL) algorithms are\ntwo popular approaches to policy search. The former is widely applicable and\nrather stable, but suffers from low sample efficiency. By contrast, the latter\nis more sample efficient, but the most sample efficient variants are also\nrather unstable and highly sensitive to hyper-parameter setting. So far, these\nfamilies of methods have mostly been compared as competing tools. However, an\nemerging approach consists in combining them so as to get the best of both\nworlds. Two previously existing combinations use either an ad hoc evolutionary\nalgorithm or a goal exploration process together with the Deep Deterministic\nPolicy Gradient (DDPG) algorithm, a sample efficient off-policy deep RL\nalgorithm. In this paper, we propose a different combination scheme using the\nsimple cross-entropy method (CEM) and Twin Delayed Deep Deterministic policy\ngradient (td3), another off-policy deep RL algorithm which improves over ddpg.\nWe evaluate the resulting method, cem-rl, on a set of benchmarks classically\nused in deep RL. We show that cem-rl benefits from several advantages over its\ncompetitors and offers a satisfactory trade-off between performance and sample\nefficiency. \n\n"}
{"id": "1810.01367", "contents": "Title: FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative\n  Models Abstract: A promising class of generative models maps points from a simple distribution\nto a complex distribution through an invertible neural network.\nLikelihood-based training of these models requires restricting their\narchitectures to allow cheap computation of Jacobian determinants.\nAlternatively, the Jacobian trace can be used if the transformation is\nspecified by an ordinary differential equation. In this paper, we use\nHutchinson's trace estimator to give a scalable unbiased estimate of the\nlog-density. The result is a continuous-time invertible generative model with\nunbiased density estimation and one-pass sampling, while allowing unrestricted\nneural network architectures. We demonstrate our approach on high-dimensional\ndensity estimation, image generation, and variational inference, achieving the\nstate-of-the-art among exact likelihood methods with efficient sampling. \n\n"}
{"id": "1810.01392", "contents": "Title: WAIC, but Why? Generative Ensembles for Robust Anomaly Detection Abstract: Machine learning models encounter Out-of-Distribution (OoD) errors when the\ndata seen at test time are generated from a different stochastic generator than\nthe one used to generate the training data. One proposal to scale OoD detection\nto high-dimensional data is to learn a tractable likelihood approximation of\nthe training distribution, and use it to reject unlikely inputs. However,\nlikelihood models on natural data are themselves susceptible to OoD errors, and\neven assign large likelihoods to samples from other datasets. To mitigate this\nproblem, we propose Generative Ensembles, which robustify density-based OoD\ndetection by way of estimating epistemic uncertainty of the likelihood model.\nWe present a puzzling observation in need of an explanation -- although\nlikelihood measures cannot account for the typical set of a distribution, and\ntherefore should not be suitable on their own for OoD detection, WAIC performs\nsurprisingly well in practice. \n\n"}
{"id": "1810.01566", "contents": "Title: Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable\n  Objects, and Fluids Abstract: Real-life control tasks involve matters of various substances---rigid or soft\nbodies, liquid, gas---each with distinct physical behaviors. This poses\nchallenges to traditional rigid-body physics engines. Particle-based simulators\nhave been developed to model the dynamics of these complex scenes; however,\nrelying on approximation techniques, their simulation often deviates from\nreal-world physics, especially in the long term. In this paper, we propose to\nlearn a particle-based simulator for complex control tasks. Combining learning\nwith particle-based systems brings in two major benefits: first, the learned\nsimulator, just like other particle-based systems, acts widely on objects of\ndifferent materials; second, the particle-based representation poses strong\ninductive bias for learning: particles of the same type have the same dynamics\nwithin. This enables the model to quickly adapt to new environments of unknown\ndynamics within a few observations. We demonstrate robots achieving complex\nmanipulation tasks using the learned simulator, such as manipulating fluids and\ndeformable foam, with experiments both in simulation and in the real world. Our\nstudy helps lay the foundation for robot learning of dynamic scenes with\nparticle-based representations. \n\n"}
{"id": "1810.02215", "contents": "Title: XBART: Accelerated Bayesian Additive Regression Trees Abstract: Bayesian additive regression trees (BART) (Chipman et. al., 2010) is a\npowerful predictive model that often outperforms alternative models at\nout-of-sample prediction. BART is especially well-suited to settings with\nunstructured predictor variables and substantial sources of unmeasured\nvariation as is typical in the social, behavioral and health sciences. This\npaper develops a modified version of BART that is amenable to fast posterior\nestimation. We present a stochastic hill climbing algorithm that matches the\nremarkable predictive accuracy of previous BART implementations, but is many\ntimes faster and less memory intensive. Simulation studies show that the new\nmethod is comparable in computation time and more accurate at function\nestimation than both random forests and gradient boosting. \n\n"}
{"id": "1810.02966", "contents": "Title: Understanding Recurrent Neural Architectures by Analyzing and\n  Synthesizing Long Distance Dependencies in Benchmark Sequential Datasets Abstract: In order to build efficient deep recurrent neural architectures, it is\nessential to analyze the complexityof long distance dependencies (LDDs) of the\ndataset being modeled. In this paper, we presentdetailed analysis of the\ndependency decay curve exhibited by various datasets. The datasets sampledfrom\na similar process (e.g. natural language, sequential MNIST, Strictlyk-Piecewise\nlanguages,etc) display variations in the properties of the dependency decay\ncurve. Our analysis reveal thefactors resulting in these variations; such as\n(i) number of unique symbols in a dataset, (ii) size ofthe dataset, (iii)\nnumber of interacting symbols within a given LDD, and (iv) the distance\nbetweenthe interacting symbols. We test these factors by generating synthesized\ndatasets of the Strictlyk-Piecewise languages. Another advantage of these\nsynthesized datasets is that they enable targetedtesting of deep recurrent\nneural architectures in terms of their ability to model LDDs with\ndifferentcharacteristics. We also demonstrate that analysing dependency decay\ncurves can inform the selectionof optimal hyper-parameters for SOTA deep\nrecurrent neural architectures. This analysis can directlycontribute to the\ndevelopment of more accurate and efficient sequential models. \n\n"}
{"id": "1810.03442", "contents": "Title: Towards the Latent Transcriptome Abstract: In this work we propose a method to compute continuous embeddings for kmers\nfrom raw RNA-seq data, without the need for alignment to a reference genome.\nThe approach uses an RNN to transform kmers of the RNA-seq reads into a 2\ndimensional representation that is used to predict abundance of each kmer. We\nreport that our model captures information of both DNA sequence similarity as\nwell as DNA sequence abundance in the embedding latent space, that we call the\nLatent Transcriptome. We confirm the quality of these vectors by comparing them\nto known gene sub-structures and report that the latent space recovers exon\ninformation from raw RNA-Seq data from acute myeloid leukemia patients.\nFurthermore we show that this latent space allows the detection of genomic\nabnormalities such as translocations as well as patient-specific mutations,\nmaking this representation space both useful for visualization as well as\nanalysis. \n\n"}
{"id": "1810.03487", "contents": "Title: Security Analysis of Deep Neural Networks Operating in the Presence of\n  Cache Side-Channel Attacks Abstract: Recent work has introduced attacks that extract the architecture information\nof deep neural networks (DNN), as this knowledge enhances an adversary's\ncapability to conduct black-box attacks against the model. This paper presents\nthe first in-depth security analysis of DNN fingerprinting attacks that exploit\ncache side-channels. First, we define the threat model for these attacks: our\nadversary does not need the ability to query the victim model; instead, she\nruns a co-located process on the host machine victim's deep learning (DL)\nsystem is running and passively monitors the accesses of the target functions\nin the shared framework. Second, we introduce DeepRecon, an attack that\nreconstructs the architecture of the victim network by using the internal\ninformation extracted via Flush+Reload, a cache side-channel technique. Once\nthe attacker observes function invocations that map directly to architecture\nattributes of the victim network, the attacker can reconstruct the victim's\nentire network architecture. In our evaluation, we demonstrate that an attacker\ncan accurately reconstruct two complex networks (VGG19 and ResNet50) having\nobserved only one forward propagation. Based on the extracted architecture\nattributes, we also demonstrate that an attacker can build a meta-model that\naccurately fingerprints the architecture and family of the pre-trained model in\na transfer learning setting. From this meta-model, we evaluate the importance\nof the observed attributes in the fingerprinting process. Third, we propose and\nevaluate new framework-level defense techniques that obfuscate our attacker's\nobservations. Our empirical security analysis represents a step toward\nunderstanding the DNNs' vulnerability to cache side-channel attacks. \n\n"}
{"id": "1810.03522", "contents": "Title: NSGA-Net: Neural Architecture Search using Multi-Objective Genetic\n  Algorithm Abstract: This paper introduces NSGA-Net -- an evolutionary approach for neural\narchitecture search (NAS). NSGA-Net is designed with three goals in mind: (1) a\nprocedure considering multiple and conflicting objectives, (2) an efficient\nprocedure balancing exploration and exploitation of the space of potential\nneural network architectures, and (3) a procedure finding a diverse set of\ntrade-off network architectures achieved in a single run. NSGA-Net is a\npopulation-based search algorithm that explores a space of potential neural\nnetwork architectures in three steps, namely, a population initialization step\nthat is based on prior-knowledge from hand-crafted architectures, an\nexploration step comprising crossover and mutation of architectures, and\nfinally an exploitation step that utilizes the hidden useful knowledge stored\nin the entire history of evaluated neural architectures in the form of a\nBayesian Network. Experimental results suggest that combining the dual\nobjectives of minimizing an error metric and computational complexity, as\nmeasured by FLOPs, allows NSGA-Net to find competitive neural architectures.\nMoreover, NSGA-Net achieves error rate on the CIFAR-10 dataset on par with\nother state-of-the-art NAS methods while using orders of magnitude less\ncomputational resources. These results are encouraging and shows the promise to\nfurther use of EC methods in various deep-learning paradigms. \n\n"}
{"id": "1810.03587", "contents": "Title: Algorithmic Aspects of Inverse Problems Using Generative Models Abstract: The traditional approach of hand-crafting priors (such as sparsity) for\nsolving inverse problems is slowly being replaced by the use of richer learned\npriors (such as those modeled by generative adversarial networks, or GANs). In\nthis work, we study the algorithmic aspects of such a learning-based approach\nfrom a theoretical perspective. For certain generative network architectures,\nwe establish a simple non-convex algorithmic approach that (a) theoretically\nenjoys linear convergence guarantees for certain inverse problems, and (b)\nempirically improves upon conventional techniques such as back-propagation. We\nalso propose an extension of our approach that can handle model mismatch (i.e.,\nsituations where the generative network prior is not exactly applicable.)\nTogether, our contributions serve as building blocks towards a more complete\nalgorithmic understanding of generative models in inverse problems. \n\n"}
{"id": "1810.03814", "contents": "Title: SNAP: A semismooth Newton algorithm for pathwise optimization with\n  optimal local convergence rate and oracle properties Abstract: We propose a semismooth Newton algorithm for pathwise optimization (SNAP) for\nthe LASSO and Enet in sparse, high-dimensional linear regression. SNAP is\nderived from a suitable formulation of the KKT conditions based on Newton\nderivatives. It solves the semismooth KKT equations efficiently by actively and\ncontinuously seeking the support of the regression coefficients along the\nsolution path with warm start. At each knot in the path, SNAP converges locally\nsuperlinearly for the Enet criterion and achieves an optimal local convergence\nrate for the LASSO criterion, i.e., SNAP converges in one step at the cost of\ntwo matrix-vector multiplication per iteration. Under certain regularity\nconditions on the design matrix and the minimum magnitude of the nonzero\nelements of the target regression coefficients, we show that SNAP hits a\nsolution with the same signs as the regression coefficients and achieves a\nsharp estimation error bound in finite steps with high probability. The\ncomputational complexity of SNAP is shown to be the same as that of LARS and\ncoordinate descent algorithms per iteration. Simulation studies and real data\nanalysis support our theoretical results and demonstrate that SNAP is faster\nand accurate than LARS and coordinate descent algorithms. \n\n"}
{"id": "1810.03817", "contents": "Title: Learning Bounds for Greedy Approximation with Explicit Feature Maps from\n  Multiple Kernels Abstract: Nonlinear kernels can be approximated using finite-dimensional feature maps\nfor efficient risk minimization. Due to the inherent trade-off between the\ndimension of the (mapped) feature space and the approximation accuracy, the key\nproblem is to identify promising (explicit) features leading to a satisfactory\nout-of-sample performance. In this work, we tackle this problem by efficiently\nchoosing such features from multiple kernels in a greedy fashion. Our method\nsequentially selects these explicit features from a set of candidate features\nusing a correlation metric. We establish an out-of-sample error bound capturing\nthe trade-off between the error in terms of explicit features (approximation\nerror) and the error due to spectral properties of the best model in the\nHilbert space associated to the combined kernel (spectral error). The result\nverifies that when the (best) underlying data model is sparse enough, i.e., the\nspectral error is negligible, one can control the test error with a small\nnumber of explicit features, that can scale poly-logarithmically with data. Our\nempirical results show that given a fixed number of explicit features, the\nmethod can achieve a lower test error with a smaller time cost, compared to the\nstate-of-the-art in data-dependent random features. \n\n"}
{"id": "1810.04020", "contents": "Title: A Comprehensive Survey of Deep Learning for Image Captioning Abstract: Generating a description of an image is called image captioning. Image\ncaptioning requires to recognize the important objects, their attributes and\ntheir relationships in an image. It also needs to generate syntactically and\nsemantically correct sentences. Deep learning-based techniques are capable of\nhandling the complexities and challenges of image captioning. In this survey\npaper, we aim to present a comprehensive review of existing deep learning-based\nimage captioning techniques. We discuss the foundation of the techniques to\nanalyze their performances, strengths and limitations. We also discuss the\ndatasets and the evaluation metrics popularly used in deep learning based\nautomatic image captioning. \n\n"}
{"id": "1810.04021", "contents": "Title: Deep Geodesic Learning for Segmentation and Anatomical Landmarking Abstract: In this paper, we propose a novel deep learning framework for anatomy\nsegmentation and automatic landmark- ing. Specifically, we focus on the\nchallenging problem of mandible segmentation from cone-beam computed tomography\n(CBCT) scans and identification of 9 anatomical landmarks of the mandible on\nthe geodesic space. The overall approach employs three inter-related steps. In\nstep 1, we propose a deep neu- ral network architecture with carefully designed\nregularization, and network hyper-parameters to perform image segmentation\nwithout the need for data augmentation and complex post- processing refinement.\nIn step 2, we formulate the landmark localization problem directly on the\ngeodesic space for sparsely- spaced anatomical landmarks. In step 3, we propose\nto use a long short-term memory (LSTM) network to identify closely- spaced\nlandmarks, which is rather difficult to obtain using other standard detection\nnetworks. The proposed fully automated method showed superior efficacy compared\nto the state-of-the- art mandible segmentation and landmarking approaches in\ncraniofacial anomalies and diseased states. We used a very challenging CBCT\ndataset of 50 patients with a high-degree of craniomaxillofacial (CMF)\nvariability that is realistic in clinical practice. Complementary to the\nquantitative analysis, the qualitative visual inspection was conducted for\ndistinct CBCT scans from 250 patients with high anatomical variability. We have\nalso shown feasibility of the proposed work in an independent dataset from\nMICCAI Head-Neck Challenge (2015) achieving the state-of-the-art performance.\nLastly, we present an in-depth analysis of the proposed deep networks with\nrespect to the choice of hyper-parameters such as pooling and activation\nfunctions. \n\n"}
{"id": "1810.04115", "contents": "Title: The infinite Viterbi alignment and decay-convexity Abstract: The infinite Viterbi alignment is the limiting maximum a-posteriori estimate\nof the unobserved path in a hidden Markov model as the length of the time\nhorizon grows. For models on state-space $\\mathbb{R}^{d}$ satisfying a new\n``decay-convexity'' condition, we develop an approach to existence of the\ninfinite Viterbi alignment in an infinite dimensional Hilbert space.\nQuantitative bounds on the distance to the infinite Viterbi alignment, which\nare the first of their kind, are derived and used to illustrate how approximate\nestimation via parallelization can be accurate and scaleable to\nhigh-dimensional problems because the rate of convergence to the infinite\nViterbi alignment does not necessarily depend on $d$. The results are applied\nto approximate estimation via parallelization and a model of neural population\nactivity. \n\n"}
{"id": "1810.04650", "contents": "Title: Multi-Task Learning as Multi-Objective Optimization Abstract: In multi-task learning, multiple tasks are solved jointly, sharing inductive\nbias between them. Multi-task learning is inherently a multi-objective problem\nbecause different tasks may conflict, necessitating a trade-off. A common\ncompromise is to optimize a proxy objective that minimizes a weighted linear\ncombination of per-task losses. However, this workaround is only valid when the\ntasks do not compete, which is rarely the case. In this paper, we explicitly\ncast multi-task learning as multi-objective optimization, with the overall\nobjective of finding a Pareto optimal solution. To this end, we use algorithms\ndeveloped in the gradient-based multi-objective optimization literature. These\nalgorithms are not directly applicable to large-scale learning problems since\nthey scale poorly with the dimensionality of the gradients and the number of\ntasks. We therefore propose an upper bound for the multi-objective loss and\nshow that it can be optimized efficiently. We further prove that optimizing\nthis upper bound yields a Pareto optimal solution under realistic assumptions.\nWe apply our method to a variety of multi-task deep learning problems including\ndigit classification, scene understanding (joint semantic segmentation,\ninstance segmentation, and depth estimation), and multi-label classification.\nOur method produces higher-performing models than recent multi-task learning\nformulations or per-task training. \n\n"}
{"id": "1810.04719", "contents": "Title: Fully Supervised Speaker Diarization Abstract: In this paper, we propose a fully supervised speaker diarization approach,\nnamed unbounded interleaved-state recurrent neural networks (UIS-RNN). Given\nextracted speaker-discriminative embeddings (a.k.a. d-vectors) from input\nutterances, each individual speaker is modeled by a parameter-sharing RNN,\nwhile the RNN states for different speakers interleave in the time domain. This\nRNN is naturally integrated with a distance-dependent Chinese restaurant\nprocess (ddCRP) to accommodate an unknown number of speakers. Our system is\nfully supervised and is able to learn from examples where time-stamped speaker\nlabels are annotated. We achieved a 7.6% diarization error rate on NIST SRE\n2000 CALLHOME, which is better than the state-of-the-art method using spectral\nclustering. Moreover, our method decodes in an online fashion while most\nstate-of-the-art systems rely on offline clustering. \n\n"}
{"id": "1810.04754", "contents": "Title: Efficient Tensor Decomposition with Boolean Factors Abstract: Tensor decomposition has been extensively used as a tool for exploratory\nanalysis. Motivated by neuroscience applications, we study tensor decomposition\nwith Boolean factors. The resulting optimization problem is challenging due to\nthe non-convex objective and the combinatorial constraints. We propose Binary\nMatching Pursuit (BMP), a novel generalization of the matching pursuit strategy\nto decompose the tensor efficiently. BMP iteratively searches for atoms in a\ngreedy fashion. The greedy atom search step is solved efficiently via a\nMAXCUT-like boolean quadratic program. We prove that BMP is guaranteed to\nconverge sublinearly to the optimal solution and recover the factors under mild\nidentifiability conditions. Experiments demonstrate the superior performance of\nour method over baselines on synthetic and real datasets. We also showcase the\napplication of BMP in quantifying neural interactions underlying\nhigh-resolution spatiotemporal ECoG recordings. \n\n"}
{"id": "1810.04805", "contents": "Title: BERT: Pre-training of Deep Bidirectional Transformers for Language\n  Understanding Abstract: We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement). \n\n"}
{"id": "1810.04920", "contents": "Title: Pairwise Augmented GANs with Adversarial Reconstruction Loss Abstract: We propose a novel autoencoding model called Pairwise Augmented GANs. We\ntrain a generator and an encoder jointly and in an adversarial manner. The\ngenerator network learns to sample realistic objects. In turn, the encoder\nnetwork at the same time is trained to map the true data distribution to the\nprior in latent space. To ensure good reconstructions, we introduce an\naugmented adversarial reconstruction loss. Here we train a discriminator to\ndistinguish two types of pairs: an object with its augmentation and the one\nwith its reconstruction. We show that such adversarial loss compares objects\nbased on the content rather than on the exact match. We experimentally\ndemonstrate that our model generates samples and reconstructions of quality\ncompetitive with state-of-the-art on datasets MNIST, CIFAR10, CelebA and\nachieves good quantitative results on CIFAR10. \n\n"}
{"id": "1810.05157", "contents": "Title: Learning under Misspecified Objective Spaces Abstract: Learning robot objective functions from human input has become increasingly\nimportant, but state-of-the-art techniques assume that the human's desired\nobjective lies within the robot's hypothesis space. When this is not true, even\nmethods that keep track of uncertainty over the objective fail because they\nreason about which hypothesis might be correct, and not whether any of the\nhypotheses are correct. We focus specifically on learning from physical human\ncorrections during the robot's task execution, where not having a rich enough\nhypothesis space leads to the robot updating its objective in ways that the\nperson did not actually intend. We observe that such corrections appear\nirrelevant to the robot, because they are not the best way of achieving any of\nthe candidate objectives. Instead of naively trusting and learning from every\nhuman interaction, we propose robots learn conservatively by reasoning in real\ntime about how relevant the human's correction is for the robot's hypothesis\nspace. We test our inference method in an experiment with human interaction\ndata, and demonstrate that this alleviates unintended learning in an in-person\nuser study with a 7DoF robot manipulator. \n\n"}
{"id": "1810.05188", "contents": "Title: Fighting Contextual Bandits with Stochastic Smoothing Abstract: We introduce a new stochastic smoothing perspective to study adversarial\ncontextual bandit problems. We propose a general algorithm template that\nrepresents random perturbation based algorithms and identify several\nperturbation distributions that lead to strong regret bounds. Using the idea of\nsmoothness, we provide an $O(\\sqrt{T})$ zero-order bound for the vanilla\nalgorithm and an $O(L^{*2/3}_{T})$ first-order bound for the clipped version.\nThese bounds hold when the algorithms use with a variety of distributions that\nhave a bounded hazard rate. Our algorithm template includes EXP4 as a special\ncase corresponding to the Gumbel perturbation. Our regret bounds match existing\nresults for EXP4 without relying on the specific properties of the algorithm. \n\n"}
{"id": "1810.05246", "contents": "Title: Piano Genie Abstract: We present Piano Genie, an intelligent controller which allows non-musicians\nto improvise on the piano. With Piano Genie, a user performs on a simple\ninterface with eight buttons, and their performance is decoded into the space\nof plausible piano music in real time. To learn a suitable mapping procedure\nfor this problem, we train recurrent neural network autoencoders with discrete\nbottlenecks: an encoder learns an appropriate sequence of buttons corresponding\nto a piano piece, and a decoder learns to map this sequence back to the\noriginal piece. During performance, we substitute a user's input for the\nencoder output, and play the decoder's prediction each time the user presses a\nbutton. To improve the intuitiveness of Piano Genie's performance behavior, we\nimpose musically meaningful constraints over the encoder's outputs. \n\n"}
{"id": "1810.05270", "contents": "Title: Rethinking the Value of Network Pruning Abstract: Network pruning is widely used for reducing the heavy inference cost of deep\nmodels in low-resource settings. A typical pruning algorithm is a three-stage\npipeline, i.e., training (a large model), pruning and fine-tuning. During\npruning, according to a certain criterion, redundant weights are pruned and\nimportant weights are kept to best preserve the accuracy. In this work, we make\nseveral surprising observations which contradict common beliefs. For all\nstate-of-the-art structured pruning algorithms we examined, fine-tuning a\npruned model only gives comparable or worse performance than training that\nmodel with randomly initialized weights. For pruning algorithms which assume a\npredefined target network architecture, one can get rid of the full pipeline\nand directly train the target network from scratch. Our observations are\nconsistent for multiple network architectures, datasets, and tasks, which imply\nthat: 1) training a large, over-parameterized model is often not necessary to\nobtain an efficient final model, 2) learned \"important\" weights of the large\nmodel are typically not useful for the small pruned model, 3) the pruned\narchitecture itself, rather than a set of inherited \"important\" weights, is\nmore crucial to the efficiency in the final model, which suggests that in some\ncases pruning can be useful as an architecture search paradigm. Our results\nsuggest the need for more careful baseline evaluations in future research on\nstructured pruning methods. We also compare with the \"Lottery Ticket\nHypothesis\" (Frankle & Carbin 2019), and find that with optimal learning rate,\nthe \"winning ticket\" initialization as used in Frankle & Carbin (2019) does not\nbring improvement over random initialization. \n\n"}
{"id": "1810.05731", "contents": "Title: Image Super-Resolution Using VDSR-ResNeXt and SRCGAN Abstract: Over the past decade, many Super Resolution techniques have been developed\nusing deep learning. Among those, generative adversarial networks (GAN) and\nvery deep convolutional networks (VDSR) have shown promising results in terms\nof HR image quality and computational speed. In this paper, we propose two\napproaches based on these two algorithms: VDSR-ResNeXt, which is a deep\nmulti-branch convolutional network inspired by VDSR and ResNeXt; and SRCGAN,\nwhich is a conditional GAN that explicitly passes class labels as input to the\nGAN. The two methods were implemented on common SR benchmark datasets for both\nquantitative and qualitative assessment. \n\n"}
{"id": "1810.06118", "contents": "Title: Learning to fail: Predicting fracture evolution in brittle material\n  models using recurrent graph convolutional neural networks Abstract: We propose a machine learning approach to address a key challenge in\nmaterials science: predicting how fractures propagate in brittle materials\nunder stress, and how these materials ultimately fail. Our methods use deep\nlearning and train on simulation data from high-fidelity models, emulating the\nresults of these models while avoiding the overwhelming computational demands\nassociated with running a statistically significant sample of simulations. We\nemploy a graph convolutional network that recognizes features of the fracturing\nmaterial and a recurrent neural network that models the evolution of these\nfeatures, along with a novel form of data augmentation that compensates for the\nmodest size of our training data. We simultaneously generate predictions for\nqualitatively distinct material properties. Results on fracture damage and\nlength are within 3% of their simulated values, and results on time to material\nfailure, which is notoriously difficult to predict even with high-fidelity\nmodels, are within approximately 15% of simulated values. Once trained, our\nneural networks generate predictions within seconds, rather than the hours\nneeded to run a single simulation. \n\n"}
{"id": "1810.06207", "contents": "Title: Robust descent using smoothed multiplicative noise Abstract: To improve the off-sample generalization of classical procedures minimizing\nthe empirical risk under potentially heavy-tailed data, new robust learning\nalgorithms have been proposed in recent years, with generalized median-of-means\nstrategies being particularly salient. These procedures enjoy performance\nguarantees in the form of sharp risk bounds under weak moment assumptions on\nthe underlying loss, but typically suffer from a large computational overhead\nand substantial bias when the data happens to be sub-Gaussian, limiting their\nutility. In this work, we propose a novel robust gradient descent procedure\nwhich makes use of a smoothed multiplicative noise applied directly to\nobservations before constructing a sum of soft-truncated gradient coordinates.\nWe show that the procedure has competitive theoretical guarantees, with the\nmajor advantage of a simple implementation that does not require an iterative\nsub-routine for robustification. Empirical tests reinforce the theory, showing\nmore efficient generalization over a much wider class of data distributions. \n\n"}
{"id": "1810.06665", "contents": "Title: Stop Illegal Comments: A Multi-Task Deep Learning Approach Abstract: Deep learning methods are often difficult to apply in the legal domain due to\nthe large amount of labeled data required by deep learning methods. A recent\nnew trend in the deep learning community is the application of multi-task\nmodels that enable single deep neural networks to perform more than one task at\nthe same time, for example classification and translation tasks. These powerful\nnovel models are capable of transferring knowledge among different tasks or\ntraining sets and therefore could open up the legal domain for many deep\nlearning applications. In this paper, we investigate the transfer learning\ncapabilities of such a multi-task model on a classification task on the\npublicly available Kaggle toxic comment dataset for classifying illegal\ncomments and we can report promising results. \n\n"}
{"id": "1810.06758", "contents": "Title: Discriminator Rejection Sampling Abstract: We propose a rejection sampling scheme using the discriminator of a GAN to\napproximately correct errors in the GAN generator distribution. We show that\nunder quite strict assumptions, this will allow us to recover the data\ndistribution exactly. We then examine where those strict assumptions break down\nand design a practical algorithm - called Discriminator Rejection Sampling\n(DRS) - that can be used on real data-sets. Finally, we demonstrate the\nefficacy of DRS on a mixture of Gaussians and on the SAGAN model,\nstate-of-the-art in the image generation task at the time of developing this\nwork. On ImageNet, we train an improved baseline that increases the Inception\nScore from 52.52 to 62.36 and reduces the Frechet Inception Distance from 18.65\nto 14.79. We then use DRS to further improve on this baseline, improving the\nInception Score to 76.08 and the FID to 13.75. \n\n"}
{"id": "1810.06784", "contents": "Title: ProMP: Proximal Meta-Policy Search Abstract: Credit assignment in Meta-reinforcement learning (Meta-RL) is still poorly\nunderstood. Existing methods either neglect credit assignment to pre-adaptation\nbehavior or implement it naively. This leads to poor sample-efficiency during\nmeta-training as well as ineffective task identification strategies. This paper\nprovides a theoretical analysis of credit assignment in gradient-based Meta-RL.\nBuilding on the gained insights we develop a novel meta-learning algorithm that\novercomes both the issue of poor credit assignment and previous difficulties in\nestimating meta-policy gradients. By controlling the statistical distance of\nboth pre-adaptation and adapted policies during meta-policy search, the\nproposed algorithm endows efficient and stable meta-learning. Our approach\nleads to superior pre-adaptation policy behavior and consistently outperforms\nprevious Meta-RL algorithms in sample-efficiency, wall-clock time, and\nasymptotic performance. \n\n"}
{"id": "1810.06870", "contents": "Title: A Roadmap Towards Resilient Internet of Things for Cyber-Physical\n  Systems Abstract: The Internet of Things (IoT) is a ubiquitous system connecting many different\ndevices - the things - which can be accessed from the distance. The\ncyber-physical systems (CPS) monitor and control the things from the distance.\nAs a result, the concepts of dependability and security get deeply intertwined.\nThe increasing level of dynamicity, heterogeneity, and complexity adds to the\nsystem's vulnerability, and challenges its ability to react to faults. This\npaper summarizes state-of-the-art of existing work on anomaly detection,\nfault-tolerance and self-healing, and adds a number of other methods applicable\nto achieve resilience in an IoT. We particularly focus on non-intrusive methods\nensuring data integrity in the network. Furthermore, this paper presents the\nmain challenges in building a resilient IoT for CPS which is crucial in the era\nof smart CPS with enhanced connectivity (an excellent example of such a system\nis connected autonomous vehicles). It further summarizes our solutions,\nwork-in-progress and future work to this topic to enable \"Trustworthy IoT for\nCPS\". Finally, this framework is illustrated on a selected use case: A smart\nsensor infrastructure in the transport domain. \n\n"}
{"id": "1810.07242", "contents": "Title: Adversarial Attacks on Cognitive Self-Organizing Networks: The Challenge\n  and the Way Forward Abstract: Future communications and data networks are expected to be largely cognitive\nself-organizing networks (CSON). Such networks will have the essential property\nof cognitive self-organization, which can be achieved using machine learning\ntechniques (e.g., deep learning). Despite the potential of these techniques,\nthese techniques in their current form are vulnerable to adversarial attacks\nthat can cause cascaded damages with detrimental consequences for the whole\nnetwork. In this paper, we explore the effect of adversarial attacks on CSON.\nOur experiments highlight the level of threat that CSON have to deal with in\norder to meet the challenges of next-generation networks and point out\npromising directions for future work. \n\n"}
{"id": "1810.07320", "contents": "Title: Exploring Sentence Vector Spaces through Automatic Summarization Abstract: Given vector representations for individual words, it is necessary to compute\nvector representations of sentences for many applications in a compositional\nmanner, often using artificial neural networks.\n  Relatively little work has explored the internal structure and properties of\nsuch sentence vectors. In this paper, we explore the properties of sentence\nvectors in the context of automatic summarization. In particular, we show that\ncosine similarity between sentence vectors and document vectors is strongly\ncorrelated with sentence importance and that vector semantics can identify and\ncorrect gaps between the sentences chosen so far and the document. In addition,\nwe identify specific dimensions which are linked to effective summaries. To our\nknowledge, this is the first time specific dimensions of sentence embeddings\nhave been connected to sentence properties. We also compare the features of\ndifferent methods of sentence embeddings. Many of these insights have\napplications in uses of sentence embeddings far beyond summarization. \n\n"}
{"id": "1810.07322", "contents": "Title: Functionality-Oriented Convolutional Filter Pruning Abstract: The sophisticated structure of Convolutional Neural Network (CNN) allows for\noutstanding performance, but at the cost of intensive computation. As\nsignificant redundancies inevitably present in such a structure, many works\nhave been proposed to prune the convolutional filters for computation cost\nreduction. Although extremely effective, most works are based only on\nquantitative characteristics of the convolutional filters, and highly overlook\nthe qualitative interpretation of individual filter's specific functionality.\nIn this work, we interpreted the functionality and redundancy of the\nconvolutional filters from different perspectives, and proposed a\nfunctionality-oriented filter pruning method. With extensive experiment\nresults, we proved the convolutional filters' qualitative significance\nregardless of magnitude, demonstrated significant neural network redundancy due\nto repetitive filter functions, and analyzed the filter functionality defection\nunder inappropriate retraining process. Such an interpretable pruning approach\nnot only offers outstanding computation cost optimization over previous filter\npruning methods, but also interprets filter pruning process. \n\n"}
{"id": "1810.07590", "contents": "Title: Graphical Convergence of Subgradients in Nonconvex Optimization and\n  Learning Abstract: We investigate the stochastic optimization problem of minimizing population\nrisk, where the loss defining the risk is assumed to be weakly convex.\nCompositions of Lipschitz convex functions with smooth maps are the primary\nexamples of such losses. We analyze the estimation quality of such nonsmooth\nand nonconvex problems by their sample average approximations. Our main results\nestablish dimension-dependent rates on subgradient estimation in full\ngenerality and dimension-independent rates when the loss is a generalized\nlinear model. As an application of the developed techniques, we analyze the\nnonsmooth landscape of a robust nonlinear regression problem. \n\n"}
{"id": "1810.08217", "contents": "Title: Deep Learning Methods for Reynolds-Averaged Navier-Stokes Simulations of\n  Airfoil Flows Abstract: With this study we investigate the accuracy of deep learning models for the\ninference of Reynolds-Averaged Navier-Stokes solutions. We focus on a\nmodernized U-net architecture, and evaluate a large number of trained neural\nnetworks with respect to their accuracy for the calculation of pressure and\nvelocity distributions. In particular, we illustrate how training data size and\nthe number of weights influence the accuracy of the solutions. With our best\nmodels we arrive at a mean relative pressure and velocity error of less than 3%\nacross a range of previously unseen airfoil shapes. In addition all source code\nis publicly available in order to ensure reproducibility and to provide a\nstarting point for researchers interested in deep learning methods for physics\nproblems. While this work focuses on RANS solutions, the neural network\narchitecture and learning setup are very generic, and applicable to a wide\nrange of PDE boundary value problems on Cartesian grids. \n\n"}
{"id": "1810.08332", "contents": "Title: Zero and Few Shot Learning with Semantic Feature Synthesis and\n  Competitive Learning Abstract: Zero-shot learning (ZSL) is made possible by learning a projection function\nbetween a feature space and a semantic space (e.g.,~an attribute space). Key to\nZSL is thus to learn a projection that is robust against the often large domain\ngap between the seen and unseen class domains. In this work, this is achieved\nby unseen class data synthesis and robust projection function learning.\nSpecifically, a novel semantic data synthesis strategy is proposed, by which\nsemantic class prototypes (e.g., attribute vectors) are used to simply perturb\nseen class data for generating unseen class ones. As in any data\nsynthesis/hallucination approach, there are ambiguities and uncertainties on\nhow well the synthesised data can capture the targeted unseen class data\ndistribution. To cope with this, the second contribution of this work is a\nnovel projection learning model termed competitive bidirectional projection\nlearning (BPL) designed to best utilise the ambiguous synthesised data.\nSpecifically, we assume that each synthesised data point can belong to any\nunseen class; and the most likely two class candidates are exploited to learn a\nrobust projection function in a competitive fashion. As a third contribution,\nwe show that the proposed ZSL model can be easily extended to few-shot learning\n(FSL) by again exploiting semantic (class prototype guided) feature synthesis\nand competitive BPL. Extensive experiments show that our model achieves the\nstate-of-the-art results on both problems. \n\n"}
{"id": "1810.08452", "contents": "Title: Multitask Learning for Large-scale Semantic Change Detection Abstract: Change detection is one of the main problems in remote sensing, and is\nessential to the accurate processing and understanding of the large scale Earth\nobservation data available through programs such as Sentinel and Landsat. Most\nof the recently proposed change detection methods bring deep learning to this\ncontext, but openly available change detection datasets are still very scarce,\nwhich limits the methods that can be proposed and tested. In this paper we\npresent the first large scale high resolution semantic change detection (HRSCD)\ndataset, which enables the usage of deep learning methods for semantic change\ndetection. The dataset contains coregistered RGB image pairs, pixel-wise change\ninformation and land cover information. We then propose several methods using\nfully convolutional neural networks to perform semantic change detection. Most\nnotably, we present a network architecture that performs change detection and\nland cover mapping simultaneously, while using the predicted land cover\ninformation to help to predict changes. We also describe a sequential training\nscheme that allows this network to be trained without setting a hyperparameter\nthat balances different loss functions and achieves the best overall results. \n\n"}
{"id": "1810.08498", "contents": "Title: Network Classification Based Structural Analysis of Real Networks and\n  their Model-Generated Counterparts Abstract: Data-driven analysis of complex networks has been in the focus of research\nfor decades. An important area of research is to study how well real networks\ncan be described with a small selection of metrics, furthermore how well\nnetwork models can capture the relations between graph metrics observed in real\nnetworks. In this paper, we apply machine learning techniques to investigate\nthe aforementioned problems. We study 500 real-world networks along with 2,000\nsynthetic networks generated by four frequently used network models with\npreviously calibrated parameters to make the generated graphs as similar to the\nreal networks as possible. This paper unifies several branches of data-driven\ncomplex network analysis, such as the study of graph metrics and their\npair-wise relationships, network similarity estimation, model calibration, and\ngraph classification. We find that the correlation profiles of the structural\nmeasures significantly differ across network domains and the domain can be\nefficiently determined using a small selection of graph metrics. The structural\nproperties of the network models with fixed parameters are robust enough to\nperform parameter calibration. The goodness-of-fit of the network models highly\ndepends on the network domain. By solving classification problems, we find that\nthe models lack the capability of generating a graph with a high clustering\ncoefficient and relatively large diameter simultaneously. On the other hand,\nmodels are able to capture exactly the degree-distribution-related metrics. \n\n"}
{"id": "1810.09136", "contents": "Title: Do Deep Generative Models Know What They Don't Know? Abstract: A neural network deployed in the wild may be asked to make predictions for\ninputs that were drawn from a different distribution than that of the training\ndata. A plethora of work has demonstrated that it is easy to find or synthesize\ninputs for which a neural network is highly confident yet wrong. Generative\nmodels are widely viewed to be robust to such mistaken confidence as modeling\nthe density of the input features can be used to detect novel,\nout-of-distribution inputs. In this paper we challenge this assumption. We find\nthat the density learned by flow-based models, VAEs, and PixelCNNs cannot\ndistinguish images of common objects such as dogs, trucks, and horses (i.e.\nCIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher\nlikelihood to the latter when the model is trained on the former. Moreover, we\nfind evidence of this phenomenon when pairing several popular image data sets:\nFashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.\nTo investigate this curious behavior, we focus analysis on flow-based\ngenerative models in particular since they are trained and evaluated via the\nexact marginal likelihood. We find such behavior persists even when we restrict\nthe flows to constant-volume transformations. These transformations admit some\ntheoretical analysis, and we show that the difference in likelihoods can be\nexplained by the location and variances of the data and the model curvature.\nOur results caution against using the density estimates from deep generative\nmodels to identify inputs similar to the training distribution until their\nbehavior for out-of-distribution inputs is better understood. \n\n"}
{"id": "1810.09717", "contents": "Title: Ain't Nobody Got Time For Coding: Structure-Aware Program Synthesis From\n  Natural Language Abstract: Program synthesis from natural language (NL) is practical for humans and,\nonce technically feasible, would significantly facilitate software development\nand revolutionize end-user programming. We present SAPS, an end-to-end neural\nnetwork capable of mapping relatively complex, multi-sentence NL specifications\nto snippets of executable code. The proposed architecture relies exclusively on\nneural components, and is trained on abstract syntax trees, combined with a\npretrained word embedding and a bi-directional multi-layer LSTM for processing\nof word sequences. The decoder features a doubly-recurrent LSTM, for which we\npropose novel signal propagation schemes and soft attention mechanism. When\napplied to a large dataset of problems proposed in a previous study, SAPS\nperforms on par with or better than the method proposed there, producing\ncorrect programs in over 92% of cases. In contrast to other methods, it does\nnot require post-processing of the resulting programs, and uses a\nfixed-dimensional latent representation as the only interface between the NL\nanalyzer and the source code generator. \n\n"}
{"id": "1810.10107", "contents": "Title: Autowarp: Learning a Warping Distance from Unlabeled Time Series Using\n  Sequence Autoencoders Abstract: Measuring similarities between unlabeled time series trajectories is an\nimportant problem in domains as diverse as medicine, astronomy, finance, and\ncomputer vision. It is often unclear what is the appropriate metric to use\nbecause of the complex nature of noise in the trajectories (e.g. different\nsampling rates or outliers). Domain experts typically hand-craft or manually\nselect a specific metric, such as dynamic time warping (DTW), to apply on their\ndata. In this paper, we propose Autowarp, an end-to-end algorithm that\noptimizes and learns a good metric given unlabeled trajectories. We define a\nflexible and differentiable family of warping metrics, which encompasses common\nmetrics such as DTW, Euclidean, and edit distance. Autowarp then leverages the\nrepresentation power of sequence autoencoders to optimize for a member of this\nwarping distance family. The output is a metric which is easy to interpret and\ncan be robustly learned from relatively few trajectories. In systematic\nexperiments across different domains, we show that Autowarp often outperforms\nhand-crafted trajectory similarity metrics. \n\n"}
{"id": "1810.10659", "contents": "Title: Combinatorial Optimization with Graph Convolutional Networks and Guided\n  Tree Search Abstract: We present a learning-based approach to computing solutions for certain\nNP-hard problems. Our approach combines deep learning techniques with useful\nalgorithmic elements from classic heuristics. The central component is a graph\nconvolutional network that is trained to estimate the likelihood, for each\nvertex in a graph, of whether this vertex is part of the optimal solution. The\nnetwork is designed and trained to synthesize a diverse set of solutions, which\nenables rapid exploration of the solution space via tree search. The presented\napproach is evaluated on four canonical NP-hard problems and five datasets,\nwhich include benchmark satisfiability problems and real social network graphs\nwith up to a hundred thousand nodes. Experimental results demonstrate that the\npresented approach substantially outperforms recent deep learning work, and\nperforms on par with highly optimized state-of-the-art heuristic solvers for\nsome NP-hard problems. Experiments indicate that our approach generalizes\nacross datasets, and scales to graphs that are orders of magnitude larger than\nthose used during training. \n\n"}
{"id": "1810.10952", "contents": "Title: Differential Variable Speed Limits Control for Freeway Recurrent\n  Bottlenecks via Deep Reinforcement learning Abstract: Variable speed limits (VSL) control is a flexible way to improve traffic\ncondition,increase safety and reduce emission. There is an emerging trend of\nusing reinforcement learning technique for VSL control and recent studies have\nshown promising results. Currently, deep learning is enabling reinforcement\nlearning to develope autonomous control agents for problems that were\npreviously intractable. In this paper, we propose a more effective deep\nreinforcement learning (DRL) model for differential variable speed limits\n(DVSL) control, in which the dynamic and different speed limits among lanes can\nbe imposed. The proposed DRL models use a novel actor-critic architecture which\ncan learn a large number of discrete speed limits in a continues action space.\nDifferent reward signals, e.g. total travel time, bottleneck speed, emergency\nbraking, and vehicular emission are used to train the DVSL controller, and\ncomparison between these reward signals are conducted. We test proposed DRL\nbaased DVSL controllers on a simulated freeway recurrent bottleneck. Results\nshow that the efficiency, safety and emissions can be improved by the proposed\nmethod. We also show some interesting findings through the visulization of the\ncontrol policies generated from DRL models. \n\n"}
{"id": "1810.11580", "contents": "Title: Attacks Meet Interpretability: Attribute-steered Detection of\n  Adversarial Samples Abstract: Adversarial sample attacks perturb benign inputs to induce DNN misbehaviors.\nRecent research has demonstrated the widespread presence and the devastating\nconsequences of such attacks. Existing defense techniques either assume prior\nknowledge of specific attacks or may not work well on complex models due to\ntheir underlying assumptions. We argue that adversarial sample attacks are\ndeeply entangled with interpretability of DNN models: while classification\nresults on benign inputs can be reasoned based on the human perceptible\nfeatures/attributes, results on adversarial samples can hardly be explained.\nTherefore, we propose a novel adversarial sample detection technique for face\nrecognition models, based on interpretability. It features a novel\nbi-directional correspondence inference between attributes and internal neurons\nto identify neurons critical for individual attributes. The activation values\nof critical neurons are enhanced to amplify the reasoning part of the\ncomputation and the values of other neurons are weakened to suppress the\nuninterpretable part. The classification results after such transformation are\ncompared with those of the original model to detect adversaries. Results show\nthat our technique can achieve 94% detection accuracy for 7 different kinds of\nattacks with 9.91% false positives on benign inputs. In contrast, a\nstate-of-the-art feature squeezing technique can only achieve 55% accuracy with\n23.3% false positives. \n\n"}
{"id": "1810.11583", "contents": "Title: Learning Abstract Options Abstract: Building systems that autonomously create temporal abstractions from data is\na key challenge in scaling learning and planning in reinforcement learning. One\npopular approach for addressing this challenge is the options framework (Sutton\net al., 1999). However, only recently in (Bacon et al., 2017) was a policy\ngradient theorem derived for online learning of general purpose options in an\nend to end fashion. In this work, we extend previous work on this topic that\nonly focuses on learning a two-level hierarchy including options and primitive\nactions to enable learning simultaneously at multiple resolutions in time. We\nachieve this by considering an arbitrarily deep hierarchy of options where high\nlevel temporally extended options are composed of lower level options with\nfiner resolutions in time. We extend results from (Bacon et al., 2017) and\nderive policy gradient theorems for a deep hierarchy of options. Our proposed\nhierarchical option-critic architecture is capable of learning internal\npolicies, termination conditions, and hierarchical compositions over options\nwithout the need for any intrinsic rewards or subgoals. Our empirical results\nin both discrete and continuous environments demonstrate the efficiency of our\nframework. \n\n"}
{"id": "1810.11910", "contents": "Title: Learning to Learn without Forgetting by Maximizing Transfer and\n  Minimizing Interference Abstract: Lack of performance when it comes to continual learning over non-stationary\ndistributions of data remains a major challenge in scaling neural network\nlearning to more human realistic settings. In this work we propose a new\nconceptualization of the continual learning problem in terms of a temporally\nsymmetric trade-off between transfer and interference that can be optimized by\nenforcing gradient alignment across examples. We then propose a new algorithm,\nMeta-Experience Replay (MER), that directly exploits this view by combining\nexperience replay with optimization based meta-learning. This method learns\nparameters that make interference based on future gradients less likely and\ntransfer based on future gradients more likely. We conduct experiments across\ncontinual lifelong supervised learning benchmarks and non-stationary\nreinforcement learning environments demonstrating that our approach\nconsistently outperforms recently proposed baselines for continual learning.\nOur experiments show that the gap between the performance of MER and baseline\nalgorithms grows both as the environment gets more non-stationary and as the\nfraction of the total experiences stored gets smaller. \n\n"}
{"id": "1810.12558", "contents": "Title: Relative Importance Sampling For Off-Policy Actor-Critic in Deep\n  Reinforcement Learning Abstract: Off-policy learning is more unstable compared to on-policy learning in\nreinforcement learning (RL). One reason for the instability of off-policy\nlearning is a discrepancy between the target ($\\pi$) and behavior (b) policy\ndistributions. The discrepancy between $\\pi$ and b distributions can be\nalleviated by employing a smooth variant of the importance sampling (IS), such\nas the relative importance sampling (RIS). RIS has parameter $\\beta\\in[0, 1]$\nwhich controls smoothness. To cope with instability, we present the first\nrelative importance sampling-off-policy actor-critic (RIS-Off-PAC) model-free\nalgorithms in RL. In our method, the network yields a target policy (the\nactor), a value function (the critic) assessing the current policy ($\\pi$)\nusing samples drawn from behavior policy. We use action value generated from\nthe behavior policy in reward function to train our algorithm rather than from\nthe target policy. We also use deep neural networks to train both actor and\ncritic. We evaluated our algorithm on a number of Open AI Gym benchmark\nproblems and demonstrate better or comparable performance to several\nstate-of-the-art RL baselines. \n\n"}
{"id": "1810.12780", "contents": "Title: Advancing PICO Element Detection in Biomedical Text via Deep Neural\n  Networks Abstract: In evidence-based medicine (EBM), defining a clinical question in terms of\nthe specific patient problem aids the physicians to efficiently identify\nappropriate resources and search for the best available evidence for medical\ntreatment. In order to formulate a well-defined, focused clinical question, a\nframework called PICO is widely used, which identifies the sentences in a given\nmedical text that belong to the four components typically reported in clinical\ntrials: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome\n(O). In this work, we propose a novel deep learning model for recognizing PICO\nelements in biomedical abstracts. Based on the previous state-of-the-art\nbidirectional long-short term memory (biLSTM) plus conditional random field\n(CRF) architecture, we add another layer of biLSTM upon the sentence\nrepresentation vectors so that the contextual information from surrounding\nsentences can be gathered to help infer the interpretation of the current one.\nIn addition, we propose two methods to further generalize and improve the\nmodel: adversarial training and unsupervised pre-training over large corpora.\nWe tested our proposed approach over two benchmark datasets. One is the\nPubMed-PICO dataset, where our best results outperform the previous best by\n5.5%, 7.9%, and 5.8% for P, I, and O elements in terms of F1 score,\nrespectively. And for the other dataset named NICTA-PIBOSO, the improvements\nfor P/I/O elements are 2.4%, 13.6%, and 1.0% in F1 score, respectively.\nOverall, our proposed deep learning model can obtain unprecedented PICO element\ndetection accuracy while avoiding the need for any manual feature selection. \n\n"}
{"id": "1810.12823", "contents": "Title: DeepTwist: Learning Model Compression via Occasional Weight Distortion Abstract: Model compression has been introduced to reduce the required hardware\nresources while maintaining the model accuracy. Lots of techniques for model\ncompression, such as pruning, quantization, and low-rank approximation, have\nbeen suggested along with different inference implementation characteristics.\nAdopting model compression is, however, still challenging because the design\ncomplexity of model compression is rapidly increasing due to additional\nhyper-parameters and computation overhead in order to achieve a high\ncompression ratio. In this paper, we propose a simple and efficient model\ncompression framework called DeepTwist which distorts weights in an occasional\nmanner without modifying the underlying training algorithms. The ideas of\ndesigning weight distortion functions are intuitive and straightforward given\nformats of compressed weights. We show that our proposed framework improves\ncompression rate significantly for pruning, quantization, and low-rank\napproximation techniques while the efforts of additional retraining and/or\nhyper-parameter search are highly reduced. Regularization effects of DeepTwist\nare also reported. \n\n"}
{"id": "1810.13084", "contents": "Title: Provably Accelerated Randomized Gossip Algorithms Abstract: In this work we present novel provably accelerated gossip algorithms for\nsolving the average consensus problem. The proposed protocols are inspired from\nthe recently developed accelerated variants of the randomized Kaczmarz method -\na popular method for solving linear systems. In each gossip iteration all nodes\nof the network update their values but only a pair of them exchange their\nprivate information. Numerical experiments on popular wireless sensor networks\nshowing the benefits of our protocols are also presented. \n\n"}
{"id": "1810.13155", "contents": "Title: Structure Learning of Deep Neural Networks with Q-Learning Abstract: Recently, with convolutional neural networks gaining significant achievements\nin many challenging machine learning fields, hand-crafted neural networks no\nlonger satisfy our requirements as designing a network will cost a lot, and\nautomatically generating architectures has attracted increasingly more\nattention and focus. Some research on auto-generated networks has achieved\npromising results. However, they mainly aim at picking a series of single\nlayers such as convolution or pooling layers one by one. There are many elegant\nand creative designs in the carefully hand-crafted neural networks, such as\nInception-block in GoogLeNet, residual block in residual network and dense\nblock in dense convolutional network. Based on reinforcement learning and\ntaking advantages of the superiority of these networks, we propose a novel\nautomatic process to design a multi-block neural network, whose architecture\ncontains multiple types of blocks mentioned above, with the purpose to do\nstructure learning of deep neural networks and explore the possibility whether\ndifferent blocks can be composed together to form a well-behaved neural\nnetwork. The optimal network is created by the Q-learning agent who is trained\nto sequentially pick different types of blocks. To verify the validity of our\nproposed method, we use the auto-generated multi-block neural network to\nconduct experiments on image benchmark datasets MNIST, SVHN and CIFAR-10 image\nclassification task with restricted computational resources. The results\ndemonstrate that our method is very effective, achieving comparable or better\nperformance than hand-crafted networks and advanced auto-generated neural\nnetworks. \n\n"}
{"id": "1810.13258", "contents": "Title: On Fast Leverage Score Sampling and Optimal Learning Abstract: Leverage score sampling provides an appealing way to perform approximate\ncomputations for large matrices. Indeed, it allows to derive faithful\napproximations with a complexity adapted to the problem at hand. Yet,\nperforming leverage scores sampling is a challenge in its own right requiring\nfurther approximations. In this paper, we study the problem of leverage score\nsampling for positive definite matrices defined by a kernel. Our contribution\nis twofold. First we provide a novel algorithm for leverage score sampling and\nsecond, we exploit the proposed method in statistical learning by deriving a\nnovel solver for kernel ridge regression. Our main technical contribution is\nshowing that the proposed algorithms are currently the most efficient and\naccurate for these problems. \n\n"}
{"id": "1810.13395", "contents": "Title: Accelerating SGD with momentum for over-parameterized learning Abstract: Nesterov SGD is widely used for training modern neural networks and other\nmachine learning models. Yet, its advantages over SGD have not been\ntheoretically clarified. Indeed, as we show in our paper, both theoretically\nand empirically, Nesterov SGD with any parameter selection does not in general\nprovide acceleration over ordinary SGD. Furthermore, Nesterov SGD may diverge\nfor step sizes that ensure convergence of ordinary SGD. This is in contrast to\nthe classical results in the deterministic scenario, where the same step size\nensures accelerated convergence of the Nesterov's method over optimal gradient\ndescent.\n  To address the non-acceleration issue, we introduce a compensation term to\nNesterov SGD. The resulting algorithm, which we call MaSS, converges for same\nstep sizes as SGD. We prove that MaSS obtains an accelerated convergence rates\nover SGD for any mini-batch size in the linear setting. For full batch, the\nconvergence rate of MaSS matches the well-known accelerated rate of the\nNesterov's method.\n  We also analyze the practically important question of the dependence of the\nconvergence rate and optimal hyper-parameters on the mini-batch size,\ndemonstrating three distinct regimes: linear scaling, diminishing returns and\nsaturation.\n  Experimental evaluation of MaSS for several standard architectures of deep\nnetworks, including ResNet and convolutional networks, shows improved\nperformance over SGD, Nesterov SGD and Adam. \n\n"}
{"id": "1811.00143", "contents": "Title: Democratizing Production-Scale Distributed Deep Learning Abstract: The interest and demand for training deep neural networks have been\nexperiencing rapid growth, spanning a wide range of applications in both\nacademia and industry. However, training them distributed and at scale remains\ndifficult due to the complex ecosystem of tools and hardware involved. One\nconsequence is that the responsibility of orchestrating these complex\ncomponents is often left to one-off scripts and glue code customized for\nspecific problems. To address these restrictions, we introduce \\emph{Alchemist}\n- an internal service built at Apple from the ground up for \\emph{easy},\n\\emph{fast}, and \\emph{scalable} distributed training. We discuss its design,\nimplementation, and examples of running different flavors of distributed\ntraining. We also present case studies of its internal adoption in the\ndevelopment of autonomous systems, where training times have been reduced by\n10x to keep up with the ever-growing data collection. \n\n"}
{"id": "1811.00183", "contents": "Title: Designing an Effective Metric Learning Pipeline for Speaker Diarization Abstract: State-of-the-art speaker diarization systems utilize knowledge from external\ndata, in the form of a pre-trained distance metric, to effectively determine\nrelative speaker identities to unseen data. However, much of recent focus has\nbeen on choosing the appropriate feature extractor, ranging from pre-trained\n$i-$vectors to representations learned via different sequence modeling\narchitectures (e.g. 1D-CNNs, LSTMs, attention models), while adopting\noff-the-shelf metric learning solutions. In this paper, we argue that,\nregardless of the feature extractor, it is crucial to carefully design a metric\nlearning pipeline, namely the loss function, the sampling strategy and the\ndiscrimnative margin parameter, for building robust diarization systems.\nFurthermore, we propose to adopt a fine-grained validation process to obtain a\ncomprehensive evaluation of the generalization power of metric learning\npipelines. To this end, we measure diarization performance across different\nlanguage speakers, and variations in the number of speakers in a recording.\nUsing empirical studies, we provide interesting insights into the effectiveness\nof different design choices and make recommendations. \n\n"}
{"id": "1811.00239", "contents": "Title: Progressive Memory Banks for Incremental Domain Adaptation Abstract: This paper addresses the problem of incremental domain adaptation (IDA) in\nnatural language processing (NLP). We assume each domain comes one after\nanother, and that we could only access data in the current domain. The goal of\nIDA is to build a unified model performing well on all the domains that we have\nencountered. We adopt the recurrent neural network (RNN) widely used in NLP,\nbut augment it with a directly parameterized memory bank, which is retrieved by\nan attention mechanism at each step of RNN transition. The memory bank provides\na natural way of IDA: when adapting our model to a new domain, we progressively\nadd new slots to the memory bank, which increases the number of parameters, and\nthus the model capacity. We learn the new memory slots and fine-tune existing\nparameters by back-propagation. Experimental results show that our approach\nachieves significantly better performance than fine-tuning alone. Compared with\nexpanding hidden states, our approach is more robust for old domains, shown by\nboth empirical and theoretical results. Our model also outperforms previous\nwork of IDA including elastic weight consolidation and progressive neural\nnetworks in the experiments. \n\n"}
{"id": "1811.00401", "contents": "Title: Excessive Invariance Causes Adversarial Vulnerability Abstract: Despite their impressive performance, deep neural networks exhibit striking\nfailures on out-of-distribution inputs. One core idea of adversarial example\nresearch is to reveal neural network errors under such distribution shifts. We\ndecompose these errors into two complementary sources: sensitivity and\ninvariance. We show deep networks are not only too sensitive to task-irrelevant\nchanges of their input, as is well-known from epsilon-adversarial examples, but\nare also too invariant to a wide range of task-relevant changes, thus making\nvast regions in input space vulnerable to adversarial attacks. We show such\nexcessive invariance occurs across various tasks and architecture types. On\nMNIST and ImageNet one can manipulate the class-specific content of almost any\nimage without changing the hidden activations. We identify an insufficiency of\nthe standard cross-entropy loss as a reason for these failures. Further, we\nextend this objective based on an information-theoretic analysis so it\nencourages the model to consider all task-dependent features in its decision.\nThis provides the first approach tailored explicitly to overcome excessive\ninvariance and resulting vulnerabilities. \n\n"}
{"id": "1811.00548", "contents": "Title: Enhancing the Structural Performance of Additively Manufactured Objects Abstract: The ability to accurately quantify the performance an additively manufactured\n(AM) product is important for a widespread industry adoption of AM as the\ndesign is required to: (1) satisfy geometrical constraints, (2) satisfy\nstructural constraints dictated by its intended function, and (3) be cost\neffective compared to traditional manufacturing methods. Optimization\ntechniques offer design aids in creating cost-effective structures that meet\nthe prescribed structural objectives. The fundamental problem in existing\napproaches lies in the difficulty to quantify the structural performance as\neach unique design leads to a new set of analyses to determine the structural\nrobustness and such analyses can be very costly due to the complexity of in-use\nforces experienced by the structure. This work develops computationally\ntractable methods tailored to maximize the structural performance of AM\nproducts. A geometry preserving build orientation optimization method as well\nas data-driven shape optimization approaches to structural design are\npresented. Proposed methods greatly enhance the value of AM technology by\ntaking advantage of the design space enabled by it for a broad class of\nproblems involving complex in-use loads. \n\n"}
{"id": "1811.00552", "contents": "Title: Multiple-Attribute Text Style Transfer Abstract: The dominant approach to unsupervised \"style transfer\" in text is based on\nthe idea of learning a latent representation, which is independent of the\nattributes specifying its \"style\". In this paper, we show that this condition\nis not necessary and is not always met in practice, even with domain\nadversarial training that explicitly aims at learning such disentangled\nrepresentations. We thus propose a new model that controls several factors of\nvariation in textual data where this condition on disentanglement is replaced\nwith a simpler mechanism based on back-translation. Our method allows control\nover multiple attributes, like gender, sentiment, product type, etc., and a\nmore fine-grained control on the trade-off between content preservation and\nchange of style with a pooling operator in the latent space. Our experiments\ndemonstrate that the fully entangled model produces better generations, even\nwhen tested on new and more challenging benchmarks comprising reviews with\nmultiple sentences and multiple attributes. \n\n"}
{"id": "1811.00621", "contents": "Title: Improving Adversarial Robustness by Encouraging Discriminative Features Abstract: Deep neural networks (DNNs) have achieved state-of-the-art results in various\npattern recognition tasks. However, they perform poorly on out-of-distribution\nadversarial examples i.e. inputs that are specifically crafted by an adversary\nto cause DNNs to misbehave, questioning the security and reliability of\napplications. In this paper, we encourage DNN classifiers to learn more\ndiscriminative features by imposing a center loss in addition to the regular\nsoftmax cross-entropy loss. Intuitively, the center loss encourages DNNs to\nsimultaneously learns a center for the deep features of each class, and\nminimize the distances between the intra-class deep features and their\ncorresponding class centers. We hypothesize that minimizing distances between\nintra-class features and maximizing the distances between inter-class features\nat the same time would improve a classifier's robustness to adversarial\nexamples. Our results on state-of-the-art architectures on MNIST, CIFAR-10, and\nCIFAR-100 confirmed that intuition and highlight the importance of\ndiscriminative features. \n\n"}
{"id": "1811.00741", "contents": "Title: Stronger Data Poisoning Attacks Break Data Sanitization Defenses Abstract: Machine learning models trained on data from the outside world can be\ncorrupted by data poisoning attacks that inject malicious points into the\nmodels' training sets. A common defense against these attacks is data\nsanitization: first filter out anomalous training points before training the\nmodel. In this paper, we develop three attacks that can bypass a broad range of\ncommon data sanitization defenses, including anomaly detectors based on nearest\nneighbors, training loss, and singular-value decomposition. By adding just 3%\npoisoned data, our attacks successfully increase test error on the Enron spam\ndetection dataset from 3% to 24% and on the IMDB sentiment classification\ndataset from 12% to 29%. In contrast, existing attacks which do not explicitly\naccount for these data sanitization defenses are defeated by them. Our attacks\nare based on two ideas: (i) we coordinate our attacks to place poisoned points\nnear one another, and (ii) we formulate each attack as a constrained\noptimization problem, with constraints designed to ensure that the poisoned\npoints evade detection. As this optimization involves solving an expensive\nbilevel problem, our three attacks correspond to different ways of\napproximating this problem, based on influence functions; minimax duality; and\nthe Karush-Kuhn-Tucker (KKT) conditions. Our results underscore the need to\ndevelop more robust defenses against data poisoning attacks. \n\n"}
{"id": "1811.00836", "contents": "Title: Multi-Kernel Regression with Sparsity Constraint Abstract: In this paper, we provide a Banach-space formulation of supervised learning\nwith generalized total-variation (gTV) regularization. We identify the class of\nkernel functions that are admissible in this framework. Then, we propose a\nvariation of supervised learning in a continuous-domain hybrid search space\nwith gTV regularization. We show that the solution admits a multi-kernel\nexpansion with adaptive positions. In this representation, the number of active\nkernels is upper-bounded by the number of data points while the gTV\nregularization imposes an $\\ell_1$ penalty on the kernel coefficients. Finally,\nwe illustrate numerically the outcome of our theory. \n\n"}
{"id": "1811.01027", "contents": "Title: AiDroid: When Heterogeneous Information Network Marries Deep Neural\n  Network for Real-time Android Malware Detection Abstract: The explosive growth and increasing sophistication of Android malware call\nfor new defensive techniques that are capable of protecting mobile users\nagainst novel threats. In this paper, we first extract the runtime Application\nProgramming Interface (API) call sequences from Android apps, and then analyze\nhigher-level semantic relations within the ecosystem to comprehensively\ncharacterize the apps. To model different types of entities (i.e., app, API,\nIMEI, signature, affiliation) and the rich semantic relations among them, we\nthen construct a structural heterogeneous information network (HIN) and present\nmeta-path based approach to depict the relatedness over apps. To efficiently\nclassify nodes (e.g., apps) in the constructed HIN, we propose the HinLearning\nmethod to first obtain in-sample node embeddings and then learn representations\nof out-of-sample nodes without rerunning/adjusting HIN embeddings at the first\nattempt. Afterwards, we design a deep neural network (DNN) classifier taking\nthe learned HIN representations as inputs for Android malware detection. A\ncomprehensive experimental study on the large-scale real sample collections\nfrom Tencent Security Lab is performed to compare various baselines. Promising\nexperimental results demonstrate that our developed system AiDroid which\nintegrates our proposed method outperforms others in real-time Android malware\ndetection. AiDroid has already been incorporated into Tencent Mobile Security\nproduct that serves millions of users worldwide. \n\n"}
{"id": "1811.01132", "contents": "Title: VIREL: A Variational Inference Framework for Reinforcement Learning Abstract: Applying probabilistic models to reinforcement learning (RL) enables the\napplication of powerful optimisation tools such as variational inference to RL.\nHowever, existing inference frameworks and their algorithms pose significant\nchallenges for learning optimal policies, e.g., the absence of mode capturing\nbehaviour in pseudo-likelihood methods and difficulties learning deterministic\npolicies in maximum entropy RL based approaches. We propose VIREL, a novel,\ntheoretically grounded probabilistic inference framework for RL that utilises a\nparametrised action-value function to summarise future dynamics of the\nunderlying MDP. This gives VIREL a mode-seeking form of KL divergence, the\nability to learn deterministic optimal polices naturally from inference and the\nability to optimise value functions and policies in separate, iterative steps.\nIn applying variational expectation-maximisation to VIREL we thus show that the\nactor-critic algorithm can be reduced to expectation-maximisation, with policy\nimprovement equivalent to an E-step and policy evaluation to an M-step. We then\nderive a family of actor-critic methods from VIREL, including a scheme for\nadaptive exploration. Finally, we demonstrate that actor-critic algorithms from\nthis family outperform state-of-the-art methods based on soft value functions\nin several domains. \n\n"}
{"id": "1811.01179", "contents": "Title: Large-scale Heteroscedastic Regression via Gaussian Process Abstract: Heteroscedastic regression considering the varying noises among observations\nhas many applications in the fields like machine learning and statistics. Here\nwe focus on the heteroscedastic Gaussian process (HGP) regression which\nintegrates the latent function and the noise function together in a unified\nnon-parametric Bayesian framework. Though showing remarkable performance, HGP\nsuffers from the cubic time complexity, which strictly limits its application\nto big data. To improve the scalability, we first develop a variational sparse\ninference algorithm, named VSHGP, to handle large-scale datasets. Furthermore,\ntwo variants are developed to improve the scalability and capability of VSHGP.\nThe first is stochastic VSHGP (SVSHGP) which derives a factorized evidence\nlower bound, thus enhancing efficient stochastic variational inference. The\nsecond is distributed VSHGP (DVSHGP) which (i) follows the Bayesian committee\nmachine formalism to distribute computations over multiple local VSHGP experts\nwith many inducing points; and (ii) adopts hybrid parameters for experts to\nguard against over-fitting and capture local variety. The superiority of DVSHGP\nand SVSHGP as compared to existing scalable heteroscedastic/homoscedastic GPs\nis then extensively verified on various datasets. \n\n"}
{"id": "1811.01458", "contents": "Title: Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning Abstract: When observing the actions of others, humans make inferences about why they\nacted as they did, and what this implies about the world; humans also use the\nfact that their actions will be interpreted in this manner, allowing them to\nact informatively and thereby communicate efficiently with others. Although\nlearning algorithms have recently achieved superhuman performance in a number\nof two-player, zero-sum games, scalable multi-agent reinforcement learning\nalgorithms that can discover effective strategies and conventions in complex,\npartially observable settings have proven elusive. We present the Bayesian\naction decoder (BAD), a new multi-agent learning method that uses an\napproximate Bayesian update to obtain a public belief that conditions on the\nactions taken by all agents in the environment. BAD introduces a new Markov\ndecision process, the public belief MDP, in which the action space consists of\nall deterministic partial policies, and exploits the fact that an agent acting\nonly on this public belief state can still learn to use its private information\nif the action space is augmented to be over all partial policies mapping\nprivate information into environment actions. The Bayesian update is closely\nrelated to the theory of mind reasoning that humans carry out when observing\nothers' actions. We first validate BAD on a proof-of-principle two-step matrix\ngame, where it outperforms policy gradient methods; we then evaluate BAD on the\nchallenging, cooperative partial-information card game Hanabi, where, in the\ntwo-player setting, it surpasses all previously published learning and\nhand-coded approaches, establishing a new state of the art. \n\n"}
{"id": "1811.01850", "contents": "Title: End-to-End Sound Source Separation Conditioned On Instrument Labels Abstract: Can we perform an end-to-end music source separation with a variable number\nof sources using a deep learning model? We present an extension of the\nWave-U-Net model which allows end-to-end monaural source separation with a\nnon-fixed number of sources. Furthermore, we propose multiplicative\nconditioning with instrument labels at the bottleneck of the Wave-U-Net and\nshow its effect on the separation results. This approach leads to other types\nof conditioning such as audio-visual source separation and score-informed\nsource separation. \n\n"}
{"id": "1811.01908", "contents": "Title: Fast Non-Bayesian Poisson Factorization for Implicit-Feedback\n  Recommendations Abstract: This work explores non-negative low-rank matrix factorization based on\nregularized Poisson models (PF or \"Poisson factorization\" for short) for\nrecommender systems with implicit-feedback data. The properties of Poisson\nlikelihood allow a shortcut for very fast computations over zero-valued inputs,\nand oftentimes results in very sparse factors for both users and items.\nCompared to HPF (a popular Bayesian formulation of the problem with\nhierarchical priors), the frequentist optimization-based approach presented\nhere tends to produce better top-N recommendations with significantly shorter\nfitting times, on top of having sparse solutions. \n\n"}
{"id": "1811.01926", "contents": "Title: contextual: Evaluating Contextual Multi-Armed Bandit Problems in R Abstract: Over the past decade, contextual bandit algorithms have been gaining in\npopularity due to their effectiveness and flexibility in solving sequential\ndecision problems---from online advertising and finance to clinical trial\ndesign and personalized medicine. At the same time, there are, as of yet,\nsurprisingly few options that enable researchers and practitioners to simulate\nand compare the wealth of new and existing bandit algorithms in a standardized\nway. To help close this gap between analytical research and empirical\nevaluation the current paper introduces the object-oriented R package\n\"contextual\": a user-friendly and, through its object-oriented structure,\neasily extensible framework that facilitates parallelized comparison of\ncontextual and context-free bandit policies through both simulation and offline\nanalysis. \n\n"}
{"id": "1811.02033", "contents": "Title: Physics-Informed Generative Adversarial Networks for Stochastic\n  Differential Equations Abstract: We developed a new class of physics-informed generative adversarial networks\n(PI-GANs) to solve in a unified manner forward, inverse and mixed stochastic\nproblems based on a limited number of scattered measurements. Unlike standard\nGANs relying only on data for training, here we encoded into the architecture\nof GANs the governing physical laws in the form of stochastic differential\nequations (SDEs) using automatic differentiation. In particular, we applied\nWasserstein GANs with gradient penalty (WGAN-GP) for its enhanced stability\ncompared to vanilla GANs. We first tested WGAN-GP in approximating Gaussian\nprocesses of different correlation lengths based on data realizations collected\nfrom simultaneous reads at sparsely placed sensors. We obtained good\napproximation of the generated stochastic processes to the target ones even for\na mismatch between the input noise dimensionality and the effective\ndimensionality of the target stochastic processes. We also studied the\noverfitting issue for both the discriminator and generator, and we found that\noverfitting occurs also in the generator in addition to the discriminator as\npreviously reported. Subsequently, we considered the solution of elliptic SDEs\nrequiring approximations of three stochastic processes, namely the solution,\nthe forcing, and the diffusion coefficient. We used three generators for the\nPI-GANs, two of them were feed forward deep neural networks (DNNs) while the\nother one was the neural network induced by the SDE. Depending on the data, we\nemployed one or multiple feed forward DNNs as the discriminators in PI-GANs.\nHere, we have demonstrated the accuracy and effectiveness of PI-GANs in solving\nSDEs for up to 30 dimensions, but in principle, PI-GANs could tackle very high\ndimensional problems given more sensor data with low-polynomial growth in\ncomputational cost. \n\n"}
{"id": "1811.02067", "contents": "Title: Sample Compression, Support Vectors, and Generalization in Deep Learning Abstract: Even though Deep Neural Networks (DNNs) are widely celebrated for their\npractical performance, they possess many intriguing properties related to depth\nthat are difficult to explain both theoretically and intuitively. Understanding\nhow weights in deep networks coordinate together across layers to form useful\nlearners has proven challenging, in part because the repeated composition of\nnonlinearities has proved intractable. This paper presents a reparameterization\nof DNNs as a linear function of a feature map that is locally independent of\nthe weights. This feature map transforms depth-dependencies into simple tensor\nproducts and maps each input to a discrete subset of the feature space. Then,\nusing a max-margin assumption, the paper develops a sample compression\nrepresentation of the neural network in terms of the discrete activation state\nof neurons induced by s ``support vectors\". The paper shows that the number of\nsupport vectors s relates with learning guarantees for neural networks through\nsample compression bounds, yielding a sample complexity of O(ns/epsilon) for\nnetworks with n neurons. Finally, the number of support vectors s is found to\nmonotonically increase with width and label noise but decrease with depth. \n\n"}
{"id": "1811.02655", "contents": "Title: Sparse and Smooth Signal Estimation: Convexification of L0 Formulations Abstract: Signal estimation problems with smoothness and sparsity priors can be\nnaturally modeled as quadratic optimization with $\\ell_0$-\"norm\" constraints.\nSince such problems are non-convex and hard-to-solve, the standard approach is,\ninstead, to tackle their convex surrogates based on $\\ell_1$-norm relaxations.\nIn this paper, we propose a new iterative (convex) conic quadratic relaxations\nthat exploit not only the $\\ell_0$-\"norm\" terms, but also the fitness and\nsmoothness functions. The iterative convexification approach substantially\ncloses the gap between the $\\ell_0$-\"norm\" and its $\\ell_1$ surrogate. These\nstronger relaxations lead to significantly better estimators than $\\ell_1$-norm\napproaches and also allow one to utilize affine sparsity priors. In addition,\nthe parameters of the model and the resulting estimators are easily\ninterpretable. Experiments with a tailored Lagrangian decomposition method\nindicate that the proposed iterative convex relaxations \\rev{yield solutions\nwithin 1\\% of the exact $\\ell_0$ approach, and can tackle instances with up to\n100,000 variables under one minute. \n\n"}
{"id": "1811.02783", "contents": "Title: YASENN: Explaining Neural Networks via Partitioning Activation Sequences Abstract: We introduce a novel approach to feed-forward neural network interpretation\nbased on partitioning the space of sequences of neuron activations. In line\nwith this approach, we propose a model-specific interpretation method, called\nYASENN. Our method inherits many advantages of model-agnostic distillation,\nsuch as an ability to focus on the particular input region and to express an\nexplanation in terms of features different from those observed by a neural\nnetwork. Moreover, examination of distillation error makes the method\napplicable to the problems with low tolerance to interpretation mistakes.\nTechnically, YASENN distills the network with an ensemble of layer-wise\ngradient boosting decision trees and encodes the sequences of neuron\nactivations with leaf indices. The finite number of unique codes induces a\npartitioning of the input space. Each partition may be described in a variety\nof ways, including examination of an interpretable model (e.g. a logistic\nregression or a decision tree) trained to discriminate between objects of those\npartitions. Our experiments provide an intuition behind the method and\ndemonstrate revealed artifacts in neural network decision making. \n\n"}
{"id": "1811.02790", "contents": "Title: RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through\n  Imitation Abstract: Imitation Learning has empowered recent advances in learning robotic\nmanipulation tasks by addressing shortcomings of Reinforcement Learning such as\nexploration and reward specification. However, research in this area has been\nlimited to modest-sized datasets due to the difficulty of collecting large\nquantities of task demonstrations through existing mechanisms. This work\nintroduces RoboTurk to address this challenge. RoboTurk is a crowdsourcing\nplatform for high quality 6-DoF trajectory based teleoperation through the use\nof widely available mobile devices (e.g. iPhone). We evaluate RoboTurk on three\nmanipulation tasks of varying timescales (15-120s) and observe that our user\ninterface is statistically similar to special purpose hardware such as virtual\nreality controllers in terms of task completion times. Furthermore, we observe\nthat poor network conditions, such as low bandwidth and high delay links, do\nnot substantially affect the remote users' ability to perform task\ndemonstrations successfully on RoboTurk. Lastly, we demonstrate the efficacy of\nRoboTurk through the collection of a pilot dataset; using RoboTurk, we\ncollected 137.5 hours of manipulation data from remote workers, amounting to\nover 2200 successful task demonstrations in 22 hours of total system usage. We\nshow that the data obtained through RoboTurk enables policy learning on\nmulti-step manipulation tasks with sparse rewards and that using larger\nquantities of demonstrations during policy learning provides benefits in terms\nof both learning consistency and final performance. For additional results,\nvideos, and to download our pilot dataset, visit\n$\\href{http://roboturk.stanford.edu/}{\\texttt{roboturk.stanford.edu}}$ \n\n"}
{"id": "1811.03081", "contents": "Title: Forging new worlds: high-resolution synthetic galaxies with chained\n  generative adversarial networks Abstract: Astronomy of the 21st century increasingly finds itself with extreme\nquantities of data. This growth in data is ripe for modern technologies such as\ndeep image processing, which has the potential to allow astronomers to\nautomatically identify, classify, segment and deblend various astronomical\nobjects. In this paper, we explore the use of chained generative adversarial\nnetworks (GANs), a class of generative models that learn mappings from latent\nspaces to data distributions by modelling the joint distribution of the data,\nto produce physically realistic galaxy images as one use case of such models.\nIn cosmology, such datasets can aid in the calibration of shape measurements\nfor weak lensing by augmenting data with synthetic images. By measuring the\ndistributions of multiple physical properties, we show that images generated\nwith our approach closely follow the distributions of real galaxies, further\nestablishing state-of-the-art GAN architectures as a valuable tool for\nmodern-day astronomy. \n\n"}
{"id": "1811.03422", "contents": "Title: Explaining Deep Learning Models - A Bayesian Non-parametric Approach Abstract: Understanding and interpreting how machine learning (ML) models make\ndecisions have been a big challenge. While recent research has proposed various\ntechnical approaches to provide some clues as to how an ML model makes\nindividual predictions, they cannot provide users with an ability to inspect a\nmodel as a complete entity. In this work, we propose a novel technical approach\nthat augments a Bayesian non-parametric regression mixture model with multiple\nelastic nets. Using the enhanced mixture model, we can extract generalizable\ninsights for a target model through a global approximation. To demonstrate the\nutility of our approach, we evaluate it on different ML models in the context\nof image recognition. The empirical results indicate that our proposed approach\nnot only outperforms the state-of-the-art techniques in explaining individual\ndecisions but also provides users with an ability to discover the\nvulnerabilities of the target ML models. \n\n"}
{"id": "1811.03516", "contents": "Title: Learning from Demonstration in the Wild Abstract: Learning from demonstration (LfD) is useful in settings where hand-coding\nbehaviour or a reward function is impractical. It has succeeded in a wide range\nof problems but typically relies on manually generated demonstrations or\nspecially deployed sensors and has not generally been able to leverage the\ncopious demonstrations available in the wild: those that capture behaviours\nthat were occurring anyway using sensors that were already deployed for another\npurpose, e.g., traffic camera footage capturing demonstrations of natural\nbehaviour of vehicles, cyclists, and pedestrians. We propose Video to Behaviour\n(ViBe), a new approach to learn models of behaviour from unlabelled raw video\ndata of a traffic scene collected from a single, monocular, initially\nuncalibrated camera with ordinary resolution. Our approach calibrates the\ncamera, detects relevant objects, tracks them through time, and uses the\nresulting trajectories to perform LfD, yielding models of naturalistic\nbehaviour. We apply ViBe to raw videos of a traffic intersection and show that\nit can learn purely from videos, without additional expert knowledge. \n\n"}
{"id": "1811.03537", "contents": "Title: Iterative Classroom Teaching Abstract: We consider the machine teaching problem in a classroom-like setting wherein\nthe teacher has to deliver the same examples to a diverse group of students.\nTheir diversity stems from differences in their initial internal states as well\nas their learning rates. We prove that a teacher with full knowledge about the\nlearning dynamics of the students can teach a target concept to the entire\nclassroom using O(min{d,N} log(1/eps)) examples, where d is the ambient\ndimension of the problem, N is the number of learners, and eps is the accuracy\nparameter. We show the robustness of our teaching strategy when the teacher has\nlimited knowledge of the learners' internal dynamics as provided by a noisy\noracle. Further, we study the trade-off between the learners' workload and the\nteacher's cost in teaching the target concept. Our experiments validate our\ntheoretical results and suggest that appropriately partitioning the classroom\ninto homogenous groups provides a balance between these two objectives. \n\n"}
{"id": "1811.03909", "contents": "Title: Evidence Transfer for Improving Clustering Tasks Using External\n  Categorical Evidence Abstract: In this paper we introduce evidence transfer for clustering, a deep learning\nmethod that can incrementally manipulate the latent representations of an\nautoencoder, according to external categorical evidence, in order to improve a\nclustering outcome. By evidence transfer we define the process by which the\ncategorical outcome of an external, auxiliary task is exploited to improve a\nprimary task, in this case representation learning for clustering. Our proposed\nmethod makes no assumptions regarding the categorical evidence presented, nor\nthe structure of the latent space. We compare our method, against the baseline\nsolution by performing k-means clustering before and after its deployment.\nExperiments with three different kinds of evidence show that our method\neffectively manipulates the latent representations when introduced with real\ncorresponding evidence, while remaining robust when presented with low quality\nevidence. \n\n"}
{"id": "1811.04026", "contents": "Title: Adversarial Uncertainty Quantification in Physics-Informed Neural\n  Networks Abstract: We present a deep learning framework for quantifying and propagating\nuncertainty in systems governed by non-linear differential equations using\nphysics-informed neural networks. Specifically, we employ latent variable\nmodels to construct probabilistic representations for the system states, and\nput forth an adversarial inference procedure for training them on data, while\nconstraining their predictions to satisfy given physical laws expressed by\npartial differential equations. Such physics-informed constraints provide a\nregularization mechanism for effectively training deep generative models as\nsurrogates of physical systems in which the cost of data acquisition is high,\nand training data-sets are typically small. This provides a flexible framework\nfor characterizing uncertainty in the outputs of physical systems due to\nrandomness in their inputs or noise in their observations that entirely\nbypasses the need for repeatedly sampling expensive experiments or numerical\nsimulators. We demonstrate the effectiveness of our approach through a series\nof examples involving uncertainty propagation in non-linear conservation laws,\nand the discovery of constitutive laws for flow through porous media directly\nfrom noisy data. \n\n"}
{"id": "1811.04127", "contents": "Title: Policy Regret in Repeated Games Abstract: The notion of \\emph{policy regret} in online learning is a well defined?\nperformance measure for the common scenario of adaptive adversaries, which more\ntraditional quantities such as external regret do not take into account. We\nrevisit the notion of policy regret and first show that there are online\nlearning settings in which policy regret and external regret are incompatible:\nany sequence of play that achieves a favorable regret with respect to one\ndefinition must do poorly with respect to the other. We then focus on the\ngame-theoretic setting where the adversary is a self-interested agent. In that\nsetting, we show that external regret and policy regret are not in conflict\nand, in fact, that a wide class of algorithms can ensure a favorable regret\nwith respect to both definitions, so long as the adversary is also using such\nan algorithm. We also show that the sequence of play of no-policy regret\nalgorithms converges to a \\emph{policy equilibrium}, a new notion of\nequilibrium that we introduce. Relating this back to external regret, we show\nthat coarse correlated equilibria, which no-external regret players converge\nto, are a strict subset of policy equilibria. Thus, in game-theoretic settings,\nevery sequence of play with no external regret also admits no policy regret,\nbut the converse does not hold. \n\n"}
{"id": "1811.04689", "contents": "Title: Adversarial Learning of Label Dependency: A Novel Framework for\n  Multi-class Classification Abstract: Recent work has shown that exploiting relations between labels improves the\nperformance of multi-label classification. We propose a novel framework based\non generative adversarial networks (GANs) to model label dependency. The\ndiscriminator learns to model label dependency by discriminating real and\ngenerated label sets. To fool the discriminator, the classifier, or generator,\nlearns to generate label sets with dependencies close to real data. Extensive\nexperiments and comparisons on two large-scale image classification benchmark\ndatasets (MS-COCO and NUS-WIDE) show that the discriminator improves\ngeneralization ability for different kinds of models \n\n"}
{"id": "1811.04770", "contents": "Title: Packing Sparse Convolutional Neural Networks for Efficient Systolic\n  Array Implementations: Column Combining Under Joint Optimization Abstract: This paper describes a novel approach of packing sparse convolutional neural\nnetworks for their efficient systolic array implementations. By combining\nsubsets of columns in the original filter matrix associated with a\nconvolutional layer, we increase the utilization efficiency of the systolic\narray substantially (e.g., ~4x) due to the increased density of nonzeros in the\nresulting packed filter matrix. In combining columns, for each row, all filter\nweights but one with the largest magnitude are pruned. We retrain the remaining\nweights to preserve high accuracy. We demonstrate that in mitigating data\nprivacy concerns the retraining can be accomplished with only fractions of the\noriginal dataset (e.g., 10\\% for CIFAR-10). We study the effectiveness of this\njoint optimization for both high utilization and classification accuracy with\nASIC and FPGA designs based on efficient bit-serial implementations of\nmultiplier-accumulators. We present analysis and empirical evidence on the\nsuperior performance of our column combining approach against prior arts under\nmetrics such as energy efficiency (3x) and inference latency (12x). \n\n"}
{"id": "1811.04911", "contents": "Title: Boosting Model Performance through Differentially Private Model\n  Aggregation Abstract: A key factor in developing high performing machine learning models is the\navailability of sufficiently large datasets. This work is motivated by\napplications arising in Software as a Service (SaaS) companies where there\nexist numerous similar yet disjoint datasets from multiple client companies. To\novercome the challenges of insufficient data without explicitly aggregating the\nclients' datasets due to privacy concerns, one solution is to collect more data\nfor each individual client, another is to privately aggregate information from\nmodels trained on each client's data. In this work, two approaches for private\nmodel aggregation are proposed that enable the transfer of knowledge from\nexisting models trained on other companies' datasets to a new company with\nlimited labeled data while protecting each client company's underlying\nindividual sensitive information. The two proposed approaches are based on\nstate-of-the-art private learning algorithms: Differentially Private\nPermutation-based Stochastic Gradient Descent and Approximate Minima\nPerturbation. We empirically show that by leveraging differentially private\ntechniques, we can enable private model aggregation and augment data utility\nwhile providing provable mathematical guarantees on privacy. The proposed\nmethods thus provide significant business value for SaaS companies and their\nclients, specifically as a solution for the cold-start problem. \n\n"}
{"id": "1811.04985", "contents": "Title: Generalized Ternary Connect: End-to-End Learning and Compression of\n  Multiplication-Free Deep Neural Networks Abstract: The use of deep neural networks in edge computing devices hinges on the\nbalance between accuracy and complexity of computations. Ternary Connect (TC)\n\\cite{lin2015neural} addresses this issue by restricting the parameters to\nthree levels $-1, 0$, and $+1$, thus eliminating multiplications in the forward\npass of the network during prediction. We propose Generalized Ternary Connect\n(GTC), which allows an arbitrary number of levels while at the same time\neliminating multiplications by restricting the parameters to integer powers of\ntwo. The primary contribution is that GTC learns the number of levels and their\nvalues for each layer, jointly with the weights of the network in an end-to-end\nfashion. Experiments on MNIST and CIFAR-10 show that GTC naturally converges to\nan `almost binary' network for deep classification networks (e.g. VGG-16) and\ndeep variational auto-encoders, with negligible loss of classification accuracy\nand comparable visual quality of generated samples respectively. We demonstrate\nsuperior compression and similar accuracy of GTC in comparison to several\nstate-of-the-art methods for neural network compression. We conclude with\nsimulations showing the potential benefits of GTC in hardware. \n\n"}
{"id": "1811.06100", "contents": "Title: Newton Methods for Convolutional Neural Networks Abstract: Deep learning involves a difficult non-convex optimization problem, which is\noften solved by stochastic gradient (SG) methods. While SG is usually\neffective, it may not be robust in some situations. Recently, Newton methods\nhave been investigated as an alternative optimization technique, but nearly all\nexisting studies consider only fully-connected feedforward neural networks.\nThey do not investigate other types of networks such as Convolutional Neural\nNetworks (CNN), which are more commonly used in deep-learning applications. One\nreason is that Newton methods for CNN involve complicated operations, and so\nfar no works have conducted a thorough investigation. In this work, we give\ndetails of all building blocks including function, gradient, and Jacobian\nevaluation, and Gauss-Newton matrix-vector products. These basic components are\nvery important because with them further developments of Newton methods for CNN\nbecome possible. We show that an efficient MATLAB implementation can be done in\njust several hundred lines of code and demonstrate that the Newton method gives\ncompetitive test accuracy. \n\n"}
{"id": "1811.06128", "contents": "Title: Machine Learning for Combinatorial Optimization: a Methodological Tour\n  d'Horizon Abstract: This paper surveys the recent attempts, both from the machine learning and\noperations research communities, at leveraging machine learning to solve\ncombinatorial optimization problems. Given the hard nature of these problems,\nstate-of-the-art algorithms rely on handcrafted heuristics for making decisions\nthat are otherwise too expensive to compute or mathematically not well defined.\nThus, machine learning looks like a natural candidate to make such decisions in\na more principled and optimized way. We advocate for pushing further the\nintegration of machine learning and combinatorial optimization and detail a\nmethodology to do so. A main point of the paper is seeing generic optimization\nproblems as data points and inquiring what is the relevant distribution of\nproblems to use for learning on a given task. \n\n"}
{"id": "1811.06529", "contents": "Title: On transfer learning using a MAC model variant Abstract: We introduce a variant of the MAC model (Hudson and Manning, ICLR 2018) with\na simplified set of equations that achieves comparable accuracy, while training\nfaster. We evaluate both models on CLEVR and CoGenT, and show that, transfer\nlearning with fine-tuning results in a 15 point increase in accuracy, matching\nthe state of the art. Finally, in contrast, we demonstrate that improper\nfine-tuning can actually reduce a model's accuracy as well. \n\n"}
{"id": "1811.06817", "contents": "Title: Evaluating Uncertainty Quantification in End-to-End Autonomous Driving\n  Control Abstract: A rise in popularity of Deep Neural Networks (DNNs), attributed to more\npowerful GPUs and widely available datasets, has seen them being increasingly\nused within safety-critical domains. One such domain, self-driving, has\nbenefited from significant performance improvements, with millions of miles\nhaving been driven with no human intervention. Despite this, crashes and\nerroneous behaviours still occur, in part due to the complexity of verifying\nthe correctness of DNNs and a lack of safety guarantees.\n  In this paper, we demonstrate how quantitative measures of uncertainty can be\nextracted in real-time, and their quality evaluated in end-to-end controllers\nfor self-driving cars. To this end we utilise a recent method for gathering\napproximate uncertainty information from DNNs without changing the network's\narchitecture. We propose evaluation techniques for the uncertainty on two\nseparate architectures which use the uncertainty to predict crashes up to five\nseconds in advance. We find that mutual information, a measure of uncertainty\nin classification networks, is a promising indicator of forthcoming crashes. \n\n"}
{"id": "1811.07630", "contents": "Title: SEIGAN: Towards Compositional Image Generation by Simultaneously\n  Learning to Segment, Enhance, and Inpaint Abstract: We present a novel approach to image manipulation and understanding by\nsimultaneously learning to segment object masks, paste objects to another\nbackground image, and remove them from original images. For this purpose, we\ndevelop a novel generative model for compositional image generation, SEIGAN\n(Segment-Enhance-Inpaint Generative Adversarial Network), which learns these\nthree operations together in an adversarial architecture with additional cycle\nconsistency losses. To train, SEIGAN needs only bounding box supervision and\ndoes not require pairing or ground truth masks. SEIGAN produces better\ngenerated images (evaluated by human assessors) than other approaches and\nproduces high-quality segmentation masks, improving over other adversarially\ntrained approaches and getting closer to the results of fully supervised\ntraining. \n\n"}
{"id": "1811.08008", "contents": "Title: End-to-End Retrieval in Continuous Space Abstract: Most text-based information retrieval (IR) systems index objects by words or\nphrases. These discrete systems have been augmented by models that use\nembeddings to measure similarity in continuous space. But continuous-space\nmodels are typically used just to re-rank the top candidates. We consider the\nproblem of end-to-end continuous retrieval, where standard approximate nearest\nneighbor (ANN) search replaces the usual discrete inverted index, and rely\nentirely on distances between learned embeddings. By training simple models\nspecifically for retrieval, with an appropriate model architecture, we improve\non a discrete baseline by 8% and 26% (MAP) on two similar-question retrieval\ntasks. We also discuss the problem of evaluation for retrieval systems, and\nshow how to modify existing pairwise similarity datasets for this purpose. \n\n"}
{"id": "1811.08019", "contents": "Title: Role action embeddings: scalable representation of network positions Abstract: We consider the question of embedding nodes with similar local neighborhoods\ntogether in embedding space, commonly referred to as \"role embeddings.\" We\npropose RAE, an unsupervised framework that learns role embeddings. It combines\na within-node loss function and a graph neural network (GNN) architecture to\nplace nodes with similar local neighborhoods close in embedding space. We also\npropose a faster way of generating negative examples called neighbor shuffling,\nwhich quickly creates negative examples directly within batches. These\ntechniques can be easily combined with existing GNN methods to create\nunsupervised role embeddings at scale. We then explore role action embeddings,\nwhich summarize the non-structural features in a node's neighborhood, leading\nto better performance on node classification tasks. We find that the model\narchitecture proposed here provides strong performance on both graph and node\nclassification tasks, in some cases competitive with semi-supervised methods. \n\n"}
{"id": "1811.08295", "contents": "Title: T-CGAN: Conditional Generative Adversarial Network for Data Augmentation\n  in Noisy Time Series with Irregular Sampling Abstract: In this paper we propose a data augmentation method for time series with\nirregular sampling, Time-Conditional Generative Adversarial Network (T-CGAN).\nOur approach is based on Conditional Generative Adversarial Networks (CGAN),\nwhere the generative step is implemented by a deconvolutional NN and the\ndiscriminative step by a convolutional NN. Both the generator and the\ndiscriminator are conditioned on the sampling timestamps, to learn the hidden\nrelationship between data and timestamps, and consequently to generate new time\nseries. We evaluate our model with synthetic and real-world datasets. For the\nsynthetic data, we compare the performance of a classifier trained with\nT-CGAN-generated data, against the performance of the same classifier trained\non the original data. Results show that classifiers trained on T-CGAN-generated\ndata perform the same as classifiers trained on real data, even with very short\ntime series and small training sets. For the real world datasets, we compare\nour method with other techniques of data augmentation for time series, such as\ntime slicing and time warping, over a classification problem with unbalanced\ndatasets. Results show that our method always outperforms the other approaches,\nboth in case of regularly sampled and irregularly sampled time series. We\nachieve particularly good performance in case with a small training set and\nshort, noisy, irregularly-sampled time series. \n\n"}
{"id": "1811.08382", "contents": "Title: Locally Private Gaussian Estimation Abstract: We study a basic private estimation problem: each of $n$ users draws a single\ni.i.d. sample from an unknown Gaussian distribution, and the goal is to\nestimate the mean of this Gaussian distribution while satisfying local\ndifferential privacy for each user. Informally, local differential privacy\nrequires that each data point is individually and independently privatized\nbefore it is passed to a learning algorithm. Locally private Gaussian\nestimation is therefore difficult because the data domain is unbounded: users\nmay draw arbitrarily different inputs, but local differential privacy\nnonetheless mandates that different users have (worst-case) similar privatized\noutput distributions. We provide both adaptive two-round solutions and\nnonadaptive one-round solutions for locally private Gaussian estimation. We\nthen partially match these upper bounds with an information-theoretic lower\nbound. This lower bound shows that our accuracy guarantees are tight up to\nlogarithmic factors for all sequentially interactive\n$(\\varepsilon,\\delta)$-locally private protocols. \n\n"}
{"id": "1811.09054", "contents": "Title: Enhanced Expressive Power and Fast Training of Neural Networks by Random\n  Projections Abstract: Random projections are able to perform dimension reduction efficiently for\ndatasets with nonlinear low-dimensional structures. One well-known example is\nthat random matrices embed sparse vectors into a low-dimensional subspace\nnearly isometrically, known as the restricted isometric property in compressed\nsensing. In this paper, we explore some applications of random projections in\ndeep neural networks. We provide the expressive power of fully connected neural\nnetworks when the input data are sparse vectors or form a low-dimensional\nsmooth manifold. We prove that the number of neurons required for approximating\na Lipschitz function with a prescribed precision depends on the sparsity or the\ndimension of the manifold and weakly on the dimension of the input vector. The\nkey in our proof is that random projections embed stably the set of sparse\nvectors or a low-dimensional smooth manifold into a low-dimensional subspace.\nBased on this fact, we also propose some new neural network models, where at\neach layer the input is first projected onto a low-dimensional subspace by a\nrandom projection and then the standard linear connection and non-linear\nactivation are applied. In this way, the number of parameters in neural\nnetworks is significantly reduced, and therefore the training of neural\nnetworks can be accelerated without too much performance loss. \n\n"}
{"id": "1811.09567", "contents": "Title: How does Lipschitz Regularization Influence GAN Training? Abstract: Despite the success of Lipschitz regularization in stabilizing GAN training,\nthe exact reason of its effectiveness remains poorly understood. The direct\neffect of $K$-Lipschitz regularization is to restrict the $L2$-norm of the\nneural network gradient to be smaller than a threshold $K$ (e.g., $K=1$) such\nthat $\\|\\nabla f\\| \\leq K$. In this work, we uncover an even more important\neffect of Lipschitz regularization by examining its impact on the loss\nfunction: It degenerates GAN loss functions to almost linear ones by\nrestricting their domain and interval of attainable gradient values. Our\nanalysis shows that loss functions are only successful if they are degenerated\nto almost linear ones. We also show that loss functions perform poorly if they\nare not degenerated and that a wide range of functions can be used as loss\nfunction as long as they are sufficiently degenerated by regularization.\nBasically, Lipschitz regularization ensures that all loss functions effectively\nwork in the same way. Empirically, we verify our proposition on the MNIST,\nCIFAR10 and CelebA datasets. \n\n"}
{"id": "1811.09621", "contents": "Title: GuacaMol: Benchmarking Models for De Novo Molecular Design Abstract: De novo design seeks to generate molecules with required property profiles by\nvirtual design-make-test cycles. With the emergence of deep learning and neural\ngenerative models in many application areas, models for molecular design based\non neural networks appeared recently and show promising results. However, the\nnew models have not been profiled on consistent tasks, and comparative studies\nto well-established algorithms have only seldom been performed.\n  To standardize the assessment of both classical and neural models for de novo\nmolecular design, we propose an evaluation framework, GuacaMol, based on a\nsuite of standardized benchmarks. The benchmark tasks encompass measuring the\nfidelity of the models to reproduce the property distribution of the training\nsets, the ability to generate novel molecules, the exploration and exploitation\nof chemical space, and a variety of single and multi-objective optimization\ntasks. The benchmarking open-source Python code, and a leaderboard can be found\non https://benevolent.ai/guacamol \n\n"}
{"id": "1811.10146", "contents": "Title: Frequency Principle in Deep Learning with General Loss Functions and Its\n  Potential Application Abstract: Previous studies have shown that deep neural networks (DNNs) with common\nsettings often capture target functions from low to high frequency, which is\ncalled Frequency Principle (F-Principle). It has also been shown that\nF-Principle can provide an understanding to the often observed good\ngeneralization ability of DNNs. However, previous studies focused on the loss\nfunction of mean square error, while various loss functions are used in\npractice. In this work, we show that the F-Principle holds for a general loss\nfunction (e.g., mean square error, cross entropy, etc.). In addition, DNN's\nF-Principle may be applied to develop numerical schemes for solving various\nproblems which would benefit from a fast converging of low frequency. As an\nexample of the potential usage of F-Principle, we apply DNN in solving\ndifferential equations, in which conventional methods (e.g., Jacobi method) is\nusually slow in solving problems due to the convergence from high to low\nfrequency. \n\n"}
{"id": "1811.10636", "contents": "Title: Evolving Space-Time Neural Architectures for Videos Abstract: We present a new method for finding video CNN architectures that capture rich\nspatio-temporal information in videos. Previous work, taking advantage of 3D\nconvolutions, obtained promising results by manually designing video CNN\narchitectures. We here develop a novel evolutionary search algorithm that\nautomatically explores models with different types and combinations of layers\nto jointly learn interactions between spatial and temporal aspects of video\nrepresentations. We demonstrate the generality of this algorithm by applying it\nto two meta-architectures, obtaining new architectures superior to manually\ndesigned architectures. Further, we propose a new component, the iTGM layer,\nwhich more efficiently utilizes its parameters to allow learning of space-time\ninteractions over longer time horizons. The iTGM layer is often preferred by\nthe evolutionary algorithm and allows building cost-efficient networks. The\nproposed approach discovers new and diverse video architectures that were\npreviously unknown. More importantly they are both more accurate and faster\nthan prior models, and outperform the state-of-the-art results on multiple\ndatasets we test, including HMDB, Kinetics, and Moments in Time. We will open\nsource the code and models, to encourage future model development. \n\n"}
{"id": "1811.10658", "contents": "Title: HELOC Applicant Risk Performance Evaluation by Topological Hierarchical\n  Decomposition Abstract: Strong regulations in the financial industry mean that any decisions based on\nmachine learning need to be explained. This precludes the use of powerful\nsupervised techniques such as neural networks. In this study we propose a new\nunsupervised and semi-supervised technique known as the topological\nhierarchical decomposition (THD). This process breaks a dataset down into ever\nsmaller groups, where groups are associated with a simplicial complex that\napproximate the underlying topology of a dataset. We apply THD to the FICO\nmachine learning challenge dataset, consisting of anonymized home equity loan\napplications using the MAPPER algorithm to build simplicial complexes. We\nidentify different groups of individuals unable to pay back loans, and\nillustrate how the distribution of feature values in a simplicial complex can\nbe used to explain the decision to grant or deny a loan by extracting\nillustrative explanations from two THDs on the dataset. \n\n"}
{"id": "1811.10902", "contents": "Title: Kernel-based Multi-Task Contextual Bandits in Cellular Network\n  Configuration Abstract: Cellular network configuration plays a critical role in network performance.\nIn current practice, network configuration depends heavily on field experience\nof engineers and often remains static for a long period of time. This practice\nis far from optimal. To address this limitation, online-learning-based\napproaches have great potentials to automate and optimize network\nconfiguration. Learning-based approaches face the challenges of learning a\nhighly complex function for each base station and balancing the fundamental\nexploration-exploitation tradeoff while minimizing the exploration cost.\nFortunately, in cellular networks, base stations (BSs) often have similarities\neven though they are not identical. To leverage such similarities, we propose\nkernel-based multi-BS contextual bandit algorithm based on multi-task learning.\nIn the algorithm, we leverage the similarity among different BSs defined by\nconditional kernel embedding. We present theoretical analysis of the proposed\nalgorithm in terms of regret and multi-task-learning efficiency. We evaluate\nthe effectiveness of our algorithm based on a simulator built by real traces. \n\n"}
{"id": "1811.11079", "contents": "Title: Robust Classification of Financial Risk Abstract: Algorithms are increasingly common components of high-impact decision-making,\nand a growing body of literature on adversarial examples in laboratory settings\nindicates that standard machine learning models are not robust. This suggests\nthat real-world systems are also susceptible to manipulation or\nmisclassification, which especially poses a challenge to machine learning\nmodels used in financial services. We use the loan grade classification problem\nto explore how machine learning models are sensitive to small changes in\nuser-reported data, using adversarial attacks documented in the literature and\nan original, domain-specific attack. Our work shows that a robust optimization\nalgorithm can build models for financial services that are resistant to\nmisclassification on perturbations. To the best of our knowledge, this is the\nfirst study of adversarial attacks and defenses for deep learning in financial\nservices. \n\n"}
{"id": "1811.11083", "contents": "Title: Generative Adversarial Network Training is a Continual Learning Problem Abstract: Generative Adversarial Networks (GANs) have proven to be a powerful framework\nfor learning to draw samples from complex distributions. However, GANs are also\nnotoriously difficult to train, with mode collapse and oscillations a common\nproblem. We hypothesize that this is at least in part due to the evolution of\nthe generator distribution and the catastrophic forgetting tendency of neural\nnetworks, which leads to the discriminator losing the ability to remember\nsynthesized samples from previous instantiations of the generator. Recognizing\nthis, our contributions are twofold. First, we show that GAN training makes for\na more interesting and realistic benchmark for continual learning methods\nevaluation than some of the more canonical datasets. Second, we propose\nleveraging continual learning techniques to augment the discriminator,\npreserving its ability to recognize previous generator samples. We show that\nthe resulting methods add only a light amount of computation, involve minimal\nchanges to the model, and result in better overall performance on the examined\nimage and text generation tasks. \n\n"}
{"id": "1811.11209", "contents": "Title: Iterative Transformer Network for 3D Point Cloud Abstract: 3D point cloud is an efficient and flexible representation of 3D structures.\nRecently, neural networks operating on point clouds have shown superior\nperformance on 3D understanding tasks such as shape classification and part\nsegmentation. However, performance on such tasks is evaluated on complete\nshapes aligned in a canonical frame, while real world 3D data are partial and\nunaligned. A key challenge in learning from partial, unaligned point cloud data\nis to learn features that are invariant or equivariant with respect to\ngeometric transformations. To address this challenge, we propose the Iterative\nTransformer Network (IT-Net), a network module that canonicalizes the pose of a\npartial object with a series of 3D rigid transformations predicted in an\niterative fashion. We demonstrate the efficacy of IT-Net as an anytime pose\nestimator from partial point clouds without using complete object models.\nFurther, we show that IT-Net achieves superior performance over alternative 3D\ntransformer networks on various tasks, such as partial shape classification and\nobject part segmentation. \n\n"}
{"id": "1811.11293", "contents": "Title: Questioning the assumptions behind fairness solutions Abstract: In addition to their benefits, optimization systems can have negative\neconomic, moral, social, and political effects on populations as well as their\nenvironments. Frameworks like fairness have been proposed to aid service\nproviders in addressing subsequent bias and discrimination during data\ncollection and algorithm design. However, recent reports of neglect,\nunresponsiveness, and malevolence cast doubt on whether service providers can\neffectively implement fairness solutions. These reports invite us to revisit\nassumptions made about the service providers in fairness solutions. Namely,\nthat service providers have (i) the incentives or (ii) the means to mitigate\noptimization externalities. Moreover, the environmental impact of these systems\nsuggests that we need (iii) novel frameworks that consider systems other than\nalgorithmic decision-making and recommender systems, and (iv) solutions that go\nbeyond removing related algorithmic biases. Going forward, we propose\nProtective Optimization Technologies that enable optimization subjects to\ndefend against negative consequences of optimization systems. \n\n"}
{"id": "1811.11400", "contents": "Title: FADL:Federated-Autonomous Deep Learning for Distributed Electronic\n  Health Record Abstract: Electronic health record (EHR) data is collected by individual institutions\nand often stored across locations in silos. Getting access to these data is\ndifficult and slow due to security, privacy, regulatory, and operational\nissues. We show, using ICU data from 58 different hospitals, that machine\nlearning models to predict patient mortality can be trained efficiently without\nmoving health data out of their silos using a distributed machine learning\nstrategy. We propose a new method, called Federated-Autonomous Deep Learning\n(FADL) that trains part of the model using all data sources in a distributed\nmanner and other parts using data from specific data sources. We observed that\nFADL outperforms traditional federated learning strategy and conclude that\nbalance between global and local training is an important factor to consider\nwhen design distributed machine learning methods , especially in healthcare. \n\n"}
{"id": "1811.11441", "contents": "Title: Trajectory-based Learning for Ball-in-Maze Games Abstract: Deep Reinforcement Learning has shown tremendous success in solving several\ngames and tasks in robotics. However, unlike humans, it generally requires a\nlot of training instances. Trajectories imitating to solve the task at hand can\nhelp to increase sample-efficiency of deep RL methods. In this paper, we\npresent a simple approach to use such trajectories, applied to the challenging\nBall-in-Maze Games, recently introduced in the literature. We show that in\nspite of not using human-generated trajectories and just using the simulator as\na model to generate a limited number of trajectories, we can get a speed-up of\nabout 2-3x in the learning process. We also discuss some challenges we observed\nwhile using trajectory-based learning for very sparse reward functions. \n\n"}
{"id": "1811.11474", "contents": "Title: Improved Calibration of Numerical Integration Error in Sigma-Point\n  Filters Abstract: The sigma-point filters, such as the UKF, which exploit numerical quadrature\nto obtain an additional order of accuracy in the moment transformation step,\nare popular alternatives to the ubiquitous EKF. The classical quadrature rules\nused in the sigma-point filters are motivated via polynomial approximation of\nthe integrand, however in the applied context these assumptions cannot always\nbe justified. As a result, quadrature error can introduce bias into estimated\nmoments, for which there is no compensatory mechanism in the classical\nsigma-point filters. This can lead in turn to estimates and predictions that\nare poorly calibrated. In this article, we investigate the Bayes-Sard\nquadrature method in the context of sigma-point filters, which enables\nuncertainty due to quadrature error to be formalised within a probabilistic\nmodel. Our first contribution is to derive the well-known classical quadratures\nas special cases of the Bayes-Sard quadrature method. Then a general-purpose\nmoment transform is developed and utilised in the design of novel sigma-point\nfilters, so that uncertainty due to quadrature error is explicitly quantified.\nNumerical experiments on a challenging tracking example with misspecified\ninitial conditions show that the additional uncertainty quantification built\ninto our method leads to better-calibrated state estimates with improved RMSE. \n\n"}
{"id": "1811.11818", "contents": "Title: Disease phenotyping using deep learning: A diabetes case study Abstract: Characterization of a patient clinical phenotype is central to biomedical\ninformatics. ICD codes, assigned to inpatient encounters by coders, is\nimportant for population health and cohort discovery when clinical information\nis limited. While ICD codes are assigned to patients by professionals trained\nand certified in coding there is substantial variability in coding. We present\na methodology that uses deep learning methods to model coder decision making\nand that predicts ICD codes. Our approach predicts codes based on demographics,\nlab results, and medications, as well as codes from previous encounters. We are\nable to predict existing codes with high accuracy for all three of the test\ncases we investigated: diabetes, acute renal failure, and chronic kidney\ndisease. We employed a panel of clinicians, in a blinded manner, to assess\nground truth and compared the predictions of coders, model and clinicians. When\ndisparities between the model prediction and coder assigned codes were\nreviewed, our model outperformed coder assigned ICD codes. \n\n"}
{"id": "1811.12156", "contents": "Title: Improved Deep Embeddings for Inferencing with Multi-Layered Networks Abstract: Inferencing with network data necessitates the mapping of its nodes into a\nvector space, where the relationships are preserved. However, with\nmulti-layered networks, where multiple types of relationships exist for the\nsame set of nodes, it is crucial to exploit the information shared between\nlayers, in addition to the distinct aspects of each layer. In this paper, we\npropose a novel approach that first obtains node embeddings in all layers\njointly via DeepWalk on a \\textit{supra} graph, which allows interactions\nbetween layers, and then fine-tunes the embeddings to encourage cohesive\nstructure in the latent space. With empirical studies in node classification,\nlink prediction and multi-layered community detection, we show that the\nproposed approach outperforms existing single- and multi-layered network\nembedding algorithms on several benchmarks. In addition to effectively scaling\nto a large number of layers (tested up to $37$), our approach consistently\nproduces highly modular community structure, even when compared to methods that\ndirectly optimize for the modularity function. \n\n"}
{"id": "1811.12181", "contents": "Title: What Should I Learn First: Introducing LectureBank for NLP Education and\n  Prerequisite Chain Learning Abstract: Recent years have witnessed the rising popularity of Natural Language\nProcessing (NLP) and related fields such as Artificial Intelligence (AI) and\nMachine Learning (ML). Many online courses and resources are available even for\nthose without a strong background in the field. Often the student is curious\nabout a specific topic but does not quite know where to begin studying. To\nanswer the question of \"what should one learn first,\" we apply an\nembedding-based method to learn prerequisite relations for course concepts in\nthe domain of NLP. We introduce LectureBank, a dataset containing 1,352 English\nlecture files collected from university courses which are each classified\naccording to an existing taxonomy as well as 208 manually-labeled prerequisite\nrelation topics, which is publicly available. The dataset will be useful for\neducational purposes such as lecture preparation and organization as well as\napplications such as reading list generation. Additionally, we experiment with\nneural graph-based networks and non-neural classifiers to learn these\nprerequisite relations from our dataset. \n\n"}
{"id": "1811.12297", "contents": "Title: Incremental Scene Synthesis Abstract: We present a method to incrementally generate complete 2D or 3D scenes with\nthe following properties: (a) it is globally consistent at each step according\nto a learned scene prior, (b) real observations of a scene can be incorporated\nwhile observing global consistency, (c) unobserved regions can be hallucinated\nlocally in consistence with previous observations, hallucinations and global\npriors, and (d) hallucinations are statistical in nature, i.e., different\nscenes can be generated from the same observations. To achieve this, we model\nthe virtual scene, where an active agent at each step can either perceive an\nobserved part of the scene or generate a local hallucination. The latter can be\ninterpreted as the agent's expectation at this step through the scene and can\nbe applied to autonomous navigation. In the limit of observing real data at\neach point, our method converges to solving the SLAM problem. It can otherwise\nsample entirely imagined scenes from prior distributions. Besides autonomous\nagents, applications include problems where large data is required for building\nrobust real-world applications, but few samples are available. We demonstrate\nefficacy on various 2D as well as 3D data. \n\n"}
{"id": "1811.12752", "contents": "Title: Practical methods for graph two-sample testing Abstract: Hypothesis testing for graphs has been an important tool in applied research\nfields for more than two decades, and still remains a challenging problem as\none often needs to draw inference from few replicates of large graphs. Recent\nstudies in statistics and learning theory have provided some theoretical\ninsights about such high-dimensional graph testing problems, but the\npracticality of the developed theoretical methods remains an open question.\n  In this paper, we consider the problem of two-sample testing of large graphs.\nWe demonstrate the practical merits and limitations of existing theoretical\ntests and their bootstrapped variants. We also propose two new tests based on\nasymptotic distributions. We show that these tests are computationally less\nexpensive and, in some cases, more reliable than the existing methods. \n\n"}
{"id": "1811.12866", "contents": "Title: Super-Resolution via Image-Adapted Denoising CNNs: Incorporating\n  External and Internal Learning Abstract: While deep neural networks exhibit state-of-the-art results in the task of\nimage super-resolution (SR) with a fixed known acquisition process (e.g., a\nbicubic downscaling kernel), they experience a huge performance loss when the\nreal observation model mismatches the one used in training. Recently, two\ndifferent techniques suggested to mitigate this deficiency, i.e., enjoy the\nadvantages of deep learning without being restricted by the training phase. The\nfirst one follows the plug-and-play (P&P) approach that solves general inverse\nproblems (e.g., SR) by using Gaussian denoisers for handling the prior term in\nmodel-based optimization schemes. The second builds on internal recurrence of\ninformation inside a single image, and trains a super-resolver network at test\ntime on examples synthesized from the low-resolution image. Our work\nincorporates these two independent strategies, enjoying the impressive\ngeneralization capabilities of deep learning, captured by the first, and\nfurther improving it through internal learning at test time. First, we apply a\nrecent P&P strategy to SR. Then, we show how it may become image-adaptive in\ntest time. This technique outperforms the above two strategies on popular\ndatasets and gives better results than other state-of-the-art methods in\npractical cases where the observation model is inexact or unknown in advance. \n\n"}
{"id": "1812.00249", "contents": "Title: On Compressing U-net Using Knowledge Distillation Abstract: We study the use of knowledge distillation to compress the U-net\narchitecture. We show that, while standard distillation is not sufficient to\nreliably train a compressed U-net, introducing other regularization methods,\nsuch as batch normalization and class re-weighting, in knowledge distillation\nsignificantly improves the training process. This allows us to compress a U-net\nby over 1000x, i.e., to 0.1% of its original number of parameters, at a\nnegligible decrease in performance. \n\n"}
{"id": "1812.00371", "contents": "Title: Predicting Inpatient Discharge Prioritization With Electronic Health\n  Records Abstract: Identifying patients who will be discharged within 24 hours can improve\nhospital resource management and quality of care. We studied this problem using\neight years of Electronic Health Records (EHR) data from Stanford Hospital. We\nfit models to predict 24 hour discharge across the entire inpatient population.\nThe best performing models achieved an area under the receiver-operator\ncharacteristic curve (AUROC) of 0.85 and an AUPRC of 0.53 on a held out test\nset. This model was also well calibrated. Finally, we analyzed the utility of\nthis model in a decision theoretic framework to identify regions of ROC space\nin which using the model increases expected utility compared to the trivial\nalways negative or always positive classifiers. \n\n"}
{"id": "1812.00769", "contents": "Title: Testing Changes in Communities for the Stochastic Block Model Abstract: We propose and analyze the problems of \\textit{community goodness-of-fit and\ntwo-sample testing} for stochastic block models (SBM), where changes arise due\nto modification in community memberships of nodes. Motivated by practical\napplications, we consider the challenging sparse regime, where expected node\ndegrees are constant, and the inter-community mean degree ($b$) scales\nproportionally to intra-community mean degree ($a$). Prior work has sharply\ncharacterized partial or full community recovery in terms of a \"signal-to-noise\nratio\" ($\\mathrm{SNR}$) based on $a$ and $b$. For both problems, we propose\ncomputationally-efficient tests that can succeed far beyond the regime where\nrecovery of community membership is even possible. Overall, for large changes,\n$s \\gg \\sqrt{n}$, we need only $\\mathrm{SNR}= O(1)$ whereas a na\\\"ive test\nbased on community recovery with $O(s)$ errors requires $\\mathrm{SNR}=\n\\Theta(\\log n)$. Conversely, in the small change regime, $s \\ll \\sqrt{n}$, via\nan information-theoretic lower bound, we show that, surprisingly, no algorithm\ncan do better than the na\\\"ive algorithm that first estimates the community up\nto $O(s)$ errors and then detects changes. We validate these phenomena\nnumerically on SBMs and on real-world datasets as well as Markov Random Fields\nwhere we only observe node data rather than the existence of links. \n\n"}
{"id": "1812.00880", "contents": "Title: Joint Mapping and Calibration via Differentiable Sensor Fusion Abstract: We leverage automatic differentiation (AD) and probabilistic programming to\ndevelop an end-to-end optimization algorithm for batch triangulation of a large\nnumber of unknown objects. Given noisy detections extracted from noisily\ngeo-located street level imagery without depth information, we jointly estimate\nthe number and location of objects of different types, together with parameters\nfor sensor noise characteristics and prior distribution of objects conditioned\non side information. The entire algorithm is framed as nested stochastic\nvariational inference. An inner loop solves a soft data association problem via\nloopy belief propagation; a middle loop performs soft EM clustering using a\nregularized Newton solver (leveraging an AD framework); an outer loop\nbackpropagates through the inner loops to train global parameters. We place\npriors over sensor parameters for different traffic object types, and\ndemonstrate improvements with richer priors incorporating knowledge of the\nenvironment.\n  We test our algorithm on detections of road signs observed by cars with\nmounted cameras, though in practice this technique can be used for any\ngeo-tagged images. The detections were extracted by neural image detectors and\nclassifiers, and we independently triangulate each type of sign (e.g. stop,\ntraffic light). We find that our model is more robust to DNN misclassifications\nthan current methods, generalizes across sign types, and can use geometric\ninformation to increase precision. Our algorithm outperforms our current\nproduction baseline based on k-means clustering. We show that variational\ninference training allows generalization by learning sign-specific parameters. \n\n"}
{"id": "1812.00922", "contents": "Title: Multi-agent Deep Reinforcement Learning with Extremely Noisy\n  Observations Abstract: Multi-agent reinforcement learning systems aim to provide interacting agents\nwith the ability to collaboratively learn and adapt to the behaviour of other\nagents. In many real-world applications, the agents can only acquire a partial\nview of the world. Here we consider a setting whereby most agents' observations\nare also extremely noisy, hence only weakly correlated to the true state of the\nenvironment. Under these circumstances, learning an optimal policy becomes\nparticularly challenging, even in the unrealistic case that an agent's policy\ncan be made conditional upon all other agents' observations. To overcome these\ndifficulties, we propose a multi-agent deep deterministic policy gradient\nalgorithm enhanced by a communication medium (MADDPG-M), which implements a\ntwo-level, concurrent learning mechanism. An agent's policy depends on its own\nprivate observations as well as those explicitly shared by others through a\ncommunication medium. At any given point in time, an agent must decide whether\nits private observations are sufficiently informative to be shared with others.\nHowever, our environments provide no explicit feedback informing an agent\nwhether a communication action is beneficial, rather the communication policies\nmust also be learned through experience concurrently to the main policies. Our\nexperimental results demonstrate that the algorithm performs well in six highly\nnon-stationary environments of progressively higher complexity, and offers\nsubstantial performance gains compared to the baselines. \n\n"}
{"id": "1812.01243", "contents": "Title: Efficient Attention: Attention with Linear Complexities Abstract: Dot-product attention has wide applications in computer vision and natural\nlanguage processing. However, its memory and computational costs grow\nquadratically with the input size. Such growth prohibits its application on\nhigh-resolution inputs. To remedy this drawback, this paper proposes a novel\nefficient attention mechanism equivalent to dot-product attention but with\nsubstantially less memory and computational costs. Its resource efficiency\nallows more widespread and flexible integration of attention modules into a\nnetwork, which leads to better accuracies. Empirical evaluations demonstrated\nthe effectiveness of its advantages. Efficient attention modules brought\nsignificant performance boosts to object detectors and instance segmenters on\nMS-COCO 2017. Further, the resource efficiency democratizes attention to\ncomplex models, where high costs prohibit the use of dot-product attention. As\nan exemplar, a model with efficient attention achieved state-of-the-art\naccuracies for stereo depth estimation on the Scene Flow dataset. Code is\navailable at https://github.com/cmsflash/efficient-attention. \n\n"}
{"id": "1812.01552", "contents": "Title: Exploration versus exploitation in reinforcement learning: a stochastic\n  control approach Abstract: We consider reinforcement learning (RL) in continuous time and study the\nproblem of achieving the best trade-off between exploration of a black box\nenvironment and exploitation of current knowledge. We propose an\nentropy-regularized reward function involving the differential entropy of the\ndistributions of actions, and motivate and devise an exploratory formulation\nfor the feature dynamics that captures repetitive learning under exploration.\nThe resulting optimization problem is a revitalization of the classical relaxed\nstochastic control. We carry out a complete analysis of the problem in the\nlinear--quadratic (LQ) setting and deduce that the optimal feedback control\ndistribution for balancing exploitation and exploration is Gaussian. This in\nturn interprets and justifies the widely adopted Gaussian exploration in RL,\nbeyond its simplicity for sampling. Moreover, the exploitation and exploration\nare captured, respectively and mutual-exclusively, by the mean and variance of\nthe Gaussian distribution. We also find that a more random environment contains\nmore learning opportunities in the sense that less exploration is needed. We\ncharacterize the cost of exploration, which, for the LQ case, is shown to be\nproportional to the entropy regularization weight and inversely proportional to\nthe discount rate. Finally, as the weight of exploration decays to zero, we\nprove the convergence of the solution of the entropy-regularized LQ problem to\nthe one of the classical LQ problem. \n\n"}
{"id": "1812.01664", "contents": "Title: A Stable Cardinality Distance for Topological Classification Abstract: This work incorporates topological features via persistence diagrams to\nclassify point cloud data arising from materials science. Persistence diagrams\nare multisets summarizing the connectedness and holes of given data. A new\ndistance on the space of persistence diagrams generates relevant input features\nfor a classification algorithm for materials science data. This distance\nmeasures the similarity of persistence diagrams using the cost of matching\npoints and a regularization term corresponding to cardinality differences\nbetween diagrams. Establishing stability properties of this distance provides\ntheoretical justification for the use of the distance in comparisons of such\ndiagrams. The classification scheme succeeds in determining the crystal\nstructure of materials on noisy and sparse data retrieved from synthetic atom\nprobe tomography experiments. \n\n"}
{"id": "1812.01729", "contents": "Title: Boltzmann Generators -- Sampling Equilibrium States of Many-Body Systems\n  with Deep Learning Abstract: Computing equilibrium states in condensed-matter many-body systems, such as\nsolvated proteins, is a long-standing challenge. Lacking methods for generating\nstatistically independent equilibrium samples in \"one shot\", vast computational\neffort is invested for simulating these system in small steps, e.g., using\nMolecular Dynamics. Combining deep learning and statistical mechanics, we here\ndevelop Boltzmann Generators, that are shown to generate unbiased one-shot\nequilibrium samples of representative condensed matter systems and proteins.\nBoltzmann Generators use neural networks to learn a coordinate transformation\nof the complex configurational equilibrium distribution to a distribution that\ncan be easily sampled. Accurate computation of free energy differences and\ndiscovery of new configurations are demonstrated, providing a statistical\nmechanics tool that can avoid rare events during sampling without prior\nknowledge of reaction coordinates. \n\n"}
{"id": "1812.02222", "contents": "Title: Predicting pregnancy using large-scale data from a women's health\n  tracking mobile application Abstract: Predicting pregnancy has been a fundamental problem in women's health for\nmore than 50 years. Previous datasets have been collected via carefully curated\nmedical studies, but the recent growth of women's health tracking mobile apps\noffers potential for reaching a much broader population. However, the\nfeasibility of predicting pregnancy from mobile health tracking data is\nunclear. Here we develop four models -- a logistic regression model, and 3 LSTM\nmodels -- to predict a woman's probability of becoming pregnant using data from\na women's health tracking app, Clue by BioWink GmbH. Evaluating our models on a\ndataset of 79 million logs from 65,276 women with ground truth pregnancy test\ndata, we show that our predicted pregnancy probabilities meaningfully stratify\nwomen: women in the top 10% of predicted probabilities have a 89% chance of\nbecoming pregnant over 6 menstrual cycles, as compared to a 27% chance for\nwomen in the bottom 10%. We develop a technique for extracting interpretable\ntime trends from our deep learning models, and show these trends are consistent\nwith previous fertility research. Our findings illustrate the potential that\nwomen's health tracking data offers for predicting pregnancy on a broader\npopulation; we conclude by discussing the steps needed to fulfill this\npotential. \n\n"}
{"id": "1812.03365", "contents": "Title: Neuromodulated Learning in Deep Neural Networks Abstract: In the brain, learning signals change over time and synaptic location, and\nare applied based on the learning history at the synapse, in the complex\nprocess of neuromodulation. Learning in artificial neural networks, on the\nother hand, is shaped by hyper-parameters set before learning starts, which\nremain static throughout learning, and which are uniform for the entire\nnetwork. In this work, we propose a method of deep artificial neuromodulation\nwhich applies the concepts of biological neuromodulation to stochastic gradient\ndescent. Evolved neuromodulatory dynamics modify learning parameters at each\nlayer in a deep neural network over the course of the network's training. We\nshow that the same neuromodulatory dynamics can be applied to different models\nand can scale to new problems not encountered during evolution. Finally, we\nexamine the evolved neuromodulation, showing that evolution found dynamic,\nlocation-specific learning strategies. \n\n"}
{"id": "1812.03395", "contents": "Title: Learning Graph Representation via Formal Concept Analysis Abstract: We present a novel method that can learn a graph representation from\nmultivariate data. In our representation, each node represents a cluster of\ndata points and each edge represents the subset-superset relationship between\nclusters, which can be mutually overlapped. The key to our method is to use\nformal concept analysis (FCA), which can extract hierarchical relationships\nbetween clusters based on the algebraic closedness property. We empirically\nshow that our method can effectively extract hierarchical structures of\nclusters compared to the baseline method. \n\n"}
{"id": "1812.04218", "contents": "Title: Learning Controllable Fair Representations Abstract: Learning data representations that are transferable and are fair with respect\nto certain protected attributes is crucial to reducing unfair decisions while\npreserving the utility of the data. We propose an information-theoretically\nmotivated objective for learning maximally expressive representations subject\nto fairness constraints. We demonstrate that a range of existing approaches\noptimize approximations to the Lagrangian dual of our objective. In contrast to\nthese existing approaches, our objective allows the user to control the\nfairness of the representations by specifying limits on unfairness. Exploiting\nduality, we introduce a method that optimizes the model parameters as well as\nthe expressiveness-fairness trade-off. Empirical evidence suggests that our\nproposed method can balance the trade-off between multiple notions of fairness\nand achieves higher expressiveness at a lower computational cost. \n\n"}
{"id": "1812.04407", "contents": "Title: Learning Item-Interaction Embeddings for User Recommendations Abstract: Industry-scale recommendation systems have become a cornerstone of the\ne-commerce shopping experience. For Etsy, an online marketplace with over 50\nmillion handmade and vintage items, users come to rely on personalized\nrecommendations to surface relevant items from its massive inventory. One\nhallmark of Etsy's shopping experience is the multitude of ways in which a user\ncan interact with an item they are interested in: they can view it, favorite\nit, add it to a collection, add it to cart, purchase it, etc. We hypothesize\nthat the different ways in which a user interacts with an item indicates\ndifferent kinds of intent. Consequently, a user's recommendations should be\nbased not only on the item from their past activity, but also the way in which\nthey interacted with that item. In this paper, we propose a novel method for\nlearning interaction-based item embeddings that encode the co-occurrence\npatterns of not only the item itself, but also the interaction type. The\nlearned embeddings give us a convenient way of approximating the likelihood\nthat one item-interaction pair would co-occur with another by way of a simple\ninner product. Because of its computational efficiency, our model lends itself\nnaturally as a candidate set selection method, and we evaluate it as such in an\nindustry-scale recommendation system that serves live traffic on Etsy.com. Our\nexperiments reveal that taking interaction type into account shows promising\nresults in improving the accuracy of modeling user shopping behavior. \n\n"}
{"id": "1812.05645", "contents": "Title: Sequence Prediction using Spectral RNNs Abstract: Fourier methods have a long and proven track record as an excellent tool in\ndata processing. As memory and computational constraints gain importance in\nembedded and mobile applications, we propose to combine Fourier methods and\nrecurrent neural network architectures. The short-time Fourier transform allows\nus to efficiently process multiple samples at a time. Additionally, weight\nreductions trough low pass filtering is possible. We predict time series data\ndrawn from the chaotic Mackey-Glass differential equation and real-world power\nload and motion capture data. \n\n"}
{"id": "1812.05793", "contents": "Title: Adversarial Sample Detection for Deep Neural Network through Model\n  Mutation Testing Abstract: Deep neural networks (DNN) have been shown to be useful in a wide range of\napplications. However, they are also known to be vulnerable to adversarial\nsamples. By transforming a normal sample with some carefully crafted human\nimperceptible perturbations, even highly accurate DNN make wrong decisions.\nMultiple defense mechanisms have been proposed which aim to hinder the\ngeneration of such adversarial samples. However, a recent work show that most\nof them are ineffective. In this work, we propose an alternative approach to\ndetect adversarial samples at runtime. Our main observation is that adversarial\nsamples are much more sensitive than normal samples if we impose random\nmutations on the DNN. We thus first propose a measure of `sensitivity' and show\nempirically that normal samples and adversarial samples have distinguishable\nsensitivity. We then integrate statistical hypothesis testing and model\nmutation testing to check whether an input sample is likely to be normal or\nadversarial at runtime by measuring its sensitivity. We evaluated our approach\non the MNIST and CIFAR10 datasets. The results show that our approach detects\nadversarial samples generated by state-of-the-art attacking methods efficiently\nand accurately. \n\n"}
{"id": "1812.06080", "contents": "Title: Reconciling meta-learning and continual learning with online mixtures of\n  tasks Abstract: Learning-to-learn or meta-learning leverages data-driven inductive bias to\nincrease the efficiency of learning on a novel task. This approach encounters\ndifficulty when transfer is not advantageous, for instance, when tasks are\nconsiderably dissimilar or change over time. We use the connection between\ngradient-based meta-learning and hierarchical Bayes to propose a Dirichlet\nprocess mixture of hierarchical Bayesian models over the parameters of an\narbitrary parametric model such as a neural network. In contrast to\nconsolidating inductive biases into a single set of hyperparameters, our\napproach of task-dependent hyperparameter selection better handles latent\ndistribution shift, as demonstrated on a set of evolving, image-based, few-shot\nlearning benchmarks. \n\n"}
{"id": "1812.06083", "contents": "Title: Coupled Representation Learning for Domains, Intents and Slots in Spoken\n  Language Understanding Abstract: Representation learning is an essential problem in a wide range of\napplications and it is important for performing downstream tasks successfully.\nIn this paper, we propose a new model that learns coupled representations of\ndomains, intents, and slots by taking advantage of their hierarchical\ndependency in a Spoken Language Understanding system. Our proposed model learns\nthe vector representation of intents based on the slots tied to these intents\nby aggregating the representations of the slots. Similarly, the vector\nrepresentation of a domain is learned by aggregating the representations of the\nintents tied to a specific domain. To the best of our knowledge, it is the\nfirst approach to jointly learning the representations of domains, intents, and\nslots using their hierarchical relationships. The experimental results\ndemonstrate the effectiveness of the representations learned by our model, as\nevidenced by improved performance on the contextual cross-domain reranking\ntask. \n\n"}
{"id": "1812.06576", "contents": "Title: Learning Incremental Triplet Margin for Person Re-identification Abstract: Person re-identification (ReID) aims to match people across multiple\nnon-overlapping video cameras deployed at different locations. To address this\nchallenging problem, many metric learning approaches have been proposed, among\nwhich triplet loss is one of the state-of-the-arts. In this work, we explore\nthe margin between positive and negative pairs of triplets and prove that large\nmargin is beneficial. In particular, we propose a novel multi-stage training\nstrategy which learns incremental triplet margin and improves triplet loss\neffectively. Multiple levels of feature maps are exploited to make the learned\nfeatures more discriminative. Besides, we introduce global hard identity\nsearching method to sample hard identities when generating a training batch.\nExtensive experiments on Market-1501, CUHK03, and DukeMTMCreID show that our\napproach yields a performance boost and outperforms most existing\nstate-of-the-art methods. \n\n"}
{"id": "1812.06625", "contents": "Title: Semi-supervised mp-MRI Data Synthesis with StitchLayer and Auxiliary\n  Distance Maximization Abstract: In this paper, we address the problem of synthesizing multi-parameter\nmagnetic resonance imaging (mp-MRI) data, i.e. Apparent Diffusion Coefficients\n(ADC) and T2-weighted (T2w), containing clinically significant (CS) prostate\ncancer (PCa) via semi-supervised adversarial learning. Specifically, our\nsynthesizer generates mp-MRI data in a sequential manner: first generating ADC\nmaps from 128-d latent vectors, followed by translating them to the T2w images.\nThe synthesizer is trained in a semisupervised manner. In the supervised\ntraining process, a limited amount of paired ADC-T2w images and the\ncorresponding ADC encodings are provided and the synthesizer learns the paired\nrelationship by explicitly minimizing the reconstruction losses between\nsynthetic and real images. To avoid overfitting limited ADC encodings, an\nunlimited amount of random latent vectors and unpaired ADC-T2w Images are\nutilized in the unsupervised training process for learning the marginal image\ndistributions of real images. To improve the robustness of synthesizing, we\ndecompose the difficult task of generating full-size images into several\nsimpler tasks which generate sub-images only. A StitchLayer is then employed to\nfuse sub-images together in an interlaced manner into a full-size image. To\nenforce the synthetic images to indeed contain distinguishable CS PCa lesions,\nwe propose to also maximize an auxiliary distance of Jensen-Shannon divergence\n(JSD) between CS and nonCS images. Experimental results show that our method\ncan effectively synthesize a large variety of mpMRI images which contain\nmeaningful CS PCa lesions, display a good visual quality and have the correct\npaired relationship. Compared to the state-of-the-art synthesis methods, our\nmethod achieves a significant improvement in terms of both visual and\nquantitative evaluation metrics. \n\n"}
{"id": "1812.07858", "contents": "Title: Machine Learning in Cyber-Security - Problems, Challenges and Data Sets Abstract: We present cyber-security problems of high importance. We show that in order\nto solve these cyber-security problems, one must cope with certain machine\nlearning challenges. We provide novel data sets representing the problems in\norder to enable the academic community to investigate the problems and suggest\nmethods to cope with the challenges. We also present a method to generate\nlabels via pivoting, providing a solution to common problems of lack of labels\nin cyber-security. \n\n"}
{"id": "1812.08733", "contents": "Title: Heteroscedastic Gaussian processes for uncertainty modeling in\n  large-scale crowdsourced traffic data Abstract: Accurately modeling traffic speeds is a fundamental part of efficient\nintelligent transportation systems. Nowadays, with the widespread deployment of\nGPS-enabled devices, it has become possible to crowdsource the collection of\nspeed information to road users (e.g. through mobile applications or dedicated\nin-vehicle devices). Despite its rather wide spatial coverage, crowdsourced\nspeed data also brings very important challenges, such as the highly variable\nmeasurement noise in the data due to a variety of driving behaviors and sample\nsizes. When not properly accounted for, this noise can severely compromise any\napplication that relies on accurate traffic data. In this article, we propose\nthe use of heteroscedastic Gaussian processes (HGP) to model the time-varying\nuncertainty in large-scale crowdsourced traffic data. Furthermore, we develop a\nHGP conditioned on sample size and traffic regime (SRC-HGP), which makes use of\nsample size information (probe vehicles per minute) as well as previous\nobserved speeds, in order to more accurately model the uncertainty in observed\nspeeds. Using 6 months of crowdsourced traffic data from Copenhagen, we\nempirically show that the proposed heteroscedastic models produce significantly\nbetter predictive distributions when compared to current state-of-the-art\nmethods for both speed imputation and short-term forecasting tasks. \n\n"}
{"id": "1812.08997", "contents": "Title: Stochastic Doubly Robust Gradient Abstract: When training a machine learning model with observational data, it is often\nencountered that some values are systemically missing. Learning from the\nincomplete data in which the missingness depends on some covariates may lead to\nbiased estimation of parameters and even harm the fairness of decision outcome.\nThis paper proposes how to adjust the causal effect of covariates on the\nmissingness when training models using stochastic gradient descent (SGD).\nInspired by the design of doubly robust estimator and its theoretical property\nof double robustness, we introduce stochastic doubly robust gradient (SDRG)\nconsisting of two models: weight-corrected gradients for inverse propensity\nscore weighting and per-covariate control variates for regression adjustment.\nAlso, we identify the connection between double robustness and variance\nreduction in SGD by demonstrating the SDRG algorithm with a unifying framework\nfor variance reduced SGD. The performance of our approach is empirically tested\nby showing the convergence in training image classifiers with several examples\nof missing data. \n\n"}
{"id": "1812.09111", "contents": "Title: Generative Models from the perspective of Continual Learning Abstract: Which generative model is the most suitable for Continual Learning? This\npaper aims at evaluating and comparing generative models on disjoint sequential\nimage generation tasks. We investigate how several models learn and forget,\nconsidering various strategies: rehearsal, regularization, generative replay\nand fine-tuning. We used two quantitative metrics to estimate the generation\nquality and memory ability. We experiment with sequential tasks on three\ncommonly used benchmarks for Continual Learning (MNIST, Fashion MNIST and\nCIFAR10). We found that among all models, the original GAN performs best and\namong Continual Learning strategies, generative replay outperforms all other\nmethods. Even if we found satisfactory combinations on MNIST and Fashion MNIST,\ntraining generative models sequentially on CIFAR10 is particularly instable,\nand remains a challenge. Our code is available online\n\\footnote{\\url{https://github.com/TLESORT/Generative\\_Continual\\_Learning}}. \n\n"}
{"id": "1812.09323", "contents": "Title: Unsupervised Speech Recognition via Segmental Empirical Output\n  Distribution Matching Abstract: We consider the problem of training speech recognition systems without using\nany labeled data, under the assumption that the learner can only access to the\ninput utterances and a phoneme language model estimated from a non-overlapping\ncorpus. We propose a fully unsupervised learning algorithm that alternates\nbetween solving two sub-problems: (i) learn a phoneme classifier for a given\nset of phoneme segmentation boundaries, and (ii) refining the phoneme\nboundaries based on a given classifier. To solve the first sub-problem, we\nintroduce a novel unsupervised cost function named Segmental Empirical Output\nDistribution Matching, which generalizes the work in (Liu et al., 2017) to\nsegmental structures. For the second sub-problem, we develop an approximate MAP\napproach to refining the boundaries obtained from Wang et al. (2017).\nExperimental results on TIMIT dataset demonstrate the success of this fully\nunsupervised phoneme recognition system, which achieves a phone error rate\n(PER) of 41.6%. Although it is still far away from the state-of-the-art\nsupervised systems, we show that with oracle boundaries and matching language\nmodel, the PER could be improved to 32.5%.This performance approaches the\nsupervised system of the same model architecture, demonstrating the great\npotential of the proposed method. \n\n"}
{"id": "1812.09664", "contents": "Title: Non-Autoregressive Neural Machine Translation with Enhanced Decoder\n  Input Abstract: Non-autoregressive translation (NAT) models, which remove the dependence on\nprevious target tokens from the inputs of the decoder, achieve significantly\ninference speedup but at the cost of inferior accuracy compared to\nautoregressive translation (AT) models. Previous work shows that the quality of\nthe inputs of the decoder is important and largely impacts the model accuracy.\nIn this paper, we propose two methods to enhance the decoder inputs so as to\nimprove NAT models. The first one directly leverages a phrase table generated\nby conventional SMT approaches to translate source tokens to target tokens,\nwhich are then fed into the decoder as inputs. The second one transforms\nsource-side word embeddings to target-side word embeddings through\nsentence-level alignment and word-level adversary learning, and then feeds the\ntransformed word embeddings into the decoder as inputs. Experimental results\nshow our method largely outperforms the NAT baseline~\\citep{gu2017non} by\n$5.11$ BLEU scores on WMT14 English-German task and $4.72$ BLEU scores on WMT16\nEnglish-Romanian task. \n\n"}
{"id": "1812.09836", "contents": "Title: Moment Matching Training for Neural Machine Translation: A Preliminary\n  Study Abstract: In previous works, neural sequence models have been shown to improve\nsignificantly if external prior knowledge can be provided, for instance by\nallowing the model to access the embeddings of explicit features during both\ntraining and inference. In this work, we propose a different point of view on\nhow to incorporate prior knowledge in a principled way, using a moment matching\nframework. In this approach, the standard local cross-entropy training of the\nsequential model is combined with a moment matching training mode that\nencourages the equality of the expectations of certain predefined features\nbetween the model distribution and the empirical distribution. In particular,\nwe show how to derive unbiased estimates of some stochastic gradients that are\ncentral to the training, and compare our framework with a formally related one:\npolicy gradient training in reinforcement learning, pointing out some important\ndifferences in terms of the kinds of prior assumptions in both approaches. Our\ninitial results are promising, showing the effectiveness of our proposed\nframework. \n\n"}
{"id": "1812.10048", "contents": "Title: Parallel Clustering of Single Cell Transcriptomic Data with Split-Merge\n  Sampling on Dirichlet Process Mixtures Abstract: Motivation: With the development of droplet based systems, massive single\ncell transcriptome data has become available, which enables analysis of\ncellular and molecular processes at single cell resolution and is instrumental\nto understanding many biological processes. While state-of-the-art clustering\nmethods have been applied to the data, they face challenges in the following\naspects: (1) the clustering quality still needs to be improved; (2) most models\nneed prior knowledge on number of clusters, which is not always available; (3)\nthere is a demand for faster computational speed. Results: We propose to tackle\nthese challenges with Parallel Split Merge Sampling on Dirichlet Process\nMixture Model (the Para-DPMM model). Unlike classic DPMM methods that perform\nsampling on each single data point, the split merge mechanism samples on the\ncluster level, which significantly improves convergence and optimality of the\nresult. The model is highly parallelized and can utilize the computing power of\nhigh performance computing (HPC) clusters, enabling massive clustering on huge\ndatasets. Experiment results show the model outperforms current widely used\nmodels in both clustering quality and computational speed. Availability: Source\ncode is publicly available on\nhttps://github.com/tiehangd/Para_DPMM/tree/master/Para_DPMM_package \n\n"}
{"id": "1812.10071", "contents": "Title: Coupled Recurrent Network (CRN) Abstract: Many semantic video analysis tasks can benefit from multiple, heterogenous\nsignals. For example, in addition to the original RGB input sequences,\nsequences of optical flow are usually used to boost the performance of human\naction recognition in videos. To learn from these heterogenous input sources,\nexisting methods reply on two-stream architectural designs that contain\nindependent, parallel streams of Recurrent Neural Networks (RNNs). However,\ntwo-stream RNNs do not fully exploit the reciprocal information contained in\nthe multiple signals, let alone exploit it in a recurrent manner. To this end,\nwe propose in this paper a novel recurrent architecture, termed Coupled\nRecurrent Network (CRN), to deal with multiple input sources. In CRN, the\nparallel streams of RNNs are coupled together. Key design of CRN is a Recurrent\nInterpretation Block (RIB) that supports learning of reciprocal feature\nrepresentations from multiple signals in a recurrent manner. Different from\nRNNs which stack the training loss at each time step or the last time step, we\npropose an effective and efficient training strategy for CRN. Experiments show\nthe efficacy of the proposed CRN. In particular, we achieve the new state of\nthe art on the benchmark datasets of human action recognition and multi-person\npose estimation. \n\n"}
{"id": "1812.10464", "contents": "Title: Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual\n  Transfer and Beyond Abstract: We introduce an architecture to learn joint multilingual sentence\nrepresentations for 93 languages, belonging to more than 30 different families\nand written in 28 different scripts. Our system uses a single BiLSTM encoder\nwith a shared BPE vocabulary for all languages, which is coupled with an\nauxiliary decoder and trained on publicly available parallel corpora. This\nenables us to learn a classifier on top of the resulting embeddings using\nEnglish annotated data only, and transfer it to any of the 93 languages without\nany modification. Our experiments in cross-lingual natural language inference\n(XNLI dataset), cross-lingual document classification (MLDoc dataset) and\nparallel corpus mining (BUCC dataset) show the effectiveness of our approach.\nWe also introduce a new test set of aligned sentences in 112 languages, and\nshow that our sentence embeddings obtain strong results in multilingual\nsimilarity search even for low-resource languages. Our implementation, the\npre-trained encoder and the multilingual test set are available at\nhttps://github.com/facebookresearch/LASER \n\n"}
{"id": "1812.10912", "contents": "Title: On Computation and Generalization of GANs with Spectrum Control Abstract: Generative Adversarial Networks (GANs), though powerful, is hard to train.\nSeveral recent works (brock2016neural,miyato2018spectral) suggest that\ncontrolling the spectra of weight matrices in the discriminator can\nsignificantly improve the training of GANs. Motivated by their discovery, we\npropose a new framework for training GANs, which allows more flexible spectrum\ncontrol (e.g., making the weight matrices of the discriminator have slow\nsingular value decays). Specifically, we propose a new reparameterization\napproach for the weight matrices of the discriminator in GANs, which allows us\nto directly manipulate the spectra of the weight matrices through various\nregularizers and constraints, without intensively computing singular value\ndecompositions. Theoretically, we further show that the spectrum control\nimproves the generalization ability of GANs. Our experiments on CIFAR-10,\nSTL-10, and ImageNet datasets confirm that compared to other methods, our\nproposed method is capable of generating images with competitive quality by\nutilizing spectral normalization and encouraging the slow singular value decay. \n\n"}
{"id": "1812.11321", "contents": "Title: Attention-Based Capsule Networks with Dynamic Routing for Relation\n  Extraction Abstract: A capsule is a group of neurons, whose activity vector represents the\ninstantiation parameters of a specific type of entity. In this paper, we\nexplore the capsule networks used for relation extraction in a multi-instance\nmulti-label learning framework and propose a novel neural approach based on\ncapsule networks with attention mechanisms. We evaluate our method with\ndifferent benchmarks, and it is demonstrated that our method improves the\nprecision of the predicted relations. Particularly, we show that capsule\nnetworks improve multiple entity pairs relation extraction. \n\n"}
{"id": "1812.11737", "contents": "Title: The meaning of \"most\" for visual question answering models Abstract: The correct interpretation of quantifier statements in the context of a\nvisual scene requires non-trivial inference mechanisms. For the example of\n\"most\", we discuss two strategies which rely on fundamentally different\ncognitive concepts. Our aim is to identify what strategy deep learning models\nfor visual question answering learn when trained on such questions. To this\nend, we carefully design data to replicate experiments from psycholinguistics\nwhere the same question was investigated for humans. Focusing on the FiLM\nvisual question answering model, our experiments indicate that a form of\napproximate number system emerges whose performance declines with more\ndifficult scenes as predicted by Weber's law. Moreover, we identify confounding\nfactors, like spatial arrangement of the scene, which impede the effectiveness\nof this system. \n\n"}
{"id": "1812.11948", "contents": "Title: Differentiable Satisfiability and Differentiable Answer Set Programming\n  for Sampling-Based Multi-Model Optimization Abstract: We propose Differentiable Satisfiability and Differentiable Answer Set\nProgramming (Differentiable SAT/ASP) for multi-model optimization. Models\n(answer sets or satisfying truth assignments) are sampled using a novel SAT/ASP\nsolving approach which uses a gradient descent-based branching mechanism.\nSampling proceeds until the value of a user-defined multi-model cost function\nreaches a given threshold. As major use cases for our approach we propose\ndistribution-aware model sampling and expressive yet scalable probabilistic\nlogic programming. As our main algorithmic approach to Differentiable SAT/ASP,\nwe introduce an enhancement of the state-of-the-art CDNL/CDCL algorithm for\nSAT/ASP solving. Additionally, we present alternative algorithms which use an\nunmodified ASP solver (Clingo/clasp) and map the optimization task to\nconventional answer set optimization or use so-called propagators. We also\nreport on the open source software DelSAT, a recent prototype implementation of\nour main algorithm, and on initial experimental results which indicate that\nDelSATs performance is, when applied to the use case of probabilistic logic\ninference, on par with Markov Logic Network (MLN) inference performance,\ndespite having advantageous properties compared to MLNs, such as the ability to\nexpress inductive definitions and to work with probabilities as weights\ndirectly in all cases. Our experiments also indicate that our main algorithm is\nstrongly superior in terms of performance compared to the presented alternative\napproaches which reduce a common instance of the general problem to regular\nSAT/ASP. \n\n"}
{"id": "1901.00117", "contents": "Title: An Active Learning Framework for Efficient Robust Policy Search Abstract: Robust Policy Search is the problem of learning policies that do not degrade\nin performance when subject to unseen environment model parameters. It is\nparticularly relevant for transferring policies learned in a simulation\nenvironment to the real world. Several existing approaches involve sampling\nlarge batches of trajectories which reflect the differences in various possible\nenvironments, and then selecting some subset of these to learn robust policies,\nsuch as the ones that result in the worst performance. We propose an active\nlearning based framework, EffAcTS, to selectively choose model parameters for\nthis purpose so as to collect only as much data as necessary to select such a\nsubset. We apply this framework using Linear Bandits, and experimentally\nvalidate the gains in sample efficiency and the performance of our approach on\nstandard continuous control tasks. We also present a Multi-Task Learning\nperspective to the problem of Robust Policy Search, and draw connections from\nour proposed framework to existing work on Multi-Task Learning. \n\n"}
{"id": "1901.00158", "contents": "Title: Text Infilling Abstract: Recent years have seen remarkable progress of text generation in different\ncontexts, such as the most common setting of generating text from scratch, and\nthe emerging paradigm of retrieval-and-rewriting. Text infilling, which fills\nmissing text portions of a sentence or paragraph, is also of numerous use in\nreal life, yet is under-explored. Previous work has focused on restricted\nsettings by either assuming single word per missing portion or limiting to a\nsingle missing portion to the end of the text. This paper studies the general\ntask of text infilling, where the input text can have an arbitrary number of\nportions to be filled, each of which may require an arbitrary unknown number of\ntokens. We study various approaches for the task, including a self-attention\nmodel with segment-aware position encoding and bidirectional context modeling.\nWe create extensive supervised data by masking out text with varying\nstrategies. Experiments show the self-attention model greatly outperforms\nothers, creating a strong baseline for future research. \n\n"}
{"id": "1901.00248", "contents": "Title: A Survey on Multi-output Learning Abstract: Multi-output learning aims to simultaneously predict multiple outputs given\nan input. It is an important learning problem due to the pressing need for\nsophisticated decision making in real-world applications. Inspired by big data,\nthe 4Vs characteristics of multi-output imposes a set of challenges to\nmulti-output learning, in terms of the volume, velocity, variety and veracity\nof the outputs. Increasing number of works in the literature have been devoted\nto the study of multi-output learning and the development of novel approaches\nfor addressing the challenges encountered. However, it lacks a comprehensive\noverview on different types of challenges of multi-output learning brought by\nthe characteristics of the multiple outputs and the techniques proposed to\novercome the challenges. This paper thus attempts to fill in this gap to\nprovide a comprehensive review on this area. We first introduce different\nstages of the life cycle of the output labels. Then we present the paradigm on\nmulti-output learning, including its myriads of output structures, definitions\nof its different sub-problems, model evaluation metrics and popular data\nrepositories used in the study. Subsequently, we review a number of\nstate-of-the-art multi-output learning methods, which are categorized based on\nthe challenges. \n\n"}
{"id": "1901.00276", "contents": "Title: Multi-level CNN for lung nodule classification with Gaussian Process\n  assisted hyperparameter optimization Abstract: This paper investigates lung nodule classification by using deep neural\nnetworks (DNNs). Hyperparameter optimization in DNNs is a computationally\nexpensive problem, where evaluating a hyperparameter configuration may take\nseveral hours or even days. Bayesian optimization has been recently introduced\nfor the automatically searching of optimal hyperparameter configurations of\nDNNs. It applies probabilistic surrogate models to approximate the validation\nerror function of hyperparameter configurations, such as Gaussian processes,\nand reduce the computational complexity to a large extent. However, most\nexisting surrogate models adopt stationary covariance functions to measure the\ndifference between hyperparameter points based on spatial distance without\nconsidering its spatial locations. This distance-based assumption together with\nthe condition of constant smoothness throughout the whole hyperparameter search\nspace clearly violates the property that the points far away from optimal\npoints usually get similarly poor performance even though each two of them have\nhuge spatial distance between them. In this paper, a non-stationary kernel is\nproposed which allows the surrogate model to adapt to functions whose\nsmoothness varies with the spatial location of inputs, and a multi-level\nconvolutional neural network (ML-CNN) is built for lung nodule classification\nwhose hyperparameter configuration is optimized by using the proposed\nnon-stationary kernel based Gaussian surrogate model. Our algorithm searches\nthe surrogate for optimal setting via hyperparameter importance based\nevolutionary strategy, and the experiments demonstrate our algorithm\noutperforms manual tuning and well-established hyperparameter optimization\nmethods such as Random search, Gaussian processes with stationary kernels, and\nrecently proposed Hyperparameter Optimization via RBF and Dynamic coordinate\nsearch. \n\n"}
{"id": "1901.01334", "contents": "Title: An Adaptive Weighted Deep Forest Classifier Abstract: A modification of the confidence screening mechanism based on adaptive\nweighing of every training instance at each cascade level of the Deep Forest is\nproposed. The idea underlying the modification is very simple and stems from\nthe confidence screening mechanism idea proposed by Pang et al. to simplify the\nDeep Forest classifier by means of updating the training set at each level in\naccordance with the classification accuracy of every training instance.\nHowever, if the confidence screening mechanism just removes instances from\ntraining and testing processes, then the proposed modification is more flexible\nand assigns weights by taking into account the classification accuracy. The\nmodification is similar to the AdaBoost to some extent. Numerical experiments\nillustrate good performance of the proposed modification in comparison with the\noriginal Deep Forest proposed by Zhou and Feng. \n\n"}
{"id": "1901.01379", "contents": "Title: Deep Reinforcement Learning for Imbalanced Classification Abstract: Data in real-world application often exhibit skewed class distribution which\nposes an intense challenge for machine learning. Conventional classification\nalgorithms are not effective in the case of imbalanced data distribution, and\nmay fail when the data distribution is highly imbalanced. To address this\nissue, we propose a general imbalanced classification model based on deep\nreinforcement learning. We formulate the classification problem as a sequential\ndecision-making process and solve it by deep Q-learning network. The agent\nperforms a classification action on one sample at each time step, and the\nenvironment evaluates the classification action and returns a reward to the\nagent. The reward from minority class sample is larger so the agent is more\nsensitive to the minority class. The agent finally finds an optimal\nclassification policy in imbalanced data under the guidance of specific reward\nfunction and beneficial learning environment. Experiments show that our\nproposed model outperforms the other imbalanced classification algorithms, and\nit can identify more minority samples and has great classification performance. \n\n"}
{"id": "1901.01653", "contents": "Title: Learning Nonlinear Input-Output Maps with Dissipative Quantum Systems Abstract: In this paper, we develop a theory of learning nonlinear input-output maps\nwith fading memory by dissipative quantum systems, as a quantum counterpart of\nthe theory of approximating such maps using classical dynamical systems. The\ntheory identifies the properties required for a class of dissipative quantum\nsystems to be {\\em universal}, in that any input-output map with fading memory\ncan be approximated arbitrarily closely by an element of this class. We then\nintroduce an example class of dissipative quantum systems that is provably\nuniversal. Numerical experiments illustrate that with a small number of qubits,\nthis class can achieve comparable performance to classical learning schemes\nwith a large number of tunable parameters. Further numerical analysis suggests\nthat the exponentially increasing Hilbert space presents a potential resource\nfor dissipative quantum systems to surpass classical learning schemes for\ninput-output maps. \n\n"}
{"id": "1901.02413", "contents": "Title: Interpretable CNNs for Object Classification Abstract: This paper proposes a generic method to learn interpretable convolutional\nfilters in a deep convolutional neural network (CNN) for object classification,\nwhere each interpretable filter encodes features of a specific object part. Our\nmethod does not require additional annotations of object parts or textures for\nsupervision. Instead, we use the same training data as traditional CNNs. Our\nmethod automatically assigns each interpretable filter in a high conv-layer\nwith an object part of a certain category during the learning process. Such\nexplicit knowledge representations in conv-layers of CNN help people clarify\nthe logic encoded in the CNN, i.e., answering what patterns the CNN extracts\nfrom an input image and uses for prediction. We have tested our method using\ndifferent benchmark CNNs with various structures to demonstrate the broad\napplicability of our method. Experiments have shown that our interpretable\nfilters are much more semantically meaningful than traditional filters. \n\n"}
{"id": "1901.02477", "contents": "Title: Differentially Private Generative Adversarial Networks for Time Series,\n  Continuous, and Discrete Open Data Abstract: Open data plays a fundamental role in the 21th century by stimulating\neconomic growth and by enabling more transparent and inclusive societies.\nHowever, it is always difficult to create new high-quality datasets with the\nrequired privacy guarantees for many use cases. This paper aims at creating a\nframework for releasing new open data while protecting the individuality of the\nusers through a strict definition of privacy called differential privacy.\nUnlike previous work, this paper provides a framework for privacy preserving\ndata publishing that can be easily adapted to different use cases, from the\ngeneration of time-series to continuous data, and discrete data; no previous\nwork has focused on the later class. Indeed, many use cases expose discrete\ndata or at least a combination between categorical and numerical values. Thanks\nto the latest developments in deep learning and generative models, it is now\npossible to model rich-semantic data maintaining both the original distribution\nof the features and the correlations between them. The output of this framework\nis a deep network, namely a generator, able to create new data on demand. We\ndemonstrate the efficiency of our approach on real datasets from the French\npublic administration and classic benchmark datasets. \n\n"}
{"id": "1901.02920", "contents": "Title: TraceCaps: A Capsule-based Neural Network for Semantic Segmentation Abstract: In this paper, we propose a capsule-based neural network model to solve the\nsemantic segmentation problem. By taking advantage of the extractable\npart-whole dependencies available in capsule layers, we derive the\nprobabilities of the class labels for individual capsules through a recursive,\nlayer-by-layer procedure. We model this procedure as a traceback pipeline and\ntake it as a central piece to build an end-to-end segmentation network. Under\nthe proposed framework, image-level class labels and object boundaries are\njointly sought in an explicit manner, which poses a significant advantage over\nthe state-of-the-art fully convolutional network (FCN) solutions. With the\ncapability to extracted part-whole information, our traceback pipeline can\npotentially be utilized as the building blocks to design interpretable neural\nnetworks. Experiments conducted on modified MNIST and neuroimages demonstrate\nthat our model considerably enhance the segmentation performance compared to\nthe leading FCN variants. \n\n"}
{"id": "1901.03317", "contents": "Title: Accelerated Flow for Probability Distributions Abstract: This paper presents a methodology and numerical algorithms for constructing\naccelerated gradient flows on the space of probability distributions. In\nparticular, we extend the recent variational formulation of accelerated\ngradient methods in (wibisono, et. al. 2016) from vector valued variables to\nprobability distributions. The variational problem is modeled as a mean-field\noptimal control problem. The maximum principle of optimal control theory is\nused to derive Hamilton's equations for the optimal gradient flow. The\nHamilton's equation are shown to achieve the accelerated form of density\ntransport from any initial probability distribution to a target probability\ndistribution. A quantitative estimate on the asymptotic convergence rate is\nprovided based on a Lyapunov function construction, when the objective\nfunctional is displacement convex. Two numerical approximations are presented\nto implement the Hamilton's equations as a system of $N$ interacting particles.\nThe continuous limit of the Nesterov's algorithm is shown to be a special case\nwith $N=1$. The algorithm is illustrated with numerical examples. \n\n"}
{"id": "1901.03611", "contents": "Title: The Benefits of Over-parameterization at Initialization in Deep ReLU\n  Networks Abstract: It has been noted in existing literature that over-parameterization in ReLU\nnetworks generally improves performance. While there could be several factors\ninvolved behind this, we prove some desirable theoretical properties at\ninitialization which may be enjoyed by ReLU networks. Specifically, it is known\nthat He initialization in deep ReLU networks asymptotically preserves variance\nof activations in the forward pass and variance of gradients in the backward\npass for infinitely wide networks, thus preserving the flow of information in\nboth directions. Our paper goes beyond these results and shows novel properties\nthat hold under He initialization: i) the norm of hidden activation of each\nlayer is equal to the norm of the input, and, ii) the norm of weight gradient\nof each layer is equal to the product of norm of the input vector and the error\nat output layer. These results are derived using the PAC analysis framework,\nand hold true for finitely sized datasets such that the width of the ReLU\nnetwork only needs to be larger than a certain finite lower bound. As we show,\nthis lower bound depends on the depth of the network and the number of samples,\nand by the virtue of being a lower bound, over-parameterized ReLU networks are\nendowed with these desirable properties. For the aforementioned hidden\nactivation norm property under He initialization, we further extend our theory\nand show that this property holds for a finite width network even when the\nnumber of data samples is infinite. Thus we overcome several limitations of\nexisting papers, and show new properties of deep ReLU networks at\ninitialization. \n\n"}
{"id": "1901.03912", "contents": "Title: Real-time Joint Object Detection and Semantic Segmentation Network for\n  Automated Driving Abstract: Convolutional Neural Networks (CNN) are successfully used for various visual\nperception tasks including bounding box object detection, semantic\nsegmentation, optical flow, depth estimation and visual SLAM. Generally these\ntasks are independently explored and modeled. In this paper, we present a joint\nmulti-task network design for learning object detection and semantic\nsegmentation simultaneously. The main motivation is to achieve real-time\nperformance on a low power embedded SOC by sharing of encoder for both the\ntasks. We construct an efficient architecture using a small ResNet10 like\nencoder which is shared for both decoders. Object detection uses YOLO v2 like\ndecoder and semantic segmentation uses FCN8 like decoder. We evaluate the\nproposed network in two public datasets (KITTI, Cityscapes) and in our private\nfisheye camera dataset, and demonstrate that joint network provides the same\naccuracy as that of separate networks. We further optimize the network to\nachieve 30 fps for 1280x384 resolution image. \n\n"}
{"id": "1901.04321", "contents": "Title: Large-scale Collaborative Filtering with Product Embeddings Abstract: The application of machine learning techniques to large-scale personalized\nrecommendation problems is a challenging task. Such systems must make sense of\nenormous amounts of implicit feedback in order to understand user preferences\nacross numerous product categories. This paper presents a deep learning based\nsolution to this problem within the collaborative filtering with implicit\nfeedback framework. Our approach combines neural attention mechanisms, which\nallow for context dependent weighting of past behavioral signals, with\nrepresentation learning techniques to produce models which obtain extremely\nhigh coverage, can easily incorporate new information as it becomes available,\nand are computationally efficient. Offline experiments demonstrate significant\nperformance improvements when compared to several alternative methods from the\nliterature. Results from an online setting show that the approach compares\nfavorably with current production techniques used to produce personalized\nproduct recommendations. \n\n"}
{"id": "1901.04592", "contents": "Title: Interpretable machine learning: definitions, methods, and applications Abstract: Machine-learning models have demonstrated great success in learning complex\npatterns that enable them to make predictions about unobserved data. In\naddition to using models for prediction, the ability to interpret what a model\nhas learned is receiving an increasing amount of attention. However, this\nincreased focus has led to considerable confusion about the notion of\ninterpretability. In particular, it is unclear how the wide array of proposed\ninterpretation methods are related, and what common concepts can be used to\nevaluate them.\n  We aim to address these concerns by defining interpretability in the context\nof machine learning and introducing the Predictive, Descriptive, Relevant (PDR)\nframework for discussing interpretations. The PDR framework provides three\noverarching desiderata for evaluation: predictive accuracy, descriptive\naccuracy and relevancy, with relevancy judged relative to a human audience.\nMoreover, to help manage the deluge of interpretation methods, we introduce a\ncategorization of existing techniques into model-based and post-hoc categories,\nwith sub-groups including sparsity, modularity and simulatability. To\ndemonstrate how practitioners can use the PDR framework to evaluate and\nunderstand interpretations, we provide numerous real-world examples. These\nexamples highlight the often under-appreciated role played by human audiences\nin discussions of interpretability. Finally, based on our framework, we discuss\nlimitations of existing methods and directions for future work. We hope that\nthis work will provide a common vocabulary that will make it easier for both\npractitioners and researchers to discuss and choose from the full range of\ninterpretation methods. \n\n"}
{"id": "1901.05350", "contents": "Title: TensorFlow.js: Machine Learning for the Web and Beyond Abstract: TensorFlow.js is a library for building and executing machine learning\nalgorithms in JavaScript. TensorFlow.js models run in a web browser and in the\nNode.js environment. The library is part of the TensorFlow ecosystem, providing\na set of APIs that are compatible with those in Python, allowing models to be\nported between the Python and JavaScript ecosystems. TensorFlow.js has\nempowered a new set of developers from the extensive JavaScript community to\nbuild and deploy machine learning models and enabled new classes of on-device\ncomputation. This paper describes the design, API, and implementation of\nTensorFlow.js, and highlights some of the impactful use cases. \n\n"}
{"id": "1901.05947", "contents": "Title: Stochastic Gradient Descent on a Tree: an Adaptive and Robust Approach\n  to Stochastic Convex Optimization Abstract: Online minimization of an unknown convex function over the interval $[0,1]$\nis considered under first-order stochastic bandit feedback, which returns a\nrandom realization of the gradient of the function at each query point. Without\nknowing the distribution of the random gradients, a learning algorithm\nsequentially chooses query points with the objective of minimizing regret\ndefined as the expected cumulative loss of the function values at the query\npoints in excess to the minimum value of the function. An approach based on\ndevising a biased random walk on an infinite-depth binary tree constructed\nthrough successive partitioning of the domain of the function is developed.\nEach move of the random walk is guided by a sequential test based on confidence\nbounds on the empirical mean constructed using the law of the iterated\nlogarithm. With no tuning parameters, this learning algorithm is robust to\nheavy-tailed noise with infinite variance and adaptive to unknown function\ncharacteristics (specifically, convex, strongly convex, and nonsmooth). It\nachieves the corresponding optimal regret orders (up to a $\\sqrt{\\log T}$ or a\n$\\log\\log T$ factor) in each class of functions and offers better or matching\nregret orders than the classical stochastic gradient descent approach which\nrequires the knowledge of the function characteristics for tuning the sequence\nof step-sizes. \n\n"}
{"id": "1901.06261", "contents": "Title: NeuNetS: An Automated Synthesis Engine for Neural Network Design Abstract: Application of neural networks to a vast variety of practical applications is\ntransforming the way AI is applied in practice. Pre-trained neural network\nmodels available through APIs or capability to custom train pre-built neural\nnetwork architectures with customer data has made the consumption of AI by\ndevelopers much simpler and resulted in broad adoption of these complex AI\nmodels. While prebuilt network models exist for certain scenarios, to try and\nmeet the constraints that are unique to each application, AI teams need to\nthink about developing custom neural network architectures that can meet the\ntradeoff between accuracy and memory footprint to achieve the tight constraints\nof their unique use-cases. However, only a small proportion of data science\nteams have the skills and experience needed to create a neural network from\nscratch, and the demand far exceeds the supply. In this paper, we present\nNeuNetS : An automated Neural Network Synthesis engine for custom neural\nnetwork design that is available as part of IBM's AI OpenScale's product.\nNeuNetS is available for both Text and Image domains and can build neural\nnetworks for specific tasks in a fraction of the time it takes today with human\neffort, and with accuracy similar to that of human-designed AI models. \n\n"}
{"id": "1901.06514", "contents": "Title: The RobotriX: An eXtremely Photorealistic and Very-Large-Scale Indoor\n  Dataset of Sequences with Robot Trajectories and Interactions Abstract: Enter the RobotriX, an extremely photorealistic indoor dataset designed to\nenable the application of deep learning techniques to a wide variety of robotic\nvision problems. The RobotriX consists of hyperrealistic indoor scenes which\nare explored by robot agents which also interact with objects in a visually\nrealistic manner in that simulated world. Photorealistic scenes and robots are\nrendered by Unreal Engine into a virtual reality headset which captures gaze so\nthat a human operator can move the robot and use controllers for the robotic\nhands; scene information is dumped on a per-frame basis so that it can be\nreproduced offline to generate raw data and ground truth labels. By taking this\napproach, we were able to generate a dataset of 38 semantic classes totaling 8M\nstills recorded at +60 frames per second with full HD resolution. For each\nframe, RGB-D and 3D information is provided with full annotations in both\nspaces. Thanks to the high quality and quantity of both raw information and\nannotations, the RobotriX will serve as a new milestone for investigating 2D\nand 3D robotic vision tasks with large-scale data-driven techniques. \n\n"}
{"id": "1901.07445", "contents": "Title: Accelerated Linear Convergence of Stochastic Momentum Methods in\n  Wasserstein Distances Abstract: Momentum methods such as Polyak's heavy ball (HB) method, Nesterov's\naccelerated gradient (AG) as well as accelerated projected gradient (APG)\nmethod have been commonly used in machine learning practice, but their\nperformance is quite sensitive to noise in the gradients. We study these\nmethods under a first-order stochastic oracle model where noisy estimates of\nthe gradients are available. For strongly convex problems, we show that the\ndistribution of the iterates of AG converges with the accelerated\n$O(\\sqrt{\\kappa}\\log(1/\\varepsilon))$ linear rate to a ball of radius\n$\\varepsilon$ centered at a unique invariant distribution in the 1-Wasserstein\nmetric where $\\kappa$ is the condition number as long as the noise variance is\nsmaller than an explicit upper bound we can provide. Our analysis also\ncertifies linear convergence rates as a function of the stepsize, momentum\nparameter and the noise variance; recovering the accelerated rates in the\nnoiseless case and quantifying the level of noise that can be tolerated to\nachieve a given performance. In the special case of strongly convex quadratic\nobjectives, we can show accelerated linear rates in the $p$-Wasserstein metric\nfor any $p\\geq 1$ with improved sensitivity to noise for both AG and HB through\na non-asymptotic analysis under some additional assumptions on the noise\nstructure. Our analysis for HB and AG also leads to improved non-asymptotic\nconvergence bounds in suboptimality for both deterministic and stochastic\nsettings which is of independent interest. To the best of our knowledge, these\nare the first linear convergence results for stochastic momentum methods under\nthe stochastic oracle model. We also extend our results to the APG method and\nweakly convex functions showing accelerated rates when the noise magnitude is\nsufficiently small. \n\n"}
{"id": "1901.07487", "contents": "Title: Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for\n  Non-Convex Optimization Abstract: Recent studies on diffusion-based sampling methods have shown that Langevin\nMonte Carlo (LMC) algorithms can be beneficial for non-convex optimization, and\nrigorous theoretical guarantees have been proven for both asymptotic and\nfinite-time regimes. Algorithmically, LMC-based algorithms resemble the\nwell-known gradient descent (GD) algorithm, where the GD recursion is perturbed\nby an additive Gaussian noise whose variance has a particular form. Fractional\nLangevin Monte Carlo (FLMC) is a recently proposed extension of LMC, where the\nGaussian noise is replaced by a heavy-tailed {\\alpha}-stable noise. As opposed\nto its Gaussian counterpart, these heavy-tailed perturbations can incur large\njumps and it has been empirically demonstrated that the choice of\n{\\alpha}-stable noise can provide several advantages in modern machine learning\nproblems, both in optimization and sampling contexts. However, as opposed to\nLMC, only asymptotic convergence properties of FLMC have been yet established.\nIn this study, we analyze the non-asymptotic behavior of FLMC for non-convex\noptimization and prove finite-time bounds for its expected suboptimality. Our\nresults show that the weak-error of FLMC increases faster than LMC, which\nsuggests using smaller step-sizes in FLMC. We finally extend our results to the\ncase where the exact gradients are replaced by stochastic gradients and show\nthat similar results hold in this setting as well. \n\n"}
{"id": "1901.07643", "contents": "Title: Solving All Regression Models For Learning Gaussian Networks Using\n  Givens Rotations Abstract: Score based learning (SBL) is a promising approach for learning Bayesian\nnetworks. The initial step in the majority of the SBL algorithms consists of\ncomputing the scores of all possible child and parent-set combinations for the\nvariables. For Bayesian networks with continuous variables, a particular score\nis usually calculated as a function of the regression of the child over the\nvariables in the parent-set. The sheer number of regressions models to be\nsolved necessitates the design of efficient numerical algorithms. In this\npaper, we propose an algorithm for an efficient and exact calculation of\nregressions for all child and parent-set combinations. In the proposed\nalgorithm, we use QR decompositions (QRDs) to capture the dependencies between\nthe regressions for different families and Givens rotations to efficiently\ntraverse through the space of QRDs such that all the regression models are\naccounted for in the shortest path possible. We compare the complexity of the\nsuggested method with different algorithms, mainly those arising in all subset\nregression problems, and show that our algorithm has the smallest algorithmic\ncomplexity. We also explain how to parallelize the proposed method so as to\ndecrease the runtime by a factor proportional to the number of processors\nutilized. \n\n"}
{"id": "1901.07666", "contents": "Title: Rapid identification of pathogenic bacteria using Raman spectroscopy and\n  deep learning Abstract: Rapid identification of bacteria is essential to prevent the spread of\ninfectious disease, help combat antimicrobial resistance, and improve patient\noutcomes. Raman optical spectroscopy promises to combine bacterial detection,\nidentification, and antibiotic susceptibility testing in a single step.\nHowever, achieving clinically relevant speeds and accuracies remains\nchallenging due to the weak Raman signal from bacterial cells and the large\nnumber of bacterial species and phenotypes. By amassing the largest known\ndataset of bacterial Raman spectra, we are able to apply state-of-the-art deep\nlearning approaches to identify 30 of the most common bacterial pathogens from\nnoisy Raman spectra, achieving antibiotic treatment identification accuracies\nof 99.0$\\pm$0.1%. This novel approach distinguishes between\nmethicillin-resistant and -susceptible isolates of Staphylococcus aureus (MRSA\nand MSSA) as well as a pair of isogenic MRSA and MSSA that are genetically\nidentical apart from deletion of the mecA resistance gene, indicating the\npotential for culture-free detection of antibiotic resistance. Results from\ninitial clinical validation are promising: using just 10 bacterial spectra from\neach of 25 isolates, we achieve 99.0$\\pm$1.9% species identification accuracy.\nOur combined Raman-deep learning system represents an important\nproof-of-concept for rapid, culture-free identification of bacterial isolates\nand antibiotic resistance and could be readily extended for diagnostics on\nblood, urine, and sputum. \n\n"}
{"id": "1901.07821", "contents": "Title: Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff Abstract: Lossy compression algorithms are typically designed and analyzed through the\nlens of Shannon's rate-distortion theory, where the goal is to achieve the\nlowest possible distortion (e.g., low MSE or high SSIM) at any given bit rate.\nHowever, in recent years, it has become increasingly accepted that \"low\ndistortion\" is not a synonym for \"high perceptual quality\", and in fact\noptimization of one often comes at the expense of the other. In light of this\nunderstanding, it is natural to seek for a generalization of rate-distortion\ntheory which takes perceptual quality into account. In this paper, we adopt the\nmathematical definition of perceptual quality recently proposed by Blau &\nMichaeli (2018), and use it to study the three-way tradeoff between rate,\ndistortion, and perception. We show that restricting the perceptual quality to\nbe high, generally leads to an elevation of the rate-distortion curve, thus\nnecessitating a sacrifice in either rate or distortion. We prove several\nfundamental properties of this triple-tradeoff, calculate it in closed form for\na Bernoulli source, and illustrate it visually on a toy MNIST example. \n\n"}
{"id": "1901.08241", "contents": "Title: Location reference identification from tweets during emergencies: A deep\n  learning approach Abstract: Twitter is recently being used during crises to communicate with officials\nand provide rescue and relief operation in real time. The geographical location\ninformation of the event, as well as users, are vitally important in such\nscenarios. The identification of geographic location is one of the challenging\ntasks as the location information fields, such as user location and place name\nof tweets are not reliable. The extraction of location information from tweet\ntext is difficult as it contains a lot of non-standard English, grammatical\nerrors, spelling mistakes, non-standard abbreviations, and so on. This research\naims to extract location words used in the tweet using a Convolutional Neural\nNetwork (CNN) based model. We achieved the exact matching score of 0.929,\nHamming loss of 0.002, and $F_1$-score of 0.96 for the tweets related to the\nearthquake. Our model was able to extract even three- to four-word long\nlocation references which is also evident from the exact matching score of over\n92\\%. The findings of this paper can help in early event localization,\nemergency situations, real-time road traffic management, localized\nadvertisement, and in various location-based services. \n\n"}
{"id": "1901.08255", "contents": "Title: Confidence-based Graph Convolutional Networks for Semi-Supervised\n  Learning Abstract: Predicting properties of nodes in a graph is an important problem with\napplications in a variety of domains. Graph-based Semi-Supervised Learning\n(SSL) methods aim to address this problem by labeling a small subset of the\nnodes as seeds and then utilizing the graph structure to predict label scores\nfor the rest of the nodes in the graph. Recently, Graph Convolutional Networks\n(GCNs) have achieved impressive performance on the graph-based SSL task. In\naddition to label scores, it is also desirable to have confidence scores\nassociated with them. Unfortunately, confidence estimation in the context of\nGCN has not been previously explored. We fill this important gap in this paper\nand propose ConfGCN, which estimates labels scores along with their confidences\njointly in GCN-based setting. ConfGCN uses these estimated confidences to\ndetermine the influence of one node on another during neighborhood aggregation,\nthereby acquiring anisotropic capabilities. Through extensive analysis and\nexperiments on standard benchmarks, we find that ConfGCN is able to outperform\nstate-of-the-art baselines. We have made ConfGCN's source code available to\nencourage reproducible research. \n\n"}
{"id": "1901.08275", "contents": "Title: Multi-fidelity Bayesian Optimization with Max-value Entropy Search and\n  its parallelization Abstract: In a standard setting of Bayesian optimization (BO), the objective function\nevaluation is assumed to be highly expensive. Multi-fidelity Bayesian\noptimization (MFBO) accelerates BO by incorporating lower fidelity observations\navailable with a lower sampling cost. In this paper, we focus on the\ninformation-based approach, which is a popular and empirically successful\napproach in BO. For MFBO, however, existing information-based methods are\nplagued by difficulty in estimating the information gain. We propose an\napproach based on max-value entropy search (MES), which greatly facilitates\ncomputations by considering the entropy of the optimal function value instead\nof the optimal input point. We show that, in our multi-fidelity MES (MF-MES),\nmost of additional computations, compared with usual MES, is reduced to\nanalytical computations. Although an additional numerical integration is\nnecessary for the information across different fidelities, this is only in one\ndimensional space, which can be performed efficiently and accurately. Further,\nwe also propose parallelization of MF-MES. Since there exist a variety of\ndifferent sampling costs, queries typically occur asynchronously in MFBO. We\nshow that similar simple computations can be derived for asynchronous parallel\nMFBO. We demonstrate effectiveness of our approach by using benchmark datasets\nand a real-world application to materials science data. \n\n"}
{"id": "1901.08396", "contents": "Title: Self-Supervised Deep Learning on Point Clouds by Reconstructing Space Abstract: Point clouds provide a flexible and natural representation usable in\ncountless applications such as robotics or self-driving cars. Recently, deep\nneural networks operating on raw point cloud data have shown promising results\non supervised learning tasks such as object classification and semantic\nsegmentation. While massive point cloud datasets can be captured using modern\nscanning technology, manually labelling such large 3D point clouds for\nsupervised learning tasks is a cumbersome process. This necessitates methods\nthat can learn from unlabelled data to significantly reduce the number of\nannotated samples needed in supervised learning. We propose a self-supervised\nlearning task for deep learning on raw point cloud data in which a neural\nnetwork is trained to reconstruct point clouds whose parts have been randomly\nrearranged. While solving this task, representations that capture semantic\nproperties of the point cloud are learned. Our method is agnostic of network\narchitecture and outperforms current unsupervised learning approaches in\ndownstream object classification tasks. We show experimentally, that\npre-training with our method before supervised training improves the\nperformance of state-of-the-art models and significantly improves sample\nefficiency. \n\n"}
{"id": "1901.08508", "contents": "Title: Maximum Entropy Generators for Energy-Based Models Abstract: Maximum likelihood estimation of energy-based models is a challenging problem\ndue to the intractability of the log-likelihood gradient. In this work, we\npropose learning both the energy function and an amortized approximate sampling\nmechanism using a neural generator network, which provides an efficient\napproximation of the log-likelihood gradient. The resulting objective requires\nmaximizing entropy of the generated samples, which we perform using recently\nproposed nonparametric mutual information estimators. Finally, to stabilize the\nresulting adversarial game, we use a zero-centered gradient penalty derived as\na necessary condition from the score matching literature. The proposed\ntechnique can generate sharp images with Inception and FID scores competitive\nwith recent GAN techniques, does not suffer from mode collapse, and is\ncompetitive with state-of-the-art anomaly detection techniques. \n\n"}
{"id": "1901.08571", "contents": "Title: Nonparametric Inference under B-bits Quantization Abstract: Statistical inference based on lossy or incomplete samples is often needed in\nresearch areas such as signal/image processing, medical image storage, remote\nsensing, signal transmission. In this paper, we propose a nonparametric testing\nprocedure based on samples quantized to $B$ bits through a computationally\nefficient algorithm. Under mild technical conditions, we establish the\nasymptotic properties of the proposed test statistic and investigate how the\ntesting power changes as $B$ increases. In particular, we show that if $B$\nexceeds a certain threshold, the proposed nonparametric testing procedure\nachieves the classical minimax rate of testing (Shang and Cheng, 2015) for\nspline models. We further extend our theoretical investigations to a\nnonparametric linearity test and an adaptive nonparametric test, expanding the\napplicability of the proposed methods. Extensive simulation studies {together\nwith a real-data analysis} are used to demonstrate the validity and\neffectiveness of the proposed tests. \n\n"}
{"id": "1901.08585", "contents": "Title: Graph heat mixture model learning Abstract: Graph inference methods have recently attracted a great interest from the\nscientific community, due to the large value they bring in data interpretation\nand analysis. However, most of the available state-of-the-art methods focus on\nscenarios where all available data can be explained through the same graph, or\ngroups corresponding to each graph are known a priori. In this paper, we argue\nthat this is not always realistic and we introduce a generative model for mixed\nsignals following a heat diffusion process on multiple graphs. We propose an\nexpectation-maximisation algorithm that can successfully separate signals into\ncorresponding groups, and infer multiple graphs that govern their behaviour. We\ndemonstrate the benefits of our method on both synthetic and real data. \n\n"}
{"id": "1901.08652", "contents": "Title: Learning agile and dynamic motor skills for legged robots Abstract: Legged robots pose one of the greatest challenges in robotics. Dynamic and\nagile maneuvers of animals cannot be imitated by existing methods that are\ncrafted by humans. A compelling alternative is reinforcement learning, which\nrequires minimal craftsmanship and promotes the natural evolution of a control\npolicy. However, so far, reinforcement learning research for legged robots is\nmainly limited to simulation, and only few and comparably simple examples have\nbeen deployed on real systems. The primary reason is that training with real\nrobots, particularly with dynamically balancing systems, is complicated and\nexpensive. In the present work, we introduce a method for training a neural\nnetwork policy in simulation and transferring it to a state-of-the-art legged\nsystem, thereby leveraging fast, automated, and cost-effective data generation\nschemes. The approach is applied to the ANYmal robot, a sophisticated\nmedium-dog-sized quadrupedal system. Using policies trained in simulation, the\nquadrupedal machine achieves locomotion skills that go beyond what had been\nachieved with prior methods: ANYmal is capable of precisely and\nenergy-efficiently following high-level body velocity commands, running faster\nthan before, and recovering from falling even in complex configurations. \n\n"}
{"id": "1901.08991", "contents": "Title: Diffusion Variational Autoencoders Abstract: A standard Variational Autoencoder, with a Euclidean latent space, is\nstructurally incapable of capturing topological properties of certain datasets.\nTo remove topological obstructions, we introduce Diffusion Variational\nAutoencoders with arbitrary manifolds as a latent space. A Diffusion\nVariational Autoencoder uses transition kernels of Brownian motion on the\nmanifold. In particular, it uses properties of the Brownian motion to implement\nthe reparametrization trick and fast approximations to the KL divergence. We\nshow that the Diffusion Variational Autoencoder is capable of capturing\ntopological properties of synthetic datasets. Additionally, we train MNIST on\nspheres, tori, projective spaces, SO(3), and a torus embedded in R3. Although a\nnatural dataset like MNIST does not have latent variables with a clear-cut\ntopological structure, training it on a manifold can still highlight\ntopological and geometrical properties. \n\n"}
{"id": "1901.09047", "contents": "Title: Faster Boosting with Smaller Memory Abstract: State-of-the-art implementations of boosting, such as XGBoost and LightGBM,\ncan process large training sets extremely fast. However, this performance\nrequires that the memory size is sufficient to hold a 2-3 multiple of the\ntraining set size. This paper presents an alternative approach to implementing\nthe boosted trees, which achieves a significant speedup over XGBoost and\nLightGBM, especially when the memory size is small. This is achieved using a\ncombination of three techniques: early stopping, effective sample size, and\nstratified sampling. Our experiments demonstrate a 10-100 speedup over XGBoost\nwhen the training data is too large to fit in memory. \n\n"}
{"id": "1901.09085", "contents": "Title: Generalisation dynamics of online learning in over-parameterised neural\n  networks Abstract: Deep neural networks achieve stellar generalisation on a variety of problems,\ndespite often being large enough to easily fit all their training data. Here we\nstudy the generalisation dynamics of two-layer neural networks in a\nteacher-student setup, where one network, the student, is trained using\nstochastic gradient descent (SGD) on data generated by another network, called\nthe teacher. We show how for this problem, the dynamics of SGD are captured by\na set of differential equations. In particular, we demonstrate analytically\nthat the generalisation error of the student increases linearly with the\nnetwork size, with other relevant parameters held constant. Our results\nindicate that achieving good generalisation in neural networks depends on the\ninterplay of at least the algorithm, its learning rate, the model architecture,\nand the data set. \n\n"}
{"id": "1901.09102", "contents": "Title: On Learning Meaningful Code Changes via Neural Machine Translation Abstract: Recent years have seen the rise of Deep Learning (DL) techniques applied to\nsource code. Researchers have exploited DL to automate several development and\nmaintenance tasks, such as writing commit messages, generating comments and\ndetecting vulnerabilities among others. One of the long lasting dreams of\napplying DL to source code is the possibility to automate non-trivial coding\nactivities. While some steps in this direction have been taken (e.g., learning\nhow to fix bugs), there is still a glaring lack of empirical evidence on the\ntypes of code changes that can be learned and automatically applied by DL. Our\ngoal is to make this first important step by quantitatively and qualitatively\ninvestigating the ability of a Neural Machine Translation (NMT) model to learn\nhow to automatically apply code changes implemented by developers during pull\nrequests. We train and experiment with the NMT model on a set of 236k pairs of\ncode components before and after the implementation of the changes provided in\nthe pull requests. We show that, when applied in a narrow enough context (i.e.,\nsmall/medium-sized pairs of methods before/after the pull request changes), NMT\ncan automatically replicate the changes implemented by developers during pull\nrequests in up to 36% of the cases. Moreover, our qualitative analysis shows\nthat the model is capable of learning and replicating a wide variety of\nmeaningful code changes, especially refactorings and bug-fixing activities. Our\nresults pave the way for novel research in the area of DL on code, such as the\nautomatic learning and applications of refactoring. \n\n"}
{"id": "1901.09178", "contents": "Title: A general model for plane-based clustering with loss function Abstract: In this paper, we propose a general model for plane-based clustering. The\ngeneral model contains many existing plane-based clustering methods, e.g.,\nk-plane clustering (kPC), proximal plane clustering (PPC), twin support vector\nclustering (TWSVC) and its extensions. Under this general model, one may obtain\nan appropriate clustering method for specific purpose. The general model is a\nprocedure corresponding to an optimization problem, where the optimization\nproblem minimizes the total loss of the samples. Thereinto, the loss of a\nsample derives from both within-cluster and between-cluster. In theory, the\ntermination conditions are discussed, and we prove that the general model\nterminates in a finite number of steps at a local or weak local optimal point.\nFurthermore, based on this general model, we propose a plane-based clustering\nmethod by introducing a new loss function to capture the data distribution\nprecisely. Experimental results on artificial and public available datasets\nverify the effectiveness of the proposed method. \n\n"}
{"id": "1901.09458", "contents": "Title: Learning Transformation Synchronization Abstract: Reconstructing the 3D model of a physical object typically requires us to\nalign the depth scans obtained from different camera poses into the same\ncoordinate system. Solutions to this global alignment problem usually proceed\nin two steps. The first step estimates relative transformations between pairs\nof scans using an off-the-shelf technique. Due to limited information presented\nbetween pairs of scans, the resulting relative transformations are generally\nnoisy. The second step then jointly optimizes the relative transformations\namong all input depth scans. A natural constraint used in this step is the\ncycle-consistency constraint, which allows us to prune incorrect relative\ntransformations by detecting inconsistent cycles. The performance of such\napproaches, however, heavily relies on the quality of the input relative\ntransformations. Instead of merely using the relative transformations as the\ninput to perform transformation synchronization, we propose to use a neural\nnetwork to learn the weights associated with each relative transformation. Our\napproach alternates between transformation synchronization using weighted\nrelative transformations and predicting new weights of the input relative\ntransformations using a neural network. We demonstrate the usefulness of this\napproach across a wide range of datasets. \n\n"}
{"id": "1901.09491", "contents": "Title: Stiffness: A New Perspective on Generalization in Neural Networks Abstract: In this paper we develop a new perspective on generalization of neural\nnetworks by proposing and investigating the concept of a neural network\nstiffness. We measure how stiff a network is by looking at how a small gradient\nstep in the network's parameters on one example affects the loss on another\nexample. Higher stiffness suggests that a network is learning features that\ngeneralize. In particular, we study how stiffness depends on 1) class\nmembership, 2) distance between data points in the input space, 3) training\niteration, and 4) learning rate. We present experiments on MNIST, FASHION\nMNIST, and CIFAR-10/100 using fully-connected and convolutional neural\nnetworks, as well as on a transformer-based NLP model. We demonstrate the\nconnection between stiffness and generalization, and observe its dependence on\nlearning rate. When training on CIFAR-100, the stiffness matrix exhibits a\ncoarse-grained behavior indicative of the model's awareness of super-class\nmembership. In addition, we measure how stiffness between two data points\ndepends on their mutual input-space distance, and establish the concept of a\ndynamical critical length -- a distance below which a parameter update based on\na data point influences its neighbors. \n\n"}
{"id": "1901.09515", "contents": "Title: Black Box Submodular Maximization: Discrete and Continuous Settings Abstract: In this paper, we consider the problem of black box continuous submodular\nmaximization where we only have access to the function values and no\ninformation about the derivatives is provided. For a monotone and continuous\nDR-submodular function, and subject to a bounded convex body constraint, we\npropose Black-box Continuous Greedy, a derivative-free algorithm that provably\nachieves the tight $[(1-1/e)OPT-\\epsilon]$ approximation guarantee with\n$O(d/\\epsilon^3)$ function evaluations. We then extend our result to the\nstochastic setting where function values are subject to stochastic zero-mean\nnoise. It is through this stochastic generalization that we revisit the\ndiscrete submodular maximization problem and use the multi-linear extension as\na bridge between discrete and continuous settings. Finally, we extensively\nevaluate the performance of our algorithm on continuous and discrete submodular\nobjective functions using both synthetic and real data. \n\n"}
{"id": "1901.09839", "contents": "Title: Interpreting Deep Neural Networks Through Variable Importance Abstract: While the success of deep neural networks (DNNs) is well-established across a\nvariety of domains, our ability to explain and interpret these methods is\nlimited. Unlike previously proposed local methods which try to explain\nparticular classification decisions, we focus on global interpretability and\nask a universally applicable question: given a trained model, which features\nare the most important? In the context of neural networks, a feature is rarely\nimportant on its own, so our strategy is specifically designed to leverage\npartial covariance structures and incorporate variable dependence into feature\nranking. Our methodological contributions in this paper are two-fold. First, we\npropose an effect size analogue for DNNs that is appropriate for applications\nwith highly collinear predictors (ubiquitous in computer vision). Second, we\nextend the recently proposed \"RelATive cEntrality\" (RATE) measure (Crawford et\nal., 2019) to the Bayesian deep learning setting. RATE applies an information\ntheoretic criterion to the posterior distribution of effect sizes to assess\nfeature significance. We apply our framework to three broad application areas:\ncomputer vision, natural language processing, and social science. \n\n"}
{"id": "1901.10275", "contents": "Title: Differentially Private Markov Chain Monte Carlo Abstract: Recent developments in differentially private (DP) machine learning and DP\nBayesian learning have enabled learning under strong privacy guarantees for the\ntraining data subjects. In this paper, we further extend the applicability of\nDP Bayesian learning by presenting the first general DP Markov chain Monte\nCarlo (MCMC) algorithm whose privacy-guarantees are not subject to unrealistic\nassumptions on Markov chain convergence and that is applicable to posterior\ninference in arbitrary models. Our algorithm is based on a decomposition of the\nBarker acceptance test that allows evaluating the R\\'enyi DP privacy cost of\nthe accept-reject choice. We further show how to improve the DP guarantee\nthrough data subsampling and approximate acceptance tests. \n\n"}
{"id": "1901.10517", "contents": "Title: Reparameterizable Subset Sampling via Continuous Relaxations Abstract: Many machine learning tasks require sampling a subset of items from a\ncollection based on a parameterized distribution. The Gumbel-softmax trick can\nbe used to sample a single item, and allows for low-variance reparameterized\ngradients with respect to the parameters of the underlying distribution.\nHowever, stochastic optimization involving subset sampling is typically not\nreparameterizable. To overcome this limitation, we define a continuous\nrelaxation of subset sampling that provides reparameterization gradients by\ngeneralizing the Gumbel-max trick. We use this approach to sample subsets of\nfeatures in an instance-wise feature selection task for model interpretability,\nsubsets of neighbors to implement a deep stochastic k-nearest neighbors model,\nand sub-sequences of neighbors to implement parametric t-SNE by directly\ncomparing the identities of local neighbors. We improve performance in all\nthese tasks by incorporating subset sampling in end-to-end training. \n\n"}
{"id": "1901.10634", "contents": "Title: Privacy-preserving Q-Learning with Functional Noise in Continuous State\n  Spaces Abstract: We consider differentially private algorithms for reinforcement learning in\ncontinuous spaces, such that neighboring reward functions are\nindistinguishable. This protects the reward information from being exploited by\nmethods such as inverse reinforcement learning. Existing studies that guarantee\ndifferential privacy are not extendable to infinite state spaces, as the noise\nlevel to ensure privacy will scale accordingly to infinity. Our aim is to\nprotect the value function approximator, without regard to the number of states\nqueried to the function. It is achieved by adding functional noise to the value\nfunction iteratively in the training. We show rigorous privacy guarantees by a\nseries of analyses on the kernel of the noise space, the probabilistic bound of\nsuch noise samples, and the composition over the iterations. We gain insight\ninto the utility analysis by proving the algorithm's approximate optimality\nwhen the state space is discrete. Experiments corroborate our theoretical\nfindings and show improvement over existing approaches. \n\n"}
{"id": "1901.10668", "contents": "Title: Doubly Sparse: Sparse Mixture of Sparse Experts for Efficient Softmax\n  Inference Abstract: Computations for the softmax function are significantly expensive when the\nnumber of output classes is large. In this paper, we present a novel softmax\ninference speedup method, Doubly Sparse Softmax (DS-Softmax), that leverages\nsparse mixture of sparse experts to efficiently retrieve top-k classes.\nDifferent from most existing methods that require and approximate a fixed\nsoftmax, our method is learning-based and can adapt softmax weights for a\nbetter inference speedup. In particular, our method learns a two-level\nhierarchy which divides entire output class space into several partially\noverlapping experts. Each expert is sparse and only contains a subset of output\nclasses. To find top-k classes, a sparse mixture enables us to find the most\nprobable expert quickly, and the sparse expert enables us to search within a\nsmall-scale softmax. We empirically conduct evaluation on several real-world\ntasks, including neural machine translation, language modeling and image\nclassification, and demonstrate that significant computation reductions can be\nachieved at no performance loss. \n\n"}
{"id": "1901.10864", "contents": "Title: Benefits and Pitfalls of the Exponential Mechanism with Applications to\n  Hilbert Spaces and Functional PCA Abstract: The exponential mechanism is a fundamental tool of Differential Privacy (DP)\ndue to its strong privacy guarantees and flexibility. We study its extension to\nsettings with summaries based on infinite dimensional outputs such as with\nfunctional data analysis, shape analysis, and nonparametric statistics. We show\nthat one can design the mechanism with respect to a specific base measure over\nthe output space, such as a Guassian process. We provide a positive result that\nestablishes a Central Limit Theorem for the exponential mechanism quite\nbroadly. We also provide an apparent negative result, showing that the\nmagnitude of the noise introduced for privacy is asymptotically non-negligible\nrelative to the statistical estimation error. We develop an \\ep-DP mechanism\nfor functional principal component analysis, applicable in separable Hilbert\nspaces. We demonstrate its performance via simulations and applications to two\ndatasets. \n\n"}
{"id": "1901.11149", "contents": "Title: Which Factorization Machine Modeling is Better: A Theoretical Answer\n  with Optimal Guarantee Abstract: Factorization machine (FM) is a popular machine learning model to capture the\nsecond order feature interactions. The optimal learning guarantee of FM and its\ngeneralized version is not yet developed. For a rank $k$ generalized FM of $d$\ndimensional input, the previous best known sampling complexity is\n$\\mathcal{O}[k^{3}d\\cdot\\mathrm{polylog}(kd)]$ under Gaussian distribution.\nThis bound is sub-optimal comparing to the information theoretical lower bound\n$\\mathcal{O}(kd)$. In this work, we aim to tighten this bound towards optimal\nand generalize the analysis to sub-gaussian distribution. We prove that when\nthe input data satisfies the so-called $\\tau$-Moment Invertible Property, the\nsampling complexity of generalized FM can be improved to\n$\\mathcal{O}[k^{2}d\\cdot\\mathrm{polylog}(kd)/\\tau^{2}]$. When the second order\nself-interaction terms are excluded in the generalized FM, the bound can be\nimproved to the optimal $\\mathcal{O}[kd\\cdot\\mathrm{polylog}(kd)]$ up to the\nlogarithmic factors. Our analysis also suggests that the positive semi-definite\nconstraint in the conventional FM is redundant as it does not improve the\nsampling complexity while making the model difficult to optimize. We evaluate\nour improved FM model in real-time high precision GPS signal calibration task\nto validate its superiority. \n\n"}
{"id": "1901.11512", "contents": "Title: Minimizing Negative Transfer of Knowledge in Multivariate Gaussian\n  Processes: A Scalable and Regularized Approach Abstract: Recently there has been an increasing interest in the multivariate Gaussian\nprocess (MGP) which extends the Gaussian process (GP) to deal with multiple\noutputs. One approach to construct the MGP and account for non-trivial\ncommonalities amongst outputs employs a convolution process (CP). The CP is\nbased on the idea of sharing latent functions across several convolutions.\nDespite the elegance of the CP construction, it provides new challenges that\nneed yet to be tackled. First, even with a moderate number of outputs, model\nbuilding is extremely prohibitive due to the huge increase in computational\ndemands and number of parameters to be estimated. Second, the negative transfer\nof knowledge may occur when some outputs do not share commonalities. In this\npaper we address these issues. We propose a regularized pairwise modeling\napproach for the MGP established using CP. The key feature of our approach is\nto distribute the estimation of the full multivariate model into a group of\nbivariate GPs which are individually built. Interestingly pairwise modeling\nturns out to possess unique characteristics, which allows us to tackle the\nchallenge of negative transfer through penalizing the latent function that\nfacilitates information sharing in each bivariate model. Predictions are then\nmade through combining predictions from the bivariate models within a Bayesian\nframework. The proposed method has excellent scalability when the number of\noutputs is large and minimizes the negative transfer of knowledge between\nuncorrelated outputs. Statistical guarantees for the proposed method are\nstudied and its advantageous features are demonstrated through numerical\nstudies. \n\n"}
