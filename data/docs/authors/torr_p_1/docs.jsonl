{"id": "0705.4566", "contents": "Title: Loop corrections for message passing algorithms in continuous variable\n  models Abstract: In this paper we derive the equations for Loop Corrected Belief Propagation\non a continuous variable Gaussian model. Using the exactness of the averages\nfor belief propagation for Gaussian models, a different way of obtaining the\ncovariances is found, based on Belief Propagation on cavity graphs. We discuss\nthe relation of this loop correction algorithm to Expectation Propagation\nalgorithms for the case in which the model is no longer Gaussian, but slightly\nperturbed by nonlinear terms. \n\n"}
{"id": "1109.2304", "contents": "Title: Efficient Minimization of Higher Order Submodular Functions using\n  Monotonic Boolean Functions Abstract: Submodular function minimization is a key problem in a wide variety of\napplications in machine learning, economics, game theory, computer vision, and\nmany others. The general solver has a complexity of $O(n^3 \\log^2 n . E +n^4\n{\\log}^{O(1)} n)$ where $E$ is the time required to evaluate the function and\n$n$ is the number of variables \\cite{Lee2015}. On the other hand, many computer\nvision and machine learning problems are defined over special subclasses of\nsubmodular functions that can be written as the sum of many submodular cost\nfunctions defined over cliques containing few variables. In such functions, the\npseudo-Boolean (or polynomial) representation \\cite{BorosH02} of these\nsubclasses are of degree (or order, or clique size) $k$ where $k \\ll n$. In\nthis work, we develop efficient algorithms for the minimization of this useful\nsubclass of submodular functions. To do this, we define novel mapping that\ntransform submodular functions of order $k$ into quadratic ones. The underlying\nidea is to use auxiliary variables to model the higher order terms and the\ntransformation is found using a carefully constructed linear program. In\nparticular, we model the auxiliary variables as monotonic Boolean functions,\nallowing us to obtain a compact transformation using as few auxiliary variables\nas possible. \n\n"}
{"id": "1109.5114", "contents": "Title: Improvements on \"Fast space-variant elliptical filtering using box\n  splines\" Abstract: It is well-known that box filters can be efficiently computed using\npre-integrations and local finite-differences\n[Crow1984,Heckbert1986,Viola2001]. By generalizing this idea and by combining\nit with a non-standard variant of the Central Limit Theorem, a constant-time or\nO(1) algorithm was proposed in [Chaudhury2010] that allowed one to perform\nspace-variant filtering using Gaussian-like kernels. The algorithm was based on\nthe observation that both isotropic and anisotropic Gaussians could be\napproximated using certain bivariate splines called box splines. The attractive\nfeature of the algorithm was that it allowed one to continuously control the\nshape and size (covariance) of the filter, and that it had a fixed\ncomputational cost per pixel, irrespective of the size of the filter. The\nalgorithm, however, offered a limited control on the covariance and accuracy of\nthe Gaussian approximation. In this work, we propose some improvements by\nappropriately modifying the algorithm in [Chaudhury2010]. \n\n"}
{"id": "1203.0076", "contents": "Title: Using Barriers to Reduce the Sensitivity to Edge Miscalculations of\n  Casting-Based Object Projection Feature Estimation Abstract: 3D motion tracking is a critical task in many computer vision applications.\nUnsupervised markerless 3D motion tracking systems determine the most relevant\nobject in the screen and then track it by continuously estimating its\nprojection features (center and area) from the edge image and a point inside\nthe relevant object projection (namely, inner point), until the tracking fails.\nExisting reliable object projection feature estimation techniques are based on\nray-casting or grid-filling from the inner point. These techniques assume the\nedge image to be accurate. However, in real case scenarios, edge\nmiscalculations may arise from low contrast between the target object and its\nsurroundings or motion blur caused by low frame rates or fast moving target\nobjects. In this paper, we propose a barrier extension to casting-based\ntechniques that mitigates the effect of edge miscalculations. \n\n"}
{"id": "1203.1513", "contents": "Title: Invariant Scattering Convolution Networks Abstract: A wavelet scattering network computes a translation invariant image\nrepresentation, which is stable to deformations and preserves high frequency\ninformation for classification. It cascades wavelet transform convolutions with\nnon-linear modulus and averaging operators. The first network layer outputs\nSIFT-type descriptors whereas the next layers provide complementary invariant\ninformation which improves classification. The mathematical analysis of wavelet\nscattering networks explains important properties of deep convolution networks\nfor classification.\n  A scattering representation of stationary processes incorporates higher order\nmoments and can thus discriminate textures having the same Fourier power\nspectrum. State of the art classification results are obtained for handwritten\ndigits and texture discrimination, using a Gaussian kernel SVM and a generative\nPCA classifier. \n\n"}
{"id": "1208.5365", "contents": "Title: A Missing and Found Recognition System for Hajj and Umrah Abstract: This note describes an integrated recognition system for identifying missing\nand found objects as well as missing, dead, and found people during Hajj and\nUmrah seasons in the two Holy cities of Makkah and Madina in the Kingdom of\nSaudi Arabia. It is assumed that the total estimated number of pilgrims will\nreach 20 millions during the next decade. The ultimate goal of this system is\nto integrate facial recognition and object identification solutions into the\nHajj and Umrah rituals. The missing and found computerized system is part of\nthe CrowdSensing system for Hajj and Umrah crowd estimation, management and\nsafety. \n\n"}
{"id": "1209.0654", "contents": "Title: Compressive Optical Deflectometric Tomography: A Constrained\n  Total-Variation Minimization Approach Abstract: Optical Deflectometric Tomography (ODT) provides an accurate characterization\nof transparent materials whose complex surfaces present a real challenge for\nmanufacture and control. In ODT, the refractive index map (RIM) of a\ntransparent object is reconstructed by measuring light deflection under\nmultiple orientations. We show that this imaging modality can be made\n\"compressive\", i.e., a correct RIM reconstruction is achievable with far less\nobservations than required by traditional Filtered Back Projection (FBP)\nmethods. Assuming a cartoon-shape RIM model, this reconstruction is driven by\nminimizing the map Total-Variation under a fidelity constraint with the\navailable observations. Moreover, two other realistic assumptions are added to\nimprove the stability of our approach: the map positivity and a frontier\ncondition. Numerically, our method relies on an accurate ODT sensing model and\non a primal-dual minimization scheme, including easily the sensing operator and\nthe proposed RIM constraints. We conclude this paper by demonstrating the power\nof our method on synthetic and experimental data under various compressive\nscenarios. In particular, the compressiveness of the stabilized ODT problem is\ndemonstrated by observing a typical gain of 20 dB compared to FBP at only 5% of\n360 incident light angles for moderately noisy sensing. \n\n"}
{"id": "1210.2352", "contents": "Title: A notion of continuity in discrete spaces and applications Abstract: We propose a notion of continuous path for locally finite metric spaces,\ntaking inspiration from the recent development of A-theory for locally finite\nconnected graphs. We use this notion of continuity to derive an analogue in Z^2\nof the Jordan curve theorem and to extend to a quite large class of locally\nfinite metric spaces (containing all finite metric spaces) an inequality for\nthe \\ell^p-distortion of a metric space that has been recently proved by\nPierre-Nicolas Jolissaint and Alain Valette for finite connected graphs. \n\n"}
{"id": "1210.8353", "contents": "Title: Temporal Autoencoding Restricted Boltzmann Machine Abstract: Much work has been done refining and characterizing the receptive fields\nlearned by deep learning algorithms. A lot of this work has focused on the\ndevelopment of Gabor-like filters learned when enforcing sparsity constraints\non a natural image dataset. Little work however has investigated how these\nfilters might expand to the temporal domain, namely through training on natural\nmovies. Here we investigate exactly this problem in established temporal deep\nlearning algorithms as well as a new learning paradigm suggested here, the\nTemporal Autoencoding Restricted Boltzmann Machine (TARBM). \n\n"}
{"id": "1211.7180", "contents": "Title: Multislice Modularity Optimization in Community Detection and Image\n  Segmentation Abstract: Because networks can be used to represent many complex systems, they have\nattracted considerable attention in physics, computer science, sociology, and\nmany other disciplines. One of the most important areas of network science is\nthe algorithmic detection of cohesive groups (i.e., \"communities\") of nodes. In\nthis paper, we algorithmically detect communities in social networks and image\ndata by optimizing multislice modularity. A key advantage of modularity\noptimization is that it does not require prior knowledge of the number or sizes\nof communities, and it is capable of finding network partitions that are\ncomposed of communities of different sizes. By optimizing multislice modularity\nand subsequently calculating diagnostics on the resulting network partitions,\nit is thereby possible to obtain information about network structure across\nmultiple system scales. We illustrate this method on data from both social\nnetworks and images, and we find that optimization of multislice modularity\nperforms well on these two tasks without the need for extensive\nproblem-specific adaptation. However, improving the computational speed of this\nmethod remains a challenging open problem. \n\n"}
{"id": "1212.3530", "contents": "Title: A Multi-Orientation Analysis Approach to Retinal Vessel Tracking Abstract: This paper presents a method for retinal vasculature extraction based on\nbiologically inspired multi-orientation analysis. We apply multi-orientation\nanalysis via so-called invertible orientation scores, modeling the cortical\ncolumns in the visual system of higher mammals. This allows us to generically\ndeal with many hitherto complex problems inherent to vessel tracking, such as\ncrossings, bifurcations, parallel vessels, vessels of varying widths and\nvessels with high curvature. Our approach applies tracking in invertible\norientation scores via a novel geometrical principle for curve optimization in\nthe Euclidean motion group SE(2). The method runs fully automatically and\nprovides a detailed model of the retinal vasculature, which is crucial as a\nsound basis for further quantitative analysis of the retina, especially in\nscreening applications. \n\n"}
{"id": "1212.4527", "contents": "Title: GMM-Based Hidden Markov Random Field for Color Image and 3D Volume\n  Segmentation Abstract: In this project, we first study the Gaussian-based hidden Markov random field\n(HMRF) model and its expectation-maximization (EM) algorithm. Then we\ngeneralize it to Gaussian mixture model-based hidden Markov random field. The\nalgorithm is implemented in MATLAB. We also apply this algorithm to color image\nsegmentation problems and 3D volume segmentation problems. \n\n"}
{"id": "1301.1299", "contents": "Title: Automated Variational Inference in Probabilistic Programming Abstract: We present a new algorithm for approximate inference in probabilistic\nprograms, based on a stochastic gradient for variational programs. This method\nis efficient without restrictions on the probabilistic program; it is\nparticularly practical for distributions which are not analytically tractable,\nincluding highly structured distributions that arise in probabilistic programs.\nWe show how to automatically derive mean-field probabilistic programs and\noptimize them, and demonstrate that our perspective improves inference\nefficiency over other algorithms. \n\n"}
{"id": "1301.3468", "contents": "Title: Boltzmann Machines and Denoising Autoencoders for Image Denoising Abstract: Image denoising based on a probabilistic model of local image patches has\nbeen employed by various researchers, and recently a deep (denoising)\nautoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] as\na good model for this. In this paper, we propose that another popular family of\nmodels in the field of deep learning, called Boltzmann machines, can perform\nimage denoising as well as, or in certain cases of high level of noise, better\nthan denoising autoencoders. We empirically evaluate the two models on three\ndifferent sets of images with different types and levels of noise. Throughout\nthe experiments we also examine the effect of the depth of the models. The\nexperiments confirmed our claim and revealed that the performance can be\nimproved by adding more hidden layers, especially when the level of noise is\nhigh. \n\n"}
{"id": "1307.0060", "contents": "Title: Approximate Bayesian Image Interpretation using Generative Probabilistic\n  Graphics Programs Abstract: The idea of computer vision as the Bayesian inverse problem to computer\ngraphics has a long history and an appealing elegance, but it has proved\ndifficult to directly implement. Instead, most vision tasks are approached via\ncomplex bottom-up processing pipelines. Here we show that it is possible to\nwrite short, simple probabilistic graphics programs that define flexible\ngenerative models and to automatically invert them to interpret real-world\nimages. Generative probabilistic graphics programs consist of a stochastic\nscene generator, a renderer based on graphics software, a stochastic likelihood\nmodel linking the renderer's output and the data, and latent variables that\nadjust the fidelity of the renderer and the tolerance of the likelihood model.\nRepresentations and algorithms from computer graphics, originally designed to\nproduce high-quality images, are instead used as the deterministic backbone for\nhighly approximate and stochastic generative models. This formulation combines\nprobabilistic programming, computer graphics, and approximate Bayesian\ncomputation, and depends only on general-purpose, automatic inference\ntechniques. We describe two applications: reading sequences of degraded and\nadversarially obscured alphanumeric characters, and inferring 3D road models\nfrom vehicle-mounted camera images. Each of the probabilistic graphics programs\nwe present relies on under 20 lines of probabilistic code, and supports\naccurate, approximately Bayesian inferences about ambiguous real-world images. \n\n"}
{"id": "1307.3546", "contents": "Title: The Matrix Element Method: Past, Present, and Future Abstract: The increasing use of multivariate methods, and in particular the Matrix\nElement Method (MEM), represents a revolution in experimental particle physics.\nWith continued exponential growth in computing capabilities, the use of\nsophisticated multivariate methods-- already common-- will soon become\nubiquitous and ultimately almost compulsory. While the existence of\nsophisticated algorithms for disentangling signal and background might naively\nsuggest a diminished role for theorists, the use of the MEM, with its inherent\nconnection to the calculation of differential cross sections will benefit from\ncollaboration between theorists and experimentalists. In this white paper, we\nwill briefly describe the MEM and some of its recent uses, note some current\nissues and potential resolutions, and speculate about exciting future\nopportunities. \n\n"}
{"id": "1307.7852", "contents": "Title: Scalable $k$-NN graph construction Abstract: The $k$-NN graph has played a central role in increasingly popular\ndata-driven techniques for various learning and vision tasks; yet, finding an\nefficient and effective way to construct $k$-NN graphs remains a challenge,\nespecially for large-scale high-dimensional data. In this paper, we propose a\nnew approach to construct approximate $k$-NN graphs with emphasis in:\nefficiency and accuracy. We hierarchically and randomly divide the data points\ninto subsets and build an exact neighborhood graph over each subset, achieving\na base approximate neighborhood graph; we then repeat this process for several\ntimes to generate multiple neighborhood graphs, which are combined to yield a\nmore accurate approximate neighborhood graph. Furthermore, we propose a\nneighborhood propagation scheme to further enhance the accuracy. We show both\ntheoretical and empirical accuracy and efficiency of our approach to $k$-NN\ngraph construction and demonstrate significant speed-up in dealing with large\nscale visual data. \n\n"}
{"id": "1309.0671", "contents": "Title: BayesOpt: A Library for Bayesian optimization with Robotics Applications Abstract: The purpose of this paper is twofold. On one side, we present a general\nframework for Bayesian optimization and we compare it with some related fields\nin active learning and Bayesian numerical analysis. On the other hand, Bayesian\noptimization and related problems (bandits, sequential experimental design) are\nhighly dependent on the surrogate model that is selected. However, there is no\nclear standard in the literature. Thus, we present a fast and flexible toolbox\nthat allows to test and combine different models and criteria with little\neffort. It includes most of the state-of-the-art contributions, algorithms and\nmodels. Its speed also removes part of the stigma that Bayesian optimization\nmethods are only good for \"expensive functions\". The software is free and it\ncan be used in many operating systems and computer languages. \n\n"}
{"id": "1309.4306", "contents": "Title: Sparsity Based Poisson Denoising with Dictionary Learning Abstract: The problem of Poisson denoising appears in various imaging applications,\nsuch as low-light photography, medical imaging and microscopy. In cases of high\nSNR, several transformations exist so as to convert the Poisson noise into an\nadditive i.i.d. Gaussian noise, for which many effective algorithms are\navailable. However, in a low SNR regime, these transformations are\nsignificantly less accurate, and a strategy that relies directly on the true\nnoise statistics is required. A recent work by Salmon et al. took this route,\nproposing a patch-based exponential image representation model based on GMM\n(Gaussian mixture model), leading to state-of-the-art results. In this paper,\nwe propose to harness sparse-representation modeling to the image patches,\nadopting the same exponential idea. Our scheme uses a greedy pursuit with\nboot-strapping based stopping condition and dictionary learning within the\ndenoising process. The reconstruction performance of the proposed scheme is\ncompetitive with leading methods in high SNR, and achieving state-of-the-art\nresults in cases of low SNR. \n\n"}
{"id": "1310.7443", "contents": "Title: On Convergent Finite Difference Schemes for Variational - PDE Based\n  Image Processing Abstract: We study an adaptive anisotropic Huber functional based image restoration\nscheme. By using a combination of L2-L1 regularization functions, an adaptive\nHuber functional based energy minimization model provides denoising with edge\npreservation in noisy digital images. We study a convergent finite difference\nscheme based on continuous piecewise linear functions and use a variable\nsplitting scheme, namely the Split Bregman, to obtain the discrete minimizer.\nExperimental results are given in image denoising and comparison with additive\noperator splitting, dual fixed point, and projected gradient schemes illustrate\nthat the best convergence rates are obtained for our algorithm. \n\n"}
{"id": "1312.6034", "contents": "Title: Deep Inside Convolutional Networks: Visualising Image Classification\n  Models and Saliency Maps Abstract: This paper addresses the visualisation of image classification models, learnt\nusing deep Convolutional Networks (ConvNets). We consider two visualisation\ntechniques, based on computing the gradient of the class score with respect to\nthe input image. The first one generates an image, which maximises the class\nscore [Erhan et al., 2009], thus visualising the notion of the class, captured\nby a ConvNet. The second technique computes a class saliency map, specific to a\ngiven image and class. We show that such maps can be employed for weakly\nsupervised object segmentation using classification ConvNets. Finally, we\nestablish the connection between the gradient-based ConvNet visualisation\nmethods and deconvolutional networks [Zeiler et al., 2013]. \n\n"}
{"id": "1312.6199", "contents": "Title: Intriguing properties of neural networks Abstract: Deep neural networks are highly expressive models that have recently achieved\nstate of the art performance on speech and visual recognition tasks. While\ntheir expressiveness is the reason they succeed, it also causes them to learn\nuninterpretable solutions that could have counter-intuitive properties. In this\npaper we report two such properties.\n  First, we find that there is no distinction between individual high level\nunits and random linear combinations of high level units, according to various\nmethods of unit analysis. It suggests that it is the space, rather than the\nindividual units, that contains of the semantic information in the high layers\nof neural networks.\n  Second, we find that deep neural networks learn input-output mappings that\nare fairly discontinuous to a significant extend. We can cause the network to\nmisclassify an image by applying a certain imperceptible perturbation, which is\nfound by maximizing the network's prediction error. In addition, the specific\nnature of these perturbations is not a random artifact of learning: the same\nperturbation can cause a different network, that was trained on a different\nsubset of the dataset, to misclassify the same input. \n\n"}
{"id": "1312.6203", "contents": "Title: Spectral Networks and Locally Connected Networks on Graphs Abstract: Convolutional Neural Networks are extremely efficient architectures in image\nand audio recognition tasks, thanks to their ability to exploit the local\ntranslational invariance of signal classes over their domain. In this paper we\nconsider possible generalizations of CNNs to signals defined on more general\ndomains without the action of a translation group. In particular, we propose\ntwo constructions, one based upon a hierarchical clustering of the domain, and\nanother based on the spectrum of the graph Laplacian. We show through\nexperiments that for low-dimensional graphs it is possible to learn\nconvolutional layers with a number of parameters independent of the input size,\nresulting in efficient deep architectures. \n\n"}
{"id": "1312.6813", "contents": "Title: New explicit thresholding/shrinkage formulas for one class of\n  regularization problems with overlapping group sparsity and their\n  applications Abstract: The least-square regression problems or inverse problems have been widely\nstudied in many fields such as compressive sensing, signal processing, and\nimage processing. To solve this kind of ill-posed problems, a regularization\nterm (i.e., regularizer) should be introduced, under the assumption that the\nsolutions have some specific properties, such as sparsity and group sparsity.\nWidely used regularizers include the $\\ell_1$ norm, total variation (TV)\nsemi-norm, and so on.\n  Recently, a new regularization term with overlapping group sparsity has been\nconsidered. Majorization minimization iteration method or variable duplication\nmethods are often applied to solve them. However, there have been no direct\nmethods for solve the relevant problems because of the difficulty of\noverlapping. In this paper, we proposed new explicit shrinkage formulas for one\nclass of these relevant problems, whose regularization terms have translation\ninvariant overlapping groups. Moreover, we apply our results in TV deblurring\nand denoising with overlapping group sparsity. We use alternating direction\nmethod of multipliers to iterate solve it. Numerical results also verify the\nvalidity and effectiveness of our new explicit shrinkage formulas. \n\n"}
{"id": "1312.7167", "contents": "Title: Near-separable Non-negative Matrix Factorization with $\\ell_1$- and\n  Bregman Loss Functions Abstract: Recently, a family of tractable NMF algorithms have been proposed under the\nassumption that the data matrix satisfies a separability condition Donoho &\nStodden (2003); Arora et al. (2012). Geometrically, this condition reformulates\nthe NMF problem as that of finding the extreme rays of the conical hull of a\nfinite set of vectors. In this paper, we develop several extensions of the\nconical hull procedures of Kumar et al. (2013) for robust ($\\ell_1$)\napproximations and Bregman divergences. Our methods inherit all the advantages\nof Kumar et al. (2013) including scalability and noise-tolerance. We show that\non foreground-background separation problems in computer vision, robust\nnear-separable NMFs match the performance of Robust PCA, considered state of\nthe art on these problems, with an order of magnitude faster training time. We\nalso demonstrate applications in exemplar selection settings. \n\n"}
{"id": "1402.0170", "contents": "Title: Collaborative Receptive Field Learning Abstract: The challenge of object categorization in images is largely due to arbitrary\ntranslations and scales of the foreground objects. To attack this difficulty,\nwe propose a new approach called collaborative receptive field learning to\nextract specific receptive fields (RF's) or regions from multiple images, and\nthe selected RF's are supposed to focus on the foreground objects of a common\ncategory. To this end, we solve the problem by maximizing a submodular function\nover a similarity graph constructed by a pool of RF candidates. However,\nmeasuring pairwise distance of RF's for building the similarity graph is a\nnontrivial problem. Hence, we introduce a similarity metric called\npyramid-error distance (PED) to measure their pairwise distances through\nsumming up pyramid-like matching errors over a set of low-level features.\nBesides, in consistent with the proposed PED, we construct a simple\nnonparametric classifier for classification. Experimental results show that our\nmethod effectively discovers the foreground objects in images, and improves\nclassification performance. \n\n"}
{"id": "1402.0240", "contents": "Title: Graph Cuts with Interacting Edge Costs - Examples, Approximations, and\n  Algorithms Abstract: We study an extension of the classical graph cut problem, wherein we replace\nthe modular (sum of edge weights) cost function by a submodular set function\ndefined over graph edges. Special cases of this problem have appeared in\ndifferent applications in signal processing, machine learning, and computer\nvision. In this paper, we connect these applications via the generic\nformulation of \"cooperative graph cuts\", for which we study complexity,\nalgorithms, and connections to polymatroidal network flows. Finally, we compare\nthe proposed algorithms empirically. \n\n"}
{"id": "1402.0595", "contents": "Title: Scene Labeling with Contextual Hierarchical Models Abstract: Scene labeling is the problem of assigning an object label to each pixel. It\nunifies the image segmentation and object recognition problems. The importance\nof using contextual information in scene labeling frameworks has been widely\nrealized in the field. We propose a contextual framework, called contextual\nhierarchical model (CHM), which learns contextual information in a hierarchical\nframework for scene labeling. At each level of the hierarchy, a classifier is\ntrained based on downsampled input images and outputs of previous levels. Our\nmodel then incorporates the resulting multi-resolution contextual information\ninto a classifier to segment the input image at original resolution. This\ntraining strategy allows for optimization of a joint posterior probability at\nmultiple resolutions through the hierarchy. Contextual hierarchical model is\npurely based on the input image patches and does not make use of any fragments\nor shape examples. Hence, it is applicable to a variety of problems such as\nobject segmentation and edge detection. We demonstrate that CHM outperforms\nstate-of-the-art on Stanford background and Weizmann horse datasets. It also\noutperforms state-of-the-art edge detection methods on NYU depth dataset and\nachieves state-of-the-art on Berkeley segmentation dataset (BSDS 500). \n\n"}
{"id": "1402.1801", "contents": "Title: Efficient Low Dose X-ray CT Reconstruction through Sparsity-Based MAP\n  Modeling Abstract: Ultra low radiation dose in X-ray Computed Tomography (CT) is an important\nclinical objective in order to minimize the risk of carcinogenesis. Compressed\nSensing (CS) enables significant reductions in radiation dose to be achieved by\nproducing diagnostic images from a limited number of CT projections. However,\nthe excessive computation time that conventional CS-based CT reconstruction\ntypically requires has limited clinical implementation. In this paper, we first\ndemonstrate that a thorough analysis of CT reconstruction through a Maximum a\nPosteriori objective function results in a weighted compressive sensing\nproblem. This analysis enables us to formulate a low dose fan beam and helical\ncone beam CT reconstruction. Subsequently, we provide an efficient solution to\nthe formulated CS problem based on a Fast Composite Splitting Algorithm-Latent\nExpected Maximization (FCSA-LEM) algorithm. In the proposed method we use\npseudo polar Fourier transform as the measurement matrix in order to decrease\nthe computational complexity; and rebinning of the projections to parallel rays\nin order to extend its application to fan beam and helical cone beam scans. The\nweight involved in the proposed weighted CS model, denoted by Error Adaptation\nWeight (EAW), is calculated based on the statistical characteristics of CT\nreconstruction and is a function of Poisson measurement noise and rebinning\ninterpolation error. Simulation results show that low computational complexity\nof the proposed method made the fast recovery of the CT images possible and\nusing EAW reduces the reconstruction error by one order of magnitude. Recovery\nof a high quality 512$\\times$ 512 image was achieved in less than 20 sec on a\ndesktop computer without numerical optimizations. \n\n"}
{"id": "1402.2031", "contents": "Title: Deeply Coupled Auto-encoder Networks for Cross-view Classification Abstract: The comparison of heterogeneous samples extensively exists in many\napplications, especially in the task of image classification. In this paper, we\npropose a simple but effective coupled neural network, called Deeply Coupled\nAutoencoder Networks (DCAN), which seeks to build two deep neural networks,\ncoupled with each other in every corresponding layers. In DCAN, each deep\nstructure is developed via stacking multiple discriminative coupled\nauto-encoders, a denoising auto-encoder trained with maximum margin criterion\nconsisting of intra-class compactness and inter-class penalty. This single\nlayer component makes our model simultaneously preserve the local consistency\nand enhance its discriminative capability. With increasing number of layers,\nthe coupled networks can gradually narrow the gap between the two views.\nExtensive experiments on cross-view image classification tasks demonstrate the\nsuperiority of our method over state-of-the-art methods. \n\n"}
{"id": "1403.0700", "contents": "Title: Random Projections on Manifolds of Symmetric Positive Definite Matrices\n  for Image Classification Abstract: Recent advances suggest that encoding images through Symmetric Positive\nDefinite (SPD) matrices and then interpreting such matrices as points on\nRiemannian manifolds can lead to increased classification performance. Taking\ninto account manifold geometry is typically done via (1) embedding the\nmanifolds in tangent spaces, or (2) embedding into Reproducing Kernel Hilbert\nSpaces (RKHS). While embedding into tangent spaces allows the use of existing\nEuclidean-based learning algorithms, manifold shape is only approximated which\ncan cause loss of discriminatory information. The RKHS approach retains more of\nthe manifold structure, but may require non-trivial effort to kernelise\nEuclidean-based learning algorithms. In contrast to the above approaches, in\nthis paper we offer a novel solution that allows SPD matrices to be used with\nunmodified Euclidean-based learning algorithms, with the true manifold shape\nwell-preserved. Specifically, we propose to project SPD matrices using a set of\nrandom projection hyperplanes over RKHS into a random projection space, which\nleads to representing each matrix as a vector of projection coefficients.\nExperiments on face recognition, person re-identification and texture\nclassification show that the proposed approach outperforms several recent\nmethods, such as Tensor Sparse Coding, Histogram Plus Epitome, Riemannian\nLocality Preserving Projection and Relational Divergence Classification. \n\n"}
{"id": "1403.5074", "contents": "Title: Convergence of Stochastic Proximal Gradient Algorithm Abstract: We prove novel convergence results for a stochastic proximal gradient\nalgorithm suitable for solving a large class of convex optimization problems,\nwhere a convex objective function is given by the sum of a smooth and a\npossibly non-smooth component. We consider the iterates convergence and derive\n$O(1/n)$ non asymptotic bounds in expectation in the strongly convex case, as\nwell as almost sure convergence results under weaker assumptions. Our approach\nallows to avoid averaging and weaken boundedness assumptions which are often\nconsidered in theoretical studies and might not be satisfied in practice. \n\n"}
{"id": "1404.2903", "contents": "Title: Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep\n  Object Recognition Abstract: We propose a general multi-class visual recognition model, termed the\nClassifier Graph, which aims to generalize and integrate ideas from many of\ntoday's successful hierarchical recognition approaches. Our graph-based model\nhas the advantage of enabling rich interactions between classes from different\nlevels of interpretation and abstraction. The proposed multi-class system is\nefficiently learned using step by step updates. The structure consists of\nsimple logistic linear layers with inputs from features that are automatically\nselected from a large pool. Each newly learned classifier becomes a potential\nnew feature. Thus, our feature pool can consist both of initial manually\ndesigned features as well as learned classifiers from previous steps (graph\nnodes), each copied many times at different scales and locations. In this\nmanner we can learn and grow both a deep, complex graph of classifiers and a\nrich pool of features at different levels of abstraction and interpretation.\nOur proposed graph of classifiers becomes a multi-class system with a recursive\nstructure, suitable for deep detection and recognition of several classes\nsimultaneously. \n\n"}
{"id": "1404.5009", "contents": "Title: Efficient Semidefinite Branch-and-Cut for MAP-MRF Inference Abstract: We propose a Branch-and-Cut (B&C) method for solving general MAP-MRF\ninference problems. The core of our method is a very efficient bounding\nprocedure, which combines scalable semidefinite programming (SDP) and a\ncutting-plane method for seeking violated constraints. In order to further\nspeed up the computation, several strategies have been exploited, including\nmodel reduction, warm start and removal of inactive constraints.\n  We analyze the performance of the proposed method under different settings,\nand demonstrate that our method either outperforms or performs on par with\nstate-of-the-art approaches. Especially when the connectivities are dense or\nwhen the relative magnitudes of the unary costs are low, we achieve the best\nreported results. Experiments show that the proposed algorithm achieves better\napproximation than the state-of-the-art methods within a variety of time\nbudgets on challenging non-submodular MAP-MRF inference problems. \n\n"}
{"id": "1406.2528", "contents": "Title: Denosing Using Wavelets and Projections onto the L1-Ball Abstract: Both wavelet denoising and denosing methods using the concept of sparsity are\nbased on soft-thresholding. In sparsity based denoising methods, it is assumed\nthat the original signal is sparse in some transform domains such as the\nwavelet domain and the wavelet subsignals of the noisy signal are projected\nonto L1-balls to reduce noise. In this lecture note, it is shown that the size\nof the L1-ball or equivalently the soft threshold value can be determined using\nlinear algebra. The key step is an orthogonal projection onto the epigraph set\nof the L1-norm cost function. \n\n"}
{"id": "1409.2329", "contents": "Title: Recurrent Neural Network Regularization Abstract: We present a simple regularization technique for Recurrent Neural Networks\n(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful\ntechnique for regularizing neural networks, does not work well with RNNs and\nLSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show\nthat it substantially reduces overfitting on a variety of tasks. These tasks\ninclude language modeling, speech recognition, image caption generation, and\nmachine translation. \n\n"}
{"id": "1409.7556", "contents": "Title: Location Recognition Over Large Time Lags Abstract: Would it be possible to automatically associate ancient pictures to modern\nones and create fancy cultural heritage city maps? We introduce here the task\nof recognizing the location depicted in an old photo given modern annotated\nimages collected from the Internet. We present an extensive analysis on\ndifferent features, looking for the most discriminative and most robust to the\nimage variability induced by large time lags. Moreover, we show that the\ndescribed task benefits from domain adaptation. \n\n"}
{"id": "1411.0126", "contents": "Title: Detection of texts in natural images Abstract: A framework that makes use of Connected components and supervised Support\nmachine to recognise texts is proposed. The image is preprocessed and and edge\ngraph is calculated using a probabilistic framework to compensate for\nphotometric noise. Connected components over the resultant image is calculated,\nwhich is bounded and then pruned using geometric constraints. Finally a Gabor\nFeature based SVM is used to classify the presence of text in the candidates.\nThe proposed method was tested with ICDAR 10 dataset and few other images\navailable on the internet. It resulted in a recall and precision metric of 0.72\nand 0.88 comfortably better than the benchmark Eiphstein's algorithm. The\nproposed method recorded a 0.70 and 0.74 in natural images which is\nsignificantly better than current methods on natural images. The proposed\nmethod also scales almost linearly for high resolution, cluttered images. \n\n"}
{"id": "1411.1784", "contents": "Title: Conditional Generative Adversarial Nets Abstract: Generative Adversarial Nets [8] were recently introduced as a novel way to\ntrain generative models. In this work we introduce the conditional version of\ngenerative adversarial nets, which can be constructed by simply feeding the\ndata, y, we wish to condition on to both the generator and discriminator. We\nshow that this model can generate MNIST digits conditioned on class labels. We\nalso illustrate how this model could be used to learn a multi-modal model, and\nprovide preliminary examples of an application to image tagging in which we\ndemonstrate how this approach can generate descriptive tags which are not part\nof training labels. \n\n"}
{"id": "1411.1784", "contents": "Title: Conditional Generative Adversarial Nets Abstract: Generative Adversarial Nets [8] were recently introduced as a novel way to\ntrain generative models. In this work we introduce the conditional version of\ngenerative adversarial nets, which can be constructed by simply feeding the\ndata, y, we wish to condition on to both the generator and discriminator. We\nshow that this model can generate MNIST digits conditioned on class labels. We\nalso illustrate how this model could be used to learn a multi-modal model, and\nprovide preliminary examples of an application to image tagging in which we\ndemonstrate how this approach can generate descriptive tags which are not part\nof training labels. \n\n"}
{"id": "1411.2539", "contents": "Title: Unifying Visual-Semantic Embeddings with Multimodal Neural Language\n  Models Abstract: Inspired by recent advances in multimodal learning and machine translation,\nwe introduce an encoder-decoder pipeline that learns (a): a multimodal joint\nembedding space with images and text and (b): a novel language model for\ndecoding distributed representations from our space. Our pipeline effectively\nunifies joint image-text embedding models with multimodal neural language\nmodels. We introduce the structure-content neural language model that\ndisentangles the structure of a sentence to its content, conditioned on\nrepresentations produced by the encoder. The encoder allows one to rank images\nand sentences while the decoder can generate novel descriptions from scratch.\nUsing LSTM to encode sentences, we match the state-of-the-art performance on\nFlickr8K and Flickr30K without using object detections. We also set new best\nresults when using the 19-layer Oxford convolutional network. Furthermore we\nshow that with linear encoders, the learned embedding space captures multimodal\nregularities in terms of vector space arithmetic e.g. *image of a blue car* -\n\"blue\" + \"red\" is near images of red cars. Sample captions generated for 800\nimages are made available for comparison. \n\n"}
{"id": "1411.2861", "contents": "Title: Computational Baby Learning Abstract: Intuitive observations show that a baby may inherently possess the capability\nof recognizing a new visual concept (e.g., chair, dog) by learning from only\nvery few positive instances taught by parent(s) or others, and this recognition\ncapability can be gradually further improved by exploring and/or interacting\nwith the real instances in the physical world. Inspired by these observations,\nwe propose a computational model for slightly-supervised object detection,\nbased on prior knowledge modelling, exemplar learning and learning with video\ncontexts. The prior knowledge is modeled with a pre-trained Convolutional\nNeural Network (CNN). When very few instances of a new concept are given, an\ninitial concept detector is built by exemplar learning over the deep features\nfrom the pre-trained CNN. Simulating the baby's interaction with physical\nworld, the well-designed tracking solution is then used to discover more\ndiverse instances from the massive online unlabeled videos. Once a positive\ninstance is detected/identified with high score in each video, more variable\ninstances possibly from different view-angles and/or different distances are\ntracked and accumulated. Then the concept detector can be fine-tuned based on\nthese new instances. This process can be repeated again and again till we\nobtain a very mature concept detector. Extensive experiments on Pascal\nVOC-07/10/12 object detection datasets well demonstrate the effectiveness of\nour framework. It can beat the state-of-the-art full-training based\nperformances by learning from very few samples for each object category, along\nwith about 20,000 unlabeled videos. \n\n"}
{"id": "1411.4670", "contents": "Title: AlexU-Word: A New Dataset for Isolated-Word Closed-Vocabulary Offline\n  Arabic Handwriting Recognition Abstract: In this paper, we introduce the first phase of a new dataset for offline\nArabic handwriting recognition. The aim is to collect a very large dataset of\nisolated Arabic words that covers all letters of the alphabet in all possible\nshapes using a small number of simple words. The end goal is to collect a very\nlarge dataset of segmented letter images, which can be used to build and\nevaluate Arabic handwriting recognition systems that are based on segmented\nletter recognition. The current version of the dataset contains $25114$ samples\nof $109$ unique Arabic words that cover all possible shapes of all alphabet\nletters. The samples were collected from $907$ writers. In its current form,\nthe dataset can be used for the problem of closed-vocabulary word recognition.\nWe evaluated a number of window-based descriptors and classifiers on this task\nand obtained an accuracy of $92.16\\%$ using a SIFT-based descriptor and ANN. \n\n"}
{"id": "1411.5908", "contents": "Title: Understanding image representations by measuring their equivariance and\n  equivalence Abstract: Despite the importance of image representations such as histograms of\noriented gradients and deep Convolutional Neural Networks (CNN), our\ntheoretical understanding of them remains limited. Aiming at filling this gap,\nwe investigate three key mathematical properties of representations:\nequivariance, invariance, and equivalence. Equivariance studies how\ntransformations of the input image are encoded by the representation,\ninvariance being a special case where a transformation has no effect.\nEquivalence studies whether two representations, for example two different\nparametrisations of a CNN, capture the same visual information or not. A number\nof methods to establish these properties empirically are proposed, including\nintroducing transformation and stitching layers in CNNs. These methods are then\napplied to popular representations to reveal insightful aspects of their\nstructure, including clarifying at which layers in a CNN certain geometric\ninvariances are achieved. While the focus of the paper is theoretical, direct\napplications to structured-output regression are demonstrated too. \n\n"}
{"id": "1411.6369", "contents": "Title: Scale-Invariant Convolutional Neural Networks Abstract: Even though convolutional neural networks (CNN) has achieved near-human\nperformance in various computer vision tasks, its ability to tolerate scale\nvariations is limited. The popular practise is making the model bigger first,\nand then train it with data augmentation using extensive scale-jittering. In\nthis paper, we propose a scaleinvariant convolutional neural network (SiCNN), a\nmodeldesigned to incorporate multi-scale feature exaction and classification\ninto the network structure. SiCNN uses a multi-column architecture, with each\ncolumn focusing on a particular scale. Unlike previous multi-column strategies,\nthese columns share the same set of filter parameters by a scale transformation\namong them. This design deals with scale variation without blowing up the model\nsize. Experimental results show that SiCNN detects features at various scales,\nand the classification result exhibits strong robustness against object scale\nvariations. \n\n"}
{"id": "1412.1740", "contents": "Title: Image Data Compression for Covariance and Histogram Descriptors Abstract: Covariance and histogram image descriptors provide an effective way to\ncapture information about images. Both excel when used in combination with\nspecial purpose distance metrics. For covariance descriptors these metrics\nmeasure the distance along the non-Euclidean Riemannian manifold of symmetric\npositive definite matrices. For histogram descriptors the Earth Mover's\ndistance measures the optimal transport between two histograms. Although more\nprecise, these distance metrics are very expensive to compute, making them\nimpractical in many applications, even for data sets of only a few thousand\nexamples. In this paper we present two methods to compress the size of\ncovariance and histogram datasets with only marginal increases in test error\nfor k-nearest neighbor classification. Specifically, we show that we can reduce\ndata sets to 16% and in some cases as little as 2% of their original size,\nwhile approximately matching the test error of kNN classification on the full\ntraining set. In fact, because the compressed set is learned in a supervised\nfashion, it sometimes even outperforms the full data set, while requiring only\na fraction of the space and drastically reducing test-time computation. \n\n"}
{"id": "1412.1897", "contents": "Title: Deep Neural Networks are Easily Fooled: High Confidence Predictions for\n  Unrecognizable Images Abstract: Deep neural networks (DNNs) have recently been achieving state-of-the-art\nperformance on a variety of pattern-recognition tasks, most notably visual\nclassification problems. Given that DNNs are now able to classify objects in\nimages with near-human-level performance, questions naturally arise as to what\ndifferences remain between computer and human vision. A recent study revealed\nthat changing an image (e.g. of a lion) in a way imperceptible to humans can\ncause a DNN to label the image as something else entirely (e.g. mislabeling a\nlion a library). Here we show a related result: it is easy to produce images\nthat are completely unrecognizable to humans, but that state-of-the-art DNNs\nbelieve to be recognizable objects with 99.99% confidence (e.g. labeling with\ncertainty that white noise static is a lion). Specifically, we take\nconvolutional neural networks trained to perform well on either the ImageNet or\nMNIST datasets and then find images with evolutionary algorithms or gradient\nascent that DNNs label with high confidence as belonging to each dataset class.\nIt is possible to produce images totally unrecognizable to human eyes that DNNs\nbelieve with near certainty are familiar objects, which we call \"fooling\nimages\" (more generally, fooling examples). Our results shed light on\ninteresting differences between human vision and current DNNs, and raise\nquestions about the generality of DNN computer vision. \n\n"}
{"id": "1412.4564", "contents": "Title: MatConvNet - Convolutional Neural Networks for MATLAB Abstract: MatConvNet is an implementation of Convolutional Neural Networks (CNNs) for\nMATLAB. The toolbox is designed with an emphasis on simplicity and flexibility.\nIt exposes the building blocks of CNNs as easy-to-use MATLAB functions,\nproviding routines for computing linear convolutions with filter banks, feature\npooling, and many more. In this manner, MatConvNet allows fast prototyping of\nnew CNN architectures; at the same time, it supports efficient computation on\nCPU and GPU allowing to train complex models on large datasets such as ImageNet\nILSVRC. This document provides an overview of CNNs and how they are implemented\nin MatConvNet and gives the technical details of each computational block in\nthe toolbox. \n\n"}
{"id": "1412.5126", "contents": "Title: A Robust Regression Approach for Background/Foreground Segmentation Abstract: Background/foreground segmentation has a lot of applications in image and\nvideo processing. In this paper, a segmentation algorithm is proposed which is\nmainly designed for text and line extraction in screen content. The proposed\nmethod makes use of the fact that the background in each block is usually\nsmoothly varying and can be modeled well by a linear combination of a few\nsmoothly varying basis functions, while the foreground text and graphics create\nsharp discontinuity. The algorithm separates the background and foreground\npixels by trying to fit pixel values in the block into a smooth function using\na robust regression method. The inlier pixels that can fit well will be\nconsidered as background, while remaining outlier pixels will be considered\nforeground. This algorithm has been extensively tested on several images from\nHEVC standard test sequences for screen content coding, and is shown to have\nsuperior performance over other methods, such as the k-means clustering based\nsegmentation algorithm in DjVu. This background/foreground segmentation can be\nused in different applications such as: text extraction, separate coding of\nbackground and foreground for compression of screen content and mixed content\ndocuments, principle line extraction from palmprint and crease detection in\nfingerprint images. \n\n"}
{"id": "1412.6115", "contents": "Title: Compressing Deep Convolutional Networks using Vector Quantization Abstract: Deep convolutional neural networks (CNN) has become the most promising method\nfor object recognition, repeatedly demonstrating record breaking results for\nimage classification and object detection in recent years. However, a very deep\nCNN generally involves many layers with millions of parameters, making the\nstorage of the network model to be extremely large. This prohibits the usage of\ndeep CNNs on resource limited hardware, especially cell phones or other\nembedded devices. In this paper, we tackle this model storage issue by\ninvestigating information theoretical vector quantization methods for\ncompressing the parameters of CNNs. In particular, we have found in terms of\ncompressing the most storage demanding dense connected layers, vector\nquantization methods have a clear gain over existing matrix factorization\nmethods. Simply applying k-means clustering to the weights or conducting\nproduct quantization can lead to a very good balance between model size and\nrecognition accuracy. For the 1000-category classification task in the ImageNet\nchallenge, we are able to achieve 16-24 times compression of the network with\nonly 1% loss of classification accuracy using the state-of-the-art CNN. \n\n"}
{"id": "1412.6154", "contents": "Title: Effective persistent homology of digital images Abstract: In this paper, three Computational Topology methods (namely effective\nhomology, persistent homology and discrete vector fields) are mixed together to\nproduce algorithms for homological digital image processing. The algorithms\nhave been implemented as extensions of the Kenzo system and have shown a good\nperformance when applied on some actual images extracted from a public dataset. \n\n"}
{"id": "1412.6572", "contents": "Title: Explaining and Harnessing Adversarial Examples Abstract: Several machine learning models, including neural networks, consistently\nmisclassify adversarial examples---inputs formed by applying small but\nintentionally worst-case perturbations to examples from the dataset, such that\nthe perturbed input results in the model outputting an incorrect answer with\nhigh confidence. Early attempts at explaining this phenomenon focused on\nnonlinearity and overfitting. We argue instead that the primary cause of neural\nnetworks' vulnerability to adversarial perturbation is their linear nature.\nThis explanation is supported by new quantitative results while giving the\nfirst explanation of the most intriguing fact about them: their generalization\nacross architectures and training sets. Moreover, this view yields a simple and\nfast method of generating adversarial examples. Using this approach to provide\nexamples for adversarial training, we reduce the test set error of a maxout\nnetwork on the MNIST dataset. \n\n"}
{"id": "1412.6614", "contents": "Title: In Search of the Real Inductive Bias: On the Role of Implicit\n  Regularization in Deep Learning Abstract: We present experiments demonstrating that some other form of capacity\ncontrol, different from network size, plays a central role in learning\nmultilayer feed-forward networks. We argue, partially through analogy to matrix\nfactorization, that this is an inductive bias that can help shed light on deep\nlearning. \n\n"}
{"id": "1412.6806", "contents": "Title: Striving for Simplicity: The All Convolutional Net Abstract: Most modern convolutional neural networks (CNNs) used for object recognition\nare built using the same principles: Alternating convolution and max-pooling\nlayers followed by a small number of fully connected layers. We re-evaluate the\nstate of the art for object recognition from small images with convolutional\nnetworks, questioning the necessity of different components in the pipeline. We\nfind that max-pooling can simply be replaced by a convolutional layer with\nincreased stride without loss in accuracy on several image recognition\nbenchmarks. Following this finding -- and building on other recent work for\nfinding simple network structures -- we propose a new architecture that\nconsists solely of convolutional layers and yields competitive or state of the\nart performance on several object recognition datasets (CIFAR-10, CIFAR-100,\nImageNet). To analyze the network we introduce a new variant of the\n\"deconvolution approach\" for visualizing features learned by CNNs, which can be\napplied to a broader range of network structures than existing approaches. \n\n"}
{"id": "1412.6808", "contents": "Title: Learning the nonlinear geometry of high-dimensional data: Models and\n  algorithms Abstract: Modern information processing relies on the axiom that high-dimensional data\nlie near low-dimensional geometric structures. This paper revisits the problem\nof data-driven learning of these geometric structures and puts forth two new\nnonlinear geometric models for data describing \"related\" objects/phenomena. The\nfirst one of these models straddles the two extremes of the subspace model and\nthe union-of-subspaces model, and is termed the metric-constrained\nunion-of-subspaces (MC-UoS) model. The second one of these models---suited for\ndata drawn from a mixture of nonlinear manifolds---generalizes the kernel\nsubspace model, and is termed the metric-constrained kernel union-of-subspaces\n(MC-KUoS) model. The main contributions of this paper in this regard include\nthe following. First, it motivates and formalizes the problems of MC-UoS and\nMC-KUoS learning. Second, it presents algorithms that efficiently learn an\nMC-UoS or an MC-KUoS underlying data of interest. Third, it extends these\nalgorithms to the case when parts of the data are missing. Last, but not least,\nit reports the outcomes of a series of numerical experiments involving both\nsynthetic and real data that demonstrate the superiority of the proposed\ngeometric models and learning algorithms over existing approaches in the\nliterature. These experiments also help clarify the connections between this\nwork and the literature on (subspace and kernel k-means) clustering. \n\n"}
{"id": "1501.02530", "contents": "Title: A Dataset for Movie Description Abstract: Descriptive video service (DVS) provides linguistic descriptions of movies\nand allows visually impaired people to follow a movie along with their peers.\nSuch descriptions are by design mainly visual and thus naturally form an\ninteresting data source for computer vision and computational linguistics. In\nthis work we propose a novel dataset which contains transcribed DVS, which is\ntemporally aligned to full length HD movies. In addition we also collected the\naligned movie scripts which have been used in prior work and compare the two\ndifferent sources of descriptions. In total the Movie Description dataset\ncontains a parallel corpus of over 54,000 sentences and video snippets from 72\nHD movies. We characterize the dataset by benchmarking different approaches for\ngenerating video descriptions. Comparing DVS to scripts, we find that DVS is\nfar more visual and describes precisely what is shown rather than what should\nhappen according to the scripts created prior to movie production. \n\n"}
{"id": "1501.04690", "contents": "Title: Naive-Deep Face Recognition: Touching the Limit of LFW Benchmark or Not? Abstract: Face recognition performance improves rapidly with the recent deep learning\ntechnique developing and underlying large training dataset accumulating. In\nthis paper, we report our observations on how big data impacts the recognition\nperformance. According to these observations, we build our Megvii Face\nRecognition System, which achieves 99.50% accuracy on the LFW benchmark,\noutperforming the previous state-of-the-art. Furthermore, we report the\nperformance in a real-world security certification scenario. There still exists\na clear gap between machine recognition and human performance. We summarize our\nexperiments and present three challenges lying ahead in recent face\nrecognition. And we indicate several possible solutions towards these\nchallenges. We hope our work will stimulate the community's discussion of the\ndifference between research benchmark and real-world applications. \n\n"}
{"id": "1502.01852", "contents": "Title: Delving Deep into Rectifiers: Surpassing Human-Level Performance on\n  ImageNet Classification Abstract: Rectified activation units (rectifiers) are essential for state-of-the-art\nneural networks. In this work, we study rectifier neural networks for image\nclassification from two aspects. First, we propose a Parametric Rectified\nLinear Unit (PReLU) that generalizes the traditional rectified unit. PReLU\nimproves model fitting with nearly zero extra computational cost and little\noverfitting risk. Second, we derive a robust initialization method that\nparticularly considers the rectifier nonlinearities. This method enables us to\ntrain extremely deep rectified models directly from scratch and to investigate\ndeeper or wider network architectures. Based on our PReLU networks\n(PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012\nclassification dataset. This is a 26% relative improvement over the ILSVRC 2014\nwinner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass\nhuman-level performance (5.1%, Russakovsky et al.) on this visual recognition\nchallenge. \n\n"}
{"id": "1502.04492", "contents": "Title: Towards Building Deep Networks with Bayesian Factor Graphs Abstract: We propose a Multi-Layer Network based on the Bayesian framework of the\nFactor Graphs in Reduced Normal Form (FGrn) applied to a two-dimensional\nlattice. The Latent Variable Model (LVM) is the basic building block of a\nquadtree hierarchy built on top of a bottom layer of random variables that\nrepresent pixels of an image, a feature map, or more generally a collection of\nspatially distributed discrete variables. The multi-layer architecture\nimplements a hierarchical data representation that, via belief propagation, can\nbe used for learning and inference. Typical uses are pattern completion,\ncorrection and classification. The FGrn paradigm provides great flexibility and\nmodularity and appears as a promising candidate for building deep networks: the\nsystem can be easily extended by introducing new and different (in cardinality\nand in type) variables. Prior knowledge, or supervised information, can be\nintroduced at different scales. The FGrn paradigm provides a handy way for\nbuilding all kinds of architectures by interconnecting only three types of\nunits: Single Input Single Output (SISO) blocks, Sources and Replicators. The\nnetwork is designed like a circuit diagram and the belief messages flow\nbidirectionally in the whole system. The learning algorithms operate only\nlocally within each block. The framework is demonstrated in this paper in a\nthree-layer structure applied to images extracted from a standard data set. \n\n"}
{"id": "1502.04726", "contents": "Title: ICR: Iterative Convex Refinement for Sparse Signal Recovery Using Spike\n  and Slab Priors Abstract: In this letter, we address sparse signal recovery using spike and slab\npriors. In particular, we focus on a Bayesian framework where sparsity is\nenforced on reconstruction coefficients via probabilistic priors. The\noptimization resulting from spike and slab prior maximization is known to be a\nhard non-convex problem, and existing solutions involve simplifying assumptions\nand/or relaxations. We propose an approach called Iterative Convex Refinement\n(ICR) that aims to solve the aforementioned optimization problem directly\nallowing for greater generality in the sparse structure. Essentially, ICR\nsolves a sequence of convex optimization problems such that sequence of\nsolutions converges to a sub-optimal solution of the original hard optimization\nproblem. We propose two versions of our algorithm: a.) an unconstrained\nversion, and b.) with a non-negativity constraint on sparse coefficients, which\nmay be required in some real-world problems. Experimental validation is\nperformed on both synthetic data and for a real-world image recovery problem,\nwhich illustrates merits of ICR over state of the art alternatives. \n\n"}
{"id": "1502.07770", "contents": "Title: Total variation on a tree Abstract: We consider the problem of minimizing the continuous valued total variation\nsubject to different unary terms on trees and propose fast direct algorithms\nbased on dynamic programming to solve these problems. We treat both the convex\nand the non-convex case and derive worst case complexities that are equal or\nbetter than existing methods. We show applications to total variation based 2D\nimage processing and computer vision problems based on a Lagrangian\ndecomposition approach. The resulting algorithms are very efficient, offer a\nhigh degree of parallelism and come along with memory requirements which are\nonly in the order of the number of image pixels. \n\n"}
{"id": "1502.08029", "contents": "Title: Describing Videos by Exploiting Temporal Structure Abstract: Recent progress in using recurrent neural networks (RNNs) for image\ndescription has motivated the exploration of their application for video\ndescription. However, while images are static, working with videos requires\nmodeling their dynamic temporal structure and then properly integrating that\ninformation into a natural language description. In this context, we propose an\napproach that successfully takes into account both the local and global\ntemporal structure of videos to produce descriptions. First, our approach\nincorporates a spatial temporal 3-D convolutional neural network (3-D CNN)\nrepresentation of the short temporal dynamics. The 3-D CNN representation is\ntrained on video action recognition tasks, so as to produce a representation\nthat is tuned to human motion and behavior. Second we propose a temporal\nattention mechanism that allows to go beyond local temporal modeling and learns\nto automatically select the most relevant temporal segments given the\ntext-generating RNN. Our approach exceeds the current state-of-art for both\nBLEU and METEOR metrics on the Youtube2Text dataset. We also present results on\na new, larger and more challenging dataset of paired video and natural language\ndescriptions. \n\n"}
{"id": "1503.00075", "contents": "Title: Improved Semantic Representations From Tree-Structured Long Short-Term\n  Memory Networks Abstract: Because of their superior ability to preserve sequence information over time,\nLong Short-Term Memory (LSTM) networks, a type of recurrent neural network with\na more complex computational unit, have obtained strong results on a variety of\nsequence modeling tasks. The only underlying LSTM structure that has been\nexplored so far is a linear chain. However, natural language exhibits syntactic\nproperties that would naturally combine words to phrases. We introduce the\nTree-LSTM, a generalization of LSTMs to tree-structured network topologies.\nTree-LSTMs outperform all existing systems and strong LSTM baselines on two\ntasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task\n1) and sentiment classification (Stanford Sentiment Treebank). \n\n"}
{"id": "1503.01558", "contents": "Title: What's Cookin'? Interpreting Cooking Videos using Text, Speech and\n  Vision Abstract: We present a novel method for aligning a sequence of instructions to a video\nof someone carrying out a task. In particular, we focus on the cooking domain,\nwhere the instructions correspond to the recipe. Our technique relies on an HMM\nto align the recipe steps to the (automatically generated) speech transcript.\nWe then refine this alignment using a state-of-the-art visual food detector,\nbased on a deep convolutional neural network. We show that our technique\noutperforms simpler techniques based on keyword spotting. It also enables\ninteresting applications, such as automatically illustrating recipes with\nkeyframes, and searching within a video for events of interest. \n\n"}
{"id": "1503.03231", "contents": "Title: Adaptive-Rate Sparse Signal Reconstruction With Application in\n  Compressive Background Subtraction Abstract: We propose and analyze an online algorithm for reconstructing a sequence of\nsignals from a limited number of linear measurements. The signals are assumed\nsparse, with unknown support, and evolve over time according to a generic\nnonlinear dynamical model. Our algorithm, based on recent theoretical results\nfor $\\ell_1$-$\\ell_1$ minimization, is recursive and computes the number of\nmeasurements to be taken at each time on-the-fly. As an example, we apply the\nalgorithm to compressive video background subtraction, a problem that can be\nstated as follows: given a set of measurements of a sequence of images with a\nstatic background, simultaneously reconstruct each image while separating its\nforeground from the background. The performance of our method is illustrated on\nsequences of real images: we observe that it allows a dramatic reduction in the\nnumber of measurements with respect to state-of-the-art compressive background\nsubtraction schemes. \n\n"}
{"id": "1503.06350", "contents": "Title: Boosting Convolutional Features for Robust Object Proposals Abstract: Deep Convolutional Neural Networks (CNNs) have demonstrated excellent\nperformance in image classification, but still show room for improvement in\nobject-detection tasks with many categories, in particular for cluttered scenes\nand occlusion. Modern detection algorithms like Regions with CNNs (Girshick et\nal., 2014) rely on Selective Search (Uijlings et al., 2013) to propose regions\nwhich with high probability represent objects, where in turn CNNs are deployed\nfor classification. Selective Search represents a family of sophisticated\nalgorithms that are engineered with multiple segmentation, appearance and\nsaliency cues, typically coming with a significant run-time overhead.\nFurthermore, (Hosang et al., 2014) have shown that most methods suffer from low\nreproducibility due to unstable superpixels, even for slight image\nperturbations. Although CNNs are subsequently used for classification in\ntop-performing object-detection pipelines, current proposal methods are\nagnostic to how these models parse objects and their rich learned\nrepresentations. As a result they may propose regions which may not resemble\nhigh-level objects or totally miss some of them. To overcome these drawbacks we\npropose a boosting approach which directly takes advantage of hierarchical CNN\nfeatures for detecting regions of interest fast. We demonstrate its performance\non ImageNet 2013 detection benchmark and compare it with state-of-the-art\nmethods. \n\n"}
{"id": "1504.00641", "contents": "Title: A Probabilistic Theory of Deep Learning Abstract: A grand challenge in machine learning is the development of computational\nalgorithms that match or outperform humans in perceptual inference tasks that\nare complicated by nuisance variation. For instance, visual object recognition\ninvolves the unknown object position, orientation, and scale in object\nrecognition while speech recognition involves the unknown voice pronunciation,\npitch, and speed. Recently, a new breed of deep learning algorithms have\nemerged for high-nuisance inference tasks that routinely yield pattern\nrecognition systems with near- or super-human capabilities. But a fundamental\nquestion remains: Why do they work? Intuitions abound, but a coherent framework\nfor understanding, analyzing, and synthesizing deep learning architectures has\nremained elusive. We answer this question by developing a new probabilistic\nframework for deep learning based on the Deep Rendering Model: a generative\nprobabilistic model that explicitly captures latent nuisance variation. By\nrelaxing the generative model to a discriminative one, we can recover two of\nthe current leading deep learning systems, deep convolutional neural networks\nand random decision forests, providing insights into their successes and\nshortcomings, as well as a principled route to their improvement. \n\n"}
{"id": "1504.01052", "contents": "Title: Fast algorithms for morphological operations using run-length encoded\n  binary images Abstract: This paper presents innovative algorithms to efficiently compute erosions and\ndilations of run-length encoded (RLE) binary images with arbitrary shaped\nstructuring elements. An RLE image is given by a set of runs, where a run is a\nhorizontal concatenation of foreground pixels. The proposed algorithms extract\nthe skeleton of the structuring element and build distance tables of the input\nimage, which are storing the distance to the next background pixel on the left\nand right hand sides. This information is then used to speed up the\ncalculations of the erosion and dilation operator by enabling the use of\ntechniques which allow to skip the analysis of certain pixels whenever a hit or\nmiss occurs. Additionally the input image gets trimmed during the preprocessing\nsteps on the base of two primitive criteria. Experimental results show the\nadvantages over other algorithms. The source code of our algorithms is\navailable in C++. \n\n"}
{"id": "1504.01806", "contents": "Title: Kernelized Low Rank Representation on Grassmann Manifolds Abstract: Low rank representation (LRR) has recently attracted great interest due to\nits pleasing efficacy in exploring low-dimensional subspace structures embedded\nin data. One of its successful applications is subspace clustering which means\ndata are clustered according to the subspaces they belong to. In this paper, at\na higher level, we intend to cluster subspaces into classes of subspaces. This\nis naturally described as a clustering problem on Grassmann manifold. The\nnovelty of this paper is to generalize LRR on Euclidean space onto an LRR model\non Grassmann manifold in a uniform kernelized framework. The new methods have\nmany applications in computer vision tasks. Several clustering experiments are\nconducted on handwritten digit images, dynamic textures, human face clips and\ntraffic scene sequences. The experimental results show that the proposed\nmethods outperform a number of state-of-the-art subspace clustering methods. \n\n"}
{"id": "1504.06785", "contents": "Title: Complete Dictionary Recovery over the Sphere Abstract: We consider the problem of recovering a complete (i.e., square and\ninvertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb R^{n \\times p}$\nwith $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is\nsufficiently sparse. This recovery problem is central to the theoretical\nunderstanding of dictionary learning, which seeks a sparse representation for a\ncollection of input signals, and finds numerous applications in modern signal\nprocessing and machine learning. We give the first efficient algorithm that\nprovably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per\ncolumn, under suitable probability model for $\\mathbf X_0$. In contrast, prior\nresults based on efficient algorithms provide recovery guarantees when $\\mathbf\nX_0$ has only $O(n^{1-\\delta})$ nonzeros per column for any constant $\\delta\n\\in (0, 1)$.\n  Our algorithmic pipeline centers around solving a certain nonconvex\noptimization problem with a spherical constraint, and hence is naturally\nphrased in the language of manifold optimization. To show this apparently hard\nproblem is tractable, we first provide a geometric characterization of the\nhigh-dimensional objective landscape, which shows that with high probability\nthere are no \"spurious\" local minima. This particular geometric structure\nallows us to design a Riemannian trust region algorithm over the sphere that\nprovably converges to one local minimizer with an arbitrary initialization,\ndespite the presence of saddle points. The geometric approach we develop here\nmay also shed light on other problems arising from nonconvex recovery of\nstructured signals. \n\n"}
{"id": "1504.06852", "contents": "Title: FlowNet: Learning Optical Flow with Convolutional Networks Abstract: Convolutional neural networks (CNNs) have recently been very successful in a\nvariety of computer vision tasks, especially on those linked to recognition.\nOptical flow estimation has not been among the tasks where CNNs were\nsuccessful. In this paper we construct appropriate CNNs which are capable of\nsolving the optical flow estimation problem as a supervised learning task. We\npropose and compare two architectures: a generic architecture and another one\nincluding a layer that correlates feature vectors at different image locations.\n  Since existing ground truth data sets are not sufficiently large to train a\nCNN, we generate a synthetic Flying Chairs dataset. We show that networks\ntrained on this unrealistic data still generalize very well to existing\ndatasets such as Sintel and KITTI, achieving competitive accuracy at frame\nrates of 5 to 10 fps. \n\n"}
{"id": "1504.06897", "contents": "Title: Linear Spatial Pyramid Matching Using Non-convex and non-negative Sparse\n  Coding for Image Classification Abstract: Recently sparse coding have been highly successful in image classification\nmainly due to its capability of incorporating the sparsity of image\nrepresentation. In this paper, we propose an improved sparse coding model based\non linear spatial pyramid matching(SPM) and Scale Invariant Feature Transform\n(SIFT ) descriptors. The novelty is the simultaneous non-convex and\nnon-negative characters added to the sparse coding model. Our numerical\nexperiments show that the improved approach using non-convex and non-negative\nsparse coding is superior than the original ScSPM[1] on several typical\ndatabases. \n\n"}
{"id": "1505.00066", "contents": "Title: Pose Induction for Novel Object Categories Abstract: We address the task of predicting pose for objects of unannotated object\ncategories from a small seed set of annotated object classes. We present a\ngeneralized classifier that can reliably induce pose given a single instance of\na novel category. In case of availability of a large collection of novel\ninstances, our approach then jointly reasons over all instances to improve the\ninitial estimates. We empirically validate the various components of our\nalgorithm and quantitatively show that our method produces reliable pose\nestimates. We also show qualitative results on a diverse set of classes and\nfurther demonstrate the applicability of our system for learning shape models\nof novel object classes. \n\n"}
{"id": "1505.00387", "contents": "Title: Highway Networks Abstract: There is plenty of theoretical and empirical evidence that depth of neural\nnetworks is a crucial ingredient for their success. However, network training\nbecomes more difficult with increasing depth and training of very deep networks\nremains an open problem. In this extended abstract, we introduce a new\narchitecture designed to ease gradient-based training of very deep networks. We\nrefer to networks with this architecture as highway networks, since they allow\nunimpeded information flow across several layers on \"information highways\". The\narchitecture is characterized by the use of gating units which learn to\nregulate the flow of information through a network. Highway networks with\nhundreds of layers can be trained directly using stochastic gradient descent\nand with a variety of activation functions, opening up the possibility of\nstudying extremely deep and efficient architectures. \n\n"}
{"id": "1505.00571", "contents": "Title: Higher Order Maximum Persistency and Comparison Theorems Abstract: We address combinatorial problems that can be formulated as minimization of a\npartially separable function of discrete variables (energy minimization in\ngraphical models, weighted constraint satisfaction, pseudo-Boolean\noptimization, 0-1 polynomial programming). For polyhedral relaxations of such\nproblems it is generally not true that variables integer in the relaxed\nsolution will retain the same values in the optimal discrete solution. Those\nwhich do are called persistent. Such persistent variables define a part of a\nglobally optimal solution. Once identified, they can be excluded from the\nproblem, reducing its size.\n  To any polyhedral relaxation we associate a sufficient condition proving\npersistency of a subset of variables. We set up a specially constructed linear\nprogram which determines the set of persistent variables maximal with respect\nto the relaxation. The condition improves as the relaxation is tightened and\npossesses all its invariances. The proposed framework explains a variety of\nexisting methods originating from different areas of research and based on\ndifferent principles. A theoretical comparison is established that relates\nthese methods to the standard linear relaxation and proves that the proposed\ntechnique identifies same or larger set of persistent variables. \n\n"}
{"id": "1505.00663", "contents": "Title: See the Difference: Direct Pre-Image Reconstruction and Pose Estimation\n  by Differentiating HOG Abstract: The Histogram of Oriented Gradient (HOG) descriptor has led to many advances\nin computer vision over the last decade and is still part of many state of the\nart approaches. We realize that the associated feature computation is piecewise\ndifferentiable and therefore many pipelines which build on HOG can be made\ndifferentiable. This lends to advanced introspection as well as opportunities\nfor end-to-end optimization. We present our implementation of $\\nabla$HOG based\non the auto-differentiation toolbox Chumpy and show applications to pre-image\nvisualization and pose estimation which extends the existing differentiable\nrenderer OpenDR pipeline. Both applications improve on the respective\nstate-of-the-art HOG approaches. \n\n"}
{"id": "1505.00853", "contents": "Title: Empirical Evaluation of Rectified Activations in Convolutional Network Abstract: In this paper we investigate the performance of different types of rectified\nactivation functions in convolutional neural network: standard rectified linear\nunit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified\nlinear unit (PReLU) and a new randomized leaky rectified linear units (RReLU).\nWe evaluate these activation function on standard image classification task.\nOur experiments suggest that incorporating a non-zero slope for negative part\nin rectified activation units could consistently improve the results. Thus our\nfindings are negative on the common belief that sparsity is the key of good\nperformance in ReLU. Moreover, on small scale dataset, using deterministic\nnegative slope or learning it are both prone to overfitting. They are not as\neffective as using their randomized counterpart. By using RReLU, we achieved\n75.68\\% accuracy on CIFAR-100 test set without multiple test or ensemble. \n\n"}
{"id": "1505.02890", "contents": "Title: Sparse 3D convolutional neural networks Abstract: We have implemented a convolutional neural network designed for processing\nsparse three-dimensional input data. The world we live in is three dimensional\nso there are a large number of potential applications including 3D object\nrecognition and analysis of space-time objects. In the quest for efficiency, we\nexperiment with CNNs on the 2D triangular-lattice and 3D tetrahedral-lattice. \n\n"}
{"id": "1505.03229", "contents": "Title: APAC: Augmented PAttern Classification with Neural Networks Abstract: Deep neural networks have been exhibiting splendid accuracies in many of\nvisual pattern classification problems. Many of the state-of-the-art methods\nemploy a technique known as data augmentation at the training stage. This paper\naddresses an issue of decision rule for classifiers trained with augmented\ndata. Our method is named as APAC: the Augmented PAttern Classification, which\nis a way of classification using the optimal decision rule for augmented data\nlearning. Discussion of methods of data augmentation is not our primary focus.\nWe show clear evidences that APAC gives far better generalization performance\nthan the traditional way of class prediction in several experiments. Our\nconvolutional neural network model with APAC achieved a state-of-the-art\naccuracy on the MNIST dataset among non-ensemble classifiers. Even our\nmultilayer perceptron model beats some of the convolutional models with\nrecently invented stochastic regularization techniques on the CIFAR-10 dataset. \n\n"}
{"id": "1505.05561", "contents": "Title: Why Regularized Auto-Encoders learn Sparse Representation? Abstract: While the authors of Batch Normalization (BN) identify and address an\nimportant problem involved in training deep networks-- \\textit{Internal\nCovariate Shift}-- the current solution has certain drawbacks. For instance, BN\ndepends on batch statistics for layerwise input normalization during training\nwhich makes the estimates of mean and standard deviation of input\n(distribution) to hidden layers inaccurate due to shifting parameter values\n(especially during initial training epochs). Another fundamental problem with\nBN is that it cannot be used with batch-size $ 1 $ during training. We address\nthese drawbacks of BN by proposing a non-adaptive normalization technique for\nremoving covariate shift, that we call \\textit{Normalization Propagation}. Our\napproach does not depend on batch statistics, but rather uses a\ndata-independent parametric estimate of mean and standard-deviation in every\nlayer thus being computationally faster compared with BN. We exploit the\nobservation that the pre-activation before Rectified Linear Units follow\nGaussian distribution in deep networks, and that once the first and second\norder statistics of any given dataset are normalized, we can forward propagate\nthis normalization without the need for recalculating the approximate\nstatistics for hidden layers. \n\n"}
{"id": "1505.05770", "contents": "Title: Variational Inference with Normalizing Flows Abstract: The choice of approximate posterior distribution is one of the core problems\nin variational inference. Most applications of variational inference employ\nsimple families of posterior approximations in order to allow for efficient\ninference, focusing on mean-field or other simple structured approximations.\nThis restriction has a significant impact on the quality of inferences made\nusing variational methods. We introduce a new approach for specifying flexible,\narbitrarily complex and scalable approximate posterior distributions. Our\napproximations are distributions constructed through a normalizing flow,\nwhereby a simple initial density is transformed into a more complex one by\napplying a sequence of invertible transformations until a desired level of\ncomplexity is attained. We use this view of normalizing flows to develop\ncategories of finite and infinitesimal flows and provide a unified view of\napproaches for constructing rich posterior approximations. We demonstrate that\nthe theoretical advantages of having posteriors that better match the true\nposterior, combined with the scalability of amortized variational approaches,\nprovides a clear improvement in performance and applicability of variational\ninference. \n\n"}
{"id": "1505.05770", "contents": "Title: Variational Inference with Normalizing Flows Abstract: The choice of approximate posterior distribution is one of the core problems\nin variational inference. Most applications of variational inference employ\nsimple families of posterior approximations in order to allow for efficient\ninference, focusing on mean-field or other simple structured approximations.\nThis restriction has a significant impact on the quality of inferences made\nusing variational methods. We introduce a new approach for specifying flexible,\narbitrarily complex and scalable approximate posterior distributions. Our\napproximations are distributions constructed through a normalizing flow,\nwhereby a simple initial density is transformed into a more complex one by\napplying a sequence of invertible transformations until a desired level of\ncomplexity is attained. We use this view of normalizing flows to develop\ncategories of finite and infinitesimal flows and provide a unified view of\napproaches for constructing rich posterior approximations. We demonstrate that\nthe theoretical advantages of having posteriors that better match the true\nposterior, combined with the scalability of amortized variational approaches,\nprovides a clear improvement in performance and applicability of variational\ninference. \n\n"}
{"id": "1505.06821", "contents": "Title: Deep Ranking for Person Re-identification via Joint Representation\n  Learning Abstract: This paper proposes a novel approach to person re-identification, a\nfundamental task in distributed multi-camera surveillance systems. Although a\nvariety of powerful algorithms have been presented in the past few years, most\nof them usually focus on designing hand-crafted features and learning metrics\neither individually or sequentially. Different from previous works, we\nformulate a unified deep ranking framework that jointly tackles both of these\nkey components to maximize their strengths. We start from the principle that\nthe correct match of the probe image should be positioned in the top rank\nwithin the whole gallery set. An effective learning-to-rank algorithm is\nproposed to minimize the cost corresponding to the ranking disorders of the\ngallery. The ranking model is solved with a deep convolutional neural network\n(CNN) that builds the relation between input image pairs and their similarity\nscores through joint representation learning directly from raw image pixels.\nThe proposed framework allows us to get rid of feature engineering and does not\nrely on any assumption. An extensive comparative evaluation is given,\ndemonstrating that our approach significantly outperforms all state-of-the-art\napproaches, including both traditional and CNN-based methods on the challenging\nVIPeR, CUHK-01 and CAVIAR4REID datasets. Additionally, our approach has better\nability to generalize across datasets without fine-tuning. \n\n"}
{"id": "1505.07293", "contents": "Title: SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust\n  Semantic Pixel-Wise Labelling Abstract: We propose a novel deep architecture, SegNet, for semantic pixel wise image\nlabelling. SegNet has several attractive properties; (i) it only requires\nforward evaluation of a fully learnt function to obtain smooth label\npredictions, (ii) with increasing depth, a larger context is considered for\npixel labelling which improves accuracy, and (iii) it is easy to visualise the\neffect of feature activation(s) in the pixel label space at any depth. SegNet\nis composed of a stack of encoders followed by a corresponding decoder stack\nwhich feeds into a soft-max classification layer. The decoders help map low\nresolution feature maps at the output of the encoder stack to full input image\nsize feature maps. This addresses an important drawback of recent deep learning\napproaches which have adopted networks designed for object categorization for\npixel wise labelling. These methods lack a mechanism to map deep layer feature\nmaps to input dimensions. They resort to ad hoc methods to upsample features,\ne.g. by replication. This results in noisy predictions and also restricts the\nnumber of pooling layers in order to avoid too much upsampling and thus reduces\nspatial context. SegNet overcomes these problems by learning to map encoder\noutputs to image pixel labels. We test the performance of SegNet on outdoor RGB\nscenes from CamVid, KITTI and indoor scenes from the NYU dataset. Our results\nshow that SegNet achieves state-of-the-art performance even without use of\nadditional cues such as depth, video frames or post-processing with CRF models. \n\n"}
{"id": "1506.00333", "contents": "Title: Learning to Answer Questions From Image Using Convolutional Neural\n  Network Abstract: In this paper, we propose to employ the convolutional neural network (CNN)\nfor the image question answering (QA). Our proposed CNN provides an end-to-end\nframework with convolutional architectures for learning not only the image and\nquestion representations, but also their inter-modal interactions to produce\nthe answer. More specifically, our model consists of three CNNs: one image CNN\nto encode the image content, one sentence CNN to compose the words of the\nquestion, and one multimodal convolution layer to learn their joint\nrepresentation for the classification in the space of candidate answer words.\nWe demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA\ndatasets, which are two benchmark datasets for the image QA, with the\nperformances significantly outperforming the state-of-the-art. \n\n"}
{"id": "1506.02169", "contents": "Title: Approximating Likelihood Ratios with Calibrated Discriminative\n  Classifiers Abstract: In many fields of science, generalized likelihood ratio tests are established\ntools for statistical inference. At the same time, it has become increasingly\ncommon that a simulator (or generative model) is used to describe complex\nprocesses that tie parameters $\\theta$ of an underlying theory and measurement\napparatus to high-dimensional observations $\\mathbf{x}\\in \\mathbb{R}^p$.\nHowever, simulator often do not provide a way to evaluate the likelihood\nfunction for a given observation $\\mathbf{x}$, which motivates a new class of\nlikelihood-free inference algorithms. In this paper, we show that likelihood\nratios are invariant under a specific class of dimensionality reduction maps\n$\\mathbb{R}^p \\mapsto \\mathbb{R}$. As a direct consequence, we show that\ndiscriminative classifiers can be used to approximate the generalized\nlikelihood ratio statistic when only a generative model for the data is\navailable. This leads to a new machine learning-based approach to\nlikelihood-free inference that is complementary to Approximate Bayesian\nComputation, and which does not require a prior on the model parameters.\nExperimental results on artificial problems with known exact likelihoods\nillustrate the potential of the proposed method. \n\n"}
{"id": "1506.03478", "contents": "Title: Generative Image Modeling Using Spatial LSTMs Abstract: Modeling the distribution of natural images is challenging, partly because of\nstrong statistical dependencies which can extend over hundreds of pixels.\nRecurrent neural networks have been successful in capturing long-range\ndependencies in a number of problems but only recently have found their way\ninto generative image models. We here introduce a recurrent image model based\non multi-dimensional long short-term memory units which are particularly suited\nfor image modeling due to their spatial structure. Our model scales to images\nof arbitrary size and its likelihood is computationally tractable. We find that\nit outperforms the state of the art in quantitative comparisons on several\nimage datasets and produces promising results when used for texture synthesis\nand inpainting. \n\n"}
{"id": "1506.05163", "contents": "Title: Deep Convolutional Networks on Graph-Structured Data Abstract: Deep Learning's recent successes have mostly relied on Convolutional\nNetworks, which exploit fundamental statistical properties of images, sounds\nand video data: the local stationarity and multi-scale compositional structure,\nthat allows expressing long range interactions in terms of shorter, localized\ninteractions. However, there exist other important examples, such as text\ndocuments or bioinformatic data, that may lack some or all of these strong\nstatistical regularities.\n  In this paper we consider the general question of how to construct deep\narchitectures with small learning complexity on general non-Euclidean domains,\nwhich are typically unknown and need to be estimated from the data. In\nparticular, we develop an extension of Spectral Networks which incorporates a\nGraph Estimation procedure, that we test on large-scale classification\nproblems, matching or improving over Dropout Networks with far less parameters\nto estimate. \n\n"}
{"id": "1506.06579", "contents": "Title: Understanding Neural Networks Through Deep Visualization Abstract: Recent years have produced great advances in training large, deep neural\nnetworks (DNNs), including notable successes in training convolutional neural\nnetworks (convnets) to recognize natural images. However, our understanding of\nhow these models work, especially what computations they perform at\nintermediate layers, has lagged behind. Progress in the field will be further\naccelerated by the development of better tools for visualizing and interpreting\nneural nets. We introduce two such tools here. The first is a tool that\nvisualizes the activations produced on each layer of a trained convnet as it\nprocesses an image or video (e.g. a live webcam stream). We have found that\nlooking at live activations that change in response to user input helps build\nvaluable intuitions about how convnets work. The second tool enables\nvisualizing features at each layer of a DNN via regularized optimization in\nimage space. Because previous versions of this idea produced less recognizable\nimages, here we introduce several new regularization methods that combine to\nproduce qualitatively clearer, more interpretable visualizations. Both tools\nare open source and work on a pre-trained convnet with minimal setup. \n\n"}
{"id": "1506.08316", "contents": "Title: Keypoint Encoding for Improved Feature Extraction from Compressed Video\n  at Low Bitrates Abstract: In many mobile visual analysis applications, compressed video is transmitted\nover a communication network and analyzed by a server. Typical processing steps\nperformed at the server include keypoint detection, descriptor calculation, and\nfeature matching. Video compression has been shown to have an adverse effect on\nfeature-matching performance. The negative impact of compression can be reduced\nby using the keypoints extracted from the uncompressed video to calculate\ndescriptors from the compressed video. Based on this observation, we propose to\nprovide these keypoints to the server as side information and to extract only\nthe descriptors from the compressed video. First, we introduce four different\nframe types for keypoint encoding to address different types of changes in\nvideo content. These frame types represent a new scene, the same scene, a\nslowly changing scene, or a rapidly moving scene and are determined by\ncomparing features between successive video frames. Then, we propose Intra,\nSkip and Inter modes of encoding the keypoints for different frame types. For\nexample, keypoints for new scenes are encoded using the Intra mode, and\nkeypoints for unchanged scenes are skipped. As a result, the bitrate of the\nside information related to keypoint encoding is significantly reduced.\nFinally, we present pairwise matching and image retrieval experiments conducted\nto evaluate the performance of the proposed approach using the Stanford mobile\naugmented reality dataset and 720p format videos. The results show that the\nproposed approach offers significantly improved feature matching and image\nretrieval performance at a given bitrate. \n\n"}
{"id": "1507.06105", "contents": "Title: Banzhaf Random Forests Abstract: Random forests are a type of ensemble method which makes predictions by\ncombining the results of several independent trees. However, the theory of\nrandom forests has long been outpaced by their application. In this paper, we\npropose a novel random forests algorithm based on cooperative game theory.\nBanzhaf power index is employed to evaluate the power of each feature by\ntraversing possible feature coalitions. Unlike the previously used information\ngain rate of information theory, which simply chooses the most informative\nfeature, the Banzhaf power index can be considered as a metric of the\nimportance of each feature on the dependency among a group of features. More\nimportantly, we have proved the consistency of the proposed algorithm, named\nBanzhaf random forests (BRF). This theoretical analysis takes a step towards\nnarrowing the gap between the theory and practice of random forests for\nclassification problems. Experiments on several UCI benchmark data sets show\nthat BRF is competitive with state-of-the-art classifiers and dramatically\noutperforms previous consistent random forests. Particularly, it is much more\nefficient than previous consistent random forests. \n\n"}
{"id": "1508.03755", "contents": "Title: Beat-Event Detection in Action Movie Franchises Abstract: While important advances were recently made towards temporally localizing and\nrecognizing specific human actions or activities in videos, efficient detection\nand classification of long video chunks belonging to semantically defined\ncategories such as \"pursuit\" or \"romance\" remains challenging.We introduce a\nnew dataset, Action Movie Franchises, consisting of a collection of Hollywood\naction movie franchises. We define 11 non-exclusive semantic categories -\ncalled beat-categories - that are broad enough to cover most of the movie\nfootage. The corresponding beat-events are annotated as groups of video shots,\npossibly overlapping.We propose an approach for localizing beat-events based on\nclassifying shots into beat-categories and learning the temporal constraints\nbetween shots. We show that temporal constraints significantly improve the\nclassification performance. We set up an evaluation protocol for beat-event\nlocalization as well as for shot classification, depending on whether movies\nfrom the same franchise are present or not in the training data. \n\n"}
{"id": "1508.05056", "contents": "Title: Diving Deep into Sentiment: Understanding Fine-tuned CNNs for Visual\n  Sentiment Prediction Abstract: Visual media are powerful means of expressing emotions and sentiments. The\nconstant generation of new content in social networks highlights the need of\nautomated visual sentiment analysis tools. While Convolutional Neural Networks\n(CNNs) have established a new state-of-the-art in several vision problems,\ntheir application to the task of sentiment analysis is mostly unexplored and\nthere are few studies regarding how to design CNNs for this purpose. In this\nwork, we study the suitability of fine-tuning a CNN for visual sentiment\nprediction as well as explore performance boosting techniques within this deep\nlearning setting. Finally, we provide a deep-dive analysis into a benchmark,\nstate-of-the-art network architecture to gain insight about how to design\npatterns for CNNs on the task of visual sentiment prediction. \n\n"}
{"id": "1508.06073", "contents": "Title: Cooking in the kitchen: Recognizing and Segmenting Human Activities in\n  Videos Abstract: As research on action recognition matures, the focus is shifting away from\ncategorizing basic task-oriented actions using hand-segmented video datasets to\nunderstanding complex goal-oriented daily human activities in real-world\nsettings. Temporally structured models would seem obvious to tackle this set of\nproblems, but so far, cases where these models have outperformed simpler\nunstructured bag-of-word types of models are scarce. With the increasing\navailability of large human activity datasets, combined with the development of\nnovel feature coding techniques that yield more compact representations, it is\ntime to revisit structured generative approaches.\n  Here, we describe an end-to-end generative approach from the encoding of\nfeatures to the structural modeling of complex human activities by applying\nFisher vectors and temporal models for the analysis of video sequences.\n  We systematically evaluate the proposed approach on several available\ndatasets (ADL, MPIICooking, and Breakfast datasets) using a variety of\nperformance metrics. Through extensive system evaluations, we demonstrate that\ncombining compact video representations based on Fisher Vectors with HMM-based\nmodeling yields very significant gains in accuracy and when properly trained\nwith sufficient training samples, structured temporal models outperform\nunstructured bag-of-word types of models by a large margin on the tested\nperformance metric. \n\n"}
{"id": "1508.06535", "contents": "Title: Deep Convolutional Neural Networks for Smile Recognition Abstract: This thesis describes the design and implementation of a smile detector based\non deep convolutional neural networks. It starts with a summary of neural\nnetworks, the difficulties of training them and new training methods, such as\nRestricted Boltzmann Machines or autoencoders. It then provides a literature\nreview of convolutional neural networks and recurrent neural networks. In order\nto select databases for smile recognition, comprehensive statistics of\ndatabases popular in the field of facial expression recognition were generated\nand are summarized in this thesis. It then proposes a model for smile\ndetection, of which the main part is implemented. The experimental results are\ndiscussed in this thesis and justified based on a comprehensive model selection\nperformed. All experiments were run on a Tesla K40c GPU benefiting from a\nspeedup of up to factor 10 over the computations on a CPU. A smile detection\ntest accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous\nFacial Action (DISFA) database, significantly outperforming existing approaches\nwith accuracies ranging from 65.55% to 79.67%. This experiment is re-run under\nvarious variations, such as retaining less neutral images or only the low or\nhigh intensities, of which the results are extensively compared. \n\n"}
{"id": "1509.00568", "contents": "Title: Exploring Online Ad Images Using a Deep Convolutional Neural Network\n  Approach Abstract: Online advertising is a huge, rapidly growing advertising market in today's\nworld. One common form of online advertising is using image ads. A decision is\nmade (often in real time) every time a user sees an ad, and the advertiser is\neager to determine the best ad to display. Consequently, many algorithms have\nbeen developed that calculate the optimal ad to show to the current user at the\npresent time. Typically, these algorithms focus on variations of the ad,\noptimizing among different properties such as background color, image size, or\nset of images. However, there is a more fundamental layer. Our study looks at\nnew qualities of ads that can be determined before an ad is shown (rather than\nonline optimization) and defines which ads are most likely to be successful.\n  We present a set of novel algorithms that utilize deep-learning image\nprocessing, machine learning, and graph theory to investigate online\nadvertising and to construct prediction models which can foresee an image ad's\nsuccess. We evaluated our algorithms on a dataset with over 260,000 ad images,\nas well as a smaller dataset specifically related to the automotive industry,\nand we succeeded in constructing regression models for ad image click rate\nprediction. The obtained results emphasize the great potential of using\ndeep-learning algorithms to effectively and efficiently analyze image ads and\nto create better and more innovative online ads. Moreover, the algorithms\npresented in this paper can help predict ad success and can be applied to\nanalyze other large-scale image corpora. \n\n"}
{"id": "1509.01404", "contents": "Title: Coordinate Descent Methods for Symmetric Nonnegative Matrix\n  Factorization Abstract: Given a symmetric nonnegative matrix $A$, symmetric nonnegative matrix\nfactorization (symNMF) is the problem of finding a nonnegative matrix $H$,\nusually with much fewer columns than $A$, such that $A \\approx HH^T$. SymNMF\ncan be used for data analysis and in particular for various clustering tasks.\nIn this paper, we propose simple and very efficient coordinate descent schemes\nto solve this problem, and that can handle large and sparse input matrices. The\neffectiveness of our methods is illustrated on synthetic and real-world data\nsets, and we show that they perform favorably compared to recent\nstate-of-the-art methods. \n\n"}
{"id": "1509.04232", "contents": "Title: gSLICr: SLIC superpixels at over 250Hz Abstract: We introduce a parallel GPU implementation of the Simple Linear Iterative\nClustering (SLIC) superpixel segmentation. Using a single graphic card, our\nimplementation achieves speedups of up to $83\\times$ from the standard\nsequential implementation. Our implementation is fully compatible with the\nstandard sequential implementation and the software is now available online and\nis open source. \n\n"}
{"id": "1509.04767", "contents": "Title: Zero-Shot Learning via Semantic Similarity Embedding Abstract: In this paper we consider a version of the zero-shot learning problem where\nseen class source and target domain data are provided. The goal during\ntest-time is to accurately predict the class label of an unseen target domain\ninstance based on revealed source domain side information (\\eg attributes) for\nunseen classes. Our method is based on viewing each source or target data as a\nmixture of seen class proportions and we postulate that the mixture patterns\nhave to be similar if the two instances belong to the same unseen class. This\nperspective leads us to learning source/target embedding functions that map an\narbitrary source/target domain data into a same semantic space where similarity\ncan be readily measured. We develop a max-margin framework to learn these\nsimilarity functions and jointly optimize parameters by means of cross\nvalidation. Our test results are compelling, leading to significant improvement\nin terms of accuracy on most benchmark datasets for zero-shot recognition. \n\n"}
{"id": "1509.05909", "contents": "Title: Modelling Uncertainty in Deep Learning for Camera Relocalization Abstract: We present a robust and real-time monocular six degree of freedom visual\nrelocalization system. We use a Bayesian convolutional neural network to\nregress the 6-DOF camera pose from a single RGB image. It is trained in an\nend-to-end manner with no need of additional engineering or graph optimisation.\nThe algorithm can operate indoors and outdoors in real time, taking under 6ms\nto compute. It obtains approximately 2m and 6 degrees accuracy for very large\nscale outdoor scenes and 0.5m and 10 degrees accuracy indoors. Using a Bayesian\nconvolutional neural network implementation we obtain an estimate of the\nmodel's relocalization uncertainty and improve state of the art localization\naccuracy on a large scale outdoor dataset. We leverage the uncertainty measure\nto estimate metric relocalization error and to detect the presence or absence\nof the scene in the input image. We show that the model's uncertainty is caused\nby images being dissimilar to the training dataset in either pose or\nappearance. \n\n"}
{"id": "1509.07845", "contents": "Title: Selecting Relevant Web Trained Concepts for Automated Event Retrieval Abstract: Complex event retrieval is a challenging research problem, especially when no\ntraining videos are available. An alternative to collecting training videos is\nto train a large semantic concept bank a priori. Given a text description of an\nevent, event retrieval is performed by selecting concepts linguistically\nrelated to the event description and fusing the concept responses on unseen\nvideos. However, defining an exhaustive concept lexicon and pre-training it\nrequires vast computational resources. Therefore, recent approaches automate\nconcept discovery and training by leveraging large amounts of weakly annotated\nweb data. Compact visually salient concepts are automatically obtained by the\nuse of concept pairs or, more generally, n-grams. However, not all visually\nsalient n-grams are necessarily useful for an event query--some combinations of\nconcepts may be visually compact but irrelevant--and this drastically affects\nperformance. We propose an event retrieval algorithm that constructs pairs of\nautomatically discovered concepts and then prunes those concepts that are\nunlikely to be helpful for retrieval. Pruning depends both on the query and on\nthe specific video instance being evaluated. Our approach also addresses\ncalibration and domain adaptation issues that arise when applying concept\ndetectors to unseen videos. We demonstrate large improvements over other vision\nbased systems on the TRECVID MED 13 dataset. \n\n"}
{"id": "1510.01064", "contents": "Title: Boosting in the presence of outliers: adaptive classification with\n  non-convex loss functions Abstract: This paper examines the role and efficiency of the non-convex loss functions\nfor binary classification problems. In particular, we investigate how to design\na simple and effective boosting algorithm that is robust to the outliers in the\ndata. The analysis of the role of a particular non-convex loss for prediction\naccuracy varies depending on the diminishing tail properties of the gradient of\nthe loss -- the ability of the loss to efficiently adapt to the outlying data,\nthe local convex properties of the loss and the proportion of the contaminated\ndata. In order to use these properties efficiently, we propose a new family of\nnon-convex losses named $\\gamma$-robust losses. Moreover, we present a new\nboosting framework, {\\it Arch Boost}, designed for augmenting the existing work\nsuch that its corresponding classification algorithm is significantly more\nadaptable to the unknown data contamination. Along with the Arch Boosting\nframework, the non-convex losses lead to the new class of boosting algorithms,\nnamed adaptive, robust, boosting (ARB). Furthermore, we present theoretical\nexamples that demonstrate the robustness properties of the proposed algorithms.\nIn particular, we develop a new breakdown point analysis and a new influence\nfunction analysis that demonstrate gains in robustness. Moreover, we present\nnew theoretical results, based only on local curvatures, which may be used to\nestablish statistical and optimization properties of the proposed Arch boosting\nalgorithms with highly non-convex loss functions. Extensive numerical\ncalculations are used to illustrate these theoretical properties and reveal\nadvantages over the existing boosting methods when data exhibits a number of\noutliers. \n\n"}
{"id": "1510.04389", "contents": "Title: Sketch-based Manga Retrieval using Manga109 Dataset Abstract: Manga (Japanese comics) are popular worldwide. However, current e-manga\narchives offer very limited search support, including keyword-based search by\ntitle or author, or tag-based categorization. To make the manga search\nexperience more intuitive, efficient, and enjoyable, we propose a content-based\nmanga retrieval system. First, we propose a manga-specific image-describing\nframework. It consists of efficient margin labeling, edge orientation histogram\nfeature description, and approximate nearest-neighbor search using product\nquantization. Second, we propose a sketch-based interface as a natural way to\ninteract with manga content. The interface provides sketch-based querying,\nrelevance feedback, and query retouch. For evaluation, we built a novel dataset\nof manga images, Manga109, which consists of 109 comic books of 21,142 pages\ndrawn by professional manga artists. To the best of our knowledge, Manga109 is\ncurrently the biggest dataset of manga images available for research. We\nconducted a comparative study, a localization evaluation, and a large-scale\nqualitative study. From the experiments, we verified that: (1) the retrieval\naccuracy of the proposed method is higher than those of previous methods; (2)\nthe proposed method can localize an object instance with reasonable runtime and\naccuracy; and (3) sketch querying is useful for manga search. \n\n"}
{"id": "1510.04706", "contents": "Title: Shape Complexes in Continuous Max-Flow Hierarchical Multi-Labeling\n  Problems Abstract: Although topological considerations amongst multiple labels have been\npreviously investigated in the context of continuous max-flow image\nsegmentation, similar investigations have yet to be made about shape\nconsiderations in a general and extendable manner. This paper presents shape\ncomplexes for segmentation, which capture more complex shapes by combining\nmultiple labels and super-labels constrained by geodesic star convexity. Shape\ncomplexes combine geodesic star convexity constraints with hierarchical label\norganization, which together allow for more complex shapes to be represented.\nThis framework avoids the use of co-ordinate system warping techniques to\nconvert shape constraints into topological constraints, which may be ambiguous\nor ill-defined for certain segmentation problems. \n\n"}
{"id": "1511.01865", "contents": "Title: Convolutional Neural Network for Stereotypical Motor Movement Detection\n  in Autism Abstract: Autism Spectrum Disorders (ASDs) are often associated with specific atypical\npostural or motor behaviors, of which Stereotypical Motor Movements (SMMs) have\na specific visibility. While the identification and the quantification of SMM\npatterns remain complex, its automation would provide support to accurate\ntuning of the intervention in the therapy of autism. Therefore, it is essential\nto develop automatic SMM detection systems in a real world setting, taking care\nof strong inter-subject and intra-subject variability. Wireless accelerometer\nsensing technology can provide a valid infrastructure for real-time SMM\ndetection, however such variability remains a problem also for machine learning\nmethods, in particular whenever handcrafted features extracted from\naccelerometer signal are considered. Here, we propose to employ the deep\nlearning paradigm in order to learn discriminating features from multi-sensor\naccelerometer signals. Our results provide preliminary evidence that feature\nlearning and transfer learning embedded in the deep architecture achieve higher\naccurate SMM detectors in longitudinal scenarios. \n\n"}
{"id": "1511.03328", "contents": "Title: Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs\n  and a Discriminatively Trained Domain Transform Abstract: Deep convolutional neural networks (CNNs) are the backbone of state-of-art\nsemantic image segmentation systems. Recent work has shown that complementing\nCNNs with fully-connected conditional random fields (CRFs) can significantly\nenhance their object localization accuracy, yet dense CRF inference is\ncomputationally expensive. We propose replacing the fully-connected CRF with\ndomain transform (DT), a modern edge-preserving filtering method in which the\namount of smoothing is controlled by a reference edge map. Domain transform\nfiltering is several times faster than dense CRF inference and we show that it\nyields comparable semantic segmentation results, accurately capturing object\nboundaries. Importantly, our formulation allows learning the reference edge map\nfrom intermediate CNN features instead of using the image gradient magnitude as\nin standard DT filtering. This produces task-specific edges in an end-to-end\ntrainable system optimizing the target semantic segmentation quality. \n\n"}
{"id": "1511.04192", "contents": "Title: DISC: Deep Image Saliency Computing via Progressive Representation\n  Learning Abstract: Salient object detection increasingly receives attention as an important\ncomponent or step in several pattern recognition and image processing tasks.\nAlthough a variety of powerful saliency models have been intensively proposed,\nthey usually involve heavy feature (or model) engineering based on priors (or\nassumptions) about the properties of objects and backgrounds. Inspired by the\neffectiveness of recently developed feature learning, we provide a novel Deep\nImage Saliency Computing (DISC) framework for fine-grained image saliency\ncomputing. In particular, we model the image saliency from both the coarse- and\nfine-level observations, and utilize the deep convolutional neural network\n(CNN) to learn the saliency representation in a progressive manner.\nSpecifically, our saliency model is built upon two stacked CNNs. The first CNN\ngenerates a coarse-level saliency map by taking the overall image as the input,\nroughly identifying saliency regions in the global context. Furthermore, we\nintegrate superpixel-based local context information in the first CNN to refine\nthe coarse-level saliency map. Guided by the coarse saliency map, the second\nCNN focuses on the local context to produce fine-grained and accurate saliency\nmap while preserving object details. For a testing image, the two CNNs\ncollaboratively conduct the saliency computing in one shot. Our DISC framework\nis capable of uniformly highlighting the objects-of-interest from complex\nbackground while preserving well object details. Extensive experiments on\nseveral standard benchmarks suggest that DISC outperforms other\nstate-of-the-art methods and it also generalizes well across datasets without\nadditional training. The executable version of DISC is available online:\nhttp://vision.sysu.edu.cn/projects/DISC. \n\n"}
{"id": "1511.04404", "contents": "Title: Robust Face Alignment Using a Mixture of Invariant Experts Abstract: Face alignment, which is the task of finding the locations of a set of facial\nlandmark points in an image of a face, is useful in widespread application\nareas. Face alignment is particularly challenging when there are large\nvariations in pose (in-plane and out-of-plane rotations) and facial expression.\nTo address this issue, we propose a cascade in which each stage consists of a\nmixture of regression experts. Each expert learns a customized regression model\nthat is specialized to a different subset of the joint space of pose and\nexpressions. The system is invariant to a predefined class of transformations\n(e.g., affine), because the input is transformed to match each expert's\nprototype shape before the regression is applied. We also present a method to\ninclude deformation constraints within the discriminative alignment framework,\nwhich makes our algorithm more robust. Our algorithm significantly outperforms\nprevious methods on publicly available face alignment datasets. \n\n"}
{"id": "1511.04534", "contents": "Title: Learning Fine-grained Features via a CNN Tree for Large-scale\n  Classification Abstract: We propose a novel approach to enhance the discriminability of Convolutional\nNeural Networks (CNN). The key idea is to build a tree structure that could\nprogressively learn fine-grained features to distinguish a subset of classes,\nby learning features only among these classes. Such features are expected to be\nmore discriminative, compared to features learned for all the classes. We\ndevelop a new algorithm to effectively learn the tree structure from a large\nnumber of classes. Experiments on large-scale image classification tasks\ndemonstrate that our method could boost the performance of a given basic CNN\nmodel. Our method is quite general, hence it can potentially be used in\ncombination with many other deep learning models. \n\n"}
{"id": "1511.05261", "contents": "Title: Robust PCA via Nonconvex Rank Approximation Abstract: Numerous applications in data mining and machine learning require recovering\na matrix of minimal rank. Robust principal component analysis (RPCA) is a\ngeneral framework for handling this kind of problems. Nuclear norm based convex\nsurrogate of the rank function in RPCA is widely investigated. Under certain\nassumptions, it can recover the underlying true low rank matrix with high\nprobability. However, those assumptions may not hold in real-world\napplications. Since the nuclear norm approximates the rank by adding all\nsingular values together, which is essentially a $\\ell_1$-norm of the singular\nvalues, the resulting approximation error is not trivial and thus the resulting\nmatrix estimator can be significantly biased. To seek a closer approximation\nand to alleviate the above-mentioned limitations of the nuclear norm, we\npropose a nonconvex rank approximation. This approximation to the matrix rank\nis tighter than the nuclear norm. To solve the associated nonconvex\nminimization problem, we develop an efficient augmented Lagrange multiplier\nbased optimization algorithm. Experimental results demonstrate that our method\noutperforms current state-of-the-art algorithms in both accuracy and\nefficiency. \n\n"}
{"id": "1511.05952", "contents": "Title: Prioritized Experience Replay Abstract: Experience replay lets online reinforcement learning agents remember and\nreuse experiences from the past. In prior work, experience transitions were\nuniformly sampled from a replay memory. However, this approach simply replays\ntransitions at the same frequency that they were originally experienced,\nregardless of their significance. In this paper we develop a framework for\nprioritizing experience, so as to replay important transitions more frequently,\nand therefore learn more efficiently. We use prioritized experience replay in\nDeep Q-Networks (DQN), a reinforcement learning algorithm that achieved\nhuman-level performance across many Atari games. DQN with prioritized\nexperience replay achieves a new state-of-the-art, outperforming DQN with\nuniform replay on 41 out of 49 games. \n\n"}
{"id": "1511.06196", "contents": "Title: Importance Sampling: Intrinsic Dimension and Computational Cost Abstract: The basic idea of importance sampling is to use independent samples from a\nproposal measure in order to approximate expectations with respect to a target\nmeasure. It is key to understand how many samples are required in order to\nguarantee accurate approximations. Intuitively, some notion of distance between\nthe target and the proposal should determine the computational cost of the\nmethod. A major challenge is to quantify this distance in terms of parameters\nor statistics that are pertinent for the practitioner. The subject has\nattracted substantial interest from within a variety of communities. The\nobjective of this paper is to overview and unify the resulting literature by\ncreating an overarching framework. A general theory is presented, with a focus\non the use of importance sampling in Bayesian inverse problems and filtering. \n\n"}
{"id": "1511.06348", "contents": "Title: How much data is needed to train a medical image deep learning system to\n  achieve necessary high accuracy? Abstract: The use of Convolutional Neural Networks (CNN) in natural image\nclassification systems has produced very impressive results. Combined with the\ninherent nature of medical images that make them ideal for deep-learning,\nfurther application of such systems to medical image classification holds much\npromise. However, the usefulness and potential impact of such a system can be\ncompletely negated if it does not reach a target accuracy. In this paper, we\npresent a study on determining the optimum size of the training data set\nnecessary to achieve high classification accuracy with low variance in medical\nimage classification systems. The CNN was applied to classify axial Computed\nTomography (CT) images into six anatomical classes. We trained the CNN using\nsix different sizes of training data set (5, 10, 20, 50, 100, and 200) and then\ntested the resulting system with a total of 6000 CT images. All images were\nacquired from the Massachusetts General Hospital (MGH) Picture Archiving and\nCommunication System (PACS). Using this data, we employ the learning curve\napproach to predict classification accuracy at a given training sample size.\nOur research will present a general methodology for determining the training\ndata set size necessary to achieve a certain target classification accuracy\nthat can be easily applied to other problems within such systems. \n\n"}
{"id": "1511.06434", "contents": "Title: Unsupervised Representation Learning with Deep Convolutional Generative\n  Adversarial Networks Abstract: In recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and\nunsupervised learning. We introduce a class of CNNs called deep convolutional\ngenerative adversarial networks (DCGANs), that have certain architectural\nconstraints, and demonstrate that they are a strong candidate for unsupervised\nlearning. Training on various image datasets, we show convincing evidence that\nour deep convolutional adversarial pair learns a hierarchy of representations\nfrom object parts to scenes in both the generator and discriminator.\nAdditionally, we use the learned features for novel tasks - demonstrating their\napplicability as general image representations. \n\n"}
{"id": "1511.06442", "contents": "Title: Fast Metric Learning For Deep Neural Networks Abstract: Similarity metrics are a core component of many information retrieval and\nmachine learning systems. In this work we propose a method capable of learning\na similarity metric from data equipped with a binary relation. By considering\nonly the similarity constraints, and initially ignoring the features, we are\nable to learn target vectors for each instance using one of several\nappropriately designed loss functions. A regression model can then be\nconstructed that maps novel feature vectors to the same target vector space,\nresulting in a feature extractor that computes vectors for which a predefined\nmetric is a meaningful measure of similarity. We present results on both\nmulticlass and multi-label classification datasets that demonstrate\nconsiderably faster convergence, as well as higher accuracy on the majority of\nthe intrinsic evaluation tasks and all extrinsic evaluation tasks. \n\n"}
{"id": "1511.06522", "contents": "Title: Integrating Deep Features for Material Recognition Abstract: We propose a method for integration of features extracted using deep\nrepresentations of Convolutional Neural Networks (CNNs) each of which is\nlearned using a different image dataset of objects and materials for material\nrecognition. Given a set of representations of multiple pre-trained CNNs, we\nfirst compute activations of features using the representations on the images\nto select a set of samples which are best represented by the features. Then, we\nmeasure the uncertainty of the features by computing the entropy of class\ndistributions for each sample set. Finally, we compute the contribution of each\nfeature to representation of classes for feature selection and integration. We\nexamine the proposed method on three benchmark datasets for material\nrecognition. Experimental results show that the proposed method achieves\nstate-of-the-art performance by integrating deep features. Additionally, we\nintroduce a new material dataset called EFMD by extending Flickr Material\nDatabase (FMD). By the employment of the EFMD with transfer learning for\nupdating the learned CNN models, we achieve 84.0%+/-1.8% accuracy on the FMD\ndataset which is close to human performance that is 84.9%. \n\n"}
{"id": "1511.06586", "contents": "Title: Crowd Behavior Analysis: A Review where Physics meets Biology Abstract: Although the traits emerged in a mass gathering are often non-deliberative,\nthe act of mass impulse may lead to irre- vocable crowd disasters. The two-fold\nincrease of carnage in crowd since the past two decades has spurred significant\nadvances in the field of computer vision, towards effective and proactive crowd\nsurveillance. Computer vision stud- ies related to crowd are observed to\nresonate with the understanding of the emergent behavior in physics (complex\nsystems) and biology (animal swarm). These studies, which are inspired by\nbiology and physics, share surprisingly common insights, and interesting\ncontradictions. However, this aspect of discussion has not been fully explored.\nTherefore, this survey provides the readers with a review of the\nstate-of-the-art methods in crowd behavior analysis from the physics and\nbiologically inspired perspectives. We provide insights and comprehensive\ndiscussions for a broader understanding of the underlying prospect of blending\nphysics and biology studies in computer vision. \n\n"}
{"id": "1511.06811", "contents": "Title: Learning visual groups from co-occurrences in space and time Abstract: We propose a self-supervised framework that learns to group visual entities\nbased on their rate of co-occurrence in space and time. To model statistical\ndependencies between the entities, we set up a simple binary classification\nproblem in which the goal is to predict if two visual primitives occur in the\nsame spatial or temporal context. We apply this framework to three domains:\nlearning patch affinities from spatial adjacency in images, learning frame\naffinities from temporal adjacency in videos, and learning photo affinities\nfrom geospatial proximity in image collections. We demonstrate that in each\ncase the learned affinities uncover meaningful semantic groupings. From patch\naffinities we generate object proposals that are competitive with\nstate-of-the-art supervised methods. From frame affinities we generate movie\nscene segmentations that correlate well with DVD chapter structure. Finally,\nfrom geospatial affinities we learn groups that relate well to semantic place\ncategories. \n\n"}
{"id": "1511.06919", "contents": "Title: Semantic Segmentation of Colon Glands with Deep Convolutional Neural\n  Networks and Total Variation Segmentation Abstract: Segmentation of histopathology sections is an ubiquitous requirement in\ndigital pathology and due to the large variability of biological tissue,\nmachine learning techniques have shown superior performance over standard image\nprocessing methods. As part of the GlaS@MICCAI2015 colon gland segmentation\nchallenge, we present a learning-based algorithm to segment glands in tissue of\nbenign and malignant colorectal cancer. Images are preprocessed according to\nthe Hematoxylin-Eosin staining protocol and two deep convolutional neural\nnetworks (CNN) are trained as pixel classifiers. The CNN predictions are then\nregularized using a figure-ground segmentation based on weighted total\nvariation to produce the final segmentation result. On two test sets, our\napproach achieves a tissue classification accuracy of 98% and 94%, making use\nof the inherent capability of our system to distinguish between benign and\nmalignant tissue. \n\n"}
{"id": "1511.07247", "contents": "Title: NetVLAD: CNN architecture for weakly supervised place recognition Abstract: We tackle the problem of large scale visual place recognition, where the task\nis to quickly and accurately recognize the location of a given query\nphotograph. We present the following three principal contributions. First, we\ndevelop a convolutional neural network (CNN) architecture that is trainable in\nan end-to-end manner directly for the place recognition task. The main\ncomponent of this architecture, NetVLAD, is a new generalized VLAD layer,\ninspired by the \"Vector of Locally Aggregated Descriptors\" image representation\ncommonly used in image retrieval. The layer is readily pluggable into any CNN\narchitecture and amenable to training via backpropagation. Second, we develop a\ntraining procedure, based on a new weakly supervised ranking loss, to learn\nparameters of the architecture in an end-to-end manner from images depicting\nthe same places over time downloaded from Google Street View Time Machine.\nFinally, we show that the proposed architecture significantly outperforms\nnon-learnt image representations and off-the-shelf CNN descriptors on two\nchallenging place recognition benchmarks, and improves over current\nstate-of-the-art compact image representations on standard image retrieval\nbenchmarks. \n\n"}
{"id": "1511.07386", "contents": "Title: Pushing the Boundaries of Boundary Detection using Deep Learning Abstract: In this work we show that adapting Deep Convolutional Neural Network training\nto the task of boundary detection can result in substantial improvements over\nthe current state-of-the-art in boundary detection.\n  Our contributions consist firstly in combining a careful design of the loss\nfor boundary detection training, a multi-resolution architecture and training\nwith external data to improve the detection accuracy of the current state of\nthe art. When measured on the standard Berkeley Segmentation Dataset, we\nimprove theoptimal dataset scale F-measure from 0.780 to 0.808 - while human\nperformance is at 0.803. We further improve performance to 0.813 by combining\ndeep learning with grouping, integrating the Normalized Cuts technique within a\ndeep network.\n  We also examine the potential of our boundary detector in conjunction with\nthe task of semantic segmentation and demonstrate clear improvements over\nstate-of-the-art systems. Our detector is fully integrated in the popular Caffe\nframework and processes a 320x420 image in less than a second. \n\n"}
{"id": "1511.07401", "contents": "Title: MazeBase: A Sandbox for Learning from Games Abstract: This paper introduces MazeBase: an environment for simple 2D games, designed\nas a sandbox for machine learning approaches to reasoning and planning. Within\nit, we create 10 simple games embodying a range of algorithmic tasks (e.g.\nif-then statements or set negation). A variety of neural models (fully\nconnected, convolutional network, memory network) are deployed via\nreinforcement learning on these games, with and without a procedurally\ngenerated curriculum. Despite the tasks' simplicity, the performance of the\nmodels is far from optimal, suggesting directions for future development. We\nalso demonstrate the versatility of MazeBase by using it to emulate small\ncombat scenarios from StarCraft. Models trained on the MazeBase version can be\ndirectly applied to StarCraft, where they consistently beat the in-game AI. \n\n"}
{"id": "1511.07409", "contents": "Title: Top-Down Learning for Structured Labeling with Convolutional Pseudoprior Abstract: Current practice in convolutional neural networks (CNN) remains largely\nbottom-up and the role of top-down process in CNN for pattern analysis and\nvisual inference is not very clear. In this paper, we propose a new method for\nstructured labeling by developing convolutional pseudo-prior (ConvPP) on the\nground-truth labels. Our method has several interesting properties: (1)\ncompared with classical machine learning algorithms like CRFs and Structural\nSVM, ConvPP automatically learns rich convolutional kernels to capture both\nshort- and long- range contexts; (2) compared with cascade classifiers like\nAuto-Context, ConvPP avoids the iterative steps of learning a series of\ndiscriminative classifiers and automatically learns contextual configurations;\n(3) compared with recent efforts combing CNN models with CRFs and RNNs, ConvPP\nlearns convolution in the labeling space with much improved modeling capability\nand less manual specification; (4) compared with Bayesian models like MRFs,\nConvPP capitalizes on the rich representation power of convolution by\nautomatically learning priors built on convolutional filters. We accomplish our\ntask using pseudo-likelihood approximation to the prior under a novel\nfixed-point network structure that facilitates an end-to-end learning process.\nWe show state-of-the-art results on sequential labeling and image labeling\nbenchmarks. \n\n"}
{"id": "1511.08779", "contents": "Title: Multiagent Cooperation and Competition with Deep Reinforcement Learning Abstract: Multiagent systems appear in most social, economical, and political\nsituations. In the present work we extend the Deep Q-Learning Network\narchitecture proposed by Google DeepMind to multiagent environments and\ninvestigate how two agents controlled by independent Deep Q-Networks interact\nin the classic videogame Pong. By manipulating the classical rewarding scheme\nof Pong we demonstrate how competitive and collaborative behaviors emerge.\nCompetitive agents learn to play and score efficiently. Agents trained under\ncollaborative rewarding schemes find an optimal strategy to keep the ball in\nthe game as long as possible. We also describe the progression from competitive\nto collaborative behavior. The present work demonstrates that Deep Q-Networks\ncan become a practical tool for studying the decentralized learning of\nmultiagent systems living in highly complex environments. \n\n"}
{"id": "1512.00486", "contents": "Title: Loss Functions for Top-k Error: Analysis and Insights Abstract: In order to push the performance on realistic computer vision tasks, the\nnumber of classes in modern benchmark datasets has significantly increased in\nrecent years. This increase in the number of classes comes along with increased\nambiguity between the class labels, raising the question if top-1 error is the\nright performance measure. In this paper, we provide an extensive comparison\nand evaluation of established multiclass methods comparing their top-k\nperformance both from a practical as well as from a theoretical perspective.\nMoreover, we introduce novel top-k loss functions as modifications of the\nsoftmax and the multiclass SVM losses and provide efficient optimization\nschemes for them. In the experiments, we compare on various datasets all of the\nproposed and established methods for top-k error optimization. An interesting\ninsight of this paper is that the softmax loss yields competitive top-k\nperformance for all k simultaneously. For a specific top-k error, our new top-k\nlosses lead typically to further improvements while being faster to train than\nthe softmax. \n\n"}
{"id": "1512.03385", "contents": "Title: Deep Residual Learning for Image Recognition Abstract: Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation. \n\n"}
{"id": "1512.04605", "contents": "Title: Semantic-enriched Visual Vocabulary Construction in a Weakly Supervised\n  Context Abstract: One of the prevalent learning tasks involving images is content-based image\nclassification. This is a difficult task especially because the low-level\nfeatures used to digitally describe images usually capture little information\nabout the semantics of the images. In this paper, we tackle this difficulty by\nenriching the semantic content of the image representation by using external\nknowledge. The underlying hypothesis of our work is that creating a more\nsemantically rich representation for images would yield higher machine learning\nperformances, without the need to modify the learning algorithms themselves.\nThe external semantic information is presented under the form of non-positional\nimage labels, therefore positioning our work in a weakly supervised context.\nTwo approaches are proposed: the first one leverages the labels into the visual\nvocabulary construction algorithm, the result being dedicated visual\nvocabularies. The second approach adds a filtering phase as a pre-processing of\nthe vocabulary construction. Known positive and known negative sets are\nconstructed and features that are unlikely to be associated with the objects\ndenoted by the labels are filtered. We apply our proposition to the task of\ncontent-based image classification and we show that semantically enriching the\nimage representation yields higher classification performances than the\nbaseline representation. \n\n"}
{"id": "1512.04785", "contents": "Title: On Deep Representation Learning from Noisy Web Images Abstract: The keep-growing content of Web images may be the next important data source\nto scale up deep neural networks, which recently obtained a great success in\nthe ImageNet classification challenge and related tasks. This prospect,\nhowever, has not been validated on convolutional networks (convnet) -- one of\nbest performing deep models -- because of their supervised regime. While\nunsupervised alternatives are not so good as convnet in generalizing the\nlearned model to new domains, we use convnet to leverage semi-supervised\nrepresentation learning. Our approach is to use massive amounts of unlabeled\nand noisy Web images to train convnets as general feature detectors despite\nchallenges coming from data such as high level of mislabeled data, outliers,\nand data biases. Extensive experiments are conducted at several data scales,\ndifferent network architectures, and data reranking techniques. The learned\nrepresentations are evaluated on nine public datasets of various topics. The\nbest results obtained by our convnets, trained on 3.14 million Web images,\noutperform AlexNet trained on 1.2 million clean images of ILSVRC 2012 and is\nclosing the gap with VGG-16. These prominent results suggest a budget solution\nto use deep learning in practice and motivate more research in semi-supervised\nrepresentation learning. \n\n"}
{"id": "1512.08086", "contents": "Title: Part-Stacked CNN for Fine-Grained Visual Categorization Abstract: In the context of fine-grained visual categorization, the ability to\ninterpret models as human-understandable visual manuals is sometimes as\nimportant as achieving high classification accuracy. In this paper, we propose\na novel Part-Stacked CNN architecture that explicitly explains the fine-grained\nrecognition process by modeling subtle differences from object parts. Based on\nmanually-labeled strong part annotations, the proposed architecture consists of\na fully convolutional network to locate multiple object parts and a two-stream\nclassification network that en- codes object-level and part-level cues\nsimultaneously. By adopting a set of sharing strategies between the computation\nof multiple object parts, the proposed architecture is very efficient running\nat 20 frames/sec during inference. Experimental results on the CUB-200-2011\ndataset reveal the effectiveness of the proposed architecture, from both the\nperspective of classification accuracy and model interpretability. \n\n"}
{"id": "1601.04568", "contents": "Title: Content Aware Neural Style Transfer Abstract: This paper presents a content-aware style transfer algorithm for paintings\nand photos of similar content using pre-trained neural network, obtaining\nbetter results than the previous work. In addition, the numerical experiments\nshow that the style pattern and the content information is not completely\nseparated by neural network. \n\n"}
{"id": "1601.06032", "contents": "Title: Learning Support Correlation Filters for Visual Tracking Abstract: Sampling and budgeting training examples are two essential factors in\ntracking algorithms based on support vector machines (SVMs) as a trade-off\nbetween accuracy and efficiency. Recently, the circulant matrix formed by dense\nsampling of translated image patches has been utilized in correlation filters\nfor fast tracking. In this paper, we derive an equivalent formulation of a SVM\nmodel with circulant matrix expression and present an efficient alternating\noptimization method for visual tracking. We incorporate the discrete Fourier\ntransform with the proposed alternating optimization process, and pose the\ntracking problem as an iterative learning of support correlation filters (SCFs)\nwhich find the global optimal solution with real-time performance. For a given\ncirculant data matrix with n^2 samples of size n*n, the computational\ncomplexity of the proposed algorithm is O(n^2*logn) whereas that of the\nstandard SVM-based approaches is at least O(n^4). In addition, we extend the\nSCF-based tracking algorithm with multi-channel features, kernel functions, and\nscale-adaptive approaches to further improve the tracking performance.\nExperimental results on a large benchmark dataset show that the proposed\nSCF-based algorithms perform favorably against the state-of-the-art tracking\nmethods in terms of accuracy and speed. \n\n"}
{"id": "1602.00224", "contents": "Title: Order-aware Convolutional Pooling for Video Based Action Recognition Abstract: Most video based action recognition approaches create the video-level\nrepresentation by temporally pooling the features extracted at each frame. The\npooling methods that they adopt, however, usually completely or partially\nneglect the dynamic information contained in the temporal domain, which may\nundermine the discriminative power of the resulting video representation since\nthe video sequence order could unveil the evolution of a specific event or\naction. To overcome this drawback and explore the importance of incorporating\nthe temporal order information, in this paper we propose a novel temporal\npooling approach to aggregate the frame-level features. Inspired by the\ncapacity of Convolutional Neural Networks (CNN) in making use of the internal\nstructure of images for information abstraction, we propose to apply the\ntemporal convolution operation to the frame-level representations to extract\nthe dynamic information. However, directly implementing this idea on the\noriginal high-dimensional feature would inevitably result in parameter\nexplosion.\n  To tackle this problem, we view the temporal evolution of the feature value\nat each feature dimension as a 1D signal and learn a unique convolutional\nfilter bank for each of these 1D signals. We conduct experiments on two\nchallenging video-based action recognition datasets, HMDB51 and UCF101; and\ndemonstrate that the proposed method is superior to the conventional pooling\nmethods. \n\n"}
{"id": "1602.00328", "contents": "Title: Novel Views of Objects from a Single Image Abstract: Taking an image of an object is at its core a lossy process. The rich\ninformation about the three-dimensional structure of the world is flattened to\nan image plane and decisions such as viewpoint and camera parameters are final\nand not easily revertible. As a consequence, possibilities of changing\nviewpoint are limited. Given a single image depicting an object, novel-view\nsynthesis is the task of generating new images that render the object from a\ndifferent viewpoint than the one given. The main difficulty is to synthesize\nthe parts that are disoccluded; disocclusion occurs when parts of an object are\nhidden by the object itself under a specific viewpoint. In this work, we show\nhow to improve novel-view synthesis by making use of the correlations observed\nin 3D models and applying them to new image instances. We propose a technique\nto use the structural information extracted from a 3D model that matches the\nimage object in terms of viewpoint and shape. For the latter part, we propose\nan efficient 2D-to-3D alignment method that associates precisely the image\nappearance with the 3D model geometry with minimal user interaction. Our\ntechnique is able to simulate plausible viewpoint changes for a variety of\nobject classes within seconds. Additionally, we show that our synthesized\nimages can be used as additional training data that improves the performance of\nstandard object detectors. \n\n"}
{"id": "1602.06632", "contents": "Title: Denoising and Covariance Estimation of Single Particle Cryo-EM Images Abstract: The problem of image restoration in cryo-EM entails correcting for the\neffects of the Contrast Transfer Function (CTF) and noise. Popular methods for\nimage restoration include `phase flipping', which corrects only for the Fourier\nphases but not amplitudes, and Wiener filtering, which requires the spectral\nsignal to noise ratio. We propose a new image restoration method which we call\n`Covariance Wiener Filtering' (CWF). In CWF, the covariance matrix of the\nprojection images is used within the classical Wiener filtering framework for\nsolving the image restoration deconvolution problem. Our estimation procedure\nfor the covariance matrix is new and successfully corrects for the CTF. We\ndemonstrate the efficacy of CWF by applying it to restore both simulated and\nexperimental cryo-EM images. Results with experimental datasets demonstrate\nthat CWF provides a good way to evaluate the particle images and to see what\nthe dataset contains even without 2D classification and averaging. \n\n"}
{"id": "1602.07373", "contents": "Title: On Study of the Binarized Deep Neural Network for Image Classification Abstract: Recently, the deep neural network (derived from the artificial neural\nnetwork) has attracted many researchers' attention by its outstanding\nperformance. However, since this network requires high-performance GPUs and\nlarge storage, it is very hard to use it on individual devices. In order to\nimprove the deep neural network, many trials have been made by refining the\nnetwork structure or training strategy. Unlike those trials, in this paper, we\nfocused on the basic propagation function of the artificial neural network and\nproposed the binarized deep neural network. This network is a pure binary\nsystem, in which all the values and calculations are binarized. As a result,\nour network can save a lot of computational resource and storage. Therefore, it\nis possible to use it on various devices. Moreover, the experimental results\nproved the feasibility of the proposed network. \n\n"}
{"id": "1602.08671", "contents": "Title: Lie Access Neural Turing Machine Abstract: Following the recent trend in explicit neural memory structures, we present a\nnew design of an external memory, wherein memories are stored in an Euclidean\nkey space $\\mathbb R^n$. An LSTM controller performs read and write via\nspecialized read and write heads. It can move a head by either providing a new\naddress in the key space (aka random access) or moving from its previous\nposition via a Lie group action (aka Lie access). In this way, the \"L\" and \"R\"\ninstructions of a traditional Turing Machine are generalized to arbitrary\nelements of a fixed Lie group action. For this reason, we name this new model\nthe Lie Access Neural Turing Machine, or LANTM.\n  We tested two different configurations of LANTM against an LSTM baseline in\nseveral basic experiments. We found the right configuration of LANTM to\noutperform the baseline in all of our experiments. In particular, we trained\nLANTM on addition of $k$-digit numbers for $2 \\le k \\le 16$, but it was able to\ngeneralize almost perfectly to $17 \\le k \\le 32$, all with the number of\nparameters 2 orders of magnitude below the LSTM baseline. \n\n"}
{"id": "1603.00788", "contents": "Title: Automatic Differentiation Variational Inference Abstract: Probabilistic modeling is iterative. A scientist posits a simple model, fits\nit to her data, refines it according to her analysis, and repeats. However,\nfitting complex models to large data is a bottleneck in this process. Deriving\nalgorithms for new models can be both mathematically and computationally\nchallenging, which makes it difficult to efficiently cycle through the steps.\nTo this end, we develop automatic differentiation variational inference (ADVI).\nUsing our method, the scientist only provides a probabilistic model and a\ndataset, nothing else. ADVI automatically derives an efficient variational\ninference algorithm, freeing the scientist to refine and explore many models.\nADVI supports a broad class of models-no conjugacy assumptions are required. We\nstudy ADVI across ten different models and apply it to a dataset with millions\nof observations. ADVI is integrated into Stan, a probabilistic programming\nsystem; it is available for immediate use. \n\n"}
{"id": "1603.01976", "contents": "Title: Deep Contrast Learning for Salient Object Detection Abstract: Salient object detection has recently witnessed substantial progress due to\npowerful features extracted using deep convolutional neural networks (CNNs).\nHowever, existing CNN-based methods operate at the patch level instead of the\npixel level. Resulting saliency maps are typically blurry, especially near the\nboundary of salient objects. Furthermore, image patches are treated as\nindependent samples even when they are overlapping, giving rise to significant\nredundancy in computation and storage. In this CVPR 2016 paper, we propose an\nend-to-end deep contrast network to overcome the aforementioned limitations.\nOur deep network consists of two complementary components, a pixel-level fully\nconvolutional stream and a segment-wise spatial pooling stream. The first\nstream directly produces a saliency map with pixel-level accuracy from an input\nimage. The second stream extracts segment-wise features very efficiently, and\nbetter models saliency discontinuities along object boundaries. Finally, a\nfully connected CRF model can be optionally incorporated to improve spatial\ncoherence and contour localization in the fused result from these two streams.\nExperimental results demonstrate that our deep model significantly improves the\nstate of the art. \n\n"}
{"id": "1603.02649", "contents": "Title: A regularization-based approach for unsupervised image segmentation Abstract: We propose a novel unsupervised image segmentation algorithm, which aims to\nsegment an image into several coherent parts. It requires no user input, no\nsupervised learning phase and assumes an unknown number of segments. It\nachieves this by first over-segmenting the image into several hundred\nsuperpixels. These are iteratively joined on the basis of a discriminative\nclassifier trained on color and texture information obtained from each\nsuperpixel. The output of the classifier is regularized by a Markov random\nfield that lends more influence to neighbouring superpixels that are more\nsimilar. In each iteration, similar superpixels fall under the same label,\nuntil only a few coherent regions remain in the image. The algorithm was tested\non a standard evaluation data set, where it performs on par with\nstate-of-the-art algorithms in term of precision and greatly outperforms the\nstate of the art by reducing the oversegmentation of the object of interest. \n\n"}
{"id": "1603.04118", "contents": "Title: Active Algorithms For Preference Learning Problems with Multiple\n  Populations Abstract: In this paper we model the problem of learning preferences of a population as\nan active learning problem. We propose an algorithm can adaptively choose pairs\nof items to show to users coming from a heterogeneous population, and use the\nobtained reward to decide which pair of items to show next. We provide\ncomputationally efficient algorithms with provable sample complexity guarantees\nfor this problem in both the noiseless and noisy cases. In the process of\nestablishing sample complexity guarantees for our algorithms, we establish new\nresults using a Nystr{\\\"o}m-like method which can be of independent interest.\nWe supplement our theoretical results with experimental comparisons. \n\n"}
{"id": "1603.04467", "contents": "Title: TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed\n  Systems Abstract: TensorFlow is an interface for expressing machine learning algorithms, and an\nimplementation for executing such algorithms. A computation expressed using\nTensorFlow can be executed with little or no change on a wide variety of\nheterogeneous systems, ranging from mobile devices such as phones and tablets\nup to large-scale distributed systems of hundreds of machines and thousands of\ncomputational devices such as GPU cards. The system is flexible and can be used\nto express a wide variety of algorithms, including training and inference\nalgorithms for deep neural network models, and it has been used for conducting\nresearch and for deploying machine learning systems into production across more\nthan a dozen areas of computer science and other fields, including speech\nrecognition, computer vision, robotics, information retrieval, natural language\nprocessing, geographic information extraction, and computational drug\ndiscovery. This paper describes the TensorFlow interface and an implementation\nof that interface that we have built at Google. The TensorFlow API and a\nreference implementation were released as an open-source package under the\nApache 2.0 license in November, 2015 and are available at www.tensorflow.org. \n\n"}
{"id": "1603.04525", "contents": "Title: Pushing the Limits of Deep CNNs for Pedestrian Detection Abstract: Compared to other applications in computer vision, convolutional neural\nnetworks have under-performed on pedestrian detection. A breakthrough was made\nvery recently by using sophisticated deep CNN models, with a number of\nhand-crafted features, or explicit occlusion handling mechanism. In this work,\nwe show that by re-using the convolutional feature maps (CFMs) of a deep\nconvolutional neural network (DCNN) model as image features to train an\nensemble of boosted decision models, we are able to achieve the best reported\naccuracy without using specially designed learning algorithms. We empirically\nidentify and disclose important implementation details. We also show that pixel\nlabelling may be simply combined with a detector to boost the detection\nperformance. By adding complementary hand-crafted features such as optical\nflow, the DCNN based detector can be further improved. We set a new record on\nthe Caltech pedestrian dataset, lowering the log-average miss rate from\n$11.7\\%$ to $8.9\\%$, a relative improvement of $24\\%$. We also achieve a\ncomparable result to the state-of-the-art approaches on the KITTI dataset. \n\n"}
{"id": "1603.05145", "contents": "Title: Suppressing the Unusual: towards Robust CNNs using Symmetric Activation\n  Functions Abstract: Many deep Convolutional Neural Networks (CNN) make incorrect predictions on\nadversarial samples obtained by imperceptible perturbations of clean samples.\nWe hypothesize that this is caused by a failure to suppress unusual signals\nwithin network layers. As remedy we propose the use of Symmetric Activation\nFunctions (SAF) in non-linear signal transducer units. These units suppress\nsignals of exceptional magnitude. We prove that SAF networks can perform\nclassification tasks to arbitrary precision in a simplified situation. In\npractice, rather than use SAFs alone, we add them into CNNs to improve their\nrobustness. The modified CNNs can be easily trained using popular strategies\nwith the moderate training load. Our experiments on MNIST and CIFAR-10 show\nthat the modified CNNs perform similarly to plain ones on clean samples, and\nare remarkably more robust against adversarial and nonsense samples. \n\n"}
{"id": "1603.06568", "contents": "Title: Modelling Temporal Information Using Discrete Fourier Transform for\n  Recognizing Emotions in User-generated Videos Abstract: With the widespread of user-generated Internet videos, emotion recognition in\nthose videos attracts increasing research efforts. However, most existing works\nare based on framelevel visual features and/or audio features, which might fail\nto model the temporal information, e.g. characteristics accumulated along time.\nIn order to capture video temporal information, in this paper, we propose to\nanalyse features in frequency domain transformed by discrete Fourier transform\n(DFT features). Frame-level features are firstly extract by a pre-trained deep\nconvolutional neural network (CNN). Then, time domain features are transferred\nand interpolated into DFT features. CNN and DFT features are further encoded\nand fused for emotion classification. By this way, static image features\nextracted from a pre-trained deep CNN and temporal information represented by\nDFT features are jointly considered for video emotion recognition. Experimental\nresults demonstrate that combining DFT features can effectively capture\ntemporal information and therefore improve emotion recognition performance. Our\napproach has achieved a state-of-the-art performance on the largest video\nemotion dataset (VideoEmotion-8 dataset), improving accuracy from 51.1% to\n62.6%. \n\n"}
{"id": "1603.07063", "contents": "Title: Semantic Object Parsing with Graph LSTM Abstract: By taking the semantic object parsing task as an exemplar application\nscenario, we propose the Graph Long Short-Term Memory (Graph LSTM) network,\nwhich is the generalization of LSTM from sequential data or multi-dimensional\ndata to general graph-structured data. Particularly, instead of evenly and\nfixedly dividing an image to pixels or patches in existing multi-dimensional\nLSTM structures (e.g., Row, Grid and Diagonal LSTMs), we take each\narbitrary-shaped superpixel as a semantically consistent node, and adaptively\nconstruct an undirected graph for each image, where the spatial relations of\nthe superpixels are naturally used as edges. Constructed on such an adaptive\ngraph topology, the Graph LSTM is more naturally aligned with the visual\npatterns in the image (e.g., object boundaries or appearance similarities) and\nprovides a more economical information propagation route. Furthermore, for each\noptimization step over Graph LSTM, we propose to use a confidence-driven scheme\nto update the hidden and memory states of nodes progressively till all nodes\nare updated. In addition, for each node, the forgets gates are adaptively\nlearned to capture different degrees of semantic correlation with neighboring\nnodes. Comprehensive evaluations on four diverse semantic object parsing\ndatasets well demonstrate the significant superiority of our Graph LSTM over\nother state-of-the-art solutions. \n\n"}
{"id": "1603.07772", "contents": "Title: Co-occurrence Feature Learning for Skeleton based Action Recognition\n  using Regularized Deep LSTM Networks Abstract: Skeleton based action recognition distinguishes human actions using the\ntrajectories of skeleton joints, which provide a very good representation for\ndescribing actions. Considering that recurrent neural networks (RNNs) with Long\nShort-Term Memory (LSTM) can learn feature representations and model long-term\ntemporal dependencies automatically, we propose an end-to-end fully connected\ndeep LSTM network for skeleton based action recognition. Inspired by the\nobservation that the co-occurrences of the joints intrinsically characterize\nhuman actions, we take the skeleton as the input at each time slot and\nintroduce a novel regularization scheme to learn the co-occurrence features of\nskeleton joints. To train the deep LSTM network effectively, we propose a new\ndropout algorithm which simultaneously operates on the gates, cells, and output\nresponses of the LSTM neurons. Experimental results on three human action\nrecognition datasets consistently demonstrate the effectiveness of the proposed\nmodel. \n\n"}
{"id": "1603.09742", "contents": "Title: Object Boundary Guided Semantic Segmentation Abstract: Semantic segmentation is critical to image content understanding and object\nlocalization. Recent development in fully-convolutional neural network (FCN)\nhas enabled accurate pixel-level labeling. One issue in previous works is that\nthe FCN based method does not exploit the object boundary information to\ndelineate segmentation details since the object boundary label is ignored in\nthe network training. To tackle this problem, we introduce a double branch\nfully convolutional neural network, which separates the learning of the\ndesirable semantic class labeling with mask-level object proposals guided by\nrelabeled boundaries. This network, called object boundary guided FCN\n(OBG-FCN), is able to integrate the distinct properties of object shape and\nclass features elegantly in a fully convolutional way with a designed masking\narchitecture. We conduct experiments on the PASCAL VOC segmentation benchmark,\nand show that the end-to-end trainable OBG-FCN system offers great improvement\nin optimizing the target semantic segmentation quality. \n\n"}
{"id": "1604.00187", "contents": "Title: PHOCNet: A Deep Convolutional Neural Network for Word Spotting in\n  Handwritten Documents Abstract: In recent years, deep convolutional neural networks have achieved state of\nthe art performance in various computer vision task such as classification,\ndetection or segmentation. Due to their outstanding performance, CNNs are more\nand more used in the field of document image analysis as well. In this work, we\npresent a CNN architecture that is trained with the recently proposed PHOC\nrepresentation. We show empirically that our CNN architecture is able to\noutperform state of the art results for various word spotting benchmarks while\nexhibiting short training and test times. \n\n"}
{"id": "1604.00289", "contents": "Title: Building Machines That Learn and Think Like People Abstract: Recent progress in artificial intelligence (AI) has renewed interest in\nbuilding systems that learn and think like people. Many advances have come from\nusing deep neural networks trained end-to-end in tasks such as object\nrecognition, video games, and board games, achieving performance that equals or\neven beats humans in some respects. Despite their biological inspiration and\nperformance achievements, these systems differ from human intelligence in\ncrucial ways. We review progress in cognitive science suggesting that truly\nhuman-like learning and thinking machines will have to reach beyond current\nengineering trends in both what they learn, and how they learn it.\nSpecifically, we argue that these machines should (a) build causal models of\nthe world that support explanation and understanding, rather than merely\nsolving pattern recognition problems; (b) ground learning in intuitive theories\nof physics and psychology, to support and enrich the knowledge that is learned;\nand (c) harness compositionality and learning-to-learn to rapidly acquire and\ngeneralize knowledge to new tasks and situations. We suggest concrete\nchallenges and promising routes towards these goals that can combine the\nstrengths of recent neural network advances with more structured cognitive\nmodels. \n\n"}
{"id": "1604.00906", "contents": "Title: Detecting Engagement in Egocentric Video Abstract: In a wearable camera video, we see what the camera wearer sees. While this\nmakes it easy to know roughly what he chose to look at, it does not immediately\nreveal when he was engaged with the environment. Specifically, at what moments\ndid his focus linger, as he paused to gather more information about something\nhe saw? Knowing this answer would benefit various applications in video\nsummarization and augmented reality, yet prior work focuses solely on the\n\"what\" question (estimating saliency, gaze) without considering the \"when\"\n(engagement). We propose a learning-based approach that uses long-term\negomotion cues to detect engagement, specifically in browsing scenarios where\none frequently takes in new visual information (e.g., shopping, touring). We\nintroduce a large, richly annotated dataset for ego-engagement that is the\nfirst of its kind. Our approach outperforms a wide array of existing methods.\nWe show engagement can be detected well independent of both scene appearance\nand the camera wearer's identity. \n\n"}
{"id": "1604.00970", "contents": "Title: Extended Object Tracking: Introduction, Overview and Applications Abstract: This article provides an elaborate overview of current research in extended\nobject tracking. We provide a clear definition of the extended object tracking\nproblem and discuss its delimitation to other types of object tracking. Next,\ndifferent aspects of extended object modelling are extensively discussed.\nSubsequently, we give a tutorial introduction to two basic and well used\nextended object tracking approaches - the random matrix approach and the Kalman\nfilter-based approach for star-convex shapes. The next part treats the tracking\nof multiple extended objects and elaborates how the large number of feasible\nassociation hypotheses can be tackled using both Random Finite Set (RFS) and\nNon-RFS multi-object trackers. The article concludes with a summary of current\napplications, where four example applications involving camera, X-band radar,\nlight detection and ranging (lidar), red-green-blue-depth (RGB-D) sensors are\nhighlighted. \n\n"}
{"id": "1604.02182", "contents": "Title: Families in the Wild (FIW): Large-Scale Kinship Image Database and\n  Benchmarks Abstract: We present the largest kinship recognition dataset to date, Families in the\nWild (FIW). Motivated by the lack of a single, unified dataset for kinship\nrecognition, we aim to provide a dataset that captivates the interest of the\nresearch community. With only a small team, we were able to collect, organize,\nand label over 10,000 family photos of 1,000 families with our annotation tool\ndesigned to mark complex hierarchical relationships and local label information\nin a quick and efficient manner. We include several benchmarks for two\nimage-based tasks, kinship verification and family recognition. For this, we\nincorporate several visual features and metric learning methods as baselines.\nAlso, we demonstrate that a pre-trained Convolutional Neural Network (CNN) as\nan off-the-shelf feature extractor outperforms the other feature types. Then,\nresults were further boosted by fine-tuning two deep CNNs on FIW data: (1) for\nkinship verification, a triplet loss function was learned on top of the network\nof pre-trained weights; (2) for family recognition, a family-specific softmax\nclassifier was added to the network. \n\n"}
{"id": "1604.02275", "contents": "Title: Online Open World Recognition Abstract: As we enter into the big data age and an avalanche of images have become\nreadily available, recognition systems face the need to move from close, lab\nsettings where the number of classes and training data are fixed, to dynamic\nscenarios where the number of categories to be recognized grows continuously\nover time, as well as new data providing useful information to update the\nsystem. Recent attempts, like the open world recognition framework, tried to\ninject dynamics into the system by detecting new unknown classes and adding\nthem incrementally, while at the same time continuously updating the models for\nthe known classes. incrementally adding new classes and detecting instances\nfrom unknown classes, while at the same time continuously updating the models\nfor the known classes. In this paper we argue that to properly capture the\nintrinsic dynamic of open world recognition, it is necessary to add to these\naspects (a) the incremental learning of the underlying metric, (b) the\nincremental estimate of confidence thresholds for the unknown classes, and (c)\nthe use of local learning to precisely describe the space of classes. We extend\nthree existing metric learning algorithms towards these goals by using online\nmetric learning. Experimentally we validate our approach on two large-scale\ndatasets in different learning scenarios. For all these scenarios our proposed\nmethods outperform their non-online counterparts. We conclude that local and\nonline learning is important to capture the full dynamics of open world\nrecognition. \n\n"}
{"id": "1604.02388", "contents": "Title: STD2P: RGBD Semantic Segmentation Using Spatio-Temporal Data-Driven\n  Pooling Abstract: We propose a novel superpixel-based multi-view convolutional neural network\nfor semantic image segmentation. The proposed network produces a high quality\nsegmentation of a single image by leveraging information from additional views\nof the same scene. Particularly in indoor videos such as captured by robotic\nplatforms or handheld and bodyworn RGBD cameras, nearby video frames provide\ndiverse viewpoints and additional context of objects and scenes. To leverage\nsuch information, we first compute region correspondences by optical flow and\nimage boundary-based superpixels. Given these region correspondences, we\npropose a novel spatio-temporal pooling layer to aggregate information over\nspace and time. We evaluate our approach on the NYU--Depth--V2 and the SUN3D\ndatasets and compare it to various state-of-the-art single-view and multi-view\napproaches. Besides a general improvement over the state-of-the-art, we also\nshow the benefits of making use of unlabeled frames during training for\nmulti-view as well as single-view prediction. \n\n"}
{"id": "1604.02715", "contents": "Title: Soccer Field Localization from a Single Image Abstract: In this work, we propose a novel way of efficiently localizing a soccer field\nfrom a single broadcast image of the game. Related work in this area relies on\nmanually annotating a few key frames and extending the localization to similar\nimages, or installing fixed specialized cameras in the stadium from which the\nlayout of the field can be obtained. In contrast, we formulate this problem as\na branch and bound inference in a Markov random field where an energy function\nis defined in terms of field cues such as grass, lines and circles. Moreover,\nour approach is fully automatic and depends only on single images from the\nbroadcast video of the game. We demonstrate the effectiveness of our method by\napplying it to various games and obtain promising results. Finally, we posit\nthat our approach can be applied easily to other sports such as hockey and\nbasketball. \n\n"}
{"id": "1604.03058", "contents": "Title: Binarized Neural Networks on the ImageNet Classification Task Abstract: We trained Binarized Neural Networks (BNNs) on the high resolution ImageNet\nILSVRC-2102 dataset classification task and achieved a good performance. With a\nmoderate size network of 13 layers, we obtained top-5 classification accuracy\nrate of 84.1 % on validation set through network distillation, much better than\nprevious published results of 73.2% on XNOR network and 69.1% on binarized\nGoogleNET. We expect networks of better performance can be obtained by\nfollowing our current strategies. We provide a detailed discussion and\npreliminary analysis on strategies used in the network training. \n\n"}
{"id": "1604.03517", "contents": "Title: Fast Object Localization Using a CNN Feature Map Based Multi-Scale\n  Search Abstract: Object localization is an important task in computer vision but requires a\nlarge amount of computational power due mainly to an exhaustive multiscale\nsearch on the input image. In this paper, we describe a near real-time\nmultiscale search on a deep CNN feature map that does not use region proposals.\nThe proposed approach effectively exploits local semantic information preserved\nin the feature map of the outermost convolutional layer. A multi-scale search\nis performed on the feature map by processing all the sub-regions of different\nsizes using separate expert units of fully connected layers. Each expert unit\nreceives as input local semantic features only from the corresponding\nsub-regions of a specific geometric shape. Therefore, it contains more nearly\noptimal parameters tailored to the corresponding shape. This multi-scale and\nmulti-aspect ratio scanning strategy can effectively localize a potential\nobject of an arbitrary size. The proposed approach is fast and able to localize\nobjects of interest with a frame rate of 4 fps while providing improved\ndetection performance over the state-of-the art on the PASCAL VOC 12 and MSCOCO\ndata sets. \n\n"}
{"id": "1604.03539", "contents": "Title: Cross-stitch Networks for Multi-task Learning Abstract: Multi-task learning in Convolutional Networks has displayed remarkable\nsuccess in the field of recognition. This success can be largely attributed to\nlearning shared representations from multiple supervisory tasks. However,\nexisting multi-task approaches rely on enumerating multiple network\narchitectures specific to the tasks at hand, that do not generalize. In this\npaper, we propose a principled approach to learn shared representations in\nConvNets using multi-task learning. Specifically, we propose a new sharing\nunit: \"cross-stitch\" unit. These units combine the activations from multiple\nnetworks and can be trained end-to-end. A network with cross-stitch units can\nlearn an optimal combination of shared and task-specific representations. Our\nproposed method generalizes across multiple tasks and shows dramatically\nimproved performance over baseline methods for categories with few training\nexamples. \n\n"}
{"id": "1604.04784", "contents": "Title: ACD: Action Concept Discovery from Image-Sentence Corpora Abstract: Action classification in still images is an important task in computer\nvision. It is challenging as the appearances of ac- tions may vary depending on\ntheir context (e.g. associated objects). Manually labeling of context\ninformation would be time consuming and difficult to scale up. To address this\nchallenge, we propose a method to automatically discover and cluster action\nconcepts, and learn their classifiers from weakly supervised image-sentence\ncorpora. It obtains candidate action concepts by extracting verb-object pairs\nfrom sentences and verifies their visualness with the associated images.\nCandidate action concepts are then clustered by using a multi-modal\nrepresentation with image embeddings from deep convolutional networks and text\nembeddings from word2vec. More than one hundred human action concept\nclassifiers are learned from the Flickr 30k dataset with no additional human\neffort and promising classification results are obtained. We further apply the\nAdaBoost algorithm to automatically select and combine relevant action concepts\ngiven an action query. Promising results have been shown on the PASCAL VOC 2012\naction classification benchmark, which has zero overlap with Flickr30k. \n\n"}
{"id": "1604.06433", "contents": "Title: Walk and Learn: Facial Attribute Representation Learning from Egocentric\n  Video and Contextual Data Abstract: The way people look in terms of facial attributes (ethnicity, hair color,\nfacial hair, etc.) and the clothes or accessories they wear (sunglasses, hat,\nhoodies, etc.) is highly dependent on geo-location and weather condition,\nrespectively. This work explores, for the first time, the use of this\ncontextual information, as people with wearable cameras walk across different\nneighborhoods of a city, in order to learn a rich feature representation for\nfacial attribute classification, without the costly manual annotation required\nby previous methods. By tracking the faces of casual walkers on more than 40\nhours of egocentric video, we are able to cover tens of thousands of different\nidentities and automatically extract nearly 5 million pairs of images connected\nby or from different face tracks, along with their weather and location\ncontext, under pose and lighting variations. These image pairs are then fed\ninto a deep network that preserves similarity of images connected by the same\ntrack, in order to capture identity-related attribute features, and optimizes\nfor location and weather prediction to capture additional facial attribute\nfeatures. Finally, the network is fine-tuned with manually annotated samples.\nWe perform an extensive experimental analysis on wearable data and two standard\nbenchmark datasets based on web images (LFWA and CelebA). Our method\noutperforms by a large margin a network trained from scratch. Moreover, even\nwithout using manually annotated identity labels for pre-training as in\nprevious methods, our approach achieves results that are better than the state\nof the art. \n\n"}
{"id": "1604.06838", "contents": "Title: Word2VisualVec: Image and Video to Sentence Matching by Visual Feature\n  Prediction Abstract: This paper strives to find the sentence best describing the content of an\nimage or video. Different from existing works, which rely on a joint subspace\nfor image / video to sentence matching, we propose to do so in a visual space\nonly. We contribute Word2VisualVec, a deep neural network architecture that\nlearns to predict a deep visual encoding of textual input based on sentence\nvectorization and a multi-layer perceptron. We thoroughly analyze its\narchitectural design, by varying the sentence vectorization strategy, network\ndepth and the deep feature to predict for image to sentence matching. We also\ngeneralize Word2VisualVec for matching a video to a sentence, by extending the\npredictive abilities to 3-D ConvNet features as well as a visual-audio\nrepresentation. Experiments on four challenging image and video benchmarks\ndetail Word2VisualVec's properties, capabilities for image and video to\nsentence matching, and on all datasets its state-of-the-art results. \n\n"}
{"id": "1604.08610", "contents": "Title: Artistic style transfer for videos Abstract: In the past, manually re-drawing an image in a certain artistic style\nrequired a professional artist and a long time. Doing this for a video sequence\nsingle-handed was beyond imagination. Nowadays computers provide new\npossibilities. We present an approach that transfers the style from one image\n(for example, a painting) to a whole video sequence. We make use of recent\nadvances in style transfer in still images and propose new initializations and\nloss functions applicable to videos. This allows us to generate consistent and\nstable stylized video sequences, even in cases with large motion and strong\nocclusion. We show that the proposed method clearly outperforms simpler\nbaselines both qualitatively and quantitatively. \n\n"}
{"id": "1605.01177", "contents": "Title: A metric on the space of finite sets of trajectories for evaluation of\n  multi-target tracking algorithms Abstract: In this paper, we propose a metric on the space of finite sets of\ntrajectories for assessing multi-target tracking algorithms in a mathematically\nsound way. The main use of the metric is to compare estimates of trajectories\nfrom different algorithms with the ground truth of trajectories. The proposed\nmetric includes intuitive costs associated to localization error for properly\ndetected targets, missed and false targets and track switches at each time\nstep. The metric computation is based on solving a multi-dimensional assignment\nproblem. We also propose a lower bound for the metric, which is also a metric\nfor sets of trajectories and is computable in polynomial time using linear\nprogramming. We also extend the proposed metrics on sets of trajectories to\nrandom finite sets of trajectories. \n\n"}
{"id": "1605.01569", "contents": "Title: Classification of Human Whole-Body Motion using Hidden Markov Models Abstract: Human motion plays an important role in many fields. Large databases exist\nthat store and make available recordings of human motions. However, annotating\neach motion with multiple labels is a cumbersome and error-prone process. This\nbachelor's thesis presents different approaches to solve the multi-label\nclassification problem using Hidden Markov Models (HMMs). First, different\nfeatures that can be directly obtained from the raw data are introduced. Next,\nadditional features are derived to improve classification performance. These\nfeatures are then used to perform the multi-label classification using two\ndifferent approaches. The first approach simply transforms the multi-label\nproblem into a multi-class problem. The second, novel approach solves the same\nproblem without the need to construct a transformation by predicting the labels\ndirectly from the likelihood scores. The second approach scales linearly with\nthe number of labels whereas the first approach is subject to combinatorial\nexplosion. All aspects of the classification process are evaluated on a data\nset that consists of 454 motions. System 1 achieves an accuracy of 98.02% and\nsystem 2 an accuracy of 93.39% on the test set. \n\n"}
{"id": "1605.02289", "contents": "Title: Detecting Ground Control Points via Convolutional Neural Network for\n  Stereo Matching Abstract: In this paper, we present a novel approach to detect ground control points\n(GCPs) for stereo matching problem. First of all, we train a convolutional\nneural network (CNN) on a large stereo set, and compute the matching confidence\nof each pixel by using the trained CNN model. Secondly, we present a ground\ncontrol points selection scheme according to the maximum matching confidence of\neach pixel. Finally, the selected GCPs are used to refine the matching costs,\nand we apply the new matching costs to perform optimization with semi-global\nmatching algorithm for improving the final disparity maps. We evaluate our\napproach on the KITTI 2012 stereo benchmark dataset. Our experiments show that\nthe proposed approach significantly improves the accuracy of disparity maps. \n\n"}
{"id": "1605.03259", "contents": "Title: Deep Attributes Driven Multi-Camera Person Re-identification Abstract: The visual appearance of a person is easily affected by many factors like\npose variations, viewpoint changes and camera parameter differences. This makes\nperson Re-Identification (ReID) among multiple cameras a very challenging task.\nThis work is motivated to learn mid-level human attributes which are robust to\nsuch visual appearance variations. And we propose a semi-supervised attribute\nlearning framework which progressively boosts the accuracy of attributes only\nusing a limited number of labeled data. Specifically, this framework involves a\nthree-stage training. A deep Convolutional Neural Network (dCNN) is first\ntrained on an independent dataset labeled with attributes. Then it is\nfine-tuned on another dataset only labeled with person IDs using our defined\ntriplet loss. Finally, the updated dCNN predicts attribute labels for the\ntarget dataset, which is combined with the independent dataset for the final\nround of fine-tuning. The predicted attributes, namely \\emph{deep attributes}\nexhibit superior generalization ability across different datasets. By directly\nusing the deep attributes with simple Cosine distance, we have obtained\nsurprisingly good accuracy on four person ReID datasets. Experiments also show\nthat a simple metric learning modular further boosts our method, making it\nsignificantly outperform many recent works. \n\n"}
{"id": "1605.04006", "contents": "Title: A Gaussian Mixture MRF for Model-Based Iterative Reconstruction with\n  Applications to Low-Dose X-ray CT Abstract: Markov random fields (MRFs) have been widely used as prior models in various\ninverse problems such as tomographic reconstruction. While MRFs provide a\nsimple and often effective way to model the spatial dependencies in images,\nthey suffer from the fact that parameter estimation is difficult. In practice,\nthis means that MRFs typically have very simple structure that cannot\ncompletely capture the subtle characteristics of complex images.\n  In this paper, we present a novel Gaussian mixture Markov random field model\n(GM-MRF) that can be used as a very expressive prior model for inverse problems\nsuch as denoising and reconstruction. The GM-MRF forms a global image model by\nmerging together individual Gaussian-mixture models (GMMs) for image patches.\nIn addition, we present a novel analytical framework for computing MAP\nestimates using the GM-MRF prior model through the construction of surrogate\nfunctions that result in a sequence of quadratic optimizations. We also\nintroduce a simple but effective method to adjust the GM-MRF so as to control\nthe sharpness in low- and high-contrast regions of the reconstruction\nseparately. We demonstrate the value of the model with experiments including\nimage denoising and low-dose CT reconstruction. \n\n"}
{"id": "1605.04068", "contents": "Title: Fast Semantic Image Segmentation with High Order Context and Guided\n  Filtering Abstract: This paper describes a fast and accurate semantic image segmentation approach\nthat encodes not only the discriminative features from deep neural networks,\nbut also the high-order context compatibility among adjacent objects as well as\nlow level image features. We formulate the underlying problem as the\nconditional random field that embeds local feature extraction, clique potential\nconstruction, and guided filtering within the same framework, and provide an\nefficient coarse-to-fine solver. At the coarse level, we combine local feature\nrepresentation and context interaction using a deep convolutional network, and\ndirectly learn the interaction from high order cliques with a message passing\nroutine, avoiding time-consuming explicit graph inference for joint probability\ndistribution. At the fine level, we introduce a guided filtering interpretation\nfor the mean field algorithm, and achieve accurate object boundaries with 100+\nfaster than classic learning methods. The two parts are connected and jointly\ntrained in an end-to-end fashion. Experimental results on Pascal VOC 2012\ndataset have shown that the proposed algorithm outperforms the\nstate-of-the-art, and that it achieves the rank 1 performance at the time of\nsubmission, both of which prove the effectiveness of this unified framework for\nsemantic image segmentation. \n\n"}
{"id": "1605.05396", "contents": "Title: Generative Adversarial Text to Image Synthesis Abstract: Automatic synthesis of realistic images from text would be interesting and\nuseful, but current AI systems are still far from this goal. However, in recent\nyears generic and powerful recurrent neural network architectures have been\ndeveloped to learn discriminative text feature representations. Meanwhile, deep\nconvolutional generative adversarial networks (GANs) have begun to generate\nhighly compelling images of specific categories, such as faces, album covers,\nand room interiors. In this work, we develop a novel deep architecture and GAN\nformulation to effectively bridge these advances in text and image model- ing,\ntranslating visual concepts from characters to pixels. We demonstrate the\ncapability of our model to generate plausible images of birds and flowers from\ndetailed text descriptions. \n\n"}
{"id": "1605.05440", "contents": "Title: Beyond Caption To Narrative: Video Captioning With Multiple Sentences Abstract: Recent advances in image captioning task have led to increasing interests in\nvideo captioning task. However, most works on video captioning are focused on\ngenerating single input of aggregated features, which hardly deviates from\nimage captioning process and does not fully take advantage of dynamic contents\npresent in videos. We attempt to generate video captions that convey richer\ncontents by temporally segmenting the video with action localization,\ngenerating multiple captions from multiple frames, and connecting them with\nnatural language processing techniques, in order to generate a story-like\ncaption. We show that our proposed method can generate captions that are richer\nin contents and can compete with state-of-the-art method without explicitly\nusing video-level features as input. \n\n"}
{"id": "1605.05466", "contents": "Title: Image segmentation with superpixel-based covariance descriptors in\n  low-rank representation Abstract: This paper investigates the problem of image segmentation using superpixels.\nWe propose two approaches to enhance the discriminative ability of the\nsuperpixel's covariance descriptors. In the first one, we employ the\nLog-Euclidean distance as the metric on the covariance manifolds, and then use\nthe RBF kernel to measure the similarities between covariance descriptors. The\nsecond method is focused on extracting the subspace structure of the set of\ncovariance descriptors by extending a low rank representation algorithm on to\nthe covariance manifolds. Experiments are carried out with the Berkly\nSegmentation Dataset, and compared with the state-of-the-art segmentation\nalgorithms, both methods are competitive. \n\n"}
{"id": "1605.06069", "contents": "Title: A Hierarchical Latent Variable Encoder-Decoder Model for Generating\n  Dialogues Abstract: Sequential data often possesses a hierarchical structure with complex\ndependencies between subsequences, such as found between the utterances in a\ndialogue. In an effort to model this kind of generative process, we propose a\nneural network-based generative architecture, with latent stochastic variables\nthat span a variable number of time steps. We apply the proposed model to the\ntask of dialogue response generation and compare it with recent neural network\narchitectures. We evaluate the model performance through automatic evaluation\nmetrics and by carrying out a human evaluation. The experiments demonstrate\nthat our model improves upon recently proposed models and that the latent\nvariables facilitate the generation of long outputs and maintain the context. \n\n"}
{"id": "1605.07133", "contents": "Title: Towards Multi-Agent Communication-Based Language Learning Abstract: We propose an interactive multimodal framework for language learning. Instead\nof being passively exposed to large amounts of natural text, our learners\n(implemented as feed-forward neural networks) engage in cooperative referential\ngames starting from a tabula rasa setup, and thus develop their own language\nfrom the need to communicate in order to succeed at the game. Preliminary\nexperiments provide promising results, but also suggest that it is important to\nensure that agents trained in this way do not develop an adhoc communication\ncode only effective for the game they are playing \n\n"}
{"id": "1605.07277", "contents": "Title: Transferability in Machine Learning: from Phenomena to Black-Box Attacks\n  using Adversarial Samples Abstract: Many machine learning models are vulnerable to adversarial examples: inputs\nthat are specially crafted to cause a machine learning model to produce an\nincorrect output. Adversarial examples that affect one model often affect\nanother model, even if the two models have different architectures or were\ntrained on different training sets, so long as both models were trained to\nperform the same task. An attacker may therefore train their own substitute\nmodel, craft adversarial examples against the substitute, and transfer them to\na victim model, with very little information about the victim. Recent work has\nfurther developed a technique that uses the victim model as an oracle to label\na synthetic training set for the substitute, so the attacker need not even\ncollect a training set to mount the attack. We extend these recent techniques\nusing reservoir sampling to greatly enhance the efficiency of the training\nprocedure for the substitute model. We introduce new transferability attacks\nbetween previously unexplored (substitute, victim) pairs of machine learning\nmodel classes, most notably SVMs and decision trees. We demonstrate our attacks\non two commercial machine learning classification systems from Amazon (96.19%\nmisclassification rate) and Google (88.94%) using only 800 queries of the\nvictim model, thereby showing that existing machine learning approaches are in\ngeneral vulnerable to systematic black-box attacks regardless of their\nstructure. \n\n"}
{"id": "1605.07648", "contents": "Title: FractalNet: Ultra-Deep Neural Networks without Residuals Abstract: We introduce a design strategy for neural network macro-architecture based on\nself-similarity. Repeated application of a simple expansion rule generates deep\nnetworks whose structural layouts are precisely truncated fractals. These\nnetworks contain interacting subpaths of different lengths, but do not include\nany pass-through or residual connections; every internal signal is transformed\nby a filter and nonlinearity before being seen by subsequent layers. In\nexperiments, fractal networks match the excellent performance of standard\nresidual networks on both CIFAR and ImageNet classification tasks, thereby\ndemonstrating that residual representations may not be fundamental to the\nsuccess of extremely deep convolutional neural networks. Rather, the key may be\nthe ability to transition, during training, from effectively shallow to deep.\nWe note similarities with student-teacher behavior and develop drop-path, a\nnatural extension of dropout, to regularize co-adaptation of subpaths in\nfractal architectures. Such regularization allows extraction of\nhigh-performance fixed-depth subnetworks. Additionally, fractal networks\nexhibit an anytime property: shallow subnetworks provide a quick answer, while\ndeeper subnetworks, with higher latency, provide a more accurate answer. \n\n"}
{"id": "1605.07723", "contents": "Title: Data Programming: Creating Large Training Sets, Quickly Abstract: Large labeled training sets are the critical building blocks of supervised\nlearning methods and are key enablers of deep learning techniques. For some\napplications, creating labeled training sets is the most time-consuming and\nexpensive part of applying machine learning. We therefore propose a paradigm\nfor the programmatic creation of training sets called data programming in which\nusers express weak supervision strategies or domain heuristics as labeling\nfunctions, which are programs that label subsets of the data, but that are\nnoisy and may conflict. We show that by explicitly representing this training\nset labeling process as a generative model, we can \"denoise\" the generated\ntraining set, and establish theoretically that we can recover the parameters of\nthese generative models in a handful of settings. We then show how to modify a\ndiscriminative loss function to make it noise-aware, and demonstrate our method\nover a range of discriminative models including logistic regression and LSTMs.\nExperimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data\nprogramming would have led to a new winning score, and also show that applying\ndata programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points\nover a state-of-the-art LSTM baseline (and into second place in the\ncompetition). Additionally, in initial user studies we observed that data\nprogramming may be an easier way for non-experts to create machine learning\nmodels when training data is limited or unavailable. \n\n"}
{"id": "1606.01286", "contents": "Title: Incorporating long-range consistency in CNN-based texture generation Abstract: Gatys et al. (2015) showed that pair-wise products of features in a\nconvolutional network are a very effective representation of image textures. We\npropose a simple modification to that representation which makes it possible to\nincorporate long-range structure into image generation, and to render images\nthat satisfy various symmetry constraints. We show how this can greatly improve\nrendering of regular textures and of images that contain other kinds of\nsymmetric structure. We also present applications to inpainting and season\ntransfer. \n\n"}
{"id": "1606.01981", "contents": "Title: Deep neural networks are robust to weight binarization and other\n  non-linear distortions Abstract: Recent results show that deep neural networks achieve excellent performance\neven when, during training, weights are quantized and projected to a binary\nrepresentation. Here, we show that this is just the tip of the iceberg: these\nsame networks, during testing, also exhibit a remarkable robustness to\ndistortions beyond quantization, including additive and multiplicative noise,\nand a class of non-linear projections where binarization is just a special\ncase. To quantify this robustness, we show that one such network achieves 11%\ntest error on CIFAR-10 even with 0.68 effective bits per weight. Furthermore,\nwe find that a common training heuristic--namely, projecting quantized weights\nduring backpropagation--can be altered (or even removed) and networks still\nachieve a base level of robustness during testing. Specifically, training with\nweight projections other than quantization also works, as does simply clipping\nthe weights, both of which have never been reported before. We confirm our\nresults for CIFAR-10 and ImageNet datasets. Finally, drawing from these ideas,\nwe propose a stochastic projection rule that leads to a new state of the art\nnetwork with 7.64% test error on CIFAR-10 using no data augmentation. \n\n"}
{"id": "1606.02147", "contents": "Title: ENet: A Deep Neural Network Architecture for Real-Time Semantic\n  Segmentation Abstract: The ability to perform pixel-wise semantic segmentation in real-time is of\nparamount importance in mobile applications. Recent deep neural networks aimed\nat this task have the disadvantage of requiring a large number of floating\npoint operations and have long run-times that hinder their usability. In this\npaper, we propose a novel deep neural network architecture named ENet\n(efficient neural network), created specifically for tasks requiring low\nlatency operation. ENet is up to 18$\\times$ faster, requires 75$\\times$ less\nFLOPs, has 79$\\times$ less parameters, and provides similar or better accuracy\nto existing models. We have tested it on CamVid, Cityscapes and SUN datasets\nand report on comparisons with existing state-of-the-art methods, and the\ntrade-offs between accuracy and processing time of a network. We present\nperformance measurements of the proposed architecture on embedded systems and\nsuggest possible software improvements that could make ENet even faster. \n\n"}
{"id": "1606.02393", "contents": "Title: Progressive Attention Networks for Visual Attribute Prediction Abstract: We propose a novel attention model that can accurately attends to target\nobjects of various scales and shapes in images. The model is trained to\ngradually suppress irrelevant regions in an input image via a progressive\nattentive process over multiple layers of a convolutional neural network. The\nattentive process in each layer determines whether to pass or block features at\ncertain spatial locations for use in the subsequent layers. The proposed\nprogressive attention mechanism works well especially when combined with hard\nattention. We further employ local contexts to incorporate neighborhood\nfeatures of each location and estimate a better attention probability map. The\nexperiments on synthetic and real datasets show that the proposed attention\nnetworks outperform traditional attention methods in visual attribute\nprediction tasks. \n\n"}
{"id": "1606.02396", "contents": "Title: Deep Successor Reinforcement Learning Abstract: Learning robust value functions given raw observations and rewards is now\npossible with model-free and model-based deep reinforcement learning\nalgorithms. There is a third alternative, called Successor Representations\n(SR), which decomposes the value function into two components -- a reward\npredictor and a successor map. The successor map represents the expected future\nstate occupancy from any given state and the reward predictor maps states to\nscalar rewards. The value function of a state can be computed as the inner\nproduct between the successor map and the reward weights. In this paper, we\npresent DSR, which generalizes SR within an end-to-end deep reinforcement\nlearning framework. DSR has several appealing properties including: increased\nsensitivity to distal reward changes due to factorization of reward and world\ndynamics, and the ability to extract bottleneck states (subgoals) given\nsuccessor maps trained under a random policy. We show the efficacy of our\napproach on two diverse environments given raw pixel observations -- simple\ngrid-world domains (MazeBase) and the Doom game engine. \n\n"}
{"id": "1606.03498", "contents": "Title: Improved Techniques for Training GANs Abstract: We present a variety of new architectural features and training procedures\nthat we apply to the generative adversarial networks (GANs) framework. We focus\non two applications of GANs: semi-supervised learning, and the generation of\nimages that humans find visually realistic. Unlike most work on generative\nmodels, our primary goal is not to train a model that assigns high likelihood\nto test data, nor do we require the model to be able to learn well without\nusing any labels. Using our new techniques, we achieve state-of-the-art results\nin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated\nimages are of high quality as confirmed by a visual Turing test: our model\ngenerates MNIST samples that humans cannot distinguish from real data, and\nCIFAR-10 samples that yield a human error rate of 21.3%. We also present\nImageNet samples with unprecedented resolution and show that our methods enable\nthe model to learn recognizable features of ImageNet classes. \n\n"}
{"id": "1606.03657", "contents": "Title: InfoGAN: Interpretable Representation Learning by Information Maximizing\n  Generative Adversarial Nets Abstract: This paper describes InfoGAN, an information-theoretic extension to the\nGenerative Adversarial Network that is able to learn disentangled\nrepresentations in a completely unsupervised manner. InfoGAN is a generative\nadversarial network that also maximizes the mutual information between a small\nsubset of the latent variables and the observation. We derive a lower bound to\nthe mutual information objective that can be optimized efficiently, and show\nthat our training procedure can be interpreted as a variation of the Wake-Sleep\nalgorithm. Specifically, InfoGAN successfully disentangles writing styles from\ndigit shapes on the MNIST dataset, pose from lighting of 3D rendered images,\nand background digits from the central digit on the SVHN dataset. It also\ndiscovers visual concepts that include hair styles, presence/absence of\neyeglasses, and emotions on the CelebA face dataset. Experiments show that\nInfoGAN learns interpretable representations that are competitive with\nrepresentations learned by existing fully supervised methods. \n\n"}
{"id": "1606.04671", "contents": "Title: Progressive Neural Networks Abstract: Learning to solve complex sequences of tasks--while both leveraging transfer\nand avoiding catastrophic forgetting--remains a key obstacle to achieving\nhuman-level intelligence. The progressive networks approach represents a step\nforward in this direction: they are immune to forgetting and can leverage prior\nknowledge via lateral connections to previously learned features. We evaluate\nthis architecture extensively on a wide variety of reinforcement learning tasks\n(Atari and 3D maze games), and show that it outperforms common baselines based\non pretraining and finetuning. Using a novel sensitivity measure, we\ndemonstrate that transfer occurs at both low-level sensory and high-level\ncontrol layers of the learned policy. \n\n"}
{"id": "1606.06164", "contents": "Title: Pragmatic factors in image description: the case of negations Abstract: We provide a qualitative analysis of the descriptions containing negations\n(no, not, n't, nobody, etc) in the Flickr30K corpus, and a categorization of\nnegation uses. Based on this analysis, we provide a set of requirements that an\nimage description system should have in order to generate negation sentences.\nAs a pilot experiment, we used our categorization to manually annotate\nsentences containing negations in the Flickr30K corpus, with an agreement score\nof K=0.67. With this paper, we hope to open up a broader discussion of\nsubjective language in image descriptions. \n\n"}
{"id": "1606.07536", "contents": "Title: Coupled Generative Adversarial Networks Abstract: We propose coupled generative adversarial network (CoGAN) for learning a\njoint distribution of multi-domain images. In contrast to the existing\napproaches, which require tuples of corresponding images in different domains\nin the training set, CoGAN can learn a joint distribution without any tuple of\ncorresponding images. It can learn a joint distribution with just samples drawn\nfrom the marginal distributions. This is achieved by enforcing a weight-sharing\nconstraint that limits the network capacity and favors a joint distribution\nsolution over a product of marginal distributions one. We apply CoGAN to\nseveral joint distribution learning tasks, including learning a joint\ndistribution of color and depth images, and learning a joint distribution of\nface images with different attributes. For each task it successfully learns the\njoint distribution without any tuple of corresponding images. We also\ndemonstrate its applications to domain adaptation and image transformation. \n\n"}
{"id": "1607.00345", "contents": "Title: Convergence Rate of Frank-Wolfe for Non-Convex Objectives Abstract: We give a simple proof that the Frank-Wolfe algorithm obtains a stationary\npoint at a rate of $O(1/\\sqrt{t})$ on non-convex objectives with a Lipschitz\ncontinuous gradient. Our analysis is affine invariant and is the first, to the\nbest of our knowledge, giving a similar rate to what was already proven for\nprojected gradient methods (though on slightly different measures of\nstationarity). \n\n"}
{"id": "1607.01205", "contents": "Title: Learning the semantic structure of objects from Web supervision Abstract: While recent research in image understanding has often focused on recognizing\nmore types of objects, understanding more about the objects is just as\nimportant. Recognizing object parts and attributes has been extensively studied\nbefore, yet learning large space of such concepts remains elusive due to the\nhigh cost of providing detailed object annotations for supervision. The key\ncontribution of this paper is an algorithm to learn the nameable parts of\nobjects automatically, from images obtained by querying Web search engines. The\nkey challenge is the high level of noise in the annotations; to address it, we\npropose a new unified embedding space where the appearance and geometry of\nobjects and their semantic parts are represented uniformly. Geometric\nrelationships are induced in a soft manner by a rich set of nonsemantic\nmid-level anchors, bridging the gap between semantic and non-semantic parts. We\nalso show that the resulting embedding provides a visually-intuitive mechanism\nto navigate the learned concepts and their corresponding images. \n\n"}
{"id": "1607.02533", "contents": "Title: Adversarial examples in the physical world Abstract: Most existing machine learning classifiers are highly vulnerable to\nadversarial examples. An adversarial example is a sample of input data which\nhas been modified very slightly in a way that is intended to cause a machine\nlearning classifier to misclassify it. In many cases, these modifications can\nbe so subtle that a human observer does not even notice the modification at\nall, yet the classifier still makes a mistake. Adversarial examples pose\nsecurity concerns because they could be used to perform an attack on machine\nlearning systems, even if the adversary has no access to the underlying model.\nUp to now, all previous work have assumed a threat model in which the adversary\ncan feed data directly into the machine learning classifier. This is not always\nthe case for systems operating in the physical world, for example those which\nare using signals from cameras and other sensors as an input. This paper shows\nthat even in such physical world scenarios, machine learning systems are\nvulnerable to adversarial examples. We demonstrate this by feeding adversarial\nimages obtained from cell-phone camera to an ImageNet Inception classifier and\nmeasuring the classification accuracy of the system. We find that a large\nfraction of adversarial examples are classified incorrectly even when perceived\nthrough the camera. \n\n"}
{"id": "1607.02748", "contents": "Title: Adversarial Training For Sketch Retrieval Abstract: Generative Adversarial Networks (GAN) are able to learn excellent\nrepresentations for unlabelled data which can be applied to image generation\nand scene classification. Representations learned by GANs have not yet been\napplied to retrieval. In this paper, we show that the representations learned\nby GANs can indeed be used for retrieval. We consider heritage documents that\ncontain unlabelled Merchant Marks, sketch-like symbols that are similar to\nhieroglyphs. We introduce a novel GAN architecture with design features that\nmake it suitable for sketch retrieval. The performance of this sketch-GAN is\ncompared to a modified version of the original GAN architecture with respect to\nsimple invariance properties. Experiments suggest that sketch-GANs learn\nrepresentations that are suitable for retrieval and which also have increased\nstability to rotation, scale and translation compared to the standard GAN\narchitecture. \n\n"}
{"id": "1607.03468", "contents": "Title: Event-based, 6-DOF Camera Tracking from Photometric Depth Maps Abstract: Event cameras are bio-inspired vision sensors that output pixel-level\nbrightness changes instead of standard intensity frames. These cameras do not\nsuffer from motion blur and have a very high dynamic range, which enables them\nto provide reliable visual information during high-speed motions or in scenes\ncharacterized by high dynamic range. These features, along with a very low\npower consumption, make event cameras an ideal complement to standard cameras\nfor VR/AR and video game applications. With these applications in mind, this\npaper tackles the problem of accurate, low-latency tracking of an event camera\nfrom an existing photometric depth map (i.e., intensity plus depth information)\nbuilt via classic dense reconstruction pipelines. Our approach tracks the 6-DOF\npose of the event camera upon the arrival of each event, thus virtually\neliminating latency. We successfully evaluate the method in both indoor and\noutdoor scenes and show that---because of the technological advantages of the\nevent camera---our pipeline works in scenes characterized by high-speed motion,\nwhich are still unaccessible to standard cameras. \n\n"}
{"id": "1607.03516", "contents": "Title: Deep Reconstruction-Classification Networks for Unsupervised Domain\n  Adaptation Abstract: In this paper, we propose a novel unsupervised domain adaptation algorithm\nbased on deep learning for visual object recognition. Specifically, we design a\nnew model called Deep Reconstruction-Classification Network (DRCN), which\njointly learns a shared encoding representation for two tasks: i) supervised\nclassification of labeled source data, and ii) unsupervised reconstruction of\nunlabeled target data.In this way, the learnt representation not only preserves\ndiscriminability, but also encodes useful information from the target domain.\nOur new DRCN model can be optimized by using backpropagation similarly as the\nstandard neural networks.\n  We evaluate the performance of DRCN on a series of cross-domain object\nrecognition tasks, where DRCN provides a considerable improvement (up to ~8% in\naccuracy) over the prior state-of-the-art algorithms. Interestingly, we also\nobserve that the reconstruction pipeline of DRCN transforms images from the\nsource domain into images whose appearance resembles the target dataset. This\nsuggests that DRCN's performance is due to constructing a single composite\nrepresentation that encodes information about both the structure of target\nimages and the classification of source images. Finally, we provide a formal\nanalysis to justify the algorithm's objective in domain adaptation context. \n\n"}
{"id": "1607.04433", "contents": "Title: End-to-End Learning for Image Burst Deblurring Abstract: We present a neural network model approach for multi-frame blind\ndeconvolution. The discriminative approach adopts and combines two recent\ntechniques for image deblurring into a single neural network architecture. Our\nproposed hybrid-architecture combines the explicit prediction of a\ndeconvolution filter and non-trivial averaging of Fourier coefficients in the\nfrequency domain. In order to make full use of the information contained in all\nimages in one burst, the proposed network embeds smaller networks, which\nexplicitly allow the model to transfer information between images in early\nlayers. Our system is trained end-to-end using standard backpropagation on a\nset of artificially generated training examples, enabling competitive\nperformance in multi-frame blind deconvolution, both with respect to quality\nand runtime. \n\n"}
{"id": "1607.05140", "contents": "Title: Learning to Hash with Binary Deep Neural Network Abstract: This work proposes deep network models and learning algorithms for\nunsupervised and supervised binary hashing. Our novel network design constrains\none hidden layer to directly output the binary codes. This addresses a\nchallenging issue in some previous works: optimizing non-smooth objective\nfunctions due to binarization. Moreover, we incorporate independence and\nbalance properties in the direct and strict forms in the learning. Furthermore,\nwe include similarity preserving property in our objective function. Our\nresulting optimization with these binary, independence, and balance constraints\nis difficult to solve. We propose to attack it with alternating optimization\nand careful relaxation. Experimental results on three benchmark datasets show\nthat our proposed methods compare favorably with the state of the art. \n\n"}
{"id": "1607.05194", "contents": "Title: HeMIS: Hetero-Modal Image Segmentation Abstract: We introduce a deep learning image segmentation framework that is extremely\nrobust to missing imaging modalities. Instead of attempting to impute or\nsynthesize missing data, the proposed approach learns, for each modality, an\nembedding of the input image into a single latent vector space for which\narithmetic operations (such as taking the mean) are well defined. Points in\nthat space, which are averaged over modalities available at inference time, can\nthen be further processed to yield the desired segmentation. As such, any\ncombinatorial subset of available modalities can be provided as input, without\nhaving to learn a combinatorial number of imputation models. Evaluated on two\nneurological MRI datasets (brain tumors and MS lesions), the approach yields\nstate-of-the-art segmentation results when provided with all modalities;\nmoreover, its performance degrades remarkably gracefully when modalities are\nremoved, significantly more so than alternative mean-filling or other synthesis\napproaches. \n\n"}
{"id": "1607.07155", "contents": "Title: A Unified Multi-scale Deep Convolutional Neural Network for Fast Object\n  Detection Abstract: A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is\nproposed for fast multi-scale object detection. The MS-CNN consists of a\nproposal sub-network and a detection sub-network. In the proposal sub-network,\ndetection is performed at multiple output layers, so that receptive fields\nmatch objects of different scales. These complementary scale-specific detectors\nare combined to produce a strong multi-scale object detector. The unified\nnetwork is learned end-to-end, by optimizing a multi-task loss. Feature\nupsampling by deconvolution is also explored, as an alternative to input\nupsampling, to reduce the memory and computation costs. State-of-the-art object\ndetection performance, at up to 15 fps, is reported on datasets, such as KITTI\nand Caltech, containing a substantial number of small objects. \n\n"}
{"id": "1607.07304", "contents": "Title: Tracking with multi-level features Abstract: We present a novel formulation of the multiple object tracking problem which\nintegrates low and mid-level features. In particular, we formulate the tracking\nproblem as a quadratic program coupling detections and dense point\ntrajectories. Due to the computational complexity of the initial QP, we propose\nan approximation by two auxiliary problems, a temporal and spatial association,\nwhere the temporal subproblem can be efficiently solved by a linear program and\nthe spatial association by a clustering algorithm. The objective function of\nthe QP is used in order to find the optimal number of clusters, where each\ncluster ideally represents one person. Evaluation is provided for multiple\nscenarios, showing the superiority of our method with respect to classic\ntracking-by-detection methods and also other methods that greedily integrate\nlow-level features. \n\n"}
{"id": "1608.00797", "contents": "Title: CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2016 Abstract: This paper presents the method that underlies our submission to the untrimmed\nvideo classification task of ActivityNet Challenge 2016. We follow the basic\npipeline of temporal segment networks and further raise the performance via a\nnumber of other techniques. Specifically, we use the latest deep model\narchitecture, e.g., ResNet and Inception V3, and introduce new aggregation\nschemes (top-k and attention-weighted pooling). Additionally, we incorporate\nthe audio as a complementary channel, extracting relevant information via a CNN\napplied to the spectrograms. With these techniques, we derive an ensemble of\ndeep models, which, together, attains a high classification accuracy (mAP\n$93.23\\%$) on the testing set and secured the first place in the challenge. \n\n"}
{"id": "1608.00853", "contents": "Title: A study of the effect of JPG compression on adversarial images Abstract: Neural network image classifiers are known to be vulnerable to adversarial\nimages, i.e., natural images which have been modified by an adversarial\nperturbation specifically designed to be imperceptible to humans yet fool the\nclassifier. Not only can adversarial images be generated easily, but these\nimages will often be adversarial for networks trained on disjoint subsets of\ndata or with different architectures. Adversarial images represent a potential\nsecurity risk as well as a serious machine learning challenge---it is clear\nthat vulnerable neural networks perceive images very differently from humans.\nNoting that virtually every image classification data set is composed of JPG\nimages, we evaluate the effect of JPG compression on the classification of\nadversarial images. For Fast-Gradient-Sign perturbations of small magnitude, we\nfound that JPG compression often reverses the drop in classification accuracy\nto a large extent, but not always. As the magnitude of the perturbations\nincreases, JPG recompression alone is insufficient to reverse the effect. \n\n"}
{"id": "1608.02164", "contents": "Title: Adapting Deep Network Features to Capture Psychological Representations Abstract: Deep neural networks have become increasingly successful at solving classic\nperception problems such as object recognition, semantic segmentation, and\nscene understanding, often reaching or surpassing human-level accuracy. This\nsuccess is due in part to the ability of DNNs to learn useful representations\nof high-dimensional inputs, a problem that humans must also solve. We examine\nthe relationship between the representations learned by these networks and\nhuman psychological representations recovered from similarity judgments. We\nfind that deep features learned in service of object classification account for\na significant amount of the variance in human similarity judgments for a set of\nanimal images. However, these features do not capture some qualitative\ndistinctions that are a key part of human representations. To remedy this, we\ndevelop a method for adapting deep features to align with human similarity\njudgments, resulting in image representations that can potentially be used to\nextend the scope of psychological experiments. \n\n"}
{"id": "1608.02717", "contents": "Title: Mean Box Pooling: A Rich Image Representation and Output Embedding for\n  the Visual Madlibs Task Abstract: We present Mean Box Pooling, a novel visual representation that pools over\nCNN representations of a large number, highly overlapping object proposals. We\nshow that such representation together with nCCA, a successful multimodal\nembedding technique, achieves state-of-the-art performance on the Visual\nMadlibs task. Moreover, inspired by the nCCA's objective function, we extend\nclassical CNN+LSTM approach to train the network by directly maximizing the\nsimilarity between the internal representation of the deep learning\narchitecture and candidate answers. Again, such approach achieves a significant\nimprovement over the prior work that also uses CNN+LSTM approach on Visual\nMadlibs. \n\n"}
{"id": "1608.02989", "contents": "Title: Deep Convolutional Neural Networks for Microscopy-Based Point of Care\n  Diagnostics Abstract: Point of care diagnostics using microscopy and computer vision methods have\nbeen applied to a number of practical problems, and are particularly relevant\nto low-income, high disease burden areas. However, this is subject to the\nlimitations in sensitivity and specificity of the computer vision methods used.\nIn general, deep learning has recently revolutionised the field of computer\nvision, in some cases surpassing human performance for other object recognition\ntasks. In this paper, we evaluate the performance of deep convolutional neural\nnetworks on three different microscopy tasks: diagnosis of malaria in thick\nblood smears, tuberculosis in sputum samples, and intestinal parasite eggs in\nstool samples. In all cases accuracy is very high and substantially better than\nan alternative approach more representative of traditional medical imaging\ntechniques. \n\n"}
{"id": "1608.05971", "contents": "Title: STFCN: Spatio-Temporal FCN for Semantic Video Segmentation Abstract: This paper presents a novel method to involve both spatial and temporal\nfeatures for semantic video segmentation. Current work on convolutional neural\nnetworks(CNNs) has shown that CNNs provide advanced spatial features supporting\na very good performance of solutions for both image and video analysis,\nespecially for the semantic segmentation task. We investigate how involving\ntemporal features also has a good effect on segmenting video data. We propose a\nmodule based on a long short-term memory (LSTM) architecture of a recurrent\nneural network for interpreting the temporal characteristics of video frames\nover time. Our system takes as input frames of a video and produces a\ncorrespondingly-sized output; for segmenting the video our method combines the\nuse of three components: First, the regional spatial features of frames are\nextracted using a CNN; then, using LSTM the temporal features are added;\nfinally, by deconvolving the spatio-temporal features we produce pixel-wise\npredictions. Our key insight is to build spatio-temporal convolutional networks\n(spatio-temporal CNNs) that have an end-to-end architecture for semantic video\nsegmentation. We adapted fully some known convolutional network architectures\n(such as FCN-AlexNet and FCN-VGG16), and dilated convolution into our\nspatio-temporal CNNs. Our spatio-temporal CNNs achieve state-of-the-art\nsemantic segmentation, as demonstrated for the Camvid and NYUDv2 datasets. \n\n"}
{"id": "1608.06010", "contents": "Title: Feedback-Controlled Sequential Lasso Screening Abstract: One way to solve lasso problems when the dictionary does not fit into\navailable memory is to first screen the dictionary to remove unneeded features.\nPrior research has shown that sequential screening methods offer the greatest\npromise in this endeavor. Most existing work on sequential screening targets\nthe context of tuning parameter selection, where one screens and solves a\nsequence of $N$ lasso problems with a fixed grid of geometrically spaced\nregularization parameters. In contrast, we focus on the scenario where a target\nregularization parameter has already been chosen via cross-validated model\nselection, and we then need to solve many lasso instances using this fixed\nvalue. In this context, we propose and explore a feedback controlled sequential\nscreening scheme. Feedback is used at each iteration to select the next problem\nto be solved. This allows the sequence of problems to be adapted to the\ninstance presented and the number of intermediate problems to be automatically\nselected. We demonstrate our feedback scheme using several datasets including a\ndictionary of approximate size 100,000 by 300,000. \n\n"}
{"id": "1608.06014", "contents": "Title: The Symmetry of a Simple Optimization Problem in Lasso Screening Abstract: Recently dictionary screening has been proposed as an effective way to\nimprove the computational efficiency of solving the lasso problem, which is one\nof the most commonly used method for learning sparse representations. To\naddress today's ever increasing large dataset, effective screening relies on a\ntight region bound on the solution to the dual lasso. Typical region bounds are\nin the form of an intersection of a sphere and multiple half spaces. One way to\ntighten the region bound is using more half spaces, which however, adds to the\noverhead of solving the high dimensional optimization problem in lasso\nscreening. This paper reveals the interesting property that the optimization\nproblem only depends on the projection of features onto the subspace spanned by\nthe normals of the half spaces. This property converts an optimization problem\nin high dimension to much lower dimension, and thus sheds light on reducing the\ncomputation overhead of lasso screening based on tighter region bounds. \n\n"}
{"id": "1608.07017", "contents": "Title: Ambient Sound Provides Supervision for Visual Learning Abstract: The sound of crashing waves, the roar of fast-moving cars -- sound conveys\nimportant information about the objects in our surroundings. In this work, we\nshow that ambient sounds can be used as a supervisory signal for learning\nvisual models. To demonstrate this, we train a convolutional neural network to\npredict a statistical summary of the sound associated with a video frame. We\nshow that, through this process, the network learns a representation that\nconveys information about objects and scenes. We evaluate this representation\non several recognition tasks, finding that its performance is comparable to\nthat of other state-of-the-art unsupervised learning methods. Finally, we show\nthrough visualizations that the network learns units that are selective to\nobjects that are often associated with characteristic sounds. \n\n"}
{"id": "1608.07441", "contents": "Title: Hard Negative Mining for Metric Learning Based Zero-Shot Classification Abstract: Zero-Shot learning has been shown to be an efficient strategy for domain\nadaptation. In this context, this paper builds on the recent work of Bucher et\nal. [1], which proposed an approach to solve Zero-Shot classification problems\n(ZSC) by introducing a novel metric learning based objective function. This\nobjective function allows to learn an optimal embedding of the attributes\njointly with a measure of similarity between images and attributes. This paper\nextends their approach by proposing several schemes to control the generation\nof the negative pairs, resulting in a significant improvement of the\nperformance and giving above state-of-the-art results on three challenging ZSC\ndatasets. \n\n"}
{"id": "1608.07690", "contents": "Title: A Boundary Tilting Persepective on the Phenomenon of Adversarial\n  Examples Abstract: Deep neural networks have been shown to suffer from a surprising weakness:\ntheir classification outputs can be changed by small, non-random perturbations\nof their inputs. This adversarial example phenomenon has been explained as\noriginating from deep networks being \"too linear\" (Goodfellow et al., 2014). We\nshow here that the linear explanation of adversarial examples presents a number\nof limitations: the formal argument is not convincing, linear classifiers do\nnot always suffer from the phenomenon, and when they do their adversarial\nexamples are different from the ones affecting deep networks.\n  We propose a new perspective on the phenomenon. We argue that adversarial\nexamples exist when the classification boundary lies close to the submanifold\nof sampled data, and present a mathematical analysis of this new perspective in\nthe linear case. We define the notion of adversarial strength and show that it\ncan be reduced to the deviation angle between the classifier considered and the\nnearest centroid classifier. Then, we show that the adversarial strength can be\nmade arbitrarily high independently of the classification performance due to a\nmechanism that we call boundary tilting. This result leads us to defining a new\ntaxonomy of adversarial examples. Finally, we show that the adversarial\nstrength observed in practice is directly dependent on the level of\nregularisation used and the strongest adversarial examples, symptomatic of\noverfitting, can be avoided by using a proper level of regularisation. \n\n"}
{"id": "1608.07916", "contents": "Title: Vehicle Detection from 3D Lidar Using Fully Convolutional Network Abstract: Convolutional network techniques have recently achieved great success in\nvision based detection tasks. This paper introduces the recent development of\nour research on transplanting the fully convolutional network technique to the\ndetection tasks on 3D range scan data. Specifically, the scenario is set as the\nvehicle detection task from the range data of Velodyne 64E lidar. We proposes\nto present the data in a 2D point map and use a single 2D end-to-end fully\nconvolutional network to predict the objectness confidence and the bounding\nboxes simultaneously. By carefully design the bounding box encoding, it is able\nto predict full 3D bounding boxes even using a 2D convolutional network.\nExperiments on the KITTI dataset shows the state-of-the-art performance of the\nproposed method. \n\n"}
{"id": "1609.02907", "contents": "Title: Semi-Supervised Classification with Graph Convolutional Networks Abstract: We present a scalable approach for semi-supervised learning on\ngraph-structured data that is based on an efficient variant of convolutional\nneural networks which operate directly on graphs. We motivate the choice of our\nconvolutional architecture via a localized first-order approximation of\nspectral graph convolutions. Our model scales linearly in the number of graph\nedges and learns hidden layer representations that encode both local graph\nstructure and features of nodes. In a number of experiments on citation\nnetworks and on a knowledge graph dataset we demonstrate that our approach\noutperforms related methods by a significant margin. \n\n"}
{"id": "1609.02993", "contents": "Title: Episodic Exploration for Deep Deterministic Policies: An Application to\n  StarCraft Micromanagement Tasks Abstract: We consider scenarios from the real-time strategy game StarCraft as new\nbenchmarks for reinforcement learning algorithms. We propose micromanagement\ntasks, which present the problem of the short-term, low-level control of army\nmembers during a battle. From a reinforcement learning point of view, these\nscenarios are challenging because the state-action space is very large, and\nbecause there is no obvious feature representation for the state-action\nevaluation function. We describe our approach to tackle the micromanagement\nscenarios with deep neural network controllers from raw state features given by\nthe game engine. In addition, we present a heuristic reinforcement learning\nalgorithm which combines direct exploration in the policy space and\nbackpropagation. This algorithm allows for the collection of traces for\nlearning using deterministic policies, which appears much more efficient than,\nfor example, {\\epsilon}-greedy exploration. Experiments show that with this\nalgorithm, we successfully learn non-trivial strategies for scenarios with\narmies of up to 15 agents, where both Q-learning and REINFORCE struggle. \n\n"}
{"id": "1609.04802", "contents": "Title: Photo-Realistic Single Image Super-Resolution Using a Generative\n  Adversarial Network Abstract: Despite the breakthroughs in accuracy and speed of single image\nsuper-resolution using faster and deeper convolutional neural networks, one\ncentral problem remains largely unsolved: how do we recover the finer texture\ndetails when we super-resolve at large upscaling factors? The behavior of\noptimization-based super-resolution methods is principally driven by the choice\nof the objective function. Recent work has largely focused on minimizing the\nmean squared reconstruction error. The resulting estimates have high peak\nsignal-to-noise ratios, but they are often lacking high-frequency details and\nare perceptually unsatisfying in the sense that they fail to match the fidelity\nexpected at the higher resolution. In this paper, we present SRGAN, a\ngenerative adversarial network (GAN) for image super-resolution (SR). To our\nknowledge, it is the first framework capable of inferring photo-realistic\nnatural images for 4x upscaling factors. To achieve this, we propose a\nperceptual loss function which consists of an adversarial loss and a content\nloss. The adversarial loss pushes our solution to the natural image manifold\nusing a discriminator network that is trained to differentiate between the\nsuper-resolved images and original photo-realistic images. In addition, we use\na content loss motivated by perceptual similarity instead of similarity in\npixel space. Our deep residual network is able to recover photo-realistic\ntextures from heavily downsampled images on public benchmarks. An extensive\nmean-opinion-score (MOS) test shows hugely significant gains in perceptual\nquality using SRGAN. The MOS scores obtained with SRGAN are closer to those of\nthe original high-resolution images than to those obtained with any\nstate-of-the-art method. \n\n"}
{"id": "1609.04861", "contents": "Title: Visual Stability Prediction and Its Application to Manipulation Abstract: Understanding physical phenomena is a key competence that enables humans and\nanimals to act and interact under uncertain perception in previously unseen\nenvironments containing novel objects and their configurations. Developmental\npsychology has shown that such skills are acquired by infants from observations\nat a very early stage.\n  In this paper, we contrast a more traditional approach of taking a\nmodel-based route with explicit 3D representations and physical simulation by\nan {\\em end-to-end} approach that directly predicts stability from appearance.\nWe ask the question if and to what extent and quality such a skill can directly\nbe acquired in a data-driven way---bypassing the need for an explicit\nsimulation at run-time.\n  We present a learning-based approach based on simulated data that predicts\nstability of towers comprised of wooden blocks under different conditions and\nquantities related to the potential fall of the towers. We first evaluate the\napproach on synthetic data and compared the results to human judgments on the\nsame stimuli. Further, we extend this approach to reason about future states of\nsuch towers that in turn enables successful stacking. \n\n"}
{"id": "1609.05130", "contents": "Title: SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural\n  Networks Abstract: Ever more robust, accurate and detailed mapping using visual sensing has\nproven to be an enabling factor for mobile robots across a wide variety of\napplications. For the next level of robot intelligence and intuitive user\ninteraction, maps need extend beyond geometry and appearence - they need to\ncontain semantics. We address this challenge by combining Convolutional Neural\nNetworks (CNNs) and a state of the art dense Simultaneous Localisation and\nMapping (SLAM) system, ElasticFusion, which provides long-term dense\ncorrespondence between frames of indoor RGB-D video even during loopy scanning\ntrajectories. These correspondences allow the CNN's semantic predictions from\nmultiple view points to be probabilistically fused into a map. This not only\nproduces a useful semantic 3D map, but we also show on the NYUv2 dataset that\nfusing multiple predictions leads to an improvement even in the 2D semantic\nlabelling over baseline single frame predictions. We also show that for a\nsmaller reconstruction dataset with larger variation in prediction viewpoint,\nthe improvement over single frame segmentation increases. Our system is\nefficient enough to allow real-time interactive use at frame-rates of\napproximately 25Hz. \n\n"}
{"id": "1609.06666", "contents": "Title: Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient\n  Convolutional Neural Networks Abstract: This paper proposes a computationally efficient approach to detecting objects\nnatively in 3D point clouds using convolutional neural networks (CNNs). In\nparticular, this is achieved by leveraging a feature-centric voting scheme to\nimplement novel convolutional layers which explicitly exploit the sparsity\nencountered in the input. To this end, we examine the trade-off between\naccuracy and speed for different architectures and additionally propose to use\nan L1 penalty on the filter activations to further encourage sparsity in the\nintermediate representations. To the best of our knowledge, this is the first\nwork to propose sparse convolutional layers and L1 regularisation for efficient\nlarge-scale processing of 3D data. We demonstrate the efficacy of our approach\non the KITTI object detection benchmark and show that Vote3Deep models with as\nfew as three layers outperform the previous state of the art in both laser and\nlaser-vision based approaches by margins of up to 40% while remaining highly\ncompetitive in terms of processing time. \n\n"}
{"id": "1609.08965", "contents": "Title: Graph Based Convolutional Neural Network Abstract: The benefit of localized features within the regular domain has given rise to\nthe use of Convolutional Neural Networks (CNNs) in machine learning, with great\nproficiency in the image classification. The use of CNNs becomes problematic\nwithin the irregular spatial domain due to design and convolution of a kernel\nfilter being non-trivial. One solution to this problem is to utilize graph\nsignal processing techniques and the convolution theorem to perform\nconvolutions on the graph of the irregular domain to obtain feature map\nresponses to learnt filters. We propose graph convolution and pooling operators\nanalogous to those in the regular domain. We also provide gradient calculations\non the input data and spectral filters, which allow for the deep learning of an\nirregular spatial domain problem. Signal filters take the form of spectral\nmultipliers, applying convolution in the graph spectral domain. Applying smooth\nmultipliers results in localized convolutions in the spatial domain, with\nsmoother multipliers providing sharper feature maps. Algebraic Multigrid is\npresented as a graph pooling method, reducing the resolution of the graph\nthrough agglomeration of nodes between layers of the network. Evaluation of\nperformance on the MNIST digit classification problem in both the regular and\nirregular domain is presented, with comparison drawn to standard CNN. The\nproposed graph CNN provides a deep learning method for the irregular domains\npresent in the machine learning community, obtaining 94.23% on the regular\ngrid, and 94.96% on a spatially irregular subsampled MNIST. \n\n"}
{"id": "1610.03295", "contents": "Title: Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving Abstract: Autonomous driving is a multi-agent setting where the host vehicle must apply\nsophisticated negotiation skills with other road users when overtaking, giving\nway, merging, taking left and right turns and while pushing ahead in\nunstructured urban roadways. Since there are many possible scenarios, manually\ntackling all possible cases will likely yield a too simplistic policy.\nMoreover, one must balance between unexpected behavior of other\ndrivers/pedestrians and at the same time not to be too defensive so that normal\ntraffic flow is maintained.\n  In this paper we apply deep reinforcement learning to the problem of forming\nlong term driving strategies. We note that there are two major challenges that\nmake autonomous driving different from other robotic tasks. First, is the\nnecessity for ensuring functional safety - something that machine learning has\ndifficulty with given that performance is optimized at the level of an\nexpectation over many instances. Second, the Markov Decision Process model\noften used in robotics is problematic in our case because of unpredictable\nbehavior of other agents in this multi-agent scenario. We make three\ncontributions in our work. First, we show how policy gradient iterations can be\nused without Markovian assumptions. Second, we decompose the problem into a\ncomposition of a Policy for Desires (which is to be learned) and trajectory\nplanning with hard constraints (which is not learned). The goal of Desires is\nto enable comfort of driving, while hard constraints guarantees the safety of\ndriving. Third, we introduce a hierarchical temporal abstraction we call an\n\"Option Graph\" with a gating mechanism that significantly reduces the effective\nhorizon and thereby reducing the variance of the gradient estimation even\nfurther. \n\n"}
{"id": "1610.03518", "contents": "Title: Transfer from Simulation to Real World through Learning Deep Inverse\n  Dynamics Model Abstract: Developing control policies in simulation is often more practical and safer\nthan directly running experiments in the real world. This applies to policies\nobtained from planning and optimization, and even more so to policies obtained\nfrom reinforcement learning, which is often very data demanding. However, a\npolicy that succeeds in simulation often doesn't work when deployed on a real\nrobot. Nevertheless, often the overall gist of what the policy does in\nsimulation remains valid in the real world. In this paper we investigate such\nsettings, where the sequence of states traversed in simulation remains\nreasonable for the real world, even if the details of the controls are not, as\ncould be the case when the key differences lie in detailed friction, contact,\nmass and geometry properties. During execution, at each time step our approach\ncomputes what the simulation-based control policy would do, but then, rather\nthan executing these controls on the real robot, our approach computes what the\nsimulation expects the resulting next state(s) will be, and then relies on a\nlearned deep inverse dynamics model to decide which real-world action is most\nsuitable to achieve those next states. Deep models are only as good as their\ntraining data, and we also propose an approach for data collection to\n(incrementally) learn the deep inverse dynamics model. Our experiments shows\nour approach compares favorably with various baselines that have been developed\nfor dealing with simulation to real world model discrepancy, including output\nerror control and Gaussian dynamics adaptation. \n\n"}
{"id": "1610.03898", "contents": "Title: Semi-Coupled Two-Stream Fusion ConvNets for Action Recognition at\n  Extremely Low Resolutions Abstract: Deep convolutional neural networks (ConvNets) have been recently shown to\nattain state-of-the-art performance for action recognition on\nstandard-resolution videos. However, less attention has been paid to\nrecognition performance at extremely low resolutions (eLR) (e.g., 16 x 12\npixels). Reliable action recognition using eLR cameras would address privacy\nconcerns in various application environments such as private homes, hospitals,\nnursing/rehabilitation facilities, etc. In this paper, we propose a\nsemi-coupled filter-sharing network that leverages high resolution (HR) videos\nduring training in order to assist an eLR ConvNet. We also study methods for\nfusing spatial and temporal ConvNets customized for eLR videos in order to take\nadvantage of appearance and motion information. Our method outperforms\nstate-of-the-art methods at extremely low resolutions on IXMAS (93.7%) and HMDB\n(29.2%) datasets. \n\n"}
{"id": "1610.04575", "contents": "Title: Comparing Face Detection and Recognition Techniques Abstract: This paper implements and compares different techniques for face detection\nand recognition. One is find where the face is located in the images that is\nface detection and second is face recognition that is identifying the person.\nWe study three techniques in this paper: Face detection using self organizing\nmap (SOM), Face recognition by projection and nearest neighbor and Face\nrecognition using SVM. \n\n"}
{"id": "1610.06918", "contents": "Title: Learning to Protect Communications with Adversarial Neural Cryptography Abstract: We ask whether neural networks can learn to use secret keys to protect\ninformation from other neural networks. Specifically, we focus on ensuring\nconfidentiality properties in a multiagent system, and we specify those\nproperties in terms of an adversary. Thus, a system may consist of neural\nnetworks named Alice and Bob, and we aim to limit what a third neural network\nnamed Eve learns from eavesdropping on the communication between Alice and Bob.\nWe do not prescribe specific cryptographic algorithms to these neural networks;\ninstead, we train end-to-end, adversarially. We demonstrate that the neural\nnetworks can learn how to perform forms of encryption and decryption, and also\nhow to apply these operations selectively in order to meet confidentiality\ngoals. \n\n"}
{"id": "1610.07584", "contents": "Title: Learning a Probabilistic Latent Space of Object Shapes via 3D\n  Generative-Adversarial Modeling Abstract: We study the problem of 3D object generation. We propose a novel framework,\nnamely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects\nfrom a probabilistic space by leveraging recent advances in volumetric\nconvolutional networks and generative adversarial nets. The benefits of our\nmodel are three-fold: first, the use of an adversarial criterion, instead of\ntraditional heuristic criteria, enables the generator to capture object\nstructure implicitly and to synthesize high-quality 3D objects; second, the\ngenerator establishes a mapping from a low-dimensional probabilistic space to\nthe space of 3D objects, so that we can sample objects without a reference\nimage or CAD models, and explore the 3D object manifold; third, the adversarial\ndiscriminator provides a powerful 3D shape descriptor which, learned without\nsupervision, has wide applications in 3D object recognition. Experiments\ndemonstrate that our method generates high-quality 3D objects, and our\nunsupervisedly learned features achieve impressive performance on 3D object\nrecognition, comparable with those of supervised learning methods. \n\n"}
{"id": "1610.09204", "contents": "Title: Judging a Book By its Cover Abstract: Book covers communicate information to potential readers, but can that same\ninformation be learned by computers? We propose using a deep Convolutional\nNeural Network (CNN) to predict the genre of a book based on the visual clues\nprovided by its cover. The purpose of this research is to investigate whether\nrelationships between books and their covers can be learned. However,\ndetermining the genre of a book is a difficult task because covers can be\nambiguous and genres can be overarching. Despite this, we show that a CNN can\nextract features and learn underlying design rules set by the designer to\ndefine a genre. Using machine learning, we can bring the large amount of\nresources available to the book cover design process. In addition, we present a\nnew challenging dataset that can be used for many pattern recognition tasks. \n\n"}
{"id": "1611.00137", "contents": "Title: Embedding Deep Metric for Person Re-identication A Study Against Large\n  Variations Abstract: Person re-identification is challenging due to the large variations of pose,\nillumination, occlusion and camera view. Owing to these variations, the\npedestrian data is distributed as highly-curved manifolds in the feature space,\ndespite the current convolutional neural networks (CNN)'s capability of feature\nextraction. However, the distribution is unknown, so it is difficult to use the\ngeodesic distance when comparing two samples. In practice, the current deep\nembedding methods use the Euclidean distance for the training and test. On the\nother hand, the manifold learning methods suggest to use the Euclidean distance\nin the local range, combining with the graphical relationship between samples,\nfor approximating the geodesic distance. From this point of view, selecting\nsuitable positive i.e. intra-class) training samples within a local range is\ncritical for training the CNN embedding, especially when the data has large\nintra-class variations. In this paper, we propose a novel moderate positive\nsample mining method to train robust CNN for person re-identification, dealing\nwith the problem of large variation. In addition, we improve the learning by a\nmetric weight constraint, so that the learned metric has a better\ngeneralization ability. Experiments show that these two strategies are\neffective in learning robust deep metrics for person re-identification, and\naccordingly our deep model significantly outperforms the state-of-the-art\nmethods on several benchmarks of person re-identification. Therefore, the study\npresented in this paper may be useful in inspiring new designs of deep models\nfor person re-identification. \n\n"}
{"id": "1611.00625", "contents": "Title: TorchCraft: a Library for Machine Learning Research on Real-Time\n  Strategy Games Abstract: We present TorchCraft, a library that enables deep learning research on\nReal-Time Strategy (RTS) games such as StarCraft: Brood War, by making it\neasier to control these games from a machine learning framework, here Torch.\nThis white paper argues for using RTS games as a benchmark for AI research, and\ndescribes the design and components of TorchCraft. \n\n"}
{"id": "1611.00847", "contents": "Title: Deep Convolutional Neural Network Design Patterns Abstract: Recent research in the deep learning field has produced a plethora of new\narchitectures. At the same time, a growing number of groups are applying deep\nlearning to new applications. Some of these groups are likely to be composed of\ninexperienced deep learning practitioners who are baffled by the dizzying array\nof architecture choices and therefore opt to use an older architecture (i.e.,\nAlexnet). Here we attempt to bridge this gap by mining the collective knowledge\ncontained in recent deep learning research to discover underlying principles\nfor designing neural network architectures. In addition, we describe several\narchitectural innovations, including Fractal of FractalNet network, Stagewise\nBoosting Networks, and Taylor Series Networks (our Caffe code and prototxt\nfiles is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope\nothers are inspired to build on our preliminary work. \n\n"}
{"id": "1611.01224", "contents": "Title: Sample Efficient Actor-Critic with Experience Replay Abstract: This paper presents an actor-critic deep reinforcement learning agent with\nexperience replay that is stable, sample efficient, and performs remarkably\nwell on challenging environments, including the discrete 57-game Atari domain\nand several continuous control problems. To achieve this, the paper introduces\nseveral innovations, including truncated importance sampling with bias\ncorrection, stochastic dueling network architectures, and a new trust region\npolicy optimization method. \n\n"}
{"id": "1611.01236", "contents": "Title: Adversarial Machine Learning at Scale Abstract: Adversarial examples are malicious inputs designed to fool machine learning\nmodels. They often transfer from one model to another, allowing attackers to\nmount black box attacks without knowledge of the target model's parameters.\nAdversarial training is the process of explicitly training a model on\nadversarial examples, in order to make it more robust to attack or to reduce\nits test error on clean inputs. So far, adversarial training has primarily been\napplied to small problems. In this research, we apply adversarial training to\nImageNet. Our contributions include: (1) recommendations for how to succesfully\nscale adversarial training to large models and datasets, (2) the observation\nthat adversarial training confers robustness to single-step attack methods, (3)\nthe finding that multi-step attack methods are somewhat less transferable than\nsingle-step attack methods, so single-step attacks are the best for mounting\nblack-box attacks, and (4) resolution of a \"label leaking\" effect that causes\nadversarially trained models to perform better on adversarial examples than on\nclean examples, because the adversarial example construction process uses the\ntrue label and the model can learn to exploit regularities in the construction\nprocess. \n\n"}
{"id": "1611.01260", "contents": "Title: Learning Identity Mappings with Residual Gates Abstract: We propose a new layer design by adding a linear gating mechanism to shortcut\nconnections. By using a scalar parameter to control each gate, we provide a way\nto learn identity mappings by optimizing only one parameter. We build upon the\nmotivation behind Residual Networks, where a layer is reformulated in order to\nmake learning identity mappings less problematic to the optimizer. The\naugmentation introduces only one extra parameter per layer, and provides easier\noptimization by making degeneration into identity mappings simpler. We propose\na new model, the Gated Residual Network, which is the result when augmenting\nResidual Networks. Experimental results show that augmenting layers provides\nbetter optimization, increased performance, and more layer independence. We\nevaluate our method on MNIST using fully-connected networks, showing empirical\nindications that our augmentation facilitates the optimization of deep models,\nand that it provides high tolerance to full layer removal: the model retains\nover 90% of its performance even after half of its layers have been randomly\nremoved. We also evaluate our model on CIFAR-10 and CIFAR-100 using Wide Gated\nResNets, achieving 3.65% and 18.27% error, respectively. \n\n"}
{"id": "1611.01639", "contents": "Title: Robustly representing uncertainty in deep neural networks through\n  sampling Abstract: As deep neural networks (DNNs) are applied to increasingly challenging\nproblems, they will need to be able to represent their own uncertainty.\nModeling uncertainty is one of the key features of Bayesian methods. Using\nBernoulli dropout with sampling at prediction time has recently been proposed\nas an efficient and well performing variational inference method for DNNs.\nHowever, sampling from other multiplicative noise based variational\ndistributions has not been investigated in depth. We evaluated Bayesian DNNs\ntrained with Bernoulli or Gaussian multiplicative masking of either the units\n(dropout) or the weights (dropconnect). We tested the calibration of the\nprobabilistic predictions of Bayesian convolutional neural networks (CNNs) on\nMNIST and CIFAR-10. Sampling at prediction time increased the calibration of\nthe DNNs' probabalistic predictions. Sampling weights, whether Gaussian or\nBernoulli, led to more robust representation of uncertainty compared to\nsampling of units. However, using either Gaussian or Bernoulli dropout led to\nincreased test set classification accuracy. Based on these findings we used\nboth Bernoulli dropout and Gaussian dropconnect concurrently, which we show\napproximates the use of a spike-and-slab variational distribution without\nincreasing the number of learned parameters. We found that spike-and-slab\nsampling had higher test set performance than Gaussian dropconnect and more\nrobustly represented its uncertainty compared to Bernoulli dropout. \n\n"}
{"id": "1611.01673", "contents": "Title: Generative Multi-Adversarial Networks Abstract: Generative adversarial networks (GANs) are a framework for producing a\ngenerative model by way of a two-player minimax game. In this paper, we propose\nthe \\emph{Generative Multi-Adversarial Network} (GMAN), a framework that\nextends GANs to multiple discriminators. In previous work, the successful\ntraining of GANs requires modifying the minimax objective to accelerate\ntraining early on. In contrast, GMAN can be reliably trained with the original,\nuntampered objective. We explore a number of design perspectives with the\ndiscriminator role ranging from formidable adversary to forgiving teacher.\nImage generation tasks comparing the proposed framework to standard GANs\ndemonstrate GMAN produces higher quality samples in a fraction of the\niterations when measured by a pairwise GAM-type metric. \n\n"}
{"id": "1611.01982", "contents": "Title: Chinese/English mixed Character Segmentation as Semantic Segmentation Abstract: OCR character segmentation for multilingual printed documents is difficult\ndue to the diversity of different linguistic characters. Previous approaches\nmainly focus on monolingual texts and are not suitable for multilingual-lingual\ncases. In this work, we particularly tackle the Chinese/English mixed case by\nreframing it as a semantic segmentation problem. We take advantage of the\nsuccessful architecture called fully convolutional networks (FCN) in the field\nof semantic segmentation. Given a wide enough receptive field, FCN can utilize\nthe necessary context around a horizontal position to determinate whether this\nis a splitting point or not. As a deep neural architecture, FCN can\nautomatically learn useful features from raw text line images. Although trained\non synthesized samples with simulated random disturbance, our FCN model\ngeneralizes well to real-world samples. The experimental results show that our\nmodel significantly outperforms the previous methods. \n\n"}
{"id": "1611.02443", "contents": "Title: Domain Adaptation with L2 constraints for classifying images from\n  different endoscope systems Abstract: This paper proposes a method for domain adaptation that extends the maximum\nmargin domain transfer (MMDT) proposed by Hoffman et al., by introducing L2\ndistance constraints between samples of different domains; thus, our method is\ndenoted as MMDTL2. Motivated by the differences between the images taken by\nnarrow band imaging (NBI) endoscopic devices, we utilize different NBI devices\nas different domains and estimate the transformations between samples of\ndifferent domains, i.e., image samples taken by different NBI endoscope\nsystems. We first formulate the problem in the primal form, and then derive the\ndual form with much lesser computational costs as compared to the naive\napproach. From our experimental results using NBI image datasets from two\ndifferent NBI endoscopic devices, we find that MMDTL2 is better than MMDT and\nalso support vector machines without adaptation, especially when NBI image\nfeatures are high-dimensional and the per-class training samples are greater\nthan 20. \n\n"}
{"id": "1611.02639", "contents": "Title: Gradients of Counterfactuals Abstract: Gradients have been used to quantify feature importance in machine learning\nmodels. Unfortunately, in nonlinear deep networks, not only individual neurons\nbut also the whole network can saturate, and as a result an important input\nfeature can have a tiny gradient. We study various networks, and observe that\nthis phenomena is indeed widespread, across many inputs.\n  We propose to examine interior gradients, which are gradients of\ncounterfactual inputs constructed by scaling down the original input. We apply\nour method to the GoogleNet architecture for object recognition in images, as\nwell as a ligand-based virtual screening network with categorical features and\nan LSTM based language model for the Penn Treebank dataset. We visualize how\ninterior gradients better capture feature importance. Furthermore, interior\ngradients are applicable to a wide variety of deep networks, and have the\nattribution property that the feature importance scores sum to the the\nprediction score.\n  Best of all, interior gradients can be computed just as easily as gradients.\nIn contrast, previous methods are complex to implement, which hinders practical\nadoption. \n\n"}
{"id": "1611.02770", "contents": "Title: Delving into Transferable Adversarial Examples and Black-box Attacks Abstract: An intriguing property of deep neural networks is the existence of\nadversarial examples, which can transfer among different architectures. These\ntransferable adversarial examples may severely hinder deep neural network-based\napplications. Previous works mostly study the transferability using small scale\ndatasets. In this work, we are the first to conduct an extensive study of the\ntransferability over large models and a large scale dataset, and we are also\nthe first to study the transferability of targeted adversarial examples with\ntheir target labels. We study both non-targeted and targeted adversarial\nexamples, and show that while transferable non-targeted adversarial examples\nare easy to find, targeted adversarial examples generated using existing\napproaches almost never transfer with their target labels. Therefore, we\npropose novel ensemble-based approaches to generating transferable adversarial\nexamples. Using such approaches, we observe a large proportion of targeted\nadversarial examples that are able to transfer with their target labels for the\nfirst time. We also present some geometric studies to help understanding the\ntransferable adversarial examples. Finally, we show that the adversarial\nexamples generated using ensemble-based approaches can successfully attack\nClarifai.com, which is a black-box image classification system. \n\n"}
{"id": "1611.03218", "contents": "Title: Learning to Play Guess Who? and Inventing a Grounded Language as a\n  Consequence Abstract: Acquiring your first language is an incredible feat and not easily\nduplicated. Learning to communicate using nothing but a few pictureless books,\na corpus, would likely be impossible even for humans. Nevertheless, this is the\ndominating approach in most natural language processing today. As an\nalternative, we propose the use of situated interactions between agents as a\ndriving force for communication, and the framework of Deep Recurrent Q-Networks\nfor evolving a shared language grounded in the provided environment. We task\nthe agents with interactive image search in the form of the game Guess Who?.\nThe images from the game provide a non trivial environment for the agents to\ndiscuss and a natural grounding for the concepts they decide to encode in their\ncommunication. Our experiments show that the agents learn not only to encode\nphysical concepts in their words, i.e. grounding, but also that the agents\nlearn to hold a multi-step dialogue remembering the state of the dialogue from\nstep to step. \n\n"}
{"id": "1611.03673", "contents": "Title: Learning to Navigate in Complex Environments Abstract: Learning to navigate in complex environments with dynamic elements is an\nimportant milestone in developing AI agents. In this work we formulate the\nnavigation question as a reinforcement learning problem and show that data\nefficiency and task performance can be dramatically improved by relying on\nadditional auxiliary tasks leveraging multimodal sensory inputs. In particular\nwe consider jointly learning the goal-driven reinforcement learning problem\nwith auxiliary depth prediction and loop closure classification tasks. This\napproach can learn to navigate from raw sensory input in complicated 3D mazes,\napproaching human-level performance even under conditions where the goal\nlocation changes frequently. We provide detailed analysis of the agent\nbehaviour, its ability to localise, and its network activity dynamics, showing\nthat the agent implicitly learns key navigation abilities. \n\n"}
{"id": "1611.04201", "contents": "Title: CAD2RL: Real Single-Image Flight without a Single Real Image Abstract: Deep reinforcement learning has emerged as a promising and powerful technique\nfor automatically acquiring control policies that can process raw sensory\ninputs, such as images, and perform complex behaviors. However, extending deep\nRL to real-world robotic tasks has proven challenging, particularly in\nsafety-critical domains such as autonomous flight, where a trial-and-error\nlearning process is often impractical. In this paper, we explore the following\nquestion: can we train vision-based navigation policies entirely in simulation,\nand then transfer them into the real world to achieve real-world flight without\na single real training image? We propose a learning method that we call\nCAD$^2$RL, which can be used to perform collision-free indoor flight in the\nreal world while being trained entirely on 3D CAD models. Our method uses\nsingle RGB images from a monocular camera, without needing to explicitly\nreconstruct the 3D geometry of the environment or perform explicit motion\nplanning. Our learned collision avoidance policy is represented by a deep\nconvolutional neural network that directly processes raw monocular images and\noutputs velocity commands. This policy is trained entirely on simulated images,\nwith a Monte Carlo policy evaluation algorithm that directly optimizes the\nnetwork's ability to produce collision-free flight. By highly randomizing the\nrendering settings for our simulated training set, we show that we can train a\npolicy that generalizes to the real world, without requiring the simulator to\nbe particularly realistic or high-fidelity. We evaluate our method by flying a\nreal quadrotor through indoor environments, and further evaluate the design\nchoices in our simulator through a series of ablation studies on depth\nprediction. For supplementary video see: https://youtu.be/nXBWmzFrj5s \n\n"}
{"id": "1611.05109", "contents": "Title: Low-rank Bilinear Pooling for Fine-Grained Classification Abstract: Pooling second-order local feature statistics to form a high-dimensional\nbilinear feature has been shown to achieve state-of-the-art performance on a\nvariety of fine-grained classification tasks. To address the computational\ndemands of high feature dimensionality, we propose to represent the covariance\nfeatures as a matrix and apply a low-rank bilinear classifier. The resulting\nclassifier can be evaluated without explicitly computing the bilinear feature\nmap which allows for a large reduction in the compute time as well as\ndecreasing the effective number of parameters to be learned.\n  To further compress the model, we propose classifier co-decomposition that\nfactorizes the collection of bilinear classifiers into a common factor and\ncompact per-class terms. The co-decomposition idea can be deployed through two\nconvolutional layers and trained in an end-to-end architecture. We suggest a\nsimple yet effective initialization that avoids explicitly first training and\nfactorizing the larger bilinear classifiers. Through extensive experiments, we\nshow that our model achieves state-of-the-art performance on several public\ndatasets for fine-grained classification trained with only category labels.\nImportantly, our final model is an order of magnitude smaller than the recently\nproposed compact bilinear model, and three orders smaller than the standard\nbilinear CNN model. \n\n"}
{"id": "1611.05358", "contents": "Title: Lip Reading Sentences in the Wild Abstract: The goal of this work is to recognise phrases and sentences being spoken by a\ntalking face, with or without the audio. Unlike previous works that have\nfocussed on recognising a limited number of words or phrases, we tackle lip\nreading as an open-world problem - unconstrained natural language sentences,\nand in the wild videos.\n  Our key contributions are: (1) a 'Watch, Listen, Attend and Spell' (WLAS)\nnetwork that learns to transcribe videos of mouth motion to characters; (2) a\ncurriculum learning strategy to accelerate training and to reduce overfitting;\n(3) a 'Lip Reading Sentences' (LRS) dataset for visual speech recognition,\nconsisting of over 100,000 natural sentences from British television.\n  The WLAS model trained on the LRS dataset surpasses the performance of all\nprevious work on standard lip reading benchmark datasets, often by a\nsignificant margin. This lip reading performance beats a professional lip\nreader on videos from BBC television, and we also demonstrate that visual\ninformation helps to improve speech recognition performance even when the audio\nis available. \n\n"}
{"id": "1611.05397", "contents": "Title: Reinforcement Learning with Unsupervised Auxiliary Tasks Abstract: Deep reinforcement learning agents have achieved state-of-the-art results by\ndirectly maximising cumulative reward. However, environments contain a much\nwider variety of possible training signals. In this paper, we introduce an\nagent that also maximises many other pseudo-reward functions simultaneously by\nreinforcement learning. All of these tasks share a common representation that,\nlike unsupervised learning, continues to develop in the absence of extrinsic\nrewards. We also introduce a novel mechanism for focusing this representation\nupon extrinsic rewards, so that learning can rapidly adapt to the most relevant\naspects of the actual task. Our agent significantly outperforms the previous\nstate-of-the-art on Atari, averaging 880\\% expert human performance, and a\nchallenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks\nleading to a mean speedup in learning of 10$\\times$ and averaging 87\\% expert\nhuman performance on Labyrinth. \n\n"}
{"id": "1611.05644", "contents": "Title: Inverting The Generator Of A Generative Adversarial Network Abstract: Generative adversarial networks (GANs) learn to synthesise new samples from a\nhigh-dimensional distribution by passing samples drawn from a latent space\nthrough a generative network. When the high-dimensional distribution describes\nimages of a particular data set, the network should learn to generate visually\nsimilar image samples for latent variables that are close to each other in the\nlatent space. For tasks such as image retrieval and image classification, it\nmay be useful to exploit the arrangement of the latent space by projecting\nimages into it, and using this as a representation for discriminative tasks.\nGANs often consist of multiple layers of non-linear computations, making them\nvery difficult to invert. This paper introduces techniques for projecting image\nsamples into the latent space using any pre-trained GAN, provided that the\ncomputational graph is available. We evaluate these techniques on both MNIST\ndigits and Omniglot handwritten characters. In the case of MNIST digits, we\nshow that projections into the latent space maintain information about the\nstyle and the identity of the digit. In the case of Omniglot characters, we\nshow that even characters from alphabets that have not been seen during\ntraining may be projected well into the latent space; this suggests that this\napproach may have applications in one-shot learning. \n\n"}
{"id": "1611.05725", "contents": "Title: PolyNet: A Pursuit of Structural Diversity in Very Deep Networks Abstract: A number of studies have shown that increasing the depth or width of\nconvolutional networks is a rewarding approach to improve the performance of\nimage recognition. In our study, however, we observed difficulties along both\ndirections. On one hand, the pursuit for very deep networks is met with a\ndiminishing return and increased training difficulty; on the other hand,\nwidening a network would result in a quadratic growth in both computational\ncost and memory demand. These difficulties motivate us to explore structural\ndiversity in designing deep networks, a new dimension beyond just depth and\nwidth. Specifically, we present a new family of modules, namely the\nPolyInception, which can be flexibly inserted in isolation or in a composition\nas replacements of different parts of a network. Choosing PolyInception modules\nwith the guidance of architectural efficiency can improve the expressive power\nwhile preserving comparable computational cost. The Very Deep PolyNet, designed\nfollowing this direction, demonstrates substantial improvements over the\nstate-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2,\nit reduces the top-5 validation error on single crops from 4.9% to 4.25%, and\nthat on multi-crops from 3.7% to 3.45%. \n\n"}
{"id": "1611.05755", "contents": "Title: Cross-Domain Face Verification: Matching ID Document and Self-Portrait\n  Photographs Abstract: Cross-domain biometrics has been emerging as a new necessity, which poses\nseveral additional challenges, including harsh illumination changes, noise,\npose variation, among others. In this paper, we explore approaches to\ncross-domain face verification, comparing self-portrait photographs (\"selfies\")\nto ID documents. We approach the problem with proper image photometric\nadjustment and data standardization techniques, along with deep learning\nmethods to extract the most prominent features from the data, reducing the\neffects of domain shift in this problem. We validate the methods using a novel\ndataset comprising 50 individuals. The obtained results are promising and\nindicate that the adopted path is worth further investigation. \n\n"}
{"id": "1611.06624", "contents": "Title: Temporal Generative Adversarial Nets with Singular Value Clipping Abstract: In this paper, we propose a generative model, Temporal Generative Adversarial\nNets (TGAN), which can learn a semantic representation of unlabeled videos, and\nis capable of generating videos. Unlike existing Generative Adversarial Nets\n(GAN)-based methods that generate videos with a single generator consisting of\n3D deconvolutional layers, our model exploits two different types of\ngenerators: a temporal generator and an image generator. The temporal generator\ntakes a single latent variable as input and outputs a set of latent variables,\neach of which corresponds to an image frame in a video. The image generator\ntransforms a set of such latent variables into a video. To deal with\ninstability in training of GAN with such advanced networks, we adopt a recently\nproposed model, Wasserstein GAN, and propose a novel method to train it stably\nin an end-to-end manner. The experimental results demonstrate the effectiveness\nof our methods. \n\n"}
{"id": "1611.06950", "contents": "Title: Statistical Learning for OCR Text Correction Abstract: The accuracy of Optical Character Recognition (OCR) is crucial to the success\nof subsequent applications used in text analyzing pipeline. Recent models of\nOCR post-processing significantly improve the quality of OCR-generated text,\nbut are still prone to suggest correction candidates from limited observations\nwhile insufficiently accounting for the characteristics of OCR errors. In this\npaper, we show how to enlarge candidate suggestion space by using external\ncorpus and integrating OCR-specific features in a regression approach to\ncorrect OCR-generated errors. The evaluation results show that our model can\ncorrect 61.5% of the OCR-errors (considering the top 1 suggestion) and 71.5% of\nthe OCR-errors (considering the top 3 suggestions), for cases where the\ntheoretical correction upper-bound is 78%. \n\n"}
{"id": "1611.07004", "contents": "Title: Image-to-Image Translation with Conditional Adversarial Networks Abstract: We investigate conditional adversarial networks as a general-purpose solution\nto image-to-image translation problems. These networks not only learn the\nmapping from input image to output image, but also learn a loss function to\ntrain this mapping. This makes it possible to apply the same generic approach\nto problems that traditionally would require very different loss formulations.\nWe demonstrate that this approach is effective at synthesizing photos from\nlabel maps, reconstructing objects from edge maps, and colorizing images, among\nother tasks. Indeed, since the release of the pix2pix software associated with\nthis paper, a large number of internet users (many of them artists) have posted\ntheir own experiments with our system, further demonstrating its wide\napplicability and ease of adoption without the need for parameter tweaking. As\na community, we no longer hand-engineer our mapping functions, and this work\nsuggests we can achieve reasonable results without hand-engineering our loss\nfunctions either. \n\n"}
{"id": "1611.07385", "contents": "Title: Smart Library: Identifying Books in a Library using Richly Supervised\n  Deep Scene Text Reading Abstract: Physical library collections are valuable and long standing resources for\nknowledge and learning. However, managing books in a large bookshelf and\nfinding books on it often leads to tedious manual work, especially for large\nbook collections where books might be missing or misplaced. Recently, deep\nneural models, such as Convolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN) have achieved great success for scene text detection and\nrecognition. Motivated by these recent successes, we aim to investigate their\nviability in facilitating book management, a task that introduces further\nchallenges including large amounts of cluttered scene text, distortion, and\nvaried lighting conditions. In this paper, we present a library inventory\nbuilding and retrieval system based on scene text reading methods. We\nspecifically design our scene text recognition model using rich supervision to\naccelerate training and achieve state-of-the-art performance on several\nbenchmark datasets. Our proposed system has the potential to greatly reduce the\namount of human labor required in managing book inventories as well as the\nspace needed to store book information. \n\n"}
{"id": "1611.08050", "contents": "Title: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields Abstract: We present an approach to efficiently detect the 2D pose of multiple people\nin an image. The approach uses a nonparametric representation, which we refer\nto as Part Affinity Fields (PAFs), to learn to associate body parts with\nindividuals in the image. The architecture encodes global context, allowing a\ngreedy bottom-up parsing step that maintains high accuracy while achieving\nrealtime performance, irrespective of the number of people in the image. The\narchitecture is designed to jointly learn part locations and their association\nvia two branches of the same sequential prediction process. Our method placed\nfirst in the inaugural COCO 2016 keypoints challenge, and significantly exceeds\nthe previous state-of-the-art result on the MPII Multi-Person benchmark, both\nin performance and efficiency. \n\n"}
{"id": "1611.08991", "contents": "Title: Object Detection Free Instance Segmentation With Labeling\n  Transformations Abstract: Instance segmentation has attracted recent attention in computer vision and\nexisting methods in this domain mostly have an object detection stage. In this\npaper, we study the intrinsic challenge of the instance segmentation problem,\nthe presence of a quotient space (swapping the labels of different instances\nleads to the same result), and propose new methods that are object proposal-\nand object detection- free. We propose three alternative methods, namely\npixel-based affinity mapping, superpixel-based affinity learning, and\nboundary-based component segmentation, all focusing on performing labeling\ntransformations to cope with the quotient space problem. By adopting fully\nconvolutional neural networks (FCN) like models, our framework attains\ncompetitive results on both the PASCAL dataset (object-centric) and the Gland\ndataset (texture-centric), which the existing methods are not able to do. Our\nwork also has the advantages in its transparency, simplicity, and being all\nsegmentation based. \n\n"}
{"id": "1611.10080", "contents": "Title: Wider or Deeper: Revisiting the ResNet Model for Visual Recognition Abstract: The trend towards increasingly deep neural networks has been driven by a\ngeneral observation that increasing depth increases the performance of a\nnetwork. Recently, however, evidence has been amassing that simply increasing\ndepth may not be the best way to increase performance, particularly given other\nlimitations. Investigations into deep residual networks have also suggested\nthat they may not in fact be operating as a single deep network, but rather as\nan ensemble of many relatively shallow networks. We examine these issues, and\nin doing so arrive at a new interpretation of the unravelled view of deep\nresidual networks which explains some of the behaviours that have been observed\nexperimentally. As a result, we are able to derive a new, shallower,\narchitecture of residual networks which significantly outperforms much deeper\nmodels such as ResNet-200 on the ImageNet classification dataset. We also show\nthat this performance is transferable to other problem domains by developing a\nsemantic segmentation approach which outperforms the state-of-the-art by a\nremarkable margin on datasets including PASCAL VOC, PASCAL Context, and\nCityscapes. The architecture that we propose thus outperforms its comparators,\nincluding very deep ResNets, and yet is more efficient in memory use and\nsometimes also in training time. The code and models are available at\nhttps://github.com/itijyou/ademxapp \n\n"}
{"id": "1611.10152", "contents": "Title: Combining Data-driven and Model-driven Methods for Robust Facial\n  Landmark Detection Abstract: Facial landmark detection is an important yet challenging task for real-world\ncomputer vision applications. This paper proposes an effective and robust\napproach for facial landmark detection by combining data- and model-driven\nmethods. Firstly, a Fully Convolutional Network (FCN) is trained to compute\nresponse maps of all facial landmark points. Such a data-driven method could\nmake full use of holistic information in a facial image for global estimation\nof facial landmarks. After that, the maximum points in the response maps are\nfitted with a pre-trained Point Distribution Model (PDM) to generate the\ninitial facial shape. This model-driven method is able to correct the\ninaccurate locations of outliers by considering the shape prior information.\nFinally, a weighted version of Regularized Landmark Mean-Shift (RLMS) is\nemployed to fine-tune the facial shape iteratively. This\nEstimation-Correction-Tuning process perfectly combines the advantages of the\nglobal robustness of data-driven method (FCN), outlier correction capability of\nmodel-driven method (PDM) and non-parametric optimization of RLMS. Results of\nextensive experiments demonstrate that our approach achieves state-of-the-art\nperformances on challenging datasets including 300W, AFLW, AFW and COFW. The\nproposed method is able to produce satisfying detection results on face images\nwith exaggerated expressions, large head poses, and partial occlusions. \n\n"}
{"id": "1611.10242", "contents": "Title: Likelihood-free inference by ratio estimation Abstract: We consider the problem of parametric statistical inference when likelihood\ncomputations are prohibitively expensive but sampling from the model is\npossible. Several so-called likelihood-free methods have been developed to\nperform inference in the absence of a likelihood function. The popular\nsynthetic likelihood approach infers the parameters by modelling summary\nstatistics of the data by a Gaussian probability distribution. In another\npopular approach called approximate Bayesian computation, the inference is\nperformed by identifying parameter values for which the summary statistics of\nthe simulated data are close to those of the observed data. Synthetic\nlikelihood is easier to use as no measure of `closeness' is required but the\nGaussianity assumption is often limiting. Moreover, both approaches require\njudiciously chosen summary statistics. We here present an alternative inference\napproach that is as easy to use as synthetic likelihood but not as restricted\nin its assumptions, and that, in a natural way, enables automatic selection of\nrelevant summary statistic from a large set of candidates. The basic idea is to\nframe the problem of estimating the posterior as a problem of estimating the\nratio between the data generating distribution and the marginal distribution.\nThis problem can be solved by logistic regression, and including regularising\npenalty terms enables automatic selection of the summary statistics relevant to\nthe inference task. We illustrate the general theory on canonical examples and\nemploy it to perform inference for challenging stochastic nonlinear dynamical\nsystems and high-dimensional summary statistics. \n\n"}
{"id": "1612.00192", "contents": "Title: Flight Dynamics-based Recovery of a UAV Trajectory using Ground Cameras Abstract: We propose a new method to estimate the 6-dof trajectory of a flying object\nsuch as a quadrotor UAV within a 3D airspace monitored using multiple fixed\nground cameras. It is based on a new structure from motion formulation for the\n3D reconstruction of a single moving point with known motion dynamics. Our main\ncontribution is a new bundle adjustment procedure which in addition to\noptimizing the camera poses, regularizes the point trajectory using a prior\nbased on motion dynamics (or specifically flight dynamics). Furthermore, we can\ninfer the underlying control input sent to the UAV's autopilot that determined\nits flight trajectory.\n  Our method requires neither perfect single-view tracking nor appearance\nmatching across views. For robustness, we allow the tracker to generate\nmultiple detections per frame in each video. The true detections and the data\nassociation across videos is estimated using robust multi-view triangulation\nand subsequently refined during our bundle adjustment procedure. Quantitative\nevaluation on simulated data and experiments on real videos from indoor and\noutdoor scenes demonstrates the effectiveness of our method. \n\n"}
{"id": "1612.00334", "contents": "Title: A Theoretical Framework for Robustness of (Deep) Classifiers against\n  Adversarial Examples Abstract: Most machine learning classifiers, including deep neural networks, are\nvulnerable to adversarial examples. Such inputs are typically generated by\nadding small but purposeful modifications that lead to incorrect outputs while\nimperceptible to human eyes. The goal of this paper is not to introduce a\nsingle method, but to make theoretical steps towards fully understanding\nadversarial examples. By using concepts from topology, our theoretical analysis\nbrings forth the key reasons why an adversarial example can fool a classifier\n($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By\ninvestigating the topological relationship between two (pseudo)metric spaces\ncorresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and\nsufficient conditions that can determine if $f_1$ is always robust\n(strong-robust) against adversarial examples according to $f_2$. Interestingly\nour theorems indicate that just one unnecessary feature can make $f_1$ not\nstrong-robust, and the right feature representation learning is the key to\ngetting a classifier that is both accurate and strong-robust. \n\n"}
{"id": "1612.00380", "contents": "Title: Playing Doom with SLAM-Augmented Deep Reinforcement Learning Abstract: A number of recent approaches to policy learning in 2D game domains have been\nsuccessful going directly from raw input images to actions. However when\nemployed in complex 3D environments, they typically suffer from challenges\nrelated to partial observability, combinatorial exploration spaces, path\nplanning, and a scarcity of rewarding scenarios. Inspired from prior work in\nhuman cognition that indicates how humans employ a variety of semantic concepts\nand abstractions (object categories, localisation, etc.) to reason about the\nworld, we build an agent-model that incorporates such abstractions into its\npolicy-learning framework. We augment the raw image input to a Deep Q-Learning\nNetwork (DQN), by adding details of objects and structural elements\nencountered, along with the agent's localisation. The different components are\nautomatically extracted and composed into a topological representation using\non-the-fly object detection and 3D-scene reconstruction.We evaluate the\nefficacy of our approach in Doom, a 3D first-person combat game that exhibits a\nnumber of challenges discussed, and show that our augmented framework\nconsistently learns better, more effective policies. \n\n"}
{"id": "1612.00596", "contents": "Title: Learning to Search on Manifolds for 3D Pose Estimation of Articulated\n  Objects Abstract: This paper focuses on the challenging problem of 3D pose estimation of a\ndiverse spectrum of articulated objects from single depth images. A novel\nstructured prediction approach is considered, where 3D poses are represented as\nskeletal models that naturally operate on manifolds. Given an input depth\nimage, the problem of predicting the most proper articulation of underlying\nskeletal model is thus formulated as sequentially searching for the optimal\nskeletal configuration. This is subsequently addressed by convolutional neural\nnets trained end-to-end to render sequential prediction of the joint locations\nas regressing a set of tangent vectors of the underlying manifolds. Our\napproach is examined on various articulated objects including human hand,\nmouse, and fish benchmark datasets. Empirically it is shown to deliver highly\ncompetitive performance with respect to the state-of-the-arts, while operating\nin real-time (over 30 FPS). \n\n"}
{"id": "1612.01294", "contents": "Title: Message Passing Multi-Agent GANs Abstract: Communicating and sharing intelligence among agents is an important facet of\nachieving Artificial General Intelligence. As a first step towards this\nchallenge, we introduce a novel framework for image generation: Message Passing\nMulti-Agent Generative Adversarial Networks (MPM GANs). While GANs have\nrecently been shown to be very effective for image generation and other tasks,\nthese networks have been limited to mostly single generator-discriminator\nnetworks. We show that we can obtain multi-agent GANs that communicate through\nmessage passing to achieve better image generation. The objectives of the\nindividual agents in this framework are two fold: a co-operation objective and\na competing objective. The co-operation objective ensures that the message\nsharing mechanism guides the other generator to generate better than itself\nwhile the competing objective encourages each generator to generate better than\nits counterpart. We analyze and visualize the messages that these GANs share\namong themselves in various scenarios. We quantitatively show that the message\nsharing formulation serves as a regularizer for the adversarial training.\nQualitatively, we show that the different generators capture different traits\nof the underlying data distribution. \n\n"}
{"id": "1612.01294", "contents": "Title: Message Passing Multi-Agent GANs Abstract: Communicating and sharing intelligence among agents is an important facet of\nachieving Artificial General Intelligence. As a first step towards this\nchallenge, we introduce a novel framework for image generation: Message Passing\nMulti-Agent Generative Adversarial Networks (MPM GANs). While GANs have\nrecently been shown to be very effective for image generation and other tasks,\nthese networks have been limited to mostly single generator-discriminator\nnetworks. We show that we can obtain multi-agent GANs that communicate through\nmessage passing to achieve better image generation. The objectives of the\nindividual agents in this framework are two fold: a co-operation objective and\na competing objective. The co-operation objective ensures that the message\nsharing mechanism guides the other generator to generate better than itself\nwhile the competing objective encourages each generator to generate better than\nits counterpart. We analyze and visualize the messages that these GANs share\namong themselves in various scenarios. We quantitatively show that the message\nsharing formulation serves as a regularizer for the adversarial training.\nQualitatively, we show that the different generators capture different traits\nof the underlying data distribution. \n\n"}
{"id": "1612.02101", "contents": "Title: Bottom-Up Top-Down Cues for Weakly-Supervised Semantic Segmentation Abstract: We consider the task of learning a classifier for semantic segmentation using\nweak supervision in the form of image labels which specify the object classes\npresent in the image. Our method uses deep convolutional neural networks (CNNs)\nand adopts an Expectation-Maximization (EM) based approach. We focus on the\nfollowing three aspects of EM: (i) initialization; (ii) latent posterior\nestimation (E-step) and (iii) the parameter update (M-step). We show that\nsaliency and attention maps, our bottom-up and top-down cues respectively, of\nsimple images provide very good cues to learn an initialization for the\nEM-based algorithm. Intuitively, we show that before trying to learn to segment\ncomplex images, it is much easier and highly effective to first learn to\nsegment a set of simple images and then move towards the complex ones. Next, in\norder to update the parameters, we propose minimizing the combination of the\nstandard softmax loss and the KL divergence between the true latent posterior\nand the likelihood given by the CNN. We argue that this combination is more\nrobust to wrong predictions made by the expectation step of the EM method. We\nsupport this argument with empirical and visual results. Extensive experiments\nand discussions show that: (i) our method is very simple and intuitive; (ii)\nrequires only image-level labels; and (iii) consistently outperforms other\nweakly-supervised state-of-the-art methods with a very high margin on the\nPASCAL VOC 2012 dataset. \n\n"}
{"id": "1612.02374", "contents": "Title: Automatic Detection of ADHD and ASD from Expressive Behaviour in RGBD\n  Data Abstract: Attention Deficit Hyperactivity Disorder (ADHD) and Autism Spectrum Disorder\n(ASD) are neurodevelopmental conditions which impact on a significant number of\nchildren and adults. Currently, the diagnosis of such disorders is done by\nexperts who employ standard questionnaires and look for certain behavioural\nmarkers through manual observation. Such methods for their diagnosis are not\nonly subjective, difficult to repeat, and costly but also extremely time\nconsuming. In this work, we present a novel methodology to aid diagnostic\npredictions about the presence/absence of ADHD and ASD by automatic visual\nanalysis of a person's behaviour. To do so, we conduct the questionnaires in a\ncomputer-mediated way while recording participants with modern RGBD\n(Colour+Depth) sensors. In contrast to previous automatic approaches which have\nfocussed only detecting certain behavioural markers, our approach provides a\nfully automatic end-to-end system for directly predicting ADHD and ASD in\nadults. Using state of the art facial expression analysis based on Dynamic Deep\nLearning and 3D analysis of behaviour, we attain classification rates of 96%\nfor Controls vs Condition (ADHD/ASD) group and 94% for Comorbid (ADHD+ASD) vs\nASD only group. We show that our system is a potentially useful time saving\ncontribution to the diagnostic field of ADHD and ASD. \n\n"}
{"id": "1612.02401", "contents": "Title: DeMoN: Depth and Motion Network for Learning Monocular Stereo Abstract: In this paper we formulate structure from motion as a learning problem. We\ntrain a convolutional network end-to-end to compute depth and camera motion\nfrom successive, unconstrained image pairs. The architecture is composed of\nmultiple stacked encoder-decoder networks, the core part being an iterative\nnetwork that is able to improve its own predictions. The network estimates not\nonly depth and motion, but additionally surface normals, optical flow between\nthe images and confidence of the matching. A crucial component of the approach\nis a training loss based on spatial relative differences. Compared to\ntraditional two-frame structure from motion methods, results are more accurate\nand more robust. In contrast to the popular depth-from-single-image networks,\nDeMoN learns the concept of matching and, thus, better generalizes to\nstructures not seen during training. \n\n"}
{"id": "1612.03365", "contents": "Title: Multiple Instance Learning: A Survey of Problem Characteristics and\n  Applications Abstract: Multiple instance learning (MIL) is a form of weakly supervised learning\nwhere training instances are arranged in sets, called bags, and a label is\nprovided for the entire bag. This formulation is gaining interest because it\nnaturally fits various problems and allows to leverage weakly labeled data.\nConsequently, it has been used in diverse application fields such as computer\nvision and document classification. However, learning from bags raises\nimportant challenges that are unique to MIL. This paper provides a\ncomprehensive survey of the characteristics which define and differentiate the\ntypes of MIL problems. Until now, these problem characteristics have not been\nformally identified and described. As a result, the variations in performance\nof MIL algorithms from one data set to another are difficult to explain. In\nthis paper, MIL problem characteristics are grouped into four broad categories:\nthe composition of the bags, the types of data distribution, the ambiguity of\ninstance labels, and the task to be performed. Methods specialized to address\neach category are reviewed. Then, the extent to which these characteristics\nmanifest themselves in key MIL application areas are described. Finally,\nexperiments are conducted to compare the performance of 16 state-of-the-art MIL\nmethods on selected problem characteristics. This paper provides insight on how\nthe problem characteristics affect MIL algorithms, recommendations for future\nbenchmarking and promising avenues for research. \n\n"}
{"id": "1612.03557", "contents": "Title: Text-guided Attention Model for Image Captioning Abstract: Visual attention plays an important role to understand images and\ndemonstrates its effectiveness in generating natural language descriptions of\nimages. On the other hand, recent studies show that language associated with an\nimage can steer visual attention in the scene during our cognitive process.\nInspired by this, we introduce a text-guided attention model for image\ncaptioning, which learns to drive visual attention using associated captions.\nFor this model, we propose an exemplar-based learning approach that retrieves\nfrom training data associated captions with each image, and use them to learn\nattention on visual features. Our attention model enables to describe a\ndetailed state of scenes by distinguishing small or confusable objects\neffectively. We validate our model on MS-COCO Captioning benchmark and achieve\nthe state-of-the-art performance in standard metrics. \n\n"}
{"id": "1612.03928", "contents": "Title: Paying More Attention to Attention: Improving the Performance of\n  Convolutional Neural Networks via Attention Transfer Abstract: Attention plays a critical role in human visual experience. Furthermore, it\nhas recently been demonstrated that attention can also play an important role\nin the context of applying artificial neural networks to a variety of tasks\nfrom fields such as computer vision and NLP. In this work we show that, by\nproperly defining attention for convolutional neural networks, we can actually\nuse this type of information in order to significantly improve the performance\nof a student CNN network by forcing it to mimic the attention maps of a\npowerful teacher network. To that end, we propose several novel methods of\ntransferring attention, showing consistent improvement across a variety of\ndatasets and convolutional neural network architectures. Code and models for\nour experiments are available at\nhttps://github.com/szagoruyko/attention-transfer \n\n"}
{"id": "1612.04052", "contents": "Title: Theory and Tools for the Conversion of Analog to Spiking Convolutional\n  Neural Networks Abstract: Deep convolutional neural networks (CNNs) have shown great potential for\nnumerous real-world machine learning applications, but performing inference in\nlarge CNNs in real-time remains a challenge. We have previously demonstrated\nthat traditional CNNs can be converted into deep spiking neural networks\n(SNNs), which exhibit similar accuracy while reducing both latency and\ncomputational load as a consequence of their data-driven, event-based style of\ncomputing. Here we provide a novel theory that explains why this conversion is\nsuccessful, and derive from it several new tools to convert a larger and more\npowerful class of deep networks into SNNs. We identify the main sources of\napproximation errors in previous conversion methods, and propose simple\nmechanisms to fix these issues. Furthermore, we develop spiking implementations\nof common CNN operations such as max-pooling, softmax, and batch-normalization,\nwhich allow almost loss-less conversion of arbitrary CNN architectures into the\nspiking domain. Empirical evaluation of different network architectures on the\nMNIST and CIFAR10 benchmarks leads to the best SNN results reported to date. \n\n"}
{"id": "1612.04337", "contents": "Title: Fast Patch-based Style Transfer of Arbitrary Style Abstract: Artistic style transfer is an image synthesis problem where the content of an\nimage is reproduced with the style of another. Recent works show that a\nvisually appealing style transfer can be achieved by using the hidden\nactivations of a pretrained convolutional neural network. However, existing\nmethods either apply (i) an optimization procedure that works for any style\nimage but is very expensive, or (ii) an efficient feedforward network that only\nallows a limited number of trained styles. In this work we propose a simpler\noptimization objective based on local matching that combines the content\nstructure and style textures in a single layer of the pretrained network. We\nshow that our objective has desirable properties such as a simpler optimization\nlandscape, intuitive parameter tuning, and consistent frame-by-frame\nperformance on video. Furthermore, we use 80,000 natural images and 80,000\npaintings to train an inverse network that approximates the result of the\noptimization. This results in a procedure for artistic style transfer that is\nefficient but also allows arbitrary content and style images. \n\n"}
{"id": "1612.04642", "contents": "Title: Harmonic Networks: Deep Translation and Rotation Equivariance Abstract: Translating or rotating an input image should not affect the results of many\ncomputer vision tasks. Convolutional neural networks (CNNs) are already\ntranslation equivariant: input image translations produce proportionate feature\nmap translations. This is not the case for rotations. Global rotation\nequivariance is typically sought through data augmentation, but patch-wise\nequivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN\nexhibiting equivariance to patch-wise translation and 360-rotation. We achieve\nthis by replacing regular CNN filters with circular harmonics, returning a\nmaximal response and orientation for every receptive field patch.\n  H-Nets use a rich, parameter-efficient and low computational complexity\nrepresentation, and we show that deep feature maps within the network encode\ncomplicated rotational invariants. We demonstrate that our layers are general\nenough to be used in conjunction with the latest architectures and techniques,\nsuch as deep supervision and batch normalization. We also achieve\nstate-of-the-art classification on rotated-MNIST, and competitive results on\nother benchmark challenges. \n\n"}
{"id": "1612.04799", "contents": "Title: Deep Function Machines: Generalized Neural Networks for Topological\n  Layer Expression Abstract: In this paper we propose a generalization of deep neural networks called deep\nfunction machines (DFMs). DFMs act on vector spaces of arbitrary (possibly\ninfinite) dimension and we show that a family of DFMs are invariant to the\ndimension of input data; that is, the parameterization of the model does not\ndirectly hinge on the quality of the input (eg. high resolution images). Using\nthis generalization we provide a new theory of universal approximation of\nbounded non-linear operators between function spaces. We then suggest that DFMs\nprovide an expressive framework for designing new neural network layer types\nwith topological considerations in mind. Finally, we introduce a novel\narchitecture, RippLeNet, for resolution invariant computer vision, which\nempirically achieves state of the art invariance. \n\n"}
{"id": "1612.05424", "contents": "Title: Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial\n  Networks Abstract: Collecting well-annotated image datasets to train modern machine learning\nalgorithms is prohibitively expensive for many tasks. One appealing alternative\nis rendering synthetic data where ground-truth annotations are generated\nautomatically. Unfortunately, models trained purely on rendered images often\nfail to generalize to real images. To address this shortcoming, prior work\nintroduced unsupervised domain adaptation algorithms that attempt to map\nrepresentations between the two domains or learn to extract features that are\ndomain-invariant. In this work, we present a new approach that learns, in an\nunsupervised manner, a transformation in the pixel space from one domain to the\nother. Our generative adversarial network (GAN)-based method adapts\nsource-domain images to appear as if drawn from the target domain. Our approach\nnot only produces plausible samples, but also outperforms the state-of-the-art\non a number of unsupervised domain adaptation scenarios by large margins.\nFinally, we demonstrate that the adaptation process generalizes to object\nclasses unseen during training. \n\n"}
{"id": "1612.06704", "contents": "Title: Action-Driven Object Detection with Top-Down Visual Attentions Abstract: A dominant paradigm for deep learning based object detection relies on a\n\"bottom-up\" approach using \"passive\" scoring of class agnostic proposals. These\napproaches are efficient but lack of holistic analysis of scene-level context.\nIn this paper, we present an \"action-driven\" detection mechanism using our\n\"top-down\" visual attention model. We localize an object by taking sequential\nactions that the attention model provides. The attention model conditioned with\nan image region provides required actions to get closer toward a target object.\nAn action at each time step is weak itself but an ensemble of the sequential\nactions makes a bounding-box accurately converge to a target object boundary.\nThis attention model we call AttentionNet is composed of a convolutional neural\nnetwork. During our whole detection procedure, we only utilize the actions from\na single AttentionNet without any modules for object proposals nor post\nbounding-box regression. We evaluate our top-down detection mechanism over the\nPASCAL VOC series and ILSVRC CLS-LOC dataset, and achieve state-of-the-art\nperformances compared to the major bottom-up detection methods. In particular,\nour detection mechanism shows a strong advantage in elaborate localization by\noutperforming Faster R-CNN with a margin of +7.1% over PASCAL VOC 2007 when we\nincrease the IoU threshold for positive detection to 0.7. \n\n"}
{"id": "1612.06851", "contents": "Title: Beyond Skip Connections: Top-Down Modulation for Object Detection Abstract: In recent years, we have seen tremendous progress in the field of object\ndetection. Most of the recent improvements have been achieved by targeting\ndeeper feedforward networks. However, many hard object categories such as\nbottle, remote, etc. require representation of fine details and not just\ncoarse, semantic representations. But most of these fine details are lost in\nthe early convolutional layers. What we need is a way to incorporate finer\ndetails from lower layers into the detection architecture. Skip connections\nhave been proposed to combine high-level and low-level features, but we argue\nthat selecting the right features from low-level requires top-down contextual\ninformation. Inspired by the human visual pathway, in this paper we propose\ntop-down modulations as a way to incorporate fine details into the detection\nframework. Our approach supplements the standard bottom-up, feedforward ConvNet\nwith a top-down modulation (TDM) network, connected using lateral connections.\nThese connections are responsible for the modulation of lower layer filters,\nand the top-down network handles the selection and integration of contextual\ninformation and low-level features. The proposed TDM architecture provides a\nsignificant boost on the COCO testdev benchmark, achieving 28.6 AP for VGG16,\n35.2 AP for ResNet101, and 37.3 for InceptionResNetv2 network, without any\nbells and whistles (e.g., multi-scale, iterative box refinement, etc.). \n\n"}
{"id": "1612.07828", "contents": "Title: Learning from Simulated and Unsupervised Images through Adversarial\n  Training Abstract: With recent progress in graphics, it has become more tractable to train\nmodels on synthetic images, potentially avoiding the need for expensive\nannotations. However, learning from synthetic images may not achieve the\ndesired performance due to a gap between synthetic and real image\ndistributions. To reduce this gap, we propose Simulated+Unsupervised (S+U)\nlearning, where the task is to learn a model to improve the realism of a\nsimulator's output using unlabeled real data, while preserving the annotation\ninformation from the simulator. We develop a method for S+U learning that uses\nan adversarial network similar to Generative Adversarial Networks (GANs), but\nwith synthetic images as inputs instead of random vectors. We make several key\nmodifications to the standard GAN algorithm to preserve annotations, avoid\nartifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a\nlocal adversarial loss, and (iii) updating the discriminator using a history of\nrefined images. We show that this enables generation of highly realistic\nimages, which we demonstrate both qualitatively and with a user study. We\nquantitatively evaluate the generated images by training models for gaze\nestimation and hand pose estimation. We show a significant improvement over\nusing synthetic images, and achieve state-of-the-art results on the MPIIGaze\ndataset without any labeled real data. \n\n"}
{"id": "1612.08810", "contents": "Title: The Predictron: End-To-End Learning and Planning Abstract: One of the key challenges of artificial intelligence is to learn models that\nare effective in the context of planning. In this document we introduce the\npredictron architecture. The predictron consists of a fully abstract model,\nrepresented by a Markov reward process, that can be rolled forward multiple\n\"imagined\" planning steps. Each forward pass of the predictron accumulates\ninternal rewards and values over multiple planning depths. The predictron is\ntrained end-to-end so as to make these accumulated values accurately\napproximate the true value function. We applied the predictron to procedurally\ngenerated random mazes and a simulator for the game of pool. The predictron\nyielded significantly more accurate predictions than conventional deep neural\nnetwork architectures. \n\n"}
{"id": "1701.01546", "contents": "Title: Abnormal Event Detection in Videos using Spatiotemporal Autoencoder Abstract: We present an efficient method for detecting anomalies in videos. Recent\napplications of convolutional neural networks have shown promises of\nconvolutional layers for object detection and recognition, especially in\nimages. However, convolutional neural networks are supervised and require\nlabels as learning signals. We propose a spatiotemporal architecture for\nanomaly detection in videos including crowded scenes. Our architecture includes\ntwo main components, one for spatial feature representation, and one for\nlearning the temporal evolution of the spatial features. Experimental results\non Avenue, Subway and UCSD benchmarks confirm that the detection accuracy of\nour method is comparable to state-of-the-art methods at a considerable speed of\nup to 140 fps. \n\n"}
{"id": "1701.02892", "contents": "Title: Multivariate Regression with Grossly Corrupted Observations: A Robust\n  Approach and its Applications Abstract: This paper studies the problem of multivariate linear regression where a\nportion of the observations is grossly corrupted or is missing, and the\nmagnitudes and locations of such occurrences are unknown in priori. To deal\nwith this problem, we propose a new approach by explicitly consider the error\nsource as well as its sparseness nature. An interesting property of our\napproach lies in its ability of allowing individual regression output elements\nor tasks to possess their unique noise levels. Moreover, despite working with a\nnon-smooth optimization problem, our approach still guarantees to converge to\nits optimal solution. Experiments on synthetic data demonstrate the\ncompetitiveness of our approach compared with existing multivariate regression\nmodels. In addition, empirically our approach has been validated with very\npromising results on two exemplar real-world applications: The first concerns\nthe prediction of \\textit{Big-Five} personality based on user behaviors at\nsocial network sites (SNSs), while the second is 3D human hand pose estimation\nfrom depth images. The implementation of our approach and comparison methods as\nwell as the involved datasets are made publicly available in support of the\nopen-source and reproducible research initiatives. \n\n"}
{"id": "1701.03757", "contents": "Title: Deep Probabilistic Programming Abstract: We propose Edward, a Turing-complete probabilistic programming language.\nEdward defines two compositional representations---random variables and\ninference. By treating inference as a first class citizen, on a par with\nmodeling, we show that probabilistic programming can be as flexible and\ncomputationally efficient as traditional deep learning. For flexibility, Edward\nmakes it easy to fit the same model using a variety of composable inference\nmethods, ranging from point estimation to variational inference to MCMC. In\naddition, Edward can reuse the modeling representation as part of inference,\nfacilitating the design of rich variational models and generative adversarial\nnetworks. For efficiency, Edward is integrated into TensorFlow, providing\nsignificant speedups over existing probabilistic systems. For example, we show\non a benchmark logistic regression task that Edward is at least 35x faster than\nStan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it\nis as fast as handwritten TensorFlow. \n\n"}
{"id": "1701.04752", "contents": "Title: 3D Reconstruction of Simple Objects from A Single View Silhouette Image Abstract: While recent deep neural networks have achieved promising results for 3D\nreconstruction from a single-view image, these rely on the availability of RGB\ntextures in images and extra information as supervision. In this work, we\npropose novel stacked hierarchical networks and an end to end training strategy\nto tackle a more challenging task for the first time, 3D reconstruction from a\nsingle-view 2D silhouette image. We demonstrate that our model is able to\nconduct 3D reconstruction from a single-view silhouette image both\nqualitatively and quantitatively. Evaluation is performed using Shapenet for\nthe single-view reconstruction and results are presented in comparison with a\nsingle network, to highlight the improvements obtained with the proposed\nstacked networks and the end to end training strategy. Furthermore, 3D re-\nconstruction in forms of IoU is compared with the state of art 3D\nreconstruction from a single-view RGB image, and the proposed model achieves\nhigher IoU than the state of art of reconstruction from a single view RGB\nimage. \n\n"}
{"id": "1701.06190", "contents": "Title: A New Convolutional Network-in-Network Structure and Its Applications in\n  Skin Detection, Semantic Segmentation, and Artifact Reduction Abstract: The inception network has been shown to provide good performance on image\nclassification problems, but there are not much evidences that it is also\neffective for the image restoration or pixel-wise labeling problems. For image\nrestoration problems, the pooling is generally not used because the decimated\nfeatures are not helpful for the reconstruction of an image as the output.\nMoreover, most deep learning architectures for the restoration problems do not\nuse dense prediction that need lots of training parameters. From these\nobservations, for enjoying the performance of inception-like structure on the\nimage based problems we propose a new convolutional network-in-network\nstructure. The proposed network can be considered a modification of inception\nstructure where pool projection and pooling layer are removed for maintaining\nthe entire feature map size, and a larger kernel filter is added instead.\nProposed network greatly reduces the number of parameters on account of removed\ndense prediction and pooling, which is an advantage, but may also reduce the\nreceptive field in each layer. Hence, we add a larger kernel than the original\ninception structure for not increasing the depth of layers. The proposed\nstructure is applied to typical image-to-image learning problems, i.e., the\nproblems where the size of input and output are same such as skin detection,\nsemantic segmentation, and compression artifacts reduction. Extensive\nexperiments show that the proposed network brings comparable or better results\nthan the state-of-the-art convolutional neural networks for these problems. \n\n"}
{"id": "1701.07875", "contents": "Title: Wasserstein GAN Abstract: We introduce a new algorithm named WGAN, an alternative to traditional GAN\ntraining. In this new model, we show that we can improve the stability of\nlearning, get rid of problems like mode collapse, and provide meaningful\nlearning curves useful for debugging and hyperparameter searches. Furthermore,\nwe show that the corresponding optimization problem is sound, and provide\nextensive theoretical work highlighting the deep connections to other distances\nbetween distributions. \n\n"}
{"id": "1701.08810", "contents": "Title: Reinforcement Learning Algorithm Selection Abstract: This paper formalises the problem of online algorithm selection in the\ncontext of Reinforcement Learning. The setup is as follows: given an episodic\ntask and a finite number of off-policy RL algorithms, a meta-algorithm has to\ndecide which RL algorithm is in control during the next episode so as to\nmaximize the expected return. The article presents a novel meta-algorithm,\ncalled Epochal Stochastic Bandit Algorithm Selection (ESBAS). Its principle is\nto freeze the policy updates at each epoch, and to leave a rebooted stochastic\nbandit in charge of the algorithm selection. Under some assumptions, a thorough\ntheoretical analysis demonstrates its near-optimality considering the\nstructural sampling budget limitations. ESBAS is first empirically evaluated on\na dialogue task where it is shown to outperform each individual algorithm in\nmost configurations. ESBAS is then adapted to a true online setting where\nalgorithms update their policies after each transition, which we call SSBAS.\nSSBAS is evaluated on a fruit collection task where it is shown to adapt the\nstepsize parameter more efficiently than the classical hyperbolic decay, and on\nan Atari game, where it improves the performance by a wide margin. \n\n"}
{"id": "1702.01847", "contents": "Title: Low Rank Matrix Recovery with Simultaneous Presence of Outliers and\n  Sparse Corruption Abstract: We study a data model in which the data matrix D can be expressed as D = L +\nS + C, where L is a low rank matrix, S an element-wise sparse matrix and C a\nmatrix whose non-zero columns are outlying data points. To date, robust PCA\nalgorithms have solely considered models with either S or C, but not both. As\nsuch, existing algorithms cannot account for simultaneous element-wise and\ncolumn-wise corruptions. In this paper, a new robust PCA algorithm that is\nrobust to simultaneous types of corruption is proposed. Our approach hinges on\nthe sparse approximation of a sparsely corrupted column so that the sparse\nexpansion of a column with respect to the other data points is used to\ndistinguish a sparsely corrupted inlier column from an outlying data point. We\nalso develop a randomized design which provides a scalable implementation of\nthe proposed approach. The core idea of sparse approximation is analyzed\nanalytically where we show that the underlying ell_1-norm minimization can\nobtain the representation of an inlier in presence of sparse corruptions. \n\n"}
{"id": "1702.03037", "contents": "Title: Multi-agent Reinforcement Learning in Sequential Social Dilemmas Abstract: Matrix games like Prisoner's Dilemma have guided research on social dilemmas\nfor decades. However, they necessarily treat the choice to cooperate or defect\nas an atomic action. In real-world social dilemmas these choices are temporally\nextended. Cooperativeness is a property that applies to policies, not\nelementary actions. We introduce sequential social dilemmas that share the\nmixed incentive structure of matrix game social dilemmas but also require\nagents to learn policies that implement their strategic intentions. We analyze\nthe dynamics of policies learned by multiple self-interested independent\nlearning agents, each using its own deep Q-network, on two Markov games we\nintroduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We\ncharacterize how learned behavior in each domain changes as a function of\nenvironmental factors including resource abundance. Our experiments show how\nconflict can emerge from competition over shared resources and shed light on\nhow the sequential nature of real world social dilemmas affects cooperation. \n\n"}
{"id": "1702.03435", "contents": "Title: Distributed Mapping with Privacy and Communication Constraints:\n  Lightweight Algorithms and Object-based Models Abstract: We consider the following problem: a team of robots is deployed in an unknown\nenvironment and it has to collaboratively build a map of the area without a\nreliable infrastructure for communication. The backbone for modern mapping\ntechniques is pose graph optimization, which estimates the trajectory of the\nrobots, from which the map can be easily built. The first contribution of this\npaper is a set of distributed algorithms for pose graph optimization: rather\nthan sending all sensor data to a remote sensor fusion server, the robots\nexchange very partial and noisy information to reach an agreement on the pose\ngraph configuration. Our approach can be considered as a distributed\nimplementation of the two-stage approach of Carlone et al., where we use the\nSuccessive Over-Relaxation (SOR) and the Jacobi Over-Relaxation (JOR) as\nworkhorses to split the computation among the robots. As a second contribution,\nwe extend %and demonstrate the applicability of the proposed distributed\nalgorithms to work with object-based map models. The use of object-based models\navoids the exchange of raw sensor measurements (e.g., point clouds) further\nreducing the communication burden. Our third contribution is an extensive\nexperimental evaluation of the proposed techniques, including tests in\nrealistic Gazebo simulations and field experiments in a military test facility.\nAbundant experimental evidence suggests that one of the proposed algorithms\n(the Distributed Gauss-Seidel method or DGS) has excellent performance. The DGS\nrequires minimal information exchange, has an anytime flavor, scales well to\nlarge teams, is robust to noise, and is easy to implement. Our field tests show\nthat the combined use of our distributed algorithms and object-based models\nreduces the communication requirements by several orders of magnitude and\nenables distributed mapping with large teams of robots in real-world problems. \n\n"}
{"id": "1702.03920", "contents": "Title: Cognitive Mapping and Planning for Visual Navigation Abstract: We introduce a neural architecture for navigation in novel environments. Our\nproposed architecture learns to map from first-person views and plans a\nsequence of actions towards goals in the environment. The Cognitive Mapper and\nPlanner (CMP) is based on two key ideas: a) a unified joint architecture for\nmapping and planning, such that the mapping is driven by the needs of the task,\nand b) a spatial memory with the ability to plan given an incomplete set of\nobservations about the world. CMP constructs a top-down belief map of the world\nand applies a differentiable neural net planner to produce the next action at\neach time step. The accumulated belief of the world enables the agent to track\nvisited regions of the environment. We train and test CMP on navigation\nproblems in simulation environments derived from scans of real world buildings.\nOur experiments demonstrate that CMP outperforms alternate learning-based\narchitectures, as well as, classical mapping and path planning approaches in\nmany cases. Furthermore, it naturally extends to semantically specified goals,\nsuch as 'going to a chair'. We also deploy CMP on physical robots in indoor\nenvironments, where it achieves reasonable performance, even though it is\ntrained entirely in simulation. \n\n"}
{"id": "1702.04593", "contents": "Title: Deep Multi-camera People Detection Abstract: This paper addresses the problem of multi-view people occupancy map\nestimation. Existing solutions for this problem either operate per-view, or\nrely on a background subtraction pre-processing. Both approaches lessen the\ndetection performance as scenes become more crowded. The former does not\nexploit joint information, whereas the latter deals with ambiguous input due to\nthe foreground blobs becoming more and more interconnected as the number of\ntargets increases.\n  Although deep learning algorithms have proven to excel on remarkably numerous\ncomputer vision tasks, such a method has not been applied yet to this problem.\nIn large part this is due to the lack of large-scale multi-camera data-set.\n  The core of our method is an architecture which makes use of monocular\npedestrian data-set, available at larger scale then the multi-view ones,\napplies parallel processing to the multiple video streams, and jointly utilises\nit. Our end-to-end deep learning method outperforms existing methods by large\nmargins on the commonly used PETS 2009 data-set. Furthermore, we make publicly\navailable a new three-camera HD data-set. Our source code and trained models\nwill be made available under an open-source license. \n\n"}
{"id": "1702.06280", "contents": "Title: On the (Statistical) Detection of Adversarial Examples Abstract: Machine Learning (ML) models are applied in a variety of tasks such as\nnetwork intrusion detection or Malware classification. Yet, these models are\nvulnerable to a class of malicious inputs known as adversarial examples. These\nare slightly perturbed inputs that are classified incorrectly by the ML model.\nThe mitigation of these adversarial inputs remains an open problem. As a step\ntowards understanding adversarial examples, we show that they are not drawn\nfrom the same distribution than the original data, and can thus be detected\nusing statistical tests. Using thus knowledge, we introduce a complimentary\napproach to identify specific inputs that are adversarial. Specifically, we\naugment our ML model with an additional output, in which the model is trained\nto classify all adversarial inputs. We evaluate our approach on multiple\nadversarial example crafting methods (including the fast gradient sign and\nsaliency map methods) with several datasets. The statistical test flags sample\nsets containing adversarial inputs confidently at sample sizes between 10 and\n100 data points. Furthermore, our augmented model either detects adversarial\nexamples as outliers with high accuracy (> 80%) or increases the adversary's\ncost - the perturbation added - by more than 150%. In this way, we show that\nstatistical properties of adversarial examples are essential to their\ndetection. \n\n"}
{"id": "1702.06506", "contents": "Title: PixelNet: Representation of the pixels, by the pixels, and for the\n  pixels Abstract: We explore design principles for general pixel-level prediction problems,\nfrom low-level edge detection to mid-level surface normal estimation to\nhigh-level semantic segmentation. Convolutional predictors, such as the\nfully-convolutional network (FCN), have achieved remarkable success by\nexploiting the spatial redundancy of neighboring pixels through convolutional\nprocessing. Though computationally efficient, we point out that such approaches\nare not statistically efficient during learning precisely because spatial\nredundancy limits the information learned from neighboring pixels. We\ndemonstrate that stratified sampling of pixels allows one to (1) add diversity\nduring batch updates, speeding up learning; (2) explore complex nonlinear\npredictors, improving accuracy; and (3) efficiently train state-of-the-art\nmodels tabula rasa (i.e., \"from scratch\") for diverse pixel-labeling tasks. Our\nsingle architecture produces state-of-the-art results for semantic segmentation\non PASCAL-Context dataset, surface normal estimation on NYUDv2 depth dataset,\nand edge detection on BSDS. \n\n"}
{"id": "1702.06506", "contents": "Title: PixelNet: Representation of the pixels, by the pixels, and for the\n  pixels Abstract: We explore design principles for general pixel-level prediction problems,\nfrom low-level edge detection to mid-level surface normal estimation to\nhigh-level semantic segmentation. Convolutional predictors, such as the\nfully-convolutional network (FCN), have achieved remarkable success by\nexploiting the spatial redundancy of neighboring pixels through convolutional\nprocessing. Though computationally efficient, we point out that such approaches\nare not statistically efficient during learning precisely because spatial\nredundancy limits the information learned from neighboring pixels. We\ndemonstrate that stratified sampling of pixels allows one to (1) add diversity\nduring batch updates, speeding up learning; (2) explore complex nonlinear\npredictors, improving accuracy; and (3) efficiently train state-of-the-art\nmodels tabula rasa (i.e., \"from scratch\") for diverse pixel-labeling tasks. Our\nsingle architecture produces state-of-the-art results for semantic segmentation\non PASCAL-Context dataset, surface normal estimation on NYUDv2 depth dataset,\nand edge detection on BSDS. \n\n"}
{"id": "1702.06674", "contents": "Title: Unsupervised Diverse Colorization via Generative Adversarial Networks Abstract: Colorization of grayscale images has been a hot topic in computer vision.\nPrevious research mainly focuses on producing a colored image to match the\noriginal one. However, since many colors share the same gray value, an input\ngrayscale image could be diversely colored while maintaining its reality. In\nthis paper, we design a novel solution for unsupervised diverse colorization.\nSpecifically, we leverage conditional generative adversarial networks to model\nthe distribution of real-world item colors, in which we develop a fully\nconvolutional generator with multi-layer noise to enhance diversity, with\nmulti-layer condition concatenation to maintain reality, and with stride 1 to\nkeep spatial information. With such a novel network architecture, the model\nyields highly competitive performance on the open LSUN bedroom dataset. The\nTuring test of 80 humans further indicates our generated color schemes are\nhighly convincible. \n\n"}
{"id": "1702.06763", "contents": "Title: DeepCloak: Masking Deep Neural Network Models for Robustness Against\n  Adversarial Samples Abstract: Recent studies have shown that deep neural networks (DNN) are vulnerable to\nadversarial samples: maliciously-perturbed samples crafted to yield incorrect\nmodel outputs. Such attacks can severely undermine DNN systems, particularly in\nsecurity-sensitive settings. It was observed that an adversary could easily\ngenerate adversarial samples by making a small perturbation on irrelevant\nfeature dimensions that are unnecessary for the current classification task. To\novercome this problem, we introduce a defensive mechanism called DeepCloak. By\nidentifying and removing unnecessary features in a DNN model, DeepCloak limits\nthe capacity an attacker can use generating adversarial samples and therefore\nincrease the robustness against such inputs. Comparing with other defensive\napproaches, DeepCloak is easy to implement and computationally efficient.\nExperimental results show that DeepCloak can increase the performance of\nstate-of-the-art DNN models against adversarial samples. \n\n"}
{"id": "1702.06813", "contents": "Title: RenderMap: Exploiting the Link Between Perception and Rendering for\n  Dense Mapping Abstract: We introduce an approach for the real-time (2Hz) creation of a dense map and\nalignment of a moving robotic agent within that map by rendering using a\nGraphics Processing Unit (GPU). This is done by recasting the scan alignment\npart of the dense mapping process as a rendering task. Alignment errors are\ncomputed from rendering the scene, comparing with range data from the sensors,\nand minimized by an optimizer. The proposed approach takes advantage of the\nadvances in rendering techniques for computer graphics and GPU hardware to\naccelerate the algorithm. Moreover, it allows one to exploit information not\nused in classic dense mapping algorithms such as Iterative Closest Point (ICP)\nby rendering interfaces between the free space, occupied space and the unknown.\nThe proposed approach leverages directly the rendering capabilities of the GPU,\nin contrast to other GPU-based approaches that deploy the GPU as a general\npurpose parallel computation platform.\n  We argue that the proposed concept is a general consequence of treating\nperception problems as inverse problems of rendering. Many perception problems\ncan be recast into a form where much of the computation is replaced by render\noperations. This is not only efficient since rendering is fast, but also\nsimpler to implement and will naturally benefit from future advancements in GPU\nspeed and rendering techniques. Furthermore, this general concept can go beyond\naddressing perception problems and can be used for other problem domains such\nas path planning. \n\n"}
{"id": "1702.07392", "contents": "Title: WaterGAN: Unsupervised Generative Network to Enable Real-time Color\n  Correction of Monocular Underwater Images Abstract: This paper reports on WaterGAN, a generative adversarial network (GAN) for\ngenerating realistic underwater images from in-air image and depth pairings in\nan unsupervised pipeline used for color correction of monocular underwater\nimages. Cameras onboard autonomous and remotely operated vehicles can capture\nhigh resolution images to map the seafloor, however, underwater image formation\nis subject to the complex process of light propagation through the water\ncolumn. The raw images retrieved are characteristically different than images\ntaken in air due to effects such as absorption and scattering, which cause\nattenuation of light at different rates for different wavelengths. While this\nphysical process is well described theoretically, the model depends on many\nparameters intrinsic to the water column as well as the objects in the scene.\nThese factors make recovery of these parameters difficult without simplifying\nassumptions or field calibration, hence, restoration of underwater images is a\nnon-trivial problem. Deep learning has demonstrated great success in modeling\ncomplex nonlinear systems but requires a large amount of training data, which\nis difficult to compile in deep sea environments. Using WaterGAN, we generate a\nlarge training dataset of paired imagery, both raw underwater and true color\nin-air, as well as depth data. This data serves as input to a novel end-to-end\nnetwork for color correction of monocular underwater images. Due to the\ndepth-dependent water column effects inherent to underwater environments, we\nshow that our end-to-end network implicitly learns a coarse depth estimate of\nthe underwater scene from monocular underwater images. Our proposed pipeline is\nvalidated with testing on real data collected from both a pure water tank and\nfrom underwater surveys in field testing. Source code is made publicly\navailable with sample datasets and pretrained models. \n\n"}
{"id": "1702.07836", "contents": "Title: Synthesizing Training Data for Object Detection in Indoor Scenes Abstract: Detection of objects in cluttered indoor environments is one of the key\nenabling functionalities for service robots. The best performing object\ndetection approaches in computer vision exploit deep Convolutional Neural\nNetworks (CNN) to simultaneously detect and categorize the objects of interest\nin cluttered scenes. Training of such models typically requires large amounts\nof annotated training data which is time consuming and costly to obtain. In\nthis work we explore the ability of using synthetically generated composite\nimages for training state-of-the-art object detectors, especially for object\ninstance detection. We superimpose 2D images of textured object models into\nimages of real environments at variety of locations and scales. Our experiments\nevaluate different superimposition strategies ranging from purely image-based\nblending all the way to depth and semantics informed positioning of the object\nmodels into real scenes. We demonstrate the effectiveness of these object\ndetector training strategies on two publicly available datasets, the\nGMU-Kitchens and the Washington RGB-D Scenes v2. As one observation, augmenting\nsome hand-labeled training data with synthetic examples carefully composed onto\nscenes yields object detectors with comparable performance to using much more\nhand-labeled data. Broadly, this work charts new opportunities for training\ndetectors for new objects by exploiting existing object model repositories in\neither a purely automatic fashion or with only a very small number of\nhuman-annotated examples. \n\n"}
{"id": "1702.08014", "contents": "Title: Adversarial Networks for the Detection of Aggressive Prostate Cancer Abstract: Semantic segmentation constitutes an integral part of medical image analyses\nfor which breakthroughs in the field of deep learning were of high relevance.\nThe large number of trainable parameters of deep neural networks however\nrenders them inherently data hungry, a characteristic that heavily challenges\nthe medical imaging community. Though interestingly, with the de facto standard\ntraining of fully convolutional networks (FCNs) for semantic segmentation being\nagnostic towards the `structure' of the predicted label maps, valuable\ncomplementary information about the global quality of the segmentation lies\nidle. In order to tap into this potential, we propose utilizing an adversarial\nnetwork which discriminates between expert and generated annotations in order\nto train FCNs for semantic segmentation. Because the adversary constitutes a\nlearned parametrization of what makes a good segmentation at a global level, we\nhypothesize that the method holds particular advantages for segmentation tasks\non complex structured, small datasets. This holds true in our experiments: We\nlearn to segment aggressive prostate cancer utilizing MRI images of 152\npatients and show that the proposed scheme is superior over the de facto\nstandard in terms of the detection sensitivity and the dice-score for\naggressive prostate cancer. The achieved relative gains are shown to be\nparticularly pronounced in the small dataset limit. \n\n"}
{"id": "1702.08887", "contents": "Title: Stabilising Experience Replay for Deep Multi-Agent Reinforcement\n  Learning Abstract: Many real-world problems, such as network packet routing and urban traffic\ncontrol, are naturally modeled as multi-agent reinforcement learning (RL)\nproblems. However, existing multi-agent RL methods typically scale poorly in\nthe problem size. Therefore, a key challenge is to translate the success of\ndeep learning on single-agent RL to the multi-agent setting. A major stumbling\nblock is that independent Q-learning, the most popular multi-agent RL method,\nintroduces nonstationarity that makes it incompatible with the experience\nreplay memory on which deep Q-learning relies. This paper proposes two methods\nthat address this problem: 1) using a multi-agent variant of importance\nsampling to naturally decay obsolete data and 2) conditioning each agent's\nvalue function on a fingerprint that disambiguates the age of the data sampled\nfrom the replay memory. Results on a challenging decentralised variant of\nStarCraft unit micromanagement confirm that these methods enable the successful\ncombination of experience replay with multi-agent RL. \n\n"}
{"id": "1703.00144", "contents": "Title: Theoretical Properties for Neural Networks with Weight Matrices of Low\n  Displacement Rank Abstract: Recently low displacement rank (LDR) matrices, or so-called structured\nmatrices, have been proposed to compress large-scale neural networks. Empirical\nresults have shown that neural networks with weight matrices of LDR matrices,\nreferred as LDR neural networks, can achieve significant reduction in space and\ncomputational complexity while retaining high accuracy. We formally study LDR\nmatrices in deep learning. First, we prove the universal approximation property\nof LDR neural networks with a mild condition on the displacement operators. We\nthen show that the error bounds of LDR neural networks are as efficient as\ngeneral neural networks with both single-layer and multiple-layer structure.\nFinally, we propose back-propagation based training algorithm for general LDR\nneural networks. \n\n"}
{"id": "1703.00410", "contents": "Title: Detecting Adversarial Samples from Artifacts Abstract: Deep neural networks (DNNs) are powerful nonlinear architectures that are\nknown to be robust to random perturbations of the input. However, these models\nare vulnerable to adversarial perturbations--small input changes crafted\nexplicitly to fool the model. In this paper, we ask whether a DNN can\ndistinguish adversarial samples from their normal and noisy counterparts. We\ninvestigate model confidence on adversarial samples by looking at Bayesian\nuncertainty estimates, available in dropout neural networks, and by performing\ndensity estimation in the subspace of deep features learned by the model. The\nresult is a method for implicit adversarial detection that is oblivious to the\nattack algorithm. We evaluate this method on a variety of standard datasets\nincluding MNIST and CIFAR-10 and show that it generalizes well across different\narchitectures and attacks. Our findings report that 85-93% ROC-AUC can be\nachieved on a number of standard classification tasks with a negative class\nthat consists of both normal and noisy samples. \n\n"}
{"id": "1703.00573", "contents": "Title: Generalization and Equilibrium in Generative Adversarial Nets (GANs) Abstract: We show that training of generative adversarial network (GAN) may not have\ngood generalization properties; e.g., training may appear successful but the\ntrained distribution may be far from target distribution in standard metrics.\nHowever, generalization does occur for a weaker metric called neural net\ndistance. It is also shown that an approximate pure equilibrium exists in the\ndiscriminator/generator game for a special class of generators with natural\ntraining objectives when generator capacity and training set sizes are\nmoderate.\n  This existence of equilibrium inspires MIX+GAN protocol, which can be\ncombined with any existing GAN training, and empirically shown to improve some\nof them. \n\n"}
{"id": "1703.00810", "contents": "Title: Opening the Black Box of Deep Neural Networks via Information Abstract: Despite their great success, there is still no comprehensive theoretical\nunderstanding of learning with Deep Neural Networks (DNNs) or their inner\norganization. Previous work proposed to analyze DNNs in the \\textit{Information\nPlane}; i.e., the plane of the Mutual Information values that each layer\npreserves on the input and output variables. They suggested that the goal of\nthe network is to optimize the Information Bottleneck (IB) tradeoff between\ncompression and prediction, successively, for each layer.\n  In this work we follow up on this idea and demonstrate the effectiveness of\nthe Information-Plane visualization of DNNs. Our main results are: (i) most of\nthe training epochs in standard DL are spent on {\\emph compression} of the\ninput to efficient representation and not on fitting the training labels. (ii)\nThe representation compression phase begins when the training errors becomes\nsmall and the Stochastic Gradient Decent (SGD) epochs change from a fast drift\nto smaller training error into a stochastic relaxation, or random diffusion,\nconstrained by the training error value. (iii) The converged layers lie on or\nvery close to the Information Bottleneck (IB) theoretical bound, and the maps\nfrom the input to any hidden layer and from this hidden layer to the output\nsatisfy the IB self-consistent equations. This generalization through noise\nmechanism is unique to Deep Neural Networks and absent in one layer networks.\n(iv) The training time is dramatically reduced when adding more hidden layers.\nThus the main advantage of the hidden layers is computational. This can be\nexplained by the reduced relaxation time, as this it scales super-linearly\n(exponentially for simple diffusion) with the information compression from the\nprevious layer. \n\n"}
{"id": "1703.03347", "contents": "Title: A Self-supervised Learning System for Object Detection using Physics\n  Simulation and Multi-view Pose Estimation Abstract: Progress has been achieved recently in object detection given advancements in\ndeep learning. Nevertheless, such tools typically require a large amount of\ntraining data and significant manual effort to label objects. This limits their\napplicability in robotics, where solutions must scale to a large number of\nobjects and variety of conditions. This work proposes an autonomous process for\ntraining a Convolutional Neural Network (CNN) for object detection and pose\nestimation in robotic setups. The focus is on detecting objects placed in\ncluttered, tight environments, such as a shelf with multiple objects. In\nparticular, given access to 3D object models, several aspects of the\nenvironment are physically simulated. The models are placed in physically\nrealistic poses with respect to their environment to generate a labeled\nsynthetic dataset. To further improve object detection, the network self-trains\nover real images that are labeled using a robust multi-view pose estimation\nprocess. The proposed training process is evaluated on several existing\ndatasets and on a dataset collected for this paper with a Motoman robotic arm.\nResults show that the proposed approach outperforms popular training processes\nrelying on synthetic - but not physically realistic - data and manual\nannotation. The key contributions are the incorporation of physical reasoning\nin the synthetic data generation process and the automation of the annotation\nprocess over real images. \n\n"}
{"id": "1703.03888", "contents": "Title: Segmentation of skin lesions based on fuzzy classification of pixels and\n  histogram thresholding Abstract: This paper proposes an innovative method for segmentation of skin lesions in\ndermoscopy images developed by the authors, based on fuzzy classification of\npixels and histogram thresholding. \n\n"}
{"id": "1703.04967", "contents": "Title: Comparison of the Deep-Learning-Based Automated Segmentation Methods for\n  the Head Sectioned Images of the Virtual Korean Human Project Abstract: This paper presents an end-to-end pixelwise fully automated segmentation of\nthe head sectioned images of the Visible Korean Human (VKH) project based on\nDeep Convolutional Neural Networks (DCNNs). By converting classification\nnetworks into Fully Convolutional Networks (FCNs), a coarse prediction map,\nwith smaller size than the original input image, can be created for\nsegmentation purposes. To refine this map and to obtain a dense pixel-wise\noutput, standard FCNs use deconvolution layers to upsample the coarse map.\nHowever, upsampling based on deconvolution increases the number of network\nparameters and causes loss of detail because of interpolation. On the other\nhand, dilated convolution is a new technique introduced recently that attempts\nto capture multi-scale contextual information without increasing the network\nparameters while keeping the resolution of the prediction maps high. We used\nboth a standard FCN and a dilated convolution based FCN for semantic\nsegmentation of the head sectioned images of the VKH dataset. Quantitative\nresults showed approximately 20% improvement in the segmentation accuracy when\nusing FCNs with dilated convolutions. \n\n"}
{"id": "1703.06182", "contents": "Title: Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under\n  Partial Observability Abstract: Many real-world tasks involve multiple agents with partial observability and\nlimited communication. Learning is challenging in these settings due to local\nviewpoints of agents, which perceive the world as non-stationary due to\nconcurrently-exploring teammates. Approaches that learn specialized policies\nfor individual tasks face problems when applied to the real world: not only do\nagents have to learn and store distinct policies for each task, but in practice\nidentities of tasks are often non-observable, making these approaches\ninapplicable. This paper formalizes and addresses the problem of multi-task\nmulti-agent reinforcement learning under partial observability. We introduce a\ndecentralized single-task learning approach that is robust to concurrent\ninteractions of teammates, and present an approach for distilling single-task\npolicies into a unified policy that performs well across multiple related\ntasks, without explicit provision of task identity. \n\n"}
{"id": "1703.06233", "contents": "Title: Recurrent Models for Situation Recognition Abstract: This work proposes Recurrent Neural Network (RNN) models to predict\nstructured 'image situations' -- actions and noun entities fulfilling semantic\nroles related to the action. In contrast to prior work relying on Conditional\nRandom Fields (CRFs), we use a specialized action prediction network followed\nby an RNN for noun prediction. Our system obtains state-of-the-art accuracy on\nthe challenging recent imSitu dataset, beating CRF-based models, including ones\ntrained with additional data. Further, we show that specialized features\nlearned from situation prediction can be transferred to the task of image\ncaptioning to more accurately describe human-object interactions. \n\n"}
{"id": "1703.06618", "contents": "Title: Twitter100k: A Real-world Dataset for Weakly Supervised Cross-Media\n  Retrieval Abstract: This paper contributes a new large-scale dataset for weakly supervised\ncross-media retrieval, named Twitter100k. Current datasets, such as Wikipedia,\nNUS Wide and Flickr30k, have two major limitations. First, these datasets are\nlacking in content diversity, i.e., only some pre-defined classes are covered.\nSecond, texts in these datasets are written in well-organized language, leading\nto inconsistency with realistic applications. To overcome these drawbacks, the\nproposed Twitter100k dataset is characterized by two aspects: 1) it has 100,000\nimage-text pairs randomly crawled from Twitter and thus has no constraint in\nthe image categories; 2) text in Twitter100k is written in informal language by\nthe users.\n  Since strongly supervised methods leverage the class labels that may be\nmissing in practice, this paper focuses on weakly supervised learning for\ncross-media retrieval, in which only text-image pairs are exploited during\ntraining. We extensively benchmark the performance of four subspace learning\nmethods and three variants of the Correspondence AutoEncoder, along with\nvarious text features on Wikipedia, Flickr30k and Twitter100k. Novel insights\nare provided. As a minor contribution, inspired by the characteristic of\nTwitter100k, we propose an OCR-based cross-media retrieval method. In\nexperiment, we show that the proposed OCR-based method improves the baseline\nperformance. \n\n"}
{"id": "1703.07140", "contents": "Title: Deep generative-contrastive networks for facial expression recognition Abstract: As the expressive depth of an emotional face differs with individuals or\nexpressions, recognizing an expression using a single facial image at a moment\nis difficult. A relative expression of a query face compared to a reference\nface might alleviate this difficulty. In this paper, we propose to utilize\ncontrastive representation that embeds a distinctive expressive factor for a\ndiscriminative purpose. The contrastive representation is calculated at the\nembedding layer of deep networks by comparing a given (query) image with the\nreference image. We attempt to utilize a generative reference image that is\nestimated based on the given image. Consequently, we deploy deep neural\nnetworks that embed a combination of a generative model, a contrastive model,\nand a discriminative model with an end-to-end training manner. In our proposed\nnetworks, we attempt to disentangle a facial expressive factor in two steps\nincluding learning of a generator network and a contrastive encoder network. We\nconducted extensive experiments on publicly available face expression databases\n(CK+, MMI, Oulu-CASIA, and in-the-wild databases) that have been widely adopted\nin the recent literatures. The proposed method outperforms the known\nstate-of-the art methods in terms of the recognition accuracy. \n\n"}
{"id": "1703.07737", "contents": "Title: In Defense of the Triplet Loss for Person Re-Identification Abstract: In the past few years, the field of computer vision has gone through a\nrevolution fueled mainly by the advent of large datasets and the adoption of\ndeep convolutional neural networks for end-to-end learning. The person\nre-identification subfield is no exception to this. Unfortunately, a prevailing\nbelief in the community seems to be that the triplet loss is inferior to using\nsurrogate losses (classification, verification) followed by a separate metric\nlearning step. We show that, for models trained from scratch as well as\npretrained ones, using a variant of the triplet loss to perform end-to-end deep\nmetric learning outperforms most other published methods by a large margin. \n\n"}
{"id": "1703.08274", "contents": "Title: View Adaptive Recurrent Neural Networks for High Performance Human\n  Action Recognition from Skeleton Data Abstract: Skeleton-based human action recognition has recently attracted increasing\nattention due to the popularity of 3D skeleton data. One main challenge lies in\nthe large view variations in captured human actions. We propose a novel view\nadaptation scheme to automatically regulate observation viewpoints during the\noccurrence of an action. Rather than re-positioning the skeletons based on a\nhuman defined prior criterion, we design a view adaptive recurrent neural\nnetwork (RNN) with LSTM architecture, which enables the network itself to adapt\nto the most suitable observation viewpoints from end to end. Extensive\nexperiment analyses show that the proposed view adaptive RNN model strives to\n(1) transform the skeletons of various views to much more consistent viewpoints\nand (2) maintain the continuity of the action rather than transforming every\nframe to the same position with the same body orientation. Our model achieves\nsignificant improvement over the state-of-the-art approaches on three benchmark\ndatasets. \n\n"}
{"id": "1703.08289", "contents": "Title: Deep Direct Regression for Multi-Oriented Scene Text Detection Abstract: In this paper, we first provide a new perspective to divide existing high\nperformance object detection methods into direct and indirect regressions.\nDirect regression performs boundary regression by predicting the offsets from a\ngiven point, while indirect regression predicts the offsets from some bounding\nbox proposals. Then we analyze the drawbacks of the indirect regression, which\nthe recent state-of-the-art detection structures like Faster-RCNN and SSD\nfollows, for multi-oriented scene text detection, and point out the potential\nsuperiority of direct regression. To verify this point of view, we propose a\ndeep direct regression based method for multi-oriented scene text detection.\nOur detection framework is simple and effective with a fully convolutional\nnetwork and one-step post processing. The fully convolutional network is\noptimized in an end-to-end way and has bi-task outputs where one is pixel-wise\nclassification between text and non-text, and the other is direct regression to\ndetermine the vertex coordinates of quadrilateral text boundaries. The proposed\nmethod is particularly beneficial for localizing incidental scene texts. On the\nICDAR2015 Incidental Scene Text benchmark, our method achieves the F1-measure\nof 81%, which is a new state-of-the-art and significantly outperforms previous\napproaches. On other standard datasets with focused scene texts, our method\nalso reaches the state-of-the-art performance. \n\n"}
{"id": "1703.08359", "contents": "Title: Scalable Person Re-identification on Supervised Smoothed Manifold Abstract: Most existing person re-identification algorithms either extract robust\nvisual features or learn discriminative metrics for person images. However, the\nunderlying manifold which those images reside on is rarely investigated. That\nraises a problem that the learned metric is not smooth with respect to the\nlocal geometry structure of the data manifold.\n  In this paper, we study person re-identification with manifold-based affinity\nlearning, which did not receive enough attention from this area. An\nunconventional manifold-preserving algorithm is proposed, which can 1) make the\nbest use of supervision from training data, whose label information is given as\npairwise constraints; 2) scale up to large repositories with low on-line time\ncomplexity; and 3) be plunged into most existing algorithms, serving as a\ngeneric postprocessing procedure to further boost the identification\naccuracies. Extensive experimental results on five popular person\nre-identification benchmarks consistently demonstrate the effectiveness of our\nmethod. Especially, on the largest CUHK03 and Market-1501, our method\noutperforms the state-of-the-art alternatives by a large margin with high\nefficiency, which is more appropriate for practical applications. \n\n"}
{"id": "1703.09200", "contents": "Title: The Deep Poincar\\'e Map: A Novel Approach for Left Ventricle\n  Segmentation Abstract: Precise segmentation of the left ventricle (LV) within cardiac MRI images is\na prerequisite for the quantitative measurement of heart function. However,\nthis task is challenging due to the limited availability of labeled data and\nmotion artifacts from cardiac imaging. In this work, we present an iterative\nsegmentation algorithm for LV delineation. By coupling deep learning with a\nnovel dynamic-based labeling scheme, we present a new methodology where a\npolicy model is learned to guide an agent to travel over the the image, tracing\nout a boundary of the ROI -- using the magnitude difference of the Poincar\\'e\nmap as a stopping criterion. Our method is evaluated on two datasets, namely\nthe Sunnybrook Cardiac Dataset (SCD) and data from the STACOM 2011 LV\nsegmentation challenge. Our method outperforms the previous research over many\nmetrics. In order to demonstrate the transferability of our method we present\nencouraging results over the STACOM 2011 data, when using a model trained on\nthe SCD dataset. \n\n"}
{"id": "1703.09393", "contents": "Title: Mixture of Counting CNNs: Adaptive Integration of CNNs Specialized to\n  Specific Appearance for Crowd Counting Abstract: This paper proposes a crowd counting method. Crowd counting is difficult\nbecause of large appearance changes of a target which caused by density and\nscale changes. Conventional crowd counting methods generally utilize one\npredictor (e,g., regression and multi-class classifier). However, such only one\npredictor can not count targets with large appearance changes well. In this\npaper, we propose to predict the number of targets using multiple CNNs\nspecialized to a specific appearance, and those CNNs are adaptively selected\naccording to the appearance of a test image. By integrating the selected CNNs,\nthe proposed method has the robustness to large appearance changes. In\nexperiments, we confirm that the proposed method can count crowd with lower\ncounting error than a CNN and integration of CNNs with fixed weights. Moreover,\nwe confirm that each predictor automatically specialized to a specific\nappearance. \n\n"}
{"id": "1703.09438", "contents": "Title: Octree Generating Networks: Efficient Convolutional Architectures for\n  High-resolution 3D Outputs Abstract: We present a deep convolutional decoder architecture that can generate\nvolumetric 3D outputs in a compute- and memory-efficient manner by using an\noctree representation. The network learns to predict both the structure of the\noctree, and the occupancy values of individual cells. This makes it a\nparticularly valuable technique for generating 3D shapes. In contrast to\nstandard decoders acting on regular voxel grids, the architecture does not have\ncubic complexity. This allows representing much higher resolution outputs with\na limited memory budget. We demonstrate this in several application domains,\nincluding 3D convolutional autoencoders, generation of objects and whole scenes\nfrom high-level representations, and shape from a single image. \n\n"}
{"id": "1703.09916", "contents": "Title: Towards thinner convolutional neural networks through Gradually Global\n  Pruning Abstract: Deep network pruning is an effective method to reduce the storage and\ncomputation cost of deep neural networks when applying them to resource-limited\ndevices. Among many pruning granularities, neuron level pruning will remove\nredundant neurons and filters in the model and result in thinner networks. In\nthis paper, we propose a gradually global pruning scheme for neuron level\npruning. In each pruning step, a small percent of neurons were selected and\ndropped across all layers in the model. We also propose a simple method to\neliminate the biases in evaluating the importance of neurons to make the scheme\nfeasible. Compared with layer-wise pruning scheme, our scheme avoid the\ndifficulty in determining the redundancy in each layer and is more effective\nfor deep networks. Our scheme would automatically find a thinner sub-network in\noriginal network under a given performance. \n\n"}
{"id": "1703.10069", "contents": "Title: Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level\n  Coordination in Learning to Play StarCraft Combat Games Abstract: Many artificial intelligence (AI) applications often require multiple\nintelligent agents to work in a collaborative effort. Efficient learning for\nintra-agent communication and coordination is an indispensable step towards\ngeneral AI. In this paper, we take StarCraft combat game as a case study, where\nthe task is to coordinate multiple agents as a team to defeat their enemies. To\nmaintain a scalable yet effective communication protocol, we introduce a\nMultiagent Bidirectionally-Coordinated Network (BiCNet ['bIknet]) with a\nvectorised extension of actor-critic formulation. We show that BiCNet can\nhandle different types of combats with arbitrary numbers of AI agents for both\nsides. Our analysis demonstrates that without any supervisions such as human\ndemonstrations or labelled data, BiCNet could learn various types of advanced\ncoordination strategies that have been commonly used by experienced game\nplayers. In our experiments, we evaluate our approach against multiple\nbaselines under different scenarios; it shows state-of-the-art performance, and\npossesses potential values for large-scale real-world applications. \n\n"}
{"id": "1703.10717", "contents": "Title: BEGAN: Boundary Equilibrium Generative Adversarial Networks Abstract: We propose a new equilibrium enforcing method paired with a loss derived from\nthe Wasserstein distance for training auto-encoder based Generative Adversarial\nNetworks. This method balances the generator and discriminator during training.\nAdditionally, it provides a new approximate convergence measure, fast and\nstable training and high visual quality. We also derive a way of controlling\nthe trade-off between image diversity and visual quality. We focus on the image\ngeneration task, setting a new milestone in visual quality, even at higher\nresolutions. This is achieved while using a relatively simple model\narchitecture and a standard training procedure. \n\n"}
{"id": "1703.10764", "contents": "Title: A Hybrid Data Association Framework for Robust Online Multi-Object\n  Tracking Abstract: Global optimization algorithms have shown impressive performance in\ndata-association based multi-object tracking, but handling online data remains\na difficult hurdle to overcome. In this paper, we present a hybrid data\nassociation framework with a min-cost multi-commodity network flow for robust\nonline multi-object tracking. We build local target-specific models interleaved\nwith global optimization of the optimal data association over multiple video\nframes. More specifically, in the min-cost multi-commodity network flow, the\ntarget-specific similarities are online learned to enforce the local\nconsistency for reducing the complexity of the global data association.\nMeanwhile, the global data association taking multiple video frames into\naccount alleviates irrecoverable errors caused by the local data association\nbetween adjacent frames. To ensure the efficiency of online tracking, we give\nan efficient near-optimal solution to the proposed min-cost multi-commodity\nflow problem, and provide the empirical proof of its sub-optimality. The\ncomprehensive experiments on real data demonstrate the superior tracking\nperformance of our approach in various challenging situations. \n\n"}
{"id": "1704.00260", "contents": "Title: Aligned Image-Word Representations Improve Inductive Transfer Across\n  Vision-Language Tasks Abstract: An important goal of computer vision is to build systems that learn visual\nrepresentations over time that can be applied to many tasks. In this paper, we\ninvestigate a vision-language embedding as a core representation and show that\nit leads to better cross-task transfer than standard multi-task learning. In\nparticular, the task of visual recognition is aligned to the task of visual\nquestion answering by forcing each to use the same word-region embeddings. We\nshow this leads to greater inductive transfer from recognition to VQA than\nstandard multitask learning. Visual recognition also improves, especially for\ncategories that have relatively few recognition training labels but appear\noften in the VQA setting. Thus, our paper takes a small step towards creating\nmore general vision systems by showing the benefit of interpretable, flexible,\nand trainable core representations. \n\n"}
{"id": "1704.01155", "contents": "Title: Feature Squeezing: Detecting Adversarial Examples in Deep Neural\n  Networks Abstract: Although deep neural networks (DNNs) have achieved great success in many\ntasks, they can often be fooled by \\emph{adversarial examples} that are\ngenerated by adding small but purposeful distortions to natural examples.\nPrevious studies to defend against adversarial examples mostly focused on\nrefining the DNN models, but have either shown limited success or required\nexpensive computation. We propose a new strategy, \\emph{feature squeezing},\nthat can be used to harden DNN models by detecting adversarial examples.\nFeature squeezing reduces the search space available to an adversary by\ncoalescing samples that correspond to many different feature vectors in the\noriginal space into a single sample. By comparing a DNN model's prediction on\nthe original input with that on squeezed inputs, feature squeezing detects\nadversarial examples with high accuracy and few false positives. This paper\nexplores two feature squeezing methods: reducing the color bit depth of each\npixel and spatial smoothing. These simple strategies are inexpensive and\ncomplementary to other defenses, and can be combined in a joint detection\nframework to achieve high detection rates against state-of-the-art attacks. \n\n"}
{"id": "1704.01249", "contents": "Title: A Structured Approach to Predicting Image Enhancement Parameters Abstract: Social networking on mobile devices has become a commonplace of everyday\nlife. In addition, photo capturing process has become trivial due to the\nadvances in mobile imaging. Hence people capture a lot of photos everyday and\nthey want them to be visually-attractive. This has given rise to automated,\none-touch enhancement tools. However, the inability of those tools to provide\npersonalized and content-adaptive enhancement has paved way for machine-learned\nmethods to do the same. The existing typical machine-learned methods\nheuristically (e.g. kNN-search) predict the enhancement parameters for a new\nimage by relating the image to a set of similar training images. These\nheuristic methods need constant interaction with the training images which\nmakes the parameter prediction sub-optimal and computationally expensive at\ntest time which is undesired. This paper presents a novel approach to\npredicting the enhancement parameters given a new image using only its\nfeatures, without using any training images. We propose to model the\ninteraction between the image features and its corresponding enhancement\nparameters using the matrix factorization (MF) principles. We also propose a\nway to integrate the image features in the MF formulation. We show that our\napproach outperforms heuristic approaches as well as recent approaches in MF\nand structured prediction on synthetic as well as real-world data of image\nenhancement. \n\n"}
{"id": "1704.01716", "contents": "Title: Action Representation Using Classifier Decision Boundaries Abstract: Most popular deep learning based models for action recognition are designed\nto generate separate predictions within their short temporal windows, which are\noften aggregated by heuristic means to assign an action label to the full video\nsegment. Given that not all frames from a video characterize the underlying\naction, pooling schemes that impose equal importance to all frames might be\nunfavorable. In an attempt towards tackling this challenge, we propose a novel\npooling scheme, dubbed SVM pooling, based on the notion that among the bag of\nfeatures generated by a CNN on all temporal windows, there is at least one\nfeature that characterizes the action. To this end, we learn a decision\nhyperplane that separates this unknown yet useful feature from the rest.\nApplying multiple instance learning in an SVM setup, we use the parameters of\nthis separating hyperplane as a descriptor for the video. Since these\nparameters are directly related to the support vectors in a max-margin\nframework, they serve as robust representations for pooling of the CNN\nfeatures. We devise a joint optimization objective and an efficient solver that\nlearns these hyperplanes per video and the corresponding action classifiers\nover the hyperplanes. Showcased experiments on the standard HMDB and UCF101\ndatasets demonstrate state-of-the-art performance. \n\n"}
{"id": "1704.01719", "contents": "Title: Beyond triplet loss: a deep quadruplet network for person\n  re-identification Abstract: Person re-identification (ReID) is an important task in wide area video\nsurveillance which focuses on identifying people across different cameras.\nRecently, deep learning networks with a triplet loss become a common framework\nfor person ReID. However, the triplet loss pays main attentions on obtaining\ncorrect orders on the training set. It still suffers from a weaker\ngeneralization capability from the training set to the testing set, thus\nresulting in inferior performance. In this paper, we design a quadruplet loss,\nwhich can lead to the model output with a larger inter-class variation and a\nsmaller intra-class variation compared to the triplet loss. As a result, our\nmodel has a better generalization ability and can achieve a higher performance\non the testing set. In particular, a quadruplet deep network using a\nmargin-based online hard negative mining is proposed based on the quadruplet\nloss for the person ReID. In extensive experiments, the proposed network\noutperforms most of the state-of-the-art algorithms on representative datasets\nwhich clearly demonstrates the effectiveness of our proposed method. \n\n"}
{"id": "1704.02602", "contents": "Title: Automatic Image Filtering on Social Networks Using Deep Learning and\n  Perceptual Hashing During Crises Abstract: The extensive use of social media platforms, especially during disasters,\ncreates unique opportunities for humanitarian organizations to gain situational\nawareness and launch relief operations accordingly. In addition to the textual\ncontent, people post overwhelming amounts of imagery data on social networks\nwithin minutes of a disaster hit. Studies point to the importance of this\nonline imagery content for emergency response. Despite recent advances in the\ncomputer vision field, automatic processing of the crisis-related social media\nimagery data remains a challenging task. It is because a majority of which\nconsists of redundant and irrelevant content. In this paper, we present an\nimage processing pipeline that comprises de-duplication and relevancy filtering\nmechanisms to collect and filter social media image content in real-time during\na crisis event. Results obtained from extensive experiments on real-world\ncrisis datasets demonstrate the significance of the proposed pipeline for\noptimal utilization of both human and machine computing resources. \n\n"}
{"id": "1704.02906", "contents": "Title: Multi-Agent Diverse Generative Adversarial Networks Abstract: We propose MAD-GAN, an intuitive generalization to the Generative Adversarial\nNetworks (GANs) and its conditional variants to address the well known problem\nof mode collapse. First, MAD-GAN is a multi-agent GAN architecture\nincorporating multiple generators and one discriminator. Second, to enforce\nthat different generators capture diverse high probability modes, the\ndiscriminator of MAD-GAN is designed such that along with finding the real and\nfake samples, it is also required to identify the generator that generated the\ngiven fake sample. Intuitively, to succeed in this task, the discriminator must\nlearn to push different generators towards different identifiable modes. We\nperform extensive experiments on synthetic and real datasets and compare\nMAD-GAN with different variants of GAN. We show high quality diverse sample\ngenerations for challenging tasks such as image-to-image translation and face\ngeneration. In addition, we also show that MAD-GAN is able to disentangle\ndifferent modalities when trained using highly challenging diverse-class\ndataset (e.g. dataset with images of forests, icebergs, and bedrooms). In the\nend, we show its efficacy on the unsupervised feature representation task. In\nAppendix, we introduce a similarity based competing objective (MAD-GAN-Sim)\nwhich encourages different generators to generate diverse samples based on a\nuser defined similarity metric. We show its performance on the image-to-image\ntranslation, and also show its effectiveness on the unsupervised feature\nrepresentation task. \n\n"}
{"id": "1704.03966", "contents": "Title: Collaborative Low-Rank Subspace Clustering Abstract: In this paper we present Collaborative Low-Rank Subspace Clustering. Given\nmultiple observations of a phenomenon we learn a unified representation matrix.\nThis unified matrix incorporates the features from all the observations, thus\nincreasing the discriminative power compared with learning the representation\nmatrix on each observation separately. Experimental evaluation shows that our\nmethod outperforms subspace clustering on separate observations and the state\nof the art collaborative learning algorithm. \n\n"}
{"id": "1704.03971", "contents": "Title: On the Effects of Batch and Weight Normalization in Generative\n  Adversarial Networks Abstract: Generative adversarial networks (GANs) are highly effective unsupervised\nlearning frameworks that can generate very sharp data, even for data such as\nimages with complex, highly multimodal distributions. However GANs are known to\nbe very hard to train, suffering from problems such as mode collapse and\ndisturbing visual artifacts. Batch normalization (BN) techniques have been\nintroduced to address the training. Though BN accelerates the training in the\nbeginning, our experiments show that the use of BN can be unstable and\nnegatively impact the quality of the trained model. The evaluation of BN and\nnumerous other recent schemes for improving GAN training is hindered by the\nlack of an effective objective quality measure for GAN models. To address these\nissues, we first introduce a weight normalization (WN) approach for GAN\ntraining that significantly improves the stability, efficiency and the quality\nof the generated samples. To allow a methodical evaluation, we introduce\nsquared Euclidean reconstruction error on a test set as a new objective\nmeasure, to assess training performance in terms of speed, stability, and\nquality of generated samples. Our experiments with a standard DCGAN\narchitecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10)\nindicate that training using WN is generally superior to BN for GANs, achieving\n10% lower mean squared loss for reconstruction and significantly better\nqualitative results than BN. We further demonstrate the stability of WN on a\n21-layer ResNet trained with the CelebA data set. The code for this paper is\navailable at https://github.com/stormraiser/gan-weightnorm-resnet \n\n"}
{"id": "1704.04394", "contents": "Title: DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting\n  Agents Abstract: We introduce a Deep Stochastic IOC RNN Encoderdecoder framework, DESIRE, for\nthe task of future predictions of multiple interacting agents in dynamic\nscenes. DESIRE effectively predicts future locations of objects in multiple\nscenes by 1) accounting for the multi-modal nature of the future prediction\n(i.e., given the same context, future may vary), 2) foreseeing the potential\nfuture outcomes and make a strategic prediction based on that, and 3) reasoning\nnot only from the past motion history, but also from the scene context as well\nas the interactions among the agents. DESIRE achieves these in a single\nend-to-end trainable neural network model, while being computationally\nefficient. The model first obtains a diverse set of hypothetical future\nprediction samples employing a conditional variational autoencoder, which are\nranked and refined by the following RNN scoring-regression module. Samples are\nscored by accounting for accumulated future rewards, which enables better\nlong-term strategic decisions similar to IOC frameworks. An RNN scene context\nfusion module jointly captures past motion histories, the semantic scene\ncontext and interactions among multiple agents. A feedback mechanism iterates\nover the ranking and refinement to further boost the prediction accuracy. We\nevaluate our model on two publicly available datasets: KITTI and Stanford Drone\nDataset. Our experiments show that the proposed model significantly improves\nthe prediction accuracy compared to other baseline methods. \n\n"}
{"id": "1704.04865", "contents": "Title: Gang of GANs: Generative Adversarial Networks with Maximum Margin\n  Ranking Abstract: Traditional generative adversarial networks (GAN) and many of its variants\nare trained by minimizing the KL or JS-divergence loss that measures how close\nthe generated data distribution is from the true data distribution. A recent\nadvance called the WGAN based on Wasserstein distance can improve on the KL and\nJS-divergence based GANs, and alleviate the gradient vanishing, instability,\nand mode collapse issues that are common in the GAN training. In this work, we\naim at improving on the WGAN by first generalizing its discriminator loss to a\nmargin-based one, which leads to a better discriminator, and in turn a better\ngenerator, and then carrying out a progressive training paradigm involving\nmultiple GANs to contribute to the maximum margin ranking loss so that the GAN\nat later stages will improve upon early stages. We call this method Gang of\nGANs (GoGAN). We have shown theoretically that the proposed GoGAN can reduce\nthe gap between the true data distribution and the generated data distribution\nby at least half in an optimally trained WGAN. We have also proposed a new way\nof measuring GAN quality which is based on image completion tasks. We have\nevaluated our method on four visual datasets: CelebA, LSUN Bedroom, CIFAR-10,\nand 50K-SSFF, and have seen both visual and quantitative improvement over\nbaseline WGAN. \n\n"}
{"id": "1704.04886", "contents": "Title: Multi-View Image Generation from a Single-View Abstract: This paper addresses a challenging problem -- how to generate multi-view\ncloth images from only a single view input. To generate realistic-looking\nimages with different views from the input, we propose a new image generation\nmodel termed VariGANs that combines the strengths of the variational inference\nand the Generative Adversarial Networks (GANs). Our proposed VariGANs model\ngenerates the target image in a coarse-to-fine manner instead of a single pass\nwhich suffers from severe artifacts. It first performs variational inference to\nmodel global appearance of the object (e.g., shape and color) and produce a\ncoarse image with a different view. Conditioned on the generated low resolution\nimages, it then proceeds to perform adversarial learning to fill details and\ngenerate images of consistent details with the input. Extensive experiments\nconducted on two clothing datasets, MVC and DeepFashion, have demonstrated that\nimages of a novel view generated by our model are more plausible than those\ngenerated by existing approaches, in terms of more consistent global appearance\nas well as richer and sharper details. \n\n"}
{"id": "1704.05519", "contents": "Title: Computer Vision for Autonomous Vehicles: Problems, Datasets and State of\n  the Art Abstract: Recent years have witnessed enormous progress in AI-related fields such as\ncomputer vision, machine learning, and autonomous vehicles. As with any rapidly\ngrowing field, it becomes increasingly difficult to stay up-to-date or enter\nthe field as a beginner. While several survey papers on particular sub-problems\nhave appeared, no comprehensive survey on problems, datasets, and methods in\ncomputer vision for autonomous vehicles has been published. This book attempts\nto narrow this gap by providing a survey on the state-of-the-art datasets and\ntechniques. Our survey includes both the historically most relevant literature\nas well as the current state of the art on several specific topics, including\nrecognition, reconstruction, motion estimation, tracking, scene understanding,\nand end-to-end learning for autonomous driving. Towards this goal, we analyze\nthe performance of the state of the art on several challenging benchmarking\ndatasets, including KITTI, MOT, and Cityscapes. Besides, we discuss open\nproblems and current research challenges. To ease accessibility and accommodate\nmissing references, we also provide a website that allows navigating topics as\nwell as methods and provides additional information. \n\n"}
{"id": "1704.05796", "contents": "Title: Network Dissection: Quantifying Interpretability of Deep Visual\n  Representations Abstract: We propose a general framework called Network Dissection for quantifying the\ninterpretability of latent representations of CNNs by evaluating the alignment\nbetween individual hidden units and a set of semantic concepts. Given any CNN\nmodel, the proposed method draws on a broad data set of visual concepts to\nscore the semantics of hidden units at each intermediate convolutional layer.\nThe units with semantics are given labels across a range of objects, parts,\nscenes, textures, materials, and colors. We use the proposed method to test the\nhypothesis that interpretability of units is equivalent to random linear\ncombinations of units, then we apply our method to compare the latent\nrepresentations of various networks when trained to solve different supervised\nand self-supervised training tasks. We further analyze the effect of training\niterations, compare networks trained with different initializations, examine\nthe impact of network depth and width, and measure the effect of dropout and\nbatch normalization on the interpretability of deep visual representations. We\ndemonstrate that the proposed method can shed light on characteristics of CNN\nmodels and training methods that go beyond measurements of their discriminative\npower. \n\n"}
{"id": "1704.05832", "contents": "Title: SkiMap: An Efficient Mapping Framework for Robot Navigation Abstract: We present a novel mapping framework for robot navigation which features a\nmulti-level querying system capable to obtain rapidly representations as\ndiverse as a 3D voxel grid, a 2.5D height map and a 2D occupancy grid. These\nare inherently embedded into a memory and time efficient core data structure\norganized as a Tree of SkipLists. Compared to the well-known Octree\nrepresentation, our approach exhibits a better time efficiency, thanks to its\nsimple and highly parallelizable computational structure, and a similar memory\nfootprint when mapping large workspaces. Peculiarly within the realm of mapping\nfor robot navigation, our framework supports realtime erosion and\nre-integration of measurements upon reception of optimized poses from the\nsensor tracker, so as to improve continuously the accuracy of the map. \n\n"}
{"id": "1704.06065", "contents": "Title: End-to-End Unsupervised Deformable Image Registration with a\n  Convolutional Neural Network Abstract: In this work we propose a deep learning network for deformable image\nregistration (DIRNet). The DIRNet consists of a convolutional neural network\n(ConvNet) regressor, a spatial transformer, and a resampler. The ConvNet\nanalyzes a pair of fixed and moving images and outputs parameters for the\nspatial transformer, which generates the displacement vector field that enables\nthe resampler to warp the moving image to the fixed image. The DIRNet is\ntrained end-to-end by unsupervised optimization of a similarity metric between\ninput image pairs. A trained DIRNet can be applied to perform registration on\nunseen image pairs in one pass, thus non-iteratively. Evaluation was performed\nwith registration of images of handwritten digits (MNIST) and cardiac cine MR\nscans (Sunnybrook Cardiac Data). The results demonstrate that registration with\nDIRNet is as accurate as a conventional deformable image registration method\nwith substantially shorter execution times. \n\n"}
{"id": "1704.06228", "contents": "Title: Temporal Action Detection with Structured Segment Networks Abstract: Detecting actions in untrimmed videos is an important yet challenging task.\nIn this paper, we present the structured segment network (SSN), a novel\nframework which models the temporal structure of each action instance via a\nstructured temporal pyramid. On top of the pyramid, we further introduce a\ndecomposed discriminative model comprising two classifiers, respectively for\nclassifying actions and determining completeness. This allows the framework to\neffectively distinguish positive proposals from background or incomplete ones,\nthus leading to both accurate recognition and localization. These components\nare integrated into a unified network that can be efficiently trained in an\nend-to-end fashion. Additionally, a simple yet effective temporal action\nproposal scheme, dubbed temporal actionness grouping (TAG) is devised to\ngenerate high quality action proposals. On two challenging benchmarks, THUMOS14\nand ActivityNet, our method remarkably outperforms previous state-of-the-art\nmethods, demonstrating superior accuracy and strong adaptivity in handling\nactions with various temporal structures. \n\n"}
{"id": "1704.06610", "contents": "Title: Context-based Object Viewpoint Estimation: A 2D Relational Approach Abstract: The task of object viewpoint estimation has been a challenge since the early\ndays of computer vision. To estimate the viewpoint (or pose) of an object,\npeople have mostly looked at object intrinsic features, such as shape or\nappearance. Surprisingly, informative features provided by other, extrinsic\nelements in the scene, have so far mostly been ignored. At the same time,\ncontextual cues have been proven to be of great benefit for related tasks such\nas object detection or action recognition. In this paper, we explore how\ninformation from other objects in the scene can be exploited for viewpoint\nestimation. In particular, we look at object configurations by following a\nrelational neighbor-based approach for reasoning about object relations. We\nshow that, starting from noisy object detections and viewpoint estimates,\nexploiting the estimated viewpoint and location of other objects in the scene\ncan lead to improved object viewpoint predictions. Experiments on the KITTI\ndataset demonstrate that object configurations can indeed be used as a\ncomplementary cue to appearance-based viewpoint estimation. Our analysis\nreveals that the proposed context-based method can improve object viewpoint\nestimation by reducing specific types of viewpoint estimation errors commonly\nmade by methods that only consider local information. Moreover, considering\ncontextual information produces superior performance in scenes where a high\nnumber of object instances occur. Finally, our results suggest that, following\na cautious relational neighbor formulation brings improvements over its\naggressive counterpart for the task of object viewpoint estimation. \n\n"}
{"id": "1704.07019", "contents": "Title: Model-based Iterative Restoration for Binary Document Image Compression\n  with Dictionary Learning Abstract: The inherent noise in the observed (e.g., scanned) binary document image\ndegrades the image quality and harms the compression ratio through breaking the\npattern repentance and adding entropy to the document images. In this paper, we\ndesign a cost function in Bayesian framework with dictionary learning.\nMinimizing our cost function produces a restored image which has better quality\nthan that of the observed noisy image, and a dictionary for representing and\nencoding the image. After the restoration, we use this dictionary (from the\nsame cost function) to encode the restored image following the\nsymbol-dictionary framework by JBIG2 standard with the lossless mode.\nExperimental results with a variety of document images demonstrate that our\nmethod improves the image quality compared with the observed image, and\nsimultaneously improves the compression ratio. For the test images with\nsynthetic noise, our method reduces the number of flipped pixels by 48.2% and\nimproves the compression ratio by 36.36% as compared with the best encoding\nmethods. For the test images with real noise, our method visually improves the\nimage quality, and outperforms the cutting-edge method by 28.27% in terms of\nthe compression ratio. \n\n"}
{"id": "1704.08243", "contents": "Title: C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0\n  Dataset Abstract: Visual Question Answering (VQA) has received a lot of attention over the past\ncouple of years. A number of deep learning models have been proposed for this\ntask. However, it has been shown that these models are heavily driven by\nsuperficial correlations in the training data and lack compositionality -- the\nability to answer questions about unseen compositions of seen concepts. This\ncompositionality is desirable and central to intelligence. In this paper, we\npropose a new setting for Visual Question Answering where the test\nquestion-answer pairs are compositionally novel compared to training\nquestion-answer pairs. To facilitate developing models under this setting, we\npresent a new compositional split of the VQA v1.0 dataset, which we call\nCompositional VQA (C-VQA). We analyze the distribution of questions and answers\nin the C-VQA splits. Finally, we evaluate several existing VQA models under\nthis new setting and show that the performances of these models degrade by a\nsignificant amount compared to the original VQA setting. \n\n"}
{"id": "1704.08545", "contents": "Title: ICNet for Real-Time Semantic Segmentation on High-Resolution Images Abstract: We focus on the challenging task of real-time semantic segmentation in this\npaper. It finds many practical applications and yet is with fundamental\ndifficulty of reducing a large portion of computation for pixel-wise label\ninference. We propose an image cascade network (ICNet) that incorporates\nmulti-resolution branches under proper label guidance to address this\nchallenge. We provide in-depth analysis of our framework and introduce the\ncascade feature fusion unit to quickly achieve high-quality segmentation. Our\nsystem yields real-time inference on a single GPU card with decent quality\nresults evaluated on challenging datasets like Cityscapes, CamVid and\nCOCO-Stuff. \n\n"}
{"id": "1704.08992", "contents": "Title: A Unified Approach of Multi-scale Deep and Hand-crafted Features for\n  Defocus Estimation Abstract: In this paper, we introduce robust and synergetic hand-crafted features and a\nsimple but efficient deep feature from a convolutional neural network (CNN)\narchitecture for defocus estimation. This paper systematically analyzes the\neffectiveness of different features, and shows how each feature can compensate\nfor the weaknesses of other features when they are concatenated. For a full\ndefocus map estimation, we extract image patches on strong edges sparsely,\nafter which we use them for deep and hand-crafted feature extraction. In order\nto reduce the degree of patch-scale dependency, we also propose a multi-scale\npatch extraction strategy. A sparse defocus map is generated using a neural\nnetwork classifier followed by a probability-joint bilateral filter. The final\ndefocus map is obtained from the sparse defocus map with guidance from an\nedge-preserving filtered input image. Experimental results show that our\nalgorithm is superior to state-of-the-art algorithms in terms of defocus\nestimation. Our work can be used for applications such as segmentation, blur\nmagnification, all-in-focus image generation, and 3-D estimation. \n\n"}
{"id": "1705.00053", "contents": "Title: The Pose Knows: Video Forecasting by Generating Pose Futures Abstract: Current approaches in video forecasting attempt to generate videos directly\nin pixel space using Generative Adversarial Networks (GANs) or Variational\nAutoencoders (VAEs). However, since these approaches try to model all the\nstructure and scene dynamics at once, in unconstrained settings they often\ngenerate uninterpretable results. Our insight is to model the forecasting\nproblem at a higher level of abstraction. Specifically, we exploit human pose\ndetectors as a free source of supervision and break the video forecasting\nproblem into two discrete steps. First we explicitly model the high level\nstructure of active objects in the scene---humans---and use a VAE to model the\npossible future movements of humans in the pose space. We then use the future\nposes generated as conditional information to a GAN to predict the future\nframes of the video in pixel space. By using the structured space of pose as an\nintermediate representation, we sidestep the problems that GANs have in\ngenerating video pixels directly. We show through quantitative and qualitative\nevaluation that our method outperforms state-of-the-art methods for video\nprediction. \n\n"}
{"id": "1705.00601", "contents": "Title: The Promise of Premise: Harnessing Question Premises in Visual Question\n  Answering Abstract: In this paper, we make a simple observation that questions about images often\ncontain premises - objects and relationships implied by the question - and that\nreasoning about premises can help Visual Question Answering (VQA) models\nrespond more intelligently to irrelevant or previously unseen questions. When\npresented with a question that is irrelevant to an image, state-of-the-art VQA\nmodels will still answer purely based on learned language biases, resulting in\nnon-sensical or even misleading answers. We note that a visual question is\nirrelevant to an image if at least one of its premises is false (i.e. not\ndepicted in the image). We leverage this observation to construct a dataset for\nQuestion Relevance Prediction and Explanation (QRPE) by searching for false\npremises. We train novel question relevance detection models and show that\nmodels that reason about premises consistently outperform models that do not.\nWe also find that forcing standard VQA models to reason about premises during\ntraining can lead to improvements on tasks requiring compositional reasoning. \n\n"}
{"id": "1705.00771", "contents": "Title: Lesion detection and Grading of Diabetic Retinopathy via Two-stages Deep\n  Convolutional Neural Networks Abstract: We propose an automatic diabetic retinopathy (DR) analysis algorithm based on\ntwo-stages deep convolutional neural networks (DCNN). Compared to existing\nDCNN-based DR detection methods, the proposed algorithm have the following\nadvantages: (1) Our method can point out the location and type of lesions in\nthe fundus images, as well as giving the severity grades of DR. Moreover, since\nretina lesions and DR severity appear with different scales in fundus images,\nthe integration of both local and global networks learn more complete and\nspecific features for DR analysis. (2) By introducing imbalanced weighting map,\nmore attentions will be given to lesion patches for DR grading, which\nsignificantly improve the performance of the proposed algorithm. In this study,\nwe label 12,206 lesion patches and re-annotate the DR grades of 23,595 fundus\nimages from Kaggle competition dataset. Under the guidance of clinical\nophthalmologists, the experimental results show that our local lesion detection\nnet achieve comparable performance with trained human observers, and the\nproposed imbalanced weighted scheme also be proved to significantly improve the\ncapability of our DCNN-based DR grading algorithm. \n\n"}
{"id": "1705.01247", "contents": "Title: Unsupervised Part-based Weighting Aggregation of Deep Convolutional\n  Features for Image Retrieval Abstract: In this paper, we propose a simple but effective semantic part-based\nweighting aggregation (PWA) for image retrieval. The proposed PWA utilizes the\ndiscriminative filters of deep convolutional layers as part detectors.\nMoreover, we propose the effective unsupervised strategy to select some part\ndetectors to generate the \"probabilistic proposals\", which highlight certain\ndiscriminative parts of objects and suppress the noise of background. The final\nglobal PWA representation could then be acquired by aggregating the regional\nrepresentations weighted by the selected \"probabilistic proposals\"\ncorresponding to various semantic content. We conduct comprehensive experiments\non four standard datasets and show that our unsupervised PWA outperforms the\nstate-of-the-art unsupervised and supervised aggregation methods. Code is\navailable at https://github.com/XJhaoren/PWA. \n\n"}
{"id": "1705.02757", "contents": "Title: What Can Help Pedestrian Detection? Abstract: Aggregating extra features has been considered as an effective approach to\nboost traditional pedestrian detection methods. However, there is still a lack\nof studies on whether and how CNN-based pedestrian detectors can benefit from\nthese extra features. The first contribution of this paper is exploring this\nissue by aggregating extra features into CNN-based pedestrian detection\nframework. Through extensive experiments, we evaluate the effects of different\nkinds of extra features quantitatively. Moreover, we propose a novel network\narchitecture, namely HyperLearner, to jointly learn pedestrian detection as\nwell as the given extra feature. By multi-task training, HyperLearner is able\nto utilize the information of given features and improve detection performance\nwithout extra inputs in inference. The experimental results on multiple\npedestrian benchmarks validate the effectiveness of the proposed HyperLearner. \n\n"}
{"id": "1705.02900", "contents": "Title: Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with\n  JPEG Compression Abstract: Deep neural networks (DNNs) have achieved great success in solving a variety\nof machine learning (ML) problems, especially in the domain of image\nrecognition. However, recent research showed that DNNs can be highly vulnerable\nto adversarially generated instances, which look seemingly normal to human\nobservers, but completely confuse DNNs. These adversarial samples are crafted\nby adding small perturbations to normal, benign images. Such perturbations,\nwhile imperceptible to the human eye, are picked up by DNNs and cause them to\nmisclassify the manipulated instances with high confidence. In this work, we\nexplore and demonstrate how systematic JPEG compression can work as an\neffective pre-processing step in the classification pipeline to counter\nadversarial attacks and dramatically reduce their effects (e.g., Fast Gradient\nSign Method, DeepFool). An important component of JPEG compression is its\nability to remove high frequency signal components, inside square blocks of an\nimage. Such an operation is equivalent to selective blurring of the image,\nhelping remove additive perturbations. Further, we propose an ensemble-based\ntechnique that can be constructed quickly from a given well-performing DNN, and\nempirically show how such an ensemble that leverages JPEG compression can\nprotect a model from multiple types of adversarial attacks, without requiring\nknowledge about the model. \n\n"}
{"id": "1705.02997", "contents": "Title: Light Field Video Capture Using a Learning-Based Hybrid Imaging System Abstract: Light field cameras have many advantages over traditional cameras, as they\nallow the user to change various camera settings after capture. However,\ncapturing light fields requires a huge bandwidth to record the data: a modern\nlight field camera can only take three images per second. This prevents current\nconsumer light field cameras from capturing light field videos. Temporal\ninterpolation at such extreme scale (10x, from 3 fps to 30 fps) is infeasible\nas too much information will be entirely missing between adjacent frames.\nInstead, we develop a hybrid imaging system, adding another standard video\ncamera to capture the temporal information. Given a 3 fps light field sequence\nand a standard 30 fps 2D video, our system can then generate a full light field\nvideo at 30 fps. We adopt a learning-based approach, which can be decomposed\ninto two steps: spatio-temporal flow estimation and appearance estimation. The\nflow estimation propagates the angular information from the light field\nsequence to the 2D video, so we can warp input images to the target view. The\nappearance estimation then combines these warped images to output the final\npixels. The whole process is trained end-to-end using convolutional neural\nnetworks. Experimental results demonstrate that our algorithm outperforms\ncurrent video interpolation methods, enabling consumer light field videography,\nand making applications such as refocusing and parallax view generation\nachievable on videos for the first time. \n\n"}
{"id": "1705.04279", "contents": "Title: Image-based immersed boundary model of the aortic root Abstract: Each year, approximately 300,000 heart valve repair or replacement procedures\nare performed worldwide, including approximately 70,000 aortic valve\nreplacement surgeries in the United States alone. This paper describes progress\nin constructing anatomically and physiologically realistic immersed boundary\n(IB) models of the dynamics of the aortic root and ascending aorta. This work\nbuilds on earlier IB models of fluid-structure interaction (FSI) in the aortic\nroot, which previously achieved realistic hemodynamics over multiple cardiac\ncycles, but which also were limited to simplified aortic geometries and\nidealized descriptions of the biomechanics of the aortic valve cusps. By\ncontrast, the model described herein uses an anatomical geometry reconstructed\nfrom patient-specific computed tomography angiography (CTA) data, and employs a\ndescription of the elasticity of the aortic valve leaflets based on a\nfiber-reinforced constitutive model fit to experimental tensile test data.\nNumerical tests show that the model is able to resolve the leaflet biomechanics\nin diastole and early systole at practical grid spacings. The model is also\nused to examine differences in the mechanics and fluid dynamics yielded by\nfresh valve leaflets and glutaraldehyde-fixed leaflets similar to those used in\nbioprosthetic heart valves. Although there are large differences in the leaflet\ndeformations during diastole, the differences in the open configurations of the\nvalve models are relatively small, and nearly identical hemodynamics are\nobtained in all cases considered. \n\n"}
{"id": "1705.04838", "contents": "Title: Revisiting IM2GPS in the Deep Learning Era Abstract: Image geolocalization, inferring the geographic location of an image, is a\nchallenging computer vision problem with many potential applications. The\nrecent state-of-the-art approach to this problem is a deep image classification\napproach in which the world is spatially divided into cells and a deep network\nis trained to predict the correct cell for a given image. We propose to combine\nthis approach with the original Im2GPS approach in which a query image is\nmatched against a database of geotagged images and the location is inferred\nfrom the retrieved set. We estimate the geographic location of a query image by\napplying kernel density estimation to the locations of its nearest neighbors in\nthe reference database. Interestingly, we find that the best features for our\nretrieval task are derived from networks trained with classification loss even\nthough we do not use a classification approach at test time. Training with\nclassification loss outperforms several deep feature learning methods (e.g.\nSiamese networks with contrastive of triplet loss) more typical for retrieval\napplications. Our simple approach achieves state-of-the-art geolocalization\naccuracy while also requiring significantly less training data. \n\n"}
{"id": "1705.06260", "contents": "Title: A deep level set method for image segmentation Abstract: This paper proposes a novel image segmentation approachthat integrates fully\nconvolutional networks (FCNs) with a level setmodel. Compared with a FCN, the\nintegrated method can incorporatesmoothing and prior information to achieve an\naccurate segmentation.Furthermore, different than using the level set model as\na post-processingtool, we integrate it into the training phase to fine-tune the\nFCN. Thisallows the use of unlabeled data during training in a\nsemi-supervisedsetting. Using two types of medical imaging data (liver CT and\nleft ven-tricle MRI data), we show that the integrated method achieves\ngoodperformance even when little training data is available, outperformingthe\nFCN or the level set model alone. \n\n"}
{"id": "1705.07057", "contents": "Title: Masked Autoregressive Flow for Density Estimation Abstract: Autoregressive models are among the best performing neural density\nestimators. We describe an approach for increasing the flexibility of an\nautoregressive model, based on modelling the random numbers that the model uses\ninternally when generating data. By constructing a stack of autoregressive\nmodels, each modelling the random numbers of the next model in the stack, we\nobtain a type of normalizing flow suitable for density estimation, which we\ncall Masked Autoregressive Flow. This type of flow is closely related to\nInverse Autoregressive Flow and is a generalization of Real NVP. Masked\nAutoregressive Flow achieves state-of-the-art performance in a range of\ngeneral-purpose density estimation tasks. \n\n"}
{"id": "1705.07204", "contents": "Title: Ensemble Adversarial Training: Attacks and Defenses Abstract: Adversarial examples are perturbed inputs designed to fool machine learning\nmodels. Adversarial training injects such examples into training data to\nincrease robustness. To scale this technique to large datasets, perturbations\nare crafted using fast single-step methods that maximize a linear approximation\nof the model's loss. We show that this form of adversarial training converges\nto a degenerate global minimum, wherein small curvature artifacts near the data\npoints obfuscate a linear approximation of the loss. The model thus learns to\ngenerate weak perturbations, rather than defend against strong ones. As a\nresult, we find that adversarial training remains vulnerable to black-box\nattacks, where we transfer perturbations computed on undefended models, as well\nas to a powerful novel single-step attack that escapes the non-smooth vicinity\nof the input data via a small random step. We further introduce Ensemble\nAdversarial Training, a technique that augments training data with\nperturbations transferred from other models. On ImageNet, Ensemble Adversarial\nTraining yields models with strong robustness to black-box attacks. In\nparticular, our most robust model won the first round of the NIPS 2017\ncompetition on Defenses against Adversarial Attacks. However, subsequent work\nfound that more elaborate black-box attacks could significantly enhance\ntransferability and reduce the accuracy of our models. \n\n"}
{"id": "1705.07263", "contents": "Title: Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection\n  Methods Abstract: Neural networks are known to be vulnerable to adversarial examples: inputs\nthat are close to natural inputs but classified incorrectly. In order to better\nunderstand the space of adversarial examples, we survey ten recent proposals\nthat are designed for detection and compare their efficacy. We show that all\ncan be defeated by constructing new loss functions. We conclude that\nadversarial examples are significantly harder to detect than previously\nappreciated, and the properties believed to be intrinsic to adversarial\nexamples are in fact not. Finally, we propose several simple guidelines for\nevaluating future proposed defenses. \n\n"}
{"id": "1705.08078", "contents": "Title: Patchnet: Interpretable Neural Networks for Image Classification Abstract: Understanding how a complex machine learning model makes a classification\ndecision is essential for its acceptance in sensitive areas such as health\ncare. Towards this end, we present PatchNet, a method that provides the\nfeatures indicative of each class in an image using a tradeoff between\nrestricting global image context and classification error. We mathematically\nanalyze this tradeoff, demonstrate Patchnet's ability to construct sharp visual\nheatmap representations of the learned features, and quantitatively compare\nthese features with features selected by domain experts by applying PatchNet to\nthe classification of benign/malignant skin lesions from the ISBI-ISIC 2017\nmelanoma classification challenge. \n\n"}
{"id": "1705.08551", "contents": "Title: Safe Model-based Reinforcement Learning with Stability Guarantees Abstract: Reinforcement learning is a powerful paradigm for learning optimal policies\nfrom experimental data. However, to find optimal policies, most reinforcement\nlearning algorithms explore all possible actions, which may be harmful for\nreal-world systems. As a consequence, learning algorithms are rarely applied on\nsafety-critical systems in the real world. In this paper, we present a learning\nalgorithm that explicitly considers safety, defined in terms of stability\nguarantees. Specifically, we extend control-theoretic results on Lyapunov\nstability verification and show how to use statistical models of the dynamics\nto obtain high-performance control policies with provable stability\ncertificates. Moreover, under additional regularity assumptions in terms of a\nGaussian process prior, we prove that one can effectively and safely collect\ndata in order to learn about the dynamics and thus both improve control\nperformance and expand the safe region of the state space. In our experiments,\nwe show how the resulting algorithm can safely optimize a neural network policy\non a simulated inverted pendulum, without the pendulum ever falling down. \n\n"}
{"id": "1705.08562", "contents": "Title: Hashing as Tie-Aware Learning to Rank Abstract: Hashing, or learning binary embeddings of data, is frequently used in nearest\nneighbor retrieval. In this paper, we develop learning to rank formulations for\nhashing, aimed at directly optimizing ranking-based evaluation metrics such as\nAverage Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We\nfirst observe that the integer-valued Hamming distance often leads to tied\nrankings, and propose to use tie-aware versions of AP and NDCG to evaluate\nhashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive\ntheir continuous relaxations, and perform gradient-based optimization with deep\nneural networks. Our results establish the new state-of-the-art for image\nretrieval by Hamming ranking in common benchmarks. \n\n"}
{"id": "1705.09552", "contents": "Title: Classification regions of deep neural networks Abstract: The goal of this paper is to analyze the geometric properties of deep neural\nnetwork classifiers in the input space. We specifically study the topology of\nclassification regions created by deep networks, as well as their associated\ndecision boundary. Through a systematic empirical investigation, we show that\nstate-of-the-art deep nets learn connected classification regions, and that the\ndecision boundary in the vicinity of datapoints is flat along most directions.\nWe further draw an essential connection between two seemingly unrelated\nproperties of deep networks: their sensitivity to additive perturbations in the\ninputs, and the curvature of their decision boundary. The directions where the\ndecision boundary is curved in fact remarkably characterize the directions to\nwhich the classifier is the most vulnerable. We finally leverage a fundamental\nasymmetry in the curvature of the decision boundary of deep nets, and propose a\nmethod to discriminate between original images, and images perturbed with small\nadversarial examples. We show the effectiveness of this purely geometric\napproach for detecting small adversarial perturbations in images, and for\nrecovering the labels of perturbed images. \n\n"}
{"id": "1705.10732", "contents": "Title: Deep manifold-to-manifold transforming network for action recognition Abstract: Symmetric positive definite (SPD) matrices (e.g., covariances, graph\nLaplacians, etc.) are widely used to model the relationship of spatial or\ntemporal domain. Nevertheless, SPD matrices are theoretically embedded on\nRiemannian manifolds. In this paper, we propose an end-to-end deep\nmanifold-to-manifold transforming network (DMT-Net) which can make SPD matrices\nflow from one Riemannian manifold to another more discriminative one. To learn\ndiscriminative SPD features characterizing both spatial and temporal\ndependencies, we specifically develop three novel layers on manifolds: (i) the\nlocal SPD convolutional layer, (ii) the non-linear SPD activation layer, and\n(iii) the Riemannian-preserved recursive layer. The SPD property is preserved\nthrough all layers without any requirement of singular value decomposition\n(SVD), which is often used in the existing methods with expensive computation\ncost. Furthermore, a diagonalizing SPD layer is designed to efficiently\ncalculate the final metric for the classification task. To evaluate our\nproposed method, we conduct extensive experiments on the task of action\nrecognition, where input signals are popularly modeled as SPD matrices. The\nexperimental results demonstrate that our DMT-Net is much more competitive over\nstate-of-the-art. \n\n"}
{"id": "1705.10882", "contents": "Title: Morphological Error Detection in 3D Segmentations Abstract: Deep learning algorithms for connectomics rely upon localized classification,\nrather than overall morphology. This leads to a high incidence of erroneously\nmerged objects. Humans, by contrast, can easily detect such errors by acquiring\nintuition for the correct morphology of objects. Biological neurons have\ncomplicated and variable shapes, which are challenging to learn, and merge\nerrors take a multitude of different forms. We present an algorithm, MergeNet,\nthat shows 3D ConvNets can, in fact, detect merge errors from high-level\nneuronal morphology. MergeNet follows unsupervised training and operates across\ndatasets. We demonstrate the performance of MergeNet both on a variety of\nconnectomics data and on a dataset created from merged MNIST images. \n\n"}
{"id": "1706.00530", "contents": "Title: Integrated Deep and Shallow Networks for Salient Object Detection Abstract: Deep convolutional neural network (CNN) based salient object detection\nmethods have achieved state-of-the-art performance and outperform those\nunsupervised methods with a wide margin. In this paper, we propose to integrate\ndeep and unsupervised saliency for salient object detection under a unified\nframework. Specifically, our method takes results of unsupervised saliency\n(Robust Background Detection, RBD) and normalized color images as inputs, and\ndirectly learns an end-to-end mapping between inputs and the corresponding\nsaliency maps. The color images are fed into a Fully Convolutional Neural\nNetworks (FCNN) adapted from semantic segmentation to exploit high-level\nsemantic cues for salient object detection. Then the results from deep FCNN and\nRBD are concatenated to feed into a shallow network to map the concatenated\nfeature maps to saliency maps. Finally, to obtain a spatially consistent\nsaliency map with sharp object boundaries, we fuse superpixel level saliency\nmap at multi-scale. Extensive experimental results on 8 benchmark datasets\ndemonstrate that the proposed method outperforms the state-of-the-art\napproaches with a margin. \n\n"}
{"id": "1706.01000", "contents": "Title: Image Compression Based on Compressive Sensing: End-to-End Comparison\n  with JPEG Abstract: We present an end-to-end image compression system based on compressive\nsensing. The presented system integrates the conventional scheme of compressive\nsampling and reconstruction with quantization and entropy coding. The\ncompression performance, in terms of decoded image quality versus data rate, is\nshown to be comparable with JPEG and significantly better at the low rate\nrange. We study the parameters that influence the system performance, including\n(i) the choice of sensing matrix, (ii) the trade-off between quantization and\ncompression ratio, and (iii) the reconstruction algorithms. We propose an\neffective method to jointly control the quantization step and compression ratio\nin order to achieve near optimal quality at any given bit rate. Furthermore,\nour proposed image compression system can be directly used in the compressive\nsensing camera, e.g. the single pixel camera, to construct a hardware\ncompressive sampling system. \n\n"}
{"id": "1706.01805", "contents": "Title: SegAN: Adversarial Network with Multi-scale $L_1$ Loss for Medical Image\n  Segmentation Abstract: Inspired by classic generative adversarial networks (GAN), we propose a novel\nend-to-end adversarial neural network, called SegAN, for the task of medical\nimage segmentation. Since image segmentation requires dense, pixel-level\nlabeling, the single scalar real/fake output of a classic GAN's discriminator\nmay be ineffective in producing stable and sufficient gradient feedback to the\nnetworks. Instead, we use a fully convolutional neural network as the segmentor\nto generate segmentation label maps, and propose a novel adversarial critic\nnetwork with a multi-scale $L_1$ loss function to force the critic and\nsegmentor to learn both global and local features that capture long- and\nshort-range spatial relationships between pixels. In our SegAN framework, the\nsegmentor and critic networks are trained in an alternating fashion in a\nmin-max game: The critic takes as input a pair of images, (original_image $*$\npredicted_label_map, original_image $*$ ground_truth_label_map), and then is\ntrained by maximizing a multi-scale loss function; The segmentor is trained\nwith only gradients passed along by the critic, with the aim to minimize the\nmulti-scale loss function. We show that such a SegAN framework is more\neffective and stable for the segmentation task, and it leads to better\nperformance than the state-of-the-art U-net segmentation method. We tested our\nSegAN method using datasets from the MICCAI BRATS brain tumor segmentation\nchallenge. Extensive experimental results demonstrate the effectiveness of the\nproposed SegAN with multi-scale loss: on BRATS 2013 SegAN gives performance\ncomparable to the state-of-the-art for whole tumor and tumor core segmentation\nwhile achieves better precision and sensitivity for Gd-enhance tumor core\nsegmentation; on BRATS 2015 SegAN achieves better performance than the\nstate-of-the-art in both dice score and precision. \n\n"}
{"id": "1706.02275", "contents": "Title: Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments Abstract: We explore deep reinforcement learning methods for multi-agent domains. We\nbegin by analyzing the difficulty of traditional algorithms in the multi-agent\ncase: Q-learning is challenged by an inherent non-stationarity of the\nenvironment, while policy gradient suffers from a variance that increases as\nthe number of agents grows. We then present an adaptation of actor-critic\nmethods that considers action policies of other agents and is able to\nsuccessfully learn policies that require complex multi-agent coordination.\nAdditionally, we introduce a training regimen utilizing an ensemble of policies\nfor each agent that leads to more robust multi-agent policies. We show the\nstrength of our approach compared to existing methods in cooperative as well as\ncompetitive scenarios, where agent populations are able to discover various\nphysical and informational coordination strategies. \n\n"}
{"id": "1706.02393", "contents": "Title: ShiftCNN: Generalized Low-Precision Architecture for Inference of\n  Convolutional Neural Networks Abstract: In this paper we introduce ShiftCNN, a generalized low-precision architecture\nfor inference of multiplierless convolutional neural networks (CNNs). ShiftCNN\nis based on a power-of-two weight representation and, as a result, performs\nonly shift and addition operations. Furthermore, ShiftCNN substantially reduces\ncomputational cost of convolutional layers by precomputing convolution terms.\nSuch an optimization can be applied to any CNN architecture with a relatively\nsmall codebook of weights and allows to decrease the number of product\noperations by at least two orders of magnitude. The proposed architecture\ntargets custom inference accelerators and can be realized on FPGAs or ASICs.\nExtensive evaluation on ImageNet shows that the state-of-the-art CNNs can be\nconverted without retraining into ShiftCNN with less than 1% drop in accuracy\nwhen the proposed quantization algorithm is employed. RTL simulations,\ntargeting modern FPGAs, show that power consumption of convolutional layers is\nreduced by a factor of 4 compared to conventional 8-bit fixed-point\narchitectures. \n\n"}
{"id": "1706.02416", "contents": "Title: Generalized Value Iteration Networks: Life Beyond Lattices Abstract: In this paper, we introduce a generalized value iteration network (GVIN),\nwhich is an end-to-end neural network planning module. GVIN emulates the value\niteration algorithm by using a novel graph convolution operator, which enables\nGVIN to learn and plan on irregular spatial graphs. We propose three novel\ndifferentiable kernels as graph convolution operators and show that the\nembedding based kernel achieves the best performance. We further propose\nepisodic Q-learning, an improvement upon traditional n-step Q-learning that\nstabilizes training for networks that contain a planning module. Lastly, we\nevaluate GVIN on planning problems in 2D mazes, irregular graphs, and\nreal-world street networks, showing that GVIN generalizes well for both\narbitrary graphs and unseen graphs of larger scale and outperforms a naive\ngeneralization of VIN (discretizing a spatial graph into a 2D image). \n\n"}
{"id": "1706.02631", "contents": "Title: Sliced Wasserstein Generative Models Abstract: In generative modeling, the Wasserstein distance (WD) has emerged as a useful\nmetric to measure the discrepancy between generated and real data\ndistributions. Unfortunately, it is challenging to approximate the WD of\nhigh-dimensional distributions. In contrast, the sliced Wasserstein distance\n(SWD) factorizes high-dimensional distributions into their multiple\none-dimensional marginal distributions and is thus easier to approximate. In\nthis paper, we introduce novel approximations of the primal and dual SWD.\nInstead of using a large number of random projections, as it is done by\nconventional SWD approximation methods, we propose to approximate SWDs with a\nsmall number of parameterized orthogonal projections in an end-to-end deep\nlearning fashion. As concrete applications of our SWD approximations, we design\ntwo types of differentiable SWD blocks to equip modern generative\nframeworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In\nthe experiments, we not only show the superiority of the proposed generative\nmodels on standard image synthesis benchmarks, but also demonstrate the\nstate-of-the-art performance on challenging high resolution image and video\ngeneration in an unsupervised manner. \n\n"}
{"id": "1706.02684", "contents": "Title: Learning Local Receptive Fields and their Weight Sharing Scheme on\n  Graphs Abstract: We propose a simple and generic layer formulation that extends the properties\nof convolutional layers to any domain that can be described by a graph. Namely,\nwe use the support of its adjacency matrix to design learnable weight sharing\nfilters able to exploit the underlying structure of signals in the same fashion\nas for images. The proposed formulation makes it possible to learn the weights\nof the filter as well as a scheme that controls how they are shared across the\ngraph. We perform validation experiments with image datasets and show that\nthese filters offer performances comparable with convolutional ones. \n\n"}
{"id": "1706.02863", "contents": "Title: Face Detection through Scale-Friendly Deep Convolutional Networks Abstract: In this paper, we share our experience in designing a convolutional\nnetwork-based face detector that could handle faces of an extremely wide range\nof scales. We show that faces with different scales can be modeled through a\nspecialized set of deep convolutional networks with different structures. These\ndetectors can be seamlessly integrated into a single unified network that can\nbe trained end-to-end. In contrast to existing deep models that are designed\nfor wide scale range, our network does not require an image pyramid input and\nthe model is of modest complexity. Our network, dubbed ScaleFace, achieves\npromising performance on WIDER FACE and FDDB datasets with practical runtime\nspeed. Specifically, our method achieves 76.4 average precision on the\nchallenging WIDER FACE dataset and 96% recall rate on the FDDB dataset with 7\nframes per second (fps) for 900 * 1300 input image. \n\n"}
{"id": "1706.03123", "contents": "Title: Diversity-aware Multi-Video Summarization Abstract: Most video summarization approaches have focused on extracting a summary from\na single video; we propose an unsupervised framework for summarizing a\ncollection of videos. We observe that each video in the collection may contain\nsome information that other videos do not have, and thus exploring the\nunderlying complementarity could be beneficial in creating a diverse\ninformative summary. We develop a novel diversity-aware sparse optimization\nmethod for multi-video summarization by exploring the complementarity within\nthe videos. Our approach extracts a multi-video summary which is both\ninteresting and representative in describing the whole video collection. To\nefficiently solve our optimization problem, we develop an alternating\nminimization algorithm that minimizes the overall objective function with\nrespect to one video at a time while fixing the other videos. Moreover, we\nintroduce a new benchmark dataset, Tour20, that contains 140 videos with\nmultiple human created summaries, which were acquired in a controlled\nexperiment. Finally, by extensive experiments on the new Tour20 dataset and\nseveral other multi-view datasets, we show that the proposed approach clearly\noutperforms the state-of-the-art methods on the two problems-topic-oriented\nvideo summarization and multi-view video summarization in a camera network. \n\n"}
{"id": "1706.03282", "contents": "Title: Segmentation of nearly isotropic overlapped tracks in photomicrographs\n  using successive erosions as watershed markers Abstract: The major challenges of automatic track counting are distinguishing tracks\nand material defects, identifying small tracks and defects of similar size, and\ndetecting overlapping tracks. Here we address the latter issue using WUSEM, an\nalgorithm which combines the watershed transform, morphological erosions and\nlabeling to separate regions in photomicrographs. WUSEM shows reliable results\nwhen used in photomicrographs presenting almost isotropic objects. We tested\nthis method in two datasets of diallyl phthalate (DAP) photomicrographs and\ncompared the results when counting manually and using the classic watershed.\nThe mean automatic/manual efficiency ratio when using WUSEM in the test\ndatasets is 0.97 +/- 0.11. \n\n"}
{"id": "1706.03702", "contents": "Title: Progressive and Multi-Path Holistically Nested Neural Networks for\n  Pathological Lung Segmentation from CT Images Abstract: Pathological lung segmentation (PLS) is an important, yet challenging,\nmedical image application due to the wide variability of pathological lung\nappearance and shape. Because PLS is often a pre-requisite for other imaging\nanalytics, methodological simplicity and generality are key factors in\nusability. Along those lines, we present a bottom-up deep-learning based\napproach that is expressive enough to handle variations in appearance, while\nremaining unaffected by any variations in shape. We incorporate the deeply\nsupervised learning framework, but enhance it with a simple, yet effective,\nprogressive multi-path scheme, which more reliably merges outputs from\ndifferent network stages. The result is a deep model able to produce finer\ndetailed masks, which we call progressive holistically-nested networks\n(P-HNNs). Using extensive cross-validation, our method is tested on\nmulti-institutional datasets comprising 929 CT scans (848 publicly available),\nof pathological lungs, reporting mean dice scores of 0.985 and demonstrating\nsignificant qualitative and quantitative improvements over state-of-the art\napproaches. \n\n"}
{"id": "1706.03860", "contents": "Title: Subspace Clustering via Optimal Direction Search Abstract: This letter presents a new spectral-clustering-based approach to the subspace\nclustering problem. Underpinning the proposed method is a convex program for\noptimal direction search, which for each data point d finds an optimal\ndirection in the span of the data that has minimum projection on the other data\npoints and non-vanishing projection on d. The obtained directions are\nsubsequently leveraged to identify a neighborhood set for each data point. An\nalternating direction method of multipliers framework is provided to\nefficiently solve for the optimal directions. The proposed method is shown to\nnotably outperform the existing subspace clustering methods, particularly for\nunwieldy scenarios involving high levels of noise and close subspaces, and\nyields the state-of-the-art results for the problem of face clustering using\nsubspace segmentation. \n\n"}
{"id": "1706.04701", "contents": "Title: Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong Abstract: Ongoing research has proposed several methods to defend neural networks\nagainst adversarial examples, many of which researchers have shown to be\nineffective. We ask whether a strong defense can be created by combining\nmultiple (possibly weak) defenses. To answer this question, we study three\ndefenses that follow this approach. Two of these are recently proposed defenses\nthat intentionally combine components designed to work well together. A third\ndefense combines three independent defenses. For all the components of these\ndefenses and the combined defenses themselves, we show that an adaptive\nadversary can create adversarial examples successfully with low distortion.\nThus, our work implies that ensemble of weak defenses is not sufficient to\nprovide strong defense against adversarial examples. \n\n"}
{"id": "1706.05098", "contents": "Title: An Overview of Multi-Task Learning in Deep Neural Networks Abstract: Multi-task learning (MTL) has led to successes in many applications of\nmachine learning, from natural language processing and speech recognition to\ncomputer vision and drug discovery. This article aims to give a general\noverview of MTL, particularly in deep neural networks. It introduces the two\nmost common methods for MTL in Deep Learning, gives an overview of the\nliterature, and discusses recent advances. In particular, it seeks to help ML\npractitioners apply MTL by shedding light on how MTL works and providing\nguidelines for choosing appropriate auxiliary tasks. \n\n"}
{"id": "1706.05296", "contents": "Title: Value-Decomposition Networks For Cooperative Multi-Agent Learning Abstract: We study the problem of cooperative multi-agent reinforcement learning with a\nsingle joint reward signal. This class of learning problems is difficult\nbecause of the often large combined action and observation spaces. In the fully\ncentralized and decentralized approaches, we find the problem of spurious\nrewards and a phenomenon we call the \"lazy agent\" problem, which arises due to\npartial observability. We address these problems by training individual agents\nwith a novel value decomposition network architecture, which learns to\ndecompose the team value function into agent-wise value functions. We perform\nan experimental evaluation across a range of partially-observable multi-agent\ndomains and show that learning such value-decompositions leads to superior\nresults, in particular when combined with weight sharing, role information and\ninformation channels. \n\n"}
{"id": "1706.05587", "contents": "Title: Rethinking Atrous Convolution for Semantic Image Segmentation Abstract: In this work, we revisit atrous convolution, a powerful tool to explicitly\nadjust filter's field-of-view as well as control the resolution of feature\nresponses computed by Deep Convolutional Neural Networks, in the application of\nsemantic image segmentation. To handle the problem of segmenting objects at\nmultiple scales, we design modules which employ atrous convolution in cascade\nor in parallel to capture multi-scale context by adopting multiple atrous\nrates. Furthermore, we propose to augment our previously proposed Atrous\nSpatial Pyramid Pooling module, which probes convolutional features at multiple\nscales, with image-level features encoding global context and further boost\nperformance. We also elaborate on implementation details and share our\nexperience on training our system. The proposed `DeepLabv3' system\nsignificantly improves over our previous DeepLab versions without DenseCRF\npost-processing and attains comparable performance with other state-of-art\nmodels on the PASCAL VOC 2012 semantic image segmentation benchmark. \n\n"}
{"id": "1706.06083", "contents": "Title: Towards Deep Learning Models Resistant to Adversarial Attacks Abstract: Recent work has demonstrated that deep neural networks are vulnerable to\nadversarial examples---inputs that are almost indistinguishable from natural\ndata and yet classified incorrectly by the network. In fact, some of the latest\nfindings suggest that the existence of adversarial attacks may be an inherent\nweakness of deep learning models. To address this problem, we study the\nadversarial robustness of neural networks through the lens of robust\noptimization. This approach provides us with a broad and unifying view on much\nof the prior work on this topic. Its principled nature also enables us to\nidentify methods for both training and attacking neural networks that are\nreliable and, in a certain sense, universal. In particular, they specify a\nconcrete security guarantee that would protect against any adversary. These\nmethods let us train networks with significantly improved resistance to a wide\nrange of adversarial attacks. They also suggest the notion of security against\na first-order adversary as a natural and broad security guarantee. We believe\nthat robustness against such well-defined classes of adversaries is an\nimportant stepping stone towards fully resistant deep learning models. Code and\npre-trained models are available at https://github.com/MadryLab/mnist_challenge\nand https://github.com/MadryLab/cifar10_challenge. \n\n"}
{"id": "1706.06275", "contents": "Title: Using Artificial Tokens to Control Languages for Multilingual Image\n  Caption Generation Abstract: Recent work in computer vision has yielded impressive results in\nautomatically describing images with natural language. Most of these systems\ngenerate captions in a sin- gle language, requiring multiple language-specific\nmodels to build a multilingual captioning system. We propose a very simple\ntechnique to build a single unified model across languages, using artificial\ntokens to control the language, making the captioning system more compact. We\nevaluate our approach on generating English and Japanese captions, and show\nthat a typical neural captioning architecture is capable of learning a single\nmodel that can switch between two different languages. \n\n"}
{"id": "1706.06551", "contents": "Title: Grounded Language Learning in a Simulated 3D World Abstract: We are increasingly surrounded by artificially intelligent technology that\ntakes decisions and executes actions on our behalf. This creates a pressing\nneed for general means to communicate with, instruct and guide artificial\nagents, with human language the most compelling means for such communication.\nTo achieve this in a scalable fashion, agents must be able to relate language\nto the world and to actions; that is, their understanding of language must be\ngrounded and embodied. However, learning grounded language is a notoriously\nchallenging problem in artificial intelligence research. Here we present an\nagent that learns to interpret language in a simulated 3D environment where it\nis rewarded for the successful execution of written instructions. Trained via a\ncombination of reinforcement and unsupervised learning, and beginning with\nminimal prior knowledge, the agent learns to relate linguistic symbols to\nemergent perceptual representations of its physical surroundings and to\npertinent sequences of actions. The agent's comprehension of language extends\nbeyond its prior experience, enabling it to apply familiar language to\nunfamiliar situations and to interpret entirely novel instructions. Moreover,\nthe speed with which this agent learns new words increases as its semantic\nknowledge grows. This facility for generalising and bootstrapping semantic\nknowledge indicates the potential of the present approach for reconciling\nambiguous natural language with the complexity of the physical world. \n\n"}
{"id": "1706.07351", "contents": "Title: An approach to reachability analysis for feed-forward ReLU neural\n  networks Abstract: We study the reachability problem for systems implemented as feed-forward\nneural networks whose activation function is implemented via ReLU functions. We\ndraw a correspondence between establishing whether some arbitrary output can\never be outputed by a neural system and linear problems characterising a neural\nsystem of interest. We present a methodology to solve cases of practical\ninterest by means of a state-of-the-art linear programs solver. We evaluate the\ntechnique presented by discussing the experimental results obtained by\nanalysing reachability properties for a number of benchmarks in the literature. \n\n"}
{"id": "1706.07901", "contents": "Title: Deep Mixture of Diverse Experts for Large-Scale Visual Recognition Abstract: In this paper, a deep mixture of diverse experts algorithm is developed for\nseamlessly combining a set of base deep CNNs (convolutional neural networks)\nwith diverse outputs (task spaces), e.g., such base deep CNNs are trained to\nrecognize different subsets of tens of thousands of atomic object classes.\nFirst, a two-layer (category layer and object class layer) ontology is\nconstructed to achieve more effective solution for task group generation, e.g.,\nassigning the semantically-related atomic object classes at the sibling leaf\nnodes into the same task group because they may share similar learning\ncomplexities. Second, one particular base deep CNNs with $M+1$ ($M \\leq 1,000$)\noutputs is learned for each task group to recognize its $M$ atomic object\nclasses effectively and identify one special class of \"not-in-group\"\nautomatically, and the network structure (numbers of layers and units in each\nlayer) of the well-designed AlexNet is directly used to configure such base\ndeep CNNs. A deep multi-task learning algorithm is developed to leverage the\ninter-class visual similarities to learn more discriminative base deep CNNs and\nmulti-task softmax for enhancing the separability of the atomic object classes\nin the same task group. Finally, all these base deep CNNs with diverse outputs\n(task spaces) are seamlessly combined to form a deep mixture of diverse experts\nfor recognizing tens of thousands of atomic object classes. Our experimental\nresults have demonstrated that our deep mixture of diverse experts algorithm\ncan achieve very competitive results on large-scale visual recognition. \n\n"}
{"id": "1706.08260", "contents": "Title: Deep Semantics-Aware Photo Adjustment Abstract: Automatic photo adjustment is to mimic the photo retouching style of\nprofessional photographers and automatically adjust photos to the learned\nstyle. There have been many attempts to model the tone and the color adjustment\nglobally with low-level color statistics. Also, spatially varying photo\nadjustment methods have been studied by exploiting high-level features and\nsemantic label maps. Those methods are semantics-aware since the color mapping\nis dependent on the high-level semantic context. However, their performance is\nlimited to the pre-computed hand-crafted features and it is hard to reflect\nuser's preference to the adjustment. In this paper, we propose a deep neural\nnetwork that models the semantics-aware photo adjustment. The proposed network\nexploits bilinear models that are the multiplicative interaction of the color\nand the contexual features. As the contextual features we propose the semantic\nadjustment map, which discovers the inherent photo retouching presets that are\napplied according to the scene context. The proposed method is trained using a\nrobust loss with a scene parsing task. The experimental results show that the\nproposed method outperforms the existing method both quantitatively and\nqualitatively. The proposed method also provides users a way to retouch the\nphoto by their own likings by giving customized adjustment maps. \n\n"}
{"id": "1706.09317", "contents": "Title: Alternative Semantic Representations for Zero-Shot Human Action\n  Recognition Abstract: A proper semantic representation for encoding side information is key to the\nsuccess of zero-shot learning. In this paper, we explore two alternative\nsemantic representations especially for zero-shot human action recognition:\ntextual descriptions of human actions and deep features extracted from still\nimages relevant to human actions. Such side information are accessible on Web\nwith little cost, which paves a new way in gaining side information for\nlarge-scale zero-shot human action recognition. We investigate different\nencoding methods to generate semantic representations for human actions from\nsuch side information. Based on our zero-shot visual recognition method, we\nconducted experiments on UCF101 and HMDB51 to evaluate two proposed semantic\nrepresentations . The results suggest that our proposed text- and image-based\nsemantic representations outperform traditional attributes and word vectors\nconsiderably for zero-shot human action recognition. In particular, the\nimage-based semantic representations yield the favourable performance even\nthough the representation is extracted from a small number of images per class. \n\n"}
{"id": "1706.09520", "contents": "Title: Neural SLAM: Learning to Explore with External Memory Abstract: We present an approach for agents to learn representations of a global map\nfrom sensor data, to aid their exploration in new environments. To achieve\nthis, we embed procedures mimicking that of traditional Simultaneous\nLocalization and Mapping (SLAM) into the soft attention based addressing of\nexternal memory architectures, in which the external memory acts as an internal\nrepresentation of the environment. This structure encourages the evolution of\nSLAM-like behaviors inside a completely differentiable deep neural network. We\nshow that this approach can help reinforcement learning agents to successfully\nexplore new environments where long-term memory is essential. We validate our\napproach in both challenging grid-world environments and preliminary Gazebo\nexperiments. A video of our experiments can be found at: https://goo.gl/G2Vu5y. \n\n"}
{"id": "1706.10217", "contents": "Title: SMC Faster R-CNN: Toward a scene-specialized multi-object detector Abstract: Generally, the performance of a generic detector decreases significantly when\nit is tested on a specific scene due to the large variation between the source\ntraining dataset and the samples from the target scene. To solve this problem,\nwe propose a new formalism of transfer learning based on the theory of a\nSequential Monte Carlo (SMC) filter to automatically specialize a\nscene-specific Faster R-CNN detector. The suggested framework uses different\nstrategies based on the SMC filter steps to approximate iteratively the target\ndistribution as a set of samples in order to specialize the Faster R-CNN\ndetector towards a target scene. Moreover, we put forward a likelihood function\nthat combines spatio-temporal information extracted from the target video\nsequence and the confidence-score given by the output layer of the Faster\nR-CNN, to favor the selection of target samples associated with the right\nlabel. The effectiveness of the suggested framework is demonstrated through\nexperiments on several public traffic datasets. Compared with the\nstate-of-the-art specialization frameworks, the proposed framework presents\nencouraging results for both single and multi-traffic object detections. \n\n"}
{"id": "1707.00433", "contents": "Title: Detection and Localization of Image Forgeries using Resampling Features\n  and Deep Learning Abstract: Resampling is an important signature of manipulated images. In this paper, we\npropose two methods to detect and localize image manipulations based on a\ncombination of resampling features and deep learning. In the first method, the\nRadon transform of resampling features are computed on overlapping image\npatches. Deep learning classifiers and a Gaussian conditional random field\nmodel are then used to create a heatmap. Tampered regions are located using a\nRandom Walker segmentation method. In the second method, resampling features\ncomputed on overlapping image patches are passed through a Long short-term\nmemory (LSTM) based network for classification and localization. We compare the\nperformance of detection/localization of both these methods. Our experimental\nresults show that both techniques are effective in detecting and localizing\ndigital image forgeries. \n\n"}
{"id": "1707.00703", "contents": "Title: Automated Problem Identification: Regression vs Classification via\n  Evolutionary Deep Networks Abstract: Regression or classification? This is perhaps the most basic question faced\nwhen tackling a new supervised learning problem. We present an Evolutionary\nDeep Learning (EDL) algorithm that automatically solves this by identifying the\nquestion type with high accuracy, along with a proposed deep architecture.\nTypically, a significant amount of human insight and preparation is required\nprior to executing machine learning algorithms. For example, when creating deep\nneural networks, the number of parameters must be selected in advance and\nfurthermore, a lot of these choices are made based upon pre-existing knowledge\nof the data such as the use of a categorical cross entropy loss function.\nHumans are able to study a dataset and decide whether it represents a\nclassification or a regression problem, and consequently make decisions which\nwill be applied to the execution of the neural network. We propose the\nAutomated Problem Identification (API) algorithm, which uses an evolutionary\nalgorithm interface to TensorFlow to manipulate a deep neural network to decide\nif a dataset represents a classification or a regression problem. We test API\non 16 different classification, regression and sentiment analysis datasets with\nup to 10,000 features and up to 17,000 unique target values. API achieves an\naverage accuracy of $96.3\\%$ in identifying the problem type without hardcoding\nany insights about the general characteristics of regression or classification\nproblems. For example, API successfully identifies classification problems even\nwith 1000 target values. Furthermore, the algorithm recommends which loss\nfunction to use and also recommends a neural network architecture. Our work is\ntherefore a step towards fully automated machine learning. \n\n"}
{"id": "1707.00819", "contents": "Title: Causal Consistency of Structural Equation Models Abstract: Complex systems can be modelled at various levels of detail. Ideally, causal\nmodels of the same system should be consistent with one another in the sense\nthat they agree in their predictions of the effects of interventions. We\nformalise this notion of consistency in the case of Structural Equation Models\n(SEMs) by introducing exact transformations between SEMs. This provides a\ngeneral language to consider, for instance, the different levels of description\nin the following three scenarios: (a) models with large numbers of variables\nversus models in which the `irrelevant' or unobservable variables have been\nmarginalised out; (b) micro-level models versus macro-level models in which the\nmacro-variables are aggregate features of the micro-variables; (c) dynamical\ntime series models versus models of their stationary behaviour. Our analysis\nstresses the importance of well specified interventions in the causal modelling\nprocess and sheds light on the interpretation of cyclic SEMs. \n\n"}
{"id": "1707.01058", "contents": "Title: Skeleton-aided Articulated Motion Generation Abstract: This work make the first attempt to generate articulated human motion\nsequence from a single image. On the one hand, we utilize paired inputs\nincluding human skeleton information as motion embedding and a single human\nimage as appearance reference, to generate novel motion frames, based on the\nconditional GAN infrastructure. On the other hand, a triplet loss is employed\nto pursue appearance-smoothness between consecutive frames. As the proposed\nframework is capable of jointly exploiting the image appearance space and\narticulated/kinematic motion space, it generates realistic articulated motion\nsequence, in contrast to most previous video generation methods which yield\nblurred motion effects. We test our model on two human action datasets\nincluding KTH and Human3.6M, and the proposed framework generates very\npromising results on both datasets. \n\n"}
{"id": "1707.01202", "contents": "Title: A Survey of Recent Advances in CNN-based Single Image Crowd Counting and\n  Density Estimation Abstract: Estimating count and density maps from crowd images has a wide range of\napplications such as video surveillance, traffic monitoring, public safety and\nurban planning. In addition, techniques developed for crowd counting can be\napplied to related tasks in other fields of study such as cell microscopy,\nvehicle counting and environmental survey. The task of crowd counting and\ndensity map estimation is riddled with many challenges such as occlusions,\nnon-uniform density, intra-scene and inter-scene variations in scale and\nperspective. Nevertheless, over the last few years, crowd count analysis has\nevolved from earlier methods that are often limited to small variations in\ncrowd density and scales to the current state-of-the-art methods that have\ndeveloped the ability to perform successfully on a wide range of scenarios. The\nsuccess of crowd counting methods in the recent years can be largely attributed\nto deep learning and publications of challenging datasets. In this paper, we\nprovide a comprehensive survey of recent Convolutional Neural Network (CNN)\nbased approaches that have demonstrated significant improvements over earlier\nmethods that rely largely on hand-crafted representations. First, we briefly\nreview the pioneering methods that use hand-crafted representations and then we\ndelve in detail into the deep learning-based approaches and recently published\ndatasets. Furthermore, we discuss the merits and drawbacks of existing\nCNN-based approaches and identify promising avenues of research in this rapidly\nevolving field. \n\n"}
{"id": "1707.01357", "contents": "Title: Improving Content-Invariance in Gated Autoencoders for 2D and 3D Object\n  Rotation Abstract: Content-invariance in mapping codes learned by GAEs is a useful feature for\nvarious relation learning tasks. In this paper we show that the\ncontent-invariance of mapping codes for images of 2D and 3D rotated objects can\nbe substantially improved by extending the standard GAE loss (symmetric\nreconstruction error) with a regularization term that penalizes the symmetric\ncross-reconstruction error. This error term involves reconstruction of pairs\nwith mapping codes obtained from other pairs exhibiting similar\ntransformations. Although this would principally require knowledge of the\ntransformations exhibited by training pairs, our experiments show that a\nbootstrapping approach can sidestep this issue, and that the regularization\nterm can effectively be used in an unsupervised setting. \n\n"}
{"id": "1707.01698", "contents": "Title: Automated Lane Detection in Crowds using Proximity Graphs Abstract: Studying the behavior of crowds is vital for understanding and predicting\nhuman interactions in public areas. Research has shown that, under certain\nconditions, large groups of people can form collective behavior patterns: local\ninteractions between individuals results in global movements patterns. To\ndetect these patterns in a crowd, we assume each person is carrying an on-body\ndevice that acts a local proximity sensor, e.g., smartphone or bluetooth badge,\nand represent the texture of the crowd as a proximity graph. Our goal is\nextract information about crowds from these proximity graphs. In this work, we\nfocus on one particular type of pattern: lane formation. We present a formal\ndefinition of a lane, proposed a simple probabilistic model that simulates\nlanes moving through a stationary crowd, and present an automated\nlane-detection method. Our preliminary results show that our method is able to\ndetect lanes of different shapes and sizes. We see our work as an initial step\ntowards rich pattern recognition using proximity graphs. \n\n"}
{"id": "1707.02290", "contents": "Title: TasselNet: Counting maize tassels in the wild via local counts\n  regression network Abstract: Accurately counting maize tassels is important for monitoring the growth\nstatus of maize plants. This tedious task, however, is still mainly done by\nmanual efforts. In the context of modern plant phenotyping, automating this\ntask is required to meet the need of large-scale analysis of genotype and\nphenotype. In recent years, computer vision technologies have experienced a\nsignificant breakthrough due to the emergence of large-scale datasets and\nincreased computational resources. Naturally image-based approaches have also\nreceived much attention in plant-related studies. Yet a fact is that most\nimage-based systems for plant phenotyping are deployed under controlled\nlaboratory environment. When transferring the application scenario to\nunconstrained in-field conditions, intrinsic and extrinsic variations in the\nwild pose great challenges for accurate counting of maize tassels, which goes\nbeyond the ability of conventional image processing techniques. This calls for\nfurther robust computer vision approaches to address in-field variations. This\npaper studies the in-field counting problem of maize tassels. To our knowledge,\nthis is the first time that a plant-related counting problem is considered\nusing computer vision technologies under unconstrained field-based environment. \n\n"}
{"id": "1707.03194", "contents": "Title: Sensitivity Analysis for Mirror-Stratifiable Convex Functions Abstract: This paper provides a set of sensitivity analysis and activity identification\nresults for a class of convex functions with a strong geometric structure, that\nwe coined \"mirror-stratifiable\". These functions are such that there is a\nbijection between a primal and a dual stratification of the space into\npartitioning sets, called strata. This pairing is crucial to track the strata\nthat are identifiable by solutions of parametrized optimization problems or by\niterates of optimization algorithms. This class of functions encompasses all\nregularizers routinely used in signal and image processing, machine learning,\nand statistics. We show that this \"mirror-stratifiable\" structure enjoys a nice\nsensitivity theory, allowing us to study stability of solutions of optimization\nproblems to small perturbations, as well as activity identification of\nfirst-order proximal splitting-type algorithms. Existing results in the\nliterature typically assume that, under a non-degeneracy condition, the active\nset associated to a minimizer is stable to small perturbations and is\nidentified in finite time by optimization schemes. In contrast, our results do\nnot require any non-degeneracy assumption: in consequence, the optimal active\nset is not necessarily stable anymore, but we are able to track precisely the\nset of identifiable strata.We show that these results have crucial implications\nwhen solving challenging ill-posed inverse problems via regularization, a\ntypical scenario where the non-degeneracy condition is not fulfilled. Our\ntheoretical results, illustrated by numerical simulations, allow to\ncharacterize the instability behaviour of the regularized solutions, by\nlocating the set of all low-dimensional strata that can be potentially\nidentified by these solutions. \n\n"}
{"id": "1707.03497", "contents": "Title: Value Prediction Network Abstract: This paper proposes a novel deep reinforcement learning (RL) architecture,\ncalled Value Prediction Network (VPN), which integrates model-free and\nmodel-based RL methods into a single neural network. In contrast to typical\nmodel-based RL methods, VPN learns a dynamics model whose abstract states are\ntrained to make option-conditional predictions of future values (discounted sum\nof rewards) rather than of future observations. Our experimental results show\nthat VPN has several advantages over both model-free and model-based baselines\nin a stochastic environment where careful planning is required but building an\naccurate observation-prediction model is difficult. Furthermore, VPN\noutperforms Deep Q-Network (DQN) on several Atari games even with\nshort-lookahead planning, demonstrating its potential as a new way of learning\na good state representation. \n\n"}
{"id": "1707.03501", "contents": "Title: NO Need to Worry about Adversarial Examples in Object Detection in\n  Autonomous Vehicles Abstract: It has been shown that most machine learning algorithms are susceptible to\nadversarial perturbations. Slightly perturbing an image in a carefully chosen\ndirection in the image space may cause a trained neural network model to\nmisclassify it. Recently, it was shown that physical adversarial examples\nexist: printing perturbed images then taking pictures of them would still\nresult in misclassification. This raises security and safety concerns.\n  However, these experiments ignore a crucial property of physical objects: the\ncamera can view objects from different distances and at different angles. In\nthis paper, we show experiments that suggest that current constructions of\nphysical adversarial examples do not disrupt object detection from a moving\nplatform. Instead, a trained neural network classifies most of the pictures\ntaken from different distances and angles of a perturbed image correctly. We\nbelieve this is because the adversarial property of the perturbation is\nsensitive to the scale at which the perturbed picture is viewed, so (for\nexample) an autonomous car will misclassify a stop sign only from a small range\nof distances.\n  Our work raises an important question: can one construct examples that are\nadversarial for many or most viewing conditions? If so, the construction should\noffer very significant insights into the internal representation of patterns by\ndeep networks. If not, there is a good prospect that adversarial examples can\nbe reduced to a curiosity with little practical impact. \n\n"}
{"id": "1707.03979", "contents": "Title: A Brief Study of In-Domain Transfer and Learning from Fewer Samples\n  using A Few Simple Priors Abstract: Domain knowledge can often be encoded in the structure of a network, such as\nconvolutional layers for vision, which has been shown to increase\ngeneralization and decrease sample complexity, or the number of samples\nrequired for successful learning. In this study, we ask whether sample\ncomplexity can be reduced for systems where the structure of the domain is\nunknown beforehand, and the structure and parameters must both be learned from\nthe data. We show that sample complexity reduction through learning structure\nis possible for at least two simple cases. In studying these cases, we also\ngain insight into how this might be done for more complex domains. \n\n"}
{"id": "1707.03985", "contents": "Title: Towards End-to-end Text Spotting with Convolutional Recurrent Neural\n  Networks Abstract: In this work, we jointly address the problem of text detection and\nrecognition in natural scene images based on convolutional recurrent neural\nnetworks. We propose a unified network that simultaneously localizes and\nrecognizes text with a single forward pass, avoiding intermediate processes\nlike image cropping and feature re-calculation, word separation, or character\ngrouping. In contrast to existing approaches that consider text detection and\nrecognition as two distinct tasks and tackle them one by one, the proposed\nframework settles these two tasks concurrently. The whole framework can be\ntrained end-to-end, requiring only images, the ground-truth bounding boxes and\ntext labels. Through end-to-end training, the learned features can be more\ninformative, which improves the overall performance. The convolutional features\nare calculated only once and shared by both detection and recognition, which\nsaves processing time. Our proposed method has achieved competitive performance\non several benchmark datasets. \n\n"}
{"id": "1707.04993", "contents": "Title: MoCoGAN: Decomposing Motion and Content for Video Generation Abstract: Visual signals in a video can be divided into content and motion. While\ncontent specifies which objects are in the video, motion describes their\ndynamics. Based on this prior, we propose the Motion and Content decomposed\nGenerative Adversarial Network (MoCoGAN) framework for video generation. The\nproposed framework generates a video by mapping a sequence of random vectors to\na sequence of video frames. Each random vector consists of a content part and a\nmotion part. While the content part is kept fixed, the motion part is realized\nas a stochastic process. To learn motion and content decomposition in an\nunsupervised manner, we introduce a novel adversarial learning scheme utilizing\nboth image and video discriminators. Extensive experimental results on several\nchallenging datasets with qualitative and quantitative comparison to the\nstate-of-the-art approaches, verify effectiveness of the proposed framework. In\naddition, we show that MoCoGAN allows one to generate videos with same content\nbut different motion as well as videos with different content and same motion. \n\n"}
{"id": "1707.05474", "contents": "Title: APE-GAN: Adversarial Perturbation Elimination with GAN Abstract: Although neural networks could achieve state-of-the-art performance while\nrecongnizing images, they often suffer a tremendous defeat from adversarial\nexamples--inputs generated by utilizing imperceptible but intentional\nperturbation to clean samples from the datasets. How to defense against\nadversarial examples is an important problem which is well worth researching.\nSo far, very few methods have provided a significant defense to adversarial\nexamples. In this paper, a novel idea is proposed and an effective framework\nbased Generative Adversarial Nets named APE-GAN is implemented to defense\nagainst the adversarial examples. The experimental results on three benchmark\ndatasets including MNIST, CIFAR10 and ImageNet indicate that APE-GAN is\neffective to resist adversarial examples generated from five attacks. \n\n"}
{"id": "1707.05574", "contents": "Title: One-shot Face Recognition by Promoting Underrepresented Classes Abstract: In this paper, we study the problem of training large-scale face\nidentification model with imbalanced training data. This problem naturally\nexists in many real scenarios including large-scale celebrity recognition,\nmovie actor annotation, etc. Our solution contains two components. First, we\nbuild a face feature extraction model, and improve its performance, especially\nfor the persons with very limited training samples, by introducing a\nregularizer to the cross entropy loss for the multi-nomial logistic regression\n(MLR) learning. This regularizer encourages the directions of the face features\nfrom the same class to be close to the direction of their corresponding\nclassification weight vector in the logistic regression. Second, we build a\nmulti-class classifier using MLR on top of the learned face feature extraction\nmodel. Since the standard MLR has poor generalization capability for the\none-shot classes even if these classes have been oversampled, we propose a\nnovel supervision signal called underrepresented-classes promotion loss, which\naligns the norms of the weight vectors of the one-shot classes (a.k.a.\nunderrepresented-classes) to those of the normal classes. In addition to the\noriginal cross entropy loss, this new loss term effectively promotes the\nunderrepresented classes in the learned model and leads to a remarkable\nimprovement in face recognition performance.\n  We test our solution on the MS-Celeb-1M low-shot learning benchmark task. Our\nsolution recognizes 94.89% of the test images at the precision of 99\\% for the\none-shot classes. To the best of our knowledge, this is the best performance\namong all the published methods using this benchmark task with the same setup,\nincluding all the participants in the recent MS-Celeb-1M challenge at ICCV\n2017. \n\n"}
{"id": "1707.05776", "contents": "Title: Optimizing the Latent Space of Generative Networks Abstract: Generative Adversarial Networks (GANs) have achieved remarkable results in\nthe task of generating realistic natural images. In most successful\napplications, GAN models share two common aspects: solving a challenging saddle\npoint optimization problem, interpreted as an adversarial game between a\ngenerator and a discriminator functions; and parameterizing the generator and\nthe discriminator as deep convolutional neural networks. The goal of this paper\nis to disentangle the contribution of these two factors to the success of GANs.\nIn particular, we introduce Generative Latent Optimization (GLO), a framework\nto train deep convolutional generators using simple reconstruction losses.\nThroughout a variety of experiments, we show that GLO enjoys many of the\ndesirable properties of GANs: synthesizing visually-appealing samples,\ninterpolating meaningfully between samples, and performing linear arithmetic\nwith noise vectors; all of this without the adversarial optimization scheme. \n\n"}
{"id": "1707.06203", "contents": "Title: Imagination-Augmented Agents for Deep Reinforcement Learning Abstract: We introduce Imagination-Augmented Agents (I2As), a novel architecture for\ndeep reinforcement learning combining model-free and model-based aspects. In\ncontrast to most existing model-based reinforcement learning and planning\nmethods, which prescribe how a model should be used to arrive at a policy, I2As\nlearn to interpret predictions from a learned environment model to construct\nimplicit plans in arbitrary ways, by using the predictions as additional\ncontext in deep policy networks. I2As show improved data efficiency,\nperformance, and robustness to model misspecification compared to several\nbaselines. \n\n"}
{"id": "1707.06347", "contents": "Title: Proximal Policy Optimization Algorithms Abstract: We propose a new family of policy gradient methods for reinforcement\nlearning, which alternate between sampling data through interaction with the\nenvironment, and optimizing a \"surrogate\" objective function using stochastic\ngradient ascent. Whereas standard policy gradient methods perform one gradient\nupdate per data sample, we propose a novel objective function that enables\nmultiple epochs of minibatch updates. The new methods, which we call proximal\npolicy optimization (PPO), have some of the benefits of trust region policy\noptimization (TRPO), but they are much simpler to implement, more general, and\nhave better sample complexity (empirically). Our experiments test PPO on a\ncollection of benchmark tasks, including simulated robotic locomotion and Atari\ngame playing, and we show that PPO outperforms other online policy gradient\nmethods, and overall strikes a favorable balance between sample complexity,\nsimplicity, and wall-time. \n\n"}
{"id": "1707.06427", "contents": "Title: Scalable Full Flow with Learned Binary Descriptors Abstract: We propose a method for large displacement optical flow in which local\nmatching costs are learned by a convolutional neural network (CNN) and a\nsmoothness prior is imposed by a conditional random field (CRF). We tackle the\ncomputation- and memory-intensive operations on the 4D cost volume by a\nmin-projection which reduces memory complexity from quadratic to linear and\nbinary descriptors for efficient matching. This enables evaluation of the cost\non the fly and allows to perform learning and CRF inference on high resolution\nimages without ever storing the 4D cost volume. To address the problem of\nlearning binary descriptors we propose a new hybrid learning scheme. In\ncontrast to current state of the art approaches for learning binary CNNs we can\ncompute the exact non-zero gradient within our model. We compare several\nmethods for training binary descriptors and show results on public available\nbenchmarks. \n\n"}
{"id": "1707.06440", "contents": "Title: A Novel Space-Time Representation on the Positive Semidefinite Con for\n  Facial Expression Recognition Abstract: In this paper, we study the problem of facial expression recognition using a\nnovel space-time geometric representation. We describe the temporal evolution\nof facial landmarks as parametrized trajectories on the Riemannian manifold of\npositive semidefinite matrices of fixed-rank. Our representation has the\nadvantage to bring naturally a second desirable quantity when comparing shapes\n-- the spatial covariance -- in addition to the conventional affine-shape\nrepresentation. We derive then geometric and computational tools for\nrate-invariant analysis and adaptive re-sampling of trajectories, grounding on\nthe Riemannian geometry of the manifold. Specifically, our approach involves\nthree steps: 1) facial landmarks are first mapped into the Riemannian manifold\nof positive semidefinite matrices of rank 2, to build time-parameterized\ntrajectories; 2) a temporal alignment is performed on the trajectories,\nproviding a geometry-aware (dis-)similarity measure between them; 3) finally,\npairwise proximity function SVM (ppfSVM) is used to classify them,\nincorporating the latter (dis-)similarity measure into the kernel function. We\nshow the effectiveness of the proposed approach on four publicly available\nbenchmarks (CK+, MMI, Oulu-CASIA, and AFEW). The results of the proposed\napproach are comparable to or better than the state-of-the-art methods when\ninvolving only facial landmarks. \n\n"}
{"id": "1707.06719", "contents": "Title: Generalized Convolutional Neural Networks for Point Cloud Data Abstract: The introduction of cheap RGB-D cameras, stereo cameras, and LIDAR devices\nhas given the computer vision community 3D information that conventional RGB\ncameras cannot provide. This data is often stored as a point cloud. In this\npaper, we present a novel method to apply the concept of convolutional neural\nnetworks to this type of data. By creating a mapping of nearest neighbors in a\ndataset, and individually applying weights to spatial relationships between\npoints, we achieve an architecture that works directly with point clouds, but\nclosely resembles a convolutional neural net in both design and behavior. Such\na method bypasses the need for extensive feature engineering, while proving to\nbe computationally efficient and requiring few parameters. \n\n"}
{"id": "1707.06865", "contents": "Title: Retinal Microaneurysms Detection using Local Convergence Index Features Abstract: Retinal microaneurysms are the earliest clinical sign of diabetic retinopathy\ndisease. Detection of microaneurysms is crucial for the early diagnosis of\ndiabetic retinopathy and prevention of blindness. In this paper, a novel and\nreliable method for automatic detection of microaneurysms in retinal images is\nproposed. In the first stage of the proposed method, several preliminary\nmicroaneurysm candidates are extracted using a gradient weighting technique and\nan iterative thresholding approach. In the next stage, in addition to intensity\nand shape descriptors, a new set of features based on local convergence index\nfilters is extracted for each candidate. Finally, the collective set of\nfeatures is fed to a hybrid sampling/boosting classifier to discriminate the\nMAs from non-MAs candidates. The method is evaluated on images with different\nresolutions and modalities (RGB and SLO) using five publicly available datasets\nincluding the Retinopathy Online Challenge's dataset. The proposed method\nachieves an average sensitivity score of 0.471 on the ROC dataset outperforming\nstate-of-the-art approaches in an extensive comparison. The experimental\nresults on the other four datasets demonstrate the effectiveness and robustness\nof the proposed microaneurysms detection method regardless of different image\nresolutions and modalities. \n\n"}
{"id": "1707.07336", "contents": "Title: Person Re-identification Using Visual Attention Abstract: Despite recent attempts for solving the person re-identification problem, it\nremains a challenging task since a person's appearance can vary significantly\nwhen large variations in view angle, human pose, and illumination are involved.\nIn this paper, we propose a novel approach based on using a gradient-based\nattention mechanism in deep convolution neural network for solving the person\nre-identification problem. Our model learns to focus selectively on parts of\nthe input image for which the networks' output is most sensitive to and\nprocesses them with high resolution while perceiving the surrounding image in\nlow resolution. Extensive comparative evaluations demonstrate that the proposed\nmethod outperforms state-of-the-art approaches on the challenging CUHK01,\nCUHK03, and Market 1501 datasets. \n\n"}
{"id": "1707.07341", "contents": "Title: Prediction-Constrained Training for Semi-Supervised Mixture and Topic\n  Models Abstract: Supervisory signals have the potential to make low-dimensional data\nrepresentations, like those learned by mixture and topic models, more\ninterpretable and useful. We propose a framework for training latent variable\nmodels that explicitly balances two goals: recovery of faithful generative\nexplanations of high-dimensional data, and accurate prediction of associated\nsemantic labels. Existing approaches fail to achieve these goals due to an\nincomplete treatment of a fundamental asymmetry: the intended application is\nalways predicting labels from data, not data from labels. Our\nprediction-constrained objective for training generative models coherently\nintegrates loss-based supervisory signals while enabling effective\nsemi-supervised learning from partially labeled data. We derive learning\nalgorithms for semi-supervised mixture and topic models using stochastic\ngradient descent with automatic differentiation. We demonstrate improved\nprediction quality compared to several previous supervised topic models,\nachieving predictions competitive with high-dimensional logistic regression on\ntext sentiment analysis and electronic health records tasks while\nsimultaneously learning interpretable topics. \n\n"}
{"id": "1707.07397", "contents": "Title: Synthesizing Robust Adversarial Examples Abstract: Standard methods for generating adversarial examples for neural networks do\nnot consistently fool neural network classifiers in the physical world due to a\ncombination of viewpoint shifts, camera noise, and other natural\ntransformations, limiting their relevance to real-world systems. We demonstrate\nthe existence of robust 3D adversarial objects, and we present the first\nalgorithm for synthesizing examples that are adversarial over a chosen\ndistribution of transformations. We synthesize two-dimensional adversarial\nimages that are robust to noise, distortion, and affine transformation. We\napply our algorithm to complex three-dimensional objects, using 3D-printing to\nmanufacture the first physical adversarial objects. Our results demonstrate the\nexistence of 3D adversarial objects in the physical world. \n\n"}
{"id": "1707.07548", "contents": "Title: Towards Accurate Markerless Human Shape and Pose Estimation over Time Abstract: Existing marker-less motion capture methods often assume known backgrounds,\nstatic cameras, and sequence specific motion priors, which narrows its\napplication scenarios. Here we propose a fully automatic method that given\nmulti-view video, estimates 3D human motion and body shape. We take recent\nSMPLify \\cite{bogo2016keep} as the base method, and extend it in several ways.\nFirst we fit the body to 2D features detected in multi-view images. Second, we\nuse a CNN method to segment the person in each image and fit the 3D body model\nto the contours to further improves accuracy. Third we utilize a generic and\nrobust DCT temporal prior to handle the left and right side swapping issue\nsometimes introduced by the 2D pose estimator. Validation on standard\nbenchmarks shows our results are comparable to the state of the art and also\nprovide a realistic 3D shape avatar. We also demonstrate accurate results on\nHumanEva and on challenging dance sequences from YouTube in monocular case. \n\n"}
{"id": "1707.08814", "contents": "Title: Representation-Aggregation Networks for Segmentation of Multi-Gigapixel\n  Histology Images Abstract: Convolutional Neural Network (CNN) models have become the state-of-the-art\nfor most computer vision tasks with natural images. However, these are not best\nsuited for multi-gigapixel resolution Whole Slide Images (WSIs) of histology\nslides due to large size of these images. Current approaches construct smaller\npatches from WSIs which results in the loss of contextual information. We\npropose to capture the spatial context using novel Representation-Aggregation\nNetwork (RAN) for segmentation purposes, wherein the first network learns\npatch-level representation and the second network aggregates context from a\ngrid of neighbouring patches. We can use any CNN for representation learning,\nand can utilize CNN or 2D-Long Short Term Memory (2D-LSTM) for\ncontext-aggregation. Our method significantly outperformed conventional\npatch-based CNN approaches on segmentation of tumour in WSIs of breast cancer\ntissue sections. \n\n"}
{"id": "1707.08945", "contents": "Title: Robust Physical-World Attacks on Deep Learning Models Abstract: Recent studies show that the state-of-the-art deep neural networks (DNNs) are\nvulnerable to adversarial examples, resulting from small-magnitude\nperturbations added to the input. Given that that emerging physical systems are\nusing DNNs in safety-critical situations, adversarial examples could mislead\nthese systems and cause dangerous situations.Therefore, understanding\nadversarial examples in the physical world is an important step towards\ndeveloping resilient learning algorithms. We propose a general attack\nalgorithm,Robust Physical Perturbations (RP2), to generate robust visual\nadversarial perturbations under different physical conditions. Using the\nreal-world case of road sign classification, we show that adversarial examples\ngenerated using RP2 achieve high targeted misclassification rates against\nstandard-architecture road sign classifiers in the physical world under various\nenvironmental conditions, including viewpoints. Due to the current lack of a\nstandardized testing method, we propose a two-stage evaluation methodology for\nrobust physical adversarial examples consisting of lab and field tests. Using\nthis methodology, we evaluate the efficacy of physical adversarial\nmanipulations on real objects. Witha perturbation in the form of only black and\nwhite stickers,we attack a real stop sign, causing targeted misclassification\nin 100% of the images obtained in lab settings, and in 84.8%of the captured\nvideo frames obtained on a moving vehicle(field test) for the target\nclassifier. \n\n"}
{"id": "1707.09240", "contents": "Title: Human Pose Forecasting via Deep Markov Models Abstract: Human pose forecasting is an important problem in computer vision with\napplications to human-robot interaction, visual surveillance, and autonomous\ndriving. Usually, forecasting algorithms use 3D skeleton sequences and are\ntrained to forecast for a few milliseconds into the future. Long-range\nforecasting is challenging due to the difficulty of estimating how long a\nperson continues an activity. To this end, our contributions are threefold: (i)\nwe propose a generative framework for poses using variational autoencoders\nbased on Deep Markov Models (DMMs); (ii) we evaluate our pose forecasts using a\npose-based action classifier, which we argue better reflects the subjective\nquality of pose forecasts than distance in coordinate space; (iii) last, for\nevaluation of the new model, we introduce a 480,000-frame video dataset called\nIkea Furniture Assembly (Ikea FA), which depicts humans repeatedly assembling\nand disassembling furniture. We demonstrate promising results for our approach\non both Ikea FA and the existing NTU RGB+D dataset. \n\n"}
{"id": "1707.09457", "contents": "Title: Men Also Like Shopping: Reducing Gender Bias Amplification using\n  Corpus-level Constraints Abstract: Language is increasingly being used to define rich visual recognition\nproblems with supporting image collections sourced from the web. Structured\nprediction models are used in these tasks to take advantage of correlations\nbetween co-occurring labels and visual input but risk inadvertently encoding\nsocial biases found in web corpora. In this work, we study data and models\nassociated with multilabel object classification and visual semantic role\nlabeling. We find that (a) datasets for these tasks contain significant gender\nbias and (b) models trained on these datasets further amplify existing bias.\nFor example, the activity cooking is over 33% more likely to involve females\nthan males in a training set, and a trained model further amplifies the\ndisparity to 68% at test time. We propose to inject corpus-level constraints\nfor calibrating existing structured prediction models and design an algorithm\nbased on Lagrangian relaxation for collective inference. Our method results in\nalmost no performance loss for the underlying recognition task but decreases\nthe magnitude of bias amplification by 47.5% and 40.5% for multilabel\nclassification and visual semantic role labeling, respectively. \n\n"}
{"id": "1707.09482", "contents": "Title: Deep Feature Consistent Deep Image Transformations: Downscaling,\n  Decolorization and HDR Tone Mapping Abstract: Building on crucial insights into the determining factors of the visual\nintegrity of an image and the property of deep convolutional neural network\n(CNN), we have developed the Deep Feature Consistent Deep Image Transformation\n(DFC-DIT) framework which unifies challenging one-to-many mapping image\nprocessing problems such as image downscaling, decolorization (colour to\ngrayscale conversion) and high dynamic range (HDR) image tone mapping. We train\none CNN as a non-linear mapper to transform an input image to an output image\nfollowing what we term the deep feature consistency principle which is enforced\nthrough another pretrained and fixed deep CNN. This is the first work that uses\ndeep learning to solve and unify these three common image processing tasks. We\npresent experimental results to demonstrate the effectiveness of the DFC-DIT\ntechnique and its state of the art performances. \n\n"}
{"id": "1707.09733", "contents": "Title: Camera Relocalization by Computing Pairwise Relative Poses Using\n  Convolutional Neural Network Abstract: We propose a new deep learning based approach for camera relocalization. Our\napproach localizes a given query image by using a convolutional neural network\n(CNN) for first retrieving similar database images and then predicting the\nrelative pose between the query and the database images, whose poses are known.\nThe camera location for the query image is obtained via triangulation from two\nrelative translation estimates using a RANSAC based approach. Each relative\npose estimate provides a hypothesis for the camera orientation and they are\nfused in a second RANSAC scheme. The neural network is trained for relative\npose estimation in an end-to-end manner using training image pairs. In contrast\nto previous work, our approach does not require scene-specific training of the\nnetwork, which improves scalability, and it can also be applied to scenes which\nare not available during the training of the network. As another main\ncontribution, we release a challenging indoor localisation dataset covering 5\ndifferent scenes registered to a common coordinate frame. We evaluate our\napproach using both our own dataset and the standard 7 Scenes benchmark. The\nresults show that the proposed approach generalizes well to previously unseen\nscenes and compares favourably to other recent CNN-based methods. \n\n"}
{"id": "1707.09747", "contents": "Title: Synthesis of Positron Emission Tomography (PET) Images via Multi-channel\n  Generative Adversarial Networks (GANs) Abstract: Positron emission tomography (PET) image synthesis plays an important role,\nwhich can be used to boost the training data for computer aided diagnosis\nsystems. However, existing image synthesis methods have problems in\nsynthesizing the low resolution PET images. To address these limitations, we\npropose multi-channel generative adversarial networks (M-GAN) based PET image\nsynthesis method. Different to the existing methods which rely on using\nlow-level features, the proposed M-GAN is capable to represent the features in\na high-level of semantic based on the adversarial learning concept. In\naddition, M-GAN enables to take the input from the annotation (label) to\nsynthesize the high uptake regions e.g., tumors and from the computed\ntomography (CT) images to constrain the appearance consistency and output the\nsynthetic PET images directly. Our results on 50 lung cancer PET-CT studies\nindicate that our method was much closer to the real PET images when compared\nwith the existing methods. \n\n"}
{"id": "1707.09813", "contents": "Title: 2D-3D Fully Convolutional Neural Networks for Cardiac MR Segmentation Abstract: In this paper, we develop a 2D and 3D segmentation pipelines for fully\nautomated cardiac MR image segmentation using Deep Convolutional Neural\nNetworks (CNN). Our models are trained end-to-end from scratch using the ACD\nChallenge 2017 dataset comprising of 100 studies, each containing Cardiac MR\nimages in End Diastole and End Systole phase. We show that both our\nsegmentation models achieve near state-of-the-art performance scores in terms\nof distance metrics and have convincing accuracy in terms of clinical\nparameters. A comparative analysis is provided by introducing a novel dice loss\nfunction and its combination with cross entropy loss. By exploring different\nnetwork structures and comprehensive experiments, we discuss several key\ninsights to obtain optimal model performance, which also is central to the\ntheme of this challenge. \n\n"}
{"id": "1708.00945", "contents": "Title: Predicting Human Activities Using Stochastic Grammar Abstract: This paper presents a novel method to predict future human activities from\npartially observed RGB-D videos. Human activity prediction is generally\ndifficult due to its non-Markovian property and the rich context between human\nand environments.\n  We use a stochastic grammar model to capture the compositional structure of\nevents, integrating human actions, objects, and their affordances. We represent\nthe event by a spatial-temporal And-Or graph (ST-AOG). The ST-AOG is composed\nof a temporal stochastic grammar defined on sub-activities, and spatial graphs\nrepresenting sub-activities that consist of human actions, objects, and their\naffordances. Future sub-activities are predicted using the temporal grammar and\nEarley parsing algorithm. The corresponding action, object, and affordance\nlabels are then inferred accordingly. Extensive experiments are conducted to\nshow the effectiveness of our model on both semantic event parsing and future\nactivity prediction. \n\n"}
{"id": "1708.01547", "contents": "Title: Lifelong Learning with Dynamically Expandable Networks Abstract: We propose a novel deep network architecture for lifelong learning which we\nrefer to as Dynamically Expandable Network (DEN), that can dynamically decide\nits network capacity as it trains on a sequence of tasks, to learn a compact\noverlapping knowledge sharing structure among tasks. DEN is efficiently trained\nin an online manner by performing selective retraining, dynamically expands\nnetwork capacity upon arrival of each task with only the necessary number of\nunits, and effectively prevents semantic drift by splitting/duplicating units\nand timestamping them. We validate DEN on multiple public datasets under\nlifelong learning scenarios, on which it not only significantly outperforms\nexisting lifelong learning methods for deep networks, but also achieves the\nsame level of performance as the batch counterparts with substantially fewer\nnumber of parameters. Further, the obtained network fine-tuned on all tasks\nobtained significantly better performance over the batch models, which shows\nthat it can be used to estimate the optimal network structure even when all\ntasks are available in the first place. \n\n"}
{"id": "1708.01663", "contents": "Title: Accelerated Image Reconstruction for Nonlinear Diffractive Imaging Abstract: The problem of reconstructing an object from the measurements of the light it\nscatters is common in numerous imaging applications. While the most popular\nformulations of the problem are based on linearizing the object-light\nrelationship, there is an increased interest in considering nonlinear\nformulations that can account for multiple light scattering. In this paper, we\npropose an image reconstruction method, called CISOR, for nonlinear diffractive\nimaging, based on a nonconvex optimization formulation with total variation\n(TV) regularization. The nonconvex solver used in CISOR is our new variant of\nfast iterative shrinkage/thresholding algorithm (FISTA). We provide fast and\nmemory-efficient implementation of the new FISTA variant and prove that it\nreliably converges for our nonconvex optimization problem. In addition, we\nsystematically compare our method with other state-of-the-art methods on\nsimulated as well as experimentally measured data in both 2D and 3D settings. \n\n"}
{"id": "1708.02215", "contents": "Title: Learning a CNN-based End-to-End Controller for a Formula SAE Racecar Abstract: We present a set of CNN-based end-to-end models for controls of a Formula SAE\nracecar, along with various benchmarking and visualization tools to understand\nmodel performance. We tackled three main problems in the context of\ncone-delineated racetrack driving: (1) discretized steering, which translates a\nfirst-person frame along to the track to a predicted steering direction. (2)\nreal-value steering, which translates a frame view to a real-value steering\nangle, and (3) a network design for predicting brake and throttle. We\ndemonstrate high accuracy on our discretization task, low theoretical testing\nerrors with our model for real-value steering, and a starting point for future\nwork regarding a controller for our vehicle's brake and throttle. Timing\nbenchmarks suggests that the networks we propose have the latency and\nthroughput required for real-time controllers, when run on GPU-enabled\nhardware. \n\n"}
{"id": "1708.02237", "contents": "Title: Image Quality Assessment Techniques Show Improved Training and\n  Evaluation of Autoencoder Generative Adversarial Networks Abstract: We propose a training and evaluation approach for autoencoder Generative\nAdversarial Networks (GANs), specifically the Boundary Equilibrium Generative\nAdversarial Network (BEGAN), based on methods from the image quality assessment\nliterature. Our approach explores a multidimensional evaluation criterion that\nutilizes three distance functions: an $l_1$ score, the Gradient Magnitude\nSimilarity Mean (GMSM) score, and a chrominance score. We show that each of the\ndifferent distance functions captures a slightly different set of properties in\nimage space and, consequently, requires its own evaluation criterion to\nproperly assess whether the relevant property has been adequately learned. We\nshow that models using the new distance functions are able to produce better\nimages than the original BEGAN model in predicted ways. \n\n"}
{"id": "1708.02313", "contents": "Title: GPLAC: Generalizing Vision-Based Robotic Skills using Weakly Labeled\n  Images Abstract: We tackle the problem of learning robotic sensorimotor control policies that\ncan generalize to visually diverse and unseen environments. Achieving broad\ngeneralization typically requires large datasets, which are difficult to obtain\nfor task-specific interactive processes such as reinforcement learning or\nlearning from demonstration. However, much of the visual diversity in the world\ncan be captured through passively collected datasets of images or videos. In\nour method, which we refer to as GPLAC (Generalized Policy Learning with\nAttentional Classifier), we use both interaction data and weakly labeled image\ndata to augment the generalization capacity of sensorimotor policies. Our\nmethod combines multitask learning on action selection and an auxiliary binary\nclassification objective, together with a convolutional neural network\narchitecture that uses an attentional mechanism to avoid distractors. We show\nthat pairing interaction data from just a single environment with a diverse\ndataset of weakly labeled data results in greatly improved generalization to\nunseen environments, and show that this generalization depends on both the\nauxiliary objective and the attentional architecture that we propose. We\ndemonstrate our results in both simulation and on a real robotic manipulator,\nand demonstrate substantial improvement over standard convolutional\narchitectures and domain adaptation methods. \n\n"}
{"id": "1708.02550", "contents": "Title: Fast Scene Understanding for Autonomous Driving Abstract: Most approaches for instance-aware semantic labeling traditionally focus on\naccuracy. Other aspects like runtime and memory footprint are arguably as\nimportant for real-time applications such as autonomous driving. Motivated by\nthis observation and inspired by recent works that tackle multiple tasks with a\nsingle integrated architecture, in this paper we present a real-time efficient\nimplementation based on ENet that solves three autonomous driving related tasks\nat once: semantic scene segmentation, instance segmentation and monocular depth\nestimation. Our approach builds upon a branched ENet architecture with a shared\nencoder but different decoder branches for each of the three tasks. The\npresented method can run at 21 fps at a resolution of 1024x512 on the\nCityscapes dataset without sacrificing accuracy compared to running each task\nseparately. \n\n"}
{"id": "1708.02551", "contents": "Title: Semantic Instance Segmentation with a Discriminative Loss Function Abstract: Semantic instance segmentation remains a challenging task. In this work we\npropose to tackle the problem with a discriminative loss function, operating at\nthe pixel level, that encourages a convolutional network to produce a\nrepresentation of the image that can easily be clustered into instances with a\nsimple post-processing step. The loss function encourages the network to map\neach pixel to a point in feature space so that pixels belonging to the same\ninstance lie close together while different instances are separated by a wide\nmargin. Our approach of combining an off-the-shelf network with a principled\nloss function inspired by a metric learning objective is conceptually simple\nand distinct from recent efforts in instance segmentation. In contrast to\nprevious works, our method does not rely on object proposals or recurrent\nmechanisms. A key contribution of our work is to demonstrate that such a simple\nsetup without bells and whistles is effective and can perform on par with more\ncomplex methods. Moreover, we show that it does not suffer from some of the\nlimitations of the popular detect-and-segment approaches. We achieve\ncompetitive performance on the Cityscapes and CVPPP leaf segmentation\nbenchmarks. \n\n"}
{"id": "1708.03280", "contents": "Title: Exploring Temporal Preservation Networks for Precise Temporal Action\n  Localization Abstract: Temporal action localization is an important task of computer vision. Though\na variety of methods have been proposed, it still remains an open question how\nto predict the temporal boundaries of action segments precisely. Most works use\nsegment-level classifiers to select video segments pre-determined by action\nproposal or dense sliding windows. However, in order to achieve more precise\naction boundaries, a temporal localization system should make dense predictions\nat a fine granularity. A newly proposed work exploits\nConvolutional-Deconvolutional-Convolutional (CDC) filters to upsample the\npredictions of 3D ConvNets, making it possible to perform per-frame action\npredictions and achieving promising performance in terms of temporal action\nlocalization. However, CDC network loses temporal information partially due to\nthe temporal downsampling operation. In this paper, we propose an elegant and\npowerful Temporal Preservation Convolutional (TPC) Network that equips 3D\nConvNets with TPC filters. TPC network can fully preserve temporal resolution\nand downsample the spatial resolution simultaneously, enabling frame-level\ngranularity action localization. TPC network can be trained in an end-to-end\nmanner. Experiment results on public datasets show that TPC network achieves\nsignificant improvement on per-frame action prediction and competing results on\nsegment-level temporal action localization. \n\n"}
{"id": "1708.03322", "contents": "Title: Output Reachable Set Estimation and Verification for Multi-Layer Neural\n  Networks Abstract: In this paper, the output reachable estimation and safety verification\nproblems for multi-layer perceptron neural networks are addressed. First, a\nconception called maximum sensitivity in introduced and, for a class of\nmulti-layer perceptrons whose activation functions are monotonic functions, the\nmaximum sensitivity can be computed via solving convex optimization problems.\nThen, using a simulation-based method, the output reachable set estimation\nproblem for neural networks is formulated into a chain of optimization\nproblems. Finally, an automated safety verification is developed based on the\noutput reachable set estimation result. An application to the safety\nverification for a robotic arm model with two joints is presented to show the\neffectiveness of proposed approaches. \n\n"}
{"id": "1708.03979", "contents": "Title: SSH: Single Stage Headless Face Detector Abstract: We introduce the Single Stage Headless (SSH) face detector. Unlike two stage\nproposal-classification detectors, SSH detects faces in a single stage directly\nfrom the early convolutional layers in a classification network. SSH is\nheadless. That is, it is able to achieve state-of-the-art results while\nremoving the \"head\" of its underlying classification network -- i.e. all fully\nconnected layers in the VGG-16 which contains a large number of parameters.\nAdditionally, instead of relying on an image pyramid to detect faces with\nvarious scales, SSH is scale-invariant by design. We simultaneously detect\nfaces with different scales in a single forward pass of the network, but from\ndifferent layers. These properties make SSH fast and light-weight.\nSurprisingly, with a headless VGG-16, SSH beats the ResNet-101-based\nstate-of-the-art on the WIDER dataset. Even though, unlike the current\nstate-of-the-art, SSH does not use an image pyramid and is 5X faster. Moreover,\nif an image pyramid is deployed, our light-weight network achieves\nstate-of-the-art on all subsets of the WIDER dataset, improving the AP by 2.5%.\nSSH also reaches state-of-the-art results on the FDDB and Pascal-Faces datasets\nwhile using a small input size, leading to a runtime of 50 ms/image on a GPU.\nThe code is available at https://github.com/mahyarnajibi/SSH. \n\n"}
{"id": "1708.04099", "contents": "Title: Context-based Normalization of Histological Stains using Deep\n  Convolutional Features Abstract: While human observers are able to cope with variations in color and\nappearance of histological stains, digital pathology algorithms commonly\nrequire a well-normalized setting to achieve peak performance, especially when\na limited amount of labeled data is available. This work provides a fully\nautomated, end-to-end learning-based setup for normalizing histological stains,\nwhich considers the texture context of the tissue. We introduce Feature Aware\nNormalization, which extends the framework of batch normalization in\ncombination with gating elements from Long Short-Term Memory units for\nnormalization among different spatial regions of interest. By incorporating a\npretrained deep neural network as a feature extractor steering a pixelwise\nprocessing pipeline, we achieve excellent normalization results and ensure a\nconsistent representation of color and texture. The evaluation comprises a\ncomparison of color histogram deviations, structural similarity and measures\nthe color volume obtained by the different methods. \n\n"}
{"id": "1708.04370", "contents": "Title: Dockerface: an Easy to Install and Use Faster R-CNN Face Detector in a\n  Docker Container Abstract: Face detection is a very important task and a necessary pre-processing step\nfor many applications such as facial landmark detection, pose estimation,\nsentiment analysis and face recognition. Not only is face detection an\nimportant pre-processing step in computer vision applications but also in\ncomputational psychology, behavioral imaging and other fields where researchers\nmight not be initiated in computer vision frameworks and state-of-the-art\ndetection applications. A large part of existing research that includes face\ndetection as a pre-processing step uses existing out-of-the-box detectors such\nas the HoG-based dlib and the OpenCV Haar face detector which are no longer\nstate-of-the-art - they are primarily used because of their ease of use and\naccessibility. We introduce Dockerface, a very accurate Faster R-CNN face\ndetector in a Docker container which requires no training and is easy to\ninstall and use. \n\n"}
{"id": "1708.05256", "contents": "Title: Deep Learning at 15PF: Supervised and Semi-Supervised Classification for\n  Scientific Data Abstract: This paper presents the first, 15-PetaFLOP Deep Learning system for solving\nscientific pattern classification problems on contemporary HPC architectures.\nWe develop supervised convolutional architectures for discriminating signals in\nhigh-energy physics data as well as semi-supervised architectures for\nlocalizing and classifying extreme weather in climate data. Our\nIntelcaffe-based implementation obtains $\\sim$2TFLOP/s on a single Cori\nPhase-II Xeon-Phi node. We use a hybrid strategy employing synchronous\nnode-groups, while using asynchronous communication across groups. We use this\nstrategy to scale training of a single model to $\\sim$9600 Xeon-Phi nodes;\nobtaining peak performance of 11.73-15.07 PFLOP/s and sustained performance of\n11.41-13.27 PFLOP/s. At scale, our HEP architecture produces state-of-the-art\nclassification accuracy on a dataset with 10M images, exceeding that achieved\nby selections on high-level physics-motivated features. Our semi-supervised\narchitecture successfully extracts weather patterns in a 15TB climate dataset.\nOur results demonstrate that Deep Learning can be optimized and scaled\neffectively on many-core, HPC systems. \n\n"}
{"id": "1708.06519", "contents": "Title: Learning Efficient Convolutional Networks through Network Slimming Abstract: The deployment of deep convolutional neural networks (CNNs) in many real\nworld applications is largely hindered by their high computational cost. In\nthis paper, we propose a novel learning scheme for CNNs to simultaneously 1)\nreduce the model size; 2) decrease the run-time memory footprint; and 3) lower\nthe number of computing operations, without compromising accuracy. This is\nachieved by enforcing channel-level sparsity in the network in a simple but\neffective way. Different from many existing approaches, the proposed method\ndirectly applies to modern CNN architectures, introduces minimum overhead to\nthe training process, and requires no special software/hardware accelerators\nfor the resulting models. We call our approach network slimming, which takes\nwide and large networks as input models, but during training insignificant\nchannels are automatically identified and pruned afterwards, yielding thin and\ncompact models with comparable accuracy. We empirically demonstrate the\neffectiveness of our approach with several state-of-the-art CNN models,\nincluding VGGNet, ResNet and DenseNet, on various image classification\ndatasets. For VGGNet, a multi-pass version of network slimming gives a 20x\nreduction in model size and a 5x reduction in computing operations. \n\n"}
{"id": "1708.06720", "contents": "Title: WordSup: Exploiting Word Annotations for Character based Text Detection Abstract: Imagery texts are usually organized as a hierarchy of several visual\nelements, i.e. characters, words, text lines and text blocks. Among these\nelements, character is the most basic one for various languages such as\nWestern, Chinese, Japanese, mathematical expression and etc. It is natural and\nconvenient to construct a common text detection engine based on character\ndetectors. However, training character detectors requires a vast of location\nannotated characters, which are expensive to obtain. Actually, the existing\nreal text datasets are mostly annotated in word or line level. To remedy this\ndilemma, we propose a weakly supervised framework that can utilize word\nannotations, either in tight quadrangles or the more loose bounding boxes, for\ncharacter detector training. When applied in scene text detection, we are thus\nable to train a robust character detector by exploiting word annotations in the\nrich large-scale real scene text datasets, e.g. ICDAR15 and COCO-text. The\ncharacter detector acts as a key role in the pipeline of our text detection\nengine. It achieves the state-of-the-art performance on several challenging\nscene text detection benchmarks. We also demonstrate the flexibility of our\npipeline by various scenarios, including deformed text detection and math\nexpression recognition. \n\n"}
{"id": "1708.07199", "contents": "Title: 3D Morphable Models as Spatial Transformer Networks Abstract: In this paper, we show how a 3D Morphable Model (i.e. a statistical model of\nthe 3D shape of a class of objects such as faces) can be used to spatially\ntransform input data as a module (a 3DMM-STN) within a convolutional neural\nnetwork. This is an extension of the original spatial transformer network in\nthat we are able to interpret and normalise 3D pose changes and\nself-occlusions. The trained localisation part of the network is independently\nuseful since it learns to fit a 3D morphable model to a single image. We show\nthat the localiser can be trained using only simple geometric loss functions on\na relatively small dataset yet is able to perform robust normalisation on\nhighly uncontrolled images including occlusion, self-occlusion and large pose\nchanges. \n\n"}
{"id": "1708.07280", "contents": "Title: Learning Generalized Reactive Policies using Deep Neural Networks Abstract: We present a new approach to learning for planning, where knowledge acquired\nwhile solving a given set of planning problems is used to plan faster in\nrelated, but new problem instances. We show that a deep neural network can be\nused to learn and represent a \\emph{generalized reactive policy} (GRP) that\nmaps a problem instance and a state to an action, and that the learned GRPs\nefficiently solve large classes of challenging problem instances. In contrast\nto prior efforts in this direction, our approach significantly reduces the\ndependence of learning on handcrafted domain knowledge or feature selection.\nInstead, the GRP is trained from scratch using a set of successful execution\ntraces. We show that our approach can also be used to automatically learn a\nheuristic function that can be used in directed search algorithms. We evaluate\nour approach using an extensive suite of experiments on two challenging\nplanning problem domains and show that our approach facilitates learning\ncomplex decision making policies and powerful heuristic functions with minimal\nhuman input. Videos of our results are available at goo.gl/Hpy4e3. \n\n"}
{"id": "1708.07747", "contents": "Title: Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n  Algorithms Abstract: We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images\nof 70,000 fashion products from 10 categories, with 7,000 images per category.\nThe training set has 60,000 images and the test set has 10,000 images.\nFashion-MNIST is intended to serve as a direct drop-in replacement for the\noriginal MNIST dataset for benchmarking machine learning algorithms, as it\nshares the same image size, data format and the structure of training and\ntesting splits. The dataset is freely available at\nhttps://github.com/zalandoresearch/fashion-mnist \n\n"}
{"id": "1708.08333", "contents": "Title: Framing U-Net via Deep Convolutional Framelets: Application to\n  Sparse-view CT Abstract: X-ray computed tomography (CT) using sparse projection views is a recent\napproach to reduce the radiation dose. However, due to the insufficient\nprojection views, an analytic reconstruction approach using the filtered back\nprojection (FBP) produces severe streaking artifacts. Recently, deep learning\napproaches using large receptive field neural networks such as U-Net have\ndemonstrated impressive performance for sparse- view CT reconstruction.\nHowever, theoretical justification is still lacking. Inspired by the recent\ntheory of deep convolutional framelets, the main goal of this paper is,\ntherefore, to reveal the limitation of U-Net and propose new multi-resolution\ndeep learning schemes. In particular, we show that the alternative U- Net\nvariants such as dual frame and the tight frame U-Nets satisfy the so-called\nframe condition which make them better for effective recovery of high frequency\nedges in sparse view- CT. Using extensive experiments with real patient data\nset, we demonstrate that the new network architectures provide better\nreconstruction performance. \n\n"}
{"id": "1709.01237", "contents": "Title: Newton-type Methods for Inference in Higher-Order Markov Random Fields Abstract: Linear programming relaxations are central to {\\sc map} inference in discrete\nMarkov Random Fields. The ability to properly solve the Lagrangian dual is a\ncritical component of such methods. In this paper, we study the benefit of\nusing Newton-type methods to solve the Lagrangian dual of a smooth version of\nthe problem. We investigate their ability to achieve superior convergence\nbehavior and to better handle the ill-conditioned nature of the formulation, as\ncompared to first order methods. We show that it is indeed possible to\nefficiently apply a trust region Newton method for a broad range of {\\sc map}\ninference problems. In this paper we propose a provably convergent and\nefficient framework that includes (i) excellent compromise between\ncomputational complexity and precision concerning the Hessian matrix\nconstruction, (ii) a damping strategy that aids efficient optimization, (iii) a\ntruncation strategy coupled with a generic pre-conditioner for Conjugate\nGradients, (iv) efficient sum-product computation for sparse clique potentials.\nResults for higher-order Markov Random Fields demonstrate the potential of this\napproach. \n\n"}
{"id": "1709.02908", "contents": "Title: Image Processing Operations Identification via Convolutional Neural\n  Network Abstract: In recent years, image forensics has attracted more and more attention, and\nmany forensic methods have been proposed for identifying image processing\noperations. Up to now, most existing methods are based on hand crafted\nfeatures, and just one specific operation is considered in their methods. In\nmany forensic scenarios, however, multiple classification for various image\nprocessing operations is more practical. Besides, it is difficult to obtain\neffective features by hand for some image processing operations. In this paper,\ntherefore, we propose a new convolutional neural network (CNN) based method to\nadaptively learn discriminative features for identifying typical image\nprocessing operations. We carefully design the high pass filter bank to get the\nimage residuals of the input image, the channel expansion layer to mix up the\nresulting residuals, the pooling layers, and the activation functions employed\nin our method. The extensive results show that the proposed method can\noutperform the currently best method based on hand crafted features and three\nrelated methods based on CNN for image steganalysis and/or forensics, achieving\nthe state-of-the-art results. Furthermore, we provide more supplementary\nresults to show the rationality and robustness of the proposed model. \n\n"}
{"id": "1709.03919", "contents": "Title: End-to-End United Video Dehazing and Detection Abstract: The recent development of CNN-based image dehazing has revealed the\neffectiveness of end-to-end modeling. However, extending the idea to end-to-end\nvideo dehazing has not been explored yet. In this paper, we propose an\nEnd-to-End Video Dehazing Network (EVD-Net), to exploit the temporal\nconsistency between consecutive video frames. A thorough study has been\nconducted over a number of structure options, to identify the best temporal\nfusion strategy. Furthermore, we build an End-to-End United Video Dehazing and\nDetection Network(EVDD-Net), which concatenates and jointly trains EVD-Net with\na video object detection model. The resulting augmented end-to-end pipeline has\ndemonstrated much more stable and accurate detection results in hazy video. \n\n"}
{"id": "1709.04396", "contents": "Title: A Tutorial on Deep Learning for Music Information Retrieval Abstract: Following their success in Computer Vision and other areas, deep learning\ntechniques have recently become widely adopted in Music Information Retrieval\n(MIR) research. However, the majority of works aim to adopt and assess methods\nthat have been shown to be effective in other domains, while there is still a\ngreat need for more original research focusing on music primarily and utilising\nmusical knowledge and insight. The goal of this paper is to boost the interest\nof beginners by providing a comprehensive tutorial and reducing the barriers to\nentry into deep learning for MIR. We lay out the basic principles and review\nprominent works in this hard to navigate the field. We then outline the network\nstructures that have been successful in MIR problems and facilitate the\nselection of building blocks for the problems at hand. Finally, guidelines for\nnew tasks and some advanced topics in deep learning are discussed to stimulate\nnew research in this fascinating field. \n\n"}
{"id": "1709.04577", "contents": "Title: DeepVoting: A Robust and Explainable Deep Network for Semantic Part\n  Detection under Partial Occlusion Abstract: In this paper, we study the task of detecting semantic parts of an object,\ne.g., a wheel of a car, under partial occlusion. We propose that all models\nshould be trained without seeing occlusions while being able to transfer the\nlearned knowledge to deal with occlusions. This setting alleviates the\ndifficulty in collecting an exponentially large dataset to cover occlusion\npatterns and is more essential. In this scenario, the proposal-based deep\nnetworks, like RCNN-series, often produce unsatisfactory results, because both\nthe proposal extraction and classification stages may be confused by the\nirrelevant occluders. To address this, [25] proposed a voting mechanism that\ncombines multiple local visual cues to detect semantic parts. The semantic\nparts can still be detected even though some visual cues are missing due to\nocclusions. However, this method is manually-designed, thus is hard to be\noptimized in an end-to-end manner.\n  In this paper, we present DeepVoting, which incorporates the robustness shown\nby [25] into a deep network, so that the whole pipeline can be jointly\noptimized. Specifically, it adds two layers after the intermediate features of\na deep network, e.g., the pool-4 layer of VGGNet. The first layer extracts the\nevidence of local visual cues, and the second layer performs a voting mechanism\nby utilizing the spatial relationship between visual cues and semantic parts.\nWe also propose an improved version DeepVoting+ by learning visual cues from\ncontext outside objects. In experiments, DeepVoting achieves significantly\nbetter performance than several baseline methods, including Faster-RCNN, for\nsemantic part detection under occlusion. In addition, DeepVoting enjoys\nexplainability as the detection results can be diagnosed via looking up the\nvoting cues. \n\n"}
{"id": "1709.05087", "contents": "Title: Viewpoint Invariant Action Recognition using RGB-D Videos Abstract: In video-based action recognition, viewpoint variations often pose major\nchallenges because the same actions can appear different from different views.\nWe use the complementary RGB and Depth information from the RGB-D cameras to\naddress this problem. The proposed technique capitalizes on the spatio-temporal\ninformation available in the two data streams to the extract action features\nthat are largely insensitive to the viewpoint variations. We use the RGB data\nto compute dense trajectories that are translated to viewpoint insensitive deep\nfeatures under a non-linear knowledge transfer model. Similarly, the Depth\nstream is used to extract CNN-based view invariant features on which Fourier\nTemporal Pyramid is computed to incorporate the temporal information. The\nheterogeneous features from the two streams are combined and used as a\ndictionary to predict the label of the test samples. To that end, we propose a\nsparse-dense collaborative representation classification scheme that strikes a\nbalance between the discriminative abilities of the dense and the sparse\nrepresentations of the samples over the extracted heterogeneous dictionary. \n\n"}
{"id": "1709.05107", "contents": "Title: Multi-Label Zero-Shot Human Action Recognition via Joint Latent Ranking\n  Embedding Abstract: Human action recognition refers to automatic recognizing human actions from a\nvideo clip. In reality, there often exist multiple human actions in a video\nstream. Such a video stream is often weakly-annotated with a set of relevant\nhuman action labels at a global level rather than assigning each label to a\nspecific video episode corresponding to a single action, which leads to a\nmulti-label learning problem. Furthermore, there are many meaningful human\nactions in reality but it would be extremely difficult to collect/annotate\nvideo clips regarding all of various human actions, which leads to a zero-shot\nlearning scenario. To the best of our knowledge, there is no work that has\naddressed all the above issues together in human action recognition. In this\npaper, we formulate a real-world human action recognition task as a multi-label\nzero-shot learning problem and propose a framework to tackle this problem in a\nholistic way. Our framework holistically tackles the issue of unknown temporal\nboundaries between different actions for multi-label learning and exploits the\nside information regarding the semantic relationship between different human\nactions for knowledge transfer. Consequently, our framework leads to a joint\nlatent ranking embedding for multi-label zero-shot human action recognition. A\nnovel neural architecture of two component models and an alternate learning\nalgorithm are proposed to carry out the joint latent ranking embedding\nlearning. Thus, multi-label zero-shot recognition is done by measuring\nrelatedness scores of action labels to a test video clip in the joint latent\nvisual and semantic embedding spaces. We evaluate our framework with different\nsettings, including a novel data split scheme designed especially for\nevaluating multi-label zero-shot learning, on two datasets: Breakfast and\nCharades. The experimental results demonstrate the effectiveness of our\nframework. \n\n"}
{"id": "1709.05273", "contents": "Title: Cooperative Motion Planning for Non-Holonomic Agents with Value\n  Iteration Networks Abstract: Cooperative motion planning is still a challenging task for robots. Recently,\nValue Iteration Networks (VINs) were proposed to model motion planning tasks as\nNeural Networks. In this work, we extend VINs to solve cooperative planning\ntasks under non-holonomic constraints. For this, we interconnect multiple VINs\nto pay respect to each other's outputs. Policies for cooperation are generated\nvia iterative gradient descend. Validation in simulation shows that the\nresulting networks can resolve non-holonomic motion planning problems that\nrequire cooperation. \n\n"}
{"id": "1709.05324", "contents": "Title: Cystoid macular edema segmentation of Optical Coherence Tomography\n  images using fully convolutional neural networks and fully connected CRFs Abstract: In this paper we present a new method for cystoid macular edema (CME)\nsegmentation in retinal Optical Coherence Tomography (OCT) images, using a\nfully convolutional neural network (FCN) and a fully connected conditional\nrandom fields (dense CRFs). As a first step, the framework trains the FCN model\nto extract features from retinal layers in OCT images, which exhibit CME, and\nthen segments CME regions using the trained model. Thereafter, dense CRFs are\nused to refine the segmentation according to the edema appearance. We have\ntrained and tested the framework with OCT images from 10 patients with diabetic\nmacular edema (DME). Our experimental results show that fluid and concrete\nmacular edema areas were segmented with good adherence to boundaries. A\nsegmentation accuracy of $0.61\\pm 0.21$ (Dice coefficient) was achieved, with\nrespect to the ground truth, which compares favourably with the previous\nstate-of-the-art that used a kernel regression based method ($0.51\\pm 0.34$).\nOur approach is versatile and we believe it can be easily adapted to detect\nother macular defects. \n\n"}
{"id": "1709.05706", "contents": "Title: Memory Augmented Control Networks Abstract: Planning problems in partially observable environments cannot be solved\ndirectly with convolutional networks and require some form of memory. But, even\nmemory networks with sophisticated addressing schemes are unable to learn\nintelligent reasoning satisfactorily due to the complexity of simultaneously\nlearning to access memory and plan. To mitigate these challenges we introduce\nthe Memory Augmented Control Network (MACN). The proposed network architecture\nconsists of three main parts. The first part uses convolutions to extract\nfeatures and the second part uses a neural network-based planning module to\npre-plan in the environment. The third part uses a network controller that\nlearns to store those specific instances of past information that are necessary\nfor planning. The performance of the network is evaluated in discrete grid\nworld environments for path planning in the presence of simple and complex\nobstacles. We show that our network learns to plan and can generalize to new\nenvironments. \n\n"}
{"id": "1709.06308", "contents": "Title: Exploring Human-like Attention Supervision in Visual Question Answering Abstract: Attention mechanisms have been widely applied in the Visual Question\nAnswering (VQA) task, as they help to focus on the area-of-interest of both\nvisual and textual information. To answer the questions correctly, the model\nneeds to selectively target different areas of an image, which suggests that an\nattention-based model may benefit from an explicit attention supervision. In\nthis work, we aim to address the problem of adding attention supervision to VQA\nmodels. Since there is a lack of human attention data, we first propose a Human\nAttention Network (HAN) to generate human-like attention maps, training on a\nrecently released dataset called Human ATtention Dataset (VQA-HAT). Then, we\napply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the\nhuman-like attention maps for all image-question pairs. The generated\nhuman-like attention map dataset for the VQA v2.0 dataset is named as\nHuman-Like ATtention (HLAT) dataset. Finally, we apply human-like attention\nsupervision to an attention-based VQA model. The experiments show that adding\nhuman-like supervision yields a more accurate attention together with a better\nperformance, showing a promising future for human-like attention supervision in\nVQA. \n\n"}
{"id": "1709.06662", "contents": "Title: Verifying Properties of Binarized Deep Neural Networks Abstract: Understanding properties of deep neural networks is an important challenge in\ndeep learning. In this paper, we take a step in this direction by proposing a\nrigorous way of verifying properties of a popular class of neural networks,\nBinarized Neural Networks, using the well-developed means of Boolean\nsatisfiability. Our main contribution is a construction that creates a\nrepresentation of a binarized neural network as a Boolean formula. Our encoding\nis the first exact Boolean representation of a deep neural network. Using this\nencoding, we leverage the power of modern SAT solvers along with a proposed\ncounterexample-guided search procedure to verify various properties of these\nnetworks. A particular focus will be on the critical property of robustness to\nadversarial perturbations. For this property, our experimental results\ndemonstrate that our approach scales to medium-size deep neural networks used\nin image classification tasks. To the best of our knowledge, this is the first\nwork on verifying properties of deep neural networks using an exact Boolean\nencoding of the network. \n\n"}
{"id": "1709.07326", "contents": "Title: AffordanceNet: An End-to-End Deep Learning Approach for Object\n  Affordance Detection Abstract: We propose AffordanceNet, a new deep learning approach to simultaneously\ndetect multiple objects and their affordances from RGB images. Our\nAffordanceNet has two branches: an object detection branch to localize and\nclassify the object, and an affordance detection branch to assign each pixel in\nthe object to its most probable affordance label. The proposed framework\nemploys three key components for effectively handling the multiclass problem in\nthe affordance mask: a sequence of deconvolutional layers, a robust resizing\nstrategy, and a multi-task loss function. The experimental results on the\npublic datasets show that our AffordanceNet outperforms recent state-of-the-art\nmethods by a fair margin, while its end-to-end architecture allows the\ninference at the speed of 150ms per image. This makes our AffordanceNet well\nsuitable for real-time robotic applications. Furthermore, we demonstrate the\neffectiveness of AffordanceNet in different testing environments and in real\nrobotic applications. The source code is available at\nhttps://github.com/nqanh/affordance-net \n\n"}
{"id": "1709.07796", "contents": "Title: On overfitting and asymptotic bias in batch reinforcement learning with\n  partial observability Abstract: This paper provides an analysis of the tradeoff between asymptotic bias\n(suboptimality with unlimited data) and overfitting (additional suboptimality\ndue to limited data) in the context of reinforcement learning with partial\nobservability. Our theoretical analysis formally characterizes that while\npotentially increasing the asymptotic bias, a smaller state representation\ndecreases the risk of overfitting. This analysis relies on expressing the\nquality of a state representation by bounding L1 error terms of the associated\nbelief states. Theoretical results are empirically illustrated when the state\nrepresentation is a truncated history of observations, both on synthetic POMDPs\nand on a large-scale POMDP in the context of smartgrids, with real-world data.\nFinally, similarly to known results in the fully observable setting, we also\nbriefly discuss and empirically illustrate how using function approximators and\nadapting the discount factor may enhance the tradeoff between asymptotic bias\nand overfitting in the partially observable context. \n\n"}
{"id": "1709.07857", "contents": "Title: Using Simulation and Domain Adaptation to Improve Efficiency of Deep\n  Robotic Grasping Abstract: Instrumenting and collecting annotated visual grasping datasets to train\nmodern machine learning algorithms can be extremely time-consuming and\nexpensive. An appealing alternative is to use off-the-shelf simulators to\nrender synthetic data for which ground-truth annotations are generated\nautomatically. Unfortunately, models trained purely on simulated data often\nfail to generalize to the real world. We study how randomized simulated\nenvironments and domain adaptation methods can be extended to train a grasping\nsystem to grasp novel objects from raw monocular RGB images. We extensively\nevaluate our approaches with a total of more than 25,000 physical test grasps,\nstudying a range of simulation conditions and domain adaptation methods,\nincluding a novel extension of pixel-level domain adaptation that we term the\nGraspGAN. We show that, by using synthetic data and domain adaptation, we are\nable to reduce the number of real-world samples needed to achieve a given level\nof performance by up to 50 times, using only randomly generated simulated\nobjects. We also show that by using only unlabeled real-world data and our\nGraspGAN methodology, we obtain real-world grasping performance without any\nreal-world labels that is similar to that achieved with 939,777 labeled\nreal-world samples. \n\n"}
{"id": "1709.08374", "contents": "Title: Deep Sparse Subspace Clustering Abstract: In this paper, we present a deep extension of Sparse Subspace Clustering,\ntermed Deep Sparse Subspace Clustering (DSSC). Regularized by the unit sphere\ndistribution assumption for the learned deep features, DSSC can infer a new\ndata affinity matrix by simultaneously satisfying the sparsity principle of SSC\nand the nonlinearity given by neural networks. One of the appealing advantages\nbrought by DSSC is: when original real-world data do not meet the\nclass-specific linear subspace distribution assumption, DSSC can employ neural\nnetworks to make the assumption valid with its hierarchical nonlinear\ntransformations. To the best of our knowledge, this is among the first deep\nlearning based subspace clustering methods. Extensive experiments are conducted\non four real-world datasets to show the proposed DSSC is significantly superior\nto 12 existing methods for subspace clustering. \n\n"}
{"id": "1709.08430", "contents": "Title: Towards continuous control of flippers for a multi-terrain robot using\n  deep reinforcement learning Abstract: In this paper we focus on developing a control algorithm for multi-terrain\ntracked robots with flippers using a reinforcement learning (RL) approach. The\nwork is based on the deep deterministic policy gradient (DDPG) algorithm,\nproven to be very successful in simple simulation environments. The algorithm\nworks in an end-to-end fashion in order to control the continuous position of\nthe flippers. This end-to-end approach makes it easy to apply the controller to\na wide array of circumstances, but the huge flexibility comes to the cost of an\nincreased difficulty of solution. The complexity of the task is enlarged even\nmore by the fact that real multi-terrain robots move in partially observable\nenvironments. Notwithstanding these complications, being able to smoothly\ncontrol a multi-terrain robot can produce huge benefits in impaired people\ndaily lives or in search and rescue situations. \n\n"}
{"id": "1709.08693", "contents": "Title: Fooling Vision and Language Models Despite Localization and Attention\n  Mechanism Abstract: Adversarial attacks are known to succeed on classifiers, but it has been an\nopen question whether more complex vision systems are vulnerable. In this\npaper, we study adversarial examples for vision and language models, which\nincorporate natural language understanding and complex structures such as\nattention, localization, and modular architectures. In particular, we\ninvestigate attacks on a dense captioning model and on two visual question\nanswering (VQA) models. Our evaluation shows that we can generate adversarial\nexamples with a high success rate (i.e., > 90%) for these models. Our work\nsheds new light on understanding adversarial attacks on vision systems which\nhave a language component and shows that attention, bounding box localization,\nand compositional internal structures are vulnerable to adversarial attacks.\nThese observations will inform future work towards building effective defenses. \n\n"}
{"id": "1710.00166", "contents": "Title: PCANet-II: When PCANet Meets the Second Order Pooling Abstract: PCANet, as one noticeable shallow network, employs the histogram\nrepresentation for feature pooling. However, there are three main problems\nabout this kind of pooling method. First, the histogram-based pooling method\nbinarizes the feature maps and leads to inevitable discriminative information\nloss. Second, it is difficult to effectively combine other visual cues into a\ncompact representation, because the simple concatenation of various visual cues\nleads to feature representation inefficiency. Third, the dimensionality of\nhistogram-based output grows exponentially with the number of feature maps\nused. In order to overcome these problems, we propose a novel shallow network\nmodel, named as PCANet-II. Compared with the histogram-based output, the second\norder pooling not only provides more discriminative information by preserving\nboth the magnitude and sign of convolutional responses, but also dramatically\nreduces the size of output features. Thus we combine the second order\nstatistical pooling method with the shallow network, i.e., PCANet. Moreover, it\nis easy to combine other discriminative and robust cues by using the second\norder pooling. So we introduce the binary feature difference encoding scheme\ninto our PCANet-II to further improve robustness. Experiments demonstrate the\neffectiveness and robustness of our proposed PCANet-II method. \n\n"}
{"id": "1710.01691", "contents": "Title: Context Embedding Networks Abstract: Low dimensional embeddings that capture the main variations of interest in\ncollections of data are important for many applications. One way to construct\nthese embeddings is to acquire estimates of similarity from the crowd. However,\nsimilarity is a multi-dimensional concept that varies from individual to\nindividual. Existing models for learning embeddings from the crowd typically\nmake simplifying assumptions such as all individuals estimate similarity using\nthe same criteria, the list of criteria is known in advance, or that the crowd\nworkers are not influenced by the data that they see. To overcome these\nlimitations we introduce Context Embedding Networks (CENs). In addition to\nlearning interpretable embeddings from images, CENs also model worker biases\nfor different attributes along with the visual context i.e. the visual\nattributes highlighted by a set of images. Experiments on two noisy crowd\nannotated datasets show that modeling both worker bias and visual context\nresults in more interpretable embeddings compared to existing approaches. \n\n"}
{"id": "1710.03107", "contents": "Title: Verification of Binarized Neural Networks via Inter-Neuron Factoring Abstract: We study the problem of formal verification of Binarized Neural Networks\n(BNN), which have recently been proposed as a energy-efficient alternative to\ntraditional learning networks. The verification of BNNs, using the reduction to\nhardware verification, can be even more scalable by factoring computations\namong neurons within the same layer. By proving the NP-hardness of finding\noptimal factoring as well as the hardness of PTAS approximability, we design\npolynomial-time search heuristics to generate factoring solutions. The overall\nframework allows applying verification techniques to moderately-sized BNNs for\nembedded devices with thousands of neurons and inputs. \n\n"}
{"id": "1710.03285", "contents": "Title: Coresets for Dependency Networks Abstract: Many applications infer the structure of a probabilistic graphical model from\ndata to elucidate the relationships between variables. But how can we train\ngraphical models on a massive data set? In this paper, we show how to construct\ncoresets -compressed data sets which can be used as proxy for the original data\nand have provably bounded worst case error- for Gaussian dependency networks\n(DNs), i.e., cyclic directed graphical models over Gaussians, where the parents\nof each variable are its Markov blanket. Specifically, we prove that Gaussian\nDNs admit coresets of size independent of the size of the data set.\nUnfortunately, this does not extend to DNs over members of the exponential\nfamily in general. As we will prove, Poisson DNs do not admit small coresets.\nDespite this worst-case result, we will provide an argument why our coreset\nconstruction for DNs can still work well in practice on count data. To\ncorroborate our theoretical results, we empirically evaluated the resulting\nCore DNs on real data sets. The results \n\n"}
{"id": "1710.03337", "contents": "Title: Standard detectors aren't (currently) fooled by physical adversarial\n  stop signs Abstract: An adversarial example is an example that has been adjusted to produce the\nwrong label when presented to a system at test time. If adversarial examples\nexisted that could fool a detector, they could be used to (for example) wreak\nhavoc on roads populated with smart vehicles. Recently, we described our\ndifficulties creating physical adversarial stop signs that fool a detector.\nMore recently, Evtimov et al. produced a physical adversarial stop sign that\nfools a proxy model of a detector. In this paper, we show that these physical\nadversarial stop signs do not fool two standard detectors (YOLO and Faster\nRCNN) in standard configuration. Evtimov et al.'s construction relies on a crop\nof the image to the stop sign; this crop is then resized and presented to a\nclassifier. We argue that the cropping and resizing procedure largely\neliminates the effects of rescaling and of view angle. Whether an adversarial\nattack is robust under rescaling and change of view direction remains moot. We\nargue that attacking a classifier is very different from attacking a detector,\nand that the structure of detectors - which must search for their own bounding\nbox, and which cannot estimate that box very accurately - likely makes it\ndifficult to make adversarial patterns. Finally, an adversarial pattern on a\nphysical object that could fool a detector would have to be adversarial in the\nface of a wide family of parametric distortions (scale; view angle; box shift\ninside the detector; illumination; and so on). Such a pattern would be of great\ntheoretical and practical interest. There is currently no evidence that such\npatterns exist. \n\n"}
{"id": "1710.04934", "contents": "Title: RADNET: Radiologist Level Accuracy using Deep Learning for HEMORRHAGE\n  detection in CT Scans Abstract: We describe a deep learning approach for automated brain hemorrhage detection\nfrom computed tomography (CT) scans. Our model emulates the procedure followed\nby radiologists to analyse a 3D CT scan in real-world. Similar to radiologists,\nthe model sifts through 2D cross-sectional slices while paying close attention\nto potential hemorrhagic regions. Further, the model utilizes 3D context from\nneighboring slices to improve predictions at each slice and subsequently,\naggregates the slice-level predictions to provide diagnosis at CT level. We\nrefer to our proposed approach as Recurrent Attention DenseNet (RADnet) as it\nemploys original DenseNet architecture along with adding the components of\nattention for slice level predictions and recurrent neural network layer for\nincorporating 3D context. The real-world performance of RADnet has been\nbenchmarked against independent analysis performed by three senior radiologists\nfor 77 brain CTs. RADnet demonstrates 81.82% hemorrhage prediction accuracy at\nCT level that is comparable to radiologists. Further, RADnet achieves higher\nrecall than two of the three radiologists, which is remarkable. \n\n"}
{"id": "1710.05381", "contents": "Title: A systematic study of the class imbalance problem in convolutional\n  neural networks Abstract: In this study, we systematically investigate the impact of class imbalance on\nclassification performance of convolutional neural networks (CNNs) and compare\nfrequently used methods to address the issue. Class imbalance is a common\nproblem that has been comprehensively studied in classical machine learning,\nyet very limited systematic research is available in the context of deep\nlearning. In our study, we use three benchmark datasets of increasing\ncomplexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of\nimbalance on classification and perform an extensive comparison of several\nmethods to address the issue: oversampling, undersampling, two-phase training,\nand thresholding that compensates for prior class probabilities. Our main\nevaluation metric is area under the receiver operating characteristic curve\n(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is\nassociated with notable difficulties in the context of imbalanced data. Based\non results from our experiments we conclude that (i) the effect of class\nimbalance on classification performance is detrimental; (ii) the method of\naddressing class imbalance that emerged as dominant in almost all analyzed\nscenarios was oversampling; (iii) oversampling should be applied to the level\nthat completely eliminates the imbalance, whereas the optimal undersampling\nratio depends on the extent of imbalance; (iv) as opposed to some classical\nmachine learning models, oversampling does not cause overfitting of CNNs; (v)\nthresholding should be applied to compensate for prior class probabilities when\noverall number of properly classified cases is of interest. \n\n"}
{"id": "1710.05664", "contents": "Title: What is (missing or wrong) in the scene? A Hybrid Deep Boltzmann Machine\n  For Contextualized Scene Modeling Abstract: Scene models allow robots to reason about what is in the scene, what else\nshould be in it, and what should not be in it. In this paper, we propose a\nhybrid Boltzmann Machine (BM) for scene modeling where relations between\nobjects are integrated. To be able to do that, we extend BM to include tri-way\nedges between visible (object) nodes and make the network to share the\nrelations across different objects. We evaluate our method against several\nbaseline models (Deep Boltzmann Machines, and Restricted Boltzmann Machines) on\na scene classification dataset, and show that it performs better in several\nscene reasoning tasks. \n\n"}
{"id": "1710.05719", "contents": "Title: Lung Cancer Screening Using Adaptive Memory-Augmented Recurrent Networks Abstract: In this paper, we investigate the effectiveness of deep learning techniques\nfor lung nodule classification in computed tomography scans. Using less than\n10,000 training examples, our deep networks perform two times better than a\nstandard radiology software. Visualization of the networks' neurons reveals\nsemantically meaningful features that are consistent with the clinical\nknowledge and radiologists' perception. Our paper also proposes a novel\nframework for rapidly adapting deep networks to the radiologists' feedback, or\nchange in the data due to the shift in sensor's resolution or patient\npopulation. The classification accuracy of our approach remains above 80% while\npopular deep networks' accuracy is around chance. Finally, we provide in-depth\nanalysis of our framework by asking a radiologist to examine important\nnetworks' features and perform blind re-labeling of networks' mistakes. \n\n"}
{"id": "1710.05772", "contents": "Title: Data-Efficient Decentralized Visual SLAM Abstract: Decentralized visual simultaneous localization and mapping (SLAM) is a\npowerful tool for multi-robot applications in environments where absolute\npositioning systems are not available. Being visual, it relies on cameras,\ncheap, lightweight and versatile sensors, and being decentralized, it does not\nrely on communication to a central ground station. In this work, we integrate\nstate-of-the-art decentralized SLAM components into a new, complete\ndecentralized visual SLAM system. To allow for data association and\nco-optimization, existing decentralized visual SLAM systems regularly exchange\nthe full map data between all robots, incurring large data transfers at a\ncomplexity that scales quadratically with the robot count. In contrast, our\nmethod performs efficient data association in two stages: in the first stage a\ncompact full-image descriptor is deterministically sent to only one robot. In\nthe second stage, which is only executed if the first stage succeeded, the data\nrequired for relative pose estimation is sent, again to only one robot. Thus,\ndata association scales linearly with the robot count and uses highly compact\nplace representations. For optimization, a state-of-the-art decentralized\npose-graph optimization method is used. It exchanges a minimum amount of data\nwhich is linear with trajectory overlap. We characterize the resulting system\nand identify bottlenecks in its components. The system is evaluated on publicly\navailable data and we provide open access to the code. \n\n"}
{"id": "1710.06555", "contents": "Title: Learning Deep Context-aware Features over Body and Latent Parts for\n  Person Re-identification Abstract: Person Re-identification (ReID) is to identify the same person across\ndifferent cameras. It is a challenging task due to the large variations in\nperson pose, occlusion, background clutter, etc How to extract powerful\nfeatures is a fundamental problem in ReID and is still an open problem today.\nIn this paper, we design a Multi-Scale Context-Aware Network (MSCAN) to learn\npowerful features over full body and body parts, which can well capture the\nlocal context knowledge by stacking multi-scale convolutions in each layer.\nMoreover, instead of using predefined rigid parts, we propose to learn and\nlocalize deformable pedestrian parts using Spatial Transformer Networks (STN)\nwith novel spatial constraints. The learned body parts can release some\ndifficulties, eg pose variations and background clutters, in part-based\nrepresentation. Finally, we integrate the representation learning processes of\nfull body and body parts into a unified framework for person ReID through\nmulti-class person identification tasks. Extensive evaluations on current\nchallenging large-scale person ReID datasets, including the image-based\nMarket1501, CUHK03 and sequence-based MARS datasets, show that the proposed\nmethod achieves the state-of-the-art results. \n\n"}
{"id": "1710.06929", "contents": "Title: Unsupervised Object Discovery and Segmentation of RGBD-images Abstract: In this paper we introduce a system for unsupervised object discovery and\nsegmentation of RGBD-images. The system models the sensor noise directly from\ndata, allowing accurate segmentation without sensor specific hand tuning of\nmeasurement noise models making use of the recently introduced Statistical\nInlier Estimation (SIE) method. Through a fully probabilistic formulation, the\nsystem is able to apply probabilistic inference, enabling reliable segmentation\nin previously challenging scenarios. In addition, we introduce new methods for\nfiltering out false positives, significantly improving the signal to noise\nratio. We show that the system significantly outperform state-of-the-art in on\na challenging real-world dataset. \n\n"}
{"id": "1710.07084", "contents": "Title: Emerging from Water: Underwater Image Color Correction Based on Weakly\n  Supervised Color Transfer Abstract: Underwater vision suffers from severe effects due to selective attenuation\nand scattering when light propagates through water. Such degradation not only\naffects the quality of underwater images but limits the ability of vision\ntasks. Different from existing methods which either ignore the wavelength\ndependency of the attenuation or assume a specific spectral profile, we tackle\ncolor distortion problem of underwater image from a new view. In this letter,\nwe propose a weakly supervised color transfer method to correct color\ndistortion, which relaxes the need of paired underwater images for training and\nallows for the underwater images unknown where were taken. Inspired by\nCycle-Consistent Adversarial Networks, we design a multi-term loss function\nincluding adversarial loss, cycle consistency loss, and SSIM (Structural\nSimilarity Index Measure) loss, which allows the content and structure of the\ncorrected result the same as the input, but the color as if the image was taken\nwithout the water. Experiments on underwater images captured under diverse\nscenes show that our method produces visually pleasing results, even\noutperforms the art-of-the-state methods. Besides, our method can improve the\nperformance of vision tasks. \n\n"}
{"id": "1710.07168", "contents": "Title: Combining Multiple Views for Visual Speech Recognition Abstract: Visual speech recognition is a challenging research problem with a particular\npractical application of aiding audio speech recognition in noisy scenarios.\nMultiple camera setups can be beneficial for the visual speech recognition\nsystems in terms of improved performance and robustness. In this paper, we\nexplore this aspect and provide a comprehensive study on combining multiple\nviews for visual speech recognition. The thorough analysis covers fusion of all\npossible view angle combinations both at feature level and decision level. The\nemployed visual speech recognition system in this study extracts features\nthrough a PCA-based convolutional neural network, followed by an LSTM network.\nFinally, these features are processed in a tandem system, being fed into a\nGMM-HMM scheme. The decision fusion acts after this point by combining the\nViterbi path log-likelihoods. The results show that the complementary\ninformation contained in recordings from different view angles improves the\nresults significantly. For example, the sentence correctness on the test set is\nincreased from 76% for the highest performing single view ($30^\\circ$) to up to\n83% when combining this view with the frontal and $60^\\circ$ view angles. \n\n"}
{"id": "1710.07177", "contents": "Title: Findings of the Second Shared Task on Multimodal Machine Translation and\n  Multilingual Image Description Abstract: We present the results from the second shared task on multimodal machine\ntranslation and multilingual image description. Nine teams submitted 19 systems\nto two tasks. The multimodal translation task, in which the source sentence is\nsupplemented by an image, was extended with a new language (French) and two new\ntest sets. The multilingual image description task was changed such that at\ntest time, only the image is given. Compared to last year, multimodal systems\nimproved, but text-only systems remain competitive. \n\n"}
{"id": "1710.08543", "contents": "Title: Neural Stain-Style Transfer Learning using GAN for Histopathological\n  Images Abstract: Performance of data-driven network for tumor classification varies with\nstain-style of histopathological images. This article proposes the stain-style\ntransfer (SST) model based on conditional generative adversarial networks\n(GANs) which is to learn not only the certain color distribution but also the\ncorresponding histopathological pattern. Our model considers feature-preserving\nloss in addition to well-known GAN loss. Consequently our model does not only\ntransfers initial stain-styles to the desired one but also prevent the\ndegradation of tumor classifier on transferred images. The model is examined\nusing the CAMELYON16 dataset. \n\n"}
{"id": "1710.08864", "contents": "Title: One pixel attack for fooling deep neural networks Abstract: Recent research has revealed that the output of Deep Neural Networks (DNN)\ncan be easily altered by adding relatively small perturbations to the input\nvector. In this paper, we analyze an attack in an extremely limited scenario\nwhere only one pixel can be modified. For that we propose a novel method for\ngenerating one-pixel adversarial perturbations based on differential evolution\n(DE). It requires less adversarial information (a black-box attack) and can\nfool more types of networks due to the inherent features of DE. The results\nshow that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and\n16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least\none target class by modifying just one pixel with 74.03% and 22.91% confidence\non average. We also show the same vulnerability on the original CIFAR-10\ndataset. Thus, the proposed attack explores a different take on adversarial\nmachine learning in an extreme limited scenario, showing that current DNNs are\nalso vulnerable to such low dimension attacks. Besides, we also illustrate an\nimportant application of DE (or broadly speaking, evolutionary computation) in\nthe domain of adversarial machine learning: creating tools that can effectively\ngenerate low-cost adversarial attacks against neural networks for evaluating\nrobustness. \n\n"}
{"id": "1710.09471", "contents": "Title: Inductive Representation Learning in Large Attributed Graphs Abstract: Graphs (networks) are ubiquitous and allow us to model entities (nodes) and\nthe dependencies (edges) between them. Learning a useful feature representation\nfrom graph data lies at the heart and success of many machine learning tasks\nsuch as classification, anomaly detection, link prediction, among many others.\nMany existing techniques use random walks as a basis for learning features or\nestimating the parameters of a graph model for a downstream prediction task.\nExamples include recent node embedding methods such as DeepWalk, node2vec, as\nwell as graph-based deep learning algorithms. However, the simple random walk\nused by these methods is fundamentally tied to the identity of the node. This\nhas three main disadvantages. First, these approaches are inherently\ntransductive and do not generalize to unseen nodes and other graphs. Second,\nthey are not space-efficient as a feature vector is learned for each node which\nis impractical for large graphs. Third, most of these approaches lack support\nfor attributed graphs.\n  To make these methods more generally applicable, we propose a framework for\ninductive network representation learning based on the notion of attributed\nrandom walk that is not tied to node identity and is instead based on learning\na function $\\Phi : \\mathrm{\\rm \\bf x} \\rightarrow w$ that maps a node attribute\nvector $\\mathrm{\\rm \\bf x}$ to a type $w$. This framework serves as a basis for\ngeneralizing existing methods such as DeepWalk, node2vec, and many other\nprevious methods that leverage traditional random walks. \n\n"}
{"id": "1710.10000", "contents": "Title: PoseTrack: A Benchmark for Human Pose Estimation and Tracking Abstract: Human poses and motions are important cues for analysis of videos with people\nand there is strong evidence that representations based on body pose are highly\neffective for a variety of tasks such as activity recognition, content\nretrieval and social signal processing. In this work, we aim to further advance\nthe state of the art by establishing \"PoseTrack\", a new large-scale benchmark\nfor video-based human pose estimation and articulated tracking, and bringing\ntogether the community of researchers working on visual human analysis. The\nbenchmark encompasses three competition tracks focusing on i) single-frame\nmulti-person pose estimation, ii) multi-person pose estimation in videos, and\niii) multi-person articulated tracking. To facilitate the benchmark and\nchallenge we collect, annotate and release a new %large-scale benchmark dataset\nthat features videos with multiple people labeled with person tracks and\narticulated pose. A centralized evaluation server is provided to allow\nparticipants to evaluate on a held-out test set. We envision that the proposed\nbenchmark will stimulate productive research both by providing a large and\nrepresentative training dataset as well as providing a platform to objectively\nevaluate and compare the proposed methods. The benchmark is freely accessible\nat https://posetrack.net. \n\n"}
{"id": "1710.10460", "contents": "Title: Toward predictive machine learning for active vision Abstract: We develop a comprehensive description of the active inference framework, as\nproposed by Friston (2010), under a machine-learning compliant perspective.\nStemming from a biological inspiration and the auto-encoding principles, the\nsketch of a cognitive architecture is proposed that should provide ways to\nimplement estimation-oriented control policies. Computer simulations illustrate\nthe effectiveness of the approach through a foveated inspection of the input\ndata. The pros and cons of the control policy are analyzed in detail, showing\ninteresting promises in terms of processing compression. Though optimizing\nfuture posterior entropy over the actions set is shown enough to attain locally\noptimal action selection, offline calculation using class-specific saliency\nmaps is shown better for it saves processing costs through saccades pathways\npre-processing, with a negligible effect on the recognition/compression rates. \n\n"}
{"id": "1710.10736", "contents": "Title: Can you find a face in a HEVC bitstream? Abstract: Finding faces in images is one of the most important tasks in computer\nvision, with applications in biometrics, surveillance, human-computer\ninteraction, and other areas. In our earlier work, we demonstrated that it is\npossible to tell whether or not an image contains a face by only examining the\nHEVC syntax, without fully reconstructing the image. In the present work we\nmove further in this direction by showing how to localize faces in HEVC-coded\nimages, without full reconstruction. We also demonstrate the benefits that such\napproach can have in privacy-friendly face localization. \n\n"}
{"id": "1710.10903", "contents": "Title: Graph Attention Networks Abstract: We present graph attention networks (GATs), novel neural network\narchitectures that operate on graph-structured data, leveraging masked\nself-attentional layers to address the shortcomings of prior methods based on\ngraph convolutions or their approximations. By stacking layers in which nodes\nare able to attend over their neighborhoods' features, we enable (implicitly)\nspecifying different weights to different nodes in a neighborhood, without\nrequiring any kind of costly matrix operation (such as inversion) or depending\non knowing the graph structure upfront. In this way, we address several key\nchallenges of spectral-based graph neural networks simultaneously, and make our\nmodel readily applicable to inductive as well as transductive problems. Our GAT\nmodels have achieved or matched state-of-the-art results across four\nestablished transductive and inductive graph benchmarks: the Cora, Citeseer and\nPubmed citation network datasets, as well as a protein-protein interaction\ndataset (wherein test graphs remain unseen during training). \n\n"}
{"id": "1710.11063", "contents": "Title: Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks Abstract: Over the last decade, Convolutional Neural Network (CNN) models have been\nhighly successful in solving complex vision problems. However, these deep\nmodels are perceived as \"black box\" methods considering the lack of\nunderstanding of their internal functioning. There has been a significant\nrecent interest in developing explainable deep learning models, and this paper\nis an effort in this direction. Building on a recently proposed method called\nGrad-CAM, we propose a generalized method called Grad-CAM++ that can provide\nbetter visual explanations of CNN model predictions, in terms of better object\nlocalization as well as explaining occurrences of multiple object instances in\na single image, when compared to state-of-the-art. We provide a mathematical\nderivation for the proposed method, which uses a weighted combination of the\npositive partial derivatives of the last convolutional layer feature maps with\nrespect to a specific class score as weights to generate a visual explanation\nfor the corresponding class label. Our extensive experiments and evaluations,\nboth subjective and objective, on standard datasets showed that Grad-CAM++\nprovides promising human-interpretable visual explanations for a given CNN\narchitecture across multiple tasks including classification, image caption\ngeneration and 3D action recognition; as well as in new settings such as\nknowledge distillation. \n\n"}
{"id": "1710.11417", "contents": "Title: TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep\n  Reinforcement Learning Abstract: Combining deep model-free reinforcement learning with on-line planning is a\npromising approach to building on the successes of deep RL. On-line planning\nwith look-ahead trees has proven successful in environments where transition\nmodels are known a priori. However, in complex environments where transition\nmodels need to be learned from data, the deficiencies of learned models have\nlimited their utility for planning. To address these challenges, we propose\nTreeQN, a differentiable, recursive, tree-structured model that serves as a\ndrop-in replacement for any value function network in deep RL with discrete\nactions. TreeQN dynamically constructs a tree by recursively applying a\ntransition model in a learned abstract state space and then aggregating\npredicted rewards and state-values using a tree backup to estimate Q-values. We\nalso propose ATreeC, an actor-critic variant that augments TreeQN with a\nsoftmax layer to form a stochastic policy network. Both approaches are trained\nend-to-end, such that the learned model is optimised for its actual use in the\ntree. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a\nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et\nal. 2017) on multiple Atari games. Furthermore, we present ablation studies\nthat demonstrate the effect of different auxiliary losses on learning\ntransition models. \n\n"}
{"id": "1711.00139", "contents": "Title: Segmentation-by-Detection: A Cascade Network for Volumetric Medical\n  Image Segmentation Abstract: We propose an attention mechanism for 3D medical image segmentation. The\nmethod, named segmentation-by-detection, is a cascade of a detection module\nfollowed by a segmentation module. The detection module enables a region of\ninterest to come to attention and produces a set of object region candidates\nwhich are further used as an attention model. Rather than dealing with the\nentire volume, the segmentation module distills the information from the\npotential region. This scheme is an efficient solution for volumetric data as\nit reduces the influence of the surrounding noise which is especially important\nfor medical data with low signal-to-noise ratio. Experimental results on 3D\nultrasound data of the femoral head shows superiority of the proposed method\nwhen compared with a standard fully convolutional network like the U-Net. \n\n"}
{"id": "1711.00248", "contents": "Title: Query-free Clothing Retrieval via Implicit Relevance Feedback Abstract: Image-based clothing retrieval is receiving increasing interest with the\ngrowth of online shopping. In practice, users may often have a desired piece of\nclothing in mind (e.g., either having seen it before on the street or requiring\ncertain specific clothing attributes) but may be unable to supply an image as a\nquery. We model this problem as a new type of image retrieval task in which the\ntarget image resides only in the user's mind (called \"mental image retrieval\"\nhereafter). Because of the absence of an explicit query image, we propose to\nsolve this problem through relevance feedback. Specifically, a new Bayesian\nformulation is proposed that simultaneously models the retrieval target and its\nhigh-level representation in the mind of the user (called the \"user metric\"\nhereafter) as posterior distributions of pre-fetched shop images and\nheterogeneous features extracted from multiple clothing attributes,\nrespectively. Requiring only clicks as user feedback, the proposed algorithm is\nable to account for the variability in human decision-making. Experiments with\nreal users demonstrate the effectiveness of the proposed algorithm. \n\n"}
{"id": "1711.00455", "contents": "Title: A Unified View of Piecewise Linear Neural Network Verification Abstract: The success of Deep Learning and its potential use in many safety-critical\napplications has motivated research on formal verification of Neural Network\n(NN) models. Despite the reputation of learned NN models to behave as black\nboxes and the theoretical hardness of proving their properties, researchers\nhave been successful in verifying some classes of models by exploiting their\npiecewise linear structure and taking insights from formal methods such as\nSatisifiability Modulo Theory. These methods are however still far from scaling\nto realistic neural networks. To facilitate progress on this crucial area, we\nmake two key contributions. First, we present a unified framework that\nencompasses previous methods. This analysis results in the identification of\nnew methods that combine the strengths of multiple existing approaches,\naccomplishing a speedup of two orders of magnitude compared to the previous\nstate of the art. Second, we propose a new data set of benchmarks which\nincludes a collection of previously released testcases. We use the benchmark to\nprovide the first experimental comparison of existing algorithms and identify\nthe factors impacting the hardness of verification problems. \n\n"}
{"id": "1711.00848", "contents": "Title: Variational Inference of Disentangled Latent Concepts from Unlabeled\n  Observations Abstract: Disentangled representations, where the higher level data generative factors\nare reflected in disjoint latent dimensions, offer several benefits such as\nease of deriving invariant representations, transferability to other tasks,\ninterpretability, etc. We consider the problem of unsupervised learning of\ndisentangled representations from large pool of unlabeled observations, and\npropose a variational inference based approach to infer disentangled latent\nfactors. We introduce a regularizer on the expectation of the approximate\nposterior over observed data that encourages the disentanglement. We also\npropose a new disentanglement metric which is better aligned with the\nqualitative disentanglement observed in the decoder's output. We empirically\nobserve significant improvement over existing methods in terms of both\ndisentanglement and data likelihood (reconstruction quality). \n\n"}
{"id": "1711.00851", "contents": "Title: Provable defenses against adversarial examples via the convex outer\n  adversarial polytope Abstract: We propose a method to learn deep ReLU-based classifiers that are provably\nrobust against norm-bounded adversarial perturbations on the training data. For\npreviously unseen examples, the approach is guaranteed to detect all\nadversarial examples, though it may flag some non-adversarial examples as well.\nThe basic idea is to consider a convex outer approximation of the set of\nactivations reachable through a norm-bounded perturbation, and we develop a\nrobust optimization procedure that minimizes the worst case loss over this\nouter region (via a linear program). Crucially, we show that the dual problem\nto this linear program can be represented itself as a deep network similar to\nthe backpropagation network, leading to very efficient optimization approaches\nthat produce guaranteed bounds on the robust loss. The end result is that by\nexecuting a few more forward and backward passes through a slightly modified\nversion of the original network (though possibly with much larger batch sizes),\nwe can learn a classifier that is provably robust to any norm-bounded\nadversarial attack. We illustrate the approach on a number of tasks to train\nclassifiers with robust adversarial guarantees (e.g. for MNIST, we produce a\nconvolutional classifier that provably has less than 5.8% test error for any\nadversarial attack with bounded $\\ell_\\infty$ norm less than $\\epsilon = 0.1$),\nand code for all experiments in the paper is available at\nhttps://github.com/locuslab/convex_adversarial. \n\n"}
{"id": "1711.00889", "contents": "Title: Structured Generative Adversarial Networks Abstract: We study the problem of conditional generative modeling based on designated\nsemantics or structures. Existing models that build conditional generators\neither require massive labeled instances as supervision or are unable to\naccurately control the semantics of generated samples. We propose structured\ngenerative adversarial networks (SGANs) for semi-supervised conditional\ngenerative modeling. SGAN assumes the data x is generated conditioned on two\nindependent latent variables: y that encodes the designated semantics, and z\nthat contains other factors of variation. To ensure disentangled semantics in y\nand z, SGAN builds two collaborative games in the hidden space to minimize the\nreconstruction error of y and z, respectively. Training SGAN also involves\nsolving two adversarial games that have their equilibrium concentrating at the\ntrue joint data distributions p(x, z) and p(x, y), avoiding distributing the\nprobability mass diffusely over data space that MLE-based methods may suffer.\nWe assess SGAN by evaluating its trained networks, and its performance on\ndownstream tasks. We show that SGAN delivers a highly controllable generator,\nand disentangled representations; it also establishes start-of-the-art results\nacross multiple datasets when applied for semi-supervised image classification\n(1.27%, 5.73%, 17.26% error rates on MNIST, SVHN and CIFAR-10 using 50, 1000\nand 4000 labels, respectively). Benefiting from the separate modeling of y and\nz, SGAN can generate images with high visual quality and strictly following the\ndesignated semantic, and can be extended to a wide spectrum of applications,\nsuch as style transfer. \n\n"}
{"id": "1711.01506", "contents": "Title: Towards Automatic 3D Shape Instantiation for Deployed Stent Grafts: 2D\n  Multiple-class and Class-imbalance Marker Segmentation with Equally-weighted\n  Focal U-Net Abstract: Robot-assisted Fenestrated Endovascular Aortic Repair (FEVAR) is currently\nnavigated by 2D fluoroscopy which is insufficiently informative. Previously, a\nsemi-automatic 3D shape instantiation method was developed to instantiate the\n3D shape of a main, deployed, and fenestrated stent graft from a single\nfluoroscopy projection in real-time, which could help 3D FEVAR navigation and\nrobotic path planning. This proposed semi-automatic method was based on the\nRobust Perspective-5-Point (RP5P) method, graft gap interpolation and\nsemi-automatic multiple-class marker center determination. In this paper, an\nautomatic 3D shape instantiation could be achieved by automatic multiple-class\nmarker segmentation and hence automatic multiple-class marker center\ndetermination. Firstly, the markers were designed into five different shapes.\nThen, Equally-weighted Focal U-Net was proposed to segment the fluoroscopy\nprojections of customized markers into five classes and hence to determine the\nmarker centers. The proposed Equally-weighted Focal U-Net utilized U-Net as the\nnetwork architecture, equally-weighted loss function for initial marker\nsegmentation, and then equally-weighted focal loss function for improving the\ninitial marker segmentation. This proposed network outperformed traditional\nWeighted U-Net on the class-imbalance segmentation in this paper with reducing\none hyper-parameter - the weight. An overall mean Intersection over Union\n(mIoU) of 0.6943 was achieved on 78 testing images, where 81.01% markers were\nsegmented with a center position error <1.6mm. Comparable accuracy of 3D shape\ninstantiation was also achieved and stated. The data, trained models and\nTensorFlow codes are available on-line. \n\n"}
{"id": "1711.01577", "contents": "Title: Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence\n  Learning Abstract: Long Short-Term Memory (LSTM) is a popular approach to boosting the ability\nof Recurrent Neural Networks to store longer term temporal information. The\ncapacity of an LSTM network can be increased by widening and adding layers.\nHowever, usually the former introduces additional parameters, while the latter\nincreases the runtime. As an alternative we propose the Tensorized LSTM in\nwhich the hidden states are represented by tensors and updated via a\ncross-layer convolution. By increasing the tensor size, the network can be\nwidened efficiently without additional parameters since the parameters are\nshared across different locations in the tensor; by delaying the output, the\nnetwork can be deepened implicitly with little additional runtime since deep\ncomputations for each timestep are merged into temporal computations of the\nsequence. Experiments conducted on five challenging sequence learning tasks\nshow the potential of the proposed model. \n\n"}
{"id": "1711.01991", "contents": "Title: Mitigating Adversarial Effects Through Randomization Abstract: Convolutional neural networks have demonstrated high accuracy on various\ntasks in recent years. However, they are extremely vulnerable to adversarial\nexamples. For example, imperceptible perturbations added to clean images can\ncause convolutional neural networks to fail. In this paper, we propose to\nutilize randomization at inference time to mitigate adversarial effects.\nSpecifically, we use two randomization operations: random resizing, which\nresizes the input images to a random size, and random padding, which pads zeros\naround the input images in a random manner. Extensive experiments demonstrate\nthat the proposed randomization method is very effective at defending against\nboth single-step and iterative attacks. Our method provides the following\nadvantages: 1) no additional training or fine-tuning, 2) very few additional\ncomputations, 3) compatible with other adversarial defense methods. By\ncombining the proposed randomization method with an adversarially trained\nmodel, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense\nteams) in the NIPS 2017 adversarial examples defense challenge, which is far\nbetter than using adversarial training alone with a normalized score of 0.773\n(ranked No.56). The code is public available at\nhttps://github.com/cihangxie/NIPS2017_adv_challenge_defense. \n\n"}
{"id": "1711.02010", "contents": "Title: Artificial Generation of Big Data for Improving Image Classification: A\n  Generative Adversarial Network Approach on SAR Data Abstract: Very High Spatial Resolution (VHSR) large-scale SAR image databases are still\nan unresolved issue in the Remote Sensing field. In this work, we propose such\na dataset and use it to explore patch-based classification in urban and\nperiurban areas, considering 7 distinct semantic classes. In this context, we\ninvestigate the accuracy of large CNN classification models and pre-trained\nnetworks for SAR imaging systems. Furthermore, we propose a Generative\nAdversarial Network (GAN) for SAR image generation and test, whether the\nsynthetic data can actually improve classification accuracy. \n\n"}
{"id": "1711.02254", "contents": "Title: Doppler-Radar Based Hand Gesture Recognition System Using Convolutional\n  Neural Networks Abstract: Hand gesture recognition has long been a hot topic in human computer\ninteraction. Traditional camera-based hand gesture recognition systems cannot\nwork properly under dark circumstances. In this paper, a Doppler Radar based\nhand gesture recognition system using convolutional neural networks is\nproposed. A cost-effective Doppler radar sensor with dual receiving channels at\n5.8GHz is used to acquire a big database of four standard gestures. The\nreceived hand gesture signals are then processed with time-frequency analysis.\nConvolutional neural networks are used to classify different gestures.\nExperimental results verify the effectiveness of the system with an accuracy of\n98%. Besides, related factors such as recognition distance and gesture scale\nare investigated. \n\n"}
{"id": "1711.02613", "contents": "Title: Moonshine: Distilling with Cheap Convolutions Abstract: Many engineers wish to deploy modern neural networks in memory-limited\nsettings; but the development of flexible methods for reducing memory use is in\nits infancy, and there is little knowledge of the resulting cost-benefit. We\npropose structural model distillation for memory reduction using a strategy\nthat produces a student architecture that is a simple transformation of the\nteacher architecture: no redesign is needed, and the same hyperparameters can\nbe used. Using attention transfer, we provide Pareto curves/tables for\ndistillation of residual networks with four benchmark datasets, indicating the\nmemory versus accuracy payoff. We show that substantial memory savings are\npossible with very little loss of accuracy, and confirm that distillation\nprovides student network performance that is better than training that student\narchitecture directly on data. \n\n"}
{"id": "1711.02846", "contents": "Title: Intriguing Properties of Adversarial Examples Abstract: It is becoming increasingly clear that many machine learning classifiers are\nvulnerable to adversarial examples. In attempting to explain the origin of\nadversarial examples, previous studies have typically focused on the fact that\nneural networks operate on high dimensional data, they overfit, or they are too\nlinear. Here we argue that the origin of adversarial examples is primarily due\nto an inherent uncertainty that neural networks have about their predictions.\nWe show that the functional form of this uncertainty is independent of\narchitecture, dataset, and training protocol; and depends only on the\nstatistics of the logit differences of the network, which do not change\nsignificantly during training. This leads to adversarial error having a\nuniversal scaling, as a power-law, with respect to the size of the adversarial\nperturbation. We show that this universality holds for a broad range of\ndatasets (MNIST, CIFAR10, ImageNet, and random data), models (including\nstate-of-the-art deep networks, linear models, adversarially trained networks,\nand networks trained on randomly shuffled labels), and attacks (FGSM, step\nl.l., PGD). Motivated by these results, we study the effects of reducing\nprediction entropy on adversarial robustness. Finally, we study the effect of\nnetwork architectures on adversarial sensitivity. To do this, we use neural\narchitecture search with reinforcement learning to find adversarially robust\narchitectures on CIFAR10. Our resulting architecture is more robust to white\n\\emph{and} black box attacks compared to previous attempts. \n\n"}
{"id": "1711.03938", "contents": "Title: CARLA: An Open Urban Driving Simulator Abstract: We introduce CARLA, an open-source simulator for autonomous driving research.\nCARLA has been developed from the ground up to support development, training,\nand validation of autonomous urban driving systems. In addition to open-source\ncode and protocols, CARLA provides open digital assets (urban layouts,\nbuildings, vehicles) that were created for this purpose and can be used freely.\nThe simulation platform supports flexible specification of sensor suites and\nenvironmental conditions. We use CARLA to study the performance of three\napproaches to autonomous driving: a classic modular pipeline, an end-to-end\nmodel trained via imitation learning, and an end-to-end model trained via\nreinforcement learning. The approaches are evaluated in controlled scenarios of\nincreasing difficulty, and their performance is examined via metrics provided\nby CARLA, illustrating the platform's utility for autonomous driving research.\nThe supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E \n\n"}
{"id": "1711.05407", "contents": "Title: MARGIN: Uncovering Deep Neural Networks using Graph Signal Analysis Abstract: Interpretability has emerged as a crucial aspect of building trust in machine\nlearning systems, aimed at providing insights into the working of complex\nneural networks that are otherwise opaque to a user. There are a plethora of\nexisting solutions addressing various aspects of interpretability ranging from\nidentifying prototypical samples in a dataset to explaining image predictions\nor explaining mis-classifications. While all of these diverse techniques\naddress seemingly different aspects of interpretability, we hypothesize that a\nlarge family of interepretability tasks are variants of the same central\nproblem which is identifying \\emph{relative} change in a model's prediction.\nThis paper introduces MARGIN, a simple yet general approach to address a large\nset of interpretability tasks MARGIN exploits ideas rooted in graph signal\nanalysis to determine influential nodes in a graph, which are defined as those\nnodes that maximally describe a function defined on the graph. By carefully\ndefining task-specific graphs and functions, we demonstrate that MARGIN\noutperforms existing approaches in a number of disparate interpretability\nchallenges. \n\n"}
{"id": "1711.05726", "contents": "Title: Markov Decision Processes with Continuous Side Information Abstract: We consider a reinforcement learning (RL) setting in which the agent\ninteracts with a sequence of episodic MDPs. At the start of each episode the\nagent has access to some side-information or context that determines the\ndynamics of the MDP for that episode. Our setting is motivated by applications\nin healthcare where baseline measurements of a patient at the start of a\ntreatment episode form the context that may provide information about how the\npatient might respond to treatment decisions. We propose algorithms for\nlearning in such Contextual Markov Decision Processes (CMDPs) under an\nassumption that the unobserved MDP parameters vary smoothly with the observed\ncontext. We also give lower and upper PAC bounds under the smoothness\nassumption. Because our lower bound has an exponential dependence on the\ndimension, we consider a tractable linear setting where the context is used to\ncreate linear combinations of a finite set of MDPs. For the linear setting, we\ngive a PAC learning algorithm based on KWIK learning techniques. \n\n"}
{"id": "1711.05941", "contents": "Title: Skepxels: Spatio-temporal Image Representation of Human Skeleton Joints\n  for Action Recognition Abstract: Human skeleton joints are popular for action analysis since they can be\neasily extracted from videos to discard background noises. However, current\nskeleton representations do not fully benefit from machine learning with CNNs.\nWe propose \"Skepxels\" a spatio-temporal representation for skeleton sequences\nto fully exploit the \"local\" correlations between joints using the 2D\nconvolution kernels of CNN. We transform skeleton videos into images of\nflexible dimensions using Skepxels and develop a CNN-based framework for\neffective human action recognition using the resulting images. Skepxels encode\nrich spatio-temporal information about the skeleton joints in the frames by\nmaximizing a unique distance metric, defined collaboratively over the distinct\njoint arrangements used in the skeletal image. Moreover, they are flexible in\nencoding compound semantic notions such as location and speed of the joints.\nThe proposed action recognition exploits the representation in a hierarchical\nmanner by first capturing the micro-temporal relations between the skeleton\njoints with the Skepxels and then exploiting their macro-temporal relations by\ncomputing the Fourier Temporal Pyramids over the CNN features of the skeletal\nimages. We extend the Inception-ResNet CNN architecture with the proposed\nmethod and improve the state-of-the-art accuracy by 4.4% on the large scale NTU\nhuman activity dataset. On the medium-sized N-UCLA and UTH-MHAD datasets, our\nmethod outperforms the existing results by 5.7% and 9.3% respectively. \n\n"}
{"id": "1711.05954", "contents": "Title: Zero-Annotation Object Detection with Web Knowledge Transfer Abstract: Object detection is one of the major problems in computer vision, and has\nbeen extensively studied. Most of the existing detection works rely on\nlabor-intensive supervision, such as ground truth bounding boxes of objects or\nat least image-level annotations. On the contrary, we propose an object\ndetection method that does not require any form of human annotation on target\ntasks, by exploiting freely available web images. In order to facilitate\neffective knowledge transfer from web images, we introduce a multi-instance\nmulti-label domain adaption learning framework with two key innovations. First\nof all, we propose an instance-level adversarial domain adaptation network with\nattention on foreground objects to transfer the object appearances from web\ndomain to target domain. Second, to preserve the class-specific semantic\nstructure of transferred object features, we propose a simultaneous transfer\nmechanism to transfer the supervision across domains through pseudo strong\nlabel generation. With our end-to-end framework that simultaneously learns a\nweakly supervised detector and transfers knowledge across domains, we achieved\nsignificant improvements over baseline methods on the benchmark datasets. \n\n"}
{"id": "1711.06288", "contents": "Title: Language-Based Image Editing with Recurrent Attentive Models Abstract: We investigate the problem of Language-Based Image Editing (LBIE). Given a\nsource image and a natural language description, we want to generate a target\nimage by editing the source image based on the description. We propose a\ngeneric modeling framework for two sub-tasks of LBIE: language-based image\nsegmentation and image colorization. The framework uses recurrent attentive\nmodels to fuse image and language features. Instead of using a fixed step size,\nwe introduce for each region of the image a termination gate to dynamically\ndetermine after each inference step whether to continue extrapolating\nadditional information from the textual description. The effectiveness of the\nframework is validated on three datasets. First, we introduce a synthetic\ndataset, called CoSaL, to evaluate the end-to-end performance of our LBIE\nsystem. Second, we show that the framework leads to state-of-the-art\nperformance on image segmentation on the ReferIt dataset. Third, we present the\nfirst language-based colorization result on the Oxford-102 Flowers dataset. \n\n"}
{"id": "1711.06454", "contents": "Title: Separating Style and Content for Generalized Style Transfer Abstract: Neural style transfer has drawn broad attention in recent years. However,\nmost existing methods aim to explicitly model the transformation between\ndifferent styles, and the learned model is thus not generalizable to new\nstyles. We here attempt to separate the representations for styles and\ncontents, and propose a generalized style transfer network consisting of style\nencoder, content encoder, mixer and decoder. The style encoder and content\nencoder are used to extract the style and content factors from the style\nreference images and content reference images, respectively. The mixer employs\na bilinear model to integrate the above two factors and finally feeds it into a\ndecoder to generate images with target style and content. To separate the style\nfeatures and content features, we leverage the conditional dependence of styles\nand contents given an image. During training, the encoder network learns to\nextract styles and contents from two sets of reference images in limited size,\none with shared style and the other with shared content. This learning\nframework allows simultaneous style transfer among multiple styles and can be\ndeemed as a special `multi-task' learning scenario. The encoders are expected\nto capture the underlying features for different styles and contents which is\ngeneralizable to new styles and contents. For validation, we applied the\nproposed algorithm to the Chinese Typeface transfer problem. Extensive\nexperiment results on character generation have demonstrated the effectiveness\nand robustness of our method. \n\n"}
{"id": "1711.06640", "contents": "Title: Neural Motifs: Scene Graph Parsing with Global Context Abstract: We investigate the problem of producing structured graph representations of\nvisual scenes. Our work analyzes the role of motifs: regularly appearing\nsubstructures in scene graphs. We present new quantitative insights on such\nrepeated structures in the Visual Genome dataset. Our analysis shows that\nobject labels are highly predictive of relation labels but not vice-versa. We\nalso find that there are recurring patterns even in larger subgraphs: more than\n50% of graphs contain motifs involving at least two relations. Our analysis\nmotivates a new baseline: given object detections, predict the most frequent\nrelation between object pairs with the given labels, as seen in the training\nset. This baseline improves on the previous state-of-the-art by an average of\n3.6% relative improvement across evaluation settings. We then introduce Stacked\nMotif Networks, a new architecture designed to capture higher order motifs in\nscene graphs that further improves over our strong baseline by an average 7.1%\nrelative gain. Our code is available at github.com/rowanz/neural-motifs. \n\n"}
{"id": "1711.07201", "contents": "Title: End-to-end Trained CNN Encode-Decoder Networks for Image Steganography Abstract: All the existing image steganography methods use manually crafted features to\nhide binary payloads into cover images. This leads to small payload capacity\nand image distortion. Here we propose a convolutional neural network based\nencoder-decoder architecture for embedding of images as payload. To this end,\nwe make following three major contributions: (i) we propose a deep learning\nbased generic encoder-decoder architecture for image steganography; (ii) we\nintroduce a new loss function that ensures joint end-to-end training of\nencoder-decoder networks; (iii) we perform extensive empirical evaluation of\nproposed architecture on a range of challenging publicly available datasets\n(MNIST, CIFAR10, PASCAL-VOC12, ImageNet, LFW) and report state-of-the-art\npayload capacity at high PSNR and SSIM values. \n\n"}
{"id": "1711.07245", "contents": "Title: Optical Character Recognition (OCR) for Telugu: Database, Algorithm and\n  Application Abstract: Telugu is a Dravidian language spoken by more than 80 million people\nworldwide. The optical character recognition (OCR) of the Telugu script has\nwide ranging applications including education, health-care, administration etc.\nThe beautiful Telugu script however is very different from Germanic scripts\nlike English and German. This makes the use of transfer learning of Germanic\nOCR solutions to Telugu a non-trivial task. To address the challenge of OCR for\nTelugu, we make three contributions in this work: (i) a database of Telugu\ncharacters, (ii) a deep learning based OCR algorithm, and (iii) a client server\nsolution for the online deployment of the algorithm. For the benefit of the\nTelugu people and the research community, we will make our code freely\navailable at https://gayamtrishal.github.io/OCR_Telugu.github.io/ \n\n"}
{"id": "1711.07354", "contents": "Title: Convergent Block Coordinate Descent for Training Tikhonov Regularized\n  Deep Neural Networks Abstract: By lifting the ReLU function into a higher dimensional space, we develop a\nsmooth multi-convex formulation for training feed-forward deep neural networks\n(DNNs). This allows us to develop a block coordinate descent (BCD) training\nalgorithm consisting of a sequence of numerically well-behaved convex\noptimizations. Using ideas from proximal point methods in convex analysis, we\nprove that this BCD algorithm will converge globally to a stationary point with\nR-linear convergence rate of order one. In experiments with the MNIST database,\nDNNs trained with this BCD algorithm consistently yielded better test-set error\nrates than identical DNN architectures trained via all the stochastic gradient\ndescent (SGD) variants in the Caffe toolbox. \n\n"}
{"id": "1711.07356", "contents": "Title: Evaluating Robustness of Neural Networks with Mixed Integer Programming Abstract: Neural networks have demonstrated considerable success on a wide variety of\nreal-world problems. However, networks trained only to optimize for training\naccuracy can often be fooled by adversarial examples - slightly perturbed\ninputs that are misclassified with high confidence. Verification of networks\nenables us to gauge their vulnerability to such adversarial examples. We\nformulate verification of piecewise-linear neural networks as a mixed integer\nprogram. On a representative task of finding minimum adversarial distortions,\nour verifier is two to three orders of magnitude quicker than the\nstate-of-the-art. We achieve this computational speedup via tight formulations\nfor non-linearities, as well as a novel presolve algorithm that makes full use\nof all information available. The computational speedup allows us to verify\nproperties on convolutional networks with an order of magnitude more ReLUs than\nnetworks previously verified by any complete verifier. In particular, we\ndetermine for the first time the exact adversarial accuracy of an MNIST\nclassifier to perturbations with bounded $l_\\infty$ norm $\\epsilon=0.1$: for\nthis classifier, we find an adversarial example for 4.38% of samples, and a\ncertificate of robustness (to perturbations with bounded norm) for the\nremainder. Across all robust training procedures and network architectures\nconsidered, we are able to certify more samples than the state-of-the-art and\nfind more adversarial examples than a strong first-order attack. \n\n"}
{"id": "1711.07607", "contents": "Title: Knowledge Concentration: Learning 100K Object Classifiers in a Single\n  CNN Abstract: Fine-grained image labels are desirable for many computer vision\napplications, such as visual search or mobile AI assistant. These applications\nrely on image classification models that can produce hundreds of thousands\n(e.g. 100K) of diversified fine-grained image labels on input images. However,\ntraining a network at this vocabulary scale is challenging, and suffers from\nintolerable large model size and slow training speed, which leads to\nunsatisfying classification performance. A straightforward solution would be\ntraining separate expert networks (specialists), with each specialist focusing\non learning one specific vertical (e.g. cars, birds...). However, deploying\ndozens of expert networks in a practical system would significantly increase\nsystem complexity and inference latency, and consumes large amounts of\ncomputational resources. To address these challenges, we propose a Knowledge\nConcentration method, which effectively transfers the knowledge from dozens of\nspecialists (multiple teacher networks) into one single model (one student\nnetwork) to classify 100K object categories. There are three salient aspects in\nour method: (1) a multi-teacher single-student knowledge distillation\nframework; (2) a self-paced learning mechanism to allow the student to learn\nfrom different teachers at various paces; (3) structurally connected layers to\nexpand the student network capacity with limited extra parameters. We validate\nour method on OpenImage and a newly collected dataset, Entity-Foto-Tree (EFT),\nwith 100K categories, and show that the proposed model performs significantly\nbetter than the baseline generalist model. \n\n"}
{"id": "1711.07613", "contents": "Title: Are You Talking to Me? Reasoned Visual Dialog Generation through\n  Adversarial Learning Abstract: The Visual Dialogue task requires an agent to engage in a conversation about\nan image with a human. It represents an extension of the Visual Question\nAnswering task in that the agent needs to answer a question about an image, but\nit needs to do so in light of the previous dialogue that has taken place. The\nkey challenge in Visual Dialogue is thus maintaining a consistent, and natural\ndialogue while continuing to answer questions correctly. We present a novel\napproach that combines Reinforcement Learning and Generative Adversarial\nNetworks (GANs) to generate more human-like responses to questions. The GAN\nhelps overcome the relative paucity of training data, and the tendency of the\ntypical MLE-based approach to generate overly terse answers. Critically, the\nGAN is tightly integrated into the attention mechanism that generates\nhuman-interpretable reasons for each answer. This means that the discriminative\nmodel of the GAN has the task of assessing whether a candidate answer is\ngenerated by a human or not, given the provided reason. This is significant\nbecause it drives the generative model to produce high quality answers that are\nwell supported by the associated reasoning. The method also generates the\nstate-of-the-art results on the primary benchmark. \n\n"}
{"id": "1711.09250", "contents": "Title: Learning 3D Human Pose from Structure and Motion Abstract: 3D human pose estimation from a single image is a challenging problem,\nespecially for in-the-wild settings due to the lack of 3D annotated data. We\npropose two anatomically inspired loss functions and use them with a\nweakly-supervised learning framework to jointly learn from large-scale\nin-the-wild 2D and indoor/synthetic 3D data. We also present a simple temporal\nnetwork that exploits temporal and structural cues present in predicted pose\nsequences to temporally harmonize the pose estimations. We carefully analyze\nthe proposed contributions through loss surface visualizations and sensitivity\nanalysis to facilitate deeper understanding of their working mechanism. Our\ncomplete pipeline improves the state-of-the-art by 11.8% and 12% on Human3.6M\nand MPI-INF-3DHP, respectively, and runs at 30 FPS on a commodity graphics\ncard. \n\n"}
{"id": "1711.09492", "contents": "Title: Robust Subspace Learning: Robust PCA, Robust Subspace Tracking, and\n  Robust Subspace Recovery Abstract: PCA is one of the most widely used dimension reduction techniques. A related\neasier problem is \"subspace learning\" or \"subspace estimation\". Given\nrelatively clean data, both are easily solved via singular value decomposition\n(SVD). The problem of subspace learning or PCA in the presence of outliers is\ncalled robust subspace learning or robust PCA (RPCA). For long data sequences,\nif one tries to use a single lower dimensional subspace to represent the data,\nthe required subspace dimension may end up being quite large. For such data, a\nbetter model is to assume that it lies in a low-dimensional subspace that can\nchange over time, albeit gradually. The problem of tracking such data (and the\nsubspaces) while being robust to outliers is called robust subspace tracking\n(RST). This article provides a magazine-style overview of the entire field of\nrobust subspace learning and tracking. In particular solutions for three\nproblems are discussed in detail: RPCA via sparse+low-rank matrix decomposition\n(S+LR), RST via S+LR, and \"robust subspace recovery (RSR)\". RSR assumes that an\nentire data vector is either an outlier or an inlier. The S+LR formulation\ninstead assumes that outliers occur on only a few data vector indices and hence\nare well modeled as sparse corruptions. \n\n"}
{"id": "1711.10370", "contents": "Title: Learning to Segment Every Thing Abstract: Most methods for object instance segmentation require all training examples\nto be labeled with segmentation masks. This requirement makes it expensive to\nannotate new categories and has restricted instance segmentation models to ~100\nwell-annotated classes. The goal of this paper is to propose a new partially\nsupervised training paradigm, together with a novel weight transfer function,\nthat enables training instance segmentation models on a large set of categories\nall of which have box annotations, but only a small fraction of which have mask\nannotations. These contributions allow us to train Mask R-CNN to detect and\nsegment 3000 visual concepts using box annotations from the Visual Genome\ndataset and mask annotations from the 80 classes in the COCO dataset. We\nevaluate our approach in a controlled study on the COCO dataset. This work is a\nfirst step towards instance segmentation models that have broad comprehension\nof the visual world. \n\n"}
{"id": "1711.11543", "contents": "Title: Embodied Question Answering Abstract: We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where\nan agent is spawned at a random location in a 3D environment and asked a\nquestion (\"What color is the car?\"). In order to answer, the agent must first\nintelligently navigate to explore the environment, gather information through\nfirst-person (egocentric) vision, and then answer the question (\"orange\").\n  This challenging task requires a range of AI skills -- active perception,\nlanguage understanding, goal-driven navigation, commonsense reasoning, and\ngrounding of language into actions. In this work, we develop the environments,\nend-to-end-trained reinforcement learning agents, and evaluation protocols for\nEmbodiedQA. \n\n"}
{"id": "1711.11561", "contents": "Title: Measuring the tendency of CNNs to Learn Surface Statistical Regularities Abstract: Deep CNNs are known to exhibit the following peculiarity: on the one hand\nthey generalize extremely well to a test set, while on the other hand they are\nextremely sensitive to so-called adversarial perturbations. The extreme\nsensitivity of high performance CNNs to adversarial examples casts serious\ndoubt that these networks are learning high level abstractions in the dataset.\nWe are concerned with the following question: How can a deep CNN that does not\nlearn any high level semantics of the dataset manage to generalize so well? The\ngoal of this article is to measure the tendency of CNNs to learn surface\nstatistical regularities of the dataset. To this end, we use Fourier filtering\nto construct datasets which share the exact same high level abstractions but\nexhibit qualitatively different surface statistical regularities. For the SVHN\nand CIFAR-10 datasets, we present two Fourier filtered variants: a low\nfrequency variant and a randomly filtered variant. Each of the Fourier\nfiltering schemes is tuned to preserve the recognizability of the objects. Our\nmain finding is that CNNs exhibit a tendency to latch onto the Fourier image\nstatistics of the training dataset, sometimes exhibiting up to a 28%\ngeneralization gap across the various test sets. Moreover, we observe that\nsignificantly increasing the depth of a network has a very marginal impact on\nclosing the aforementioned generalization gap. Thus we provide quantitative\nevidence supporting the hypothesis that deep CNNs tend to learn surface\nstatistical regularities in the dataset rather than higher-level abstract\nconcepts. \n\n"}
{"id": "1712.00269", "contents": "Title: GANosaic: Mosaic Creation with Generative Texture Manifolds Abstract: This paper presents a novel framework for generating texture mosaics with\nconvolutional neural networks. Our method is called GANosaic and performs\noptimization in the latent noise space of a generative texture model, which\nallows the transformation of a content image into a mosaic exhibiting the\nvisual properties of the underlying texture manifold. To represent that\nmanifold, we use a state-of-the-art generative adversarial method for texture\nsynthesis, which can learn expressive texture representations from data and\nproduce mosaic images with very high resolution. This fully convolutional model\ngenerates smooth (without any visible borders) mosaic images which morph and\nblend different textures locally. In addition, we develop a new type of\ndifferentiable statistical regularization appropriate for optimization over the\nprior noise space of the PSGAN model. \n\n"}
{"id": "1712.00311", "contents": "Title: Folded Recurrent Neural Networks for Future Video Prediction Abstract: Future video prediction is an ill-posed Computer Vision problem that recently\nreceived much attention. Its main challenges are the high variability in video\ncontent, the propagation of errors through time, and the non-specificity of the\nfuture frames: given a sequence of past frames there is a continuous\ndistribution of possible futures. This work introduces bijective Gated\nRecurrent Units, a double mapping between the input and output of a GRU layer.\nThis allows for recurrent auto-encoders with state sharing between encoder and\ndecoder, stratifying the sequence representation and helping to prevent\ncapacity problems. We show how with this topology only the encoder or decoder\nneeds to be applied for input encoding and prediction, respectively. This\nreduces the computational cost and avoids re-encoding the predictions when\ngenerating a sequence of frames, mitigating the propagation of errors.\nFurthermore, it is possible to remove layers from an already trained model,\ngiving an insight to the role performed by each layer and making the model more\nexplainable. We evaluate our approach on three video datasets, outperforming\nstate of the art prediction results on MMNIST and UCF101, and obtaining\ncompetitive results on KTH with 2 and 3 times less memory usage and\ncomputational cost than the best scored approach. \n\n"}
{"id": "1712.00523", "contents": "Title: Towards understanding feedback from supermassive black holes using\n  convolutional neural networks Abstract: Supermassive black holes at centers of clusters of galaxies strongly interact\nwith their host environment via AGN feedback. Key tracers of such activity are\nX-ray cavities -- regions of lower X-ray brightness within the cluster. We\npresent an automatic method for detecting, and characterizing X-ray cavities in\nnoisy, low-resolution X-ray images. We simulate clusters of galaxies, insert\ncavities into them, and produce realistic low-quality images comparable to\nobservations at high redshifts. We then train a custom-built convolutional\nneural network to generate pixel-wise analysis of presence of cavities in a\ncluster. A ResNet architecture is then used to decode radii of cavities from\nthe pixel-wise predictions. We surpass the accuracy, stability, and speed of\ncurrent visual inspection based methods on simulated data. \n\n"}
{"id": "1712.00736", "contents": "Title: Towards Real-Time Advancement of Underwater Visual Quality with GAN Abstract: Low visual quality has prevented underwater robotic vision from a wide range\nof applications. Although several algorithms have been developed, real-time and\nadaptive methods are deficient for real-world tasks. In this paper, we address\nthis difficulty based on generative adversarial networks (GAN), and propose a\nGAN-based restoration scheme (GAN-RS). In particular, we develop a multi-branch\ndiscriminator including an adversarial branch and a critic branch for the\npurpose of simultaneously preserving image content and removing underwater\nnoise. In addition to adversarial learning, a novel dark channel prior loss\nalso promotes the generator to produce realistic vision. More specifically, an\nunderwater index is investigated to describe underwater properties, and a loss\nfunction based on the underwater index is designed to train the critic branch\nfor underwater noise suppression. Through extensive comparisons on visual\nquality and feature restoration, we confirm the superiority of the proposed\napproach. Consequently, the GAN-RS can adaptively improve underwater visual\nquality in real time and induce an overall superior restoration performance.\nFinally, a real-world experiment is conducted on the seabed for grasping marine\nproducts, and the results are quite promising. The source code is publicly\navailable at https://github.com/SeanChenxy/GAN_RS. \n\n"}
{"id": "1712.00840", "contents": "Title: Visual Explanation by High-Level Abduction: On Answer-Set Programming\n  Driven Reasoning about Moving Objects Abstract: We propose a hybrid architecture for systematically computing robust visual\nexplanation(s) encompassing hypothesis formation, belief revision, and default\nreasoning with video data. The architecture consists of two tightly integrated\nsynergistic components: (1) (functional) answer set programming based abductive\nreasoning with space-time tracklets as native entities; and (2) a visual\nprocessing pipeline for detection based object tracking and motion analysis.\n  We present the formal framework, its general implementation as a\n(declarative) method in answer set programming, and an example application and\nevaluation based on two diverse video datasets: the MOTChallenge benchmark\ndeveloped by the vision community, and a recently developed Movie Dataset. \n\n"}
{"id": "1712.00912", "contents": "Title: Deep Learning Diffuse Optical Tomography Abstract: Diffuse optical tomography (DOT) has been investigated as an alternative\nimaging modality for breast cancer detection thanks to its excellent contrast\nto hemoglobin oxidization level. However, due to the complicated non-linear\nphoton scattering physics and ill-posedness, the conventional reconstruction\nalgorithms are sensitive to imaging parameters such as boundary conditions. To\naddress this, here we propose a novel deep learning approach that learns\nnon-linear photon scattering physics and obtains an accurate three dimensional\n(3D) distribution of optical anomalies. In contrast to the traditional\nblack-box deep learning approaches, our deep network is designed to invert the\nLippman-Schwinger integral equation using the recent mathematical theory of\ndeep convolutional framelets. As an example of clinical relevance, we applied\nthe method to our prototype DOT system. We show that our deep neural network,\ntrained with only simulation data, can accurately recover the location of\nanomalies within biomimetic phantoms and live animals without the use of an\nexogenous contrast agent. \n\n"}
{"id": "1712.01358", "contents": "Title: Long-Term Visual Object Tracking Benchmark Abstract: We propose a new long video dataset (called Track Long and Prosper - TLP) and\nbenchmark for single object tracking. The dataset consists of 50 HD videos from\nreal world scenarios, encompassing a duration of over 400 minutes (676K\nframes), making it more than 20 folds larger in average duration per sequence\nand more than 8 folds larger in terms of total covered duration, as compared to\nexisting generic datasets for visual tracking. The proposed dataset paves a way\nto suitably assess long term tracking performance and train better deep\nlearning architectures (avoiding/reducing augmentation, which may not reflect\nreal world behaviour). We benchmark the dataset on 17 state of the art trackers\nand rank them according to tracking accuracy and run time speeds. We further\npresent thorough qualitative and quantitative evaluation highlighting the\nimportance of long term aspect of tracking. Our most interesting observations\nare (a) existing short sequence benchmarks fail to bring out the inherent\ndifferences in tracking algorithms which widen up while tracking on long\nsequences and (b) the accuracy of trackers abruptly drops on challenging long\nsequences, suggesting the potential need of research efforts in the direction\nof long-term tracking. \n\n"}
{"id": "1712.01443", "contents": "Title: 4DFAB: A Large Scale 4D Facial Expression Database for Biometric\n  Applications Abstract: The progress we are currently witnessing in many computer vision\napplications, including automatic face analysis, would not be made possible\nwithout tremendous efforts in collecting and annotating large scale visual\ndatabases. To this end, we propose 4DFAB, a new large scale database of dynamic\nhigh-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings\nof 180 subjects captured in four different sessions spanning over a five-year\nperiod. It contains 4D videos of subjects displaying both spontaneous and posed\nfacial behaviours. The database can be used for both face and facial expression\nrecognition, as well as behavioural biometrics. It can also be used to learn\nvery powerful blendshapes for parametrising facial behaviour. In this paper, we\nconduct several experiments and demonstrate the usefulness of the database for\nvarious applications. The database will be made publicly available for research\npurposes. \n\n"}
{"id": "1712.01641", "contents": "Title: Fully Convolutional Measurement Network for Compressive Sensing Image\n  Reconstruction Abstract: Recently, deep learning methods have made a significant improvement in\ncompressive sensing image reconstruction task. In the existing methods, the\nscene is measured block by block due to the high computational complexity. This\nresults in block-effect of the recovered images. In this paper, we propose a\nfully convolutional measurement network, where the scene is measured as a\nwhole. The proposed method powerfully removes the block-effect since the\nstructure information of scene images is preserved. To make the measure more\nflexible, the measurement and the recovery parts are jointly trained. From the\nexperiments, it is shown that the results by the proposed method outperforms\nthose by the existing methods in PSNR, SSIM, and visual effect. \n\n"}
{"id": "1712.01651", "contents": "Title: Dilated FCN for Multi-Agent 2D/3D Medical Image Registration Abstract: 2D/3D image registration to align a 3D volume and 2D X-ray images is a\nchallenging problem due to its ill-posed nature and various artifacts presented\nin 2D X-ray images. In this paper, we propose a multi-agent system with an auto\nattention mechanism for robust and efficient 2D/3D image registration.\nSpecifically, an individual agent is trained with dilated Fully Convolutional\nNetwork (FCN) to perform registration in a Markov Decision Process (MDP) by\nobserving a local region, and the final action is then taken based on the\nproposals from multiple agents and weighted by their corresponding confidence\nlevels. The contributions of this paper are threefold. First, we formulate\n2D/3D registration as a MDP with observations, actions, and rewards properly\ndefined with respect to X-ray imaging systems. Second, to handle various\nartifacts in 2D X-ray images, multiple local agents are employed efficiently\nvia FCN-based structures, and an auto attention mechanism is proposed to favor\nthe proposals from regions with more reliable visual cues. Third, a dilated\nFCN-based training mechanism is proposed to significantly reduce the Degree of\nFreedom in the simulation of registration environment, and drastically improve\ntraining efficiency by an order of magnitude compared to standard CNN-based\ntraining method. We demonstrate that the proposed method achieves high\nrobustness on both spine cone beam Computed Tomography data with a low\nsignal-to-noise ratio and data from minimally invasive spine surgery where\nsevere image artifacts and occlusions are presented due to metal screws and\nguide wires, outperforming other state-of-the-art methods (single agent-based\nand optimization-based) by a large margin. \n\n"}
{"id": "1712.01938", "contents": "Title: Learning Latent Super-Events to Detect Multiple Activities in Videos Abstract: In this paper, we introduce the concept of learning latent super-events from\nactivity videos, and present how it benefits activity detection in continuous\nvideos. We define a super-event as a set of multiple events occurring together\nin videos with a particular temporal organization; it is the opposite concept\nof sub-events. Real-world videos contain multiple activities and are rarely\nsegmented (e.g., surveillance videos), and learning latent super-events allows\nthe model to capture how the events are temporally related in videos. We design\ntemporal structure filters that enable the model to focus on particular\nsub-intervals of the videos, and use them together with a soft attention\nmechanism to learn representations of latent super-events. Super-event\nrepresentations are combined with per-frame or per-segment CNNs to provide\nframe-level annotations. Our approach is designed to be fully differentiable,\nenabling end-to-end learning of latent super-event representations jointly with\nthe activity detector using them. Our experiments with multiple public video\ndatasets confirm that the proposed concept of latent super-event learning\nsignificantly benefits activity detection, advancing the state-of-the-arts. \n\n"}
{"id": "1712.02310", "contents": "Title: From Lifestyle Vlogs to Everyday Interactions Abstract: A major stumbling block to progress in understanding basic human\ninteractions, such as getting out of bed or opening a refrigerator, is lack of\ngood training data. Most past efforts have gathered this data explicitly:\nstarting with a laundry list of action labels, and then querying search engines\nfor videos tagged with each label. In this work, we do the reverse and search\nimplicitly: we start with a large collection of interaction-rich video data and\nthen annotate and analyze it. We use Internet Lifestyle Vlogs as the source of\nsurprisingly large and diverse interaction data. We show that by collecting the\ndata first, we are able to achieve greater scale and far greater diversity in\nterms of actions and actors. Additionally, our data exposes biases built into\ncommon explicitly gathered data. We make sense of our data by analyzing the\ncentral component of interaction -- hands. We benchmark two tasks: identifying\nsemantic object contact at the video level and non-semantic contact state at\nthe frame level. We additionally demonstrate future prediction of hands. \n\n"}
{"id": "1712.02502", "contents": "Title: Take it in your stride: Do we need striding in CNNs? Abstract: Since their inception, CNNs have utilized some type of striding operator to\nreduce the overlap of receptive fields and spatial dimensions. Although having\nclear heuristic motivations (i.e. lowering the number of parameters to learn)\nthe mathematical role of striding within CNN learning remains unclear. This\npaper offers a novel and mathematical rigorous perspective on the role of the\nstriding operator within modern CNNs. Specifically, we demonstrate\ntheoretically that one can always represent a CNN that incorporates striding\nwith an equivalent non-striding CNN which has more filters and smaller size.\nThrough this equivalence we are then able to characterize striding as an\nadditional mechanism for parameter sharing among channels, thus reducing\ntraining complexity. Finally, the framework presented in this paper offers a\nnew mathematical perspective on the role of striding which we hope shall\nfacilitate and simplify the future theoretical analysis of CNNs. \n\n"}
{"id": "1712.02527", "contents": "Title: Learning Random Fourier Features by Hybrid Constrained Optimization Abstract: The kernel embedding algorithm is an important component for adapting kernel\nmethods to large datasets. Since the algorithm consumes a major computation\ncost in the testing phase, we propose a novel teacher-learner framework of\nlearning computation-efficient kernel embeddings from specific data. In the\nframework, the high-precision embeddings (teacher) transfer the data\ninformation to the computation-efficient kernel embeddings (learner). We\njointly select informative embedding functions and pursue an orthogonal\ntransformation between two embeddings. We propose a novel approach of\nconstrained variational expectation maximization (CVEM), where the alternate\ndirection method of multiplier (ADMM) is applied over a nonconvex domain in the\nmaximization step. We also propose two specific formulations based on the\nprevalent Random Fourier Feature (RFF), the masked and blocked version of\nComputation-Efficient RFF (CERF), by imposing a random binary mask or a block\nstructure on the transformation matrix. By empirical studies of several\napplications on different real-world datasets, we demonstrate that the CERF\nsignificantly improves the performance of kernel methods upon the RFF, under\ncertain arithmetic operation requirements, and suitable for structured matrix\nmultiplication in Fastfood type algorithms. \n\n"}
{"id": "1712.02621", "contents": "Title: Disentangled Person Image Generation Abstract: Generating novel, yet realistic, images of persons is a challenging task due\nto the complex interplay between the different image factors, such as the\nforeground, background and pose information. In this work, we aim at generating\nsuch images based on a novel, two-stage reconstruction pipeline that learns a\ndisentangled representation of the aforementioned image factors and generates\nnovel person images at the same time. First, a multi-branched reconstruction\nnetwork is proposed to disentangle and encode the three factors into embedding\nfeatures, which are then combined to re-compose the input image itself. Second,\nthree corresponding mapping functions are learned in an adversarial manner in\norder to map Gaussian noise to the learned embedding feature space, for each\nfactor respectively. Using the proposed framework, we can manipulate the\nforeground, background and pose of the input image, and also sample new\nembedding features to generate such targeted manipulations, that provide more\ncontrol over the generation process. Experiments on Market-1501 and Deepfashion\ndatasets show that our model does not only generate realistic person images\nwith new foregrounds, backgrounds and poses, but also manipulates the generated\nfactors and interpolates the in-between states. Another set of experiments on\nMarket-1501 shows that our model can also be beneficial for the person\nre-identification task. \n\n"}
{"id": "1712.02743", "contents": "Title: End-to-end Learning of Deterministic Decision Trees Abstract: Conventional decision trees have a number of favorable properties, including\ninterpretability, a small computational footprint and the ability to learn from\nlittle training data. However, they lack a key quality that has helped fuel the\ndeep learning revolution: that of being end-to-end trainable, and to learn from\nscratch those features that best allow to solve a given supervised learning\nproblem. Recent work (Kontschieder 2015) has addressed this deficit, but at the\ncost of losing a main attractive trait of decision trees: the fact that each\nsample is routed along a small subset of tree nodes only. We here propose a\nmodel and Expectation-Maximization training scheme for decision trees that are\nfully probabilistic at train time, but after a deterministic annealing process\nbecome deterministic at test time. We also analyze the learned oblique split\nparameters on image datasets and show that Neural Networks can be trained at\neach split node. In summary, we present the first end-to-end learning scheme\nfor deterministic decision trees and present results on par with or superior to\npublished standard oblique decision tree algorithms. \n\n"}
{"id": "1712.03141", "contents": "Title: Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning Abstract: Learning-based pattern classifiers, including deep networks, have shown\nimpressive performance in several application domains, ranging from computer\nvision to cybersecurity. However, it has also been shown that adversarial input\nperturbations carefully crafted either at training or at test time can easily\nsubvert their predictions. The vulnerability of machine learning to such wild\npatterns (also referred to as adversarial examples), along with the design of\nsuitable countermeasures, have been investigated in the research field of\nadversarial machine learning. In this work, we provide a thorough overview of\nthe evolution of this research area over the last ten years and beyond,\nstarting from pioneering, earlier work on the security of non-deep learning\nalgorithms up to more recent work aimed to understand the security properties\nof deep learning algorithms, in the context of computer vision and\ncybersecurity tasks. We report interesting connections between these\napparently-different lines of work, highlighting common misconceptions related\nto the security evaluation of machine-learning algorithms. We review the main\nthreat models and attacks defined to this end, and discuss the main limitations\nof current work, along with the corresponding future challenges towards the\ndesign of more secure learning algorithms. \n\n"}
{"id": "1712.03382", "contents": "Title: Visual aesthetic analysis using deep neural network: model and\n  techniques to increase accuracy without transfer learning Abstract: We train a deep Convolutional Neural Network (CNN) from scratch for visual\naesthetic analysis in images and discuss techniques we adopt to improve the\naccuracy. We avoid the prevalent best transfer learning approaches of using\npretrained weights to perform the task and train a model from scratch to get\naccuracy of 78.7% on AVA2 Dataset close to the best models available (85.6%).\nWe further show that accuracy increases to 81.48% on increasing the training\nset by incremental 10 percentile of entire AVA dataset showing our algorithm\ngets better with more data. \n\n"}
{"id": "1712.04143", "contents": "Title: Benchmarking Single Image Dehazing and Beyond Abstract: We present a comprehensive study and evaluation of existing single image\ndehazing algorithms, using a new large-scale benchmark consisting of both\nsynthetic and real-world hazy images, called REalistic Single Image DEhazing\n(RESIDE). RESIDE highlights diverse data sources and image contents, and is\ndivided into five subsets, each serving different training or evaluation\npurposes. We further provide a rich variety of criteria for dehazing algorithm\nevaluation, ranging from full-reference metrics, to no-reference metrics, to\nsubjective evaluation and the novel task-driven evaluation. Experiments on\nRESIDE shed light on the comparisons and limitations of state-of-the-art\ndehazing algorithms, and suggest promising future directions. \n\n"}
{"id": "1712.04248", "contents": "Title: Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box\n  Machine Learning Models Abstract: Many machine learning algorithms are vulnerable to almost imperceptible\nperturbations of their inputs. So far it was unclear how much risk adversarial\nperturbations carry for the safety of real-world machine learning applications\nbecause most methods used to generate such perturbations rely either on\ndetailed model information (gradient-based attacks) or on confidence scores\nsuch as class probabilities (score-based attacks), neither of which are\navailable in most real-world scenarios. In many such cases one currently needs\nto retreat to transfer-based attacks which rely on cumbersome substitute\nmodels, need access to the training data and can be defended against. Here we\nemphasise the importance of attacks which solely rely on the final model\ndecision. Such decision-based attacks are (1) applicable to real-world\nblack-box models such as autonomous cars, (2) need less knowledge and are\neasier to apply than transfer-based attacks and (3) are more robust to simple\ndefences than gradient- or score-based attacks. Previous attacks in this\ncategory were limited to simple models or simple datasets. Here we introduce\nthe Boundary Attack, a decision-based attack that starts from a large\nadversarial perturbation and then seeks to reduce the perturbation while\nstaying adversarial. The attack is conceptually simple, requires close to no\nhyperparameter tuning, does not rely on substitute models and is competitive\nwith the best gradient-based attacks in standard computer vision tasks like\nImageNet. We apply the attack on two black-box algorithms from Clarifai.com.\nThe Boundary Attack in particular and the class of decision-based attacks in\ngeneral open new avenues to study the robustness of machine learning models and\nraise new questions regarding the safety of deployed machine learning systems.\nAn implementation of the attack is available as part of Foolbox at\nhttps://github.com/bethgelab/foolbox . \n\n"}
{"id": "1712.05080", "contents": "Title: Weakly Supervised Action Localization by Sparse Temporal Pooling Network Abstract: We propose a weakly supervised temporal action localization algorithm on\nuntrimmed videos using convolutional neural networks. Our algorithm learns from\nvideo-level class labels and predicts temporal intervals of human actions with\nno requirement of temporal localization annotations. We design our network to\nidentify a sparse subset of key segments associated with target actions in a\nvideo using an attention module and fuse the key segments through adaptive\ntemporal pooling. Our loss function is comprised of two terms that minimize the\nvideo-level action classification error and enforce the sparsity of the segment\nselection. At inference time, we extract and score temporal proposals using\ntemporal class activations and class-agnostic attentions to estimate the time\nintervals that correspond to target actions. The proposed algorithm attains\nstate-of-the-art results on the THUMOS14 dataset and outstanding performance on\nActivityNet1.3 even with its weak supervision. \n\n"}
{"id": "1712.05116", "contents": "Title: Multi-appearance Segmentation and Extended 0-1 Program for Dense Small\n  Object Tracking Abstract: Aiming to address the fast multi-object tracking for dense small object in\nthe cluster background, we review track orientated multi-hypothesis\ntracking(TOMHT) with consideration of batch optimization. Employing\nautocorrelation based motion score test and staged hypotheses merging approach,\nwe build our homologous hypothesis generation and management method. A new\none-to-many constraint is proposed and applied to tackle the track exclusions\nduring complex occlusions. Besides, to achieve better results, we develop a\nmulti-appearance segmentation for detection, which exploits tree-like\ntopological information and realizes one threshold for one object. Experimental\nresults verify the strength of our methods, indicating speed and performance\nadvantages of our tracker. \n\n"}
{"id": "1712.05577", "contents": "Title: The exploding gradient problem demystified - definition, prevalence,\n  impact, origin, tradeoffs, and solutions Abstract: Whereas it is believed that techniques such as Adam, batch normalization and,\nmore recently, SeLU nonlinearities \"solve\" the exploding gradient problem, we\nshow that this is not the case in general and that in a range of popular MLP\narchitectures, exploding gradients exist and that they limit the depth to which\nnetworks can be effectively trained, both in theory and in practice. We explain\nwhy exploding gradients occur and highlight the *collapsing domain problem*,\nwhich can arise in architectures that avoid exploding gradients.\n  ResNets have significantly lower gradients and thus can circumvent the\nexploding gradient problem, enabling the effective training of much deeper\nnetworks. We show this is a direct consequence of the Pythagorean equation. By\nnoticing that *any neural network is a residual network*, we devise the\n*residual trick*, which reveals that introducing skip connections simplifies\nthe network mathematically, and that this simplicity may be the major cause for\ntheir success. \n\n"}
{"id": "1712.05969", "contents": "Title: Learning a Virtual Codec Based on Deep Convolutional Neural Network to\n  Compress Image Abstract: Although deep convolutional neural network has been proved to efficiently\neliminate coding artifacts caused by the coarse quantization of traditional\ncodec, it's difficult to train any neural network in front of the encoder for\ngradient's back-propagation. In this paper, we propose an end-to-end image\ncompression framework based on convolutional neural network to resolve the\nproblem of non-differentiability of the quantization function in the standard\ncodec. First, the feature description neural network is used to get a valid\ndescription in the low-dimension space with respect to the ground-truth image\nso that the amount of image data is greatly reduced for storage or\ntransmission. After image's valid description, standard image codec such as\nJPEG is leveraged to further compress image, which leads to image's great\ndistortion and compression artifacts, especially blocking artifacts, detail\nmissing, blurring, and ringing artifacts. Then, we use a post-processing neural\nnetwork to remove these artifacts. Due to the challenge of directly learning a\nnon-linear function for a standard codec based on convolutional neural network,\nwe propose to learn a virtual codec neural network to approximate the\nprojection from the valid description image to the post-processed compressed\nimage, so that the gradient could be efficiently back-propagated from the\npost-processing neural network to the feature description neural network during\ntraining. Meanwhile, an advanced learning algorithm is proposed to train our\ndeep neural networks for compression. Obviously, the priority of the proposed\nmethod is compatible with standard existing codecs and our learning strategy\ncan be easily extended into these codecs based on convolutional neural network.\nExperimental results have demonstrated the advances of the proposed method as\ncompared to several state-of-the-art approaches, especially at very low\nbit-rate. \n\n"}
{"id": "1712.06096", "contents": "Title: Efficient B-mode Ultrasound Image Reconstruction from Sub-sampled RF\n  Data using Deep Learning Abstract: In portable, three dimensional, and ultra-fast ultrasound imaging systems,\nthere is an increasing demand for the reconstruction of high quality images\nfrom a limited number of radio-frequency (RF) measurements due to receiver (Rx)\nor transmit (Xmit) event sub-sampling. However, due to the presence of side\nlobe artifacts from RF sub-sampling, the standard beamformer often produces\nblurry images with less contrast, which are unsuitable for diagnostic purposes.\nExisting compressed sensing approaches often require either hardware changes or\ncomputationally expensive algorithms, but their quality improvements are\nlimited. To address this problem, here we propose a novel deep learning\napproach that directly interpolates the missing RF data by utilizing redundancy\nin the Rx-Xmit plane. Our extensive experimental results using sub-sampled RF\ndata from a multi-line acquisition B-mode system confirm that the proposed\nmethod can effectively reduce the data rate without sacrificing image quality. \n\n"}
{"id": "1712.06228", "contents": "Title: Visual Explanations from Hadamard Product in Multimodal Deep Networks Abstract: The visual explanation of learned representation of models helps to\nunderstand the fundamentals of learning. The attentional models of previous\nworks used to visualize the attended regions over an image or text using their\nlearned weights to confirm their intended mechanism. Kim et al. (2016) show\nthat the Hadamard product in multimodal deep networks, which is well-known for\nthe joint function of visual question answering tasks, implicitly performs an\nattentional mechanism for visual inputs. In this work, we extend their work to\nshow that the Hadamard product in multimodal deep networks performs not only\nfor visual inputs but also for textual inputs simultaneously using the proposed\ngradient-based visualization technique. The attentional effect of Hadamard\nproduct is visualized for both visual and textual inputs by analyzing the two\ninputs and an output of the Hadamard product with the proposed method and\ncompared with learned attentional weights of a visual question answering model. \n\n"}
{"id": "1712.06584", "contents": "Title: End-to-end Recovery of Human Shape and Pose Abstract: We describe Human Mesh Recovery (HMR), an end-to-end framework for\nreconstructing a full 3D mesh of a human body from a single RGB image. In\ncontrast to most current methods that compute 2D or 3D joint locations, we\nproduce a richer and more useful mesh representation that is parameterized by\nshape and 3D joint angles. The main objective is to minimize the reprojection\nloss of keypoints, which allow our model to be trained using images in-the-wild\nthat only have ground truth 2D annotations. However, the reprojection loss\nalone leaves the model highly under constrained. In this work we address this\nproblem by introducing an adversary trained to tell whether a human body\nparameter is real or not using a large database of 3D human meshes. We show\nthat HMR can be trained with and without using any paired 2D-to-3D supervision.\nWe do not rely on intermediate 2D keypoint detections and infer 3D pose and\nshape parameters directly from image pixels. Our model runs in real-time given\na bounding box containing the person. We demonstrate our approach on various\nimages in-the-wild and out-perform previous optimization based methods that\noutput 3D meshes and show competitive results on tasks such as 3D joint\nlocation estimation and part segmentation. \n\n"}
{"id": "1712.08087", "contents": "Title: Learning Intelligent Dialogs for Bounding Box Annotation Abstract: We introduce Intelligent Annotation Dialogs for bounding box annotation. We\ntrain an agent to automatically choose a sequence of actions for a human\nannotator to produce a bounding box in a minimal amount of time. Specifically,\nwe consider two actions: box verification, where the annotator verifies a box\ngenerated by an object detector, and manual box drawing. We explore two kinds\nof agents, one based on predicting the probability that a box will be\npositively verified, and the other based on reinforcement learning. We\ndemonstrate that (1) our agents are able to learn efficient annotation\nstrategies in several scenarios, automatically adapting to the image\ndifficulty, the desired quality of the boxes, and the detector strength; (2) in\nall scenarios the resulting annotation dialogs speed up annotation compared to\nmanual box drawing alone and box verification alone, while also outperforming\nany fixed combination of verification and drawing in most scenarios; (3) in a\nrealistic scenario where the detector is iteratively re-trained, our agents\nevolve a series of strategies that reflect the shifting trade-off between\nverification and drawing as the detector grows stronger. \n\n"}
{"id": "1712.08266", "contents": "Title: Federated Control with Hierarchical Multi-Agent Deep Reinforcement\n  Learning Abstract: We present a framework combining hierarchical and multi-agent deep\nreinforcement learning approaches to solve coordination problems among a\nmultitude of agents using a semi-decentralized model. The framework extends the\nmulti-agent learning setup by introducing a meta-controller that guides the\ncommunication between agent pairs, enabling agents to focus on communicating\nwith only one other agent at any step. This hierarchical decomposition of the\ntask allows for efficient exploration to learn policies that identify globally\noptimal solutions even as the number of collaborating agents increases. We show\npromising initial experimental results on a simulated distributed scheduling\nproblem. \n\n"}
{"id": "1712.08604", "contents": "Title: Automated Surgical Skill Assessment in RMIS Training Abstract: Purpose: Manual feedback in basic RMIS training can consume a significant\namount of time from expert surgeons' schedule and is prone to subjectivity.\nWhile VR-based training tasks can generate automated score reports, there is no\nmechanism of generating automated feedback for surgeons performing basic\nsurgical tasks in RMIS training. In this paper, we explore the usage of\ndifferent holistic features for automated skill assessment using only robot\nkinematic data and propose a weighted feature fusion technique for improving\nscore prediction performance.\n  Methods: We perform our experiments on the publicly available JIGSAWS dataset\nand evaluate four different types of holistic features from robot kinematic\ndata - Sequential Motion Texture (SMT), Discrete Fourier Transform (DFT),\nDiscrete Cosine Transform (DCT) and Approximate Entropy (ApEn). The features\nare then used for skill classification and exact skill score prediction. Along\nwith using these features individually, we also evaluate the performance using\nour proposed weighted combination technique.\n  Results: Our results demonstrate that these holistic features outperform all\nprevious HMM based state-of-the-art methods for skill classification on the\nJIGSAWS dataset. Also, our proposed feature fusion strategy significantly\nimproves performance for skill score predictions achieving up to 0.61 average\nspearman correlation coefficient.\n  Conclusions: Holistic features capturing global information from robot\nkinematic data can successfully be used for evaluating surgeon skill in basic\nsurgical tasks on the da Vinci robot. Using the framework presented can\npotentially allow for real time score feedback in RMIS training. \n\n"}
{"id": "1712.09300", "contents": "Title: Zero-Shot Learning via Latent Space Encoding Abstract: Zero-Shot Learning (ZSL) is typically achieved by resorting to a class\nsemantic embedding space to transfer the knowledge from the seen classes to\nunseen ones. Capturing the common semantic characteristics between the visual\nmodality and the class semantic modality (e.g., attributes or word vector) is a\nkey to the success of ZSL. In this paper, we propose a novel encoder-decoder\napproach, namely Latent Space Encoding (LSE), to connect the semantic relations\nof different modalities. Instead of requiring a projection function to transfer\ninformation across different modalities like most previous work, LSE per- forms\nthe interactions of different modalities via a feature aware latent space,\nwhich is learned in an implicit way. Specifically, different modalities are\nmodeled separately but optimized jointly. For each modality, an encoder-decoder\nframework is performed to learn a feature aware latent space via jointly\nmaximizing the recoverability of the original space from the latent space and\nthe predictability of the latent space from the original space. To relate\ndifferent modalities together, their features referring to the same concept are\nenforced to share the same latent codings. In this way, the common semantic\ncharacteristics of different modalities are generalized with the latent\nrepresentations. Another property of the proposed approach is that it is easily\nextended to more modalities. Extensive experimental results on four benchmark\ndatasets (AwA, CUB, aPY, and ImageNet) clearly demonstrate the superiority of\nthe proposed approach on several ZSL tasks, including traditional ZSL,\ngeneralized ZSL, and zero-shot retrieval (ZSR). \n\n"}
{"id": "1712.09665", "contents": "Title: Adversarial Patch Abstract: We present a method to create universal, robust, targeted adversarial image\npatches in the real world. The patches are universal because they can be used\nto attack any scene, robust because they work under a wide variety of\ntransformations, and targeted because they can cause a classifier to output any\ntarget class. These adversarial patches can be printed, added to any scene,\nphotographed, and presented to image classifiers; even when the patches are\nsmall, they cause the classifiers to ignore the other items in the scene and\nreport a chosen target class.\n  To reproduce the results from the paper, our code is available at\nhttps://github.com/tensorflow/cleverhans/tree/master/examples/adversarial_patch \n\n"}
{"id": "1801.00055", "contents": "Title: Deformable GANs for Pose-based Human Image Generation Abstract: In this paper we address the problem of generating person images conditioned\non a given pose. Specifically, given an image of a person and a target pose, we\nsynthesize a new image of that person in the novel pose. In order to deal with\npixel-to-pixel misalignments caused by the pose differences, we introduce\ndeformable skip connections in the generator of our Generative Adversarial\nNetwork. Moreover, a nearest-neighbour loss is proposed instead of the common\nL1 and L2 losses in order to match the details of the generated image with the\ntarget image. We test our approach using photos of persons in different poses\nand we compare our method with previous work in this area showing\nstate-of-the-art results in two benchmarks. Our method can be applied to the\nwider field of deformable object generation, provided that the pose of the\narticulated object can be extracted using a keypoint detector. \n\n"}
{"id": "1801.00085", "contents": "Title: Learning Structural Weight Uncertainty for Sequential Decision-Making Abstract: Learning probability distributions on the weights of neural networks (NNs)\nhas recently proven beneficial in many applications. Bayesian methods, such as\nStein variational gradient descent (SVGD), offer an elegant framework to reason\nabout NN model uncertainty. However, by assuming independent Gaussian priors\nfor the individual NN weights (as often applied), SVGD does not impose prior\nknowledge that there is often structural information (dependence) among\nweights. We propose efficient posterior learning of structural weight\nuncertainty, within an SVGD framework, by employing matrix variate Gaussian\npriors on NN parameters. We further investigate the learned structural\nuncertainty in sequential decision-making problems, including contextual\nbandits and reinforcement learning. Experiments on several synthetic and real\ndatasets indicate the superiority of our model, compared with state-of-the-art\nmethods. \n\n"}
{"id": "1801.00857", "contents": "Title: Optimal Bayesian Transfer Learning Abstract: Transfer learning has recently attracted significant research attention, as\nit simultaneously learns from different source domains, which have plenty of\nlabeled data, and transfers the relevant knowledge to the target domain with\nlimited labeled data to improve the prediction performance. We propose a\nBayesian transfer learning framework where the source and target domains are\nrelated through the joint prior density of the model parameters. The modeling\nof joint prior densities enables better understanding of the \"transferability\"\nbetween domains. We define a joint Wishart density for the precision matrices\nof the Gaussian feature-label distributions in the source and target domains to\nact like a bridge that transfers the useful information of the source domain to\nhelp classification in the target domain by improving the target posteriors.\nUsing several theorems in multivariate statistics, the posteriors and posterior\npredictive densities are derived in closed forms with hypergeometric functions\nof matrix argument, leading to our novel closed-form and fast Optimal Bayesian\nTransfer Learning (OBTL) classifier. Experimental results on both synthetic and\nreal-world benchmark data confirm the superb performance of the OBTL compared\nto the other state-of-the-art transfer learning and domain adaptation methods. \n\n"}
{"id": "1801.00868", "contents": "Title: Panoptic Segmentation Abstract: We propose and study a task we name panoptic segmentation (PS). Panoptic\nsegmentation unifies the typically distinct tasks of semantic segmentation\n(assign a class label to each pixel) and instance segmentation (detect and\nsegment each object instance). The proposed task requires generating a coherent\nscene segmentation that is rich and complete, an important step toward\nreal-world vision systems. While early work in computer vision addressed\nrelated image/scene parsing tasks, these are not currently popular, possibly\ndue to lack of appropriate metrics or associated recognition challenges. To\naddress this, we propose a novel panoptic quality (PQ) metric that captures\nperformance for all classes (stuff and things) in an interpretable and unified\nmanner. Using the proposed metric, we perform a rigorous study of both human\nand machine performance for PS on three existing datasets, revealing\ninteresting insights about the task. The aim of our work is to revive the\ninterest of the community in a more unified view of image segmentation. \n\n"}
{"id": "1801.02279", "contents": "Title: Identity-preserving Face Recovery from Portraits Abstract: Recovering the latent photorealistic faces from their artistic portraits aids\nhuman perception and facial analysis. However, a recovery process that can\npreserve identity is challenging because the fine details of real faces can be\ndistorted or lost in stylized images. In this paper, we present a new\nIdentity-preserving Face Recovery from Portraits (IFRP) to recover latent\nphotorealistic faces from unaligned stylized portraits. Our IFRP method\nconsists of two components: Style Removal Network (SRN) and Discriminative\nNetwork (DN). The SRN is designed to transfer feature maps of stylized images\nto the feature maps of the corresponding photorealistic faces. By embedding\nspatial transformer networks into the SRN, our method can compensate for\nmisalignments of stylized faces automatically and output aligned realistic face\nimages. The role of the DN is to enforce recovered faces to be similar to\nauthentic faces. To ensure the identity preservation, we promote the recovered\nand ground-truth faces to share similar visual features via a distance measure\nwhich compares features of recovered and ground-truth faces extracted from a\npre-trained VGG network. We evaluate our method on a large-scale synthesized\ndataset of real and stylized face pairs and attain state of the art results. In\naddition, our method can recover photorealistic faces from previously unseen\nstylized portraits, original paintings and human-drawn sketches. \n\n"}
{"id": "1801.02722", "contents": "Title: End-to-end detection-segmentation network with ROI convolution Abstract: We propose an end-to-end neural network that improves the segmentation\naccuracy of fully convolutional networks by incorporating a localization unit.\nThis network performs object localization first, which is then used as a cue to\nguide the training of the segmentation network. We test the proposed method on\na segmentation task of small objects on a clinical dataset of ultrasound\nimages. We show that by jointly learning for detection and segmentation, the\nproposed network is able to improve the segmentation accuracy compared to only\nlearning for segmentation. Code is publicly available at\nhttps://github.com/vincentzhang/roi-fcn. \n\n"}
{"id": "1801.03299", "contents": "Title: Simultaneous Tensor Completion and Denoising by Noise Inequality\n  Constrained Convex Optimization Abstract: Tensor completion is a technique of filling missing elements of the\nincomplete data tensors. It being actively studied based on the convex\noptimization scheme such as nuclear-norm minimization. When given data tensors\ninclude some noises, the nuclear-norm minimization problem is usually converted\nto the nuclear-norm `regularization' problem which simultaneously minimize\npenalty and error terms with some trade-off parameter. However, the good value\nof trade-off is not easily determined because of the difference of two units\nand the data dependence. In the sense of trade-off tuning, the noisy tensor\ncompletion problem with the `noise inequality constraint' is better choice than\nthe `regularization' because the good noise threshold can be easily bounded\nwith noise standard deviation. In this study, we tackle to solve the convex\ntensor completion problems with two types of noise inequality constraints:\nGaussian and Laplace distributions. The contributions of this study are\nfollows: (1) New tensor completion and denoising models using tensor total\nvariation and nuclear-norm are proposed which can be characterized as a\ngeneralization/extension of many past matrix and tensor completion models, (2)\nproximal mappings for noise inequalities are derived which are analytically\ncomputable with low computational complexity, (3) convex optimization algorithm\nis proposed based on primal-dual splitting framework, (4) new step-size\nadaptation method is proposed to accelerate the optimization, and (5) extensive\nexperiments demonstrated the advantages of the proposed method for visual data\nretrieval such as for color images, movies, and 3D-volumetric data. \n\n"}
{"id": "1801.04134", "contents": "Title: Deep Episodic Memory: Encoding, Recalling, and Predicting Episodic\n  Experiences for Robot Action Execution Abstract: We present a novel deep neural network architecture for representing robot\nexperiences in an episodic-like memory which facilitates encoding, recalling,\nand predicting action experiences. Our proposed unsupervised deep episodic\nmemory model 1) encodes observed actions in a latent vector space and, based on\nthis latent encoding, 2) infers most similar episodes previously experienced,\n3) reconstructs original episodes, and 4) predicts future frames in an\nend-to-end fashion. Results show that conceptually similar actions are mapped\ninto the same region of the latent vector space. Based on these results, we\nintroduce an action matching and retrieval mechanism, benchmark its performance\non two large-scale action datasets, 20BN-something-something and ActivityNet\nand evaluate its generalization capability in a real-world scenario on a\nhumanoid robot. \n\n"}
{"id": "1801.04180", "contents": "Title: Boosted Top Tagging Method Overview Abstract: We briefly review common tools and methods to identify boosted, hadronically\ndecaying top quarks at the LHC experiments. This includes generic jet\nsubstructure variables, specific top identification algorithms, and recent\ndevelopments in deep learning techniques. \n\n"}
{"id": "1801.05558", "contents": "Title: Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace Abstract: Gradient-based meta-learning methods leverage gradient descent to learn the\ncommonalities among various tasks. While previous such methods have been\nsuccessful in meta-learning tasks, they resort to simple gradient descent\nduring meta-testing. Our primary contribution is the {\\em MT-net}, which\nenables the meta-learner to learn on each layer's activation space a subspace\nthat the task-specific learner performs gradient descent on. Additionally, a\ntask-specific learner of an {\\em MT-net} performs gradient descent with respect\nto a meta-learned distance metric, which warps the activation space to be more\nsensitive to task identity. We demonstrate that the dimension of this learned\nsubspace reflects the complexity of the task-specific learner's adaptation\ntask, and also that our model is less sensitive to the choice of initial\nlearning rates than previous gradient-based meta-learning methods. Our method\nachieves state-of-the-art or comparable performance on few-shot classification\nand regression tasks. \n\n"}
{"id": "1801.06313", "contents": "Title: BinaryRelax: A Relaxation Approach For Training Deep Neural Networks\n  With Quantized Weights Abstract: We propose BinaryRelax, a simple two-phase algorithm, for training deep\nneural networks with quantized weights. The set constraint that characterizes\nthe quantization of weights is not imposed until the late stage of training,\nand a sequence of \\emph{pseudo} quantized weights is maintained. Specifically,\nwe relax the hard constraint into a continuous regularizer via Moreau envelope,\nwhich turns out to be the squared Euclidean distance to the set of quantized\nweights. The pseudo quantized weights are obtained by linearly interpolating\nbetween the float weights and their quantizations. A continuation strategy is\nadopted to push the weights towards the quantized state by gradually increasing\nthe regularization parameter. In the second phase, exact quantization scheme\nwith a small learning rate is invoked to guarantee fully quantized weights. We\ntest BinaryRelax on the benchmark CIFAR and ImageNet color image datasets to\ndemonstrate the superiority of the relaxed quantization approach and the\nimproved accuracy over the state-of-the-art training methods. Finally, we prove\nthe convergence of BinaryRelax under an approximate orthogonality condition. \n\n"}
{"id": "1801.06457", "contents": "Title: Quantitative analysis of patch-based fully convolutional neural networks\n  for tissue segmentation on brain magnetic resonance imaging Abstract: Accurate brain tissue segmentation in Magnetic Resonance Imaging (MRI) has\nattracted the attention of medical doctors and researchers since variations in\ntissue volume help in diagnosing and monitoring neurological diseases. Several\nproposals have been designed throughout the years comprising conventional\nmachine learning strategies as well as convolutional neural networks (CNN)\napproaches. In particular, in this paper, we analyse a sub-group of deep\nlearning methods producing dense predictions. This branch, referred in the\nliterature as Fully CNN (FCNN), is of interest as these architectures can\nprocess an input volume in less time than CNNs and local spatial dependencies\nmay be encoded since several voxels are classified at once. Our study focuses\non understanding architectural strengths and weaknesses of literature-like\napproaches. Hence, we implement eight FCNN architectures inspired by robust\nstate-of-the-art methods on brain segmentation related tasks. We evaluate them\nusing the IBSR18, MICCAI2012 and iSeg2017 datasets as they contain infant and\nadult data and exhibit varied voxel spacing, image quality, number of scans and\navailable imaging modalities. The discussion is driven in three directions:\ncomparison between 2D and 3D approaches, the importance of multiple modalities\nand overlapping as a sampling strategy for training and testing models. To\nencourage other researchers to explore the evaluation framework, a public\nversion is accessible to download from our research website. \n\n"}
{"id": "1801.07145", "contents": "Title: E-swish: Adjusting Activations to Different Network Depths Abstract: Activation functions have a notorious impact on neural networks on both\ntraining and testing the models against the desired problem. Currently, the\nmost used activation function is the Rectified Linear Unit (ReLU). This paper\nintroduces a new and novel activation function, closely related with the new\nactivation $Swish = x * sigmoid(x)$ (Ramachandran et al., 2017) which\ngeneralizes it. We call the new activation $E-swish = \\beta x * sigmoid(x)$. We\nshow that E-swish outperforms many other well-known activations including both\nReLU and Swish. For example, using E-swish provided 1.5% and 4.6% accuracy\nimprovements on Cifar10 and Cifar100 respectively for the WRN 10-2 when\ncompared to ReLU and 0.35% and 0.6% respectively when compared to Swish. The\ncode to reproduce all our experiments can be found at\nhttps://github.com/EricAlcaide/E-swish \n\n"}
{"id": "1801.07198", "contents": "Title: Three Dimensional Fluorescence Microscopy Image Synthesis and\n  Segmentation Abstract: Advances in fluorescence microscopy enable acquisition of 3D image volumes\nwith better image quality and deeper penetration into tissue. Segmentation is a\nrequired step to characterize and analyze biological structures in the images\nand recent 3D segmentation using deep learning has achieved promising results.\nOne issue is that deep learning techniques require a large set of groundtruth\ndata which is impractical to annotate manually for large 3D microscopy volumes.\nThis paper describes a 3D deep learning nuclei segmentation method using\nsynthetic 3D volumes for training. A set of synthetic volumes and the\ncorresponding groundtruth are generated using spatially constrained\ncycle-consistent adversarial networks. Segmentation results demonstrate that\nour proposed method is capable of segmenting nuclei successfully for various\ndata sets. \n\n"}
{"id": "1801.07637", "contents": "Title: DeepGestalt - Identifying Rare Genetic Syndromes Using Deep Learning Abstract: Facial analysis technologies have recently measured up to the capabilities of\nexpert clinicians in syndrome identification. To date, these technologies could\nonly identify phenotypes of a few diseases, limiting their role in clinical\nsettings where hundreds of diagnoses must be considered.\n  We developed a facial analysis framework, DeepGestalt, using computer vision\nand deep learning algorithms, that quantifies similarities to hundreds of\ngenetic syndromes based on unconstrained 2D images. DeepGestalt is currently\ntrained with over 26,000 patient cases from a rapidly growing\nphenotype-genotype database, consisting of tens of thousands of validated\nclinical cases, curated through a community-driven platform. DeepGestalt\ncurrently achieves 91% top-10-accuracy in identifying over 215 different\ngenetic syndromes and has outperformed clinical experts in three separate\nexperiments.\n  We suggest that this form of artificial intelligence is ready to support\nmedical genetics in clinical and laboratory practices and will play a key role\nin the future of precision medicine. \n\n"}
{"id": "1801.08361", "contents": "Title: Collaborative Large-Scale Dense 3D Reconstruction with Online\n  Inter-Agent Pose Optimisation Abstract: Reconstructing dense, volumetric models of real-world 3D scenes is important\nfor many tasks, but capturing large scenes can take significant time, and the\nrisk of transient changes to the scene goes up as the capture time increases.\nThese are good reasons to want instead to capture several smaller sub-scenes\nthat can be joined to make the whole scene. Achieving this has traditionally\nbeen difficult: joining sub-scenes that may never have been viewed from the\nsame angle requires a high-quality camera relocaliser that can cope with novel\nposes, and tracking drift in each sub-scene can prevent them from being joined\nto make a consistent overall scene. Recent advances, however, have\nsignificantly improved our ability to capture medium-sized sub-scenes with\nlittle to no tracking drift: real-time globally consistent reconstruction\nsystems can close loops and re-integrate the scene surface on the fly, whilst\nnew visual-inertial odometry approaches can significantly reduce tracking drift\nduring live reconstruction. Moreover, high-quality regression forest-based\nrelocalisers have recently been made more practical by the introduction of a\nmethod to allow them to be trained and used online. In this paper, we leverage\nthese advances to present what to our knowledge is the first system to allow\nmultiple users to collaborate interactively to reconstruct dense, voxel-based\nmodels of whole buildings using only consumer-grade hardware, a task that has\ntraditionally been both time-consuming and dependent on the availability of\nspecialised hardware. Using our system, an entire house or lab can be\nreconstructed in under half an hour and at a far lower cost than was previously\npossible. \n\n"}
{"id": "1801.08390", "contents": "Title: Global and Local Consistent Age Generative Adversarial Networks Abstract: Age progression/regression is a challenging task due to the complicated and\nnon-linear transformation in human aging process. Many researches have shown\nthat both global and local facial features are essential for face\nrepresentation, but previous GAN based methods mainly focused on the global\nfeature in age synthesis. To utilize both global and local facial information,\nwe propose a Global and Local Consistent Age Generative Adversarial Network\n(GLCA-GAN). In our generator, a global network learns the whole facial\nstructure and simulates the aging trend of the whole face, while three crucial\nfacial patches are progressed or regressed by three local networks aiming at\nimitating subtle changes of crucial facial subregions. To preserve most of the\ndetails in age-attribute-irrelevant areas, our generator learns the residual\nface. Moreover, we employ an identity preserving loss to better preserve the\nidentity information, as well as age preserving loss to enhance the accuracy of\nage synthesis. A pixel loss is also adopted to preserve detailed facial\ninformation of the input face. Our proposed method is evaluated on three face\naging datasets, i.e., CACD dataset, Morph dataset and FG-NET dataset.\nExperimental results show appealing performance of the proposed method by\ncomparing with the state-of-the-art. \n\n"}
{"id": "1801.08577", "contents": "Title: Effective Building Block Design for Deep Convolutional Neural Networks\n  using Search Abstract: Deep learning has shown promising results on many machine learning tasks but\nDL models are often complex networks with large number of neurons and layers,\nand recently, complex layer structures known as building blocks. Finding the\nbest deep model requires a combination of finding both the right architecture\nand the correct set of parameters appropriate for that architecture. In\naddition, this complexity (in terms of layer types, number of neurons, and\nnumber of layers) also present problems with generalization since larger\nnetworks are easier to overfit to the data. In this paper, we propose a search\nframework for finding effective architectural building blocks for convolutional\nneural networks (CNN). Our approach is much faster at finding models that are\nclose to state-of-the-art in performance. In addition, the models discovered by\nour approach are also smaller than models discovered by similar techniques. We\nachieve these twin advantages by designing our search space in such a way that\nit searches over a reduced set of state-of-the-art building blocks for CNNs\nincluding residual block, inception block, inception-residual block, ResNeXt\nblock and many others. We apply this technique to generate models for multiple\nimage datasets and show that these models achieve performance comparable to\nstate-of-the-art (and even surpassing the state-of-the-art in one case). We\nalso show that learned models are transferable between datasets. \n\n"}
{"id": "1801.08616", "contents": "Title: DeepPap: Deep Convolutional Networks for Cervical Cell Classification Abstract: Automation-assisted cervical screening via Pap smear or liquid-based cytology\n(LBC) is a highly effective cell imaging based cancer detection tool, where\ncells are partitioned into \"abnormal\" and \"normal\" categories. However, the\nsuccess of most traditional classification methods relies on the presence of\naccurate cell segmentations. Despite sixty years of research in this field,\naccurate segmentation remains a challenge in the presence of cell clusters and\npathologies. Moreover, previous classification methods are only built upon the\nextraction of hand-crafted features, such as morphology and texture. This paper\naddresses these limitations by proposing a method to directly classify cervical\ncells - without prior segmentation - based on deep features, using\nconvolutional neural networks (ConvNets). First, the ConvNet is pre-trained on\na natural image dataset. It is subsequently fine-tuned on a cervical cell\ndataset consisting of adaptively re-sampled image patches coarsely centered on\nthe nuclei. In the testing phase, aggregation is used to average the prediction\nscores of a similar set of image patches. The proposed method is evaluated on\nboth Pap smear and LBC datasets. Results show that our method outperforms\nprevious algorithms in classification accuracy (98.3%), area under the curve\n(AUC) (0.99) values, and especially specificity (98.3%), when applied to the\nHerlev benchmark Pap smear dataset and evaluated using five-fold\ncross-validation. Similar superior performances are also achieved on the HEMLBC\n(H&E stained manual LBC) dataset. Our method is promising for the development\nof automation-assisted reading systems in primary cervical screening. \n\n"}
{"id": "1801.09646", "contents": "Title: Improving Multiple Object Tracking with Optical Flow and Edge\n  Preprocessing Abstract: In this paper, we present a new method for detecting road users in an urban\nenvironment which leads to an improvement in multiple object tracking. Our\nmethod takes as an input a foreground image and improves the object detection\nand segmentation. This new image can be used as an input to trackers that use\nforeground blobs from background subtraction. The first step is to create\nforeground images for all the frames in an urban video. Then, starting from the\noriginal blobs of the foreground image, we merge the blobs that are close to\none another and that have similar optical flow. The next step is extracting the\nedges of the different objects to detect multiple objects that might be very\nclose (and be merged in the same blob) and to adjust the size of the original\nblobs. At the same time, we use the optical flow to detect occlusion of objects\nthat are moving in opposite directions. Finally, we make a decision on which\ninformation we keep in order to construct a new foreground image with blobs\nthat can be used for tracking. The system is validated on four videos of an\nurban traffic dataset. Our method improves the recall and precision metrics for\nthe object detection task compared to the vanilla background subtraction method\nand improves the CLEAR MOT metrics in the tracking tasks for most videos. \n\n"}
{"id": "1801.10111", "contents": "Title: Video-based Sign Language Recognition without Temporal Segmentation Abstract: Millions of hearing impaired people around the world routinely use some\nvariants of sign languages to communicate, thus the automatic translation of a\nsign language is meaningful and important. Currently, there are two\nsub-problems in Sign Language Recognition (SLR), i.e., isolated SLR that\nrecognizes word by word and continuous SLR that translates entire sentences.\nExisting continuous SLR methods typically utilize isolated SLRs as building\nblocks, with an extra layer of preprocessing (temporal segmentation) and\nanother layer of post-processing (sentence synthesis). Unfortunately, temporal\nsegmentation itself is non-trivial and inevitably propagates errors into\nsubsequent steps. Worse still, isolated SLR methods typically require strenuous\nlabeling of each word separately in a sentence, severely limiting the amount of\nattainable training data. To address these challenges, we propose a novel\ncontinuous sign recognition framework, the Hierarchical Attention Network with\nLatent Space (LS-HAN), which eliminates the preprocessing of temporal\nsegmentation. The proposed LS-HAN consists of three components: a two-stream\nConvolutional Neural Network (CNN) for video feature representation generation,\na Latent Space (LS) for semantic gap bridging, and a Hierarchical Attention\nNetwork (HAN) for latent space based recognition. Experiments are carried out\non two large scale datasets. Experimental results demonstrate the effectiveness\nof the proposed framework. \n\n"}
{"id": "1801.10441", "contents": "Title: Weighted Nonlocal Total Variation in Image Processing Abstract: In this paper, a novel weighted nonlocal total variation (WNTV) method is\nproposed. Compared to the classical nonlocal total variation methods, our\nmethod modifies the energy functional to introduce a weight to balance between\nthe labeled sets and unlabeled sets. With extensive numerical examples in\nsemi-supervised clustering, image inpainting and image colorization, we\ndemonstrate that WNTV provides an effective and efficient method in many image\nprocessing and machine learning problems. \n\n"}
{"id": "1801.10443", "contents": "Title: Counting Cells in Time-Lapse Microscopy using Deep Neural Networks Abstract: An automatic approach to counting any kind of cells could alleviate work of\nthe experts and boost the research in fields such as regenerative medicine. In\nthis paper, a method for microscopy cell counting using multiple frames (hence\ntemporal information) is proposed. Unlike previous approaches where the cell\ncounting is done independently in each frame (static cell counting), in this\nwork the cell counting prediction is done using multiple frames (dynamic cell\ncounting). A spatiotemporal model using ConvNets and long short term memory\n(LSTM) recurrent neural networks is proposed to overcome temporal variations.\nThe model outperforms static cell counting in a publicly available dataset of\nstem cells. The advantages, working conditions and limitations of the\nConvNet-LSTM method are discussed. Although our method is tested in cell\ncounting, it can be extrapolated to quantify in video (or correlated image\nseries) any kind of objects or volumes. \n\n"}
{"id": "1802.00420", "contents": "Title: Obfuscated Gradients Give a False Sense of Security: Circumventing\n  Defenses to Adversarial Examples Abstract: We identify obfuscated gradients, a kind of gradient masking, as a phenomenon\nthat leads to a false sense of security in defenses against adversarial\nexamples. While defenses that cause obfuscated gradients appear to defeat\niterative optimization-based attacks, we find defenses relying on this effect\ncan be circumvented. We describe characteristic behaviors of defenses\nexhibiting the effect, and for each of the three types of obfuscated gradients\nwe discover, we develop attack techniques to overcome it. In a case study,\nexamining non-certified white-box-secure defenses at ICLR 2018, we find\nobfuscated gradients are a common occurrence, with 7 of 9 defenses relying on\nobfuscated gradients. Our new attacks successfully circumvent 6 completely, and\n1 partially, in the original threat model each paper considers. \n\n"}
{"id": "1802.00977", "contents": "Title: Pose Flow: Efficient Online Pose Tracking Abstract: Multi-person articulated pose tracking in unconstrained videos is an\nimportant while challenging problem. In this paper, going along the road of\ntop-down approaches, we propose a decent and efficient pose tracker based on\npose flows. First, we design an online optimization framework to build the\nassociation of cross-frame poses and form pose flows (PF-Builder). Second, a\nnovel pose flow non-maximum suppression (PF-NMS) is designed to robustly reduce\nredundant pose flows and re-link temporal disjoint ones. Extensive experiments\nshow that our method significantly outperforms best-reported results on two\nstandard Pose Tracking datasets by 13 mAP 25 MOTA and 6 mAP 3 MOTA\nrespectively. Moreover, in the case of working on detected poses in individual\nframes, the extra computation of pose tracker is very minor, guaranteeing\nonline 10FPS tracking. Our source codes are made publicly\navailable(https://github.com/YuliangXiu/PoseFlow). \n\n"}
{"id": "1802.01268", "contents": "Title: ASMCNN: An Efficient Brain Extraction Using Active Shape Model and\n  Convolutional Neural Networks Abstract: Brain extraction (skull stripping) is a challenging problem in neuroimaging.\nIt is due to the variability in conditions from data acquisition or\nabnormalities in images, making brain morphology and intensity characteristics\nchangeable and complicated. In this paper, we propose an algorithm for skull\nstripping in Magnetic Resonance Imaging (MRI) scans, namely ASMCNN, by\ncombining the Active Shape Model (ASM) and Convolutional Neural Network (CNN)\nfor taking full of their advantages to achieve remarkable results. Instead of\nworking with 3D structures, we process 2D image sequences in the sagittal\nplane. First, we divide images into different groups such that, in each group,\nshapes and structures of brain boundaries have similar appearances. Second, a\nmodified version of ASM is used to detect brain boundaries by utilizing prior\nknowledge of each group. Finally, CNN and post-processing methods, including\nConditional Random Field (CRF), Gaussian processes, and several special rules\nare applied to refine the segmentation contours. Experimental results show that\nour proposed method outperforms current state-of-the-art algorithms by a\nsignificant margin in all experiments. \n\n"}
{"id": "1802.01279", "contents": "Title: Zero-Shot Kernel Learning Abstract: In this paper, we address an open problem of zero-shot learning. Its\nprinciple is based on learning a mapping that associates feature vectors\nextracted from i.e. images and attribute vectors that describe objects and/or\nscenes of interest. In turns, this allows classifying unseen object classes\nand/or scenes by matching feature vectors via mapping to a newly defined\nattribute vector describing a new class. Due to importance of such a learning\ntask, there exist many methods that learn semantic, probabilistic, linear or\npiece-wise linear mappings. In contrast, we apply well-established kernel\nmethods to learn a non-linear mapping between the feature and attribute spaces.\nWe propose an easy learning objective inspired by the Linear Discriminant\nAnalysis, Kernel-Target Alignment and Kernel Polarization methods that promotes\nincoherence. We evaluate performance of our algorithm on the Polynomial as well\nas shift-invariant Gaussian and Cauchy kernels. Despite simplicity of our\napproach, we obtain state-of-the-art results on several zero-shot learning\ndatasets and benchmarks including a recent AWA2 dataset. \n\n"}
{"id": "1802.01421", "contents": "Title: First-order Adversarial Vulnerability of Neural Networks and Input\n  Dimension Abstract: Over the past few years, neural networks were proven vulnerable to\nadversarial images: targeted but imperceptible image perturbations lead to\ndrastically different predictions. We show that adversarial vulnerability\nincreases with the gradients of the training objective when viewed as a\nfunction of the inputs. Surprisingly, vulnerability does not depend on network\ntopology: for many standard network architectures, we prove that at\ninitialization, the $\\ell_1$-norm of these gradients grows as the square root\nof the input dimension, leaving the networks increasingly vulnerable with\ngrowing image size. We empirically show that this dimension dependence persists\nafter either usual or robust training, but gets attenuated with higher\nregularization. \n\n"}
{"id": "1802.02202", "contents": "Title: Object Detection on Dynamic Occupancy Grid Maps Using Deep Learning and\n  Automatic Label Generation Abstract: We tackle the problem of object detection and pose estimation in a shared\nspace downtown environment. For perception multiple laser scanners with\n360{\\deg} coverage were fused in a dynamic occupancy grid map (DOGMa). A\nsingle-stage deep convolutional neural network is trained to provide object\nhypotheses comprising of shape, position, orientation and an existence score\nfrom a single input DOGMa. Furthermore, an algorithm for offline object\nextraction was developed to automatically label several hours of training data.\nThe algorithm is based on a two-pass trajectory extraction, forward and\nbackward in time. Typical for engineered algorithms, the automatic label\ngeneration suffers from misdetections, which makes hard negative mining\nimpractical. Therefore, we propose a loss function counteracting the high\nimbalance between mostly static background and extremely rare dynamic grid\ncells. Experiments indicate, that the trained network has good generalization\ncapabilities since it detects objects occasionally lost by the label algorithm.\nEvaluation reaches an average precision (AP) of 75.9% \n\n"}
{"id": "1802.02216", "contents": "Title: The Heart of an Image: Quantum Superposition and Entanglement in Visual\n  Perception Abstract: We analyse the way in which the principle that 'the whole is greater than the\nsum of its parts' manifests itself with phenomena of visual perception. For\nthis investigation we use insights and techniques coming from quantum\ncognition, and more specifically we are inspired by the correspondence of this\nprinciple with the phenomenon of the conjunction effect in human cognition. We\nidentify entities of meaning within artefacts of visual perception and rely on\nhow such entities are modelled for corpuses of texts such as the webpages of\nthe World-Wide Web for our study of how they appear in phenomena of visual\nperception. We identify concretely the conjunction effect in visual artefacts\nand analyse its structure in the example of a photograph. We also analyse\nquantum entanglement between different aspects of meaning in artefacts of\nvisual perception. We confirm its presence by showing that well elected\nexperiments on images retrieved accordingly by Google Images give rise to\nprobabilities and expectation values violating the Clauser Horne Shimony Holt\nversion of Bell's inequalities. We point out how this approach can lead to a\nmathematical description of the meaning content of a visual artefact such as a\nphotograph. \n\n"}
{"id": "1802.02568", "contents": "Title: VISER: Visual Self-Regularization Abstract: In this work, we propose the use of large set of unlabeled images as a source\nof regularization data for learning robust visual representation. Given a\nvisual model trained by a labeled dataset in a supervised fashion, we augment\nour training samples by incorporating large number of unlabeled data and train\na semi-supervised model. We demonstrate that our proposed learning approach\nleverages an abundance of unlabeled images and boosts the visual recognition\nperformance which alleviates the need to rely on large labeled datasets for\nlearning robust representation. To increment the number of image instances\nneeded to learn robust visual models in our approach, each labeled image\npropagates its label to its nearest unlabeled image instances. These retrieved\nunlabeled images serve as local perturbations of each labeled image to perform\nVisual Self-Regularization (VISER). To retrieve such visual self regularizers,\nwe compute the cosine similarity in a semantic space defined by the penultimate\nlayer in a fully convolutional neural network. We use the publicly available\nYahoo Flickr Creative Commons 100M dataset as the source of our unlabeled image\nset and propose a distributed approximate nearest neighbor algorithm to make\nretrieval practical at that scale. Using the labeled instances and their\nregularizer samples we show that we significantly improve object categorization\nand localization performance on the MS COCO and Visual Genome datasets where\nobjects appear in context. \n\n"}
{"id": "1802.02904", "contents": "Title: Deep Reinforcement Learning for Image Hashing Abstract: Deep hashing methods have received much attention recently, which achieve\npromising results by taking advantage of the strong representation power of\ndeep networks. However, most existing deep hashing methods learn a whole set of\nhashing functions independently, while ignore the correlations between\ndifferent hashing functions that can promote the retrieval accuracy greatly.\nInspired by the sequential decision ability of deep reinforcement learning, we\npropose a new Deep Reinforcement Learning approach for Image Hashing (DRLIH).\nOur proposed DRLIH approach models the hashing learning problem as a sequential\ndecision process, which learns each hashing function by correcting the errors\nimposed by previous ones and promotes retrieval accuracy. To the best of our\nknowledge, this is the first work to address hashing problem from deep\nreinforcement learning perspective. The main contributions of our proposed\nDRLIH approach can be summarized as follows: (1) We propose a deep\nreinforcement learning hashing network. In the proposed network, we utilize\nrecurrent neural network (RNN) as agents to model the hashing functions, which\ntake actions of projecting images into binary codes sequentially, so that the\ncurrent hashing function learning can take previous hashing functions' error\ninto account. (2) We propose a sequential learning strategy based on proposed\nDRLIH. We define the state as a tuple of internal features of RNN's hidden\nlayers and image features, which can reflect history decisions made by the\nagents. We also propose an action group method to enhance the correlation of\nhash functions in the same group. Experiments on three widely-used datasets\ndemonstrate the effectiveness of our proposed DRLIH approach. \n\n"}
{"id": "1802.03480", "contents": "Title: GraphVAE: Towards Generation of Small Graphs Using Variational\n  Autoencoders Abstract: Deep learning on graphs has become a popular research topic with many\napplications. However, past work has concentrated on learning graph embedding\ntasks, which is in contrast with advances in generative models for images and\ntext. Is it possible to transfer this progress to the domain of graphs? We\npropose to sidestep hurdles associated with linearization of such discrete\nstructures by having a decoder output a probabilistic fully-connected graph of\na predefined maximum size directly at once. Our method is formulated as a\nvariational autoencoder. We evaluate on the challenging task of molecule\ngeneration. \n\n"}
{"id": "1802.03685", "contents": "Title: Learning a SAT Solver from Single-Bit Supervision Abstract: We present NeuroSAT, a message passing neural network that learns to solve\nSAT problems after only being trained as a classifier to predict\nsatisfiability. Although it is not competitive with state-of-the-art SAT\nsolvers, NeuroSAT can solve problems that are substantially larger and more\ndifficult than it ever saw during training by simply running for more\niterations. Moreover, NeuroSAT generalizes to novel distributions; after\ntraining only on random SAT problems, at test time it can solve SAT problems\nencoding graph coloring, clique detection, dominating set, and vertex cover\nproblems, all on a range of distributions over small random graphs. \n\n"}
{"id": "1802.03835", "contents": "Title: Edge-Host Partitioning of Deep Neural Networks with Feature Space\n  Encoding for Resource-Constrained Internet-of-Things Platforms Abstract: This paper introduces partitioning an inference task of a deep neural network\nbetween an edge and a host platform in the IoT environment. We present a DNN as\nan encoding pipeline, and propose to transmit the output feature space of an\nintermediate layer to the host. The lossless or lossy encoding of the feature\nspace is proposed to enhance the maximum input rate supported by the edge\nplatform and/or reduce the energy of the edge platform. Simulation results show\nthat partitioning a DNN at the end of convolutional (feature extraction) layers\ncoupled with feature space encoding enables significant improvement in the\nenergy-efficiency and throughput over the baseline configurations that perform\nthe entire inference at the edge or at the host. \n\n"}
{"id": "1802.03931", "contents": "Title: Deep feature compression for collaborative object detection Abstract: Recent studies have shown that the efficiency of deep neural networks in\nmobile applications can be significantly improved by distributing the\ncomputational workload between the mobile device and the cloud. This paradigm,\ntermed collaborative intelligence, involves communicating feature data between\nthe mobile and the cloud. The efficiency of such approach can be further\nimproved by lossy compression of feature data, which has not been examined to\ndate. In this work we focus on collaborative object detection and study the\nimpact of both near-lossless and lossy compression of feature data on its\naccuracy. We also propose a strategy for improving the accuracy under lossy\nfeature compression. Experiments indicate that using this strategy, the\ncommunication overhead can be reduced by up to 70% without sacrificing\naccuracy. \n\n"}
{"id": "1802.04697", "contents": "Title: Learning to Search with MCTSnets Abstract: Planning problems are among the most important and well-studied problems in\nartificial intelligence. They are most typically solved by tree search\nalgorithms that simulate ahead into the future, evaluate future states, and\nback-up those evaluations to the root of a search tree. Among these algorithms,\nMonte-Carlo tree search (MCTS) is one of the most general, powerful and widely\nused. A typical implementation of MCTS uses cleverly designed rules, optimized\nto the particular characteristics of the domain. These rules control where the\nsimulation traverses, what to evaluate in the states that are reached, and how\nto back-up those evaluations. In this paper we instead learn where, what and\nhow to search. Our architecture, which we call an MCTSnet, incorporates\nsimulation-based search inside a neural network, by expanding, evaluating and\nbacking-up a vector embedding. The parameters of the network are trained\nend-to-end using gradient-based optimisation. When applied to small searches in\nthe well known planning problem Sokoban, the learned search algorithm\nsignificantly outperformed MCTS baselines. \n\n"}
{"id": "1802.05097", "contents": "Title: The Multiscale Bowler-Hat Transform for Vessel Enhancement in 3D\n  Biomedical Images Abstract: Enhancement and detection of 3D vessel-like structures has long been an open\nproblem as most existing image processing methods fail in many aspects,\nincluding a lack of uniform enhancement between vessels of different radii and\na lack of enhancement at the junctions.\n  Here, we propose a method based on mathematical morphology to enhance 3D\nvessel-like structures in biomedical images. The proposed method, 3D bowler-hat\ntransform, combines sphere and line structuring elements to enhance vessel-like\nstructures. The proposed method is validated on synthetic and real data and\ncompared with state-of-the-art methods.\n  Our results show that the proposed method achieves a high-quality vessel-like\nstructures enhancement in both synthetic and real biomedical images, and is\nable to cope with variations in vessels thickness throughout vascular networks\nwhile remaining robust at junctions. \n\n"}
{"id": "1802.05451", "contents": "Title: Mapping Images to Scene Graphs with Permutation-Invariant Structured\n  Prediction Abstract: Machine understanding of complex images is a key goal of artificial\nintelligence. One challenge underlying this task is that visual scenes contain\nmultiple inter-related objects, and that global context plays an important role\nin interpreting the scene. A natural modeling framework for capturing such\neffects is structured prediction, which optimizes over complex labels, while\nmodeling within-label interactions. However, it is unclear what principles\nshould guide the design of a structured prediction model that utilizes the\npower of deep learning components. Here we propose a design principle for such\narchitectures that follows from a natural requirement of permutation\ninvariance. We prove a necessary and sufficient characterization for\narchitectures that follow this invariance, and discuss its implication on model\ndesign. Finally, we show that the resulting model achieves new state of the art\nresults on the Visual Genome scene graph labeling benchmark, outperforming all\nrecent approaches. \n\n"}
{"id": "1802.05666", "contents": "Title: Adversarial Risk and the Dangers of Evaluating Against Weak Attacks Abstract: This paper investigates recently proposed approaches for defending against\nadversarial examples and evaluating adversarial robustness. We motivate\n'adversarial risk' as an objective for achieving models robust to worst-case\ninputs. We then frame commonly used attacks and evaluation metrics as defining\na tractable surrogate objective to the true adversarial risk. This suggests\nthat models may optimize this surrogate rather than the true adversarial risk.\nWe formalize this notion as 'obscurity to an adversary,' and develop tools and\nheuristics for identifying obscured models and designing transparent models. We\ndemonstrate that this is a significant problem in practice by repurposing\ngradient-free optimization techniques into adversarial attacks, which we use to\ndecrease the accuracy of several recently proposed defenses to near zero. Our\nhope is that our formulations and results will help researchers to develop more\npowerful defenses. \n\n"}
{"id": "1802.06869", "contents": "Title: Invertible Autoencoder for domain adaptation Abstract: The unsupervised image-to-image translation aims at finding a mapping between\nthe source ($A$) and target ($B$) image domains, where in many applications\naligned image pairs are not available at training. This is an ill-posed\nlearning problem since it requires inferring the joint probability distribution\nfrom marginals. Joint learning of coupled mappings $F_{AB}: A \\rightarrow B$\nand $F_{BA}: B \\rightarrow A$ is commonly used by the state-of-the-art methods,\nlike CycleGAN [Zhu et al., 2017], to learn this translation by introducing\ncycle consistency requirement to the learning problem, i.e. $F_{AB}(F_{BA}(B))\n\\approx B$ and $F_{BA}(F_{AB}(A)) \\approx A$. Cycle consistency enforces the\npreservation of the mutual information between input and translated images.\nHowever, it does not explicitly enforce $F_{BA}$ to be an inverse operation to\n$F_{AB}$. We propose a new deep architecture that we call invertible\nautoencoder (InvAuto) to explicitly enforce this relation. This is done by\nforcing an encoder to be an inverted version of the decoder, where\ncorresponding layers perform opposite mappings and share parameters. The\nmappings are constrained to be orthonormal. The resulting architecture leads to\nthe reduction of the number of trainable parameters (up to $2$ times). We\npresent image translation results on benchmark data sets and demonstrate\nstate-of-the art performance of our approach. Finally, we test the proposed\ndomain adaptation method on the task of road video conversion. We demonstrate\nthat the videos converted with InvAuto have high quality and show that the\nNVIDIA neural-network-based end-to-end learning system for autonomous driving,\nknown as PilotNet, trained on real road videos performs well when tested on the\nconverted ones. \n\n"}
{"id": "1802.06898", "contents": "Title: EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based\n  Cameras Abstract: Event-based cameras have shown great promise in a variety of situations where\nframe based cameras suffer, such as high speed motions and high dynamic range\nscenes. However, developing algorithms for event measurements requires a new\nclass of hand crafted algorithms. Deep learning has shown great success in\nproviding model free solutions to many problems in the vision community, but\nexisting networks have been developed with frame based images in mind, and\nthere does not exist the wealth of labeled data for events as there does for\nimages for supervised training. To these points, we present EV-FlowNet, a novel\nself-supervised deep learning pipeline for optical flow estimation for event\nbased cameras. In particular, we introduce an image based representation of a\ngiven event stream, which is fed into a self-supervised neural network as the\nsole input. The corresponding grayscale images captured from the same camera at\nthe same time as the events are then used as a supervisory signal to provide a\nloss function at training time, given the estimated flow from the network. We\nshow that the resulting network is able to accurately predict optical flow from\nevents only in a variety of different scenes, with performance competitive to\nimage based networks. This method not only allows for accurate estimation of\ndense optical flow, but also provides a framework for the transfer of other\nself-supervised methods to the event-based domain. \n\n"}
{"id": "1802.06927", "contents": "Title: On Lyapunov exponents and adversarial perturbation Abstract: In this paper, we would like to disseminate a serendipitous discovery\ninvolving Lyapunov exponents of a 1-D time series and their use in serving as a\nfiltering defense tool against a specific kind of deep adversarial\nperturbation. To this end, we use the state-of-the-art CleverHans library to\ngenerate adversarial perturbations against a standard Convolutional Neural\nNetwork (CNN) architecture trained on the MNIST as well as the Fashion-MNIST\ndatasets. We empirically demonstrate how the Lyapunov exponents computed on the\nflattened 1-D vector representations of the images served as highly\ndiscriminative features that could be to pre-classify images as adversarial or\nlegitimate before feeding the image into the CNN for classification. We also\nexplore the issue of possible false-alarms when the input images are noisy in a\nnon-adversarial sense. \n\n"}
{"id": "1802.07442", "contents": "Title: Learning to Play with Intrinsically-Motivated Self-Aware Agents Abstract: Infants are experts at playing, with an amazing ability to generate novel\nstructured behaviors in unstructured environments that lack clear extrinsic\nreward signals. We seek to mathematically formalize these abilities using a\nneural network that implements curiosity-driven intrinsic motivation. Using a\nsimple but ecologically naturalistic simulated environment in which an agent\ncan move and interact with objects it sees, we propose a \"world-model\" network\nthat learns to predict the dynamic consequences of the agent's actions.\nSimultaneously, we train a separate explicit \"self-model\" that allows the agent\nto track the error map of its own world-model, and then uses the self-model to\nadversarially challenge the developing world-model. We demonstrate that this\npolicy causes the agent to explore novel and informative interactions with its\nenvironment, leading to the generation of a spectrum of complex behaviors,\nincluding ego-motion prediction, object attention, and object gathering.\nMoreover, the world-model that the agent learns supports improved performance\non object dynamics prediction, detection, localization and recognition tasks.\nTaken together, our results are initial steps toward creating flexible\nautonomous agents that self-supervise in complex novel physical environments. \n\n"}
{"id": "1802.07918", "contents": "Title: Video Person Re-identification by Temporal Residual Learning Abstract: In this paper, we propose a novel feature learning framework for video person\nre-identification (re-ID). The proposed framework largely aims to exploit the\nadequate temporal information of video sequences and tackle the poor spatial\nalignment of moving pedestrians. More specifically, for exploiting the temporal\ninformation, we design a temporal residual learning (TRL) module to\nsimultaneously extract the generic and specific features of consecutive frames.\nThe TRL module is equipped with two bi-directional LSTM (BiLSTM), which are\nrespectively responsible to describe a moving person in different aspects,\nproviding complementary information for better feature representations. To deal\nwith the poor spatial alignment in video re-ID datasets, we propose a\nspatial-temporal transformer network (ST^2N) module. Transformation parameters\nin the ST^2N module are learned by leveraging the high-level semantic\ninformation of the current frame as well as the temporal context knowledge from\nother frames. The proposed ST^2N module with less learnable parameters allows\neffective person alignments under significant appearance changes. Extensive\nexperimental results on the large-scale MARS, PRID2011, ILIDS-VID and SDU-VID\ndatasets demonstrate that the proposed method achieves consistently superior\nperformance and outperforms most of the very recent state-of-the-art methods. \n\n"}
{"id": "1802.08216", "contents": "Title: ChatPainter: Improving Text to Image Generation using Dialogue Abstract: Synthesizing realistic images from text descriptions on a dataset like\nMicrosoft Common Objects in Context (MS COCO), where each image can contain\nseveral objects, is a challenging task. Prior work has used text captions to\ngenerate images. However, captions might not be informative enough to capture\nthe entire image and insufficient for the model to be able to understand which\nobjects in the images correspond to which words in the captions. We show that\nadding a dialogue that further describes the scene leads to significant\nimprovement in the inception score and in the quality of generated images on\nthe MS COCO dataset. \n\n"}
{"id": "1802.08735", "contents": "Title: A DIRT-T Approach to Unsupervised Domain Adaptation Abstract: Domain adaptation refers to the problem of leveraging labeled data in a\nsource domain to learn an accurate model in a target domain where labels are\nscarce or unavailable. A recent approach for finding a common representation of\nthe two domains is via domain adversarial training (Ganin & Lempitsky, 2015),\nwhich attempts to induce a feature extractor that matches the source and target\nfeature distributions in some feature space. However, domain adversarial\ntraining faces two critical limitations: 1) if the feature extraction function\nhas high-capacity, then feature distribution matching is a weak constraint, 2)\nin non-conservative domain adaptation (where no single classifier can perform\nwell in both the source and target domains), training the model to do well on\nthe source domain hurts performance on the target domain. In this paper, we\naddress these issues through the lens of the cluster assumption, i.e., decision\nboundaries should not cross high-density data regions. We propose two novel and\nrelated models: 1) the Virtual Adversarial Domain Adaptation (VADA) model,\nwhich combines domain adversarial training with a penalty term that punishes\nthe violation the cluster assumption; 2) the Decision-boundary Iterative\nRefinement Training with a Teacher (DIRT-T) model, which takes the VADA model\nas initialization and employs natural gradient steps to further minimize the\ncluster assumption violation. Extensive empirical results demonstrate that the\ncombination of these two models significantly improve the state-of-the-art\nperformance on the digit, traffic sign, and Wi-Fi recognition domain adaptation\nbenchmarks. \n\n"}
{"id": "1802.08831", "contents": "Title: Convolutional Neural Networks combined with Runge-Kutta Methods Abstract: A convolutional neural network can be constructed using numerical methods for\nsolving dynamical systems, since the forward pass of the network can be\nregarded as a trajectory of a dynamical system. However, existing models based\non numerical solvers cannot avoid the iterations of implicit methods, which\nmakes the models inefficient at inference time. In this paper, we reinterpret\nthe pre-activation Residual Networks (ResNets) and their variants from the\ndynamical systems view. We consider that the iterations of implicit Runge-Kutta\nmethods are fused into the training of these models. Moreover, we propose a\nnovel approach to constructing network models based on high-order Runge-Kutta\nmethods in order to achieve higher efficiency. Our proposed models are referred\nto as the Runge-Kutta Convolutional Neural Networks (RKCNNs). The RKCNNs are\nevaluated on multiple benchmark datasets. The experimental results show that\nRKCNNs are vastly superior to other dynamical system network models: they\nachieve higher accuracy with much fewer resources. They also expand the family\nof network models based on numerical methods for dynamical systems. \n\n"}
{"id": "1802.08946", "contents": "Title: Teacher Improves Learning by Selecting a Training Subset Abstract: We call a learner super-teachable if a teacher can trim down an iid training\nset while making the learner learn even better. We provide sharp super-teaching\nguarantees on two learners: the maximum likelihood estimator for the mean of a\nGaussian, and the large margin classifier in 1D. For general learners, we\nprovide a mixed-integer nonlinear programming-based algorithm to find a super\nteaching set. Empirical experiments show that our algorithm is able to find\ngood super-teaching sets for both regression and classification problems. \n\n"}
{"id": "1802.09653", "contents": "Title: On the Suitability of $L_p$-norms for Creating and Preventing\n  Adversarial Examples Abstract: Much research effort has been devoted to better understanding adversarial\nexamples, which are specially crafted inputs to machine-learning models that\nare perceptually similar to benign inputs, but are classified differently\n(i.e., misclassified). Both algorithms that create adversarial examples and\nstrategies for defending against them typically use $L_p$-norms to measure the\nperceptual similarity between an adversarial input and its benign original.\nPrior work has already shown, however, that two images need not be close to\neach other as measured by an $L_p$-norm to be perceptually similar. In this\nwork, we show that nearness according to an $L_p$-norm is not just unnecessary\nfor perceptual similarity, but is also insufficient. Specifically, focusing on\ndatasets (CIFAR10 and MNIST), $L_p$-norms, and thresholds used in prior work,\nwe show through online user studies that \"adversarial examples\" that are closer\nto their benign counterparts than required by commonly used $L_p$-norm\nthresholds can nevertheless be perceptually different to humans from the\ncorresponding benign examples. Namely, the perceptual distance between two\nimages that are \"near\" each other according to an $L_p$-norm can be high enough\nthat participants frequently classify the two images as representing different\nobjects or digits. Combined with prior work, we thus demonstrate that nearness\nof inputs as measured by $L_p$-norms is neither necessary nor sufficient for\nperceptual similarity, which has implications for both creating and defending\nagainst adversarial examples. We propose and discuss alternative similarity\nmetrics to stimulate future research in the area. \n\n"}
{"id": "1802.09756", "contents": "Title: Real-Time Bidding with Multi-Agent Reinforcement Learning in Display\n  Advertising Abstract: Real-time advertising allows advertisers to bid for each impression for a\nvisiting user. To optimize specific goals such as maximizing revenue and return\non investment (ROI) led by ad placements, advertisers not only need to estimate\nthe relevance between the ads and user's interests, but most importantly\nrequire a strategic response with respect to other advertisers bidding in the\nmarket. In this paper, we formulate bidding optimization with multi-agent\nreinforcement learning. To deal with a large number of advertisers, we propose\na clustering method and assign each cluster with a strategic bidding agent. A\npractical Distributed Coordinated Multi-Agent Bidding (DCMAB) has been proposed\nand implemented to balance the tradeoff between the competition and cooperation\namong advertisers. The empirical study on our industry-scaled real-world data\nhas demonstrated the effectiveness of our methods. Our results show\ncluster-based bidding would largely outperform single-agent and bandit\napproaches, and the coordinated bidding achieves better overall objectives than\npurely self-interested bidding agents. \n\n"}
{"id": "1802.09850", "contents": "Title: Solving Inverse Computational Imaging Problems using Deep Pixel-level\n  Prior Abstract: Signal reconstruction is a challenging aspect of computational imaging as it\noften involves solving ill-posed inverse problems. Recently, deep feed-forward\nneural networks have led to state-of-the-art results in solving various inverse\nimaging problems. However, being task specific, these networks have to be\nlearned for each inverse problem. On the other hand, a more flexible approach\nwould be to learn a deep generative model once and then use it as a signal\nprior for solving various inverse problems. We show that among the various\nstate of the art deep generative models, autoregressive models are especially\nsuitable for our purpose for the following reasons. First, they explicitly\nmodel the pixel level dependencies and hence are capable of reconstructing\nlow-level details such as texture patterns and edges better. Second, they\nprovide an explicit expression for the image prior which can then be used for\nMAP based inference along with the forward model. Third, they can model long\nrange dependencies in images which make them ideal for handling global\nmultiplexing as encountered in various compressive imaging systems. We\ndemonstrate the efficacy of our proposed approach in solving three\ncomputational imaging problems: Single Pixel Camera (SPC), LiSens and FlatCam.\nFor both real and simulated cases, we obtain better reconstructions than the\nstate-of-the-art methods in terms of perceptual and quantitative metrics. \n\n"}
{"id": "1802.10062", "contents": "Title: CSRNet: Dilated Convolutional Neural Networks for Understanding the\n  Highly Congested Scenes Abstract: We propose a network for Congested Scene Recognition called CSRNet to provide\na data-driven and deep learning method that can understand highly congested\nscenes and perform accurate count estimation as well as present high-quality\ndensity maps. The proposed CSRNet is composed of two major components: a\nconvolutional neural network (CNN) as the front-end for 2D feature extraction\nand a dilated CNN for the back-end, which uses dilated kernels to deliver\nlarger reception fields and to replace pooling operations. CSRNet is an\neasy-trained model because of its pure convolutional structure. We demonstrate\nCSRNet on four datasets (ShanghaiTech dataset, the UCF_CC_50 dataset, the\nWorldEXPO'10 dataset, and the UCSD dataset) and we deliver the state-of-the-art\nperformance. In the ShanghaiTech Part_B dataset, CSRNet achieves 47.3% lower\nMean Absolute Error (MAE) than the previous state-of-the-art method. We extend\nthe targeted applications for counting other objects, such as the vehicle in\nTRANCOS dataset. Results show that CSRNet significantly improves the output\nquality with 15.4% lower MAE than the previous state-of-the-art approach. \n\n"}
{"id": "1802.10204", "contents": "Title: Improved Explainability of Capsule Networks: Relevance Path by Agreement Abstract: Recent advancements in signal processing and machine learning domains have\nresulted in an extensive surge of interest in deep learning models due to their\nunprecedented performance and high accuracy for different and challenging\nproblems of significant engineering importance. However, when such deep\nlearning architectures are utilized for making critical decisions such as the\nones that involve human lives (e.g., in medical applications), it is of\nparamount importance to understand, trust, and in one word \"explain\" the\nrational behind deep models' decisions. Currently, deep learning models are\ntypically considered as black-box systems, which do not provide any clue on\ntheir internal processing actions. Although some recent efforts have been\ninitiated to explain behavior and decisions of deep networks, explainable\nartificial intelligence (XAI) domain is still in its infancy. In this regard,\nwe consider capsule networks (referred to as CapsNets), which are novel deep\nstructures; recently proposed as an alternative counterpart to convolutional\nneural networks (CNNs), and posed to change the future of machine intelligence.\nIn this paper, we investigate and analyze structures and behaviors of the\nCapsNets and illustrate potential explainability properties of such networks.\nFurthermore, we show possibility of transforming deep learning architectures in\nto transparent networks via incorporation of capsules in different layers\ninstead of convolution layers of the CNNs. \n\n"}
{"id": "1802.10548", "contents": "Title: Using Deep Learning for Segmentation and Counting within Microscopy Data Abstract: Cell counting is a ubiquitous, yet tedious task that would greatly benefit\nfrom automation. From basic biological questions to clinical trials, cell\ncounts provide key quantitative feedback that drive research. Unfortunately,\ncell counting is most commonly a manual task and can be time-intensive. The\ntask is made even more difficult due to overlapping cells, existence of\nmultiple focal planes, and poor imaging quality, among other factors. Here, we\ndescribe a convolutional neural network approach, using a recently described\nfeature pyramid network combined with a VGG-style neural network, for\nsegmenting and subsequent counting of cells in a given microscopy image. \n\n"}
{"id": "1803.00385", "contents": "Title: MAGAN: Aligning Biological Manifolds Abstract: It is increasingly common in many types of natural and physical systems\n(especially biological systems) to have different types of measurements\nperformed on the same underlying system. In such settings, it is important to\nalign the manifolds arising from each measurement in order to integrate such\ndata and gain an improved picture of the system. We tackle this problem using\ngenerative adversarial networks (GANs). Recently, GANs have been utilized to\ntry to find correspondences between sets of samples. However, these GANs are\nnot explicitly designed for proper alignment of manifolds. We present a new GAN\ncalled the Manifold-Aligning GAN (MAGAN) that aligns two manifolds such that\nrelated points in each measurement space are aligned together. We demonstrate\napplications of MAGAN in single-cell biology in integrating two different\nmeasurement types together. In our demonstrated examples, cells from the same\ntissue are measured with both genomic (single-cell RNA-sequencing) and\nproteomic (mass cytometry) technologies. We show that the MAGAN successfully\naligns them such that known correlations between measured markers are improved\ncompared to other recently proposed models. \n\n"}
{"id": "1803.00386", "contents": "Title: Context-Aware Learning using Transferable Features for Classification of\n  Breast Cancer Histology Images Abstract: Convolutional neural networks (CNNs) have been recently used for a variety of\nhistology image analysis. However, availability of a large dataset is a major\nprerequisite for training a CNN which limits its use by the computational\npathology community. In previous studies, CNNs have demonstrated their\npotential in terms of feature generalizability and transferability accompanied\nwith better performance. Considering these traits of CNN, we propose a simple\nyet effective method which leverages the strengths of CNN combined with the\nadvantages of including contextual information, particularly designed for a\nsmall dataset. Our method consists of two main steps: first it uses the\nactivation features of CNN trained for a patch-based classification and then it\ntrains a separate classifier using features of overlapping patches to perform\nimage-based classification using the contextual information. The proposed\nframework outperformed the state-of-the-art method for breast cancer\nclassification. \n\n"}
{"id": "1803.00387", "contents": "Title: A General Pipeline for 3D Detection of Vehicles Abstract: Autonomous driving requires 3D perception of vehicles and other objects in\nthe in environment. Much of the current methods support 2D vehicle detection.\nThis paper proposes a flexible pipeline to adopt any 2D detection network and\nfuse it with a 3D point cloud to generate 3D information with minimum changes\nof the 2D detection networks. To identify the 3D box, an effective model\nfitting algorithm is developed based on generalised car models and score maps.\nA two-stage convolutional neural network (CNN) is proposed to refine the\ndetected 3D box. This pipeline is tested on the KITTI dataset using two\ndifferent 2D detection networks. The 3D detection results based on these two\nnetworks are similar, demonstrating the flexibility of the proposed pipeline.\nThe results rank second among the 3D detection algorithms, indicating its\ncompetencies in 3D detection. \n\n"}
{"id": "1803.00404", "contents": "Title: Deep Defense: Training DNNs with Improved Adversarial Robustness Abstract: Despite the efficacy on a variety of computer vision tasks, deep neural\nnetworks (DNNs) are vulnerable to adversarial attacks, limiting their\napplications in security-critical systems. Recent works have shown the\npossibility of generating imperceptibly perturbed image inputs (a.k.a.,\nadversarial examples) to fool well-trained DNN classifiers into making\narbitrary predictions. To address this problem, we propose a training recipe\nnamed \"deep defense\". Our core idea is to integrate an adversarial\nperturbation-based regularizer into the classification objective, such that the\nobtained models learn to resist potential attacks, directly and precisely. The\nwhole optimization problem is solved just like training a recursive network.\nExperimental results demonstrate that our method outperforms training with\nadversarial/Parseval regularizations by large margins on various datasets\n(including MNIST, CIFAR-10 and ImageNet) and different DNN architectures. Code\nand models for reproducing our results are available at\nhttps://github.com/ZiangYan/deepdefense.pytorch \n\n"}
{"id": "1803.00557", "contents": "Title: The 2018 DAVIS Challenge on Video Object Segmentation Abstract: We present the 2018 DAVIS Challenge on Video Object Segmentation, a public\ncompetition specifically designed for the task of video object segmentation. It\nbuilds upon the DAVIS 2017 dataset, which was presented in the previous edition\nof the DAVIS Challenge, and added 100 videos with multiple objects per sequence\nto the original DAVIS 2016 dataset. Motivated by the analysis of the results of\nthe 2017 edition, the main track of the competition will be the same than in\nthe previous edition (segmentation given the full mask of the objects in the\nfirst frame -- semi-supervised scenario). This edition, however, also adds an\ninteractive segmentation teaser track, where the participants will interact\nwith a web service simulating the input of a human that provides scribbles to\niteratively improve the result. \n\n"}
{"id": "1803.00638", "contents": "Title: Fast and accurate computation of orthogonal moments for texture analysis Abstract: In this work we describe a fast and stable algorithm for the computation of\nthe orthogonal moments of an image. Indeed, orthogonal moments are\ncharacterized by a high discriminative power, but some of their possible\nformulations are characterized by a large computational complexity, which\nlimits their real-time application. This paper describes in detail an approach\nbased on recurrence relations, and proposes an optimized Matlab implementation\nof the corresponding computational procedure, aiming to solve the above\nlimitations and put at the community's disposal an efficient and easy to use\nsoftware. In our experiments we evaluate the effectiveness of the recurrence\nformulation, as well as its performance for the reconstruction task, in\ncomparison to the closed form representation, often used in the literature. The\nresults show a sensible reduction in the computational complexity, together\nwith a greater accuracy in reconstruction. In order to assess and compare the\naccuracy of the computed moments in texture analysis, we perform classification\nexperiments on six well-known databases of texture images. Again, the\nrecurrence formulation performs better in classification than the closed form\nrepresentation. More importantly, if computed from the GLCM of the image using\nthe proposed stable procedure, the orthogonal moments outperform in some\nsituations some of the most diffused state-of-the-art descriptors for texture\nclassification. \n\n"}
{"id": "1803.00967", "contents": "Title: Active model learning and diverse action sampling for task and motion\n  planning Abstract: The objective of this work is to augment the basic abilities of a robot by\nlearning to use new sensorimotor primitives to enable the solution of complex\nlong-horizon problems. Solving long-horizon problems in complex domains\nrequires flexible generative planning that can combine primitive abilities in\nnovel combinations to solve problems as they arise in the world. In order to\nplan to combine primitive actions, we must have models of the preconditions and\neffects of those actions: under what circumstances will executing this\nprimitive achieve some particular effect in the world?\n  We use, and develop novel improvements on, state-of-the-art methods for\nactive learning and sampling. We use Gaussian process methods for learning the\nconditions of operator effectiveness from small numbers of expensive training\nexamples collected by experimentation on a robot. We develop adaptive sampling\nmethods for generating diverse elements of continuous sets (such as robot\nconfigurations and object poses) during planning for solving a new task, so\nthat planning is as efficient as possible. We demonstrate these methods in an\nintegrated system, combining newly learned models with an efficient\ncontinuous-space robot task and motion planner to learn to solve long horizon\nproblems more efficiently than was previously possible. \n\n"}
{"id": "1803.01534", "contents": "Title: Path Aggregation Network for Instance Segmentation Abstract: The way that information propagates in neural networks is of great\nimportance. In this paper, we propose Path Aggregation Network (PANet) aiming\nat boosting information flow in proposal-based instance segmentation framework.\nSpecifically, we enhance the entire feature hierarchy with accurate\nlocalization signals in lower layers by bottom-up path augmentation, which\nshortens the information path between lower layers and topmost feature. We\npresent adaptive feature pooling, which links feature grid and all feature\nlevels to make useful information in each feature level propagate directly to\nfollowing proposal subnetworks. A complementary branch capturing different\nviews for each proposal is created to further improve mask prediction. These\nimprovements are simple to implement, with subtle extra computational overhead.\nOur PANet reaches the 1st place in the COCO 2017 Challenge Instance\nSegmentation task and the 2nd place in Object Detection task without\nlarge-batch training. It is also state-of-the-art on MVD and Cityscapes. Code\nis available at https://github.com/ShuLiu1993/PANet \n\n"}
{"id": "1803.02007", "contents": "Title: Occupancy Map Prediction Using Generative and Fully Convolutional\n  Networks for Vehicle Navigation Abstract: Fast, collision-free motion through unknown environments remains a\nchallenging problem for robotic systems. In these situations, the robot's\nability to reason about its future motion is often severely limited by sensor\nfield of view (FOV). By contrast, biological systems routinely make decisions\nby taking into consideration what might exist beyond their FOV based on prior\nexperience. In this paper, we present an approach for predicting occupancy map\nrepresentations of sensor data for future robot motions using deep neural\nnetworks. We evaluate several deep network architectures, including purely\ngenerative and adversarial models. Testing on both simulated and real\nenvironments we demonstrated performance both qualitatively and quantitatively,\nwith SSIM similarity measure up to 0.899. We showed that it is possible to make\npredictions about occupied space beyond the physical robot's FOV from simulated\ntraining data. In the future, this method will allow robots to navigate through\nunknown environments in a faster, safer manner. \n\n"}
{"id": "1803.02536", "contents": "Title: Sparse Adversarial Perturbations for Videos Abstract: Although adversarial samples of deep neural networks (DNNs) have been\nintensively studied on static images, their extensions in videos are never\nexplored. Compared with images, attacking a video needs to consider not only\nspatial cues but also temporal cues. Moreover, to improve the imperceptibility\nas well as reduce the computation cost, perturbations should be added on as\nfewer frames as possible, i.e., adversarial perturbations are temporally\nsparse. This further motivates the propagation of perturbations, which denotes\nthat perturbations added on the current frame can transfer to the next frames\nvia their temporal interactions. Thus, no (or few) extra perturbations are\nneeded for these frames to misclassify them. To this end, we propose an\nl2,1-norm based optimization algorithm to compute the sparse adversarial\nperturbations for videos. We choose the action recognition as the targeted\ntask, and networks with a CNN+RNN architecture as threat models to verify our\nmethod. Thanks to the propagation, we can compute perturbations on a shortened\nversion video, and then adapt them to the long version video to fool DNNs.\nExperimental results on the UCF101 dataset demonstrate that even only one frame\nin a video is perturbed, the fooling rate can still reach 59.7%. \n\n"}
{"id": "1803.02865", "contents": "Title: WNGrad: Learn the Learning Rate in Gradient Descent Abstract: Adjusting the learning rate schedule in stochastic gradient methods is an\nimportant unresolved problem which requires tuning in practice. If certain\nparameters of the loss function such as smoothness or strong convexity\nconstants are known, theoretical learning rate schedules can be applied.\nHowever, in practice, such parameters are not known, and the loss function of\ninterest is not convex in any case. The recently proposed batch normalization\nreparametrization is widely adopted in most neural network architectures today\nbecause, among other advantages, it is robust to the choice of Lipschitz\nconstant of the gradient in loss function, allowing one to set a large learning\nrate without worry. Inspired by batch normalization, we propose a general\nnonlinear update rule for the learning rate in batch and stochastic gradient\ndescent so that the learning rate can be initialized at a high value, and is\nsubsequently decreased according to gradient observations along the way. The\nproposed method is shown to achieve robustness to the relationship between the\nlearning rate and the Lipschitz constant, and near-optimal convergence rates in\nboth the batch and stochastic settings ($O(1/T)$ for smooth loss in the batch\nsetting, and $O(1/\\sqrt{T})$ for convex loss in the stochastic setting). We\nalso show through numerical evidence that such robustness of the proposed\nmethod extends to highly nonconvex and possibly non-smooth loss function in\ndeep learning problems.Our analysis establishes some first theoretical\nunderstanding into the observed robustness for batch normalization and weight\nnormalization. \n\n"}
{"id": "1803.03243", "contents": "Title: Domain Adaptive Faster R-CNN for Object Detection in the Wild Abstract: Object detection typically assumes that training and test data are drawn from\nan identical distribution, which, however, does not always hold in practice.\nSuch a distribution mismatch will lead to a significant performance drop. In\nthis work, we aim to improve the cross-domain robustness of object detection.\nWe tackle the domain shift on two levels: 1) the image-level shift, such as\nimage style, illumination, etc, and 2) the instance-level shift, such as object\nappearance, size, etc. We build our approach based on the recent\nstate-of-the-art Faster R-CNN model, and design two domain adaptation\ncomponents, on image level and instance level, to reduce the domain\ndiscrepancy. The two domain adaptation components are based on H-divergence\ntheory, and are implemented by learning a domain classifier in adversarial\ntraining manner. The domain classifiers on different levels are further\nreinforced with a consistency regularization to learn a domain-invariant region\nproposal network (RPN) in the Faster R-CNN model. We evaluate our newly\nproposed approach using multiple datasets including Cityscapes, KITTI, SIM10K,\netc. The results demonstrate the effectiveness of our proposed approach for\nrobust object detection in various domain shift scenarios. \n\n"}
{"id": "1803.03415", "contents": "Title: Fusing Hierarchical Convolutional Features for Human Body Segmentation\n  and Clothing Fashion Classification Abstract: The clothing fashion reflects the common aesthetics that people share with\neach other in dressing. To recognize the fashion time of a clothing is\nmeaningful for both an individual and the industry. In this paper, under the\nassumption that the clothing fashion changes year by year, the fashion-time\nrecognition problem is mapped into a clothing-fashion classification problem.\nSpecifically, a novel deep neural network is proposed which achieves accurate\nhuman body segmentation by fusing multi-scale convolutional features in a fully\nconvolutional network, and then feature learning and fashion classification are\nperformed on the segmented parts avoiding the influence of image background. In\nthe experiments, 9,339 fashion images from 8 continuous years are collected for\nperformance evaluation. The results demonstrate the effectiveness of the\nproposed body segmentation and fashion classification methods. \n\n"}
{"id": "1803.03711", "contents": "Title: Local Kernels that Approximate Bayesian Regularization and Proximal\n  Operators Abstract: In this work, we broadly connect kernel-based filtering (e.g. approaches such\nas the bilateral filters and nonlocal means, but also many more) with general\nvariational formulations of Bayesian regularized least squares, and the related\nconcept of proximal operators. The latter set of variational/Bayesian/proximal\nformulations often result in optimization problems that do not have closed-form\nsolutions, and therefore typically require global iterative solutions. Our main\ncontribution here is to establish how one can approximate the solution of the\nresulting global optimization problems with use of locally adaptive filters\nwith specific kernels. Our results are valid for small regularization strength\nbut the approach is powerful enough to be useful for a wide range of\napplications because we expose how to derive a \"kernelized\" solution to these\nproblems that approximates the global solution in one-shot, using only local\noperations. As another side benefit in the reverse direction, given a local\ndata-adaptive filter constructed with a particular choice of kernel, we enable\nthe interpretation of such filters in the variational/Bayesian/proximal\nframework. \n\n"}
{"id": "1803.05407", "contents": "Title: Averaging Weights Leads to Wider Optima and Better Generalization Abstract: Deep neural networks are typically trained by optimizing a loss function with\nan SGD variant, in conjunction with a decaying learning rate, until\nconvergence. We show that simple averaging of multiple points along the\ntrajectory of SGD, with a cyclical or constant learning rate, leads to better\ngeneralization than conventional training. We also show that this Stochastic\nWeight Averaging (SWA) procedure finds much flatter solutions than SGD, and\napproximates the recent Fast Geometric Ensembling (FGE) approach with a single\nmodel. Using SWA we achieve notable improvement in test accuracy over\nconventional SGD training on a range of state-of-the-art residual networks,\nPyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and\nImageNet. In short, SWA is extremely easy to implement, improves\ngeneralization, and has almost no computational overhead. \n\n"}
{"id": "1803.05675", "contents": "Title: Training of Convolutional Networks on Multiple Heterogeneous Datasets\n  for Street Scene Semantic Segmentation Abstract: We propose a convolutional network with hierarchical classifiers for\nper-pixel semantic segmentation, which is able to be trained on multiple,\nheterogeneous datasets and exploit their semantic hierarchy. Our network is the\nfirst to be simultaneously trained on three different datasets from the\nintelligent vehicles domain, i.e. Cityscapes, GTSDB and Mapillary Vistas, and\nis able to handle different semantic level-of-detail, class imbalances, and\ndifferent annotation types, i.e. dense per-pixel and sparse bounding-box\nlabels. We assess our hierarchical approach, by comparing against flat,\nnon-hierarchical classifiers and we show improvements in mean pixel accuracy of\n13.0% for Cityscapes classes and 2.4% for Vistas classes and 32.3% for GTSDB\nclasses. Our implementation achieves inference rates of 17 fps at a resolution\nof 520x706 for 108 classes running on a GPU. \n\n"}
{"id": "1803.06111", "contents": "Title: Vulnerability of Deep Learning Abstract: The Renormalisation Group (RG) provides a framework in which it is possible\nto assess whether a deep-learning network is sensitive to small changes in the\ninput data and hence prone to error, or susceptible to adversarial attack.\nDistinct classification outputs are associated with different RG fixed points\nand sensitivity to small changes in the input data is due to the presence of\nrelevant operators at a fixed point. A numerical scheme, based on Monte Carlo\nRG ideas, is proposed for identifying the existence of relevant operators and\nthe corresponding directions of greatest sensitivity in the input data. Thus, a\ntrained deep-learning network may be tested for its robustness and, if it is\nvulnerable to attack, dangerous perturbations of the input data identified. \n\n"}
{"id": "1803.06554", "contents": "Title: Fusion of an Ensemble of Augmented Image Detectors for Robust Object\n  Detection Abstract: A significant challenge in object detection is accurate identification of an\nobject's position in image space, whereas one algorithm with one set of\nparameters is usually not enough, and the fusion of multiple algorithms and/or\nparameters can lead to more robust results. Herein, a new computational\nintelligence fusion approach based on the dynamic analysis of agreement among\nobject detection outputs is proposed. Furthermore, we propose an online versus\njust in training image augmentation strategy. Experiments comparing the results\nboth with and without fusion are presented. We demonstrate that the augmented\nand fused combination results are the best, with respect to higher accuracy\nrates and reduction of outlier influences. The approach is demonstrated in the\ncontext of cone, pedestrian and box detection for Advanced Driver Assistance\nSystems (ADAS) applications. \n\n"}
{"id": "1803.06815", "contents": "Title: ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic\n  Segmentation Abstract: We introduce a fast and efficient convolutional neural network, ESPNet, for\nsemantic segmentation of high resolution images under resource constraints.\nESPNet is based on a new convolutional module, efficient spatial pyramid (ESP),\nwhich is efficient in terms of computation, memory, and power. ESPNet is 22\ntimes faster (on a standard GPU) and 180 times smaller than the\nstate-of-the-art semantic segmentation network PSPNet, while its category-wise\naccuracy is only 8% less. We evaluated ESPNet on a variety of semantic\nsegmentation datasets including Cityscapes, PASCAL VOC, and a breast biopsy\nwhole slide image dataset. Under the same constraints on memory and\ncomputation, ESPNet outperforms all the current efficient CNN networks such as\nMobileNet, ShuffleNet, and ENet on both standard metrics and our newly\nintroduced performance metrics that measure efficiency on edge devices. Our\nnetwork can process high resolution images at a rate of 112 and 9 frames per\nsecond on a standard GPU and edge device, respectively. \n\n"}
{"id": "1803.07226", "contents": "Title: Learning the Hierarchical Parts of Objects by Deep Non-Smooth\n  Nonnegative Matrix Factorization Abstract: Nonsmooth Nonnegative Matrix Factorization (nsNMF) is capable of producing\nmore localized, less overlapped feature representations than other variants of\nNMF while keeping satisfactory fit to data. However, nsNMF as well as other\nexisting NMF methods is incompetent to learn hierarchical features of complex\ndata due to its shallow structure. To fill this gap, we propose a deep nsNMF\nmethod coined by the fact that it possesses a deeper architecture compared with\nstandard nsNMF. The deep nsNMF not only gives parts-based features due to the\nnonnegativity constraints, but also creates higher-level, more abstract\nfeatures by combing lower-level ones. The in-depth description of how deep\narchitecture can help to efficiently discover abstract features in dnsNMF is\npresented. And we also show that the deep nsNMF has close relationship with the\ndeep autoencoder, suggesting that the proposed model inherits the major\nadvantages from both deep learning and NMF. Extensive experiments demonstrate\nthe standout performance of the proposed method in clustering analysis. \n\n"}
{"id": "1803.07293", "contents": "Title: Unsupervised Cross-dataset Person Re-identification by Transfer Learning\n  of Spatial-Temporal Patterns Abstract: Most of the proposed person re-identification algorithms conduct supervised\ntraining and testing on single labeled datasets with small size, so directly\ndeploying these trained models to a large-scale real-world camera network may\nlead to poor performance due to underfitting. It is challenging to\nincrementally optimize the models by using the abundant unlabeled data\ncollected from the target domain. To address this challenge, we propose an\nunsupervised incremental learning algorithm, TFusion, which is aided by the\ntransfer learning of the pedestrians' spatio-temporal patterns in the target\ndomain. Specifically, the algorithm firstly transfers the visual classifier\ntrained from small labeled source dataset to the unlabeled target dataset so as\nto learn the pedestrians' spatial-temporal patterns. Secondly, a Bayesian\nfusion model is proposed to combine the learned spatio-temporal patterns with\nvisual features to achieve a significantly improved classifier. Finally, we\npropose a learning-to-rank based mutual promotion procedure to incrementally\noptimize the classifiers based on the unlabeled data in the target domain.\nComprehensive experiments based on multiple real surveillance datasets are\nconducted, and the results show that our algorithm gains significant\nimprovement compared with the state-of-art cross-dataset unsupervised person\nre-identification algorithms. \n\n"}
{"id": "1803.08018", "contents": "Title: Monocular Depth Estimation by Learning from Heterogeneous Datasets Abstract: Depth estimation provides essential information to perform autonomous driving\nand driver assistance. Especially, Monocular Depth Estimation is interesting\nfrom a practical point of view, since using a single camera is cheaper than\nmany other options and avoids the need for continuous calibration strategies as\nrequired by stereo-vision approaches. State-of-the-art methods for Monocular\nDepth Estimation are based on Convolutional Neural Networks (CNNs). A promising\nline of work consists of introducing additional semantic information about the\ntraffic scene when training CNNs for depth estimation. In practice, this means\nthat the depth data used for CNN training is complemented with images having\npixel-wise semantic labels, which usually are difficult to annotate (e.g.\ncrowded urban images). Moreover, so far it is common practice to assume that\nthe same raw training data is associated with both types of ground truth, i.e.,\ndepth and semantic labels. The main contribution of this paper is to show that\nthis hard constraint can be circumvented, i.e., that we can train CNNs for\ndepth estimation by leveraging the depth and semantic information coming from\nheterogeneous datasets. In order to illustrate the benefits of our approach, we\ncombine KITTI depth and Cityscapes semantic segmentation datasets,\noutperforming state-of-the-art results on Monocular Depth Estimation. \n\n"}
{"id": "1803.09179", "contents": "Title: FaceForensics: A Large-scale Video Dataset for Forgery Detection in\n  Human Faces Abstract: With recent advances in computer vision and graphics, it is now possible to\ngenerate videos with extremely realistic synthetic faces, even in real time.\nCountless applications are possible, some of which raise a legitimate alarm,\ncalling for reliable detectors of fake videos. In fact, distinguishing between\noriginal and manipulated video can be a challenge for humans and computers\nalike, especially when the videos are compressed or have low resolution, as it\noften happens on social networks. Research on the detection of face\nmanipulations has been seriously hampered by the lack of adequate datasets. To\nthis end, we introduce a novel face manipulation dataset of about half a\nmillion edited images (from over 1000 videos). The manipulations have been\ngenerated with a state-of-the-art face editing approach. It exceeds all\nexisting video manipulation datasets by at least an order of magnitude. Using\nour new dataset, we introduce benchmarks for classical image forensic tasks,\nincluding classification and segmentation, considering videos compressed at\nvarious quality levels. In addition, we introduce a benchmark evaluation for\ncreating indistinguishable forgeries with known ground truth; for instance with\ngenerative refinement models. \n\n"}
{"id": "1803.09218", "contents": "Title: Image Recognition Using Scale Recurrent Neural Networks Abstract: Convolutional Neural Network(CNN) has been widely used for image recognition\nwith great success. However, there are a number of limitations of the current\nCNN based image recognition paradigm. First, the receptive field of CNN is\ngenerally fixed, which limits its recognition capacity when the input image is\nvery large. Second, it lacks the computational scalability for dealing with\nimages with different sizes. Third, it is quite different from human visual\nsystem for image recognition, which involves both feadforward and recurrent\nproprocessing. This paper proposes a different paradigm of image recognition,\nwhich can take advantages of variable scales of the input images, has more\ncomputational scalabilities, and is more similar to image recognition by human\nvisual system. It is based on recurrent neural network (RNN) defined on image\nscale with an embeded base CNN, which is named Scale Recurrent Neural\nNetwork(SRNN). This RNN based approach makes it easier to deal with images with\nvariable sizes, and allows us to borrow existing RNN techniques, such as LSTM\nand GRU, to further enhance the recognition accuracy. Our experiments show that\nthe recognition accuracy of a base CNN can be significantly boosted using the\nproposed SRNN models. It also significantly outperforms the scale ensemble\nmethod, which integrate the results of performing CNN to the input image at\ndifferent scales, although the computational overhead of using SRNN is\nnegligible. \n\n"}
{"id": "1803.09845", "contents": "Title: Neural Baby Talk Abstract: We introduce a novel framework for image captioning that can produce natural\nlanguage explicitly grounded in entities that object detectors find in the\nimage. Our approach reconciles classical slot filling approaches (that are\ngenerally better grounded in images) with modern neural captioning approaches\n(that are generally more natural sounding and accurate). Our approach first\ngenerates a sentence `template' with slot locations explicitly tied to specific\nimage regions. These slots are then filled in by visual concepts identified in\nthe regions by object detectors. The entire architecture (sentence template\ngeneration and slot filling with object detectors) is end-to-end\ndifferentiable. We verify the effectiveness of our proposed model on different\nimage captioning tasks. On standard image captioning and novel object\ncaptioning, our model reaches state-of-the-art on both COCO and Flickr30k\ndatasets. We also demonstrate that our model has unique advantages when the\ntrain and test distributions of scene compositions -- and hence language priors\nof associated captions -- are different. Code has been made available at:\nhttps://github.com/jiasenlu/NeuralBabyTalk \n\n"}
{"id": "1803.10335", "contents": "Title: Adaptive Affinity Fields for Semantic Segmentation Abstract: Semantic segmentation has made much progress with increasingly powerful\npixel-wise classifiers and incorporating structural priors via Conditional\nRandom Fields (CRF) or Generative Adversarial Networks (GAN). We propose a\nsimpler alternative that learns to verify the spatial structure of segmentation\nduring training only. Unlike existing approaches that enforce semantic labels\non individual pixels and match labels between neighbouring pixels, we propose\nthe concept of Adaptive Affinity Fields (AAF) to capture and match the semantic\nrelations between neighbouring pixels in the label space. We use adversarial\nlearning to select the optimal affinity field size for each semantic category.\nIt is formulated as a minimax problem, optimizing our segmentation neural\nnetwork in a best worst-case learning scenario. AAF is versatile for\nrepresenting structures as a collection of pixel-centric relations, easier to\ntrain than GAN and more efficient than CRF without run-time inference. Our\nextensive evaluations on PASCAL VOC 2012, Cityscapes, and GTA5 datasets\ndemonstrate its above-par segmentation performance and robust generalization\nacross domains. \n\n"}
{"id": "1803.10348", "contents": "Title: Structural inpainting Abstract: Scene-agnostic visual inpainting remains very challenging despite progress in\npatch-based methods. Recently, Pathak et al. 2016 have introduced convolutional\n\"context encoders\" (CEs) for unsupervised feature learning through image\ncompletion tasks. With the additional help of adversarial training, CEs turned\nout to be a promising tool to complete complex structures in real inpainting\nproblems. In the present paper we propose to push further this key ability by\nrelying on perceptual reconstruction losses at training time. We show on a wide\nvariety of visual scenes the merit of the approach for structural inpainting,\nand confirm it through a user study. Combined with the optimization-based\nrefinement of Yang et al. 2016 with neural patches, our context encoder opens\nup new opportunities for prior-free visual inpainting. \n\n"}
{"id": "1803.10910", "contents": "Title: Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling\n  Perspective Abstract: The success of current deep saliency detection methods heavily depends on the\navailability of large-scale supervision in the form of per-pixel labeling. Such\nsupervision, while labor-intensive and not always possible, tends to hinder the\ngeneralization ability of the learned models. By contrast, traditional\nhandcrafted features based unsupervised saliency detection methods, even though\nhave been surpassed by the deep supervised methods, are generally\ndataset-independent and could be applied in the wild. This raises a natural\nquestion that \"Is it possible to learn saliency maps without using labeled data\nwhile improving the generalization ability?\". To this end, we present a novel\nperspective to unsupervised saliency detection through learning from multiple\nnoisy labeling generated by \"weak\" and \"noisy\" unsupervised handcrafted\nsaliency methods. Our end-to-end deep learning framework for unsupervised\nsaliency detection consists of a latent saliency prediction module and a noise\nmodeling module that work collaboratively and are optimized jointly. Explicit\nnoise modeling enables us to deal with noisy saliency maps in a probabilistic\nway. Extensive experimental results on various benchmarking datasets show that\nour model not only outperforms all the unsupervised saliency methods with a\nlarge margin but also achieves comparable performance with the recent\nstate-of-the-art supervised deep saliency methods. \n\n"}
{"id": "1803.10914", "contents": "Title: Adversarial Binary Coding for Efficient Person Re-identification Abstract: Person re-identification (ReID) aims at matching persons across different\nviews/scenes. In addition to accuracy, the matching efficiency has received\nmore and more attention because of demanding applications using large-scale\ndata. Several binary coding based methods have been proposed for efficient\nReID, which either learn projections to map high-dimensional features to\ncompact binary codes, or directly adopt deep neural networks by simply\ninserting an additional fully-connected layer with tanh-like activations.\nHowever, the former approach requires time-consuming hand-crafted feature\nextraction and complicated (discrete) optimizations; the latter lacks the\nnecessary discriminative information greatly due to the straightforward\nactivation functions. In this paper, we propose a simple yet effective\nframework for efficient ReID inspired by the recent advances in adversarial\nlearning. Specifically, instead of learning explicit projections or adding\nfully-connected mapping layers, the proposed Adversarial Binary Coding (ABC)\nframework guides the extraction of binary codes implicitly and effectively. The\ndiscriminability of the extracted codes is further enhanced by equipping the\nABC with a deep triplet network for the ReID task. More importantly, the ABC\nand triplet network are simultaneously optimized in an end-to-end manner.\nExtensive experiments on three large-scale ReID benchmarks demonstrate the\nsuperiority of our approach over the state-of-the-art methods. \n\n"}
{"id": "1803.11187", "contents": "Title: MaskRNN: Instance Level Video Object Segmentation Abstract: Instance level video object segmentation is an important technique for video\nediting and compression. To capture the temporal coherence, in this paper, we\ndevelop MaskRNN, a recurrent neural net approach which fuses in each frame the\noutput of two deep nets for each object instance -- a binary segmentation net\nproviding a mask and a localization net providing a bounding box. Due to the\nrecurrent component and the localization component, our method is able to take\nadvantage of long-term temporal structures of the video data as well as\nrejecting outliers. We validate the proposed algorithm on three challenging\nbenchmark datasets, the DAVIS-2016 dataset, the DAVIS-2017 dataset, and the\nSegtrack v2 dataset, achieving state-of-the-art performance on all of them. \n\n"}
{"id": "1803.11303", "contents": "Title: Pancreas Segmentation in CT and MRI Images via Domain Specific Network\n  Designing and Recurrent Neural Contextual Learning Abstract: Automatic pancreas segmentation in radiology images, eg., computed tomography\n(CT) and magnetic resonance imaging (MRI), is frequently required by\ncomputer-aided screening, diagnosis, and quantitative assessment. Yet pancreas\nis a challenging abdominal organ to segment due to the high inter-patient\nanatomical variability in both shape and volume metrics. Recently,\nconvolutional neural networks (CNNs) have demonstrated promising performance on\naccurate segmentation of pancreas. However, the CNN-based method often suffers\nfrom segmentation discontinuity for reasons such as noisy image quality and\nblurry pancreatic boundary. From this point, we propose to introduce recurrent\nneural networks (RNNs) to address the problem of spatial non-smoothness of\ninter-slice pancreas segmentation across adjacent image slices. To inference\ninitial segmentation, we first train a 2D CNN sub-network, where we modify its\nnetwork architecture with deep-supervision and multi-scale feature map\naggregation so that it can be trained from scratch with small-sized training\ndata and presents superior performance than transferred models. Thereafter, the\nsuccessive CNN outputs are processed by another RNN sub-network, which refines\nthe consistency of segmented shapes. More specifically, the RNN sub-network\nconsists convolutional long short-term memory (CLSTM) units in both top-down\nand bottom-up directions, which regularizes the segmentation of an image by\nintegrating predictions of its neighboring slices. We train the stacked CNN-RNN\nmodel end-to-end and perform quantitative evaluations on both CT and MRI\nimages. \n\n"}
{"id": "1803.11320", "contents": "Title: Transductive Unbiased Embedding for Zero-Shot Learning Abstract: Most existing Zero-Shot Learning (ZSL) methods have the strong bias problem,\nin which instances of unseen (target) classes tend to be categorized as one of\nthe seen (source) classes. So they yield poor performance after being deployed\nin the generalized ZSL settings. In this paper, we propose a straightforward\nyet effective method named Quasi-Fully Supervised Learning (QFSL) to alleviate\nthe bias problem. Our method follows the way of transductive learning, which\nassumes that both the labeled source images and unlabeled target images are\navailable for training. In the semantic embedding space, the labeled source\nimages are mapped to several fixed points specified by the source categories,\nand the unlabeled target images are forced to be mapped to other points\nspecified by the target categories. Experiments conducted on AwA2, CUB and SUN\ndatasets demonstrate that our method outperforms existing state-of-the-art\napproaches by a huge margin of 9.3~24.5% following generalized ZSL settings,\nand by a large margin of 0.2~16.2% following conventional ZSL settings. \n\n"}
{"id": "1803.11493", "contents": "Title: 3D Pose Estimation and 3D Model Retrieval for Objects in the Wild Abstract: We propose a scalable, efficient and accurate approach to retrieve 3D models\nfor objects in the wild. Our contribution is twofold. We first present a 3D\npose estimation approach for object categories which significantly outperforms\nthe state-of-the-art on Pascal3D+. Second, we use the estimated pose as a prior\nto retrieve 3D models which accurately represent the geometry of objects in RGB\nimages. For this purpose, we render depth images from 3D models under our\npredicted pose and match learned image descriptors of RGB images against those\nof rendered depth images using a CNN-based multi-view metric learning approach.\nIn this way, we are the first to report quantitative results for 3D model\nretrieval on Pascal3D+, where our method chooses the same models as human\nannotators for 50% of the validation images on average. In addition, we show\nthat our method, which was trained purely on Pascal3D+, retrieves rich and\naccurate 3D models from ShapeNet given RGB images of objects in the wild. \n\n"}
{"id": "1803.11544", "contents": "Title: Guide Me: Interacting with Deep Networks Abstract: Interaction and collaboration between humans and intelligent machines has\nbecome increasingly important as machine learning methods move into real-world\napplications that involve end users. While much prior work lies at the\nintersection of natural language and vision, such as image captioning or image\ngeneration from text descriptions, less focus has been placed on the use of\nlanguage to guide or improve the performance of a learned visual processing\nalgorithm. In this paper, we explore methods to flexibly guide a trained\nconvolutional neural network through user input to improve its performance\nduring inference. We do so by inserting a layer that acts as a spatio-semantic\nguide into the network. This guide is trained to modify the network's\nactivations, either directly via an energy minimization scheme or indirectly\nthrough a recurrent model that translates human language queries to interaction\nweights. Learning the verbal interaction is fully automatic and does not\nrequire manual text annotations. We evaluate the method on two datasets,\nshowing that guiding a pre-trained network can improve performance, and provide\nextensive insights into the interaction between the guide and the CNN. \n\n"}
{"id": "1804.00064", "contents": "Title: Learning Beyond Human Expertise with Generative Models for Dental\n  Restorations Abstract: Computer vision has advanced significantly that many discriminative\napproaches such as object recognition are now widely used in real applications.\nWe present another exciting development that utilizes generative models for the\nmass customization of medical products such as dental crowns. In the dental\nindustry, it takes a technician years of training to design synthetic crowns\nthat restore the function and integrity of missing teeth. Each crown must be\ncustomized to individual patients, and it requires human expertise in a\ntime-consuming and labor-intensive process, even with computer-assisted design\nsoftware. We develop a fully automatic approach that learns not only from human\ndesigns of dental crowns, but also from natural spatial profiles between\nopposing teeth. The latter is hard to account for by technicians but important\nfor proper biting and chewing functions. Built upon a Generative Adversar-ial\nNetwork architecture (GAN), our deep learning model predicts the customized\ncrown-filled depth scan from the crown-missing depth scan and opposing depth\nscan. We propose to incorporate additional space constraints and statistical\ncompatibility into learning. Our automatic designs exceed human technicians'\nstandards for good morphology and functionality, and our algorithm is being\ntested for production use. \n\n"}
{"id": "1804.00113", "contents": "Title: Tagging like Humans: Diverse and Distinct Image Annotation Abstract: In this work we propose a new automatic image annotation model, dubbed {\\bf\ndiverse and distinct image annotation} (D2IA). The generative model D2IA is\ninspired by the ensemble of human annotations, which create semantically\nrelevant, yet distinct and diverse tags. In D2IA, we generate a relevant and\ndistinct tag subset, in which the tags are relevant to the image contents and\nsemantically distinct to each other, using sequential sampling from a\ndeterminantal point process (DPP) model. Multiple such tag subsets that cover\ndiverse semantic aspects or diverse semantic levels of the image contents are\ngenerated by randomly perturbing the DPP sampling process. We leverage a\ngenerative adversarial network (GAN) model to train D2IA. Extensive experiments\nincluding quantitative and qualitative comparisons, as well as human subject\nstudies, on two benchmark datasets demonstrate that the proposed model can\nproduce more diverse and distinct tags than the state-of-the-arts. \n\n"}
{"id": "1804.00256", "contents": "Title: One-Two-One Networks for Compression Artifacts Reduction in Remote\n  Sensing Abstract: Compression artifacts reduction (CAR) is a challenging problem in the field\nof remote sensing. Most recent deep learning based methods have demonstrated\nsuperior performance over the previous hand-crafted methods. In this paper, we\npropose an end-to-end one-two-one (OTO) network, to combine different deep\nmodels, i.e., summation and difference models, to solve the CAR problem.\nParticularly, the difference model motivated by the Laplacian pyramid is\ndesigned to obtain the high frequency information, while the summation model\naggregates the low frequency information. We provide an in-depth investigation\ninto our OTO architecture based on the Taylor expansion, which shows that these\ntwo kinds of information can be fused in a nonlinear scheme to gain more\ncapacity of handling complicated image compression artifacts, especially the\nblocking effect in compression. Extensive experiments are conducted to\ndemonstrate the superior performance of the OTO networks, as compared to the\nstate-of-the-arts on remote sensing datasets and other benchmark datasets. \n\n"}
{"id": "1804.00521", "contents": "Title: CompNet: Complementary Segmentation Network for Brain MRI Extraction Abstract: Brain extraction is a fundamental step for most brain imaging studies. In\nthis paper, we investigate the problem of skull stripping and propose\ncomplementary segmentation networks (CompNets) to accurately extract the brain\nfrom T1-weighted MRI scans, for both normal and pathological brain images. The\nproposed networks are designed in the framework of encoder-decoder networks and\nhave two pathways to learn features from both the brain tissue and its\ncomplementary part located outside of the brain. The complementary pathway\nextracts the features in the non-brain region and leads to a robust solution to\nbrain extraction from MRIs with pathologies, which do not exist in our training\ndataset. We demonstrate the effectiveness of our networks by evaluating them on\nthe OASIS dataset, resulting in the state of the art performance under the\ntwo-fold cross-validation setting. Moreover, the robustness of our networks is\nverified by testing on images with introduced pathologies and by showing its\ninvariance to unseen brain pathologies. In addition, our complementary network\ndesign is general and can be extended to address other image segmentation\nproblems with better generalization. \n\n"}
{"id": "1804.00582", "contents": "Title: Learning Intrinsic Image Decomposition from Watching the World Abstract: Single-view intrinsic image decomposition is a highly ill-posed problem, and\nso a promising approach is to learn from large amounts of data. However, it is\ndifficult to collect ground truth training data at scale for intrinsic images.\nIn this paper, we explore a different approach to learning intrinsic images:\nobserving image sequences over time depicting the same scene under changing\nillumination, and learning single-view decompositions that are consistent with\nthese changes. This approach allows us to learn without ground truth\ndecompositions, and to instead exploit information available from multiple\nimages when training. Our trained model can then be applied at test time to\nsingle views. We describe a new learning framework based on this idea,\nincluding new loss functions that can be efficiently evaluated over entire\nsequences. While prior learning-based methods achieve good performance on\nspecific benchmarks, we show that our approach generalizes well to several\ndiverse datasets, including MIT intrinsic images, Intrinsic Images in the Wild\nand Shading Annotations in the Wild. \n\n"}
{"id": "1804.00880", "contents": "Title: Weakly Supervised Instance Segmentation using Class Peak Response Abstract: Weakly supervised instance segmentation with image-level labels, instead of\nexpensive pixel-level masks, remains unexplored. In this paper, we tackle this\nchallenging problem by exploiting class peak responses to enable a\nclassification network for instance mask extraction. With image labels\nsupervision only, CNN classifiers in a fully convolutional manner can produce\nclass response maps, which specify classification confidence at each image\nlocation. We observed that local maximums, i.e., peaks, in a class response map\ntypically correspond to strong visual cues residing inside each instance.\nMotivated by this, we first design a process to stimulate peaks to emerge from\na class response map. The emerged peaks are then back-propagated and\neffectively mapped to highly informative regions of each object instance, such\nas instance boundaries. We refer to the above maps generated from class peak\nresponses as Peak Response Maps (PRMs). PRMs provide a fine-detailed\ninstance-level representation, which allows instance masks to be extracted even\nwith some off-the-shelf methods. To the best of our knowledge, we for the first\ntime report results for the challenging image-level supervised instance\nsegmentation task. Extensive experiments show that our method also boosts\nweakly supervised pointwise localization as well as semantic segmentation\nperformance, and reports state-of-the-art results on popular benchmarks,\nincluding PASCAL VOC 2012 and MS COCO. \n\n"}
{"id": "1804.01050", "contents": "Title: Training VAEs Under Structured Residuals Abstract: Variational auto-encoders (VAEs) are a popular and powerful deep generative\nmodel. Previous works on VAEs have assumed a factorized likelihood model,\nwhereby the output uncertainty of each pixel is assumed to be independent. This\napproximation is clearly limited as demonstrated by observing a residual image\nfrom a VAE reconstruction, which often possess a high level of structure. This\npaper demonstrates a novel scheme to incorporate a structured Gaussian\nlikelihood prediction network within the VAE that allows the residual\ncorrelations to be modeled. Our novel architecture, with minimal increase in\ncomplexity, incorporates the covariance matrix prediction within the VAE. We\nalso propose a new mechanism for allowing structured uncertainty on color\nimages. Furthermore, we provide a scheme for effectively training this model,\nand include some suggestions for improving performance in terms of efficiency\nor modeling longer range correlations. \n\n"}
{"id": "1804.01307", "contents": "Title: Btrfly Net: Vertebrae Labelling with Energy-based Adversarial Learning\n  of Local Spine Prior Abstract: Robust localisation and identification of vertebrae is essential for\nautomated spine analysis. The contribution of this work to the task is\ntwo-fold: (1) Inspired by the human expert, we hypothesise that a sagittal and\ncoronal reformation of the spine contain sufficient information for labelling\nthe vertebrae. Thereby, we propose a butterfly-shaped network architecture\n(termed Btrfly Net) that efficiently combines the information across\nreformations. (2) Underpinning the Btrfly net, we present an energy-based\nadversarial training regime that encodes local spine structure as an anatomical\nprior into the network, thereby enabling it to achieve state-of-art performance\nin all standard metrics on a benchmark dataset of 302 scans without any\npost-processing during inference. \n\n"}
{"id": "1804.02032", "contents": "Title: A Multi-Layer Approach to Superpixel-based Higher-order Conditional\n  Random Field for Semantic Image Segmentation Abstract: Superpixel-based Higher-order Conditional random fields (SP-HO-CRFs) are\nknown for their effectiveness in enforcing both short and long spatial\ncontiguity for pixelwise labelling in computer vision. However, their\nhigher-order potentials are usually too complex to learn and often incur a high\ncomputational cost in performing inference. We propose an new approximation\napproach to SP-HO-CRFs that resolves these problems. Our approach is a\nmulti-layer CRF framework that inherits the simplicity from pairwise CRFs by\nformulating both the higher-order and pairwise cues into the same pairwise\npotentials in the first layer. Essentially, this approach provides accuracy\nenhancement on the basis of pairwise CRFs without training by reusing their\npre-trained parameters and/or weights. The proposed multi-layer approach\nperforms especially well in delineating the boundary details (boarders) of\nobject categories such as \"trees\" and \"bushes\". Multiple sets of experiments\nconducted on dataset MSRC-21 and PASCAL VOC 2012 validate the effectiveness and\nefficiency of the proposed methods. \n\n"}
{"id": "1804.02047", "contents": "Title: Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and\n  Beyond Abstract: State-of-the-art pedestrian detection models have achieved great success in\nmany benchmarks. However, these models require lots of annotation information\nand the labeling process usually takes much time and efforts. In this paper, we\npropose a method to generate labeled pedestrian data and adapt them to support\nthe training of pedestrian detectors. The proposed framework is built on the\nGenerative Adversarial Network (GAN) with multiple discriminators, trying to\nsynthesize realistic pedestrians and learn the background context\nsimultaneously. To handle the pedestrians of different sizes, we adopt the\nSpatial Pyramid Pooling (SPP) layer in the discriminator. We conduct\nexperiments on two benchmarks. The results show that our framework can smoothly\nsynthesize pedestrians on background images of variations and different levels\nof details. To quantitatively evaluate our approach, we add the generated\nsamples into training data of the baseline pedestrian detectors and show the\nsynthetic images are able to improve the detectors' performance. \n\n"}
{"id": "1804.02740", "contents": "Title: Facial Aging and Rejuvenation by Conditional Multi-Adversarial\n  Autoencoder with Ordinal Regression Abstract: Facial aging and facial rejuvenation analyze a given face photograph to\npredict a future look or estimate a past look of the person. To achieve this,\nit is critical to preserve human identity and the corresponding aging\nprogression and regression with high accuracy. However, existing methods cannot\nsimultaneously handle these two objectives well. We propose a novel generative\nadversarial network based approach, named the Conditional Multi-Adversarial\nAutoEncoder with Ordinal Regression (CMAAE-OR). It utilizes an age estimation\ntechnique to control the aging accuracy and takes a high-level feature\nrepresentation to preserve personalized identity. Specifically, the face is\nfirst mapped to a latent vector through a convolutional encoder. The latent\nvector is then projected onto the face manifold conditional on the age through\na deconvolutional generator. The latent vector preserves personalized face\nfeatures and the age controls facial aging and rejuvenation. A discriminator\nand an ordinal regression are imposed on the encoder and the generator in\ntandem, making the generated face images to be more photorealistic while\nsimultaneously exhibiting desirable aging effects. Besides, a high-level\nfeature representation is utilized to preserve personalized identity of the\ngenerated face. Experiments on two benchmark datasets demonstrate appealing\nperformance of the proposed method over the state-of-the-art. \n\n"}
{"id": "1804.02941", "contents": "Title: Distribution-Aware Binarization of Neural Networks for Sketch\n  Recognition Abstract: Deep neural networks are highly effective at a range of computational tasks.\nHowever, they tend to be computationally expensive, especially in\nvision-related problems, and also have large memory requirements. One of the\nmost effective methods to achieve significant improvements in\ncomputational/spatial efficiency is to binarize the weights and activations in\na network. However, naive binarization results in accuracy drops when applied\nto networks for most tasks. In this work, we present a highly generalized,\ndistribution-aware approach to binarizing deep networks that allows us to\nretain the advantages of a binarized network, while reducing accuracy drops. We\nalso develop efficient implementations for our proposed approach across\ndifferent architectures. We present a theoretical analysis of the technique to\nshow the effective representational power of the resulting layers, and explore\nthe forms of data they model best. Experiments on popular datasets show that\nour technique offers better accuracies than naive binarization, while retaining\nthe same benefits that binarization provides - with respect to run-time\ncompression, reduction of computational costs, and power consumption. \n\n"}
{"id": "1804.03270", "contents": "Title: Towards Deep Cellular Phenotyping in Placental Histology Abstract: The placenta is a complex organ, playing multiple roles during fetal\ndevelopment. Very little is known about the association between placental\nmorphological abnormalities and fetal physiology. In this work, we present an\nopen sourced, computationally tractable deep learning pipeline to analyse\nplacenta histology at the level of the cell. By utilising two deep\nConvolutional Neural Network architectures and transfer learning, we can\nrobustly localise and classify placental cells within five classes with an\naccuracy of 89%. Furthermore, we learn deep embeddings encoding phenotypic\nknowledge that is capable of both stratifying five distinct cell populations\nand learn intraclass phenotypic variance. We envisage that the automation of\nthis pipeline to population scale studies of placenta histology has the\npotential to improve our understanding of basic cellular placental biology and\nits variations, particularly its role in predicting adverse birth outcomes. \n\n"}
{"id": "1804.03286", "contents": "Title: On the Robustness of the CVPR 2018 White-Box Adversarial Example\n  Defenses Abstract: Neural networks are known to be vulnerable to adversarial examples. In this\nnote, we evaluate the two white-box defenses that appeared at CVPR 2018 and\nfind they are ineffective: when applying existing techniques, we can reduce the\naccuracy of the defended models to 0%. \n\n"}
{"id": "1804.03368", "contents": "Title: Learning Deep Gradient Descent Optimization for Image Deconvolution Abstract: As an integral component of blind image deblurring, non-blind deconvolution\nremoves image blur with a given blur kernel, which is essential but difficult\ndue to the ill-posed nature of the inverse problem. The predominant approach is\nbased on optimization subject to regularization functions that are either\nmanually designed, or learned from examples. Existing learning based methods\nhave shown superior restoration quality but are not practical enough due to\ntheir restricted and static model design. They solely focus on learning a prior\nand require to know the noise level for deconvolution. We address the gap\nbetween the optimization-based and learning-based approaches by learning a\nuniversal gradient descent optimizer. We propose a Recurrent Gradient Descent\nNetwork (RGDN) by systematically incorporating deep neural networks into a\nfully parameterized gradient descent scheme. A hyper-parameter-free update unit\nshared across steps is used to generate updates from the current estimates,\nbased on a convolutional neural network. By training on diverse examples, the\nRecurrent Gradient Descent Network learns an implicit image prior and a\nuniversal update rule through recursive supervision. The learned optimizer can\nbe repeatedly used to improve the quality of diverse degenerated observations.\nThe proposed method possesses strong interpretability and high generalization.\nExtensive experiments on synthetic benchmarks and challenging real-world images\ndemonstrate that the proposed deep optimization method is effective and robust\nto produce favorable results as well as practical for real-world image\ndeblurring applications. \n\n"}
{"id": "1804.03599", "contents": "Title: Understanding disentangling in $\\beta$-VAE Abstract: We present new intuitions and theoretical assessments of the emergence of\ndisentangled representation in variational autoencoders. Taking a\nrate-distortion theory perspective, we show the circumstances under which\nrepresentations aligned with the underlying generative factors of variation of\ndata emerge when optimising the modified ELBO bound in $\\beta$-VAE, as training\nprogresses. From these insights, we propose a modification to the training\nregime of $\\beta$-VAE, that progressively increases the information capacity of\nthe latent code during training. This modification facilitates the robust\nlearning of disentangled representations in $\\beta$-VAE, without the previous\ntrade-off in reconstruction accuracy. \n\n"}
{"id": "1804.04326", "contents": "Title: STAIR Actions: A Video Dataset of Everyday Home Actions Abstract: A new large-scale video dataset for human action recognition, called STAIR\nActions is introduced. STAIR Actions contains 100 categories of action labels\nrepresenting fine-grained everyday home actions so that it can be applied to\nresearch in various home tasks such as nursing, caring, and security. In STAIR\nActions, each video has a single action label. Moreover, for each action\ncategory, there are around 1,000 videos that were obtained from YouTube or\nproduced by crowdsource workers. The duration of each video is mostly five to\nsix seconds. The total number of videos is 102,462. We explain how we\nconstructed STAIR Actions and show the characteristics of STAIR Actions\ncompared to existing datasets for human action recognition. Experiments with\nthree major models for action recognition show that STAIR Actions can train\nlarge models and achieve good performance. STAIR Actions can be downloaded from\nhttp://actions.stair.center \n\n"}
{"id": "1804.04591", "contents": "Title: Improving Classification Rate of Schizophrenia Using a Multimodal\n  Multi-Layer Perceptron Model with Structural and Functional MR Abstract: The wide variety of brain imaging technologies allows us to exploit\ninformation inherent to different data modalities. The richness of multimodal\ndatasets may increase predictive power and reveal latent variables that\notherwise would have not been found. However, the analysis of multimodal data\nis often conducted by assuming linear interactions which impact the accuracy of\nthe results. We propose the use of a multimodal multi-layer perceptron model to\nenhance the predictive power of structural and functional magnetic resonance\nimaging (sMRI and fMRI) combined.\n  We also use a synthetic data generator to pre-train each modality input\nlayers, alleviating the effects of the small sample size that is often the case\nfor brain imaging modalities. The proposed model improved the average and\nuncertainty of the area under the ROC curve to 0.850+-0.051 compared to the\nbest results on individual modalities (0.741+-0.075 for sMRI, and 0.833+-0.050\nfor fMRI). \n\n"}
{"id": "1804.04804", "contents": "Title: Learning Deep Sketch Abstraction Abstract: Human free-hand sketches have been studied in various contexts including\nsketch recognition, synthesis and fine-grained sketch-based image retrieval\n(FG-SBIR). A fundamental challenge for sketch analysis is to deal with\ndrastically different human drawing styles, particularly in terms of\nabstraction level. In this work, we propose the first stroke-level sketch\nabstraction model based on the insight of sketch abstraction as a process of\ntrading off between the recognizability of a sketch and the number of strokes\nused to draw it. Concretely, we train a model for abstract sketch generation\nthrough reinforcement learning of a stroke removal policy that learns to\npredict which strokes can be safely removed without affecting recognizability.\nWe show that our abstraction model can be used for various sketch analysis\ntasks including: (1) modeling stroke saliency and understanding the decision of\nsketch recognition models, (2) synthesizing sketches of variable abstraction\nfor a given category, or reference object instance in a photo, and (3) training\na FG-SBIR model with photos only, bypassing the expensive photo-sketch pair\ncollection step. \n\n"}
{"id": "1804.05018", "contents": "Title: Comparatives, Quantifiers, Proportions: A Multi-Task Model for the\n  Learning of Quantities from Vision Abstract: The present work investigates whether different quantification mechanisms\n(set comparison, vague quantification, and proportional estimation) can be\njointly learned from visual scenes by a multi-task computational model. The\nmotivation is that, in humans, these processes underlie the same cognitive,\nnon-symbolic ability, which allows an automatic estimation and comparison of\nset magnitudes. We show that when information about lower-complexity tasks is\navailable, the higher-level proportional task becomes more accurate than when\nperformed in isolation. Moreover, the multi-task model is able to generalize to\nunseen combinations of target/non-target objects. Consistently with behavioral\nevidence showing the interference of absolute number in the proportional task,\nthe multi-task model no longer works when asked to provide the number of target\nobjects in the scene. \n\n"}
{"id": "1804.05422", "contents": "Title: FDMO: Feature Assisted Direct Monocular Odometry Abstract: Visual Odometry (VO) can be categorized as being either direct or feature\nbased. When the system is calibrated photometrically, and images are captured\nat high rates, direct methods have shown to outperform feature-based ones in\nterms of accuracy and processing time; they are also more robust to failure in\nfeature-deprived environments. On the downside, Direct methods rely on\nheuristic motion models to seed the estimation of camera motion between frames;\nin the event that these models are violated (e.g., erratic motion), Direct\nmethods easily fail. This paper proposes a novel system entitled FDMO (Feature\nassisted Direct Monocular Odometry), which complements the advantages of both\ndirect and featured based techniques. FDMO bootstraps indirect feature tracking\nupon the sub-pixel accurate localized direct keyframes only when failure modes\n(e.g., large baselines) of direct tracking occur. Control returns back to\ndirect odometry when these conditions are no longer violated. Efficiencies are\nintroduced to help FDMO perform in real time. FDMO shows significant drift\n(alignment, rotation & scale) reduction when compared to DSO & ORB SLAM when\nevaluated using the TumMono and EuroC datasets. \n\n"}
{"id": "1804.06078", "contents": "Title: Cross-Domain Adversarial Auto-Encoder Abstract: In this paper, we propose the Cross-Domain Adversarial Auto-Encoder (CDAAE)\nto address the problem of cross-domain image inference, generation and\ntransformation. We make the assumption that images from different domains share\nthe same latent code space for content, while having separate latent code space\nfor style. The proposed framework can map cross-domain data to a latent code\nvector consisting of a content part and a style part. The latent code vector is\nmatched with a prior distribution so that we can generate meaningful samples\nfrom any part of the prior space. Consequently, given a sample of one domain,\nour framework can generate various samples of the other domain with the same\ncontent of the input. This makes the proposed framework different from the\ncurrent work of cross-domain transformation. Besides, the proposed framework\ncan be trained with both labeled and unlabeled data, which makes it also\nsuitable for domain adaptation. Experimental results on data sets SVHN, MNIST\nand CASIA show the proposed framework achieved visually appealing performance\nfor image generation task. Besides, we also demonstrate the proposed method\nachieved superior results for domain adaptation. Code of our experiments is\navailable in https://github.com/luckycallor/CDAAE. \n\n"}
{"id": "1804.07091", "contents": "Title: Detecting Regions of Maximal Divergence for Spatio-Temporal Anomaly\n  Detection Abstract: Automatic detection of anomalies in space- and time-varying measurements is\nan important tool in several fields, e.g., fraud detection, climate analysis,\nor healthcare monitoring. We present an algorithm for detecting anomalous\nregions in multivariate spatio-temporal time-series, which allows for spotting\nthe interesting parts in large amounts of data, including video and text data.\nIn opposition to existing techniques for detecting isolated anomalous data\npoints, we propose the \"Maximally Divergent Intervals\" (MDI) framework for\nunsupervised detection of coherent spatial regions and time intervals\ncharacterized by a high Kullback-Leibler divergence compared with all other\ndata given. In this regard, we define an unbiased Kullback-Leibler divergence\nthat allows for ranking regions of different size and show how to enable the\nalgorithm to run on large-scale data sets in reasonable time using an interval\nproposal technique. Experiments on both synthetic and real data from various\ndomains, such as climate analysis, video surveillance, and text forensics,\ndemonstrate that our method is widely applicable and a valuable tool for\nfinding interesting events in different types of data. \n\n"}
{"id": "1804.08366", "contents": "Title: VLocNet++: Deep Multitask Learning for Semantic Visual Localization and\n  Odometry Abstract: Semantic understanding and localization are fundamental enablers of robot\nautonomy that have for the most part been tackled as disjoint problems. While\ndeep learning has enabled recent breakthroughs across a wide spectrum of scene\nunderstanding tasks, its applicability to state estimation tasks has been\nlimited due to the direct formulation that renders it incapable of encoding\nscene-specific constrains. In this work, we propose the VLocNet++ architecture\nthat employs a multitask learning approach to exploit the inter-task\nrelationship between learning semantics, regressing 6-DoF global pose and\nodometry, for the mutual benefit of each of these tasks. Our network overcomes\nthe aforementioned limitation by simultaneously embedding geometric and\nsemantic knowledge of the world into the pose regression network. We propose a\nnovel adaptive weighted fusion layer to aggregate motion-specific temporal\ninformation and to fuse semantic features into the localization stream based on\nregion activations. Furthermore, we propose a self-supervised warping technique\nthat uses the relative motion to warp intermediate network representations in\nthe segmentation stream for learning consistent semantics. Finally, we\nintroduce a first-of-a-kind urban outdoor localization dataset with pixel-level\nsemantic labels and multiple loops for training deep networks. Extensive\nexperiments on the challenging Microsoft 7-Scenes benchmark and our DeepLoc\ndataset demonstrate that our approach exceeds the state-of-the-art\noutperforming local feature-based methods while simultaneously performing\nmultiple tasks and exhibiting substantial robustness in challenging scenarios. \n\n"}
{"id": "1804.08606", "contents": "Title: Zero-Shot Visual Imitation Abstract: The current dominant paradigm for imitation learning relies on strong\nsupervision of expert actions to learn both 'what' and 'how' to imitate. We\npursue an alternative paradigm wherein an agent first explores the world\nwithout any expert supervision and then distills its experience into a\ngoal-conditioned skill policy with a novel forward consistency loss. In our\nframework, the role of the expert is only to communicate the goals (i.e., what\nto imitate) during inference. The learned policy is then employed to mimic the\nexpert (i.e., how to imitate) after seeing just a sequence of images\ndemonstrating the desired task. Our method is 'zero-shot' in the sense that the\nagent never has access to expert actions during training or for the task\ndemonstration at inference. We evaluate our zero-shot imitator in two\nreal-world settings: complex rope manipulation with a Baxter robot and\nnavigation in previously unseen office environments with a TurtleBot. Through\nfurther experiments in VizDoom simulation, we provide evidence that better\nmechanisms for exploration lead to learning a more capable policy which in turn\nimproves end task performance. Videos, models, and more details are available\nat https://pathak22.github.io/zeroshot-imitation/ \n\n"}
{"id": "1804.08750", "contents": "Title: A machine learning model for identifying cyclic alternating patterns in\n  the sleeping brain Abstract: Electroencephalography (EEG) is a method to record the electrical signals in\nthe brain. Recognizing the EEG patterns in the sleeping brain gives insights\ninto the understanding of sleeping disorders. The dataset under consideration\ncontains EEG data points associated with various physiological conditions. This\nstudy attempts to generalize the detection of particular patterns associated\nwith the Non-Rapid Eye Movement (NREM) sleep cycle of the brain using a machine\nlearning model. The proposed model uses additional feature engineering to\nincorporate sequential information for training a classifier to predict the\noccurrence of Cyclic Alternating Pattern (CAP) sequences in the sleep cycle,\nwhich are often associated with sleep disorders. \n\n"}
{"id": "1804.09555", "contents": "Title: Robust Video Content Alignment and Compensation for Clear Vision Through\n  the Rain Abstract: Outdoor vision-based systems suffer from atmospheric turbulences, and rain is\none of the worst factors for vision degradation. Current rain removal methods\nshow limitations either for complex dynamic scenes, or under torrential rain\nwith opaque occlusions. We propose a novel derain framework which applies\nsuperpixel (SP) segmentation to decompose the scene into depth consistent\nunits. Alignment of scene contents are done at the SP level, which proves to be\nrobust against rain occlusion interferences and fast camera motion. Two\nalignment output tensors, i.e., optimal temporal match tensor and sorted\nspatial-temporal match tensor, provide informative clues for the location of\nrain streaks and the occluded background contents. Different classical and\nnovel methods such as Robust Principle Component Analysis and Convolutional\nNeural Networks are applied and compared for their respective advantages in\nefficiently exploiting the rich spatial-temporal features provided by the two\ntensors. Extensive evaluations show that advantage of up to 5dB is achieved on\nthe scene restoration PSNR over state-of-the-art methods, and the advantage is\nespecially obvious with highly complex and dynamic scenes. Visual evaluations\nshow that the proposed framework is not only able to suppress heavy and opaque\noccluding rain streaks, but also large semi-transparent regional fluctuations\nand distortions. \n\n"}
{"id": "1804.09858", "contents": "Title: Generative Model for Heterogeneous Inference Abstract: Generative models (GMs) such as Generative Adversary Network (GAN) and\nVariational Auto-Encoder (VAE) have thrived these years and achieved high\nquality results in generating new samples. Especially in Computer Vision, GMs\nhave been used in image inpainting, denoising and completion, which can be\ntreated as the inference from observed pixels to corrupted pixels. However,\nimages are hierarchically structured which are quite different from many\nreal-world inference scenarios with non-hierarchical features. These inference\nscenarios contain heterogeneous stochastic variables and irregular mutual\ndependences. Traditionally they are modeled by Bayesian Network (BN). However,\nthe learning and inference of BN model are NP-hard thus the number of\nstochastic variables in BN is highly constrained. In this paper, we adapt\ntypical GMs to enable heterogeneous learning and inference in polynomial\ntime.We also propose an extended autoregressive (EAR) model and an EAR with\nadversary loss (EARA) model and give theoretical results on their\neffectiveness. Experiments on several BN datasets show that our proposed EAR\nmodel achieves the best performance in most cases compared to other GMs. Except\nfor black box analysis, we've also done a serial of experiments on Markov\nborder inference of GMs for white box analysis and give theoretical results. \n\n"}
{"id": "1804.10819", "contents": "Title: Learning Cross-Modal Deep Embeddings for Multi-Object Image Retrieval\n  using Text and Sketch Abstract: In this work we introduce a cross modal image retrieval system that allows\nboth text and sketch as input modalities for the query. A cross-modal deep\nnetwork architecture is formulated to jointly model the sketch and text input\nmodalities as well as the the image output modality, learning a common\nembedding between text and images and between sketches and images. In addition,\nan attention model is used to selectively focus the attention on the different\nobjects of the image, allowing for retrieval with multiple objects in the\nquery. Experiments show that the proposed method performs the best in both\nsingle and multiple object image retrieval in standard datasets. \n\n"}
{"id": "1804.11256", "contents": "Title: On the Feasibility of Real-Time 3D Hand Tracking using Edge GPGPU\n  Acceleration Abstract: This paper presents the case study of a non-intrusive porting of a monolithic\nC++ library for real-time 3D hand tracking, to the domain of edge-based\ncomputation. Towards a proof of concept, the case study considers a pair of\nworkstations, a computationally powerful and a computationally weak one. By\nwrapping the C++ library in Java container and by capitalizing on a Java-based\noffloading infrastructure that supports both CPU and GPGPU computations, we are\nable to establish automatically the required server-client workflow that best\naddresses the resource allocation problem in the effort to execute from the\nweak workstation. As a result, the weak workstation can perform well at the\ntask, despite lacking the sufficient hardware to do the required computations\nlocally. This is achieved by offloading computations which rely on GPGPU, to\nthe powerful workstation, across the network that connects them. We show the\nedge-based computation challenges associated with the information flow of the\nported algorithm, demonstrate how we cope with them, and identify what needs to\nbe improved for achieving even better performance. \n\n"}
{"id": "1804.11332", "contents": "Title: On the iterative refinement of densely connected representation levels\n  for semantic segmentation Abstract: State-of-the-art semantic segmentation approaches increase the receptive\nfield of their models by using either a downsampling path composed of\npoolings/strided convolutions or successive dilated convolutions. However, it\nis not clear which operation leads to best results. In this paper, we\nsystematically study the differences introduced by distinct receptive field\nenlargement methods and their impact on the performance of a novel\narchitecture, called Fully Convolutional DenseResNet (FC-DRN). FC-DRN has a\ndensely connected backbone composed of residual networks. Following standard\nimage segmentation architectures, receptive field enlargement operations that\nchange the representation level are interleaved among residual networks. This\nallows the model to exploit the benefits of both residual and dense\nconnectivity patterns, namely: gradient flow, iterative refinement of\nrepresentations, multi-scale feature combination and deep supervision. In order\nto highlight the potential of our model, we test it on the challenging CamVid\nurban scene understanding benchmark and make the following observations: 1)\ndownsampling operations outperform dilations when the model is trained from\nscratch, 2) dilations are useful during the finetuning step of the model, 3)\ncoarser representations require less refinement steps, and 4) ResNets (by model\nconstruction) are good regularizers, since they can reduce the model capacity\nwhen needed. Finally, we compare our architecture to alternative methods and\nreport state-of-the-art result on the Camvid dataset, with at least twice fewer\nparameters. \n\n"}
{"id": "1805.00145", "contents": "Title: Dialog-based Interactive Image Retrieval Abstract: Existing methods for interactive image retrieval have demonstrated the merit\nof integrating user feedback, improving retrieval results. However, most\ncurrent systems rely on restricted forms of user feedback, such as binary\nrelevance responses, or feedback based on a fixed set of relative attributes,\nwhich limits their impact. In this paper, we introduce a new approach to\ninteractive image search that enables users to provide feedback via natural\nlanguage, allowing for more natural and effective interaction. We formulate the\ntask of dialog-based interactive image retrieval as a reinforcement learning\nproblem, and reward the dialog system for improving the rank of the target\nimage during each dialog turn. To mitigate the cumbersome and costly process of\ncollecting human-machine conversations as the dialog system learns, we train\nour system with a user simulator, which is itself trained to describe the\ndifferences between target and candidate images. The efficacy of our approach\nis demonstrated in a footwear retrieval application. Experiments on both\nsimulated and real-world data show that 1) our proposed learning framework\nachieves better accuracy than other supervised and reinforcement learning\nbaselines and 2) user feedback based on natural language rather than\npre-specified attributes leads to more effective retrieval results, and a more\nnatural and expressive communication interface. \n\n"}
{"id": "1805.00348", "contents": "Title: OMG - Emotion Challenge Solution Abstract: This short paper describes our solution to the 2018 IEEE World Congress on\nComputational Intelligence One-Minute Gradual-Emotional Behavior Challenge,\nwhose goal was to estimate continuous arousal and valence values from short\nvideos. We designed four base regression models using visual and audio\nfeatures, and then used a spectral approach to fuse them to obtain improved\nperformance. \n\n"}
{"id": "1805.00553", "contents": "Title: Generating Synthetic X-ray Images of a Person from the Surface Geometry Abstract: We present a novel framework that learns to predict human anatomy from body\nsurface. Specifically, our approach generates a synthetic X-ray image of a\nperson only from the person's surface geometry. Furthermore, the synthetic\nX-ray image is parametrized and can be manipulated by adjusting a set of body\nmarkers which are also generated during the X-ray image prediction. With the\nproposed framework, multiple synthetic X-ray images can easily be generated by\nvarying surface geometry. By perturbing the parameters, several additional\nsynthetic X-ray images can be generated from the same surface geometry. As a\nresult, our approach offers a potential to overcome the training data barrier\nin the medical domain. This capability is achieved by learning a pair of\nnetworks - one learns to generate the full image from the partial image and a\nset of parameters, and the other learns to estimate the parameters given the\nfull image. During training, the two networks are trained iteratively such that\nthey would converge to a solution where the predicted parameters and the full\nimage are consistent with each other. In addition to medical data enrichment,\nour framework can also be used for image completion as well as anomaly\ndetection. \n\n"}
{"id": "1805.00655", "contents": "Title: Convolutional Sequence to Sequence Model for Human Dynamics Abstract: Human motion modeling is a classic problem in computer vision and graphics.\nChallenges in modeling human motion include high dimensional prediction as well\nas extremely complicated dynamics.We present a novel approach to human motion\nmodeling based on convolutional neural networks (CNN). The hierarchical\nstructure of CNN makes it capable of capturing both spatial and temporal\ncorrelations effectively. In our proposed approach,a convolutional long-term\nencoder is used to encode the whole given motion sequence into a long-term\nhidden variable, which is used with a decoder to predict the remainder of the\nsequence. The decoder itself also has an encoder-decoder structure, in which\nthe short-term encoder encodes a shorter sequence to a short-term hidden\nvariable, and the spatial decoder maps the long and short-term hidden variable\nto motion predictions. By using such a model, we are able to capture both\ninvariant and dynamic information of human motion, which results in more\naccurate predictions. Experiments show that our algorithm outperforms the\nstate-of-the-art methods on the Human3.6M and CMU Motion Capture datasets. Our\ncode is available at the project website. \n\n"}
{"id": "1805.00862", "contents": "Title: Spectral clustering algorithms for the detection of clusters in\n  block-cyclic and block-acyclic graphs Abstract: We propose two spectral algorithms for partitioning nodes in directed graphs\nrespectively with a cyclic and an acyclic pattern of connection between groups\nof nodes. Our methods are based on the computation of extremal eigenvalues of\nthe transition matrix associated to the directed graph. The two algorithms\noutperform state-of-the art methods for directed graph clustering on synthetic\ndatasets, including methods based on blockmodels, bibliometric symmetrization\nand random walks. Our algorithms have the same space complexity as classical\nspectral clustering algorithms for undirected graphs and their time complexity\nis also linear in the number of edges in the graph. One of our methods is\napplied to a trophic network based on predator-prey relationships. It\nsuccessfully extracts common categories of preys and predators encountered in\nfood chains. The same method is also applied to highlight the hierarchical\nstructure of a worldwide network of Autonomous Systems depicting business\nagreements between Internet Service Providers. \n\n"}
{"id": "1805.01803", "contents": "Title: Unsupervised learning for concept detection in medical images: a\n  comparative analysis Abstract: As digital medical imaging becomes more prevalent and archives increase in\nsize, representation learning exposes an interesting opportunity for enhanced\nmedical decision support systems. On the other hand, medical imaging data is\noften scarce and short on annotations. In this paper, we present an assessment\nof unsupervised feature learning approaches for images in the biomedical\nliterature, which can be applied to automatic biomedical concept detection. Six\nunsupervised representation learning methods were built, including traditional\nbags of visual words, autoencoders, and generative adversarial networks. Each\nmodel was trained, and their respective feature space evaluated using images\nfrom the ImageCLEF 2017 concept detection task. We conclude that it is possible\nto obtain more powerful representations with modern deep learning approaches,\nin contrast with previously popular computer vision methods. Although\ngenerative adversarial networks can provide good results, they are harder to\nsucceed in highly varied data sets. The possibility of semi-supervised\nlearning, as well as their use in medical information retrieval problems, are\nthe next steps to be strongly considered. \n\n"}
{"id": "1805.03642", "contents": "Title: Adversarial Contrastive Estimation Abstract: Learning by contrasting positive and negative samples is a general strategy\nadopted by many methods. Noise contrastive estimation (NCE) for word embeddings\nand translating embeddings for knowledge graphs are examples in NLP employing\nthis approach. In this work, we view contrastive learning as an abstraction of\nall such methods and augment the negative sampler into a mixture distribution\ncontaining an adversarially learned sampler. The resulting adaptive sampler\nfinds harder negative examples, which forces the main model to learn a better\nrepresentation of the data. We evaluate our proposal on learning word\nembeddings, order embeddings and knowledge graph embeddings and observe both\nfaster convergence and improved results on multiple metrics. \n\n"}
{"id": "1805.03707", "contents": "Title: A Continuous, Full-scope, Spatio-temporal Tracking Metric based on\n  KL-divergence Abstract: A unified metric is given for the evaluation of object tracking systems. The\nmetric is inspired by KL-divergence or relative entropy, which is commonly used\nto evaluate clustering techniques. Since tracking problems are fundamentally\ndifferent from clustering, the components of KL-divergence are recast to handle\nvarious types of tracking errors (i.e., false alarms, missed detections,\nmerges, splits). Scoring results are given on a standard tracking dataset\n(Oxford Town Centre Dataset), as well as several simulated scenarios. Also,\nthis new metric is compared with several other metrics including the commonly\nused Multiple Object Tracking Accuracy metric. In the final section, advantages\nof this metric are given including the fact that it is continuous,\nparameter-less and comprehensive. \n\n"}
{"id": "1805.04384", "contents": "Title: Exploiting Images for Video Recognition with Hierarchical Generative\n  Adversarial Networks Abstract: Existing deep learning methods of video recognition usually require a large\nnumber of labeled videos for training. But for a new task, videos are often\nunlabeled and it is also time-consuming and labor-intensive to annotate them.\nInstead of human annotation, we try to make use of existing fully labeled\nimages to help recognize those videos. However, due to the problem of domain\nshifts and heterogeneous feature representations, the performance of\nclassifiers trained on images may be dramatically degraded for video\nrecognition tasks. In this paper, we propose a novel method, called\nHierarchical Generative Adversarial Networks (HiGAN), to enhance recognition in\nvideos (i.e., target domain) by transferring knowledge from images (i.e.,\nsource domain). The HiGAN model consists of a \\emph{low-level} conditional GAN\nand a \\emph{high-level} conditional GAN. By taking advantage of these two-level\nadversarial learning, our method is capable of learning a domain-invariant\nfeature representation of source images and target videos. Comprehensive\nexperiments on two challenging video recognition datasets (i.e. UCF101 and\nHMDB51) demonstrate the effectiveness of the proposed method when compared with\nthe existing state-of-the-art domain adaptation methods. \n\n"}
{"id": "1805.04582", "contents": "Title: TensOrMachine: Probabilistic Boolean Tensor Decomposition Abstract: Boolean tensor decomposition approximates data of multi-way binary\nrelationships as product of interpretable low-rank binary factors, following\nthe rules of Boolean algebra. Here, we present its first probabilistic\ntreatment. We facilitate scalable sampling-based posterior inference by\nexploitation of the combinatorial structure of the factor conditionals. Maximum\na posteriori decompositions feature higher accuracies than existing techniques\nthroughout a wide range of simulated conditions. Moreover, the probabilistic\napproach facilitates the treatment of missing data and enables model selection\nwith much greater accuracy. We investigate three real-world data-sets. First,\ntemporal interaction networks in a hospital ward and behavioural data of\nuniversity students demonstrate the inference of instructive latent patterns.\nNext, we decompose a tensor with more than 10 billion data points, indicating\nrelations of gene expression in cancer patients. Not only does this demonstrate\nscalability, it also provides an entirely novel perspective on relational\nproperties of continuous data and, in the present example, on the molecular\nheterogeneity of cancer. Our implementation is available on GitHub:\nhttps://github.com/TammoR/LogicalFactorisationMachines. \n\n"}
{"id": "1805.04680", "contents": "Title: AdvEntuRe: Adversarial Training for Textual Entailment with\n  Knowledge-Guided Examples Abstract: We consider the problem of learning textual entailment models with limited\nsupervision (5K-10K training examples), and present two complementary\napproaches for it. First, we propose knowledge-guided adversarial example\ngenerators for incorporating large lexical resources in entailment models via\nonly a handful of rule templates. Second, to make the entailment model - a\ndiscriminator - more robust, we propose the first GAN-style approach for\ntraining it using a natural language example generator that iteratively adjusts\nbased on the discriminator's performance. We demonstrate effectiveness using\ntwo entailment datasets, where the proposed methods increase accuracy by 4.7%\non SciTail and by 2.8% on a 1% training sub-sample of SNLI. Notably, even a\nsingle hand-written rule, negate, improves the accuracy on the negation\nexamples in SNLI by 6.1%. \n\n"}
{"id": "1805.05086", "contents": "Title: Unsupervised Intuitive Physics from Visual Observations Abstract: While learning models of intuitive physics is an increasingly active area of\nresearch, current approaches still fall short of natural intelligences in one\nimportant regard: they require external supervision, such as explicit access to\nphysical states, at training and sometimes even at test times. Some authors\nhave relaxed such requirements by supplementing the model with an handcrafted\nphysical simulator. Still, the resulting methods are unable to automatically\nlearn new complex environments and to understand physical interactions within\nthem. In this work, we demonstrated for the first time learning such predictors\ndirectly from raw visual observations and without relying on simulators. We do\nso in two steps: first, we learn to track mechanically-salient objects in\nvideos using causality and equivariance, two unsupervised learning principles\nthat do not require auto-encoding. Second, we demonstrate that the extracted\npositions are sufficient to successfully train visual motion predictors that\ncan take the underlying environment into account. We validate our predictors on\nsynthetic datasets; then, we introduce a new dataset, ROLL4REAL, consisting of\nreal objects rolling on complex terrains (pool table, elliptical bowl, and\nrandom height-field). We show that in all such cases it is possible to learn\nreliable extrapolators of the object trajectories from raw videos alone,\nwithout any form of external supervision and with no more prior knowledge than\nthe choice of a convolutional neural network architecture. \n\n"}
{"id": "1805.05132", "contents": "Title: Exploiting the Value of the Center-dark Channel Prior for Salient Object\n  Detection Abstract: Saliency detection aims to detect the most attractive objects in images and\nis widely used as a foundation for various applications. In this paper, we\npropose a novel salient object detection algorithm for RGB-D images using\ncenter-dark channel priors. First, we generate an initial saliency map based on\na color saliency map and a depth saliency map of a given RGB-D image. Then, we\ngenerate a center-dark channel map based on center saliency and dark channel\npriors. Finally, we fuse the initial saliency map with the center dark channel\nmap to generate the final saliency map. Extensive evaluations over four\nbenchmark datasets demonstrate that our proposed method performs favorably\nagainst most of the state-of-the-art approaches. Besides, we further discuss\nthe application of the proposed algorithm in small target detection and\ndemonstrate the universal value of center-dark channel priors in the field of\nobject detection. \n\n"}
{"id": "1805.05553", "contents": "Title: On Learning Associations of Faces and Voices Abstract: In this paper, we study the associations between human faces and voices.\nAudiovisual integration, specifically the integration of facial and vocal\ninformation is a well-researched area in neuroscience. It is shown that the\noverlapping information between the two modalities plays a significant role in\nperceptual tasks such as speaker identification. Through an online study on a\nnew dataset we created, we confirm previous findings that people can associate\nunseen faces with corresponding voices and vice versa with greater than chance\naccuracy. We computationally model the overlapping information between faces\nand voices and show that the learned cross-modal representation contains enough\ninformation to identify matching faces and voices with performance similar to\nthat of humans. Our representation exhibits correlations to certain demographic\nattributes and features obtained from either visual or aural modality alone. We\nrelease our dataset of audiovisual recordings and demographic annotations of\npeople reading out short text used in our studies. \n\n"}
{"id": "1805.05563", "contents": "Title: Facial Landmark Detection: a Literature Survey Abstract: The locations of the fiducial facial landmark points around facial components\nand facial contour capture the rigid and non-rigid facial deformations due to\nhead movements and facial expressions. They are hence important for various\nfacial analysis tasks. Many facial landmark detection algorithms have been\ndeveloped to automatically detect those key points over the years, and in this\npaper, we perform an extensive review of them. We classify the facial landmark\ndetection algorithms into three major categories: holistic methods, Constrained\nLocal Model (CLM) methods, and the regression-based methods. They differ in the\nways to utilize the facial appearance and shape information. The holistic\nmethods explicitly build models to represent the global facial appearance and\nshape information. The CLMs explicitly leverage the global shape model but\nbuild the local appearance models. The regression-based methods implicitly\ncapture facial shape and appearance information. For algorithms within each\ncategory, we discuss their underlying theories as well as their differences. We\nalso compare their performances on both controlled and in the wild benchmark\ndatasets, under varying facial expressions, head poses, and occlusion. Based on\nthe evaluations, we point out their respective strengths and weaknesses. There\nis also a separate section to review the latest deep learning-based algorithms.\n  The survey also includes a listing of the benchmark databases and existing\nsoftware. Finally, we identify future research directions, including combining\nmethods in different categories to leverage their respective strengths to solve\nlandmark detection \"in-the-wild\". \n\n"}
{"id": "1805.06173", "contents": "Title: Lightweight Pyramid Networks for Image Deraining Abstract: Existing deep convolutional neural networks have found major success in image\nderaining, but at the expense of an enormous number of parameters. This limits\ntheir potential application, for example in mobile devices. In this paper, we\npropose a lightweight pyramid of networks (LPNet) for single image deraining.\nInstead of designing a complex network structures, we use domain-specific\nknowledge to simplify the learning process. Specifically, we find that by\nintroducing the mature Gaussian-Laplacian image pyramid decomposition\ntechnology to the neural network, the learning problem at each pyramid level is\ngreatly simplified and can be handled by a relatively shallow network with few\nparameters. We adopt recursive and residual network structures to build the\nproposed LPNet, which has less than 8K parameters while still achieving\nstate-of-the-art performance on rain removal. We also discuss the potential\nvalue of LPNet for other low- and high-level vision tasks. \n\n"}
{"id": "1805.06502", "contents": "Title: First Experiments with Neural Translation of Informal to Formal\n  Mathematics Abstract: We report on our experiments to train deep neural networks that automatically\ntranslate informalized LaTeX-written Mizar texts into the formal Mizar\nlanguage. To the best of our knowledge, this is the first time when neural\nnetworks have been adopted in the formalization of mathematics. Using Luong et\nal.'s neural machine translation model (NMT), we tested our aligned\ninformal-formal corpora against various hyperparameters and evaluated their\nresults. Our experiments show that our best performing model configurations are\nable to generate correct Mizar statements on 65.73\\% of the inference data,\nwith the union of all models covering 79.17\\%. These results indicate that\nformalization through artificial neural network is a promising approach for\nautomated formalization of mathematics. We present several case studies to\nillustrate our results. \n\n"}
{"id": "1805.06561", "contents": "Title: DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images Abstract: We present the DeepGlobe 2018 Satellite Image Understanding Challenge, which\nincludes three public competitions for segmentation, detection, and\nclassification tasks on satellite images. Similar to other challenges in\ncomputer vision domain such as DAVIS and COCO, DeepGlobe proposes three\ndatasets and corresponding evaluation methodologies, coherently bundled in\nthree competitions with a dedicated workshop co-located with CVPR 2018.\n  We observed that satellite imagery is a rich and structured source of\ninformation, yet it is less investigated than everyday images by computer\nvision researchers. However, bridging modern computer vision with remote\nsensing data analysis could have critical impact to the way we understand our\nenvironment and lead to major breakthroughs in global urban planning or climate\nchange research. Keeping such bridging objective in mind, DeepGlobe aims to\nbring together researchers from different domains to raise awareness of remote\nsensing in the computer vision community and vice-versa. We aim to improve and\nevaluate state-of-the-art satellite image understanding approaches, which can\nhopefully serve as reference benchmarks for future research in the same topic.\nIn this paper, we analyze characteristics of each dataset, define the\nevaluation criteria of the competitions, and provide baselines for each task. \n\n"}
{"id": "1805.06771", "contents": "Title: Convolutional Social Pooling for Vehicle Trajectory Prediction Abstract: Forecasting the motion of surrounding vehicles is a critical ability for an\nautonomous vehicle deployed in complex traffic. Motion of all vehicles in a\nscene is governed by the traffic context, i.e., the motion and relative spatial\nconfiguration of neighboring vehicles. In this paper we propose an LSTM\nencoder-decoder model that uses convolutional social pooling as an improvement\nto social pooling layers for robustly learning interdependencies in vehicle\nmotion. Additionally, our model outputs a multi-modal predictive distribution\nover future trajectories based on maneuver classes. We evaluate our model using\nthe publicly available NGSIM US-101 and I-80 datasets. Our results show\nimprovement over the state of the art in terms of RMS values of prediction\nerror and negative log-likelihoods of true future trajectories under the\nmodel's predictive distribution. We also present a qualitative analysis of the\nmodel's predicted distributions for various traffic scenarios. \n\n"}
{"id": "1805.07473", "contents": "Title: Progressive Ensemble Networks for Zero-Shot Recognition Abstract: Despite the advancement of supervised image recognition algorithms, their\ndependence on the availability of labeled data and the rapid expansion of image\ncategories raise the significant challenge of zero-shot learning. Zero-shot\nlearning (ZSL) aims to transfer knowledge from labeled classes into unlabeled\nclasses to reduce human labeling effort. In this paper, we propose a novel\nprogressive ensemble network model with multiple projected label embeddings to\naddress zero-shot image recognition. The ensemble network is built by learning\nmultiple image classification functions with a shared feature extraction\nnetwork but different label embedding representations, which enhance the\ndiversity of the classifiers and facilitate information transfer to unlabeled\nclasses. A progressive training framework is then deployed to gradually label\nthe most confident images in each unlabeled class with predicted pseudo-labels\nand update the ensemble network with the training data augmented by the\npseudo-labels. The proposed model performs training on both labeled and\nunlabeled data. It can naturally bridge the domain shift problem in visual\nappearances and be extended to the generalized zero-shot learning scenario. We\nconduct experiments on multiple ZSL datasets and the empirical results\ndemonstrate the efficacy of the proposed model. \n\n"}
{"id": "1805.07566", "contents": "Title: Wildest Faces: Face Detection and Recognition in Violent Settings Abstract: With the introduction of large-scale datasets and deep learning models\ncapable of learning complex representations, impressive advances have emerged\nin face detection and recognition tasks. Despite such advances, existing\ndatasets do not capture the difficulty of face recognition in the wildest\nscenarios, such as hostile disputes or fights. Furthermore, existing datasets\ndo not represent completely unconstrained cases of low resolution, high blur\nand large pose/occlusion variances. To this end, we introduce the Wildest Faces\ndataset, which focuses on such adverse effects through violent scenes. The\ndataset consists of an extensive set of violent scenes of celebrities from\nmovies. Our experimental results demonstrate that state-of-the-art techniques\nare not well-suited for violent scenes, and therefore, Wildest Faces is likely\nto stir further interest in face detection and recognition research. \n\n"}
{"id": "1805.07621", "contents": "Title: CapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule\n  Subspaces Abstract: In this paper, we formalize the idea behind capsule nets of using a capsule\nvector rather than a neuron activation to predict the label of samples. To this\nend, we propose to learn a group of capsule subspaces onto which an input\nfeature vector is projected. Then the lengths of resultant capsules are used to\nscore the probability of belonging to different classes. We train such a\nCapsule Projection Network (CapProNet) by learning an orthogonal projection\nmatrix for each capsule subspace, and show that each capsule subspace is\nupdated until it contains input feature vectors corresponding to the associated\nclass. We will also show that the capsule projection can be viewed as\nnormalizing the multiple columns of the weight matrix simultaneously to form an\northogonal basis, which makes it more effective in incorporating novel\ncomponents of input features to update capsule representations. In other words,\nthe capsule projection can be viewed as a multi-dimensional weight\nnormalization in capsule subspaces, where the conventional weight normalization\nis simply a special case of the capsule projection onto 1D lines. Only a small\nnegligible computing overhead is incurred to train the network in\nlow-dimensional capsule subspaces or through an alternative hyper-power\niteration to estimate the normalization matrix. Experiment results on image\ndatasets show the presented model can greatly improve the performance of the\nstate-of-the-art ResNet backbones by $10-20\\%$ and that of the Densenet by\n$5-7\\%$ respectively at the same level of computing and memory expenses. The\nCapProNet establishes the competitive state-of-the-art performance for the\nfamily of capsule nets by significantly reducing test errors on the benchmark\ndatasets. \n\n"}
{"id": "1805.08000", "contents": "Title: Adversarial Noise Layer: Regularize Neural Network By Adding Noise Abstract: In this paper, we introduce a novel regularization method called Adversarial\nNoise Layer (ANL) and its efficient version called Class Adversarial Noise\nLayer (CANL), which are able to significantly improve CNN's generalization\nability by adding carefully crafted noise into the intermediate layer\nactivations. ANL and CANL can be easily implemented and integrated with most of\nthe mainstream CNN-based models. We compared the effects of the different types\nof noise and visually demonstrate that our proposed adversarial noise instruct\nCNN models to learn to extract cleaner feature maps, which further reduce the\nrisk of over-fitting. We also conclude that models trained with ANL or CANL are\nmore robust to the adversarial examples generated by FGSM than the traditional\nadversarial training approaches. \n\n"}
{"id": "1805.08324", "contents": "Title: Measurement-wise Occlusion in Multi-object Tracking Abstract: Handling object interaction is a fundamental challenge in practical\nmulti-object tracking, even for simple interactive effects such as one object\ntemporarily occluding another. We formalize the problem of occlusion in\ntracking with two different abstractions. In object-wise occlusion, objects\nthat are occluded by other objects do not generate measurements. In\nmeasurement-wise occlusion, a previously unstudied approach, all objects may\ngenerate measurements but some measurements may be occluded by others. While\nthe relative validity of each abstraction depends on the situation and sensor,\nmeasurement-wise occlusion fits into probabilistic multi-object tracking\nalgorithms with much looser assumptions on object interaction. Its value is\ndemonstrated by showing that it naturally derives a popular approximation for\nlidar tracking, and by an example of visual tracking in image space. \n\n"}
{"id": "1805.08340", "contents": "Title: Reducing Parameter Space for Neural Network Training Abstract: For neural networks (NNs) with rectified linear unit (ReLU) or binary\nactivation functions, we show that their training can be accomplished in a\nreduced parameter space. Specifically, the weights in each neuron can be\ntrained on the unit sphere, as opposed to the entire space, and the threshold\ncan be trained in a bounded interval, as opposed to the real line. We show that\nthe NNs in the reduced parameter space are mathematically equivalent to the\nstandard NNs with parameters in the whole space. The reduced parameter space\nshall facilitate the optimization procedure for the network training, as the\nsearch space becomes (much) smaller. We demonstrate the improved training\nperformance using numerical examples. \n\n"}
{"id": "1805.08522", "contents": "Title: Deep learning generalizes because the parameter-function map is biased\n  towards simple functions Abstract: Deep neural networks (DNNs) generalize remarkably well without explicit\nregularization even in the strongly over-parametrized regime where classical\nlearning theory would instead predict that they would severely overfit. While\nmany proposals for some kind of implicit regularization have been made to\nrationalise this success, there is no consensus for the fundamental reason why\nDNNs do not strongly overfit. In this paper, we provide a new explanation. By\napplying a very general probability-complexity bound recently derived from\nalgorithmic information theory (AIT), we argue that the parameter-function map\nof many DNNs should be exponentially biased towards simple functions. We then\nprovide clear evidence for this strong simplicity bias in a model DNN for\nBoolean functions, as well as in much larger fully connected and convolutional\nnetworks applied to CIFAR10 and MNIST. As the target functions in many real\nproblems are expected to be highly structured, this intrinsic simplicity bias\nhelps explain why deep networks generalize well on real world problems. This\npicture also facilitates a novel PAC-Bayes approach where the prior is taken\nover the DNN input-output function space, rather than the more conventional\nprior over parameter space. If we assume that the training algorithm samples\nparameters close to uniformly within the zero-error region then the PAC-Bayes\ntheorem can be used to guarantee good expected generalization for target\nfunctions producing high-likelihood training sets. By exploiting recently\ndiscovered connections between DNNs and Gaussian processes to estimate the\nmarginal likelihood, we produce relatively tight generalization PAC-Bayes error\nbounds which correlate well with the true error on realistic datasets such as\nMNIST and CIFAR10 and for architectures including convolutional and fully\nconnected networks. \n\n"}
{"id": "1805.08657", "contents": "Title: Robust Conditional Generative Adversarial Networks Abstract: Conditional generative adversarial networks (cGAN) have led to large\nimprovements in the task of conditional image generation, which lies at the\nheart of computer vision. The major focus so far has been on performance\nimprovement, while there has been little effort in making cGAN more robust to\nnoise. The regression (of the generator) might lead to arbitrarily large errors\nin the output, which makes cGAN unreliable for real-world applications. In this\nwork, we introduce a novel conditional GAN model, called RoCGAN, which\nleverages structure in the target space of the model to address the issue. Our\nmodel augments the generator with an unsupervised pathway, which promotes the\noutputs of the generator to span the target manifold even in the presence of\nintense noise. We prove that RoCGAN share similar theoretical properties as GAN\nand experimentally verify that our model outperforms existing state-of-the-art\ncGAN architectures by a large margin in a variety of domains including images\nfrom natural scenes and faces. \n\n"}
{"id": "1805.08805", "contents": "Title: Resource Aware Person Re-identification across Multiple Resolutions Abstract: Not all people are equally easy to identify: color statistics might be enough\nfor some cases while others might require careful reasoning about high- and\nlow-level details. However, prevailing person re-identification(re-ID) methods\nuse one-size-fits-all high-level embeddings from deep convolutional networks\nfor all cases. This might limit their accuracy on difficult examples or makes\nthem needlessly expensive for the easy ones. To remedy this, we present a new\nperson re-ID model that combines effective embeddings built on multiple\nconvolutional network layers, trained with deep-supervision. On traditional\nre-ID benchmarks, our method improves substantially over the previous\nstate-of-the-art results on all five datasets that we evaluate on. We then\npropose two new formulations of the person re-ID problem under\nresource-constraints, and show how our model can be used to effectively trade\noff accuracy and computation in the presence of resource constraints. Code and\npre-trained models are available at https://github.com/mileyan/DARENet. \n\n"}
{"id": "1805.08913", "contents": "Title: Amortized Inference Regularization Abstract: The variational autoencoder (VAE) is a popular model for density estimation\nand representation learning. Canonically, the variational principle suggests to\nprefer an expressive inference model so that the variational approximation is\naccurate. However, it is often overlooked that an overly-expressive inference\nmodel can be detrimental to the test set performance of both the amortized\nposterior approximator and, more importantly, the generative density estimator.\nIn this paper, we leverage the fact that VAEs rely on amortized inference and\npropose techniques for amortized inference regularization (AIR) that control\nthe smoothness of the inference model. We demonstrate that, by applying AIR, it\nis possible to improve VAE generalization on both inference and generative\nperformance. Our paper challenges the belief that amortized inference is simply\na mechanism for approximating maximum likelihood training and illustrates that\nregularization of the amortization family provides a new direction for\nunderstanding and improving generalization in VAEs. \n\n"}
{"id": "1805.08974", "contents": "Title: Do Better ImageNet Models Transfer Better? Abstract: Transfer learning is a cornerstone of computer vision, yet little work has\nbeen done to evaluate the relationship between architecture and transfer. An\nimplicit hypothesis in modern computer vision research is that models that\nperform better on ImageNet necessarily perform better on other vision tasks.\nHowever, this hypothesis has never been systematically tested. Here, we compare\nthe performance of 16 classification networks on 12 image classification\ndatasets. We find that, when networks are used as fixed feature extractors or\nfine-tuned, there is a strong correlation between ImageNet accuracy and\ntransfer accuracy ($r = 0.99$ and $0.96$, respectively). In the former setting,\nwe find that this relationship is very sensitive to the way in which networks\nare trained on ImageNet; many common forms of regularization slightly improve\nImageNet accuracy but yield penultimate layer features that are much worse for\ntransfer learning. Additionally, we find that, on two small fine-grained image\nclassification datasets, pretraining on ImageNet provides minimal benefits,\nindicating the learned features from ImageNet do not transfer well to\nfine-grained tasks. Together, our results show that ImageNet architectures\ngeneralize well across datasets, but ImageNet features are less general than\npreviously suggested. \n\n"}
{"id": "1805.09097", "contents": "Title: Image Restoration by Estimating Frequency Distribution of Local Patches Abstract: In this paper, we propose a method to solve the image restoration problem,\nwhich tries to restore the details of a corrupted image, especially due to the\nloss caused by JPEG compression. We have treated an image in the frequency\ndomain to explicitly restore the frequency components lost during image\ncompression. In doing so, the distribution in the frequency domain is learned\nusing the cross entropy loss. Unlike recent approaches, we have reconstructed\nthe details of an image without using the scheme of adversarial training.\nRather, the image restoration problem is treated as a classification problem to\ndetermine the frequency coefficient for each frequency band in an image patch.\nIn this paper, we show that the proposed method effectively restores a\nJPEG-compressed image with more detailed high frequency components, making the\nrestored image more vivid. \n\n"}
{"id": "1805.09313", "contents": "Title: End-to-End Speech-Driven Facial Animation with Temporal GANs Abstract: Speech-driven facial animation is the process which uses speech signals to\nautomatically synthesize a talking character. The majority of work in this\ndomain creates a mapping from audio features to visual features. This often\nrequires post-processing using computer graphics techniques to produce\nrealistic albeit subject dependent results. We present a system for generating\nvideos of a talking head, using a still image of a person and an audio clip\ncontaining speech, that doesn't rely on any handcrafted intermediate features.\nTo the best of our knowledge, this is the first method capable of generating\nsubject independent realistic videos directly from raw audio. Our method can\ngenerate videos which have (a) lip movements that are in sync with the audio\nand (b) natural facial expressions such as blinks and eyebrow movements. We\nachieve this by using a temporal GAN with 2 discriminators, which are capable\nof capturing different aspects of the video. The effect of each component in\nour system is quantified through an ablation study. The generated videos are\nevaluated based on their sharpness, reconstruction quality, and lip-reading\naccuracy. Finally, a user study is conducted, confirming that temporal GANs\nlead to more natural sequences than a static GAN-based approach. \n\n"}
{"id": "1805.09791", "contents": "Title: Multi-Task Zipping via Layer-wise Neuron Sharing Abstract: Future mobile devices are anticipated to perceive, understand and react to\nthe world on their own by running multiple correlated deep neural networks\non-device. Yet the complexity of these neural networks needs to be trimmed down\nboth within-model and cross-model to fit in mobile storage and memory. Previous\nstudies focus on squeezing the redundancy within a single neural network. In\nthis work, we aim to reduce the redundancy across multiple models. We propose\nMulti-Task Zipping (MTZ), a framework to automatically merge correlated,\npre-trained deep neural networks for cross-model compression. Central in MTZ is\na layer-wise neuron sharing and incoming weight updating scheme that induces a\nminimal change in the error function. MTZ inherits information from each model\nand demands light retraining to re-boost the accuracy of individual tasks.\nEvaluations show that MTZ is able to fully merge the hidden layers of two\nVGG-16 networks with a 3.18% increase in the test error averaged on ImageNet\nand CelebA, or share 39.61% parameters between the two networks with <0.5%\nincrease in the test errors for both tasks. The number of iterations to retrain\nthe combined network is at least 17.8 times lower than that of training a\nsingle VGG-16 network. Moreover, experiments show that MTZ is also able to\neffectively merge multiple residual networks. \n\n"}
{"id": "1805.12081", "contents": "Title: CuisineNet: Food Attributes Classification using Multi-scale Convolution\n  Network Abstract: Diversity of food and its attributes represents the culinary habits of\npeoples from different countries. Thus, this paper addresses the problem of\nidentifying food culture of people around the world and its flavor by\nclassifying two main food attributes, cuisine and flavor. A deep learning model\nbased on multi-scale convotuional networks is proposed for extracting more\naccurate features from input images. The aggregation of multi-scale convolution\nlayers with different kernel size is also used for weighting the features\nresults from different scales. In addition, a joint loss function based on\nNegative Log Likelihood (NLL) is used to fit the model probability to multi\nlabeled classes for multi-modal classification task. Furthermore, this work\nprovides a new dataset for food attributes, so-called Yummly48K, extracted from\nthe popular food website, Yummly. Our model is assessed on the constructed\nYummly48K dataset. The experimental results show that our proposed method\nyields 65% and 62% average F1 score on validation and test set which\noutperforming the state-of-the-art models. \n\n"}
{"id": "1805.12487", "contents": "Title: Sequential Attacks on Agents for Long-Term Adversarial Goals Abstract: Reinforcement learning (RL) has advanced greatly in the past few years with\nthe employment of effective deep neural networks (DNNs) on the policy networks.\nWith the great effectiveness came serious vulnerability issues with DNNs that\nsmall adversarial perturbations on the input can change the output of the\nnetwork. Several works have pointed out that learned agents with a DNN policy\nnetwork can be manipulated against achieving the original task through a\nsequence of small perturbations on the input states. In this paper, we\ndemonstrate furthermore that it is also possible to impose an arbitrary\nadversarial reward on the victim policy network through a sequence of attacks.\nOur method involves the latest adversarial attack technique, Adversarial\nTransformer Network (ATN), that learns to generate the attack and is easy to\nintegrate into the policy network. As a result of our attack, the victim agent\nis misguided to optimise for the adversarial reward over time. Our results\nexpose serious security threats for RL applications in safety-critical systems\nincluding drones, medical analysis, and self-driving cars. \n\n"}
{"id": "1806.00578", "contents": "Title: SCAN: Sliding Convolutional Attention Network for Scene Text Recognition Abstract: Scene text recognition has drawn great attentions in the community of\ncomputer vision and artificial intelligence due to its challenges and wide\napplications. State-of-the-art recurrent neural networks (RNN) based models map\nan input sequence to a variable length output sequence, but are usually applied\nin a black box manner and lack of transparency for further improvement, and the\nmaintaining of the entire past hidden states prevents parallel computation in a\nsequence. In this paper, we investigate the intrinsic characteristics of text\nrecognition, and inspired by human cognition mechanisms in reading texts, we\npropose a scene text recognition method with sliding convolutional attention\nnetwork (SCAN). Similar to the eye movement during reading, the process of SCAN\ncan be viewed as an alternation between saccades and visual fixations. Compared\nto the previous recurrent models, computations over all elements of SCAN can be\nfully parallelized during training. Experimental results on several challenging\nbenchmarks, including the IIIT5k, SVT and ICDAR 2003/2013 datasets, demonstrate\nthe superiority of SCAN over state-of-the-art methods in terms of both the\nmodel interpretability and performance. \n\n"}
{"id": "1806.00672", "contents": "Title: Optimal Clustering under Uncertainty Abstract: Classical clustering algorithms typically either lack an underlying\nprobability framework to make them predictive or focus on parameter estimation\nrather than defining and minimizing a notion of error. Recent work addresses\nthese issues by developing a probabilistic framework based on the theory of\nrandom labeled point processes and characterizing a Bayes clusterer that\nminimizes the number of misclustered points. The Bayes clusterer is analogous\nto the Bayes classifier. Whereas determining a Bayes classifier requires full\nknowledge of the feature-label distribution, deriving a Bayes clusterer\nrequires full knowledge of the point process. When uncertain of the point\nprocess, one would like to find a robust clusterer that is optimal over the\nuncertainty, just as one may find optimal robust classifiers with uncertain\nfeature-label distributions. Herein, we derive an optimal robust clusterer by\nfirst finding an effective random point process that incorporates all\nrandomness within its own probabilistic structure and from which a Bayes\nclusterer can be derived that provides an optimal robust clusterer relative to\nthe uncertainty. This is analogous to the use of effective class-conditional\ndistributions in robust classification. After evaluating the performance of\nrobust clusterers in synthetic mixtures of Gaussians models, we apply the\nframework to granular imaging, where we make use of the asymptotic\ngranulometric moment theory for granular images to relate robust clustering\ntheory to the application. \n\n"}
{"id": "1806.01261", "contents": "Title: Relational inductive biases, deep learning, and graph networks Abstract: Artificial intelligence (AI) has undergone a renaissance recently, making\nmajor progress in key domains such as vision, language, control, and\ndecision-making. This has been due, in part, to cheap data and cheap compute\nresources, which have fit the natural strengths of deep learning. However, many\ndefining characteristics of human intelligence, which developed under much\ndifferent pressures, remain out of reach for current approaches. In particular,\ngeneralizing beyond one's experiences--a hallmark of human intelligence from\ninfancy--remains a formidable challenge for modern AI.\n  The following is part position paper, part review, and part unification. We\nargue that combinatorial generalization must be a top priority for AI to\nachieve human-like abilities, and that structured representations and\ncomputations are key to realizing this objective. Just as biology uses nature\nand nurture cooperatively, we reject the false choice between\n\"hand-engineering\" and \"end-to-end\" learning, and instead advocate for an\napproach which benefits from their complementary strengths. We explore how\nusing relational inductive biases within deep learning architectures can\nfacilitate learning about entities, relations, and rules for composing them. We\npresent a new building block for the AI toolkit with a strong relational\ninductive bias--the graph network--which generalizes and extends various\napproaches for neural networks that operate on graphs, and provides a\nstraightforward interface for manipulating structured knowledge and producing\nstructured behaviors. We discuss how graph networks can support relational\nreasoning and combinatorial generalization, laying the foundation for more\nsophisticated, interpretable, and flexible patterns of reasoning. As a\ncompanion to this paper, we have released an open-source software library for\nbuilding graph networks, with demonstrations of how to use them in practice. \n\n"}
{"id": "1806.01673", "contents": "Title: Recurrent Convolutional Fusion for RGB-D Object Recognition Abstract: Providing machines with the ability to recognize objects like humans has\nalways been one of the primary goals of machine vision. The introduction of\nRGB-D cameras has paved the way for a significant leap forward in this\ndirection thanks to the rich information provided by these sensors. However,\nthe machine vision community still lacks an effective method to synergically\nuse the RGB and depth data to improve object recognition. In order to take a\nstep in this direction, we introduce a novel end-to-end architecture for RGB-D\nobject recognition called recurrent convolutional fusion (RCFusion). Our method\ngenerates compact and highly discriminative multi-modal features by combining\ncomplementary RGB and depth information representing different levels of\nabstraction. Extensive experiments on two popular datasets, RGB-D Object\nDataset and JHUIT-50, show that RCFusion significantly outperforms\nstate-of-the-art approaches in both the object categorization and instance\nrecognition tasks. \n\n"}
{"id": "1806.01794", "contents": "Title: Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects Abstract: We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep\ngenerative model for videos of moving objects. It can reliably discover and\ntrack objects throughout the sequence of frames, and can also generate future\nframes conditioning on the current frame, thereby simulating expected motion of\nobjects. This is achieved by explicitly encoding object presence, locations and\nappearances in the latent variables of the model. SQAIR retains all strengths\nof its predecessor, Attend, Infer, Repeat (AIR, Eslami et. al., 2016),\nincluding learning in an unsupervised manner, and addresses its shortcomings.\nWe use a moving multi-MNIST dataset to show limitations of AIR in detecting\noverlapping or partially occluded objects, and show how SQAIR overcomes them by\nleveraging temporal consistency of objects. Finally, we also apply SQAIR to\nreal-world pedestrian CCTV data, where it learns to reliably detect, track and\ngenerate walking pedestrians with no supervision. \n\n"}
{"id": "1806.02664", "contents": "Title: Probabilistic AND-OR Attribute Grouping for Zero-Shot Learning Abstract: In zero-shot learning (ZSL), a classifier is trained to recognize visual\nclasses without any image samples. Instead, it is given semantic information\nabout the class, like a textual description or a set of attributes. Learning\nfrom attributes could benefit from explicitly modeling structure of the\nattribute space. Unfortunately, learning of general structure from empirical\nsamples is hard with typical dataset sizes.\n  Here we describe LAGO, a probabilistic model designed to capture natural soft\nand-or relations across groups of attributes. We show how this model can be\nlearned end-to-end with a deep attribute-detection model. The soft group\nstructure can be learned from data jointly as part of the model, and can also\nreadily incorporate prior knowledge about groups if available. The soft and-or\nstructure succeeds to capture meaningful and predictive structures, improving\nthe accuracy of zero-shot learning on two of three benchmarks.\n  Finally, LAGO reveals a unified formulation over two ZSL approaches: DAP\n(Lampert et al., 2009) and ESZSL (Romera-Paredes & Torr, 2015). Interestingly,\ntaking only one singleton group for each attribute, introduces a new\nsoft-relaxation of DAP, that outperforms DAP by ~40. \n\n"}
{"id": "1806.02919", "contents": "Title: Non-Local Recurrent Network for Image Restoration Abstract: Many classic methods have shown non-local self-similarity in natural images\nto be an effective prior for image restoration. However, it remains unclear and\nchallenging to make use of this intrinsic property via deep networks. In this\npaper, we propose a non-local recurrent network (NLRN) as the first attempt to\nincorporate non-local operations into a recurrent neural network (RNN) for\nimage restoration. The main contributions of this work are: (1) Unlike existing\nmethods that measure self-similarity in an isolated manner, the proposed\nnon-local module can be flexibly integrated into existing deep networks for\nend-to-end training to capture deep feature correlation between each location\nand its neighborhood. (2) We fully employ the RNN structure for its parameter\nefficiency and allow deep feature correlation to be propagated along adjacent\nrecurrent states. This new design boosts robustness against inaccurate\ncorrelation estimation due to severely degraded images. (3) We show that it is\nessential to maintain a confined neighborhood for computing deep feature\ncorrelation given degraded images. This is in contrast to existing practice\nthat deploys the whole image. Extensive experiments on both image denoising and\nsuper-resolution tasks are conducted. Thanks to the recurrent non-local\noperations and correlation propagation, the proposed NLRN achieves superior\nresults to state-of-the-art methods with much fewer parameters. \n\n"}
{"id": "1806.03265", "contents": "Title: PatchFCN for Intracranial Hemorrhage Detection Abstract: This paper studies the problem of detecting and segmenting acute intracranial\nhemorrhage on head computed tomography (CT) scans. We propose to solve both\ntasks as a semantic segmentation problem using a patch-based fully\nconvolutional network (PatchFCN). This formulation allows us to accurately\nlocalize hemorrhages while bypassing the complexity of object detection. Our\nsystem demonstrates competitive performance with a human expert and the\nstate-of-the-art on classification tasks (0.976, 0.966 AUC of ROC on\nretrospective and prospective test sets) and on segmentation tasks (0.785 pixel\nAP, 0.766 Dice score), while using much less data and a simpler system. In\naddition, we conduct a series of controlled experiments to understand \"why\"\nPatchFCN outperforms standard FCN. Our studies show that PatchFCN finds a good\ntrade-off between batch diversity and the amount of context during training.\nThese findings may also apply to other medical segmentation tasks. \n\n"}
{"id": "1806.03430", "contents": "Title: Efficient Optimization Algorithms for Robust Principal Component\n  Analysis and Its Variants Abstract: Robust PCA has drawn significant attention in the last decade due to its\nsuccess in numerous application domains, ranging from bio-informatics,\nstatistics, and machine learning to image and video processing in computer\nvision. Robust PCA and its variants such as sparse PCA and stable PCA can be\nformulated as optimization problems with exploitable special structures. Many\nspecialized efficient optimization methods have been proposed to solve robust\nPCA and related problems. In this paper we review existing optimization methods\nfor solving convex and nonconvex relaxations/variants of robust PCA, discuss\ntheir advantages and disadvantages, and elaborate on their convergence\nbehaviors. We also provide some insights for possible future research\ndirections including new algorithmic frameworks that might be suitable for\nimplementing on multi-processor setting to handle large-scale problems. \n\n"}
{"id": "1806.03852", "contents": "Title: Data augmentation instead of explicit regularization Abstract: Contrary to most machine learning models, modern deep artificial neural\nnetworks typically include multiple components that contribute to\nregularization. Despite the fact that some (explicit) regularization\ntechniques, such as weight decay and dropout, require costly fine-tuning of\nsensitive hyperparameters, the interplay between them and other elements that\nprovide implicit regularization is not well understood yet. Shedding light upon\nthese interactions is key to efficiently using computational resources and may\ncontribute to solving the puzzle of generalization in deep learning. Here, we\nfirst provide formal definitions of explicit and implicit regularization that\nhelp understand essential differences between techniques. Second, we contrast\ndata augmentation with weight decay and dropout. Our results show that visual\nobject categorization models trained with data augmentation alone achieve the\nsame performance or higher than models trained also with weight decay and\ndropout, as is common practice. We conclude that the contribution on\ngeneralization of weight decay and dropout is not only superfluous when\nsufficient implicit regularization is provided, but also such techniques can\ndramatically deteriorate the performance if the hyperparameters are not\ncarefully tuned for the architecture and data set. In contrast, data\naugmentation systematically provides large generalization gains and does not\nrequire hyperparameter re-tuning. In view of our results, we suggest to\noptimize neural networks without weight decay and dropout to save computational\nresources, hence carbon emissions, and focus more on data augmentation and\nother inductive biases to improve performance and robustness. \n\n"}
{"id": "1806.03994", "contents": "Title: Learning to Estimate Indoor Lighting from 3D Objects Abstract: In this work, we propose a step towards a more accurate prediction of the\nenvironment light given a single picture of a known object. To achieve this, we\ndeveloped a deep learning method that is able to encode the latent space of\nindoor lighting using few parameters and that is trained on a database of\nenvironment maps. This latent space is then used to generate predictions of the\nlight that are both more realistic and accurate than previous methods. To\nachieve this, our first contribution is a deep autoencoder which is capable of\nlearning the feature space that compactly models lighting. Our second\ncontribution is a convolutional neural network that predicts the light from a\nsingle image of a known object. To train these networks, our third contribution\nis a novel dataset that contains 21,000 HDR indoor environment maps. The\nresults indicate that the predictor can generate plausible lighting estimations\neven from diffuse objects. \n\n"}
{"id": "1806.04171", "contents": "Title: Synthetic Depth-of-Field with a Single-Camera Mobile Phone Abstract: Shallow depth-of-field is commonly used by photographers to isolate a subject\nfrom a distracting background. However, standard cell phone cameras cannot\nproduce such images optically, as their short focal lengths and small apertures\ncapture nearly all-in-focus images. We present a system to computationally\nsynthesize shallow depth-of-field images with a single mobile camera and a\nsingle button press. If the image is of a person, we use a person segmentation\nnetwork to separate the person and their accessories from the background. If\navailable, we also use dense dual-pixel auto-focus hardware, effectively a\n2-sample light field with an approximately 1 millimeter baseline, to compute a\ndense depth map. These two signals are combined and used to render a defocused\nimage. Our system can process a 5.4 megapixel image in 4 seconds on a mobile\nphone, is fully automatic, and is robust enough to be used by non-experts. The\nmodular nature of our system allows it to degrade naturally in the absence of a\ndual-pixel sensor or a human subject. \n\n"}
{"id": "1806.04360", "contents": "Title: MSplit LBI: Realizing Feature Selection and Dense Estimation\n  Simultaneously in Few-shot and Zero-shot Learning Abstract: It is one typical and general topic of learning a good embedding model to\nefficiently learn the representation coefficients between two spaces/subspaces.\nTo solve this task, $L_{1}$ regularization is widely used for the pursuit of\nfeature selection and avoiding overfitting, and yet the sparse estimation of\nfeatures in $L_{1}$ regularization may cause the underfitting of training data.\n$L_{2}$ regularization is also frequently used, but it is a biased estimator.\nIn this paper, we propose the idea that the features consist of three\northogonal parts, \\emph{namely} sparse strong signals, dense weak signals and\nrandom noise, in which both strong and weak signals contribute to the fitting\nof data. To facilitate such novel decomposition, \\emph{MSplit} LBI is for the\nfirst time proposed to realize feature selection and dense estimation\nsimultaneously. We provide theoretical and simulational verification that our\nmethod exceeds $L_{1}$ and $L_{2}$ regularization, and extensive experimental\nresults show that our method achieves state-of-the-art performance in the\nfew-shot and zero-shot learning. \n\n"}
{"id": "1806.05129", "contents": "Title: What Is It Like Down There? Generating Dense Ground-Level Views and\n  Image Features From Overhead Imagery Using Conditional Generative Adversarial\n  Networks Abstract: This paper investigates conditional generative adversarial networks (cGANs)\nto overcome a fundamental limitation of using geotagged media for geographic\ndiscovery, namely its sparse and uneven spatial distribution. We train a cGAN\nto generate ground-level views of a location given overhead imagery. We show\nthe \"fake\" ground-level images are natural looking and are structurally similar\nto the real images. More significantly, we show the generated images are\nrepresentative of the locations and that the representations learned by the\ncGANs are informative. In particular, we show that dense feature maps generated\nusing our framework are more effective for land-cover classification than\napproaches which spatially interpolate features extracted from sparse\nground-level images. To our knowledge, ours is the first work to use cGANs to\ngenerate ground-level views given overhead imagery and to explore the benefits\nof the learned representations. \n\n"}
{"id": "1806.05250", "contents": "Title: What About Applied Fairness? Abstract: Machine learning practitioners are often ambivalent about the ethical aspects\nof their products. We believe anything that gets us from that current state to\none in which our systems are achieving some degree of fairness is an\nimprovement that should be welcomed. This is true even when that progress does\nnot get us 100% of the way to the goal of \"complete\" fairness or perfectly\nalign with our personal belief on which measure of fairness is used. Some\nmeasure of fairness being built would still put us in a better position than\nthe status quo. Impediments to getting fairness and ethical concerns applied in\nreal applications, whether they are abstruse philosophical debates or technical\noverhead such as the introduction of ever more hyper-parameters, should be\navoided. In this paper we further elaborate on our argument for this viewpoint\nand its importance. \n\n"}
{"id": "1806.05452", "contents": "Title: Deep Generative Models in the Real-World: An Open Challenge from Medical\n  Imaging Abstract: Recent advances in deep learning led to novel generative modeling techniques\nthat achieve unprecedented quality in generated samples and performance in\nlearning complex distributions in imaging data. These new models in medical\nimage computing have important applications that form clinically relevant and\nvery challenging unsupervised learning problems. In this paper, we explore the\nfeasibility of using state-of-the-art auto-encoder-based deep generative\nmodels, such as variational and adversarial auto-encoders, for one such task:\nabnormality detection in medical imaging. We utilize typical, publicly\navailable datasets with brain scans from healthy subjects and patients with\nstroke lesions and brain tumors. We use the data from healthy subjects to train\ndifferent auto-encoder based models to learn the distribution of healthy images\nand detect pathologies as outliers. Models that can better learn the data\ndistribution should be able to detect outliers more accurately. We evaluate the\ndetection performance of deep generative models and compare them with non-deep\nlearning based approaches to provide a benchmark of the current state of\nresearch. We conclude that abnormality detection is a challenging task for deep\ngenerative models and large room exists for improvement. In order to facilitate\nfurther research, we aim to provide carefully pre-processed imaging data\navailable to the research community. \n\n"}
{"id": "1806.05510", "contents": "Title: ReConvNet: Video Object Segmentation with Spatio-Temporal Features\n  Modulation Abstract: We introduce ReConvNet, a recurrent convolutional architecture for\nsemi-supervised video object segmentation that is able to fast adapt its\nfeatures to focus on any specific object of interest at inference time.\nGeneralization to new objects never observed during training is known to be a\nhard task for supervised approaches that would need to be retrained. To tackle\nthis problem, we propose a more efficient solution that learns spatio-temporal\nfeatures self-adapting to the object of interest via conditional affine\ntransformations. This approach is simple, can be trained end-to-end and does\nnot necessarily require extra training steps at inference time. Our method\nshows competitive results on DAVIS2016 with respect to state-of-the art\napproaches that use online fine-tuning, and outperforms them on DAVIS2017.\nReConvNet shows also promising results on the DAVIS-Challenge 2018 winning the\n$10$-th position. \n\n"}
{"id": "1806.05594", "contents": "Title: There Are Many Consistent Explanations of Unlabeled Data: Why You Should\n  Average Abstract: Presently the most successful approaches to semi-supervised learning are\nbased on consistency regularization, whereby a model is trained to be robust to\nsmall perturbations of its inputs and parameters. To understand consistency\nregularization, we conceptually explore how loss geometry interacts with\ntraining procedures. The consistency loss dramatically improves generalization\nperformance over supervised-only training; however, we show that SGD struggles\nto converge on the consistency loss and continues to make large steps that lead\nto changes in predictions on the test data. Motivated by these observations, we\npropose to train consistency-based methods with Stochastic Weight Averaging\n(SWA), a recent approach which averages weights along the trajectory of SGD\nwith a modified learning rate schedule. We also propose fast-SWA, which further\naccelerates convergence by averaging multiple points within each cycle of a\ncyclical learning rate schedule. With weight averaging, we achieve the best\nknown semi-supervised results on CIFAR-10 and CIFAR-100, over many different\nquantities of labeled training data. For example, we achieve 5.0% error on\nCIFAR-10 with only 4000 labels, compared to the previous best result in the\nliterature of 6.3%. \n\n"}
{"id": "1806.05622", "contents": "Title: VoxCeleb2: Deep Speaker Recognition Abstract: The objective of this paper is speaker recognition under noisy and\nunconstrained conditions.\n  We make two key contributions. First, we introduce a very large-scale\naudio-visual speaker recognition dataset collected from open-source media.\nUsing a fully automated pipeline, we curate VoxCeleb2 which contains over a\nmillion utterances from over 6,000 speakers. This is several times larger than\nany publicly available speaker recognition dataset.\n  Second, we develop and compare Convolutional Neural Network (CNN) models and\ntraining strategies that can effectively recognise identities from voice under\nvarious conditions. The models trained on the VoxCeleb2 dataset surpass the\nperformance of previous works on a benchmark dataset by a significant margin. \n\n"}
{"id": "1806.06003", "contents": "Title: On Machine Learning and Structure for Mobile Robots Abstract: Due to recent advances - compute, data, models - the role of learning in\nautonomous systems has expanded significantly, rendering new applications\npossible for the first time. While some of the most significant benefits are\nobtained in the perception modules of the software stack, other aspects\ncontinue to rely on known manual procedures based on prior knowledge on\ngeometry, dynamics, kinematics etc. Nonetheless, learning gains relevance in\nthese modules when data collection and curation become easier than manual rule\ndesign. Building on this coarse and broad survey of current research, the final\nsections aim to provide insights into future potentials and challenges as well\nas the necessity of structure in current practical applications. \n\n"}
{"id": "1806.06034", "contents": "Title: The Toybox Dataset of Egocentric Visual Object Transformations Abstract: In object recognition research, many commonly used datasets (e.g., ImageNet\nand similar) contain relatively sparse distributions of object instances and\nviews, e.g., one might see a thousand different pictures of a thousand\ndifferent giraffes, mostly taken from a few conventionally photographed angles.\nThese distributional properties constrain the types of computational\nexperiments that are able to be conducted with such datasets, and also do not\nreflect naturalistic patterns of embodied visual experience. As a contribution\nto the small (but growing) number of multi-view object datasets that have been\ncreated to bridge this gap, we introduce a new video dataset called Toybox that\ncontains egocentric (i.e., first-person perspective) videos of common household\nobjects and toys being manually manipulated to undergo structured\ntransformations, such as rotation, translation, and zooming. To illustrate\npotential uses of Toybox, we also present initial neural network experiments\nthat examine 1) how training on different distributions of object instances and\nviews affects recognition performance, and 2) how viewpoint-dependent object\nconcepts are represented within the hidden layers of a trained network. \n\n"}
{"id": "1806.06769", "contents": "Title: Kid-Net: Convolution Networks for Kidney Vessels Segmentation from\n  CT-Volumes Abstract: Semantic image segmentation plays an important role in modeling\npatient-specific anatomy. We propose a convolution neural network, called\nKid-Net, along with a training schema to segment kidney vessels: artery, vein\nand collecting system. Such segmentation is vital during the surgical planning\nphase in which medical decisions are made before surgical incision. Our main\ncontribution is developing a training schema that handles unbalanced data,\nreduces false positives and enables high-resolution segmentation with a limited\nmemory budget. These objectives are attained using dynamic weighting, random\nsampling and 3D patch segmentation. Manual medical image annotation is both\ntime-consuming and expensive. Kid-Net reduces kidney vessels segmentation time\nfrom matter of hours to minutes. It is trained end-to-end using 3D patches from\nvolumetric CT-images. A complete segmentation for a 512x512x512 CT-volume is\nobtained within a few minutes (1-2 mins) by stitching the output 3D patches\ntogether. Feature down-sampling and up-sampling are utilized to achieve higher\nclassification and localization accuracies. Quantitative and qualitative\nevaluation results on a challenging testing dataset show Kid-Net competence. \n\n"}
{"id": "1806.07492", "contents": "Title: On the Learning of Deep Local Features for Robust Face Spoofing\n  Detection Abstract: Biometrics emerged as a robust solution for security systems. However, given\nthe dissemination of biometric applications, criminals are developing\ntechniques to circumvent them by simulating physical or behavioral traits of\nlegal users (spoofing attacks). Despite face being a promising characteristic\ndue to its universality, acceptability and presence of cameras almost\neverywhere, face recognition systems are extremely vulnerable to such frauds\nsince they can be easily fooled with common printed facial photographs.\nState-of-the-art approaches, based on Convolutional Neural Networks (CNNs),\npresent good results in face spoofing detection. However, these methods do not\nconsider the importance of learning deep local features from each facial\nregion, even though it is known from face recognition that each facial region\npresents different visual aspects, which can also be exploited for face\nspoofing detection. In this work we propose a novel CNN architecture trained in\ntwo steps for such task. Initially, each part of the neural network learns\nfeatures from a given facial region. Afterwards, the whole model is fine-tuned\non the whole facial images. Results show that such pre-training step allows the\nCNN to learn different local spoofing cues, improving the performance and the\nconvergence speed of the final model, outperforming the state-of-the-art\napproaches. \n\n"}
{"id": "1806.07781", "contents": "Title: Histological images segmentation of mucous glands Abstract: Mucous glands lesions analysis and assessing of malignant potential of colon\npolyps are very important tasks of surgical pathology. However, differential\ndiagnosis of colon polyps often seems impossible by classical methods and it is\nnecessary to involve computer methods capable of assessing minimal differences\nto extend the capabilities of the classical pathology examination. Accurate\nsegmentation of mucous glands from histology images is a crucial step to obtain\nreliable morphometric criteria for quantitative diagnostic methods. We review\nmajor trends in histological images segmentation and design a new convolutional\nneural network for mucous gland segmentation. \n\n"}
{"id": "1806.08174", "contents": "Title: Crowd disagreement about medical images is informative Abstract: Classifiers for medical image analysis are often trained with a single\nconsensus label, based on combining labels given by experts or crowds. However,\ndisagreement between annotators may be informative, and thus removing it may\nnot be the best strategy. As a proof of concept, we predict whether a skin\nlesion from the ISIC 2017 dataset is a melanoma or not, based on crowd\nannotations of visual characteristics of that lesion. We compare using the mean\nannotations, illustrating consensus, to standard deviations and other\ndistribution moments, illustrating disagreement. We show that the mean\nannotations perform best, but that the disagreement measures are still\ninformative. We also make the crowd annotations used in this paper available at\n\\url{https://figshare.com/s/5cbbce14647b66286544}. \n\n"}
{"id": "1806.08600", "contents": "Title: KinshipGAN: Synthesizing of Kinship Faces From Family Photos by\n  Regularizing a Deep Face Network Abstract: In this paper, we propose a kinship generator network that can synthesize a\npossible child face by analyzing his/her parent's photo. For this purpose, we\nfocus on to handle the scarcity of kinship datasets throughout the paper by\nproposing novel solutions in particular. To extract robust features, we\nintegrate a pre-trained face model to the kinship face generator. Moreover, the\ngenerator network is regularized with an additional face dataset and\nadversarial loss to decrease the overfitting of the limited samples. Lastly, we\nadapt cycle-domain transformation to attain a more stable results. Experiments\nare conducted on Families in the Wild (FIW) dataset. The experimental results\nshow that the contributions presented in the paper provide important\nperformance improvements compared to the baseline architecture and our proposed\nmethod yields promising perceptual results. \n\n"}
{"id": "1806.08852", "contents": "Title: Multi-Task Handwritten Document Layout Analysis Abstract: Document Layout Analysis is a fundamental step in Handwritten Text Processing\nsystems, from the extraction of the text lines to the type of zone it belongs\nto. We present a system based on artificial neural networks which is able to\ndetermine not only the baselines of text lines present in the document, but\nalso performs geometric and logic layout analysis of the document. Experiments\nin three different datasets demonstrate the potential of the method and show\ncompetitive results with respect to state-of-the-art methods. \n\n"}
{"id": "1806.09228", "contents": "Title: Deep $k$-Means: Re-Training and Parameter Sharing with Harder Cluster\n  Assignments for Compressing Deep Convolutions Abstract: The current trend of pushing CNNs deeper with convolutions has created a\npressing demand to achieve higher compression gains on CNNs where convolutions\ndominate the computation and parameter amount (e.g., GoogLeNet, ResNet and Wide\nResNet). Further, the high energy consumption of convolutions limits its\ndeployment on mobile devices. To this end, we proposed a simple yet effective\nscheme for compressing convolutions though applying k-means clustering on the\nweights, compression is achieved through weight-sharing, by only recording $K$\ncluster centers and weight assignment indexes. We then introduced a novel\nspectrally relaxed $k$-means regularization, which tends to make hard\nassignments of convolutional layer weights to $K$ learned cluster centers\nduring re-training. We additionally propose an improved set of metrics to\nestimate energy consumption of CNN hardware implementations, whose estimation\nresults are verified to be consistent with previously proposed energy\nestimation tool extrapolated from actual hardware measurements. We finally\nevaluated Deep $k$-Means across several CNN models in terms of both compression\nratio and energy consumption reduction, observing promising results without\nincurring accuracy loss. The code is available at\nhttps://github.com/Sandbox3aster/Deep-K-Means \n\n"}
{"id": "1806.09278", "contents": "Title: Best Vision Technologies Submission to ActivityNet Challenge 2018-Task:\n  Dense-Captioning Events in Videos Abstract: This note describes the details of our solution to the dense-captioning\nevents in videos task of ActivityNet Challenge 2018. Specifically, we solve\nthis problem with a two-stage way, i.e., first temporal event proposal and then\nsentence generation. For temporal event proposal, we directly leverage the\nthree-stage workflow in [13, 16]. For sentence generation, we capitalize on\nLSTM-based captioning framework with temporal attention mechanism (dubbed as\nLSTM-T). Moreover, the input visual sequence to the LSTM-based video captioning\nmodel is comprised of RGB and optical flow images. At inference, we adopt a\nlate fusion scheme to fuse the two LSTM-based captioning models for sentence\ngeneration. \n\n"}
{"id": "1806.10181", "contents": "Title: Unsupervised Learning by Competing Hidden Units Abstract: It is widely believed that the backpropagation algorithm is essential for\nlearning good feature detectors in early layers of artificial neural networks,\nso that these detectors are useful for the task performed by the higher layers\nof that neural network. At the same time, the traditional form of\nbackpropagation is biologically implausible. In the present paper we propose an\nunusual learning rule, which has a degree of biological plausibility, and which\nis motivated by Hebb's idea that change of the synapse strength should be local\n- i.e. should depend only on the activities of the pre and post synaptic\nneurons. We design a learning algorithm that utilizes global inhibition in the\nhidden layer, and is capable of learning early feature detectors in a\ncompletely unsupervised way. These learned lower layer feature detectors can be\nused to train higher layer weights in a usual supervised way so that the\nperformance of the full network is comparable to the performance of standard\nfeedforward networks trained end-to-end with a backpropagation algorithm. \n\n"}
{"id": "1806.10359", "contents": "Title: Context Proposals for Saliency Detection Abstract: One of the fundamental properties of a salient object region is its contrast\nwith the immediate context. The problem is that numerous object regions exist\nwhich potentially can all be salient. One way to prevent an exhaustive search\nover all object regions is by using object proposal algorithms. These return a\nlimited set of regions which are most likely to contain an object. Several\nsaliency estimation methods have used object proposals. However, they focus on\nthe saliency of the proposal only, and the importance of its immediate context\nhas not been evaluated.\n  In this paper, we aim to improve salient object detection. Therefore, we\nextend object proposal methods with context proposals, which allow to\nincorporate the immediate context in the saliency computation. We propose\nseveral saliency features which are computed from the context proposals. In the\nexperiments, we evaluate five object proposal methods for the task of saliency\nsegmentation, and find that Multiscale Combinatorial Grouping outperforms the\nothers. Furthermore, experiments show that the proposed context features\nimprove performance, and that our method matches results on the FT datasets and\nobtains competitive results on three other datasets (PASCAL-S, MSRA-B and\nECSSD). \n\n"}
{"id": "1806.10457", "contents": "Title: Physics-based Scene-level Reasoning for Object Pose Estimation in\n  Clutter Abstract: This paper focuses on vision-based pose estimation for multiple rigid objects\nplaced in clutter, especially in cases involving occlusions and objects resting\non each other. Progress has been achieved recently in object recognition given\nadvancements in deep learning. Nevertheless, such tools typically require a\nlarge amount of training data and significant manual effort to label objects.\nThis limits their applicability in robotics, where solutions must scale to a\nlarge number of objects and variety of conditions. Moreover, the combinatorial\nnature of the scenes that could arise from the placement of multiple objects is\nhard to capture in the training dataset. Thus, the learned models might not\nproduce the desired level of precision required for tasks, such as robotic\nmanipulation. This work proposes an autonomous process for pose estimation that\nspans from data generation to scene-level reasoning and self-learning. In\nparticular, the proposed framework first generates a labeled dataset for\ntraining a Convolutional Neural Network (CNN) for object detection in clutter.\nThese detections are used to guide a scene-level optimization process, which\nconsiders the interactions between the different objects present in the clutter\nto output pose estimates of high precision. Furthermore, confident estimates\nare used to label online real images from multiple views and re-train the\nprocess in a self-learning pipeline. Experimental results indicate that this\nprocess is quickly able to identify in cluttered scenes physically-consistent\nobject poses that are more precise than the ones found by reasoning over\nindividual instances of objects. Furthermore, the quality of pose estimates\nincreases over time given the self-learning process. \n\n"}
{"id": "1806.10574", "contents": "Title: This Looks Like That: Deep Learning for Interpretable Image Recognition Abstract: When we are faced with challenging image classification tasks, we often\nexplain our reasoning by dissecting the image, and pointing out prototypical\naspects of one class or another. The mounting evidence for each of the classes\nhelps us make our final decision. In this work, we introduce a deep network\narchitecture -- prototypical part network (ProtoPNet), that reasons in a\nsimilar way: the network dissects the image by finding prototypical parts, and\ncombines evidence from the prototypes to make a final classification. The model\nthus reasons in a way that is qualitatively similar to the way ornithologists,\nphysicians, and others would explain to people on how to solve challenging\nimage classification tasks. The network uses only image-level labels for\ntraining without any annotations for parts of images. We demonstrate our method\non the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show\nthat ProtoPNet can achieve comparable accuracy with its analogous\nnon-interpretable counterpart, and when several ProtoPNets are combined into a\nlarger network, it can achieve an accuracy that is on par with some of the\nbest-performing deep models. Moreover, ProtoPNet provides a level of\ninterpretability that is absent in other interpretable deep models. \n\n"}
{"id": "1806.11186", "contents": "Title: A New Angle on L2 Regularization Abstract: Imagine two high-dimensional clusters and a hyperplane separating them.\nConsider in particular the angle between: the direction joining the two\nclusters' centroids and the normal to the hyperplane. In linear classification,\nthis angle depends on the level of L2 regularization used. Can you explain why? \n\n"}
{"id": "1806.11269", "contents": "Title: Action Recognition for Depth Video using Multi-view Dynamic Images Abstract: Dynamic imaging is a recently proposed action description paradigm for\nsimultaneously capturing motion and temporal evolution information,\nparticularly in the context of deep convolutional neural networks (CNNs).\nCompared with optical flow for motion characterization, dynamic imaging\nexhibits superior efficiency and compactness. Inspired by the success of\ndynamic imaging in RGB video, this study extends it to the depth domain. To\nbetter exploit three-dimensional (3D) characteristics, multi-view dynamic\nimages are proposed. In particular, the raw depth video is densely projected\nwith respect to different virtual imaging viewpoints by rotating the virtual\ncamera within the 3D space. Subsequently, dynamic images are extracted from the\nobtained multi-view depth videos and multi-view dynamic images are thus\nconstructed from these images. Accordingly, more view-tolerant visual cues can\nbe involved. A novel CNN model is then proposed to perform feature learning on\nmulti-view dynamic images. Particularly, the dynamic images from different\nviews share the same convolutional layers but correspond to different fully\nconnected layers. This is aimed at enhancing the tuning effectiveness on\nshallow convolutional layers by alleviating the gradient vanishing problem.\nMoreover, as the spatial occurrence variation of the actions may impair the\nCNN, an action proposal approach is also put forth. In experiments, the\nproposed approach can achieve state-of-the-art performance on three challenging\ndatasets. \n\n"}
{"id": "1807.00275", "contents": "Title: Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from\n  LiDAR and Monocular Camera Abstract: Depth completion, the technique of estimating a dense depth image from sparse\ndepth measurements, has a variety of applications in robotics and autonomous\ndriving. However, depth completion faces 3 main challenges: the irregularly\nspaced pattern in the sparse depth input, the difficulty in handling multiple\nsensor modalities (when color images are available), as well as the lack of\ndense, pixel-level ground truth depth labels. In this work, we address all\nthese challenges. Specifically, we develop a deep regression model to learn a\ndirect mapping from sparse depth (and color images) to dense depth. We also\npropose a self-supervised training framework that requires only sequences of\ncolor and sparse depth images, without the need for dense depth labels. Our\nexperiments demonstrate that our network, when trained with semi-dense\nannotations, attains state-of-the- art accuracy and is the winning approach on\nthe KITTI depth completion benchmark at the time of submission. Furthermore,\nthe self-supervised framework outperforms a number of existing solutions\ntrained with semi- dense annotations. \n\n"}
{"id": "1807.00392", "contents": "Title: Gradient Reversal Against Discrimination Abstract: No methods currently exist for making arbitrary neural networks fair. In this\nwork we introduce GRAD, a new and simplified method to producing fair neural\nnetworks that can be used for auto-encoding fair representations or directly\nwith predictive networks. It is easy to implement and add to existing\narchitectures, has only one (insensitive) hyper-parameter, and provides\nimproved individual and group fairness. We use the flexibility of GRAD to\ndemonstrate multi-attribute protection. \n\n"}
{"id": "1807.00399", "contents": "Title: cilantro: A Lean, Versatile, and Efficient Library for Point Cloud Data\n  Processing Abstract: We introduce cilantro, an open-source C++ library for geometric and\ngeneral-purpose point cloud data processing. The library provides functionality\nthat covers low-level point cloud operations, spatial reasoning, various\nmethods for point cloud segmentation and generic data clustering, flexible\nalgorithms for robust or local geometric alignment, model fitting, as well as\npowerful visualization tools. To accommodate all kinds of workflows, cilantro\nis almost fully templated, and most of its generic algorithms operate in\narbitrary data dimension. At the same time, the library is easy to use and\nhighly expressive, promoting a clean and concise coding style. cilantro is\nhighly optimized, has a minimal set of external dependencies, and supports\nrapid development of performant point cloud processing software in a wide\nvariety of contexts. \n\n"}
{"id": "1807.00511", "contents": "Title: COSMO: Contextualized Scene Modeling with Boltzmann Machines Abstract: Scene modeling is very crucial for robots that need to perceive, reason about\nand manipulate the objects in their environments. In this paper, we adapt and\nextend Boltzmann Machines (BMs) for contextualized scene modeling. Although\nthere are many models on the subject, ours is the first to bring together\nobjects, relations, and affordances in a highly-capable generative model. For\nthis end, we introduce a hybrid version of BMs where relations and affordances\nare introduced with shared, tri-way connections into the model. Moreover, we\ncontribute a dataset for relation estimation and modeling studies. We evaluate\nour method in comparison with several baselines on object estimation,\nout-of-context object detection, relation estimation, and affordance estimation\ntasks. Moreover, to illustrate the generative capability of the model, we show\nseveral example scenes that the model is able to generate. \n\n"}
{"id": "1807.00601", "contents": "Title: Crowd Counting using Deep Recurrent Spatial-Aware Network Abstract: Crowd counting from unconstrained scene images is a crucial task in many\nreal-world applications like urban surveillance and management, but it is\ngreatly challenged by the camera's perspective that causes huge appearance\nvariations in people's scales and rotations. Conventional methods address such\nchallenges by resorting to fixed multi-scale architectures that are often\nunable to cover the largely varied scales while ignoring the rotation\nvariations. In this paper, we propose a unified neural network framework, named\nDeep Recurrent Spatial-Aware Network, which adaptively addresses the two issues\nin a learnable spatial transform module with a region-wise refinement process.\nSpecifically, our framework incorporates a Recurrent Spatial-Aware Refinement\n(RSAR) module iteratively conducting two components: i) a Spatial Transformer\nNetwork that dynamically locates an attentional region from the crowd density\nmap and transforms it to the suitable scale and rotation for optimal crowd\nestimation; ii) a Local Refinement Network that refines the density map of the\nattended region with residual learning. Extensive experiments on four\nchallenging benchmarks show the effectiveness of our approach. Specifically,\ncomparing with the existing best-performing methods, we achieve an improvement\nof 12% on the largest dataset WorldExpo'10 and 22.8% on the most challenging\ndataset UCF_CC_50. \n\n"}
{"id": "1807.00703", "contents": "Title: Introducing the Simulated Flying Shapes and Simulated Planar Manipulator\n  Datasets Abstract: We release two artificial datasets, Simulated Flying Shapes and Simulated\nPlanar Manipulator that allow to test the learning ability of video processing\nsystems. In particular, the dataset is meant as a tool which allows to easily\nassess the sanity of deep neural network models that aim to encode, reconstruct\nor predict video frame sequences. The datasets each consist of 90000 videos.\nThe Simulated Flying Shapes dataset comprises scenes showing two objects of\nequal shape (rectangle, triangle and circle) and size in which one object\napproaches its counterpart. The Simulated Planar Manipulator shows a 3-DOF\nplanar manipulator that executes a pick-and-place task in which it has to place\na size-varying circle on a squared platform. Different from other widely used\ndatasets such as moving MNIST [1], [2], the two presented datasets involve\ngoal-oriented tasks (e.g. the manipulator grasping an object and placing it on\na platform), rather than showing random movements. This makes our datasets more\nsuitable for testing prediction capabilities and the learning of sophisticated\nmotions by a machine learning model. This technical document aims at providing\nan introduction into the usage of both datasets. \n\n"}
{"id": "1807.00751", "contents": "Title: Understanding the Effectiveness of Lipschitz-Continuity in Generative\n  Adversarial Nets Abstract: In this paper, we investigate the underlying factor that leads to failure and\nsuccess in the training of GANs. We study the property of the optimal\ndiscriminative function and show that in many GANs, the gradient from the\noptimal discriminative function is not reliable, which turns out to be the\nfundamental cause of failure in training of GANs. We further demonstrate that a\nwell-defined distance metric does not necessarily guarantee the convergence of\nGANs. Finally, we prove in this paper that Lipschitz-continuity condition is a\ngeneral solution to make the gradient of the optimal discriminative function\nreliable, and characterized the necessary condition where Lipschitz-continuity\nensures the convergence, which leads to a broad family of valid GAN objectives\nunder Lipschitz-continuity condition, where Wasserstein distance is one special\ncase. We experiment with several new objectives, which are sound according to\nour theorems, and we found that, compared with Wasserstein distance, the\noutputs of the discriminator with new objectives are more stable and the final\nqualities of generated samples are also consistently higher than those produced\nby Wasserstein distance. \n\n"}
{"id": "1807.00780", "contents": "Title: Ambient Hidden Space of Generative Adversarial Networks Abstract: Generative adversarial models are powerful tools to model structure in\ncomplex distributions for a variety of tasks. Current techniques for learning\ngenerative models require an access to samples which have high quality, and\nadvanced generative models are applied to generate samples from noisy training\ndata through ambient modules. However, the modules are only practical for the\noutput space of the generator, and their application in the hidden space is not\nwell studied. In this paper, we extend the ambient module to the hidden space\nof the generator, and provide the uniqueness condition and the corresponding\nstrategy for the ambient hidden generator in the adversarial training process.\nWe report the practicality of the proposed method on the benchmark dataset. \n\n"}
{"id": "1807.01424", "contents": "Title: Unbiased Image Style Transfer Abstract: Recent fast image style transferring methods use feed-forward neural networks\nto generate an output image of desired style strength from the input pair of a\ncontent and a target style image. In the existing methods, the image of\nintermediate style between the content and the target style is obtained by\ndecoding a linearly interpolated feature in encoded feature space. However,\nthere has been no work on analyzing the effectiveness of this kind of style\nstrength interpolation so far. In this paper, we tackle the missing work on the\nin-depth analysis of style interpolation and propose a method that is more\neffective in controlling style strength. We interpret the training task of a\nstyle transfer network as a regression learning between the control parameter\nand output style strength. In this understanding, the existing methods are\nbiased due to the fact that training is performed with one-sided data of full\nstyle strength (alpha = 1.0). Thus, this biased learning does not guarantee the\ngeneration of a desired intermediate style corresponding to the style control\nparameter between 0.0 and 1.0. To solve this problem of the biased network, we\npropose an unbiased learning technique which uses unbiased training data and\ncorresponding unbiased loss for alpha = 0.0 to make the feed-forward networks\nto generate a zero-style image, i.e., content image when alpha = 0.0. Our\nexperimental results verified that our unbiased learning method achieved the\nreconstruction of a content image with zero style strength, better regression\nspecification between style control parameter and output style, and more stable\nstyle transfer that is insensitive to the weight of style loss without additive\ncomplexity in image generating process. \n\n"}
{"id": "1807.01702", "contents": "Title: Restructuring Batch Normalization to Accelerate CNN Training Abstract: Batch Normalization (BN) has become a core design block of modern\nConvolutional Neural Networks (CNNs). A typical modern CNN has a large number\nof BN layers in its lean and deep architecture. BN requires mean and variance\ncalculations over each mini-batch during training. Therefore, the existing\nmemory access reduction techniques, such as fusing multiple CONV layers, are\nnot effective for accelerating BN due to their inability to optimize mini-batch\nrelated calculations during training. To address this increasingly important\nproblem, we propose to restructure BN layers by first splitting a BN layer into\ntwo sub-layers (fission) and then combining the first sub-layer with its\npreceding CONV layer and the second sub-layer with the following activation and\nCONV layers (fusion). The proposed solution can significantly reduce\nmain-memory accesses while training the latest CNN models, and the experiments\non a chip multiprocessor show that the proposed BN restructuring can improve\nthe performance of DenseNet-121 by 25.7%. \n\n"}
{"id": "1807.02437", "contents": "Title: Deep Sequential Segmentation of Organs in Volumetric Medical Scans Abstract: Segmentation in 3D scans is playing an increasingly important role in current\nclinical practice supporting diagnosis, tissue quantification, or treatment\nplanning. The current 3D approaches based on convolutional neural networks\nusually suffer from at least three main issues caused predominantly by\nimplementation constraints - first, they require resizing the volume to the\nlower-resolutional reference dimensions, second, the capacity of such\napproaches is very limited due to memory restrictions, and third, all slices of\nvolumes have to be available at any given training or testing time. We address\nthese problems by a U-Net-like architecture consisting of bidirectional\nconvolutional LSTM and convolutional, pooling, upsampling and concatenation\nlayers enclosed into time-distributed wrappers. Our network can either process\nthe full volumes in a sequential manner, or segment slabs of slices on demand.\nWe demonstrate performance of our architecture on vertebrae and liver\nsegmentation tasks in 3D CT scans. \n\n"}
{"id": "1807.02480", "contents": "Title: A Fully Convolutional Two-Stream Fusion Network for Interactive Image\n  Segmentation Abstract: In this paper, we propose a novel fully convolutional two-stream fusion\nnetwork (FCTSFN) for interactive image segmentation. The proposed network\nincludes two sub-networks: a two-stream late fusion network (TSLFN) that\npredicts the foreground at a reduced resolution, and a multi-scale refining\nnetwork (MSRN) that refines the foreground at full resolution. The TSLFN\nincludes two distinct deep streams followed by a fusion network. The intuition\nis that, since user interactions are more direct information on\nforeground/background than the image itself, the two-stream structure of the\nTSLFN reduces the number of layers between the pure user interaction features\nand the network output, allowing the user interactions to have a more direct\nimpact on the segmentation result. The MSRN fuses the features from different\nlayers of TSLFN with different scales, in order to seek the local to global\ninformation on the foreground to refine the segmentation result at full\nresolution. We conduct comprehensive experiments on four benchmark datasets.\nThe results show that the proposed network achieves competitive performance\ncompared to current state-of-the-art interactive image segmentation methods \n\n"}
{"id": "1807.02716", "contents": "Title: A Deep-Learning-Based Geological Parameterization for History Matching\n  Complex Models Abstract: A new low-dimensional parameterization based on principal component analysis\n(PCA) and convolutional neural networks (CNN) is developed to represent complex\ngeological models. The CNN-PCA method is inspired by recent developments in\ncomputer vision using deep learning. CNN-PCA can be viewed as a generalization\nof an existing optimization-based PCA (O-PCA) method. Both CNN-PCA and O-PCA\nentail post-processing a PCA model to better honor complex geological features.\nIn CNN-PCA, rather than use a histogram-based regularization as in O-PCA, a new\nregularization involving a set of metrics for multipoint statistics is\nintroduced. The metrics are based on summary statistics of the nonlinear filter\nresponses of geological models to a pre-trained deep CNN. In addition, in the\nCNN-PCA formulation presented here, a convolutional neural network is trained\nas an explicit transform function that can post-process PCA models quickly.\nCNN-PCA is shown to provide both unconditional and conditional realizations\nthat honor the geological features present in reference SGeMS geostatistical\nrealizations for a binary channelized system. Flow statistics obtained through\nsimulation of random CNN-PCA models closely match results for random SGeMS\nmodels for a demanding case in which O-PCA models lead to significant\ndiscrepancies. Results for history matching are also presented. In this\nassessment CNN-PCA is applied with derivative-free optimization, and a subspace\nrandomized maximum likelihood method is used to provide multiple posterior\nmodels. Data assimilation and significant uncertainty reduction are achieved\nfor existing wells, and physically reasonable predictions are also obtained for\nnew wells. Finally, the CNN-PCA method is extended to a more complex\nnon-stationary bimodal deltaic fan system, and is shown to provide high-quality\nrealizations for this challenging example. \n\n"}
{"id": "1807.02740", "contents": "Title: Data-driven Upsampling of Point Clouds Abstract: High quality upsampling of sparse 3D point clouds is critically useful for a\nwide range of geometric operations such as reconstruction, rendering, meshing,\nand analysis. In this paper, we propose a data-driven algorithm that enables an\nupsampling of 3D point clouds without the need for hard-coded rules. Our\napproach uses a deep network with Chamfer distance as the loss function,\ncapable of learning the latent features in point clouds belonging to different\nobject categories. We evaluate our algorithm across different amplification\nfactors, with upsampling learned and performed on objects belonging to the same\ncategory as well as different categories. We also explore the desirable\ncharacteristics of input point clouds as a function of the distribution of the\npoint samples. Finally, we demonstrate the performance of our algorithm in\nsingle-category training versus multi-category training scenarios. The final\nproposed model is compared against a baseline, optimization-based upsampling\nmethod. Results indicate that our algorithm is capable of generating more\nuniform and accurate upsamplings. \n\n"}
{"id": "1807.02799", "contents": "Title: Distillation Techniques for Pseudo-rehearsal Based Incremental Learning Abstract: The ability to learn from incrementally arriving data is essential for any\nlife-long learning system. However, standard deep neural networks forget the\nknowledge about the old tasks, a phenomenon called catastrophic forgetting,\nwhen trained on incrementally arriving data. We discuss the biases in current\nGenerative Adversarial Networks (GAN) based approaches that learn the\nclassifier by knowledge distillation from previously trained classifiers. These\nbiases cause the trained classifier to perform poorly. We propose an approach\nto remove these biases by distilling knowledge from the classifier of AC-GAN.\nExperiments on MNIST and CIFAR10 show that this method is comparable to current\nstate of the art rehearsal based approaches. The code for this paper is\navailable at https://bit.ly/incremental-learning \n\n"}
{"id": "1807.03407", "contents": "Title: High Fidelity Semantic Shape Completion for Point Clouds using Latent\n  Optimization Abstract: Semantic shape completion is a challenging problem in 3D computer vision\nwhere the task is to generate a complete 3D shape using a partial 3D shape as\ninput. We propose a learning-based approach to complete incomplete 3D shapes\nthrough generative modeling and latent manifold optimization. Our algorithm\nworks directly on point clouds. We use an autoencoder and a GAN to learn a\ndistribution of embeddings for point clouds of object classes. An input point\ncloud with missing regions is first encoded to a feature vector. The\nrepresentations learnt by the GAN are then used to find the best latent vector\non the manifold using a combined optimization that finds a vector in the\nmanifold of plausible vectors that is close to the original input (both in the\nfeature space and the output space of the decoder). Experiments show that our\nalgorithm is capable of successfully reconstructing point clouds with large\nmissing regions with very high fidelity without having to rely on exemplar\nbased database retrieval. \n\n"}
{"id": "1807.03418", "contents": "Title: AudioMNIST: Exploring Explainable Artificial Intelligence for Audio\n  Analysis on a Simple Benchmark Abstract: Explainable Artificial Intelligence (XAI) is targeted at understanding how\nmodels perform feature selection and derive their classification decisions.\nThis paper explores post-hoc explanations for deep neural networks in the audio\ndomain. Notably, we present a novel Open Source audio dataset consisting of\n30,000 audio samples of English spoken digits which we use for classification\ntasks on spoken digits and speakers' biological sex. We use the popular XAI\ntechnique Layer-wise Relevance Propagation (LRP) to identify relevant features\nfor two neural network architectures that process either waveform or\nspectrogram representations of the data. Based on the relevance scores obtained\nfrom LRP, hypotheses about the neural networks' feature selection are derived\nand subsequently tested through systematic manipulations of the input data.\nFurther, we take a step beyond visual explanations and introduce audible\nheatmaps. We demonstrate the superior interpretability of audible explanations\nover visual ones in a human user study. \n\n"}
{"id": "1807.04001", "contents": "Title: Learning Neural Models for End-to-End Clustering Abstract: We propose a novel end-to-end neural network architecture that, once trained,\ndirectly outputs a probabilistic clustering of a batch of input examples in one\npass. It estimates a distribution over the number of clusters $k$, and for each\n$1 \\leq k \\leq k_\\mathrm{max}$, a distribution over the individual cluster\nassignment for each data point. The network is trained in advance in a\nsupervised fashion on separate data to learn grouping by any perceptual\nsimilarity criterion based on pairwise labels (same/different group). It can\nthen be applied to different data containing different groups. We demonstrate\npromising performance on high-dimensional data like images (COIL-100) and\nspeech (TIMIT). We call this ``learning to cluster'' and show its conceptual\ndifference to deep metric learning, semi-supervise clustering and other related\napproaches while having the advantage of performing learnable clustering fully\nend-to-end. \n\n"}
{"id": "1807.04339", "contents": "Title: A Generic Approach to Lung Field Segmentation from Chest Radiographs\n  using Deep Space and Shape Learning Abstract: Computer-aided diagnosis (CAD) techniques for lung field segmentation from\nchest radiographs (CXR) have been proposed for adult cohorts, but rarely for\npediatric subjects. Statistical shape models (SSMs), the workhorse of most\nstate-of-the-art CXR-based lung field segmentation methods, do not efficiently\naccommodate shape variation of the lung field during the pediatric\ndevelopmental stages. The main contributions of our work are: (1) a generic\nlung field segmentation framework from CXR accommodating large shape variation\nfor adult and pediatric cohorts; (2) a deep representation learning detection\nmechanism, \\emph{ensemble space learning}, for robust object localization; and\n(3) \\emph{marginal shape deep learning} for the shape deformation parameter\nestimation. Unlike the iterative approach of conventional SSMs, the proposed\nshape learning mechanism transforms the parameter space into marginal subspaces\nthat are solvable efficiently using the recursive representation learning\nmechanism. Furthermore, our method is the first to include the challenging\nretro-cardiac region in the CXR-based lung segmentation for accurate lung\ncapacity estimation. The framework is evaluated on 668 CXRs of patients between\n3 month to 89 year of age. We obtain a mean Dice similarity coefficient of\n$0.96\\pm0.03$ (including the retro-cardiac region). For a given accuracy, the\nproposed approach is also found to be faster than conventional SSM-based\niterative segmentation methods. The computational simplicity of the proposed\ngeneric framework could be similarly applied to the fast segmentation of other\ndeformable objects. \n\n"}
{"id": "1807.04950", "contents": "Title: Deep Learning in the Wild Abstract: Deep learning with neural networks is applied by an increasing number of\npeople outside of classic research environments, due to the vast success of the\nmethodology on a wide range of machine perception tasks. While this interest is\nfueled by beautiful success stories, practical work in deep learning on novel\ntasks without existing baselines remains challenging. This paper explores the\nspecific challenges arising in the realm of real world tasks, based on case\nstudies from research \\& development in conjunction with industry, and extracts\nlessons learned from them. It thus fills a gap between the publication of\nlatest algorithmic and methodical developments, and the usually omitted\nnitty-gritty of how to make them work. Specifically, we give insight into deep\nlearning projects on face matching, print media monitoring, industrial quality\ncontrol, music scanning, strategy game playing, and automated machine learning,\nthereby providing best practices for deep learning in practice. \n\n"}
{"id": "1807.05597", "contents": "Title: Deep Learning for Semantic Segmentation on Minimal Hardware Abstract: Deep learning has revolutionised many fields, but it is still challenging to\ntransfer its success to small mobile robots with minimal hardware.\nSpecifically, some work has been done to this effect in the RoboCup humanoid\nfootball domain, but results that are performant and efficient and still\ngenerally applicable outside of this domain are lacking. We propose an approach\nconceptually different from those taken previously. It is based on semantic\nsegmentation and does achieve these desired properties. In detail, it is being\nable to process full VGA images in real-time on a low-power mobile processor.\nIt can further handle multiple image dimensions without retraining, it does not\nrequire specific domain knowledge for achieving a high frame rate and it is\napplicable on a minimal mobile hardware. \n\n"}
{"id": "1807.06535", "contents": "Title: A framework for remote sensing images processing using deep learning\n  technique Abstract: Deep learning techniques are becoming increasingly important to solve a\nnumber of image processing tasks. Among common algorithms, Convolutional Neural\nNetworks and Recurrent Neural Networks based systems achieve state of the art\nresults on satellite and aerial imagery in many applications. While these\napproaches are subject to scientific interest, there is currently no\noperational and generic implementation available at user-level for the remote\nsensing community. In this paper, we presents a framework enabling the use of\ndeep learning techniques with remote sensing images and geospatial data. Our\nsolution takes roots in two extensively used open-source libraries, the remote\nsensing image processing library Orfeo ToolBox, and the high performance\nnumerical computation library TensorFlow. It can apply deep nets without\nrestriction on images size and is computationally efficient, regardless\nhardware configuration. \n\n"}
{"id": "1807.06906", "contents": "Title: Towards Automated Deep Learning: Efficient Joint Neural Architecture and\n  Hyperparameter Search Abstract: While existing work on neural architecture search (NAS) tunes hyperparameters\nin a separate post-processing step, we demonstrate that architectural choices\nand other hyperparameter settings interact in a way that can render this\nseparation suboptimal. Likewise, we demonstrate that the common practice of\nusing very few epochs during the main NAS and much larger numbers of epochs\nduring a post-processing step is inefficient due to little correlation in the\nrelative rankings for these two training regimes. To combat both of these\nproblems, we propose to use a recent combination of Bayesian optimization and\nHyperband for efficient joint neural architecture and hyperparameter search. \n\n"}
{"id": "1807.07247", "contents": "Title: Chest X-rays Classification: A Multi-Label and Fine-Grained Problem Abstract: The widely used ChestX-ray14 dataset addresses an important medical image\nclassification problem and has the following caveats: 1) many lung pathologies\nare visually similar, 2) a variant of diseases including lung cancer,\ntuberculosis, and pneumonia are present in a single scan, i.e. multiple labels\nand 3) The incidence of healthy images is much larger than diseased samples,\ncreating imbalanced data. These properties are common in medical domain.\nExisting literature uses stateof- the-art DensetNet/Resnet models being\ntransfer learned where output neurons of the networks are trained for\nindividual diseases to cater for multiple diseases labels in each image.\nHowever, most of them don't consider relationship between multiple classes. In\nthis work we have proposed a novel error function, Multi-label Softmax Loss\n(MSML), to specifically address the properties of multiple labels and\nimbalanced data. Moreover, we have designed deep network architecture based on\nfine-grained classification concept that incorporates MSML. We have evaluated\nour proposed method on various network backbones and showed consistent\nperformance improvements of AUC-ROC scores on the ChestX-ray14 dataset. The\nproposed error function provides a new method to gain improved performance\nacross wider medical datasets. \n\n"}
{"id": "1807.07258", "contents": "Title: Visual Domain Adaptation with Manifold Embedded Distribution Alignment Abstract: Visual domain adaptation aims to learn robust classifiers for the target\ndomain by leveraging knowledge from a source domain. Existing methods either\nattempt to align the cross-domain distributions, or perform manifold subspace\nlearning. However, there are two significant challenges: (1) degenerated\nfeature transformation, which means that distribution alignment is often\nperformed in the original feature space, where feature distortions are hard to\novercome. On the other hand, subspace learning is not sufficient to reduce the\ndistribution divergence. (2) unevaluated distribution alignment, which means\nthat existing distribution alignment methods only align the marginal and\nconditional distributions with equal importance, while they fail to evaluate\nthe different importance of these two distributions in real applications. In\nthis paper, we propose a Manifold Embedded Distribution Alignment (MEDA)\napproach to address these challenges. MEDA learns a domain-invariant classifier\nin Grassmann manifold with structural risk minimization, while performing\ndynamic distribution alignment to quantitatively account for the relative\nimportance of marginal and conditional distributions. To the best of our\nknowledge, MEDA is the first attempt to perform dynamic distribution alignment\nfor manifold domain adaptation. Extensive experiments demonstrate that MEDA\nshows significant improvements in classification accuracy compared to\nstate-of-the-art traditional and deep methods. \n\n"}
{"id": "1807.07466", "contents": "Title: Guided Upsampling Network for Real-Time Semantic Segmentation Abstract: Semantic segmentation architectures are mainly built upon an encoder-decoder\nstructure. These models perform subsequent downsampling operations in the\nencoder. Since operations on high-resolution activation maps are\ncomputationally expensive, usually the decoder produces output segmentation\nmaps by upsampling with parameters-free operators like bilinear or\nnearest-neighbor. We propose a Neural Network named Guided Upsampling Network\nwhich consists of a multiresolution architecture that jointly exploits\nhigh-resolution and large context information. Then we introduce a new module\nnamed Guided Upsampling Module (GUM) that enriches upsampling operators by\nintroducing a learnable transformation for semantic maps. It can be plugged\ninto any existing encoder-decoder architecture with little modifications and\nlow additional computation cost. We show with quantitative and qualitative\nexperiments how our network benefits from the use of GUM module. A\ncomprehensive set of experiments on the publicly available Cityscapes dataset\ndemonstrates that Guided Upsampling Network can efficiently process\nhigh-resolution images in real-time while attaining state-of-the art\nperformances. \n\n"}
{"id": "1807.07716", "contents": "Title: Brain Tumor Segmentation and Tractographic Feature Extraction from\n  Structural MR Images for Overall Survival Prediction Abstract: This paper introduces a novel methodology to integrate human brain\nconnectomics and parcellation for brain tumor segmentation and survival\nprediction. For segmentation, we utilize an existing brain parcellation atlas\nin the MNI152 1mm space and map this parcellation to each individual subject\ndata. We use deep neural network architectures together with hard negative\nmining to achieve the final voxel level classification. For survival\nprediction, we present a new method for combining features from connectomics\ndata, brain parcellation information, and the brain tumor mask. We leverage the\naverage connectome information from the Human Connectome Project and map each\nsubject brain volume onto this common connectome space. From this, we compute\ntractographic features that describe potential neural disruptions due to the\nbrain tumor. These features are then used to predict the overall survival of\nthe subjects. The main novelty in the proposed methods is the use of normalized\nbrain parcellation data and tractography data from the human connectome project\nfor analyzing MR images for segmentation and survival prediction. Experimental\nresults are reported on the BraTS2018 dataset. \n\n"}
{"id": "1807.07930", "contents": "Title: Perceptual Video Super Resolution with Enhanced Temporal Consistency Abstract: With the advent of perceptual loss functions, new possibilities in\nsuper-resolution have emerged, and we currently have models that successfully\ngenerate near-photorealistic high-resolution images from their low-resolution\nobservations. Up to now, however, such approaches have been exclusively limited\nto single image super-resolution. The application of perceptual loss functions\non video processing still entails several challenges, mostly related to the\nlack of temporal consistency of the generated images, i.e., flickering\nartifacts. In this work, we present a novel adversarial recurrent network for\nvideo upscaling that is able to produce realistic textures in a temporally\nconsistent way. The proposed architecture naturally leverages information from\nprevious frames due to its recurrent architecture, i.e. the input to the\ngenerator is composed of the low-resolution image and, additionally, the warped\noutput of the network at the previous step. Together with a video\ndiscriminator, we also propose additional loss functions to further reinforce\ntemporal consistency in the generated sequences. The experimental validation of\nour algorithm shows the effectiveness of our approach which obtains images with\nhigh perceptual quality and improved temporal consistency. \n\n"}
{"id": "1807.07984", "contents": "Title: Attention Models in Graphs: A Survey Abstract: Graph-structured data arise naturally in many different application domains.\nBy representing data as graphs, we can capture entities (i.e., nodes) as well\nas their relationships (i.e., edges) with each other. Many useful insights can\nbe derived from graph-structured data as demonstrated by an ever-growing body\nof work focused on graph mining. However, in the real-world, graphs can be both\nlarge - with many complex patterns - and noisy which can pose a problem for\neffective graph mining. An effective way to deal with this issue is to\nincorporate \"attention\" into graph mining solutions. An attention mechanism\nallows a method to focus on task-relevant parts of the graph, helping it to\nmake better decisions. In this work, we conduct a comprehensive and focused\nsurvey of the literature on the emerging field of graph attention models. We\nintroduce three intuitive taxonomies to group existing work. These are based on\nproblem setting (type of input and output), the type of attention mechanism\nused, and the task (e.g., graph classification, link prediction, etc.). We\nmotivate our taxonomies through detailed examples and use each to survey\ncompeting approaches from a unique standpoint. Finally, we highlight several\nchallenges in the area and discuss promising directions for future work. \n\n"}
{"id": "1807.08058", "contents": "Title: Learning Heuristics for Quantified Boolean Formulas through Deep\n  Reinforcement Learning Abstract: We demonstrate how to learn efficient heuristics for automated reasoning\nalgorithms for quantified Boolean formulas through deep reinforcement learning.\nWe focus on a backtracking search algorithm, which can already solve formulas\nof impressive size - up to hundreds of thousands of variables. The main\nchallenge is to find a representation of these formulas that lends itself to\nmaking predictions in a scalable way. For a family of challenging problems, we\nlearned a heuristic that solves significantly more formulas compared to the\nexisting handwritten heuristics. \n\n"}
{"id": "1807.08107", "contents": "Title: Person Search via A Mask-Guided Two-Stream CNN Model Abstract: In this work, we tackle the problem of person search, which is a challenging\ntask consisted of pedestrian detection and person re-identification~(re-ID).\nInstead of sharing representations in a single joint model, we find that\nseparating detector and re-ID feature extraction yields better performance. In\norder to extract more representative features for each identity, we segment out\nthe foreground person from the original image patch. We propose a simple yet\neffective re-ID method, which models foreground person and original image\npatches individually, and obtains enriched representations from two separate\nCNN streams. From the experiments on two standard person search benchmarks of\nCUHK-SYSU and PRW, we achieve mAP of $83.0\\%$ and $32.6\\%$ respectively,\nsurpassing the state of the art by a large margin (more than 5pp). \n\n"}
{"id": "1807.08407", "contents": "Title: Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd Abstract: Pedestrian detection in crowded scenes is a challenging problem since the\npedestrians often gather together and occlude each other. In this paper, we\npropose a new occlusion-aware R-CNN (OR-CNN) to improve the detection accuracy\nin the crowd. Specifically, we design a new aggregation loss to enforce\nproposals to be close and locate compactly to the corresponding objects.\nMeanwhile, we use a new part occlusion-aware region of interest (PORoI) pooling\nunit to replace the RoI pooling layer in order to integrate the prior structure\ninformation of human body with visibility prediction into the network to handle\nocclusion. Our detector is trained in an end-to-end fashion, which achieves\nstate-of-the-art results on three pedestrian detection datasets, i.e.,\nCityPersons, ETH, and INRIA, and performs on-pair with the state-of-the-arts on\nCaltech. \n\n"}
{"id": "1807.08469", "contents": "Title: Zero-shot keyword spotting for visual speech recognition in-the-wild Abstract: Visual keyword spotting (KWS) is the problem of estimating whether a text\nquery occurs in a given recording using only video information. This paper\nfocuses on visual KWS for words unseen during training, a real-world, practical\nsetting which so far has received no attention by the community. To this end,\nwe devise an end-to-end architecture comprising (a) a state-of-the-art visual\nfeature extractor based on spatiotemporal Residual Networks, (b) a\ngrapheme-to-phoneme model based on sequence-to-sequence neural networks, and\n(c) a stack of recurrent neural networks which learn how to correlate visual\nfeatures with the keyword representation. Different to prior works on KWS,\nwhich try to learn word representations merely from sequences of graphemes\n(i.e. letters), we propose the use of a grapheme-to-phoneme encoder-decoder\nmodel which learns how to map words to their pronunciation. We demonstrate that\nour system obtains very promising visual-only KWS results on the challenging\nLRS2 database, for keywords unseen during training. We also show that our\nsystem outperforms a baseline which addresses KWS via automatic speech\nrecognition (ASR), while it drastically improves over other recently proposed\nASR-free KWS methods. \n\n"}
{"id": "1807.08902", "contents": "Title: Self-produced Guidance for Weakly-supervised Object Localization Abstract: Weakly supervised methods usually generate localization results based on\nattention maps produced by classification networks. However, the attention maps\nexhibit the most discriminative parts of the object which are small and sparse.\nWe propose to generate Self-produced Guidance (SPG) masks which separate the\nforeground, the object of interest, from the background to provide the\nclassification networks with spatial correlation information of pixels. A\nstagewise approach is proposed to incorporate high confident object regions to\nlearn the SPG masks. The high confident regions within attention maps are\nutilized to progressively learn the SPG masks. The masks are then used as an\nauxiliary pixel-level supervision to facilitate the training of classification\nnetworks. Extensive experiments on ILSVRC demonstrate that SPG is effective in\nproducing high-quality object localizations maps. Particularly, the proposed\nSPG achieves the Top-1 localization error rate of 43.83% on the ILSVRC\nvalidation set, which is a new state-of-the-art error rate. \n\n"}
{"id": "1807.09251", "contents": "Title: GANimation: Anatomically-aware Facial Animation from a Single Image Abstract: Recent advances in Generative Adversarial Networks (GANs) have shown\nimpressive results for task of facial expression synthesis. The most successful\narchitecture is StarGAN, that conditions GANs generation process with images of\na specific domain, namely a set of images of persons sharing the same\nexpression. While effective, this approach can only generate a discrete number\nof expressions, determined by the content of the dataset. To address this\nlimitation, in this paper, we introduce a novel GAN conditioning scheme based\non Action Units (AU) annotations, which describes in a continuous manifold the\nanatomical facial movements defining a human expression. Our approach allows\ncontrolling the magnitude of activation of each AU and combine several of them.\nAdditionally, we propose a fully unsupervised strategy to train the model, that\nonly requires images annotated with their activated AUs, and exploit attention\nmechanisms that make our network robust to changing backgrounds and lighting\nconditions. Extensive evaluation show that our approach goes beyond competing\nconditional generators both in the capability to synthesize a much wider range\nof expressions ruled by anatomically feasible muscle movements, as in the\ncapacity of dealing with images in the wild. \n\n"}
{"id": "1807.09372", "contents": "Title: A Synchronized Stereo and Plenoptic Visual Odometry Dataset Abstract: We present a new dataset to evaluate monocular, stereo, and plenoptic camera\nbased visual odometry algorithms. The dataset comprises a set of synchronized\nimage sequences recorded by a micro lens array (MLA) based plenoptic camera and\na stereo camera system. For this, the stereo cameras and the plenoptic camera\nwere assembled on a common hand-held platform. All sequences are recorded in a\nvery large loop, where beginning and end show the same scene. Therefore, the\ntracking accuracy of a visual odometry algorithm can be measured from the drift\nbetween beginning and end of the sequence. For both, the plenoptic camera and\nthe stereo system, we supply full intrinsic camera models, as well as\nvignetting data. The dataset consists of 11 sequences which were recorded in\nchallenging indoor and outdoor scenarios. We present, by way of example, the\nresults achieved by state-of-the-art algorithms. \n\n"}
{"id": "1807.09380", "contents": "Title: Contrastive Video Representation Learning via Adversarial Perturbations Abstract: Adversarial perturbations are noise-like patterns that can subtly change the\ndata, while failing an otherwise accurate classifier. In this paper, we propose\nto use such perturbations within a novel contrastive learning setup to build\nnegative samples, which are then used to produce improved video\nrepresentations. To this end, given a well-trained deep model for per-frame\nvideo recognition, we first generate adversarial noise adapted to this model.\nPositive and negative bags are produced using the original data features from\nthe full video sequence and their perturbed counterparts, respectively. Unlike\nthe classic contrastive learning methods, we develop a binary classification\nproblem that learns a set of discriminative hyperplanes -- as a subspace --\nthat will separate the two bags from each other. This subspace is then used as\na descriptor for the video, dubbed \\emph{discriminative subspace pooling}. As\nthe perturbed features belong to data classes that are likely to be confused\nwith the original features, the discriminative subspace will characterize parts\nof the feature space that are more representative of the original data, and\nthus may provide robust video representations. To learn such descriptors, we\nformulate a subspace learning objective on the Stiefel manifold and resort to\nRiemannian optimization methods for solving it efficiently. We provide\nexperiments on several video datasets and demonstrate state-of-the-art results. \n\n"}
{"id": "1807.09436", "contents": "Title: Deterministic consensus maximization with biconvex programming Abstract: Consensus maximization is one of the most widely used robust fitting\nparadigms in computer vision, and the development of algorithms for consensus\nmaximization is an active research topic. In this paper, we propose an\nefficient deterministic optimization algorithm for consensus maximization.\nGiven an initial solution, our method conducts a deterministic search that\nforcibly increases the consensus of the initial solution. We show how each\niteration of the update can be formulated as an instance of biconvex\nprogramming, which we solve efficiently using a novel biconvex optimization\nalgorithm. In contrast to our algorithm, previous consensus improvement\ntechniques rely on random sampling or relaxations of the objective function,\nwhich reduce their ability to significantly improve the initial consensus. In\nfact, on challenging instances, the previous techniques may even return a worse\noff solution. Comprehensive experiments show that our algorithm can\nconsistently and greatly improve the quality of the initial solution, without\nsubstantial cost. \n\n"}
{"id": "1807.09601", "contents": "Title: Linear Span Network for Object Skeleton Detection Abstract: Robust object skeleton detection requires to explore rich representative\nvisual features and effective feature fusion strategies. In this paper, we\nfirst re-visit the implementation of HED, the essential principle of which can\nbe ideally described with a linear reconstruction model. Hinted by this, we\nformalize a Linear Span framework, and propose Linear Span Network (LSN)\nmodified by Linear Span Units (LSUs), which minimize the reconstruction error\nof convolutional network. LSN further utilizes subspace linear span beside the\nfeature linear span to increase the independence of convolutional features and\nthe efficiency of feature integration, which enlarges the capability of fitting\ncomplex ground-truth. As a result, LSN can effectively suppress the cluttered\nbackgrounds and reconstruct object skeletons. Experimental results validate the\nstate-of-the-art performance of the proposed LSN. \n\n"}
{"id": "1807.09620", "contents": "Title: OmniDepth: Dense Depth Estimation for Indoors Spherical Panoramas Abstract: Recent work on depth estimation up to now has only focused on projective\nimages ignoring 360 content which is now increasingly and more easily produced.\nWe show that monocular depth estimation models trained on traditional images\nproduce sub-optimal results on omnidirectional images, showcasing the need for\ntraining directly on 360 datasets, which however, are hard to acquire. In this\nwork, we circumvent the challenges associated with acquiring high quality 360\ndatasets with ground truth depth annotations, by re-using recently released\nlarge scale 3D datasets and re-purposing them to 360 via rendering. This\ndataset, which is considerably larger than similar projective datasets, is\npublicly offered to the community to enable future research in this direction.\nWe use this dataset to learn in an end-to-end fashion the task of depth\nestimation from 360 images. We show promising results in our synthesized data\nas well as in unseen realistic images. \n\n"}
{"id": "1807.09659", "contents": "Title: A Surprising Linear Relationship Predicts Test Performance in Deep\n  Networks Abstract: Given two networks with the same training loss on a dataset, when would they\nhave drastically different test losses and errors? Better understanding of this\nquestion of generalization may improve practical applications of deep networks.\nIn this paper we show that with cross-entropy loss it is surprisingly simple to\ninduce significantly different generalization performances for two networks\nthat have the same architecture, the same meta parameters and the same training\nerror: one can either pretrain the networks with different levels of\n\"corrupted\" data or simply initialize the networks with weights of different\nGaussian standard deviations. A corollary of recent theoretical results on\noverfitting shows that these effects are due to an intrinsic problem of\nmeasuring test performance with a cross-entropy/exponential-type loss, which\ncan be decomposed into two components both minimized by SGD -- one of which is\nnot related to expected classification performance. However, if we factor out\nthis component of the loss, a linear relationship emerges between training and\ntest losses. Under this transformation, classical generalization bounds are\nsurprisingly tight: the empirical/training loss is very close to the\nexpected/test loss. Furthermore, the empirical relation between classification\nerror and normalized cross-entropy loss seem to be approximately monotonic \n\n"}
{"id": "1807.09685", "contents": "Title: Grounding Visual Explanations Abstract: Existing visual explanation generating agents learn to fluently justify a\nclass prediction. However, they may mention visual attributes which reflect a\nstrong class prior, although the evidence may not actually be in the image.\nThis is particularly concerning as ultimately such agents fail in building\ntrust with human users. To overcome this limitation, we propose a phrase-critic\nmodel to refine generated candidate explanations augmented with flipped phrases\nwhich we use as negative examples while training. At inference time, our\nphrase-critic model takes an image and a candidate explanation as input and\noutputs a score indicating how well the candidate explanation is grounded in\nthe image. Our explainable AI agent is capable of providing counter arguments\nfor an alternative prediction, i.e. counterfactuals, along with explanations\nthat justify the correct classification decisions. Our model improves the\ntextual explanation quality of fine-grained classification decisions on the CUB\ndataset by mentioning phrases that are grounded in the image. Moreover, on the\nFOIL tasks, our agent detects when there is a mistake in the sentence, grounds\nthe incorrect phrase and corrects it significantly better than other models. \n\n"}
{"id": "1807.09713", "contents": "Title: Asynchronous, Photometric Feature Tracking using Events and Frames Abstract: We present a method that leverages the complementarity of event cameras and\nstandard cameras to track visual features with low-latency. Event cameras are\nnovel sensors that output pixel-level brightness changes, called \"events\". They\noffer significant advantages over standard cameras, namely a very high dynamic\nrange, no motion blur, and a latency in the order of microseconds. However,\nbecause the same scene pattern can produce different events depending on the\nmotion direction, establishing event correspondences across time is\nchallenging. By contrast, standard cameras provide intensity measurements\n(frames) that do not depend on motion direction. Our method extracts features\non frames and subsequently tracks them asynchronously using events, thereby\nexploiting the best of both types of data: the frames provide a photometric\nrepresentation that does not depend on motion direction and the events provide\nlow-latency updates. In contrast to previous works, which are based on\nheuristics, this is the first principled method that uses raw intensity\nmeasurements directly, based on a generative event model within a\nmaximum-likelihood framework. As a result, our method produces feature tracks\nthat are both more accurate (subpixel accuracy) and longer than the state of\nthe art, across a wide variety of scenes. \n\n"}
{"id": "1807.10580", "contents": "Title: Is the Pedestrian going to Cross? Answering by 2D Pose Estimation Abstract: Our recent work suggests that, thanks to nowadays powerful CNNs, image-based\n2D pose estimation is a promising cue for determining pedestrian intentions\nsuch as crossing the road in the path of the ego-vehicle, stopping before\nentering the road, and starting to walk or bending towards the road. This\nstatement is based on the results obtained on non-naturalistic sequences\n(Daimler dataset), i.e. in sequences choreographed specifically for performing\nthe study. Fortunately, a new publicly available dataset (JAAD) has appeared\nrecently to allow developing methods for detecting pedestrian intentions in\nnaturalistic driving conditions; more specifically, for addressing the relevant\nquestion is the pedestrian going to cross? Accordingly, in this paper we use\nJAAD to assess the usefulness of 2D pose estimation for answering such a\nquestion. We combine CNN-based pedestrian detection, tracking and pose\nestimation to predict the crossing action from monocular images. Overall, the\nproposed pipeline provides new state-of-the-art results. \n\n"}
{"id": "1807.10588", "contents": "Title: A Modality-Adaptive Method for Segmenting Brain Tumors and\n  Organs-at-Risk in Radiation Therapy Planning Abstract: In this paper we present a method for simultaneously segmenting brain tumors\nand an extensive set of organs-at-risk for radiation therapy planning of\nglioblastomas. The method combines a contrast-adaptive generative model for\nwhole-brain segmentation with a new spatial regularization model of tumor shape\nusing convolutional restricted Boltzmann machines. We demonstrate\nexperimentally that the method is able to adapt to image acquisitions that\ndiffer substantially from any available training data, ensuring its\napplicability across treatment sites; that its tumor segmentation accuracy is\ncomparable to that of the current state of the art; and that it captures most\norgans-at-risk sufficiently well for radiation therapy planning purposes. The\nproposed method may be a valuable step towards automating the delineation of\nbrain tumors and organs-at-risk in glioblastoma patients undergoing radiation\ntherapy. \n\n"}
{"id": "1807.10603", "contents": "Title: A Capsule Network for Traffic Speed Prediction in Complex Road Networks Abstract: This paper proposes a deep learning approach for traffic flow prediction in\ncomplex road networks. Traffic flow data from induction loop sensors are\nessentially a time series, which is also spatially related to traffic in\ndifferent road segments. The spatio-temporal traffic data can be converted into\nan image where the traffic data are expressed in a 3D space with respect to\nspace and time axes. Although convolutional neural networks (CNNs) have been\nshowing surprising performance in understanding images, they have a major\ndrawback. In the max pooling operation, CNNs are losing important information\nby locally taking the highest activation values. The inter-relationship in\ntraffic data measured by sparsely located sensors in different time intervals\nshould not be neglected in order to obtain accurate predictions. Thus, we\npropose a neural network with capsules that replaces max pooling by dynamic\nrouting. This is the first approach that employs the capsule network on a time\nseries forecasting problem, to our best knowledge. Moreover, an experiment on\nreal traffic speed data measured in the Santander city of Spain demonstrates\nthe proposed method outperforms the state-of-the-art method based on a CNN by\n13.1% in terms of root mean squared error. \n\n"}
{"id": "1807.10850", "contents": "Title: Synthesizing CT from Ultrashort Echo-Time MR Images via Convolutional\n  Neural Networks Abstract: With the increasing popularity of PET-MR scanners in clinical applications,\nsynthesis of CT images from MR has been an important research topic. Accurate\nPET image reconstruction requires attenuation correction, which is based on the\nelectron density of tissues and can be obtained from CT images. While CT\nmeasures electron density information for x-ray photons, MR images convey\ninformation about the magnetic properties of tissues. Therefore, with the\nadvent of PET-MR systems, the attenuation coefficients need to be indirectly\nestimated from MR images. In this paper, we propose a fully convolutional\nneural network (CNN) based method to synthesize head CT from ultra-short\necho-time (UTE) dual-echo MR images. Unlike traditional $T_1$-w images which do\nnot have any bone signal, UTE images show some signal for bone, which makes it\na good candidate for MR to CT synthesis. A notable advantage of our approach is\nthat accurate results were achieved with a small training data set. Using an\natlas of a single CT and dual-echo UTE pair, we train a deep neural network\nmodel to learn the transform of MR intensities to CT using patches. We compared\nour CNN based model with a state-of-the-art registration based as well as a\nBayesian model based CT synthesis method, and showed that the proposed CNN\nmodel outperforms both of them. We also compared the proposed model when only\n$T_1$-w images are available instead of UTE, and show that UTE images produce\nbetter synthesis than using just $T_1$-w images. \n\n"}
{"id": "1807.10972", "contents": "Title: RS-Net: Regression-Segmentation 3D CNN for Synthesis of Full Resolution\n  Missing Brain MRI in the Presence of Tumours Abstract: Accurate synthesis of a full 3D MR image containing tumours from available\nMRI (e.g. to replace an image that is currently unavailable or corrupted) would\nprovide a clinician as well as downstream inference methods with important\ncomplementary information for disease analysis. In this paper, we present an\nend-to-end 3D convolution neural network that takes a set of acquired MR image\nsequences (e.g. T1, T2, T1ce) as input and concurrently performs (1) regression\nof the missing full resolution 3D MRI (e.g. FLAIR) and (2) segmentation of the\ntumour into subtypes (e.g. enhancement, core). The hypothesis is that this\nwould focus the network to perform accurate synthesis in the area of the\ntumour. Experiments on the BraTS 2015 and 2017 datasets [1] show that: (1) the\nproposed method gives better performance than state-of-the-art methods in terms\nof established global evaluation metrics (e.g. PSNR), (2) replacing real MR\nvolumes with the synthesized MRI does not lead to significant degradation in\ntumour and sub-structure segmentation accuracy. The system further provides\nuncertainty estimates based on Monte Carlo (MC) dropout [11] for the\nsynthesized volume at each voxel, permitting quantification of the system's\nconfidence in the output at each location. \n\n"}
{"id": "1807.11078", "contents": "Title: Semi-supervised Transfer Learning for Image Rain Removal Abstract: Single image rain removal is a typical inverse problem in computer vision.\nThe deep learning technique has been verified to be effective for this task and\nachieved state-of-the-art performance. However, previous deep learning methods\nneed to pre-collect a large set of image pairs with/without synthesized rain\nfor training, which tends to make the neural network be biased toward learning\nthe specific patterns of the synthesized rain, while be less able to generalize\nto real test samples whose rain types differ from those in the training data.\nTo this issue, this paper firstly proposes a semi-supervised learning paradigm\ntoward this task. Different from traditional deep learning methods which only\nuse supervised image pairs with/without synthesized rain, we further put real\nrainy images, without need of their clean ones, into the network training\nprocess. This is realized by elaborately formulating the residual between an\ninput rainy image and its expected network output (clear image without rain) as\na specific parametrized rain streaks distribution. The network is therefore\ntrained to adapt real unsupervised diverse rain types through transferring from\nthe supervised synthesized rain, and thus both the short-of-training-sample and\nbias-to-supervised-sample issues can be evidently alleviated. Experiments on\nsynthetic and real data verify the superiority of our model compared to the\nstate-of-the-arts. \n\n"}
{"id": "1807.11254", "contents": "Title: Extreme Network Compression via Filter Group Approximation Abstract: In this paper we propose a novel decomposition method based on filter group\napproximation, which can significantly reduce the redundancy of deep\nconvolutional neural networks (CNNs) while maintaining the majority of feature\nrepresentation. Unlike other low-rank decomposition algorithms which operate on\nspatial or channel dimension of filters, our proposed method mainly focuses on\nexploiting the filter group structure for each layer. For several commonly used\nCNN models, including VGG and ResNet, our method can reduce over 80%\nfloating-point operations (FLOPs) with less accuracy drop than state-of-the-art\nmethods on various image classification datasets. Besides, experiments\ndemonstrate that our method is conducive to alleviating degeneracy of the\ncompressed network, which hurts the convergence and performance of the network. \n\n"}
{"id": "1807.11440", "contents": "Title: Comparator Networks Abstract: The objective of this work is set-based verification, e.g. to decide if two\nsets of images of a face are of the same person or not. The traditional\napproach to this problem is to learn to generate a feature vector per image,\naggregate them into one vector to represent the set, and then compute the\ncosine similarity between sets. Instead, we design a neural network\narchitecture that can directly learn set-wise verification. Our contributions\nare: (i) We propose a Deep Comparator Network (DCN) that can ingest a pair of\nsets (each may contain a variable number of images) as inputs, and compute a\nsimilarity between the pair--this involves attending to multiple discriminative\nlocal regions (landmarks), and comparing local descriptors between pairs of\nfaces; (ii) To encourage high-quality representations for each set, internal\ncompetition is introduced for recalibration based on the landmark score; (iii)\nInspired by image retrieval, a novel hard sample mining regime is proposed to\ncontrol the sampling process, such that the DCN is complementary to the\nstandard image classification models. Evaluations on the IARPA Janus face\nrecognition benchmarks show that the comparator networks outperform the\nprevious state-of-the-art results by a large margin. \n\n"}
{"id": "1807.11929", "contents": "Title: Egocentric Spatial Memory Abstract: Egocentric spatial memory (ESM) defines a memory system with encoding,\nstoring, recognizing and recalling the spatial information about the\nenvironment from an egocentric perspective. We introduce an integrated deep\nneural network architecture for modeling ESM. It learns to estimate the\noccupancy state of the world and progressively construct top-down 2D global\nmaps from egocentric views in a spatially extended environment. During the\nexploration, our proposed ESM model updates belief of the global map based on\nlocal observations using a recurrent neural network. It also augments the local\nmapping with a novel external memory to encode and store latent representations\nof the visited places over long-term exploration in large environments which\nenables agents to perform place recognition and hence, loop closure. Our\nproposed ESM network contributes in the following aspects: (1) without feature\nengineering, our model predicts free space based on egocentric views\nefficiently in an end-to-end manner; (2) different from other deep\nlearning-based mapping system, ESMN deals with continuous actions and states\nwhich is vitally important for robotic control in real applications. In the\nexperiments, we demonstrate its accurate and robust global mapping capacities\nin 3D virtual mazes and realistic indoor environments by comparing with several\ncompetitive baselines. \n\n"}
{"id": "1808.01104", "contents": "Title: Improved Deep Spectral Convolution Network For Hyperspectral Unmixing\n  With Multinomial Mixture Kernel and Endmember Uncertainty Abstract: In this study, we propose a novel framework for hyperspectral unmixing by\nusing an improved deep spectral convolution network (DSCN++) combined with\nendmember uncertainty. DSCN++ is used to compute high-level representations\nwhich are further modeled with Multinomial Mixture Model to estimate abundance\nmaps. In the reconstruction step, a new trainable uncertainty term based on a\nnonlinear neural network model is introduced to provide robustness to endmember\nuncertainty. For the optimization of the coefficients of the multinomial model\nand the uncertainty term, Wasserstein Generative Adversarial Network (WGAN) is\nexploited to improve stability and to capture uncertainty. Experiments are\nperformed on both real and synthetic datasets. The results validate that the\nproposed method obtains state-of-the-art hyperspectral unmixing performance\nparticularly on the real datasets compared to the baseline techniques. \n\n"}
{"id": "1808.01124", "contents": "Title: Efficient texture retrieval using multiscale local extrema descriptors\n  and covariance embedding Abstract: This paper presents an efficient method for texture retrieval using\nmultiscale feature extraction and embedding based on the local extrema\nkeypoints. The idea is to first represent each texture image by its local\nmaximum and local minimum pixels. The image is then divided into regular\noverlapping blocks and each one is characterized by a feature vector\nconstructed from the radiometric, geometric and structural information of its\nlocal extrema. All feature vectors are finally embedded into a covariance\nmatrix which will be exploited for dissimilarity measurement within retrieval\ntask. Thanks to the method's simplicity, multiscale scheme can be easily\nimplemented to improve its scale-space representation capacity. We argue that\nour handcrafted features are easy to implement, fast to run but can provide\nvery competitive performance compared to handcrafted and CNN-based learned\ndescriptors from the literature. In particular, the proposed framework provides\nhighly competitive retrieval rate for several texture databases including\n94.95% for MIT Vistex, 79.87% for Stex, 76.15% for Outex TC-00013 and 89.74%\nfor USPtex. \n\n"}
{"id": "1808.01976", "contents": "Title: Adversarial Vision Challenge Abstract: The NIPS 2018 Adversarial Vision Challenge is a competition to facilitate\nmeasurable progress towards robust machine vision models and more generally\napplicable adversarial attacks. This document is an updated version of our\ncompetition proposal that was accepted in the competition track of 32nd\nConference on Neural Information Processing Systems (NIPS 2018). \n\n"}
{"id": "1808.02874", "contents": "Title: Visualizing Convolutional Networks for MRI-based Diagnosis of\n  Alzheimer's Disease Abstract: Visualizing and interpreting convolutional neural networks (CNNs) is an\nimportant task to increase trust in automatic medical decision making systems.\nIn this study, we train a 3D CNN to detect Alzheimer's disease based on\nstructural MRI scans of the brain. Then, we apply four different gradient-based\nand occlusion-based visualization methods that explain the network's\nclassification decisions by highlighting relevant areas in the input image. We\ncompare the methods qualitatively and quantitatively. We find that all four\nmethods focus on brain regions known to be involved in Alzheimer's disease,\nsuch as inferior and middle temporal gyrus. While the occlusion-based methods\nfocus more on specific regions, the gradient-based methods pick up distributed\nrelevance patterns. Additionally, we find that the distribution of relevance\nvaries across patients, with some having a stronger focus on the temporal lobe,\nwhereas for others more cortical areas are relevant. In summary, we show that\napplying different visualization methods is important to understand the\ndecisions of a CNN, a step that is crucial to increase clinical impact and\ntrust in computer-based decision support systems. \n\n"}
{"id": "1808.03944", "contents": "Title: Unsupervised learning for cross-domain medical image synthesis using\n  deformation invariant cycle consistency networks Abstract: Recently, the cycle-consistent generative adversarial networks (CycleGAN) has\nbeen widely used for synthesis of multi-domain medical images. The\ndomain-specific nonlinear deformations captured by CycleGAN make the\nsynthesized images difficult to be used for some applications, for example,\ngenerating pseudo-CT for PET-MR attenuation correction. This paper presents a\ndeformation-invariant CycleGAN (DicycleGAN) method using deformable\nconvolutional layers and new cycle-consistency losses. Its robustness dealing\nwith data that suffer from domain-specific nonlinear deformations has been\nevaluated through comparison experiments performed on a multi-sequence brain MR\ndataset and a multi-modality abdominal dataset. Our method has displayed its\nability to generate synthesized data that is aligned with the source while\nmaintaining a proper quality of signal compared to CycleGAN-generated data. The\nproposed model also obtained comparable performance with CycleGAN when data\nfrom the source and target domains are alignable through simple affine\ntransformations. \n\n"}
{"id": "1808.04228", "contents": "Title: DFTerNet: Towards 2-bit Dynamic Fusion Networks for Accurate Human\n  Activity Recognition Abstract: Deep Convolutional Neural Networks (DCNNs) are currently popular in human\nactivity recognition applications. However, in the face of modern artificial\nintelligence sensor-based games, many research achievements cannot be\npractically applied on portable devices. DCNNs are typically resource-intensive\nand too large to be deployed on portable devices, thus this limits the\npractical application of complex activity detection. In addition, since\nportable devices do not possess high-performance Graphic Processing Units\n(GPUs), there is hardly any improvement in Action Game (ACT) experience.\nBesides, in order to deal with multi-sensor collaboration, all previous human\nactivity recognition models typically treated the representations from\ndifferent sensor signal sources equally. However, distinct types of activities\nshould adopt different fusion strategies. In this paper, a novel scheme is\nproposed. This scheme is used to train 2-bit Convolutional Neural Networks with\nweights and activations constrained to {-0.5,0,0.5}. It takes into account the\ncorrelation between different sensor signal sources and the activity types.\nThis model, which we refer to as DFTerNet, aims at producing a more reliable\ninference and better trade-offs for practical applications. Our basic idea is\nto exploit quantization of weights and activations directly in pre-trained\nfilter banks and adopt dynamic fusion strategies for different activity types.\nExperiments demonstrate that by using dynamic fusion strategy can exceed the\nbaseline model performance by up to ~5% on activity recognition like\nOPPORTUNITY and PAMAP2 datasets. Using the quantization method proposed, we\nwere able to achieve performances closer to that of full-precision counterpart.\nThese results were also verified using the UniMiB-SHAR dataset. In addition,\nthe proposed method can achieve ~9x acceleration on CPUs and ~11x memory\nsaving. \n\n"}
{"id": "1808.04359", "contents": "Title: Community Regularization of Visually-Grounded Dialog Abstract: The task of conducting visually grounded dialog involves learning\ngoal-oriented cooperative dialog between autonomous agents who exchange\ninformation about a scene through several rounds of questions and answers in\nnatural language. We posit that requiring artificial agents to adhere to the\nrules of human language, while also requiring them to maximize information\nexchange through dialog is an ill-posed problem. We observe that humans do not\nstray from a common language because they are social creatures who live in\ncommunities, and have to communicate with many people everyday, so it is far\neasier to stick to a common language even at the cost of some efficiency loss.\nUsing this as inspiration, we propose and evaluate a multi-agent\ncommunity-based dialog framework where each agent interacts with, and learns\nfrom, multiple agents, and show that this community-enforced regularization\nresults in more relevant and coherent dialog (as judged by human evaluators)\nwithout sacrificing task performance (as judged by quantitative metrics). \n\n"}
{"id": "1808.04456", "contents": "Title: Multimodal Deep Neural Networks using Both Engineered and Learned\n  Representations for Biodegradability Prediction Abstract: Deep learning algorithms excel at extracting patterns from raw data, and with\nlarge datasets, they have been very successful in computer vision and natural\nlanguage applications. However, in other domains, large datasets on which to\nlearn representations from may not exist. In this work, we develop a novel\nmultimodal CNN-MLP neural network architecture that utilizes both\ndomain-specific feature engineering as well as learned representations from raw\ndata. We illustrate the effectiveness of such network designs in the chemical\nsciences, for predicting biodegradability. DeepBioD, a multimodal CNN-MLP\nnetwork is more accurate than either standalone network designs, and achieves\nan error classification rate of 0.125 that is 27% lower than the current\nstate-of-the-art. Thus, our work indicates that combining traditional feature\nengineering with representation learning can be effective, particularly in\nsituations where labeled data is limited. \n\n"}
{"id": "1808.04589", "contents": "Title: DeepNeuro: an open-source deep learning toolbox for neuroimaging Abstract: Translating neural networks from theory to clinical practice has unique\nchallenges, specifically in the field of neuroimaging. In this paper, we\npresent DeepNeuro, a deep learning framework that is best-suited to putting\ndeep learning algorithms for neuroimaging in practical usage with a minimum of\nfriction. We show how this framework can be used to both design and train\nneural network architectures, as well as modify state-of-the-art architectures\nin a flexible and intuitive way. We display the pre- and postprocessing\nfunctions common in the medical imaging community that DeepNeuro offers to\nensure consistent performance of networks across variable users, institutions,\nand scanners. And we show how pipelines created in DeepNeuro can be concisely\npackaged into shareable Docker containers and command-line interfaces using\nDeepNeuro's pipeline resources. \n\n"}
{"id": "1808.04752", "contents": "Title: A Survey on Methods and Theories of Quantized Neural Networks Abstract: Deep neural networks are the state-of-the-art methods for many real-world\ntasks, such as computer vision, natural language processing and speech\nrecognition. For all its popularity, deep neural networks are also criticized\nfor consuming a lot of memory and draining battery life of devices during\ntraining and inference. This makes it hard to deploy these models on mobile or\nembedded devices which have tight resource constraints. Quantization is\nrecognized as one of the most effective approaches to satisfy the extreme\nmemory requirements that deep neural network models demand. Instead of adopting\n32-bit floating point format to represent weights, quantized representations\nstore weights using more compact formats such as integers or even binary\nnumbers. Despite a possible degradation in predictive performance, quantization\nprovides a potential solution to greatly reduce the model size and the energy\nconsumption. In this survey, we give a thorough review of different aspects of\nquantized neural networks. Current challenges and trends of quantized neural\nnetworks are also discussed. \n\n"}
{"id": "1808.04795", "contents": "Title: Clumped Nuclei Segmentation with Adjacent Point Match and Local Shape\n  based Intensity Analysis for Overlapped Nuclei in Fluorescence In-Situ\n  Hybridization Images Abstract: Highly clumped nuclei clusters captured in fluorescence in situ hybridization\nmicroscopy images are common histology entities under investigations in a wide\nspectrum of tissue-related biomedical investigations. Due to their large scale\nin presence, computer based image analysis is used to facilitate such analysis\nwith improved analysis efficiency and reproducibility. To ensure the quality of\ndownstream biomedical analyses, it is essential to segment clustered nuclei\nwith high quality. However, this presents a technical challenge commonly\nencountered in a large number of biomedical research, as nuclei are often\noverlapped due to a high cell density. In this paper, we propose an\nsegmentation algorithm that identifies point pair connection candidates and\nevaluates adjacent point connections with a formulated ellipse fitting quality\nindicator. After connection relationships are determined, we recover the\nresulting dividing paths by following points with specific eigenvalues from\nHessian in a constrained searching space. We validate our algorithm with 560\nimage patches from two classes of tumor regions of seven brain tumor patients.\nBoth qualitative and quantitative experimental results suggest that our\nalgorithm is promising for dividing overlapped nuclei in fluorescence in situ\nhybridization microscopy images widely used in various biomedical research. \n\n"}
{"id": "1808.04819", "contents": "Title: VizML: A Machine Learning Approach to Visualization Recommendation Abstract: Data visualization should be accessible for all analysts with data, not just\nthe few with technical expertise. Visualization recommender systems aim to\nlower the barrier to exploring basic visualizations by automatically generating\nresults for analysts to search and select, rather than manually specify. Here,\nwe demonstrate a novel machine learning-based approach to visualization\nrecommendation that learns visualization design choices from a large corpus of\ndatasets and associated visualizations. First, we identify five key design\nchoices made by analysts while creating visualizations, such as selecting a\nvisualization type and choosing to encode a column along the X- or Y-axis. We\ntrain models to predict these design choices using one million\ndataset-visualization pairs collected from a popular online visualization\nplatform. Neural networks predict these design choices with high accuracy\ncompared to baseline models. We report and interpret feature importances from\none of these baseline models. To evaluate the generalizability and uncertainty\nof our approach, we benchmark with a crowdsourced test set, and show that the\nperformance of our model is comparable to human performance when predicting\nconsensus visualization type, and exceeds that of other ML-based systems. \n\n"}
{"id": "1808.04999", "contents": "Title: Scene Coordinate Regression with Angle-Based Reprojection Loss for\n  Camera Relocalization Abstract: Image-based camera relocalization is an important problem in computer vision\nand robotics. Recent works utilize convolutional neural networks (CNNs) to\nregress for pixels in a query image their corresponding 3D world coordinates in\nthe scene. The final pose is then solved via a RANSAC-based optimization scheme\nusing the predicted coordinates. Usually, the CNN is trained with ground truth\nscene coordinates, but it has also been shown that the network can discover 3D\nscene geometry automatically by minimizing single-view reprojection loss.\nHowever, due to the deficiencies of the reprojection loss, the network needs to\nbe carefully initialized. In this paper, we present a new angle-based\nreprojection loss, which resolves the issues of the original reprojection loss.\nWith this new loss function, the network can be trained without careful\ninitialization, and the system achieves more accurate results. The new loss\nalso enables us to utilize available multi-view constraints, which further\nimprove performance. \n\n"}
{"id": "1808.05285", "contents": "Title: DNN Feature Map Compression using Learned Representation over GF(2) Abstract: In this paper, we introduce a method to compress intermediate feature maps of\ndeep neural networks (DNNs) to decrease memory storage and bandwidth\nrequirements during inference. Unlike previous works, the proposed method is\nbased on converting fixed-point activations into vectors over the smallest\nGF(2) finite field followed by nonlinear dimensionality reduction (NDR) layers\nembedded into a DNN. Such an end-to-end learned representation finds more\ncompact feature maps by exploiting quantization redundancies within the\nfixed-point activations along the channel or spatial dimensions. We apply the\nproposed network architectures derived from modified SqueezeNet and MobileNetV2\nto the tasks of ImageNet classification and PASCAL VOC object detection.\nCompared to prior approaches, the conducted experiments show a factor of 2\ndecrease in memory requirements with minor degradation in accuracy while adding\nonly bitwise computations. \n\n"}
{"id": "1808.05508", "contents": "Title: An Experimental Evaluation of Covariates Effects on Unconstrained Face\n  Verification Abstract: Covariates are factors that have a debilitating influence on face\nverification performance. In this paper, we comprehensively study two covariate\nrelated problems for unconstrained face verification: first, how covariates\naffect the performance of deep neural networks on the large-scale unconstrained\nface verification problem; second, how to utilize covariates to improve\nverification performance. To study the first problem, we implement five\nstate-of-the-art deep convolutional networks (DCNNs) for face verification and\nevaluate them on three challenging covariates datasets. In total, seven\ncovariates are considered: pose (yaw and roll), age, facial hair, gender,\nindoor/outdoor, occlusion (nose and mouth visibility, eyes visibility, and\nforehead visibility), and skin tone. These covariates cover both intrinsic\nsubject-specific characteristics and extrinsic factors of faces. Some of the\nresults confirm and extend the findings of previous studies, others are new\nfindings that were rarely mentioned previously or did not show consistent\ntrends. For the second problem, we demonstrate that with the assistance of\ngender information, the quality of a pre-curated noisy large-scale face dataset\nfor face recognition can be further improved. After retraining the face\nrecognition model using the curated data, performance improvement is observed\nat low False Acceptance Rates (FARs) (FAR=$10^{-5}$, $10^{-6}$, $10^{-7}$). \n\n"}
{"id": "1808.05577", "contents": "Title: Deeper Image Quality Transfer: Training Low-Memory Neural Networks for\n  3D Images Abstract: In this paper we address the memory demands that come with the processing of\n3-dimensional, high-resolution, multi-channeled medical images in deep\nlearning. We exploit memory-efficient backpropagation techniques, to reduce the\nmemory complexity of network training from being linear in the network's depth,\nto being roughly constant $ - $ permitting us to elongate deep architectures\nwith negligible memory increase. We evaluate our methodology in the paradigm of\nImage Quality Transfer, whilst noting its potential application to various\ntasks that use deep learning. We study the impact of depth on accuracy and show\nthat deeper models have more predictive power, which may exploit larger\ntraining sets. We obtain substantially better results than the previous\nstate-of-the-art model with a slight memory increase, reducing the\nroot-mean-squared-error by $ 13\\% $. Our code is publicly available. \n\n"}
{"id": "1808.05727", "contents": "Title: Ensemble-based Adaptive Single-shot Multi-box Detector Abstract: We propose two improvements to the SSD---single shot multibox detector.\nFirst, we propose an adaptive approach for default box selection in SSD. This\nuses data to reduce the uncertainty in the selection of best aspect ratios for\nthe default boxes and improves performance of SSD for datasets containing small\nand complex objects (e.g., equipments at construction sites). We do so by\nfinding the distribution of aspect ratios of the given training dataset, and\nthen choosing representative values. Secondly, we propose an ensemble\nalgorithm, using SSD as components, which improves the performance of SSD,\nespecially for small amount of training datasets. Compared to the conventional\nSSD algorithm, adaptive box selection improves mean average precision by 3%,\nwhile ensemble-based SSD improves it by 8%. \n\n"}
{"id": "1808.06560", "contents": "Title: Multi-View Graph Embedding Using Randomized Shortest Paths Abstract: Real-world data sets often provide multiple types of information about the\nsame set of entities. This data is well represented by multi-view graphs, which\nconsist of several distinct sets of edges over the same nodes. These can be\nused to analyze how entities interact from different viewpoints. Combining\nmultiple views improves the quality of inferences drawn from the underlying\ndata, which has increased interest in developing efficient multi-view graph\nembedding methods. We propose an algorithm, C-RSP, that generates a common (C)\nembedding of a multi-view graph using Randomized Shortest Paths (RSP). This\nalgorithm generates a dissimilarity measure between nodes by minimizing the\nexpected cost of a random walk between any two nodes across all views of a\nmulti-view graph, in doing so encoding both the local and global structure of\nthe graph. We test C-RSP on both real and synthetic data and show that it\noutperforms benchmark algorithms at embedding and clustering tasks while\nremaining computationally efficient. \n\n"}
{"id": "1808.08273", "contents": "Title: Improving Breast Cancer Detection using Symmetry Information with Deep\n  Learning Abstract: Convolutional Neural Networks (CNN) have had a huge success in many areas of\ncomputer vision and medical image analysis. However, there is still an immense\npotential for performance improvement in mammogram breast cancer detection\nComputer-Aided Detection (CAD) systems by integrating all the information that\nthe radiologist utilizes, such as symmetry and temporal data. In this work, we\nproposed a patch based multi-input CNN that learns symmetrical difference to\ndetect breast masses. The network was trained on a large-scale dataset of 28294\nmammogram images. The performance was compared to a baseline architecture\nwithout symmetry context using Area Under the ROC Curve (AUC) and Competition\nPerformance Metric (CPM). At candidate level, AUC value of 0.933 with 95%\nconfidence interval of [0.920, 0.954] was obtained when symmetry information is\nincorporated in comparison with baseline architecture which yielded AUC value\nof 0.929 with [0.919, 0.947] confidence interval. By incorporating symmetrical\ninformation, although there was no a significant candidate level performance\nagain (p = 0.111), we have found a compelling result at exam level with CPM\nvalue of 0.733 (p = 0.001). We believe that including temporal data, and adding\nbenign class to the dataset could improve the detection performance. \n\n"}
{"id": "1808.08395", "contents": "Title: A Novel Deep Neural Network Architecture for Mars Visual Navigation Abstract: In this paper, emerging deep learning techniques are leveraged to deal with\nMars visual navigation problem. Specifically, to achieve precise landing and\nautonomous navigation, a novel deep neural network architecture with double\nbranches and non-recurrent structure is designed, which can represent both\nglobal and local deep features of Martian environment images effectively. By\nemploying this architecture, Mars rover can determine the optimal navigation\npolicy to the target point directly from original Martian environment images.\nMoreover, compared with the existing state-of-the-art algorithm, the training\ntime is reduced by 45.8%. Finally, experiment results demonstrate that the\nproposed deep neural network architecture achieves better performance and\nfaster convergence than the existing ones and generalizes well to unknown\nenvironment. \n\n"}
{"id": "1808.08692", "contents": "Title: Generalized Capsule Networks with Trainable Routing Procedure Abstract: CapsNet (Capsule Network) was first proposed by~\\citet{capsule} and later\nanother version of CapsNet was proposed by~\\citet{emrouting}. CapsNet has been\nproved effective in modeling spatial features with much fewer parameters.\nHowever, the routing procedures in both papers are not well incorporated into\nthe whole training process. The optimal number of routing procedure is misery\nwhich has to be found manually. To overcome this disadvantages of current\nrouting procedures in CapsNet, we embed the routing procedure into the\noptimization procedure with all other parameters in neural networks, namely,\nmake coupling coefficients in the routing procedure become completely\ntrainable. We call it Generalized CapsNet (G-CapsNet). We implement both\n\"full-connected\" version of G-CapsNet and \"convolutional\" version of G-CapsNet.\nG-CapsNet achieves a similar performance in the dataset MNIST as in the\noriginal papers. We also test two capsule packing method (cross feature maps or\nwith feature maps) from previous convolutional layers and see no evident\ndifference. Besides, we also explored possibility of stacking multiple capsule\nlayers. The code is shared on\n\\hyperlink{https://github.com/chenzhenhua986/CAFFE-CapsNet}{CAFFE-CapsNet}. \n\n"}
{"id": "1808.08718", "contents": "Title: Wide Activation for Efficient and Accurate Image Super-Resolution Abstract: In this report we demonstrate that with same parameters and computational\nbudgets, models with wider features before ReLU activation have significantly\nbetter performance for single image super-resolution (SISR). The resulted SR\nresidual network has a slim identity mapping pathway with wider (\\(2\\times\\) to\n\\(4\\times\\)) channels before activation in each residual block. To further\nwiden activation (\\(6\\times\\) to \\(9\\times\\)) without computational overhead,\nwe introduce linear low-rank convolution into SR networks and achieve even\nbetter accuracy-efficiency tradeoffs. In addition, compared with batch\nnormalization or no normalization, we find training with weight normalization\nleads to better accuracy for deep super-resolution networks. Our proposed SR\nnetwork \\textit{WDSR} achieves better results on large-scale DIV2K image\nsuper-resolution benchmark in terms of PSNR with same or lower computational\ncomplexity. Based on WDSR, our method also won 1st places in NTIRE 2018\nChallenge on Single Image Super-Resolution in all three realistic tracks.\nExperiments and ablation studies support the importance of wide activation for\nimage super-resolution. Code is released at:\nhttps://github.com/JiahuiYu/wdsr_ntire2018 \n\n"}
{"id": "1808.10093", "contents": "Title: CNN-PS: CNN-based Photometric Stereo for General Non-Convex Surfaces Abstract: Most conventional photometric stereo algorithms inversely solve a BRDF-based\nimage formation model. However, the actual imaging process is often far more\ncomplex due to the global light transport on the non-convex surfaces. This\npaper presents a photometric stereo network that directly learns relationships\nbetween the photometric stereo input and surface normals of a scene. For\nhandling unordered, arbitrary number of input images, we merge all the input\ndata to the intermediate representation called {\\it observation map} that has a\nfixed shape, is able to be fed into a CNN. To improve both training and\nprediction, we take into account the rotational pseudo-invariance of the\nobservation map that is derived from the isotropic constraint. For training the\nnetwork, we create a synthetic photometric stereo dataset that is generated by\na physics-based renderer, therefore the global light transport is considered.\nOur experimental results on both synthetic and real datasets show that our\nmethod outperforms conventional BRDF-based photometric stereo algorithms\nespecially when scenes are highly non-convex. \n\n"}
{"id": "1808.10146", "contents": "Title: Dense Scene Flow from Stereo Disparity and Optical Flow Abstract: Scene flow describes 3D motion in a 3D scene. It can either be modeled as a\nsingle task, or it can be reconstructed from the auxiliary tasks of stereo\ndepth and optical flow estimation. While the second method can achieve\nreal-time performance by using real-time auxiliary methods, it will typically\nproduce non-dense results. In this representation of a basic combination\napproach for scene flow estimation, we will tackle the problem of non-density\nby interpolation. \n\n"}
{"id": "1808.10646", "contents": "Title: A Unified Mammogram Analysis Method via Hybrid Deep Supervision Abstract: Automatic mammogram classification and mass segmentation play a critical role\nin a computer-aided mammogram screening system. In this work, we present a\nunified mammogram analysis framework for both whole-mammogram classification\nand segmentation. Our model is designed based on a deep U-Net with residual\nconnections, and equipped with the novel hybrid deep supervision (HDS) scheme\nfor end-to-end multi-task learning. As an extension of deep supervision (DS),\nHDS not only can force the model to learn more discriminative features like DS,\nbut also seamlessly integrates segmentation and classification tasks into one\nmodel, thus the model can benefit from both pixel-wise and image-wise\nsupervisions. We extensively validate the proposed method on the widely-used\nINbreast dataset. Ablation study corroborates that pixel-wise and image-wise\nsupervisions are mutually beneficial, evidencing the efficacy of HDS. The\nresults of 5-fold cross validation indicate that our unified model matches\nstate-of-the-art performance on both mammogram segmentation and classification\ntasks, which achieves an average segmentation Dice similarity coefficient (DSC)\nof 0.85 and a classification accuracy of 0.89. The code is available at\nhttps://github.com/angrypudding/hybrid-ds. \n\n"}
{"id": "1809.00970", "contents": "Title: Iterative multi-path tracking for video and volume segmentation with\n  sparse point supervision Abstract: Recent machine learning strategies for segmentation tasks have shown great\nability when trained on large pixel-wise annotated image datasets. It remains a\nmajor challenge however to aggregate such datasets, as the time and monetary\ncost associated with collecting extensive annotations is extremely high. This\nis particularly the case for generating precise pixel-wise annotations in video\nand volumetric image data. To this end, this work presents a novel framework to\nproduce pixel-wise segmentations using minimal supervision. Our method relies\non 2D point supervision, whereby a single 2D location within an object of\ninterest is provided on each image of the data. Our method then estimates the\nobject appearance in a semi-supervised fashion by learning\nobject-image-specific features and by using these in a semi-supervised learning\nframework. Our object model is then used in a graph-based optimization problem\nthat takes into account all provided locations and the image data in order to\ninfer the complete pixel-wise segmentation. In practice, we solve this\noptimally as a tracking problem using a K-shortest path approach. Both the\nobject model and segmentation are then refined iteratively to further improve\nthe final segmentation. We show that by collecting 2D locations using a gaze\ntracker, our approach can provide state-of-the-art segmentations on a range of\nobjects and image modalities (video and 3D volumes), and that these can then be\nused to train supervised machine learning classifiers. \n\n"}
{"id": "1809.00981", "contents": "Title: DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime\n  Classification Abstract: Deep learning has revolutionized the performance of classification, but\nmeanwhile demands sufficient labeled data for training. Given insufficient\ndata, while many techniques have been developed to help combat overfitting, the\nchallenge remains if one tries to train deep networks, especially in the\nill-posed extremely low data regimes: only a small set of labeled data are\navailable, and nothing -- including unlabeled data -- else. Such regimes arise\nfrom practical situations where not only data labeling but also data collection\nitself is expensive. We propose a deep adversarial data augmentation (DADA)\ntechnique to address the problem, in which we elaborately formulate data\naugmentation as a problem of training a class-conditional and supervised\ngenerative adversarial network (GAN). Specifically, a new discriminator loss is\nproposed to fit the goal of data augmentation, through which both real and\naugmented samples are enforced to contribute to and be consistent in finding\nthe decision boundaries. Tailored training techniques are developed\naccordingly. To quantitatively validate its effectiveness, we first perform\nextensive simulations to show that DADA substantially outperforms both\ntraditional data augmentation and a few GAN-based options. We then extend\nexperiments to three real-world small labeled datasets where existing data\naugmentation and/or transfer learning strategies are either less effective or\ninfeasible. All results endorse the superior capability of DADA in enhancing\nthe generalization ability of deep networks trained in practical extremely low\ndata regimes. Source code is available at\nhttps://github.com/SchafferZhang/DADA. \n\n"}
{"id": "1809.01015", "contents": "Title: Automated segmentation on the entire cardiac cycle using a deep learning\n  work-flow Abstract: The segmentation of the left ventricle (LV) from CINE MRI images is essential\nto infer important clinical parameters. Typically, machine learning algorithms\nfor automated LV segmentation use annotated contours from only two cardiac\nphases, diastole, and systole. In this work, we present an analysis work-flow\nfor fully-automated LV segmentation that learns from images acquired through\nthe cardiac cycle. The workflow consists of three components: first, for each\nimage in the sequence, we perform an automated localization and subsequent\ncropping of the bounding box containing the cardiac silhouette. Second, we\nidentify the LV contours using a Temporal Fully Convolutional Neural Network\n(T-FCNN), which extends Fully Convolutional Neural Networks (FCNN) through a\nrecurrent mechanism enforcing temporal coherence across consecutive frames.\nFinally, we further defined the boundaries using either one of two components:\nfully-connected Conditional Random Fields (CRFs) with Gaussian edge potentials\nand Semantic Flow. Our initial experiments suggest that significant improvement\nin performance can potentially be achieved by using a recurrent neural network\ncomponent that explicitly learns cardiac motion patterns whilst performing LV\nsegmentation. \n\n"}
{"id": "1809.01564", "contents": "Title: Traffic Density Estimation using a Convolutional Neural Network Abstract: The goal of this project is to introduce and present a machine learning\napplication that aims to improve the quality of life of people in Singapore. In\nparticular, we investigate the use of machine learning solutions to tackle the\nproblem of traffic congestion in Singapore. In layman's terms, we seek to make\nSingapore (or any other city) a smoother place. To accomplish this aim, we\npresent an end-to-end system comprising of 1. A traffic density estimation\nalgorithm at traffic lights/junctions and 2. a suitable traffic signal control\nalgorithms that make use of the density information for better traffic control.\nTraffic density estimation can be obtained from traffic junction images using\nvarious machine learning techniques (combined with CV tools). After research\ninto various advanced machine learning methods, we decided on convolutional\nneural networks (CNNs). We conducted experiments on our algorithms, using the\npublicly available traffic camera dataset published by the Land Transport\nAuthority (LTA) to demonstrate the feasibility of this approach. With these\ntraffic density estimates, different traffic algorithms can be applied to\nminimize congestion at traffic junctions in general. \n\n"}
{"id": "1809.02129", "contents": "Title: Structural Consistency and Controllability for Diverse Colorization Abstract: Colorizing a given gray-level image is an important task in the media and\nadvertising industry. Due to the ambiguity inherent to colorization (many\nshades are often plausible), recent approaches started to explicitly model\ndiversity. However, one of the most obvious artifacts, structural\ninconsistency, is rarely considered by existing methods which predict\nchrominance independently for every pixel. To address this issue, we develop a\nconditional random field based variational auto-encoder formulation which is\nable to achieve diversity while taking into account structural consistency.\nMoreover, we introduce a controllability mecha- nism that can incorporate\nexternal constraints from diverse sources in- cluding a user interface.\nCompared to existing baselines, we demonstrate that our method obtains more\ndiverse and globally consistent coloriza- tions on the LFW, LSUN-Church and\nILSVRC-2015 datasets. \n\n"}
{"id": "1809.02657", "contents": "Title: dyngraph2vec: Capturing Network Dynamics using Dynamic Graph\n  Representation Learning Abstract: Learning graph representations is a fundamental task aimed at capturing\nvarious properties of graphs in vector space. The most recent methods learn\nsuch representations for static networks. However, real world networks evolve\nover time and have varying dynamics. Capturing such evolution is key to\npredicting the properties of unseen networks. To understand how the network\ndynamics affect the prediction performance, we propose an embedding approach\nwhich learns the structure of evolution in dynamic graphs and can predict\nunseen links with higher precision. Our model, dyngraph2vec, learns the\ntemporal transitions in the network using a deep architecture composed of dense\nand recurrent layers. We motivate the need of capturing dynamics for prediction\non a toy data set created using stochastic block models. We then demonstrate\nthe efficacy of dyngraph2vec over existing state-of-the-art methods on two real\nworld data sets. We observe that learning dynamics can improve the quality of\nembedding and yield better performance in link prediction. \n\n"}
{"id": "1809.02940", "contents": "Title: Automated Strabismus Detection for Telemedicine Applications Abstract: Strabismus is one of the most influential ophthalmologic diseases in human's\nlife. Timely detection of strabismus contributes to its prognosis and\ntreatment. Telemedicine, which has great potential to alleviate the growing\ndemand of the diagnosis of ophthalmologic diseases, is an effective method to\nachieve timely strabismus detection. In this paper, a tele strabismus dataset\nis established by the ophthalmologists. Then an end-to-end framework named as\nRF-CNN is proposed to achieve automated strabismus detection on the established\ntele strabismus dataset. RF-CNN first performs eye region segmentation on each\nindividual image, and further classifies the segmented eye regions with deep\nneural networks. The experimental results on the established tele strabismus\ndataset demonstrates that the proposed RF-CNN can have a good performance on\nautomated strabismus detection for telemedicine application. Code is made\npublicly available at:\nhttps://github.com/jieWeiLu/Strabismus-Detection-for-Telemedicine-Application. \n\n"}
{"id": "1809.03239", "contents": "Title: Multi-Context Deep Network for Angle-Closure Glaucoma Screening in\n  Anterior Segment OCT Abstract: A major cause of irreversible visual impairment is angle-closure glaucoma,\nwhich can be screened through imagery from Anterior Segment Optical Coherence\nTomography (AS-OCT). Previous computational diagnostic techniques address this\nscreening problem by extracting specific clinical measurements or handcrafted\nvisual features from the images for classification. In this paper, we instead\npropose to learn from training data a discriminative representation that may\ncapture subtle visual cues not modeled by predefined features. Based on\nclinical priors, we formulate this learning with a presented Multi-Context Deep\nNetwork (MCDN) architecture, in which parallel Convolutional Neural Networks\nare applied to particular image regions and at corresponding scales known to be\ninformative for clinically diagnosing angle-closure glaucoma. The output\nfeature maps of the parallel streams are merged into a classification layer to\nproduce the deep screening result. Moreover, we incorporate estimated clinical\nparameters to further enhance performance. On a clinical AS-OCT dataset, our\nsystem is validated through comparisons to previous screening methods. \n\n"}
{"id": "1809.03318", "contents": "Title: Towards Efficient Convolutional Neural Network for Domain-Specific\n  Applications on FPGA Abstract: FPGA becomes a popular technology for implementing Convolutional Neural\nNetwork (CNN) in recent years. Most CNN applications on FPGA are\ndomain-specific, e.g., detecting objects from specific categories, in which\ncommonly-used CNN models pre-trained on general datasets may not be efficient\nenough. This paper presents TuRF, an end-to-end CNN acceleration framework to\nefficiently deploy domain-specific applications on FPGA by transfer learning\nthat adapts pre-trained models to specific domains, replacing standard\nconvolution layers with efficient convolution blocks, and applying layer fusion\nto enhance hardware design performance. We evaluate TuRF by deploying a\npre-trained VGG-16 model for a domain-specific image recognition task onto a\nStratix V FPGA. Results show that designs generated by TuRF achieve better\nperformance than prior methods for the original VGG-16 and ResNet-50 models,\nwhile for the optimised VGG-16 model TuRF designs are more accurate and easier\nto process. \n\n"}
{"id": "1809.04696", "contents": "Title: Geometric Image Synthesis Abstract: The task of generating natural images from 3D scenes has been a long standing\ngoal in computer graphics. On the other hand, recent developments in deep\nneural networks allow for trainable models that can produce natural-looking\nimages with little or no knowledge about the scene structure. While the\ngenerated images often consist of realistic looking local patterns, the overall\nstructure of the generated images is often inconsistent. In this work we\npropose a trainable, geometry-aware image generation method that leverages\nvarious types of scene information, including geometry and segmentation, to\ncreate realistic looking natural images that match the desired scene structure.\nOur geometrically-consistent image synthesis method is a deep neural network,\ncalled Geometry to Image Synthesis (GIS) framework, which retains the\nadvantages of a trainable method, e.g., differentiability and adaptiveness,\nbut, at the same time, makes a step towards the generalizability, control and\nquality output of modern graphics rendering engines. We utilize the GIS\nframework to insert vehicles in outdoor driving scenes, as well as to generate\nnovel views of objects from the Linemod dataset. We qualitatively show that our\nnetwork is able to generalize beyond the training set to novel scene\ngeometries, object shapes and segmentations. Furthermore, we quantitatively\nshow that the GIS framework can be used to synthesize large amounts of training\ndata which proves beneficial for training instance segmentation models. \n\n"}
{"id": "1809.04820", "contents": "Title: Canonical and Compact Point Cloud Representation for Shape\n  Classification Abstract: We present a novel compact point cloud representation that is inherently\ninvariant to scale, coordinate change and point permutation. The key idea is to\nparametrize a distance field around an individual shape into a unique,\ncanonical, and compact vector in an unsupervised manner. We firstly project a\ndistance field to a $4$D canonical space using singular value decomposition. We\nthen train a neural network for each instance to non-linearly embed its\ndistance field into network parameters. We employ a bias-free Extreme Learning\nMachine (ELM) with ReLU activation units, which has scale-factor commutative\nproperty between layers. We demonstrate the descriptiveness of the\ninstance-wise, shape-embedded network parameters by using them to classify\nshapes in $3$D datasets. Our learning-based representation requires minimal\naugmentation and simple neural networks, where previous approaches demand\nnumerous representations to handle coordinate change and point permutation. \n\n"}
{"id": "1809.05680", "contents": "Title: A New Multi-vehicle Trajectory Generator to Simulate Vehicle-to-Vehicle\n  Encounters Abstract: Generating multi-vehicle trajectories from existing limited data can provide\nrich resources for autonomous vehicle development and testing. This paper\nintroduces a multi-vehicle trajectory generator (MTG) that can encode\nmulti-vehicle interaction scenarios (called driving encounters) into an\ninterpretable representation from which new driving encounter scenarios are\ngenerated by sampling. The MTG consists of a bi-directional encoder and a\nmulti-branch decoder. A new disentanglement metric is then developed for model\nanalyses and comparisons in terms of model robustness and the independence of\nthe latent codes. Comparison of our proposed MTG with $\\beta$-VAE and InfoGAN\ndemonstrates that the MTG has stronger capability to purposely generate\nrational vehicle-to-vehicle encounters through operating the disentangled\nlatent codes. Thus the MTG could provide more data for engineers and\nresearchers to develop testing and evaluation scenarios for autonomous\nvehicles. \n\n"}
{"id": "1809.05848", "contents": "Title: Towards Good Practices for Multi-modal Fusion in Large-scale Video\n  Classification Abstract: Leveraging both visual frames and audio has been experimentally proven\neffective to improve large-scale video classification. Previous research on\nvideo classification mainly focuses on the analysis of visual content among\nextracted video frames and their temporal feature aggregation. In contrast,\nmultimodal data fusion is achieved by simple operators like average and\nconcatenation. Inspired by the success of bilinear pooling in the visual and\nlanguage fusion, we introduce multi-modal factorized bilinear pooling (MFB) to\nfuse visual and audio representations. We combine MFB with different\nvideo-level features and explore its effectiveness in video classification.\nExperimental results on the challenging Youtube-8M v2 dataset demonstrate that\nMFB significantly outperforms simple fusion methods in large-scale video\nclassification. \n\n"}
{"id": "1809.06065", "contents": "Title: Focal Loss in 3D Object Detection Abstract: 3D object detection is still an open problem in autonomous driving scenes.\nWhen recognizing and localizing key objects from sparse 3D inputs, autonomous\nvehicles suffer from a larger continuous searching space and higher\nfore-background imbalance compared to image-based object detection. In this\npaper, we aim to solve this fore-background imbalance in 3D object detection.\nInspired by the recent use of focal loss in image-based object detection, we\nextend this hard-mining improvement of binary cross entropy to\npoint-cloud-based object detection and conduct experiments to show its\nperformance based on two different 3D detectors: 3D-FCN and VoxelNet. The\nevaluation results show up to 11.2AP gains through the focal loss in a wide\nrange of hyperparameters for 3D object detection. \n\n"}
{"id": "1809.06181", "contents": "Title: Dual Encoding for Zero-Example Video Retrieval Abstract: This paper attacks the challenging problem of zero-example video retrieval.\nIn such a retrieval paradigm, an end user searches for unlabeled videos by\nad-hoc queries described in natural language text with no visual example\nprovided. Given videos as sequences of frames and queries as sequences of\nwords, an effective sequence-to-sequence cross-modal matching is required. The\nmajority of existing methods are concept based, extracting relevant concepts\nfrom queries and videos and accordingly establishing associations between the\ntwo modalities. In contrast, this paper takes a concept-free approach,\nproposing a dual deep encoding network that encodes videos and queries into\npowerful dense representations of their own. Dual encoding is conceptually\nsimple, practically effective and end-to-end. As experiments on three\nbenchmarks, i.e. MSR-VTT, TRECVID 2016 and 2017 Ad-hoc Video Search show, the\nproposed solution establishes a new state-of-the-art for zero-example video\nretrieval. \n\n"}
{"id": "1809.06276", "contents": "Title: Retrospective correction of Rigid and Non-Rigid MR motion artifacts\n  using GANs Abstract: Motion artifacts are a primary source of magnetic resonance (MR) image\nquality deterioration with strong repercussions on diagnostic performance.\nCurrently, MR motion correction is carried out either prospectively, with the\nhelp of motion tracking systems, or retrospectively by mainly utilizing\ncomputationally expensive iterative algorithms. In this paper, we utilize a new\nadversarial framework, titled MedGAN, for the joint retrospective correction of\nrigid and non-rigid motion artifacts in different body regions and without the\nneed for a reference image. MedGAN utilizes a unique combination of\nnon-adversarial losses and a new generator architecture to capture the textures\nand fine-detailed structures of the desired artifact-free MR images.\nQuantitative and qualitative comparisons with other adversarial techniques have\nillustrated the proposed model performance. \n\n"}
{"id": "1809.06691", "contents": "Title: SECS: Efficient Deep Stream Processing via Class Skew Dichotomy Abstract: Despite that accelerating convolutional neural network (CNN) receives an\nincreasing research focus, the save on resource consumption always comes with a\ndecrease in accuracy. To both increase accuracy and decrease resource\nconsumption, we explore an environment information, called class skew, which is\neasily available and exists widely in daily life. Since the class skew may\nswitch as time goes, we bring up probability layer to utilize class skew\nwithout any overhead during the runtime. Further, we observe class skew\ndichotomy that some class skew may appear frequently in the future, called hot\nclass skew, and others will never appear again or appear seldom, called cold\nclass skew. Inspired by techniques from source code optimization, two modes,\ni.e., interpretation and compilation, are proposed. The interpretation mode\npursues efficient adaption during runtime for cold class skew and the\ncompilation mode aggressively optimize on hot ones for more efficient\ndeployment in the future. Aggressive optimization is processed by\nclass-specific pruning and provides extra benefit. Finally, we design a\nsystematic framework, SECS, to dynamically detect class skew, processing\ninterpretation and compilation, as well as select the most accurate\narchitectures under the runtime resource budget. Extensive evaluations show\nthat SECS can realize end-to-end classification speedups by a factor of 3x to\n11x relative to state-of-the-art convolutional neural networks, at a higher\naccuracy. \n\n"}
{"id": "1809.06893", "contents": "Title: SilhoNet: An RGB Method for 6D Object Pose Estimation Abstract: Autonomous robot manipulation involves estimating the translation and\norientation of the object to be manipulated as a 6-degree-of-freedom (6D) pose.\nMethods using RGB-D data have shown great success in solving this problem.\nHowever, there are situations where cost constraints or the working environment\nmay limit the use of RGB-D sensors. When limited to monocular camera data only,\nthe problem of object pose estimation is very challenging. In this work, we\nintroduce a novel method called SilhoNet that predicts 6D object pose from\nmonocular images. We use a Convolutional Neural Network (CNN) pipeline that\ntakes in Region of Interest (ROI) proposals to simultaneously predict an\nintermediate silhouette representation for objects with an associated occlusion\nmask and a 3D translation vector. The 3D orientation is then regressed from the\npredicted silhouettes. We show that our method achieves better overall\nperformance on the YCB-Video dataset than two state-of-the art networks for 6D\npose estimation from monocular image input. \n\n"}
{"id": "1809.06973", "contents": "Title: Wearable-based Mediation State Detection in Individuals with Parkinson's\n  Disease Abstract: One of the most prevalent complaints of individuals with mid-stage and\nadvanced Parkinson's disease (PD) is the fluctuating response to their\nmedication (i.e., ON state with maximum benefit from medication and OFF state\nwith no benefit from medication). In order to address these motor fluctuations,\nthe patients go through periodic clinical examination where the treating\nphysician reviews the patients' self-report about duration in different\nmedication states and optimize therapy accordingly. Unfortunately, the\npatients' self-report can be unreliable and suffer from recall bias. There is a\nneed to a technology-based system that can provide objective measures about the\nduration in different medication states that can be used by the treating\nphysician to successfully adjust the therapy. In this paper, we developed a\nmedication state detection algorithm to detect medication states using two\nwearable motion sensors. A series of significant features are extracted from\nthe motion data and used in a classifier that is based on a support vector\nmachine with fuzzy labeling. The developed algorithm is evaluated using a\ndataset with 19 PD subjects and a total duration of 1,052.24 minutes (17.54\nhours). The algorithm resulted in an average classification accuracy of 90.5%,\nsensitivity of 94.2%, and specificity of 85.4%. \n\n"}
{"id": "1809.09326", "contents": "Title: Multigrid Backprojection Super-Resolution and Deep Filter Visualization Abstract: We introduce a novel deep-learning architecture for image upscaling by large\nfactors (e.g. 4x, 8x) based on examples of pristine high-resolution images. Our\ntarget is to reconstruct high-resolution images from their downscale versions.\nThe proposed system performs a multi-level progressive upscaling, starting from\nsmall factors (2x) and updating for higher factors (4x and 8x). The system is\nrecursive as it repeats the same procedure at each level. It is also residual\nsince we use the network to update the outputs of a classic upscaler. The\nnetwork residuals are improved by Iterative Back-Projections (IBP) computed in\nthe features of a convolutional network. To work in multiple levels we extend\nthe standard back-projection algorithm using a recursion analogous to\nMulti-Grid algorithms commonly used as solvers of large systems of linear\nequations. We finally show how the network can be interpreted as a standard\nupsampling-and-filter upscaler with a space-variant filter that adapts to the\ngeometry. This approach allows us to visualize how the network learns to\nupscale. Finally, our system reaches state of the art quality for models with\nrelatively few number of parameters. \n\n"}
{"id": "1809.09761", "contents": "Title: PhotoShape: Photorealistic Materials for Large-Scale Shape Collections Abstract: Existing online 3D shape repositories contain thousands of 3D models but lack\nphotorealistic appearance. We present an approach to automatically assign\nhigh-quality, realistic appearance models to large scale 3D shape collections.\nThe key idea is to jointly leverage three types of online data -- shape\ncollections, material collections, and photo collections, using the photos as\nreference to guide assignment of materials to shapes. By generating a large\nnumber of synthetic renderings, we train a convolutional neural network to\nclassify materials in real photos, and employ 3D-2D alignment techniques to\ntransfer materials to different parts of each shape model. Our system produces\nphotorealistic, relightable, 3D shapes (PhotoShapes). \n\n"}
{"id": "1809.09924", "contents": "Title: Hierarchy-based Image Embeddings for Semantic Image Retrieval Abstract: Deep neural networks trained for classification have been found to learn\npowerful image representations, which are also often used for other tasks such\nas comparing images w.r.t. their visual similarity. However, visual similarity\ndoes not imply semantic similarity. In order to learn semantically\ndiscriminative features, we propose to map images onto class embeddings whose\npair-wise dot products correspond to a measure of semantic similarity between\nclasses. Such an embedding does not only improve image retrieval results, but\ncould also facilitate integrating semantics for other tasks, e.g., novelty\ndetection or few-shot learning. We introduce a deterministic algorithm for\ncomputing the class centroids directly based on prior world-knowledge encoded\nin a hierarchy of classes such as WordNet. Experiments on CIFAR-100, NABirds,\nand ImageNet show that our learned semantic image embeddings improve the\nsemantic consistency of image retrieval results by a large margin. \n\n"}
{"id": "1809.10080", "contents": "Title: Multispecies fruit flower detection using a refined semantic\n  segmentation network Abstract: In fruit production, critical crop management decisions are guided by bloom\nintensity, i.e., the number of flowers present in an orchard. Despite its\nimportance, bloom intensity is still typically estimated by means of human\nvisual inspection. Existing automated computer vision systems for flower\nidentification are based on hand-engineered techniques that work only under\nspecific conditions and with limited performance. This work proposes an\nautomated technique for flower identification that is robust to uncontrolled\nenvironments and applicable to different flower species. Our method relies on\nan end-to-end residual convolutional neural network (CNN) that represents the\nstate-of-the-art in semantic segmentation. To enhance its sensitivity to\nflowers, we fine-tune this network using a single dataset of apple flower\nimages. Since CNNs tend to produce coarse segmentations, we employ a refinement\nmethod to better distinguish between individual flower instances. Without any\npre-processing or dataset-specific training, experimental results on images of\napple, peach and pear flowers, acquired under different conditions demonstrate\nthe robustness and broad applicability of our method. \n\n"}
{"id": "1809.10229", "contents": "Title: Automatic Dataset Annotation to Learn CNN Pore Description for\n  Fingerprint Recognition Abstract: High-resolution fingerprint recognition often relies on sophisticated\nmatching algorithms based on hand-crafted keypoint descriptors, with pores\nbeing the most common keypoint choice. Our method is the opposite of the\nprevalent approach: we use instead a simple matching algorithm based on robust\nlocal pore descriptors that are learned from the data using a CNN. In order to\ntrain this CNN in a fully supervised manner, we describe how the automatic\nalignment of fingerprint images can be used to obtain the required training\nannotations, which are otherwise missing in all publicly available datasets.\nThis improves the state-of-the-art recognition results for both partial and\nfull fingerprints in a public benchmark. To confirm that the observed\nimprovement is due to the adoption of learned descriptors, we conduct an\nablation study using the most successful pore descriptors previously used in\nthe literature. All our code is available at\nhttps://github.com/gdahia/high-res-fingerprint-recognition \n\n"}
{"id": "1809.11100", "contents": "Title: Rethinking Self-driving: Multi-task Knowledge for Better Generalization\n  and Accident Explanation Ability Abstract: Current end-to-end deep learning driving models have two problems: (1) Poor\ngeneralization ability of unobserved driving environment when diversity of\ntraining driving dataset is limited (2) Lack of accident explanation ability\nwhen driving models don't work as expected. To tackle these two problems,\nrooted on the believe that knowledge of associated easy task is benificial for\naddressing difficult task, we proposed a new driving model which is composed of\nperception module for \\textit{see and think} and driving module for\n\\textit{behave}, and trained it with multi-task perception-related basic\nknowledge and driving knowledge stepwisely.\n  Specifically segmentation map and depth map (pixel level understanding of\nimages) were considered as \\textit{what \\& where} and \\textit{how far}\nknowledge for tackling easier driving-related perception problems before\ngenerating final control commands for difficult driving task. The results of\nexperiments demonstrated the effectiveness of multi-task perception knowledge\nfor better generalization and accident explanation ability. With our method the\naverage sucess rate of finishing most difficult navigation tasks in untrained\ncity of CoRL test surpassed current benchmark method for 15 percent in trained\nweather and 20 percent in untrained weathers. Demonstration video link is:\nhttps://www.youtube.com/watch?v=N7ePnnZZwdE \n\n"}
{"id": "1810.00345", "contents": "Title: Pixel and Feature Level Based Domain Adaption for Object Detection in\n  Autonomous Driving Abstract: Annotating large scale datasets to train modern convolutional neural networks\nis prohibitively expensive and time-consuming for many real tasks. One\nalternative is to train the model on labeled synthetic datasets and apply it in\nthe real scenes. However, this straightforward method often fails to generalize\nwell mainly due to the domain bias between the synthetic and real datasets.\nMany unsupervised domain adaptation (UDA) methods are introduced to address\nthis problem but most of them only focus on the simple classification task. In\nthis paper, we present a novel UDA model to solve the more complex object\ndetection problem in the context of autonomous driving. Our model integrates\nboth pixel level and feature level based transformtions to fulfill the cross\ndomain detection task and can be further trained end-to-end to pursue better\nperformance. We employ objectives of the generative adversarial network and the\ncycle consistency loss for image translation in the pixel space. To address the\npotential semantic inconsistency problem, we propose region proposal based\nfeature adversarial training to preserve the semantics of our target objects as\nwell as further minimize the domain shifts. Extensive experiments are conducted\non several different datasets, and the results demonstrate the robustness and\nsuperiority of our method. \n\n"}
{"id": "1810.00475", "contents": "Title: Deep Learning for End-to-End Atrial Fibrillation Recurrence Estimation Abstract: Left atrium shape has been shown to be an independent predictor of recurrence\nafter atrial fibrillation (AF) ablation. Shape-based representation is\nimperative to such an estimation process, where correspondence-based\nrepresentation offers the most flexibility and ease-of-computation for\npopulation-level shape statistics. Nonetheless, population-level shape\nrepresentations in the form of image segmentation and correspondence models\nderived from cardiac MRI require significant human resources with sufficient\nanatomy-specific expertise. In this paper, we propose a machine learning\napproach that uses deep networks to estimate AF recurrence by predicting shape\ndescriptors directly from MRI images, with NO image pre-processing involved. We\nalso propose a novel data augmentation scheme to effectively train a deep\nnetwork in a limited training data setting. We compare this new method of\nestimating shape descriptors from images with the state-of-the-art\ncorrespondence-based shape modeling that requires image segmentation and\ncorrespondence optimization. Results show that the proposed method and the\ncurrent state-of-the-art produce statistically similar outcomes on AF\nrecurrence, eliminating the need for expensive pre-processing pipelines and\nassociated human labor. \n\n"}
{"id": "1810.00482", "contents": "Title: Few-Shot Goal Inference for Visuomotor Learning and Planning Abstract: Reinforcement learning and planning methods require an objective or reward\nfunction that encodes the desired behavior. Yet, in practice, there is a wide\nrange of scenarios where an objective is difficult to provide programmatically,\nsuch as tasks with visual observations involving unknown object positions or\ndeformable objects. In these cases, prior methods use engineered\nproblem-specific solutions, e.g., by instrumenting the environment with\nadditional sensors to measure a proxy for the objective. Such solutions require\na significant engineering effort on a per-task basis, and make it impractical\nfor robots to continuously learn complex skills outside of laboratory settings.\nWe aim to find a more general and scalable solution for specifying goals for\nrobot learning in unconstrained environments. To that end, we formulate the\nfew-shot objective learning problem, where the goal is to learn a task\nobjective from only a few example images of successful end states for that\ntask. We propose a simple solution to this problem: meta-learn a classifier\nthat can recognize new goals from a few examples. We show how this approach can\nbe used with both model-free reinforcement learning and visual model-based\nplanning and show results in three domains: rope manipulation from images in\nsimulation, visual navigation in a simulated 3D environment, and object\narrangement into user-specified configurations on a real robot. \n\n"}
{"id": "1810.00740", "contents": "Title: Improving the Generalization of Adversarial Training with Domain\n  Adaptation Abstract: By injecting adversarial examples into training data, adversarial training is\npromising for improving the robustness of deep learning models. However, most\nexisting adversarial training approaches are based on a specific type of\nadversarial attack. It may not provide sufficiently representative samples from\nthe adversarial domain, leading to a weak generalization ability on adversarial\nexamples from other attacks. Moreover, during the adversarial training,\nadversarial perturbations on inputs are usually crafted by fast single-step\nadversaries so as to scale to large datasets. This work is mainly focused on\nthe adversarial training yet efficient FGSM adversary. In this scenario, it is\ndifficult to train a model with great generalization due to the lack of\nrepresentative adversarial samples, aka the samples are unable to accurately\nreflect the adversarial domain. To alleviate this problem, we propose a novel\nAdversarial Training with Domain Adaptation (ATDA) method. Our intuition is to\nregard the adversarial training on FGSM adversary as a domain adaption task\nwith limited number of target domain samples. The main idea is to learn a\nrepresentation that is semantically meaningful and domain invariant on the\nclean domain as well as the adversarial domain. Empirical evaluations on\nFashion-MNIST, SVHN, CIFAR-10 and CIFAR-100 demonstrate that ATDA can greatly\nimprove the generalization of adversarial training and the smoothness of the\nlearned models, and outperforms state-of-the-art methods on standard benchmark\ndatasets. To show the transfer ability of our method, we also extend ATDA to\nthe adversarial training on iterative attacks such as PGD-Adversial Training\n(PAT) and the defense performance is improved considerably. \n\n"}
{"id": "1810.01104", "contents": "Title: Target Aware Network Adaptation for Efficient Representation Learning Abstract: This paper presents an automatic network adaptation method that finds a\nConvNet structure well-suited to a given target task, e.g., image\nclassification, for efficiency as well as accuracy in transfer learning. We\ncall the concept target-aware transfer learning. Given only small-scale labeled\ndata, and starting from an ImageNet pre-trained network, we exploit a scheme of\nremoving its potential redundancy for the target task through iterative\noperations of filter-wise pruning and network optimization. The basic\nmotivation is that compact networks are on one hand more efficient and should\nalso be more tolerant, being less complex, against the risk of overfitting\nwhich would hinder the generalization of learned representations in the context\nof transfer learning. Further, unlike existing methods involving network\nsimplification, we also let the scheme identify redundant portions across the\nentire network, which automatically results in a network structure adapted to\nthe task at hand. We achieve this with a few novel ideas: (i) cumulative sum of\nactivation statistics for each layer, and (ii) a priority evaluation of pruning\nacross multiple layers. Experimental results by the method on five datasets\n(Flower102, CUB200-2011, Dog120, MIT67, and Stanford40) show favorable\naccuracies over the related state-of-the-art techniques while enhancing the\ncomputational and storage efficiency of the transferred model. \n\n"}
{"id": "1810.01152", "contents": "Title: Learning Discriminators as Energy Networks in Adversarial Learning Abstract: We propose a novel framework for structured prediction via adversarial\nlearning. Existing adversarial learning methods involve two separate networks,\ni.e., the structured prediction models and the discriminative models, in the\ntraining. The information captured by discriminative models complements that in\nthe structured prediction models, but few existing researches have studied on\nutilizing such information to improve structured prediction models at the\ninference stage. In this work, we propose to refine the predictions of\nstructured prediction models by effectively integrating discriminative models\ninto the prediction. Discriminative models are treated as energy-based models.\nSimilar to the adversarial learning, discriminative models are trained to\nestimate scores which measure the quality of predicted outputs, while\nstructured prediction models are trained to predict contrastive outputs with\nmaximal energy scores. In this way, the gradient vanishing problem is\nameliorated, and thus we are able to perform inference by following the ascent\ngradient directions of discriminative models to refine structured prediction\nmodels. The proposed method is able to handle a range of tasks, e.g.,\nmulti-label classification and image segmentation. Empirical results on these\ntwo tasks validate the effectiveness of our learning method. \n\n"}
{"id": "1810.01163", "contents": "Title: An Entropic Optimal Transport Loss for Learning Deep Neural Networks\n  under Label Noise in Remote Sensing Images Abstract: Deep neural networks have established as a powerful tool for large scale\nsupervised classification tasks. The state-of-the-art performances of deep\nneural networks are conditioned to the availability of large number of\naccurately labeled samples. In practice, collecting large scale accurately\nlabeled datasets is a challenging and tedious task in most scenarios of remote\nsensing image analysis, thus cheap surrogate procedures are employed to label\nthe dataset. Training deep neural networks on such datasets with inaccurate\nlabels easily overfits to the noisy training labels and degrades the\nperformance of the classification tasks drastically. To mitigate this effect,\nwe propose an original solution with entropic optimal transportation. It allows\nto learn in an end-to-end fashion deep neural networks that are, to some\nextent, robust to inaccurately labeled samples. We empirically demonstrate on\nseveral remote sensing datasets, where both scene and pixel-based hyperspectral\nimages are considered for classification. Our method proves to be highly\ntolerant to significant amounts of label noise and achieves favorable results\nagainst state-of-the-art methods. \n\n"}
{"id": "1810.01849", "contents": "Title: SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation Abstract: Recent techniques in self-supervised monocular depth estimation are\napproaching the performance of supervised methods, but operate in low\nresolution only. We show that high resolution is key towards high-fidelity\nself-supervised monocular depth prediction. Inspired by recent deep learning\nmethods for Single-Image Super-Resolution, we propose a sub-pixel convolutional\nlayer extension for depth super-resolution that accurately synthesizes\nhigh-resolution disparities from their corresponding low-resolution\nconvolutional features. In addition, we introduce a differentiable\nflip-augmentation layer that accurately fuses predictions from the image and\nits horizontally flipped version, reducing the effect of left and right shadow\nregions generated in the disparity map due to occlusions. Both contributions\nprovide significant performance gains over the state-of-the-art in\nself-supervised depth and pose estimation on the public KITTI benchmark. A\nvideo of our approach can be found at https://youtu.be/jKNgBeBMx0I. \n\n"}
{"id": "1810.01868", "contents": "Title: Set Aggregation Network as a Trainable Pooling Layer Abstract: Global pooling, such as max- or sum-pooling, is one of the key ingredients in\ndeep neural networks used for processing images, texts, graphs and other types\nof structured data. Based on the recent DeepSets architecture proposed by\nZaheer et al. (NIPS 2017), we introduce a Set Aggregation Network (SAN) as an\nalternative global pooling layer. In contrast to typical pooling operators, SAN\nallows to embed a given set of features to a vector representation of arbitrary\nsize. We show that by adjusting the size of embedding, SAN is capable of\npreserving the whole information from the input. In experiments, we demonstrate\nthat replacing global pooling layer by SAN leads to the improvement of\nclassification accuracy. Moreover, it is less prone to overfitting and can be\nused as a regularizer. \n\n"}
{"id": "1810.02020", "contents": "Title: Transfer Incremental Learning using Data Augmentation Abstract: Deep learning-based methods have reached state of the art performances,\nrelying on large quantity of available data and computational power. Such\nmethods still remain highly inappropriate when facing a major open machine\nlearning problem, which consists of learning incrementally new classes and\nexamples over time. Combining the outstanding performances of Deep Neural\nNetworks (DNNs) with the flexibility of incremental learning techniques is a\npromising venue of research. In this contribution, we introduce Transfer\nIncremental Learning using Data Augmentation (TILDA). TILDA is based on\npre-trained DNNs as feature extractor, robust selection of feature vectors in\nsubspaces using a nearest-class-mean based technique, majority votes and data\naugmentation at both the training and the prediction stages. Experiments on\nchallenging vision datasets demonstrate the ability of the proposed method for\nlow complexity incremental learning, while achieving significantly better\naccuracy than existing incremental counterparts. \n\n"}
{"id": "1810.02068", "contents": "Title: Towards Fast and Energy-Efficient Binarized Neural Network Inference on\n  FPGA Abstract: Binarized Neural Network (BNN) removes bitwidth redundancy in classical CNN\nby using a single bit (-1/+1) for network parameters and intermediate\nrepresentations, which has greatly reduced the off-chip data transfer and\nstorage overhead. However, a large amount of computation redundancy still\nexists in BNN inference. By analyzing local properties of images and the\nlearned BNN kernel weights, we observe an average of $\\sim$78% input similarity\nand $\\sim$59% weight similarity among weight kernels, measured by our proposed\nmetric in common network architectures. Thus there does exist redundancy that\ncan be exploited to further reduce the amount of on-chip computations.\n  Motivated by the observation, in this paper, we proposed two types of fast\nand energy-efficient architectures for BNN inference. We also provide analysis\nand insights to pick the better strategy of these two for different datasets\nand network models. By reusing the results from previous computation, much\ncycles for data buffer access and computations can be skipped. By experiments,\nwe demonstrate that 80% of the computation and 40% of the buffer access can be\nskipped by exploiting BNN similarity. Thus, our design can achieve 17%\nreduction in total power consumption, 54% reduction in on-chip power\nconsumption and 2.4$\\times$ maximum speedup, compared to the baseline without\napplying our reuse technique. Our design also shows 1.9$\\times$ more\narea-efficiency compared to state-of-the-art BNN inference design. We believe\nour deployment of BNN on FPGA leads to a promising future of running deep\nlearning models on mobile devices. \n\n"}
{"id": "1810.02274", "contents": "Title: Episodic Curiosity through Reachability Abstract: Rewards are sparse in the real world and most of today's reinforcement\nlearning algorithms struggle with such sparsity. One solution to this problem\nis to allow the agent to create rewards for itself - thus making rewards dense\nand more suitable for learning. In particular, inspired by curious behaviour in\nanimals, observing something novel could be rewarded with a bonus. Such bonus\nis summed up with the real task reward - making it possible for RL algorithms\nto learn from the combined reward. We propose a new curiosity method which uses\nepisodic memory to form the novelty bonus. To determine the bonus, the current\nobservation is compared with the observations in memory. Crucially, the\ncomparison is done based on how many environment steps it takes to reach the\ncurrent observation from those in memory - which incorporates rich information\nabout environment dynamics. This allows us to overcome the known \"couch-potato\"\nissues of prior work - when the agent finds a way to instantly gratify itself\nby exploiting actions which lead to hardly predictable consequences. We test\nour approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo. In\nnavigational tasks from ViZDoom and DMLab, our agent outperforms the\nstate-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our\ncuriosity module learns locomotion out of the first-person-view curiosity only. \n\n"}
{"id": "1810.02340", "contents": "Title: SNIP: Single-shot Network Pruning based on Connection Sensitivity Abstract: Pruning large neural networks while maintaining their performance is often\ndesirable due to the reduced space and time complexity. In existing methods,\npruning is done within an iterative optimization procedure with either\nheuristically designed pruning schedules or additional hyperparameters,\nundermining their utility. In this work, we present a new approach that prunes\na given network once at initialization prior to training. To achieve this, we\nintroduce a saliency criterion based on connection sensitivity that identifies\nstructurally important connections in the network for the given task. This\neliminates the need for both pretraining and the complex pruning schedule while\nmaking it robust to architecture variations. After pruning, the sparse network\nis trained in the standard way. Our method obtains extremely sparse networks\nwith virtually the same accuracy as the reference network on the MNIST,\nCIFAR-10, and Tiny-ImageNet classification tasks and is broadly applicable to\nvarious architectures including convolutional, residual and recurrent networks.\nUnlike existing methods, our approach enables us to demonstrate that the\nretained connections are indeed relevant to the given task. \n\n"}
{"id": "1810.02936", "contents": "Title: FD-GAN: Pose-guided Feature Distilling GAN for Robust Person\n  Re-identification Abstract: Person re-identification (reID) is an important task that requires to\nretrieve a person's images from an image dataset, given one image of the person\nof interest. For learning robust person features, the pose variation of person\nimages is one of the key challenges. Existing works targeting the problem\neither perform human alignment, or learn human-region-based representations.\nExtra pose information and computational cost is generally required for\ninference. To solve this issue, a Feature Distilling Generative Adversarial\nNetwork (FD-GAN) is proposed for learning identity-related and pose-unrelated\nrepresentations. It is a novel framework based on a Siamese structure with\nmultiple novel discriminators on human poses and identities. In addition to the\ndiscriminators, a novel same-pose loss is also integrated, which requires\nappearance of a same person's generated images to be similar. After learning\npose-unrelated person features with pose guidance, no auxiliary pose\ninformation and additional computational cost is required during testing. Our\nproposed FD-GAN achieves state-of-the-art performance on three person reID\ndatasets, which demonstrates that the effectiveness and robust feature\ndistilling capability of the proposed FD-GAN. \n\n"}
{"id": "1810.03105", "contents": "Title: ASVRG: Accelerated Proximal SVRG Abstract: This paper proposes an accelerated proximal stochastic variance reduced\ngradient (ASVRG) method, in which we design a simple and effective momentum\nacceleration trick. Unlike most existing accelerated stochastic variance\nreduction methods such as Katyusha, ASVRG has only one additional variable and\none momentum parameter. Thus, ASVRG is much simpler than those methods, and has\nmuch lower per-iteration complexity. We prove that ASVRG achieves the best\nknown oracle complexities for both strongly convex and non-strongly convex\nobjectives. In addition, we extend ASVRG to mini-batch and non-smooth settings.\nWe also empirically verify our theoretical results and show that the\nperformance of ASVRG is comparable with, and sometimes even better than that of\nthe state-of-the-art stochastic methods. \n\n"}
{"id": "1810.03986", "contents": "Title: SAM-GCNN: A Gated Convolutional Neural Network with Segment-Level\n  Attention Mechanism for Home Activity Monitoring Abstract: In this paper, we propose a method for home activity monitoring. We\ndemonstrate our model on dataset of Detection and Classification of Acoustic\nScenes and Events (DCASE) 2018 Challenge Task 5. This task aims to classify\nmulti-channel audios into one of the provided pre-defined classes. All of these\nclasses are daily activities performed in a home environment. To tackle this\ntask, we propose a gated convolutional neural network with segment-level\nattention mechanism (SAM-GCNN). The proposed framework is a convolutional model\nwith two auxiliary modules: a gated convolutional neural network and a\nsegment-level attention mechanism. Furthermore, we adopted model ensemble to\nenhance the capability of generalization of our model. We evaluated our work on\nthe development dataset of DCASE 2018 Task 5 and achieved competitive\nperformance, with a macro-averaged F-1 score increasing from 83.76% to 89.33%,\ncompared with the convolutional baseline system. \n\n"}
{"id": "1810.04093", "contents": "Title: Geometry meets semantics for semi-supervised monocular depth estimation Abstract: Depth estimation from a single image represents a very exciting challenge in\ncomputer vision. While other image-based depth sensing techniques leverage on\nthe geometry between different viewpoints (e.g., stereo or structure from\nmotion), the lack of these cues within a single image renders ill-posed the\nmonocular depth estimation task. For inference, state-of-the-art\nencoder-decoder architectures for monocular depth estimation rely on effective\nfeature representations learned at training time. For unsupervised training of\nthese models, geometry has been effectively exploited by suitable images\nwarping losses computed from views acquired by a stereo rig or a moving camera.\nIn this paper, we make a further step forward showing that learning semantic\ninformation from images enables to improve effectively monocular depth\nestimation as well. In particular, by leveraging on semantically labeled images\ntogether with unsupervised signals gained by geometry through an image warping\nloss, we propose a deep learning approach aimed at joint semantic segmentation\nand depth estimation. Our overall learning framework is semi-supervised, as we\ndeploy groundtruth data only in the semantic domain. At training time, our\nnetwork learns a common feature representation for both tasks and a novel\ncross-task loss function is proposed. The experimental findings show how,\njointly tackling depth prediction and semantic segmentation, allows to improve\ndepth estimation accuracy. In particular, on the KITTI dataset our network\noutperforms state-of-the-art methods for monocular depth estimation. \n\n"}
{"id": "1810.04144", "contents": "Title: A Brief Survey on Autonomous Vehicle Possible Attacks, Exploits and\n  Vulnerabilities Abstract: Advanced driver assistance systems are advancing at a rapid pace and all\nmajor companies started investing in developing the autonomous vehicles. But\nthe security and reliability is still uncertain and debatable. Imagine that a\nvehicle is compromised by the attackers and then what they can do. An attacker\ncan control brake, accelerate and even steering which can lead to catastrophic\nconsequences. This paper gives a very short and brief overview of most of the\npossible attacks on autonomous vehicle software and hardware and their\npotential implications. \n\n"}
{"id": "1810.04246", "contents": "Title: Deep clustering: On the link between discriminative models and K-means Abstract: In the context of recent deep clustering studies, discriminative models\ndominate the literature and report the most competitive performances. These\nmodels learn a deep discriminative neural network classifier in which the\nlabels are latent. Typically, they use multinomial logistic regression\nposteriors and parameter regularization, as is very common in supervised\nlearning. It is generally acknowledged that discriminative objective functions\n(e.g., those based on the mutual information or the KL divergence) are more\nflexible than generative approaches (e.g., K-means) in the sense that they make\nfewer assumptions about the data distributions and, typically, yield much\nbetter unsupervised deep learning results. On the surface, several recent\ndiscriminative models may seem unrelated to K-means. This study shows that\nthese models are, in fact, equivalent to K-means under mild conditions and\ncommon posterior models and parameter regularization. We prove that, for the\ncommonly used logistic regression posteriors, maximizing the $L_2$ regularized\nmutual information via an approximate alternating direction method (ADM) is\nequivalent to a soft and regularized K-means loss. Our theoretical analysis not\nonly connects directly several recent state-of-the-art discriminative models to\nK-means, but also leads to a new soft and regularized deep K-means algorithm,\nwhich yields competitive performance on several image clustering benchmarks. \n\n"}
{"id": "1810.04891", "contents": "Title: Dense Object Reconstruction from RGBD Images with Embedded Deep Shape\n  Representations Abstract: Most problems involving simultaneous localization and mapping can nowadays be\nsolved using one of two fundamentally different approaches. The traditional\napproach is given by a least-squares objective, which minimizes many local\nphotometric or geometric residuals over explicitly parametrized structure and\ncamera parameters. Unmodeled effects violating the lambertian surface\nassumption or geometric invariances of individual residuals are encountered\nthrough statistical averaging or the addition of robust kernels and smoothness\nterms. Aiming at more accurate measurement models and the inclusion of\nhigher-order shape priors, the community more recently shifted its attention to\ndeep end-to-end models for solving geometric localization and mapping problems.\nHowever, at test-time, these feed-forward models ignore the more traditional\ngeometric or photometric consistency terms, thus leading to a low ability to\nrecover fine details and potentially complete failure in corner case scenarios.\nWith an application to dense object modeling from RGBD images, our work aims at\ntaking the best of both worlds by embedding modern higher-order object shape\npriors into classical iterative residual minimization objectives. We\ndemonstrate a general ability to improve mapping accuracy with respect to each\nmodality alone, and present a successful application to real data. \n\n"}
{"id": "1810.04991", "contents": "Title: SingleGAN: Image-to-Image Translation by a Single-Generator Network\n  using Multiple Generative Adversarial Learning Abstract: Image translation is a burgeoning field in computer vision where the goal is\nto learn the mapping between an input image and an output image. However, most\nrecent methods require multiple generators for modeling different domain\nmappings, which are inefficient and ineffective on some multi-domain image\ntranslation tasks. In this paper, we propose a novel method, SingleGAN, to\nperform multi-domain image-to-image translations with a single generator. We\nintroduce the domain code to explicitly control the different generative tasks\nand integrate multiple optimization goals to ensure the translation.\nExperimental results on several unpaired datasets show superior performance of\nour model in translation between two domains. Besides, we explore variants of\nSingleGAN for different tasks, including one-to-many domain translation,\nmany-to-many domain translation and one-to-one domain translation with\nmultimodality. The extended experiments show the universality and extensibility\nof our model. \n\n"}
{"id": "1810.05206", "contents": "Title: MeshAdv: Adversarial Meshes for Visual Recognition Abstract: Highly expressive models such as deep neural networks (DNNs) have been widely\napplied to various applications. However, recent studies show that DNNs are\nvulnerable to adversarial examples, which are carefully crafted inputs aiming\nto mislead the predictions. Currently, the majority of these studies have\nfocused on perturbation added to image pixels, while such manipulation is not\nphysically realistic. Some works have tried to overcome this limitation by\nattaching printable 2D patches or painting patterns onto surfaces, but can be\npotentially defended because 3D shape features are intact. In this paper, we\npropose meshAdv to generate \"adversarial 3D meshes\" from objects that have rich\nshape features but minimal textural variation. To manipulate the shape or\ntexture of the objects, we make use of a differentiable renderer to compute\naccurate shading on the shape and propagate the gradient. Extensive experiments\nshow that the generated 3D meshes are effective in attacking both classifiers\nand object detectors. We evaluate the attack under different viewpoints. In\naddition, we design a pipeline to perform black-box attack on a photorealistic\nrenderer with unknown rendering parameters. \n\n"}
{"id": "1810.05305", "contents": "Title: Block Stability for MAP Inference Abstract: To understand the empirical success of approximate MAP inference, recent work\n(Lang et al., 2018) has shown that some popular approximation algorithms\nperform very well when the input instance is stable. The simplest stability\ncondition assumes that the MAP solution does not change at all when some of the\npairwise potentials are (adversarially) perturbed. Unfortunately, this strong\ncondition does not seem to be satisfied in practice. In this paper, we\nintroduce a significantly more relaxed condition that only requires blocks\n(portions) of an input instance to be stable. Under this block stability\ncondition, we prove that the pairwise LP relaxation is persistent on the stable\nblocks. We complement our theoretical results with an empirical evaluation of\nreal-world MAP inference instances from computer vision. We design an algorithm\nto find stable blocks, and find that these real instances have large stable\nregions. Our work gives a theoretical explanation for the widespread empirical\nphenomenon of persistency for this LP relaxation. \n\n"}
{"id": "1810.05456", "contents": "Title: Modeling Varying Camera-IMU Time Offset in Optimization-Based\n  Visual-Inertial Odometry Abstract: Combining cameras and inertial measurement units (IMUs) has been proven\neffective in motion tracking, as these two sensing modalities offer\ncomplementary characteristics that are suitable for fusion. While most works\nfocus on global-shutter cameras and synchronized sensor measurements,\nconsumer-grade devices are mostly equipped with rolling-shutter cameras and\nsuffer from imperfect sensor synchronization. In this work, we propose a\nnonlinear optimization-based monocular visual inertial odometry (VIO) with\nvarying camera-IMU time offset modeled as an unknown variable. Our approach is\nable to handle the rolling-shutter effects and imperfect sensor synchronization\nin a unified way. Additionally, we introduce an efficient algorithm based on\ndynamic programming and red-black tree to speed up IMU integration over\nvariable-length time intervals during the optimization. An uncertainty-aware\ninitialization is also presented to launch the VIO robustly. Comparisons with\nstate-of-the-art methods on the Euroc dataset and mobile phone data are shown\nto validate the effectiveness of our approach. \n\n"}
{"id": "1810.05591", "contents": "Title: PointGrow: Autoregressively Learned Point Cloud Generation with\n  Self-Attention Abstract: Generating 3D point clouds is challenging yet highly desired. This work\npresents a novel autoregressive model, PointGrow, which can generate diverse\nand realistic point cloud samples from scratch or conditioned on semantic\ncontexts. This model operates recurrently, with each point sampled according to\na conditional distribution given its previously-generated points, allowing\ninter-point correlations to be well-exploited and 3D shape generative processes\nto be better interpreted. Since point cloud object shapes are typically encoded\nby long-range dependencies, we augment our model with dedicated self-attention\nmodules to capture such relations. Extensive evaluations show that PointGrow\nachieves satisfying performance on both unconditional and conditional point\ncloud generation tasks, with respect to realism and diversity. Several\nimportant applications, such as unsupervised feature learning and shape\narithmetic operations, are also demonstrated. \n\n"}
{"id": "1810.05782", "contents": "Title: Cloud Detection Algorithm for Remote Sensing Images Using Fully\n  Convolutional Neural Networks Abstract: This paper presents a deep-learning based framework for addressing the\nproblem of accurate cloud detection in remote sensing images. This framework\nbenefits from a Fully Convolutional Neural Network (FCN), which is capable of\npixel-level labeling of cloud regions in a Landsat 8 image. Also, a\ngradient-based identification approach is proposed to identify and exclude\nregions of snow/ice in the ground truths of the training set. We show that\nusing the hybrid of the two methods (threshold-based and deep-learning)\nimproves the performance of the cloud identification process without the need\nto manually correct automatically generated ground truths. In average the\nJaccard index and recall measure are improved by 4.36% and 3.62%, respectively. \n\n"}
{"id": "1810.06415", "contents": "Title: Virtualization of tissue staining in digital pathology using an\n  unsupervised deep learning approach Abstract: Histopathological evaluation of tissue samples is a key practice in patient\ndiagnosis and drug development, especially in oncology. Historically,\nHematoxylin and Eosin (H&E) has been used by pathologists as a gold standard\nstaining. However, in many cases, various target specific stains, including\nimmunohistochemistry (IHC), are needed in order to highlight specific\nstructures in the tissue. As tissue is scarce and staining procedures are\ntedious, it would be beneficial to generate images of stained tissue virtually.\nVirtual staining could also generate in-silico multiplexing of different stains\non the same tissue segment. In this paper, we present a sample application that\ngenerates FAP-CK virtual IHC images from Ki67-CD8 real IHC images using an\nunsupervised deep learning approach based on CycleGAN. We also propose a method\nto deal with tiling artifacts caused by normalization layers and we validate\nour approach by comparing the results of tissue analysis algorithms for virtual\nand real images. \n\n"}
{"id": "1810.06936", "contents": "Title: UnrealROX: An eXtremely Photorealistic Virtual Reality Environment for\n  Robotics Simulations and Synthetic Data Generation Abstract: Data-driven algorithms have surpassed traditional techniques in almost every\naspect in robotic vision problems. Such algorithms need vast amounts of quality\ndata to be able to work properly after their training process. Gathering and\nannotating that sheer amount of data in the real world is a time-consuming and\nerror-prone task. Those problems limit scale and quality. Synthetic data\ngeneration has become increasingly popular since it is faster to generate and\nautomatic to annotate. However, most of the current datasets and environments\nlack realism, interactions, and details from the real world. UnrealROX is an\nenvironment built over Unreal Engine 4 which aims to reduce that reality gap by\nleveraging hyperrealistic indoor scenes that are explored by robot agents which\nalso interact with objects in a visually realistic manner in that simulated\nworld. Photorealistic scenes and robots are rendered by Unreal Engine into a\nvirtual reality headset which captures gaze so that a human operator can move\nthe robot and use controllers for the robotic hands; scene information is\ndumped on a per-frame basis so that it can be reproduced offline to generate\nraw data and ground truth annotations. This virtual reality environment enables\nrobotic vision researchers to generate realistic and visually plausible data\nwith full ground truth for a wide variety of problems such as class and\ninstance semantic segmentation, object detection, depth estimation, visual\ngrasping, and navigation. \n\n"}
{"id": "1810.07151", "contents": "Title: Metropolis-Hastings view on variational inference and adversarial\n  training Abstract: A significant part of MCMC methods can be considered as the\nMetropolis-Hastings (MH) algorithm with different proposal distributions. From\nthis point of view, the problem of constructing a sampler can be reduced to the\nquestion - how to choose a proposal for the MH algorithm? To address this\nquestion, we propose to learn an independent sampler that maximizes the\nacceptance rate of the MH algorithm, which, as we demonstrate, is highly\nrelated to the conventional variational inference. For Bayesian inference, the\nproposed method compares favorably against alternatives to sample from the\nposterior distribution. Under the same approach, we step beyond the scope of\nclassical MCMC methods and deduce the Generative Adversarial Networks (GANs)\nframework from scratch, treating the generator as the proposal and the\ndiscriminator as the acceptance test. On real-world datasets, we improve\nFrechet Inception Distance and Inception Score, using different GANs as a\nproposal distribution for the MH algorithm. In particular, we demonstrate\nimprovements of recently proposed BigGAN model on ImageNet. \n\n"}
{"id": "1810.07911", "contents": "Title: Domain Adaptation for Semantic Segmentation via Class-Balanced\n  Self-Training Abstract: Recent deep networks achieved state of the art performance on a variety of\nsemantic segmentation tasks. Despite such progress, these models often face\nchallenges in real world `wild tasks' where large difference between labeled\ntraining/source data and unseen test/target data exists. In particular, such\ndifference is often referred to as `domain gap', and could cause significantly\ndecreased performance which cannot be easily remedied by further increasing the\nrepresentation power. Unsupervised domain adaptation (UDA) seeks to overcome\nsuch problem without target domain labels. In this paper, we propose a novel\nUDA framework based on an iterative self-training procedure, where the problem\nis formulated as latent variable loss minimization, and can be solved by\nalternatively generating pseudo labels on target data and re-training the model\nwith these labels. On top of self-training, we also propose a novel\nclass-balanced self-training framework to avoid the gradual dominance of large\nclasses on pseudo-label generation, and introduce spatial priors to refine\ngenerated labels. Comprehensive experiments show that the proposed methods\nachieve state of the art semantic segmentation performance under multiple major\nUDA settings. \n\n"}
{"id": "1810.08229", "contents": "Title: MRI Reconstruction via Cascaded Channel-wise Attention Network Abstract: We consider an MRI reconstruction problem with input of k-space data at a\nvery low undersampled rate. This can practically benefit patient due to reduced\ntime of MRI scan, but it is also challenging since quality of reconstruction\nmay be compromised. Currently, deep learning based methods dominate MRI\nreconstruction over traditional approaches such as Compressed Sensing, but they\nrarely show satisfactory performance in the case of low undersampled k-space\ndata. One explanation is that these methods treat channel-wise features\nequally, which results in degraded representation ability of the neural\nnetwork. To solve this problem, we propose a new model called MRI Cascaded\nChannel-wise Attention Network (MICCAN), highlighted by three components: (i) a\nvariant of U-net with Channel-wise Attention (UCA) module, (ii) a long skip\nconnection and (iii) a combined loss. Our model is able to attend to salient\ninformation by filtering irrelevant features and also concentrate on\nhigh-frequency information by enforcing low-frequency information bypassed to\nthe final output. We conduct both quantitative evaluation and qualitative\nanalysis of our method on a cardiac dataset. The experiment shows that our\nmethod achieves very promising results in terms of three common metrics on the\nMRI reconstruction with low undersampled k-space data. \n\n"}
{"id": "1810.09734", "contents": "Title: Domain Adaptive Segmentation in Volume Electron Microscopy Imaging Abstract: In the last years, automated segmentation has become a necessary tool for\nvolume electron microscopy (EM) imaging. So far, the best performing techniques\nhave been largely based on fully supervised encoder-decoder CNNs, requiring a\nsubstantial amount of annotated images. Domain Adaptation (DA) aims to\nalleviate the annotation burden by 'adapting' the networks trained on existing\ngroundtruth data (source domain) to work on a different (target) domain with as\nlittle additional annotation as possible. Most DA research is focused on the\nclassification task, whereas volume EM segmentation remains rather unexplored.\nIn this work, we extend recently proposed classification DA techniques to an\nencoder-decoder layout and propose a novel method that adds a reconstruction\ndecoder to the classical encoder-decoder segmentation in order to align source\nand target encoder features. The method has been validated on the task of\nsegmenting mitochondria in EM volumes. We have performed DA from brain EM\nimages to HeLa cells and from isotropic FIB/SEM volumes to anisotropic TEM\nvolumes. In all cases, the proposed method has outperformed the extended\nclassification DA techniques and the finetuning baseline. An implementation of\nour work can be found on\nhttps://github.com/JorisRoels/domain-adaptive-segmentation. \n\n"}
{"id": "1810.10351", "contents": "Title: Differentiable Fine-grained Quantization for Deep Neural Network\n  Compression Abstract: Neural networks have shown great performance in cognitive tasks. When\ndeploying network models on mobile devices with limited resources, weight\nquantization has been widely adopted. Binary quantization obtains the highest\ncompression but usually results in big accuracy drop. In practice, 8-bit or\n16-bit quantization is often used aiming at maintaining the same accuracy as\nthe original 32-bit precision. We observe different layers have different\naccuracy sensitivity of quantization. Thus judiciously selecting different\nprecision for different layers/structures can potentially produce more\nefficient models compared to traditional quantization methods by striking a\nbetter balance between accuracy and compression rate. In this work, we propose\na fine-grained quantization approach for deep neural network compression by\nrelaxing the search space of quantization bitwidth from discrete to a\ncontinuous domain. The proposed approach applies gradient descend based\noptimization to generate a mixed-precision quantization scheme that outperforms\nthe accuracy of traditional quantization methods under the same compression\nrate. \n\n"}
{"id": "1810.10786", "contents": "Title: Supervised Classification Methods for Flash X-ray single particle\n  diffraction Imaging Abstract: Current Flash X-ray single-particle diffraction Imaging (FXI) experiments,\nwhich operate on modern X-ray Free Electron Lasers (XFELs), can record millions\nof interpretable diffraction patterns from individual biomolecules per day. Due\nto the stochastic nature of the XFELs, those patterns will to a varying degree\ninclude scatterings from contaminated samples. Also, the heterogeneity of the\nsample biomolecules is unavoidable and complicates data processing. Reducing\nthe data volumes and selecting high-quality single-molecule patterns are\ntherefore critical steps in the experimental set-up.\n  In this paper, we present two supervised template-based learning methods for\nclassifying FXI patterns. Our Eigen-Image and Log-Likelihood classifier can\nfind the best-matched template for a single-molecule pattern within a few\nmilliseconds. It is also straightforward to parallelize them so as to fully\nmatch the XFEL repetition rate, thereby enabling processing at site. \n\n"}
{"id": "1810.10863", "contents": "Title: GAN Augmentation: Augmenting Training Data using Generative Adversarial\n  Networks Abstract: One of the biggest issues facing the use of machine learning in medical\nimaging is the lack of availability of large, labelled datasets. The annotation\nof medical images is not only expensive and time consuming but also highly\ndependent on the availability of expert observers. The limited amount of\ntraining data can inhibit the performance of supervised machine learning\nalgorithms which often need very large quantities of data on which to train to\navoid overfitting. So far, much effort has been directed at extracting as much\ninformation as possible from what data is available. Generative Adversarial\nNetworks (GANs) offer a novel way to unlock additional information from a\ndataset by generating synthetic samples with the appearance of real images.\nThis paper demonstrates the feasibility of introducing GAN derived synthetic\ndata to the training datasets in two brain segmentation tasks, leading to\nimprovements in Dice Similarity Coefficient (DSC) of between 1 and 5 percentage\npoints under different conditions, with the strongest effects seen fewer than\nten training image stacks are available. \n\n"}
{"id": "1810.11137", "contents": "Title: Towards improved lossy image compression: Human image reconstruction\n  with public-domain images Abstract: Lossy image compression has been studied extensively in the context of\ntypical loss functions such as RMSE, MS-SSIM, etc. However, compression at low\nbitrates generally produces unsatisfying results. Furthermore, the availability\nof massive public image datasets appears to have hardly been exploited in image\ncompression. Here, we present a paradigm for eliciting human image\nreconstruction in order to perform lossy image compression. In this paradigm,\none human describes images to a second human, whose task is to reconstruct the\ntarget image using publicly available images and text instructions. The\nresulting reconstructions are then evaluated by human raters on the Amazon\nMechanical Turk platform and compared to reconstructions obtained using\nstate-of-the-art compressor WebP. Our results suggest that prioritizing\nsemantic visual elements may be key to achieving significant improvements in\nimage compression, and that our paradigm can be used to develop a more\nhuman-centric loss function.\n  The images, results and additional data are available at\nhttps://compression.stanford.edu/human-compression \n\n"}
{"id": "1810.11335", "contents": "Title: Outlier Detection using Generative Models with Theoretical Performance\n  Guarantees Abstract: This paper considers the problem of recovering signals from compressed\nmeasurements contaminated with sparse outliers, which has arisen in many\napplications. In this paper, we propose a generative model neural network\napproach for reconstructing the ground truth signals under sparse outliers. We\npropose an iterative alternating direction method of multipliers (ADMM)\nalgorithm for solving the outlier detection problem via $\\ell_1$ norm\nminimization, and a gradient descent algorithm for solving the outlier\ndetection problem via squared $\\ell_1$ norm minimization. We establish the\nrecovery guarantees for reconstruction of signals using generative models in\nthe presence of outliers, and give an upper bound on the number of outliers\nallowed for recovery. Our results are applicable to both the linear generator\nneural network and the nonlinear generator neural network with an arbitrary\nnumber of layers. We conduct extensive experiments using variational\nauto-encoder and deep convolutional generative adversarial networks, and the\nexperimental results show that the signals can be successfully reconstructed\nunder outliers using our approach. Our approach outperforms the traditional\nLasso and $\\ell_2$ minimization approach. \n\n"}
{"id": "1810.11641", "contents": "Title: Cross-Modal Distillation for RGB-Depth Person Re-Identification Abstract: Person re-identification is a key challenge for surveillance across multiple\nsensors. Prompted by the advent of powerful deep learning models for visual\nrecognition, and inexpensive RGB-D cameras and sensor-rich mobile robotic\nplatforms, e.g. self-driving vehicles, we investigate the relatively unexplored\nproblem of cross-modal re-identification of persons between RGB (color) and\ndepth images. The considerable divergence in data distributions across\ndifferent sensor modalities introduces additional challenges to the typical\ndifficulties like distinct viewpoints, occlusions, and pose and illumination\nvariation. While some work has investigated re-identification across RGB and\ninfrared, we take inspiration from successes in transfer learning from RGB to\ndepth in object detection tasks. Our main contribution is a novel method for\ncross-modal distillation for robust person re-identification, which learns a\nshared feature representation space of person's appearance in both RGB and\ndepth images. In addition, we propose a cross-modal attention mechanism where\nthe gating signal from one modality can dynamically activate the most\ndiscriminant CNN filters of the other modality. The proposed distillation\nmethod is compared to conventional and deep learning approaches proposed for\nother cross-domain re-identification tasks. Results obtained on the public BIWI\nand RobotPKU datasets indicate that the proposed method can significantly\noutperform the state-of-the-art approaches by up to 16.1% in mean Average\nPrecision (mAP), demonstrating the benefit of the distillation paradigm. The\nexperimental results also indicate that using cross-modal attention allows to\nimprove recognition accuracy considerably with respect to the proposed\ndistillation method and relevant state-of-the-art approaches. \n\n"}
{"id": "1810.11649", "contents": "Title: Fabrik: An Online Collaborative Neural Network Editor Abstract: We present Fabrik, an online neural network editor that provides tools to\nvisualize, edit, and share neural networks from within a browser. Fabrik\nprovides a simple and intuitive GUI to import neural networks written in\npopular deep learning frameworks such as Caffe, Keras, and TensorFlow, and\nallows users to interact with, build, and edit models via simple drag and drop.\nFabrik is designed to be framework agnostic and support high interoperability,\nand can be used to export models back to any supported framework. Finally, it\nprovides powerful collaborative features to enable users to iterate over model\ndesign remotely and at scale. \n\n"}
{"id": "1810.12091", "contents": "Title: Embedding Geographic Locations for Modelling the Natural Environment\n  using Flickr Tags and Structured Data Abstract: Meta-data from photo-sharing websites such as Flickr can be used to obtain\nrich bag-of-words descriptions of geographic locations, which have proven\nvaluable, among others, for modelling and predicting ecological features. One\nimportant insight from previous work is that the descriptions obtained from\nFlickr tend to be complementary to the structured information that is available\nfrom traditional scientific resources. To better integrate these two diverse\nsources of information, in this paper we consider a method for learning vector\nspace embeddings of geographic locations. We show experimentally that this\nmethod improves on existing approaches, especially in cases where structured\ninformation is available. \n\n"}
{"id": "1810.12348", "contents": "Title: Gather-Excite: Exploiting Feature Context in Convolutional Neural\n  Networks Abstract: While the use of bottom-up local operators in convolutional neural networks\n(CNNs) matches well some of the statistics of natural images, it may also\nprevent such models from capturing contextual long-range feature interactions.\nIn this work, we propose a simple, lightweight approach for better context\nexploitation in CNNs. We do so by introducing a pair of operators: gather,\nwhich efficiently aggregates feature responses from a large spatial extent, and\nexcite, which redistributes the pooled information to local features. The\noperators are cheap, both in terms of number of added parameters and\ncomputational complexity, and can be integrated directly in existing\narchitectures to improve their performance. Experiments on several datasets\nshow that gather-excite can bring benefits comparable to increasing the depth\nof a CNN at a fraction of the cost. For example, we find ResNet-50 with\ngather-excite operators is able to outperform its 101-layer counterpart on\nImageNet with no additional learnable parameters. We also propose a parametric\ngather-excite operator pair which yields further performance gains, relate it\nto the recently-introduced Squeeze-and-Excitation Networks, and analyse the\neffects of these changes to the CNN feature activation statistics. \n\n"}
{"id": "1810.12780", "contents": "Title: Advancing PICO Element Detection in Biomedical Text via Deep Neural\n  Networks Abstract: In evidence-based medicine (EBM), defining a clinical question in terms of\nthe specific patient problem aids the physicians to efficiently identify\nappropriate resources and search for the best available evidence for medical\ntreatment. In order to formulate a well-defined, focused clinical question, a\nframework called PICO is widely used, which identifies the sentences in a given\nmedical text that belong to the four components typically reported in clinical\ntrials: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome\n(O). In this work, we propose a novel deep learning model for recognizing PICO\nelements in biomedical abstracts. Based on the previous state-of-the-art\nbidirectional long-short term memory (biLSTM) plus conditional random field\n(CRF) architecture, we add another layer of biLSTM upon the sentence\nrepresentation vectors so that the contextual information from surrounding\nsentences can be gathered to help infer the interpretation of the current one.\nIn addition, we propose two methods to further generalize and improve the\nmodel: adversarial training and unsupervised pre-training over large corpora.\nWe tested our proposed approach over two benchmark datasets. One is the\nPubMed-PICO dataset, where our best results outperform the previous best by\n5.5%, 7.9%, and 5.8% for P, I, and O elements in terms of F1 score,\nrespectively. And for the other dataset named NICTA-PIBOSO, the improvements\nfor P/I/O elements are 2.4%, 13.6%, and 1.0% in F1 score, respectively.\nOverall, our proposed deep learning model can obtain unprecedented PICO element\ndetection accuracy while avoiding the need for any manual feature selection. \n\n"}
{"id": "1810.12813", "contents": "Title: Contextual Hourglass Network for Semantic Segmentation of High\n  Resolution Aerial Imagery Abstract: Semantic segmentation for aerial imagery is a challenging and important\nproblem in remotely sensed imagery analysis. In recent years, with the success\nof deep learning, various convolutional neural network (CNN) based models have\nbeen developed. However, due to the varying sizes of the objects and imbalanced\nclass labels, it can be challenging to obtain accurate pixel-wise semantic\nsegmentation results. To address those challenges, we develop a novel semantic\nsegmentation method and call it Contextual Hourglass Network. In our method, in\norder to improve the robustness of the prediction, we design a new contextual\nhourglass module which incorporates attention mechanism on processed\nlow-resolution featuremaps to exploit the contextual semantics. We further\nexploit the stacked encoder-decoder structure by connecting multiple contextual\nhourglass modules from end to end. This architecture can effectively extract\nrich multi-scale features and add more feedback loops for better learning\ncontextual semantics through intermediate supervision. To demonstrate the\nefficacy of our semantic segmentation method, we test it on Potsdam and\nVaihingen datasets. Through the comparisons to other baseline methods, our\nmethod yields the best results on overall performance. \n\n"}
{"id": "1811.00174", "contents": "Title: Pixel Level Data Augmentation for Semantic Image Segmentation using\n  Generative Adversarial Networks Abstract: Semantic segmentation is one of the basic topics in computer vision, it aims\nto assign semantic labels to every pixel of an image. Unbalanced semantic label\ndistribution could have a negative influence on segmentation accuracy. In this\npaper, we investigate using data augmentation approach to balance the semantic\nlabel distribution in order to improve segmentation performance. We propose\nusing generative adversarial networks (GANs) to generate realistic images for\nimproving the performance of semantic segmentation networks. Experimental\nresults show that the proposed method can not only improve segmentation\nperformance on those classes with low accuracy, but also obtain 1.3% to 2.1%\nincrease in average segmentation accuracy. It shows that this augmentation\nmethod can boost accuracy and be easily applicable to any other segmentation\nmodels. \n\n"}
{"id": "1811.00312", "contents": "Title: A Local Block Coordinate Descent Algorithm for the Convolutional Sparse\n  Coding Model Abstract: The Convolutional Sparse Coding (CSC) model has recently gained considerable\ntraction in the signal and image processing communities. By providing a global,\nyet tractable, model that operates on the whole image, the CSC was shown to\novercome several limitations of the patch-based sparse model while achieving\nsuperior performance in various applications. Contemporary methods for pursuit\nand learning the CSC dictionary often rely on the Alternating Direction Method\nof Multipliers (ADMM) in the Fourier domain for the computational convenience\nof convolutions, while ignoring the local characterizations of the image. A\nrecent work by Papyan et al. suggested the SBDL algorithm for the CSC, while\noperating locally on image patches. SBDL demonstrates better performance\ncompared to the Fourier-based methods, albeit still relying on the ADMM. In\nthis work we maintain the localized strategy of the SBDL, while proposing a new\nand much simpler approach based on the Block Coordinate Descent algorithm -\nthis method is termed Local Block Coordinate Descent (LoBCoD). Furthermore, we\nintroduce a novel stochastic gradient descent version of LoBCoD for training\nthe convolutional filters. The Stochastic-LoBCoD leverages the benefits of\nonline learning, while being applicable to a single training image. We\ndemonstrate the advantages of the proposed algorithms for image inpainting and\nmulti-focus image fusion, achieving state-of-the-art results. \n\n"}
{"id": "1811.00631", "contents": "Title: MDFS - MultiDimensional Feature Selection Abstract: Identification of informative variables in an information system is often\nperformed using simple one-dimensional filtering procedures that discard\ninformation about interactions between variables. Such approach may result in\nremoving some relevant variables from consideration. Here we present an R\npackage MDFS (MultiDimensional Feature Selection) that performs identification\nof informative variables taking into account synergistic interactions between\nmultiple descriptors and the decision variable. MDFS is an implementation of an\nalgorithm based on information theory. Computational kernel of the package is\nimplemented in C++. A high-performance version implemented in CUDA C is also\navailable. The applications of MDFS are demonstrated using the well-known\nMadelon dataset that has synergistic variables by design. The dataset comes\nfrom the UCI Machine Learning Repository. It is shown that multidimensional\nanalysis is more sensitive than one-dimensional tests and returns more reliable\nrankings of importance. \n\n"}
{"id": "1811.01194", "contents": "Title: Pushing the boundaries of audiovisual word recognition using Residual\n  Networks and LSTMs Abstract: Visual and audiovisual speech recognition are witnessing a renaissance which\nis largely due to the advent of deep learning methods. In this paper, we\npresent a deep learning architecture for lipreading and audiovisual word\nrecognition, which combines Residual Networks equipped with spatiotemporal\ninput layers and Bidirectional LSTMs. The lipreading architecture attains\n11.92% misclassification rate on the challenging Lipreading-In-The-Wild\ndatabase, which is composed of excerpts from BBC-TV, each containing one of the\n500 target words. Audiovisual experiments are performed using both intermediate\nand late integration, as well as several types and levels of environmental\nnoise, and notable improvements over the audio-only network are reported, even\nin the case of clean speech. A further analysis on the utility of target word\nboundaries is provided, as well as on the capacity of the network in modeling\nthe linguistic context of the target word. Finally, we examine difficult word\npairs and discuss how visual information helps towards attaining higher\nrecognition accuracy. \n\n"}
{"id": "1811.01791", "contents": "Title: Confidence Propagation through CNNs for Guided Sparse Depth Regression Abstract: Generally, convolutional neural networks (CNNs) process data on a regular\ngrid, e.g. data generated by ordinary cameras. Designing CNNs for sparse and\nirregularly spaced input data is still an open research problem with numerous\napplications in autonomous driving, robotics, and surveillance. In this paper,\nwe propose an algebraically-constrained normalized convolution layer for CNNs\nwith highly sparse input that has a smaller number of network parameters\ncompared to related work. We propose novel strategies for determining the\nconfidence from the convolution operation and propagating it to consecutive\nlayers. We also propose an objective function that simultaneously minimizes the\ndata error while maximizing the output confidence. To integrate structural\ninformation, we also investigate fusion strategies to combine depth and RGB\ninformation in our normalized convolution network framework. In addition, we\nintroduce the use of output confidence as an auxiliary information to improve\nthe results. The capabilities of our normalized convolution network framework\nare demonstrated for the problem of scene depth completion. Comprehensive\nexperiments are performed on the KITTI-Depth and the NYU-Depth-v2 datasets. The\nresults clearly demonstrate that the proposed approach achieves superior\nperformance while requiring only about 1-5% of the number of parameters\ncompared to the state-of-the-art methods. \n\n"}
{"id": "1811.02146", "contents": "Title: TrafficPredict: Trajectory Prediction for Heterogeneous Traffic-Agents Abstract: To safely and efficiently navigate in complex urban traffic, autonomous\nvehicles must make responsible predictions in relation to surrounding\ntraffic-agents (vehicles, bicycles, pedestrians, etc.). A challenging and\ncritical task is to explore the movement patterns of different traffic-agents\nand predict their future trajectories accurately to help the autonomous vehicle\nmake reasonable navigation decision. To solve this problem, we propose a long\nshort-term memory-based (LSTM-based) realtime traffic prediction algorithm,\nTrafficPredict. Our approach uses an instance layer to learn instances'\nmovements and interactions and has a category layer to learn the similarities\nof instances belonging to the same type to refine the prediction. In order to\nevaluate its performance, we collected trajectory datasets in a large city\nconsisting of varying conditions and traffic densities. The dataset includes\nmany challenging scenarios where vehicles, bicycles, and pedestrians move among\none another. We evaluate the performance of TrafficPredict on our new dataset\nand highlight its higher accuracy for trajectory prediction by comparing with\nprior prediction methods. \n\n"}
{"id": "1811.02385", "contents": "Title: Fine-grained Apparel Classification and Retrieval without rich\n  annotations Abstract: The ability to correctly classify and retrieve apparel images has a variety\nof applications important to e-commerce, online advertising and internet\nsearch. In this work, we propose a robust framework for fine-grained apparel\nclassification, in-shop and cross-domain retrieval which eliminates the\nrequirement of rich annotations like bounding boxes and human-joints or\nclothing landmarks, and training of bounding box/ key-landmark detector for the\nsame. Factors such as subtle appearance differences, variations in human poses,\ndifferent shooting angles, apparel deformations, and self-occlusion add to the\nchallenges in classification and retrieval of apparel items. Cross-domain\nretrieval is even harder due to the presence of large variation between online\nshopping images, usually taken in ideal lighting, pose, positive angle and\nclean background as compared with street photos captured by users in\ncomplicated conditions with poor lighting and cluttered scenes. Our framework\nuses compact bilinear CNN with tensor sketch algorithm to generate embeddings\nthat capture local pairwise feature interactions in a translationally invariant\nmanner. For apparel classification, we pass the feature embeddings through a\nsoftmax classifier, while, the in-shop and cross-domain retrieval pipelines use\na triplet-loss based optimization approach, such that squared Euclidean\ndistance between embeddings measures the dissimilarity between the images.\nUnlike previous works that relied on bounding box, key clothing landmarks or\nhuman joint detectors to assist the final deep classifier, proposed framework\ncan be trained directly on the provided category labels or generated triplets\nfor triplet loss optimization. Lastly, Experimental results on the DeepFashion\nfine-grained categorization, and in-shop and consumer-to-shop retrieval\ndatasets provide a comparative analysis with previous work performed in the\ndomain. \n\n"}
{"id": "1811.02454", "contents": "Title: Synaptic Strength For Convolutional Neural Network Abstract: Convolutional Neural Networks(CNNs) are both computation and memory intensive\nwhich hindered their deployment in mobile devices. Inspired by the relevant\nconcept in neural science literature, we propose Synaptic Pruning: a\ndata-driven method to prune connections between input and output feature maps\nwith a newly proposed class of parameters called Synaptic Strength. Synaptic\nStrength is designed to capture the importance of a connection based on the\namount of information it transports. Experiment results show the effectiveness\nof our approach. On CIFAR-10, we prune connections for various CNN models with\nup to 96% , which results in significant size reduction and computation saving.\nFurther evaluation on ImageNet demonstrates that synaptic pruning is able to\ndiscover efficient models which is competitive to state-of-the-art compact CNNs\nsuch as MobileNet-V2 and NasNet-Mobile. Our contribution is summarized as\nfollowing: (1) We introduce Synaptic Strength, a new class of parameters for\nCNNs to indicate the importance of each connections. (2) Our approach can prune\nvarious CNNs with high compression without compromising accuracy. (3) Further\ninvestigation shows, the proposed Synaptic Strength is a better indicator for\nkernel pruning compared with the previous approach in both empirical result and\ntheoretical analysis. \n\n"}
{"id": "1811.03173", "contents": "Title: Automatic Thresholding of SIFT Descriptors Abstract: We introduce a method to perform automatic thresholding of SIFT descriptors\nthat improves matching performance by at least 15.9% on the Oxford image\nmatching benchmark. The method uses a contrario methodology to determine a\nunique bin magnitude threshold. This is done by building a generative uniform\nbackground model for descriptors and determining when bin magnitudes have\nreached a sufficient level. The presented method, called meaningful clamping,\ncontrasts from the current SIFT implementation by efficiently computing a\nclamping threshold that is unique for every descriptor. \n\n"}
{"id": "1811.03188", "contents": "Title: Solving Jigsaw Puzzles By the Graph Connection Laplacian Abstract: We propose a novel mathematical framework to address the problem of\nautomatically solving large jigsaw puzzles. This problem assumes a large image,\nwhich is cut into equal square pieces that are arbitrarily rotated and\nshuffled, and asks to recover the original image given the transformed pieces.\nThe main contribution of this work is a method for recovering the rotations of\nthe pieces when both shuffles and rotations are unknown. A major challenge of\nthis procedure is estimating the graph connection Laplacian without the\nknowledge of shuffles. A careful combination of our proposed method for\nestimating rotations with any existing method for estimating shuffles results\nin a practical solution for the jigsaw puzzle problem. Our theory guarantees,\nin a clean setting, that our basic idea of recovering rotations is robust to\nsome corruption of the connection graph. Numerical experiments demonstrate the\ncompetitive accuracy of this solution, its robustness to corruption and, its\ncomputational advantage for large puzzles. \n\n"}
{"id": "1811.03217", "contents": "Title: RGB-D SLAM in Dynamic Environments Using Point Correlations Abstract: In this paper, a simultaneous localization and mapping (SLAM) method that\neliminates the influence of moving objects in dynamic environments is proposed.\nThis method utilizes the correlation between map points to separate points that\nare part of the static scene and points that are part of different moving\nobjects into different groups. A sparse graph is first created using Delaunay\ntriangulation from all map points. In this graph, the vertices represent map\npoints, and each edge represents the correlation between adjacent points. If\nthe relative position between two points remains consistent over time, there is\ncorrelation between them, and they are considered to be moving together\nrigidly. If not, they are considered to have no correlation and to be in\nseparate groups. After the edges between the uncorrelated points are removed\nduring point-correlation optimization, the remaining graph separates the map\npoints of the moving objects from the map points of the static scene. The\nlargest group is assumed to be the group of reliable static map points.\nFinally, motion estimation is performed using only these points. The proposed\nmethod was implemented for RGB-D sensors, evaluated with a public RGB-D\nbenchmark, and tested in several additional challenging environments. The\nexperimental results demonstrate that robust and accurate performance can be\nachieved by the proposed SLAM method in both slightly and highly dynamic\nenvironments. Compared with other state-of-the-art methods, the proposed method\ncan provide competitive accuracy with good real-time performance. \n\n"}
{"id": "1811.03331", "contents": "Title: Improving Multi-Person Pose Estimation using Label Correction Abstract: Significant attention is being paid to multi-person pose estimation methods\nrecently, as there has been rapid progress in the field owing to convolutional\nneural networks. Especially, recent method which exploits part confidence maps\nand Part Affinity Fields (PAFs) has achieved accurate real-time prediction of\nmulti-person keypoints. However, human annotated labels are sometimes\ninappropriate for learning models. For example, if there is a limb that extends\noutside an image, a keypoint for the limb may not have annotations because it\nexists outside of the image, and thus the labels for the limb can not be\ngenerated. If a model is trained with data including such missing labels, the\noutput of the model for the location, even though it is correct, is penalized\nas a false positive, which is likely to cause negative effects on the\nperformance of the model. In this paper, we point out the existence of some\npatterns of inappropriate labels, and propose a novel method for correcting\nsuch labels with a teacher model trained on such incomplete data. Experiments\non the COCO dataset show that training with the corrected labels improves the\nperformance of the model and also speeds up training. \n\n"}
{"id": "1811.03384", "contents": "Title: Prediction of laparoscopic procedure duration using unlabeled,\n  multimodal sensor data Abstract: Purpose The course of surgical procedures is often unpredictable, making it\ndifficult to estimate the duration of procedures beforehand. A context-aware\nmethod that analyses the workflow of an intervention online and automatically\npredicts the remaining duration would alleviate these problems. As basis for\nsuch an estimate, information regarding the current state of the intervention\nis required. Methods Today, the operating room contains a diverse range of\nsensors. During laparoscopic interventions, the endoscopic video stream is an\nideal source of such information. Extracting quantitative information from the\nvideo is challenging though, due to its high dimensionality. Other surgical\ndevices (e.g. insufflator, lights, etc.) provide data streams which are, in\ncontrast to the video stream, more compact and easier to quantify. Though\nwhether such streams offer sufficient information for estimating the duration\nof surgery is uncertain. Here, we propose and compare methods, based on\nconvolutional neural networks, for continuously predicting the duration of\nlaparoscopic interventions based on unlabeled data, such as from endoscopic\nimages and surgical device streams. Results The methods are evaluated on 80\nlaparoscopic interventions of various types, for which surgical device data and\nthe endoscopic video are available. Here the combined method performs best with\nan overall average error of 37% and an average halftime error of 28%.\nConclusion In this paper, we present, to our knowledge, the first approach for\nonline procedure duration prediction using unlabeled endoscopic video data and\nsurgical device data in a laparoscopic setting. We also show that a method\nincorporating both vision and device data performs better than methods based\nonly on vision, while methods only based on tool usage and surgical device data\nperform poorly, showing the importance of the visual channel. \n\n"}
{"id": "1811.03875", "contents": "Title: Multimodal One-Shot Learning of Speech and Images Abstract: Imagine a robot is shown new concepts visually together with spoken tags,\ne.g. \"milk\", \"eggs\", \"butter\". After seeing one paired audio-visual example per\nclass, it is shown a new set of unseen instances of these objects, and asked to\npick the \"milk\". Without receiving any hard labels, could it learn to match the\nnew continuous speech input to the correct visual instance? Although unimodal\none-shot learning has been studied, where one labelled example in a single\nmodality is given per class, this example motivates multimodal one-shot\nlearning. Our main contribution is to formally define this task, and to propose\nseveral baseline and advanced models. We use a dataset of paired spoken and\nvisual digits to specifically investigate recent advances in Siamese\nconvolutional neural networks. Our best Siamese model achieves twice the\naccuracy of a nearest neighbour model using pixel-distance over images and\ndynamic time warping over speech in 11-way cross-modal matching. \n\n"}
{"id": "1811.04678", "contents": "Title: Towards Adversarial Denoising of Radar Micro-Doppler Signatures Abstract: Generative Adversarial Networks (GANs) are considered the state-of-the-art in\nthe field of image generation. They learn the joint distribution of the\ntraining data and attempt to generate new data samples in high dimensional\nspace following the same distribution as the input. Recent improvements in GANs\nopened the field to many other computer vision applications based on improving\nand changing the characteristics of the input image to follow some given\ntraining requirements. In this paper, we propose a novel technique for the\ndenoising and reconstruction of the micro-Doppler ($\\boldsymbol{\\mu}$-D)\nspectra of walking humans based on GANs. Two sets of experiments were collected\non 22 subjects walking on a treadmill at an intermediate velocity using a\n\\unit[25]{GHz} CW radar. In one set, a clean $\\boldsymbol{\\mu}$-D spectrum is\ncollected for each subject by placing the radar at a close distance to the\nsubject. In the other set, variations are introduced in the experiment setup to\nintroduce different noise and clutter effects on the spectrum by changing the\ndistance and placing reflective objects between the radar and the target.\nSynthetic paired noisy and noise-free spectra were used for training, while\nvalidation was carried out on the real noisy measured data. Finally,\nqualitative and quantitative comparison with other classical radar denoising\napproaches in the literature demonstrated the proposed GANs framework is better\nand more robust to different noise levels. \n\n"}
{"id": "1811.04768", "contents": "Title: Learning data augmentation policies using augmented random search Abstract: Previous attempts for data augmentation are designed manually, and the\naugmentation policies are dataset-specific. Recently, an automatic data\naugmentation approach, named AutoAugment, is proposed using reinforcement\nlearning. AutoAugment searches for the augmentation polices in the discrete\nsearch space, which may lead to a sub-optimal solution. In this paper, we\nemploy the Augmented Random Search method (ARS) to improve the performance of\nAutoAugment. Our key contribution is to change the discrete search space to\ncontinuous space, which will improve the searching performance and maintain the\ndiversities between sub-policies. With the proposed method, state-of-the-art\naccuracies are achieved on CIFAR-10, CIFAR-100, and ImageNet (without\nadditional data). Our code is available at https://github.com/gmy2013/ARS-Aug. \n\n"}
{"id": "1811.04983", "contents": "Title: Unseen Word Representation by Aligning Heterogeneous Lexical Semantic\n  Spaces Abstract: Word embedding techniques heavily rely on the abundance of training data for\nindividual words. Given the Zipfian distribution of words in natural language\ntexts, a large number of words do not usually appear frequently or at all in\nthe training data. In this paper we put forward a technique that exploits the\nknowledge encoded in lexical resources, such as WordNet, to induce embeddings\nfor unseen words. Our approach adapts graph embedding and cross-lingual vector\nspace transformation techniques in order to merge lexical knowledge encoded in\nontologies with that derived from corpus statistics. We show that the approach\ncan provide consistent performance improvements across multiple evaluation\nbenchmarks: in-vitro, on multiple rare word similarity datasets, and in-vivo,\nin two downstream text classification tasks. \n\n"}
{"id": "1811.05013", "contents": "Title: Blindfold Baselines for Embodied QA Abstract: We explore blindfold (question-only) baselines for Embodied Question\nAnswering. The EmbodiedQA task requires an agent to answer a question by\nintelligently navigating in a simulated environment, gathering necessary visual\ninformation only through first-person vision before finally answering.\nConsequently, a blindfold baseline which ignores the environment and visual\ninformation is a degenerate solution, yet we show through our experiments on\nthe EQAv1 dataset that a simple question-only baseline achieves\nstate-of-the-art results on the EmbodiedQA task in all cases except when the\nagent is spawned extremely close to the object. \n\n"}
{"id": "1811.05432", "contents": "Title: Deep Object-Centric Policies for Autonomous Driving Abstract: While learning visuomotor skills in an end-to-end manner is appealing, deep\nneural networks are often uninterpretable and fail in surprising ways. For\nrobotics tasks, such as autonomous driving, models that explicitly represent\nobjects may be more robust to new scenes and provide intuitive visualizations.\nWe describe a taxonomy of \"object-centric\" models which leverage both object\ninstances and end-to-end learning. In the Grand Theft Auto V simulator, we show\nthat object-centric models outperform object-agnostic methods in scenes with\nother vehicles and pedestrians, even with an imperfect detector. We also\ndemonstrate that our architectures perform well on real-world environments by\nevaluating on the Berkeley DeepDrive Video dataset, where an object-centric\nmodel outperforms object-agnostic models in the low-data regimes. \n\n"}
{"id": "1811.06284", "contents": "Title: Guiding the One-to-one Mapping in CycleGAN via Optimal Transport Abstract: CycleGAN is capable of learning a one-to-one mapping between two data\ndistributions without paired examples, achieving the task of unsupervised data\ntranslation. However, there is no theoretical guarantee on the property of the\nlearned one-to-one mapping in CycleGAN. In this paper, we experimentally find\nthat, under some circumstances, the one-to-one mapping learned by CycleGAN is\njust a random one within the large feasible solution space. Based on this\nobservation, we explore to add extra constraints such that the one-to-one\nmapping is controllable and satisfies more properties related to specific\ntasks. We propose to solve an optimal transport mapping restrained by a\ntask-specific cost function that reflects the desired properties, and use the\nbarycenters of optimal transport mapping to serve as references for CycleGAN.\nOur experiments indicate that the proposed algorithm is capable of learning a\none-to-one mapping with the desired properties. \n\n"}
{"id": "1811.06458", "contents": "Title: Psychophysical evaluation of individual low-level feature influences on\n  visual attention Abstract: In this study we provide the analysis of eye movement behavior elicited by\nlow-level feature distinctiveness with a dataset of synthetically-generated\nimage patterns. Design of visual stimuli was inspired by the ones used in\nprevious psychophysical experiments, namely in free-viewing and visual\nsearching tasks, to provide a total of 15 types of stimuli, divided according\nto the task and feature to be analyzed. Our interest is to analyze the\ninfluences of low-level feature contrast between a salient region and the rest\nof distractors, providing fixation localization characteristics and reaction\ntime of landing inside the salient region. Eye-tracking data was collected from\n34 participants during the viewing of a 230 images dataset. Results show that\nsaliency is predominantly and distinctively influenced by: 1. feature type, 2.\nfeature contrast, 3. temporality of fixations, 4. task difficulty and 5. center\nbias. This experimentation proposes a new psychophysical basis for saliency\nmodel evaluation using synthetic images. \n\n"}
{"id": "1811.06817", "contents": "Title: Evaluating Uncertainty Quantification in End-to-End Autonomous Driving\n  Control Abstract: A rise in popularity of Deep Neural Networks (DNNs), attributed to more\npowerful GPUs and widely available datasets, has seen them being increasingly\nused within safety-critical domains. One such domain, self-driving, has\nbenefited from significant performance improvements, with millions of miles\nhaving been driven with no human intervention. Despite this, crashes and\nerroneous behaviours still occur, in part due to the complexity of verifying\nthe correctness of DNNs and a lack of safety guarantees.\n  In this paper, we demonstrate how quantitative measures of uncertainty can be\nextracted in real-time, and their quality evaluated in end-to-end controllers\nfor self-driving cars. To this end we utilise a recent method for gathering\napproximate uncertainty information from DNNs without changing the network's\narchitecture. We propose evaluation techniques for the uncertainty on two\nseparate architectures which use the uncertainty to predict crashes up to five\nseconds in advance. We find that mutual information, a measure of uncertainty\nin classification networks, is a promising indicator of forthcoming crashes. \n\n"}
{"id": "1811.06937", "contents": "Title: Mode Variational LSTM Robust to Unseen Modes of Variation: Application\n  to Facial Expression Recognition Abstract: Spatio-temporal feature encoding is essential for encoding the dynamics in\nvideo sequences. Recurrent neural networks, particularly long short-term memory\n(LSTM) units, have been popular as an efficient tool for encoding\nspatio-temporal features in sequences. In this work, we investigate the effect\nof mode variations on the encoded spatio-temporal features using LSTMs. We show\nthat the LSTM retains information related to the mode variation in the\nsequence, which is irrelevant to the task at hand (e.g. classification facial\nexpressions). Actually, the LSTM forget mechanism is not robust enough to mode\nvariations and preserves information that could negatively affect the encoded\nspatio-temporal features. We propose the mode variational LSTM to encode\nspatio-temporal features robust to unseen modes of variation. The mode\nvariational LSTM modifies the original LSTM structure by adding an additional\ncell state that focuses on encoding the mode variation in the input sequence.\nTo efficiently regulate what features should be stored in the additional cell\nstate, additional gating functionality is also introduced. The effectiveness of\nthe proposed mode variational LSTM is verified using the facial expression\nrecognition task. Comparative experiments on publicly available datasets\nverified that the proposed mode variational LSTM outperforms existing methods.\nMoreover, a new dynamic facial expression dataset with different modes of\nvariation, including various modes like pose and illumination variations, was\ncollected to comprehensively evaluate the proposed mode variational LSTM.\nExperimental results verified that the proposed mode variational LSTM encodes\nspatio-temporal features robust to unseen modes of variation. \n\n"}
{"id": "1811.07484", "contents": "Title: Sharpen Focus: Learning with Attention Separability and Consistency Abstract: Recent developments in gradient-based attention modeling have seen attention\nmaps emerge as a powerful tool for interpreting convolutional neural networks.\nDespite good localization for an individual class of interest, these techniques\nproduce attention maps with substantially overlapping responses among different\nclasses, leading to the problem of visual confusion and the need for\ndiscriminative attention. In this paper, we address this problem by means of a\nnew framework that makes class-discriminative attention a principled part of\nthe learning process. Our key innovations include new learning objectives for\nattention separability and cross-layer consistency, which result in improved\nattention discriminability and reduced visual confusion. Extensive experiments\non image classification benchmarks show the effectiveness of our approach in\nterms of improved classification accuracy, including CIFAR-100 (+3.33%),\nCaltech-256 (+1.64%), ILSVRC2012 (+0.92%), CUB-200-2011 (+4.8%) and PASCAL\nVOC2012 (+5.73%). \n\n"}
{"id": "1811.07488", "contents": "Title: Quantifying Human Behavior on the Block Design Test Through Automated\n  Multi-Level Analysis of Overhead Video Abstract: The block design test is a standardized, widely used neuropsychological\nassessment of visuospatial reasoning that involves a person recreating a series\nof given designs out of a set of colored blocks. In current testing procedures,\nan expert neuropsychologist observes a person's accuracy and completion time as\nwell as overall impressions of the person's problem-solving procedures, errors,\netc., thus obtaining a holistic though subjective and often qualitative view of\nthe person's cognitive processes. We propose a new framework that combines room\nsensors and AI techniques to augment the information available to\nneuropsychologists from block design and similar tabletop assessments. In\nparticular, a ceiling-mounted camera captures an overhead view of the table\nsurface. From this video, we demonstrate how automated classification using\nmachine learning can produce a frame-level description of the state of the\nblock task and the person's actions over the course of each test problem. We\nalso show how a sequence-comparison algorithm can classify one individual's\nproblem-solving strategy relative to a database of simulated strategies, and\nhow these quantitative results can be visualized for use by neuropsychologists. \n\n"}
{"id": "1811.07502", "contents": "Title: Fast Efficient Object Detection Using Selective Attention Abstract: Retraction due to significant oversight \n\n"}
{"id": "1811.07555", "contents": "Title: Three Dimensional Convolutional Neural Network Pruning with\n  Regularization-Based Method Abstract: Despite enjoying extensive applications in video analysis, three-dimensional\nconvolutional neural networks (3D CNNs)are restricted by their massive\ncomputation and storage consumption. To solve this problem, we propose a\nthreedimensional regularization-based neural network pruning method to assign\ndifferent regularization parameters to different weight groups based on their\nimportance to the network. Further we analyze the redundancy and computation\ncost for each layer to determine the different pruning ratios. Experiments show\nthat pruning based on our method can lead to 2x theoretical speedup with only\n0.41% accuracy loss for 3DResNet18 and 3.28% accuracy loss for C3D. The\nproposed method performs favorably against other popular methods for model\ncompression and acceleration. \n\n"}
{"id": "1811.07662", "contents": "Title: Intention Oriented Image Captions with Guiding Objects Abstract: Although existing image caption models can produce promising results using\nrecurrent neural networks (RNNs), it is difficult to guarantee that an object\nwe care about is contained in generated descriptions, for example in the case\nthat the object is inconspicuous in the image. Problems become even harder when\nthese objects did not appear in training stage. In this paper, we propose a\nnovel approach for generating image captions with guiding objects (CGO). The\nCGO constrains the model to involve a human-concerned object when the object is\nin the image. CGO ensures that the object is in the generated description while\nmaintaining fluency. Instead of generating the sequence from left to right, we\nstart the description with a selected object and generate other parts of the\nsequence based on this object. To achieve this, we design a novel framework\ncombining two LSTMs in opposite directions. We demonstrate the characteristics\nof our method on MSCOCO where we generate descriptions for each detected object\nin the images. With CGO, we can extend the ability of description to the\nobjects being neglected in image caption labels and provide a set of more\ncomprehensive and diverse descriptions for an image. CGO shows advantages when\napplied to the task of describing novel objects. We show experimental results\non both MSCOCO and ImageNet datasets. Evaluations show that our method\noutperforms the state-of-the-art models in the task with average F1 75.8,\nleading to better descriptions in terms of both content accuracy and fluency. \n\n"}
{"id": "1811.07753", "contents": "Title: Contextual Face Recognition with a Nested-Hierarchical Nonparametric\n  Identity Model Abstract: Current face recognition systems typically operate via classification into\nknown identities obtained from supervised identity annotations. There are two\nproblems with this paradigm: (1) current systems are unable to benefit from\noften abundant unlabelled data; and (2) they equate successful recognition with\nlabelling a given input image. Humans, on the other hand, regularly perform\nidentification of individuals completely unsupervised, recognising the identity\nof someone they have seen before even without being able to name that\nindividual. How can we go beyond the current classification paradigm towards a\nmore human understanding of identities? In previous work, we proposed an\nintegrated Bayesian model that coherently reasons about the observed images,\nidentities, partial knowledge about names, and the situational context of each\nobservation. Here, we propose extensions of the contextual component of this\nmodel, enabling unsupervised discovery of an unbounded number of contexts for\nimproved face recognition. \n\n"}
{"id": "1811.07770", "contents": "Title: Aff-Wild2: Extending the Aff-Wild Database for Affect Recognition Abstract: Automatic understanding of human affect using visual signals is a problem\nthat has attracted significant interest over the past 20 years. However, human\nemotional states are quite complex. To appraise such states displayed in\nreal-world settings, we need expressive emotional descriptors that are capable\nof capturing and describing this complexity. The circumplex model of affect,\nwhich is described in terms of valence (i.e., how positive or negative is an\nemotion) and arousal (i.e., power of the activation of the emotion), can be\nused for this purpose. Recent progress in the emotion recognition domain has\nbeen achieved through the development of deep neural architectures and the\navailability of very large training databases. To this end, Aff-Wild has been\nthe first large-scale \"in-the-wild\" database, containing around 1,200,000\nframes. In this paper, we build upon this database, extending it with 260 more\nsubjects and 1,413,000 new video frames. We call the union of Aff-Wild with the\nadditional data, Aff-Wild2. The videos are downloaded from Youtube and have\nlarge variations in pose, age, illumination conditions, ethnicity and\nprofession. Both database-specific as well as cross-database experiments are\nperformed in this paper, by utilizing the Aff-Wild2, along with the RECOLA\ndatabase. The developed deep neural architectures are based on the joint\ntraining of state-of-the-art convolutional and recurrent neural networks with\nattention mechanism; thus exploiting both the invariant properties of\nconvolutional features, while modeling temporal dynamics that arise in human\nbehaviour via the recurrent layers. The obtained results show premise for\nutilization of the extended Aff-Wild, as well as of the developed deep neural\narchitectures for visual analysis of human behaviour in terms of continuous\nemotion dimensions. \n\n"}
{"id": "1811.08043", "contents": "Title: Recurrent Iterative Gating Networks for Semantic Segmentation Abstract: In this paper, we present an approach for Recurrent Iterative Gating called\nRIGNet. The core elements of RIGNet involve recurrent connections that control\nthe flow of information in neural networks in a top-down manner, and different\nvariants on the core structure are considered. The iterative nature of this\nmechanism allows for gating to spread in both spatial extent and feature space.\nThis is revealed to be a powerful mechanism with broad compatibility with\ncommon existing networks. Analysis shows how gating interacts with different\nnetwork characteristics, and we also show that more shallow networks with\ngating may be made to perform better than much deeper networks that do not\ninclude RIGNet modules. \n\n"}
{"id": "1811.08890", "contents": "Title: Learning from Multiview Correlations in Open-Domain Videos Abstract: An increasing number of datasets contain multiple views, such as video, sound\nand automatic captions. A basic challenge in representation learning is how to\nleverage multiple views to learn better representations. This is further\ncomplicated by the existence of a latent alignment between views, such as\nbetween speech and its transcription, and by the multitude of choices for the\nlearning objective. We explore an advanced, correlation-based representation\nlearning method on a 4-way parallel, multimodal dataset, and assess the quality\nof the learned representations on retrieval-based tasks. We show that the\nproposed approach produces rich representations that capture most of the\ninformation shared across views. Our best models for speech and textual\nmodalities achieve retrieval rates from 70.7% to 96.9% on open-domain,\nuser-generated instructional videos. This shows it is possible to learn\nreliable representations across disparate, unaligned and noisy modalities, and\nencourages using the proposed approach on larger datasets. \n\n"}
{"id": "1811.08965", "contents": "Title: Low-Resolution Face Recognition Abstract: Whilst recent face-recognition (FR) techniques have made significant progress\non recognising constrained high-resolution web images, the same cannot be said\non natively unconstrained low-resolution images at large scales. In this work,\nwe examine systematically this under-studied FR problem, and introduce a novel\nComplement Super-Resolution and Identity (CSRI) joint deep learning method with\na unified end-to-end network architecture. We further construct a new\nlarge-scale dataset TinyFace of native unconstrained low-resolution face images\nfrom selected public datasets, because none benchmark of this nature exists in\nthe literature. With extensive experiments we show there is a significant gap\nbetween the reported FR performances on popular benchmarks and the results on\nTinyFace, and the advantages of the proposed CSRI over a variety of\nstate-of-the-art FR and super-resolution deep models on solving this largely\nignored FR scenario. The TinyFace dataset is released publicly at:\nhttps://qmul-tinyface.github.io/. \n\n"}
{"id": "1811.09567", "contents": "Title: How does Lipschitz Regularization Influence GAN Training? Abstract: Despite the success of Lipschitz regularization in stabilizing GAN training,\nthe exact reason of its effectiveness remains poorly understood. The direct\neffect of $K$-Lipschitz regularization is to restrict the $L2$-norm of the\nneural network gradient to be smaller than a threshold $K$ (e.g., $K=1$) such\nthat $\\|\\nabla f\\| \\leq K$. In this work, we uncover an even more important\neffect of Lipschitz regularization by examining its impact on the loss\nfunction: It degenerates GAN loss functions to almost linear ones by\nrestricting their domain and interval of attainable gradient values. Our\nanalysis shows that loss functions are only successful if they are degenerated\nto almost linear ones. We also show that loss functions perform poorly if they\nare not degenerated and that a wide range of functions can be used as loss\nfunction as long as they are sufficiently degenerated by regularization.\nBasically, Lipschitz regularization ensures that all loss functions effectively\nwork in the same way. Empirically, we verify our proposition on the MNIST,\nCIFAR10 and CelebA datasets. \n\n"}
{"id": "1811.09681", "contents": "Title: Detailed Investigation of Deep Features with Sparse Representation and\n  Dimensionality Reduction in CBIR: A Comparative Study Abstract: Research on content-based image retrieval (CBIR) has been under development\nfor decades, and numerous methods have been competing to extract the most\ndiscriminative features for improved representation of the image content.\nRecently, deep learning methods have gained attention in computer vision,\nincluding CBIR. In this paper, we present a comparative investigation of\ndifferent features, including low-level and high-level features, for CBIR. We\ncompare the performance of CBIR systems using different deep features with\nstate-of-the-art low-level features such as SIFT, SURF, HOG, LBP, and LTP,\nusing different dictionaries and coefficient learning techniques. Furthermore,\nwe conduct comparisons with a set of primitive and popular features that have\nbeen used in this field, including colour histograms and Gabor features. We\nalso investigate the discriminative power of deep features using certain\nsimilarity measures under different validation approaches. Furthermore, we\ninvestigate the effects of the dimensionality reduction of deep features on the\nperformance of CBIR systems using principal component analysis, discrete\nwavelet transform, and discrete cosine transform. Unprecedentedly, the\nexperimental results demonstrate high (95\\% and 93\\%) mean average precisions\nwhen using the VGG-16 FC7 deep features of Corel-1000 and Coil-20 datasets with\n10-D and 20-D K-SVD, respectively. \n\n"}
{"id": "1811.10052", "contents": "Title: An overview of deep learning in medical imaging focusing on MRI Abstract: What has happened in machine learning lately, and what does it mean for the\nfuture of medical image analysis? Machine learning has witnessed a tremendous\namount of attention over the last few years. The current boom started around\n2009 when so-called deep artificial neural networks began outperforming other\nestablished models on a number of important benchmarks. Deep neural networks\nare now the state-of-the-art machine learning models across a variety of areas,\nfrom image analysis to natural language processing, and widely deployed in\nacademia and industry. These developments have a huge potential for medical\nimaging technology, medical data analysis, medical diagnostics and healthcare\nin general, slowly being realized. We provide a short overview of recent\nadvances and some associated challenges in machine learning applied to medical\nimage processing and image analysis. As this has become a very broad and fast\nexpanding field we will not survey the entire landscape of applications, but\nput particular focus on deep learning in MRI.\n  Our aim is threefold: (i) give a brief introduction to deep learning with\npointers to core references; (ii) indicate how deep learning has been applied\nto the entire MRI processing chain, from acquisition to image retrieval, from\nsegmentation to disease prediction; (iii) provide a starting point for people\ninterested in experimenting and perhaps contributing to the field of machine\nlearning for medical imaging by pointing out good educational resources,\nstate-of-the-art open-source code, and interesting sources of data and problems\nrelated medical imaging. \n\n"}
{"id": "1811.10475", "contents": "Title: Sentence Encoding with Tree-constrained Relation Networks Abstract: The meaning of a sentence is a function of the relations that hold between\nits words. We instantiate this relational view of semantics in a series of\nneural models based on variants of relation networks (RNs) which represent a\nset of objects (for us, words forming a sentence) in terms of representations\nof pairs of objects. We propose two extensions to the basic RN model for\nnatural language. First, building on the intuition that not all word pairs are\nequally informative about the meaning of a sentence, we use constraints based\non both supervised and unsupervised dependency syntax to control which\nrelations influence the representation. Second, since higher-order relations\nare poorly captured by a sum of pairwise relations, we use a recurrent\nextension of RNs to propagate information so as to form representations of\nhigher order relations. Experiments on sentence classification, sentence pair\nclassification, and machine translation reveal that, while basic RNs are only\nmodestly effective for sentence representation, recurrent RNs with latent\nsyntax are a reliably powerful representational device. \n\n"}
{"id": "1811.10515", "contents": "Title: Deep Network Interpolation for Continuous Imagery Effect Transition Abstract: Deep convolutional neural network has demonstrated its capability of learning\na deterministic mapping for the desired imagery effect. However, the large\nvariety of user flavors motivates the possibility of continuous transition\namong different output effects. Unlike existing methods that require a specific\ndesign to achieve one particular transition (e.g., style transfer), we propose\na simple yet universal approach to attain a smooth control of diverse imagery\neffects in many low-level vision tasks, including image restoration,\nimage-to-image translation, and style transfer. Specifically, our method,\nnamely Deep Network Interpolation (DNI), applies linear interpolation in the\nparameter space of two or more correlated networks. A smooth control of imagery\neffects can be achieved by tweaking the interpolation coefficients. In addition\nto DNI and its broad applications, we also investigate the mechanism of network\ninterpolation from the perspective of learned filters. \n\n"}
{"id": "1811.10519", "contents": "Title: Unsupervised 3D Shape Learning from Image Collections in the Wild Abstract: We present a method to learn the 3D surface of objects directly from a\ncollection of images. Previous work achieved this capability by exploiting\nadditional manual annotation, such as object pose, 3D surface templates,\ntemporal continuity of videos, manually selected landmarks, and\nforeground/background masks. In contrast, our method does not make use of any\nsuch annotation. Rather, it builds a generative model, a convolutional neural\nnetwork, which, given a noise vector sample, outputs the 3D surface and texture\nof an object and a background image. These 3 components combined with an\nadditional random viewpoint vector are then fed to a differential renderer to\nproduce a view of the sampled object and background. Our general principle is\nthat if the output of the renderer, the generated image, is realistic, then its\ninput, the generated 3D and texture, should also be realistic. To achieve\nrealism, the generative model is trained adversarially against a discriminator\nthat tries to distinguish between the output of the renderer and real images\nfrom the given data set. Moreover, our generative model can be paired with an\nencoder and trained as an autoencoder, to automatically extract the 3D shape,\ntexture and pose of the object in an image. Our trained generative model and\nencoder show promising results both on real and synthetic data, which\ndemonstrate for the first time that fully unsupervised 3D learning from image\ncollections is possible. \n\n"}
{"id": "1811.10984", "contents": "Title: Eliminating Exposure Bias and Loss-Evaluation Mismatch in Multiple\n  Object Tracking Abstract: Identity Switching remains one of the main difficulties Multiple Object\nTracking (MOT) algorithms have to deal with. Many state-of-the-art approaches\nnow use sequence models to solve this problem but their training can be\naffected by biases that decrease their efficiency. In this paper, we introduce\na new training procedure that confronts the algorithm to its own mistakes while\nexplicitly attempting to minimize the number of switches, which results in\nbetter training. We propose an iterative scheme of building a rich training set\nand using it to learn a scoring function that is an explicit proxy for the\ntarget tracking metric. Whether using only simple geometric features or more\nsophisticated ones that also take appearance into account, our approach\noutperforms the state-of-the-art on several MOT benchmarks. \n\n"}
{"id": "1811.10996", "contents": "Title: CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling Abstract: In real-world applications of natural language generation, there are often\nconstraints on the target sentences in addition to fluency and naturalness\nrequirements. Existing language generation techniques are usually based on\nrecurrent neural networks (RNNs). However, it is non-trivial to impose\nconstraints on RNNs while maintaining generation quality, since RNNs generate\nsentences sequentially (or with beam search) from the first word to the last.\nIn this paper, we propose CGMH, a novel approach using Metropolis-Hastings\nsampling for constrained sentence generation. CGMH allows complicated\nconstraints such as the occurrence of multiple keywords in the target\nsentences, which cannot be handled in traditional RNN-based approaches.\nMoreover, CGMH works in the inference stage, and does not require parallel\ncorpora for training. We evaluate our method on a variety of tasks, including\nkeywords-to-sentence generation, unsupervised sentence paraphrasing, and\nunsupervised sentence error correction. CGMH achieves high performance compared\nwith previous supervised methods for sentence generation. Our code is released\nat https://github.com/NingMiao/CGMH \n\n"}
{"id": "1811.11057", "contents": "Title: Fast Object Detection in Compressed Video Abstract: Object detection in videos has drawn increasing attention since it is more\npractical in real scenarios. Most of the deep learning methods use CNNs to\nprocess each decoded frame in a video stream individually. However, the free of\ncharge yet valuable motion information already embedded in the video\ncompression format is usually overlooked. In this paper, we propose a fast\nobject detection method by taking advantage of this with a novel Motion aided\nMemory Network (MMNet). The MMNet has two major advantages: 1) It significantly\naccelerates the procedure of feature extraction for compressed videos. It only\nneed to run a complete recognition network for I-frames, i.e. a few reference\nframes in a video, and it produces the features for the following P frames\n(predictive frames) with a light weight memory network, which runs fast; 2)\nUnlike existing methods that establish an additional network to model motion of\nframes, we take full advantage of both motion vectors and residual errors that\nare freely available in video streams. To our best knowledge, the MMNet is the\nfirst work that investigates a deep convolutional detector on compressed\nvideos. Our method is evaluated on the large-scale ImageNet VID dataset, and\nthe results show that it is 3x times faster than single image detector R-FCN\nand 10x times faster than high-performance detector MANet at a minor accuracy\nloss. \n\n"}
{"id": "1811.11283", "contents": "Title: A Compact Embedding for Facial Expression Similarity Abstract: Most of the existing work on automatic facial expression analysis focuses on\ndiscrete emotion recognition, or facial action unit detection. However, facial\nexpressions do not always fall neatly into pre-defined semantic categories.\nAlso, the similarity between expressions measured in the action unit space need\nnot correspond to how humans perceive expression similarity. Different from\nprevious work, our goal is to describe facial expressions in a continuous\nfashion using a compact embedding space that mimics human visual preferences.\nTo achieve this goal, we collect a large-scale faces-in-the-wild dataset with\nhuman annotations in the form: Expressions A and B are visually more similar\nwhen compared to expression C, and use this dataset to train a neural network\nthat produces a compact (16-dimensional) expression embedding. We\nexperimentally demonstrate that the learned embedding can be successfully used\nfor various applications such as expression retrieval, photo album\nsummarization, and emotion recognition. We also show that the embedding learned\nusing the proposed dataset performs better than several other embeddings\nlearned using existing emotion or action unit datasets. \n\n"}
{"id": "1811.11296", "contents": "Title: Taking Control of Intra-class Variation in Conditional GANs Under Weak\n  Supervision Abstract: Generative Adversarial Networks (GANs) are able to learn mappings between\nsimple, relatively low-dimensional, random distributions and points on the\nmanifold of realistic images in image-space. The semantics of this mapping,\nhowever, are typically entangled such that meaningful image properties cannot\nbe controlled independently of one another. Conditional GANs (cGANs) provide a\npotential solution to this problem, allowing specific semantics to be enforced\nduring training. This solution, however, depends on the availability of precise\nlabels, which are sometimes difficult or near impossible to obtain, e.g. labels\nrepresenting lighting conditions or describing the background. In this paper we\nintroduce a new formulation of the cGAN that is able to learn disentangled,\nmultivariate models of semantically meaningful variation and which has the\nadvantage of requiring only the weak supervision of binary attribute labels.\nFor example, given only labels of ambient / non-ambient lighting, our method is\nable to learn multivariate lighting models disentangled from other factors such\nas the identity and pose. We coin the method intra-class variation isolation\n(IVI) and the resulting network the IVI-GAN. We evaluate IVI-GAN on the CelebA\ndataset and on synthetic 3D morphable model data, learning to disentangle\nattributes such as lighting, pose, expression, and even the background. \n\n"}
{"id": "1811.11318", "contents": "Title: Deep Regionlets: Blended Representation and Deep Learning for Generic\n  Object Detection Abstract: In this paper, we propose a novel object detection algorithm named \"Deep\nRegionlets\" by integrating deep neural networks and a conventional detection\nschema for accurate generic object detection. Motivated by the effectiveness of\nregionlets for modeling object deformations and multiple aspect ratios, we\nincorporate regionlets into an end-to-end trainable deep learning framework.\nThe deep regionlets framework consists of a region selection network and a deep\nregionlet learning module. Specifically, given a detection bounding box\nproposal, the region selection network provides guidance on where to select\nsub-regions from which features can be learned from. An object proposal\ntypically contains 3-16 sub-regions. The regionlet learning module focuses on\nlocal feature selection and transformations to alleviate the effects of\nappearance variations. To this end, we first realize non-rectangular region\nselection within the detection framework to accommodate variations in object\nappearance. Moreover, we design a \"gating network\" within the regionlet leaning\nmodule to enable instance dependent soft feature selection and pooling. The\nDeep Regionlets framework is trained end-to-end without additional efforts. We\npresent ablation studies and extensive experiments on the PASCAL VOC dataset\nand the Microsoft COCO dataset. The proposed method yields competitive\nperformance over state-of-the-art algorithms, such as RetinaNet and Mask R-CNN,\neven without additional segmentation labels. \n\n"}
{"id": "1811.11582", "contents": "Title: Continuous Trade-off Optimization between Fast and Accurate Deep Face\n  Detectors Abstract: Although deep neural networks offer better face detection results than\nshallow or handcrafted models, their complex architectures come with higher\ncomputational requirements and slower inference speeds than shallow neural\nnetworks. In this context, we study five straightforward approaches to achieve\nan optimal trade-off between accuracy and speed in face detection. All the\napproaches are based on separating the test images in two batches, an easy\nbatch that is fed to a faster face detector and a difficult batch that is fed\nto a more accurate yet slower detector. We conduct experiments on the AFW and\nthe FDDB data sets, using MobileNet-SSD as the fast face detector and S3FD\n(Single Shot Scale-invariant Face Detector) as the accurate face detector, both\nmodels being pre-trained on the WIDER FACE data set. Our experiments show that\nthe proposed difficulty metrics compare favorably to a random split of the\nimages. \n\n"}
{"id": "1811.11903", "contents": "Title: Visual Question Answering as Reading Comprehension Abstract: Visual question answering (VQA) demands simultaneous comprehension of both\nthe image visual content and natural language questions. In some cases, the\nreasoning needs the help of common sense or general knowledge which usually\nappear in the form of text. Current methods jointly embed both the visual\ninformation and the textual feature into the same space. However, how to model\nthe complex interactions between the two different modalities is not an easy\ntask. In contrast to struggling on multimodal feature fusion, in this paper, we\npropose to unify all the input information by natural language so as to convert\nVQA into a machine reading comprehension problem. With this transformation, our\nmethod not only can tackle VQA datasets that focus on observation based\nquestions, but can also be naturally extended to handle knowledge-based VQA\nwhich requires to explore large-scale external knowledge base. It is a step\ntowards being able to exploit large volumes of text and natural language\nprocessing techniques to address VQA problem. Two types of models are proposed\nto deal with open-ended VQA and multiple-choice VQA respectively. We evaluate\nour models on three VQA benchmarks. The comparable performance with the\nstate-of-the-art demonstrates the effectiveness of the proposed method. \n\n"}
{"id": "1811.12043", "contents": "Title: MAMNet: Multi-path Adaptive Modulation Network for Image\n  Super-Resolution Abstract: In recent years, single image super-resolution (SR) methods based on deep\nconvolutional neural networks (CNNs) have made significant progress. However,\ndue to the non-adaptive nature of the convolution operation, they cannot adapt\nto various characteristics of images, which limits their representational\ncapability and, consequently, results in unnecessarily large model sizes. To\naddress this issue, we propose a novel multi-path adaptive modulation network\n(MAMNet). Specifically, we propose a multi-path adaptive modulation block\n(MAMB), which is a lightweight yet effective residual block that adaptively\nmodulates residual feature responses by fully exploiting their information via\nthree paths. The three paths model three types of information suitable for SR:\n1) channel-specific information (CSI) using global variance pooling, 2)\ninter-channel dependencies (ICD) based on the CSI, 3) and channel-specific\nspatial dependencies (CSD) via depth-wise convolution. We demonstrate that the\nproposed MAMB is effective and parameter-efficient for image SR than other\nfeature modulation methods. In addition, experimental results show that our\nMAMNet outperforms most of the state-of-the-art methods with a relatively small\nnumber of parameters. \n\n"}
{"id": "1811.12354", "contents": "Title: Touchdown: Natural Language Navigation and Spatial Reasoning in Visual\n  Street Environments Abstract: We study the problem of jointly reasoning about language and vision through a\nnavigation and spatial reasoning task. We introduce the Touchdown task and\ndataset, where an agent must first follow navigation instructions in a\nreal-life visual urban environment, and then identify a location described in\nnatural language to find a hidden object at the goal position. The data\ncontains 9,326 examples of English instructions and spatial descriptions paired\nwith demonstrations. Empirical analysis shows the data presents an open\nchallenge to existing methods, and qualitative linguistic analysis shows that\nthe data displays richer use of spatial reasoning compared to related\nresources. \n\n"}
{"id": "1811.12704", "contents": "Title: Style Decomposition for Improved Neural Style Transfer Abstract: Universal Neural Style Transfer (NST) methods are capable of performing style\ntransfer of arbitrary styles in a style-agnostic manner via feature transforms\nin (almost) real-time. Even though their unimodal parametric style modeling\napproach has been proven adequate to transfer a single style from relatively\nsimple images, they are usually not capable of effectively handling more\ncomplex styles, producing significant artifacts, as well as reducing the\nquality of the synthesized textures in the stylized image. To overcome these\nlimitations, in this paper we propose a novel universal NST approach that\nseparately models each sub-style that exists in a given style image (or a\ncollection of style images). This allows for better modeling the subtle style\ndifferences within the same style image and then using the most appropriate\nsub-style (or mixtures of different sub-styles) to stylize the content image.\nThe ability of the proposed approach to a) perform a wide range of different\nstylizations using the sub-styles that exist in one style image, while giving\nthe ability to the user to appropriate mix the different sub-styles, b)\nautomatically match the most appropriate sub-style to different semantic\nregions of the content image, improving existing state-of-the-art universal NST\napproaches, and c) detecting and transferring the sub-styles from collections\nof images are demonstrated through extensive experiments. \n\n"}
{"id": "1811.12755", "contents": "Title: Projection Convolutional Neural Networks for 1-bit CNNs via Discrete\n  Back Propagation Abstract: The advancement of deep convolutional neural networks (DCNNs) has driven\nsignificant improvement in the accuracy of recognition systems for many\ncomputer vision tasks. However, their practical applications are often\nrestricted in resource-constrained environments. In this paper, we introduce\nprojection convolutional neural networks (PCNNs) with a discrete back\npropagation via projection (DBPP) to improve the performance of binarized\nneural networks (BNNs). The contributions of our paper include: 1) for the\nfirst time, the projection function is exploited to efficiently solve the\ndiscrete back propagation problem, which leads to a new highly compressed CNNs\n(termed PCNNs); 2) by exploiting multiple projections, we learn a set of\ndiverse quantized kernels that compress the full-precision kernels in a more\nefficient way than those proposed previously; 3) PCNNs achieve the best\nclassification performance compared to other state-of-the-art BNNs on the\nImageNet and CIFAR datasets. \n\n"}
{"id": "1812.00020", "contents": "Title: TextureNet: Consistent Local Parametrizations for Learning from\n  High-Resolution Signals on Meshes Abstract: We introduce, TextureNet, a neural network architecture designed to extract\nfeatures from high-resolution signals associated with 3D surface meshes (e.g.,\ncolor texture maps). The key idea is to utilize a 4-rotational symmetric\n(4-RoSy) field to define a domain for convolution on a surface. Though 4-RoSy\nfields have several properties favorable for convolution on surfaces (low\ndistortion, few singularities, consistent parameterization, etc.), orientations\nare ambiguous up to 4-fold rotation at any sample point. So, we introduce a new\nconvolutional operator invariant to the 4-RoSy ambiguity and use it in a\nnetwork to extract features from high-resolution signals on geodesic\nneighborhoods of a surface. In comparison to alternatives, such as PointNet\nbased methods which lack a notion of orientation, the coherent structure given\nby these neighborhoods results in significantly stronger features. As an\nexample application, we demonstrate the benefits of our architecture for 3D\nsemantic segmentation of textured 3D meshes. The results show that our method\noutperforms all existing methods on the basis of mean IoU by a significant\nmargin in both geometry-only (6.4%) and RGB+Geometry (6.9-8.2%) settings. \n\n"}
{"id": "1812.00123", "contents": "Title: Snapshot Distillation: Teacher-Student Optimization in One Generation Abstract: Optimizing a deep neural network is a fundamental task in computer vision,\nyet direct training methods often suffer from over-fitting. Teacher-student\noptimization aims at providing complementary cues from a model trained\npreviously, but these approaches are often considerably slow due to the\npipeline of training a few generations in sequence, i.e., time complexity is\nincreased by several times.\n  This paper presents snapshot distillation (SD), the first framework which\nenables teacher-student optimization in one generation. The idea of SD is very\nsimple: instead of borrowing supervision signals from previous generations, we\nextract such information from earlier epochs in the same generation, meanwhile\nmake sure that the difference between teacher and student is sufficiently large\nso as to prevent under-fitting. To achieve this goal, we implement SD in a\ncyclic learning rate policy, in which the last snapshot of each cycle is used\nas the teacher for all iterations in the next cycle, and the teacher signal is\nsmoothed to provide richer information. In standard image classification\nbenchmarks such as CIFAR100 and ILSVRC2012, SD achieves consistent accuracy\ngain without heavy computational overheads. We also verify that models\npre-trained with SD transfers well to object detection and semantic\nsegmentation in the PascalVOC dataset. \n\n"}
{"id": "1812.00740", "contents": "Title: Disentangling Adversarial Robustness and Generalization Abstract: Obtaining deep networks that are robust against adversarial examples and\ngeneralize well is an open problem. A recent hypothesis even states that both\nrobust and accurate models are impossible, i.e., adversarial robustness and\ngeneralization are conflicting goals. In an effort to clarify the relationship\nbetween robustness and generalization, we assume an underlying, low-dimensional\ndata manifold and show that: 1. regular adversarial examples leave the\nmanifold; 2. adversarial examples constrained to the manifold, i.e.,\non-manifold adversarial examples, exist; 3. on-manifold adversarial examples\nare generalization errors, and on-manifold adversarial training boosts\ngeneralization; 4. regular robustness and generalization are not necessarily\ncontradicting goals. These assumptions imply that both robust and accurate\nmodels are possible. However, different models (architectures, training\nstrategies etc.) can exhibit different robustness and generalization\ncharacteristics. To confirm our claims, we present extensive experiments on\nsynthetic data (with known manifold) as well as on EMNIST, Fashion-MNIST and\nCelebA. \n\n"}
{"id": "1812.00786", "contents": "Title: Generating Material Maps to Map Informal Settlements Abstract: Detecting and mapping informal settlements encompasses several of the United\nNations sustainable development goals. This is because informal settlements are\nhome to the most socially and economically vulnerable people on the planet.\nThus, understanding where these settlements are is of paramount importance to\nboth government and non-government organizations (NGOs), such as the United\nNations Children's Fund (UNICEF), who can use this information to deliver\neffective social and economic aid. We propose a method that detects and maps\nthe locations of informal settlements using only freely available, Sentinel-2\nlow-resolution satellite spectral data and socio-economic data. This is in\ncontrast to previous studies that only use costly very-high resolution (VHR)\nsatellite and aerial imagery. We show how we can detect informal settlements by\ncombining both domain knowledge and machine learning techniques, to build a\nclassifier that looks for known roofing materials used in informal settlements.\nPlease find additional material at\nhttps://frontierdevelopmentlab.github.io/informal-settlements/. \n\n"}
{"id": "1812.01157", "contents": "Title: Cross-Classification Clustering: An Efficient Multi-Object Tracking\n  Technique for 3-D Instance Segmentation in Connectomics Abstract: Pixel-accurate tracking of objects is a key element in many computer vision\napplications, often solved by iterated individual object tracking or instance\nsegmentation followed by object matching. Here we introduce\ncross-classification clustering (3C), a technique that simultaneously tracks\ncomplex, interrelated objects in an image stack. The key idea in\ncross-classification is to efficiently turn a clustering problem into a\nclassification problem by running a logarithmic number of independent\nclassifications per image, letting the cross-labeling of these classifications\nuniquely classify each pixel to the object labels. We apply the 3C mechanism to\nachieve state-of-the-art accuracy in connectomics -- the nanoscale mapping of\nneural tissue from electron microscopy volumes. Our reconstruction system\nincreases scalability by an order of magnitude over existing single-object\ntracking methods (such as flood-filling networks). This scalability is\nimportant for the deployment of connectomics pipelines, since currently the\nbest performing techniques require computing infrastructures that are beyond\nthe reach of most laboratories. Our algorithm may offer benefits in other\ndomains that require pixel-accurate tracking of multiple objects, such as\nsegmentation of videos and medical imagery. \n\n"}
{"id": "1812.01214", "contents": "Title: Prototype-based Neural Network Layers: Incorporating Vector Quantization Abstract: Neural networks currently dominate the machine learning community and they do\nso for good reasons. Their accuracy on complex tasks such as image\nclassification is unrivaled at the moment and with recent improvements they are\nreasonably easy to train. Nevertheless, neural networks are lacking robustness\nand interpretability. Prototype-based vector quantization methods on the other\nhand are known for being robust and interpretable. For this reason, we propose\ntechniques and strategies to merge both approaches. This contribution will\nparticularly highlight the similarities between them and outline how to\nconstruct a prototype-based classification layer for multilayer networks.\nAdditionally, we provide an alternative, prototype-based, approach to the\nclassical convolution operation. Numerical results are not part of this report,\ninstead the focus lays on establishing a strong theoretical framework. By\npublishing our framework and the respective theoretical considerations and\njustifications before finalizing our numerical experiments we hope to\njump-start the incorporation of prototype-based learning in neural networks and\nvice versa. \n\n"}
{"id": "1812.02510", "contents": "Title: ForensicTransfer: Weakly-supervised Domain Adaptation for Forgery\n  Detection Abstract: Distinguishing manipulated from real images is becoming increasingly\ndifficult as new sophisticated image forgery approaches come out by the day.\nNaive classification approaches based on Convolutional Neural Networks (CNNs)\nshow excellent performance in detecting image manipulations when they are\ntrained on a specific forgery method. However, on examples from unseen\nmanipulation approaches, their performance drops significantly. To address this\nlimitation in transferability, we introduce Forensic-Transfer (FT). We devise a\nlearning-based forensic detector which adapts well to new domains, i.e., novel\nmanipulation methods and can handle scenarios where only a handful of fake\nexamples are available during training. To this end, we learn a forensic\nembedding based on a novel autoencoder-based architecture that can be used to\ndistinguish between real and fake imagery. The learned embedding acts as a form\nof anomaly detector; namely, an image manipulated from an unseen method will be\ndetected as fake provided it maps sufficiently far away from the cluster of\nreal images. Comparing to prior works, FT shows significant improvements in\ntransferability, which we demonstrate in a series of experiments on\ncutting-edge benchmarks. For instance, on unseen examples, we achieve up to 85%\nin terms of accuracy, and with only a handful of seen examples, our performance\nalready reaches around 95%. \n\n"}
{"id": "1812.02843", "contents": "Title: Fooling Network Interpretation in Image Classification Abstract: Deep neural networks have been shown to be fooled rather easily using\nadversarial attack algorithms. Practical methods such as adversarial patches\nhave been shown to be extremely effective in causing misclassification.\nHowever, these patches are highlighted using standard network interpretation\nalgorithms, thus revealing the identity of the adversary. We show that it is\npossible to create adversarial patches which not only fool the prediction, but\nalso change what we interpret regarding the cause of the prediction. Moreover,\nwe introduce our attack as a controlled setting to measure the accuracy of\ninterpretation algorithms. We show this using extensive experiments for\nGrad-CAM interpretation that transfers to occluding patch interpretation as\nwell. We believe our algorithms can facilitate developing more robust network\ninterpretation tools that truly explain the network's underlying decision\nmaking process. \n\n"}
{"id": "1812.02937", "contents": "Title: Optimizing speed/accuracy trade-off for person re-identification via\n  knowledge distillation Abstract: Finding a person across a camera network plays an important role in video\nsurveillance. For a real-world person re-identification application, in order\nto guarantee an optimal time response, it is crucial to find the balance\nbetween accuracy and speed. We analyse this trade-off, comparing a classical\nmethod, that comprises hand-crafted feature description and metric learning, in\nparticular, LOMO and XQDA, to deep learning based techniques, using image\nclassification networks, ResNet and MobileNets. Additionally, we propose and\nanalyse network distillation as a learning strategy to reduce the computational\ncost of the deep learning approach at test time. We evaluate both methods on\nthe Market-1501 and DukeMTMC-reID large-scale datasets, showing that\ndistillation helps reducing the computational cost at inference time while even\nincreasing the accuracy performance. \n\n"}
{"id": "1812.03079", "contents": "Title: ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing\n  the Worst Abstract: Our goal is to train a policy for autonomous driving via imitation learning\nthat is robust enough to drive a real vehicle. We find that standard behavior\ncloning is insufficient for handling complex driving scenarios, even when we\nleverage a perception system for preprocessing the input and a controller for\nexecuting the output on the car: 30 million examples are still not enough. We\npropose exposing the learner to synthesized data in the form of perturbations\nto the expert's driving, which creates interesting situations such as\ncollisions and/or going off the road. Rather than purely imitating all data, we\naugment the imitation loss with additional losses that penalize undesirable\nevents and encourage progress -- the perturbations then provide an important\nsignal for these losses and lead to robustness of the learned model. We show\nthat the ChauffeurNet model can handle complex situations in simulation, and\npresent ablation experiments that emphasize the importance of each of our\nproposed changes and show that the model is responding to the appropriate\ncausal factors. Finally, we demonstrate the model driving a car in the real\nworld. \n\n"}
{"id": "1812.03115", "contents": "Title: Kernel Transformer Networks for Compact Spherical Convolution Abstract: Ideally, 360{\\deg} imagery could inherit the deep convolutional neural\nnetworks (CNNs) already trained with great success on perspective projection\nimages. However, existing methods to transfer CNNs from perspective to\nspherical images introduce significant computational costs and/or degradations\nin accuracy. In this work, we present the Kernel Transformer Network (KTN).\nKTNs efficiently transfer convolution kernels from perspective images to the\nequirectangular projection of 360{\\deg} images. Given a source CNN for\nperspective images as input, the KTN produces a function parameterized by a\npolar angle and kernel as output. Given a novel 360{\\deg} image, that function\nin turn can compute convolutions for arbitrary layers and kernels as would the\nsource CNN on the corresponding tangent plane projections. Distinct from all\nexisting methods, KTNs allow model transfer: the same model can be applied to\ndifferent source CNNs with the same base architecture. This enables application\nto multiple recognition tasks without re-training the KTN. Validating our\napproach with multiple source CNNs and datasets, we show that KTNs improve the\nstate of the art for spherical convolution. KTNs successfully preserve the\nsource CNN's accuracy, while offering transferability, scalability to typical\nimage resolutions, and, in many cases, a substantially lower memory footprint. \n\n"}
{"id": "1812.03128", "contents": "Title: Backdooring Convolutional Neural Networks via Targeted Weight\n  Perturbations Abstract: We present a new type of backdoor attack that exploits a vulnerability of\nconvolutional neural networks (CNNs) that has been previously unstudied. In\nparticular, we examine the application of facial recognition. Deep learning\ntechniques are at the top of the game for facial recognition, which means they\nhave now been implemented in many production-level systems. Alarmingly, unlike\nother commercial technologies such as operating systems and network devices,\ndeep learning-based facial recognition algorithms are not presently designed\nwith security requirements or audited for security vulnerabilities before\ndeployment. Given how young the technology is and how abstract many of the\ninternal workings of these algorithms are, neural network-based facial\nrecognition systems are prime targets for security breaches. As more and more\nof our personal information begins to be guarded by facial recognition (e.g.,\nthe iPhone X), exploring the security vulnerabilities of these systems from a\npenetration testing standpoint is crucial. Along these lines, we describe a\ngeneral methodology for backdooring CNNs via targeted weight perturbations.\nUsing a five-layer CNN and ResNet-50 as case studies, we show that an attacker\nis able to significantly increase the chance that inputs they supply will be\nfalsely accepted by a CNN while simultaneously preserving the error rates for\nlegitimate enrolled classes. \n\n"}
{"id": "1812.03245", "contents": "Title: Self-Improving Visual Odometry Abstract: We propose a self-supervised learning framework that uses unlabeled monocular\nvideo sequences to generate large-scale supervision for training a Visual\nOdometry (VO) frontend, a network which computes pointwise data associations\nacross images. Our self-improving method enables a VO frontend to learn over\ntime, unlike other VO and SLAM systems which require time-consuming hand-tuning\nor expensive data collection to adapt to new environments. Our proposed\nfrontend operates on monocular images and consists of a single multi-task\nconvolutional neural network which outputs 2D keypoints locations, keypoint\ndescriptors, and a novel point stability score. We use the output of VO to\ncreate a self-supervised dataset of point correspondences to retrain the\nfrontend. When trained using VO at scale on 2.5 million monocular images from\nScanNet, the stability classifier automatically discovers a ranking for\nkeypoints that are not likely to help in VO, such as t-junctions across depth\ndiscontinuities, features on shadows and highlights, and dynamic objects like\npeople. The resulting frontend outperforms both traditional methods (SIFT, ORB,\nAKAZE) and deep learning methods (SuperPoint and LF-Net) in a 3D-to-2D pose\nestimation task on ScanNet. \n\n"}
{"id": "1812.04082", "contents": "Title: Visual Depth Mapping from Monocular Images using Recurrent Convolutional\n  Neural Networks Abstract: A reliable sense-and-avoid system is critical to enabling safe autonomous\noperation of unmanned aircraft. Existing sense-and-avoid methods often require\nspecialized sensors that are too large or power intensive for use on small\nunmanned vehicles. This paper presents a method to estimate object distances\nbased on visual image sequences, allowing for the use of low-cost, on-board\nmonocular cameras as simple collision avoidance sensors. We present a deep\nrecurrent convolutional neural network and training method to generate depth\nmaps from video sequences. Our network is trained using simulated camera and\ndepth data generated with Microsoft's AirSim simulator. Empirically, we show\nthat our model achieves superior performance compared to models generated using\nprior methods.We further demonstrate that the method can be used for\nsense-and-avoid of obstacles in simulation. \n\n"}
{"id": "1812.04202", "contents": "Title: Deep Learning on Graphs: A Survey Abstract: Deep learning has been shown to be successful in a number of domains, ranging\nfrom acoustics, images, to natural language processing. However, applying deep\nlearning to the ubiquitous graph data is non-trivial because of the unique\ncharacteristics of graphs. Recently, substantial research efforts have been\ndevoted to applying deep learning methods to graphs, resulting in beneficial\nadvances in graph analysis techniques. In this survey, we comprehensively\nreview the different types of deep learning methods on graphs. We divide the\nexisting methods into five categories based on their model architectures and\ntraining strategies: graph recurrent neural networks, graph convolutional\nnetworks, graph autoencoders, graph reinforcement learning, and graph\nadversarial methods. We then provide a comprehensive overview of these methods\nin a systematic manner mainly by following their development history. We also\nanalyze the differences and compositions of different methods. Finally, we\nbriefly outline the applications in which they have been used and discuss\npotential future research directions. \n\n"}
{"id": "1812.04353", "contents": "Title: Proximal Mean-field for Neural Network Quantization Abstract: Compressing large Neural Networks (NN) by quantizing the parameters, while\nmaintaining the performance is highly desirable due to reduced memory and time\ncomplexity. In this work, we cast NN quantization as a discrete labelling\nproblem, and by examining relaxations, we design an efficient iterative\noptimization procedure that involves stochastic gradient descent followed by a\nprojection. We prove that our simple projected gradient descent approach is, in\nfact, equivalent to a proximal version of the well-known mean-field method.\nThese findings would allow the decades-old and theoretically grounded research\non MRF optimization to be used to design better network quantization schemes.\nOur experiments on standard classification datasets (MNIST, CIFAR10/100,\nTinyImageNet) with convolutional and residual architectures show that our\nalgorithm obtains fully-quantized networks with accuracies very close to the\nfloating-point reference networks. \n\n"}
{"id": "1812.05477", "contents": "Title: Gaussian Process Deep Belief Networks: A Smooth Generative Model of\n  Shape with Uncertainty Propagation Abstract: The shape of an object is an important characteristic for many vision\nproblems such as segmentation, detection and tracking. Being independent of\nappearance, it is possible to generalize to a large range of objects from only\nsmall amounts of data. However, shapes represented as silhouette images are\nchallenging to model due to complicated likelihood functions leading to\nintractable posteriors. In this paper we present a generative model of shapes\nwhich provides a low dimensional latent encoding which importantly resides on a\nsmooth manifold with respect to the silhouette images. The proposed model\npropagates uncertainty in a principled manner allowing it to learn from small\namounts of data and providing predictions with associated uncertainty. We\nprovide experiments that show how our proposed model provides favorable\nquantitative results compared with the state-of-the-art while simultaneously\nproviding a representation that resides on a low-dimensional interpretable\nmanifold. \n\n"}
{"id": "1812.06224", "contents": "Title: A Low Effort Approach to Structured CNN Design Using PCA Abstract: Deep learning models hold state of the art performance in many fields, yet\ntheir design is still based on heuristics or grid search methods that often\nresult in overparametrized networks. This work proposes a method to analyze a\ntrained network and deduce an optimized, compressed architecture that preserves\naccuracy while keeping computational costs tractable. Model compression is an\nactive field of research that targets the problem of realizing deep learning\nmodels in hardware. However, most pruning methodologies tend to be\nexperimental, requiring large compute and time intensive iterations of\nretraining the entire network. We introduce structure into model design by\nproposing a single shot analysis of a trained network that serves as a first\norder, low effort approach to dimensionality reduction, by using PCA (Principal\nComponent Analysis). The proposed method simultaneously analyzes the\nactivations of each layer and considers the dimensionality of the space\ndescribed by the filters generating these activations. It optimizes the\narchitecture in terms of number of layers, and number of filters per layer\nwithout any iterative retraining procedures, making it a viable, low effort\ntechnique to design efficient networks. We demonstrate the proposed methodology\non AlexNet and VGG style networks on the CIFAR-10, CIFAR-100 and ImageNet\ndatasets, and successfully achieve an optimized architecture with a reduction\nof up to 3.8X and 9X in the number of operations and parameters respectively,\nwhile trading off less than 1% accuracy. We also apply the method to MobileNet,\nand achieve 1.7X and 3.9X reduction in the number of operations and parameters\nrespectively, while improving accuracy by almost one percentage point. \n\n"}
{"id": "1812.06408", "contents": "Title: Human Pose and Path Estimation from Aerial Video using Dynamic\n  Classifier Selection Abstract: We consider the problem of estimating human pose and trajectory by an aerial\nrobot with a monocular camera in near real time. We present a preliminary\nsolution whose distinguishing feature is a dynamic classifier selection\narchitecture. In our solution, each video frame is corrected for perspective\nusing projective transformation. Then, two alternative feature sets are used:\n(i) Histogram of Oriented Gradients (HOG) of the silhouette, (ii) Convolutional\nNeural Network (CNN) features of the RGB image. The features (HOG or CNN) are\nclassified using a dynamic classifier. A class is defined as a pose-viewpoint\npair, and a total of 64 classes are defined to represent a forward walking and\nturning gait sequence. Our solution provides three main advantages: (i)\nClassification is efficient due to dynamic selection (4-class vs. 64-class\nclassification). (ii) Classification errors are confined to neighbors of the\ntrue view-points. (iii) The robust temporal relationship between poses is used\nto resolve the left-right ambiguities of human silhouettes. Experiments\nconducted on both fronto-parallel videos and aerial videos confirm our solution\ncan achieve accurate pose and trajectory estimation for both scenarios. We\nfound using HOG features provides higher accuracy than using CNN features. For\nexample, applying the HOG-based variant of our scheme to the 'walking on a\nfigure 8-shaped path' dataset (1652 frames) achieved estimation accuracies of\n99.6% for viewpoints and 96.2% for number of poses. \n\n"}
{"id": "1812.06544", "contents": "Title: Towards Robust Human Activity Recognition from RGB Video Stream with\n  Limited Labeled Data Abstract: Human activity recognition based on video streams has received numerous\nattentions in recent years. Due to lack of depth information, RGB video based\nactivity recognition performs poorly compared to RGB-D video based solutions.\nOn the other hand, acquiring depth information, inertia etc. is costly and\nrequires special equipment, whereas RGB video streams are available in ordinary\ncameras. Hence, our goal is to investigate whether similar or even higher\naccuracy can be achieved with RGB-only modality. In this regard, we propose a\nnovel framework that couples skeleton data extracted from RGB video and deep\nBidirectional Long Short Term Memory (BLSTM) model for activity recognition. A\nbig challenge of training such a deep network is the limited training data, and\nexploring RGB-only stream significantly exaggerates the difficulty. We\ntherefore propose a set of algorithmic techniques to train this model\neffectively, e.g., data augmentation, dynamic frame dropout and gradient\ninjection. The experiments demonstrate that our RGB-only solution surpasses the\nstate-of-the-art approaches that all exploit RGB-D video streams by a notable\nmargin. This makes our solution widely deployable with ordinary cameras. \n\n"}
{"id": "1812.07119", "contents": "Title: Composing Text and Image for Image Retrieval - An Empirical Odyssey Abstract: In this paper, we study the task of image retrieval, where the input query is\nspecified in the form of an image plus some text that describes desired\nmodifications to the input image. For example, we may present an image of the\nEiffel tower, and ask the system to find images which are visually similar but\nare modified in small ways, such as being taken at nighttime instead of during\nthe day. To tackle this task, we learn a similarity metric between a target\nimage and a source image plus source text, an embedding and composing function\nsuch that target image feature is close to the source image plus text\ncomposition feature. We propose a new way to combine image and text using such\nfunction that is designed for the retrieval task. We show this outperforms\nexisting approaches on 3 different datasets, namely Fashion-200k, MIT-States\nand a new synthetic dataset we create based on CLEVR. We also show that our\napproach can be used to classify input queries, in addition to image retrieval. \n\n"}
{"id": "1812.07124", "contents": "Title: Multi-Level Sequence GAN for Group Activity Recognition Abstract: We propose a novel semi-supervised, Multi-Level Sequential Generative\nAdversarial Network (MLS-GAN) architecture for group activity recognition. In\ncontrast to previous works which utilise manually annotated individual human\naction predictions, we allow the models to learn it's own internal\nrepresentations to discover pertinent sub-activities that aid the final group\nactivity recognition task. The generator is fed with person-level and\nscene-level features that are mapped temporally through LSTM networks.\nAction-based feature fusion is performed through novel gated fusion units that\nare able to consider long-term dependencies, exploring the relationships among\nall individual actions, to learn an intermediate representation or `action\ncode' for the current group activity. The network achieves its semi-supervised\nbehaviour by allowing it to perform group action classification together with\nthe adversarial real/fake validation. We perform extensive evaluations on\ndifferent architectural variants to demonstrate the importance of the proposed\narchitecture. Furthermore, we show that utilising both person-level and\nscene-level features facilitates the group activity prediction better than\nusing only person-level features. Our proposed architecture outperforms current\nstate-of-the-art results for sports and pedestrian based classification tasks\non Volleyball and Collective Activity datasets, showing it's flexible nature\nfor effective learning of group activities. \n\n"}
{"id": "1812.07145", "contents": "Title: Recurrent Calibration Network for Irregular Text Recognition Abstract: Scene text recognition has received increased attention in the research\ncommunity. Text in the wild often possesses irregular arrangements, typically\nincluding perspective text, curved text, oriented text. Most existing methods\nare hard to work well for irregular text, especially for severely distorted\ntext. In this paper, we propose a novel Recurrent Calibration Network (RCN) for\nirregular scene text recognition. The RCN progressively calibrates the\nirregular text to boost the recognition performance. By decomposing the\ncalibration process into multiple steps, the irregular text can be calibrated\nto normal one step by step. Besides, in order to avoid the accumulation of lost\ninformation caused by inaccurate transformation, we further design a\nfiducial-point refinement structure to keep the integrity of text during the\nrecurrent process. Instead of the calibrated images, the coordinates of\nfiducial points are tracked and refined, which implicitly models the\ntransformation information. Based on the refined fiducial points, we estimate\nthe transformation parameters and sample from the original image at each step.\nIn this way, the original character information is preserved until the final\ntransformation. Such designs lead to optimal calibration results to boost the\nperformance of succeeding recognition. Extensive experiments on challenging\ndatasets demonstrate the superiority of our method, especially on irregular\nbenchmarks. \n\n"}
{"id": "1812.07203", "contents": "Title: Video Trajectory Classification and Anomaly Detection Using Hybrid\n  CNN-VAE Abstract: Classifying time series data using neural networks is a challenging problem\nwhen the length of the data varies. Video object trajectories, which are key to\nmany of the visual surveillance applications, are often found to be of varying\nlength. If such trajectories are used to understand the behavior (normal or\nanomalous) of moving objects, they need to be represented correctly. In this\npaper, we propose video object trajectory classification and anomaly detection\nusing a hybrid Convolutional Neural Network (CNN) and Variational Autoencoder\n(VAE) architecture. First, we introduce a high level representation of object\ntrajectories using color gradient form. In the next stage, a semi-supervised\nway to annotate moving object trajectories extracted using Temporal Unknown\nIncremental Clustering (TUIC), has been applied for trajectory class labeling.\nAnomalous trajectories are separated using t-Distributed Stochastic Neighbor\nEmbedding (t-SNE). Finally, a hybrid CNN-VAE architecture has been used for\ntrajectory classification and anomaly detection. The results obtained using\npublicly available surveillance video datasets reveal that the proposed method\ncan successfully identify some of the important traffic anomalies such as\nvehicles not following lane driving, sudden speed variations, abrupt\ntermination of vehicle movement, and vehicles moving in wrong directions. The\nproposed method is able to detect above anomalies at higher accuracy as\ncompared to existing anomaly detection methods. \n\n"}
{"id": "1812.07996", "contents": "Title: Mining Interpretable AOG Representations from Convolutional Networks via\n  Active Question Answering Abstract: In this paper, we present a method to mine object-part patterns from\nconv-layers of a pre-trained convolutional neural network (CNN). The mined\nobject-part patterns are organized by an And-Or graph (AOG). This interpretable\nAOG representation consists of a four-layer semantic hierarchy, i.e., semantic\nparts, part templates, latent patterns, and neural units. The AOG associates\neach object part with certain neural units in feature maps of conv-layers. The\nAOG is constructed in a weakly-supervised manner, i.e., very few annotations\n(e.g., 3-20) of object parts are used to guide the learning of AOGs. We develop\na question-answering (QA) method that uses active human-computer communications\nto mine patterns from a pre-trained CNN, in order to incrementally explain more\nfeatures in conv-layers. During the learning process, our QA method uses the\ncurrent AOG for part localization. The QA method actively identifies objects,\nwhose feature maps cannot be explained by the AOG. Then, our method asks people\nto annotate parts on the unexplained objects, and uses answers to discover CNN\npatterns corresponding to the newly labeled parts. In this way, our method\ngradually grows new branches and refines existing branches on the AOG to\nsemanticize CNN representations. In experiments, our method exhibited a high\nlearning efficiency. Our method used about 1/6-1/3 of the part annotations for\ntraining, but achieved similar or better part-localization performance than\nfast-RCNN methods. \n\n"}
{"id": "1812.08434", "contents": "Title: Graph Neural Networks: A Review of Methods and Applications Abstract: Lots of learning tasks require dealing with graph data which contains rich\nrelation information among elements. Modeling physics systems, learning\nmolecular fingerprints, predicting protein interface, and classifying diseases\ndemand a model to learn from graph inputs. In other domains such as learning\nfrom non-structural data like texts and images, reasoning on extracted\nstructures (like the dependency trees of sentences and the scene graphs of\nimages) is an important research topic which also needs graph reasoning models.\nGraph neural networks (GNNs) are neural models that capture the dependence of\ngraphs via message passing between the nodes of graphs. In recent years,\nvariants of GNNs such as graph convolutional network (GCN), graph attention\nnetwork (GAT), graph recurrent network (GRN) have demonstrated ground-breaking\nperformances on many deep learning tasks. In this survey, we propose a general\ndesign pipeline for GNN models and discuss the variants of each component,\nsystematically categorize the applications, and propose four open problems for\nfuture research. \n\n"}
{"id": "1812.08861", "contents": "Title: Animating Arbitrary Objects via Deep Motion Transfer Abstract: This paper introduces a novel deep learning framework for image animation.\nGiven an input image with a target object and a driving video sequence\ndepicting a moving object, our framework generates a video in which the target\nobject is animated according to the driving sequence. This is achieved through\na deep architecture that decouples appearance and motion information. Our\nframework consists of three main modules: (i) a Keypoint Detector unsupervisely\ntrained to extract object keypoints, (ii) a Dense Motion prediction network for\ngenerating dense heatmaps from sparse keypoints, in order to better encode\nmotion information and (iii) a Motion Transfer Network, which uses the motion\nheatmaps and appearance information extracted from the input image to\nsynthesize the output frames. We demonstrate the effectiveness of our method on\nseveral benchmark datasets, spanning a wide variety of object appearances, and\nshow that our approach outperforms state-of-the-art image animation and video\ngeneration methods. Our source code is publicly available. \n\n"}
{"id": "1812.09336", "contents": "Title: An Empirical Analysis of Deep Audio-Visual Models for Speech Recognition Abstract: In this project, we worked on speech recognition, specifically predicting\nindividual words based on both the video frames and audio. Empowered by\nconvolutional neural networks, the recent speech recognition and lip reading\nmodels are comparable to human level performance. We re-implemented and made\nderivations of the state-of-the-art model. Then, we conducted rich experiments\nincluding the effectiveness of attention mechanism, more accurate residual\nnetwork as the backbone with pre-trained weights and the sensitivity of our\nmodel with respect to audio input with/without noise. \n\n"}
{"id": "1812.09395", "contents": "Title: Multi-Step Prediction of Occupancy Grid Maps with Recurrent Neural\n  Networks Abstract: We investigate the multi-step prediction of the drivable space, represented\nby Occupancy Grid Maps (OGMs), for autonomous vehicles. Our motivation is that\naccurate multi-step prediction of the drivable space can efficiently improve\npath planning and navigation resulting in safe, comfortable and optimum paths\nin autonomous driving. We train a variety of Recurrent Neural Network (RNN)\nbased architectures on the OGM sequences from the KITTI dataset. The results\ndemonstrate significant improvement of the prediction accuracy using our\nproposed difference learning method, incorporating motion related features,\nover the state of the art. We remove the egomotion from the OGM sequences by\ntransforming them into a common frame. Although in the transformed sequences\nthe KITTI dataset is heavily biased toward static objects, by learning the\ndifference between subsequent OGMs, our proposed method provides accurate\nprediction over both the static and moving objects. \n\n"}
{"id": "1812.09533", "contents": "Title: Temporal Hockey Action Recognition via Pose and Optical Flows Abstract: Recognizing actions in ice hockey using computer vision poses challenges due\nto bulky equipment and inadequate image quality. A novel two-stream framework\nhas been designed to improve action recognition accuracy for hockey using three\nmain components. First, pose is estimated via the Part Affinity Fields model to\nextract meaningful cues from the player. Second, optical flow (using\nLiteFlowNet) is used to extract temporal features. Third, pose and optical flow\nstreams are fused and passed to fully-connected layers to estimate the hockey\nplayer's action. A novel publicly available dataset named HARPET (Hockey Action\nRecognition Pose Estimation, Temporal) was created, composed of sequences of\nannotated actions and pose of hockey players including their hockey sticks as\nan extension of human body pose. Three contributions are recognized. (1) The\nnovel two-stream architecture achieves 85% action recognition accuracy, with\nthe inclusion of optical flows increasing accuracy by about 10%. (2) The unique\nlocalization of hand-held objects (e.g., hockey sticks) as part of pose\nincreases accuracy by about 13%. (3) For pose estimation, a bigger and more\ngeneral dataset, MSCOCO, is successfully used for transfer learning to a\nsmaller and more specific dataset, HARPET, achieving a PCKh of 87%. \n\n"}
{"id": "1812.09924", "contents": "Title: Dual Principal Component Pursuit: Probability Analysis and Efficient\n  Algorithms Abstract: Recent methods for learning a linear subspace from data corrupted by outliers\nare based on convex $\\ell_1$ and nuclear norm optimization and require the\ndimension of the subspace and the number of outliers to be sufficiently small.\nIn sharp contrast, the recently proposed Dual Principal Component Pursuit\n(DPCP) method can provably handle subspaces of high dimension by solving a\nnon-convex $\\ell_1$ optimization problem on the sphere. However, its geometric\nanalysis is based on quantities that are difficult to interpret and are not\namenable to statistical analysis. In this paper we provide a refined geometric\nanalysis and a new statistical analysis that show that DPCP can tolerate as\nmany outliers as the square of the number of inliers, thus improving upon other\nprovably correct robust PCA methods. We also propose a scalable Projected\nSub-Gradient Method method (DPCP-PSGM) for solving the DPCP problem and show it\nadmits linear convergence even though the underlying optimization problem is\nnon-convex and non-smooth. Experiments on road plane detection from 3D point\ncloud data demonstrate that DPCP-PSGM can be more efficient than the\ntraditional RANSAC algorithm, which is one of the most popular methods for such\ncomputer vision applications. \n\n"}
{"id": "1812.10179", "contents": "Title: Deep Convolutional Generative Adversarial Network Based Food Recognition\n  Using Partially Labeled Data Abstract: Traditional machine learning algorithms using hand-crafted feature extraction\ntechniques (such as local binary pattern) have limited accuracy because of high\nvariation in images of the same class (or intra-class variation) for food\nrecognition task. In recent works, convolutional neural networks (CNN) have\nbeen applied to this task with better results than all previously reported\nmethods. However, they perform best when trained with large amount of annotated\n(labeled) food images. This is problematic when obtained in large volume,\nbecause they are expensive, laborious and impractical. Our work aims at\ndeveloping an efficient deep CNN learning-based method for food recognition\nalleviating these limitations by using partially labeled training data on\ngenerative adversarial networks (GANs). We make new enhancements to the\nunsupervised training architecture introduced by Goodfellow et al. (2014),\nwhich was originally aimed at generating new data by sampling a dataset. In\nthis work, we make modifications to deep convolutional GANs to make them robust\nand efficient for classifying food images. Experimental results on benchmarking\ndatasets show the superiority of our proposed method as compared to the\ncurrent-state-of-the-art methodologies even when trained with partially labeled\ntraining data. \n\n"}
{"id": "1812.10358", "contents": "Title: Informative Object Annotations: Tell Me Something I Don't Know Abstract: Capturing the interesting components of an image is a key aspect of image\nunderstanding. When a speaker annotates an image, selecting labels that are\ninformative greatly depends on the prior knowledge of a prospective listener.\nMotivated by cognitive theories of categorization and communication, we present\na new unsupervised approach to model this prior knowledge and quantify the\ninformativeness of a description. Specifically, we compute how knowledge of a\nlabel reduces uncertainty over the space of labels and utilize this to rank\ncandidate labels for describing an image. While the full estimation problem is\nintractable, we describe an efficient algorithm to approximate entropy\nreduction using a tree-structured graphical model. We evaluate our approach on\nthe open-images dataset using a new evaluation set of 10K ground-truth ratings\nand find that it achieves ~65% agreement with human raters, largely\noutperforming other unsupervised baseline approaches. \n\n"}
{"id": "1812.10366", "contents": "Title: A Poisson-Gaussian Denoising Dataset with Real Fluorescence Microscopy\n  Images Abstract: Fluorescence microscopy has enabled a dramatic development in modern biology.\nDue to its inherently weak signal, fluorescence microscopy is not only much\nnoisier than photography, but also presented with Poisson-Gaussian noise where\nPoisson noise, or shot noise, is the dominating noise source. To get clean\nfluorescence microscopy images, it is highly desirable to have effective\ndenoising algorithms and datasets that are specifically designed to denoise\nfluorescence microscopy images. While such algorithms exist, no such datasets\nare available. In this paper, we fill this gap by constructing a dataset - the\nFluorescence Microscopy Denoising (FMD) dataset - that is dedicated to\nPoisson-Gaussian denoising. The dataset consists of 12,000 real fluorescence\nmicroscopy images obtained with commercial confocal, two-photon, and wide-field\nmicroscopes and representative biological samples such as cells, zebrafish, and\nmouse brain tissues. We use image averaging to effectively obtain ground truth\nimages and 60,000 noisy images with different noise levels. We use this dataset\nto benchmark 10 representative denoising algorithms and find that deep learning\nmethods have the best performance. To our knowledge, this is the first real\nmicroscopy image dataset for Poisson-Gaussian denoising purposes and it could\nbe an important tool for high-quality, real-time denoising applications in\nbiomedical research. \n\n"}
{"id": "1812.10885", "contents": "Title: Coarse-to-fine Semantic Segmentation from Image-level Labels Abstract: Deep neural network-based semantic segmentation generally requires\nlarge-scale cost extensive annotations for training to obtain better\nperformance. To avoid pixel-wise segmentation annotations which are needed for\nmost methods, recently some researchers attempted to use object-level labels\n(e.g. bounding boxes) or image-level labels (e.g. image categories). In this\npaper, we propose a novel recursive coarse-to-fine semantic segmentation\nframework based on only image-level category labels. For each image, an initial\ncoarse mask is first generated by a convolutional neural network-based\nunsupervised foreground segmentation model and then is enhanced by a graph\nmodel. The enhanced coarse mask is fed to a fully convolutional neural network\nto be recursively refined. Unlike existing image-level label-based semantic\nsegmentation methods which require to label all categories for images contain\nmultiple types of objects, our framework only needs one label for each image\nand can handle images contains multi-category objects. With only trained on\nImageNet, our framework achieves comparable performance on PASCAL VOC dataset\nas other image-level label-based state-of-the-arts of semantic segmentation.\nFurthermore, our framework can be easily extended to foreground object\nsegmentation task and achieves comparable performance with the state-of-the-art\nsupervised methods on the Internet Object dataset. \n\n"}
{"id": "1812.10907", "contents": "Title: Divergence Triangle for Joint Training of Generator Model, Energy-based\n  Model, and Inference Model Abstract: This paper proposes the divergence triangle as a framework for joint training\nof generator model, energy-based model and inference model. The divergence\ntriangle is a compact and symmetric (anti-symmetric) objective function that\nseamlessly integrates variational learning, adversarial learning, wake-sleep\nalgorithm, and contrastive divergence in a unified probabilistic formulation.\nThis unification makes the processes of sampling, inference, energy evaluation\nreadily available without the need for costly Markov chain Monte Carlo methods.\nOur experiments demonstrate that the divergence triangle is capable of learning\n(1) an energy-based model with well-formed energy landscape, (2) direct\nsampling in the form of a generator network, and (3) feed-forward inference\nthat faithfully reconstructs observed as well as synthesized data. The\ndivergence triangle is a robust training method that can learn from incomplete\ndata. \n\n"}
{"id": "1812.10972", "contents": "Title: Reasoning About Physical Interactions with Object-Oriented Prediction\n  and Planning Abstract: Object-based factorizations provide a useful level of abstraction for\ninteracting with the world. Building explicit object representations, however,\noften requires supervisory signals that are difficult to obtain in practice. We\npresent a paradigm for learning object-centric representations for physical\nscene understanding without direct supervision of object properties. Our model,\nObject-Oriented Prediction and Planning (O2P2), jointly learns a perception\nfunction to map from image observations to object representations, a pairwise\nphysics interaction function to predict the time evolution of a collection of\nobjects, and a rendering function to map objects back to pixels. For\nevaluation, we consider not only the accuracy of the physical predictions of\nthe model, but also its utility for downstream tasks that require an actionable\nrepresentation of intuitive physics. After training our model on an image\nprediction task, we can use its learned representations to build block towers\nmore complicated than those observed during training. \n\n"}
{"id": "1812.11328", "contents": "Title: Skeleton Transformer Networks: 3D Human Pose and Skinned Mesh from\n  Single RGB Image Abstract: In this paper, we present Skeleton Transformer Networks (SkeletonNet), an\nend-to-end framework that can predict not only 3D joint positions but also 3D\nangular pose (bone rotations) of a human skeleton from a single color image.\nThis in turn allows us to generate skinned mesh animations. Here, we propose a\ntwo-step regression approach. The first step regresses bone rotations in order\nto obtain an initial solution by considering skeleton structure. The second\nstep performs refinement based on heatmap regressor using a 3D pose\nrepresentation called cross heatmap which stacks heatmaps of xy and zy\ncoordinates. By training the network using the proposed 3D human pose dataset\nthat is comprised of images annotated with 3D skeletal angular poses, we showed\nthat SkeletonNet can predict a full 3D human pose (joint positions and bone\nrotations) from a single image in-the-wild. \n\n"}
{"id": "1901.00275", "contents": "Title: Vector and Line Quantization for Billion-scale Similarity Search on GPUs Abstract: Billion-scale high-dimensional approximate nearest neighbour (ANN) search has\nbecome an important problem for searching similar objects among the vast amount\nof images and videos available online. The existing ANN methods are usually\ncharacterized by their specific indexing structures, including the inverted\nindex and the inverted multi-index structure. The inverted index structure is\namenable to GPU-based implementations, and the state-of-the-art systems such as\nFaiss are able to exploit the massive parallelism offered by GPUs. However, the\ninverted index requires high memory overhead to index the dataset effectively.\nThe inverted multi-index structure is difficult to implement for GPUs, and also\nineffective in dealing with database with different data distributions. In this\npaper we propose a novel hierarchical inverted index structure generated by\nvector and line quantization methods. Our quantization method improves both\nsearch efficiency and accuracy, while maintaining comparable memory\nconsumption. This is achieved by reducing search space and increasing the\nnumber of indexed regions. We introduce a new ANN search system, VLQ-ADC, that\nis based on the proposed inverted index, and perform extensive evaluation on\ntwo public billion-scale benchmark datasets SIFT1B and DEEP1B. Our evaluation\nshows that VLQ-ADC significantly outperforms the state-of-the-art GPU- and\nCPU-based systems in terms of both accuracy and search speed. The source code\nof VLQ-ADC is available at\nhttps://github.com/zjuchenwei/vector-line-quantization. \n\n"}
{"id": "1901.00898", "contents": "Title: Imminent Collision Mitigation with Reinforcement Learning and Vision Abstract: This work examines the role of reinforcement learning in reducing the\nseverity of on-road collisions by controlling velocity and steering in\nsituations in which contact is imminent. We construct a model, given camera\nimages as input, that is capable of learning and predicting the dynamics of\nobstacles, cars and pedestrians, and train our policy using this model. Two\npolicies that control both braking and steering are compared against a baseline\nwhere the only action taken is (conventional) braking in a straight line. The\ntwo policies are trained using two distinct reward structures, one where any\nand all collisions incur a fixed penalty, and a second one where the penalty is\ncalculated based on already established delta-v models of injury severity. The\nresults show that both policies exceed the performance of the baseline, with\nthe policy trained using injury models having the highest performance. \n\n"}
{"id": "1901.01706", "contents": "Title: Universal Deep Beamformer for Variable Rate Ultrasound Imaging Abstract: Ultrasound (US) imaging is based on the time-reversal principle, in which\nindividual channel RF measurements are back-propagated and accumulated to form\nan image after applying specific delays. While this time reversal is usually\nimplemented as a delay-and-sum (DAS) beamformer, the image quality quickly\ndegrades as the number of measurement channels decreases. To address this\nproblem, various types of adaptive beamforming techniques have been proposed\nusing predefined models of the signals. However, the performance of these\nadaptive beamforming approaches degrade when the underlying model is not\nsufficiently accurate. Here, we demonstrate for the first time that a single\nuniversal deep beamformer trained using a purely data-driven way can generate\nsignificantly improved images over widely varying aperture and channel\nsubsampling patterns. In particular, we design an end-to-end deep learning\nframework that can directly process sub-sampled RF data acquired at different\nsubsampling rate and detector configuration to generate high quality ultrasound\nimages using a single beamformer. Experimental results using B-mode focused\nultrasound confirm the efficacy of the proposed methods. \n\n"}
{"id": "1901.01868", "contents": "Title: Low-Shot Learning from Imaginary 3D Model Abstract: Since the advent of deep learning, neural networks have demonstrated\nremarkable results in many visual recognition tasks, constantly pushing the\nlimits. However, the state-of-the-art approaches are largely unsuitable in\nscarce data regimes. To address this shortcoming, this paper proposes employing\na 3D model, which is derived from training images. Such a model can then be\nused to hallucinate novel viewpoints and poses for the scarce samples of the\nfew-shot learning scenario. A self-paced learning approach allows for the\nselection of a diverse set of high-quality images, which facilitates the\ntraining of a classifier. The performance of the proposed approach is showcased\non the fine-grained CUB-200-2011 dataset in a few-shot setting and\nsignificantly improves our baseline accuracy. \n\n"}
{"id": "1901.01892", "contents": "Title: Scale-Aware Trident Networks for Object Detection Abstract: Scale variation is one of the key challenges in object detection. In this\nwork, we first present a controlled experiment to investigate the effect of\nreceptive fields for scale variation in object detection. Based on the findings\nfrom the exploration experiments, we propose a novel Trident Network\n(TridentNet) aiming to generate scale-specific feature maps with a uniform\nrepresentational power. We construct a parallel multi-branch architecture in\nwhich each branch shares the same transformation parameters but with different\nreceptive fields. Then, we adopt a scale-aware training scheme to specialize\neach branch by sampling object instances of proper scales for training. As a\nbonus, a fast approximation version of TridentNet could achieve significant\nimprovements without any additional parameters and computational cost compared\nwith the vanilla detector. On the COCO dataset, our TridentNet with ResNet-101\nbackbone achieves state-of-the-art single-model results of 48.4 mAP. Codes are\navailable at https://git.io/fj5vR. \n\n"}
{"id": "1901.01928", "contents": "Title: DSConv: Efficient Convolution Operator Abstract: Quantization is a popular way of increasing the speed and lowering the memory\nusage of Convolution Neural Networks (CNNs). When labelled training data is\navailable, network weights and activations have successfully been quantized\ndown to 1-bit. The same cannot be said about the scenario when labelled\ntraining data is not available, e.g. when quantizing a pre-trained model, where\ncurrent approaches show, at best, no loss of accuracy at 8-bit quantizations.\nWe introduce DSConv, a flexible quantized convolution operator that replaces\nsingle-precision operations with their far less expensive integer counterparts,\nwhile maintaining the probability distributions over both the kernel weights\nand the outputs. We test our model as a plug-and-play replacement for standard\nconvolution on most popular neural network architectures, ResNet, DenseNet,\nGoogLeNet, AlexNet and VGG-Net and demonstrate state-of-the-art results, with\nless than 1% loss of accuracy, without retraining, using only 4-bit\nquantization. We also show how a distillation-based adaptation stage with\nunlabelled data can improve results even further. \n\n"}
{"id": "1901.02199", "contents": "Title: FIGR: Few-shot Image Generation with Reptile Abstract: Generative Adversarial Networks (GAN) boast impressive capacity to generate\nrealistic images. However, like much of the field of deep learning, they\nrequire an inordinate amount of data to produce results, thereby limiting their\nusefulness in generating novelty. In the same vein, recent advances in\nmeta-learning have opened the door to many few-shot learning applications. In\nthe present work, we propose Few-shot Image Generation using Reptile (FIGR), a\nGAN meta-trained with Reptile. Our model successfully generates novel images on\nboth MNIST and Omniglot with as little as 4 images from an unseen class. We\nfurther contribute FIGR-8, a new dataset for few-shot image generation, which\ncontains 1,548,944 icons categorized in over 18,409 classes. Trained on FIGR-8,\ninitial results show that our model can generalize to more advanced concepts\n(such as \"bird\" and \"knife\") from as few as 8 samples from a previously unseen\nclass of images and as little as 10 training steps through those 8 images. This\nwork demonstrates the potential of training a GAN for few-shot image generation\nand aims to set a new benchmark for future work in the domain. \n\n"}
{"id": "1901.02404", "contents": "Title: GILT: Generating Images from Long Text Abstract: Creating an image reflecting the content of a long text is a complex process\nthat requires a sense of creativity. For example, creating a book cover or a\nmovie poster based on their summary or a food image based on its recipe. In\nthis paper we present the new task of generating images from long text that\ndoes not describe the visual content of the image directly. For this, we build\na system for generating high-resolution 256 $\\times$ 256 images of food\nconditioned on their recipes. The relation between the recipe text (without its\ntitle) to the visual content of the image is vague, and the textual structure\nof recipes is complex, consisting of two sections (ingredients and\ninstructions) both containing multiple sentences.\n  We used the recipe1M dataset to train and evaluate our model that is based on\na the StackGAN-v2 architecture. \n\n"}
{"id": "1901.02453", "contents": "Title: Neural Inverse Rendering of an Indoor Scene from a Single Image Abstract: Inverse rendering aims to estimate physical attributes of a scene, e.g.,\nreflectance, geometry, and lighting, from image(s). Inverse rendering has been\nstudied primarily for single objects or with methods that solve for only one of\nthe scene attributes. We propose the first learning-based approach that jointly\nestimates albedo, normals, and lighting of an indoor scene from a single image.\nOur key contribution is the Residual Appearance Renderer (RAR), which can be\ntrained to synthesize complex appearance effects (e.g., inter-reflection, cast\nshadows, near-field illumination, and realistic shading), which would be\nneglected otherwise. This enables us to perform self-supervised learning on\nreal data using a reconstruction loss, based on re-synthesizing the input image\nfrom the estimated components. We finetune with real data after pretraining\nwith synthetic data. To this end, we use physically-based rendering to create a\nlarge-scale synthetic dataset, which is a significant improvement over prior\ndatasets. Experimental results show that our approach outperforms\nstate-of-the-art methods that estimate one or more scene attributes. \n\n"}
{"id": "1901.03088", "contents": "Title: Fast GPU-Enabled Color Normalization for Digital Pathology Abstract: Normalizing unwanted color variations due to differences in staining\nprocesses and scanner responses has been shown to aid machine learning in\ncomputational pathology. Of the several popular techniques for color\nnormalization, structure preserving color normalization (SPCN) is\nwell-motivated, convincingly tested, and published with its code base. However,\nSPCN makes occasional errors in color basis estimation leading to artifacts\nsuch as swapping the color basis vectors between stains or giving a colored\ntinge to the background with no tissue. We made several algorithmic\nimprovements to remove these artifacts. Additionally, the original SPCN code is\nnot readily usable on gigapixel whole slide images (WSIs) due to long run\ntimes, use of proprietary software platform and libraries, and its inability to\nautomatically handle WSIs. We completely rewrote the software such that it can\nautomatically handle images of any size in popular WSI formats. Our software\nutilizes GPU-acceleration and open-source libraries that are becoming\nubiquitous with the advent of deep learning. We also made several other small\nimprovements and achieved a multifold overall speedup on gigapixel images. Our\nalgorithm and software is usable right out-of-the-box by the computational\npathology community. \n\n"}
{"id": "1901.03419", "contents": "Title: How Can We Make GAN Perform Better in Single Medical Image\n  Super-Resolution? A Lesion Focused Multi-Scale Approach Abstract: Single image super-resolution (SISR) is of great importance as a low-level\ncomputer vision task. The fast development of Generative Adversarial Network\n(GAN) based deep learning architectures realises an efficient and effective\nSISR to boost the spatial resolution of natural images captured by digital\ncameras. However, the SISR for medical images is still a very challenging\nproblem. This is due to (1) compared to natural images, in general, medical\nimages have lower signal to noise ratios, (2) GAN based models pre-trained on\nnatural images may synthesise unrealistic patterns in medical images which\ncould affect the clinical interpretation and diagnosis, and (3) the vanilla GAN\narchitecture may suffer from unstable training and collapse mode that can also\naffect the SISR results. In this paper, we propose a novel lesion focused SR\n(LFSR) method, which incorporates GAN to achieve perceptually realistic SISR\nresults for brain tumour MRI images. More importantly, we test and make\ncomparison using recently developed GAN variations, e.g., Wasserstein GAN\n(WGAN) and WGAN with Gradient Penalty (WGAN-GP), and propose a novel\nmulti-scale GAN (MS-GAN), to achieve a more stabilised and efficient training\nand improved perceptual quality of the super-resolved results. Based on both\nquantitative evaluations and our designed mean opinion score, the proposed LFSR\ncoupled with MS-GAN has performed better in terms of both perceptual quality\nand efficiency. \n\n"}
{"id": "1901.03495", "contents": "Title: FishNet: A Versatile Backbone for Image, Region, and Pixel Level\n  Prediction Abstract: The basic principles in designing convolutional neural network (CNN)\nstructures for predicting objects on different levels, e.g., image-level,\nregion-level, and pixel-level are diverging. Generally, network structures\ndesigned specifically for image classification are directly used as default\nbackbone structure for other tasks including detection and segmentation, but\nthere is seldom backbone structure designed under the consideration of unifying\nthe advantages of networks designed for pixel-level or region-level predicting\ntasks, which may require very deep features with high resolution. Towards this\ngoal, we design a fish-like network, called FishNet. In FishNet, the\ninformation of all resolutions is preserved and refined for the final task.\nBesides, we observe that existing works still cannot \\emph{directly} propagate\nthe gradient information from deep layers to shallow layers. Our design can\nbetter handle this problem. Extensive experiments have been conducted to\ndemonstrate the remarkable performance of the FishNet. In particular, on\nImageNet-1k, the accuracy of FishNet is able to surpass the performance of\nDenseNet and ResNet with fewer parameters. FishNet was applied as one of the\nmodules in the winning entry of the COCO Detection 2018 challenge. The code is\navailable at https://github.com/kevin-ssy/FishNet. \n\n"}
{"id": "1901.03707", "contents": "Title: Neumann Networks for Inverse Problems in Imaging Abstract: Many challenging image processing tasks can be described by an ill-posed\nlinear inverse problem: deblurring, deconvolution, inpainting, compressed\nsensing, and superresolution all lie in this framework. Traditional inverse\nproblem solvers minimize a cost function consisting of a data-fit term, which\nmeasures how well an image matches the observations, and a regularizer, which\nreflects prior knowledge and promotes images with desirable properties like\nsmoothness. Recent advances in machine learning and image processing have\nillustrated that it is often possible to learn a regularizer from training data\nthat can outperform more traditional regularizers. We present an end-to-end,\ndata-driven method of solving inverse problems inspired by the Neumann series,\nwhich we call a Neumann network. Rather than unroll an iterative optimization\nalgorithm, we truncate a Neumann series which directly solves the linear\ninverse problem with a data-driven nonlinear regularizer. The Neumann network\narchitecture outperforms traditional inverse problem solution methods,\nmodel-free deep learning approaches, and state-of-the-art unrolled iterative\nmethods on standard datasets. Finally, when the images belong to a union of\nsubspaces and under appropriate assumptions on the forward model, we prove\nthere exists a Neumann network configuration that well-approximates the optimal\noracle estimator for the inverse problem and demonstrate empirically that the\ntrained Neumann network has the form predicted by theory. \n\n"}
{"id": "1901.03781", "contents": "Title: DeepSpline: Data-Driven Reconstruction of Parametric Curves and Surfaces Abstract: Reconstruction of geometry based on different input modes, such as images or\npoint clouds, has been instrumental in the development of computer aided design\nand computer graphics. Optimal implementations of these applications have\ntraditionally involved the use of spline-based representations at their core.\nMost such methods attempt to solve optimization problems that minimize an\noutput-target mismatch. However, these optimization techniques require an\ninitialization that is close enough, as they are local methods by nature. We\npropose a deep learning architecture that adapts to perform spline fitting\ntasks accordingly, providing complementary results to the aforementioned\ntraditional methods. We showcase the performance of our approach, by\nreconstructing spline curves and surfaces based on input images or point\nclouds. \n\n"}
{"id": "1901.04405", "contents": "Title: Quadratization in discrete optimization and quantum mechanics Abstract: A book about turning high-degree optimization problems into quadratic\noptimization problems that maintain the same global minimum (ground state).\nThis book explores quadratizations for pseudo-Boolean optimization,\nperturbative gadgets used in QMA completeness theorems, and also\nnon-perturbative k-local to 2-local transformations used for quantum mechanics,\nquantum annealing and universal adiabatic quantum computing. The book contains\n~70 different Hamiltonian transformations, each of them on a separate page,\nwhere the cost (in number of auxiliary binary variables or auxiliary qubits, or\nnumber of sub-modular terms, or in graph connectivity, etc.), pros, cons,\nexamples, and references are given. One can therefore look up a quadratization\nappropriate for the specific term(s) that need to be quadratized, much like\nusing an integral table to look up the integral that needs to be done. This\nbook is therefore useful for writing compilers to transform general\noptimization problems, into a form that quantum annealing or universal\nadiabatic quantum computing hardware requires; or for transforming quantum\nchemistry problems written in the Jordan-Wigner or Bravyi-Kitaev form, into a\nform where all multi-qubit interactions become 2-qubit pairwise interactions,\nwithout changing the desired ground state. Applications cited include computer\nvision problems (e.g. image de-noising, un-blurring, etc.), number theory (e.g.\ninteger factoring), graph theory (e.g. Ramsey number determination), and\nquantum chemistry. The book is open source, and anyone can make modifications\nhere: https://github.com/HPQC-LABS/Book_About_Quadratization. \n\n"}
{"id": "1901.04947", "contents": "Title: Automatic Surface Area and Volume Prediction on Ellipsoidal Ham using\n  Deep Learning Abstract: This paper presents novel methods to predict the surface and volume of the\nham through a camera. This implies that the conventional weight measurement to\nobtain in the object's volume can be neglected and hence it is economically\neffective. Both of the measurements are obtained in the following two ways:\nmanually and automatically. The former is assume as the true or exact\nmeasurement and the latter is through a computer vision technique with some\ngeometrical analysis that includes mathematical derived functions. For the\nautomatic implementation, most of the existing approaches extract the features\nof the food material based on handcrafted features and to the best of our\nknowledge this is the first attempt to estimate the surface area and volume on\nham with deep learning features. We address the estimation task with a Mask\nRegion-based CNN (Mask R-CNN) approach, which well performs the ham detection\nand semantic segmentation from a video. The experimental results demonstrate\nthat the algorithm proposed is robust as promising surface area and volume\nestimation are obtained for two angles of the ellipsoidal ham (i.e., horizontal\nand vertical positions). Specifically, in the vertical ham point of view, it\nachieves an overall accuracy up to 95% whereas the horizontal ham reaches 80%\nof accuracy. \n\n"}
{"id": "1901.05031", "contents": "Title: Analysis and algorithms for $\\ell_p$-based semi-supervised learning on\n  graphs Abstract: This paper addresses theory and applications of $\\ell_p$-based Laplacian\nregularization in semi-supervised learning. The graph $p$-Laplacian for $p>2$\nhas been proposed recently as a replacement for the standard ($p=2$) graph\nLaplacian in semi-supervised learning problems with very few labels, where\nLaplacian learning is degenerate.\n  In the first part of the paper we prove new discrete to continuum convergence\nresults for $p$-Laplace problems on $k$-nearest neighbor ($k$-NN) graphs, which\nare more commonly used in practice than random geometric graphs. Our analysis\nshows that, on $k$-NN graphs, the $p$-Laplacian retains information about the\ndata distribution as $p\\to \\infty$ and Lipschitz learning ($p=\\infty$) is\nsensitive to the data distribution. This situation can be contrasted with\nrandom geometric graphs, where the $p$-Laplacian forgets the data distribution\nas $p\\to \\infty$. We also present a general framework for proving discrete to\ncontinuum convergence results in graph-based learning that only requires\npointwise consistency and monotonicity.\n  In the second part of the paper, we develop fast algorithms for solving the\nvariational and game-theoretic $p$-Laplace equations on weighted graphs for\n$p>2$. We present several efficient and scalable algorithms for both\nformulations, and present numerical results on synthetic data indicating their\nconvergence properties. Finally, we conduct extensive numerical experiments on\nthe MNIST, FashionMNIST and EMNIST datasets that illustrate the effectiveness\nof the $p$-Laplacian formulation for semi-supervised learning with few labels.\nIn particular, we find that Lipschitz learning ($p=\\infty$) performs well with\nvery few labels on $k$-NN graphs, which experimentally validates our\ntheoretical findings that Lipschitz learning retains information about the data\ndistribution (the unlabeled data) on $k$-NN graphs. \n\n"}
{"id": "1901.05375", "contents": "Title: DAFE-FD: Density Aware Feature Enrichment for Face Detection Abstract: Recent research on face detection, which is focused primarily on improving\naccuracy of detecting smaller faces, attempt to develop new anchor design\nstrategies to facilitate increased overlap between anchor boxes and ground\ntruth faces of smaller sizes. In this work, we approach the problem of small\nface detection with the motivation of enriching the feature maps using a\ndensity map estimation module. This module, inspired by recent crowd\ncounting/density estimation techniques, performs the task of estimating the per\npixel density of people/faces present in the image. Output of this module is\nemployed to accentuate the feature maps from the backbone network using a\nfeature enrichment module before being used for detecting smaller faces. The\nproposed approach can be used to complement recent anchor-design based novel\nmethods to further improve their results. Experiments conducted on different\ndatasets such as WIDER, FDDB and Pascal-Faces demonstrate the effectiveness of\nthe proposed approach. \n\n"}
{"id": "1901.05946", "contents": "Title: Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for\n  Semantic Nighttime Image Segmentation Abstract: Most progress in semantic segmentation reports on daytime images taken under\nfavorable illumination conditions. We instead address the problem of semantic\nsegmentation of nighttime images and improve the state-of-the-art, by adapting\ndaytime models to nighttime without using nighttime annotations. Moreover, we\ndesign a new evaluation framework to address the substantial uncertainty of\nsemantics in nighttime images. Our central contributions are: 1) a curriculum\nframework to gradually adapt semantic segmentation models from day to night via\nlabeled synthetic images and unlabeled real images, both for progressively\ndarker times of day, which exploits cross-time-of-day correspondences for the\nreal images to guide the inference of their labels; 2) a novel\nuncertainty-aware annotation and evaluation framework and metric for semantic\nsegmentation, designed for adverse conditions and including image regions\nbeyond human recognition capability in the evaluation in a principled fashion;\n3) the Dark Zurich dataset, which comprises 2416 unlabeled nighttime and 2920\nunlabeled twilight images with correspondences to their daytime counterparts\nplus a set of 151 nighttime images with fine pixel-level annotations created\nwith our protocol, which serves as a first benchmark to perform our novel\nevaluation. Experiments show that our guided curriculum adaptation\nsignificantly outperforms state-of-the-art methods on real nighttime sets both\nfor standard metrics and our uncertainty-aware metric. Furthermore, our\nuncertainty-aware evaluation reveals that selective invalidation of predictions\ncan lead to better results on data with ambiguous content such as our nighttime\nbenchmark and profit safety-oriented applications which involve invalid inputs. \n\n"}
{"id": "1901.06656", "contents": "Title: Training Neural Networks with Local Error Signals Abstract: Supervised training of neural networks for classification is typically\nperformed with a global loss function. The loss function provides a gradient\nfor the output layer, and this gradient is back-propagated to hidden layers to\ndictate an update direction for the weights. An alternative approach is to\ntrain the network with layer-wise loss functions. In this paper we demonstrate,\nfor the first time, that layer-wise training can approach the state-of-the-art\non a variety of image datasets. We use single-layer sub-networks and two\ndifferent supervised loss functions to generate local error signals for the\nhidden layers, and we show that the combination of these losses help with\noptimization in the context of local learning. Using local errors could be a\nstep towards more biologically plausible deep learning because the global error\ndoes not have to be transported back to hidden layers. A completely backprop\nfree variant outperforms previously reported results among methods aiming for\nhigher biological plausibility. Code is available\nhttps://github.com/anokland/local-loss \n\n"}
{"id": "1901.07295", "contents": "Title: Adversarial Pseudo Healthy Synthesis Needs Pathology Factorization Abstract: Pseudo healthy synthesis, i.e. the creation of a subject-specific `healthy'\nimage from a pathological one, could be helpful in tasks such as anomaly\ndetection, understanding changes induced by pathology and disease or even as\ndata augmentation. We treat this task as a factor decomposition problem: we aim\nto separate what appears to be healthy and where disease is (as a map). The two\nfactors are then recombined (by a network) to reconstruct the input disease\nimage. We train our models in an adversarial way using either paired or\nunpaired settings, where we pair disease images and maps (as segmentation\nmasks) when available. We quantitatively evaluate the quality of pseudo healthy\nimages. We show in a series of experiments, performed in ISLES and BraTS\ndatasets, that our method is better than conditional GAN and CycleGAN,\nhighlighting challenges in using adversarial methods in the image translation\ntask of pseudo healthy image generation. \n\n"}
{"id": "1901.07528", "contents": "Title: Learning Continuous Face Age Progression: A Pyramid of GANs Abstract: The two underlying requirements of face age progression, i.e. aging accuracy\nand identity permanence, are not well studied in the literature. This paper\npresents a novel generative adversarial network based approach to address the\nissues in a coupled manner. It separately models the constraints for the\nintrinsic subject-specific characteristics and the age-specific facial changes\nwith respect to the elapsed time, ensuring that the generated faces present\ndesired aging effects while simultaneously keeping personalized properties\nstable. To ensure photo-realistic facial details, high-level age-specific\nfeatures conveyed by the synthesized face are estimated by a pyramidal\nadversarial discriminator at multiple scales, which simulates the aging effects\nwith finer details. Further, an adversarial learning scheme is introduced to\nsimultaneously train a single generator and multiple parallel discriminators,\nresulting in smooth continuous face aging sequences. The proposed method is\napplicable even in the presence of variations in pose, expression, makeup,\netc., achieving remarkably vivid aging effects. Quantitative evaluations by a\nCOTS face recognition system demonstrate that the target age distributions are\naccurately recovered, and 99.88% and 99.98% age progressed faces can be\ncorrectly verified at 0.001% FAR after age transformations of approximately 28\nand 23 years elapsed time on the MORPH and CACD databases, respectively. Both\nvisual and quantitative assessments show that the approach advances the\nstate-of-the-art. \n\n"}
{"id": "1901.10271", "contents": "Title: Combined tract segmentation and orientation mapping for bundle-specific\n  tractography Abstract: While the major white matter tracts are of great interest to numerous studies\nin neuroscience and medicine, their manual dissection in larger cohorts from\ndiffusion MRI tractograms is time-consuming, requires expert knowledge and is\nhard to reproduce. In previous work we presented tract orientation mapping\n(TOM) as a novel concept for bundle-specific tractography. It is based on a\nlearned mapping from the original fiber orientation distribution function (FOD)\npeaks to tract specific peaks, called tract orientation maps. Each tract\norientation map represents the voxel-wise principal orientation of one tract.\nHere, we present an extension of this approach that combines TOM with accurate\nsegmentations of the tract outline and its start and end region. We also\nintroduce a custom probabilistic tracking algorithm that samples from a\nGaussian distribution with fixed standard deviation centered on each peak thus\nenabling more complete trackings on the tract orientation maps than\ndeterministic tracking. These extensions enable the automatic creation of\nbundle-specific tractograms with previously unseen accuracy. We show for 72\ndifferent bundles on high quality, low quality and phantom data that our\napproach runs faster and produces more accurate bundle-specific tractograms\nthan 7 state of the art benchmark methods while avoiding cumbersome processing\nsteps like whole brain tractography, non-linear registration, clustering or\nmanual dissection. Moreover, we show on 17 datasets that our approach\ngeneralizes well to datasets acquired with different scanners and settings as\nwell as with pathologies. The code of our method is openly available at\nhttps://github.com/MIC-DKFZ/TractSeg. \n\n"}
{"id": "1901.10415", "contents": "Title: MgNet: A Unified Framework of Multigrid and Convolutional Neural Network Abstract: We develop a unified model, known as MgNet, that simultaneously recovers some\nconvolutional neural networks (CNN) for image classification and multigrid (MG)\nmethods for solving discretized partial differential equations (PDEs). This\nmodel is based on close connections that we have observed and uncovered between\nthe CNN and MG methodologies. For example, pooling operation and feature\nextraction in CNN correspond directly to restriction operation and iterative\nsmoothers in MG, respectively. As the solution space is often the dual of the\ndata space in PDEs, the analogous concept of feature space and data space\n(which are dual to each other) is introduced in CNN. With such connections and\nnew concept in the unified model, the function of various convolution\noperations and pooling used in CNN can be better understood. As a result,\nmodified CNN models (with fewer weights and hyper parameters) are developed\nthat exhibit competitive and sometimes better performance in comparison with\nexisting CNN models when applied to both CIFAR-10 and CIFAR-100 data sets. \n\n"}
{"id": "1901.10436", "contents": "Title: Diversity in Faces Abstract: Face recognition is a long standing challenge in the field of Artificial\nIntelligence (AI). The goal is to create systems that accurately detect,\nrecognize, verify, and understand human faces. There are significant technical\nhurdles in making these systems accurate, particularly in unconstrained\nsettings due to confounding factors related to pose, resolution, illumination,\nocclusion, and viewpoint. However, with recent advances in neural networks,\nface recognition has achieved unprecedented accuracy, largely built on\ndata-driven deep learning methods. While this is encouraging, a critical aspect\nthat is limiting facial recognition accuracy and fairness is inherent facial\ndiversity. Every face is different. Every face reflects something unique about\nus. Aspects of our heritage - including race, ethnicity, culture, geography -\nand our individual identify - age, gender, and other visible manifestations of\nself-expression, are reflected in our faces. We expect face recognition to work\nequally accurately for every face. Face recognition needs to be fair. As we\nrely on data-driven methods to create face recognition technology, we need to\nensure necessary balance and coverage in training data. However, there are\nstill scientific questions about how to represent and extract pertinent facial\nfeatures and quantitatively measure facial diversity. Towards this goal,\nDiversity in Faces (DiF) provides a data set of one million annotated human\nface images for advancing the study of facial diversity. The annotations are\ngenerated using ten well-established facial coding schemes from the scientific\nliterature. The facial coding schemes provide human-interpretable quantitative\nmeasures of facial features. We believe that by making the extracted coding\nschemes available on a large set of faces, we can accelerate research and\ndevelopment towards creating more fair and accurate facial recognition systems. \n\n"}
{"id": "1901.10503", "contents": "Title: Time-Space tradeoff in deep learning models for crop classification on\n  satellite multi-spectral image time series Abstract: In this article, we investigate several structured deep learning models for\ncrop type classification on multi-spectral time series. In particular, our aim\nis to assess the respective importance of spatial and temporal structures in\nsuch data. With this objective, we consider several designs of convolutional,\nrecurrent, and hybrid neural networks, and assess their performance on a large\ndataset of freely available Sentinel-2 imagery. We find that the\nbest-performing approaches are hybrid configurations for which most of the\nparameters (up to 90%) are allocated to modeling the temporal structure of the\ndata. Our results thus constitute a set of guidelines for the design of bespoke\ndeep learning models for crop type classification. \n\n"}
{"id": "1901.10513", "contents": "Title: Adversarial Examples Are a Natural Consequence of Test Error in Noise Abstract: Over the last few years, the phenomenon of adversarial examples ---\nmaliciously constructed inputs that fool trained machine learning models ---\nhas captured the attention of the research community, especially when the\nadversary is restricted to small modifications of a correctly handled input.\nLess surprisingly, image classifiers also lack human-level performance on\nrandomly corrupted images, such as images with additive Gaussian noise. In this\npaper we provide both empirical and theoretical evidence that these are two\nmanifestations of the same underlying phenomenon, establishing close\nconnections between the adversarial robustness and corruption robustness\nresearch programs. This suggests that improving adversarial robustness should\ngo hand in hand with improving performance in the presence of more general and\nrealistic image corruptions. Based on our results we recommend that future\nadversarial defenses consider evaluating the robustness of their methods to\ndistributional shift with benchmarks such as Imagenet-C. \n\n"}
{"id": "1901.10747", "contents": "Title: Autonomous Cars: Vision based Steering Wheel Angle Estimation Abstract: Machine learning models, which are frequently used in self-driving cars, are\ntrained by matching the captured images of the road and the measured angle of\nthe steering wheel. The angle of the steering wheel is generally fetched from\nsteering angle sensor, which is tightly-coupled to the physical aspects of the\nvehicle at hand. Therefore, a model-agnostic autonomous car-kit is very\ndifficult to be developed and autonomous vehicles need more training data. The\nproposed vision based steering angle estimation system argues a new approach\nwhich basically matches the images of the road captured by an outdoor camera\nand the images of the steering wheel from an onboard camera, avoiding the\nburden of collecting model-dependent training data and the use of any other\nelectromechanical hardware. \n\n"}
{"id": "cs/0006047", "contents": "Title: Geometric Morphology of Granular Materials Abstract: We present a new method to transform the spectral pixel information of a\nmicrograph into an affine geometric description, which allows us to analyze the\nmorphology of granular materials. We use spectral and pulse-coupled neural\nnetwork based segmentation techniques to generate blobs, and a newly developed\nalgorithm to extract dilated contours. A constrained Delaunay tesselation of\nthe contour points results in a triangular mesh. This mesh is the basic\ningredient of the Chodal Axis Transform, which provides a morphological\ndecomposition of shapes. Such decomposition allows for grain separation and the\nefficient computation of the statistical features of granular materials. \n\n"}
{"id": "cs/0606048", "contents": "Title: A New Quartet Tree Heuristic for Hierarchical Clustering Abstract: We consider the problem of constructing an an optimal-weight tree from the\n3*(n choose 4) weighted quartet topologies on n objects, where optimality means\nthat the summed weight of the embedded quartet topologiesis optimal (so it can\nbe the case that the optimal tree embeds all quartets as non-optimal\ntopologies). We present a heuristic for reconstructing the optimal-weight tree,\nand a canonical manner to derive the quartet-topology weights from a given\ndistance matrix. The method repeatedly transforms a bifurcating tree, with all\nobjects involved as leaves, achieving a monotonic approximation to the exact\nsingle globally optimal tree. This contrasts to other heuristic search methods\nfrom biological phylogeny, like DNAML or quartet puzzling, which, repeatedly,\nincrementally construct a solution from a random order of objects, and\nsubsequently add agreement values. \n\n"}
