{"id": "0704.1678", "contents": "Title: Settling the Complexity of Computing Two-Player Nash Equilibria Abstract: We settle a long-standing open question in algorithmic game theory. We prove\nthat Bimatrix, the problem of finding a Nash equilibrium in a two-player game,\nis complete for the complexity class PPAD Polynomial Parity Argument, Directed\nversion) introduced by Papadimitriou in 1991.\n  This is the first of a series of results concerning the complexity of Nash\nequilibria. In particular, we prove the following theorems:\n  Bimatrix does not have a fully polynomial-time approximation scheme unless\nevery problem in PPAD is solvable in polynomial time. The smoothed complexity\nof the classic Lemke-Howson algorithm and, in fact, of any algorithm for\nBimatrix is not polynomial unless every problem in PPAD is solvable in\nrandomized polynomial time. Our results demonstrate that, even in the simplest\nform of non-cooperative games, equilibrium computation and approximation are\npolynomial-time equivalent to fixed point computation. Our results also have\ntwo broad complexity implications in mathematical economics and operations\nresearch: Arrow-Debreu market equilibria are PPAD-hard to compute. The P-Matrix\nLinear Complementary Problem is computationally harder than convex programming\nunless every problem in PPAD is solvable in polynomial time. \n\n"}
{"id": "0704.2014", "contents": "Title: Extensive Games with Possibly Unaware Players Abstract: Standard game theory assumes that the structure of the game is common\nknowledge among players. We relax this assumption by considering extensive\ngames where agents may be unaware of the complete structure of the game. In\nparticular, they may not be aware of moves that they and other agents can make.\nWe show how such games can be represented; the key idea is to describe the game\nfrom the point of view of every agent at every node of the game tree. We\nprovide a generalization of Nash equilibrium and show that every game with\nawareness has a generalized Nash equilibrium. Finally, we extend these results\nto games with awareness of unawareness, where a player i may be aware that a\nplayer j can make moves that i is not aware of, and to subjective games, where\npayers may have no common knowledge regarding the actual game and their beliefs\nare incompatible with a common prior. \n\n"}
{"id": "0704.2017", "contents": "Title: Large System Analysis of Game-Theoretic Power Control in UWB Wireless\n  Networks with Rake Receivers Abstract: This paper studies the performance of partial-Rake (PRake) receivers in\nimpulse-radio ultrawideband wireless networks when an energy-efficient power\ncontrol scheme is adopted. Due to the large bandwidth of the system, the\nmultipath channel is assumed to be frequency-selective. By using noncooperative\ngame-theoretic models and large system analysis, explicit expressions are\nderived in terms of network parameters to measure the effects of self- and\nmultiple-access interference at a receiving access point. Performance of the\nPRake is compared in terms of achieved utilities and loss to that of the\nall-Rake receiver. \n\n"}
{"id": "0705.0936", "contents": "Title: Performance Comparison of Energy-Efficient Power Control for CDMA and\n  Multiuser UWB Networks Abstract: This paper studies the performance of a wireless data network using\nenergy-efficient power control techniques when different multiple access\nschemes, namely direct-sequence code division multiple access (DS-CDMA) and\nimpulse-radio ultrawideband (IR-UWB), are considered. Due to the large\nbandwidth of the system, the multipath channel is assumed to be\nfrequency-selective. By making use of noncooperative game-theoretic models and\nlarge-system analysis tools, explicit expressions for the achieved utilities at\nthe Nash equilibrium are derived in terms of the network parameters. A measure\nof the loss of DS-CDMA with respect to IR-UWB is proposed, which proves\nsubstantial equivalence between the two schemes. Simulation results are\nprovided to validate the analysis. \n\n"}
{"id": "0706.1001", "contents": "Title: Epistemic Analysis of Strategic Games with Arbitrary Strategy Sets Abstract: We provide here an epistemic analysis of arbitrary strategic games based on\nthe possibility correspondences. Such an analysis calls for the use of\ntransfinite iterations of the corresponding operators. Our approach is based on\nTarski's Fixpoint Theorem and applies both to the notions of rationalizability\nand the iterated elimination of strictly dominated strategies. \n\n"}
{"id": "0707.1904", "contents": "Title: Generalized Solution Concepts in Games with Possibly Unaware Players Abstract: Most work in game theory assumes that players are perfect reasoners and have\ncommon knowledge of all significant aspects of the game. In earlier work, we\nproposed a framework for representing and analyzing games with possibly unaware\nplayers, and suggested a generalization of Nash equilibrium appropriate for\ngames with unaware players that we called generalized Nash equilibrium. Here,\nwe use this framework to analyze other solution concepts that have been\nconsidered in the game-theory literature, with a focus on sequential\nequilibrium. We also provide some insight into the notion of generalized Nash\nequilibrium by proving that it is closely related to the notion of\nrationalizability when we restrict the analysis to games in normal form and no\nunawareness is involved. \n\n"}
{"id": "0709.4273", "contents": "Title: Set Matrices and The Path/Cycle Problem Abstract: Presentation of set matrices and demonstration of their efficiency as a tool\nusing the path/cycle problem. \n\n"}
{"id": "0710.3536", "contents": "Title: Common Beliefs and Public Announcements in Strategic Games with\n  Arbitrary Strategy Sets Abstract: We provide an epistemic analysis of arbitrary strategic games based on\npossibility correspondences. We first establish a generic result that links\ntrue common beliefs (and, respectively, common knowledge) of players'\nrationality defined by means of `monotonic' properties, with the iterated\nelimination of strategies that do not satisfy these properties. It allows us to\ndeduce the customary results concerned with true common beliefs of rationality\nand iterated elimination of strictly dominated strategies as simple\ncorollaries. This approach relies on Tarski's Fixpoint Theorem. We also provide\nan axiomatic presentation of this generic result. This allows us to clarify the\nproof-theoretic principles assumed in players' reasoning. Finally, we provide\nan alternative characterization of the iterated elimination of strategies based\non the concept of a public announcement. It applies to `global properties'.\nBoth classes of properties include the notions of rationalizability and the\niterated elimination of strictly dominated strategies. \n\n"}
{"id": "0711.1569", "contents": "Title: Capacity as a Fundamental Metric for Mechanism Design in the Information\n  Economy Abstract: The auction theory literature has so far focused mostly on the design of\nmechanisms that takes the revenue or the efficiency as a yardstick. However,\nscenarios where the {\\it capacity}, which we define as \\textit{``the number of\nbidders the auctioneer wants to have a positive probability of getting the\nitem''}, is a fundamental concern are ubiquitous in the information economy.\nFor instance, in sponsored search auctions (SSA's) or in online ad-exchanges,\nthe true value of an ad-slot for an advertiser is inherently derived from the\nconversion-rate, which in turn depends on whether the advertiser actually\nobtained the ad-slot or not; thus, unless the capacity of the underlying\nauction is large, key parameters, such as true valuations and\nadvertiser-specific conversion rates, will remain unknown or uncertain leading\nto inherent inefficiencies in the system. In general, the same holds true for\nall information goods/digital goods. We initiate a study of mechanisms, which\ntake capacity as a yardstick, in addition to revenue/efficiency. We show that\nin the case of a single indivisible item one simple way to incorporate capacity\nconstraints is via designing mechanisms to sell probability distributions, and\nthat under certain conditions, such optimal probability distributions could be\nidentified using a Linear programming approach. We define a quantity called\n{\\it price of capacity} to capture the tradeoff between capacity and\nrevenue/efficiency. We also study the case of sponsored search auctions.\nFinally, we discuss how general such an approach via probability spikes can be\nmade, and potential directions for future investigations. \n\n"}
{"id": "0711.2618", "contents": "Title: A System for Distributed Mechanisms: Design, Implementation and\n  Applications Abstract: We describe here a structured system for distributed mechanism design\nappropriate for both Intranet and Internet applications. In our approach the\nplayers dynamically form a network in which they know neither their neighbours\nnor the size of the network and interact to jointly take decisions. The only\nassumption concerning the underlying communication layer is that for each pair\nof processes there is a path of neighbours connecting them. This allows us to\ndeal with arbitrary network topologies.\n  We also discuss the implementation of this system which consists of a\nsequence of layers. The lower layers deal with the operations that implement\nthe basic primitives of distributed computing, namely low level communication\nand distributed termination, while the upper layers use these primitives to\nimplement high level communication among players, including broadcasting and\nmulticasting, and distributed decision making.\n  This yields a highly flexible distributed system whose specific applications\nare realized as instances of its top layer. This design is implemented in Java.\n  The system supports at various levels fault-tolerance and includes a\nprovision for distributed policing the purpose of which is to exclude\n`dishonest' players. Also, it can be used for repeated creation of dynamically\nformed networks of players interested in a joint decision making implemented by\nmeans of a tax-based mechanism. We illustrate its flexibility by discussing a\nnumber of implemented examples. \n\n"}
{"id": "0801.2480", "contents": "Title: Asynchronous Iterative Waterfilling for Gaussian Frequency-Selective\n  Interference Channels Abstract: This paper considers the maximization of information rates for the Gaussian\nfrequency-selective interference channel, subject to power and spectral mask\nconstraints on each link. To derive decentralized solutions that do not require\nany cooperation among the users, the optimization problem is formulated as a\nstatic noncooperative game of complete information. To achieve the so-called\nNash equilibria of the game, we propose a new distributed algorithm called\nasynchronous iterative waterfilling algorithm. In this algorithm, the users\nupdate their power spectral density in a completely distributed and\nasynchronous way: some users may update their power allocation more frequently\nthan others and they may even use outdated measurements of the received\ninterference. The proposed algorithm represents a unified framework that\nencompasses and generalizes all known iterative waterfilling algorithms, e.g.,\nsequential and simultaneous versions. The main result of the paper consists of\na unified set of conditions that guarantee the global converge of the proposed\nalgorithm to the (unique) Nash equilibrium of the game. \n\n"}
{"id": "0802.2159", "contents": "Title: A Distributed Merge and Split Algorithm for Fair Cooperation in Wireless\n  Networks Abstract: This paper introduces a novel concept from coalitional game theory which\nallows the dynamic formation of coalitions among wireless nodes. A simple and\ndistributed merge and split algorithm for coalition formation is constructed.\nThis algorithm is applied to study the gains resulting from the cooperation\namong single antenna transmitters for virtual MIMO formation. The aim is to\nfind an ultimate transmitters coalition structure that allows cooperating users\nto maximize their utilities while accounting for the cost of coalition\nformation. Through this novel game theoretical framework, the wireless network\ntransmitters are able to self-organize and form a structured network composed\nof disjoint stable coalitions. Simulation results show that the proposed\nalgorithm can improve the average individual user utility by 26.4% as well as\ncope with the mobility of the distributed users. \n\n"}
{"id": "0802.2612", "contents": "Title: On Subgraph Isomorphism Abstract: Article explicitly expresses Subgraph Isomorphism by a polynomial size\nasymmetric linear system. \n\n"}
{"id": "0803.0731", "contents": "Title: Complexity Analysis of Reed-Solomon Decoding over GF(2^m) Without Using\n  Syndromes Abstract: For the majority of the applications of Reed-Solomon (RS) codes, hard\ndecision decoding is based on syndromes. Recently, there has been renewed\ninterest in decoding RS codes without using syndromes. In this paper, we\ninvestigate the complexity of syndromeless decoding for RS codes, and compare\nit to that of syndrome-based decoding. Aiming to provide guidelines to\npractical applications, our complexity analysis differs in several aspects from\nexisting asymptotic complexity analysis, which is typically based on\nmultiplicative fast Fourier transform (FFT) techniques and is usually in big O\nnotation. First, we focus on RS codes over characteristic-2 fields, over which\nsome multiplicative FFT techniques are not applicable. Secondly, due to\nmoderate block lengths of RS codes in practice, our analysis is complete since\nall terms in the complexities are accounted for. Finally, in addition to fast\nimplementation using additive FFT techniques, we also consider direct\nimplementation, which is still relevant for RS codes with moderate lengths.\nComparing the complexities of both syndromeless and syndrome-based decoding\nalgorithms based on direct and fast implementations, we show that syndromeless\ndecoding algorithms have higher complexities than syndrome-based ones for high\nrate RS codes regardless of the implementation. Both errors-only and\nerrors-and-erasures decoding are considered in this paper. We also derive\ntighter bounds on the complexities of fast polynomial multiplications based on\nCantor's approach and the fast extended Euclidean algorithm. \n\n"}
{"id": "0804.0362", "contents": "Title: Exhaustive enumeration unveils clustering and freezing in random 3-SAT Abstract: We study geometrical properties of the complete set of solutions of the\nrandom 3-satisfiability problem. We show that even for moderate system sizes\nthe number of clusters corresponds surprisingly well with the theoretic\nasymptotic prediction. We locate the freezing transition in the space of\nsolutions which has been conjectured to be relevant in explaining the onset of\ncomputational hardness in random constraint satisfaction problems. \n\n"}
{"id": "0804.2288", "contents": "Title: Parimutuel Betting on Permutations Abstract: We focus on a permutation betting market under parimutuel call auction model\nwhere traders bet on the final ranking of n candidates. We present a\nProportional Betting mechanism for this market. Our mechanism allows the\ntraders to bet on any subset of the n x n 'candidate-rank' pairs, and rewards\nthem proportionally to the number of pairs that appear in the final outcome. We\nshow that market organizer's decision problem for this mechanism can be\nformulated as a convex program of polynomial size. More importantly, the\nformulation yields a set of n x n unique marginal prices that are sufficient to\nprice the bets in this mechanism, and are computable in polynomial-time. The\nmarginal prices reflect the traders' beliefs about the marginal distributions\nover outcomes. We also propose techniques to compute the joint distribution\nover n! permutations from these marginal distributions. We show that using a\nmaximum entropy criterion, we can obtain a concise parametric form (with only n\nx n parameters) for the joint distribution which is defined over an\nexponentially large state space. We then present an approximation algorithm for\ncomputing the parameters of this distribution. In fact, the algorithm addresses\nthe generic problem of finding the maximum entropy distribution over\npermutations that has a given mean, and may be of independent interest. \n\n"}
{"id": "0804.2699", "contents": "Title: A Critique of a Polynomial-time SAT Solver Devised by Sergey Gubin Abstract: This paper refutes the validity of the polynomial-time algorithm for solving\nsatisfiability proposed by Sergey Gubin. Gubin introduces the algorithm using\n3-SAT and eventually expands it to accept a broad range of forms of the Boolean\nsatisfiability problem. Because 3-SAT is NP-complete, the algorithm would have\nimplied P = NP, had it been correct. Additionally, this paper refutes the\ncorrectness of his polynomial-time reduction of SAT to 2-SAT. \n\n"}
{"id": "0806.0535", "contents": "Title: Sincere-Strategy Preference-Based Approval Voting Fully Resists\n  Constructive Control and Broadly Resists Destructive Control Abstract: We study sincere-strategy preference-based approval voting (SP-AV), a system\nproposed by Brams and Sanver [Electoral Studies, 25(2):287-305, 2006], and here\nadjusted so as to coerce admissibility of the votes (rather than excluding\ninadmissible votes a priori), with respect to procedural control. In such\ncontrol scenarios, an external agent seeks to change the outcome of an election\nvia actions such as adding/deleting/partitioning either candidates or voters.\nSP-AV combines the voters' preference rankings with their approvals of\ncandidates, where in elections with at least two candidates the voters'\napproval strategies are adjusted--if needed--to approve of their most-preferred\ncandidate and to disapprove of their least-preferred candidate. This rule\ncoerces admissibility of the votes even in the presence of control actions, and\nhybridizes, in effect, approval with pluralitiy voting.\n  We prove that this system is computationally resistant (i.e., the\ncorresponding control problems are NP-hard) to 19 out of 22 types of\nconstructive and destructive control. Thus, SP-AV has more resistances to\ncontrol than is currently known for any other natural voting system with a\npolynomial-time winner problem. In particular, SP-AV is (after Copeland voting,\nsee Faliszewski et al. [AAIM-2008, Springer LNCS 5034, pp. 165-176, 2008]) the\nsecond natural voting system with an easy winner-determination procedure that\nis known to have full resistance to constructive control, and unlike Copeland\nvoting it in addition displays broad resistance to destructive control. \n\n"}
{"id": "0806.2139", "contents": "Title: Beyond Nash Equilibrium: Solution Concepts for the 21st Century Abstract: Nash equilibrium is the most commonly-used notion of equilibrium in game\ntheory. However, it suffers from numerous problems. Some are well known in the\ngame theory community; for example, the Nash equilibrium of repeated prisoner's\ndilemma is neither normatively nor descriptively reasonable. However, new\nproblems arise when considering Nash equilibrium from a computer science\nperspective: for example, Nash equilibrium is not robust (it does not tolerate\n``faulty'' or ``unexpected'' behavior), it does not deal with coalitions, it\ndoes not take computation cost into account, and it does not deal with cases\nwhere players are not aware of all aspects of the game. Solution concepts that\ntry to address these shortcomings of Nash equilibrium are discussed. \n\n"}
{"id": "0808.3746", "contents": "Title: A game-theoretic version of Oakes' example for randomized forecasting Abstract: Using the game-theoretic framework for probability, Vovk and Shafer. have\nshown that it is always possible, using randomization, to make sequential\nprobability forecasts that pass any countable set of well-behaved statistical\ntests. This result generalizes work by other authors, who consider only tests\nof calbration.\n  We complement this result with a lower bound. We show that Vovk and Shafer's\nresult is valid only when the forecasts are computed with unrestrictedly\nincreasing degree of accuracy.\n  When some level of discreteness is fixed, we present a game-theoretic\ngeneralization of Oakes' example for randomized forecasting that is a test\nfailing any given method of deferministic forecasting; originally, this example\nwas presented for deterministic calibration. \n\n"}
{"id": "0810.3150", "contents": "Title: Semidefinite Programming for Min-Max Problems and Games Abstract: We introduce two min-max problems: the first problem is to minimize the\nsupremum of finitely many rational functions over a compact basic\nsemi-algebraic set whereas the second problem is a 2-player zero-sum polynomial\ngame in randomized strategies and with compact basic semi-algebraic pure\nstrategy sets. It is proved that their optimal solution can be approximated by\nsolving a hierarchy of semidefinite relaxations, in the spirit of the moment\napproach developed in Lasserre. This provides a unified approach and a class of\nalgorithms to approximate all Nash equilibria and min-max strategies of many\nstatic and dynamic games. Each semidefinite relaxation can be solved in time\nwhich is polynomial in its input size and practice from global optimization\nsuggests that very often few relaxations are needed for a good approximation\n(and sometimes even finite convergence). \n\n"}
{"id": "0810.3182", "contents": "Title: Optimal Strategies in Sequential Bidding Abstract: We are interested in mechanisms that maximize social welfare. In [1] this\nproblem was studied for multi-unit auctions with unit demand bidders and for\nthe public project problem, and in each case social welfare undominated\nmechanisms in the class of feasible and incentive compatible mechanisms were\nidentified. One way to improve upon these optimality results is by allowing the\nplayers to move sequentially. With this in mind, we study here sequential\nversions of two feasible Groves mechanisms used for single item auctions: the\nVickrey auction and the Bailey-Cavallo mechanism. Because of the absence of\ndominant strategies in this sequential setting, we focus on a weaker concept of\nan optimal strategy. For each mechanism we introduce natural optimal strategies\nand observe that in each mechanism these strategies exhibit different\nbehaviour. However, we then show that among all optimal strategies, the one we\nintroduce for each mechanism maximizes the social welfare when each player\nfollows it. The resulting social welfare can be larger than the one obtained in\nthe simultaneous setting. Finally, we show that, when interpreting both\nmechanisms as simultaneous ones, the vectors of the proposed strategies form a\nPareto optimal Nash equilibrium in the class of optimal strategies. \n\n"}
{"id": "0811.2841", "contents": "Title: Universally Utility-Maximizing Privacy Mechanisms Abstract: A mechanism for releasing information about a statistical database with\nsensitive data must resolve a trade-off between utility and privacy. Privacy\ncan be rigorously quantified using the framework of {\\em differential privacy},\nwhich requires that a mechanism's output distribution is nearly the same\nwhether or not a given database row is included or excluded. The goal of this\npaper is strong and general utility guarantees, subject to differential\nprivacy.\n  We pursue mechanisms that guarantee near-optimal utility to every potential\nuser, independent of its side information (modeled as a prior distribution over\nquery results) and preferences (modeled via a loss function).\n  Our main result is: for each fixed count query and differential privacy\nlevel, there is a {\\em geometric mechanism} $M^*$ -- a discrete variant of the\nsimple and well-studied Laplace mechanism -- that is {\\em simultaneously\nexpected loss-minimizing} for every possible user, subject to the differential\nprivacy constraint. This is an extremely strong utility guarantee: {\\em every}\npotential user $u$, no matter what its side information and preferences,\nderives as much utility from $M^*$ as from interacting with a differentially\nprivate mechanism $M_u$ that is optimally tailored to $u$. \n\n"}
{"id": "0812.0598", "contents": "Title: Preference Games and Personalized Equilibria, with Applications to\n  Fractional BGP Abstract: We study the complexity of computing equilibria in two classes of network\ngames based on flows - fractional BGP (Border Gateway Protocol) games and\nfractional BBC (Bounded Budget Connection) games. BGP is the glue that holds\nthe Internet together and hence its stability, i.e. the equilibria of\nfractional BGP games (Haxell, Wilfong), is a matter of practical importance.\nBBC games (Laoutaris et al) follow in the tradition of the large body of work\non network formation games and capture a variety of applications ranging from\nsocial networks and overlay networks to peer-to-peer networks.\n  The central result of this paper is that there are no fully polynomial-time\napproximation schemes (unless PPAD is in FP) for computing equilibria in both\nfractional BGP games and fractional BBC games. We obtain this result by proving\nthe hardness for a new and surprisingly simple game, the fractional preference\ngame, which is reducible to both fractional BGP and BBC games.\n  We define a new flow-based notion of equilibrium for matrix games --\npersonalized equilibria -- generalizing both fractional BBC and fractional BGP\ngames. We prove not just the existence, but the existence of rational\npersonalized equilibria for all matrix games, which implies the existence of\nrational equilibria for fractional BGP and BBC games. In particular, this\nprovides an alternative proof and strengthening of the main result in [Haxell,\nWilfong]. For k-player matrix games, where k = 2, we provide a combinatorial\ncharacterization leading to a polynomial-time algorithm for computing all\npersonalized equilibria. For k >= 5, we prove that personalized equilibria are\nPPAD-hard to approximate in fully polynomial time. We believe that the concept\nof personalized equilibria has potential for real-world significance. \n\n"}
{"id": "0812.0743", "contents": "Title: A Novel Clustering Algorithm Based on Quantum Games Abstract: Enormous successes have been made by quantum algorithms during the last\ndecade. In this paper, we combine the quantum game with the problem of data\nclustering, and then develop a quantum-game-based clustering algorithm, in\nwhich data points in a dataset are considered as players who can make decisions\nand implement quantum strategies in quantum games. After each round of a\nquantum game, each player's expected payoff is calculated. Later, he uses a\nlink-removing-and-rewiring (LRR) function to change his neighbors and adjust\nthe strength of links connecting to them in order to maximize his payoff.\nFurther, algorithms are discussed and analyzed in two cases of strategies, two\npayoff matrixes and two LRR functions. Consequently, the simulation results\nhave demonstrated that data points in datasets are clustered reasonably and\nefficiently, and the clustering algorithms have fast rates of convergence.\nMoreover, the comparison with other algorithms also provides an indication of\nthe effectiveness of the proposed approach. \n\n"}
{"id": "0812.0956", "contents": "Title: EcoTRADE - a multi player network game of a tradable permit market for\n  biodiversity credits Abstract: EcoTRADE is a multi player network game of a virtual biodiversity credit\nmarket. Each player controls the land use of a certain amount of parcels on a\nvirtual landscape. The biodiversity credits of a particular parcel depend on\nneighboring parcels, which may be owned by other players. The game can be used\nto study the strategies of players in experiments or classroom games and also\nas a communication tool for stakeholders participating in credit markets that\ninclude spatially interdependent credits. \n\n"}
{"id": "0901.3692", "contents": "Title: The Complexity of Computing Minimal Unidirectional Covering Sets Abstract: Given a binary dominance relation on a set of alternatives, a common thread\nin the social sciences is to identify subsets of alternatives that satisfy\ncertain notions of stability. Examples can be found in areas as diverse as\nvoting theory, game theory, and argumentation theory. Brandt and Fischer [BF08]\nproved that it is NP-hard to decide whether an alternative is contained in some\ninclusion-minimal upward or downward covering set. For both problems, we raise\nthis lower bound to the Theta_{2}^{p} level of the polynomial hierarchy and\nprovide a Sigma_{2}^{p} upper bound. Relatedly, we show that a variety of other\nnatural problems regarding minimal or minimum-size covering sets are hard or\ncomplete for either of NP, coNP, and Theta_{2}^{p}. An important consequence of\nour results is that neither minimal upward nor minimal downward covering sets\n(even when guaranteed to exist) can be computed in polynomial time unless P=NP.\nThis sharply contrasts with Brandt and Fischer's result that minimal\nbidirectional covering sets (i.e., sets that are both minimal upward and\nminimal downward covering sets) are polynomial-time computable. \n\n"}
{"id": "0902.2537", "contents": "Title: Communication-optimal Parallel and Sequential Cholesky Decomposition Abstract: Numerical algorithms have two kinds of costs: arithmetic and communication,\nby which we mean either moving data between levels of a memory hierarchy (in\nthe sequential case) or over a network connecting processors (in the parallel\ncase). Communication costs often dominate arithmetic costs, so it is of\ninterest to design algorithms minimizing communication. In this paper we first\nextend known lower bounds on the communication cost (both for bandwidth and for\nlatency) of conventional (O(n^3)) matrix multiplication to Cholesky\nfactorization, which is used for solving dense symmetric positive definite\nlinear systems. Second, we compare the costs of various Cholesky decomposition\nimplementations to these lower bounds and identify the algorithms and data\nstructures that attain them. In the sequential case, we consider both the\ntwo-level and hierarchical memory models. Combined with prior results in [13,\n14, 15], this gives a set of communication-optimal algorithms for O(n^3)\nimplementations of the three basic factorizations of dense linear algebra: LU\nwith pivoting, QR and Cholesky. But it goes beyond this prior work on\nsequential LU by optimizing communication for any number of levels of memory\nhierarchy. \n\n"}
{"id": "0903.1972", "contents": "Title: On Competing Wireless Service Providers Abstract: We consider a situation where wireless service providers compete for\nheterogenous wireless users. The users differ in their willingness to pay as\nwell as in their individual channel gains. We prove existence and uniqueness of\nthe Nash equilibrium for the competition of two service providers, for a\ngeneric channel model. Interestingly, the competition of two providers leads to\na globally optimal outcome. We extend some of the results to the case where\nmore than two providers are competing. Finally, we provide numerical examples\nthat illustrate the effects of various parameters on the Nash equilibrium. \n\n"}
{"id": "0904.1630", "contents": "Title: Self-Assembly of a Statistically Self-Similar Fractal Abstract: We demonstrate existence of a tile assembly system that self-assembles the\nstatistically self-similar Sierpinski Triangle in the Winfree-Rothemund Tile\nAssembly Model. This appears to be the first paper that considers self-assembly\nof a random fractal, instead of a deterministic fractal or a finite, bounded\nshape. Our technical contributions include a way to remember, and use,\nunboundedly-long prefixes of an infinite coding sequence at each stage of\nfractal construction; a tile assembly mechanism for nested recursion; and a\ndefinition of \"almost-everywhere local determinism,\" to describe a tileset\nwhose assembly is locally determined, conditional upon a zeta-dimension zero\nset of (infinitely many) \"input\" tiles. This last is similar to the definition\nof randomized computation for Turing machines, in which an algorithm is\ndeterministic relative to an oracle sequence of coin flips that provides advice\nbut does not itself compute. Keywords: tile self-assembly, statistically\nself-similar Sierpinski Triangle. \n\n"}
{"id": "0904.2540", "contents": "Title: What does Newcomb's paradox teach us? Abstract: In Newcomb's paradox you choose to receive either the contents of a\nparticular closed box, or the contents of both that closed box and another one.\nBefore you choose though, an antagonist uses a prediction algorithm to deduce\nyour choice, and fills the two boxes based on that deduction. Newcomb's paradox\nis that game theory's expected utility and dominance principles appear to\nprovide conflicting recommendations for what you should choose. A recent\nextension of game theory provides a powerful tool for resolving paradoxes\nconcerning human choice, which formulates such paradoxes in terms of Bayes\nnets. Here we apply this to ol to Newcomb's scenario. We show that the\nconflicting recommendations in Newcomb's scenario use different Bayes nets to\nrelate your choice and the algorithm's prediction. These two Bayes nets are\nincompatible. This resolves the paradox: the reason there appears to be two\nconflicting recommendations is that the specification of the underlying Bayes\nnet is open to two, conflicting interpretations. We then show that the accuracy\nof the prediction algorithm in Newcomb's paradox, the focus of much previous\nwork, is irrelevant. We similarly show that the utility functions of you and\nthe antagonist are irrelevant. We end by showing that Newcomb's paradox is\ntime-reversal invariant; both the paradox and its resolution are unchanged if\nthe algorithm makes its `prediction' \\emph{after} you make your choice rather\nthan before. \n\n"}
{"id": "0904.2541", "contents": "Title: Disproof of the Neighborhood Conjecture with Implications to SAT Abstract: We study a Maker/Breaker game described by Beck. As a result we disprove a\nconjecture of Beck on positional games, establish a connection between this\ngame and SAT and construct an unsatisfiable k-CNF formula with few occurrences\nper variable, thereby improving a previous result by Hoory and Szeider and\nshowing that the bound obtained from the Lovasz Local Lemma is tight up to a\nconstant factor. The Maker/Breaker game we study is as follows. Maker and\nBreaker take turns in choosing vertices from a given n-uniform hypergraph F,\nwith Maker going first. Maker's goal is to completely occupy a hyperedge and\nBreaker tries to avoid this. Beck conjectures that if the maximum neighborhood\nsize of F is at most 2^(n-1) then Breaker has a winning strategy. We disprove\nthis conjecture by establishing an n-uniform hypergraph with maximum\nneighborhood size 3*2^(n - 3) where Maker has a winning strategy. Moreover, we\nshow how to construct an n-uniform hypergraph with maximum degree (2^(n-1))/n\nwhere maker has a winning strategy. Finally, we establish a connection between\nSAT and the Maker/Breaker game we study. We can use this connection to derive\nnew results in SAT. Kratochvil, Savicky and Tuza showed that for every k >= 3\nthere is an integer f(k) such that every (k,f(k))-formula is satisfiable, but\n(k,f(k) + 1)-SAT is already NP-complete (it is not known whether f(k) is\ncomputable). Kratochvil, Savicky and Tuza also gave the best known lower bound\nf(k) = Omega(2^k/k), which is a consequence of the Lovasz Local Lemma. We prove\nthat, in fact, f(k) = Theta(2^k/k), improving upon the best known upper bound\nO((log k) * 2^k/k) by Hoory and Szeider. \n\n"}
{"id": "0905.2485", "contents": "Title: Minimizing Communication in Linear Algebra Abstract: In 1981 Hong and Kung proved a lower bound on the amount of communication\nneeded to perform dense, matrix-multiplication using the conventional $O(n^3)$\nalgorithm, where the input matrices were too large to fit in the small, fast\nmemory. In 2004 Irony, Toledo and Tiskin gave a new proof of this result and\nextended it to the parallel case. In both cases the lower bound may be\nexpressed as $\\Omega$(#arithmetic operations / $\\sqrt{M}$), where M is the size\nof the fast memory (or local memory in the parallel case). Here we generalize\nthese results to a much wider variety of algorithms, including LU\nfactorization, Cholesky factorization, $LDL^T$ factorization, QR factorization,\nalgorithms for eigenvalues and singular values, i.e., essentially all direct\nmethods of linear algebra. The proof works for dense or sparse matrices, and\nfor sequential or parallel algorithms. In addition to lower bounds on the\namount of data moved (bandwidth) we get lower bounds on the number of messages\nrequired to move it (latency). We illustrate how to extend our lower bound\ntechnique to compositions of linear algebra operations (like computing powers\nof a matrix), to decide whether it is enough to call a sequence of simpler\noptimal algorithms (like matrix multiplication) to minimize communication, or\nif we can do better. We give examples of both. We also show how to extend our\nlower bounds to certain graph theoretic problems.\n  We point out recently designed algorithms for dense LU, Cholesky, QR,\neigenvalue and the SVD problems that attain these lower bounds; implementations\nof LU and QR show large speedups over conventional linear algebra algorithms in\nstandard libraries like LAPACK and ScaLAPACK. Many open problems remain. \n\n"}
{"id": "0905.2659", "contents": "Title: Coalitional Games for Distributed Collaborative Spectrum Sensing in\n  Cognitive Radio Networks Abstract: Collaborative spectrum sensing among secondary users (SUs) in cognitive\nnetworks is shown to yield a significant performance improvement. However,\nthere exists an inherent trade off between the gains in terms of probability of\ndetection of the primary user (PU) and the costs in terms of false alarm\nprobability. In this paper, we study the impact of this trade off on the\ntopology and the dynamics of a network of SUs seeking to reduce the\ninterference on the PU through collaborative sensing. Moreover, while existing\nliterature mainly focused on centralized solutions for collaborative sensing,\nwe propose distributed collaboration strategies through game theory. We model\nthe problem as a non-transferable coalitional game, and propose a distributed\nalgorithm for coalition formation through simple merge and split rules. Through\nthe proposed algorithm, SUs can autonomously collaborate and self-organize into\ndisjoint independent coalitions, while maximizing their detection probability\ntaking into account the cooperation costs (in terms of false alarm). We study\nthe stability of the resulting network structure, and show that a maximum\nnumber of SUs per formed coalition exists for the proposed utility model.\nSimulation results show that the proposed algorithm allows a reduction of up to\n86.6% of the average missing probability per SU (probability of missing the\ndetection of the PU) relative to the non-cooperative case, while maintaining a\ncertain false alarm level. In addition, through simulations, we compare the\nperformance of the proposed distributed solution with respect to an optimal\ncentralized solution that minimizes the average missing probability per SU.\nFinally, the results also show how the proposed algorithm autonomously adapts\nthe network topology to environmental changes such as mobility. \n\n"}
{"id": "0906.4316", "contents": "Title: Constructive Decision Theory Abstract: In most contemporary approaches to decision making, a decision problem is\ndescribed by a sets of states and set of outcomes, and a rich set of acts,\nwhich are functions from states to outcomes over which the decision maker (DM)\nhas preferences. Most interesting decision problems, however, do not come with\na state space and an outcome space. Indeed, in complex problems it is often far\nfrom clear what the state and outcome spaces would be. We present an\nalternative foundation for decision making, in which the primitive objects of\nchoice are syntactic programs. A representation theorem is proved in the spirit\nof standard representation theorems, showing that if the DM's preference\nrelation on objects of choice satisfies appropriate axioms, then there exist a\nset S of states, a set O of outcomes, a way of interpreting the objects of\nchoice as functions from S to O, a probability on S, and a utility function on\nO, such that the DM prefers choice a to choice b if and only if the expected\nutility of a is higher than that of b. Thus, the state space and outcome space\nare subjective, just like the probability and utility; they are not part of the\ndescription of the problem. In principle, a modeler can test for SEU behavior\nwithout having access to states or outcomes. We illustrate the power of our\napproach by showing that it can capture decision makers who are subject to\nframing effects. \n\n"}
{"id": "0906.4321", "contents": "Title: Reasoning About Knowledge of Unawareness Revisited Abstract: In earlier work, we proposed a logic that extends the Logic of General\nAwareness of Fagin and Halpern [1988] by allowing quantification over primitive\npropositions. This makes it possible to express the fact that an agent knows\nthat there are some facts of which he is unaware. In that logic, it is not\npossible to model an agent who is uncertain about whether he is aware of all\nformulas. To overcome this problem, we keep the syntax of the earlier paper,\nbut allow models where, with each world, a possibly different language is\nassociated. We provide a sound and complete axiomatization for this logic and\nshow that, under natural assumptions, the quantifier-free fragment of the logic\nis characterized by exactly the same axioms as the logic of Heifetz, Meier, and\nSchipper [2008]. \n\n"}
{"id": "0906.4827", "contents": "Title: Physical Layer Security: Coalitional Games for Distributed Cooperation Abstract: Cooperation between wireless network nodes is a promising technique for\nimproving the physical layer security of wireless transmission, in terms of\nsecrecy capacity, in the presence of multiple eavesdroppers. While existing\nphysical layer security literature answered the question \"what are the\nlink-level secrecy capacity gains from cooperation?\", this paper attempts to\nanswer the question of \"how to achieve those gains in a practical decentralized\nwireless network and in the presence of a secrecy capacity cost for information\nexchange?\". For this purpose, we model the physical layer security cooperation\nproblem as a coalitional game with non-transferable utility and propose a\ndistributed algorithm for coalition formation. Through the proposed algorithm,\nthe wireless users can autonomously cooperate and self-organize into disjoint\nindependent coalitions, while maximizing their secrecy capacity taking into\naccount the security costs during information exchange. We analyze the\nresulting coalitional structures, discuss their properties, and study how the\nusers can self-adapt the network topology to environmental changes such as\nmobility. Simulation results show that the proposed algorithm allows the users\nto cooperate and self-organize while improving the average secrecy capacity per\nuser up to 25.32% relative to the non-cooperative case. \n\n"}
{"id": "0909.3558", "contents": "Title: Route Distribution Incentives Abstract: We present an incentive model for route distribution in the context of path\nvector routing protocols and we focus on the Border Gateway Protocol (BGP). BGP\nis the de-facto protocol for interdomain routing on the Internet. We model BGP\nroute distribution and computation using a game in which a BGP speaker\nadvertises its prefix to its direct neighbors promising them a reward for\nfurther distributing the route deeper into the network, the neighbors do the\nsame thing with their neighbors, and so on. The result of this cascaded route\ndistribution is an advertised prefix and hence reachability of the BGP speaker.\nWe first study the convergence of BGP protocol dynamics to a unique outcome\ntree in the defined game. We then proceed to study the existence of equilibria\nin the full information game considering competition dynamics. We focus our\nwork on the simplest two classes of graphs: 1) the line (and the tree) graphs\nwhich involve no competition, and 2) the ring graph which involves competition. \n\n"}
{"id": "0910.2370", "contents": "Title: On the hardness of the noncommutative determinant Abstract: In this paper we study the computational complexity of computing the\nnoncommutative determinant. We first consider the arithmetic circuit complexity\nof computing the noncommutative determinant polynomial. Then, more generally,\nwe also examine the complexity of computing the determinant (as a function)\nover noncommutative domains. Our hardness results are summarized below:\n  1. We show that if the noncommutative determinant polynomial has small\nnoncommutative arithmetic circuits then so does the noncommutative permanent.\nConsequently, the commutative permanent polynomial has small commutative\narithmetic circuits. 2. For any field F we show that computing the n X n\npermanent over F is polynomial-time reducible to computing the 2n X 2n\n(noncommutative) determinant whose entries are O(n^2) X O(n^2) matrices over\nthe field F. 3. We also derive as a consequence that computing the n X n\npermanent over nonnegative rationals is polynomial-time reducible to computing\nthe noncommutative determinant over Clifford algebras of n^{O(1)} dimension.\n  Our techniques are elementary and use primarily the notion of the Hadamard\nProduct of noncommutative polynomials. \n\n"}
{"id": "0911.1767", "contents": "Title: A Natural Dynamics for Bargaining on Exchange Networks Abstract: Bargaining networks model the behavior of a set of players that need to reach\npairwise agreements for making profits. Nash bargaining solutions are special\noutcomes of such games that are both stable and balanced. Kleinberg and Tardos\nproved a sharp algorithmic characterization of such outcomes, but left open the\nproblem of how the actual bargaining process converges to them. A partial\nanswer was provided by Azar et al. who proposed a distributed algorithm for\nconstructing Nash bargaining solutions, but without polynomial bounds on its\nconvergence rate. In this paper, we introduce a simple and natural model for\nthis process, and study its convergence rate to Nash bargaining solutions. At\neach time step, each player proposes a deal to each of her neighbors. The\nproposal consists of a share of the potential profit in case of agreement. The\nshare is chosen to be balanced in Nash's sense as far as this is feasible (with\nrespect to the current best alternatives for both players). We prove that,\nwhenever the Nash bargaining solution is unique (and satisfies a positive gap\ncondition) this dynamics converges to it in polynomial time. Our analysis is\nbased on an approximate decoupling phenomenon between the dynamics on different\nsubstructures of the network. This approach may be of general interest for the\nanalysis of local algorithms on networks. \n\n"}
{"id": "0911.3162", "contents": "Title: Bounding Rationality by Discounting Time Abstract: Consider a game where Alice generates an integer and Bob wins if he can\nfactor that integer. Traditional game theory tells us that Bob will always win\nthis game even though in practice Alice will win given our usual assumptions\nabout the hardness of factoring.\n  We define a new notion of bounded rationality, where the payoffs of players\nare discounted by the computation time they take to produce their actions. We\nuse this notion to give a direct correspondence between the existence of\nequilibria where Alice has a winning strategy and the hardness of factoring.\nNamely, under a natural assumption on the discount rates, there is an\nequilibriumwhere Alice has a winning strategy iff there is a linear-time\nsamplable distribution with respect to which Factoring is hard on average.\n  We also give general results for discounted games over countable action\nspaces, including showing that any game with bounded and computable payoffs has\nan equilibrium in our model, even if each player is allowed a countable number\nof actions. It follows, for example, that the Largest Integer game has an\nequilibrium in our model though it has no Nash equilibria or epsilon-Nash\nequilibria. \n\n"}
{"id": "1001.0436", "contents": "Title: Truthful Assignment without Money Abstract: We study the design of truthful mechanisms that do not use payments for the\ngeneralized assignment problem (GAP) and its variants. An instance of the GAP\nconsists of a bipartite graph with jobs on one side and machines on the other.\nMachines have capacities and edges have values and sizes; the goal is to\nconstruct a welfare maximizing feasible assignment. In our model of private\nvaluations, motivated by impossibility results, the value and sizes on all\njob-machine pairs are public information; however, whether an edge exists or\nnot in the bipartite graph is a job's private information.\n  We study several variants of the GAP starting with matching. For the\nunweighted version, we give an optimal strategyproof mechanism; for maximum\nweight bipartite matching, however, we show give a 2-approximate strategyproof\nmechanism and show by a matching lowerbound that this is optimal. Next we study\nknapsack-like problems, which are APX-hard. For these problems, we develop a\ngeneral LP-based technique that extends the ideas of Lavi and Swamy to reduce\ndesigning a truthful mechanism without money to designing such a mechanism for\nthe fractional version of the problem, at a loss of a factor equal to the\nintegrality gap in the approximation ratio. We use this technique to obtain\nstrategyproof mechanisms with constant approximation ratios for these problems.\nWe then design an O(log n)-approximate strategyproof mechanism for the GAP by\nreducing, with logarithmic loss in the approximation, to our solution for the\nvalue-invariant GAP. Our technique may be of independent interest for designing\ntruthful mechanisms without money for other LP-based problems. \n\n"}
{"id": "1001.1139", "contents": "Title: Spatial search in a honeycomb network Abstract: The spatial search problem consists in minimizing the number of steps\nrequired to find a given site in a network, under the restriction that only\noracle queries or translations to neighboring sites are allowed. In this paper,\na quantum algorithm for the spatial search problem on a honeycomb lattice with\n$N$ sites and torus-like boundary conditions. The search algorithm is based on\na modified quantum walk on a hexagonal lattice and the general framework\nproposed by Ambainis, Kempe and Rivosh is used to show that the time complexity\nof this quantum search algorithm is $O(\\sqrt{N \\log N})$. \n\n"}
{"id": "1002.3864", "contents": "Title: Limits of Approximation Algorithms: PCPs and Unique Games (DIMACS\n  Tutorial Lecture Notes) Abstract: These are the lecture notes for the DIMACS Tutorial \"Limits of Approximation\nAlgorithms: PCPs and Unique Games\" held at the DIMACS Center, CoRE Building,\nRutgers University on 20-21 July, 2009. This tutorial was jointly sponsored by\nthe DIMACS Special Focus on Hardness of Approximation, the DIMACS Special Focus\non Algorithmic Foundations of the Internet, and the Center for Computational\nIntractability with support from the National Security Agency and the National\nScience Foundation.\n  The speakers at the tutorial were Matthew Andrews, Sanjeev Arora, Moses\nCharikar, Prahladh Harsha, Subhash Khot, Dana Moshkovitz and Lisa Zhang. The\nsribes were Ashkan Aazami, Dev Desai, Igor Gorodezky, Geetha Jagannathan,\nAlexander S. Kulikov, Darakhshan J. Mir, Alantha Newman, Aleksandar Nikolov,\nDavid Pritchard and Gwen Spencer. \n\n"}
{"id": "1002.4464", "contents": "Title: Deterministic Sample Sort For GPUs Abstract: We present and evaluate GPU Bucket Sort, a parallel deterministic sample sort\nalgorithm for many-core GPUs. Our method is considerably faster than Thrust\nMerge (Satish et.al., Proc. IPDPS 2009), the best comparison-based sorting\nalgorithm for GPUs, and it is as fast as the new randomized sample sort for\nGPUs by Leischner et.al. (to appear in Proc. IPDPS 2010). Our deterministic\nsample sort has the advantage that bucket sizes are guaranteed and therefore\nits running time does not have the input data dependent fluctuations that can\noccur for randomized sample sort. \n\n"}
{"id": "1002.4658", "contents": "Title: Principal Component Analysis with Contaminated Data: The High\n  Dimensional Case Abstract: We consider the dimensionality-reduction problem (finding a subspace\napproximation of observed data) for contaminated data in the high dimensional\nregime, where the number of observations is of the same magnitude as the number\nof variables of each observation, and the data set contains some (arbitrarily)\ncorrupted observations. We propose a High-dimensional Robust Principal\nComponent Analysis (HR-PCA) algorithm that is tractable, robust to contaminated\npoints, and easily kernelizable. The resulting subspace has a bounded deviation\nfrom the desired one, achieves maximal robustness -- a breakdown point of 50%\nwhile all existing algorithms have a breakdown point of zero, and unlike\nordinary PCA algorithms, achieves optimality in the limit case where the\nproportion of corrupted points goes to zero. \n\n"}
{"id": "1003.2554", "contents": "Title: Spectrum Trading: An Abstracted Bibliography Abstract: This document contains a bibliographic list of major papers on spectrum\ntrading and their abstracts. The aim of the list is to offer researchers\nentering this field a fast panorama of the current literature. The list is\ncontinually updated on the webpage\n\\url{http://www.disp.uniroma2.it/users/naldi/Ricspt.html}. Omissions and papers\nsuggested for inclusion may be pointed out to the authors through e-mail\n(\\textit{naldi@disp.uniroma2.it}). \n\n"}
{"id": "1004.2079", "contents": "Title: Bargaining dynamics in exchange networks Abstract: We consider a one-sided assignment market or exchange network with\ntransferable utility and propose a model for the dynamics of bargaining in such\na market. Our dynamical model is local, involving iterative updates of 'offers'\nbased on estimated best alternative matches, in the spirit of pairwise Nash\nbargaining. We establish that when a balanced outcome (a generalization of the\npairwise Nash bargaining solution to networks) exists, our dynamics converges\nrapidly to such an outcome. We extend our results to the cases of (i) general\nagent 'capacity constraints', i.e., an agent may be allowed to participate in\nmultiple matches, and (ii) 'unequal bargaining powers' (where we also find a\nsurprising change in rate of convergence). \n\n"}
{"id": "1004.4317", "contents": "Title: The cooperative game theory foundations of network bargaining games Abstract: We study bargaining games between suppliers and manufacturers in a network\ncontext. Agents wish to enter into contracts in order to generate surplus which\nthen must be divided among the participants. Potential contracts and their\nsurplus are represented by weighted edges in our bipartite network. Each agent\nin the market is additionally limited by a capacity representing the number of\ncontracts which he or she may undertake. When all agents are limited to just\none contract each, prior research applied natural generalizations of the Nash\nbargaining solution to the networked setting, defined the new solution concepts\nof stable and balanced, and characterized the resulting bargaining outcomes. We\nsimplify and generalize these results to a setting in which participants in\nonly one side of the market are limited to one contract each. The heart of our\nresults uses a linear-programming formulation to establish a novel connection\nbetween well-studied cooperative game theory concepts (such as core and\nprekernel) and the solution concepts of stable and balanced defined for the\nbargaining games. This immediately implies one can take advantage of the\nresults and algorithms in cooperative game theory to reproduce results such as\nthose of Azar et al. [1] and Kleinberg and Tardos [29] and also generalize them\nto our setting. The cooperative-game-theoretic connection also inspires us to\nrefine our solution space using standard solution concepts from that literature\nsuch as nucleolus and lexicographic kernel. The nucleolus is particularly\nattractive as it is unique, always exists, and is supported by experimental\ndata in the network bargaining literature. Guided by algorithms from\ncooperative game theory, we show how to compute the nucleolus by pruning and\niteratively solving a natural linear-programming formulation. \n\n"}
{"id": "1005.3730", "contents": "Title: Obtaining the Quantum Fourier Transform from the Classical FFT with QR\n  Decomposition Abstract: We present the detailed process of converting the classical Fourier Transform\nalgorithm into the quantum one by using QR decomposition. This provides an\nexample of a technique for building quantum algorithms using classical ones.\nThe Quantum Fourier Transform is one of the most important quantum subroutines\nknown at present, used in most algorithms that have exponential speed up\ncompared to the classical ones. We briefly review Fast Fourier Transform and\nthen make explicit all the steps that led to the quantum formulation of the\nalgorithm, generalizing Coppersmith's work. \n\n"}
{"id": "1005.4846", "contents": "Title: When Knowing Early Matters: Gossip, Percolation and Nash Equilibria Abstract: Continually arriving information is communicated through a network of $n$\nagents, with the value of information to the $j$'th recipient being a\ndecreasing function of $j/n$, and communication costs paid by recipient.\nRegardless of details of network and communication costs, the social optimum\npolicy is to communicate arbitrarily slowly. But selfish agent behavior leads\nto Nash equilibria which (in the $n \\to \\infty$ limit) may be efficient (Nash\npayoff $=$ social optimum payoff) or wasteful ($0 < $ Nash payoff $<$ social\noptimum payoff) or totally wasteful (Nash payoff $=0$). We study the cases of\nthe complete network (constant communication costs between all agents), the\ngrid with only nearest-neighbor communication, and the grid with communication\ncost a function of distance. The main technical tool is analysis of the\nassociated first passage percolation process or SI epidemic (representing\nspread of one item of information) and in particular its \"window width\", the\ntime interval during which most agents learn the item. Many arguments are just\noutlined, not intended as complete rigorous proofs. \n\n"}
{"id": "1005.5507", "contents": "Title: An Algebraic Approach for Computing Equilibria of a Subclass of Finite\n  Normal Form Games Abstract: A Nash equilibrium has become important solution concept for analyzing the\ndecision making in Game theory. In this paper, we consider the problem of\ncomputing Nash equilibria of a subclass of generic finite normal form games. We\ndefine \"rational payoff irrational equilibria games\" to be the games with all\nrational payoffs and all irrational equilibria. We present a purely algebraic\nmethod for computing all Nash equilibria of these games that uses knowledge of\nGalois groups. Some results, showing properties of the class of games, and an\nexample to show working of the method concludes the paper. \n\n"}
{"id": "1006.3046", "contents": "Title: Identifying Shapes Using Self-Assembly (extended abstract) Abstract: In this paper, we introduce the following problem in the theory of\nalgorithmic self-assembly: given an input shape as the seed of a tile-based\nself-assembly system, design a finite tile set that can, in some sense,\nuniquely identify whether or not the given input shape--drawn from a very\ngeneral class of shapes--matches a particular target shape. We first study the\ncomplexity of correctly identifying squares. Then we investigate the complexity\nassociated with the identification of a considerably more general class of\nnon-square, hole-free shapes. \n\n"}
{"id": "1007.0571", "contents": "Title: Quickest Detection with Social Learning: Interaction of local and global\n  decision makers Abstract: We consider how local and global decision policies interact in stopping time\nproblems such as quickest time change detection. Individual agents make myopic\nlocal decisions via social learning, that is, each agent records a private\nobservation of a noisy underlying state process, selfishly optimizes its local\nutility and then broadcasts its local decision. Given these local decisions,\nhow can a global decision maker achieve quickest time change detection when the\nunderlying state changes according to a phase-type distribution? The paper\npresents four results. First, using Blackwell dominance of measures, it is\nshown that the optimal cost incurred in social learning based quickest\ndetection is always larger than that of classical quickest detection. Second,\nit is shown that in general the optimal decision policy for social learning\nbased quickest detection is characterized by multiple thresholds within the\nspace of Bayesian distributions. Third, using lattice programming and\nstochastic dominance, sufficient conditions are given for the optimal decision\npolicy to consist of a single linear hyperplane, or, more generally, a\nthreshold curve. Estimation of the optimal linear approximation to this\nthreshold curve is formulated as a simulation-based stochastic optimization\nproblem. Finally, the paper shows that in multi-agent sensor management with\nquickest detection, where each agent views the world according to its prior,\nthe optimal policy has a similar structure to social learning. \n\n"}
{"id": "1007.1343", "contents": "Title: On the justification of applying quantum strategies to the Prisoners'\n  Dilemma and mechanism design Abstract: The Prisoners' Dilemma is perhaps the most famous model in the field of game\ntheory. Consequently, it is natural to investigate its quantum version when one\nconsiders to apply quantum strategies to game theory. There are two main\nresults in this paper: 1) The well-known Prisoners' Dilemma can be categorized\ninto three types and only the third type is adaptable for quantum strategies.\n2) As a reverse problem of game theory, mechanism design provides a better\ncircumstance for quantum strategies than game theory does. \n\n"}
{"id": "1007.1800", "contents": "Title: Multimode Control Attacks on Elections Abstract: In 1992, Bartholdi, Tovey, and Trick opened the study of control attacks on\nelections---attempts to improve the election outcome by such actions as\nadding/deleting candidates or voters. That work has led to many results on how\nalgorithms can be used to find attacks on elections and how\ncomplexity-theoretic hardness results can be used as shields against attacks.\nHowever, all the work in this line has assumed that the attacker employs just a\nsingle type of attack. In this paper, we model and study the case in which the\nattacker launches a multipronged (i.e., multimode) attack. We do so to more\nrealistically capture the richness of real-life settings. For example, an\nattacker might simultaneously try to suppress some voters, attract new voters\ninto the election, and introduce a spoiler candidate. Our model provides a\nunified framework for such varied attacks, and by constructing polynomial-time\nmultiprong attack algorithms we prove that for various election systems even\nsuch concerted, flexible attacks can be perfectly planned in deterministic\npolynomial time. \n\n"}
{"id": "1007.2694", "contents": "Title: Cache Me If You Can: Capacitated Selfish Replication Games in Networks Abstract: In Peer-to-Peer (P2P) network systems, content (object) delivery between\nnodes is often required. One way to study such a distributed system is by\ndefining games, which involve selfish nodes that make strategic choices on\nreplicating content in their local limited memory (cache) or accessing content\nfrom other nodes for a cost. These Selfish Replication games have been\nintroduced in [8] for nodes that do not have any capacity limits, leaving the\ncapacitated problem, i.e. Capacitated Selfish Replication (CSR) games, open.\n  In this work, we first form the model of the CSR games, for which we perform\na Nash equilibria analysis. In particular, we focus on hierarchical networks,\ngiven their extensive use to model communication costs of content delivery in\nP2P systems. We present an exact polynomial-time algorithm for any hierarchical\nnetwork, under two constraints on the utility functions: 1) \"Nearer is better\",\ni.e. the closest the content is to the node the less its access cost is, and 2)\n\"Independence of irrelevant alternatives\", i.e. aggregation of individual node\npreferences. This generalization represents a vast class of utilities and more\ninterestingly allows each of the nodes to have simultaneously completely\ndifferent functional forms of utility functions. In this general framework, we\npresent CSR games results on arbitrary networks and outline the boundary\nbetween intractability and effective computability in terms of the network\nstructure, object preferences, and the total number of objects. Moreover, we\nprove that the problem of equilibria existence becomes NP-hard for general CSR\ngames. \n\n"}
{"id": "1007.3801", "contents": "Title: On the Approximability of Budget Feasible Mechanisms Abstract: Budget feasible mechanisms, recently initiated by Singer (FOCS 2010), extend\nalgorithmic mechanism design problems to a realistic setting with a budget\nconstraint. We consider the problem of designing truthful budget feasible\nmechanisms for general submodular functions: we give a randomized mechanism\nwith approximation ratio $7.91$ (improving the previous best-known result 112),\nand a deterministic mechanism with approximation ratio $8.34$. Further we study\nthe knapsack problem, which is special submodular function, give a $2+\\sqrt{2}$\napproximation deterministic mechanism (improving the previous best-known result\n6), and a 3 approximation randomized mechanism. We provide a similar result for\nan extended knapsack problem with heterogeneous items, where items are divided\ninto groups and one can pick at most one item from each group.\n  Finally we show a lower bound of approximation ratio of $1+\\sqrt{2}$ for\ndeterministic mechanisms and 2 for randomized mechanisms for knapsack, as well\nas the general submodular functions. Our lower bounds are unconditional, which\ndo not rely on any computational or complexity assumptions. \n\n"}
{"id": "1007.5032", "contents": "Title: Approximation Algorithms for Secondary Spectrum Auctions Abstract: We study combinatorial auctions for the secondary spectrum market. In this\nmarket, short-term licenses shall be given to wireless nodes for communication\nin their local neighborhood. In contrast to the primary market, channels can be\nassigned to multiple bidders, provided that the corresponding devices are well\nseparated such that the interference is sufficiently low. Interference\nconflicts are described in terms of a conflict graph in which the nodes\nrepresent the bidders and the edges represent conflicts such that the feasible\nallocations for a channel correspond to the independent sets in the conflict\ngraph.\n  In this paper, we suggest a novel LP formulation for combinatorial auctions\nwith conflict graph using a non-standard graph parameter, the so-called\ninductive independence number. Taking into account this parameter enables us to\nbypass the well-known lower bound of \\Omega(n^{1-\\epsilon}) on the\napproximability of independent set in general graphs with n nodes (bidders). We\nachieve significantly better approximation results by showing that interference\nconstraints for wireless networks yield conflict graphs with bounded inductive\nindependence number.\n  Our framework covers various established models of wireless communication,\ne.g., the protocol or the physical model. For the protocol model, we achieve an\nO(\\sqrt{k})-approximation, where k is the number of available channels. For the\nmore realistic physical model, we achieve an O(\\sqrt{k} \\log^2 n) approximation\nbased on edge-weighted conflict graphs. Combining our approach with the the\nLP-based framework of Lavi and Swamy, we obtain incentive compatible mechanisms\nfor general bidders with arbitrary valuations on bundles of channels specified\nin terms of demand oracles. \n\n"}
{"id": "1008.0530", "contents": "Title: Strategy iteration is strongly polynomial for 2-player turn-based\n  stochastic games with a constant discount factor Abstract: Ye showed recently that the simplex method with Dantzig pivoting rule, as\nwell as Howard's policy iteration algorithm, solve discounted Markov decision\nprocesses (MDPs), with a constant discount factor, in strongly polynomial time.\nMore precisely, Ye showed that both algorithms terminate after at most\n$O(\\frac{mn}{1-\\gamma}\\log(\\frac{n}{1-\\gamma}))$ iterations, where $n$ is the\nnumber of states, $m$ is the total number of actions in the MDP, and\n$0<\\gamma<1$ is the discount factor. We improve Ye's analysis in two respects.\nFirst, we improve the bound given by Ye and show that Howard's policy iteration\nalgorithm actually terminates after at most\n$O(\\frac{m}{1-\\gamma}\\log(\\frac{n}{1-\\gamma}))$ iterations. Second, and more\nimportantly, we show that the same bound applies to the number of iterations\nperformed by the strategy iteration (or strategy improvement) algorithm, a\ngeneralization of Howard's policy iteration algorithm used for solving 2-player\nturn-based stochastic games with discounted zero-sum rewards. This provides the\nfirst strongly polynomial algorithm for solving these games, resolving a long\nstanding open problem. \n\n"}
{"id": "1008.1501", "contents": "Title: Dodgson's Rule Approximations and Absurdity Abstract: With the Dodgson rule, cloning the electorate can change the winner, which\nYoung (1977) considers an \"absurdity\". Removing this absurdity results in a new\nrule (Fishburn, 1977) for which we can compute the winner in polynomial time\n(Rothe et al., 2003), unlike the traditional Dodgson rule. We call this rule DC\nand introduce two new related rules (DR and D&). Dodgson did not explicitly\npropose the \"Dodgson rule\" (Tideman, 1987); we argue that DC and DR are better\nrealizations of the principle behind the Dodgson rule than the traditional\nDodgson rule. These rules, especially D&, are also effective approximations to\nthe traditional Dodgson's rule. We show that, unlike the rules we have\nconsidered previously, the DC, DR and D& scores differ from the Dodgson score\nby no more than a fixed amount given a fixed number of alternatives, and thus\nthese new rules converge to Dodgson under any reasonable assumption on voter\nbehaviour, including the Impartial Anonymous Culture assumption. \n\n"}
{"id": "1008.2267", "contents": "Title: Application Neutrality and a Paradox of Side Payments Abstract: The ongoing debate over net neutrality covers a broad set of issues related\nto the regulation of public networks. In two ways, we extend an idealized\nusage-priced game-theoretic framework based on a common linear demand-response\nmodel. First, we study the impact of \"side payments\" among a plurality of\nInternet service (access) providers and content providers. In the\nnon-monopolistic case, our analysis reveals an interesting \"paradox\" of side\npayments in that overall revenues are reduced for those that receive them.\nSecond, assuming different application types (e.g., HTTP web traffic,\npeer-to-peer file sharing, media streaming, interactive VoIP), we extend this\nmodel to accommodate differential pricing among them in order to study the\nissue of application neutrality. Revenues for neutral and non-neutral pricing\nare compared for the case of two application types. \n\n"}
{"id": "1008.3287", "contents": "Title: A note on revelation principle from an energy perspective Abstract: The revelation principle has been known in the economics society for decades.\nIn this paper, I will investigate it from an energy perspective, i.e.,\nconsidering the energy consumed by agents and the designer in participating a\nmechanism. The main result is that when the strategies of agents are actions\nrather than messages, an additional energy condition should be added to make\nthe revelation principle hold in the real world. \n\n"}
{"id": "1008.3829", "contents": "Title: Approximate Judgement Aggregation Abstract: In this paper we analyze judgement aggregation problems in which a group of\nagents independently votes on a set of complex propositions that has some\ninterdependency constraint between them(e.g., transitivity when describing\npreferences). We consider the issue of judgement aggregation from the\nperspective of approximation. That is, we generalize the previous results by\nstudying approximate judgement aggregation. We relax the main two constraints\nassumed in the current literature, Consistency and Independence and consider\nmechanisms that only approximately satisfy these constraints, that is, satisfy\nthem up to a small portion of the inputs. The main question we raise is whether\nthe relaxation of these notions significantly alters the class of satisfying\naggregation mechanisms. The recent works for preference aggregation of Kalai,\nMossel, and Keller fit into this framework. The main result of this paper is\nthat, as in the case of preference aggregation, in the case of a subclass of a\nnatural class of aggregation problems termed `truth-functional agendas', the\nset of satisfying aggregation mechanisms does not extend non-trivially when\nrelaxing the constraints. Our proof techniques involve Boolean Fourier\ntransform and analysis of voter influences for voting protocols. The question\nwe raise for Approximate Aggregation can be stated in terms of Property\nTesting. For instance, as a corollary from our result we get a generalization\nof the classic result for property testing of linearity of Boolean functions.\n  An updated version (RePEc:huj:dispap:dp574R) is available at\nhttp://www.ratio.huji.ac.il/dp_files/dp574R.pdf \n\n"}
{"id": "1008.4776", "contents": "Title: Targeting by Transnational Terrorist Groups Abstract: Many successful terrorist groups operate across international borders where\ndifferent countries host different stages of terrorist operations. Often the\nrecruits for the group come from one country or countries, while the targets of\nthe operations are in another. Stopping such attacks is difficult because\nintervention in any region or route might merely shift the terrorists\nelsewhere. Here we propose a model of transnational terrorism based on the\ntheory of activity networks. The model represents attacks on different\ncountries as paths in a network. The group is assumed to prefer paths of lowest\ncost (or risk) and maximal yield from attacks. The parameters of the model are\ncomputed for the Islamist-Salafi terrorist movement based on open source data\nand then used for estimation of risks of future attacks. The central finding is\nthat the USA has an enduring appeal as a target, due to lack of other nations\nof matching geopolitical weight or openness. It is also shown that countries in\nAfrica and Asia that have been overlooked as terrorist bases may become highly\nsignificant threats in the future. The model quantifies the dilemmas facing\ncountries in the effort to cut such networks, and points to a limitation of\ndeterrence against transnational terrorists. \n\n"}
{"id": "1009.4798", "contents": "Title: Role of feedback and broadcasting in the naming game Abstract: The naming game (NG) describes the agreement dynamics of a population of\nagents that interact locally in a pairwise fashion, and in recent years\nstatistical physics tools and techniques have greatly contributed to shed light\non its rich phenomenology. Here we investigate in details the role played by\nthe way in which the two agents update their states after an interaction. We\nshow that slightly modifying the NG rules in terms of which agent performs the\nupdate in given circumstances (i.e. after a success) can either alter\ndramatically the overall dynamics or leave it qualitatively unchanged. We\nunderstand analytically the first case by casting the model in the broader\nframework of a generalized NG. As for the second case, on the other hand, we\nnote that the modified rule reproducing the main features of the usual NG\ncorresponds in fact to a simplification of it consisting in the elimination of\nfeedback between the agents. This allows us to introduce and study a very\nnatural broadcasting scheme on networks that can be potentially relevant for\ndifferent applications, such as the design and implementation of autonomous\nsensor networks, as pointed out in the recent literature. \n\n"}
{"id": "1009.5636", "contents": "Title: One-Counter Stochastic Games Abstract: We study the computational complexity of basic decision problems for\none-counter simple stochastic games (OC-SSGs), under various objectives.\nOC-SSGs are 2-player turn-based stochastic games played on the transition graph\nof classic one-counter automata. We study primarily the termination objective,\nwhere the goal of one player is to maximize the probability of reaching counter\nvalue 0, while the other player wishes to avoid this. Partly motivated by the\ngoal of understanding termination objectives, we also study certain \"limit\" and\n\"long run average\" reward objectives that are closely related to some\nwell-studied objectives for stochastic games with rewards. Examples of problems\nwe address include: does player 1 have a strategy to ensure that the counter\neventually hits 0, i.e., terminates, almost surely, regardless of what player 2\ndoes? Or that the liminf (or limsup) counter value equals infinity with a\ndesired probability? Or that the long run average reward is >0 with desired\nprobability? We show that the qualitative termination problem for OC-SSGs is in\nNP intersection coNP, and is in P-time for 1-player OC-SSGs, or equivalently\nfor one-counter Markov Decision Processes (OC-MDPs). Moreover, we show that\nquantitative limit problems for OC-SSGs are in NP intersection coNP, and are in\nP-time for 1-player OC-MDPs. Both qualitative limit problems and qualitative\ntermination problems for OC-SSGs are already at least as hard as Condon's\nquantitative decision problem for finite-state SSGs. \n\n"}
{"id": "1010.3197", "contents": "Title: QoS-Aware Joint Policies in Cognitive Radio Networks Abstract: One of the most challenging problems in Opportunistic Spectrum Access (OSA)\nis to design channel sensing-based protocol in multi secondary users (SUs)\nnetwork. Quality of Service (QoS) requirements for SUs have significant\nimplications on this protocol design. In this paper, we propose a new method to\nfind joint policies for SUs which not only guarantees QoS requirements but also\nmaximizes network throughput. We use Decentralized Partially Observable Markov\nDecision Process (Dec-POMDP) to formulate interactions between SUs. Meanwhile,\na tractable approach for Dec-POMDP is utilized to extract sub-optimum joint\npolicies for large horizons. Among these policies, the joint policy which\nguarantees QoS requirements is selected as the joint sensing strategy for SUs.\nTo show the efficiency of the proposed method, we consider two SUs trying to\naccess two-channel primary users (PUs) network modeled by discrete Markov\nchains. Simulations demonstrate three interesting findings: 1- Optimum joint\npolicies for large horizons can be obtained using the proposed method. 2- There\nexists a joint policy for the assumed QoS constraints. 3- Our method\noutperforms other related works in terms of network throughput. \n\n"}
{"id": "1010.4458", "contents": "Title: Variable time amplitude amplification and a faster quantum algorithm for\n  solving systems of linear equations Abstract: We present two new quantum algorithms. Our first algorithm is a\ngeneralization of amplitude amplification to the case when parts of the quantum\nalgorithm that is being amplified stop at different times.\n  Our second algorithm uses the first algorithm to improve the running time of\nHarrow et al. algorithm for solving systems of linear equations from O(kappa^2\nlog N) to O(kappa log^3 kappa log N) where \\kappa is the condition number of\nthe system of equations. \n\n"}
{"id": "1010.5081", "contents": "Title: Dynamics of Profit-Sharing Games Abstract: An important task in the analysis of multiagent systems is to understand how\ngroups of selfish players can form coalitions, i.e., work together in teams. In\nthis paper, we study the dynamics of coalition formation under bounded\nrationality. We consider settings where each team's profit is given by a convex\nfunction, and propose three profit-sharing schemes, each of which is based on\nthe concept of marginal utility. The agents are assumed to be myopic, i.e.,\nthey keep changing teams as long as they can increase their payoff by doing so.\nWe study the properties (such as closeness to Nash equilibrium or total profit)\nof the states that result after a polynomial number of such moves, and prove\nbounds on the price of anarchy and the price of stability of the corresponding\ngames. \n\n"}
{"id": "1011.0253", "contents": "Title: Polynomial-time Computation of Exact Correlated Equilibrium in Compact\n  Games Abstract: In a landmark paper, Papadimitriou and Roughgarden described a\npolynomial-time algorithm (\"Ellipsoid Against Hope\") for computing sample\ncorrelated equilibria of concisely-represented games. Recently, Stein, Parrilo\nand Ozdaglar showed that this algorithm can fail to find an exact correlated\nequilibrium, but can be easily modified to efficiently compute approximate\ncorrelated equilibria. Currently, it remains unresolved whether the algorithm\ncan be modified to compute an exact correlated equilibrium. We show that it\ncan, presenting a variant of the Ellipsoid Against Hope algorithm that\nguarantees the polynomial-time identification of exact correlated equilibrium.\nOur new algorithm differs from the original primarily in its use of a\nseparation oracle that produces cuts corresponding to pure-strategy profiles.\nAs a result, we no longer face the numerical precision issues encountered by\nthe original approach, and both the resulting algorithm and its analysis are\nconsiderably simplified. Our new separation oracle can be understood as a\nderandomization of Papadimitriou and Roughgarden's original separation oracle\nvia the method of conditional probabilities. Also, the equilibria returned by\nour algorithm are distributions with polynomial-sized supports, which are\nsimpler (in the sense of being representable in fewer bits) than the mixtures\nof product distributions produced previously; no tractable algorithm has\npreviously been proposed for identifying such equilibria. \n\n"}
{"id": "1011.3168", "contents": "Title: Online Learning: Beyond Regret Abstract: We study online learnability of a wide class of problems, extending the\nresults of (Rakhlin, Sridharan, Tewari, 2010) to general notions of performance\nmeasure well beyond external regret. Our framework simultaneously captures such\nwell-known notions as internal and general Phi-regret, learning with\nnon-additive global cost functions, Blackwell's approachability, calibration of\nforecasters, adaptive regret, and more. We show that learnability in all these\nsituations is due to control of the same three quantities: a martingale\nconvergence term, a term describing the ability to perform well if future is\nknown, and a generalization of sequential Rademacher complexity, studied in\n(Rakhlin, Sridharan, Tewari, 2010). Since we directly study complexity of the\nproblem instead of focusing on efficient algorithms, we are able to improve and\nextend many known results which have been previously derived via an algorithmic\nconstruction. \n\n"}
{"id": "1011.3493", "contents": "Title: Program Size and Temperature in Self-Assembly Abstract: Winfree's abstract Tile Assembly Model (aTAM) is a model of molecular\nself-assembly of DNA complexes known as tiles, which float freely in solution\nand attach one at a time to a growing \"seed\" assembly based on specific binding\nsites on their four sides. We show that there is a polynomial-time algorithm\nthat, given an n x n square, finds the minimal tile system (i.e., the system\nwith the smallest number of distinct tile types) that uniquely self-assembles\nthe square, answering an open question of Adleman, Cheng, Goel, Huang, Kempe,\nMoisset de Espanes, and Rothemund (\"Combinatorial Optimization Problems in\nSelf-Assembly\", STOC 2002). Our investigation leading to this algorithm reveals\nother positive and negative results about the relationship between the size of\na tile system and its \"temperature\" (the binding strength threshold required\nfor a tile to attach). \n\n"}
{"id": "1011.5384", "contents": "Title: Spectrum Sharing as Spatial Congestion Games Abstract: In this paper, we present and analyze the properties of a new class of games\n- the spatial congestion game (SCG), which is a generalization of the classical\ncongestion game (CG). In a classical congestion game, multiple users share the\nsame set of resources and a user's payoff for using any resource is a function\nof the total number of users sharing it. As a potential game, this game enjoys\nsome very appealing properties, including the existence of a pure strategy Nash\nequilibrium (NE) and that every improvement path is finite and leads to such a\nNE (also called the finite improvement property or FIP). While it's tempting to\nuse this model to study spectrum sharing, it does not capture the spatial reuse\nfeature of wireless communication, where resources (interpreted as channels)\nmay be reused without increasing congestion provided that users are located far\naway from each other. This motivates us to study an extended form of the\ncongestion game where a user's payoff for using a resource is a function of the\nnumber of its interfering users sharing it. This naturally results in a spatial\ncongestion game (SCG), where users are placed over a network (or a conflict\ngraph). We study fundamental properties of a spatial congestion game; in\nparticular, we seek to answer under what conditions this game possesses the\nfinite improvement property or a Nash equilibrium. We also discuss the\nimplications of these results when applied to wireless spectrum sharing. \n\n"}
{"id": "1012.3018", "contents": "Title: On the size of data structures used in symbolic model checking Abstract: Temporal Logic Model Checking is a verification method in which we describe a\nsystem, the model, and then we verify whether some properties, expressed in a\ntemporal logic formula, hold in the system. It has many industrial\napplications. In order to improve performance, some tools allow preprocessing\nof the model, verifying on-line a set of properties reusing the same compiled\nmodel; we prove that the complexity of the Model Checking problem, without any\npreprocessing or preprocessing the model or the formula in a polynomial data\nstructure, is the same. As a result preprocessing does not always exponentially\nimprove performance.\n  Symbolic Model Checking algorithms work by manipulating sets of states, and\nthese sets are often represented by BDDs. It has been observed that the size of\nBDDs may grow exponentially as the model and formula increase in size. As a\nside result, we formally prove that a superpolynomial increase of the size of\nthese BDDs is unavoidable in the worst case. While this exponential growth has\nbeen empirically observed, to the best of our knowledge it has never been\nproved so far in general terms. This result not only holds for all types of\nBDDs regardless of the variable ordering, but also for more powerful data\nstructures, such as BEDs, RBCs, MTBDDs, and ADDs. \n\n"}
{"id": "1012.5141", "contents": "Title: Quantum Strategic Game Theory Abstract: We propose a simple yet rich model to extend the notions of Nash equilibria\nand correlated equilibria of strategic games to the quantum setting, in which\nwe then study the relations between classical and quantum equilibria. Unlike\nthe previous work that focus on qualitative questions on specific games of\nsmall sizes, we address the following fundamental and quantitative question for\ngeneral games:\n  How much \"advantage\" can playing quantum strategies provide, if any?\n  Two measures of the advantage are studied, summarized as follows.\n  1. A natural measure is the increase of payoff. We consider natural mappings\nbetween classical and quantum states, and study how well those mappings\npreserve the equilibrium properties. Among other results, we exhibit correlated\nequilibrium $p$ whose quantum superposition counterpart $\\sum_s\n\\sqrt{p(s)}\\ket{s}$ is far from being a quantum correlated equilibrium;\nactually a player can increase her payoff from almost 0 to almost 1 in a\n[0,1]-normalized game. We achieve this by a tensor product construction on\ncarefully designed base cases.\n  2. For studying the hardness of generating correlated equilibria, we propose\nto examine \\emph{correlation complexity}, a new complexity measure for\ncorrelation generation. We show that there are $n$-bit correlated equilibria\nwhich can be generated by only one EPR pair followed by local operation\n(without communication), but need at least $\\log(n)$ classical shared random\nbits plus communication. The randomized lower bound can be improved to $n$, the\nbest possible, assuming (even a much weaker version of) a recent conjecture in\nlinear algebra. We believe that the correlation complexity, as a\ncomplexity-theoretical counterpart of the celebrated Bell's inequality, has\nindependent interest in both physics and computational complexity theory and\ndeserves more explorations. \n\n"}
{"id": "1012.5173", "contents": "Title: Public Announcements in Strategic Games with Arbitrary Strategy Sets Abstract: In [Van Benthem 2007] the concept of a public announcement is used to study\nthe effect of the iterated elimination of strictly dominated strategies. We\noffer a simple generalisation of this approach to cover arbitrary strategic\ngames and many optimality notions. We distinguish between announcements of\noptimality and announcements of rationality. \n\n"}
{"id": "1101.2170", "contents": "Title: The Complexity of Finding Multiple Solutions to Betweenness and Quartet\n  Compatibility Abstract: We show that two important problems that have applications in computational\nbiology are ASP-complete, which implies that, given a solution to a problem, it\nis NP-complete to decide if another solution exists. We show first that a\nvariation of Betweenness, which is the underlying problem of questions related\nto radiation hybrid mapping, is ASP-complete. Subsequently, we use that result\nto show that Quartet Compatibility, a fundamental problem in phylogenetics that\nasks whether a set of quartets can be represented by a parent tree, is also\nASP-complete. The latter result shows that Steel's \\sc Quartet Challenge, which\nasks whether a solution to Quartet Compatibility is unique, is coNP-complete. \n\n"}
{"id": "1102.1115", "contents": "Title: Adaptive Resource Allocation in Jamming Teams Using Game Theory Abstract: In this work, we study the problem of power allocation and adaptive\nmodulation in teams of decision makers. We consider the special case of two\nteams with each team consisting of two mobile agents. Agents belonging to the\nsame team communicate over wireless ad hoc networks, and they try to split\ntheir available power between the tasks of communication and jamming the nodes\nof the other team. The agents have constraints on their total energy and\ninstantaneous power usage. The cost function adopted is the difference between\nthe rates of erroneously transmitted bits of each team. We model the adaptive\nmodulation problem as a zero-sum matrix game which in turn gives rise to a a\ncontinuous kernel game to handle power control. Based on the communications\nmodel, we present sufficient conditions on the physical parameters of the\nagents for the existence of a pure strategy saddle-point equilibrium (PSSPE). \n\n"}
{"id": "1102.3195", "contents": "Title: Auctions with a Profit Sharing Contract Abstract: We study the problem of selling a resource through an auction mechanism. The\nwinning buyer in turn develops this resource to generate profit. Two forms of\npayment are considered: charging the winning buyer a one-time payment, or an\ninitial payment plus a profit sharing contract (PSC). We consider a symmetric\ninterdependent values model with risk averse or risk neutral buyers and a risk\nneutral seller. For the second price auction and the English auction, we show\nthat the seller's expected total revenue from the auction where he also takes a\nfraction of the positive profit is higher than the expected revenue from the\nauction with only a one-time payment. Moreover, the seller can generate an even\nhigher expected total revenue if, in addition to taking a fraction of the\npositive profit, he also takes the same fraction of any loss incurred from\ndeveloping the resource. Moving beyond simple PSCs, we show that the auction\nwith a PSC from a very general class generates higher expected total revenue\nthan the auction with only a one-time payment. Finally, we show that suitable\nPSCs provide higher expected total revenue than a one-time payment even when\nthe incentives of the winning buyer to develop the resource must be addressed\nby the seller. \n\n"}
{"id": "1102.3499", "contents": "Title: Benchmark Problems for Totally Unimodular Set System Auction Abstract: We consider a generalization of the $k$-flow set system auction where the set\nto be procured by a customer corresponds to a feasible solution to a linear\nprogramming problem where the coefficient matrix and right-hand-side together\nconstitute a totally unimodular matrix. Our results generalize and strengthen\nbounds identified for several benchmarks, which form a crucial component in the\nstudy of frugality ratios of truthful auction mechanisms. \n\n"}
{"id": "1102.3561", "contents": "Title: Spatial SINR Games of Base Station Placement and Mobile Association Abstract: We study the question of determining locations of base stations that may\nbelong to the same or to competing service providers. We take into account the\nimpact of these decisions on the behavior of intelligent mobile terminals who\ncan connect to the base station that offers the best utility. The signal to\ninterference and noise ratio is used as the quantity that determines the\nassociation. We first study the SINR association-game: we determine the cells\ncorresponding to each base stations, i.e., the locations at which mobile\nterminals prefer to connect to a given base station than to others. We make\nsome surprising observations: (i) displacing a base station a little in one\ndirection may result in a displacement of the boundary of the corresponding\ncell to the opposite direction; (ii) A cell corresponding to a BS may be the\nunion of disconnected sub-cells. We then study the hierarchical equilibrium in\nthe combined BS location and mobile association problem: we determine where to\nlocate the BSs so as to maximize the revenues obtained at the induced SINR\nmobile association game. We consider the cases of single frequency band and two\nfrequency bands of operation. Finally, we also consider hierarchical equilibria\nin two frequency systems with successive interference cancellation. \n\n"}
{"id": "1102.3766", "contents": "Title: Derandomizing HSSW Algorithm for 3-SAT Abstract: We present a (full) derandomization of HSSW algorithm for 3-SAT, proposed by\nHofmeister, Sch\\\"oning, Schuler, and Watanabe in [STACS'02]. Thereby, we obtain\nan O(1.3303^n)-time deterministic algorithm for 3-SAT, which is currently\nfastest. \n\n"}
{"id": "1103.3890", "contents": "Title: The Monty Hall Problem: Switching is Forced by the Strategic Thinking Abstract: Game versions of the Monty Hall Problem are discussed. The focus is on the\nprinciple of eliminating the dominated strategies, both in the zero-sum and\nnoncooperative formulations. \n\n"}
{"id": "1104.0458", "contents": "Title: On the Payoff Mechanisms in Peer-Assisted Services with Multiple Content\n  Providers: Rationality and Fairness Abstract: This paper studies an incentive structure for cooperation and its stability\nin peer-assisted services when there exist multiple content providers, using a\ncoalition game theoretic approach. We first consider a generalized coalition\nstructure consisting of multiple providers with many assisting peers, where\npeers assist providers to reduce the operational cost in content distribution.\nTo distribute the profit from cost reduction to players (i.e., providers and\npeers), we then establish a generalized formula for individual payoffs when a\n\"Shapley-like\" payoff mechanism is adopted. We show that the grand coalition is\nunstable, even when the operational cost functions are concave, which is in\nsharp contrast to the recently studied case of a single provider where the\ngrand coalition is stable. We also show that irrespective of stability of the\ngrand coalition, there always exist coalition structures which are not\nconvergent to the grand coalition under a dynamic among coalition structures.\nOur results give us an incontestable fact that a provider does not tend to\ncooperate with other providers in peer-assisted services, and be separated from\nthem. Three facets of the noncooperative (selfish) providers are illustrated;\n(i) underpaid peers, (ii) service monopoly, and (iii) oscillatory coalition\nstructure. Lastly, we propose a stable payoff mechanism which improves fairness\nof profit-sharing by regulating the selfishness of the players as well as\ngrants the content providers a limited right of realistic bargaining. Our study\nopens many new questions such as realistic and efficient incentive structures\nand the tradeoffs between fairness and individual providers' competition in\npeer-assisted services. \n\n"}
{"id": "1104.2809", "contents": "Title: Self-Assembly with Geometric Tiles Abstract: In this work we propose a generalization of Winfree's abstract Tile Assembly\nModel (aTAM) in which tile types are assigned rigid shapes, or geometries,\nalong each tile face. We examine the number of distinct tile types needed to\nassemble shapes within this model, the temperature required for efficient\nassembly, and the problem of designing compact geometric faces to meet given\ncompatibility specifications. Our results show a dramatic decrease in the\nnumber of tile types needed to assemble $n \\times n$ squares to\n$\\Theta(\\sqrt{\\log n})$ at temperature 1 for the most simple model which meets\na lower bound from Kolmogorov complexity, and $O(\\log\\log n)$ in a model in\nwhich tile aggregates must move together through obstacle free paths within the\nplane. This stands in contrast to the $\\Theta(\\log n / \\log\\log n)$ tile types\nat temperature 2 needed in the basic aTAM. We also provide a general method for\nsimulating a large and computationally universal class of temperature 2 aTAM\nsystems with geometric tiles at temperature 1. Finally, we consider the problem\nof computing a set of compact geometric faces for a tile system to implement a\ngiven set of compatibility specifications. We show a number of bounds on the\ncomplexity of geometry size needed for various classes of compatibility\nspecifications, many of which we directly apply to our tile assembly results to\nachieve non-trivial reductions in geometry size. \n\n"}
{"id": "1104.3055", "contents": "Title: Deciding the Value 1 Problem of Probabilistic Leaktight Automata Abstract: The value 1 problem is a decision problem for probabilistic automata over\nfinite words: given a probabilistic automaton A, are there words accepted by A\nwith probability arbitrarily close to 1? This problem was proved undecidable\nrecently. We sharpen this result, showing that the undecidability result holds\neven if the probabilistic automata have only one probabilistic transition. Our\nmain contribution is to introduce a new class of probabilistic automata, called\nleaktight automata, for which the value 1 problem is shown decidable (and\nPSPACE-complete). We construct an algorithm based on the computation of a\nmonoid abstracting the behaviours of the automaton, and rely on algebraic\ntechniques developed by Simon for the correctness proof. The class of leaktight\nautomata is decidable in PSPACE, subsumes all subclasses of probabilistic\nautomata whose value 1 problem is known to be decidable (in particular\ndeterministic automata), and is closed under two natural composition operators. \n\n"}
{"id": "1104.4746", "contents": "Title: Lasserre Hierarchy, Higher Eigenvalues, and Approximation Schemes for\n  Quadratic Integer Programming with PSD Objectives Abstract: We present an approximation scheme for optimizing certain Quadratic Integer\nProgramming problems with positive semidefinite objective functions and global\nlinear constraints. This framework includes well known graph problems such as\nMinimum graph bisection, Edge expansion, Uniform sparsest cut, and Small Set\nexpansion, as well as the Unique Games problem. These problems are notorious\nfor the existence of huge gaps between the known algorithmic results and\nNP-hardness results. Our algorithm is based on rounding semidefinite programs\nfrom the Lasserre hierarchy, and the analysis uses bounds for low-rank\napproximations of a matrix in Frobenius norm using columns of the matrix.\n  For all the above graph problems, we give an algorithm running in time\n$n^{O(r/\\epsilon^2)}$ with approximation ratio\n$\\frac{1+\\epsilon}{\\min\\{1,\\lambda_r\\}}$, where $\\lambda_r$ is the $r$'th\nsmallest eigenvalue of the normalized graph Laplacian $\\mathcal{L}$. In the\ncase of graph bisection and small set expansion, the number of vertices in the\ncut is within lower-order terms of the stipulated bound. Our results imply\n$(1+O(\\epsilon))$ factor approximation in time $n^{O(r^\\ast/\\epsilon^2)}$ where\n$r^\\ast$ is the number of eigenvalues of $\\mathcal{L}$ smaller than\n$1-\\epsilon$.\n  For Unique Games, we give a factor $(1+\\frac{2+\\epsilon}{\\lambda_r})$\napproximation for minimizing the number of unsatisfied constraints in\n$n^{O(r/\\epsilon)}$ time. This improves an earlier bound for solving Unique\nGames on expanders, and also shows that Lasserre SDPs are powerful enough to\nsolve well-known integrality gap instances for the basic SDP.\n  We also give an algorithm for independent sets in graphs that performs well\nwhen the Laplacian does not have too many eigenvalues bigger than $1+o(1)$. \n\n"}
{"id": "1104.4978", "contents": "Title: Approximating the Termination Value of One-Counter MDPs and Stochastic\n  Games Abstract: One-counter MDPs (OC-MDPs) and one-counter simple stochastic games (OC-SSGs)\nare 1-player, and 2-player turn-based zero-sum, stochastic games played on the\ntransition graph of classic one-counter automata (equivalently, pushdown\nautomata with a 1-letter stack alphabet). A key objective for the analysis and\nverification of these games is the termination objective, where the players aim\nto maximize (minimize, respectively) the probability of hitting counter value\n0, starting at a given control state and given counter value. Recently, we\nstudied qualitative decision problems (\"is the optimal termination value = 1?\")\nfor OC-MDPs (and OC-SSGs) and showed them to be decidable in P-time (in NP and\ncoNP, respectively). However, quantitative decision and approximation problems\n(\"is the optimal termination value ? p\", or \"approximate the termination value\nwithin epsilon\") are far more challenging. This is so in part because optimal\nstrategies may not exist, and because even when they do exist they can have a\nhighly non-trivial structure. It thus remained open even whether any of these\nquantitative termination problems are computable. In this paper we show that\nall quantitative approximation problems for the termination value for OC-MDPs\nand OC-SSGs are computable. Specifically, given a OC-SSG, and given epsilon >\n0, we can compute a value v that approximates the value of the OC-SSG\ntermination game within additive error epsilon, and furthermore we can compute\nepsilon-optimal strategies for both players in the game. A key ingredient in\nour proofs is a subtle martingale, derived from solving certain LPs that we can\nassociate with a maximizing OC-MDP. An application of Azuma's inequality on\nthese martingales yields a computable bound for the \"wealth\" at which a \"rich\nperson's strategy\" becomes epsilon-optimal for OC-MDPs. \n\n"}
{"id": "1105.0558", "contents": "Title: If more than Analytical Modeling is Needed to Predict Real Agents'\n  Strategic Interaction Abstract: This paper presents the research on the interdisciplinary research\ninfrastructure for understanding human reasoning in game-theoretic terms.\nStrategic reasoning is considered to impact human decision making in social,\neconomical and competitive interactions. The provided introduction explains and\nconnects concepts from AI, game theory and psychology. First result is a\nconcept of interdisciplinary game description language as a part of the focused\ninterdisciplinary research infrastructure. The need of this domain-specific\nlanguage is motivated and is aimed to accelerate the current developments. As\nsecond result, the paper provides a summary of ongoing research and its\nsignificance. \n\n"}
{"id": "1105.2243", "contents": "Title: More about Base Station Location Games Abstract: This paper addresses the problem of locating base stations in a certain area\nwhich is highly populated by mobile stations; each mobile station is assumed to\nselect the closest base station. Base stations are modeled by players who\nchoose their best location for maximizing their uplink throughput. The approach\nof this paper is to make some simplifying assumptions in order to get\ninterpretable analytical results and insights to the problem under study.\nSpecifically, a relatively complete Nash equilibrium (NE) analysis is conducted\n(existence, uniqueness, determination, and efficiency). Then, assuming that the\nbase station location can be adjusted dynamically, the best-response dynamics\nand reinforcement learning algorithm are applied, discussed, and illustrated\nthrough numerical results. \n\n"}
{"id": "1105.2470", "contents": "Title: The game of go as a complex network Abstract: We study the game of go from a complex network perspective. We construct a\ndirected network using a suitable definition of tactical moves including local\npatterns, and study this network for different datasets of professional\ntournaments and amateur games. The move distribution follows Zipf's law and the\nnetwork is scale free, with statistical peculiarities different from other real\ndirected networks, such as e. g. the World Wide Web. These specificities\nreflect in the outcome of ranking algorithms applied to it. The fine study of\nthe eigenvalues and eigenvectors of matrices used by the ranking algorithms\nsingles out certain strategic situations. Our results should pave the way to a\nbetter modelization of board games and other types of human strategic scheming. \n\n"}
{"id": "1105.5032", "contents": "Title: The Complexity of Manipulative Attacks in Nearly Single-Peaked\n  Electorates Abstract: Many electoral bribery, control, and manipulation problems (which we will\nrefer to in general as \"manipulative actions\" problems) are NP-hard in the\ngeneral case. It has recently been noted that many of these problems fall into\npolynomial time if the electorate is single-peaked (i.e., is polarized along\nsome axis/issue). However, real-world electorates are not truly single-peaked.\nThere are usually some mavericks, and so real-world electorates tend to merely\nbe nearly single-peaked. This paper studies the complexity of\nmanipulative-action algorithms for elections over nearly single-peaked\nelectorates, for various notions of nearness and various election systems. We\nprovide instances where even one maverick jumps the manipulative-action\ncomplexity up to $\\np$-hardness, but we also provide many instances where a\nreasonable number of mavericks can be tolerated without increasing the\nmanipulative-action complexity. \n\n"}
{"id": "1106.2122", "contents": "Title: Parameterized complexity results for 1-safe Petri nets Abstract: We associate a graph with a 1-safe Petri net and study the parameterized\ncomplexity of various problems with parameters derived from the graph. With\ntreewidth as the parameter, we give W[1]-hardness results for many problems\nabout 1-safe Petri nets. As a corollary, this proves a conjecture of Downey et.\nal. about the hardness of some graph pebbling problems. We consider the\nparameter benefit depth (that is known to be helpful in getting better\nalgorithms for general Petri nets) and again give W[1]-hardness results for\nvarious problems on 1-safe Petri nets. We also consider the stronger parameter\nvertex cover number. Combining the well known automata-theoretic method and a\npowerful fixed parameter tractability (FPT) result about Integer Linear\nProgramming, we give a FPT algorithm for model checking Monadic Second Order\n(MSO) formulas on 1-safe Petri nets, with parameters vertex cover number and\nthe size of the formula. \n\n"}
{"id": "1106.2662", "contents": "Title: Learning Equilibria with Partial Information in Decentralized Wireless\n  Networks Abstract: In this article, a survey of several important equilibrium concepts for\ndecentralized networks is presented. The term decentralized is used here to\nrefer to scenarios where decisions (e.g., choosing a power allocation policy)\nare taken autonomously by devices interacting with each other (e.g., through\nmutual interference). The iterative long-term interaction is characterized by\nstable points of the wireless network called equilibria. The interest in these\nequilibria stems from the relevance of network stability and the fact that they\ncan be achieved by letting radio devices to repeatedly interact over time. To\nachieve these equilibria, several learning techniques, namely, the best\nresponse dynamics, fictitious play, smoothed fictitious play, reinforcement\nlearning algorithms, and regret matching, are discussed in terms of information\nrequirements and convergence properties. Most of the notions introduced here,\nfor both equilibria and learning schemes, are illustrated by a simple case\nstudy, namely, an interference channel with two transmitter-receiver pairs. \n\n"}
{"id": "1107.2722", "contents": "Title: On the Feasibility of Maintenance Algorithms in Dynamic Graphs Abstract: Near ubiquitous mobile computing has led to intense interest in dynamic graph\ntheory. This provides a new and challenging setting for algorithmics and\ncomplexity theory. For any graph-based problem, the rapid evolution of a\n(possibly disconnected) graph over time naturally leads to the important\ncomplexity question: is it better to calculate a new solution from scratch or\nto adapt the known solution on the prior graph to quickly provide a solution of\nguaranteed quality for the changed graph?\n  In this paper, we demonstrate that the former is the best approach in some\ncases, but that there are cases where the latter is feasible. We prove that,\nunder certain conditions, hard problems cannot even be approximated in any\nreasonable complexity bound --- i.e., even with a large amount of time, having\na solution to a very similar graph does not help in computing a solution to the\ncurrent graph. To achieve this, we formalize the idea as a maintenance\nalgorithm. Using r-Regular Subgraph as the primary example we show that\nW[1]-hardness for the parameterized approximation problem implies the\nnon-existence of a maintenance algorithm for the given approximation ratio.\nConversely we show that Vertex Cover, which is fixed-parameter tractable, has a\n2-approximate maintenance algorithm. The implications of NP-hardness and\nNPO-hardness are also explored. \n\n"}
{"id": "1107.2994", "contents": "Title: Budget Feasible Mechanism Design via Random Sampling Abstract: Budget feasible mechanism considers algorithmic mechanism design questions\nwhere there is a budget constraint on the total payment of the mechanism. An\nimportant question in the field is that under which valuation domains there\nexist budget feasible mechanisms that admit `small' approximations (compared to\na socially optimal solution). Singer \\cite{PS10} showed that additive and\nsubmodular functions admit a constant approximation mechanism. Recently,\nDobzinski, Papadimitriou, and Singer \\cite{DPS11} gave an $O(\\log^2n)$\napproximation mechanism for subadditive functions and remarked that: \"A\nfundamental question is whether, regardless of computational constraints, a\nconstant-factor budget feasible mechanism exists for subadditive function.\"\n  In this paper, we give the first attempt to this question. We give a\npolynomial time $O(\\frac{\\log n}{\\log\\log n})$ sub-logarithmic approximation\nratio mechanism for subadditive functions, improving the best known ratio\n$O(\\log^2 n)$. Further, we connect budget feasible mechanism design to the\nconcept of approximate core in cooperative game theory, and show that there is\na mechanism for subadditive functions whose approximation is, via a\ncharacterization of the integrality gap of a linear program, linear to the\nlargest value to which an approximate core exists. Our result implies in\nparticular that the class of XOS functions, which is a superclass of submodular\nfunctions, admits a constant approximation mechanism. We believe that our work\ncould be a solid step towards solving the above fundamental problem eventually,\nand possibly, with an affirmative answer. \n\n"}
{"id": "1107.4929", "contents": "Title: Some Non-Classical Approaches to the Branderburger-Keisler Paradox Abstract: In this paper, we discuss a well-known self-referential paradox in\nfoundational game theory, the Brandenburger - Keisler paradox. We approach the\nparadox from two different perspectives: non-well-founded set theory and\nparaconsistent logic. We show that the paradox persists in both frameworks for\ncategory theoretical reasons, but, with different properties. \n\n"}
{"id": "1109.1082", "contents": "Title: A geometric and combinatorial view of weighted voting Abstract: A natural partial ordering exists on the set of all weighted games and, more\nbroadly, on all linear games. We describe several properties of the partially\nordered sets formed by these games and utilize this perspective to enumerate\nproper linear games with one generator. We introduce a geometric approach to\nweighted voting by considering the convex polytope of all possible realizations\nof a weighted game and connect this geometric perspective to the weighted games\nposet in several ways. In particular, we prove that generic vertical lines in\n$C_n$, the union of all weighted $n$-player polytopes, correspond to maximal\nsaturated chains in the poset of weighted games, i.e., the poset is a blueprint\nfor how the polytopes fit together to form $C_n$. We show how to compare the\nrelationships between the powers of the players using the polytope directly.\nFinally, we describe the facets of each polytope, from which we develop a\nmethod for determining the weightedness of any linear game that covers or is\ncovered by a weighted game. \n\n"}
{"id": "1109.2169", "contents": "Title: Quantum information approach to the ultimatum game Abstract: The paper is devoted to quantization of extensive games with the use of both\nthe Marinatto-Weber and the Eisert-Wilkens-Lewenstein concept of quantum game.\nWe revise the current conception of quantum ultimatum game and we show why the\nproposal is unacceptable. To support our comment, we present the new idea of\nthe quantum ultimatum game. Our scheme also makes a point of departure for a\nprotocol to quantize extensive games. \n\n"}
{"id": "1109.4250", "contents": "Title: Complex dynamics in learning complicated games Abstract: Game theory is the standard tool used to model strategic interactions in\nevolutionary biology and social science. Traditional game theory studies the\nequilibria of simple games. But is traditional game theory applicable if the\ngame is complicated, and if not, what is? We investigate this question here,\ndefining a complicated game as one with many possible moves, and therefore many\npossible payoffs conditional on those moves. We investigate two-person games in\nwhich the players learn based on experience. By generating games at random we\nshow that under some circumstances the strategies of the two players converge\nto fixed points, but under others they follow limit cycles or chaotic\nattractors. The dimension of the chaotic attractors can be very high, implying\nthat the dynamics of the strategies are effectively random. In the chaotic\nregime the payoffs fluctuate intermittently, showing bursts of rapid change\npunctuated by periods of quiescence, similar to what is observed in fluid\nturbulence and financial markets. Our results suggest that such intermittency\nis a highly generic phenomenon, and that there is a large parameter regime for\nwhich complicated strategic interactions generate inherently unpredictable\nbehavior that is best described in the language of dynamical systems theory \n\n"}
{"id": "1109.5615", "contents": "Title: A Regularity Measure for Context Free Grammars Abstract: Parikh's theorem states that every Context Free Language (CFL) has the same\nParikh image as that of a regular language. A finite state automaton accepting\nsuch a regular language is called a Parikh-equivalent automaton. In the worst\ncase, the number of states in any non-deterministic Parikh-equivalent automaton\nis exponentially large in the size of the Context Free Grammar (CFG). We\nassociate a regularity width d with a CFG that measures the closeness of the\nCFL with regular languages. The degree m of a CFG is one less than the maximum\nnumber of variable occurrences in the right hand side of any production. Given\na CFG with n variables, we construct a Parikh-equivalent non-deterministic\nautomaton whose number of states is upper bounded by a polynomial in $n\n(d^{2d(m+1)}), the degree of the polynomial being a small fixed constant. Our\nprocedure is constructive and runs in time polynomial in the size of the\nautomaton. In the terminology of parameterized complexity, we prove that\nconstructing a Parikh-equivalent automaton for a given CFG is Fixed Parameter\nTractable (FPT) when the degree m and regularity width d are parameters. We\nalso give an example from program verification domain where the degree and\nregularity are small compared to the size of the grammar. \n\n"}
{"id": "1109.6064", "contents": "Title: A General Framework for Computing Optimal Correlated Equilibria in\n  Compact Games Abstract: We analyze the problem of computing a correlated equilibrium that optimizes\nsome objective (e.g., social welfare). Papadimitriou and Roughgarden [2008]\ngave a sufficient condition for the tractability of this problem; however, this\ncondition only applies to a subset of existing representations. We propose a\ndifferent algorithmic approach for the optimal CE problem that applies to all\ncompact representations, and give a sufficient condition that generalizes that\nof Papadimitriou and Roughgarden. In particular, we reduce the optimal CE\nproblem to the deviation-adjusted social welfare problem, a combinatorial\noptimization problem closely related to the optimal social welfare problem.\nThis framework allows us to identify new classes of games for which the optimal\nCE problem is tractable; we show that graphical polymatrix games on tree graphs\nare one example. We also study the problem of computing the optimal coarse\ncorrelated equilibrium, a solution concept closely related to CE. Using a\nsimilar approach we derive a sufficient condition for this problem, and use it\nto prove that the problem is tractable for singleton congestion games. \n\n"}
{"id": "1110.1785", "contents": "Title: Voting with Limited Information and Many Alternatives Abstract: The traditional axiomatic approach to voting is motivated by the problem of\nreconciling differences in subjective preferences. In contrast, a dominant line\nof work in the theory of voting over the past 15 years has considered a\ndifferent kind of scenario, also fundamental to voting, in which there is a\ngenuinely \"best\" outcome that voters would agree on if they only had enough\ninformation. This type of scenario has its roots in the classical Condorcet\nJury Theorem; it includes cases such as jurors in a criminal trial who all want\nto reach the correct verdict but disagree in their inferences from the\navailable evidence, or a corporate board of directors who all want to improve\nthe company's revenue, but who have different information that favors different\noptions.\n  This style of voting leads to a natural set of questions: each voter has a\n{\\em private signal} that provides probabilistic information about which option\nis best, and a central question is whether a simple plurality voting system,\nwhich tabulates votes for different options, can cause the group decision to\narrive at the correct option. We show that plurality voting is powerful enough\nto achieve this: there is a way for voters to map their signals into votes for\noptions in such a way that --- with sufficiently many voters --- the correct\noption receives the greatest number of votes with high probability. We show\nfurther, however, that any process for achieving this is inherently expensive\nin the number of voters it requires: succeeding in identifying the correct\noption with probability at least $1 - \\eta$ requires $\\Omega(n^3 \\epsilon^{-2}\n\\log \\eta^{-1})$ voters, where $n$ is the number of options and $\\epsilon$ is a\ndistributional measure of the minimum difference between the options. \n\n"}
{"id": "1111.2885", "contents": "Title: Privacy Auctions for Recommender Systems Abstract: We study a market for private data in which a data analyst publicly releases\na statistic over a database of private information. Individuals that own the\ndata incur a cost for their loss of privacy proportional to the differential\nprivacy guarantee given by the analyst at the time of the release. The analyst\nincentivizes individuals by compensating them, giving rise to a \\emph{privacy\nauction}. Motivated by recommender systems, the statistic we consider is a\nlinear predictor function with publicly known weights. The statistic can be\nviewed as a prediction of the unknown data of a new individual, based on the\ndata of individuals in the database. We formalize the trade-off between privacy\nand accuracy in this setting, and show that a simple class of estimates\nachieves an order-optimal trade-off. It thus suffices to focus on auction\nmechanisms that output such estimates. We use this observation to design a\ntruthful, individually rational, proportional-purchase mechanism under a fixed\nbudget constraint. We show that our mechanism is 5-approximate in terms of\naccuracy compared to the optimal mechanism, and that no truthful mechanism can\nachieve a $2-\\varepsilon$ approximation, for any $\\varepsilon > 0$. \n\n"}
{"id": "1111.3350", "contents": "Title: Privacy-Aware Mechanism Design Abstract: In traditional mechanism design, agents only care about the utility they\nderive from the outcome of the mechanism. We look at a richer model where\nagents also assign non-negative dis-utility to the information about their\nprivate types leaked by the outcome of the mechanism.\n  We present a new model for privacy-aware mechanism design, where we only\nassume an upper bound on the agents' loss due to leakage, as opposed to\nprevious work where a full characterization of the loss was required.\n  In this model, under a mild assumption on the distribution of how agents\nvalue their privacy, we show a generic construction of privacy-aware mechanisms\nand demonstrate its applicability to electronic polling and pricing of a\ndigital good. \n\n"}
{"id": "1111.7299", "contents": "Title: Les crashs sont rationnels Abstract: As we show by using notions of equilibrium in infinite sequential games,\ncrashes or financial escalations are rational for economic or environmental\nagents, who have a vision of an infinite world. This contradicts a picture of a\nself-regulating, wise and pacific economic world. In other words, in this\ncontext, equilibrium is not synonymous of stability. We try to draw, from this\nstatement, methodological consequences and new ways of thinking, especially in\neconomic game theory. Among those new paths, coinduction is the basis of our\nreasoning in infinite games. \n\n"}
{"id": "1112.2117", "contents": "Title: How to Lose with Least Probability Abstract: Two players alternate tossing a biased coin where the probability of getting\nheads is p. The current player is awarded alpha points for tails and alpha+beta\nfor heads. The first player reaching n points wins. For a completely unfair\ncoin the player going first certainly wins. For other coin biases, the player\ngoing first has the advantage, but the advantage depends on the coin bias. We\ncalculate the first player's advantage and the coin bias minimizing this\nadvantage. \n\n"}
{"id": "1112.2861", "contents": "Title: On $\\alpha$-roughly weighted games Abstract: Gvozdeva, Hemaspaandra, and Slinko (2011) have introduced three hierarchies\nfor simple games in order to measure the distance of a given simple game to the\nclass of (roughly) weighted voting games. Their third class\n$\\mathcal{C}_\\alpha$ consists of all simple games permitting a weighted\nrepresentation such that each winning coalition has a weight of at least 1 and\neach losing coalition a weight of at most $\\alpha$. For a given game the\nminimal possible value of $\\alpha$ is called its critical threshold value. We\ncontinue the work on the critical threshold value, initiated by Gvozdeva et\nal., and contribute some new results on the possible values for a given number\nof voters as well as some general bounds for restricted subclasses of games. A\nstrong relation beween this concept and the cost of stability, i.e. the minimum\namount of external payment to ensure stability in a coalitional game, is\nuncovered. \n\n"}
{"id": "1112.3330", "contents": "Title: Quantum strategies are better than classical in almost any XOR game Abstract: We initiate a study of random instances of nonlocal games. We show that\nquantum strategies are better than classical for almost any 2-player XOR game.\nMore precisely, for large n, the entangled value of a random 2-player XOR game\nwith n questions to every player is at least 1.21... times the classical value,\nfor 1-o(1) fraction of all 2-player XOR games. \n\n"}
{"id": "1112.3337", "contents": "Title: Search by quantum walks on two-dimensional grid without amplitude\n  amplification Abstract: We study search by quantum walk on a finite two dimensional grid. The\nalgorithm of Ambainis, Kempe, Rivosh (quant-ph/0402107) takes O(\\sqrt{N log N})\nsteps and finds a marked location with probability O(1/log N) for grid of size\n\\sqrt{N} * \\sqrt{N}. This probability is small, thus amplitude amplification is\nneeded to achieve \\Theta(1) success probability. The amplitude amplification\nadds an additional O(\\sqrt{log N}) factor to the number of steps, making it\nO(\\sqrt{N} log N).\n  In this paper, we show that despite a small probability to find a marked\nlocation, the probability to be within an O(\\sqrt{N}) neighbourhood (at an\nO(\\sqrt[4]{N}) distance) of the marked location is \\Theta(1). This allows to\nskip amplitude amplification step and leads to an O(\\sqrt{log N}) speed-up.\n  We describe the results of numerical experiments supporting this idea, and we\nprove this fact analytically. \n\n"}
{"id": "1112.3610", "contents": "Title: The Combinatorial Game Theory of Well-Tempered Scoring Games Abstract: We consider the class of \"well-tempered\" integer-valued scoring games, which\nhave the property that the parity of the length of the game is independent of\nthe line of play. We consider disjunctive sums of these games, and develop a\ntheory for them analogous to the standard theory of disjunctive sums of\nnormal-play partizan games. We show that the monoid of well-tempered scoring\ngames modulo indistinguishability is cancellative but not a group, and we\ndescribe its structure in terms of the group of normal-play partizan games. We\nalso classify Boolean-valued well-tempered scoring games, showing that there\nare exactly seventy, up to equivalence. \n\n"}
{"id": "1112.6361", "contents": "Title: On Multiple Round Sponsored Search Auctions with Budgets Abstract: In a sponsored search auction the advertisement slots on a search result page\nare generally ordered by click-through rate. Bidders have a valuation, which is\nusually assumed to be linear in the click-through rate, a budget constraint,\nand receive at most one slot per search result page (round). We study\nmulti-round sponsored search auctions, where the different rounds are linked\nthrough the budget constraints of the bidders and the valuation of a bidder for\nall rounds is the sum of the valuations for the individual rounds. All\nmechanisms published so far either study one-round sponsored search auctions or\nthe setting where every round has only one slot and all slots have the same\nclick-through rate, which is identical to a multi-item auction.\n  This paper contains the following three results: (1) We give the first\nmechanism for the multi-round sponsored search problem where different slots\nhave different click-through rates. Our mechanism is incentive compatible in\nexpectation, individually rational in expectation, Pareto optimal in\nexpectation, and also ex-post Pareto optimal for each realized outcome. (2)\nAdditionally we study the combinatorial setting, where each bidder is only\ninterested in a subset of the rounds. We give a deterministic, incentive\ncompatible, individually rational, and Pareto optimal mechanism for the setting\nwhere all slots have the same click-through rate. (3) We present an\nimpossibility result for auctions where bidders have diminishing marginal\nvaluations. Specifically, we show that even for the multi-unit (one slot per\nround) setting there is no incentive compatible, individually rational, and\nPareto optimal mechanism for private diminishing marginal valuations and public\nbudgets. \n\n"}
{"id": "1201.1776", "contents": "Title: Stochastic Loss Aversion for Random Medium Access Abstract: We consider a slotted-ALOHA LAN with loss-averse, noncooperative greedy\nusers. To avoid non-Pareto equilibria, particularly deadlock, we assume\nprobabilistic loss-averse behavior. This behavior is modeled as a modulated\nwhite noise term, in addition to the greedy term, creating a diffusion process\nmodeling the game. We observe that when player's modulate with their\nthroughput, a more efficient exploration of play-space results, and so finding\na Pareto equilibrium is more likely over a given interval of time. \n\n"}
{"id": "1201.2892", "contents": "Title: Algebraic Relaxations and Hardness Results in Polynomial Optimization\n  and Lyapunov Analysis Abstract: This thesis settles a number of questions related to computational complexity\nand algebraic, semidefinite programming based relaxations in optimization and\ncontrol. \n\n"}
{"id": "1201.3184", "contents": "Title: Partial Degree Bounded Edge Packing Problem Abstract: In [1], whether a target binary string s can be represented from a boolean\nformula with operands chosen from a set of binary strings W was studied. In\nthis paper, we first examine selecting a maximum subset X from W, so that for\nany string t in X, t is not representable by X\\{t}. We rephrase this problem as\ngraph, and surprisingly find it give rise to a broad model of edge packing\nproblem, which itself falls into the model of forbidden subgraph problem.\nSpecifically, given a graph G(V;E) and a constant c, the problem asks to choose\nas many as edges to form a subgraph G'. So that in G', for each edge, at least\none of its endpoints has degree no more than c. We call such G' partial c\ndegree bounded. When c = 1, it turns out to be the complement of dominating\nset. We present several results about hardness, approximation for the general\ngraph and efficient exact algorithm on trees. This edge packing problem model\nalso has a direct interpretation in resource allocation. There are n types of\nresources and m jobs. Each job needs two types of resources. A job can be\naccomplished if either one of its necessary resources is shared by no more than\nc other jobs. The problem then asks to nish as many jobs as possible. We\nbelieve this partial degree bounded graph problem merits more attention. \n\n"}
{"id": "1202.1089", "contents": "Title: Bargaining Dynamics in Exchange Networks Abstract: We consider a dynamical system for computing Nash bargaining solutions on\ngraphs and focus on its rate of convergence. More precisely, we analyze the\nedge-balanced dynamical system by Azar et al and fully specify its convergence\nfor an important class of elementary graph structures that arise in Kleinberg\nand Tardos' procedure for computing a Nash bargaining solution on general\ngraphs. We show that all these dynamical systems are either linear or\neventually become linear and that their convergence times are quadratic in the\nnumber of matched edges. \n\n"}
{"id": "1202.2080", "contents": "Title: Quantum Financial Economics of Games of Strategy and Financial Decisions Abstract: A quantum financial approach to finite games of strategy is addressed, with\nan extension of Nash's theorem to the quantum financial setting, allowing for\nan entanglement of games of strategy with two-period financial allocation\nproblems that are expressed in terms of: the consumption plans' optimization\nproblem in pure exchange economies and the finite-state securities market\noptimization problem, thus addressing, within the financial setting, the\ninterplay between companies' business games and financial agents' behavior.\n  A complete set of quantum Arrow-Debreu prices, resulting from the game of\nstrategy's quantum Nash equilibrium, is shown to hold, even in the absence of\nsecurities' market completeness, such that Pareto optimal results are obtained\nwithout having to assume the completeness condition that the rank of the\nsecurities' payoff matrix is equal to the number of alternative lottery states. \n\n"}
{"id": "1202.4798", "contents": "Title: Polynomial Time Algorithms for Branching Markov Decision Processes and\n  Probabilistic Min(Max) Polynomial Bellman Equations Abstract: We show that one can approximate the least fixed point solution for a\nmultivariate system of monotone probabilistic max(min) polynomial equations,\nreferred to as maxPPSs (and minPPSs, respectively), in time polynomial in both\nthe encoding size of the system of equations and in log(1/epsilon), where\nepsilon > 0 is the desired additive error bound of the solution. (The model of\ncomputation is the standard Turing machine model.) We establish this result\nusing a generalization of Newton's method which applies to maxPPSs and minPPSs,\neven though the underlying functions are only piecewise-differentiable. This\ngeneralizes our recent work which provided a P-time algorithm for purely\nprobabilistic PPSs.\n  These equations form the Bellman optimality equations for several important\nclasses of infinite-state Markov Decision Processes (MDPs). Thus, as a\ncorollary, we obtain the first polynomial time algorithms for computing to\nwithin arbitrary desired precision the optimal value vector for several classes\nof infinite-state MDPs which arise as extensions of classic, and heavily\nstudied, purely stochastic processes. These include both the problem of\nmaximizing and mininizing the termination (extinction) probability of\nmulti-type branching MDPs, stochastic context-free MDPs, and 1-exit Recursive\nMDPs.\n  Furthermore, we also show that we can compute in P-time an epsilon-optimal\npolicy for both maximizing and minimizing branching, context-free, and\n1-exit-Recursive MDPs, for any given desired epsilon > 0. This is despite the\nfact that actually computing optimal strategies is Sqrt-Sum-hard and\nPosSLP-hard in this setting.\n  We also derive, as an easy consequence of these results, an FNP upper bound\non the complexity of computing the value (within arbitrary desired precision)\nof branching simple stochastic games (BSSGs). \n\n"}
{"id": "1203.1940", "contents": "Title: Graph Pricing Problem on Bounded Treewidth, Bounded Genus and k-partite\n  graphs Abstract: Consider the following problem. A seller has infinite copies of $n$ products\nrepresented by nodes in a graph. There are $m$ consumers, each has a budget and\nwants to buy two products. Consumers are represented by weighted edges. Given\nthe prices of products, each consumer will buy both products she wants, at the\ngiven price, if she can afford to. Our objective is to help the seller price\nthe products to maximize her profit.\n  This problem is called {\\em graph vertex pricing} ({\\sf GVP}) problem and has\nresisted several recent attempts despite its current simple solution. This\nmotivates the study of this problem on special classes of graphs. In this\npaper, we study this problem on a large class of graphs such as graphs with\nbounded treewidth, bounded genus and $k$-partite graphs.\n  We show that there exists an {\\sf FPTAS} for {\\sf GVP} on graphs with bounded\ntreewidth. This result is also extended to an {\\sf FPTAS} for the more general\n{\\em single-minded pricing} problem. On bounded genus graphs we present a {\\sf\nPTAS} and show that {\\sf GVP} is {\\sf NP}-hard even on planar graphs.\n  We study the Sherali-Adams hierarchy applied to a natural Integer Program\nformulation that $(1+\\epsilon)$-approximates the optimal solution of {\\sf GVP}.\nSherali-Adams hierarchy has gained much interest recently as a possible\napproach to develop new approximation algorithms. We show that, when the input\ngraph has bounded treewidth or bounded genus, applying a constant number of\nrounds of Sherali-Adams hierarchy makes the integrality gap of this natural\n{\\sf LP} arbitrarily small, thus giving a $(1+\\epsilon)$-approximate solution\nto the original {\\sf GVP} instance.\n  On $k$-partite graphs, we present a constant-factor approximation algorithm.\nWe further improve the approximation factors for paths, cycles and graphs with\ndegree at most three. \n\n"}
{"id": "1203.2801", "contents": "Title: On Exact Algorithms for Permutation CSP Abstract: In the Permutation Constraint Satisfaction Problem (Permutation CSP) we are\ngiven a set of variables $V$ and a set of constraints C, in which constraints\nare tuples of elements of V. The goal is to find a total ordering of the\nvariables, $\\pi\\ : V \\rightarrow [1,...,|V|]$, which satisfies as many\nconstraints as possible. A constraint $(v_1,v_2,...,v_k)$ is satisfied by an\nordering $\\pi$ when $\\pi(v_1)<\\pi(v_2)<...<\\pi(v_k)$. An instance has arity $k$\nif all the constraints involve at most $k$ elements.\n  This problem expresses a variety of permutation problems including {\\sc\nFeedback Arc Set} and {\\sc Betweenness} problems. A naive algorithm, listing\nall the $n!$ permutations, requires $2^{O(n\\log{n})}$ time. Interestingly, {\\sc\nPermutation CSP} for arity 2 or 3 can be solved by Held-Karp type algorithms in\ntime $O^*(2^n)$, but no algorithm is known for arity at least 4 with running\ntime significantly better than $2^{O(n\\log{n})}$. In this paper we resolve the\ngap by showing that {\\sc Arity 4 Permutation CSP} cannot be solved in time\n$2^{o(n\\log{n})}$ unless ETH fails. \n\n"}
{"id": "1203.3967", "contents": "Title: Control Complexity in Bucklin, Fallback, and Plurality Voting: An\n  Experimental Approach Abstract: Walsh [Wal10, Wal09], Davies et al. [DKNW10, DKNW11], and Narodytska et al.\n[NWX11] studied various voting systems empirically and showed that they can\noften be manipulated effectively, despite their manipulation problems being\nNP-hard. Such an experimental approach is sorely missing for NP-hard control\nproblems, where control refers to attempts to tamper with the outcome of\nelections by adding/deleting/partitioning either voters or candidates. We\nexperimentally tackle NP-hard control problems for Bucklin and fallback voting.\nAmong natural voting systems with efficient winner determination, fallback\nvoting is currently known to display the broadest resistance to control in\nterms of NP-hardness, and Bucklin voting has been shown to behave almost as\nwell in terms of control resistance [ER10, EPR11, EFPR11]. We also investigate\ncontrol resistance experimentally for plurality voting, one of the first voting\nsystems analyzed with respect to electoral control [BTT92, HHR07]. Our findings\nindicate that NP-hard control problems can often be solved effectively in\npractice. Moreover, our experiments allow a more fine-grained analysis and\ncomparison-across various control scenarios, vote distribution models, and\nvoting systems-than merely stating NP-hardness for all these control problems. \n\n"}
{"id": "1203.4455", "contents": "Title: Budget Feasible Mechanism Design: From Prior-Free to Bayesian Abstract: Budget feasible mechanism design studies procurement combinatorial auctions\nwhere the sellers have private costs to produce items, and the\nbuyer(auctioneer) aims to maximize a social valuation function on subsets of\nitems, under the budget constraint on the total payment. One of the most\nimportant questions in the field is \"which valuation domains admit truthful\nbudget feasible mechanisms with `small' approximations (compared to the social\noptimum)?\" Singer showed that additive and submodular functions have such\nconstant approximations. Recently, Dobzinski, Papadimitriou, and Singer gave an\nO(log^2 n)-approximation mechanism for subadditive functions; they also\nremarked that: \"A fundamental question is whether, regardless of computational\nconstraints, a constant-factor budget feasible mechanism exists for subadditive\nfunctions.\"\n  We address this question from two viewpoints: prior-free worst case analysis\nand Bayesian analysis. For the prior-free framework, we use an LP that\ndescribes the fractional cover of the valuation function; it is also connected\nto the concept of approximate core in cooperative game theory. We provide an\nO(I)-approximation mechanism for subadditive functions, via the worst case\nintegrality gap I of LP. This implies an O(log n)-approximation for subadditive\nvaluations, O(1)-approximation for XOS valuations, and for valuations with a\nconstant I. XOS valuations are an important class of functions that lie between\nsubmodular and subadditive classes. We give another polynomial time O(log\nn/loglog n) sub-logarithmic approximation mechanism for subadditive valuations.\n  For the Bayesian framework, we provide a constant approximation mechanism for\nall subadditive functions, using the above prior-free mechanism for XOS\nvaluations as a subroutine. Our mechanism allows correlations in the\ndistribution of private information and is universally truthful. \n\n"}
{"id": "1204.3283", "contents": "Title: Automated synthesis of reliable and efficient systems through game\n  theory: a case study Abstract: Reactive computer systems bear inherent complexity due to continuous\ninteractions with their environment. While this environment often proves to be\nuncontrollable, we still want to ensure that critical computer systems will not\nfail, no matter what they face. Examples are legion: railway traffic, power\nplants, plane navigation systems, etc. Formal verification of a system may\nensure that it satisfies a given specification, but only applies to an already\nexisting model of a system. In this work, we address the problem of synthesis:\nstarting from a specification of the desired behavior, we show how to build a\nsuitable system controller that will enforce this specification. In particular,\nwe discuss recent developments of that approach for systems that must ensure\nBoolean behaviors (e.g., reachability, liveness) along with quantitative\nrequirements over their execution (e.g., never drop out of fuel, ensure a\nsuitable mean response time). We notably illustrate a powerful, practically\nuseable algorithm for the automated synthesis of provably safe reactive\nsystems. \n\n"}
{"id": "1204.3494", "contents": "Title: Optimizing Scrip Systems: Crashes, Altruists, Hoarders, Sybils and\n  Collusion Abstract: Scrip, or artificial currency, is a useful tool for designing systems that\nare robust to selfish behavior by users. However, it also introduces problems\nfor a system designer, such as how the amount of money in the system should be\nset. In this paper, the effect of varying the total amount of money in a scrip\nsystem on efficiency (i.e., social welfare---the total utility of all the\nagents in the system) is analyzed, and it is shown that by maintaining the\nappropriate ratio between the total amount of money and the number of agents,\nefficiency is maximized. This ratio can be found by increasing the money supply\nto just below the point that the system would experience a \"monetary crash,\"\nwhere money is sufficiently devalued that no agent is willing to perform a\nservice. The implications of the presence of altruists, hoarders, sybils, and\ncollusion on the performance of the system are examined. Approaches are\ndiscussed to identify the strategies and types of agents. \n\n"}
{"id": "1204.4031", "contents": "Title: Approximately Optimal Auctions for Selling Privacy when Costs are\n  Correlated with Data Abstract: We consider a scenario in which a database stores sensitive data of users and\nan analyst wants to estimate statistics of the data. The users may suffer a\ncost when their data are used in which case they should be compensated. The\nanalyst wishes to get an accurate estimate, while the users want to maximize\ntheir utility. We want to design a mechanism that can estimate statistics\naccurately without compromising users' privacy.\n  Since users' costs and sensitive data may be correlated, it is important to\nprotect the privacy of both data and cost. We model this correlation by\nassuming that a user's unknown sensitive data determines a distribution from a\nset of publicly known distributions and a user's cost is drawn from that\ndistribution. We propose a stronger model of privacy preserving mechanism where\nusers are compensated whenever they reveal information about their data to the\nmechanism. In this model, we design a Bayesian incentive compatible and privacy\npreserving mechanism that guarantees accuracy and protects the privacy of both\ncost and data. \n\n"}
{"id": "1204.4145", "contents": "Title: Learning From An Optimization Viewpoint Abstract: In this dissertation we study statistical and online learning problems from\nan optimization viewpoint.The dissertation is divided into two parts :\n  I. We first consider the question of learnability for statistical learning\nproblems in the general learning setting. The question of learnability is well\nstudied and fully characterized for binary classification and for real valued\nsupervised learning problems using the theory of uniform convergence. However\nwe show that for the general learning setting uniform convergence theory fails\nto characterize learnability. To fill this void we use stability of learning\nalgorithms to fully characterize statistical learnability in the general\nsetting. Next we consider the problem of online learning. Unlike the\nstatistical learning framework there is a dearth of generic tools that can be\nused to establish learnability and rates for online learning problems in\ngeneral. We provide online analogs to classical tools from statistical learning\ntheory like Rademacher complexity, covering numbers, etc. We further use these\ntools to fully characterize learnability for online supervised learning\nproblems.\n  II. In the second part, for general classes of convex learning problems, we\nprovide appropriate mirror descent (MD) updates for online and statistical\nlearning of these problems. Further, we show that the the MD is near optimal\nfor online convex learning and for most cases, is also near optimal for\nstatistical convex learning. We next consider the problem of convex\noptimization and show that oracle complexity can be lower bounded by the so\ncalled fat-shattering dimension of the associated linear class. Thus we\nestablish a strong connection between offline convex optimization problems and\nstatistical learning problems. We also show that for a large class of high\ndimensional optimization problems, MD is in fact near optimal even for convex\noptimization. \n\n"}
{"id": "1204.5213", "contents": "Title: Solving Weighted Voting Game Design Problems Optimally: Representations,\n  Synthesis, and Enumeration Abstract: We study the inverse power index problem for weighted voting games: the\nproblem of finding a weighted voting game in which the power of the players is\nas close as possible to a certain target distribution. Our goal is to find\nalgorithms that solve this problem exactly. Thereto, we study various\nsubclasses of simple games, and their associated representation methods. We\nsurvey algorithms and impossibility results for the synthesis problem, i.e.,\nconverting a representation of a simple game into another representation.\n  We contribute to the synthesis problem by showing that it is impossible to\ncompute in polynomial time the list of ceiling coalitions (also known as\nshift-maximal losing coalitions) of a game from its list of roof coalitions\n(also known as shift-minimal winning coalitions), and vice versa.\n  Then, we proceed by studying the problem of enumerating the set of weighted\nvoting games. We present first a naive algorithm for this, running in doubly\nexponential time. Using our knowledge of the synthesis problem, we then improve\non this naive algorithm, and we obtain an enumeration algorithm that runs in\nquadratic exponential time (that is, O(2^(n^2) p(n)) for a polynomial p).\nMoreover, we show that this algorithm runs in output-polynomial time, making it\nthe best possible enumeration algorithm up to a polynomial factor.\n  Finally, we propose an exact anytime algorithm for the inverse power index\nproblem that runs in exponential time. This algorithm is straightforward and\ngeneral: it computes the error for each game enumerated, and outputs the game\nthat minimizes this error. By the genericity of our approach, our algorithm can\nbe used to find a weighted voting game that optimizes any exponential time\ncomputable function. We implement our algorithm for the case of the normalized\nBanzhaf index, and we perform experiments in order to study performance and\nerror convergence. \n\n"}
{"id": "1204.6552", "contents": "Title: A Game-Theoretic Model Motivated by the DARPA Network Challenge Abstract: In this paper we propose a game-theoretic model to analyze events similar to\nthe 2009 \\emph{DARPA Network Challenge}, which was organized by the Defense\nAdvanced Research Projects Agency (DARPA) for exploring the roles that the\nInternet and social networks play in incentivizing wide-area collaborations.\nThe challenge was to form a group that would be the first to find the locations\nof ten moored weather balloons across the United States. We consider a model in\nwhich $N$ people (who can form groups) are located in some topology with a\nfixed coverage volume around each person's geographical location. We consider\nvarious topologies where the players can be located such as the Euclidean\n$d$-dimension space and the vertices of a graph. A balloon is placed in the\nspace and a group wins if it is the first one to report the location of the\nballoon. A larger team has a higher probability of finding the balloon, but we\nassume that the prize money is divided equally among the team members. Hence\nthere is a competing tension to keep teams as small as possible.\n  \\emph{Risk aversion} is the reluctance of a person to accept a bargain with\nan uncertain payoff rather than another bargain with a more certain, but\npossibly lower, expected payoff. In our model we consider the \\emph{isoelastic}\nutility function derived from the Arrow-Pratt measure of relative risk\naversion. The main aim is to analyze the structures of the groups in Nash\nequilibria for our model. For the $d$-dimensional Euclidean space ($d\\geq 1$)\nand the class of bounded degree regular graphs we show that in any Nash\nEquilibrium the \\emph{richest} group (having maximum expected utility per\nperson) covers a constant fraction of the total volume. \n\n"}
{"id": "1205.1183", "contents": "Title: On the Complexity of Trial and Error Abstract: Motivated by certain applications from physics, biochemistry, economics, and\ncomputer science, in which the objects under investigation are not accessible\nbecause of various limitations, we propose a trial-and-error model to examine\nalgorithmic issues in such situations. Given a search problem with a hidden\ninput, we are asked to find a valid solution, to find which we can propose\ncandidate solutions (trials), and use observed violations (errors), to prepare\nfuture proposals. In accordance with our motivating applications, we consider\nthe fairly broad class of constraint satisfaction problems, and assume that\nerrors are signaled by a verification oracle in the format of the index of a\nviolated constraint (with the content of the constraint still hidden).\n  Our discoveries are summarized as follows. On one hand, despite the seemingly\nvery little information provided by the verification oracle, efficient\nalgorithms do exist for a number of important problems. For the Nash, Core,\nStable Matching, and SAT problems, the unknown-input versions are as hard as\nthe corresponding known-input versions, up to a factor of polynomial. We\nfurther give almost tight bounds on the latter two problems' trial\ncomplexities. On the other hand, there are problems whose complexities are\nsubstantially increased in the unknown-input model. In particular, no\ntime-efficient algorithms exist (under standard hardness assumptions) for Graph\nIsomorphism and Group Isomorphism problems. The tools used to achieve these\nresults include order theory, strong ellipsoid method, and some non-standard\nreductions.\n  Our model investigates the value of information, and our results demonstrate\nthat the lack of input information can introduce various levels of extra\ndifficulty. The model exhibits intimate connections with (and we hope can also\nserve as a useful supplement to) certain existing learning and complexity\ntheories. \n\n"}
{"id": "1205.1670", "contents": "Title: Rainbow Colouring of Split and Threshold Graphs Abstract: A rainbow colouring of a connected graph is a colouring of the edges of the\ngraph, such that every pair of vertices is connected by at least one path in\nwhich no two edges are coloured the same. Such a colouring using minimum\npossible number of colours is called an optimal rainbow colouring, and the\nminimum number of colours required is called the rainbow connection number of\nthe graph. In this article, we show the following:\n  1. The problem of deciding whether a graph can be rainbow coloured using 3\ncolours remains NP-complete even when restricted to the class of split graphs.\nHowever, any split graph can be rainbow coloured in linear time using at most\none more colour than the optimum.\n  2. For every integer k larger than 2, the problem of deciding whether a graph\ncan be rainbow coloured using k colours remains NP-complete even when\nrestricted to the class of chordal graphs.\n  3. For every positive integer k, threshold graphs with rainbow connection\nnumber k can be characterised based on their degree sequence alone. Further, we\ncan optimally rainbow colour a threshold graph in linear time. \n\n"}
{"id": "1205.2074", "contents": "Title: A Smooth Transition from Powerlessness to Absolute Power Abstract: We study the phase transition of the coalitional manipulation problem for\ngeneralized scoring rules. Previously it has been shown that, under some\nconditions on the distribution of votes, if the number of manipulators is\n$o(\\sqrt{n})$, where $n$ is the number of voters, then the probability that a\nrandom profile is manipulable by the coalition goes to zero as the number of\nvoters goes to infinity, whereas if the number of manipulators is\n$\\omega(\\sqrt{n})$, then the probability that a random profile is manipulable\ngoes to one. Here we consider the critical window, where a coalition has size\n$c\\sqrt{n}$, and we show that as $c$ goes from zero to infinity, the limiting\nprobability that a random profile is manipulable goes from zero to one in a\nsmooth fashion, i.e., there is a smooth phase transition between the two\nregimes. This result analytically validates recent empirical results, and\nsuggests that deciding the coalitional manipulation problem may be of limited\ncomputational hardness in practice. \n\n"}
{"id": "1205.2152", "contents": "Title: Roughly Weighted Hierarchical Simple Games Abstract: Hierarchical simple games - both disjunctive and conjunctive - are natural\ngeneralizations of simple majority games. They take their origin in the theory\nof secret sharing. Another important generalization of simple majority games\nwith origin in economics and politics are weighted and roughly weighted\nmajority games. In this paper we characterize roughly weighted hierarchical\ngames identifying where the two approaches coincide. \n\n"}
{"id": "1205.4104", "contents": "Title: Combinatorial Auctions with Restricted Complements Abstract: Complements between goods - where one good takes on added value in the\npresence of another - have been a thorn in the side of algorithmic mechanism\ndesigners. On the one hand, complements are common in the standard motivating\napplications for combinatorial auctions, like spectrum license auctions. On the\nother, welfare maximization in the presence of complements is notoriously\ndifficult, and this intractability has stymied theoretical progress in the\narea. For example, there are no known positive results for combinatorial\nauctions in which bidder valuations are multi-parameter and\nnon-complement-free, other than the relatively weak results known for general\nvaluations.\n  To make inroads on the problem of combinatorial auction design in the\npresence of complements, we propose a model for valuations with complements\nthat is parameterized by the \"size\" of the complements. A valuation in our\nmodel is represented succinctly by a weighted hypergraph, where the size of the\nhyper-edges corresponds to degree of complementarity. Our model permits a\nvariety of computationally efficient queries, and non-trivial\nwelfare-maximization algorithms and mechanisms.\n  We design the following polynomial-time approximation algorithms and truthful\nmechanisms for welfare maximization with bidders with hypergraph valuations.\n  1- For bidders whose valuations correspond to subgraphs of a known graph that\nis planar (or more generally, excludes a fixed minor), we give a truthful and\n(1+epsilon)-approximate mechanism.\n  2- We give a polynomial-time, r-approximation algorithm for welfare\nmaximization with hypergraph-r valuations. Our algorithm randomly rounds a\ncompact linear programming relaxation of the problem.\n  3- We design a different approximation algorithm and use it to give a\npolynomial-time, truthful-in-expectation mechanism that has an approximation\nfactor of O(log^r m). \n\n"}
{"id": "1205.4484", "contents": "Title: Hypercontractivity, Sum-of-Squares Proofs, and their Applications Abstract: We study the computational complexity of approximating the 2->q norm of\nlinear operators (defined as ||A||_{2->q} = sup_v ||Av||_q/||v||_2), as well as\nconnections between this question and issues arising in quantum information\ntheory and the study of Khot's Unique Games Conjecture (UGC). We show the\nfollowing:\n  1. For any constant even integer q>=4, a graph $G$ is a \"small-set expander\"\nif and only if the projector into the span of the top eigenvectors of G's\nadjacency matrix has bounded 2->q norm. As a corollary, a good approximation to\nthe 2->q norm will refute the Small-Set Expansion Conjecture--a close variant\nof the UGC. We also show that such a good approximation can be obtained in\nexp(n^(2/q)) time, thus obtaining a different proof of the known subexponential\nalgorithm for Small Set Expansion.\n  2. Constant rounds of the \"Sum of Squares\" semidefinite programing hierarchy\ncertify an upper bound on the 2->4 norm of the projector to low-degree\npolynomials over the Boolean cube, as well certify the unsatisfiability of the\n\"noisy cube\" and \"short code\" based instances of Unique Games considered by\nprior works. This improves on the previous upper bound of exp(poly log n)\nrounds (for the \"short code\"), as well as separates the \"Sum of\nSquares\"/\"Lasserre\" hierarchy from weaker hierarchies that were known to\nrequire omega(1) rounds.\n  3. We show reductions between computing the 2->4 norm and computing the\ninjective tensor norm of a tensor, a problem with connections to quantum\ninformation theory. Three corollaries are: (i) the 2->4 norm is NP-hard to\napproximate to precision inverse-polynomial in the dimension, (ii) the 2->4\nnorm does not have a good approximation (in the sense above) unless 3-SAT can\nbe solved in time exp(sqrt(n) polylog(n)), and (iii) known algorithms for the\nquantum separability problem imply a non-trivial additive approximation for the\n2->4 norm. \n\n"}
{"id": "1205.4605", "contents": "Title: No Sublogarithmic-time Approximation Scheme for Bipartite Vertex Cover Abstract: K\\\"onig's theorem states that on bipartite graphs the size of a maximum\nmatching equals the size of a minimum vertex cover. It is known from prior work\nthat for every \\epsilon > 0 there exists a constant-time distributed algorithm\nthat finds a (1+\\epsilon)-approximation of a maximum matching on 2-coloured\ngraphs of bounded degree. In this work, we show---somewhat surprisingly---that\nno sublogarithmic-time approximation scheme exists for the dual problem: there\nis a constant \\delta > 0 so that no randomised distributed algorithm with\nrunning time o(\\log n) can find a (1+\\delta)-approximation of a minimum vertex\ncover on 2-coloured graphs of maximum degree 3. In fact, a simple application\nof the Linial--Saks (1993) decomposition demonstrates that this lower bound is\ntight.\n  Our lower-bound construction is simple and, to some extent, independent of\nprevious techniques. Along the way we prove that a certain cut minimisation\nproblem, which might be of independent interest, is hard to approximate locally\non expander graphs. \n\n"}
{"id": "1206.0981", "contents": "Title: An Informed Model of Personal Information Release in Social Networking\n  Sites Abstract: The emergence of online social networks and the growing popularity of digital\ncommunication has resulted in an increasingly amount of information about\nindividuals available on the Internet. Social network users are given the\nfreedom to create complex digital identities, and enrich them with truthful or\neven fake personal information. However, this freedom has led to serious\nsecurity and privacy incidents, due to the role users' identities play in\nestablishing social and privacy settings.\n  In this paper, we take a step toward a better understanding of online\ninformation exposure. Based on the detailed analysis of a sample of real-world\ndata, we develop a deception model for online users. The model uses a game\ntheoretic approach to characterizing a user's willingness to release, withhold\nor lie about information depending on the behavior of individuals within the\nuser's circle of friends. In the model, we take into account both the\nheterogeneous nature of users and their different attitudes, as well as the\ndifferent types of information they may expose online. \n\n"}
{"id": "1206.2957", "contents": "Title: Mechanisms for Risk Averse Agents, Without Loss Abstract: Auctions in which agents' payoffs are random variables have received\nincreased attention in recent years. In particular, recent work in algorithmic\nmechanism design has produced mechanisms employing internal randomization,\npartly in response to limitations on deterministic mechanisms imposed by\ncomputational complexity. For many of these mechanisms, which are often\nreferred to as truthful-in-expectation, incentive compatibility is contingent\non the assumption that agents are risk-neutral. These mechanisms have been\ncriticized on the grounds that this assumption is too strong, because \"real\"\nagents are typically risk averse, and moreover their precise attitude towards\nrisk is typically unknown a-priori. In response, researchers in algorithmic\nmechanism design have sought the design of universally-truthful mechanisms ---\nmechanisms for which incentive-compatibility makes no assumptions regarding\nagents' attitudes towards risk.\n  We show that any truthful-in-expectation mechanism can be generically\ntransformed into a mechanism that is incentive compatible even when agents are\nrisk averse, without modifying the mechanism's allocation rule. The transformed\nmechanism does not require reporting of agents' risk profiles. Equivalently,\nour result can be stated as follows: Every (randomized) allocation rule that is\nimplementable in dominant strategies when players are risk neutral is also\nimplementable when players are endowed with an arbitrary and unknown concave\nutility function for money. \n\n"}
{"id": "1206.4181", "contents": "Title: Higher Order Game Dynamics Abstract: Continuous-time game dynamics are typically first order systems where payoffs\ndetermine the growth rate of the players' strategy shares. In this paper, we\ninvestigate what happens beyond first order by viewing payoffs as higher order\nforces of change, specifying e.g. the acceleration of the players' evolution\ninstead of its velocity (a viewpoint which emerges naturally when it comes to\naggregating empirical data of past instances of play). To that end, we derive a\nwide class of higher order game dynamics, generalizing first order imitative\ndynamics, and, in particular, the replicator dynamics. We show that strictly\ndominated strategies become extinct in n-th order payoff-monotonic dynamics n\norders as fast as in the corresponding first order dynamics; furthermore, in\nstark contrast to first order, weakly dominated strategies also become extinct\nfor n>1. All in all, higher order payoff-monotonic dynamics lead to the\nelimination of weakly dominated strategies, followed by the iterated deletion\nof strictly dominated strategies, thus providing a dynamic justification of the\nwell-known epistemic rationalizability process of Dekel and Fudenberg (1990).\nFinally, we also establish a higher order analogue of the folk theorem of\nevolutionary game theory, and we show that con- vergence to strict equilibria\nin n-th order dynamics is n orders as fast as in first order. \n\n"}
{"id": "1206.5941", "contents": "Title: Kernelization Lower Bounds By Cross-Composition Abstract: We introduce the cross-composition framework for proving kernelization lower\nbounds. A classical problem L AND/OR-cross-composes into a parameterized\nproblem Q if it is possible to efficiently construct an instance of Q with\npolynomially bounded parameter value that expresses the logical AND or OR of a\nsequence of instances of L. Building on work by Bodlaender et al. (ICALP 2008)\nand using a result by Fortnow and Santhanam (STOC 2008) with a refinement by\nDell and van Melkebeek (STOC 2010), we show that if an NP-hard problem\nOR-cross-composes into a parameterized problem Q then Q does not admit a\npolynomial kernel unless NP \\subseteq coNP/poly and the polynomial hierarchy\ncollapses. Similarly, an AND-cross-composition for Q rules out polynomial\nkernels for Q under Bodlaender et al.'s AND-distillation conjecture.\n  Our technique generalizes and strengthens the recent techniques of using\ncomposition algorithms and of transferring the lower bounds via polynomial\nparameter transformations. We show its applicability by proving kernelization\nlower bounds for a number of important graphs problems with structural\n(non-standard) parameterizations, e.g., Clique, Chromatic Number, Weighted\nFeedback Vertex Set, and Weighted Odd Cycle Transversal do not admit polynomial\nkernels with respect to the vertex cover number of the input graphs unless the\npolynomial hierarchy collapses, contrasting the fact that these problems are\ntrivially fixed-parameter tractable for this parameter.\n  After learning of our results, several teams of authors have successfully\napplied the cross-composition framework to different parameterized problems.\nFor completeness, our presentation of the framework includes several extensions\nbased on this follow-up work. For example, we show how a relaxed version of\nOR-cross-compositions may be used to give lower bounds on the degree of the\npolynomial in the kernel size. \n\n"}
{"id": "1207.0036", "contents": "Title: The Kullback-Leibler Divergence as a Lyapunov Function for Incentive\n  Based Game Dynamics Abstract: It has been shown that the Kullback-Leibler divergence is a Lyapunov function\nfor the replicator equations at evolutionary stable states, or ESS. In this\npaper we extend the result to a more general class of game dynamics. As a\nresult, sufficient conditions can be given for the asymptotic stability of rest\npoints for the entire class of incentive dynamics. The previous known results\nwill be can be shown as corollaries to the main theorem. \n\n"}
{"id": "1207.4044", "contents": "Title: Designing Information Revelation and Intervention with an Application to\n  Flow Control Abstract: There are many familiar situations in which a manager seeks to design a\nsystem in which users share a resource, but outcomes depend on the information\nheld and actions taken by users. If communication is possible, the manager can\nask users to report their private information and then, using this information,\ninstruct them on what actions they should take. If the users are compliant,\nthis reduces the manager's optimization problem to a well-studied problem of\noptimal control. However, if the users are self-interested and not compliant,\nthe problem is much more complicated: when asked to report their private\ninformation, the users might lie; upon receiving instructions, the users might\ndisobey. Here we ask whether the manager can design the system to get around\nboth of these difficulties. To do so, the manager must provide for the users\nthe incentives to report truthfully and to follow the instructions, despite the\nfact that the users are self-interested. For a class of environments that\nincludes many resource allocation games in communication networks, we provide\ntools for the manager to design an efficient system. In addition to reports and\nrecommendations, the design we employ allows the manager to intervene in the\nsystem after the users take actions. In an abstracted environment, we find\nconditions under which the manager can achieve the same outcome it could if\nusers were compliant, and conditions under which it does not. We then apply our\nframework and results to design a flow control management system. \n\n"}
{"id": "1207.4084", "contents": "Title: Mechanism Design in Large Games: Incentives and Privacy Abstract: We study the problem of implementing equilibria of complete information games\nin settings of incomplete information, and address this problem using\n\"recommender mechanisms.\" A recommender mechanism is one that does not have the\npower to enforce outcomes or to force participation, rather it only has the\npower to suggestion outcomes on the basis of voluntary participation. We show\nthat despite these restrictions, recommender mechanisms can implement\nequilibria of complete information games in settings of incomplete information\nunder the condition that the game is large---i.e. that there are a large number\nof players, and any player's action affects any other's payoff by at most a\nsmall amount.\n  Our result follows from a novel application of differential privacy. We show\nthat any algorithm that computes a correlated equilibrium of a complete\ninformation game while satisfying a variant of differential privacy---which we\ncall joint differential privacy---can be used as a recommender mechanism while\nsatisfying our desired incentive properties. Our main technical result is an\nalgorithm for computing a correlated equilibrium of a large game while\nsatisfying joint differential privacy.\n  Although our recommender mechanisms are designed to satisfy game-theoretic\nproperties, our solution ends up satisfying a strong privacy property as well.\nNo group of players can learn \"much\" about the type of any player outside the\ngroup from the recommendations of the mechanism, even if these players collude\nin an arbitrary way. As such, our algorithm is able to implement equilibria of\ncomplete information games, without revealing information about the realized\ntypes. \n\n"}
{"id": "1207.5211", "contents": "Title: Can Quantum Communication Speed Up Distributed Computation? Abstract: The focus of this paper is on {\\em quantum distributed} computation, where we\ninvestigate whether quantum communication can help in {\\em speeding up}\ndistributed network algorithms. Our main result is that for certain fundamental\nnetwork problems such as minimum spanning tree, minimum cut, and shortest\npaths, quantum communication {\\em does not} help in substantially speeding up\ndistributed algorithms for these problems compared to the classical setting.\n  In order to obtain this result, we extend the technique of Das Sarma et al.\n[SICOMP 2012] to obtain a uniform approach to prove non-trivial lower bounds\nfor quantum distributed algorithms for several graph optimization (both exact\nand approximate versions) as well as verification problems, some of which are\nnew even in the classical setting, e.g. tight randomized lower bounds for\nHamiltonian cycle and spanning tree verification, answering an open problem of\nDas Sarma et al., and a lower bound in terms of the weight aspect ratio,\nmatching the upper bounds of Elkin [STOC 2004]. Our approach introduces the\n{\\em Server model} and {\\em Quantum Simulation Theorem} which together provide\na connection between distributed algorithms and communication complexity. The\nServer model is the standard two-party communication complexity model augmented\nwith additional power; yet, most of the hardness in the two-party model is\ncarried over to this new model. The Quantum Simulation Theorem carries this\nhardness further to quantum distributed computing. Our techniques, except the\nproof of the hardness in the Server model, require very little knowledge in\nquantum computing, and this can help overcoming a usual impediment in proving\nbounds on quantum distributed algorithms. \n\n"}
{"id": "1207.6760", "contents": "Title: Incentive Mechanisms based on Minority Game in Heterogeneous DTNs Abstract: In this paper we design an incentive mechanism for heterogeneous Delay\nTolerant Networks (DTNs). The proposed mechanism tackles a core problem of such\nsystems: how to induce coordination of DTN relays in order to achieve a target\nperformance figure, e.g., delivery probability or end-to-end delay, under a\ngiven constraint in term of network resources, e.g., number of active nodes or\nenergy consumption. Also, we account for the realistic case when the cost for\ntaking part in the forwarding process varies with the devices' technology or\nthe users' habits. Finally, the scheme is truly applicable to DTNs since it\nworks with no need for end-to-end connectivity.\n  In this context, we first introduce the basic coordination mechanism\nleveraging the notion of a Minority Game. In this game, relays compete to be in\nthe population minority and their utility is defined in combination with a\nrewarding mechanism. The rewards in turn configure as a control by which the\nnetwork operator controls the desired operating point for the DTN. To this aim,\nwe provide a full characterization of the equilibria of the game in the case of\nheterogeneous DTNs. Finally, a learning algorithm based on stochastic\napproximations provably drives the system to the equilibrium solution without\nrequiring perfect state information at relay nodes or at the source node and\nwithout using end-to-end communications to implement the rewarding scheme. We\nprovide extensive numerical results to validate the proposed scheme. \n\n"}
{"id": "1208.3994", "contents": "Title: Coordination in Network Security Games: a Monotone Comparative Statics\n  Approach Abstract: Malicious softwares or malwares for short have become a major security\nthreat. While originating in criminal behavior, their impact are also\ninfluenced by the decisions of legitimate end users. Getting agents in the\nInternet, and in networks in general, to invest in and deploy security features\nand protocols is a challenge, in particular because of economic reasons arising\nfrom the presence of network externalities.\n  In this paper, we focus on the question of incentive alignment for agents of\na large network towards a better security. We start with an economic model for\na single agent, that determines the optimal amount to invest in protection. The\nmodel takes into account the vulnerability of the agent to a security breach\nand the potential loss if a security breach occurs. We derive conditions on the\nquality of the protection to ensure that the optimal amount spent on security\nis an increasing function of the agent's vulnerability and potential loss. We\nalso show that for a large class of risks, only a small fraction of the\nexpected loss should be invested.\n  Building on these results, we study a network of interconnected agents\nsubject to epidemic risks. We derive conditions to ensure that the incentives\nof all agents are aligned towards a better security. When agents are strategic,\nwe show that security investments are always socially inefficient due to the\nnetwork externalities. Moreover alignment of incentives typically implies a\ncoordination problem, leading to an equilibrium with a very high price of\nanarchy. \n\n"}
{"id": "1208.4589", "contents": "Title: Road Pricing for Spreading Peak Travel: Modeling and Design Abstract: A case study of the Singapore road network provides empirical evidence that\nroad pricing can significantly affect commuter trip timing behaviors. In this\npaper, we propose a model of trip timing decisions that reasonably matches the\nobserved commuters' behaviors. Our model explicitly captures the difference in\nindividuals' sensitivity to price, travel time and early or late arrival at\ndestination. New pricing schemes are suggested to better spread peak travel and\nreduce traffic congestion. Simulation results based on the proposed model are\nprovided in comparison with the real data for the Singapore case study. \n\n"}
{"id": "1208.5076", "contents": "Title: Opinion Dynamics in Social Networks: A Local Interaction Game with\n  Stubborn Agents Abstract: The process by which new ideas, innovations, and behaviors spread through a\nlarge social network can be thought of as a networked interaction game: Each\nagent obtains information from certain number of agents in his friendship\nneighborhood, and adapts his idea or behavior to increase his benefit. In this\npaper, we are interested in how opinions, about a certain topic, form in social\nnetworks. We model opinions as continuous scalars ranging from 0 to 1 with 1(0)\nrepresenting extremely positive(negative) opinion. Each agent has an initial\nopinion and incurs some cost depending on the opinions of his neighbors, his\ninitial opinion, and his stubbornness about his initial opinion. Agents\niteratively update their opinions based on their own initial opinions and\nobserving the opinions of their neighbors. The iterative update of an agent can\nbe viewed as a myopic cost-minimization response (i.e., the so-called best\nresponse) to the others' actions. We study whether an equilibrium can emerge as\na result of such local interactions and how such equilibrium possibly depends\non the network structure, initial opinions of the agents, and the location of\nstubborn agents and the extent of their stubbornness. We also study the\nconvergence speed to such equilibrium and characterize the convergence time as\na function of aforementioned factors. We also discuss the implications of such\nresults in a few well-known graphs such as Erdos-Renyi random graphs and\nsmall-world graphs. \n\n"}
{"id": "1209.4703", "contents": "Title: Simultaneous Auctions are (almost) Efficient Abstract: Simultaneous item auctions are simple procedures for allocating items to\nbidders with potentially complex preferences over different item sets. In a\nsimultaneous auction, every bidder submits bids on all items simultaneously.\nThe allocation and prices are then resolved for each item separately, based\nsolely on the bids submitted on that item. Such procedures occur in practice\n(e.g. eBay) but are not truthful. We study the efficiency of Bayesian Nash\nequilibrium (BNE) outcomes of simultaneous first- and second-price auctions\nwhen bidders have complement-free (a.k.a. subadditive) valuations. We show that\nthe expected social welfare of any BNE is at least 1/2 of the optimal social\nwelfare in the case of first-price auctions, and at least 1/4 in the case of\nsecond-price auctions. These results improve upon the previously-known\nlogarithmic bounds, which were established by [Hassidim, Kaplan, Mansour and\nNisan '11] for first-price auctions and by [Bhawalkar and Roughgarden '11] for\nsecond-price auctions. \n\n"}
{"id": "1210.0268", "contents": "Title: Two Species Evolutionary Game Model of User and Moderator Dynamics Abstract: We construct a two species evolutionary game model of an online society\nconsisting of ordinary users and behavior enforcers (moderators). Among\nthemselves, moderators play a coordination game choosing between being\n\"positive\" or \"negative\" (or harsh) while ordinary users play prisoner's\ndilemma. When interacting, moderators motivate good behavior (cooperation)\namong the users through punitive actions while the moderators themselves are\nencouraged or discouraged in their strategic choice by these interactions. We\nshow the following results: (i) We show that the $\\omega$-limit set of the\nproposed system is sensitive both to the degree of punishment and the\nproportion of moderators in closed form. (ii) We demonstrate that the basin of\nattraction for the Pareto optimal strategy $(\\text{Cooperate},\\text{Positive})$\ncan be computed exactly. (iii) We demonstrate that for certain initial\nconditions the system is self-regulating. These results partially explain the\nstability of many online users communities such as Reddit. We illustrate our\nresults with examples from this online system. \n\n"}
{"id": "1210.2457", "contents": "Title: Down the Borel Hierarchy: Solving Muller Games via Safety Games Abstract: We transform a Muller game with n vertices into a safety game with (n!)^3\nvertices whose solution allows to determine the winning regions of the Muller\ngame and to compute a finite-state winning strategy for one player. This yields\na novel antichain-based memory structure and a natural notion of permissive\nstrategies for Muller games. Moreover, we generalize our construction by\npresenting a new type of game reduction from infinite games to safety games and\nshow its applicability to several other winning conditions. \n\n"}
{"id": "1210.3277", "contents": "Title: Shortest, Fastest, and Foremost Broadcast in Dynamic Networks Abstract: Highly dynamic networks rarely offer end-to-end connectivity at a given time.\nYet, connectivity in these networks can be established over time and space,\nbased on temporal analogues of multi-hop paths (also called {\\em journeys}).\nAttempting to optimize the selection of the journeys in these networks\nnaturally leads to the study of three cases: shortest (minimum hop), fastest\n(minimum duration), and foremost (earliest arrival) journeys. Efficient\ncentralized algorithms exists to compute all cases, when the full knowledge of\nthe network evolution is given.\n  In this paper, we study the {\\em distributed} counterparts of these problems,\ni.e. shortest, fastest, and foremost broadcast with termination detection\n(TDB), with minimal knowledge on the topology.\n  We show that the feasibility of each of these problems requires distinct\nfeatures on the evolution, through identifying three classes of dynamic graphs\nwherein the problems become gradually feasible: graphs in which the\nre-appearance of edges is {\\em recurrent} (class R), {\\em bounded-recurrent}\n(B), or {\\em periodic} (P), together with specific knowledge that are\nrespectively $n$ (the number of nodes), $\\Delta$ (a bound on the recurrence\ntime), and $p$ (the period). In these classes it is not required that all pairs\nof nodes get in contact -- only that the overall {\\em footprint} of the graph\nis connected over time.\n  Our results, together with the strict inclusion between $P$, $B$, and $R$,\nimplies a feasibility order among the three variants of the problem, i.e.\nTDB[foremost] requires weaker assumptions on the topology dynamics than\nTDB[shortest], which itself requires less than TDB[fastest]. Reversely, these\ndifferences in feasibility imply that the computational powers of $R_n$,\n$B_\\Delta$, and $P_p$ also form a strict hierarchy. \n\n"}
{"id": "1210.3539", "contents": "Title: Synthesis from LTL Specifications with Mean-Payoff Objectives Abstract: The classical LTL synthesis problem is purely qualitative: the given LTL\nspecification is realized or not by a reactive system. LTL is not expressive\nenough to formalize the correctness of reactive systems with respect to some\nquantitative aspects. This paper extends the qualitative LTL synthesis setting\nto a quantitative setting. The alphabet of actions is extended with a weight\nfunction ranging over the rational numbers. The value of an infinite word is\nthe mean-payoff of the weights of its letters. The synthesis problem then\namounts to automatically construct (if possible) a reactive system whose\nexecutions all satisfy a given LTL formula and have mean-payoff values greater\nthan or equal to some given threshold. The latter problem is called LTLMP\nsynthesis and the LTLMP realizability problem asks to check whether such a\nsystem exists. We first show that LTLMP realizability is not more difficult\nthan LTL realizability: it is 2ExpTime-Complete. This is done by reduction to\ntwo-player mean-payoff parity games. While infinite memory strategies are\nrequired to realize LTLMP specifications in general, we show that\nepsilon-optimality can be obtained with finite memory strategies, for any\nepsilon > 0. To obtain an efficient algorithm in practice, we define a\nSafraless procedure to decide whether there exists a finite-memory strategy\nthat realizes a given specification for some given threshold. This procedure is\nbased on a reduction to two-player energy safety games which are in turn\nreduced to safety games. Finally, we show that those safety games can be solved\nefficiently by exploiting the structure of their state spaces and by using\nantichains as a symbolic data-structure. All our results extend to\nmulti-dimensional weights. We have implemented an antichain-based procedure and\nwe report on some promising experimental results. \n\n"}
{"id": "1210.3548", "contents": "Title: Multiplayer Cost Games with Simple Nash Equilibria Abstract: Multiplayer games with selfish agents naturally occur in the design of\ndistributed and embedded systems. As the goals of selfish agents are usually\nneither equivalent nor antagonistic to each other, such games are non zero-sum\ngames. We study such games and show that a large class of these games,\nincluding games where the individual objectives are mean- or discounted-payoff,\nor quantitative reachability, and show that they do not only have a solution,\nbut a simple solution. We establish the existence of Nash equilibria that are\ncomposed of k memoryless strategies for each agent in a setting with k agents,\none main and k-1 minor strategies. The main strategy describes what happens\nwhen all agents comply, whereas the minor strategies ensure that all other\nagents immediately start to co-operate against the agent who first deviates\nfrom the plan. This simplicity is important, as rational agents are an\nidealisation. Realistically, agents have to decide on their moves with very\nlimited resources, and complicated strategies that require exponential--or even\nnon-elementary--implementations cannot realistically be implemented. The\nexistence of simple strategies that we prove in this paper therefore holds a\npromise of implementability. \n\n"}
{"id": "1210.3978", "contents": "Title: Fixed-Parameter Tractability of Workflow Satisfiability in the Presence\n  of Seniority Constraints Abstract: The workflow satisfiability problem is concerned with determining whether it\nis possible to find an allocation of authorized users to the steps in a\nworkflow in such a way that all constraints are satisfied. The problem is\nNP-hard in general, but is known to be fixed-parameter tractable for certain\nclasses of constraints. The known results on fixed-parameter tractability rely\non the symmetry (in some sense) of the constraints. In this paper, we provide\nthe first results that establish fixed-parameter tractability of the\nsatisfiability problem when the constraints are asymmetric. In particular, we\nintroduce the notion of seniority constraints, in which the execution of steps\nis determined, in part, by the relative seniority of the users that perform\nthem. Our results require new techniques, which make use of tree decompositions\nof the graph of the binary relation defining the constraint. Finally, we\nestablish a lower bound for the hardness of the workflow satisfiability\nproblem. \n\n"}
{"id": "1210.6414", "contents": "Title: Efficient Instantiation of Parameterised Boolean Equation Systems to\n  Parity Games Abstract: Parameterised Boolean Equation Systems (PBESs) are sequences of Boolean fixed\npoint equations with data variables, used for, e.g., verification of modal\nmu-calculus formulae for process algebraic specifications with data.\n  Solving a PBES is usually done by instantiation to a Parity Game and then\nsolving the game. Practical game solvers exist, but the instantiation step is\nthe bottleneck.\n  We enhance the instantiation in two steps. First, we transform the PBES to a\nParameterised Parity Game (PPG), a PBES with each equation either conjunctive\nor disjunctive. Then we use LTSmin, that offers transition caching, efficient\nstorage of states and both distributed and symbolic state space generation, for\ngenerating the game graph. To that end we define a language module for LTSmin,\nconsisting of an encoding of variables with parameters into state vectors, a\ngrouped transition relation and a dependency matrix to indicate the\ndependencies between parts of the state vector and transition groups.\n  Benchmarks on some large case studies, show that the method speeds up the\ninstantiation significantly and decreases memory usage drastically. \n\n"}
{"id": "1211.1325", "contents": "Title: Composable and Efficient Mechanisms Abstract: We initiate the study of efficient mechanism design with guaranteed good\nproperties even when players participate in multiple different mechanisms\nsimultaneously or sequentially. We define the class of smooth mechanisms,\nrelated to smooth games defined by Roughgarden, that can be thought of as\nmechanisms that generate approximately market clearing prices. We show that\nsmooth mechanisms result in high quality outcome in equilibrium both in the\nfull information setting and in the Bayesian setting with uncertainty about\nparticipants, as well as in learning outcomes. Our main result is to show that\nsuch mechanisms compose well: smoothness locally at each mechanism implies\nefficiency globally.\n  For mechanisms where good performance requires that bidders do not bid above\ntheir value, we identify the notion of a weakly smooth mechanism. Weakly smooth\nmechanisms, such as the Vickrey auction, are approximately efficient under the\nno-overbidding assumption. Similar to smooth mechanisms, weakly smooth\nmechanisms behave well in composition, and have high quality outcome in\nequilibrium (assuming no overbidding) both in the full information setting and\nin the Bayesian setting, as well as in learning outcomes.\n  In most of the paper we assume participants have quasi-linear valuations. We\nalso extend some of our results to settings where participants have budget\nconstraints. \n\n"}
{"id": "1211.2268", "contents": "Title: Tatonnement in Ongoing Markets of Complementary Goods Abstract: This paper continues the study, initiated by Cole and Fleischer, of the\nbehavior of a tatonnement price update rule in Ongoing Fisher Markets. The\nprior work showed fast convergence toward an equilibrium when the goods\nsatisfied the weak gross substitutes property and had bounded demand and income\nelasticities.\n  The current work shows that fast convergence also occurs for the following\ntypes of markets:\n  - All pairs of goods are complements to each other, and - the demand and\nincome elasticities are suitably bounded.\n  In particular, these conditions hold when all buyers in the market are\nequipped with CES utilities, where all the parameters $\\rho$, one per buyer,\nsatisfy $-1 < \\rho \\le 0$.\n  In addition, we extend the above result to markets in which a mixture of\ncomplements and substitutes occur. This includes characterizing a class of\nnested CES utilities for which fast convergence holds.\n  An interesting technical contribution, which may be of independent interest,\nis an amortized analysis for handling asynchronous events in settings in which\nthere are a mix of continuous changes and discrete events. \n\n"}
{"id": "1211.3043", "contents": "Title: General Truthfulness Characterizations Via Convex Analysis Abstract: We present a model of truthful elicitation which generalizes and extends\nmechanisms, scoring rules, and a number of related settings that do not qualify\nas one or the other. Our main result is a characterization theorem, yielding\ncharacterizations for all of these settings, including a new characterization\nof scoring rules for non-convex sets of distributions. We generalize this model\nto eliciting some property of the agent's private information, and provide the\nfirst general characterization for this setting. We combine this\ncharacterization with duality to give a simple construction to convert between\nscoring rules and randomized mechanisms. We also show how this characterization\ngives a new proof of a mechanism design result due to Saks and Yu. \n\n"}
{"id": "1211.4174", "contents": "Title: Energy-Efficient Nonstationary Spectrum Sharing Abstract: We develop a novel design framework for energy-efficient spectrum sharing\namong autonomous users who aim to minimize their energy consumptions subject to\nminimum throughput requirements. Most existing works proposed stationary\nspectrum sharing policies, in which users transmit at fixed power levels. Since\nusers transmit simultaneously under stationary policies, to fulfill minimum\nthroughput requirements, they need to transmit at high power levels to overcome\ninterference. To improve energy efficiency, we construct nonstationary spectrum\nsharing policies, in which the users transmit at time-varying power levels.\nSpecifically, we focus on TDMA (time-division multiple access) policies in\nwhich one user transmits at each time (but not in a round-robin fashion). The\nproposed policy can be implemented by each user running a low-complexity\nalgorithm in a decentralized manner. It achieves high energy efficiency even\nwhen the users have erroneous and binary feedback about their interference\nlevels. Moreover, it can adapt to the dynamic entry and exit of users. The\nproposed policy is also deviation-proof, namely autonomous users will find it\nin their self-interests to follow it. Compared to existing policies, the\nproposed policy can achieve an energy saving of up to 90% when the number of\nusers is high. \n\n"}
{"id": "1211.6244", "contents": "Title: A Computational Model and Convergence Theorem for Rumor Dissemination in\n  Social Networks Abstract: The spread of rumors, which are known as unverified statements of uncertain\norigin, may cause tremendous number of social problems. If it would be possible\nto identify factors affecting spreading a rumor (such as agents' desires, trust\nnetwork, etc.), then this could be used to slowdown or stop its spreading. A\ncomputational model that includes rumor features and the way a rumor is spread\namong society's members, based on their desires, is therefore needed. Our\nresearch is centering on the relation between the homogeneity of the society\nand rumor convergence in it and result shows that the homogeneity of the\nsociety is a necessary condition for convergence of the spreading rumor. \n\n"}
{"id": "1212.2284", "contents": "Title: The Complexity of Planar Boolean #CSP with Complex Weights Abstract: We prove a complexity dichotomy theorem for symmetric complex-weighted\nBoolean #CSP when the constraint graph of the input must be planar. The\nproblems that are #P-hard over general graphs but tractable over planar graphs\nare precisely those with a holographic reduction to matchgates. This\ngeneralizes a theorem of Cai, Lu, and Xia for the case of real weights. We also\nobtain a dichotomy theorem for a symmetric arity 4 signature with complex\nweights in the planar Holant framework, which we use in the proof of our #CSP\ndichotomy. In particular, we reduce the problem of evaluating the Tutte\npolynomial of a planar graph at the point (3,3) to counting the number of\nEulerian orientations over planar 4-regular graphs to show the latter is\n#P-hard. This strengthens a theorem by Huang and Lu to the planar setting. Our\nproof techniques combine new ideas with refinements and extensions of existing\ntechniques. These include planar pairings, the recursive unary construction,\nthe anti-gadget technique, and pinning in the Hadamard basis. \n\n"}
{"id": "1212.3170", "contents": "Title: Improving Macrocell - Small Cell Coexistence through Adaptive\n  Interference Draining Abstract: The deployment of underlay small base stations (SBSs) is expected to\nsignificantly boost the spectrum efficiency and the coverage of next-generation\ncellular networks. However, the coexistence of SBSs underlaid to an existing\nmacro-cellular network faces important challenges, notably in terms of spectrum\nsharing and interference management. In this paper, we propose a novel\ngame-theoretic model that enables the SBSs to optimize their transmission rates\nby making decisions on the resource occupation jointly in the frequency and\nspatial domains. This procedure, known as interference draining, is performed\namong cooperative SBSs and allows to drastically reduce the interference\nexperienced by both macro- and small cell users. At the macrocell side, we\nconsider a modified water-filling policy for the power allocation that allows\neach macrocell user (MUE) to focus the transmissions on the degrees of freedom\nover which the MUE experiences the best channel and interference conditions.\nThis approach not only represents an effective way to decrease the received\ninterference at the MUEs but also grants the SBSs tier additional transmission\nopportunities and allows for a more agile interference management. Simulation\nresults show that the proposed approach yields significant gains at both\nmacrocell and small cell tiers, in terms of average achievable rate per user,\nreaching up to 37%, relative to the non-cooperative case, for a network with\n150 MUEs and 200 SBSs. \n\n"}
{"id": "1212.3782", "contents": "Title: Can Selfish Groups be Self-Enforcing? Abstract: Algorithmic graph theory has thoroughly analyzed how, given a network\ndescribing constraints between various nodes, groups can be formed among these\nso that the resulting configuration optimizes a \\emph{global} metric. In\ncontrast, for various social and economic networks, groups are formed \\emph{de\nfacto} by the choices of selfish players. A fundamental problem in this setting\nis the existence and convergence to a \\emph{self-enforcing} configuration:\nassignment of players into groups such that no player has an incentive to move\ninto another group than hers. Motivated by information sharing on social\nnetworks -- and the difficult tradeoff between its benefits and the associated\nprivacy risk -- we study the possible emergence of such stable configurations\nin a general selfish group formation game.\n  Our paper considers this general game for the first time, and it completes\nits analysis. We show that convergence critically depends on the level of\n\\emph{collusions} among the players -- which allow multiple players to move\nsimultaneously as long as \\emph{all of them} benefit. Solving a previously open\nproblem we exactly show when, depending on collusions, convergence occurs\nwithin polynomial time, non-polynomial time, and when it never occurs. We also\nprove that previously known bounds on convergence time are all loose: by a\nnovel combinatorial analysis of the evolution of this game we are able to\nprovide the first \\emph{asymptotically exact} formula on its convergence.\nMoreover, we extend these results by providing a complete analysis when groups\nmay \\emph{overlap}, and for general utility functions representing\n\\emph{multi-modal} interactions. Finally, we prove that collusions have a\nsignificant and \\emph{positive} effect on the \\emph{efficiency} of the\nequilibrium that is attained. \n\n"}
{"id": "1212.3849", "contents": "Title: Every locally characterized affine-invariant property is testable Abstract: Let F = F_p for any fixed prime p >= 2. An affine-invariant property is a\nproperty of functions on F^n that is closed under taking affine transformations\nof the domain. We prove that all affine-invariant property having local\ncharacterizations are testable. In fact, we show a proximity-oblivious test for\nany such property P, meaning that there is a test that, given an input function\nf, makes a constant number of queries to f, always accepts if f satisfies P,\nand rejects with positive probability if the distance between f and P is\nnonzero. More generally, we show that any affine-invariant property that is\nclosed under taking restrictions to subspaces and has bounded complexity is\ntestable.\n  We also prove that any property that can be described as the property of\ndecomposing into a known structure of low-degree polynomials is locally\ncharacterized and is, hence, testable. For example, whether a function is a\nproduct of two degree-d polynomials, whether a function splits into a product\nof d linear polynomials, and whether a function has low rank are all examples\nof degree-structural properties and are therefore locally characterized.\n  Our results depend on a new Gowers inverse theorem by Tao and Ziegler for low\ncharacteristic fields that decomposes any polynomial with large Gowers norm\ninto a function of low-degree non-classical polynomials. We establish a new\nequidistribution result for high rank non-classical polynomials that drives the\nproofs of both the testability results and the local characterization of\ndegree-structural properties. \n\n"}
{"id": "1212.4372", "contents": "Title: Sliding Windows with Limited Storage Abstract: We consider time-space tradeoffs for exactly computing frequency moments and\norder statistics over sliding windows. Given an input of length 2n-1, the task\nis to output the function of each window of length n, giving n outputs in\ntotal. Computations over sliding windows are related to direct sum problems\nexcept that inputs to instances almost completely overlap.\n  We show an average case and randomized time-space tradeoff lower bound of TS\nin Omega(n^2) for multi-way branching programs, and hence standard RAM and\nword-RAM models, to compute the number of distinct elements, F_0, in sliding\nwindows over alphabet [n]. The same lower bound holds for computing the\nlow-order bit of F_0 and computing any frequency moment F_k for k not equal to\n1. We complement this lower bound with a TS in \\tilde O(n^2) deterministic RAM\nalgorithm for exactly computing F_k in sliding windows.\n  We show time-space separations between the complexity of sliding-window\nelement distinctness and that of sliding-window $F_0\\bmod 2$ computation. In\nparticular for alphabet [n] there is a very simple errorless sliding-window\nalgorithm for element distinctness that runs in O(n) time on average and uses\nO(log{n}) space.\n  We show that any algorithm for a single element distinctness instance can be\nextended to an algorithm for the sliding-window version of element distinctness\nwith at most a polylogarithmic increase in the time-space product.\n  Finally, we show that the sliding-window computation of order statistics such\nas the maximum and minimum can be computed with only a logarithmic increase in\ntime, but that a TS in Omega(n^2) lower bound holds for sliding-window\ncomputation of order statistics such as the median, a nearly linear increase in\ntime when space is small. \n\n"}
{"id": "1212.6355", "contents": "Title: Efficient Decomposition of Bimatrix Games Abstract: Exploiting the algebraic structure of the set of bimatrix games, a\ndivide-and-conquer algorithm for finding Nash equilibria is proposed. The\nalgorithm is fixed-parameter tractable with the size of the largest irreducible\ncomponent of a game as parameter. An implementation of the algorithm is shown\nto yield a significant performance increase on inputs with small parameters. \n\n"}
{"id": "1212.6925", "contents": "Title: Superlinear lower bounds for multipass graph processing Abstract: We prove $n^{1+\\Omega(1/p)}/p^{O(1)}$ lower bounds for the space complexity\nof $p$-pass streaming algorithms solving the following problems on $n$-vertex\ngraphs:\n  * testing if an undirected graph has a perfect matching (this implies lower\nbounds for computing a maximum matching or even just the maximum matching\nsize),\n  * testing if two specific vertices are at distance at most $2(p+1)$ in an\nundirected graph,\n  * testing if there is a directed path from $s$ to $t$ for two specific\nvertices $s$ and $t$ in a directed graph.\n  Prior to our result, it was known that these problems require $\\Omega(n^2)$\nspace in one pass, but no $n^{1+\\Omega(1)}$ lower bound was known for any $p\\ge\n2$.\n  These streaming results follow from a communication complexity lower bound\nfor a communication game in which the players hold two graphs on the same set\nof vertices. The task of the players is to find out whether the sets of\nvertices at distance exactly $p+1$ from a specific vertex intersect. The game\nrequires a significant amount of communication only if the players are forced\nto speak in a specific difficult order. This is reminiscent of lower bounds for\ncommunication problems such as indexing and pointer chasing. Among other\nthings, our line of attack requires proving an information cost lower bound for\na decision version of the classic pointer chasing problem and a direct sum type\ntheorem for the disjunction of several instances of this problem. \n\n"}
{"id": "1301.0820", "contents": "Title: Moment-Matching Polynomials Abstract: We give a new framework for proving the existence of low-degree, polynomial\napproximators for Boolean functions with respect to broad classes of\nnon-product distributions. Our proofs use techniques related to the classical\nmoment problem and deviate significantly from known Fourier-based methods,\nwhich require the underlying distribution to have some product structure.\n  Our main application is the first polynomial-time algorithm for agnostically\nlearning any function of a constant number of halfspaces with respect to any\nlog-concave distribution (for any constant accuracy parameter). This result was\nnot known even for the case of learning the intersection of two halfspaces\nwithout noise. Additionally, we show that in the \"smoothed-analysis\" setting,\nthe above results hold with respect to distributions that have sub-exponential\ntails, a property satisfied by many natural and well-studied distributions in\nmachine learning.\n  Given that our algorithms can be implemented using Support Vector Machines\n(SVMs) with a polynomial kernel, these results give a rigorous theoretical\nexplanation as to why many kernel methods work so well in practice. \n\n"}
{"id": "1301.1420", "contents": "Title: Is it ever safe to vote strategically? Abstract: There are many situations in which mis-coordinated strategic voting can leave\nstrategic voters worse off than they would have been had they not tried to\nstrategize. We analyse the simplest of such scenarios, in which the set of\nstrategic voters all have the same sincere preferences and all cast the same\nstrategic vote, while all other voters vote sincerely. Most mis-coordinations\nin this framework can be classified as instances of either strategic\novershooting (too many voted strategically) or strategic undershooting (too\nfew). If mis-coordination can result in strategic voters ending up worse off\nthan they would have been had they all just voted sincerely, we call the\nrelevant strategic vote unsafe. We show that under every onto and\nnon-dictatorial social choice rule there exist circumstances where a voter has\nan incentive to cast a safe strategic vote. We extend the Gibbard-Satterthwaite\nTheorem by proving that every onto and non-dictatorial social choice rule can\nbe individually manipulated by a voter casting a safe strategic vote. \n\n"}
{"id": "1301.2533", "contents": "Title: A Novel Analytical Method for Evolutionary Graph Theory Problems Abstract: Evolutionary graph theory studies the evolutionary dynamics of populations\nstructured on graphs. A central problem is determining the probability that a\nsmall number of mutants overtake a population. Currently, Monte Carlo\nsimulations are used for estimating such fixation probabilities on general\ndirected graphs, since no good analytical methods exist. In this paper, we\nintroduce a novel deterministic framework for computing fixation probabilities\nfor strongly connected, directed, weighted evolutionary graphs under neutral\ndrift. We show how this framework can also be used to calculate the expected\nnumber of mutants at a given time step (even if we relax the assumption that\nthe graph is strongly connected), how it can extend to other related models\n(e.g. voter model), how our framework can provide non-trivial bounds for\nfixation probability in the case of an advantageous mutant, and how it can be\nused to find a non-trivial lower bound on the mean time to fixation. We provide\nvarious experimental results determining fixation probabilities and expected\nnumber of mutants on different graphs. Among these, we show that our method\nconsistently outperforms Monte Carlo simulations in speed by several orders of\nmagnitude. Finally we show how our approach can provide insight into synaptic\ncompetition in neurology. \n\n"}
{"id": "1301.4958", "contents": "Title: Improved algorithms and analysis for the laminar matroid secretary\n  problem Abstract: In a matroid secretary problem, one is presented with a sequence of objects\nof various weights in a random order, and must choose irrevocably to accept or\nreject each item. There is a further constraint that the set of items selected\nmust form an independent set of an associated matroid. Constant-competitive\nalgorithms (algorithms whose expected solution weight is within a constant\nfactor of the optimal) are known for many types of matroid secretary problems.\nWe examine the laminar matroid and show an algorithm achieving provably 0.053\ncompetitive ratio. \n\n"}
{"id": "1301.5844", "contents": "Title: Ranking Games that have Competitiveness-based Strategies Abstract: An extensive literature in economics and social science addresses contests,\nin which players compete to outperform each other on some measurable criterion,\noften referred to as a player's score, or output. Players incur costs that are\nan increasing function of score, but receive prizes for obtaining higher score\nthan their competitors. In this paper we study finite games that are\ndiscretized contests, and the problems of computing exact and approximate Nash\nequilibria. Our motivation is the worst-case hardness of Nash equilibrium\ncomputation, and the resulting interest in important classes of games that\nadmit polynomial-time algorithms. For games that have a tie-breaking rule for\nplayers' scores, we present a polynomial-time algorithm for computing an exact\nequilibrium in the 2-player case, and for multiple players, a characterization\nof Nash equilibria that shows an interesting parallel between these games and\nunrestricted 2-player games in normal form. When ties are allowed, via a\nreduction from these games to a subclass of anonymous games, we give\napproximation schemes for two special cases: constant-sized set of strategies,\nand constant number of players. \n\n"}
{"id": "1302.0418", "contents": "Title: Arthur-Merlin Streaming Complexity Abstract: We study the power of Arthur-Merlin probabilistic proof systems in the data\nstream model. We show a canonical $\\mathcal{AM}$ streaming algorithm for a wide\nclass of data stream problems. The algorithm offers a tradeoff between the\nlength of the proof and the space complexity that is needed to verify it.\n  As an application, we give an $\\mathcal{AM}$ streaming algorithm for the\n\\emph{Distinct Elements} problem. Given a data stream of length $m$ over\nalphabet of size $n$, the algorithm uses $\\tilde O(s)$ space and a proof of\nsize $\\tilde O(w)$, for every $s,w$ such that $s \\cdot w \\ge n$ (where $\\tilde\nO$ hides a $\\polylog(m,n)$ factor). We also prove a lower bound, showing that\nevery $\\mathcal{MA}$ streaming algorithm for the \\emph{Distinct Elements}\nproblem that uses $s$ bits of space and a proof of size $w$, satisfies $s \\cdot\nw = \\Omega(n)$.\n  As a part of the proof of the lower bound for the \\emph{Distinct Elements}\nproblem, we show a new lower bound of $\\Omega(\\sqrt n)$ on the $\\mathcal{MA}$\ncommunication complexity of the \\emph{Gap Hamming Distance} problem, and prove\nits tightness. \n\n"}
{"id": "1302.0948", "contents": "Title: Non-monetary fair scheduling---a cooperative game theory approach Abstract: We consider a multi-organizational system in which each organization\ncontributes processors to the global pool but also jobs to be processed on the\ncommon resources. The fairness of the scheduling algorithm is essential for the\nstability and even for the existence of such systems (as organizations may\nrefuse to join an unfair system).\n  We consider on-line, non-clairvoyant scheduling of sequential jobs. The\nstarted jobs cannot be stopped, canceled, preempted, or moved to other\nprocessors. We consider identical processors, but most of our results can be\nextended to related or unrelated processors.\n  We model the fair scheduling problem as a cooperative game and we use the\nShapley value to determine the ideal fair schedule. In contrast to the current\nliterature, we do not use money to assess the relative utilities of jobs.\nInstead, to calculate the contribution of an organization, we determine how the\npresence of this organization influences the performance of other\norganizations. Our approach can be used with arbitrary utility function (e.g.,\nflow time, tardiness, resource utilization), but we argue that the utility\nfunction should be strategy resilient. The organizations should be discouraged\nfrom splitting, merging or delaying their jobs. We present the unique (to\nwithin a multiplicative and additive constants) strategy resilient utility\nfunction.\n  We show that the problem of fair scheduling is NP-hard and hard to\napproximate. However, for unit-size jobs, we present an FPRAS. Also, we show\nthat the problem parametrized with the number of organizations is FPT. Although\nfor the large number of the organizations the problem is computationally hard,\nthe presented exponential algorithm can be used as a fairness benchmark. \n\n"}
{"id": "1302.1669", "contents": "Title: Possible and Necessary Winner Problem in Social Polls Abstract: Social networks are increasingly being used to conduct polls. We introduce a\nsimple model of such social polling. We suppose agents vote sequentially, but\nthe order in which agents choose to vote is not necessarily fixed. We also\nsuppose that an agent's vote is influenced by the votes of their friends who\nhave already voted. Despite its simplicity, this model provides useful insights\ninto a number of areas including social polling, sequential voting, and\nmanipulation. We prove that the number of candidates and the network structure\naffect the computational complexity of computing which candidate necessarily or\npossibly can win in such a social poll. For social networks with bounded\ntreewidth and a bounded number of candidates, we provide polynomial algorithms\nfor both problems. In other cases, we prove that computing which candidates\nnecessarily or possibly win are computationally intractable. \n\n"}
{"id": "1302.2787", "contents": "Title: Acquaintance Time of a Graph Abstract: We define the following parameter of connected graphs. For a given graph $G$\nwe place one agent in each vertex of $G$. Every pair of agents sharing a common\nedge is declared to be acquainted. In each round we choose some matching of $G$\n(not necessarily a maximal matching), and for each edge in the matching the\nagents on this edge swap places. After the swap, again, every pair of agents\nsharing a common edge become acquainted, and the process continues. We define\nthe \\emph{acquaintance time} of a graph $G$, denoted by $AC(G)$, to be the\nminimal number of rounds required until every two agents are acquainted.\n  We first study the acquaintance time for some natural families of graphs\nincluding the path, expanders, the binary tree, and the complete bipartite\ngraph. We also show that for all positive integers $n$ and $k \\leq n^{1.5}$\nthere exists an $n$-vertex graph $G$ such that $AC(G) =\\Theta(k)$. We also\nprove that for all $n$-vertex connected graphs $G$ we have $AC(G) =\nO\\left(\\frac{n^2}{\\log(n)/\\log\\log(n)}\\right)$, improving the $O(n^2)$ trivial\nupper bound achieved by sequentially letting each agent perform depth-first\nsearch along a spanning tree of $G$.\n  Studying the computational complexity of this problem, we prove that for any\nconstant $t \\geq 1$ the problem of deciding that a given graph $G$ has $AC(G)\n\\leq t$ or $AC(G) \\geq 2t$ is $\\mathcal{NP}$-complete. That is, $AC(G)$ is\n$\\mathcal{NP}$-hard to approximate within multiplicative factor of 2, as well\nas within any additive constant factor.\n  On the algorithmic side, we give a deterministic algorithm that given a graph\n$G$ with $AC(G)=1$ finds a ${\\lceil n/c\\rceil}$-rounds strategy for\nacquaintance in time $n^{c+O(1)}$. We also design a randomized polynomial time\nalgorithm that given a graph $G$ with $AC(G)=1$ finds with high probability an\n$O(\\log(n))$-rounds strategy for acquaintance. \n\n"}
{"id": "1302.2869", "contents": "Title: Searching and Bargaining with Middlemen Abstract: We study decentralized markets with the presence of middlemen, modeled by a\nnon-cooperative bargaining game in trading networks. Our goal is to investigate\nhow the network structure of the market and the role of middlemen influence the\nmarket's efficiency and fairness. We introduce the concept of limit stationary\nequilibrium in a general trading network and use it to analyze how competition\namong middlemen is influenced by the network structure, how endogenous delay\nemerges in trade and how surplus is shared between producers and consumers. \n\n"}
{"id": "1302.3116", "contents": "Title: Learning Equilibria of Games via Payoff Queries Abstract: A recent body of experimental literature has studied empirical\ngame-theoretical analysis, in which we have partial knowledge of a game,\nconsisting of observations of a subset of the pure-strategy profiles and their\nassociated payoffs to players. The aim is to find an exact or approximate Nash\nequilibrium of the game, based on these observations. It is usually assumed\nthat the strategy profiles may be chosen in an on-line manner by the algorithm.\nWe study a corresponding computational learning model, and the query complexity\nof learning equilibria for various classes of games. We give basic results for\nbimatrix and graphical games. Our focus is on symmetric network congestion\ngames. For directed acyclic networks, we can learn the cost functions (and\nhence compute an equilibrium) while querying just a small fraction of\npure-strategy profiles. For the special case of parallel links, we have the\nstronger result that an equilibrium can be identified while only learning a\nsmall fraction of the cost values. \n\n"}
{"id": "1302.3309", "contents": "Title: Socially Stable Matchings Abstract: In two-sided matching markets, the agents are partitioned into two sets. Each\nagent wishes to be matched to an agent in the other set and has a strict\npreference over these potential matches. A matching is stable if there are no\nblocking pairs, i.e., no pair of agents that prefer each other to their\nassigned matches. In this paper we study a variant of stable matching motivated\nby the fact that, in most centralized markets, many agents do not have direct\ncommunication with each other. Hence even if some blocking pairs exist, the\nagents involved in those pairs may not be able to coordinate a deviation. We\nmodel communication channels with a bipartite graph between the two sets of\nagents which we call the social graph, and we study socially stable matchings.\nA matching is socially stable if there are no blocking pairs that are connected\nby an edge in the social graph. Socially stable matchings vary in size and so\nwe look for a maximum socially stable matching. We prove that this problem is\nNP-hard and, assuming the unique games conjecture, hard to approximate within a\nfactor of 3/2-{\\epsilon}, for any constant {\\epsilon}>0. We complement the\nhardness results with a 3/2-approximation algorithm. \n\n"}
{"id": "1302.3496", "contents": "Title: On Polynomial Kernels for Integer Linear Programs: Covering, Packing and\n  Feasibility Abstract: We study the existence of polynomial kernels for the problem of deciding\nfeasibility of integer linear programs (ILPs), and for finding good solutions\nfor covering and packing ILPs. Our main results are as follows: First, we show\nthat the ILP Feasibility problem admits no polynomial kernelization when\nparameterized by both the number of variables and the number of constraints,\nunless NP \\subseteq coNP/poly. This extends to the restricted cases of bounded\nvariable degree and bounded number of variables per constraint, and to covering\nand packing ILPs. Second, we give a polynomial kernelization for the Cover ILP\nproblem, asking for a solution to Ax >= b with c^Tx <= k, parameterized by k,\nwhen A is row-sparse; this generalizes a known polynomial kernelization for the\nspecial case with 0/1-variables and coefficients (d-Hitting Set). \n\n"}
{"id": "1302.4248", "contents": "Title: Looking at Mean-Payoff and Total-Payoff through Windows Abstract: We consider two-player games played on weighted directed graphs with\nmean-payoff and total-payoff objectives, two classical quantitative objectives.\nWhile for single-dimensional games the complexity and memory bounds for both\nobjectives coincide, we show that in contrast to multi-dimensional mean-payoff\ngames that are known to be coNP-complete, multi-dimensional total-payoff games\nare undecidable. We introduce conservative approximations of these objectives,\nwhere the payoff is considered over a local finite window sliding along a play,\ninstead of the whole play. For single dimension, we show that (i) if the window\nsize is polynomial, deciding the winner takes polynomial time, and (ii) the\nexistence of a bounded window can be decided in NP $\\cap$ coNP, and is at least\nas hard as solving mean-payoff games. For multiple dimensions, we show that (i)\nthe problem with fixed window size is EXPTIME-complete, and (ii) there is no\nprimitive-recursive algorithm to decide the existence of a bounded window. \n\n"}
{"id": "1302.4713", "contents": "Title: Constrained Signaling in Auction Design Abstract: We consider the problem of an auctioneer who faces the task of selling a good\n(drawn from a known distribution) to a set of buyers, when the auctioneer does\nnot have the capacity to describe to the buyers the exact identity of the good\nthat he is selling. Instead, he must come up with a constrained signalling\nscheme: a (non injective) mapping from goods to signals, that satisfies the\nconstraints of his setting. For example, the auctioneer may be able to\ncommunicate only a bounded length message for each good, or he might be legally\nconstrained in how he can advertise the item being sold. Each candidate\nsignaling scheme induces an incomplete-information game among the buyers, and\nthe goal of the auctioneer is to choose the signaling scheme and accompanying\nauction format that optimizes welfare. In this paper, we use techniques from\nsubmodular function maximization and no-regret learning to give algorithms for\ncomputing constrained signaling schemes for a variety of constrained signaling\nproblems. \n\n"}
{"id": "1302.5724", "contents": "Title: Budget Feasible Mechanisms for Experimental Design Abstract: In the classical experimental design setting, an experimenter E has access to\na population of $n$ potential experiment subjects $i\\in \\{1,...,n\\}$, each\nassociated with a vector of features $x_i\\in R^d$. Conducting an experiment\nwith subject $i$ reveals an unknown value $y_i\\in R$ to E. E typically assumes\nsome hypothetical relationship between $x_i$'s and $y_i$'s, e.g., $y_i \\approx\n\\beta x_i$, and estimates $\\beta$ from experiments, e.g., through linear\nregression. As a proxy for various practical constraints, E may select only a\nsubset of subjects on which to conduct the experiment.\n  We initiate the study of budgeted mechanisms for experimental design. In this\nsetting, E has a budget $B$. Each subject $i$ declares an associated cost $c_i\n>0$ to be part of the experiment, and must be paid at least her cost. In\nparticular, the Experimental Design Problem (EDP) is to find a set $S$ of\nsubjects for the experiment that maximizes $V(S) = \\log\\det(I_d+\\sum_{i\\in\nS}x_i\\T{x_i})$ under the constraint $\\sum_{i\\in S}c_i\\leq B$; our objective\nfunction corresponds to the information gain in parameter $\\beta$ that is\nlearned through linear regression methods, and is related to the so-called\n$D$-optimality criterion. Further, the subjects are strategic and may lie about\ntheir costs.\n  We present a deterministic, polynomial time, budget feasible mechanism\nscheme, that is approximately truthful and yields a constant factor\napproximation to EDP. In particular, for any small $\\delta > 0$ and $\\epsilon >\n0$, we can construct a (12.98, $\\epsilon$)-approximate mechanism that is\n$\\delta$-truthful and runs in polynomial time in both $n$ and\n$\\log\\log\\frac{B}{\\epsilon\\delta}$. We also establish that no truthful,\nbudget-feasible algorithms is possible within a factor 2 approximation, and\nshow how to generalize our approach to a wide class of learning problems,\nbeyond linear regression. \n\n"}
{"id": "1302.5843", "contents": "Title: Ising formulations of many NP problems Abstract: We provide Ising formulations for many NP-complete and NP-hard problems,\nincluding all of Karp's 21 NP-complete problems. This collects and extends\nmappings to the Ising model from partitioning, covering and satisfiability. In\neach case, the required number of spins is at most cubic in the size of the\nproblem. This work may be useful in designing adiabatic quantum optimization\nalgorithms. \n\n"}
{"id": "1303.1646", "contents": "Title: On the Inefficiency of Standard Multi-Unit Auctions Abstract: We study two standard multi-unit auction formats for allocating multiple\nunits of a single good to multi-demand bidders. The first one is the\nDiscriminatory Auction, which charges every winner his winning bids. The second\nis the Uniform Price Auction, which determines a uniform price to be paid per\nunit. Variants of both formats find applications ranging from the allocation of\nstate bonds to investors, to online sales over the internet, facilitated by\npopular online brokers. For these formats, we consider two bidding interfaces:\n(i) standard bidding, which is most prevalent in the scientific literature, and\n(ii) uniform bidding, which is more popular in practice. In this work, we\nevaluate the economic inefficiency of both multi-unit auction formats for both\nbidding interfaces, by means of upper and lower bounds on the Price of Anarchy\nfor pure Nash equilibria and mixed Bayes-Nash equilibria. Our developments\nimprove significantly upon bounds that have been obtained recently in\n[Markakis, Telelis, ToCS 2014] and [Syrgkanis, Tardos, STOC 2013] for\nsubmodular valuation functions. Moreover, we consider for the first time\nbidders with subadditive valuation functions for these auction formats. Our\nresults signify that these auctions are nearly efficient, which provides\nfurther justification for their use in practice. \n\n"}
{"id": "1303.2147", "contents": "Title: On Influence, Stable Behavior, and the Most Influential Individuals in\n  Networks: A Game-Theoretic Approach Abstract: We introduce a new approach to the study of influence in strategic settings\nwhere the action of an individual depends on that of others in a\nnetwork-structured way. We propose \\emph{influence games} as a\n\\emph{game-theoretic} model of the behavior of a large but finite networked\npopulation. Influence games allow \\emph{both} positive and negative\n\\emph{influence factors}, permitting reversals in behavioral choices. We\nembrace \\emph{pure-strategy Nash equilibrium (PSNE)}, an important solution\nconcept in non-cooperative game theory, to formally define the \\emph{stable\noutcomes} of an influence game and to predict potential outcomes without\nexplicitly considering intricate dynamics. We address an important problem in\nnetwork influence, the identification of the \\emph{most influential\nindividuals}, and approach it algorithmically using PSNE computation.\n\\emph{Computationally}, we provide (a) complexity characterizations of various\nproblems on influence games; (b) efficient algorithms for several special cases\nand heuristics for hard cases; and (c) approximation algorithms, with provable\nguarantees, for the problem of identifying the most influential individuals.\n\\emph{Experimentally}, we evaluate our approach using both synthetic influence\ngames as well as several real-world settings of general interest, each\ncorresponding to a separate branch of the U.S. Government.\n\\emph{Mathematically,} we connect influence games to important game-theoretic\nmodels: \\emph{potential and polymatrix games}. \n\n"}
{"id": "1303.2643", "contents": "Title: Revealing Cluster Structure of Graph by Path Following Replicator\n  Dynamic Abstract: In this paper, we propose a path following replicator dynamic, and\ninvestigate its potentials in uncovering the underlying cluster structure of a\ngraph. The proposed dynamic is a generalization of the discrete replicator\ndynamic. The replicator dynamic has been successfully used to extract dense\nclusters of graphs; however, it is often sensitive to the degree distribution\nof a graph, and usually biased by vertices with large degrees, thus may fail to\ndetect the densest cluster. To overcome this problem, we introduce a dynamic\nparameter, called path parameter, into the evolution process. The path\nparameter can be interpreted as the maximal possible probability of a current\ncluster containing a vertex, and it monotonically increases as evolution\nprocess proceeds. By limiting the maximal probability, the phenomenon of some\nvertices dominating the early stage of evolution process is suppressed, thus\nmaking evolution process more robust. To solve the optimization problem with a\nfixed path parameter, we propose an efficient fixed point algorithm. The time\ncomplexity of the path following replicator dynamic is only linear in the\nnumber of edges of a graph, thus it can analyze graphs with millions of\nvertices and tens of millions of edges on a common PC in a few minutes.\nBesides, it can be naturally generalized to hypergraph and graph with edges of\ndifferent orders. We apply it to four important problems: maximum clique\nproblem, densest k-subgraph problem, structure fitting, and discovery of\nhigh-density regions. The extensive experimental results clearly demonstrate\nits advantages, in terms of robustness, scalability and flexility. \n\n"}
{"id": "1303.2981", "contents": "Title: On the Complexity of the Orbit Problem Abstract: We consider higher-dimensional versions of Kannan and Lipton's Orbit\nProblem---determining whether a target vector space V may be reached from a\nstarting point x under repeated applications of a linear transformation A.\nAnswering two questions posed by Kannan and Lipton in the 1980s, we show that\nwhen V has dimension one, this problem is solvable in polynomial time, and when\nV has dimension two or three, the problem is in NP^{RP}. \n\n"}
{"id": "1303.4438", "contents": "Title: On Random Sampling Auctions for Digital Goods Abstract: In the context of auctions for digital goods, an interesting random sampling\nauction has been proposed by Goldberg, Hartline, and Wright [2001]. This\nauction has been analyzed by Feige, Flaxman, Hartline, and Kleinberg [2005],\nwho have shown that it is 15-competitive in the worst case {which is\nsubstantially better than the previously proven constant bounds but still far\nfrom the conjectured competitive ratio of 4. In this paper, we prove that the\naforementioned random sampling auction is indeed 4-competitive for a large\nclass of instances where the number of bids above (or equal to) the optimal\nsale price is at least 6. We also show that it is 4:68-competitive for the\nsmall class of remaining instances thus leaving a negligible gap between the\nlower and upper bound. We employ a mix of probabilistic techniques and dynamic\nprogramming to compute these bounds. \n\n"}
{"id": "1304.0539", "contents": "Title: A Real-time Group Auction System for Efficient Allocation of Cloud\n  Internet Applications Abstract: Increasing number of the cloud-based Internet applications demands for\nefficient resource and cost management. This paper proposes a real-time group\nauction system for the cloud instance market. The system is designed based on a\ncombinatorial double auction, and its applicability and effectiveness are\nevaluated in terms of resource efficiency and monetary benefits to auction\nparticipants (e.g., cloud users and providers). The proposed auction system\nassists them to decide when and how providers allocate their resources to which\nusers. Furthermore, we propose a distributed algorithm using a group formation\ngame that determines which users and providers will trade resources by their\ncooperative decisions. To find how to allocate the resources, the utility\noptimization problem is formulated as a binary integer programming problem, and\nthe nearly optimal solution is obtained by a heuristic algorithm with quadratic\ntime complexity. In comparison studies, the proposed real-time group auction\nsystem with cooperation outperforms an individual auction in terms of the\nresource efficiency (e.g., the request acceptance rate for users and resource\nutilization for providers) and monetary benefits (e.g., average payments for\nusers and total profits for providers). \n\n"}
{"id": "1304.0730", "contents": "Title: Representation, Approximation and Learning of Submodular Functions Using\n  Low-rank Decision Trees Abstract: We study the complexity of approximate representation and learning of\nsubmodular functions over the uniform distribution on the Boolean hypercube\n$\\{0,1\\}^n$. Our main result is the following structural theorem: any\nsubmodular function is $\\epsilon$-close in $\\ell_2$ to a real-valued decision\ntree (DT) of depth $O(1/\\epsilon^2)$. This immediately implies that any\nsubmodular function is $\\epsilon$-close to a function of at most\n$2^{O(1/\\epsilon^2)}$ variables and has a spectral $\\ell_1$ norm of\n$2^{O(1/\\epsilon^2)}$. It also implies the closest previous result that states\nthat submodular functions can be approximated by polynomials of degree\n$O(1/\\epsilon^2)$ (Cheraghchi et al., 2012). Our result is proved by\nconstructing an approximation of a submodular function by a DT of rank\n$4/\\epsilon^2$ and a proof that any rank-$r$ DT can be $\\epsilon$-approximated\nby a DT of depth $\\frac{5}{2}(r+\\log(1/\\epsilon))$.\n  We show that these structural results can be exploited to give an\nattribute-efficient PAC learning algorithm for submodular functions running in\ntime $\\tilde{O}(n^2) \\cdot 2^{O(1/\\epsilon^{4})}$. The best previous algorithm\nfor the problem requires $n^{O(1/\\epsilon^{2})}$ time and examples (Cheraghchi\net al., 2012) but works also in the agnostic setting. In addition, we give\nimproved learning algorithms for a number of related settings.\n  We also prove that our PAC and agnostic learning algorithms are essentially\noptimal via two lower bounds: (1) an information-theoretic lower bound of\n$2^{\\Omega(1/\\epsilon^{2/3})}$ on the complexity of learning monotone\nsubmodular functions in any reasonable model; (2) computational lower bound of\n$n^{\\Omega(1/\\epsilon^{2/3})}$ based on a reduction to learning of sparse\nparities with noise, widely-believed to be intractable. These are the first\nlower bounds for learning of submodular functions over the uniform\ndistribution. \n\n"}
{"id": "1304.0748", "contents": "Title: Mini-maximizing two qubit quantum computations Abstract: Two qubit quantum computations are viewed as two player, strictly competitive\ngames and a game-theoretic measure of optimality of these computations is\ndeveloped. To this end, the geometry of Hilbert space of quantum computations\nis used to establish the equivalence of game-theoretic solution concepts of\nNash equilibrium and mini-max outcomes in games of this type, and quantum\nmechanisms are designed for realizing these mini-max outcomes. \n\n"}
{"id": "1304.1007", "contents": "Title: Linear-in-$\\Delta$ Lower Bounds in the LOCAL Model Abstract: By prior work, there is a distributed algorithm that finds a maximal\nfractional matching (maximal edge packing) in $O(\\Delta)$ rounds, where\n$\\Delta$ is the maximum degree of the graph. We show that this is optimal:\nthere is no distributed algorithm that finds a maximal fractional matching in\n$o(\\Delta)$ rounds.\n  Our work gives the first linear-in-$\\Delta$ lower bound for a natural graph\nproblem in the standard model of distributed computing---prior lower bounds for\na wide range of graph problems have been at best logarithmic in $\\Delta$. \n\n"}
{"id": "1304.1351", "contents": "Title: On the complexity of strong Nash equilibrium: Hard-to-solve instances\n  and smoothed complexity Abstract: The computational characterization of game-theoretic solution concepts is a\ncentral topic in artificial intelligence, with the aim of developing\ncomputationally efficient tools for finding optimal ways to behave in strategic\ninteractions. The central solution concept in game theory is Nash equilibrium\n(NE). However, it fails to capture the possibility that agents can form\ncoalitions (even in the 2-agent case). Strong Nash equilibrium (SNE) refines NE\nto this setting. It is known that finding an SNE is NP-complete when the number\nof agents is constant. This hardness is solely due to the existence of\nmixed-strategy SNEs, given that the problem of enumerating all pure-strategy\nSNEs is trivially in P. Our central result is that, in order for a game to have\nat least one non-pure-strategy SNE, the agents' payoffs restricted to the\nagents' supports must, in the case of 2 agents, lie on the same line, and, in\nthe case of n agents, lie on an (n - 1)-dimensional hyperplane. Leveraging this\nresult, we provide two contributions. First, we develop worst-case instances\nfor support-enumeration algorithms. These instances have only one SNE and the\nsupport size can be chosen to be of any size-in particular, arbitrarily large.\nSecond, we prove that, unlike NE, finding an SNE is in smoothed polynomial\ntime: generic game instances (i.e., all instances except knife-edge cases) have\nonly pure-strategy SNEs. \n\n"}
{"id": "1304.2079", "contents": "Title: Learning Coverage Functions and Private Release of Marginals Abstract: We study the problem of approximating and learning coverage functions. A\nfunction $c: 2^{[n]} \\rightarrow \\mathbf{R}^{+}$ is a coverage function, if\nthere exists a universe $U$ with non-negative weights $w(u)$ for each $u \\in U$\nand subsets $A_1, A_2, \\ldots, A_n$ of $U$ such that $c(S) = \\sum_{u \\in\n\\cup_{i \\in S} A_i} w(u)$. Alternatively, coverage functions can be described\nas non-negative linear combinations of monotone disjunctions. They are a\nnatural subclass of submodular functions and arise in a number of applications.\n  We give an algorithm that for any $\\gamma,\\delta>0$, given random and uniform\nexamples of an unknown coverage function $c$, finds a function $h$ that\napproximates $c$ within factor $1+\\gamma$ on all but $\\delta$-fraction of the\npoints in time $poly(n,1/\\gamma,1/\\delta)$. This is the first fully-polynomial\nalgorithm for learning an interesting class of functions in the demanding PMAC\nmodel of Balcan and Harvey (2011). Our algorithms are based on several new\nstructural properties of coverage functions. Using the results in (Feldman and\nKothari, 2014), we also show that coverage functions are learnable agnostically\nwith excess $\\ell_1$-error $\\epsilon$ over all product and symmetric\ndistributions in time $n^{\\log(1/\\epsilon)}$. In contrast, we show that,\nwithout assumptions on the distribution, learning coverage functions is at\nleast as hard as learning polynomial-size disjoint DNF formulas, a class of\nfunctions for which the best known algorithm runs in time\n$2^{\\tilde{O}(n^{1/3})}$ (Klivans and Servedio, 2004).\n  As an application of our learning results, we give simple\ndifferentially-private algorithms for releasing monotone conjunction counting\nqueries with low average error. In particular, for any $k \\leq n$, we obtain\nprivate release of $k$-way marginals with average error $\\bar{\\alpha}$ in time\n$n^{O(\\log(1/\\bar{\\alpha}))}$. \n\n"}
{"id": "1304.3548", "contents": "Title: Crowdsourcing Dilemma Abstract: Crowdsourcing offers unprecedented potential for solving tasks efficiently by\ntapping into the skills of large groups of people. A salient feature of\ncrowdsourcing---its openness of entry---makes it vulnerable to malicious\nbehavior. Such behavior took place in a number of recent popular crowdsourcing\ncompetitions. We provide game-theoretic analysis of a fundamental tradeoff\nbetween the potential for increased productivity and the possibility of being\nset back by malicious behavior. Our results show that in crowdsourcing\ncompetitions malicious behavior is the norm, not the anomaly---a result\ncontrary to the conventional wisdom in the area. Counterintuitively, making the\nattacks more costly does not deter them but leads to a less desirable outcome.\nThese findings have cautionary implications for the design of crowdsourcing\ncompetitions. \n\n"}
{"id": "1304.3812", "contents": "Title: Time-Optimal Interactive Proofs for Circuit Evaluation Abstract: Recently, researchers have been working toward the development of practical\ngeneral-purpose protocols for verifiable computation. These protocols enable a\ncomputationally weak verifier to offload computations to a powerful but\nuntrusted prover, while providing the verifier with a guarantee that the prover\nperformed the computations correctly. Despite substantial progress, existing\nimplementations are not yet practical. The main bottleneck is typically the\nextra effort required by the prover to return an answer with a guarantee of\ncorrectness, compared to returning an answer with no guarantee.\n  We describe a refinement of a powerful interactive proof protocol originally\ndue to Goldwasser, Kalai, and Rothblum. Cormode, Mitzenmacher, and Thaler show\nhow to implement the prover in this protocol in time O(S log S), where S is the\nsize of an arithmetic circuit computing the function of interest. Our\nrefinements apply to circuits whose wiring pattern is sufficiently \"regular\";\nfor these circuits, we bring the runtime of the prover down to O(S). That is,\nour prover can evaluate the circuit with a guarantee of correctness, with only\na constant-factor blowup in work compared to evaluating the circuit with no\nguarantee.\n  We argue that our refinements capture a large class of circuits, and prove\nsome theorems formalizing this. Experimentally, our refinements yield a 200x\nspeedup for the prover over the implementation of Cormode et al., and our\nprover is less than 10x slower than a C++ program that simply evaluates the\ncircuit. Along the way, we describe a special-purpose protocol for matrix\nmultiplication that is of interest in its own right.\n  Our final contribution is a protocol targeted at general data parallel\ncomputation. Compared to prior work, this protocol can more efficiently verify\ncomplicated computations as long as that computation is applied independently\nto many pieces of data. \n\n"}
{"id": "1304.3816", "contents": "Title: Annotations for Sparse Data Streams Abstract: Motivated by cloud computing, a number of recent works have studied annotated\ndata streams and variants thereof. In this setting, a computationally weak\nverifier (cloud user), lacking the resources to store and manipulate his\nmassive input locally, accesses a powerful but untrusted prover (cloud\nservice). The verifier must work within the restrictive data streaming\nparadigm. The prover, who can annotate the data stream as it is read, must not\njust supply the answer but also convince the verifier of its correctness.\nIdeally, both the amount of annotation and the space used by the verifier\nshould be sublinear in the relevant input size parameters.\n  A rich theory of such algorithms -- which we call schemes -- has emerged.\nPrior work has shown how to leverage the prover's power to efficiently solve\nproblems that have no non-trivial standard data stream algorithms. However,\nwhile optimal schemes are now known for several basic problems, such optimality\nholds only for streams whose length is commensurate with the size of the data\nuniverse. In contrast, many real-world datasets are relatively sparse,\nincluding graphs that contain only O(n^2) edges, and IP traffic streams that\ncontain much fewer than the total number of possible IP addresses, 2^128 in\nIPv6.\n  We design the first schemes that allow both the annotation and the space\nusage to be sublinear in the total number of stream updates rather than the\nsize of the data universe. We solve significant problems, including variations\nof INDEX, SET-DISJOINTNESS, and FREQUENCY-MOMENTS, plus several natural\nproblems on graphs. On the other hand, we give a new lower bound that, for the\nfirst time, rules out smooth tradeoffs between annotation and space usage for a\nspecific problem. Our technique brings out new nuances in Merlin-Arthur\ncommunication complexity models, and provides a separation between online\nversions of the MA and AMA models. \n\n"}
{"id": "1304.5719", "contents": "Title: Synchronous Counting and Computational Algorithm Design Abstract: Consider a complete communication network on $n$ nodes, each of which is a\nstate machine. In synchronous 2-counting, the nodes receive a common clock\npulse and they have to agree on which pulses are \"odd\" and which are \"even\". We\nrequire that the solution is self-stabilising (reaching the correct operation\nfrom any initial state) and it tolerates $f$ Byzantine failures (nodes that\nsend arbitrary misinformation). Prior algorithms are expensive to implement in\nhardware: they require a source of random bits or a large number of states.\n  This work consists of two parts. In the first part, we use computational\ntechniques (often known as synthesis) to construct very compact deterministic\nalgorithms for the first non-trivial case of $f = 1$. While no algorithm exists\nfor $n < 4$, we show that as few as 3 states per node are sufficient for all\nvalues $n \\ge 4$. Moreover, the problem cannot be solved with only 2 states per\nnode for $n = 4$, but there is a 2-state solution for all values $n \\ge 6$.\n  In the second part, we develop and compare two different approaches for\nsynthesising synchronous counting algorithms. Both approaches are based on\ncasting the synthesis problem as a propositional satisfiability (SAT) problem\nand employing modern SAT-solvers. The difference lies in how to solve the SAT\nproblem: either in a direct fashion, or incrementally within a counter-example\nguided abstraction refinement loop. Empirical results suggest that the former\ntechnique is more efficient if we want to synthesise time-optimal algorithms,\nwhile the latter technique discovers non-optimal algorithms more quickly. \n\n"}
{"id": "1304.7718", "contents": "Title: A Dynamic Axiomatic Approach to First-Price Auctions Abstract: The first-price auction is popular in practice for its simplicity and\ntransparency. Moreover, its potential virtues grow in complex settings where\nincentive compatible auctions may generate little or no revenue. Unfortunately,\nthe first-price auction is poorly understood in theory because equilibrium is\nnot {\\em a priori} a credible predictor of bidder behavior.\n  We take a dynamic approach to studying first-price auctions: rather than\nbasing performance guarantees solely on static equilibria, we study the\nrepeated setting and show that robust performance guarantees may be derived\nfrom simple axioms of bidder behavior. For example, as long as a loser raises\nher bid quickly, a standard first-price auction will generate at least as much\nrevenue as a second-price auction. We generalize this dynamic technique to\ncomplex pay-your-bid auction settings and show that progressively stronger\nassumptions about bidder behavior imply progressively stronger guarantees about\nthe auction's performance.\n  Along the way, we find that the auctioneer's choice of bidding language is\ncritical when generalizing beyond the single-item setting, and we propose a\nspecific construction called the {\\em utility-target auction} that performs\nwell. The utility-target auction includes a bidder's final utility as an\nadditional parameter, identifying the single dimension along which she wishes\nto compete. This auction is closely related to profit-target bidding in\nfirst-price and ascending proxy package auctions and gives strong revenue\nguarantees for a variety of complex auction environments. Of particular\ninterest, the guaranteed existence of a pure-strategy equilibrium in the\nutility-target auction shows how Overture might have eliminated the cyclic\nbehavior in their generalized first-price sponsored search auction if bidders\ncould have placed more sophisticated bids. \n\n"}
{"id": "1305.1021", "contents": "Title: Parameterized Quantum Query Complexity of Graph Collision Abstract: We present three new quantum algorithms in the quantum query model for\n\\textsc{graph-collision} problem: \\begin{itemize} \\item an algorithm based on\ntree decomposition that uses $O\\left(\\sqrt{n}t^{\\sfrac{1}{6}}\\right)$ queries\nwhere $t$ is the treewidth of the graph; \\item an algorithm constructed on a\nspan program that improves a result by Gavinsky and Ito. The algorithm uses\n$O(\\sqrt{n}+\\sqrt{\\alpha^{**}})$ queries, where $\\alpha^{**}(G)$ is a graph\nparameter defined by \\[\\alpha^{**}(G):=\\min_{VC\\text{-- vertex cover\nof}G}{\\max_{\\substack{I\\subseteq VC\\\\I\\text{-- independent set}}}{\\sum_{v\\in\nI}{\\deg{v}}}};\\] \\item an algorithm for a subclass of circulant graphs that\nuses $O(\\sqrt{n})$ queries. \\end{itemize} We also present an example of a\npossibly difficult graph $G$ for which all the known graphs fail to solve graph\ncollision in $O(\\sqrt{n} \\log^c n)$ queries. \n\n"}
{"id": "1305.2446", "contents": "Title: Approximately Optimal Mechanisms for Strategyproof Facility Location:\n  Minimizing $L_p$ Norm of Costs Abstract: We consider the problem of locating a single facility on the real line. This\nfacility serves a set of agents, each of whom is located on the line, and\nincurs a cost equal to his distance from the facility. An agent's location is\nprivate information that is known only to him. Agents report their location to\na central planner who decides where to locate the facility. The planner's\nobjective is to minimize a \"social\" cost function that depends on the\nagent-costs. However, agents might not report truthfully; to address this\nissue, the planner must restrict himself to {\\em strategyproof} mechanisms, in\nwhich truthful reporting is a dominant strategy for each agent. A mechanism\nthat simply chooses the optimal solution is generally not strategyproof, and so\nthe planner aspires to use a mechanism that effectively {\\em approximates} his\nobjective function. In our paper, we study the problem described above with the\nsocial cost function being the $L_p$ norm of the vector of agent-costs. We show\nthat the median mechanism (which is known to be strategyproof) provides a\n$2^{1-\\frac{1}{p}}$ approximation ratio, and that is the optimal approximation\nratio among all deterministic strategyproof mechanisms. For randomized\nmechanisms, we present two results. First, we present a negative result: we\nshow that for integer $\\infty>p>2$, no mechanism---from a rather large class of\nrandomized mechanisms--- has an approximation ratio better than that of the\nmedian mechanism. This is in contrast to the case of $p=2$ and $p=\\infty$ where\na randomized mechanism provably helps improve the worst case approximation\nratio. Second, for the case of 2 agents, we show that a mechanism called LRM,\nfirst designed by Procaccia and Tennenholtz for the special case of\n$L_{\\infty}$, provides the optimal approximation ratio among all randomized\nmechanisms. \n\n"}
{"id": "1305.3334", "contents": "Title: Online Learning in a Contract Selection Problem Abstract: In an online contract selection problem there is a seller which offers a set\nof contracts to sequentially arriving buyers whose types are drawn from an\nunknown distribution. If there exists a profitable contract for the buyer in\nthe offered set, i.e., a contract with payoff higher than the payoff of not\naccepting any contracts, the buyer chooses the contract that maximizes its\npayoff. In this paper we consider the online contract selection problem to\nmaximize the sellers profit. Assuming that a structural property called ordered\npreferences holds for the buyer's payoff function, we propose online learning\nalgorithms that have sub-linear regret with respect to the best set of\ncontracts given the distribution over the buyer's type. This problem has many\napplications including spectrum contracts, wireless service provider data plans\nand recommendation systems. \n\n"}
{"id": "1305.6376", "contents": "Title: Fractional Pebbling Game Lower Bounds Abstract: Fractional pebbling is a generalization of black-white pebbling introduced\nrecently. In this reasearch paper we solve an open problem by proving a tight\nlower bound on the pebble weight required to fractionally pebble a balanced\nd-ary tree of height h. This bound has close ties with branching programs and\nthe separation of P from NL. \n\n"}
{"id": "1306.0816", "contents": "Title: A Critical Assessment of Cost-Based Nash Methods for Demand Scheduling\n  in Smart Grids Abstract: Demand-side management (DSM) is becoming an increasingly important component\nof the envisioned smart grid. The ability to improve the efficiency of energy\nuse in the power system by altering demand is widely viewed as being not merely\npromising but in fact essential. However, while the advantages of DSM are\nclear, arriving at an efficient implementation has so far proven to be less\nstraightforward. There have recently been many proposals put forth in the\nliterature to tackle the demand scheduling aspect of DSM. One particular\napproach based on a game-theoretic treatment of the day-ahead load-scheduling\nproblem has recently gained tremendous popularity in the DSM literature. In\nthis letter, an assessment of this approach is conducted, and its main result\nis challenged. \n\n"}
{"id": "1306.2217", "contents": "Title: Multi-parameter complexity analysis for constrained size graph problems:\n  using greediness for parameterization Abstract: We study the parameterized complexity of a broad class of problems called\n\"local graph partitioning problems\" that includes the classical fixed\ncardinality problems as max k-vertex cover, k-densest subgraph, etc. By\ndeveloping a technique \"greediness-for-parameterization\", we obtain fixed\nparameter algorithms with respect to a pair of parameters k, the size of the\nsolution (but not its value) and \\Delta, the maximum degree of the input graph.\nIn particular, greediness-for-parameterization improves asymptotic running\ntimes for these problems upon random separation (that is a special case of\ncolor coding) and is more intuitive and simple. Then, we show how these results\ncan be easily extended for getting standard-parameterization results (i.e.,\nwith parameter the value of the optimal solution) for a well known local graph\npartitioning problem. \n\n"}
{"id": "1306.2437", "contents": "Title: Query Complexity of Correlated Equilibrium Abstract: We study lower bounds on the query complexity of determining correlated\nequilibrium. In particular, we consider a query model in which an n-player game\nis specified via a black box that returns players' utilities at pure action\nprofiles. In this model we establish that in order to compute a correlated\nequilibrium any deterministic algorithm must query the black box an exponential\n(in n) number of times. \n\n"}
{"id": "1306.4302", "contents": "Title: Network bargaining with general capacities Abstract: We study balanced solutions for network bargaining games with general\ncapacities, where agents can participate in a fixed but arbitrary number of\ncontracts. We provide the first polynomial time algorithm for computing\nbalanced solutions for these games. In addition, we prove that an instance has\na balanced solution if and only if it has a stable one. Our methods use a new\nidea of reducing an instance with general capacities to a network bargaining\ngame with unit capacities defined on an auxiliary graph. This represents a\ndeparture from previous approaches, which rely on computing an allocation in\nthe intersection of the core and prekernel of a corresponding cooperative game,\nand then proving that the solution corresponding to this allocation is\nbalanced. In fact, we show that such cooperative game methods do not extend to\ngeneral capacity games, since contrary to the case of unit capacities, there\nexist allocations in the intersection of the core and prekernel with no\ncorresponding balanced solution. Finally, we identify two sufficient conditions\nunder which the set of balanced solutions corresponds to the intersection of\nthe core and prekernel, thereby extending the class of games for which this\nresult was previously known. \n\n"}
{"id": "1306.6929", "contents": "Title: Power indices of influence games and new centrality measures for social\n  networks Abstract: In social network analysis, there is a common perception that influence is\nrelevant to determine the global behavior of the society and thus it can be\nused to enforce cooperation by targeting an adequate initial set of individuals\nor to analyze global choice processes. Here we propose centrality measures that\ncan be used to analyze the relevance of the actors in process related to spread\nof influence. In [39] it was considered a multiagent system in which the agents\nare eager to perform a collective task depending on the perception of the\nwillingness to perform the task of other individuals. The setting is modeled\nusing a notion of simple games called influence games. Those games are defined\non graphs were the nodes are labeled by their influence threshold and the\nspread of influence between its nodes is used to determine whether a coalition\nis winning or not. Influence games provide tools to measure the importance of\nthe actors of a social network by means of classic power indices and provide a\nframework to consider new centrality criteria. In this paper we consider two of\nthe most classical power indices, i.e., Banzhaf and Shapley-Shubik indices, as\ncentrality measures for social networks in influence games. Although there is\nsome work related to specific scenarios of game-theoretic networks, here we use\nsuch indices as centrality measures in any social network where the spread of\ninfluence phenomenon can be applied. Further, we define new centrality measures\nsuch as satisfaction and effort that, as far as we know, have not been\nconsidered so far. We also perform a comparison of the proposed measures with\nother three classic centrality measures, degree, closeness and betweenness,\nconsidering three social networks. We show that in some cases our measurements\nprovide centrality hierarchies similar to those of other measures, while in\nother cases provide different hierarchies. \n\n"}
{"id": "1307.1670", "contents": "Title: A New Mathematical Model for Evolutionary Games on Finite Networks of\n  Players Abstract: A new mathematical model for evolutionary games on graphs is proposed to\nextend the classical replicator equation to finite populations of players\norganized on a network with generic topology. Classical results from game\ntheory, evolutionary game theory and graph theory are used. More specifically,\neach player is placed in a vertex of the graph and he is seen as an infinite\npopulation of replicators which replicate within the vertex. At each time\ninstant, a game is played by two replicators belonging to different connected\nvertices, and the outcome of the game influences their ability of producing\noffspring. Then, the behavior of a vertex player is determined by the\ndistribution of strategies used by the internal replicators. Under suitable\nhypotheses, the proposed model is equivalent to the classical replicator\nequation. Extended simulations are performed to show the dynamical behavior of\nthe solutions and the potentialities of the developed model. \n\n"}
{"id": "1307.2225", "contents": "Title: An Algorithmic Framework for Strategic Fair Division Abstract: We study the paradigmatic fair division problem of allocating a divisible\ngood among agents with heterogeneous preferences, commonly known as cake\ncutting. Classical cake cutting protocols are susceptible to manipulation. Do\ntheir strategic outcomes still guarantee fairness?\n  To address this question we adopt a novel algorithmic approach, by designing\na concrete computational framework for fair division---the class of Generalized\nCut and Choose (GCC) protocols}---and reasoning about the game-theoretic\nproperties of algorithms that operate in this model. The class of GCC protocols\nincludes the most important discrete cake cutting protocols, and turns out to\nbe compatible with the study of fair division among strategic agents. In\nparticular, GCC protocols are guaranteed to have approximate subgame perfect\nNash equilibria, or even exact equilibria if the protocol's tie-breaking rule\nis flexible. We further observe that the (approximate) equilibria of\nproportional GCC protocols---which guarantee each of the $n$ agents a\n$1/n$-fraction of the cake---must be (approximately) proportional. Finally, we\ndesign a protocol in this framework with the property that its Nash equilibrium\nallocations coincide with the set of (contiguous) envy-free allocations. \n\n"}
{"id": "1307.4228", "contents": "Title: A Model of Human Cooperation in Social Dilemmas Abstract: Social dilemmas are situations in which collective interests are at odds with\nprivate interests: pollution, depletion of natural resources, and intergroup\nconflicts, are at their core social dilemmas.\n  Because of their multidisciplinarity and their importance, social dilemmas\nhave been studied by economists, biologists, psychologists, sociologists, and\npolitical scientists. These studies typically explain tendency to cooperation\nby dividing people in proself and prosocial types, or appealing to forms of\nexternal control or, in iterated social dilemmas, to long-term strategies.\n  But recent experiments have shown that cooperation is possible even in\none-shot social dilemmas without forms of external control and the rate of\ncooperation typically depends on the payoffs. This makes impossible a\npredictive division between proself and prosocial people and proves that people\nhave attitude to cooperation by nature.\n  The key innovation of this article is in fact to postulate that humans have\nattitude to cooperation by nature and consequently they do not act a priori as\nsingle agents, as assumed by standard economic models, but they forecast how a\nsocial dilemma would evolve if they formed coalitions and then they act\naccording to their most optimistic forecast. Formalizing this idea we propose\nthe first predictive model of human cooperation able to organize a number of\ndifferent experimental findings that are not explained by the standard model.\nWe show also that the model makes satisfactorily accurate quantitative\npredictions of population average behavior in one-shot social dilemmas. \n\n"}
{"id": "1307.4475", "contents": "Title: Slot Games for Detecting Timing Leaks of Programs Abstract: In this paper we describe a method for verifying secure information flow of\nprograms, where apart from direct and indirect flows a secret information can\nbe leaked through covert timing channels. That is, no two computations of a\nprogram that differ only on high-security inputs can be distinguished by\nlow-security outputs and timing differences. We attack this problem by using\nslot-game semantics for a quantitative analysis of programs. We show how\nslot-games model can be used for performing a precise security analysis of\nprograms, that takes into account both extensional and intensional properties\nof programs. The practicality of this approach for automated verification is\nalso shown. \n\n"}
{"id": "1307.7322", "contents": "Title: Complexity of Manipulation, Bribery, and Campaign Management in Bucklin\n  and Fallback Voting Abstract: A central theme in computational social choice is to study the extent to\nwhich voting systems computationally resist manipulative attacks seeking to\ninfluence the outcome of elections, such as manipulation (i.e., strategic\nvoting), control, and bribery. Bucklin and fallback voting are among the voting\nsystems with the broadest resistance (i.e., NP-hardness) to control attacks.\nHowever, only little is known about their behavior regarding manipulation and\nbribery attacks. We comprehensively investigate the computational resistance of\nBucklin and fallback voting for many of the common manipulation and bribery\nscenarios; we also complement our discussion by considering several campaign\nmanagement problems for Bucklin and fallback. \n\n"}
{"id": "1307.7838", "contents": "Title: Asymmetric-valued Spectrum Auction and Competition in Wireless Broadband\n  Services Abstract: We study bidding and pricing competition between two spiteful mobile network\noperators (MNOs) with considering their existing spectrum holdings. Given\nasymmetric-valued spectrum blocks are auctioned off to them via a first-price\nsealed-bid auction, we investigate the interactions between two spiteful MNOs\nand users as a three-stage dynamic game and characterize the dynamic game's\nequilibria. We show an asymmetric pricing structure and different market share\nbetween two spiteful MNOs. Perhaps counter-intuitively, our results show that\nthe MNO who acquires the less-valued spectrum block always lowers his service\nprice despite providing double-speed LTE service to users. We also show that\nthe MNO who acquires the high-valued spectrum block, despite charing a higher\nprice, still achieves more market share than the other MNO. We further show\nthat the competition between two MNOs leads to some loss of their revenues. By\ninvestigating a cross-over point at which the MNOs' profits are switched, it\nserves as the benchmark of practical auction designs. \n\n"}
{"id": "1308.0068", "contents": "Title: Communication lower bounds and optimal algorithms for programs that\n  reference arrays -- Part 1 Abstract: The movement of data (communication) between levels of a memory hierarchy, or\nbetween parallel processors on a network, can greatly dominate the cost of\ncomputation, so algorithms that minimize communication are of interest.\nMotivated by this, attainable lower bounds for the amount of communication\nrequired by algorithms were established by several groups for a variety of\nalgorithms, including matrix computations. Prior work of\nBallard-Demmel-Holtz-Schwartz relied on a geometric inequality of Loomis and\nWhitney for this purpose. In this paper the general theory of discrete\nmultilinear Holder-Brascamp-Lieb (HBL) inequalities is used to establish\ncommunication lower bounds for a much wider class of algorithms. In some cases,\nalgorithms are presented which attain these lower bounds.\n  Several contributions are made to the theory of HBL inequalities proper. The\noptimal constant in such an inequality for torsion-free Abelian groups is shown\nto equal one whenever it is finite. Bennett-Carbery-Christ-Tao had\ncharacterized the tuples of exponents for which such an inequality is valid as\nthe convex polyhedron defined by a certain finite list of inequalities. The\nproblem of constructing an algorithm to decide whether a given inequality is on\nthis list, is shown to be equivalent to Hilbert's Tenth Problem over the\nrationals, which remains open. Nonetheless, an algorithm which computes the\npolyhedron itself is constructed. \n\n"}
{"id": "1308.1382", "contents": "Title: Pricing Ad Slots with Consecutive Multi-unit Demand Abstract: We consider the optimal pricing problem for a model of the rich media\nadvertisement market, as well as other related applications. In this market,\nthere are multiple buyers (advertisers), and items (slots) that are arranged in\na line such as a banner on a website. Each buyer desires a particular number of\n{\\em consecutive} slots and has a per-unit-quality value $v_i$ (dependent on\nthe ad only) while each slot $j$ has a quality $q_j$ (dependent on the position\nonly such as click-through rate in position auctions). Hence, the valuation of\nthe buyer $i$ for item $j$ is $v_iq_j$. We want to decide the allocations and\nthe prices in order to maximize the total revenue of the market maker.\n  A key difference from the traditional position auction is the advertiser's\nrequirement of a fixed number of consecutive slots. Consecutive slots may be\nneeded for a large size rich media ad. We study three major pricing mechanisms,\nthe Bayesian pricing model, the maximum revenue market equilibrium model and an\nenvy-free solution model. Under the Bayesian model, we design a polynomial time\ncomputable truthful mechanism which is optimum in revenue. For the market\nequilibrium paradigm, we find a polynomial time algorithm to obtain the maximum\nrevenue market equilibrium solution. In envy-free settings, an optimal solution\nis presented when the buyers have the same demand for the number of consecutive\nslots. We conduct a simulation that compares the revenues from the above\nschemes and gives convincing results. \n\n"}
{"id": "1308.2409", "contents": "Title: On the Parameterized Complexity of Reconfiguration Problems Abstract: We present the first results on the parameterized complexity of\nreconfiguration problems, where a reconfiguration version of an optimization\nproblem $Q$ takes as input two feasible solutions $S$ and $T$ and determines if\nthere is a sequence of {\\em reconfiguration steps} that can be applied to\ntransform $S$ into $T$ such that each step results in a feasible solution to\n$Q$. For most of the results in this paper, $S$ and $T$ are subsets of vertices\nof a given graph and a reconfiguration step adds or deletes a vertex. Our study\nis motivated by recent results establishing that for most NP-hard problems, the\nclassical complexity of reconfiguration is PSPACE-complete. We address the\nquestion for several important graph properties under two natural\nparameterizations: $k$, the size of the solutions, and $\\ell$, the length of\nthe sequence of steps. Our first general result is an algorithmic paradigm, the\n{\\em reconfiguration kernel}, used to obtain fixed-parameter algorithms for the\nreconfiguration versions of {\\sc Vertex Cover} and, more generally, {\\sc\nBounded Hitting Set} and {\\sc Feedback Vertex Set}, all parameterized by $k$.\nIn contrast, we show that reconfiguring {\\sc Unbounded Hitting Set} is\n$W[2]$-hard when parameterized by $k+\\ell$. We also demonstrate the\n$W[1]$-hardness of the reconfiguration versions of a large class of\nmaximization problems parameterized by $k+\\ell$, and of their corresponding\ndeletion problems parameterized by $\\ell$; in doing so, we show that there\nexist problems in FPT when parameterized by $k$, but whose reconfiguration\nversions are $W[1]$-hard when parameterized by $k+\\ell$. \n\n"}
{"id": "1308.2576", "contents": "Title: Evolutionary Extortion and Mischief: Zero Determinant strategies in\n  iterated 2x2 games Abstract: This paper studies the mechanisms, implications, and potential applications\nof the recently discovered class of Zero Determinant (ZD) strategies in\niterated 2x2 games. These strategies were reported to successfully extort pure\neconomic maximizers, and to mischievously determine the set of feasible\nlong-term payoffs in iterated Prisoners' Dilemma by enforcing linear\nconstraints on both players' expected average scores.\n  These results are generalized for all symmetric 2x2 games and a general\nBattle of the Sexes, exemplified by four common games. Additionally, a\ncomparison to conventional strategies is made and typical ZD gameplay\nsimulations are analyzed along with convergence speeds. Several response\nstrategies are discussed, including a glance on how time preferences change\nprevious results. Furthermore, a possibility of retaliation is presented: when\nmaximin scores exceed the minimum symmetric payoff, it is possible to extort\nthe extortioner.\n  Finally, a summary of findings from evolutionary game theory shows that\nmischief is limited by its own malice. Nevertheless, this does not challenge\nthe result that mindless economic maximization is subject to extortion: the\nstudy of ZD strategies reveals exciting new perspectives and opportunities in\ngame theory, both evolutionary and classic. \n\n"}
{"id": "1308.2617", "contents": "Title: Independent Set, Induced Matching, and Pricing: Connections and Tight\n  (Subexponential Time) Approximation Hardnesses Abstract: We present a series of almost settled inapproximability results for three\nfundamental problems. The first in our series is the subexponential-time\ninapproximability of the maximum independent set problem, a question studied in\nthe area of parameterized complexity. The second is the hardness of\napproximating the maximum induced matching problem on bounded-degree bipartite\ngraphs. The last in our series is the tight hardness of approximating the\nk-hypergraph pricing problem, a fundamental problem arising from the area of\nalgorithmic game theory. In particular, assuming the Exponential Time\nHypothesis, our two main results are:\n  - For any r larger than some constant, any r-approximation algorithm for the\nmaximum independent set problem must run in at least\n2^{n^{1-\\epsilon}/r^{1+\\epsilon}} time. This nearly matches the upper bound of\n2^{n/r} (Cygan et al., 2008). It also improves some hardness results in the\ndomain of parameterized complexity (e.g., Escoffier et al., 2012 and Chitnis et\nal., 2013)\n  - For any k larger than some constant, there is no polynomial time min\n(k^{1-\\epsilon}, n^{1/2-\\epsilon})-approximation algorithm for the k-hypergraph\npricing problem, where n is the number of vertices in an input graph. This\nalmost matches the upper bound of min (O(k), \\tilde O(\\sqrt{n})) (by Balcan and\nBlum, 2007 and an algorithm in this paper).\n  We note an interesting fact that, in contrast to n^{1/2-\\epsilon} hardness\nfor polynomial-time algorithms, the k-hypergraph pricing problem admits\nn^{\\delta} approximation for any \\delta >0 in quasi-polynomial time. This puts\nthis problem in a rare approximability class in which approximability\nthresholds can be improved significantly by allowing algorithms to run in\nquasi-polynomial time. \n\n"}
{"id": "1308.3506", "contents": "Title: Computational Rationalization: The Inverse Equilibrium Problem Abstract: Modeling the purposeful behavior of imperfect agents from a small number of\nobservations is a challenging task. When restricted to the single-agent\ndecision-theoretic setting, inverse optimal control techniques assume that\nobserved behavior is an approximately optimal solution to an unknown decision\nproblem. These techniques learn a utility function that explains the example\nbehavior and can then be used to accurately predict or imitate future behavior\nin similar observed or unobserved situations.\n  In this work, we consider similar tasks in competitive and cooperative\nmulti-agent domains. Here, unlike single-agent settings, a player cannot\nmyopically maximize its reward; it must speculate on how the other agents may\nact to influence the game's outcome. Employing the game-theoretic notion of\nregret and the principle of maximum entropy, we introduce a technique for\npredicting and generalizing behavior. \n\n"}
{"id": "1308.3613", "contents": "Title: Polynomial kernels collapse the W-hierarchy Abstract: We prove that, for many parameterized problems in the class FPT, the\nexistence of polynomial kernels implies the collapse of the W-hierarchy (i.e.,\nW[P] = FPT). The collapsing results are also extended to assumed exponential\nkernels for problems in the class FPT. In particular, we establish a close\nrelationship between polynomial (and exponential) kernelizability and the\nexistence of sub-exponential time algorithms for a spectrum of circuit\nsatisfiability problems in FPT. To the best of our knowledge, this is the first\nwork that connects hardness for polynomial kernelizability of FPT problems to\nparameterized intractability. Our work also offers some new insights into the\nclass FPT. \n\n"}
{"id": "1308.5835", "contents": "Title: Backhaul-Aware Interference Management in the Uplink of Wireless Small\n  Cell Networks Abstract: The design of distributed mechanisms for interference management is one of\nthe key challenges in emerging wireless small cell networks whose backhaul is\ncapacity limited and heterogeneous (wired, wireless and a mix thereof). In this\npaper, a novel, backhaul-aware approach to interference management in wireless\nsmall cell networks is proposed. The proposed approach enables macrocell user\nequipments (MUEs) to optimize their uplink performance, by exploiting the\npresence of neighboring small cell base stations. The problem is formulated as\na noncooperative game among the MUEs that seek to optimize their delay-rate\ntradeoff, given the conditions of both the radio access network and the --\npossibly heterogeneous -- backhaul. To solve this game, a novel, distributed\nlearning algorithm is proposed using which the MUEs autonomously choose their\noptimal uplink transmission strategies, given a limited amount of available\ninformation. The convergence of the proposed algorithm is shown and its\nproperties are studied. Simulation results show that, under various types of\nbackhauls, the proposed approach yields significant performance gains, in terms\nof both average throughput and delay for the MUEs, when compared to existing\nbenchmark algorithms. \n\n"}
{"id": "1308.6025", "contents": "Title: Small-Support Approximate Correlated Equilibria Abstract: We prove the existence of approximate correlated equilibrium of support size\npolylogarithmic in the number of players and the number of actions per player.\nIn particular, using the probabilistic method, we show that there exists a\nmultiset of polylogarithmic size such that the uniform distribution over this\nmultiset forms an approximate correlated equilibrium. Along similar lines, we\nestablish the existence of approximate coarse correlated equilibrium with\nlogarithmic support.\n  We complement these results by considering the computational complexity of\ndetermining small-support approximate equilibria. We show that random sampling\ncan be used to efficiently determine an approximate coarse correlated\nequilibrium with logarithmic support. But, such a tight result does not hold\nfor correlated equilibrium, i.e., sampling might generate an approximate\ncorrelated equilibrium of support size \\Omega(m) where m is the number of\nactions per player. Finally, we show that finding an exact correlated\nequilibrium with smallest possible support is NP-hard under Cook reductions,\neven in the case of two-player zero-sum games. \n\n"}
{"id": "1309.0563", "contents": "Title: Approximate Constraint Satisfaction Requires Large LP Relaxations Abstract: We prove super-polynomial lower bounds on the size of linear programming\nrelaxations for approximation versions of constraint satisfaction problems. We\nshow that for these problems, polynomial-sized linear programs are exactly as\npowerful as programs arising from a constant number of rounds of the\nSherali-Adams hierarchy.\n  In particular, any polynomial-sized linear program for Max Cut has an\nintegrality gap of 1/2 and any such linear program for Max 3-Sat has an\nintegrality gap of 7/8. \n\n"}
{"id": "1309.1220", "contents": "Title: A Mean Field Game Approach to Scheduling in Cellular Systems Abstract: We study auction-theoretic scheduling in cellular networks using the idea of\nmean field equilibrium (MFE). Here, agents model their opponents through a\ndistribution over their action spaces and play the best response. The system is\nat an MFE if this action is itself a sample drawn from the assumed\ndistribution. In our setting, the agents are smart phone apps that generate\nservice requests, experience waiting costs, and bid for service from base\nstations. We show that if we conduct a second-price auction at each base\nstation, there exists an MFE that would schedule the app with the longest queue\nat each time. The result suggests that auctions can attain the same desirable\nresults as queue-length-based scheduling. We present results on the asymptotic\nconvergence of a system with a finite number of agents to the mean field case,\nand conclude with simulation results illustrating the simplicity of computation\nof the MFE. \n\n"}
{"id": "1309.2529", "contents": "Title: Limits of Efficiency in Sequential Auctions Abstract: We study the efficiency of sequential first-price item auctions at (subgame\nperfect) equilibrium. This auction format has recently attracted much\nattention, with previous work establishing positive results for unit-demand\nvaluations and negative results for submodular valuations. This leaves a large\ngap in our understanding between these valuation classes. In this work we\nresolve this gap on the negative side. In particular, we show that even in the\nvery restricted case in which each bidder has either an additive valuation or a\nunit-demand valuation, there exist instances in which the inefficiency at\nequilibrium grows linearly with the minimum of the number of items and the\nnumber of bidders. Moreover, these inefficient equilibria persist even under\niterated elimination of weakly dominated strategies. Our main result implies\nlinear inefficiency for many natural settings, including auctions with gross\nsubstitute valuations, capacitated valuations, budget-additive valuations, and\nadditive valuations with hard budget constraints on the payments. Another\nimplication is that the inefficiency in sequential auctions is driven by the\nmaximum number of items contained in any player's optimal set, and this is\ntight. For capacitated valuations, our results imply a lower bound that equals\nthe maximum capacity of any bidder, which is tight following the upper-bound\ntechnique established by Paes Leme et al. \\cite{PaesLeme2012}. \n\n"}
{"id": "1309.5206", "contents": "Title: New Algorithms for Solving Tropical Linear Systems Abstract: The problem of solving tropical linear systems, a natural problem of tropical\nmathematics, has already proven to be very interesting from the algorithmic\npoint of view: it is known to be in $NP\\cap coNP$ but no polynomial time\nalgorithm is known, although counterexamples for existing pseudopolynomial\nalgorithms are (and have to be) very complex.\n  In this work, we continue the study of algorithms for solving tropical linear\nsystems. First, we present a new reformulation of Grigoriev's algorithm that\nbrings it closer to the algorithm of Akian, Gaubert, and Guterman; this lets us\nformulate a whole family of new algorithms, and we present algorithms from this\nfamily for which no known superpolynomial counterexamples work. Second, we\npresent a family of algorithms for solving overdetermined tropical systems. We\nshow that for weakly overdetermined systems, there are polynomial algorithms in\nthis family. We also present a concrete algorithm from this family that can\nsolve a tropical linear system defined by an $m\\times n$ matrix with maximal\nelement $M$ in time $\\Theta\\left({m \\choose n} \\mathrm{poly}\\left(m, n, \\log\nM\\right)\\right)$, and this time matches the complexity of the best of\npreviously known algorithms for feasibility testing. \n\n"}
{"id": "1309.5439", "contents": "Title: Meet Your Expectations With Guarantees: Beyond Worst-Case Synthesis in\n  Quantitative Games Abstract: We extend the quantitative synthesis framework by going beyond the\nworst-case. On the one hand, classical analysis of two-player games involves an\nadversary (modeling the environment of the system) which is purely antagonistic\nand asks for strict guarantees. On the other hand, stochastic models like\nMarkov decision processes represent situations where the system is faced to a\npurely randomized environment: the aim is then to optimize the expected payoff,\nwith no guarantee on individual outcomes. We introduce the beyond worst-case\nsynthesis problem, which is to construct strategies that guarantee some\nquantitative requirement in the worst-case while providing an higher expected\nvalue against a particular stochastic model of the environment given as input.\nThis problem is relevant to produce system controllers that provide nice\nexpected performance in the everyday situation while ensuring a strict (but\nrelaxed) performance threshold even in the event of very bad (while unlikely)\ncircumstances. We study the beyond worst-case synthesis problem for two\nimportant quantitative settings: the mean-payoff and the shortest path. In both\ncases, we show how to decide the existence of finite-memory strategies\nsatisfying the problem and how to synthesize one if one exists. We establish\nalgorithms and we study complexity bounds and memory requirements. \n\n"}
{"id": "1309.6474", "contents": "Title: On the Stability of Generalized Second Price Auctions with Budgets Abstract: The Generalized Second Price (GSP) auction used typically to model sponsored\nsearch auctions does not include the notion of budget constraints, which is\npresent in practice. Motivated by this, we introduce the different variants of\nGSP auctions that take budgets into account in natural ways. We examine their\nstability by focusing on the existence of Nash equilibria and envy-free\nassignments. We highlight the differences between these mechanisms and find\nthat only some of them exhibit both notions of stability. This shows the\nimportance of carefully picking the right mechanism to ensure stable outcomes\nin the presence of budgets \n\n"}
{"id": "1309.7258", "contents": "Title: Polylogarithmic Supports are required for Approximate Well-Supported\n  Nash Equilibria below 2/3 Abstract: In an epsilon-approximate Nash equilibrium, a player can gain at most epsilon\nin expectation by unilateral deviation. An epsilon well-supported approximate\nNash equilibrium has the stronger requirement that every pure strategy used\nwith positive probability must have payoff within epsilon of the best response\npayoff. Daskalakis, Mehta and Papadimitriou conjectured that every win-lose\nbimatrix game has a 2/3-well-supported Nash equilibrium that uses supports of\ncardinality at most three. Indeed, they showed that such an equilibrium will\nexist subject to the correctness of a graph-theoretic conjecture. Regardless of\nthe correctness of this conjecture, we show that the barrier of a 2/3 payoff\nguarantee cannot be broken with constant size supports; we construct win-lose\ngames that require supports of cardinality at least Omega((log n)^(1/3)) in any\nepsilon-well supported equilibrium with epsilon < 2/3. The key tool in showing\nthe validity of the construction is a proof of a bipartite digraph variant of\nthe well-known Caccetta-Haggkvist conjecture. A probabilistic argument shows\nthat there exist epsilon-well-supported equilibria with supports of cardinality\nO(log n/(epsilon^2)), for any epsilon> 0; thus, the polylogarithmic cardinality\nbound presented cannot be greatly improved. We also show that for any delta >\n0, there exist win-lose games for which no pair of strategies with support\nsizes at most two is a (1-delta)-well-supported Nash equilibrium. In contrast,\nevery bimatrix game with payoffs in [0,1] has a 1/2-approximate Nash\nequilibrium where the supports of the players have cardinality at most two. \n\n"}
{"id": "1309.7713", "contents": "Title: Span-program-based quantum algorithm for tree detection Abstract: Span program is a linear-algebraic model of computation originally proposed\nfor studying the complexity theory. Recently, it has become a useful tool for\ndesigning quantum algorithms. In this paper, we present a time-efficient\nspan-program-based quantum algorithm for the following problem. Let $T$ be an\narbitrary tree. Given query access to the adjacency matrix of a graph $G$ with\n$n$ vertices, we need to determine whether $G$ contains $T$ as a subgraph, or\n$G$ does not contain $T$ as a minor, under the promise that one of these cases\nholds. We call this problem the subgraph/not-a-minor problem for $T$. We show\nthat this problem can be solved by a bounded-error quantum algorithm with\n$O(n)$ query complexity and $\\tilde{O}(n)$ time complexity. The query\ncomplexity is optimal, and the time complexity is tight up to polylog factors. \n\n"}
{"id": "1310.0398", "contents": "Title: On the optimality of approximation schemes for the classical scheduling\n  problem Abstract: We consider the classical scheduling problem on parallel identical machines\nto minimize the makespan, and achieve the following results under the\nExponential Time Hypothesis (ETH)\n  1. The scheduling problem on a constant number $m$ of identical machines,\nwhich is denoted as $Pm||C_{max}$, is known to admit a fully polynomial time\napproximation scheme (FPTAS) of running time $O(n) + (1/\\epsilon)^{O(m)}$\n(indeed, the algorithm works for an even more general problem where machines\nare unrelated). We prove this algorithm is essentially the best possible in the\nsense that a $(1/\\epsilon)^{O(m^{1-\\delta})}+n^{O(1)}$ time FPTAS for any\n$\\delta>0$ implies that ETH fails.\n  2. The scheduling problem on an arbitrary number of identical machines, which\nis denoted as $P||C_{max}$, is known to admit a polynomial time approximation\nscheme (PTAS) of running time $2^{O(1/\\epsilon^2\\log^3(1/\\epsilon))}+n^{O(1)}$.\nWe prove this algorithm is nearly optimal in the sense that a\n$2^{O((1/\\epsilon)^{1-\\delta})}+n^{O(1)}$ time PTAS for any $\\delta>0$ implies\nthat ETH fails, leaving a small room for improvement.\n  To obtain these results we will provide two new reductions from 3SAT, one for\n$Pm||C_{max}$ and another for $P||C_{max}$. Indeed, the new reductions explore\nthe structure of scheduling problems and can also lead to other interesting\nresults. For example, using the framework of our reduction for $P||C_{max}$,\nChen et al. (arXiv:1306.3727) is able to prove the APX-hardness of the\nscheduling problem in which the matrix of job processing times\n$P=(p_{ij})_{m\\times n}$ is of rank 3, solving the open problem mentioned by\nBhaskara et al. (SODA 2013). \n\n"}
{"id": "1310.0720", "contents": "Title: A Survey on Device-to-Device Communication in Cellular Networks Abstract: Device-to-Device (D2D) communication was initially proposed in cellular\nnetworks as a new paradigm to enhance network performance. The emergence of new\napplications such as content distribution and location-aware advertisement\nintroduced new use-cases for D2D communications in cellular networks. The\ninitial studies showed that D2D communication has advantages such as increased\nspectral efficiency and reduced communication delay. However, this\ncommunication mode introduces complications in terms of interference control\noverhead and protocols that are still open research problems. The feasibility\nof D2D communications in LTE-A is being studied by academia, industry, and the\nstandardization bodies. To date, there are more than 100 papers available on\nD2D communications in cellular networks and, there is no survey on this field.\nIn this article, we provide a taxonomy based on the D2D communicating spectrum\nand review the available literature extensively under the proposed taxonomy.\nMoreover, we provide new insights to the over-explored and under-explored areas\nwhich lead us to identify open research problems of D2D communication in\ncellular networks. \n\n"}
{"id": "1310.2935", "contents": "Title: Limit Synchronization in Markov Decision Processes Abstract: Markov decision processes (MDP) are finite-state systems with both strategic\nand probabilistic choices. After fixing a strategy, an MDP produces a sequence\nof probability distributions over states. The sequence is eventually\nsynchronizing if the probability mass accumulates in a single state, possibly\nin the limit. Precisely, for 0 <= p <= 1 the sequence is p-synchronizing if a\nprobability distribution in the sequence assigns probability at least p to some\nstate, and we distinguish three synchronization modes: (i) sure winning if\nthere exists a strategy that produces a 1-synchronizing sequence; (ii)\nalmost-sure winning if there exists a strategy that produces a sequence that\nis, for all epsilon > 0, a (1-epsilon)-synchronizing sequence; (iii) limit-sure\nwinning if for all epsilon > 0, there exists a strategy that produces a\n(1-epsilon)-synchronizing sequence.\n  We consider the problem of deciding whether an MDP is sure, almost-sure,\nlimit-sure winning, and we establish the decidability and optimal complexity\nfor all modes, as well as the memory requirements for winning strategies. Our\nmain contributions are as follows: (a) for each winning modes we present\ncharacterizations that give a PSPACE complexity for the decision problems, and\nwe establish matching PSPACE lower bounds; (b) we show that for sure winning\nstrategies, exponential memory is sufficient and may be necessary, and that in\ngeneral infinite memory is necessary for almost-sure winning, and unbounded\nmemory is necessary for limit-sure winning; (c) along with our results, we\nestablish new complexity results for alternating finite automata over a\none-letter alphabet. \n\n"}
{"id": "1310.3593", "contents": "Title: Stability of Mixed-Strategy-Based Iterative Logit Quantal Response\n  Dynamics in Game Theory Abstract: Using the Logit quantal response form as the response function in each step,\nthe original definition of static quantal response equilibrium (QRE) is\nextended into an iterative evolution process. QREs remain as the fixed points\nof the dynamic process. However, depending on whether such fixed points are the\nlong-term solutions of the dynamic process, they can be classified into stable\n(SQREs) and unstable (USQREs) equilibriums. This extension resembles the\nextension from static Nash equilibriums (NEs) to evolutionary stable solutions\nin the framework of evolutionary game theory. The relation between SQREs and\nother solution concepts of games, including NEs and QREs, is discussed. Using\nexperimental data from other published papers, we perform a preliminary\ncomparison between SQREs, NEs, QREs and the observed behavioral outcomes of\nthose experiments. For certain games, we determine that SQREs have better\npredictive power than QREs and NEs. \n\n"}
{"id": "1310.3673", "contents": "Title: Evaluation of DNF Formulas Abstract: Stochastic Boolean Function Evaluation (SBFE) is the problem of determining\nthe value of a given Boolean function $f$ on an unknown input $x$, when each\nbit of $x_i$ of $x$ can only be determined by paying a given associated cost\n$c_i$. Further, $x$ is drawn from a given product distribution: for each $x_i$,\n$Prob[x_i=1] = p_i$, and the bits are independent. The goal is to minimize the\nexpected cost of evaluation. Stochastic Boolean Function Evaluation (SBFE) is\nthe problem of determining the value of a given Boolean function $f$ on an\nunknown input $x$, when each bit of $x_i$ of $x$ can only be determined by\npaying a given associated cost $c_i$. Further, $x$ is drawn from a given\nproduct distribution: for each $x_i$, $Prob[x_i=1] = p_i$, and the bits are\nindependent. The goal is to minimize the expected cost of evaluation. In this\npaper, we study the complexity of the SBFE problem for classes of DNF formulas.\nWe consider both exact and approximate versions of the problem for subclasses\nof DNF, for arbitrary costs and product distributions, and for unit costs\nand/or the uniform distribution. \n\n"}
{"id": "1310.7247", "contents": "Title: Optimizing scanning strategies: Selecting scanning bandwidth in\n  adversarial RF environments Abstract: In this paper we investigate the problem of designing a spectrum scanning\nstrategy to detect an intelligent Invader who wants to utilize spectrum\nundetected for his/her unapproved purposes. To deal with this problem we apply\ngame-theoretical tools. We model the situation as a game between a Scanner and\nan Invader where the Invader faces a dilemma: the more bandwidth the Invader\nattempts to use leads to a larger payoff if he is not detected, but at the same\ntime also increases the probability of being detected and thus fined.\nSimilarly, the Scanner faces a dilemma: the wider the bandwidth scanned, the\nhigher the probability of detecting the Invader, but at the expense of\nincreasing the cost of building the scanning system. The equilibrium strategies\nare found explicitly and reveal interesting properties. In particular, we have\nfound a discontinuous dependence of the equilibrium strategies on the network\nparameters, fine and the type of the Invader's award. This discontinuity on\nfine means that the network provider has to take into account a human factor\nsince some threshold values of fine could be very sensible for the Invader,\nwhile in other situations simply increasing the fine has minimal deterrence\nimpact. Also we show how different reward types for the Invader (e.g. motivated\nby using different type of application, say, video-streaming or downloading\nfiles) can be incorporated into scanning strategy to increase its efficiency. \n\n"}
{"id": "1310.7419", "contents": "Title: Finding Approximate Nash Equilibria of Bimatrix Games via Payoff Queries Abstract: We study the deterministic and randomized query complexity of finding\napproximate equilibria in bimatrix games. We show that the deterministic query\ncomplexity of finding an $\\epsilon$-Nash equilibrium when $\\epsilon <\n\\frac{1}{2}$ is $\\Omega(k^2)$, even in zero-one constant-sum games. In\ncombination with previous results \\cite{FGGS13}, this provides a complete\ncharacterization of the deterministic query complexity of approximate Nash\nequilibria. We also study randomized querying algorithms. We give a randomized\nalgorithm for finding a $(\\frac{3 - \\sqrt{5}}{2} + \\epsilon)$-Nash equilibrium\nusing $O(\\frac{k \\cdot \\log k}{\\epsilon^2})$ payoff queries, which shows that\nthe $\\frac{1}{2}$ barrier for deterministic algorithms can be broken by\nrandomization. For well-supported Nash equilibria (WSNE), we first give a\nrandomized algorithm for finding an $\\epsilon$-WSNE of a zero-sum bimatrix game\nusing $O(\\frac{k \\cdot \\log k}{\\epsilon^4})$ payoff queries, and we then use\nthis to obtain a randomized algorithm for finding a $(\\frac{2}{3} +\n\\epsilon)$-WSNE in a general bimatrix game using $O(\\frac{k \\cdot \\log\nk}{\\epsilon^4})$ payoff queries. Finally, we initiate the study of lower bounds\nagainst randomized algorithms in the context of bimatrix games, by showing that\nrandomized algorithms require $\\Omega(k^2)$ payoff queries in order to find a\n$\\frac{1}{6k}$-Nash equilibrium, even in zero-one constant-sum games. In\nparticular, this rules out query-efficient randomized algorithms for finding\nexact Nash equilibria. \n\n"}
{"id": "1311.2109", "contents": "Title: How to Gamble Against All Odds Abstract: A decision maker observes the evolving state of the world while constantly\ntrying to predict the next state given the history of past states. The ability\nto benefit from such predictions depends not only on the ability to recognize\npatters in history, but also on the range of actions available to the decision\nmaker.\n  We assume there are two possible states of the world. The decision maker is a\ngambler who has to bet a certain amount of money on the bits of an announced\nbinary sequence of states. If he makes a correct prediction he wins his wager,\notherwise he loses it.\n  We compare the power of betting strategies (aka martingales) whose wagers\ntake values in different sets of reals. A martingale whose wagers take values\nin a set $A$ is called an $A$-martingale. A set of reals $B$ anticipates a set\n$A$, if for every $A$-martingale there is a countable set of $B$-martingales,\nsuch that on every binary sequence on which the $A$-martingale gains an\ninfinite amount at least one of the $B$-martingales gains an infinite amount,\ntoo.\n  We show that for two important classes of pairs of sets $A$ and $B$, $B$\nanticipates $A$ if and only if the closure of $B$ contains $rA$, for some\npositive $r$. One class is when $A$ is bounded and $B$ is bounded away from\nzero; the other class is when $B$ is well ordered (has no left-accumulation\npoints). Our results generalize several recent results in algorithmic\nrandomness and answer a question posed by Chalcraft et al. (2012). \n\n"}
{"id": "1311.2138", "contents": "Title: The Complexity of Optimal Multidimensional Pricing Abstract: We resolve the complexity of revenue-optimal deterministic auctions in the\nunit-demand single-buyer Bayesian setting, i.e., the optimal item pricing\nproblem, when the buyer's values for the items are independent. We show that\nthe problem of computing a revenue-optimal pricing can be solved in polynomial\ntime for distributions of support size 2, and its decision version is\nNP-complete for distributions of support size 3. We also show that the problem\nremains NP-complete for the case of identical distributions. \n\n"}
{"id": "1311.2578", "contents": "Title: Primal Beats Dual on Online Packing LPs in the Random-Order Model Abstract: We study packing LPs in an online model where the columns are presented to\nthe algorithm in random order. This natural problem was investigated in various\nrecent studies motivated, e.g., by online ad allocations and yield management\nwhere rows correspond to resources and columns to requests specifying demands\nfor resources. Our main contribution is a $1-O(\\sqrt{(\\log{d})/B})$-competitive\nonline algorithm, where $d$ denotes the column sparsity, i.e., the maximum\nnumber of resources that occur in a single column, and $B$ denotes the capacity\nratio $B$, i.e., the ratio between the capacity of a resource and the maximum\ndemand for this resource. In other words, we achieve a $(1 -\n\\epsilon)$-approximation if the capacity ratio satisfies $B=\\Omega((\\log\nd)/\\epsilon^2)$, which is known to be best-possible for any (randomized) online\nalgorithms.\n  Our result improves exponentially on previous work with respect to the\ncapacity ratio. In contrast to existing results on packing LP problems, our\nalgorithm does not use dual prices to guide the allocation of resources.\nInstead, it simply solves, for each request, a scaled version of the partially\nknown primal program and randomly rounds the obtained fractional solution to\nobtain an integral allocation for this request. We show that this simple\nalgorithmic technique is not restricted to packing LPs with large capacity\nratio: We prove an upper bound on the competitive ratio of\n$\\Omega(d^{-1/(B-1)})$, for any $B \\ge 2$. In addition, we show that our\napproach can be combined with VCG payments and obtain an incentive compatible\n$(1-\\epsilon)$-competitive mechanism for packing LPs with $B=\\Omega((\\log\nm)/\\epsilon^2)$, where $m$ is the number of constraints. Finally, we apply our\ntechnique to the generalized assignment problem for which we obtain the first\nonline algorithm with competitive ratio $O(1)$. \n\n"}
{"id": "1311.3054", "contents": "Title: Losing Weight by Gaining Edges Abstract: We present a new way to encode weighted sums into unweighted pairwise\nconstraints, obtaining the following results.\n  - Define the k-SUM problem to be: given n integers in [-n^2k, n^2k] are there\nk which sum to zero? (It is well known that the same problem over arbitrary\nintegers is equivalent to the above definition, by linear-time randomized\nreductions.) We prove that this definition of k-SUM remains W[1]-hard, and is\nin fact W[1]-complete: k-SUM can be reduced to f(k) * n^o(1) instances of\nk-Clique.\n  - The maximum node-weighted k-Clique and node-weighted k-dominating set\nproblems can be reduced to n^o(1) instances of the unweighted k-Clique and\nk-dominating set problems, respectively. This implies a strong equivalence\nbetween the time complexities of the node weighted problems and the unweighted\nproblems: any polynomial improvement on one would imply an improvement for the\nother.\n  - A triangle of weight 0 in a node weighted graph with m edges can be\ndeterministically found in m^1.41 time. \n\n"}
{"id": "1311.3088", "contents": "Title: Endogenous games with goals: side-payments among goal-directed\n  artificial agents Abstract: Artificial agents are typically oriented to the realization of an externally\nassigned task and try to optimize over secondary aspects of plan execution such\ntime lapse or power consumption, technically displaying a quasi-dichotomous\npreference relation. Boolean games have been developed as a paradigm for\nmodelling societies of agents with this type of preference. In boolean games\nagents exercise control over propositional variables and strive to achieve a\ngoal formula whose realization might require the opponents' cooperation.\nRecently, a theory of incentive engineering for such games has been devised,\nwhere an external authority steers the outcome of the game towards certain\ndesirable properties consistent with players' goals, by imposing a taxation\nmechanism on the players that makes the outcomes that do not comply with those\nproperties less appealing to them. The present contribution stems from a\ncomplementary perspective and studies, instead, how games with\nquasi-dichotomous preferences can be transformed from inside, rather than from\noutside, by endowing players with the possibility of sacrificing a part of\ntheir payoff received at a certain outcome in order to convince other players\nto play a certain strategy. Concretely we explore the properties of endogenous\ngames with goals, obtained coupling strategic games with goals, a\ngeneralization of boolean games, with the machinery of endogenous games coming\nfrom game theory. We analyze equilibria in those structures, showing the\npreconditions needed for desirable outcomes to be achieved without external\nintervention. What our results show is that endogenous games with goals display\nspecific irreducible features - with respect to what already known for\nendogenous games - which makes them worth studying in their own sake. \n\n"}
{"id": "1311.3141", "contents": "Title: Cloud Compute-and-Forward with Relay Cooperation Abstract: We study a cloud network with M distributed receiving antennas and L users,\nwhich transmit their messages towards a centralized decoder (CD), where M>=L.\nWe consider that the cloud network applies the Compute-and-Forward (C&F)\nprotocol, where L antennas/relays are selected to decode integer equations of\nthe transmitted messages. In this work, we focus on the best relay selection\nand the optimization of the Physical-Layer Network Coding (PNC) at the relays,\naiming at the throughput maximization of the network. Existing literature\noptimizes PNC with respect to the maximization of the minimum rate among users.\nThe proposed strategy maximizes the sum rate of the users allowing nonsymmetric\nrates, while the optimal solution is explored with the aid of the Pareto\nfrontier. The problem of relay selection is matched to a coalition formation\ngame, where the relays and the CD cooperate in order to maximize their profit.\nEfficient coalition formation algorithms are proposed, which perform joint\nrelay selection and PNC optimization. Simulation results show that a\nconsiderable improvement is achieved compared to existing results, both in\nterms of the network sum rate and the players' profits. \n\n"}
{"id": "1311.4066", "contents": "Title: Polynomial-time Solvable #CSP Problems via Algebraic Models and Pfaffian\n  Circuits Abstract: A Pfaffian circuit is a tensor contraction network where the edges are\nlabeled with changes of bases in such a way that a very specific set of\ncombinatorial properties are satisfied. By modeling the permissible changes of\nbases as systems of polynomial equations, and then solving via computation, we\nare able to identify classes of 0/1 planar #CSP problems solvable in\npolynomial-time via the Pfaffian circuit evaluation theorem (a variant of L.\nValiant's Holant Theorem). We present two different models of 0/1 variables,\none that is possible under a homogeneous change of basis, and one that is\npossible under a heterogeneous change of basis only. We enumerate a series of\n1,2,3, and 4-arity gates/cogates that represent constraints, and define a class\nof constraints that is possible under the assumption of a ``bridge\" between two\nparticular changes of bases. We discuss the issue of planarity of Pfaffian\ncircuits, and demonstrate possible directions in algebraic computation for\ndesigning a Pfaffian tensor contraction network fragment that can simulate a\nswap gate/cogate. We conclude by developing the notion of a decomposable\ngate/cogate, and discuss the computational benefits of this definition. \n\n"}
{"id": "1311.4721", "contents": "Title: Economic Efficiency Requires Interaction Abstract: We study the necessity of interaction between individuals for obtaining\napproximately efficient allocations. The role of interaction in markets has\nreceived significant attention in economic thinking, e.g. in Hayek's 1945\nclassic paper.\n  We consider this problem in the framework of simultaneous communication\ncomplexity. We analyze the amount of simultaneous communication required for\nachieving an approximately efficient allocation. In particular, we consider two\nsettings: combinatorial auctions with unit demand bidders (bipartite matching)\nand combinatorial auctions with subadditive bidders. For both settings we first\nshow that non-interactive systems have enormous communication costs relative to\ninteractive ones. On the other hand, we show that limited interaction enables\nus to find approximately efficient allocations. \n\n"}
{"id": "1311.6230", "contents": "Title: Privacy-Preserving Verifiable Incentive Mechanism for Crowdsourcing\n  Market Applications Abstract: Recently, a novel class of incentive mechanisms is proposed to attract\nextensive users to truthfully participate in crowd sensing applications with a\ngiven budget constraint. The class mechanisms also bring good service quality\nfor the requesters in crowd sensing applications. Although it is so important,\nthere still exists many verification and privacy challenges, including users'\nbids and subtask information privacy and identification privacy, winners' set\nprivacy of the platform, and the security of the payment outcomes. In this\npaper, we present a privacy-preserving verifiable incentive mechanism for crowd\nsensing applications with the budget constraint, not only to explore how to\nprotect the privacies of users and the platform, but also to make the\nverifiable payment correct between the platform and users for crowd sensing\napplications. Results indicate that our privacy-preserving verifiable incentive\nmechanism achieves the same results as the generic one without privacy\npreservation. \n\n"}
{"id": "1311.6638", "contents": "Title: $K$-anonymous Signaling Scheme Abstract: We incorporate signaling scheme into Ad Auction setting, to achieve better\nwelfare and revenue while protect users' privacy. We propose a new\n\\emph{$K$-anonymous signaling scheme setting}, prove the hardness of the\ncorresponding welfare/revenue maximization problem, and finally propose the\nalgorithms to approximate the optimal revenue or welfare. \n\n"}
{"id": "1311.7178", "contents": "Title: Efficient deterministic approximate counting for low-degree polynomial\n  threshold functions Abstract: We give a deterministic algorithm for approximately counting satisfying\nassignments of a degree-$d$ polynomial threshold function (PTF). Given a\ndegree-$d$ input polynomial $p(x_1,\\dots,x_n)$ over $R^n$ and a parameter\n$\\epsilon> 0$, our algorithm approximates $\\Pr_{x \\sim \\{-1,1\\}^n}[p(x) \\geq\n0]$ to within an additive $\\pm \\epsilon$ in time $O_{d,\\epsilon}(1)\\cdot\n\\mathop{poly}(n^d)$. (Any sort of efficient multiplicative approximation is\nimpossible even for randomized algorithms assuming $NP\\not=RP$.) Note that the\nrunning time of our algorithm (as a function of $n^d$, the number of\ncoefficients of a degree-$d$ PTF) is a \\emph{fixed} polynomial. The fastest\nprevious algorithm for this problem (due to Kane), based on constructions of\nunconditional pseudorandom generators for degree-$d$ PTFs, runs in time\n$n^{O_{d,c}(1) \\cdot \\epsilon^{-c}}$ for all $c > 0$.\n  The key novel contributions of this work are: A new multivariate central\nlimit theorem, proved using tools from Malliavin calculus and Stein's Method.\nThis new CLT shows that any collection of Gaussian polynomials with small\neigenvalues must have a joint distribution which is very close to a\nmultidimensional Gaussian distribution. A new decomposition of low-degree\nmultilinear polynomials over Gaussian inputs. Roughly speaking we show that (up\nto some small error) any such polynomial can be decomposed into a bounded\nnumber of multilinear polynomials all of which have extremely small\neigenvalues. We use these new ingredients to give a deterministic algorithm for\na Gaussian-space version of the approximate counting problem, and then employ\nstandard techniques for working with low-degree PTFs (invariance principles and\nregularity lemmas) to reduce the original approximate counting problem over the\nBoolean hypercube to the Gaussian version. \n\n"}
{"id": "1312.0137", "contents": "Title: Approximation Algorithms for Non-Single-minded Profit-Maximization\n  Problems with Limited Supply Abstract: We consider {\\em profit-maximization} problems for {\\em combinatorial\nauctions} with {\\em non-single minded valuation functions} and {\\em limited\nsupply}.\n  We obtain fairly general results that relate the approximability of the\nprofit-maximization problem to that of the corresponding {\\em\nsocial-welfare-maximization} (SWM) problem, which is the problem of finding an\nallocation $(S_1,\\ldots,S_n)$ satisfying the capacity constraints that has\nmaximum total value $\\sum_j v_j(S_j)$. For {\\em subadditive valuations} (and\nhence {\\em submodular, XOS valuations}), we obtain a solution with profit\n$\\OPT_\\swm/O(\\log c_{\\max})$, where $\\OPT_\\swm$ is the optimum social welfare\nand $c_{\\max}$ is the maximum item-supply; thus, this yields an $O(\\log\nc_{\\max})$-approximation for the profit-maximization problem. Furthermore,\ngiven {\\em any} class of valuation functions, if the SWM problem for this\nvaluation class has an LP-relaxation (of a certain form) and an algorithm\n\"verifying\" an {\\em integrality gap} of $\\al$ for this LP, then we obtain a\nsolution with profit $\\OPT_\\swm/O(\\al\\log c_{\\max})$, thus obtaining an\n$O(\\al\\log c_{\\max})$-approximation.\n  For the special case, when the tree is a path, we also obtain an incomparable\n$O(\\log m)$-approximation (via a different approach) for subadditive\nvaluations, and arbitrary valuations with unlimited supply. Our approach for\nthe latter problem also gives an $\\frac{e}{e-1}$-approximation algorithm for\nthe multi-product pricing problem in the Max-Buy model, with limited supply,\nimproving on the previously known approximation factor of 2. \n\n"}
{"id": "1312.0659", "contents": "Title: Prioritizing Consumers in Smart Grid: A Game Theoretic Approach Abstract: This paper proposes an energy management technique for a consumer-to-grid\nsystem in smart grid. The benefit to consumers is made the primary concern to\nencourage consumers to participate voluntarily in energy trading with the\ncentral power station (CPS) in situations of energy deficiency. A novel system\nmodel motivating energy trading under the goal of social optimality is\nproposed. A single-leader multiple-follower Stackelberg game is then studied to\nmodel the interactions between the CPS and a number of energy consumers (ECs),\nand to find optimal distributed solutions for the optimization problem based on\nthe system model. The CPS is considered as a leader seeking to minimize its\ntotal cost of buying energy from the ECs, and the ECs are the followers who\ndecide on how much energy they will sell to the CPS for maximizing their\nutilities. It is shown that the game, which can be implemented distributedly,\npossesses a socially optimal solution, in which the benefits-sum to all\nconsumers is maximized, as the total cost to the CPS is minimized. Numerical\nanalysis confirms the effectiveness of the game. \n\n"}
{"id": "1312.1394", "contents": "Title: Incentive Design and Utility Learning via Energy Disaggregation Abstract: The utility company has many motivations for modifying energy consumption\npatterns of consumers such as revenue decoupling and demand response programs.\nWe model the utility company--consumer interaction as a principal--agent\nproblem. We present an iterative algorithm for designing incentives while\nestimating the consumer's utility function. Incentives are designed using the\naggregated as well as the disaggregated (device level) consumption data. We\nsimulate the iterative control (incentive design) and estimation (utility\nlearning and disaggregation) process for examples including the design of\nincentives based on the aggregate consumption data as well as the disaggregated\nconsumption data. \n\n"}
{"id": "1312.2141", "contents": "Title: Dynamic Complexity of Planar 3-connected Graph Isomorphism Abstract: Dynamic Complexity (as introduced by Patnaik and Immerman) tries to express\nhow hard it is to update the solution to a problem when the input is changed\nslightly. It considers the changes required to some stored data structure\n(possibly a massive database) as small quantities of data (or a tuple) are\ninserted or deleted from the database (or a structure over some vocabulary).\nThe main difference from previous notions of dynamic complexity is that instead\nof treating the update quantitatively by finding the the time/space trade-offs,\nit tries to consider the update qualitatively, by finding the complexity class\nin which the update can be expressed (or made). In this setting, DynFO, or\nDynamic First-Order, is one of the smallest and the most natural complexity\nclass (since SQL queries can be expressed in First-Order Logic), and contains\nthose problems whose solutions (or the stored data structure from which the\nsolution can be found) can be updated in First-Order Logic when the data\nstructure undergoes small changes.\n  Etessami considered the problem of isomorphism in the dynamic setting, and\nshowed that Tree Isomorphism can be decided in DynFO. In this work, we show\nthat isomorphism of Planar 3-connected graphs can be decided in DynFO+ (which\nis DynFO with some polynomial precomputation). We maintain a canonical\ndescription of 3-connected Planar graphs by maintaining a database which is\naccessed and modified by First-Order queries when edges are added to or deleted\nfrom the graph. We specifically exploit the ideas of Breadth-First Search and\nCanonical Breadth-First Search to prove the results. We also introduce a novel\nmethod for canonizing a 3-connected planar graph in First-Order Logic from\nCanonical Breadth-First Search Trees. \n\n"}
{"id": "1312.3797", "contents": "Title: Infinite Games Specified by 2-Tape Automata Abstract: We prove that the determinacy of Gale-Stewart games whose winning sets are\ninfinitary rational relations accepted by 2-tape B\\\"uchi automata is equivalent\nto the determinacy of (effective) analytic Gale-Stewart games which is known to\nbe a large cardinal assumption. Then we prove that winning strategies, when\nthey exist, can be very complex, i.e. highly non-effective, in these games. We\nprove the same results for Gale-Stewart games with winning sets accepted by\nreal-time 1-counter B\\\"uchi automata, then extending previous results obtained\nabout these games. Then we consider the strenghs of determinacy for these\ngames, and we prove that there is a transfinite sequence of 2-tape B\\\"uchi\nautomata (respectively, of real-time 1-counter B\\\"uchi automata) $A_\\alpha$,\nindexed by recursive ordinals, such that the games $G(L(A_\\alpha))$ have\nstrictly increasing strenghs of determinacy. Moreover there is a 2-tape B\\\"uchi\nautomaton (respectively, a real-time 1-counter B\\\"uchi automaton) B such that\nthe determinacy of G(L(B)) is equivalent to the (effective) analytic\ndeterminacy and thus has the maximal strength of determinacy. We show also that\nthe determinacy of Wadge games between two players in charge of infinitary\nrational relations accepted by 2-tape B\\\"uchi automata is equivalent to the\n(effective) analytic determinacy, and thus not provable in ZFC. \n\n"}
{"id": "1312.4628", "contents": "Title: Counting Triangulations and other Crossing-free Structures via Onion\n  Layers Abstract: Let $P$ be a set of $n$ points in the plane. A crossing-free structure on $P$\nis a plane graph with vertex set $P$. Examples of crossing-free structures\ninclude triangulations of $P$, spanning cycles of $P$, also known as\npolygonalizations of $P$, among others. In this paper we develop a general\ntechnique for computing the number of crossing-free structures of an input set\n$P$. We apply the technique to obtain algorithms for computing the number of\ntriangulations, matchings, and spanning cycles of $P$. The running time of our\nalgorithms is upper bounded by $n^{O(k)}$, where $k$ is the number of onion\nlayers of $P$. In particular, for $k = O(1)$ our algorithms run in polynomial\ntime. In addition, we show that our algorithm for counting triangulations is\nnever slower than $O^{*}(3.1414^{n})$, even when $k = \\Theta(n)$. Given that\nthere are several well-studied configurations of points with at least\n$\\Omega(3.464^{n})$ triangulations, and some even with $\\Omega(8^{n})$\ntriangulations, our algorithm asymptotically outperforms any enumeration\nalgorithm for such instances. In fact, it is widely believed that any set of\n$n$ points must have at least $\\Omega(3.464^{n})$ triangulations. If this is\ntrue, then our algorithm is strictly sub-linear in the number of triangulations\ncounted. We also show that our techniques are general enough to solve the\n\"Restricted-Triangulation-Counting-Problem\", which we prove to be $W[2]$-hard\nin the parameter $k$. This implies a \"no free lunch\" result: In order to be\nfixed-parameter tractable, our general algorithm must rely on additional\nproperties that are specific to the considered class of structures. \n\n"}
{"id": "1312.5936", "contents": "Title: Measuring voting power in convex policy spaces Abstract: Classical power index analysis considers the individual's ability to\ninfluence the aggregated group decision by changing its own vote, where all\ndecisions and votes are assumed to be binary. In many practical applications we\nhave more options than either \"yes\" or \"no\". Here we generalize three important\npower indices to continuous convex policy spaces. This allows the analysis of a\ncollection of economic problems like e.g. tax rates or spending that otherwise\nwould not be covered in binary models. \n\n"}
{"id": "1312.5972", "contents": "Title: A Comprehensive Analysis of Polyhedral Lift-and-Project Methods Abstract: We consider lift-and-project methods for combinatorial optimization problems\nand focus mostly on those lift-and-project methods which generate polyhedral\nrelaxations of the convex hull of integer solutions. We introduce many new\nvariants of Sherali--Adams and Bienstock--Zuckerberg operators. These new\noperators fill the spectrum of polyhedral lift-and-project operators in a way\nwhich makes all of them more transparent, easier to relate to each other, and\neasier to analyze. We provide new techniques to analyze the worst-case\nperformances as well as relative strengths of these operators in a unified way.\nIn particular, using the new techniques and a result of Mathieu and Sinclair\nfrom 2009, we prove that the polyhedral Bienstock--Zuckerberg operator requires\nat least $\\sqrt{2n}- \\frac{3}{2}$ iterations to compute the matching polytope\nof the $(2n+1)$-clique. We further prove that the operator requires\napproximately $\\frac{n}{2}$ iterations to reach the stable set polytope of the\n$n$-clique, if we start with the fractional stable set polytope. Lastly, we\nshow that some of the worst-case instances for the positive semidefinite\nLov\\'asz--Schrijver lift-and-project operator are also bad instances for the\nstrongest variants of the Sherali--Adams operator with positive semidefinite\nstrengthenings, and discuss some consequences for integrality gaps of convex\nrelaxations. \n\n"}
{"id": "1312.7050", "contents": "Title: Nash Equilibrium Computation in Subnetwork Zero-Sum Games with Switching\n  Communications Abstract: In this paper, we investigate a distributed Nash equilibrium computation\nproblem for a time-varying multi-agent network consisting of two subnetworks,\nwhere the two subnetworks share the same objective function. We first propose a\nsubgradient-based distributed algorithm with heterogeneous stepsizes to compute\na Nash equilibrium of a zero-sum game. We then prove that the proposed\nalgorithm can achieve a Nash equilibrium under uniformly jointly strongly\nconnected (UJSC) weight-balanced digraphs with homogenous stepsizes. Moreover,\nwe demonstrate that for weighted-unbalanced graphs a Nash equilibrium may not\nbe achieved with homogenous stepsizes unless certain conditions on the\nobjective function hold. We show that there always exist heterogeneous\nstepsizes for the proposed algorithm to guarantee that a Nash equilibrium can\nbe achieved for UJSC digraphs. Finally, in two standard weight-unbalanced\ncases, we verify the convergence to a Nash equilibrium by adaptively updating\nthe stepsizes along with the arc weights in the proposed algorithm. \n\n"}
{"id": "1312.7468", "contents": "Title: Tree-width and Logspace: Determinants and Counting Euler Tours Abstract: Motivated by the recent result of [EJT10] showing that MSO properties are\nLogspace computable on graphs of bounded tree-width, we consider the complexity\nof computing the determinant of the adjacency matrix of a bounded tree-width\ngraph and prove that it is L-complete. It is important to notice that the\ndeterminant is neither an MSO-property nor counts the number of solutions of an\nMSO-predicate. We extend this technique to count the number of spanning\narborescences and directed Euler tours in bounded tree-width digraphs, and\nfurther to counting the number of spanning trees and the number of Euler tours\nin undirected graphs, all in L. Notice that undirected Euler tours are not\nknown to be MSO-expressible and the corresponding counting problem is in fact\n#P-hard for general graphs. Counting undirected Euler tours in bounded\ntree-width graphs was not known to be polynomial time computable till very\nrecently Chebolu et al [CCM13] gave a polynomial time algorithm for this\nproblem (concurrently and independently of this work). Finally, we also show\nsome linear algebraic extensions of the determinant algorithm to show how to\ncompute the charcteristic polynomial and trace of the powers of a bounded\ntree-width graph in L. \n\n"}
{"id": "1401.3289", "contents": "Title: The Complexity of Partial-observation Stochastic Parity Games With\n  Finite-memory Strategies Abstract: We consider two-player partial-observation stochastic games on finite-state\ngraphs where player 1 has partial observation and player 2 has perfect\nobservation. The winning condition we study are \\omega-regular conditions\nspecified as parity objectives. The qualitative-analysis problem given a\npartial-observation stochastic game and a parity objective asks whether there\nis a strategy to ensure that the objective is satisfied with probability~1\n(resp. positive probability). These qualtitative-analysis problems are known to\nbe undecidable. However in many applications the relevant question is the\nexistence of finite-memory strategies, and the qualitative-analysis problems\nunder finite-memory strategies was recently shown to be decidable in 2EXPTIME.\nWe improve the complexity and show that the qualitative-analysis problems for\npartial-observation stochastic parity games under finite-memory strategies are\nEXPTIME-complete; and also establish optimal (exponential) memory bounds for\nfinite-memory strategies required for qualitative analysis. \n\n"}
{"id": "1401.4092", "contents": "Title: Redrawing the Boundaries on Purchasing Data from Privacy-Sensitive\n  Individuals Abstract: We prove new positive and negative results concerning the existence of\ntruthful and individually rational mechanisms for purchasing private data from\nindividuals with unbounded and sensitive privacy preferences. We strengthen the\nimpossibility results of Ghosh and Roth (EC 2011) by extending it to a much\nwider class of privacy valuations. In particular, these include privacy\nvaluations that are based on ({\\epsilon}, {\\delta})-differentially private\nmechanisms for non-zero {\\delta}, ones where the privacy costs are measured in\na per-database manner (rather than taking the worst case), and ones that do not\ndepend on the payments made to players (which might not be observable to an\nadversary). To bypass this impossibility result, we study a natural special\nsetting where individuals have mono- tonic privacy valuations, which captures\ncommon contexts where certain values for private data are expected to lead to\nhigher valuations for privacy (e.g. having a particular disease). We give new\nmech- anisms that are individually rational for all players with monotonic\nprivacy valuations, truthful for all players whose privacy valuations are not\ntoo large, and accurate if there are not too many players with too-large\nprivacy valuations. We also prove matching lower bounds showing that in some\nrespects our mechanism cannot be improved significantly. \n\n"}
{"id": "1401.4267", "contents": "Title: Iterated crowdsourcing dilemma game Abstract: The Internet has enabled the emergence of collective problem solving, also\nknown as crowdsourcing, as a viable option for solving complex tasks. However,\nthe openness of crowdsourcing presents a challenge because solutions obtained\nby it can be sabotaged, stolen, and manipulated at a low cost for the attacker.\nWe extend a previously proposed crowdsourcing dilemma game to an iterated game\nto address this question. We enumerate pure evolutionarily stable strategies\nwithin the class of so-called reactive strategies, i.e., those depending on the\nlast action of the opponent. Among the 4096 possible reactive strategies, we\nfind 16 strategies each of which is stable in some parameter regions. Repeated\nencounters of the players can improve social welfare when the damage inflicted\nby an attack and the cost of attack are both small. Under the current\nframework, repeated interactions do not really ameliorate the crowdsourcing\ndilemma in a majority of the parameter space. \n\n"}
{"id": "1401.4720", "contents": "Title: Computing low-degree factors of lacunary polynomials: a Newton-Puiseux\n  approach Abstract: We present a new algorithm for the computation of the irreducible factors of\ndegree at most $d$, with multiplicity, of multivariate lacunary polynomials\nover fields of characteristic zero. The algorithm reduces this computation to\nthe computation of irreducible factors of degree at most $d$ of univariate\nlacunary polynomials and to the factorization of low-degree multivariate\npolynomials. The reduction runs in time polynomial in the size of the input\npolynomial and in $d$. As a result, we obtain a new polynomial-time algorithm\nfor the computation of low-degree factors, with multiplicity, of multivariate\nlacunary polynomials over number fields, but our method also gives partial\nresults for other fields, such as the fields of $p$-adic numbers or for\nabsolute or approximate factorization for instance.\n  The core of our reduction uses the Newton polygon of the input polynomial,\nand its validity is based on the Newton-Puiseux expansion of roots of bivariate\npolynomials. In particular, we bound the valuation of $f(X,\\phi)$ where $f$ is\na lacunary polynomial and $\\phi$ a Puiseux series whose vanishing polynomial\nhas low degree. \n\n"}
{"id": "1401.4786", "contents": "Title: Common Information based Markov Perfect Equilibria for Linear-Gaussian\n  Games with Asymmetric Information Abstract: We consider a class of two-player dynamic stochastic nonzero-sum games where\nthe state transition and observation equations are linear, and the primitive\nrandom variables are Gaussian. Each controller acquires possibly different\ndynamic information about the state process and the other controller's past\nactions and observations. This leads to a dynamic game of asymmetric\ninformation among the controllers. Building on our earlier work on finite games\nwith asymmetric information, we devise an algorithm to compute a Nash\nequilibrium by using the common information among the controllers. We call such\nequilibria common information based Markov perfect equilibria of the game,\nwhich can be viewed as a refinement of Nash equilibrium in games with\nasymmetric information. If the players' cost functions are quadratic, then we\nshow that under certain conditions a unique common information based Markov\nperfect equilibrium exists. Furthermore, this equilibrium can be computed by\nsolving a sequence of linear equations. We also show through an example that\nthere could be other Nash equilibria in a game of asymmetric information, not\ncorresponding to common information based Markov perfect equilibria. \n\n"}
{"id": "1401.6523", "contents": "Title: Strategic aspects of the probabilistic serial rule for the allocation of\n  goods Abstract: The probabilistic serial (PS) rule is one of the most prominent randomized\nrules for the assignment problem. It is well-known for its superior fairness\nand welfare properties. However, PS is not immune to manipulative behaviour by\nthe agents. We examine computational and non-computational aspects of\nstrategising under the PS rule. Firstly, we study the computational complexity\nof an agent manipulating the PS rule. We present polynomial-time algorithms for\noptimal manipulation. Secondly, we show that expected utility best responses\ncan cycle. Thirdly, we examine the existence and computation of Nash\nequilibrium profiles under the PS rule. We show that a pure Nash equilibrium is\nguaranteed to exist under the PS rule. For two agents, we identify two\ndifferent types of preference profiles that are not only in Nash equilibrium\nbut can also be computed in linear time. Finally, we conduct experiments to\ncheck the frequency of manipulability of the PS rule under different\ncombinations of the number of agents, objects, and utility functions. \n\n"}
{"id": "1401.8219", "contents": "Title: On the Properties of the Priority Deriving Procedure in the Pairwise\n  Comparisons Method Abstract: The pairwise comparisons method is a convenient tool used when the relative\norder of preferences among different concepts (alternatives) needs to be\ndetermined. There are several popular implementations of this method, including\nthe Eigenvector Method, the Least Squares Method, the Chi Squares Method and\nothers. Each of the above methods comes with one or more inconsistency indices\nthat help to decide whether the consistency of input guarantees obtaining a\nreliable output, thus taking the optimal decision. This article explores the\nrelationship between inconsistency of input and discrepancy of output. A global\nranking discrepancy describes to what extent the obtained results correspond to\nthe single expert's assessments. On the basis of the inconsistency and\ndiscrepancy indices, two properties of the weight deriving procedure are\nformulated. These properties are proven for Eigenvector Method and Koczkodaj's\nInconsistency Index. Several estimates using Koczkodaj's Inconsistency Index\nfor a principal eigenvalue, Saaty's inconsistency index and the Condition of\nOrder Preservation are also provided. \n\n"}
{"id": "1402.0052", "contents": "Title: Performance of the Survey Propagation-guided decimation algorithm for\n  the random NAE-K-SAT problem Abstract: We show that the Survey Propagation-guided decimation algorithm fails to find\nsatisfying assignments on random instances of the \"Not-All-Equal-$K$-SAT\"\nproblem if the number of message passing iterations is bounded by a constant\nindependent of the size of the instance and the clause-to-variable ratio is\nabove $(1+o_K(1)){2^{K-1}\\over K}\\log^2 K$ for sufficiently large $K$. Our\nanalysis in fact applies to a broad class of algorithms described as\n\"sequential local algorithms\". Such algorithms iteratively set variables based\non some local information and then recurse on the reduced instance. Survey\nPropagation-guided as well as Belief Propagation-guided decimation algorithms -\ntwo widely studied message passing based algorithms, fall under this category\nof algorithms provided the number of message passing iterations is bounded by a\nconstant. Another well-known algorithm falling into this category is the Unit\nClause algorithm. Our work constitutes the first rigorous analysis of the\nperformance of the SP-guided decimation algorithm.\n  The approach underlying our paper is based on an intricate geometry of the\nsolution space of random NAE-$K$-SAT problem. We show that above the\n$(1+o_K(1)){2^{K-1}\\over K}\\log^2 K$ threshold, the overlap structure of\n$m$-tuples of satisfying assignments exhibit a certain clustering behavior\nexpressed in the form of constraints on distances between the $m$ assignments,\nfor appropriately chosen $m$. We further show that if a sequential local\nalgorithm succeeds in finding a satisfying assignment with probability bounded\naway from zero, then one can construct an $m$-tuple of solutions violating\nthese constraints, thus leading to a contradiction. Along with (citation), this\nresult is the first work which directly links the clustering property of random\nconstraint satisfaction problems to the computational hardness of finding\nsatisfying assignments. \n\n"}
{"id": "1402.0988", "contents": "Title: The inverse problem for power distributions in committees Abstract: Several power indices have been introduced in the literature in order to\nmeasure the influence of individual committee members on the aggregated\ndecision. Here we ask the inverse question and aim to design voting rules for a\ncommittee such that a given desired power distribution is met as closely as\npossible. We present an exact algorithm for a large class of different power\nindices based on integer linear programming. With respect to negative\napproximation results we generalize the approach of Alon and Edelman who\nstudied power distributions for the Banzhaf index, where most of the power is\nconcentrated on few coordinates. It turned out that each Banzhaf vector of an\nn-member committee that is near to such a desired power distribution, has to be\nalso near to the Banzhaf vector of a k-member committee. We show that such\nAlon-Edelman type results are possible for other power indices like e.g. the\nPublic Good index or the Coleman index to prevent actions, while they are\nprincipally impossible for e.g. the Johnston index. \n\n"}
{"id": "1402.2801", "contents": "Title: An Anti-Folk Theorem for Large Repeated Games with Imperfect Monitoring Abstract: We study infinitely repeated games in settings of imperfect monitoring. We\nfirst prove a family of theorems that show that when the signals observed by\nthe players satisfy a condition known as $(\\epsilon, \\gamma)$-differential\nprivacy, that the folk theorem has little bite: for values of $\\epsilon$ and\n$\\gamma$ sufficiently small, for a fixed discount factor, any equilibrium of\nthe repeated game involve players playing approximate equilibria of the stage\ngame in every period. Next, we argue that in large games ($n$ player games in\nwhich unilateral deviations by single players have only a small impact on the\nutility of other players), many monitoring settings naturally lead to signals\nthat satisfy $(\\epsilon,\\gamma)$-differential privacy, for $\\epsilon$ and\n$\\gamma$ tending to zero as the number of players $n$ grows large. We conclude\nthat in such settings, the set of equilibria of the repeated game collapse to\nthe set of equilibria of the stage game. \n\n"}
{"id": "1402.3426", "contents": "Title: Privacy Games: Optimal User-Centric Data Obfuscation Abstract: In this paper, we design user-centric obfuscation mechanisms that impose the\nminimum utility loss for guaranteeing user's privacy. We optimize utility\nsubject to a joint guarantee of differential privacy (indistinguishability) and\ndistortion privacy (inference error). This double shield of protection limits\nthe information leakage through obfuscation mechanism as well as the posterior\ninference. We show that the privacy achieved through joint\ndifferential-distortion mechanisms against optimal attacks is as large as the\nmaximum privacy that can be achieved by either of these mechanisms separately.\nTheir utility cost is also not larger than what either of the differential or\ndistortion mechanisms imposes. We model the optimization problem as a\nleader-follower game between the designer of obfuscation mechanism and the\npotential adversary, and design adaptive mechanisms that anticipate and protect\nagainst optimal inference algorithms. Thus, the obfuscation mechanism is\noptimal against any inference algorithm. \n\n"}
{"id": "1402.3447", "contents": "Title: Welfare guarantees for proportional allocations Abstract: According to the proportional allocation mechanism from the network\noptimization literature, users compete for a divisible resource -- such as\nbandwidth -- by submitting bids. The mechanism allocates to each user a\nfraction of the resource that is proportional to her bid and collects an amount\nequal to her bid as payment. Since users act as utility-maximizers, this\nnaturally defines a proportional allocation game. Recently, Syrgkanis and\nTardos (STOC 2013) quantified the inefficiency of equilibria in this game with\nrespect to the social welfare and presented a lower bound of 26.8% on the price\nof anarchy over coarse-correlated and Bayes-Nash equilibria in the full and\nincomplete information settings, respectively. In this paper, we improve this\nbound to 50% over both equilibrium concepts. Our analysis is simpler and,\nfurthermore, we argue that it cannot be improved by arguments that do not take\nthe equilibrium structure into account. We also extend it to settings with\nbudget constraints where we show the first constant bound (between 36% and 50%)\non the price of anarchy of the corresponding game with respect to an effective\nwelfare benchmark that takes budgets into account. \n\n"}
{"id": "1402.4343", "contents": "Title: Minimum Entropy Submodular Optimization (and Fairness in Cooperative\n  Games) Abstract: We study minimum entropy submodular optimization, a common generalization of\nthe minimum entropy set cover problem, studied earlier by Cardinal et al., and\nthe submodular set cover problem.\n  We give a general bound of the approximation performance of the greedy\nalgorithm using an approach that can be interpreted in terms of a particular\ntype of biased network flows. As an application we rederive known results for\nthe Minimum Entropy Set Cover and Minimum Entropy Orientation problems, and\nobtain a nontrivial bound for a new problem called the Minimum Entropy Spanning\nTree problem.\n  The problem can be applied to (and is partly motivated by) the definition of\nworst-case approaches to fairness in concave cooperative games, similar to the\nnotion of price of anarchy in noncooperative settings. \n\n"}
{"id": "1402.4376", "contents": "Title: On Coloring Resilient Graphs Abstract: We introduce a new notion of resilience for constraint satisfaction problems,\nwith the goal of more precisely determining the boundary between NP-hardness\nand the existence of efficient algorithms for resilient instances. In\nparticular, we study $r$-resiliently $k$-colorable graphs, which are those\n$k$-colorable graphs that remain $k$-colorable even after the addition of any\n$r$ new edges. We prove lower bounds on the NP-hardness of coloring resiliently\ncolorable graphs, and provide an algorithm that colors sufficiently resilient\ngraphs. We also analyze the corresponding notion of resilience for $k$-SAT.\nThis notion of resilience suggests an array of open questions for graph\ncoloring and other combinatorial problems. \n\n"}
{"id": "1402.4488", "contents": "Title: Privacy-Preserving Public Information for Sequential Games Abstract: In settings with incomplete information, players can find it difficult to\ncoordinate to find states with good social welfare. For example, in financial\nsettings, if a collection of financial firms have limited information about\neach other's strategies, some large number of them may choose the same\nhigh-risk investment in hopes of high returns. While this might be acceptable\nin some cases, the economy can be hurt badly if many firms make investments in\nthe same risky market segment and it fails. One reason why many firms might end\nup choosing the same segment is that they do not have information about other\nfirms' investments (imperfect information may lead to `bad' game states).\nDirectly reporting all players' investments, however, raises confidentiality\nconcerns for both individuals and institutions.\n  In this paper, we explore whether information about the game-state can be\npublicly announced in a manner that maintains the privacy of the actions of the\nplayers, and still suffices to deter players from reaching bad game-states. We\nshow that in many games of interest, it is possible for players to avoid these\nbad states with the help of privacy-preserving, publicly-announced information.\nWe model behavior of players in this imperfect information setting in two ways\n-- greedy and undominated strategic behaviours, and we prove guarantees on\nsocial welfare that certain kinds of privacy-preserving information can help\nattain. Furthermore, we design a counter with improved privacy guarantees under\ncontinual observation. \n\n"}
{"id": "1402.6779", "contents": "Title: Resourceful Contextual Bandits Abstract: We study contextual bandits with ancillary constraints on resources, which\nare common in real-world applications such as choosing ads or dynamic pricing\nof items. We design the first algorithm for solving these problems that handles\nconstrained resources other than time, and improves over a trivial reduction to\nthe non-contextual case. We consider very general settings for both contextual\nbandits (arbitrary policy sets, e.g. Dudik et al. (UAI'11)) and bandits with\nresource constraints (bandits with knapsacks, Badanidiyuru et al. (FOCS'13)),\nand prove a regret guarantee with near-optimal statistical properties. \n\n"}
{"id": "1403.1508", "contents": "Title: Social welfare in one-sided matchings: Random priority and beyond Abstract: We study the problem of approximate social welfare maximization (without\nmoney) in one-sided matching problems when agents have unrestricted cardinal\npreferences over a finite set of items. Random priority is a very well-known\ntruthful-in-expectation mechanism for the problem. We prove that the\napproximation ratio of random priority is Theta(n^{-1/2}) while no\ntruthful-in-expectation mechanism can achieve an approximation ratio better\nthan O(n^{-1/2}), where n is the number of agents and items. Furthermore, we\nprove that the approximation ratio of all ordinal (not necessarily\ntruthful-in-expectation) mechanisms is upper bounded by O(n^{-1/2}), indicating\nthat random priority is asymptotically the best truthful-in-expectation\nmechanism and the best ordinal mechanism for the problem. \n\n"}
{"id": "1403.3841", "contents": "Title: Doubles and Negatives are Positive (in Self-Assembly) Abstract: In the abstract Tile Assembly Model (aTAM), the phenomenon of cooperation\noccurs when the attachment of a new tile to a growing assembly requires it to\nbind to more than one tile already in the assembly. Often referred to as\n``temperature-2'' systems, those which employ cooperation are known to be quite\npowerful (i.e. they are computationally universal and can build an enormous\nvariety of shapes and structures). Conversely, aTAM systems which do not\nenforce cooperative behavior, a.k.a. ``temperature-1'' systems, are conjectured\nto be relatively very weak, likely to be unable to perform complex computations\nor algorithmically direct the process of self-assembly. Nonetheless, a variety\nof models based on slight modifications to the aTAM have been developed in\nwhich temperature-1 systems are in fact capable of Turing universal computation\nthrough a restricted notion of cooperation. Despite that power, though, several\nof those models have previously been proven to be unable to perform or simulate\nthe stronger form of cooperation exhibited by temperature-2 aTAM systems.\n  In this paper, we first prove that another model in which temperature-1\nsystems are computationally universal, namely the restricted glue TAM (rgTAM)\nin which tiles are allowed to have edges which exhibit repulsive forces, is\nalso unable to simulate the strongly cooperative behavior of the temperature-2\naTAM. We then show that by combining the properties of two such models, the\nDupled Tile Assembly Model (DTAM) and the rgTAM into the DrgTAM, we derive a\nmodel which is actually more powerful at temperature-1 than the aTAM at\ntemperature-2. Specifically, the DrgTAM, at temperature-1, can simulate any\naTAM system of any temperature, and it also contains systems which cannot be\nsimulated by any system in the aTAM. \n\n"}
{"id": "1403.3881", "contents": "Title: Complexity of Equilibrium in Diffusion Games on Social Networks Abstract: In this paper, we consider the competitive diffusion game, and study the\nexistence of its pure-strategy Nash equilibrium when defined over general\nundirected networks. We first determine the set of pure-strategy Nash\nequilibria for two special but well-known classes of networks, namely the\nlattice and the hypercube. Characterizing the utility of the players in terms\nof graphical distances of their initial seed placements to other nodes in the\nnetwork, we show that in general networks the decision process on the existence\nof pure-strategy Nash equilibrium is an NP-hard problem. Following this, we\nprovide some necessary conditions for a given profile to be a Nash equilibrium.\nFurthermore, we study players' utilities in the competitive diffusion game over\nErdos-Renyi random graphs and show that as the size of the network grows, the\nutilities of the players are highly concentrated around their expectation, and\nare bounded below by some threshold based on the parameters of the network.\nFinally, we obtain a lower bound for the maximum social welfare of the game\nwith two players, and study sub-modularity of the players' utilities. \n\n"}
{"id": "1403.5791", "contents": "Title: Self-stabilizing uncoupled dynamics Abstract: Dynamics in a distributed system are self-stabilizing if they are guaranteed\nto reach a stable state regardless of how the system is initialized. Game\ndynamics are uncoupled if each player's behavior is independent of the other\nplayers' preferences. Recognizing an equilibrium in this setting is a\ndistributed computational task. Self-stabilizing uncoupled dynamics, then, have\nboth resilience to arbitrary initial states and distribution of knowledge. We\nstudy these dynamics by analyzing their behavior in a bounded-recall\nsynchronous environment. We determine, for every \"size\" of game, the minimum\nnumber of periods of play that stochastic (randomized) players must recall in\norder for uncoupled dynamics to be self-stabilizing. We also do this for the\nspecial case when the game is guaranteed to have unique best replies. For\ndeterministic players, we demonstrate two self-stabilizing uncoupled protocols.\nOne applies to all games and uses three steps of recall. The other uses two\nsteps of recall and applies to games where each player has at least four\navailable actions. For uncoupled deterministic players, we prove that a single\nstep of recall is insufficient to achieve self-stabilization, regardless of the\nnumber of available actions. \n\n"}
{"id": "1404.0337", "contents": "Title: The Complexity of Bounded Length Graph Recoloring Abstract: We study the following question: Given are two $k$-colorings $\\alpha$ and\n$\\beta$ of a graph $G$ on $n$ vertices, and integer $\\ell$. The question is\nwhether $\\alpha$ can be modified into $\\beta$, by recoloring vertices one at a\ntime, while maintaining a $k$-coloring throughout, and using at most $\\ell$\nsuch recoloring steps. This problem is weakly PSPACE-hard for every constant\n$k\\ge 4$. We show that it is also strongly NP-hard for every constant $k\\ge 4$.\nOn the positive side, we give an $O(f(k,\\ell) n^{O(1)})$ algorithm for the\nproblem, for some computable function $f$. Hence the problem is fixed-parameter\ntractable when parameterized by $k+\\ell$. Finally, we show that the problem is\nW[1]-hard (but in XP) when parameterized only by $\\ell$. \n\n"}
{"id": "1404.0834", "contents": "Title: Expectations or Guarantees? I Want It All! A crossroad between games and\n  MDPs Abstract: When reasoning about the strategic capabilities of an agent, it is important\nto consider the nature of its adversaries. In the particular context of\ncontroller synthesis for quantitative specifications, the usual problem is to\ndevise a strategy for a reactive system which yields some desired performance,\ntaking into account the possible impact of the environment of the system. There\nare at least two ways to look at this environment. In the classical analysis of\ntwo-player quantitative games, the environment is purely antagonistic and the\nproblem is to provide strict performance guarantees. In Markov decision\nprocesses, the environment is seen as purely stochastic: the aim is then to\noptimize the expected payoff, with no guarantee on individual outcomes.\n  In this expository work, we report on recent results introducing the beyond\nworst-case synthesis problem, which is to construct strategies that guarantee\nsome quantitative requirement in the worst-case while providing an higher\nexpected value against a particular stochastic model of the environment given\nas input. This problem is relevant to produce system controllers that provide\nnice expected performance in the everyday situation while ensuring a strict\n(but relaxed) performance threshold even in the event of very bad (while\nunlikely) circumstances. It has been studied for both the mean-payoff and the\nshortest path quantitative measures. \n\n"}
{"id": "1404.1989", "contents": "Title: A Graphical Adversarial Risk Analysis Model for Oil and Gas Drilling\n  Cybersecurity Abstract: Oil and gas drilling is based, increasingly, on operational technology, whose\ncybersecurity is complicated by several challenges. We propose a graphical\nmodel for cybersecurity risk assessment based on Adversarial Risk Analysis to\nface those challenges. We also provide an example of the model in the context\nof an offshore drilling rig. The proposed model provides a more formal and\ncomprehensive analysis of risks, still using the standard business language\nbased on decisions, risks, and value. \n\n"}
{"id": "1404.2329", "contents": "Title: Duality and Optimality of Auctions for Uniform Distributions Abstract: We develop a general duality-theory framework for revenue maximization in\nadditive Bayesian auctions. The framework extends linear programming duality\nand complementarity to constraints with partial derivatives. The dual system\nreveals the geometric nature of the problem and highlights its connection with\nthe theory of bipartite graph matchings. We demonstrate the power of the\nframework by applying it to a multiple-good monopoly setting where the buyer\nhas uniformly distributed valuations for the items, the canonical long-standing\nopen problem in the area. We propose a deterministic selling mechanism called\nStraight-Jacket Auction (SJA), which we prove to be exactly optimal for up to 6\nitems, and conjecture its optimality for any number of goods. The duality\nframework is used not only for proving optimality, but perhaps more importantly\nfor deriving the optimal mechanism itself; as a result, SJA is defined by\nnatural geometric constraints. \n\n"}
{"id": "1404.2671", "contents": "Title: The Multi-shop Ski Rental Problem Abstract: We consider the {\\em multi-shop ski rental} problem. This problem generalizes\nthe classic ski rental problem to a multi-shop setting, in which each shop has\ndifferent prices for renting and purchasing a pair of skis, and a\n\\emph{consumer} has to make decisions on when and where to buy. We are\ninterested in the {\\em optimal online (competitive-ratio minimizing) mixed\nstrategy} from the consumer's perspective. For our problem in its basic form,\nwe obtain exciting closed-form solutions and a linear time algorithm for\ncomputing them. We further demonstrate the generality of our approach by\ninvestigating three extensions of our basic problem, namely ones that consider\ncosts incurred by entering a shop or switching to another shop. Our solutions\nto these problems suggest that the consumer must assign positive probability in\n\\emph{exactly one} shop at any buying time. Our results apply to many\nreal-world applications, ranging from cost management in \\texttt{IaaS} cloud to\nscheduling in distributed computing. \n\n"}
{"id": "1404.2750", "contents": "Title: Efficient Advert Assignment Abstract: We develop a framework for the analysis of large-scale Ad-auctions where\nadverts are assigned over a continuum of search types. For this pay-per-click\nmarket, we provide an efficient mechanism that maximizes social welfare. In\nparticular, we show that the social welfare optimization can be solved in\nseparate optimizations conducted on the time-scales relevant to the search\nplatform and advertisers. Here, on each search occurrence, the platform solves\nan assignment problem and, on a slower time-scale, each advertiser submits a\nbid which matches its demand for click-throughs with supply. Importantly,\nknowledge of global parameters, such as the distribution of search terms, is\nnot required when separating the problem in this way. Exploiting the\ninformation asymmetry between the platform and advertiser, we describe a simple\nmechanism which incentivizes truthful bidding and has a unique Nash equilibrium\nthat is socially optimal, and thus implements our decomposition. Further, we\nconsider models where advertisers adapt their bids smoothly over time, and\nprove convergence to the solution that maximizes social welfare. Finally, we\ndescribe several extensions which illustrate the flexibility and tractability\nof our framework. \n\n"}
{"id": "1404.2861", "contents": "Title: Distributed Signaling Games Abstract: A recurring theme in recent computer science literature is that proper design\nof signaling schemes is a crucial aspect of effective mechanisms aiming to\noptimize social welfare or revenue. One of the research endeavors of this line\nof work is understanding the algorithmic and computational complexity of\ndesigning efficient signaling schemes. In reality, however, information is\ntypically not held by a central authority, but is distributed among multiple\nsources (third-party \"mediators\"), a fact that dramatically changes the\nstrategic and combinatorial nature of the signaling problem, making it a game\nbetween information providers, as opposed to a traditional mechanism design\nproblem.\n  In this paper we introduce {\\em distributed signaling games}, while using\ndisplay advertising as a canonical example for introducing this foundational\nframework. A distributed signaling game may be a pure coordination game (i.e.,\na distributed optimization task), or a non-cooperative game. In the context of\npure coordination games, we show a wide gap between the computational\ncomplexity of the centralized and distributed signaling problems. On the other\nhand, we show that if the information structure of each mediator is assumed to\nbe \"local\", then there is an efficient algorithm that finds a near-optimal\n($5$-approximation) distributed signaling scheme.\n  In the context of non-cooperative games, the outcome generated by the\nmediators' signals may have different value to each (due to the auctioneer's\ndesire to align the incentives of the mediators with his own by relative\ncompensations). We design a mechanism for this problem via a novel application\nof Shapley's value, and show that it possesses some interesting properties, in\nparticular, it always admits a pure Nash equilibrium, and it never decreases\nthe revenue of the auctioneer. \n\n"}
{"id": "1404.3789", "contents": "Title: Group size effect on cooperation in social dilemmas Abstract: Social dilemmas are central to human society. Depletion of natural resources,\nclimate protection, security of energy supply, and workplace collaborations are\nall examples of social dilemmas. Since cooperative behaviour in a social\ndilemma is individually costly, Nash equilibrium predicts that humans should\nnot cooperate. Yet experimental studies show that people do cooperate even in\nanonymous one-shot interactions. In spite of the large number of participants\nin many modern social dilemmas, little is known about the effect of group size\non cooperation. Does larger group size favour or prevent cooperation? We\naddress this problem both experimentally and theoretically. Experimentally, we\nfind that there is no general answer: it depends on the strategic situation.\nSpecifically, we find that larger groups are more cooperative in the Public\nGoods game, but less cooperative in the N-person Prisoner's dilemma.\nTheoretically, we show that this behaviour is not consistent with either the\nFehr & Schmidt model or (a one-parameter version of) the Charness & Rabin\nmodel, but it is consistent with the cooperative equilibrium model introduced\nby the second author. \n\n"}
{"id": "1404.3801", "contents": "Title: Shortest reconfiguration paths in the solution space of Boolean formulas Abstract: Given a Boolean formula and a satisfying assignment, a flip is an operation\nthat changes the value of a variable in the assignment so that the resulting\nassignment remains satisfying. We study the problem of computing the shortest\nsequence of flips (if one exists) that transforms a given satisfying assignment\n$s$ to another satisfying assignment $t$ of a Boolean formula. Earlier work\ncharacterized the complexity of finding any (not necessarily the shortest)\nsequence of flips from one satisfying assignment to another using Schaefer's\nframework for classification of Boolean formulas. We build on it to provide a\ntrichotomy for the complexity of finding the shortest sequence of flips and\nshow that it is either in P, NP-complete, or PSPACE-complete.\n  Our result adds to the small set of complexity results known for shortest\nreconfiguration sequence problems by providing an example where the shortest\nsequence can be found in polynomial time even though its length is not equal to\nthe symmetric difference of the values of the variables in $s$ and $t$. This is\nin contrast to all reconfiguration problems studied so far, where polynomial\ntime algorithms for computing the shortest path were known only for cases where\nthe path modified the symmetric difference only. \n\n"}
{"id": "1404.3828", "contents": "Title: Generalized Second Price Auction with Probabilistic Broad Match Abstract: Generalized Second Price (GSP) auctions are widely used by search engines\ntoday to sell their ad slots. Most search engines have supported broad match\nbetween queries and bid keywords when executing GSP auctions, however, it has\nbeen revealed that GSP auction with the standard broad-match mechanism they are\ncurrently using (denoted as SBM-GSP) has several theoretical drawbacks (e.g.,\nits theoretical properties are known only for the single-slot case and\nfull-information setting, and even in this simple setting, the corresponding\nworst-case social welfare can be rather bad). To address this issue, we propose\na novel broad-match mechanism, which we call the Probabilistic Broad-Match\n(PBM) mechanism. Different from SBM that puts together the ads bidding on all\nthe keywords matched to a given query for the GSP auction, the GSP with PBM\n(denoted as PBM-GSP) randomly samples a keyword according to a predefined\nprobability distribution and only runs the GSP auction for the ads bidding on\nthis sampled keyword. We perform a comprehensive study on the theoretical\nproperties of the PBM-GSP. Specifically, we study its social welfare in the\nworst equilibrium, in both full-information and Bayesian settings. The results\nshow that PBM-GSP can generate larger welfare than SBM-GSP under mild\nconditions. Furthermore, we also study the revenue guarantee for PBM-GSP in\nBayesian setting. To the best of our knowledge, this is the first work on\nbroad-match mechanisms for GSP that goes beyond the single-slot case and the\nfull-information setting. \n\n"}
{"id": "1404.4718", "contents": "Title: Approximate Equilibrium and Incentivizing Social Coordination Abstract: We study techniques to incentivize self-interested agents to form socially\ndesirable solutions in scenarios where they benefit from mutual coordination.\nTowards this end, we consider coordination games where agents have different\nintrinsic preferences but they stand to gain if others choose the same strategy\nas them. For non-trivial versions of our game, stable solutions like Nash\nEquilibrium may not exist, or may be socially inefficient even when they do\nexist. This motivates us to focus on designing efficient algorithms to compute\n(almost) stable solutions like Approximate Equilibrium that can be realized if\nagents are provided some additional incentives. Our results apply in many\nsettings like adoption of new products, project selection, and group formation,\nwhere a central authority can direct agents towards a strategy but agents may\ndefect if they have better alternatives. We show that for any given instance,\nwe can either compute a high quality approximate equilibrium or a near-optimal\nsolution that can be stabilized by providing small payments to some players. We\nthen generalize our model to encompass situations where player relationships\nmay exhibit complementarities and present an algorithm to compute an\nApproximate Equilibrium whose stability factor is linear in the degree of\ncomplementarity. Our results imply that a little influence is necessary in\norder to ensure that selfish players coordinate and form socially efficient\nsolutions. \n\n"}
{"id": "1404.5127", "contents": "Title: Optimising Trade-offs Among Stakeholders in Ad Auctions Abstract: We examine trade-offs among stakeholders in ad auctions. Our metrics are the\nrevenue for the utility of the auctioneer, the number of clicks for the utility\nof the users and the welfare for the utility of the advertisers. We show how to\noptimize linear combinations of the stakeholder utilities, showing that these\ncan be tackled through a GSP auction with a per-click reserve price. We then\nexamine constrained optimization of stakeholder utilities.\n  We use simulations and analysis of real-world sponsored search auction data\nto demonstrate the feasible trade-offs, examining the effect of changing the\nallowed number of ads on the utilities of the stakeholders. We investigate both\nshort term effects, when the players do not have the time to modify their\nbehavior, and long term equilibrium conditions.\n  Finally, we examine a combinatorially richer constrained optimization\nproblem, where there are several possible allowed configurations (templates) of\nad formats. This model captures richer ad formats, which allow using the\navailable screen real estate in various ways. We show that two natural\ngeneralizations of the GSP auction rules to this domain are poorly behaved,\nresulting in not having a symmetric Nash equilibrium or having one with poor\nwelfare. We also provide positive results for restricted cases. \n\n"}
{"id": "1404.6003", "contents": "Title: Buying Private Data without Verification Abstract: We consider the problem of designing a survey to aggregate non-verifiable\ninformation from a privacy-sensitive population: an analyst wants to compute\nsome aggregate statistic from the private bits held by each member of a\npopulation, but cannot verify the correctness of the bits reported by\nparticipants in his survey. Individuals in the population are strategic agents\nwith a cost for privacy, \\ie, they not only account for the payments they\nexpect to receive from the mechanism, but also their privacy costs from any\ninformation revealed about them by the mechanism's outcome---the computed\nstatistic as well as the payments---to determine their utilities. How can the\nanalyst design payments to obtain an accurate estimate of the population\nstatistic when individuals strategically decide both whether to participate and\nwhether to truthfully report their sensitive information?\n  We design a differentially private peer-prediction mechanism that supports\naccurate estimation of the population statistic as a Bayes-Nash equilibrium in\nsettings where agents have explicit preferences for privacy. The mechanism\nrequires knowledge of the marginal prior distribution on bits $b_i$, but does\nnot need full knowledge of the marginal distribution on the costs $c_i$,\ninstead requiring only an approximate upper bound. Our mechanism guarantees\n$\\epsilon$-differential privacy to each agent $i$ against any adversary who can\nobserve the statistical estimate output by the mechanism, as well as the\npayments made to the $n-1$ other agents $j\\neq i$. Finally, we show that with\nslightly more structured assumptions on the privacy cost functions of each\nagent, the cost of running the survey goes to $0$ as the number of agents\ndiverges. \n\n"}
{"id": "1404.6013", "contents": "Title: Service-Constraint Based Truthful Incentive Mechanisms for Crowd Sensing Abstract: Crowd sensing is a new paradigm which leverages the pervasive smartphones to\nefficiently collect and upload sensing data, enabling numerous novel\napplications. To achieve good service quality for a crowd sensing application,\nincentive mechanisms are necessary for attracting more user participation. Most\nof existing mechanisms apply only for the budget-constraint scenario where the\nplatform (the crowd sensing organizer) has a budget limit. On the contrary, we\nfocus on a different scenario where the platform has a service limit. Based on\nthe offline and online auction model, we consider a general problem: users\nsubmit their private profiles to the platform, and the platform aims at\nselecting a subset of users before a specified deadline for minimizing the\ntotal payment while a specific service can be completed. Specially, we design\noffline and online service-constraint incentive mechanisms for the case where\nthe value function of selected users is monotone submodular. The mechanisms are\nindividual rationality, task feasibility, computational efficiency,\ntruthfulness, consumer sovereignty, constant frugality, and also performs well\nin practice. Finally, we use extensive simulations to demonstrate the\ntheoretical properties of our mechanisms. \n\n"}
{"id": "1405.0823", "contents": "Title: Ready for the design of voting rules? Abstract: The design of fair voting rules has been addressed quite often in the\nliterature. Still, the so-called inverse problem is not entirely resolved. We\nsummarize some achievements in this direction and formulate explicit open\nquestions and conjectures. \n\n"}
{"id": "1405.1481", "contents": "Title: Graphical potential games Abstract: We study the class of potential games that are also graphical games with\nrespect to a given graph $G$ of connections between the players. We show that,\nup to strategic equivalence, this class of games can be identified with the set\nof Markov random fields on $G$.\n  From this characterization, and from the Hammersley-Clifford theorem, it\nfollows that the potentials of such games can be decomposed to local\npotentials. We use this decomposition to strongly bound the number of strategy\nchanges of a single player along a better response path. This result extends to\ngeneralized graphical potential games, which are played on infinite graphs. \n\n"}
{"id": "1405.1510", "contents": "Title: Mostly Sunny: A Forecast of Tomorrow's Power Index Research Abstract: Power index research has been a very active field in the last decades. Will\nthis continue or are all the important questions solved? We argue that there\nare still many opportunities to conduct useful research with and on power\nindices. Positive and normative questions keep calling for theoretical and\nempirical attention. Technical and technological improvements are likely to\nboost applicability. \n\n"}
{"id": "1405.2447", "contents": "Title: Reconfiguration over tree decompositions Abstract: A vertex-subset graph problem $Q$ defines which subsets of the vertices of an\ninput graph are feasible solutions. The reconfiguration version of a\nvertex-subset problem $Q$ asks whether it is possible to transform one feasible\nsolution for $Q$ into another in at most $\\ell$ steps, where each step is a\nvertex addition or deletion, and each intermediate set is also a feasible\nsolution for $Q$ of size bounded by $k$. Motivated by recent results\nestablishing W[1]-hardness of the reconfiguration versions of most\nvertex-subset problems parameterized by $\\ell$, we investigate the complexity\nof such problems restricted to graphs of bounded treewidth. We show that the\nreconfiguration versions of most vertex-subset problems remain PSPACE-complete\non graphs of treewidth at most $t$ but are fixed-parameter tractable\nparameterized by $\\ell + t$ for all vertex-subset problems definable in monadic\nsecond-order logic (MSOL). To prove the latter result, we introduce a technique\nwhich allows us to circumvent cardinality constraints and define\nreconfiguration problems in MSOL. \n\n"}
{"id": "1405.2452", "contents": "Title: Mechanism Design for Crowdsourcing: An Optimal 1-1/e Competitive\n  Budget-Feasible Mechanism for Large Markets Abstract: In this paper we consider a mechanism design problem in the context of\nlarge-scale crowdsourcing markets such as Amazon's Mechanical Turk,\nClickWorker, CrowdFlower. In these markets, there is a requester who wants to\nhire workers to accomplish some tasks. Each worker is assumed to give some\nutility to the requester. Moreover each worker has a minimum cost that he wants\nto get paid for getting hired. This minimum cost is assumed to be private\ninformation of the workers. The question then is - if the requester has a\nlimited budget, how to design a direct revelation mechanism that picks the\nright set of workers to hire in order to maximize the requester's utility.\n  We note that although the previous work has studied this problem, a crucial\ndifference in which we deviate from earlier work is the notion of large-scale\nmarkets that we introduce in our model. Without the large market assumption, it\nis known that no mechanism can achieve an approximation factor better than\n0.414 and 0.5 for deterministic and randomized mechanisms respectively (while\nthe best known deterministic and randomized mechanisms achieve an approximation\nratio of 0.292 and 0.33 respectively). In this paper, we design a\nbudget-feasible mechanism for large markets that achieves an approximation\nfactor of 1-1/e (i.e. almost 0.63). Our mechanism can be seen as a\ngeneralization of an alternate way to look at the proportional share mechanism\nwhich is used in all the previous works so far on this problem. Interestingly,\nwe also show that our mechanism is optimal by showing that no truthful\nmechanism can achieve a factor better than 1-1/e; thus, fully resolving this\nsetting. Finally we consider the more general case of submodular utility\nfunctions and give new and improved mechanisms for the case when the markets\nare large. \n\n"}
{"id": "1405.3460", "contents": "Title: Satisfaction in societies with opinion leaders and mediators: properties\n  and an axiomatization Abstract: In this paper we propose the opinion leader-follower through mediators\nsystems (OLFM systems) a multiple-action collective choice model for societies.\nIn those societies three kind of actors are considered: opinion leaders that\ncan exert certain influence over the decision of other actors, followers that\ncan be convinced to modify their original decisions, and independent actors\nthat neither are influenced nor can influence; mediators are actors that both\nare influenced and influence other actors. This is a generalization of the\nopinion leader-follower systems (OLF systems) proposed by van den Brink R, et\nal. (2011).\n  The satisfaction score is defined on the set of actors. For each actor it\nmeasures the number of society initial decisions in which the final collective\ndecision coincides with the one that the actor initially selected. We\ngeneralize in OLFM systems some properties that the satisfaction score meets\nfor OLF systems. By using these properties, we provide an axiomatization of the\nsatisfaction score for the case in which followers maintain their own initial\ndecisions unless all their opinion leaders share an opposite inclination. This\nnew axiomatization generalizes the one given by van den Brink R, et al. (2012)\nfor OLF systems under the same restrictions. \n\n"}
{"id": "1405.6791", "contents": "Title: Agnostic Learning of Disjunctions on Symmetric Distributions Abstract: We consider the problem of approximating and learning disjunctions (or\nequivalently, conjunctions) on symmetric distributions over $\\{0,1\\}^n$.\nSymmetric distributions are distributions whose PDF is invariant under any\npermutation of the variables. We give a simple proof that for every symmetric\ndistribution $\\mathcal{D}$, there exists a set of $n^{O(\\log{(1/\\epsilon)})}$\nfunctions $\\mathcal{S}$, such that for every disjunction $c$, there is function\n$p$, expressible as a linear combination of functions in $\\mathcal{S}$, such\nthat $p$ $\\epsilon$-approximates $c$ in $\\ell_1$ distance on $\\mathcal{D}$ or\n$\\mathbf{E}_{x \\sim \\mathcal{D}}[ |c(x)-p(x)|] \\leq \\epsilon$. This directly\ngives an agnostic learning algorithm for disjunctions on symmetric\ndistributions that runs in time $n^{O( \\log{(1/\\epsilon)})}$. The best known\nprevious bound is $n^{O(1/\\epsilon^4)}$ and follows from approximation of the\nmore general class of halfspaces (Wimmer, 2010). We also show that there exists\na symmetric distribution $\\mathcal{D}$, such that the minimum degree of a\npolynomial that $1/3$-approximates the disjunction of all $n$ variables is\n$\\ell_1$ distance on $\\mathcal{D}$ is $\\Omega( \\sqrt{n})$. Therefore the\nlearning result above cannot be achieved via $\\ell_1$-regression with a\npolynomial basis used in most other agnostic learning algorithms.\n  Our technique also gives a simple proof that for any product distribution\n$\\mathcal{D}$ and every disjunction $c$, there exists a polynomial $p$ of\ndegree $O(\\log{(1/\\epsilon)})$ such that $p$ $\\epsilon$-approximates $c$ in\n$\\ell_1$ distance on $\\mathcal{D}$. This was first proved by Blais et al.\n(2008) via a more involved argument. \n\n"}
{"id": "1406.0533", "contents": "Title: Distributed bargaining in dyadic-exchange networks Abstract: This paper considers dyadic-exchange networks in which individual agents\nautonomously form coalitions of size two and agree on how to split a\ntransferable utility. Valid results for this game include stable (if agents\nhave no unilateral incentive to deviate), balanced (if matched agents obtain\nsimilar benefits from collaborating), or Nash (both stable and balanced)\noutcomes. We design provably-correct continuous-time algorithms to find each of\nthese classes of outcomes in a distributed way. Our algorithmic design to find\nNash bargaining solutions builds on the other two algorithms by having the\ndynamics for finding stable outcomes feeding into the one for finding balanced\nones. Our technical approach to establish convergence and robustness combines\nnotions and tools from optimization, graph theory, nonsmooth analysis, and\nLyapunov stability theory and provides a useful framework for further\nextensions. We illustrate our results in a wireless communication scenario\nwhere single-antenna devices have the possibility of working as 2-antenna\nvirtual devices to improve channel capacity. \n\n"}
{"id": "1406.0728", "contents": "Title: A Game-theoretic Machine Learning Approach for Revenue Maximization in\n  Sponsored Search Abstract: Sponsored search is an important monetization channel for search engines, in\nwhich an auction mechanism is used to select the ads shown to users and\ndetermine the prices charged from advertisers. There have been several pieces\nof work in the literature that investigate how to design an auction mechanism\nin order to optimize the revenue of the search engine. However, due to some\nunrealistic assumptions used, the practical values of these studies are not\nvery clear. In this paper, we propose a novel \\emph{game-theoretic machine\nlearning} approach, which naturally combines machine learning and game theory,\nand learns the auction mechanism using a bilevel optimization framework. In\nparticular, we first learn a Markov model from historical data to describe how\nadvertisers change their bids in response to an auction mechanism, and then for\nany given auction mechanism, we use the learnt model to predict its\ncorresponding future bid sequences. Next we learn the auction mechanism through\nempirical revenue maximization on the predicted bid sequences. We show that the\nempirical revenue will converge when the prediction period approaches infinity,\nand a Genetic Programming algorithm can effectively optimize this empirical\nrevenue. Our experiments indicate that the proposed approach is able to produce\na much more effective auction mechanism than several baselines. \n\n"}
{"id": "1406.1790", "contents": "Title: Behavioral Mechanism Design: Optimal Contests for Simple Agents Abstract: Incentives are more likely to elicit desired outcomes when they are designed\nbased on accurate models of agents' strategic behavior. A growing literature,\nhowever, suggests that people do not quite behave like standard economic agents\nin a variety of environments, both online and offline. What consequences might\nsuch differences have for the optimal design of mechanisms in these\nenvironments? In this paper, we explore this question in the context of optimal\ncontest design for simple agents---agents who strategically reason about\nwhether or not to participate in a system, but not about the input they provide\nto it. Specifically, consider a contest where $n$ potential contestants with\ntypes $(q_i,c_i)$ each choose between participating and producing a submission\nof quality $q_i$ at cost $c_i$, versus not participating at all, to maximize\ntheir utilities. How should a principal distribute a total prize $V$ amongst\nthe $n$ ranks to maximize some increasing function of the qualities of elicited\nsubmissions in a contest with such simple agents?\n  We first solve the optimal contest design problem for settings with\nhomogenous participation costs $c_i = c$. Here, the optimal contest is always a\nsimple contest, awarding equal prizes to the top $j^*$ contestants for a\nsuitable choice of $j^*$. (In comparable models with strategic effort choices,\nthe optimal contest is either a winner-take-all contest or awards possibly\nunequal prizes, depending on the curvature of agents' effort cost functions.)\nWe next address the general case with heterogeneous costs where agents' types\nare inherently two-dimensional, significantly complicating equilibrium\nanalysis. Our main result here is that the winner-take-all contest is a\n3-approximation of the optimal contest when the principal's objective is to\nmaximize the quality of the best elicited contribution. \n\n"}
{"id": "1406.3278", "contents": "Title: An n-to-1 Bidder Reduction for Multi-item Auctions and its Applications Abstract: In this paper, we introduce a novel approach for reducing the $k$-item\n$n$-bidder auction with additive valuation to $k$-item $1$-bidder auctions.\nThis approach, called the \\emph{Best-Guess} reduction, can be applied to\naddress several central questions in optimal revenue auction theory such as the\npower of randomization, and Bayesian versus dominant-strategy implementations.\nFirst, when the items have independent valuation distributions, we present a\ndeterministic mechanism called {\\it Deterministic Best-Guess} that yields at\nleast a constant fraction of the optimal revenue by any randomized mechanism.\nSecond, if all the $nk$ valuation random variables are independent, the optimal\nrevenue achievable in {\\it dominant strategy incentive compatibility} (DSIC) is\nshown to be at least a constant fraction of that achievable in {\\it Bayesian\nincentive compatibility} (BIC). Third, when all the $nk$ values are identically\ndistributed according to a common one-dimensional distribution $F$, the optimal\nrevenue is shown to be expressible in the closed form $\\Theta(k(r+\\int_0^{mr}\n(1-F(x)^n) \\ud x))$ where $r= sup_{x\\geq 0} \\, x(1 - F(x)^n)$ and $m=\\lceil\nk/n\\rceil$; this revenue is achievable by a simple mechanism called\n\\emph{2nd-Price Bundling}. All our results apply to arbitrary distributions,\nregular or irregular. \n\n"}
{"id": "1406.6773", "contents": "Title: Approximately Optimal Mechanism Design: Motivation, Examples, and\n  Lessons Learned Abstract: Optimal mechanism design enjoys a beautiful and well-developed theory, and\nalso a number of killer applications. Rules of thumb produced by the field\ninfluence everything from how governments sell wireless spectrum licenses to\nhow the major search engines auction off online advertising. There are,\nhowever, some basic problems for which the traditional optimal mechanism design\napproach is ill-suited --- either because it makes overly strong assumptions,\nor because it advocates overly complex designs. The thesis of this paper is\nthat approximately optimal mechanisms allow us to reason about fundamental\nquestions that seem out of reach of the traditional theory.\n  This survey has three main parts. The first part describes the approximately\noptimal mechanism design paradigm --- how it works, and what we aim to learn by\napplying it. The second and third parts of the survey cover two case studies,\nwhere we instantiate the general design paradigm to investigate two basic\nquestions. In the first example, we consider revenue maximization in a\nsingle-item auction with heterogeneous bidders. Our goal is to understand if\ncomplexity --- in the sense of detailed distributional knowledge --- is an\nessential feature of good auctions for this problem, or alternatively if there\nare simpler auctions that are near-optimal. The second example considers\nwelfare maximization with multiple items. Our goal here is similar in spirit:\nwhen is complexity --- in the form of high-dimensional bid spaces --- an\nessential feature of every auction that guarantees reasonable welfare? Are\nthere interesting cases where low-dimensional bid spaces suffice? \n\n"}
{"id": "1406.6889", "contents": "Title: Noncooperative algorithms in self-assembly Abstract: We show the first non-trivial positive algorithmic results (i.e. programs\nwhose output is larger than their size), in a model of self-assembly that has\nso far resisted many attempts of formal analysis or programming: the planar\nnon-cooperative variant of Winfree's abstract Tile Assembly Model.\n  This model has been the center of several open problems and conjectures in\nthe last fifteen years, and the first fully general results on its\ncomputational power were only proven recently (SODA 2014). These results, as\nwell as ours, exemplify the intricate connections between computation and\ngeometry that can occur in self-assembly.\n  In this model, tiles can stick to an existing assembly as soon as one of\ntheir sides matches the existing assembly. This feature contrasts with the\ngeneral cooperative model, where it can be required that tiles match on\n\\emph{several} of their sides in order to bind.\n  In order to describe our algorithms, we also introduce a generalization of\nregular expressions called Baggins expressions. Finally, we compare this model\nto other automata-theoretic models. \n\n"}
{"id": "1407.0085", "contents": "Title: Improved Quantum Algorithm for Triangle Finding via Combinatorial\n  Arguments Abstract: In this paper we present a quantum algorithm solving the triangle finding\nproblem in unweighted graphs with query complexity $\\tilde O(n^{5/4})$, where\n$n$ denotes the number of vertices in the graph. This improves the previous\nupper bound $O(n^{9/7})=O(n^{1.285...})$ recently obtained by Lee, Magniez and\nSantha. Our result shows, for the first time, that in the quantum query\ncomplexity setting unweighted triangle finding is easier than its edge-weighted\nversion, since for finding an edge-weighted triangle Belovs and Rosmanis proved\nthat any quantum algorithm requires $\\Omega(n^{9/7}/\\sqrt{\\log n})$ queries.\nOur result also illustrates some limitations of the non-adaptive learning graph\napproach used to obtain the previous $O(n^{9/7})$ upper bound since, even over\nunweighted graphs, any quantum algorithm for triangle finding obtained using\nthis approach requires $\\Omega(n^{9/7}/\\sqrt{\\log n})$ queries as well. To\nbypass the obstacles characterized by these lower bounds, our quantum algorithm\nuses combinatorial ideas exploiting the graph-theoretic properties of triangle\nfinding, which cannot be used when considering edge-weighted graphs or the\nnon-adaptive learning graph approach. \n\n"}
{"id": "1407.1170", "contents": "Title: Incentive and stability in the Rock-Paper-Scissors game: an experimental\n  investigation Abstract: In a two-person Rock-Paper-Scissors (RPS) game, if we set a loss worth\nnothing and a tie worth 1, and the payoff of winning (the incentive a) as a\nvariable, this game is called as generalized RPS game. The generalized RPS game\nis a representative mathematical model to illustrate the game dynamics,\nappearing widely in textbook. However, how actual motions in these games depend\non the incentive has never been reported quantitatively. Using the data from 7\ngames with different incentives, including 84 groups of 6 subjects playing the\ngame in 300-round, with random-pair tournaments and local information recorded,\nwe find that, both on social and individual level, the actual motions are\nchanging continuously with the incentive. More expressively, some\nrepresentative findings are, (1) in social collective strategy transit views,\nthe forward transition vector field is more and more centripetal as the\nstability of the system increasing; (2) In the individual behavior of strategy\ntransit view, there exists a phase transformation as the stability of the\nsystems increasing, and the phase transformation point being near the standard\nRPS; (3) Conditional response behaviors are structurally changing accompanied\nby the controlled incentive. As a whole, the best response behavior increases\nand the win-stay lose-shift (WSLS) behavior declines with the incentive.\nFurther, the outcome of win, tie, and lose influence the best response behavior\nand WSLS behavior. Both as the best response behavior, the win-stay behavior\ndeclines with the incentive while the lose-left-shift behavior increase with\nthe incentive. And both as the WSLS behavior, the lose-left-shift behavior\nincrease with the incentive, but the lose-right-shift behaviors declines with\nthe incentive. We hope to learn which one in tens of learning models can\ninterpret the empirical observation above. \n\n"}
{"id": "1407.1556", "contents": "Title: Energy Efficiency and Spectral Efficiency Tradeoff in Device-to-Device\n  (D2D) Communications Abstract: In this letter, we investigate the tradeoff between energy efficiency (EE)\nand spectral efficiency (SE) in device-to-device (D2D) communications\nunderlaying cellular networks with uplink channel reuse. The resource\nallocation problem is modeled as a noncooperative game, in which each user\nequipment (UE) is self-interested and wants to maximize its own EE. Given the\nSE requirement and maximum transmission power constraints, a distributed\nenergy-efficient resource allocation algorithm is proposed by exploiting the\nproperties of the nonlinear fractional programming. The relationships between\nthe EE and SE tradeoff of the proposed algorithm and system parameters are\nanalyzed and verified through computer simulations. \n\n"}
{"id": "1407.1601", "contents": "Title: Deadline Differentiated Pricing of Deferrable Electric Loads Abstract: A large fraction of the total electric load is comprised of end-use devices\nwhose demand for energy is inherently deferrable in time. Of interest is the\npotential to leverage on such latent flexibility in demand to absorb\nvariability in power supplied from intermittent renewable generation. The\nchallenge, however, lies in designing incentives to reliably induce the desired\nresponse in demand. With an eye to electric vehicle charging, we propose a\nnovel forward market for differentiated electric power services, where\nconsumers consent to deferred service of pre-specified loads in exchange for a\nreduced per-unit price for energy. The longer a consumer is willing to defer,\nthe larger the reduction in price. The proposed forward contract provides a\nguarantee on the aggregate quantity of energy to be delivered by a\nconsumer-specified deadline. Under the earliest-deadline-first (EDF) scheduling\npolicy, which is shown to be optimal for the supplier, we explicitly\ncharacterize a non-discriminatory, deadline-differentiated pricing scheme that\nyields an efficient competitive equilibrium between the supplier and consumers.\nWe further show that this efficient pricing scheme, in combination with EDF\nscheduling, is incentive compatible (IC) in that every consumer would like to\nreveal her true deadline to the supplier, regardless of the actions taken by\nother consumers. \n\n"}
{"id": "1407.2143", "contents": "Title: Parameterized Algorithmics for Computational Social Choice: Nine\n  Research Challenges Abstract: Computational Social Choice is an interdisciplinary research area involving\nEconomics, Political Science, and Social Science on the one side, and\nMathematics and Computer Science (including Artificial Intelligence and\nMultiagent Systems) on the other side. Typical computational problems studied\nin this field include the vulnerability of voting procedures against attacks,\nor preference aggregation in multi-agent systems. Parameterized Algorithmics is\na subfield of Theoretical Computer Science seeking to exploit meaningful\nproblem-specific parameters in order to identify tractable special cases of in\ngeneral computationally hard problems. In this paper, we propose nine of our\nfavorite research challenges concerning the parameterized complexity of\nproblems appearing in this context. \n\n"}
{"id": "1407.2576", "contents": "Title: The size of the core in assignment markets Abstract: Assignment markets involve matching with transfers, as in labor markets and\nhousing markets. We consider a two-sided assignment market with agent types and\nstochastic structure similar to models used in empirical studies, and\ncharacterize the size of the core in such markets. Each agent has a randomly\ndrawn productivity with respect to each type of agent on the other side. The\nvalue generated from a match between a pair of agents is the sum of the two\nproductivity terms, each of which depends only on the type but not the identity\nof one of the agents, and a third deterministic term driven by the pair of\ntypes. We allow the number of agents to grow, keeping the number of agent types\nfixed. Let $n$ be the number of agents and $K$ be the number of types on the\nside of the market with more types. We find, under reasonable assumptions, that\nthe relative variation in utility per agent over core outcomes is bounded as\n$O^*(1/n^{1/K})$, where polylogarithmic factors have been suppressed. Further,\nwe show that this bound is tight in worst case. We also provide a tighter bound\nunder more restrictive assumptions. Our results provide partial justification\nfor the typical assumption of a unique core outcome in empirical studies. \n\n"}
{"id": "1407.2641", "contents": "Title: Private Pareto Optimal Exchange Abstract: We consider the problem of implementing an individually rational,\nasymptotically Pareto optimal allocation in a barter-exchange economy where\nagents are endowed with goods and have preferences over the goods of others,\nbut may not use money as a medium of exchange. Because one of the most\nimportant instantiations of such economies is kidney exchange -- where the\n\"input\"to the problem consists of sensitive patient medical records -- we ask\nto what extent such exchanges can be carried out while providing formal privacy\nguarantees to the participants. We show that individually rational allocations\ncannot achieve any non-trivial approximation to Pareto optimality if carried\nout under the constraint of differential privacy -- or even the relaxation of\n\\emph{joint} differential privacy, under which it is known that asymptotically\noptimal allocations can be computed in two-sided markets, where there is a\ndistinction between buyers and sellers and we are concerned only with privacy\nof the buyers~\\citep{Matching}. We therefore consider a further relaxation that\nwe call \\emph{marginal} differential privacy -- which promises, informally,\nthat the privacy of every agent $i$ is protected from every other agent $j \\neq\ni$ so long as $j$ does not collude or share allocation information with other\nagents. We show that, under marginal differential privacy, it is possible to\ncompute an individually rational and asymptotically Pareto optimal allocation\nin such exchange economies. \n\n"}
{"id": "1407.2929", "contents": "Title: Complexity of counting subgraphs: only the boundedness of the\n  vertex-cover number counts Abstract: For a class $\\mathcal{H}$ of graphs, #Sub$(\\mathcal{H})$ is the counting\nproblem that, given a graph $H\\in \\mathcal{H}$ and an arbitrary graph $G$, asks\nfor the number of subgraphs of $G$ isomorphic to $H$. It is known that if\n$\\mathcal{H}$ has bounded vertex-cover number (equivalently, the size of the\nmaximum matching in $\\mathcal{H}$ is bounded), then #Sub$(\\mathcal{H})$ is\npolynomial-time solvable. We complement this result with a corresponding lower\nbound: if $\\mathcal{H}$ is any recursively enumerable class of graphs with\nunbounded vertex-cover number, then #Sub$(\\mathcal{H})$ is #W[1]-hard\nparameterized by the size of $H$ and hence not polynomial-time solvable and not\neven fixed-parameter tractable, unless FPT = #W[1].\n  As a first step of the proof, we show that counting $k$-matchings in\nbipartite graphs is #W[1]-hard. Recently, Curticapean [ICALP 2013] proved the\n#W[1]-hardness of counting $k$-matchings in general graphs; our result\nstrengthens this statement to bipartite graphs with a considerably simpler\nproof and even shows that, assuming the Exponential Time Hypothesis (ETH),\nthere is no $f(k)n^{o(k/\\log k)}$ time algorithm for counting $k$-matchings in\nbipartite graphs for any computable function $f(k)$. As a consequence, we\nobtain an independent and somewhat simpler proof of the classical result of\nFlum and Grohe [SICOMP 2004] stating that counting paths of length $k$ is\n#W[1]-hard, as well as a similar almost-tight ETH-based lower bound on the\nexponent. \n\n"}
{"id": "1407.5030", "contents": "Title: To Reach or not to Reach? Efficient Algorithms for Total-Payoff Games Abstract: Quantitative games are two-player zero-sum games played on directed weighted\ngraphs. Total-payoff games (that can be seen as a refinement of the\nwell-studied mean-payoff games) are the variant where the payoff of a play is\ncomputed as the sum of the weights. Our aim is to describe the first\npseudo-polynomial time algorithm for total-payoff games in the presence of\narbitrary weights. It consists of a non-trivial application of the value\niteration paradigm. Indeed, it requires to study, as a milestone, a refinement\nof these games, called min-cost reachability games, where we add a reachability\nobjective to one of the players. For these games, we give an efficient value\niteration algorithm to compute the values and optimal strategies (when they\nexist), that runs in pseudo-polynomial time. We also propose heuristics\nallowing one to possibly speed up the computations in both cases. \n\n"}
{"id": "1407.5442", "contents": "Title: Cooperative Game Theoretic Solution Concepts for top-$k$ Problems Abstract: The problem of finding the $k$ most critical nodes, referred to as the\n$top\\text{-}k$ problem, is a very important one in several contexts such as\ninformation diffusion and preference aggregation in social networks, clustering\nof data points, etc. It has been observed in the literature that the value\nallotted to a node by most of the popular cooperative game theoretic solution\nconcepts, acts as a good measure of appropriateness of that node (or a data\npoint) to be included in the $top\\text{-}k$ set, by itself. However, in\ngeneral, nodes having the highest $k$ values are not the desirable\n$top\\text{-}k$ nodes, because the appropriateness of a node to be a part of the\n$top\\text{-}k$ set depends on other nodes in the set. As this is not explicitly\ncaptured by cooperative game theoretic solution concepts, it is necessary to\npost-process the obtained values in order to output the suitable $top\\text{-}k$\nnodes. In this paper, we propose several such post-processing methods and give\nreasoning behind each of them, and also propose a standalone algorithm that\ncombines cooperative game theoretic solution concepts with the popular greedy\nhill-climbing algorithm. \n\n"}
{"id": "1407.5587", "contents": "Title: Weihrauch degrees of finding equilibria in sequential games Abstract: We consider the degrees of non-computability (Weihrauch degrees) of finding\nwinning strategies (or more generally, Nash equilibria) in infinite sequential\ngames with certain winning sets (or more generally, outcome sets). In\nparticular, we show that as the complexity of the winning sets increases in the\ndifference hierarchy, the complexity of constructing winning strategies\nincreases in the effective Borel hierarchy. \n\n"}
{"id": "1407.7216", "contents": "Title: PTAS for Minimax Approval Voting Abstract: We consider Approval Voting systems where each voter decides on a subset to\ncandidates he/she approves. We focus on the optimization problem of finding the\ncommittee of fixed size k minimizing the maximal Hamming distance from a vote.\nIn this paper we give a PTAS for this problem and hence resolve the open\nquestion raised by Carragianis et al. [AAAI'10]. The result is obtained by\nadapting the techniques developed by Li et al. [JACM'02] originally used for\nthe less constrained Closest String problem. The technique relies on extracting\ninformation and structural properties of constant size subsets of votes. \n\n"}
{"id": "1407.7294", "contents": "Title: Online Learning and Profit Maximization from Revealed Preferences Abstract: We consider the problem of learning from revealed preferences in an online\nsetting. In our framework, each period a consumer buys an optimal bundle of\ngoods from a merchant according to her (linear) utility function and current\nprices, subject to a budget constraint. The merchant observes only the\npurchased goods, and seeks to adapt prices to optimize his profits. We give an\nefficient algorithm for the merchant's problem that consists of a learning\nphase in which the consumer's utility function is (perhaps partially) inferred,\nfollowed by a price optimization step. We also consider an alternative online\nlearning algorithm for the setting where prices are set exogenously, but the\nmerchant would still like to predict the bundle that will be bought by the\nconsumer for purposes of inventory or supply chain management. In contrast with\nmost prior work on the revealed preferences problem, we demonstrate that by\nmaking stronger assumptions on the form of utility functions, efficient\nalgorithms for both learning and profit maximization are possible, even in\nadaptive, online settings. \n\n"}
{"id": "1407.8269", "contents": "Title: Justified Representation in Approval-Based Committee Voting Abstract: We consider approval-based committee voting, i.e. the setting where each\nvoter approves a subset of candidates, and these votes are then used to select\na fixed-size set of winners (committee). We propose a natural axiom for this\nsetting, which we call justified representation (JR). This axiom requires that\nif a large enough group of voters exhibits agreement by supporting the same\ncandidate, then at least one voter in this group has an approved candidate in\nthe winning committee. We show that for every list of ballots it is possible to\nselect a committee that provides JR. However, it turns out that several\nprominent approval-based voting rules may fail to output such a committee. In\nparticular, while Proportional Approval Voting (PAV) always outputs a committee\nthat provides JR, Reweighted Approval Voting (RAV), a tractable approximation\nto PAV, does not have this property. We then introduce a stronger version of\nthe JR axiom, which we call extended justified representation (EJR), and show\nthat PAV satisfies EJR, while other rules we consider do not; indeed, EJR can\nbe used to characterize PAV within the class of weighted PAV rules. We also\nconsider several other questions related to JR and EJR, including the\nrelationship between JR/EJR and core stability, and the complexity of the\nassociated algorithmic problems. \n\n"}
{"id": "1408.0023", "contents": "Title: Strategic Evolution of Adversaries Against Temporal Platform Diversity\n  Active Cyber Defenses Abstract: Adversarial dynamics are a critical facet within the cyber security domain,\nin which there exists a co-evolution between attackers and defenders in any\ngiven threat scenario. While defenders leverage capabilities to minimize the\npotential impact of an attack, the adversary is simultaneously developing\ncountermeasures to the observed defenses. In this study, we develop a set of\ntools to model the adaptive strategy formulation of an intelligent actor\nagainst an active cyber defensive system. We encode strategies as binary\nchromosomes representing finite state machines that evolve according to\nHolland's genetic algorithm. We study the strategic considerations including\noverall actor reward balanced against the complexity of the determined\nstrategies. We present a series of simulation results demonstrating the ability\nto automatically search a large strategy space for optimal resultant fitness\nagainst a variety of counter-strategies. \n\n"}
{"id": "1408.0258", "contents": "Title: The Pricing War Continues: On Competitive Multi-Item Pricing Abstract: We study a game with \\emph{strategic} vendors who own multiple items and a\nsingle buyer with a submodular valuation function. The goal of the vendors is\nto maximize their revenue via pricing of the items, given that the buyer will\nbuy the set of items that maximizes his net payoff.\n  We show this game may not always have a pure Nash equilibrium, in contrast to\nprevious results for the special case where each vendor owns a single item. We\ndo so by relating our game to an intermediate, discrete game in which the\nvendors only choose the available items, and their prices are set exogenously\nafterwards.\n  We further make use of the intermediate game to provide tight bounds on the\nprice of anarchy for the subset games that have pure Nash equilibria; we find\nthat the optimal PoA reached in the previous special cases does not hold, but\nonly a logarithmic one.\n  Finally, we show that for a special case of submodular functions, efficient\npure Nash equilibria always exist. \n\n"}
{"id": "1408.3783", "contents": "Title: Long-term causal effects of economic mechanisms on agent incentives Abstract: Economic mechanisms administer the allocation of resources to interested\nagents based on their self-reported types. One objective in mechanism design is\nto design a strategyproof process so that no agent will have an incentive to\nmisreport its type. However, typical analyses of the incentives properties of\nmechanisms operate under strong, usually untestable assumptions. Empirical,\ndata-oriented approaches are, at best, under-developed. Furthermore,\nmechanism/policy evaluation methods usually ignore the dynamic nature of a\nmulti-agent system and are thus inappropriate for estimating long-term effects.\nWe introduce the problem of estimating the causal effects of mechanisms on\nincentives and frame it under the Rubin causal framework \\citep{rubin74,\nrubin78}. This raises unique technical challenges since the outcome of interest\n(agent truthfulness) is confounded with strategic interactions and,\ninterestingly, is typically never observed under any mechanism. We develop a\nmethodology to estimate such causal effects that using a prior that is based on\na strategic equilibrium model. Working on the domain of kidney exchanges, we\nshow how to apply our methodology to estimate causal effects of kidney\nallocation mechanisms on hospitals' incentives. Our results demonstrate that\nthe use of game-theoretic prior captures the dynamic nature of the kidney\nexchange multiagent system and shrinks the estimates towards long-term effects,\nthus improving upon typical methods that completely ignore agents' strategic\nbehavior. \n\n"}
{"id": "1408.4901", "contents": "Title: A Study of Proxies for Shapley Allocations of Transport Costs Abstract: We propose and evaluate a number of solutions to the problem of calculating\nthe cost to serve each location in a single-vehicle transport setting. Such\ncost to serve analysis has application both strategically and operationally in\ntransportation. The problem is formally given by the traveling salesperson game\n(TSG), a cooperative total utility game in which agents correspond to locations\nin a traveling salesperson problem (TSP). The cost to serve a location is an\nallocated portion of the cost of an optimal tour. The Shapley value is one of\nthe most important normative division schemes in cooperative games, giving a\nprincipled and fair allocation both for the TSG and more generally. We consider\na number of direct and sampling-based procedures for calculating the Shapley\nvalue, and present the first proof that approximating the Shapley value of the\nTSG within a constant factor is NP-hard. Treating the Shapley value as an ideal\nbaseline allocation, we then develop six proxies for that value which are\nrelatively easy to compute. We perform an experimental evaluation using\nSynthetic Euclidean games as well as games derived from real-world tours\ncalculated for fast-moving consumer goods scenarios. Our experiments show that\nseveral computationally tractable allocation techniques correspond to good\nproxies for the Shapley value. \n\n"}
{"id": "1409.0080", "contents": "Title: Show Me the Money: Dynamic Recommendations for Revenue Maximization Abstract: Recommender Systems (RS) play a vital role in applications such as e-commerce\nand on-demand content streaming. Research on RS has mainly focused on the\ncustomer perspective, i.e., accurate prediction of user preferences and\nmaximization of user utilities. As a result, most existing techniques are not\nexplicitly built for revenue maximization, the primary business goal of\nenterprises. In this work, we explore and exploit a novel connection between RS\nand the profitability of a business. As recommendations can be seen as an\ninformation channel between a business and its customers, it is interesting and\nimportant to investigate how to make strategic dynamic recommendations leading\nto maximum possible revenue. To this end, we propose a novel \\model that takes\ninto account a variety of factors including prices, valuations, saturation\neffects, and competition amongst products. Under this model, we study the\nproblem of finding revenue-maximizing recommendation strategies over a finite\ntime horizon. We show that this problem is NP-hard, but approximation\nguarantees can be obtained for a slightly relaxed version, by establishing an\nelegant connection to matroid theory. Given the prohibitively high complexity\nof the approximation algorithm, we also design intelligent heuristics for the\noriginal problem. Finally, we conduct extensive experiments on two real and\nsynthetic datasets and demonstrate the efficiency, scalability, and\neffectiveness our algorithms, and that they significantly outperform several\nintuitive baselines. \n\n"}
{"id": "1409.3225", "contents": "Title: Strategies for Utility Maximization in Social Groups with Preferential\n  Exploration Abstract: We consider a \\emph{Social Group} of networked nodes, seeking a \"universe\" of\nsegments for maximization of their utility. Each node has a subset of the\nuniverse, and access to an expensive link for downloading data. Nodes can also\nacquire the universe by exchanging copies of segments among themselves, at low\ncost, using inter-node links. While exchanges over inter-node links ensure\nminimum or negligible cost, some nodes in the group try to exploit the system.\nWe term such nodes as `non-reciprocating nodes' and prohibit such behavior by\nproposing the \"Give-and-Take\" criterion, where exchange is allowed iff each\nparticipating node has segments unavailable with the other. Following this\ncriterion for inter-node links, each node wants to maximize its utility, which\ndepends on the node's segment set available with the node. Link activation\namong nodes requires mutual consent of participating nodes. Each node tries to\nfind a pairing partner by preferentially exploring nodes for link formation and\nunpaired nodes choose to download a segment using the expensive link with\nsegment aggressive probability. We present various linear complexity\ndecentralized algorithms based on \\emph{Stable Roommates Problem} that can be\nused by nodes (as per their behavioral nature) for choosing the best strategy\nbased on available information. Then, we present decentralized randomized\nalgorithm that performs close to optimal for large number of nodes. We define\n\\emph{Price of Choices} for benchmarking performance for social groups\n(consisting of non-aggressive nodes only). We evaluate performances of various\nalgorithms and characterize the behavioral regime that will yield best results\nfor node and social group, spending the minimal on expensive link. We consider\nsocial group consisting of non-aggressive nodes and benchmark performances of\nproposed algorithms with the optimal. \n\n"}
{"id": "1409.3741", "contents": "Title: Computing Approximate Nash Equilibria in Polymatrix Games Abstract: In an $\\epsilon$-Nash equilibrium, a player can gain at most $\\epsilon$ by\nunilaterally changing his behaviour. For two-player (bimatrix) games with\npayoffs in $[0,1]$, the best-known$\\epsilon$ achievable in polynomial time is\n0.3393. In general, for $n$-player games an $\\epsilon$-Nash equilibrium can be\ncomputed in polynomial time for an $\\epsilon$ that is an increasing function of\n$n$ but does not depend on the number of strategies of the players. For\nthree-player and four-player games the corresponding values of $\\epsilon$ are\n0.6022 and 0.7153, respectively. Polymatrix games are a restriction of general\n$n$-player games where a player's payoff is the sum of payoffs from a number of\nbimatrix games. There exists a very small but constant $\\epsilon$ such that\ncomputing an $\\epsilon$-Nash equilibrium of a polymatrix game is \\PPAD-hard.\nOur main result is that a $(0.5+\\delta)$-Nash equilibrium of an $n$-player\npolymatrix game can be computed in time polynomial in the input size and\n$\\frac{1}{\\delta}$. Inspired by the algorithm of Tsaknakis and Spirakis, our\nalgorithm uses gradient descent on the maximum regret of the players. We also\nshow that this algorithm can be applied to efficiently find a\n$(0.5+\\delta)$-Nash equilibrium in a two-player Bayesian game. \n\n"}
{"id": "1409.4503", "contents": "Title: Audit Games with Multiple Defender Resources Abstract: Modern organizations (e.g., hospitals, social networks, government agencies)\nrely heavily on audit to detect and punish insiders who inappropriately access\nand disclose confidential information. Recent work on audit games models the\nstrategic interaction between an auditor with a single audit resource and\nauditees as a Stackelberg game, augmenting associated well-studied security\ngames with a configurable punishment parameter. We significantly generalize\nthis audit game model to account for multiple audit resources where each\nresource is restricted to audit a subset of all potential violations, thus\nenabling application to practical auditing scenarios. We provide an FPTAS that\ncomputes an approximately optimal solution to the resulting non-convex\noptimization problem. The main technical novelty is in the design and\ncorrectness proof of an optimization transformation that enables the\nconstruction of this FPTAS. In addition, we experimentally demonstrate that\nthis transformation significantly speeds up computation of solutions for a\nclass of audit games and security games. \n\n"}
{"id": "1409.6690", "contents": "Title: The Value 1 Problem Under Finite-memory Strategies for Concurrent\n  Mean-payoff Games Abstract: We consider concurrent mean-payoff games, a very well-studied class of\ntwo-player (player 1 vs player 2) zero-sum games on finite-state graphs where\nevery transition is assigned a reward between 0 and 1, and the payoff function\nis the long-run average of the rewards. The value is the maximal expected\npayoff that player 1 can guarantee against all strategies of player 2. We\nconsider the computation of the set of states with value 1 under finite-memory\nstrategies for player 1, and our main results for the problem are as follows:\n(1) we present a polynomial-time algorithm; (2) we show that whenever there is\na finite-memory strategy, there is a stationary strategy that does not need\nmemory at all; and (3) we present an optimal bound (which is double\nexponential) on the patience of stationary strategies (where patience of a\ndistribution is the inverse of the smallest positive probability and represents\na complexity measure of a stationary strategy). \n\n"}
{"id": "1409.7411", "contents": "Title: A Higher-order Framework for Decision Problems and Games Abstract: We introduce a new unified framework for modelling both decision problems and\nfinite games based on quantifiers and selection functions. We show that the\ncanonical utility maximisation is one special case of a quantifier and that our\nmore abstract framework provides several additional degrees of freedom in\nmodelling. In particular, incomplete preferences, non-maximising heuristics,\nand context-dependent motives can be taken into account when describing an\nagent's goal. We introduce a suitable generalisation of Nash equilibrium for\ngames in terms of quantifiers and selection functions. Moreover, we introduce a\nrefinement of Nash that captures context-dependency of goals. Modelling in our\nframework is compositional as the parts of the game are modular and can be\neasily exchanged. We provide an extended example where we illustrate concepts\nand highlight the benefits of our alternative modelling approach. \n\n"}
{"id": "1409.7551", "contents": "Title: Algorithms for Stochastic Games on Interference Channels Abstract: We consider a wireless channel shared by multiple transmitter-receiver pairs.\nTheir transmissions interfere with each other. Each transmitter-receiver pair\naims to maximize its long-term average transmission rate subject to an average\npower constraint. This scenario is modeled as a stochastic game. We provide\nsufficient conditions for existence and uniqueness of a Nash equilibrium (NE).\nWe then formulate the problem of finding NE as a variational inequality (VI)\nproblem and present an algorithm to solve the VI using regularization. We also\nprovide distributed algorithms to compute Pareto optimal solutions for the\nproposed game. \n\n"}
{"id": "1410.0413", "contents": "Title: Risk Dynamics in Trade Networks Abstract: We introduce a new framework to model interactions among agents which seek to\ntrade to minimize their risk with respect to some future outcome. We quantify\nthis risk using the concept of risk measures from finance, and introduce a\nclass of trade dynamics which allow agents to trade contracts contingent upon\nthe future outcome. We then show that these trade dynamics exactly correspond\nto a variant of randomized coordinate descent. By extending the analysis of\nthese coordinate descent methods to account for our more organic setting, we\nare able to show convergence rates for very general trade dynamics, showing\nthat the market or network converges to a unique steady state. Applying these\nresults to prediction markets, we expand on recent results by adding\nconvergence rates and general aggregation properties. Finally, we illustrate\nthe generality of our framework by applying it to agent interactions on a\nscale-free network. \n\n"}
{"id": "1410.2652", "contents": "Title: More Natural Models of Electoral Control by Partition Abstract: \"Control\" studies attempts to set the outcome of elections through the\naddition, deletion, or partition of voters or candidates. The set of benchmark\ncontrol types was largely set in the seminal 1992 paper by Bartholdi, Tovey,\nand Trick that introduced control, and there now is a large literature studying\nhow many of the benchmark types various election systems are vulnerable to,\ni.e., have polynomial-time attack algorithms for.\n  However, although the longstanding benchmark models of addition and deletion\nmodel relatively well the real-world settings that inspire them, the\nlongstanding benchmark models of partition model settings that are arguably\nquite distant from those they seek to capture.\n  In this paper, we introduce--and for some important cases analyze the\ncomplexity of--new partition models that seek to better capture many real-world\npartition settings. In particular, in many partition settings one wants the two\nparts of the partition to be of (almost) equal size, or is partitioning into\nmore than two parts, or has groups of actors who must be placed in the same\npart of the partition. Our hope is that having these new partition types will\nallow studies of control attacks to include such models that more realistically\ncapture many settings. \n\n"}
{"id": "1410.3048", "contents": "Title: GSP with General Independent Click-Through-Rates Abstract: The popular generalized second price (GSP) auction for sponsored search is\nbuilt upon a separable model of click-through-rates that decomposes the\nlikelihood of a click into the product of a \"slot effect\" and an \"advertiser\neffect\" --- if the first slot is twice as good as the second for some bidder,\nthen it is twice as good for everyone. Though appealing in its simplicity, this\nmodel is quite suspect in practice. A wide variety of factors including\nexternalities and budgets have been studied that can and do cause it to be\nviolated. In this paper we adopt a view of GSP as an iterated second price\nauction (see, e.g., Milgrom 2010) and study how the most basic violation of\nseparability --- position dependent, arbitrary public click-through-rates that\ndo not decompose --- affects results from the foundational analysis of GSP\n(Varian 2007, Edelman et al. 2007). For the two-slot setting we prove that for\narbitrary click-through-rates, for arbitrary bidder values, an efficient\npure-strategy equilibrium always exists; however, without separability there\nalways exist values such that the VCG outcome and payments cannot be realized\nby any bids, in equilibrium or otherwise. The separability assumption is\ntherefore necessary in the two-slot case to match the payments of VCG but not\nfor efficiency. We moreover show that without separability, generic existence\nof efficient equilibria is sensitive to the choice of tie-breaking rule, and\nwhen there are more than two slots, no (bid-independent) tie-breaking rule\nyields the positive result. In light of this we suggest alternative mechanisms\nthat trade the simplicity of GSP for better equilibrium properties when there\nare three or more slots. \n\n"}
{"id": "1410.3688", "contents": "Title: A Game Theoretic Model for Network Virus Protection Abstract: The network virus propagation is influenced by various factors, and some of\nthem are neglected in most of the existed models in the literature. In this\npaper, we study the network virus propagation based on the the epidemiological\nviewpoint. We assume that nodes can be equipped with protection against virus\nand the security of a node depends not only on his protection strategy but also\nby those chosen by other nodes in the network. A crucial aspect is whether\nowners of device, e.g., either smartphones, machines or tablets, are willing to\nbe equipped to protect themselves or to take the risk to be contaminated in\norder to avoid the payment for a new antivirus. We model the interaction\nbetween nodes as a non-cooperative games where the node has two strategies:\neither to update the antivirus or not. To this aim, we provide a full\ncharacterization of the equilibria of the game and we investigate the impact of\nthe price of protection on the equilibrium as well as the efficiency of the\nprotection at equilibrium. Further we consider more realistic scenarios in\nwhich the dynamic of sources that disseminate the virus, evolves as function of\nthe popularity of virus. In this work, the interest in the virus by sources\nevolves under the Influence Linear Threshold (HILT) model. \n\n"}
{"id": "1410.5186", "contents": "Title: On the Hardness of Bribery Variants in Voting with CP-Nets Abstract: We continue previous work by Mattei et al. (Mattei, N., Pini, M., Rossi, F.,\nVenable, K.: Bribery in voting with CP-nets. Ann. of Math. and Artif. Intell.\npp. 1--26 (2013)) in which they study the computational complexity of bribery\nschemes when voters have conditional preferences that are modeled by CP-nets.\nFor most of the cases they considered, they could show that the bribery problem\nis solvable in polynomial time. Some cases remained open---we solve two of them\nand extend the previous results to the case that voters are weighted. Moreover,\nwe consider negative (weighted) bribery in CP-nets, when the briber is not\nallowed to pay voters to vote for his preferred candidate. \n\n"}
{"id": "1410.7472", "contents": "Title: A note on two notions of compliance Abstract: We establish a relation between two models of contracts: binary session\ntypes, and a model based on event structures and game-theoretic notions. In\nparticular, we show that compliance in session types corresponds to the\nexistence of certain winning strategies in game-based contracts. \n\n"}
{"id": "1411.0728", "contents": "Title: Approachability in Stackelberg Stochastic Games with Vector Costs Abstract: The notion of approachability was introduced by Blackwell [1] in the context\nof vector-valued repeated games. The famous Blackwell's approachability theorem\nprescribes a strategy for approachability, i.e., for `steering' the average\ncost of a given agent towards a given target set, irrespective of the\nstrategies of the other agents. In this paper, motivated by the multi-objective\noptimization/decision making problems in dynamically changing environments, we\naddress the approachability problem in Stackelberg stochastic games with vector\nvalued cost functions. We make two main contributions. Firstly, we give a\nsimple and computationally tractable strategy for approachability for\nStackelberg stochastic games along the lines of Blackwell's. Secondly, we give\na reinforcement learning algorithm for learning the approachable strategy when\nthe transition kernel is unknown. We also recover as a by-product Blackwell's\nnecessary and sufficient condition for approachability for convex sets in this\nset up and thus a complete characterization. We also give sufficient conditions\nfor non-convex sets. \n\n"}
{"id": "1411.0835", "contents": "Title: Variations on the Stochastic Shortest Path Problem Abstract: In this invited contribution, we revisit the stochastic shortest path\nproblem, and show how recent results allow one to improve over the classical\nsolutions: we present algorithms to synthesize strategies with multiple\nguarantees on the distribution of the length of paths reaching a given target,\nrather than simply minimizing its expected value. The concepts and algorithms\nthat we propose here are applications of more general results that have been\nobtained recently for Markov decision processes and that are described in a\nseries of recent papers. \n\n"}
{"id": "1411.0944", "contents": "Title: The cost of getting local monotonicity Abstract: Manfred Holler introduced the Public Good index as a proposal to divide a\npublic good among players. In its unnormalized version, i.e., the raw measure,\nit counts the number of times that a player belongs to a minimal winning\ncoalition. Unlike the Banzhaf index, it does not count the remaining winning\ncoalitions in which the player is crucial. Holler noticed that his index does\nnot satisfy local monotonicity, a fact that can be seen either as a major\ndrawback or as an advantage. \n\n"}
{"id": "1411.0998", "contents": "Title: Jointly Private Convex Programming Abstract: In this paper we present an extremely general method for approximately\nsolving a large family of convex programs where the solution can be divided\nbetween different agents, subject to joint differential privacy. This class\nincludes multi-commodity flow problems, general allocation problems, and\nmulti-dimensional knapsack problems, among other examples. The accuracy of our\nalgorithm depends on the \\emph{number} of constraints that bind between\nindividuals, but crucially, is \\emph{nearly independent} of the number of\nprimal variables and hence the number of agents who make up the problem. As the\nnumber of agents in a problem grows, the error we introduce often becomes\nnegligible.\n  We also consider the setting where agents are strategic and have preferences\nover their part of the solution. For any convex program in this class that\nmaximizes \\emph{social welfare}, there is a generic reduction that makes the\ncorresponding optimization \\emph{approximately dominant strategy truthful} by\ncharging agents prices for resources as a function of the approximately optimal\ndual variables, which are themselves computed under differential privacy. Our\nresults substantially expand the class of problems that are known to be\nsolvable under both privacy and incentive constraints. \n\n"}
{"id": "1411.2079", "contents": "Title: Competitive analysis via benchmark decomposition Abstract: We propose a uniform approach for the design and analysis of prior-free\ncompetitive auctions and online auctions. Our philosophy is to view the\nbenchmark function as a variable parameter of the model and study a broad class\nof functions instead of a individual target benchmark. We consider a multitude\nof well-studied auction settings, and improve upon a few previous results.\n  (1) Multi-unit auctions. Given a $\\beta$-competitive unlimited supply\nauction, the best previously known multi-unit auction is $2\\beta$-competitive.\nWe design a $(1+\\beta)$-competitive auction reducing the ratio from $4.84$ to\n$3.24$. These results carry over to matroid and position auctions.\n  (2) General downward-closed environments. We design a $6.5$-competitive\nauction improving upon the ratio of $7.5$. Our auction is noticeably simpler\nthan the previous best one.\n  (3) Unlimited supply online auctions. Our analysis yields an auction with a\ncompetitive ratio of $4.12$, which significantly narrows the margin of\n$[4,4.84]$ previously known for this problem.\n  A particularly important tool in our analysis is a simple decomposition\nlemma, which allows us to bound the competitive ratio against a sum of\nbenchmark functions. We use this lemma in a \"divide and conquer\" fashion by\ndividing the target benchmark into the sum of simpler functions. \n\n"}
{"id": "1411.2139", "contents": "Title: Incentive Design in Peer Review: Rating and Repeated Endogenous Matching Abstract: Peer review (e.g., grading assignments in Massive Open Online Courses\n(MOOCs), academic paper review) is an effective and scalable method to evaluate\nthe products (e.g., assignments, papers) of a large number of agents when the\nnumber of dedicated reviewing experts (e.g., teaching assistants, editors) is\nlimited. Peer review poses two key challenges: 1) identifying the reviewers'\nintrinsic capabilities (i.e., adverse selection) and 2) incentivizing the\nreviewers to exert high effort (i.e., moral hazard). Some works in mechanism\ndesign address pure adverse selection using one-shot matching rules, and pure\nmoral hazard was addressed in repeated games with exogenously given and fixed\nmatching rules. However, in peer review systems exhibiting both adverse\nselection and moral hazard, one-shot or exogenous matching rules do not link\nagents' current behavior with future matches and future payoffs, and as we\nprove, will induce myopic behavior (i.e., exerting the lowest effort) resulting\nin the lowest review quality.\n  In this paper, we propose for the first time a solution that simultaneously\nsolves adverse selection and moral hazard. Our solution exploits the repeated\ninteractions of agents, utilizes ratings to summarize agents' past review\nquality, and designs matching rules that endogenously depend on agents'\nratings. Our proposed matching rules are easy to implement and require no\nknowledge about agents' private information (e.g., their benefit and cost\nfunctions). Yet, they are effective in guiding the system to an equilibrium\nwhere the agents are incentivized to exert high effort and receive ratings that\nprecisely reflect their review quality. Using several illustrative examples, we\nquantify the significant performance gains obtained by our proposed mechanism\nas compared to existing one-shot or exogenous matching rules. \n\n"}
{"id": "1411.3164", "contents": "Title: A linear time algorithm for the orbit problem over cyclic groups Abstract: The orbit problem is at the heart of symmetry reduction methods for model\nchecking concurrent systems. It asks whether two given configurations in a\nconcurrent system (represented as finite strings over some finite alphabet) are\nin the same orbit with respect to a given finite permutation group (represented\nby their generators) acting on this set of configurations by permuting indices.\nIt is known that the problem is in general as hard as the graph isomorphism\nproblem, whose precise complexity (whether it is solvable in polynomial-time)\nis a long-standing open problem. In this paper, we consider the restriction of\nthe orbit problem when the permutation group is cyclic (i.e. generated by a\nsingle permutation), an important restriction of the problem. It is known that\nthis subproblem is solvable in polynomial-time. Our main result is a\nlinear-time algorithm for this subproblem. \n\n"}
{"id": "1411.3320", "contents": "Title: On Sparse Discretization for Graphical Games Abstract: This short paper concerns discretization schemes for representing and\ncomputing approximate Nash equilibria, with emphasis on graphical games, but\nbriefly touching on normal-form and poly-matrix games. The main technical\ncontribution is a representation theorem that informally states that to account\nfor every exact Nash equilibrium using a nearby approximate Nash equilibrium on\na grid over mixed strategies, a uniform discretization size linear on the\ninverse of the approximation quality and natural game-representation parameters\nsuffices. For graphical games, under natural conditions, the discretization is\nlogarithmic in the game-representation size, a substantial improvement over the\nlinear dependency previously required. The paper has five other objectives: (1)\ngiven the venue, to highlight the important, but often ignored, role that work\non constraint networks in AI has in simplifying the derivation and analysis of\nalgorithms for computing approximate Nash equilibria; (2) to summarize the\nstate-of-the-art on computing approximate Nash equilibria, with emphasis on\nrelevance to graphical games; (3) to help clarify the distinction between\nsparse-discretization and sparse-support techniques; (4) to illustrate and\nadvocate for the deliberate mathematical simplicity of the formal proof of the\nrepresentation theorem; and (5) to list and discuss important open problems,\nemphasizing graphical-game generalizations, which the AI community is most\nsuitable to solve. \n\n"}
{"id": "1412.0041", "contents": "Title: FairCache: Introducing Fairness to ICN Caching - Technical Report Abstract: Information-centric networking extensively uses universal in-network caching.\nHowever, developing an efficient and fair collaborative caching algorithm for\nselfish caches is still an open question. In addition, the communication\noverhead induced by collaboration is especially poorly understood in a general\nnetwork setting such as realistic ISP and Autonomous System networks. In this\npaper, we address these two problems by modeling the in-network caching problem\nas a Nash bargaining game. We show that the game is a convex optimization\nproblem and further derive the corresponding distributed algorithm. We\nanalytically investigate the collaboration overhead on general graph\ntopologies, and theoretically show that collaboration has to be constrained\nwithin a small neighborhood due to its cost growing exponentially. Our proposed\nalgorithm achieves at least 16% performance gain over its competitors on\ndifferent network topologies in the evaluation, and guarantees provable\nconvergence, Pareto efficiency and proportional fairness. \n\n"}
{"id": "1412.0969", "contents": "Title: Settling Some Open Problems on 2-Player Symmetric Nash Equilibria Abstract: Over the years, researchers have studied the complexity of several decision\nversions of Nash equilibrium in (symmetric) two-player games (bimatrix games).\nTo the best of our knowledge, the last remaining open problem of this sort is\nthe following; it was stated by Papadimitriou in 2007: find a non-symmetric\nNash equilibrium (NE) in a symmetric game. We show that this problem is\nNP-complete and the problem of counting the number of non-symmetric NE in a\nsymmetric game is #P-complete.\n  In 2005, Kannan and Theobald defined the \"rank of a bimatrix game\"\nrepresented by matrices (A, B) to be rank(A+B) and asked whether a NE can be\ncomputed in rank 1 games in polynomial time. Observe that the rank 0 case is\nprecisely the zero sum case, for which a polynomial time algorithm follows from\nvon Neumann's reduction of such games to linear programming. In 2011, Adsul et.\nal. obtained an algorithm for rank 1 games; however, it does not solve the case\nof symmetric rank 1 games. We resolve this problem. \n\n"}
{"id": "1412.3187", "contents": "Title: Revenue Maximization for Selling Multiple Correlated Items Abstract: We study the problem of selling $n$ items to a single buyer with an additive\nvaluation function. We consider the valuation of the items to be correlated,\ni.e., desirabilities of the buyer for the items are not drawn independently.\nIdeally, the goal is to design a mechanism to maximize the revenue. However, it\nhas been shown that a revenue optimal mechanism might be very complicated and\nas a result inapplicable to real-world auctions. Therefore, our focus is on\ndesigning a simple mechanism that achieves a constant fraction of the optimal\nrevenue. Babaioff et al. propose a simple mechanism that achieves a constant\nfraction of the optimal revenue for independent setting with a single additive\nbuyer. However, they leave the following problem as an open question: \"Is there\na simple, approximately optimal mechanism for a single additive buyer whose\nvalue for $n$ items is sampled from a common base-value distribution?\"\n  Babaioff et al. show a constant approximation factor of the optimal revenue\ncan be achieved by either selling the items separately or as a whole bundle in\nthe independent setting. We show a similar result for the correlated setting\nwhen the desirabilities of the buyer are drawn from a common base-value\ndistribution. It is worth mentioning that the core decomposition lemma which is\nmainly the heart of the proofs for efficiency of the mechanisms does not hold\nfor correlated settings. Therefore we propose a modified version of this lemma\nwhich is applicable to the correlated settings as well. Although we apply this\ntechnique to show the proposed mechanism can guarantee a constant fraction of\nthe optimal revenue in a very weak correlation, this method alone can not\ndirectly show the efficiency of the mechanism in stronger correlations. \n\n"}
{"id": "1412.3334", "contents": "Title: Computational Complexity of Competitive Diffusion on (Un)weighted Graphs Abstract: Consider an undirected graph modeling a social network, where the vertices\nrepresent users, and the edges do connections among them. In the competitive\ndiffusion game, each of a number of players chooses a vertex as a seed to\npropagate his/her opinion, and then it spreads along the edges in the graphs.\nThe objective of every player is to maximize the number of vertices the opinion\ninfects. In this paper, we investigate a computational problem of asking\nwhether a pure Nash equilibrium exists in the competitive diffusion game on\nunweighed and weighted graphs, and present several negative and positive\nresults. We first prove that the problem is W[1]-hard when parameterized by the\nnumber of players even for unweighted graphs. We also show that the problem is\nNP-hard even for series-parallel graphs with positive integer weights, and is\nNP-hard even for forests with arbitrary integer weights. Furthermore, we show\nthat the problem for forest of paths with arbitrary weights is solvable in\npseudo-polynomial time; and it is solvable in quadratic time if a given graph\nis unweighted. We also prove that the problem for chain, cochain, and threshold\ngraphs with arbitrary integer weights is solvable in polynomial time. \n\n"}
{"id": "1412.3414", "contents": "Title: Strategyproof Mechanisms for One-Dimensional Hybrid and Obnoxious\n  Facility Location Abstract: We consider a strategic variant of the facility location problem. We would\nlike to locate a facility on a closed interval. There are n agents located on\nthat interval, divided into two types: type 1 agents, who wish for the facility\nto be as far from them as possible, and type 2 agents, who wish for the\nfacility to be as close to them as possible. Our goal is to maximize a form of\naggregated social benefit: maxisum- the sum of the agents' utilities, or the\negalitarian objective- the minimal agent utility. The strategic aspect of the\nproblem is that the agents' locations are not known to us, but rather reported\nto us by the agents- an agent might misreport his location in an attempt to\nmove the facility away from or towards to his true location. We therefore\nrequire the facility-locating mechanism to be strategyproof, namely that\nreporting truthfully is a dominant strategy for each agent. As simply\nmaximizing the social benefit is generally not strategyproof, our goal is to\ndesign strategyproof mechanisms with good approximation ratios.\n  For the maxisum objective, in the deterministic setting, we provide a\nbest-possible 3- approximate strategyproof mechanism; in the randomized\nsetting, we provide a 23/13- approximate strategyproof mechanism and a lower\nbound of \\frac{2}{\\sqrt{3}}. For the egalitarian objective, we provide a lower\nbound of 3/2 in the randomized setting, and show that no bounded approximation\nratio is attainable in the deterministic setting. To obtain our deterministic\nlower bounds, we characterize all deterministic strategyproof mechanisms when\nall agents are of type 1. Finally, we consider a generalized model that allows\nan agent to control more than one location, and provide best-possible 3- and\n3/2- approximate strategyproof mechanisms for maxisum, in the deterministic and\nrandomized settings respectively, when only type 1 agents are present. \n\n"}
{"id": "1412.3570", "contents": "Title: Bounded-degree factors of lacunary multivariate polynomials Abstract: In this paper, we present a new method for computing bounded-degree factors\nof lacunary multivariate polynomials. In particular for polynomials over number\nfields, we give a new algorithm that takes as input a multivariate polynomial f\nin lacunary representation and a degree bound d and computes the irreducible\nfactors of degree at most d of f in time polynomial in the lacunary size of f\nand in d. Our algorithm, which is valid for any field of zero characteristic,\nis based on a new gap theorem that enables reducing the problem to several\ninstances of (a) the univariate case and (b) low-degree multivariate\nfactorization.\n  The reduction algorithms we propose are elementary in that they only\nmanipulate the exponent vectors of the input polynomial. The proof of\ncorrectness and the complexity bounds rely on the Newton polytope of the\npolynomial, where the underlying valued field consists of Puiseux series in a\nsingle variable. \n\n"}
{"id": "1412.3701", "contents": "Title: How Much Lookahead is Needed to Win Infinite Games? Abstract: Delay games are two-player games of infinite duration in which one player may\ndelay her moves to obtain a lookahead on her opponent's moves. For\n$\\omega$-regular winning conditions it is known that such games can be solved\nin doubly-exponential time and that doubly-exponential lookahead is sufficient.\n  We improve upon both results by giving an exponential time algorithm and an\nexponential upper bound on the necessary lookahead. This is complemented by\nshowing EXPTIME-hardness of the solution problem and tight exponential lower\nbounds on the lookahead. Both lower bounds already hold for safety conditions.\nFurthermore, solving delay games with reachability conditions is shown to be\nPSPACE-complete.\n  This is a corrected version of the paper https://arxiv.org/abs/1412.3701v4\npublished originally on August 26, 2016. \n\n"}
{"id": "1412.3978", "contents": "Title: Delay Games with WMSO+U Winning Conditions Abstract: Delay games are two-player games of infinite duration in which one player may\ndelay her moves to obtain a lookahead on her opponent's moves. We consider\ndelay games with winning conditions expressed in weak monadic second order\nlogic with the unbounding quantifier, which is able to express (un)boundedness\nproperties. We show that it is decidable whether the delaying player has a\nwinning strategy using bounded lookahead and give a doubly-exponential upper\nbound on the necessary lookahead. In contrast, we show that bounded lookahead\nis not always sufficient to win such a game. \n\n"}
{"id": "1412.6078", "contents": "Title: A Note on the Assignment Problem with Uniform Preferences Abstract: Motivated by a problem of scheduling unit-length jobs with weak preferences\nover time-slots, the random assignment problem (also called the house\nallocation problem) is considered on a uniform preference domain. For the\nsubdomain in which preferences are strict except possibly for the class of\nunacceptable objects, Bogomolnaia and Moulin characterized the probabilistic\nserial mechanism as the only mechanism satisfying equal treatment of equals,\nstrategyproofness, and ordinal efficiency. The main result in this paper is\nthat the natural extension of the probabilistic serial mechanism to the domain\nof weak, but uniform, preferences fails strategyproofness, but so does every\nother mechanism that is ordinally efficient and treats equals equally. If\nenvy-free assignments are required, then any (probabilistic or deterministic)\nmechanism that guarantees an ex post efficient outcome must fail even a weak\nform of strategyproofness. \n\n"}
{"id": "1412.6265", "contents": "Title: Inapproximability of Truthful Mechanisms via Generalizations of the VC\n  Dimension Abstract: Algorithmic mechanism design (AMD) studies the delicate interplay between\ncomputational efficiency, truthfulness, and optimality. We focus on AMD's\nparadigmatic problem: combinatorial auctions. We present a new generalization\nof the VC dimension to multivalued collections of functions, which encompasses\nthe classical VC dimension, Natarajan dimension, and Steele dimension. We\npresent a corresponding generalization of the Sauer-Shelah Lemma and harness\nthis VC machinery to establish inapproximability results for deterministic\ntruthful mechanisms. Our results essentially unify all inapproximability\nresults for deterministic truthful mechanisms for combinatorial auctions to\ndate and establish new separation gaps between truthful and non-truthful\nalgorithms. \n\n"}
{"id": "1501.00882", "contents": "Title: Observing Each Other's Observations in the Electronic Mail Game Abstract: We study a Bayesian coordination game where agents receive private\ninformation on the game's payoff structure. In addition, agents receive private\nsignals on each other's private information. We show that once agents possess\nthese different types of information, there exists a coordination game in the\nevaluation of this information. And even though the precisions of both signal\ntypes is exogenous, the precision with which agents predict each other's\nactions at equilibrium turns out to be endogenous. As a consequence, we find\nthat there exist multiple equilibria if the private signals' precision is high.\nThese equilibria differ with regard to the way that agents weight their private\ninformation to reason about each other's actions. \n\n"}
{"id": "1501.05296", "contents": "Title: Output-sensitive algorithms for sumset and sparse polynomial\n  multiplication Abstract: We present randomized algorithms to compute the sumset (Minkowski sum) of two\ninteger sets, and to multiply two univariate integer polynomials given by\nsparse representations. Our algorithm for sumset has cost softly linear in the\ncombined size of the inputs and output. This is used as part of our sparse\nmultiplication algorithm, whose cost is softly linear in the combined size of\nthe inputs, output, and the sumset of the supports of the inputs. As a\nsubroutine, we present a new method for computing the coefficients of a sparse\npolynomial, given a set containing its support. Our multiplication algorithm\nextends to multivariate Laurent polynomials over finite fields and rational\nnumbers. Our techniques are based on sparse interpolation algorithms and\nresults from analytic number theory. \n\n"}
{"id": "1502.01063", "contents": "Title: Quadratic Conditional Lower Bounds for String Problems and Dynamic Time\n  Warping Abstract: Classic similarity measures of strings are longest common subsequence and\nLevenshtein distance (i.e., the classic edit distance). A classic similarity\nmeasure of curves is dynamic time warping. These measures can be computed by\nsimple $O(n^2)$ dynamic programming algorithms, and despite much effort no\nalgorithms with significantly better running time are known.\n  We prove that, even restricted to binary strings or one-dimensional curves,\nrespectively, these measures do not have strongly subquadratic time algorithms,\ni.e., no algorithms with running time $O(n^{2-\\varepsilon})$ for any\n$\\varepsilon > 0$, unless the Strong Exponential Time Hypothesis fails. We\ngeneralize the result to edit distance for arbitrary fixed costs of the four\noperations (deletion in one of the two strings, matching, substitution), by\nidentifying trivial cases that can be solved in constant time, and proving\nquadratic-time hardness on binary strings for all other cost choices. This\nimproves and generalizes the known hardness result for Levenshtein distance\n[Backurs, Indyk STOC'15] by the restriction to binary strings and the\ngeneralization to arbitrary costs, and adds important problems to a recent line\nof research showing conditional lower bounds for a growing number of quadratic\ntime problems.\n  As our main technical contribution, we introduce a framework for proving\nquadratic-time hardness of similarity measures. To apply the framework it\nsuffices to construct a single gadget, which encapsulates all the expressive\npower necessary to emulate a reduction from satisfiability.\n  Finally, we prove quadratic-time hardness for longest palindromic subsequence\nand longest tandem subsequence via reductions from longest common subsequence,\nshowing that conditional lower bounds based on the Strong Exponential Time\nHypothesis also apply to string problems that are not necessarily similarity\nmeasures. \n\n"}
{"id": "1502.01157", "contents": "Title: Toward Fully Coordinated Multi-level Multi-carrier Energy Efficient\n  Networks Abstract: Enabling coordination between products from different vendors is a key\ncharacteristic of the design philosophy behind future wireless communication\nnetworks. As an example, different devices may have different implementations,\nleading to different user experiences. A similar story emerges when devices\nrunning different physical and link layer protocols share frequencies in the\nsame spectrum in order to maximize the system-wide spectral efficiency. In such\nsituations, coordinating multiple interfering devices presents a significant\nchallenge not only from an interworking perspective (as a result of reduced\ninfrastructure), but also from an implementation point of view. The following\nquestion may then naturally arise: How to accommodate integrating such\nheterogeneous wireless devices seamlessly? One approach is to coordinate the\nspectrum in a centralized manner. However, the desired autonomous feature of\nfuture wireless systems makes the use of a central authority for spectrum\nmanagement less appealing. Alternately, intelligent spectrum coordination have\nspurred great interest and excitement in the recent years. This paper presents\na multi-level (hierarchical) power control game where users jointly choose\ntheir channel control and power control selfishly in order to maximize their\nindividual energy efficiency. By hierarchical, we mean that some users'\ndecision priority is higher/lower than the others. We propose two simple and\nnearly-optimal algorithms that ensure complete spectrum coordination among\nusers. Interestingly, it turns out that the complexity of the two proposed\nalgorithms is, in the worst case, quadratic in the number of users, whereas the\ncomplexity of the optimal solution (obtained through exhaustive search) is N!. \n\n"}
{"id": "1502.02849", "contents": "Title: A numbers-on-foreheads game Abstract: Is there a joint distribution of $n$ random variables over the natural\nnumbers, such that they always form an increasing sequence and whenever you\ntake two subsets of the set of random variables of the same cardinality, their\ndistribution is almost the same? We show that the answer is yes, but that the\nrandom variables will have to take values as large as $2^{2^{\\dots\n^{2^{\\Theta\\left(\\frac{1}{\\epsilon}\\right)}}}}$, where $\\epsilon\\leq\n\\epsilon_n$ measures how different the two distributions can be, the tower\ncontains $n-2$ $2$'s and the constants in the $\\Theta$ notation are allowed to\ndepend on $n$. This result has an important consequence in game theory: It\nshows that even though you can define extensive form games that cannot be\nimplemented on players who can tell the time, you can have implementations that\napproximate the game arbitrarily well. \n\n"}
{"id": "1502.03316", "contents": "Title: The Hardness of Approximation of Euclidean k-means Abstract: The Euclidean $k$-means problem is a classical problem that has been\nextensively studied in the theoretical computer science, machine learning and\nthe computational geometry communities. In this problem, we are given a set of\n$n$ points in Euclidean space $R^d$, and the goal is to choose $k$ centers in\n$R^d$ so that the sum of squared distances of each point to its nearest center\nis minimized. The best approximation algorithms for this problem include a\npolynomial time constant factor approximation for general $k$ and a\n$(1+\\epsilon)$-approximation which runs in time $poly(n) 2^{O(k/\\epsilon)}$. At\nthe other extreme, the only known computational complexity result for this\nproblem is NP-hardness [ADHP'09]. The main difficulty in obtaining hardness\nresults stems from the Euclidean nature of the problem, and the fact that any\npoint in $R^d$ can be a potential center. This gap in understanding left open\nthe intriguing possibility that the problem might admit a PTAS for all $k,d$.\n  In this paper we provide the first hardness of approximation for the\nEuclidean $k$-means problem. Concretely, we show that there exists a constant\n$\\epsilon > 0$ such that it is NP-hard to approximate the $k$-means objective\nto within a factor of $(1+\\epsilon)$. We show this via an efficient reduction\nfrom the vertex cover problem on triangle-free graphs: given a triangle-free\ngraph, the goal is to choose the fewest number of vertices which are incident\non all the edges. Additionally, we give a proof that the current best hardness\nresults for vertex cover can be carried over to triangle-free graphs. To show\nthis we transform $G$, a known hard vertex cover instance, by taking a graph\nproduct with a suitably chosen graph $H$, and showing that the size of the\n(normalized) maximum independent set is almost exactly preserved in the product\ngraph using a spectral analysis, which might be of independent interest. \n\n"}
{"id": "1502.03337", "contents": "Title: A Mechanism for Fair Distribution of Resources without Payments Abstract: We design a mechanism for Fair and Efficient Distribution of Resources\n(FEDoR) in the presence of strategic agents. We consider a multiple-instances,\nBayesian setting, where in each round the preference of an agent over the set\nof resources is a private information. We assume that in each of r rounds n\nagents are competing for k non-identical indivisible goods, (n > k). In each\nround the strategic agents declare how much they value receiving any of the\ngoods in the specific round. The agent declaring the highest valuation receives\nthe good with the highest value, the agent with the second highest valuation\nreceives the second highest valued good, etc. Hence we assume a decision\nfunction that assigns goods to agents based on their valuations. The novelty of\nthe mechanism is that no payment scheme is required to achieve truthfulness in\na setting with rational/strategic agents. The FEDoR mechanism takes advantage\nof the repeated nature of the framework, and through a statistical test is able\nto punish the misreporting agents and be fair, truthful, and socially\nefficient. FEDoR is fair in the sense that, in expectation over the course of\nthe rounds, all agents will receive the same good the same amount of times.\nFEDoR is an eligible candidate for applications that require fair distribution\nof resources over time. For example, equal share of bandwidth for nodes through\nthe same point of access. But further on, FEDoR can be applied in less trivial\nsettings like sponsored search, where payment is necessary and can be given in\nthe form of a flat participation fee. To this extent we perform a comparison\nwith traditional mechanisms applied to sponsored search, presenting the\nadvantage of FEDoR. \n\n"}
{"id": "1502.04676", "contents": "Title: Optimal Scanning Bandwidth Strategy Incorporating Uncertainty about\n  Adversary's Characteristics Abstract: In this paper we investigate the problem of designing a spectrum scanning\nstrategy to detect an intelligent Invader who wants to utilize spectrum\nundetected for his/her unapproved purposes. To deal with this problem we model\nthe situation as two games, between a Scanner and an Invader, and solve them\nsequentially. The first game is formulated to design the optimal (in maxmin\nsense) scanning algorithm, while the second one allows one to find the optimal\nvalues of the parameters for the algorithm depending on parameters of the\nnetwork. These games provide solutions for two dilemmas that the rivals face.\nThe Invader's dilemma consists of the following: the more bandwidth the Invader\nattempts to use leads to a larger payoff if he is not detected, but at the same\ntime also increases the probability of being detected and thus fined.\nSimilarly, the Scanner faces a dilemma: the wider the bandwidth scanned, the\nhigher the probability of detecting the Invader, but at the expense of\nincreasing the cost of building the scanning system. The equilibrium strategies\nare found explicitly and reveal interesting properties. In particular, we have\nfound a discontinuous dependence of the equilibrium strategies on the network\nparameters, fine and the type of the Invader's award. This discontinuity of the\nfine means that the network provider has to take into account a human/social\nfactor since some threshold values of fine could be very sensible for the\nInvader, while in other situations simply increasing the fine has minimal\ndeterrence impact. Also we show how incomplete information about the Invader's\ntechnical characteristics and reward (e.g. motivated by using different type of\napplication, say, video-streaming or downloading files) can be incorporated\ninto scanning strategy to increase its efficiency. \n\n"}
{"id": "1502.05056", "contents": "Title: On Sex, Evolution, and the Multiplicative Weights Update Algorithm Abstract: We consider a recent innovative theory by Chastain et al. on the role of sex\nin evolution [PNAS'14]. In short, the theory suggests that the evolutionary\nprocess of gene recombination implements the celebrated multiplicative weights\nupdates algorithm (MWUA). They prove that the population dynamics induced by\nsexual reproduction can be precisely modeled by genes that use MWUA as their\nlearning strategy in a particular coordination game. The result holds in the\nenvironments of \\emph{weak selection}, under the assumption that the population\nfrequencies remain a product distribution.\n  We revisit the theory, eliminating both the requirement of weak selection and\nany assumption on the distribution of the population. Removing the assumption\nof product distributions is crucial, since as we show, this assumption is\ninconsistent with the population dynamics. We show that the marginal allele\ndistributions induced by the population dynamics precisely match the marginals\ninduced by a multiplicative weights update algorithm in this general setting,\nthereby affirming and substantially generalizing these earlier results.\n  We further revise the implications for convergence and utility or fitness\nguarantees in coordination games. In contrast to the claim of Chastain et\nal.[PNAS'14], we conclude that the sexual evolutionary dynamics does not entail\nany property of the population distribution, beyond those already implied by\nconvergence. \n\n"}
{"id": "1502.05675", "contents": "Title: NP-Hardness and Inapproximability of Sparse PCA Abstract: We give a reduction from {\\sc clique} to establish that sparse PCA is\nNP-hard. The reduction has a gap which we use to exclude an FPTAS for sparse\nPCA (unless P=NP). Under weaker complexity assumptions, we also exclude\npolynomial constant-factor approximation algorithms. \n\n"}
{"id": "1502.07414", "contents": "Title: Interdependent Security with Strategic Agents and Cascades of Infection Abstract: We investigate cascades in networks consisting of strategic agents with\ninterdependent security. We assume that the strategic agents have choices\nbetween i) investing in protecting themselves, ii) purchasing insurance to\ntransfer (some) risks, and iii) taking no actions. Using a population game\nmodel, we study how various system parameters, such as node degrees, infection\npropagation rate, and the probability with which infected nodes transmit\ninfection to neighbors, affect nodes' choices at Nash equilibria and the\nresultant price of anarchy/stability. In addition, we examine how the\nprobability that a single infected node can spread the infection to a\nsignificant portion of the entire network, called cascade probability, behaves\nwith respect to system parameters. In particular, we demonstrate that, at least\nfor some parameter regimes, the cascade probability increases with the average\ndegree of nodes. \n\n"}
{"id": "1502.07687", "contents": "Title: Incentive Mechanisms for Participatory Sensing: Survey and Research\n  Challenges Abstract: Participatory sensing is a powerful paradigm which takes advantage of\nsmartphones to collect and analyze data beyond the scale of what was previously\npossible. Given that participatory sensing systems rely completely on the\nusers' willingness to submit up-to-date and accurate information, it is\nparamount to effectively incentivize users' active and reliable participation.\nIn this paper, we survey existing literature on incentive mechanisms for\nparticipatory sensing systems. In particular, we present a taxonomy of existing\nincentive mechanisms for participatory sensing systems, which are subsequently\ndiscussed in depth by comparing and contrasting different approaches. Finally,\nwe discuss an agenda of open research challenges in incentivizing users in\nparticipatory sensing. \n\n"}
{"id": "1503.00260", "contents": "Title: Parameter Compilation Abstract: In resolving instances of a computational problem, if multiple instances of\ninterest share a feature in common, it may be fruitful to compile this feature\ninto a format that allows for more efficient resolution, even if the\ncompilation is relatively expensive. In this article, we introduce a formal\nframework for classifying problems according to their compilability. The basic\nobject in our framework is that of a parameterized problem, which here is a\nlanguage along with a parameterization---a map which provides, for each\ninstance, a so-called parameter on which compilation may be performed. Our\nframework is positioned within the paradigm of parameterized complexity, and\nour notions are relatable to established concepts in the theory of\nparameterized complexity. Indeed, we view our framework as playing a unifying\nrole, integrating together parameterized complexity and compilability theory. \n\n"}
{"id": "1503.01425", "contents": "Title: Combinatorial Auction-Based Pricing for Multi-tenant Autonomous Vehicle\n  Public Transportation System Abstract: A smart city provides its people with high standard of living through\nadvanced technologies and transport is one of the major foci. With the advent\nof autonomous vehicles (AVs), an AV-based public transportation system has been\nproposed recently, which is capable of providing new forms of transportation\nservices with high efficiency, high flexibility, and low cost. For the benefit\nof passengers, multitenancy can increase market competition leading to lower\nservice charge and higher quality of service. In this paper, we study the\npricing issue of the multi-tenant AV public transportation system and three\ntypes of services are defined. The pricing process for each service type is\nmodeled as a combinatorial auction, in which the service providers, as bidders,\ncompete for offering transportation services. The winners of the auction are\ndetermined through an integer linear program. To prevent the bidders from\nraising their bids for higher returns, we propose a strategy-proof\nVickrey-Clarke-Groves-based charging mechanism, which can maximize the social\nwelfare, to settle the final charges for the customers. We perform extensive\nsimulations to verify the analytical results and evaluate the performance of\nthe charging mechanism. \n\n"}
{"id": "1503.01488", "contents": "Title: Random Serial Dictatorship versus Probabilistic Serial Rule: A Tale of\n  Two Random Mechanisms Abstract: For assignment problems where agents, specifying ordinal preferences, are\nallocated indivisible objects, two widely studied randomized mechanisms are the\nRandom Serial Dictatorship (RSD) and Probabilistic Serial Rule (PS). These two\nmechanisms both have desirable economic and computational properties, but the\noutcomes they induce can be incomparable in many instances, thus creating\nchallenges in deciding which mechanism to adopt in practice. In this paper we\nfirst look at the space of lexicographic preferences and show that, as opposed\nto the general preference domain, RSD satisfies envyfreeness. Moreover, we show\nthat although under lexicographic preferences PS is strategyproof when the\nnumber of objects is less than or equal agents, it is strictly manipulable when\nthere are more objects than agents. In the space of general preferences, we\nprovide empirical results on the (in)comparability of RSD and PS, analyze\neconomic properties, and provide further insights on the applicability of each\nmechanism in different application domains. \n\n"}
{"id": "1503.02951", "contents": "Title: Mean Field Games in Nudge Systems for Societal Networks Abstract: We consider the general problem of resource sharing in societal networks,\nconsisting of interconnected communication, transportation, energy and other\nnetworks important to the functioning of society. Participants in such network\nneed to take decisions daily, both on the quantity of resources to use as well\nas the periods of usage. With this in mind, we discuss the problem of\nincentivizing users to behave in such a way that society as a whole benefits.\nIn order to perceive societal level impact, such incentives may take the form\nof rewarding users with lottery tickets based on good behavior, and\nperiodically conducting a lottery to translate these tickets into real rewards.\nWe will pose the user decision problem as a mean field game (MFG), and the\nincentives question as one of trying to select a good mean field equilibrium\n(MFE). In such a framework, each agent (a participant in the societal network)\ntakes a decision based on an assumed distribution of actions of his/her\ncompetitors, and the incentives provided by the social planner. The system is\nsaid to be at MFE if the agent's action is a sample drawn from the assumed\ndistribution. We will show the existence of such an MFE under different\nsettings, and also illustrate how to choose an attractive equilibrium using as\nan example demand-response in energy networks. \n\n"}
{"id": "1503.03185", "contents": "Title: Testing Randomness by Matching Pennies Abstract: In the game of Matching Pennies, Alice and Bob each hold a penny, and at\nevery tick of the clock they simultaneously display the head or the tail sides\nof their coins. If they both display the same side, then Alice wins Bob's\npenny; if they display different sides, then Bob wins Alice's penny. To avoid\ngiving the opponent a chance to win, both players seem to have nothing else to\ndo but to randomly play heads and tails with equal frequencies. However, while\nnot losing in this game is easy, not missing an opportunity to win is not.\nRandomizing your own moves can be made easy. Recognizing when the opponent's\nmoves are not random can be arbitrarily hard.\n  The notion of randomness is central in game theory, but it is usually taken\nfor granted. The notion of outsmarting is not central in game theory, but it is\ncentral in the practice of gaming. We pursue the idea that these two notions\ncan be usefully viewed as two sides of the same coin. \n\n"}
{"id": "1503.04099", "contents": "Title: Algorithms and complexity for Turaev-Viro invariants Abstract: The Turaev-Viro invariants are a powerful family of topological invariants\nfor distinguishing between different 3-manifolds. They are invaluable for\nmathematical software, but current algorithms to compute them require\nexponential time.\n  The invariants are parameterised by an integer $r \\geq 3$. We resolve the\nquestion of complexity for $r=3$ and $r=4$, giving simple proofs that computing\nTuraev-Viro invariants for $r=3$ is polynomial time, but for $r=4$ is \\#P-hard.\nMoreover, we give an explicit fixed-parameter tractable algorithm for arbitrary\n$r$, and show through concrete implementation and experimentation that this\nalgorithm is practical---and indeed preferable---to the prior state of the art\nfor real computation. \n\n"}
{"id": "1503.05988", "contents": "Title: Algorithmic Bayesian Persuasion Abstract: Persuasion, defined as the act of exploiting an informational advantage in\norder to effect the decisions of others, is ubiquitous. Indeed, persuasive\ncommunication has been estimated to account for almost a third of all economic\nactivity in the US. This paper examines persuasion through a computational\nlens, focusing on what is perhaps the most basic and fundamental model in this\nspace: the celebrated Bayesian persuasion model of Kamenica and Gentzkow. Here\nthere are two players, a sender and a receiver. The receiver must take one of a\nnumber of actions with a-priori unknown payoff, and the sender has access to\nadditional information regarding the payoffs. The sender can commit to\nrevealing a noisy signal regarding the realization of the payoffs of various\nactions, and would like to do so as to maximize her own payoff assuming a\nperfectly rational receiver.\n  We examine the sender's optimization task in three of the most natural input\nmodels for this problem, and essentially pin down its computational complexity\nin each. When the payoff distributions of the different actions are i.i.d. and\ngiven explicitly, we exhibit a polynomial-time (exact) algorithm, and a\n\"simple\" $(1-1/e)$-approximation algorithm. Our optimal scheme for the i.i.d.\nsetting involves an analogy to auction theory, and makes use of Border's\ncharacterization of the space of reduced-forms for single-item auctions. When\naction payoffs are independent but non-identical with marginal distributions\ngiven explicitly, we show that it is #P-hard to compute the optimal expected\nsender utility. Finally, we consider a general (possibly correlated) joint\ndistribution of action payoffs presented by a black box sampling oracle, and\nexhibit a fully polynomial-time approximation scheme (FPTAS) with a bi-criteria\nguarantee. We show that this result is the best possible in the black-box model\nfor information-theoretic reasons. \n\n"}
{"id": "1503.06447", "contents": "Title: Sum-of-squares lower bounds for planted clique Abstract: Finding cliques in random graphs and the closely related \"planted\" clique\nvariant, where a clique of size k is planted in a random G(n, 1/2) graph, have\nbeen the focus of substantial study in algorithm design. Despite much effort,\nthe best known polynomial-time algorithms only solve the problem for k ~\nsqrt(n).\n  In this paper we study the complexity of the planted clique problem under\nalgorithms from the Sum-of-squares hierarchy. We prove the first average case\nlower bound for this model: for almost all graphs in G(n,1/2), r rounds of the\nSOS hierarchy cannot find a planted k-clique unless k > n^{1/2r} (up to\nlogarithmic factors). Thus, for any constant number of rounds planted cliques\nof size n^{o(1)} cannot be found by this powerful class of algorithms. This is\nshown via an integrability gap for the natural formulation of maximum clique\nproblem on random graphs for SOS and Lasserre hierarchies, which in turn follow\nfrom degree lower bounds for the Positivestellensatz proof system.\n  We follow the usual recipe for such proofs. First, we introduce a natural\n\"dual certificate\" (also known as a \"vector-solution\" or \"pseudo-expectation\")\nfor the given system of polynomial equations representing the problem for every\nfixed input graph. Then we show that the matrix associated with this dual\ncertificate is PSD (positive semi-definite) with high probability over the\nchoice of the input graph.This requires the use of certain tools. One is the\ntheory of association schemes, and in particular the eigenspaces and\neigenvalues of the Johnson scheme. Another is a combinatorial method we develop\nto compute (via traces) norm bounds for certain random matrices whose entries\nare highly dependent; we hope this method will be useful elsewhere. \n\n"}
{"id": "1503.07220", "contents": "Title: Individual Planning in Agent Populations: Exploiting Anonymity and\n  Frame-Action Hypergraphs Abstract: Interactive partially observable Markov decision processes (I-POMDP) provide\na formal framework for planning for a self-interested agent in multiagent\nsettings. An agent operating in a multiagent environment must deliberate about\nthe actions that other agents may take and the effect these actions have on the\nenvironment and the rewards it receives. Traditional I-POMDPs model this\ndependence on the actions of other agents using joint action and model spaces.\nTherefore, the solution complexity grows exponentially with the number of\nagents thereby complicating scalability. In this paper, we model and extend\nanonymity and context-specific independence -- problem structures often present\nin agent populations -- for computational gain. We empirically demonstrate the\nefficiency from exploiting these problem structures by solving a new multiagent\nproblem involving more than 1,000 agents. \n\n"}
{"id": "1503.07389", "contents": "Title: Sorting in Networks: Adversity and Structure Abstract: People choose friendships with people similar to themselves, i.e. they sort\nby resemblence. Economic studies have shown when sorting is optimal and\nconstitute an equilibrium, however, this presumes lack of beneficial\nspillovers. We investigate formation of economic and social networks where\nagents may form or cut ties. We combine a setup with link formation where\nagents have types that determine the value of a connection. We provide\nconditions for sorting in friendships, i.e. that agents tend to partner only\nwith those with those sufficiently similar to themselves. Conditions are\nprovided with and without beneficial spillovers from indirect connections. We\nshow that sorting may be suboptimal, yet a socially stable outcome, despite\notherwise obeying the conditions for sorting in Becker (1973). We analyze\npolicy tools to mitigate suboptimal sorting. Another feature is that agents\nwith higher value are more central in networks under certain conditions; a side\neffect is sorting by degree centrality under certain conditions. Finally we\nillustrate the limits to patterns of sorting and centrality. \n\n"}
{"id": "1504.01431", "contents": "Title: If the Current Clique Algorithms are Optimal, so is Valiant's Parser Abstract: The CFG recognition problem is: given a context-free grammar $\\mathcal{G}$\nand a string $w$ of length $n$, decide if $w$ can be obtained from\n$\\mathcal{G}$. This is the most basic parsing question and is a core computer\nscience problem. Valiant's parser from 1975 solves the problem in\n$O(n^{\\omega})$ time, where $\\omega<2.373$ is the matrix multiplication\nexponent. Dozens of parsing algorithms have been proposed over the years, yet\nValiant's upper bound remains unbeaten. The best combinatorial algorithms have\nmildly subcubic $O(n^3/\\log^3{n})$ complexity.\n  Lee (JACM'01) provided evidence that fast matrix multiplication is needed for\nCFG parsing, and that very efficient and practical algorithms might be hard or\neven impossible to obtain. Lee showed that any algorithm for a more general\nparsing problem with running time $O(|\\mathcal{G}|\\cdot n^{3-\\varepsilon})$ can\nbe converted into a surprising subcubic algorithm for Boolean Matrix\nMultiplication. Unfortunately, Lee's hardness result required that the grammar\nsize be $|\\mathcal{G}|=\\Omega(n^6)$. Nothing was known for the more relevant\ncase of constant size grammars.\n  In this work, we prove that any improvement on Valiant's algorithm, even for\nconstant size grammars, either in terms of runtime or by avoiding the\ninefficiencies of fast matrix multiplication, would imply a breakthrough\nalgorithm for the $k$-Clique problem: given a graph on $n$ nodes, decide if\nthere are $k$ that form a clique.\n  Besides classifying the complexity of a fundamental problem, our reduction\nhas led us to similar lower bounds for more modern and well-studied cubic time\nproblems for which faster algorithms are highly desirable in practice: RNA\nFolding, a central problem in computational biology, and Dyck Language Edit\nDistance, answering an open question of Saha (FOCS'14). \n\n"}
{"id": "1504.02411", "contents": "Title: Can Almost Everybody be Almost Happy? PCP for PPAD and the\n  Inapproximability of Nash Abstract: We conjecture that PPAD has a PCP-like complete problem, seeking a near\nequilibrium in which all but very few players have very little incentive to\ndeviate. We show that, if one assumes that this problem requires exponential\ntime, several open problems in this area are settled. The most important\nimplication, proved via a \"birthday repetition\" reduction, is that the n^O(log\nn) approximation scheme of [LMM03] for the Nash equilibrium of two-player games\nis essentially optimum. Two other open problems in the area are resolved once\none assumes this conjecture, establishing that certain approximate equilibria\nare PPAD-complete: Finding a relative approximation of two-player Nash\nequilibria (without the well-supported restriction of [Das13]), and an\napproximate competitive equilibrium with equal incomes [Bud11] with small\nclearing error and near-optimal Gini coefficient. \n\n"}
{"id": "1504.02926", "contents": "Title: Strategic Interaction Among Different Entities in Internet of Things Abstract: The economic model of the Internet of Things (IoT) consists of end users,\nadvertisers and three different kinds of providers--IoT service provider\n(IoTSP), Wireless service provider (WSP) and cloud service provider (CSP). We\ninvestigate three different kinds of interactions among the providers. First,\nwe consider that the IoTSP prices a bundled service to the end-users, and the\nWSP and CSP pay the IoTSP (push model). Next, we consider the model where the\nend-users independently pay the each provider (pull model). Finally, we\nconsider a hybrid model of the above two where the IoTSP and WSP quote their\nprices to the end-users, but the CSP quotes its price to the IoTSP.\n  We characterize and quantify the impact of the advertisement revenue on the\nequilibrium pricing strategy and payoff of providers, and corresponding demands\nof end users in each of the above interaction models. Our analysis reveals that\nthe demand of end-users, and the payoffs of the providers are non decreasing\nfunctions of the advertisement revenue. For sufficiently high advertisement\nrevenue, the IoTSP will offer its service free of cost in each interaction\nmodel. However, the payoffs of the providers, and the demand of end-users vary\nacross different interaction models. Our analysis shows that the demand of\nend-users, and the payoff of the WSP are the highest in the pull (push, resp.)\nmodel in the low (high, resp.) advertisement revenue regime. The payoff of the\nIoTSP is always higher in the pull model irrespective of the advertisement\nrevenue. The payoff of the CSP is the highest in the hybrid model in the low\nadvertisement revenue regime. However, in the high advertisement revenue regime\nthe payoff of the CSP in the hybrid model or in the push model can be higher\ndepending on the equilibrium chosen in the push model. \n\n"}
{"id": "1504.03856", "contents": "Title: Sparse multivariate polynomial interpolation in the basis of Schubert\n  polynomials Abstract: Schubert polynomials were discovered by A. Lascoux and M. Sch\\\"utzenberger in\nthe study of cohomology rings of flag manifolds in 1980's. These polynomials\ngeneralize Schur polynomials, and form a linear basis of multivariate\npolynomials. In 2003, Lenart and Sottile introduced skew Schubert polynomials,\nwhich generalize skew Schur polynomials, and expand in the Schubert basis with\nthe generalized Littlewood-Richardson coefficients.\n  In this paper we initiate the study of these two families of polynomials from\nthe perspective of computational complexity theory. We first observe that skew\nSchubert polynomials, and therefore Schubert polynomials, are in $\\CountP$\n(when evaluating on non-negative integral inputs) and $\\VNP$.\n  Our main result is a deterministic algorithm that computes the expansion of a\npolynomial $f$ of degree $d$ in $\\Z[x_1, \\dots, x_n]$ in the basis of Schubert\npolynomials, assuming an oracle computing Schubert polynomials. This algorithm\nruns in time polynomial in $n$, $d$, and the bit size of the expansion. This\ngeneralizes, and derandomizes, the sparse interpolation algorithm of symmetric\npolynomials in the Schur basis by Barvinok and Fomin (Advances in Applied\nMathematics, 18(3):271--285). In fact, our interpolation algorithm is general\nenough to accommodate any linear basis satisfying certain natural properties.\n  Applications of the above results include a new algorithm that computes the\ngeneralized Littlewood-Richardson coefficients. \n\n"}
{"id": "1504.03903", "contents": "Title: Learning to be green: robust energy efficiency maximization in dynamic\n  MIMO-OFDM systems Abstract: In this paper, we examine the maximization of energy efficiency (EE) in\nnext-generation multi-user MIMO-OFDM networks that evolve dynamically over time\n- e.g. due to user mobility, fluctuations in the wireless medium, modulations\nin the users' load, etc. Contrary to the static/stationary regime, the system\nmay evolve in an arbitrary manner so, targeting a fixed optimum state (either\nstatic or in the mean) becomes obsolete; instead, users must adjust to changes\nin the system \"on the fly\", without being able to predict the state of the\nsystem in advance. To tackle these issues, we propose a simple and distributed\nonline optimization policy that leads to no regret, i.e. it allows users to\nmatch (and typically outperform) even the best fixed transmit policy in\nhindsight, irrespective of how the system varies with time. Moreover, to\naccount for the scarcity of perfect channel state information (CSI) in massive\nMIMO systems, we also study the algorithm's robustness in the presence of\nmeasurement errors and observation noise. Importantly, the proposed policy\nretains its no-regret properties under very mild assumptions on the error\nstatistics and, on average, it enjoys the same performance guarantees as in the\nnoiseless, deterministic case. Our analysis is supplemented by extensive\nnumerical simulations which show that, in realistic network environments, users\ntrack their individually optimum transmit profile even under rapidly changing\nchannel conditions, achieving gains of up to 600% in energy efficiency over\nuniform power allocation policies. \n\n"}
{"id": "1504.05670", "contents": "Title: Mechanism Design for Fair Allocation Abstract: Mechanism design for a social utility being the sum of agents' utilities\n(SoU) is a well-studied problem. There are, however, a number of problems of\ntheoretical and practical interest where a designer may have a different\nobjective than maximization of the SoU. One motivation for this is the desire\nfor more equitable allocation of resources among agents. A second, more subtle,\nmotivation is the fact that a fairer allocation indirectly implies less\nvariation in taxes which can be desirable in a situation where (implicit)\nindividual agent budgetary constraints make payment of large taxes unrealistic.\nIn this paper we study a family of social utilities that provide fair\nallocation (with SoU being subsumed as an extreme case) and derive conditions\nunder which Bayesian and Dominant strategy implementation is possible.\nFurthermore, it is shown how a simple modification of the above mechanism can\nguarantee full Bayesian implementation. Through a numerical example it is shown\nthat the proposed method can result in significant gains both in allocation\nfairness and tax reduction. \n\n"}
{"id": "1504.06828", "contents": "Title: A bi-convex optimization problem to compute Nash equilibrium in n-player\n  games and an algorithm Abstract: In this paper we present optimization problems with biconvex objective\nfunction and linear constraints such that the set of global minima of the\noptimization problems is the same as the set of Nash equilibria of a n-player\ngeneral-sum normal form game. We further show that the objective function is an\ninvex function and consider a projected gradient descent algorithm. We prove\nthat the projected gradient descent scheme converges to a partial optimum of\nthe objective function. We also present simulation results on certain test\ncases showing convergence to a Nash equilibrium strategy. \n\n"}
{"id": "1504.07342", "contents": "Title: On Potential Equations of Finite Games Abstract: In this paper, some new criteria for detecting whether a finite game is\npotential are proposed by solving potential equations. The verification\nequations with the minimal number for checking a potential game are obtained\nfor the first time. Some connections between the potential equations and the\nexisting characterizations of potential games are established. It is revealed\nthat a finite game is potential if and only if its every bi-matrix sub-game is\npotential. \n\n"}
{"id": "1504.07545", "contents": "Title: Matroids are Immune to Braess Paradox Abstract: The famous Braess paradox describes the following phenomenon: It might happen\nthat the improvement of resources, like building a new street within a\ncongested network, may in fact lead to larger costs for the players in an\nequilibrium. In this paper we consider general nonatomic congestion games and\ngive a characterization of the maximal combinatorial property of strategy\nspaces for which Braess paradox does not occur. In a nutshell, bases of\nmatroids are exactly this maximal structure. We prove our characterization by\ntwo novel sensitivity results for convex separable optimization problems over\npolymatroid base polyhedra which may be of independent interest. \n\n"}
{"id": "1504.07830", "contents": "Title: On k-Submodular Relaxation Abstract: $k$-submodular functions, introduced by Huber and Kolmogorov, are functions\ndefined on $\\{0, 1, 2, \\dots, k\\}^n$ satisfying certain submodular-type\ninequalities. $k$-submodular functions typically arise as relaxations of\nNP-hard problems, and the relaxations by $k$-submodular functions play key\nroles in design of efficient, approximation, or fixed-parameter tractable\nalgorithms. Motivated by this, we consider the following problem: Given a\nfunction $f : \\{1, 2, \\dots, k\\}^n \\rightarrow \\mathbb{R} \\cup \\{\\infty\\}$,\ndetermine whether $f$ is extended to a $k$-submodular function $g : \\{0, 1, 2,\n\\dots, k\\}^n \\rightarrow \\mathbb{R} \\cup \\{\\infty\\}$, where $g$ is called a\n$k$-submodular relaxation of $f$.\n  We give a polymorphic characterization of those functions which admit a\n$k$-submodular relaxation, and also give a combinatorial $O((k^n)^2)$-time\nalgorithm to find a $k$-submodular relaxation or establish that a\n$k$-submodular relaxation does not exist. Our algorithm has interesting\nproperties: (1) If the input function is integer valued, then our algorithm\noutputs a half-integral relaxation, and (2) if the input function is binary,\nthen our algorithm outputs the unique optimal relaxation. We present\napplications of our algorithm to valued constraint satisfaction problems. \n\n"}
{"id": "1504.08363", "contents": "Title: On the Structure, Covering, and Learning of Poisson Multinomial\n  Distributions Abstract: An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of the\nsum of $n$ independent random vectors supported on the set ${\\cal\nB}_k=\\{e_1,\\ldots,e_k\\}$ of standard basis vectors in $\\mathbb{R}^k$. We prove\na structural characterization of these distributions, showing that, for all\n$\\varepsilon >0$, any $(n, k)$-Poisson multinomial random vector is\n$\\varepsilon$-close, in total variation distance, to the sum of a discretized\nmultidimensional Gaussian and an independent $(\\text{poly}(k/\\varepsilon),\nk)$-Poisson multinomial random vector. Our structural characterization extends\nthe multi-dimensional CLT of Valiant and Valiant, by simultaneously applying to\nall approximation requirements $\\varepsilon$. In particular, it overcomes\nfactors depending on $\\log n$ and, importantly, the minimum eigenvalue of the\nPMD's covariance matrix from the distance to a multidimensional Gaussian random\nvariable.\n  We use our structural characterization to obtain an $\\varepsilon$-cover, in\ntotal variation distance, of the set of all $(n, k)$-PMDs, significantly\nimproving the cover size of Daskalakis and Papadimitriou, and obtaining the\nsame qualitative dependence of the cover size on $n$ and $\\varepsilon$ as the\n$k=2$ cover of Daskalakis and Papadimitriou. We further exploit this structure\nto show that $(n,k)$-PMDs can be learned to within $\\varepsilon$ in total\nvariation distance from $\\tilde{O}_k(1/\\varepsilon^2)$ samples, which is\nnear-optimal in terms of dependence on $\\varepsilon$ and independent of $n$. In\nparticular, our result generalizes the single-dimensional result of Daskalakis,\nDiakonikolas, and Servedio for Poisson Binomials to arbitrary dimension. \n\n"}
{"id": "1505.00039", "contents": "Title: Learning Cooperative Games Abstract: This paper explores a PAC (probably approximately correct) learning model in\ncooperative games. Specifically, we are given $m$ random samples of coalitions\nand their values, taken from some unknown cooperative game; can we predict the\nvalues of unseen coalitions? We study the PAC learnability of several\nwell-known classes of cooperative games, such as network flow games, threshold\ntask games, and induced subgraph games. We also establish a novel connection\nbetween PAC learnability and core stability: for games that are efficiently\nlearnable, it is possible to find payoff divisions that are likely to be stable\nusing a polynomial number of samples. \n\n"}
{"id": "1505.02993", "contents": "Title: A Holant Dichotomy: Is the FKT Algorithm Universal? Abstract: We prove a complexity dichotomy for complex-weighted Holant problems with an\narbitrary set of symmetric constraint functions on Boolean variables. This\ndichotomy is specifically to answer the question: Is the FKT algorithm under a\nholographic transformation a \\emph{universal} strategy to obtain\npolynomial-time algorithms for problems over planar graphs that are intractable\nin general? This dichotomy is a culmination of previous ones, including those\nfor Spin Systems, Holant, and #CSP. A recurring theme has been that a\nholographic reduction to FKT is a universal strategy. Surprisingly, for planar\nHolant, we discover new planar tractable problems that are not expressible by a\nholographic reduction to FKT.\n  In previous work, an important tool was a dichotomy for #CSP^d, which denotes\n#CSP where every variable appears a multiple of d times. However its proof\nviolates planarity. We prove a dichotomy for planar #CSP^2. We apply this\nplanar #CSP^2 dichotomy in the proof of the planar Holant dichotomy.\n  As a special case of our new planar tractable problems, counting perfect\nmatchings (#PM) over k-uniform hypergraphs is polynomial-time computable when\nthe incidence graph is planar and k >= 5. The same problem is #P-hard when k=3\nor k=4, which is also a consequence of our dichotomy. When k=2, it becomes #PM\nover planar graphs and is tractable again. More generally, over hypergraphs\nwith specified hyperedge sizes and the same planarity assumption, #PM is\npolynomial-time computable if the greatest common divisor of all hyperedge\nsizes is at least 5. \n\n"}
{"id": "1505.03424", "contents": "Title: Beating the random assignment on constraint satisfaction problems of\n  bounded degree Abstract: We show that for any odd $k$ and any instance of the Max-kXOR constraint\nsatisfaction problem, there is an efficient algorithm that finds an assignment\nsatisfying at least a $\\frac{1}{2} + \\Omega(1/\\sqrt{D})$ fraction of\nconstraints, where $D$ is a bound on the number of constraints that each\nvariable occurs in. This improves both qualitatively and quantitatively on the\nrecent work of Farhi, Goldstone, and Gutmann (2014), which gave a\n\\emph{quantum} algorithm to find an assignment satisfying a $\\frac{1}{2} +\n\\Omega(D^{-3/4})$ fraction of the equations.\n  For arbitrary constraint satisfaction problems, we give a similar result for\n\"triangle-free\" instances; i.e., an efficient algorithm that finds an\nassignment satisfying at least a $\\mu + \\Omega(1/\\sqrt{D})$ fraction of\nconstraints, where $\\mu$ is the fraction that would be satisfied by a uniformly\nrandom assignment. \n\n"}
{"id": "1505.04383", "contents": "Title: How to refute a random CSP Abstract: Let $P$ be a $k$-ary predicate over a finite alphabet. Consider a random\nCSP$(P)$ instance $I$ over $n$ variables with $m$ constraints. When $m \\gg n$\nthe instance $I$ will be unsatisfiable with high probability, and we want to\nfind a refutation - i.e., a certificate of unsatisfiability. When $P$ is the\n$3$-ary OR predicate, this is the well studied problem of refuting random\n$3$-SAT formulas, and an efficient algorithm is known only when $m \\gg\nn^{3/2}$. Understanding the density required for refutation of other predicates\nis important in cryptography, proof complexity, and learning theory.\nPreviously, it was known that for a $k$-ary predicate, having $m \\gg n^{\\lceil\nk/2 \\rceil}$ constraints suffices for refutation. We give a criterion for\npredicates that often yields efficient refutation algorithms at much lower\ndensities. Specifically, if $P$ fails to support a $t$-wise uniform\ndistribution, then there is an efficient algorithm that refutes random CSP$(P)$\ninstances $I$ whp when $m \\gg n^{t/2}$. Indeed, our algorithm will \"somewhat\nstrongly\" refute $I$, certifying $\\mathrm{Opt}(I) \\leq 1-\\Omega_k(1)$, if $t =\nk$ then we get the strongest possible refutation, certifying $\\mathrm{Opt}(I)\n\\leq \\mathrm{E}[P] + o(1)$. This last result is new even in the context of\nrandom $k$-SAT. Regarding the optimality of our $m \\gg n^{t/2}$ requirement,\nprior work on SDP hierarchies has given some evidence that efficient refutation\nof random CSP$(P)$ may be impossible when $m \\ll n^{t/2}$. Thus there is an\nindication our algorithm's dependence on $m$ is optimal for every $P$, at least\nin the context of SDP hierarchies. Along these lines, we show that our\nrefutation algorithm can be carried out by the $O(1)$-round SOS SDP hierarchy.\nFinally, as an application of our result, we falsify assumptions used to show\nhardness-of-learning results in recent work of Daniely, Linial, and\nShalev-Shwartz. \n\n"}
{"id": "1505.07737", "contents": "Title: Aggregation of Votes with Multiple Positions on Each Issue Abstract: We consider the problem of aggregating votes cast by a society on a fixed set\nof issues, where each member of the society may vote for one of several\npositions on each issue, but the combination of votes on the various issues is\nrestricted to a set of feasible voting patterns. We require the aggregation to\nbe supportive, i.e. for every issue $j$ the corresponding component $f_j$ of\nevery aggregator on every issue should satisfy $f_j(x_1, ,\\ldots, x_n) \\in\n\\{x_1, ,\\ldots, x_n\\}$. We prove that, in such a set-up, non-dictatorial\naggregation of votes in a society of some size is possible if and only if\neither non-dictatorial aggregation is possible in a society of only two members\nor a ternary aggregator exists that either on every issue $j$ is a majority\noperation, i.e. the corresponding component satisfies $f_j(x,x,y) = f_j(x,y,x)\n= f_j(y,x,x) =x, \\forall x,y$, or on every issue is a minority operation, i.e.\nthe corresponding component satisfies $f_j(x,x,y) = f_j(x,y,x) = f_j(y,x,x) =y,\n\\forall x,y.$ We then introduce a notion of uniformly non-dictatorial\naggregator, which is defined to be an aggregator that on every issue, and when\nrestricted to an arbitrary two-element subset of the votes for that issue,\ndiffers from all projection functions. We first give a characterization of sets\nof feasible voting patterns that admit a uniformly non-dictatorial aggregator.\nThen making use of Bulatov's dichotomy theorem for conservative constraint\nsatisfaction problems, we connect social choice theory with combinatorial\ncomplexity by proving that if a set of feasible voting patterns $X$ has a\nuniformly non-dictatorial aggregator of some arity then the multi-sorted\nconservative constraint satisfaction problem on $X$, in the sense introduced by\nBulatov and Jeavons, with each issue representing a sort, is tractable;\notherwise it is NP-complete. \n\n"}
{"id": "1506.02243", "contents": "Title: On approximating tree spanners that are breadth first search trees Abstract: A tree $t$-spanner $T$ of a graph $G$ is a spanning tree of $G$ such that the\ndistance in $T$ between every pair of verices is at most $t$ times the distance\nin $G$ between them. There are efficient algorithms that find a tree $t\\cdot\nO(\\log n)$-spanner of a graph $G$, when $G$ admits a tree $t$-spanner. In this\npaper, the search space is narrowed to $v$-concentrated spanning trees, a\nsimple family that includes all the breadth first search trees starting from\nvertex $v$. In this case, it is not easy to find approximate tree spanners\nwithin factor almost $o(\\log n)$. Specifically, let $m$ and $t$ be integers,\nsuch that $m>0$ and $t\\geq 7$. If there is an efficient algorithm that receives\nas input a graph $G$ and a vertex $v$ and returns a $v$-concentrated tree\n$t\\cdot o((\\log n)^{m/(m+1)})$-spanner of $G$, when $G$ admits a\n$v$-concentrated tree $t$-spanner, then there is an algorithm that decides\n3-SAT in quasi-polynomial time. \n\n"}
{"id": "1506.02719", "contents": "Title: Non-parametric Revenue Optimization for Generalized Second Price\n  Auctions Abstract: We present an extensive analysis of the key problem of learning optimal\nreserve prices for generalized second price auctions. We describe two\nalgorithms for this task: one based on density estimation, and a novel\nalgorithm benefiting from solid theoretical guarantees and with a very\nfavorable running-time complexity of $O(n S \\log (n S))$, where $n$ is the\nsample size and $S$ the number of slots. Our theoretical guarantees are more\nfavorable than those previously presented in the literature. Additionally, we\nshow that even if bidders do not play at an equilibrium, our second algorithm\nis still well defined and minimizes a quantity of interest. To our knowledge,\nthis is the first attempt to apply learning algorithms to the problem of\nreserve price optimization in GSP auctions. Finally, we present the first\nconvergence analysis of empirical equilibrium bidding functions to the unique\nsymmetric Bayesian-Nash equilibrium of a GSP. \n\n"}
{"id": "1506.03489", "contents": "Title: Truthful Linear Regression Abstract: We consider the problem of fitting a linear model to data held by individuals\nwho are concerned about their privacy. Incentivizing most players to truthfully\nreport their data to the analyst constrains our design to mechanisms that\nprovide a privacy guarantee to the participants; we use differential privacy to\nmodel individuals' privacy losses. This immediately poses a problem, as\ndifferentially private computation of a linear model necessarily produces a\nbiased estimation, and existing approaches to design mechanisms to elicit data\nfrom privacy-sensitive individuals do not generalize well to biased estimators.\nWe overcome this challenge through an appropriate design of the computation and\npayment scheme. \n\n"}
{"id": "1506.03684", "contents": "Title: The Pseudo-Dimension of Near-Optimal Auctions Abstract: This paper develops a general approach, rooted in statistical learning\ntheory, to learning an approximately revenue-maximizing auction from data. We\nintroduce $t$-level auctions to interpolate between simple auctions, such as\nwelfare maximization with reserve prices, and optimal auctions, thereby\nbalancing the competing demands of expressivity and simplicity. We prove that\nsuch auctions have small representation error, in the sense that for every\nproduct distribution $F$ over bidders' valuations, there exists a $t$-level\nauction with small $t$ and expected revenue close to optimal. We show that the\nset of $t$-level auctions has modest pseudo-dimension (for polynomial $t$) and\ntherefore leads to small learning error. One consequence of our results is\nthat, in arbitrary single-parameter settings, one can learn a mechanism with\nexpected revenue arbitrarily close to optimal from a polynomial number of\nsamples. \n\n"}
{"id": "1506.03885", "contents": "Title: Games with Delays. A Frankenstein Approach Abstract: We investigate infinite games on finite graphs where the information flow is\nperturbed by nondeterministic signalling delays. It is known that such\nperturbations make synthesis problems virtually unsolvable, in the general\ncase. On the classical model where signals are attached to states, tractable\ncases are rare and difficult to identify.\n  Here, we propose a model where signals are detached from control states, and\nwe identify a subclass on which equilibrium outcomes can be preserved, even if\nsignals are delivered with a delay that is finitely bounded. To offset the\nperturbation, our solution procedure combines responses from a collection of\nvirtual plays following an equilibrium strategy in the instant- signalling game\nto synthesise, in a Frankenstein manner, an equivalent equilibrium strategy for\nthe delayed-signalling game. \n\n"}
{"id": "1506.04047", "contents": "Title: Approximation Algorithm for the Binary-Preference Capacitated Selfish\n  Replication Game and a Tight Bound on its Price of Anarchy Abstract: We consider the capacitated selfish replication (CSR) game with binary\npreferences, over general undirected networks. We first show that such games\nhave an associated ordinary potential function, and hence always admit a\npure-strategy Nash equilibrium (NE). Further, when the minimum degree of the\nnetwork and the number of resources are of the same order, there exists an\nexact polynomial time algorithm which can find a NE. Following this, we study\nthe price of anarchy of such games, and show that it is bounded above by 3; we\nfurther provide some instances for which the price of anarchy is at least 2. We\ndevelop a quasi-polynomial algorithm O(n^2D^{ln n}), where n is the number of\nplayers and D is the diameter of the network, which can find, in a distributed\nmanner, an allocation profile that is within a constant factor of the optimal\nallocation, and hence of any pure-strategy NE of the game. Proof of this result\nuses a novel potential function. \n\n"}
{"id": "1506.04525", "contents": "Title: Unified Systems of FB-SPDEs/FB-SDEs with Jumps/Skew Reflections and\n  Stochastic Differential Games Abstract: We study four systems and their interactions. First, we formulate a unified\nsystem of coupled forward-backward stochastic partial differential equations\n(FB-SPDEs) with Levy jumps, whose drift, diffusion, and jump coefficients may\ninvolve partial differential operators. A solution to the FB-SPDEs is defined\nby a 4-tuple general dimensional random vector-field process evolving in time\ntogether with position parameters over a domain (e.g., a hyperbox or a\nmanifold). Under an infinite sequence of generalized local linear growth and\nLipschitz conditions, the well-posedness of an adapted 4-tuple strong solution\nis proved over a suitably constructed topological space. Second, we consider a\nunified system of FB-SDEs, a special form of the FB-SPDEs, however, with skew\nboundary reflections. Under randomized linear growth and Lipschitz conditions\ntogether with a general completely-S condition on reflections, we prove the\nwell-posedness of an adapted 6-tuple weak solution with boundary regulators to\nthe FB-SDEs by the Skorohod problem and an oscillation inequality.\nParticularly, if the spectral radii in some sense for reflection matrices are\nstrictly less than the unity, an adapted 6-tuple strong solution is concerned.\nThird, we formulate a stochastic differential game (SDG) with general number of\nplayers based on the FB-SDEs. By a solution to the FB-SPDEs, we get a solution\nto the FB-SDEs under a given control rule and then obtain a Pareto optimal Nash\nequilibrium policy process to the SDG. Fourth, we study the applications of the\nFB-SPDEs/FB-SDEs in queueing systems and quantum statistics while we use them\nto motivate the SDG. \n\n"}
{"id": "1506.06302", "contents": "Title: Inapproximability of $H$-Transversal/Packing Abstract: Given an undirected graph $G = (V_G, E_G)$ and a fixed \"pattern\" graph $H =\n(V_H, E_H)$ with $k$ vertices, we consider the $H$-Transversal and $H$-Packing\nproblems. The former asks to find the smallest $S \\subseteq V_G$ such that the\nsubgraph induced by $V_G \\setminus S$ does not have $H$ as a subgraph, and the\nlatter asks to find the maximum number of pairwise disjoint $k$-subsets $S_1,\n..., S_m \\subseteq V_G$ such that the subgraph induced by each $S_i$ has $H$ as\na subgraph.\n  We prove that if $H$ is 2-connected, $H$-Transversal and $H$-Packing are\nalmost as hard to approximate as general $k$-Hypergraph Vertex Cover and\n$k$-Set Packing, so it is NP-hard to approximate them within a factor of\n$\\Omega (k)$ and $\\widetilde \\Omega (k)$ respectively. We also show that there\nis a 1-connected $H$ where $H$-Transversal admits an $O(\\log k)$-approximation\nalgorithm, so that the connectivity requirement cannot be relaxed from 2 to 1.\nFor a special case of $H$-Transversal where $H$ is a (family of) cycles, we\nmention the implication of our result to the related Feedback Vertex Set\nproblem, and give a different hardness proof for directed graphs. \n\n"}
{"id": "1506.07773", "contents": "Title: Maximum weighted independent sets with a budget Abstract: Given a graph $G$, a non-negative integer $k$, and a weight function that\nmaps each vertex in $G$ to a positive real number, the \\emph{Maximum Weighted\nBudgeted Independent Set (MWBIS) problem} is about finding a maximum weighted\nindependent set in $G$ of cardinality at most $k$. A special case of MWBIS,\nwhen the weight assigned to each vertex is equal to its degree in $G$, is\ncalled the \\emph{Maximum Independent Vertex Coverage (MIVC)} problem. In other\nwords, the MIVC problem is about finding an independent set of cardinality at\nmost $k$ with maximum coverage.\n  Since it is a generalization of the well-known Maximum Weighted Independent\nSet (MWIS) problem, MWBIS too does not have any constant factor polynomial time\napproximation algorithm assuming $P \\neq NP$. In this paper, we study MWBIS in\nthe context of bipartite graphs. We show that, unlike MWIS, the MIVC (and\nthereby the MWBIS) problem in bipartite graphs is NP-hard. Then, we show that\nthe MWBIS problem admits a $\\frac{1}{2}$-factor approximation algorithm in the\nclass of bipartite graphs, which matches the integrality gap of a natural LP\nrelaxation. \n\n"}
{"id": "1507.00576", "contents": "Title: Flip the Cloud: Cyber-Physical Signaling Games in the Presence of\n  Advanced Persistent Threats Abstract: Access to the cloud has the potential to provide scalable and cost effective\nenhancements of physical devices through the use of advanced computational\nprocesses run on apparently limitless cyber infrastructure. On the other hand,\ncyber-physical systems and cloud-controlled devices are subject to numerous\ndesign challenges; among them is that of security. In particular, recent\nadvances in adversary technology pose Advanced Persistent Threats (APTs) which\nmay stealthily and completely compromise a cyber system. In this paper, we\ndesign a framework for the security of cloud-based systems that specifies when\na device should trust commands from the cloud which may be compromised. This\ninteraction can be considered as a game between three players: a cloud\ndefender/administrator, an attacker, and a device. We use traditional signaling\ngames to model the interaction between the cloud and the device, and we use the\nrecently proposed FlipIt game to model the struggle between the defender and\nattacker for control of the cloud. Because attacks upon the cloud can occur\nwithout knowledge of the defender, we assume that strategies in both games are\npicked according to prior commitment. This framework requires a new equilibrium\nconcept, which we call Gestalt Equilibrium, a fixed-point that expresses the\ninterdependence of the signaling and FlipIt games. We present the solution to\nthis fixed-point problem under certain parameter cases, and illustrate an\nexample application of cloud control of an unmanned vehicle. Our results\ncontribute to the growing understanding of cloud-controlled systems. \n\n"}
{"id": "1507.00843", "contents": "Title: Optimal linear Bernoulli factories for small mean problems Abstract: Suppose a coin with unknown probability $p$ of heads can be flipped as often\nas desired. A Bernoulli factory for a function $f$ is an algorithm that uses\nflips of the coin together with auxiliary randomness to flip a single coin with\nprobability $f(p)$ of heads. Applications include near perfect sampling from\nthe stationary distribution of regenerative processes. When $f$ is analytic,\nthe problem can be reduced to a Bernoulli factory of the form $f(p) = Cp$ for\nconstant $C$. Presented here is a new algorithm where for small values of $Cp$,\nrequires roughly only $C$ coin flips to generate a $Cp$ coin. From information\ntheory considerations, this is also conjectured to be (to first order) the\nminimum number of flips needed by any such algorithm.\n  For $Cp$ large, the new algorithm can also be used to build a new Bernoulli\nfactory that uses only 80\\% of the expected coin flips of the older method, and\napplies to the more general problem of a multivariate Bernoulli factory, where\nthere are $k$ coins, the $k$th coin has unknown probability $p_k$ of heads, and\nthe goal is to simulate a coin flip with probability $C_1 p_1 + \\cdots + C_k\np_k$ of heads. \n\n"}
{"id": "1507.01859", "contents": "Title: A stochastic approximation algorithm for stochastic semidefinite\n  programming Abstract: Motivated by applications to multi-antenna wireless networks, we propose a\ndistributed and asynchronous algorithm for stochastic semidefinite programming.\nThis algorithm is a stochastic approximation of a continous- time matrix\nexponential scheme regularized by the addition of an entropy-like term to the\nproblem's objective function. We show that the resulting algorithm converges\nalmost surely to an $\\varepsilon$-approximation of the optimal solution\nrequiring only an unbiased estimate of the gradient of the problem's stochastic\nobjective. When applied to throughput maximization in wireless multiple-input\nand multiple-output (MIMO) systems, the proposed algorithm retains its\nconvergence properties under a wide array of mobility impediments such as user\nupdate asynchronicities, random delays and/or ergodically changing channels.\nOur theoretical analysis is complemented by extensive numerical simulations\nwhich illustrate the robustness and scalability of the proposed method in\nrealistic network conditions. \n\n"}
{"id": "1507.02615", "contents": "Title: Optimal Auctions vs. Anonymous Pricing Abstract: For selling a single item to agents with independent but non-identically\ndistributed values, the revenue optimal auction is complex. With respect to it,\nHartline and Roughgarden (2009) showed that the approximation factor of the\nsecond-price auction with an anonymous reserve is between two and four. We\nconsider the more demanding problem of approximating the revenue of the ex ante\nrelaxation of the auction problem by posting an anonymous price (while supplies\nlast) and prove that their worst-case ratio is e. As a corollary, the\nupper-bound of anonymous pricing or anonymous reserves versus the optimal\nauction improves from four to $e$. We conclude that, up to an $e$ factor,\ndiscrimination and simultaneity are unimportant for driving revenue in\nsingle-item auctions. \n\n"}
{"id": "1507.02746", "contents": "Title: Low-Risk Mechanisms for the Kidney Exchange Game Abstract: In this paper we consider the pairwise kidney exchange game. This game\nnaturally appears in situations that some service providers benefit from\npairwise allocations on a network, such as the kidney exchanges between\nhospitals.\n  Ashlagi et al. present a $2$-approximation randomized truthful mechanism for\nthis problem. This is the best known result in this setting with multiple\nplayers. However, we note that the variance of the utility of an agent in this\nmechanism may be as large as $\\Omega(n^2)$, which is not desirable in a real\napplication. In this paper we resolve this issue by providing a\n$2$-approximation randomized truthful mechanism in which the variance of the\nutility of each agent is at most $2+\\epsilon$.\n  Interestingly, we could apply our technique to design a deterministic\nmechanism such that, if an agent deviates from the mechanism, she does not gain\nmore than $2\\lceil \\log_2 m\\rceil$. We call such a mechanism an almost truthful\nmechanism. Indeed, in a practical scenario, an almost truthful mechanism is\nlikely to imply a truthful mechanism. We believe that our approach can be used\nto design low risk or almost truthful mechanisms for other problems. \n\n"}
{"id": "1507.03046", "contents": "Title: An efficient tree decomposition method for permanents and mixed\n  discriminants Abstract: We present an efficient algorithm to compute permanents, mixed discriminants\nand hyperdeterminants of structured matrices and multidimensional arrays\n(tensors). We describe the sparsity structure of an array in terms of a graph,\nand we assume that its treewidth, denoted as $\\omega$, is small. Our algorithm\nrequires $O(n 2^\\omega)$ arithmetic operations to compute permanents, and\n$O(n^2 + n 3^\\omega)$ for mixed discriminants and hyperdeterminants. We finally\nshow that mixed volume computation continues to be hard under bounded treewidth\nassumptions. \n\n"}
{"id": "1507.03269", "contents": "Title: Tensor principal component analysis via sum-of-squares proofs Abstract: We study a statistical model for the tensor principal component analysis\nproblem introduced by Montanari and Richard: Given a order-$3$ tensor $T$ of\nthe form $T = \\tau \\cdot v_0^{\\otimes 3} + A$, where $\\tau \\geq 0$ is a\nsignal-to-noise ratio, $v_0$ is a unit vector, and $A$ is a random noise\ntensor, the goal is to recover the planted vector $v_0$. For the case that $A$\nhas iid standard Gaussian entries, we give an efficient algorithm to recover\n$v_0$ whenever $\\tau \\geq \\omega(n^{3/4} \\log(n)^{1/4})$, and certify that the\nrecovered vector is close to a maximum likelihood estimator, all with high\nprobability over the random choice of $A$. The previous best algorithms with\nprovable guarantees required $\\tau \\geq \\Omega(n)$.\n  In the regime $\\tau \\leq o(n)$, natural tensor-unfolding-based spectral\nrelaxations for the underlying optimization problem break down (in the sense\nthat their integrality gap is large). To go beyond this barrier, we use convex\nrelaxations based on the sum-of-squares method. Our recovery algorithm proceeds\nby rounding a degree-$4$ sum-of-squares relaxations of the\nmaximum-likelihood-estimation problem for the statistical model. To complement\nour algorithmic results, we show that degree-$4$ sum-of-squares relaxations\nbreak down for $\\tau \\leq O(n^{3/4}/\\log(n)^{1/4})$, which demonstrates that\nimproving our current guarantees (by more than logarithmic factors) would\nrequire new techniques or might even be intractable.\n  Finally, we show how to exploit additional problem structure in order to\nsolve our sum-of-squares relaxations, up to some approximation, very\nefficiently. Our fastest algorithm runs in nearly-linear time using shifted\n(matrix) power iteration and has similar guarantees as above. The analysis of\nthis algorithm also confirms a variant of a conjecture of Montanari and Richard\nabout singular vectors of tensor unfoldings. \n\n"}
{"id": "1507.04925", "contents": "Title: Ascending-Price Algorithms for Unknown Markets Abstract: We design a simple ascending-price algorithm to compute a\n$(1+\\varepsilon)$-approximate equilibrium in Arrow-Debreu exchange markets with\nweak gross substitute (WGS) property, which runs in time polynomial in market\nparameters and $\\log 1/\\varepsilon$. This is the first polynomial-time\nalgorithm for most of the known tractable classes of Arrow-Debreu markets,\nwhich is easy to implement and avoids heavy machinery such as the ellipsoid\nmethod. In addition, our algorithm can be applied in unknown market setting\nwithout exact knowledge about the number of agents, their individual utilities\nand endowments. Instead, our algorithm only relies on queries to a global\ndemand oracle by posting prices and receiving aggregate demand for goods as\nfeedback. When demands are real-valued functions of prices, the oracles can\nonly return values of bounded precision based on real utility functions. Due to\nthis more realistic assumption, precision and representation of prices and\ndemands become a major technical challenge, and we develop new tools and\ninsights that may be of independent interest. Furthermore, our approach also\ngives the first polynomial-time algorithm to compute an exact equilibrium for\nmarkets with spending constraint utilities, a piecewise linear concave\ngeneralization of linear utilities. This resolves an open problem posed by Duan\nand Mehlhorn (2015). \n\n"}
{"id": "1507.05950", "contents": "Title: On the Worst-Case Approximability of Sparse PCA Abstract: It is well known that Sparse PCA (Sparse Principal Component Analysis) is\nNP-hard to solve exactly on worst-case instances. What is the complexity of\nsolving Sparse PCA approximately? Our contributions include: 1) a simple and\nefficient algorithm that achieves an $n^{-1/3}$-approximation; 2) NP-hardness\nof approximation to within $(1-\\varepsilon)$, for some small constant\n$\\varepsilon > 0$; 3) SSE-hardness of approximation to within any constant\nfactor; and 4) an $\\exp\\exp\\left(\\Omega\\left(\\sqrt{\\log \\log n}\\right)\\right)$\n(\"quasi-quasi-polynomial\") gap for the standard semidefinite program. \n\n"}
{"id": "1507.06827", "contents": "Title: Egalitarianism of Random Assignment Mechanisms Abstract: We consider the egalitarian welfare aspects of random assignment mechanisms\nwhen agents have unrestricted cardinal utilities over the objects. We give\nbounds on how well different random assignment mechanisms approximate the\noptimal egalitarian value and investigate the effect that different well-known\nproperties like ordinality, envy-freeness, and truthfulness have on the\nachievable egalitarian value. Finally, we conduct detailed experiments\nanalyzing the tradeoffs between efficiency with envy-freeness or truthfulness\nusing two prominent random assignment mechanisms --- random serial dictatorship\nand the probabilistic serial mechanism --- for different classes of utility\nfunctions and distributions. \n\n"}
{"id": "1507.07045", "contents": "Title: The Square Root Agreement Rule for Incentivizing Truthful Feedback on\n  Online Platforms Abstract: A major challenge in obtaining evaluations of products or services on\ne-commerce platforms is eliciting informative responses in the absence of\nverifiability. This paper proposes the Square Root Agreement Rule (SRA): a\nsimple reward mechanism that incentivizes truthful responses to objective\nevaluations on such platforms. In this mechanism, an agent gets a reward for an\nevaluation only if her answer matches that of her peer, where this reward is\ninversely proportional to a popularity index of the answer. This index is\ndefined to be the square root of the empirical frequency at which any two\nagents performing the same evaluation agree on the particular answer across\nevaluations of similar entities operating on the platform. Rarely agreed-upon\nanswers thus earn a higher reward than answers for which agreements are\nrelatively more common.\n  We show that in the many tasks regime, the truthful equilibrium under SRA is\nstrictly payoff-dominant across large classes of natural equilibria that could\narise in these settings, thus increasing the likelihood of its adoption. While\nthere exist other mechanisms achieving such guarantees, they either impose\nadditional assumptions on the response distribution that are not generally\nsatisfied for objective evaluations or they incentivize truthful behavior only\nif each agent performs a prohibitively large number of evaluations and commits\nto using the same strategy for each evaluation. SRA is the first known\nincentive mechanism satisfying such guarantees without imposing any such\nrequirements. Moreover, our empirical findings demonstrate the robustness of\nthe incentive properties of SRA in the presence of mild subjectivity or\nobservational biases in the responses. These properties make SRA uniquely\nattractive for administering reward-based incentive schemes (e.g., rebates,\ndiscounts, reputation scores, etc.) on online platforms. \n\n"}
{"id": "1507.07688", "contents": "Title: Belief and Truth in Hypothesised Behaviours Abstract: There is a long history in game theory on the topic of Bayesian or \"rational\"\nlearning, in which each player maintains beliefs over a set of alternative\nbehaviours, or types, for the other players. This idea has gained increasing\ninterest in the artificial intelligence (AI) community, where it is used as a\nmethod to control a single agent in a system composed of multiple agents with\nunknown behaviours. The idea is to hypothesise a set of types, each specifying\na possible behaviour for the other agents, and to plan our own actions with\nrespect to those types which we believe are most likely, given the observed\nactions of the agents. The game theory literature studies this idea primarily\nin the context of equilibrium attainment. In contrast, many AI applications\nhave a focus on task completion and payoff maximisation. With this perspective\nin mind, we identify and address a spectrum of questions pertaining to belief\nand truth in hypothesised types. We formulate three basic ways to incorporate\nevidence into posterior beliefs and show when the resulting beliefs are\ncorrect, and when they may fail to be correct. Moreover, we demonstrate that\nprior beliefs can have a significant impact on our ability to maximise payoffs\nin the long-term, and that they can be computed automatically with consistent\nperformance effects. Furthermore, we analyse the conditions under which we are\nable complete our task optimally, despite inaccuracies in the hypothesised\ntypes. Finally, we show how the correctness of hypothesised types can be\nascertained during the interaction via an automated statistical analysis. \n\n"}
{"id": "1507.07901", "contents": "Title: A simple and numerically stable primal-dual algorithm for computing\n  Nash-equilibria in sequential games with incomplete information Abstract: We present a simple primal-dual algorithm for computing approximate\nNash-equilibria in two-person zero-sum sequential games with incomplete\ninformation and perfect recall (like Texas Hold'em Poker). Our algorithm is\nnumerically stable, performs only basic iterations (i.e matvec multiplications,\nclipping, etc., and no calls to external first-order oracles, no matrix\ninversions, etc.), and is applicable to a broad class of two-person zero-sum\ngames including simultaneous games and sequential games with incomplete\ninformation and perfect recall. The applicability to the latter kind of games\nis thanks to the sequence-form representation which allows us to encode any\nsuch game as a matrix game with convex polytopial strategy profiles. We prove\nthat the number of iterations needed to produce a Nash-equilibrium with a given\nprecision is inversely proportional to the precision. As proof-of-concept, we\npresent experimental results on matrix games on simplexes and Kuhn Poker. \n\n"}
{"id": "1508.01110", "contents": "Title: Symmetries of matrix multiplication algorithms. I Abstract: In this work the algorithms of fast multiplication of matrices are\nconsidered. To any algorithm there associated a certain group of automorphisms.\nThese automorphism groups are found for some well-known algorithms, including\nalgorithms of Hopcroft, Laderman, and Pan. The automorphism group is isomorphic\nto $S_3\\times Z_2$ and $S_4$ for Hopcroft anf Laderman algorithms,\nrespectively. The studying of symmetry of algorithms may be a fruitful idea for\nfinding fast algorithms, by an analogy with well-known optimization problems\nfor codes, lattices, and graphs.\n  {\\em Keywords}: Strassen algorithm, symmetry, fast matrix multiplication. \n\n"}
{"id": "1508.01818", "contents": "Title: Designing Incentive Schemes For Privacy-Sensitive Users Abstract: Businesses (retailers) often wish to offer personalized advertisements\n(coupons) to individuals (consumers), but run the risk of strong reactions from\nconsumers who want a customized shopping experience but feel their privacy has\nbeen violated. Existing models for privacy such as differential privacy or\ninformation theory try to quantify privacy risk but do not capture the\nsubjective experience and heterogeneous expression of privacy-sensitivity. We\npropose a Markov decision process (MDP) model to capture (i) different consumer\nprivacy sensitivities via a time-varying state; (ii) different coupon types\n(action set) for the retailer; and (iii) the action-and-state-dependent cost\nfor perceived privacy violations. For the simple case with two states (\"Normal\"\nand \"Alerted\"), two coupons (targeted and untargeted) model, and consumer\nbehavior statistics known to the retailer, we show that a stationary\nthreshold-based policy is the optimal coupon-offering strategy for a retailer\nthat wishes to minimize its expected discounted cost. The threshold is a\nfunction of all model parameters; the retailer offers a targeted coupon if\ntheir belief that the consumer is in the \"Alerted\" state is below the\nthreshold. We extend this two-state model to consumers with multiple\nprivacy-sensitivity states as well as coupon-dependent state transition\nprobabilities. Furthermore, we study the case with imperfect (noisy) cost\nfeedback from consumers and uncertain initial belief state. \n\n"}
{"id": "1508.02440", "contents": "Title: Energy Structure of Optimal Positional Strategies in Mean Payoff Games Abstract: This note studies structural aspects concerning Optimal Positional Strategies\n(OPSs) in Mean Payoff Games (MPGs), it is a contribution to understanding the\nrelationship between OPSs in MPGs and Small Energy-Progress Measures (SEPMs) in\nreweighted Energy Games (EGs). Firstly, it is observed that the space of all\nOPSs, $\\texttt{opt}_{\\Gamma}\\Sigma^M_0$, admits a unique complete decomposition\nin terms of so-called extremal-SEPM{s} in reweighted EG{s}; this points out\nwhat we called the \"Energy-Lattice $\\mathcal{X}^*_{\\Gamma}$ of\n$\\texttt{opt}_{\\Gamma}\\Sigma^M_0$\". Secondly, it is offered a pseudo-polynomial\ntotal-time recursive procedure for enumerating (w/o repetitions) all the\nelements of $\\mathcal{X}^*_{\\Gamma}$, and for computing the corresponding\npartitioning of $\\texttt{opt}_{\\Gamma}\\Sigma^M_0$. It is observed that the\ncorresponding recursion tree defines an additional lattice\n$\\mathcal{B}^*_{\\Gamma}$, whose elements are certain subgames $\\Gamma'\\subseteq\n\\Gamma$ that we call basic subgames. The extremal-SEPMs of a given \\MPG\n$\\Gamma$ coincide with the least-SEPMs of the basic subgames of $\\Gamma$; so,\n$\\mathcal{X}^*_{\\Gamma}$ is the energy-lattice comprising all and only the\nleast-SEPMs of the \\emph{basic} subgames of $\\Gamma$. The complexity of the\nproposed enumeration for both $\\mathcal{B}^*_{\\Gamma}$ and\n$\\mathcal{X}^*_{\\Gamma}$ is $O(|V|^3|E|W |\\mathcal{B}^*_{\\Gamma}|)$ total time\nand $O(|V||E|)+\\Theta\\big(|E| \\mathcal{B}^*_{\\Gamma}|\\big)$ working space.\nFinally, it is constructed an \\MPG $\\Gamma$ for which $|\\mathcal{B}^*_{\\Gamma}|\n> |\\mathcal{X}^*_\\Gamma|$, this proves that $\\mathcal{B}^*_{\\Gamma}$ and\n$\\mathcal{X}^*_\\Gamma$ are not isomorphic. \n\n"}
{"id": "1508.03420", "contents": "Title: A strategic timing of arrivals to a linear slowdown processor sharing\n  system Abstract: We consider a discrete population of users with homogeneous service demand\nwho need to decide when to arrive to a system in which the service rate\ndeteriorates linearly with the number of users in the system. The users have\nheterogeneous desired departure times from the system, and their goal is to\nminimize a weighted sum of the travel time and square deviation from the\ndesired departure times. Users join the system sequentially, according to the\norder of their desired departure times. We model this scenario as a\nnon-cooperative game in which each user selects his actual arrival time. We\npresent explicit equilibria solutions for a two-user example, namely the\nsubgame perfect and Nash equilibria and show that multiple equilibria may\nexist. We further explain why a general solution for any number of users is\ncomputationally challenging. The difficulty lies in the fact that the objective\nfunctions are piecewise convex, i.e., non-smooth and non-convex. As a result,\nthe minimization of the costs relies on checking all arrival and departure\norder permutations, which is exponentially large with respect to the population\nsize. Instead we propose an iterated best-response algorithm which can be\nefficiently studied numerically. Finally, we compare the equilibrium arrival\nprofiles to a socially optimal solution and discuss the implications. \n\n"}
{"id": "1508.03735", "contents": "Title: Coordination Complexity: Small Information Coordinating Large\n  Populations Abstract: We initiate the study of a quantity that we call coordination complexity. In\na distributed optimization problem, the information defining a problem instance\nis distributed among $n$ parties, who need to each choose an action, which\njointly will form a solution to the optimization problem. The coordination\ncomplexity represents the minimal amount of information that a centralized\ncoordinator, who has full knowledge of the problem instance, needs to broadcast\nin order to coordinate the $n$ parties to play a nearly optimal solution.\n  We show that upper bounds on the coordination complexity of a problem imply\nthe existence of good jointly differentially private algorithms for solving\nthat problem, which in turn are known to upper bound the price of anarchy in\ncertain games with dynamically changing populations.\n  We show several results. We fully characterize the coordination complexity\nfor the problem of computing a many-to-one matching in a bipartite graph by\ngiving almost matching lower and upper bounds.Our upper bound in fact extends\nmuch more generally, to the problem of solving a linearly separable convex\nprogram. We also give a different upper bound technique, which we use to bound\nthe coordination complexity of coordinating a Nash equilibrium in a routing\ngame, and of computing a stable matching. \n\n"}
{"id": "1508.05013", "contents": "Title: Message Passing and Combinatorial Optimization Abstract: Graphical models use the intuitive and well-studied methods of graph theory\nto implicitly represent dependencies between variables in large systems. They\ncan model the global behaviour of a complex system by specifying only local\nfactors. This thesis studies inference in discrete graphical models from an\nalgebraic perspective and the ways inference can be used to express and\napproximate NP-hard combinatorial problems.\n  We investigate the complexity and reducibility of various inference problems,\nin part by organizing them in an inference hierarchy. We then investigate\ntractable approximations for a subset of these problems using distributive law\nin the form of message passing. The quality of the resulting message passing\nprocedure, called Belief Propagation (BP), depends on the influence of loops in\nthe graphical model. We contribute to three classes of approximations that\nimprove BP for loopy graphs A) loop correction techniques; B) survey\npropagation, another message passing technique that surpasses BP in some\nsettings; and C) hybrid methods that interpolate between deterministic message\npassing and Markov Chain Monte Carlo inference.\n  We then review the existing message passing solutions and provide novel\ngraphical models and inference techniques for combinatorial problems under\nthree broad classes: A) constraint satisfaction problems such as\nsatisfiability, coloring, packing, set / clique-cover and dominating /\nindependent set and their optimization counterparts; B) clustering problems\nsuch as hierarchical clustering, K-median, K-clustering, K-center and\nmodularity optimization; C) problems over permutations including assignment,\ngraph morphisms and alignment, finding symmetries and traveling salesman\nproblem. In many cases we show that message passing is able to find solutions\nthat are either near optimal or favourably compare with today's\nstate-of-the-art approaches. \n\n"}
{"id": "1508.05282", "contents": "Title: Lower bounds for the parameterized complexity of Minimum Fill-in and\n  other completion problems Abstract: In this work, we focus on several completion problems for subclasses of\nchordal graphs: Minimum Fill-In, Interval Completion, Proper Interval\nCompletion, Threshold Completion, and Trivially Perfect Completion. In these\nproblems, the task is to add at most k edges to a given graph in order to\nobtain a chordal, interval, proper interval, threshold, or trivially perfect\ngraph, respectively. We prove the following lower bounds for all these\nproblems, as well as for the related Chain Completion problem: Assuming the\nExponential Time Hypothesis, none of these problems can be solved in time\n2^O(n^(1/2) / log^c n) or 2^O(k^(1/4) / log^c k) n^O(1), for some integer c.\nAssuming the non-existence of a subexponential-time approximation scheme for\nMin Bisection on d-regular graphs, for some constant d, none of these problems\ncan be solved in time 2^o(n) or 2^o(sqrt(k)) n^O(1).\n  For all the aforementioned completion problems, apart from Proper Interval\nCompletion, FPT algorithms with running time of the form 2^O(sqrt(k) log k)\nn^O(1) are known. Thus, the second result proves that a significant improvement\nof any of these algorithms would lead to a surprising breakthrough in the\ndesign of approximation algorithms for Min Bisection.\n  To prove our results, we use a reduction methodology based on combining the\nclassic approach of starting with a sparse instance of 3-Sat, prepared using\nthe Sparsification Lemma, with the existence of almost linear-size\nProbabilistically Checkable Proofs (PCPs). Apart from our main results, we also\nobtain lower bounds excluding the existence of subexponential algorithms for\nthe Optimum Linear Arrangement problem, as well as improved, yet still not\ntight, lower bounds for Feedback Arc Set in Tournaments. \n\n"}
{"id": "1508.05288", "contents": "Title: Dynamics of Human Cooperation in Economic Games Abstract: Human decision behaviour is quite diverse. In many games humans on average do\nnot achieve maximal payoff and the behaviour of individual players remains\ninhomogeneous even after playing many rounds. For instance, in repeated\nprisoner dilemma games humans do not always optimize their mean reward and\nfrequently exhibit broad distributions of cooperativity. The reasons for these\nfailures of maximization are not known. Here we show that the dynamics\nresulting from the tendency to shift choice probabilities towards previously\nrewarding choices in closed loop interaction with the strategy of the opponent\ncan not only explain systematic deviations from 'rationality', but also\nreproduce the diversity of choice behaviours. As a representative example we\ninvestigate the dynamics of choice probabilities in prisoner dilemma games with\nopponents using strategies with different degrees of extortion and generosity.\nWe find that already a simple model for human learning can account for a\nsurprisingly wide range of human decision behaviours. It reproduces suppression\nof cooperation against extortionists and increasing cooperation when playing\nwith generous opponents, explains the broad distributions of individual choices\nin ensembles of players, and predicts the evolution of individual subjects'\ncooperation rates over the course of the games. We conclude that important\naspects of human decision behaviours are rooted in elementary learning\nmechanisms realised in the brain. \n\n"}
{"id": "1508.06420", "contents": "Title: The Stable Fixtures Problem with Payments Abstract: We generalize two well-known game-theoretic models by introducing multiple\npartners matching games, defined by a graph $G=(N,E)$, with an integer vertex\ncapacity function $b$ and an edge weighting $w$. The set $N$ consists of a\nnumber of players that are to form a set $M\\subseteq E$ of 2-player coalitions\n$ij$ with value $w(ij)$, such that each player $i$ is in at most $b(i)$\ncoalitions. A payoff vector is a mapping $p: N \\times N \\rightarrow {\\mathbb\nR}$ with $p(i,j)+p(j,i)=w(ij)$ if $ij\\in M$ and $p(i,j)=p(j,i)=0$ if $ij\\notin\nM$. The pair $(M,p)$ is called a solution. A pair of players $i,j$ with $ij\\in\nE\\setminus M$ blocks a solution $(M,p)$ if $i,j$ can form, possibly only after\nwithdrawing from one of their existing 2-player coalitions, a new 2-player\ncoalition in which they are mutually better off. A solution is stable if it has\nno blocking pairs.\n  We give a polynomial-time algorithm that either finds that a given multiple\npartners matching game has no stable solution, or obtains a stable solution for\nit. We characterize the set of stable solutions of a multiple partners matching\ngame in two different ways and show how this leads to simple proofs for a\nnumber of known results of Sotomayor (1992,1999,2007) for multiple partners\nssignment games and to generalizations of some of these results to multiple\npartners matching games. We also perform a study on the core of the\ncorresponding cooperative game, where coalitions of any size may be formed. In\nparticular we show that the standard relation between the existence of a stable\nsolution and the non-emptiness of the core, which holds in the other models\nwith payments, is no longer valid for our (most general) model. We also prove\nthat the problem of deciding if an allocation belongs to the core jumps from\nbeing polynomial-time solvable for $b\\leq 2$ to NP-complete for $b\\equiv 3$. \n\n"}
{"id": "1508.07338", "contents": "Title: A linear time algorithm for quantum 2-SAT Abstract: The Boolean constraint satisfaction problem 3-SAT is arguably the canonical\nNP-complete problem. In contrast, 2-SAT can not only be decided in polynomial\ntime, but in fact in deterministic linear time. In 2006, Bravyi proposed a\nphysically motivated generalization of k-SAT to the quantum setting, defining\nthe problem \"quantum k-SAT\". He showed that quantum 2-SAT is also solvable in\npolynomial time on a classical computer, in particular in deterministic time\nO(n^4), assuming unit-cost arithmetic over a field extension of the rational\nnumbers, where n is number of variables. In this paper, we present an algorithm\nfor quantum 2-SAT which runs in linear time, i.e. deterministic time O(n+m) for\nn and m the number of variables and clauses, respectively. Our approach\nexploits the transfer matrix techniques of Laumann et al. [QIC, 2010] used in\nthe study of phase transitions for random quantum 2-SAT, and bears similarities\nwith both the linear time 2-SAT algorithms of Even, Itai, and Shamir (based on\nbacktracking) [SICOMP, 1976] and Aspvall, Plass, and Tarjan (based on strongly\nconnected components) [IPL, 1979]. \n\n"}
{"id": "1509.00092", "contents": "Title: Explicit resilient functions matching Ajtai-Linial Abstract: A Boolean function on n variables is q-resilient if for any subset of at most\nq variables, the function is very likely to be determined by a uniformly random\nassignment to the remaining n-q variables; in other words, no coalition of at\nmost q variables has significant influence on the function. Resilient functions\nhave been extensively studied with a variety of applications in cryptography,\ndistributed computing, and pseudorandomness. The best known balanced resilient\nfunction on n variables due to Ajtai and Linial ([AL93]) is Omega(n/(log^2\nn))-resilient. However, the construction of Ajtai and Linial is by the\nprobabilistic method and does not give an efficiently computable function.\n  In this work we give an explicit monotone depth three almost-balanced Boolean\nfunction on n bits that is Omega(n/(log^2 n))-resilient matching the work of\nAjtai and Linial. The best previous explicit construction due to Meka [Meka09]\n(which only gives a logarithmic depth function) and Chattopadhyay and\nZuckermman [CZ15] were only n^{1-c}-resilient for any constant c < 1. Our\nconstruction and analysis are motivated by (and simplifies parts of) the recent\nbreakthrough of [CZ15] giving explicit two-sources extractors for\npolylogarithmic min-entropy; a key ingredient in their result was the\nconstruction of explicit constant-depth resilient functions.\n  An important ingredient in our construction is a new randomness optimal\noblivious sampler which preserves moment generating functions of sums of\nvariables and could be useful elsewhere. \n\n"}
{"id": "1509.02023", "contents": "Title: Lipschitz Continuity and Approximate Equilibria Abstract: In this paper, we study games with continuous action spaces and non-linear\npayoff functions. Our key insight is that Lipschitz continuity of the payoff\nfunction allows us to provide algorithms for finding approximate equilibria in\nthese games. We begin by studying Lipschitz games, which encompass, for\nexample, all concave games with Lipschitz continuous payoff functions. We\nprovide an efficient algorithm for computing approximate equilibria in these\ngames. Then we turn our attention to penalty games, which encompass biased\ngames and games in which players take risk into account. Here we show that if\nthe penalty function is Lipschitz continuous, then we can provide a\nquasi-polynomial time approximation scheme. Finally, we study distance biased\ngames, where we present simple strongly polynomial time algorithms for finding\nbest responses in $L_1$, $L_2^2$, and $L_\\infty$ biased games, and then use\nthese algorithms to provide strongly polynomial algorithms that find $2/3$,\n$5/7$, and $2/3$ approximations for these norms, respectively. \n\n"}
{"id": "1509.03327", "contents": "Title: Optimal Strategy in \"Guess Who?\": Beyond Binary Search Abstract: \"Guess Who?\" is a popular two player game where players ask \"Yes\"/\"No\"\nquestions to search for their opponent's secret identity from a pool of\npossible candidates. This is modeled as a simple stochastic game. Using this\nmodel, the optimal strategy is explicitly found. Contrary to popular belief,\nperforming a binary search is \\emph{not} always optimal. Instead, the optimal\nstrategy for the player who trails is to make certain bold plays in an attempt\ncatch up. This is discovered by first analyzing a continuous version of the\ngame where players play indefinitely and the winner is never decided after\nfinitely many rounds. \n\n"}
{"id": "1509.03915", "contents": "Title: An Impossibility Result for Housing Markets with Fractional Endowments Abstract: The housing market setting constitutes a fundamental model of exchange\neconomies of goods. Most of the work concerning housing markets does not cater\nfor randomized assignments or allocation of time-shares. House allocation with\nfractional endowments of houses was considered by Athanassoglou and Sethuraman\n(2011) who posed the open problem whether individual rationality, weak\nstrategyproofness, and efficiency are compatible for the setting. We show that\nthe three axioms are incompatible. \n\n"}
{"id": "1509.04344", "contents": "Title: Stable Nash Equilibria in the Gale-Shapley Matching Game Abstract: In this article we study the stable marriage game induced by the\nmen-proposing Gale-Shapley algorithm. Our setting is standard: all the lists\nare complete and the matching mechanism is the men-proposing Gale-Shapley\nalgorithm. It is well known that in this setting, men cannot cheat, but women\ncan. In fact, Teo, Sethuraman and Tan \\cite{TST01}, show that there is a\npolynomial time algorithm to obtain, for a given strategy (the set of all\nlists) $Q$ and a woman $w$, the best partner attainable by changing her list.\nHowever, what if the resulting matching is not stable with respect to $Q$?\nObviously, such a matching would be vulnerable to further manipulation, but is\nnot mentioned in \\cite{TST01}. In this paper, we consider (safe) manipulation\nthat implies a stable matching in a most general setting. Specifically, our\ngoal is to decide for a given $Q$, if w can manipulate her list to obtain a\nstrictly better partner with respect to the true strategy $P$ (which may be\ndifferent from $Q$), and also the outcome is a stable matching for $P$. \n\n"}
{"id": "1509.04595", "contents": "Title: Are there any nicely structured preference~profiles~nearby? Abstract: We investigate the problem of deciding whether a given preference profile is\nclose to having a certain nice structure, as for instance single-peaked,\nsingle-caved, single-crossing, value-restricted, best-restricted,\nworst-restricted, medium-restricted, or group-separable profiles. We measure\nthis distance by the number of voters or alternatives that have to be deleted\nto make the profile a nicely structured one. Our results classify the problem\nvariants with respect to their computational complexity, and draw a clear line\nbetween computationally tractable (polynomial-time solvable) and\ncomputationally intractable (NP-hard) questions. \n\n"}
{"id": "1509.05065", "contents": "Title: Estimating operator norms using covering nets Abstract: We present several polynomial- and quasipolynomial-time approximation schemes\nfor a large class of generalized operator norms. Special cases include the\n$2\\rightarrow q$ norm of matrices for $q>2$, the support function of the set of\nseparable quantum states, finding the least noisy output of\nentanglement-breaking quantum channels, and approximating the injective tensor\nnorm for a map between two Banach spaces whose factorization norm through\n$\\ell_1^n$ is bounded.\n  These reproduce and in some cases improve upon the performance of previous\nalgorithms by Brand\\~ao-Christandl-Yard and followup work, which were based on\nthe Sum-of-Squares hierarchy and whose analysis used techniques from quantum\ninformation such as the monogamy principle of entanglement. Our algorithms, by\ncontrast, are based on brute force enumeration over carefully chosen covering\nnets. These have the advantage of using less memory, having much simpler proofs\nand giving new geometric insights into the problem. Net-based algorithms for\nsimilar problems were also presented by Shi-Wu and Barak-Kelner-Steurer, but in\neach case with a run-time that is exponential in the rank of some matrix. We\nachieve polynomial or quasipolynomial runtimes by using the much smaller nets\nthat exist in $\\ell_1$ spaces. This principle has been used in learning theory,\nwhere it is known as Maurey's empirical method. \n\n"}
{"id": "1509.05896", "contents": "Title: On space efficiency of algorithms working on structural decompositions\n  of graphs Abstract: Dynamic programming on path and tree decompositions of graphs is a technique\nthat is ubiquitous in the field of parameterized and exponential-time\nalgorithms. However, one of its drawbacks is that the space usage is\nexponential in the decomposition's width. Following the work of Allender et al.\n[Theory of Computing, '14], we investigate whether this space complexity\nexplosion is unavoidable. Using the idea of reparameterization of Cai and\nJuedes [J. Comput. Syst. Sci., '03], we prove that the question is closely\nrelated to a conjecture that the Longest Common Subsequence problem\nparameterized by the number of input strings does not admit an algorithm that\nsimultaneously uses XP time and FPT space. Moreover, we complete the complexity\nlandscape sketched for pathwidth and treewidth by Allender et al. by\nconsidering the parameter tree-depth. We prove that computations on tree-depth\ndecompositions correspond to a model of non-deterministic machines that work in\npolynomial time and logarithmic space, with access to an auxiliary stack of\nmaximum height equal to the decomposition's depth. Together with the results of\nAllender et al., this describes a hierarchy of complexity classes for\npolynomial-time non-deterministic machines with different restrictions on the\naccess to working space, which mirrors the classic relations between treewidth,\npathwidth, and tree-depth. \n\n"}
{"id": "1509.06257", "contents": "Title: Communication Complexity (for Algorithm Designers) Abstract: This document collects the lecture notes from my course \"Communication\nComplexity (for Algorithm Designers),'' taught at Stanford in the winter\nquarter of 2015. The two primary goals of the course are: 1. Learn several\ncanonical problems that have proved the most useful for proving lower bounds\n(Disjointness, Index, Gap-Hamming, etc.). 2. Learn how to reduce lower bounds\nfor fundamental algorithmic problems to communication complexity lower bounds.\nAlong the way, we'll also: 3. Get exposure to lots of cool computational models\nand some famous results about them --- data streams and linear sketches,\ncompressive sensing, space-query time trade-offs in data structures,\nsublinear-time algorithms, and the extension complexity of linear programs. 4.\nScratch the surface of techniques for proving communication complexity lower\nbounds (fooling sets, corruption bounds, etc.). \n\n"}
{"id": "1509.07345", "contents": "Title: Composite charging games in networks of electric vehicles Abstract: An important scenario for smart grids which encompass distributed electrical\nnetworks is given by the simultaneous presence of aggregators and individual\nconsumers. In this work, an aggregator is seen as an entity (a coalition) which\nis able to manage jointly the energy demand of a large group of consumers or\nusers. More precisely, the demand consists in charging an electrical vehicle\n(EV) battery. The way the EVs user charge their batteries matters since it\nstrongly impacts the network, especially the distribution network costs (e.g.,\nin terms of Joule losses or transformer ageing). Since the charging policy is\nchosen by the users or the aggregators, the charging problem is naturally\ndistributed. It turns out that one of the tools suited to tackle this\nheterogenous scenario has been introduced only recently namely, through the\nnotion of composite games. This paper exploits for the first time in the\nliterature of smart grids the notion of composite game and equilibrium. By\nassuming a rectangular charging profile for an EV, a composite equilibrium\nanalysis is conducted, followed by a detailed analysis of a case study which\nassumes three possible charging periods or time-slots. Both the provided\nanalytical and numerical results allow one to better understand the\nrelationship between the size (which is a measure) of the coalition and the\nnetwork sum-cost. In particular, a social dilemma, a situation where everybody\nprefers unilaterally defecting to cooperating, while the consequence is the\nworst for all, is exhibited. \n\n"}
{"id": "1509.07495", "contents": "Title: Unbounded Lookahead in WMSO+U Games Abstract: Delay games are two-player games of infinite duration in which one player may\ndelay her moves to obtain a lookahead on her opponent's moves. We consider\ndelay games with winning conditions expressed in weak monadic second order\nlogic with the unbounding quantifier (WMSO+U), which is able to express\n(un)boundedness properties. It is decidable whether the delaying player is able\nto win such a game with bounded lookahead, i.e., if she only skips a finite\nnumber of moves.\n  However, bounded lookahead is not always sufficient: we present a game that\ncan be won with unbounded lookahead, but not with bounded lookahead. Then, we\nconsider WMSO+U delay games with unbounded lookahead and show that the exact\nevolution of the lookahead is irrelevant: the winner is always the same, as\nlong as the initial lookahead is large enough and the lookahead tends to\ninfinity. \n\n"}
{"id": "1509.07599", "contents": "Title: Cooperative Equilibrium beyond Social Dilemmas: Pareto Solvable Games Abstract: A recently introduced concept of \"cooperative equilibrium\", based on the\nassumption that players have a natural attitude to cooperation, has been proven\na powerful tool in predicting human behaviour in social dilemmas. In this\npaper, we extend this idea to more general game models, termed \"Pareto\nsolvable\" games, which in particular include the Nash Bargaining Problem and\nthe Ultimatum Game. We show that games in this class possess a unique pure\ncooperative equilibrium. Furthermore, for the Ultimatum Game, this notion\nappears to be strongly correlated with a suitably defined variant of the\nDictator Game. We support this observation with the results of a behavioural\nexperiment conducted using Amazon Mechanical Turk, which demonstrates that our\napproach allows for making statistically precise predictions of average\nbehaviour in such settings. \n\n"}
{"id": "1509.08193", "contents": "Title: Budget-Constrained Contract Design for Effort-Averse Sensors in\n  Averaging Based Estimation Abstract: Consider a group of effort-averse, or lazy, sensors that seek to minimize the\neffort invested to collect measurements of a variable. Increasing the effort\ninvested by the sensors improves the quality of the measurements provided to\nthe central planner but this incurs increased costs to the sensors. The central\nplanner, which processes the sensor measurements, employs an averaging\nestimator. It also determines contracts for rewarding sensors based on the\nmeasurements obtained. The problem of designing a contract that yields an\nestimation-error based quality-of-service level in return for the reward\nextended to sensors is investigated in this paper. To this end, a game is\nformulated between the central planner and the sensors. Conditions for the\nexistence and uniqueness of an equilibrium are identified. The equilibrium is\nconstructed explicitly and its properties in response to a reward based\ncontract are studied. It turns out that the central planner, while not being\nable to directly measure the effort invested by the sensors, can enhance the\nestimation quality by rewarding each sensor based on the distance of its\nmeasurements from the output of the averaging estimator. Ultimately, optimal\ncontracts are designed from the perspective of the budget required for\nachieving a specified level of estimation error. \n\n"}
{"id": "1510.00764", "contents": "Title: Information-Theoretic Approach to Strategic Communication as a\n  Hierarchical Game Abstract: This paper analyzes the information disclosure problems originated in\neconomics through the lens of information theory. Such problems are radically\ndifferent from the conventional communication paradigms in information theory\nsince they involve different objectives for the encoder and the decoder, which\nare aware of this mismatch and act accordingly. This leads, in our setting, to\na hierarchical communication game, where the transmitter announces an encoding\nstrategy with full commitment, and its distortion measure depends on a private\ninformation sequence whose realization is available at the transmitter. The\nreceiver decides on its decoding strategy that minimizes its own distortion\nbased on the announced encoding map and the statistics. Three problem settings\nare considered, focusing on the quadratic distortion measures, and jointly\nGaussian source and private information: compression, communication, and the\nsimple equilibrium conditions without any compression or communication. The\nequilibrium strategies and associated costs are characterized. The analysis is\nthen extended to the receiver side information setting and the major changes in\nthe structure of optimal strategies are identified. Finally, an extension of\nthe results to the broader context of decentralized stochastic control is\npresented. \n\n"}
{"id": "1510.00781", "contents": "Title: Prospect Pricing in Cognitive Radio Networks Abstract: Advances in cognitive radio networks have primarily focused on the design of\nspectrally agile radios and novel spectrum sharing techniques that are founded\non Expected Utility Theory (EUT). In this paper, we consider the development of\nnovel spectrum sharing algorithms in such networks taking into account human\npsychological behavior of the end-users, which often deviates from EUT.\nSpecifically, we consider the impact of end-user decision making on pricing and\nmanagement of radio resources in a cognitive radio enabled network when there\nis uncertainty in the Quality of Service (QoS) guarantees offered by the\nService Provider (SP). Using Prospect Theory (a Nobel-Prize-winning behavioral\neconomic theory that captures human decision making and its deviation from\nEUT), we design data pricing and channel allocation algorithms for use in\ncognitive radio networks by formulating a game theoretic analysis of the\ninterplay between the price offerings, bandwidth allocation by the SP and the\nservice choices made by end-users. We show that, when the end-users\nunder-weight the service guarantee, they tend to reject the offer which results\nin under-utilization of radio resources and revenue loss. We propose prospect\npricing, a pricing mechanism that can make the system robust to decision making\nand improve radio resource management. We present analytical results as well as\npreliminary human subject studies with video QoS. \n\n"}
{"id": "1510.01891", "contents": "Title: On the Hardest Problem Formulations for the 0/1 Lasserre Hierarchy Abstract: The Lasserre/Sum-of-Squares (SoS) hierarchy is a systematic procedure for\nconstructing a sequence of increasingly tight semidefinite relaxations. It is\nknown that the hierarchy converges to the 0/1 polytope in n levels and captures\nthe convex relaxations used in the best available approximation algorithms for\na wide variety of optimization problems.\n  In this paper we characterize the set of 0/1 integer linear problems and\nunconstrained 0/1 polynomial optimization problems that can still have an\nintegrality gap at level n-1. These problems are the hardest for the Lasserre\nhierarchy in this sense. \n\n"}
{"id": "1510.02045", "contents": "Title: Budget Constraints in Prediction Markets Abstract: We give a detailed characterization of optimal trades under budget\nconstraints in a prediction market with a cost-function-based automated market\nmaker. We study how the budget constraints of individual traders affect their\nability to impact the market price. As a concrete application of our\ncharacterization, we give sufficient conditions for a property we call budget\nadditivity: two traders with budgets B and B' and the same beliefs would have a\ncombined impact equal to a single trader with budget B+B'. That way, even if a\nsingle trader cannot move the market much, a crowd of like-minded traders can\nhave the same desired effect. When the set of payoff vectors associated with\noutcomes, with coordinates corresponding to securities, is affinely\nindependent, we obtain that a generalization of the heavily-used logarithmic\nmarket scoring rule is budget additive, but the quadratic market scoring rule\nis not. Our results may be used both descriptively, to understand if a\nparticular market maker is affected by budget constraints or not, and\nprescriptively, as a recipe to construct markets. \n\n"}
{"id": "1510.03399", "contents": "Title: Selling Two Goods Optimally Abstract: We provide sufficient conditions for revenue maximization in a two-good\nmonopoly where the buyer's values for the items come from independent (but not\nnecessarily identical) distributions over bounded intervals. Under certain\ndistributional assumptions, we give exact, closed-form formulas for the prices\nand allocation rule of the optimal selling mechanism. As a side result we give\nthe first example of an optimal mechanism in an i.i.d. setting over a support\nof the form $[0,b]$ which is not deterministic. Since our framework is based on\nduality techniques, we were also able to demonstrate how slightly relaxed\nversions of it can still be used to design mechanisms that have very good\napproximation ratios with respect to the optimal revenue, through a\n\"convexification\" process. \n\n"}
{"id": "1510.03495", "contents": "Title: Privacy Constrained Information Processing Abstract: This paper studies communication scenarios where the transmitter and the\nreceiver have different objectives due to privacy concerns, in the context of a\nvariation of the strategic information transfer (SIT) model of Sobel and\nCrawford. We first formulate the problem as the minimization of a common\ndistortion by the transmitter and the receiver subject to a privacy constrained\ntransmitter. We show the equivalence of this formulation to a Stackelberg\nequilibrium of the SIT problem. Assuming an entropy based privacy measure, a\nquadratic distortion measure and jointly Gaussian variables, we characterize\nthe Stackelberg equilibrium. Next, we consider asymptotically optimal\ncompression at the transmitter which inherently provides some level of privacy,\nand study equilibrium conditions. We finally analyze the impact of the presence\nof an average power constrained Gaussian communication channel between the\ntransmitter and the receiver on the equilibrium conditions. \n\n"}
{"id": "1510.03903", "contents": "Title: Fair Cake-Cutting among Families Abstract: We study the fair division of a continuous resource, such as a land-estate or\na time-interval, among pre-specified groups of agents, such as families. Each\nfamily is given a piece of the resource and this piece is used simultaneously\nby all family members, while different members may have different value\nfunctions. Three ways to assess the fairness of such a division are examined.\n(a) Average Fairness means that each family's share is fair according to the\n\"family value function\", defined as the arithmetic mean of the value functions\nof the family members. (b) Unanimous Fairness means that all members in all\nfamilies feel that their family's share is fair according to their personal\nvalue function. (c) Democratic Fairness means that in each family, at least a\nfixed fraction (e.g. a half) of the members feel that their family's share is\nfair. We compare these criteria based on the number of connected components in\nthe resulting division and on their compatibility with Pareto-efficiency. \n\n"}
{"id": "1510.04622", "contents": "Title: Subtree Isomorphism Revisited Abstract: The Subtree Isomorphism problem asks whether a given tree is contained in\nanother given tree. The problem is of fundamental importance and has been\nstudied since the 1960s. For some variants, e.g., ordered trees, near-linear\ntime algorithms are known, but for the general case truly subquadratic\nalgorithms remain elusive.\n  Our first result is a reduction from the Orthogonal Vectors problem to\nSubtree Isomorphism, showing that a truly subquadratic algorithm for the latter\nrefutes the Strong Exponential Time Hypothesis (SETH).\n  In light of this conditional lower bound, we focus on natural special cases\nfor which no truly subquadratic algorithms are known. We classify these cases\nagainst the quadratic barrier, showing in particular that:\n  -- Even for binary, rooted trees, a truly subquadratic algorithm refutes\nSETH.\n  -- Even for rooted trees of depth $O(\\log\\log{n})$, where $n$ is the total\nnumber of vertices, a truly subquadratic algorithm refutes SETH.\n  -- For every constant $d$, there is a constant $\\epsilon_d>0$ and a\nrandomized, truly subquadratic algorithm for degree-$d$ rooted trees of depth\nat most $(1+ \\epsilon_d) \\log_{d}{n}$. In particular, there is an $O(\\min\\{\n2.85^h ,n^2 \\})$ algorithm for binary trees of depth $h$.\n  Our reductions utilize new \"tree gadgets\" that are likely useful for future\nSETH-based lower bounds for problems on trees. Our upper bounds apply a\nfolklore result from randomized decision tree complexity. \n\n"}
{"id": "1510.04991", "contents": "Title: Honest signaling in zero-sum games is hard, and lying is even harder Abstract: We prove that, assuming the exponential time hypothesis, finding an\n\\epsilon-approximately optimal symmetric signaling scheme in a two-player\nzero-sum game requires quasi-polynomial time. This is tight by [Cheng et al.,\nFOCS'15] and resolves an open question of [Dughmi, FOCS'14]. We also prove that\nfinding a multiplicative approximation is NP-hard.\n  We also introduce a new model where a dishonest signaler may publicly commit\nto use one scheme, but post signals according to a different scheme. For this\nmodel, we prove that even finding a (1-2^{-n})-approximately optimal scheme is\nNP-hard. \n\n"}
{"id": "1510.05229", "contents": "Title: Monotonicity and Competitive Equilibrium in Cake-cutting Abstract: We study the monotonicity properties of solutions in the classic problem of\nfair cake-cutting --- dividing a heterogeneous resource among agents with\ndifferent preferences. Resource- and population-monotonicity relate to\nscenarios where the cake, or the number of participants who divide the cake,\nchanges. It is required that the utility of all participants change in the same\ndirection: either all of them are better-off (if there is more to share or\nfewer to share among) or all are worse-off (if there is less to share or more\nto share among).\n  We formally introduce these concepts to the cake-cutting problem and examine\nwhether they are satisfied by various common division rules. We prove that the\nNash-optimal rule, which maximizes the product of utilities, is\nresource-monotonic and population-monotonic, in addition to being\nPareto-optimal, envy-free and satisfying a strong competitive-equilibrium\ncondition. Moreover, we prove that it is the only rule among a natural family\nof welfare-maximizing rules that is both proportional and resource-monotonic. \n\n"}
{"id": "1510.07825", "contents": "Title: Span-program-based quantum algorithms for graph bipartiteness and\n  connectivity Abstract: Span program is a linear-algebraic model of computation which can be used to\ndesign quantum algorithms. For any Boolean function there exists a span program\nthat leads to a quantum algorithm with optimal quantum query complexity. In\ngeneral, finding such span programs is not an easy task. In this work, given a\nquery access to the adjacency matrix of a simple graph $G$ with $n$ vertices,\nwe provide two new span-program-based quantum algorithms: an algorithm for\ntesting if the graph is bipartite that uses $O(n\\sqrt{n})$ quantum queries; an\nalgorithm for testing if the graph is connected that uses $O(n\\sqrt{n})$\nquantum queries. \n\n"}
{"id": "1511.01411", "contents": "Title: Learning in Auctions: Regret is Hard, Envy is Easy Abstract: A line of recent work provides welfare guarantees of simple combinatorial\nauction formats, such as selling m items via simultaneous second price auctions\n(SiSPAs) (Christodoulou et al. 2008, Bhawalkar and Roughgarden 2011, Feldman et\nal. 2013). These guarantees hold even when the auctions are repeatedly executed\nand players use no-regret learning algorithms. Unfortunately, off-the-shelf\nno-regret algorithms for these auctions are computationally inefficient as the\nnumber of actions is exponential. We show that this obstacle is insurmountable:\nthere are no polynomial-time no-regret algorithms for SiSPAs, unless\nRP$\\supseteq$ NP, even when the bidders are unit-demand. Our lower bound raises\nthe question of how good outcomes polynomially-bounded bidders may discover in\nsuch auctions.\n  To answer this question, we propose a novel concept of learning in auctions,\ntermed \"no-envy learning.\" This notion is founded upon Walrasian equilibrium,\nand we show that it is both efficiently implementable and results in\napproximately optimal welfare, even when the bidders have fractionally\nsubadditive (XOS) valuations (assuming demand oracles) or coverage valuations\n(without demand oracles). No-envy learning outcomes are a relaxation of\nno-regret outcomes, which maintain their approximate welfare optimality while\nendowing them with computational tractability. Our results extend to other\nauction formats that have been studied in the literature via the smoothness\nparadigm.\n  Our results for XOS valuations are enabled by a novel\nFollow-The-Perturbed-Leader algorithm for settings where the number of experts\nis infinite, and the payoff function of the learner is non-linear. This\nalgorithm has applications outside of auction settings, such as in security\ngames. Our result for coverage valuations is based on a novel use of convex\nrounding schemes and a reduction to online convex optimization. \n\n"}
{"id": "1511.01699", "contents": "Title: Low Rank Approximation of Binary Matrices: Column Subset Selection and\n  Generalizations Abstract: Low rank matrix approximation is an important tool in machine learning. Given\na data matrix, low rank approximation helps to find factors, patterns and\nprovides concise representations for the data. Research on low rank\napproximation usually focus on real matrices. However, in many applications\ndata are binary (categorical) rather than continuous. This leads to the problem\nof low rank approximation of binary matrix. Here we are given a $d \\times n$\nbinary matrix $A$ and a small integer $k$. The goal is to find two binary\nmatrices $U$ and $V$ of sizes $d \\times k$ and $k \\times n$ respectively, so\nthat the Frobenius norm of $A - U V$ is minimized. There are two models of this\nproblem, depending on the definition of the dot product of binary vectors: The\n$\\mathrm{GF}(2)$ model and the Boolean semiring model. Unlike low rank\napproximation of real matrix which can be efficiently solved by Singular Value\nDecomposition, approximation of binary matrix is $NP$-hard even for $k=1$.\n  In this paper, we consider the problem of Column Subset Selection (CSS), in\nwhich one low rank matrix must be formed by $k$ columns of the data matrix. We\ncharacterize the approximation ratio of CSS for binary matrices. For $GF(2)$\nmodel, we show the approximation ratio of CSS is bounded by\n$\\frac{k}{2}+1+\\frac{k}{2(2^k-1)}$ and this bound is asymptotically tight. For\nBoolean model, it turns out that CSS is no longer sufficient to obtain a bound.\nWe then develop a Generalized CSS (GCSS) procedure in which the columns of one\nlow rank matrix are generated from Boolean formulas operating bitwise on\ncolumns of the data matrix. We show the approximation ratio of GCSS is bounded\nby $2^{k-1}+1$, and the exponential dependency on $k$ is inherent. \n\n"}
{"id": "1511.02235", "contents": "Title: NAND-Trees, Average Choice Complexity, and Effective Resistance Abstract: We show that the quantum query complexity of evaluating NAND-tree instances\nwith average choice complexity at most $W$ is $O(W)$, where average choice\ncomplexity is a measure of the difficulty of winning the associated two-player\ngame. This generalizes a superpolynomial speedup over classical query\ncomplexity due to Zhan et al. [Zhan et al., ITCS 2012, 249-265]. We further\nshow that the player with a winning strategy for the two-player game associated\nwith the NAND-tree can win the game with an expected\n$\\widetilde{O}(N^{1/4}\\sqrt{{\\cal C}(x)})$ quantum queries against a random\nopponent, where ${\\cal C }(x)$ is the average choice complexity of the\ninstance. This gives an improvement over the query complexity of the naive\nstrategy, which costs $\\widetilde{O}(\\sqrt{N})$ queries.\n  The results rely on a connection between NAND-tree evaluation and\n$st$-connectivity problems on certain graphs, and span programs for\n$st$-connectivity problems. Our results follow from relating average choice\ncomplexity to the effective resistance of these graphs, which itself\ncorresponds to the span program witness size. \n\n"}
{"id": "1511.02537", "contents": "Title: Exponential Segregation in a Two-Dimensional Schelling Model with\n  Tolerant Individuals Abstract: We prove that the two-dimensional Schelling segregation model yields\nmonochromatic regions of size exponential in the area of individuals'\nneighborhoods, provided that the tolerance parameter is a constant strictly\nless than 1/2 but sufficiently close to it. Our analysis makes use of a\nconnection with the first-passage percolation model from the theory of\nstochastic processes. \n\n"}
{"id": "1511.04731", "contents": "Title: Hardness of RNA Folding Problem with Four Symbols Abstract: An RNA sequence is a string composed of four types of nucleotides, $A, C, G$,\nand $U$. The goal of the RNA folding problem is to find a maximum cardinality\nset of crossing-free pairs of the form $\\{A,U\\}$ or $\\{C,G\\}$ in a given RNA\nsequence. The problem is central in bioinformatics and has received much\nattention over the years. Abboud, Backurs, and Williams (FOCS 2015)\ndemonstrated a conditional lower bound for a generalized version of the RNA\nfolding problem based on a conjectured hardness of the $k$-clique problem.\nTheir lower bound requires the RNA sequence to have at least 36 types of\nsymbols, making the result not applicable to the RNA folding problem in real\nlife (i.e., alphabet size 4). In this paper, we present an improved lower bound\nthat works for the alphabet size 4 case.\n  We also investigate the Dyck edit distance problem, which is a string problem\nclosely related to RNA folding. We demonstrate a reduction from RNA folding to\nDyck edit distance with alphabet size 10. This leads to a much simpler proof of\nthe conditional lower bound for Dyck edit distance problem given by Abboud,\nBackurs, and Williams (FOCS 2015), and lowers the alphabet size requirement. \n\n"}
{"id": "1511.05196", "contents": "Title: Strategic Network Formation with Attack and Immunization Abstract: Strategic network formation arises where agents receive benefit from\nconnections to other agents, but also incur costs for forming links. We\nconsider a new network formation game that incorporates an adversarial attack,\nas well as immunization against attack. An agent's benefit is the expected size\nof her connected component post-attack, and agents may also choose to immunize\nthemselves from attack at some additional cost. Our framework is a stylized\nmodel of settings where reachability rather than centrality is the primary\nconcern and vertices vulnerable to attacks may reduce risk via costly measures.\n  In the reachability benefit model without attack or immunization, the set of\nequilibria is the empty graph and any tree. The introduction of attack and\nimmunization changes the game dramatically; new equilibrium topologies emerge,\nsome more sparse and some more dense than trees. We show that, under a mild\nassumption on the adversary, every equilibrium network with $n$ agents contains\nat most $2n-4$ edges for $n\\geq 4$. So despite permitting topologies denser\nthan trees, the amount of overbuilding is limited. We also show that attack and\nimmunization don't significantly erode social welfare: every non-trivial\nequilibrium with respect to several adversaries has welfare at least as that of\nany equilibrium in the attack-free model.\n  We complement our theory with simulations demonstrating fast convergence of a\nnew bounded rationality dynamic which generalizes linkstable best response but\nis considerably more powerful in our game. The simulations further elucidate\nthe wide variety of asymmetric equilibria and demonstrate topological\nconsequences of the dynamics e.g. heavy-tailed degree distributions. Finally,\nwe report on a behavioral experiment on our game with over 100 participants,\nwhere despite the complexity of the game, the resulting network was\nsurprisingly close to equilibrium. \n\n"}
{"id": "1511.05546", "contents": "Title: Complexity and Approximability of Parameterized MAX-CSPs Abstract: We study the optimization version of constraint satisfaction problems\n(Max-CSPs) in the framework of parameterized complexity; the goal is to compute\nthe maximum fraction of constraints that can be satisfied simultaneously. In\nstandard CSPs, we want to decide whether this fraction equals one. The\nparameters we investigate are structural measures, such as the treewidth or the\nclique-width of the variable-constraint incidence graph of the CSP instance.\n  We consider Max-CSPs with the constraint types AND, OR, PARITY, and MAJORITY,\nand with various parameters k, and we attempt to fully classify them into the\nfollowing three cases: 1. The exact optimum can be computed in FPT time. 2. It\nis W[1]-hard to compute the exact optimum, but there is a randomized FPT\napproximation scheme (FPTAS), which computes a $(1-\\epsilon)$-approximation in\ntime $f(k,\\epsilon)\\cdot poly(n)$. 3. There is no FPTAS unless FPT=W[1].\n  For the corresponding standard CSPs, we establish FPT vs. W[1]-hardness\nresults. \n\n"}
{"id": "1511.06022", "contents": "Title: Simulating Branching Programs with Edit Distance and Friends or: A\n  Polylog Shaved is a Lower Bound Made Abstract: A recent and active line of work achieves tight lower bounds for fundamental\nproblems under the Strong Exponential Time Hypothesis (SETH). A celebrated\nresult of Backurs and Indyk (STOC'15) proves that the Edit Distance of two\nsequences of length n cannot be computed in strongly subquadratic time under\nSETH. The result was extended by follow-up works to simpler looking problems\nlike finding the Longest Common Subsequence (LCS).\n  SETH is a very strong assumption, asserting that even linear size CNF\nformulas cannot be analyzed for satisfiability with an exponential speedup over\nexhaustive search. We consider much safer assumptions, e.g. that such a speedup\nis impossible for SAT on much more expressive representations, like NC\ncircuits. Intuitively, this seems much more plausible: NC circuits can\nimplement complex cryptographic primitives, while CNFs cannot even\napproximately compute an XOR of bits.\n  Our main result is a surprising reduction from SAT on Branching Programs to\nfundamental problems in P like Edit Distance, LCS, and many others. Truly\nsubquadratic algorithms for these problems therefore have consequences that we\nconsider to be far more remarkable than merely faster CNF SAT algorithms. For\nexample, SAT on arbitrary o(n)-depth bounded fan-in circuits (and therefore\nalso NC-Circuit-SAT) can be solved in (2-eps)^n time.\n  A very interesting feature of our work is that we can prove major\nconsequences even from mildly subquadratic algorithms for Edit Distance or LCS.\nFor example, we show that if we can shave an arbitrarily large polylog factor\nfrom n^2 for Edit Distance then NEXP does not have non-uniform NC^1 circuits. A\nmore fine-grained examination shows that even shaving a $\\log^c{n}$ factor, for\na specific constant $c \\approx 10^3$, already implies new circuit lower bounds. \n\n"}
{"id": "1511.07070", "contents": "Title: Which Regular Expression Patterns are Hard to Match? Abstract: Regular expressions constitute a fundamental notion in formal language theory\nand are frequently used in computer science to define search patterns. A\nclassic algorithm for these problems constructs and simulates a\nnon-deterministic finite automaton corresponding to the expression, resulting\nin an $O(mn)$ running time (where $m$ is the length of the pattern and $n$ is\nthe length of the text). This running time can be improved slightly (by a\npolylogarithmic factor), but no significantly faster solutions are known. At\nthe same time, much faster algorithms exist for various special cases of\nregular expressions, including dictionary matching, wildcard matching, subset\nmatching, word break problem etc.\n  In this paper, we show that the complexity of regular expression matching can\nbe characterized based on its {\\em depth} (when interpreted as a formula). Our\nresults hold for expressions involving concatenation, OR, Kleene star and\nKleene plus. For regular expressions of depth two (involving any combination of\nthe above operators), we show the following dichotomy: matching and membership\ntesting can be solved in near-linear time, except for \"concatenations of\nstars\", which cannot be solved in strongly sub-quadratic time assuming the\nStrong Exponential Time Hypothesis (SETH). For regular expressions of depth\nthree the picture is more complex. Nevertheless, we show that all problems can\neither be solved in strongly sub-quadratic time, or cannot be solved in\nstrongly sub-quadratic time assuming SETH.\n  An intriguing special case of membership testing involves regular expressions\nof the form \"a star of an OR of concatenations\", e.g., $[a|ab|bc]^*$. This\ncorresponds to the so-called {\\em word break} problem, for which a dynamic\nprogramming algorithm with a runtime of (roughly) $O(n\\sqrt{m})$ is known. We\nshow that the latter bound is not tight and improve the runtime to\n$O(nm^{0.44\\ldots})$. \n\n"}
{"id": "1511.07605", "contents": "Title: On the Computational Complexity of Limit Cycles in Dynamical Systems Abstract: We study the Poincare-Bendixson theorem for two-dimensional continuous\ndynamical systems in compact domains from the point of view of computation,\nseeking algorithms for finding the limit cycle promised by this classical\nresult. We start by considering a discrete analogue of this theorem and show\nthat both finding a point on a limit cycle, and determining if a given point is\non one, are PSPACE-complete.\n  For the continuous version, we show that both problems are uncomputable in\nthe real complexity sense; i.e., their complexity is arbitrarily high.\nSubsequently, we introduce a notion of an \"approximate cycle\" and prove an\n\"approximate\" Poincar\\'e-Bendixson theorem guaranteeing that some orbits come\nvery close to forming a cycle in the absence of approximate fixpoints;\nsurprisingly, it holds for all dimensions. The corresponding computational\nproblem defined in terms of arithmetic circuits is PSPACE-complete. \n\n"}
{"id": "1511.08141", "contents": "Title: Reinstating Combinatorial Protections for Manipulation and Bribery in\n  Single-Peaked and Nearly Single-Peaked Electorates Abstract: Understanding when and how computational complexity can be used to protect\nelections against different manipulative actions has been a highly active\nresearch area over the past two decades. A recent body of work, however, has\nshown that many of the NP-hardness shields, previously obtained, vanish when\nthe electorate has single-peaked or nearly single-peaked preferences. In light\nof these results, we investigate whether it is possible to reimpose NP-hardness\nshields for such electorates by allowing the voters to specify partial\npreferences instead of insisting they cast complete ballots. In particular, we\nshow that in single-peaked and nearly single-peaked electorates, if voters are\nallowed to submit top-truncated ballots, then the complexity of manipulation\nand bribery for many voting rules increases from being in P to being\nNP-complete. \n\n"}
{"id": "1512.01764", "contents": "Title: Fast Algorithms for Game-Theoretic Centrality Measures Abstract: In this dissertation, we analyze the computational properties of\ngame-theoretic centrality measures. The key idea behind game-theoretic approach\nto network analysis is to treat nodes as players in a cooperative game, where\nthe value of each coalition of nodes is determined by certain graph properties.\nNext, the centrality of any individual node is determined by a chosen\ngame-theoretic solution concept (notably, the Shapley value) in the same way as\nthe payoff of a player in a cooperative game. On one hand, the advantage of\ngame-theoretic centrality measures is that nodes are ranked not only according\nto their individual roles but also according to how they contribute to the role\nplayed by all possible subsets of nodes. On the other hand, the disadvantage is\nthat the game-theoretic solution concepts are typically computationally\nchallenging. The main contribution of this dissertation is that we show that a\nwide variety of game-theoretic solution concepts on networks can be computed in\npolynomial time. Our focus is on centralities based on the Shapley value and\nits various extensions, such as the Semivalues and Coalitional Semivalues.\nFurthermore, we prove #P-hardness of computing the Shapley value in\nconnectivity games and propose an algorithm to compute it. Finally, we analyse\ncomputational properties of generalized version of cooperative games in which\norder of player matters. We propose a new representation for such games, called\ngeneralized marginal contribution networks, that allows for polynomial\ncomputation in the size of the representation of two dedicated extensions of\nthe Shapley value to this class of games. \n\n"}
{"id": "1512.03531", "contents": "Title: Constructive noncommutative rank computation is in deterministic\n  polynomial time Abstract: We extend our techniques developed in our earlier paper appeared in\nComputational Complexity, 2017 (preprint: arXiv:1508.00690) to obtain a\ndeterministic polynomial time algorithm for computing the non-commutative rank\ntogether with certificates of linear spaces of matrices over sufficiently large\nbase fields.\n  The key new idea is a reduction procedure that keeps the blow-up parameter\nsmall, and there are two methods to implement this idea: the first one is a\ngreedy argument that removes certain rows and columns, and the second one is an\nefficient algorithmic version of a result of Derksen and Makam. Both methods\nrely crucially on the regularity lemma in our aforementioned paper, and in this\nmanuscript we also improve that lemma by removing a coprime condition there. \n\n"}
{"id": "1512.04017", "contents": "Title: The price of anarchy and stability in general noisy best-response\n  dynamics Abstract: Logit-response dynamics (Alos-Ferrer and Netzer, Games and Economic Behavior\n2010) are a rich and natural class of noisy best-response dynamics. In this\nwork we revise the price of anarchy and the price of stability by considering\nthe quality of long-run equilibria in these dynamics. Our results show that\nprior studies on simpler dynamics of this type can strongly depend on a\nsynchronous schedule of the players' moves. In particular, a small noise by\nitself is not enough to improve the quality of equilibria as soon as other very\nnatural schedules are used. \n\n"}
{"id": "1512.04637", "contents": "Title: Graphical Exchange Mechanisms Abstract: Consider an exchange mechanism which accepts diversified offers of various\ncommodities and redistributes everything it receives. We impose certain\nconditions of fairness and convenience on such a mechanism and show that it\nadmits unique prices, which equalize the value of offers and returns for each\nindividual.\n  We next define the complexity of a mechanism in terms of certain integers\n$\\tau_{ij},\\pi_{ij}$ and $k_{i}$ that represent the time required to exchange\n$i$ for $j$, the difficulty in determining the exchange ratio, and the\ndimension of the message space. We show that there are a finite number of\nminimally complex mechanisms, in each of which all trade is conducted through\nmarkets for commodity pairs.\n  Finally we consider minimal mechanisms with smallest worst-case complexities\n$\\tau=\\max\\tau_{ij}$ and $\\pi=\\max\\pi_{ij}$. For $m>3$ commodities, there are\nprecisely three such mechanisms, one of which has a distinguished commodity --\nthe money -- that serves as the sole medium of exchange. As $m\\rightarrow\n\\infty$ the money mechanism is the only one with bounded $\\left( \\pi\n,\\tau\\right) $. \n\n"}
{"id": "1512.05868", "contents": "Title: On Voting and Facility Location Abstract: We study mechanisms for candidate selection that seek to minimize the social\ncost, where voters and candidates are associated with points in some underlying\nmetric space. The social cost of a candidate is the sum of its distances to\neach voter. Some of our work assumes that these points can be modeled on a real\nline, but other results of ours are more general.\n  A question closely related to candidate selection is that of minimizing the\nsum of distances for facility location. The difference is that in our setting\nthere is a fixed set of candidates, whereas the large body of work on facility\nlocation seems to consider every point in the metric space to be a possible\ncandidate. This gives rise to three types of mechanisms which differ in the\ngranularity of their input space (voting, ranking and location mechanisms). We\nstudy the relationships between these three classes of mechanisms.\n  While it may seem that Black's 1948 median algorithm is optimal for candidate\nselection on the line, this is not the case. We give matching upper and lower\nbounds for a variety of settings. In particular, when candidates and voters are\non the line, our universally truthful spike mechanism gives a [tight]\napproximation of two. When assessing candidate selection mechanisms, we seek\nseveral desirable properties: (a) efficiency (minimizing the social cost) (b)\ntruthfulness (dominant strategy incentive compatibility) and (c) simplicity (a\nsmaller input space). We quantify the effect that truthfulness and simplicity\nimpose on the efficiency. \n\n"}
{"id": "1512.05974", "contents": "Title: Improved Balanced Flow Computation Using Parametric Flow Abstract: We present a new algorithm for computing balanced flows in equality networks\narising in market equilibrium computations. The current best time bound for\ncomputing balanced flows in such networks requires $O(n)$ maxflow computations,\nwhere $n$ is the number of nodes in the network [Devanur et al. 2008]. Our\nalgorithm requires only a single parametric flow computation. The best\nalgorithm for computing parametric flows [Gallo et al. 1989] is only by a\nlogarithmic factor slower than the best algorithms for computing maxflows.\nHence, the running time of the algorithms in [Devanur et al. 2008] and [Duan\nand Mehlhorn 2015] for computing market equilibria in linear Fisher and\nArrow-Debreu markets improve by almost a factor of $n$. \n\n"}
{"id": "1512.05996", "contents": "Title: Advice Complexity of the Online Induced Subgraph Problem Abstract: Several well-studied graph problems aim to select a largest (or smallest)\ninduced subgraph with a given property of the input graph. Examples of such\nproblems include maximum independent set, maximum planar graph, and many\nothers. We consider these problems, where the vertices are presented online.\nWith each vertex, the online algorithm must decide whether to include it into\nthe constructed subgraph, based only on the subgraph induced by the vertices\npresented so far. We study the properties that are common to all these problems\nby investigating the generalized problem: for a hereditary property \\pty, find\nsome maximal induced subgraph having \\pty. We study this problem from the point\nof view of advice complexity. Using a result from Boyar et al. [STACS 2015], we\ngive a tight trade-off relationship stating that for inputs of length n roughly\nn/c bits of advice are both needed and sufficient to obtain a solution with\ncompetitive ratio c, regardless of the choice of \\pty, for any c (possibly a\nfunction of n). Surprisingly, a similar result cannot be obtained for the\nsymmetric problem: for a given cohereditary property \\pty, find a minimum\nsubgraph having \\pty. We show that the advice complexity of this problem varies\nsignificantly with the choice of \\pty.\n  We also consider preemptive online model, where the decision of the algorithm\nis not completely irreversible. In particular, the algorithm may discard some\nvertices previously assigned to the constructed set, but discarded vertices\ncannot be reinserted into the set again. We show that, for the maximum induced\nsubgraph problem, preemption cannot help much, giving a lower bound of\n$\\Omega(n/(c^2\\log c))$ bits of advice needed to obtain competitive ratio $c$,\nwhere $c$ is any increasing function bounded by \\sqrt{n/log n}. We also give a\nlinear lower bound for c close to 1. \n\n"}
{"id": "1512.06197", "contents": "Title: A Stackelberg Game Model for Overlay D2D Transmission with Heterogeneous\n  Rate Requirements Abstract: This paper studies the performance of overlay device-to-device (D2D)\ncommunication links via carrier sense multiple access (CSMA) protocols. We\nassume that the D2D links have heterogeneous rate requirements and different\nwillingness to pay, and each of them acts non-altruistically to achieve its\ntarget rate while maximizing its own payoff. Spatial reuse is allowed if the\nlinks are not interfering with each other. A non-cooperative game model is used\nto address the resource allocation among the D2D links, at the same time\nleveraging on the ideal CSMA network (ICN) model to address the physical\nchannel access issue. We propose a Stackelberg game in which the base station\nin the cellular network acts as a Stackelberg leader to regulate the individual\npayoff by modifying the unit service price so that the total D2D throughput is\nmaximized. The problem is shown to be quasi-convex and can be solved by a\nsequence of equivalent convex optimization problems. The pricing strategies are\ndesigned so that the network always operates within the feasible throughput\nregion. The results are verified by simulations. \n\n"}
{"id": "1601.01492", "contents": "Title: Complexity of Shift Bribery in Committee Elections Abstract: Given an election, a preferred candidate p, and a budget, the SHIFT BRIBERY\nproblem asks whether p can win the election after shifting p higher in some\nvoters' preference orders. Of course, shifting comes at a price (depending on\nthe voter and on the extent of the shift) and one must not exceed the given\nbudget. We study the (parameterized) computational complexity of S HIFT BRIBERY\nfor multiwinner voting rules where winning the election means to be part of\nsome winning committee. We focus on the well-established SNTV, Bloc, k-Borda,\nand Chamberlin-Courant rules, as well as on approximate variants of the\nChamberlin-Courant rule, since the original rule is NP-hard to compute. We show\nthat SHIFT BRIBERY tends to be harder in the multiwinner setting than in the\nsingle-winner one by showing settings where SHIFT BRIBERY is easy in the\nsingle-winner cases, but is hard (and hard to approximate) in the multiwinner\nones. Moreover, we show that the non-monotonicity of those rules which are\nbased on approximation algorithms for the Chamberlin-Courant rule sometimes\naffects the complexity of SHIFT BRIBERY. \n\n"}
{"id": "1601.01744", "contents": "Title: Performance of QAOA on Typical Instances of Constraint Satisfaction\n  Problems with Bounded Degree Abstract: We consider constraint satisfaction problems of bounded degree, with a good\nnotion of \"typicality\", e.g. the negation of the variables in each constraint\nis taken independently at random. Using the quantum approximate optimization\nalgorithm (QAOA), we show that $ \\mu+\\Omega(1/\\sqrt{D}) $ fraction of the\nconstraints can be satisfied for typical instances, with the assignment\nefficiently produced by QAOA. We do so by showing that the averaged fraction of\nconstraints being satisfied is $ \\mu+\\Omega(1/\\sqrt{D}) $, with small variance.\nHere $ \\mu $ is the fraction that would be satisfied by a uniformly random\nassignment, and $ D $ is the number of constraints that each variable can\nappear. CSPs with typicality include Max-$ k $XOR and Max-$ k $SAT. We point\nout how it can be applied to determine the typical ground-state energy of some\nlocal Hamiltonians. We also give a similar result for instances with \"no\noverlapping constraints\", using the quantum algorithm. We sketch how the\nclassical algorithm might achieve some partial result. \n\n"}
{"id": "1601.02039", "contents": "Title: Informational Braess' Paradox: The Effect of Information on Traffic\n  Congestion Abstract: To systematically study the implications of additional information about\nroutes provided to certain users (e.g., via GPS-based route guidance systems),\nwe introduce a new class of congestion games in which users have differing\ninformation sets about the available edges and can only use routes consisting\nof edges in their information set. After defining the notion of Information\nConstrained Wardrop Equilibrium (ICWE) for this class of congestion games and\nstudying its basic properties, we turn to our main focus: whether additional\ninformation can be harmful (in the sense of generating greater equilibrium\ncosts/delays). We formulate this question in the form of Informational Braes'\nParadox (IBP), which extends the classic Braess' Paradox in traffic equilibria,\nand asks whether users receiving additional information can become worse off.\nWe provide a comprehensive answer to this question showing that in any network\nin the series of linearly independent (SLI) class, which is a strict subset of\nseries-parallel networks, IBP cannot occur, and in any network that is not in\nthe SLI class, there exists a configuration of edge-specific cost functions for\nwhich IBP will occur. In the process, we establish several properties of the\nSLI class of networks, which include the characterization of the complement of\nthe SLI class in terms of embedding a specific set of networks, and also an\nalgorithm which determines whether a graph is SLI in linear time. We further\nprove that the worst-case inefficiency performance of ICWE is no worse than the\nstandard Wardrop equilibrium. \n\n"}
{"id": "1601.02298", "contents": "Title: How to Incentivize Data-Driven Collaboration Among Competing Parties Abstract: The availability of vast amounts of data is changing how we can make medical\ndiscoveries, predict global market trends, save energy, and develop educational\nstrategies. In some settings such as Genome Wide Association Studies or deep\nlearning, sheer size of data seems critical. When data is held distributedly by\nmany parties, they must share it to reap its full benefits.\n  One obstacle to this revolution is the lack of willingness of different\nparties to share data, due to reasons such as loss of privacy or competitive\nedge. Cryptographic works address privacy aspects, but shed no light on\nindividual parties' losses/gains when access to data carries tangible rewards.\nEven if it is clear that better overall conclusions can be drawn from\ncollaboration, are individual collaborators better off by collaborating?\nAddressing this question is the topic of this paper.\n  * We formalize a model of n-party collaboration for computing functions over\nprivate inputs in which participants receive their outputs in sequence, and the\norder depends on their private inputs. Each output \"improves\" on preceding\noutputs according to a score function.\n  * We say a mechanism for collaboration achieves collaborative equilibrium if\nit ensures higher reward for all participants when collaborating (rather than\nworking alone). We show that in general, computing a collaborative equilibrium\nis NP-complete, yet we design efficient algorithms to compute it in a range of\nnatural model settings.\n  Our collaboration mechanisms are in the standard model, and thus require a\ncentral trusted party; however, we show this assumption is unnecessary under\nstandard cryptographic assumptions. We show how to implement the mechanisms in\na decentralized way with new extensions of secure multiparty computation that\nimpose order/timing constraints on output delivery to different players, as\nwell as privacy and correctness. \n\n"}
{"id": "1601.07505", "contents": "Title: Game Theoretic Analysis of Tree Based Referrals for Crowd Sensing Social\n  Systems with Passive Rewards Abstract: Participatory crowd sensing social systems rely on the participation of large\nnumber of individuals. Since humans are strategic by nature, effective\nincentive mechanisms are needed to encourage participation. A popular mechanism\nto recruit individuals is through referrals and passive incentives such as\ngeometric incentive mechanisms used by the winning team in the 2009 DARPA\nNetwork Challenge and in multi level marketing schemes. The effect of such\nrecruitment schemes on the effort put in by recruited strategic individuals is\nnot clear. This paper attempts to fill this gap. Given a referral tree and the\ndirect and passive reward mechanism, we formulate a network game where agents\ncompete for finishing crowd sensing tasks. We characterize the Nash equilibrium\nefforts put in by the agents and derive closed form expressions for the same.\nWe discover free riding behavior among nodes who obtain large passive rewards.\nThis work has implications on designing effective recruitment mechanisms for\ncrowd sourced tasks. For example, usage of geometric incentive mechanisms to\nrecruit large number of individuals may not result in proportionate effort\nbecause of free riding. \n\n"}
{"id": "1602.01819", "contents": "Title: A short note on Merlin-Arthur protocols for subset sum Abstract: In the subset sum problem we are given n positive integers along with a\ntarget integer t. A solution is a subset of these integers summing to t. In\nthis short note we show that for a given subset sum instance there is a proof\nof size $O^*(\\sqrt{t})$ of what the number of solutions is that can be\nconstructed in $O^*(t)$ time and can be probabilistically verified in time\n$O^*(\\sqrt{t})$ with at most constant error probability. Here, the $O^*()$\nnotation omits factors polynomial in the input size $n\\log(t)$. \n\n"}
{"id": "1602.01963", "contents": "Title: Winning Cores in Parity Games Abstract: We introduce the novel notion of winning cores in parity games and develop a\ndeterministic polynomial-time under-approximation algorithm for solving parity\ngames based on winning core approximation. Underlying this algorithm are a\nnumber properties about winning cores which are interesting in their own right.\nIn particular, we show that the winning core and the winning region for a\nplayer in a parity game are equivalently empty. Moreover, the winning core\ncontains all fatal attractors but is not necessarily a dominion itself.\nExperimental results are very positive both with respect to quality of\napproximation and running time. It outperforms existing state-of-the-art\nalgorithms significantly on most benchmarks. \n\n"}
{"id": "1602.02174", "contents": "Title: Participation Incentives in Randomized Social Choice Abstract: When aggregating preferences of agents via voting, two desirable goals are to\nidentify outcomes that are Pareto optimal and to incentivize agents to\nparticipate in the voting process. We consider participation notions as\nformalized by Brandl, Brandt, and Hofbauer (2015) and study how far efficiency\nand participation are achievable by randomized social choice functions in\nparticular when agents' preferences are downward lexicographic (DL) or satisfy\nstochastic dominance (SD). Our results include the followings ones: we prove\nformal relations between the participation notions with respect to SD and DL\nand we show that the maximal recursive rule satisfies very strong participation\nwith respect to both SD and DL. \n\n"}
{"id": "1602.04328", "contents": "Title: Dimension and codimension of simple games Abstract: This paper studies the complexity of computing a representation of a simple\ngame as the intersection (union) of weighted majority games, as well as, the\ndimension or the codimension. We also present some examples with linear\ndimension and exponential codimension with respect to the number of players. \n\n"}
{"id": "1602.05237", "contents": "Title: FPTAS for Mixed-Strategy Nash Equilibria in Tree Graphical Games and\n  Their Generalizations Abstract: We provide the first fully polynomial time approximation scheme (FPTAS) for\ncomputing an approximate mixed-strategy Nash equilibrium in tree-structured\ngraphical multi-hypermatrix games (GMhGs). GMhGs are generalizations of\nnormal-form games, graphical games, graphical polymatrix games, and\nhypergraphical games. Computing an exact mixed-strategy Nash equilibria in\ngraphical polymatrix games is PPAD-complete and thus generally believed to be\nintractable. In contrast, to the best of our knowledge, we are the first to\nestablish an FPTAS for tree polymatrix games as well as tree graphical games\nwhen the number of actions is bounded by a constant. As a corollary, we give a\nquasi-polynomial time approximation scheme (quasi-PTAS) when the number of\nactions is bounded by the logarithm of the number of players. \n\n"}
{"id": "1602.05897", "contents": "Title: Toward Deeper Understanding of Neural Networks: The Power of\n  Initialization and a Dual View on Expressivity Abstract: We develop a general duality between neural networks and compositional\nkernels, striving towards a better understanding of deep learning. We show that\ninitial representations generated by common random initializations are\nsufficiently rich to express all functions in the dual kernel space. Hence,\nthough the training objective is hard to optimize in the worst case, the\ninitial weights form a good starting point for optimization. Our dual view also\nreveals a pragmatic and aesthetic perspective of neural networks and\nunderscores their expressive power. \n\n"}
{"id": "1602.06063", "contents": "Title: Pricing and Resource Allocation via Game Theory for a Small-Cell Video\n  Caching System Abstract: Evidence indicates that downloading on-demand videos accounts for a dramatic\nincrease in data traffic over cellular networks. Caching popular videos in the\nstorage of small-cell base stations (SBS), namely, small-cell caching, is an\nefficient technology for reducing the transmission latency whilst mitigating\nthe redundant transmissions of popular videos over back-haul channels. In this\npaper, we consider a commercialized small-cell caching system consisting of a\nnetwork service provider (NSP), several video retailers (VR), and mobile users\n(MU). The NSP leases its SBSs to the VRs for the purpose of making profits, and\nthe VRs, after storing popular videos in the rented SBSs, can provide faster\nlocal video transmissions to the MUs, thereby gaining more profits. We conceive\nthis system within the framework of Stackelberg game by treating the SBSs as a\nspecific type of resources. We first model the MUs and SBSs as two independent\nPoisson point processes, and develop, via stochastic geometry theory, the\nprobability of the specific event that an MU obtains the video of its choice\ndirectly from the memory of an SBS. Then, based on the probability derived, we\nformulate a Stackelberg game to jointly maximize the average profit of both the\nNSP and the VRs. Also, we investigate the Stackelberg equilibrium by solving a\nnon-convex optimization problem. With the aid of this game theoretic framework,\nwe shed light on the relationship between four important factors: the optimal\npricing of leasing an SBS, the SBSs allocation among the VRs, the storage size\nof the SBSs, and the popularity distribution of the VRs. Monte-Carlo\nsimulations show that our stochastic geometry-based analytical results closely\nmatch the empirical ones. Numerical results are also provided for quantifying\nthe proposed game-theoretic framework by showing its efficiency on pricing and\nresource allocation. \n\n"}
{"id": "1602.06648", "contents": "Title: Strategic Decompositions of Normal Form Games: Zero-sum Games and\n  Potential Games Abstract: We study new classes of games, called zero-sum equivalent games and zero-sum\nequivalent potential games, and prove decomposition theorems involving these\nclasses of games. We say that two games are \"strategically equivalent\" if, for\nevery player, the payoff differences between two strategies (holding other\nplayers' strategies fixed) are identical. A zero-sum equivalent game is a game\nthat is strategically equivalent to a zero-sum game; a zero-sum equivalent\npotential game is a zero-sum equivalent game that is strategically equivalent\nto a common interest game. We also call a game \"normalized\" if the sum of one\nplayer's payoffs, given the other players' strategies, is always zero. We show\nthat any normal form game can be uniquely decomposed into either (i) a zero-sum\nequivalent game and a normalized common interest game, or (ii) a zero-sum\nequivalent potential game, a normalized zero-sum game, and a normalized common\ninterest game, each with distinctive equilibrium properties. For example, we\nshow that two-player zero-sum equivalent games with finite strategy sets\ngenerically have a unique Nash equilibrium and that two-player zero-sum\nequivalent potential games with finite strategy sets generically have a\nstrictly dominant Nash equilibrium. \n\n"}
{"id": "1602.06940", "contents": "Title: Complexity of Manipulating Sequential Allocation Abstract: Sequential allocation is a simple allocation mechanism in which agents are\ngiven pre-specified turns and each agents gets the most preferred item that is\nstill available. It has long been known that sequential allocation is not\nstrategyproof.\n  Bouveret and Lang (2014) presented a polynomial-time algorithm to compute a\nbest response of an agent with respect to additively separable utilities and\nclaimed that (1) their algorithm correctly finds a best response, and (2) each\nbest response results in the same allocation for the manipulator. We show that\nboth claims are false via an example. We then show that in fact the problem of\ncomputing a best response is NP-complete. On the other hand, the insights and\nresults of Bouveret and Lang (2014) for the case of two agents still hold. \n\n"}
{"id": "1602.07570", "contents": "Title: Bayesian Exploration: Incentivizing Exploration in Bayesian Games Abstract: We consider a ubiquitous scenario in the Internet economy when individual\ndecision-makers (henceforth, agents) both produce and consume information as\nthey make strategic choices in an uncertain environment. This creates a\nthree-way tradeoff between exploration (trying out insufficiently explored\nalternatives to help others in the future), exploitation (making optimal\ndecisions given the information discovered by other agents), and incentives of\nthe agents (who are myopically interested in exploitation, while preferring the\nothers to explore). We posit a principal who controls the flow of information\nfrom agents that came before, and strives to coordinate the agents towards a\nsocially optimal balance between exploration and exploitation, not using any\nmonetary transfers. The goal is to design a recommendation policy for the\nprincipal which respects agents' incentives and minimizes a suitable notion of\nregret.\n  We extend prior work in this direction to allow the agents to interact with\none another in a shared environment: at each time step, multiple agents arrive\nto play a Bayesian game, receive recommendations, choose their actions, receive\ntheir payoffs, and then leave the game forever. The agents now face two sources\nof uncertainty: the actions of the other agents and the parameters of the\nuncertain game environment.\n  Our main contribution is to show that the principal can achieve constant\nregret when the utilities are deterministic (where the constant depends on the\nprior distribution, but not on the time horizon), and logarithmic regret when\nthe utilities are stochastic. As a key technical tool, we introduce the concept\nof explorable actions, the actions which some incentive-compatible policy can\nrecommend with non-zero probability. We show how the principal can identify\n(and explore) all explorable actions, and use the revealed information to\nperform optimally. \n\n"}
{"id": "1602.08109", "contents": "Title: Recognising Multidimensional Euclidean Preferences Abstract: Euclidean preferences are a widely studied preference model, in which\ndecision makers and alternatives are embedded in d-dimensional Euclidean space.\nDecision makers prefer those alternatives closer to them. This model, also\nknown as multidimensional unfolding, has applications in economics,\npsychometrics, marketing, and many other fields. We study the problem of\ndeciding whether a given preference profile is d-Euclidean. For the\none-dimensional case, polynomial-time algorithms are known. We show that, in\ncontrast, for every other fixed dimension d > 1, the recognition problem is\nequivalent to the existential theory of the reals (ETR), and so in particular\nNP-hard. We further show that some Euclidean preference profiles require\nexponentially many bits in order to specify any Euclidean embedding, and prove\nthat the domain of d-Euclidean preferences does not admit a finite forbidden\nminor characterisation for any d > 1. We also study dichotomous preferencesand\nthe behaviour of other metrics, and survey a variety of related work. \n\n"}
{"id": "1603.01257", "contents": "Title: New Convex Programs for Fisher's Market Model and its Generalizations Abstract: We present the following results pertaining to Fisher's market model:\n  -We give two natural generalizations of Fisher's market model: In model M_1,\nsellers can declare an upper bound on the money they wish to earn (and take\nback their unsold good), and in model M_2, buyers can declare an upper bound on\nthe amount to utility they wish to derive (and take back the unused part of\ntheir money).\n  -We derive convex programs for the linear case of these two models by\ngeneralizing a convex program due to Shmyrev and the Eisenberg-Gale program,\nrespectively.\n  -We generalize the Arrow-Hurwicz theorem to the linear case of these two\nmodels, hence deriving alternate convex programs.\n  -For the special class of convex programs having convex objective functions\nand linear constraints, we derive a simple set of rules for constructing the\ndual program (as simple as obtaining the dual of an LP). Using these rules we\nshow a formal relationship between the two seemingly different convex programs\nfor linear Fisher markets, due to Eisenberg-Gale and Shmyrev; the duals of\nthese are the same, upto a change of variables. \n\n"}
{"id": "1603.01318", "contents": "Title: Efficiently characterizing games consistent with perturbed equilibrium\n  observations Abstract: We study the problem of characterizing the set of games that are consistent\nwith observed equilibrium play. Our contribution is to develop and analyze a\nnew methodology based on convex optimization to address this problem for many\nclasses of games and observation models of interest. Our approach provides a\nsharp, computationally efficient characterization of the extent to which a\nparticular set of observations constrains the space of games that could have\ngenerated them. This allows us to solve a number of variants of this problem as\nwell as to quantify the power of games from particular classes (e.g., zero-sum,\npotential, linearly parameterized) to explain player behavior. We illustrate\nour approach with numerical simulations. \n\n"}
{"id": "1603.01925", "contents": "Title: On the Complexity of Detecting Constrained Negative Cost Cycles Abstract: Given a positive integer $k$ and a directed graph with a cost on each edge,\nthe $k$-length negative cost cycle ($k$\\emph{LNCC}) problem is to determine\nwhether there exists a negative cost cycle with at least $k$ edges, and the\nfixed-point \\emph{$k$-}length negative cost cycle \\emph{trail (FP$k$LNCCT)}\nproblem is to determine whether there exists a negative trail enrouting a given\nvertex (as the fixed point) and containing only cycles with at least $k$ edges.\nThe $k$\\emph{LNCC} problem first emerged in deadlock avoidance in synchronized\nstreaming computing network \\cite{spaa10}, generalizing two famous problems:\nnegative cycle detection and the $k$-cycle problem. As a warmup by-production,\nthe paper first shows that \\emph{FP$k$LNCCT is }${\\cal NP}$-complete in\nmultigraph\\emph{ }even for\\emph{ $k=3$} by reducing from the \\emph{3SAT}\nproblem. Then as the main result, we prove the ${\\cal NP}$-completeness of\n$k$\\emph{LNCC} by giving a sophisticated reduction from the 3 Occurrence\n3-Satisfiability (\\emph{3O3SAT}) problem, a known ${\\cal NP}$-complete special\ncase of 3SAT in which a variable occurs at most three times. The complexity\nresult is interesting, since polynomial time algorithms are known for both\n$2$\\emph{LNCC} (essentially no restriction on the value of $k$) and the\n$k$-cycle problem of fixed $k$. This paper closes the open problem proposed by\nLi et al. in \\cite{spaa10} whether $k$\\emph{LNCC} admits polynomial-time\nalgorithms. \n\n"}
{"id": "1603.02208", "contents": "Title: An Online Mechanism for Ridesharing in Autonomous Mobility-on-Demand\n  Systems Abstract: With proper management, Autonomous Mobility-on-Demand (AMoD) systems have\ngreat potential to satisfy the transport demands of urban populations by\nproviding safe, convenient, and affordable ridesharing services. Meanwhile,\nsuch systems can substantially decrease private car ownership and use, and thus\nsignificantly reduce traffic congestion, energy consumption, and carbon\nemissions. To achieve this objective, an AMoD system requires private\ninformation about the demand from passengers. However, due to\nself-interestedness, passengers are unlikely to cooperate with the service\nproviders in this regard. Therefore, an online mechanism is desirable if it\nincentivizes passengers to truthfully report their actual demand. For the\npurpose of promoting ridesharing, we hereby introduce a posted-price,\nintegrated online ridesharing mechanism (IORS) that satisfies desirable\nproperties such as ex-post incentive compatibility, individual rationality, and\nbudget-balance. Numerical results indicate the competitiveness of IORS compared\nwith two benchmarks, namely the optimal assignment and an offline,\nauction-based mechanism. \n\n"}
{"id": "1603.03151", "contents": "Title: Informed Truthfulness in Multi-Task Peer Prediction Abstract: The problem of peer prediction is to elicit information from agents in\nsettings without any objective ground truth against which to score reports.\nPeer prediction mechanisms seek to exploit correlations between signals to\nalign incentives with truthful reports. A long-standing concern has been the\npossibility of uninformative equilibria. For binary signals, a multi-task\nmechanism [Dasgupta-Ghosh '13] achieves strong truthfulness, so that the\ntruthful equilibrium strictly maximizes payoff. We characterize conditions on\nthe signal distribution for which this mechanism remains strongly-truthful with\nnon-binary signals, also providing a greatly simplified proof. We introduce the\nCorrelated Agreement (CA) mechanism, which handles multiple signals and\nprovides informed truthfulness: no strategy profile provides more payoff in\nequilibrium than truthful reporting, and the truthful equilibrium is strictly\nbetter than any uninformed strategy (where an agent avoids the effort of\nobtaining a signal). The CA mechanism is maximally strongly truthful, in that\nno mechanism in a broad class of mechanisms is strongly truthful on a larger\nfamily of signal distributions. We also give a detail-free version of the\nmechanism that removes any knowledge requirements on the part of the designer,\nusing reports on many tasks to learn statistics while retaining\nepsilon-informed truthfulness. \n\n"}
{"id": "1603.06422", "contents": "Title: Parity Game Reductions Abstract: Parity games play a central role in model checking and satisfiability\nchecking. Solving parity games is computationally expensive, among others due\nto the size of the games, which, for model checking problems, can easily\ncontain $10^9$ vertices or beyond. Equivalence relations can be used to reduce\nthe size of a parity game, thereby potentially alleviating part of the\ncomputational burden. We reconsider (governed) bisimulation and (governed)\nstuttering bisimulation, and we give detailed proofs that these relations are\nequivalences, have unique quotients and they approximate the winning regions of\nparity games. Furthermore, we present game-based characterisations of these\nrelations. Using these characterisations our equivalences are compared to\nrelations for parity games that can be found in the literature, such as direct\nsimulation equivalence and delayed simulation equivalence. To complete the\noverview we develop coinductive characterisations of direct- and delayed\nsimulation equivalence and we establish a lattice of equivalences for parity\ngames. \n\n"}
{"id": "1603.06505", "contents": "Title: Characterizations of symmetrically partial Boolean functions with exact\n  quantum query complexity Abstract: We give and prove an optimal exact quantum query algorithm with complexity\n$k+1$ for computing the promise problem (i.e., symmetric and partial Boolean\nfunction) $DJ_n^k$ defined as: $DJ_n^k(x)=1$ for $|x|=n/2$, $DJ_n^k(x)=0$ for\n$|x|$ in the set $\\{0, 1,\\ldots, k, n-k, n-k+1,\\ldots,n\\}$, and it is undefined\nfor the rest cases, where $n$ is even, $|x|$ is the Hamming weight of $x$. The\ncase of $k=0$ is the well-known Deutsch-Jozsa problem. We outline all symmetric\n(and partial) Boolean functions with degrees 1 and 2, and prove their exact\nquantum query complexity. Then we prove that any symmetrical (and partial)\nBoolean function $f$ has exact quantum 1-query complexity if and only if $f$\ncan be computed by the Deutsch-Jozsa algorithm. We also discover the optimal\nexact quantum 2-query complexity for distinguishing between inputs of Hamming\nweight $\\{ \\lfloor n/2\\rfloor, \\lceil n/2\\rceil \\}$ and Hamming weight in the\nset $\\{ 0, n\\}$ for all odd $n$. In addition, a method is provided to determine\nthe degree of any symmetrical (and partial) Boolean function. \n\n"}
{"id": "1603.06985", "contents": "Title: A Quantum Version of Sch\\\"oning's Algorithm Applied to Quantum 2-SAT Abstract: We study a quantum algorithm that consists of a simple quantum Markov\nprocess, and we analyze its behavior on restricted versions of Quantum 2-SAT.\nWe prove that the algorithm solves this decision problem with high probability\nfor n qubits, L clauses, and promise gap c in time O(n^2 L^2 c^{-2}). If the\nHamiltonian is additionally polynomially gapped, our algorithm efficiently\nproduces a state that has high overlap with the satisfying subspace. The Markov\nprocess we study is a quantum analogue of Sch\\\"oning's probabilistic algorithm\nfor k-SAT. \n\n"}
{"id": "1603.07229", "contents": "Title: Sequential Mechanisms with ex-post Participation Guarantees Abstract: We provide a characterization of revenue-optimal dynamic mechanisms in\nsettings where a monopolist sells k items over k periods to a buyer who\nrealizes his value for item i in the beginning of period i. We require that the\nmechanism satisfies a strong individual rationality constraint, requiring that\nthe stage utility of each agent be positive during each period. We show that\nthe optimum mechanism can be computed by solving a nested sequence of static\n(single-period) mechanisms that optimize a tradeoff between the surplus of the\nallocation and the buyer's utility. We also provide a simple dynamic mechanism\nthat obtains at least half of the optimal revenue. The mechanism either ignores\nhistory and posts the optimal monopoly price in each period, or allocates with\na probability that is independent of the current report of the agent and is\nbased only on previous reports. Our characterization extends to multi-agent\nauctions. We also formulate a discounted infinite horizon version of the\nproblem, where we study the performance of \"Markov mechanisms.\" \n\n"}
{"id": "1603.07751", "contents": "Title: Equilibrium Selection in Information Elicitation without Verification\n  via Information Monotonicity Abstract: Peer-prediction is a mechanism which elicits privately-held, non-variable\ninformation from self-interested agents---formally, truth-telling is a strict\nBayes Nash equilibrium of the mechanism. The original Peer-prediction mechanism\nsuffers from two main limitations: (1) the mechanism must know the \"common\nprior\" of agents' signals; (2) additional undesirable and non-truthful\nequilibria exist which often have a greater expected payoff than the\ntruth-telling equilibrium. A series of results has successfully weakened the\nknown common prior assumption. However, the equilibrium multiplicity issue\nremains a challenge.\n  In this paper, we address the above two problems. In the setting where a\ncommon prior exists but is not known to the mechanism we show (1) a general\nnegative result applying to a large class of mechanisms showing truth-telling\ncan never pay strictly more in expectation than a particular set of equilibria\nwhere agents collude to \"relabel\" the signals and tell the truth after\nrelabeling signals; (2) provide a mechanism that has no information about the\ncommon prior but where truth-telling pays as much in expectation as any\nrelabeling equilibrium and pays strictly more than any other symmetric\nequilibrium; (3) moreover in our mechanism, if the number of agents is\nsufficiently large, truth-telling pays similarly to any equilibrium close to a\n\"relabeling\" equilibrium and pays strictly more than any equilibrium that is\nnot close to a relabeling equilibrium. \n\n"}
{"id": "1603.08853", "contents": "Title: Effects of Information Heterogeneity in Bayesian Routing Games Abstract: This article studies the value of information in route choice decisions when\na fraction of players have access to high accuracy information about traffic\nincidents relative to others. To model such environments, we introduce a\nBayesian congestion game, in which players have private information about\nincidents, and each player chooses her route on a network of parallel links.\nThe links are prone to incidents that occur with an ex-ante known probability.\nThe demand is comprised of two player populations: one with access to high\naccuracy incident information and another with low accuracy information, i.e.\nthe populations differ only by their access to information. The common\nknowledge includes: (i) the demand and route cost functions, (ii) the fraction\nof highly-informed players, (iii) the incident probability, and (iv) the\nmarginal type distributions induced by the information structure of the game.\nWe present a full characterization of the Bayesian Wardrop Equilibrium of this\ngame under the assumption that low information players receive no additional\ninformation beyond common knowledge. We also compute the cost to individual\nplayers and the social cost as a function of the fraction of highly-informed\nplayers when they receive perfectly accurate information. Our first result\nsuggests that below a certain threshold of highly-informed players, both\npopulations experience a reduction in individual cost, with the highly-informed\nplayers receiving a greater reduction. However, above this threshold, both\npopulations realize the same equilibrium cost. Secondly, there exists another\n(lower or equal) threshold above which a further increase in the fraction of\nhighly-informed players does not reduce the expected social costs. Thus, once a\nsufficiently large number of players are highly informed, wider distribution of\nmore accurate information is ineffective at best, and otherwise socially\nharmful. \n\n"}
{"id": "1603.09173", "contents": "Title: Riemannian game dynamics Abstract: We study a class of evolutionary game dynamics defined by balancing a gain\ndetermined by the game's payoffs against a cost of motion that captures the\ndifficulty with which the population moves between states. Costs of motion are\nrepresented by a Riemannian metric, i.e., a state-dependent inner product on\nthe set of population states. The replicator dynamics and the (Euclidean)\nprojection dynamics are the archetypal examples of the class we study. Like\nthese representative dynamics, all Riemannian game dynamics satisfy certain\nbasic desiderata, including positive correlation and global convergence in\npotential games. Moreover, when the underlying Riemannian metric satisfies a\nHessian integrability condition, the resulting dynamics preserve many further\nproperties of the replicator and projection dynamics. We examine the close\nconnections between Hessian game dynamics and reinforcement learning in normal\nform games, extending and elucidating a well-known link between the replicator\ndynamics and exponential reinforcement learning. \n\n"}
{"id": "1604.00357", "contents": "Title: Beyond matroids: Secretary Problem and Prophet Inequality with general\n  constraints Abstract: We study generalizations of the \"Prophet Inequality\" and \"Secretary Problem\",\nwhere the algorithm is restricted to an arbitrary downward-closed set system.\nFor {0,1}-values, we give O(log n)-competitive algorithms for both problems.\nThis is close to the \\Omega(log n / loglog n) lower bound due to Babaioff,\nImmorlica, and Kleinberg. For general values, our results translate to O(log n\nlog r)-competitive algorithms, where r is the cardinality of the largest\nfeasible set. This resolves (up to the O(log r loglog n) factors) an open\nquestion posed to us by Bobby Kleinberg. \n\n"}
{"id": "1604.01529", "contents": "Title: Axiomatic Characterization of Committee Scoring Rules Abstract: Committee scoring rules form a rich class of aggregators of voters'\npreferences for the purpose of selecting subsets of objects with desired\nproperties, e.g., a shortlist of candidates for an interview, a representative\ncollective body such as a parliament, or a set of locations for a set of public\nfacilities. In the spirit of celebrated Young's characterization result that\naxiomatizes single-winner scoring rules, we provide an axiomatic\ncharacterization of multiwinner committee scoring rules. We show that committee\nscoring rules---despite forming a remarkably general class of rules---are\ncharacterized by the set of four standard axioms, anonymity, neutrality,\nconsistency and continuity, and by one axiom specific to multiwinner rules\nwhich we call committee dominance. In the course of our proof, we develop\nseveral new notions and techniques. In particular, we introduce and\naxiomatically characterize multiwinner decision scoring rules, a class of rules\nthat broadly generalizes the well-known majority relation. \n\n"}
{"id": "1604.01952", "contents": "Title: Deep Online Convex Optimization with Gated Games Abstract: Methods from convex optimization are widely used as building blocks for deep\nlearning algorithms. However, the reasons for their empirical success are\nunclear, since modern convolutional networks (convnets), incorporating\nrectifier units and max-pooling, are neither smooth nor convex. Standard\nguarantees therefore do not apply. This paper provides the first convergence\nrates for gradient descent on rectifier convnets. The proof utilizes the\nparticular structure of rectifier networks which consists in binary\nactive/inactive gates applied on top of an underlying linear network. The\napproach generalizes to max-pooling, dropout and maxout. In other words, to\nprecisely the neural networks that perform best empirically. The key step is to\nintroduce gated games, an extension of convex games with similar convergence\nproperties that capture the gating function of rectifiers. The main result is\nthat rectifier convnets converge to a critical point at a rate controlled by\nthe gated-regret of the units in the network. Corollaries of the main result\ninclude: (i) a game-theoretic description of the representations learned by a\nneural network; (ii) a logarithmic-regret algorithm for training neural nets;\nand (iii) a formal setting for analyzing conditional computation in neural nets\nthat can be applied to recently developed models of attention. \n\n"}
{"id": "1604.02606", "contents": "Title: A General Retraining Framework for Scalable Adversarial Classification Abstract: Traditional classification algorithms assume that training and test data come\nfrom similar distributions. This assumption is violated in adversarial\nsettings, where malicious actors modify instances to evade detection. A number\nof custom methods have been developed for both adversarial evasion attacks and\nrobust learning. We propose the first systematic and general-purpose retraining\nframework which can: a) boost robustness of an \\emph{arbitrary} learning\nalgorithm, in the face of b) a broader class of adversarial models than any\nprior methods. We show that, under natural conditions, the retraining framework\nminimizes an upper bound on optimal adversarial risk, and show how to extend\nthis result to account for approximations of evasion attacks. Extensive\nexperimental evaluation demonstrates that our retraining methods are nearly\nindistinguishable from state-of-the-art algorithms for optimizing adversarial\nrisk, but are more general and far more scalable. The experiments also confirm\nthat without retraining, our adversarial framework dramatically reduces the\neffectiveness of learning. In contrast, retraining significantly boosts\nrobustness to evasion attacks without significantly compromising overall\naccuracy. \n\n"}
{"id": "1604.02737", "contents": "Title: Correlated Equilibria for Approximate Variational Inference in MRFs Abstract: Almost all of the work in graphical models for game theory has mirrored\nprevious work in probabilistic graphical models. Our work considers the\nopposite direction: Taking advantage of recent advances in equilibrium\ncomputation for probabilistic inference. We present formulations of inference\nproblems in Markov random fields (MRFs) as computation of equilibria in a\ncertain class of game-theoretic graphical models. We concretely establishes the\nprecise connection between variational probabilistic inference in MRFs and\ncorrelated equilibria. No previous work exploits recent theoretical and\nempirical results from the literature on algorithmic and computational game\ntheory on the tractable, polynomial-time computation of exact or approximate\ncorrelated equilibria in graphical games with arbitrary, loopy graph structure.\nWe discuss how to design new algorithms with equally tractable guarantees for\nthe computation of approximate variational inference in MRFs. Also, inspired by\na previously stated game-theoretic view of state-of-the-art tree-reweighed\n(TRW) message-passing techniques for belief inference as zero-sum game, we\npropose a different, general-sum potential game to design approximate\nfictitious-play techniques. We perform synthetic experiments evaluating our\nproposed approximation algorithms with standard methods and TRW on several\nclasses of classical Ising models (i.e., with binary random variables). We also\nevaluate the algorithms using Ising models learned from the MNIST dataset. Our\nexperiments show that our global approach is competitive, particularly shinning\nin a class of Ising models with constant, \"highly attractive\" edge-weights, in\nwhich it is often better than all other alternatives we evaluated. With a\nnotable exception, our more local approach was not as effective. Yet, in\nfairness, almost all of the alternatives are often no better than a simple\nbaseline: estimate 0.5. \n\n"}
{"id": "1604.03632", "contents": "Title: Strategyproof Peer Selection using Randomization, Partitioning, and\n  Apportionment Abstract: Peer reviews, evaluations, and selections are a fundamental aspect of modern\nscience. Funding bodies the world over employ experts to review and select the\nbest proposals from those submitted for funding. The problem of peer selection,\nhowever, is much more general: a professional society may want to give a subset\nof its members awards based on the opinions of all members; an instructor for a\nMassive Open Online Course (MOOC) or an online course may want to crowdsource\ngrading; or a marketing company may select ideas from group brainstorming\nsessions based on peer evaluation.\n  We make three fundamental contributions to the study of peer selection, a\nspecific type of group decision-making problem, studied in computer science,\neconomics, and political science. First, we propose a novel mechanism that is\nstrategyproof, i.e., agents cannot benefit by reporting insincere valuations.\nSecond, we demonstrate the effectiveness of our mechanism by a comprehensive\nsimulation-based comparison with a suite of mechanisms found in the literature.\nFinally, our mechanism employs a randomized rounding technique that is of\nindependent interest, as it solves the apportionment problem that arises in\nvarious settings where discrete resources such as parliamentary representation\nslots need to be divided proportionally. \n\n"}
{"id": "1604.03655", "contents": "Title: A Discrete and Bounded Envy-Free Cake Cutting Protocol for Any Number of\n  Agents Abstract: We consider the well-studied cake cutting problem in which the goal is to\nfind an envy-free allocation based on queries from $n$ agents. The problem has\nreceived attention in computer science, mathematics, and economics. It has been\na major open problem whether there exists a discrete and bounded envy-free\nprotocol. We resolve the problem by proposing a discrete and bounded envy-free\nprotocol for any number of agents. The maximum number of queries required by\nthe protocol is $n^{n^{n^{n^{n^n}}}}$. We additionally show that even if we do\nnot run our protocol to completion, it can find in at most $n^3{(n^2)}^n$\nqueries a partial allocation of the cake that achieves proportionality (each\nagent gets at least $1/n$ of the value of the whole cake) and envy-freeness.\nFinally we show that an envy-free partial allocation can be computed in at most\n$n^3{(n^2)}^n$ queries such that each agent gets a connected piece that gives\nthe agent at least $1/(3n)$ of the value of the whole cake. \n\n"}
{"id": "1604.05471", "contents": "Title: Managing Overstaying Electric Vehicles in Park-and-Charge Facilities Abstract: With the increase in adoption of Electric Vehicles (EVs), proper utilization\nof the charging infrastructure is an emerging challenge for service providers.\nOverstaying of an EV after a charging event is a key contributor to low\nutilization. Since overstaying is easily detectable by monitoring the power\ndrawn from the charger, managing this problem primarily involves designing an\nappropriate \"penalty\" during the overstaying period. Higher penalties do\ndiscourage overstaying; however, due to uncertainty in parking duration, less\npeople would find such penalties acceptable, leading to decreased utilization\n(and revenue). To analyze this central trade-off, we develop a novel framework\nthat integrates models for realistic user behavior into queueing dynamics to\nlocate the optimal penalty from the points of view of utilization and revenue,\nfor different values of the external charging demand. Next, when the model\nparameters are unknown, we show how an online learning algorithm, such as UCB,\ncan be adapted to learn the optimal penalty. Our experimental validation, based\non charging data from London, shows that an appropriate penalty can increase\nboth utilization and revenue while significantly reducing overstaying. \n\n"}
{"id": "1604.05972", "contents": "Title: Optimal online escape path against a certificate Abstract: More than fifty years ago, Bellman asked for the best escape path within a\nknown forest but for an unknown starting position. This deterministic finite\npath is the shortest path that leads out of a given environment from any\nstarting point. There are some worst case positions where the full path length\nis required. Up to now such a fixed ultimate optimal escape path for a known\nshape for any starting position is only known for some special convex shapes\n(i.e., circles, strips of a given width, fat convex bodies, some isosceles\ntriangles). Therefore, we introduce a different, simple and intuitive escape\npath, the so-called certificate path. This escape path depends on the starting\nposition s and takes the distances from s to the outer boundary of the\nenvironment into account. Due to the additional information, the certificate\npath always (for any position s) leaves the environment earlier than the\nultimate escape path, in the above convex examples. Next we assume that fewer\ninformation is available. Neither the precise shape of the envir- onment, nor\nthe location of the starting point is known. For a class of environments\n(convex shapes and shapes with kernel positions), we design an online strategy\nthat always leaves the environment. We show that the path length for leaving\nthe environment is always shorter than 3.318764 the length of the corresponding\ncertificate path. We also give a lower bound of 3.313126, which shows that for\nthe above class of environments the factor 3.318764 is (almost) tight. \n\n"}
{"id": "1604.06443", "contents": "Title: Robust Estimators in High Dimensions without the Computational\n  Intractability Abstract: We study high-dimensional distribution learning in an agnostic setting where\nan adversary is allowed to arbitrarily corrupt an $\\varepsilon$-fraction of the\nsamples. Such questions have a rich history spanning statistics, machine\nlearning and theoretical computer science. Even in the most basic settings, the\nonly known approaches are either computationally inefficient or lose\ndimension-dependent factors in their error guarantees. This raises the\nfollowing question:Is high-dimensional agnostic distribution learning even\npossible, algorithmically?\n  In this work, we obtain the first computationally efficient algorithms with\ndimension-independent error guarantees for agnostically learning several\nfundamental classes of high-dimensional distributions: (1) a single Gaussian,\n(2) a product distribution on the hypercube, (3) mixtures of two product\ndistributions (under a natural balancedness condition), and (4) mixtures of\nspherical Gaussians. Our algorithms achieve error that is independent of the\ndimension, and in many cases scales nearly-linearly with the fraction of\nadversarially corrupted samples. Moreover, we develop a general recipe for\ndetecting and correcting corruptions in high-dimensions, that may be applicable\nto many other problems. \n\n"}
{"id": "1604.08179", "contents": "Title: Strategic Formation of Heterogeneous Networks Abstract: We establish a network formation game for the Internet's Autonomous System\n(AS) interconnection topology. The game includes different types of players,\naccounting for the heterogeneity of ASs in the Internet. In this network\nformation game, the utility of a player depends on the network structure, e.g.,\nthe distances between nodes and the cost of links. We also consider the case\nwhere utility (or monetary) transfers are allowed between the players. We\nincorporate reliability considerations in the player's utility function, and\nanalyze static properties of the game as well as its dynamic evolution. We\nprovide dynamic analysis of topological quantities, and explain the prevalence\nof some \"network motifs\" in the Internet graph. We assess our predictions with\nreal-world data. \n\n"}
{"id": "1604.08191", "contents": "Title: Modeling Single-Peakedness for Votes with Ties Abstract: Single-peakedness is one of the most important and well-known domain\nrestrictions on preferences. The computational study of single-peaked\nelectorates has largely been restricted to elections with tie-free votes, and\nrecent work that studies the computational complexity of manipulative attacks\nfor single-peaked elections for votes with ties has been restricted to\nnonstandard models of single-peaked preferences for top orders. We study the\ncomputational complexity of manipulation for votes with ties for the standard\nmodel of single-peaked preferences and for single-plateaued preferences. We\nshow that these models avoid the anomalous complexity behavior exhibited by the\nother models. We also state a surprising result on the relation between the\nsocietal axis and the complexity of manipulation for single-peaked preferences. \n\n"}
{"id": "1605.00405", "contents": "Title: Gradient Descent Only Converges to Minimizers: Non-Isolated Critical\n  Points and Invariant Regions Abstract: Given a non-convex twice differentiable cost function f, we prove that the\nset of initial conditions so that gradient descent converges to saddle points\nwhere \\nabla^2 f has at least one strictly negative eigenvalue has (Lebesgue)\nmeasure zero, even for cost functions f with non-isolated critical points,\nanswering an open question in [Lee, Simchowitz, Jordan, Recht, COLT2016].\nMoreover, this result extends to forward-invariant convex subspaces, allowing\nfor weak (non-globally Lipschitz) smoothness assumptions. Finally, we produce\nan upper bound on the allowable step-size. \n\n"}
{"id": "1605.01021", "contents": "Title: An Information Theoretic Framework For Designing Information Elicitation\n  Mechanisms That Reward Truth-telling Abstract: In the setting where information cannot be verified, we propose a simple yet\npowerful information theoretical framework---the Mutual Information\nParadigm---for information elicitation mechanisms. Our framework pays every\nagent a measure of mutual information between her signal and a peer's signal.\nWe require that the mutual information measurement has the key property that\nany \"data processing\" on the two random variables will decrease the mutual\ninformation between them. We identify such information measures that generalize\nShannon mutual information.\n  Our Mutual Information Paradigm overcomes the two main challenges in\ninformation elicitation without verification: (1) how to incentivize effort and\navoid agents colluding to report random or identical responses (2) how to\nmotivate agents who believe they are in the minority to report truthfully.\n  Aided by the information measures we found, (1) we use the paradigm to design\na family of novel mechanisms where truth-telling is a dominant strategy and any\nother strategy will decrease every agent's expected payment (in the\nmulti-question, detail free, minimal setting where the number of questions is\nlarge); (2) we show the versatility of our framework by providing a unified\ntheoretical understanding of existing mechanisms---Peer Prediction [Miller\n2005], Bayesian Truth Serum [Prelec 2004], and Dasgupta and Ghosh [2013]---by\nmapping them into our framework such that theoretical results of those existing\nmechanisms can be reconstructed easily.\n  We also give an impossibility result which illustrates, in a certain sense,\nthe the optimality of our framework. \n\n"}
{"id": "1605.01451", "contents": "Title: Boltzmann meets Nash: Energy-efficient routing in optical networks under\n  uncertainty Abstract: Motivated by the massive deployment of power-hungry data centers for service\nprovisioning, we examine the problem of routing in optical networks with the\naim of minimizing traffic-driven power consumption. To tackle this issue,\nrouting must take into account energy efficiency as well as capacity\nconsiderations; moreover, in rapidly-varying network environments, this must be\naccomplished in a real-time, distributed manner that remains robust in the\npresence of random disturbances and noise. In view of this, we derive a pricing\nscheme whose Nash equilibria coincide with the network's socially optimum\nstates, and we propose a distributed learning method based on the Boltzmann\ndistribution of statistical mechanics. Using tools from stochastic calculus, we\nshow that the resulting Boltzmann routing scheme exhibits remarkable\nconvergence properties under uncertainty: specifically, the long-term average\nof the network's power consumption converges within $\\varepsilon$ of its\nminimum value in time which is at most $\\tilde O(1/\\varepsilon^2)$,\nirrespective of the fluctuations' magnitude; additionally, if the network\nadmits a strict, non-mixing optimum state, the algorithm converges to it -\nagain, no matter the noise level. Our analysis is supplemented by extensive\nnumerical simulations which show that Boltzmann routing can lead to a\nsignificant decrease in power consumption over basic, shortest-path routing\nschemes in realistic network conditions. \n\n"}
{"id": "1605.01510", "contents": "Title: The Impact of Worst-Case Deviations in Non-Atomic Network Routing Games Abstract: We introduce a unifying model to study the impact of worst-case latency\ndeviations in non-atomic selfish routing games. In our model, latencies are\nsubject to (bounded) deviations which are taken into account by the players.\nThe quality deterioration caused by such deviations is assessed by the\n$\\textit{Deviation Ratio}$, i.e., the worst-case ratio of the cost of a Nash\nflow with respect to deviated latencies and the cost of a Nash flow with\nrespect to the unaltered latencies. This notion is inspired by the\n$\\textit{Price of Risk Aversion}$ recently studied by Nikolova and Stier-Moses.\nHere we generalize their model and results. In particular, we derive tight\nbounds on the Deviation Ratio for multi-commodity instances with a common\nsource and arbitrary non-negative and non-decreasing latency functions. These\nbounds exhibit a linear dependency on the size of the network (besides other\nparameters). In contrast, we show that for general multi-commodity networks an\nexponential dependency is inevitable. We also improve recent smoothness results\nto bound the Price of Risk Aversion. \n\n"}
{"id": "1605.03266", "contents": "Title: A Quantum Approach to the Unique Sink Orientation Problem Abstract: We consider quantum algorithms for the unique sink orientation problem on\ncubes. This problem is widely considered to be of intermediate computational\ncomplexity. This is because there no known polynomial algorithm (classical or\nquantum) from the problem and yet it arrises as part of a series of problems\nfor which it being intractable would imply complexity theoretic collapses. We\ngive a reduction which proves that if one can efficiently evaluate the kth\npower of the unique sink orientation outmap, then there exists a polynomial\ntime quantum algorithm for the unique sink orientation problem on cubes. \n\n"}
{"id": "1605.03826", "contents": "Title: Walrasian's Characterization and a Universal Ascending Auction Abstract: We introduce a novel characterization of all Walrasian price vectors in terms\nof forbidden over- and under demanded sets for monotone gross substitute\ncombinatorial auctions.\n  For ascending and descending auctions we suggest a universal framework for\nfinding the minimum or maximum Walrasian price vectors for monotone gross\nsubstitute combinatorial auctions. An ascending (descending) auction is\nguaranteed to find the minimum (maximum) Walrasian if and only if it follows\nthe suggested framework. \n\n"}
{"id": "1605.05114", "contents": "Title: Growth of Dimension in Complete Simple Games Abstract: The concept of dimension in simple games was introduced as a measure of the\nremoteness of a given game from a weighted game. Taylor and Zwicker (1993)\ndemonstrated that the dimension of a simple game can grow exponentially in the\nnumber of players. However, the problem of worst-case growth of the dimension\nin complete games was left open. Freixas and Puente (2008) showed that complete\ngames of arbitrary dimension exist and, in particular, their examples\ndemonstrate that the worst-case growth of dimension in complete games is at\nleast linear. In this paper, using a novel technique of Kurz and Napel (2016),\nwe demonstrate that the worst-case growth of dimension in complete simple games\nis exponential in the number of players. \n\n"}
{"id": "1605.07107", "contents": "Title: Revenue Maximization in Service Systems with Heterogeneous Customers Abstract: In this paper, we consider revenue maximization problem for a two server\nsystem in the presence of heterogeneous customers. We assume that the customers\ndiffer in their cost for unit delay and this is modeled as a continuous random\nvariable with a distribution $F.$ We also assume that each server charges an\nadmission price to each customer that decide to join its queue. We first\nconsider the monopoly problem where both the servers belong to a single\noperator. The heterogeneity of the customer makes the analysis of the problem\ndifficult. The difficulty lies in the inability to characterize the equilibrium\nqueue arrival rates as a function of the admission prices. We provide an\nequivalent formulation with the queue arrival rates as the optimization\nvariable simplifying the analysis for revenue rate maximization for the\nmonopoly. We then consider the duopoly problem where each server competes with\nthe other server to maximize its revenue rate. For the duopoly problem, the\ninterest is to obtain the set of admission prices satisfying the Nash\nequilibrium conditions. While the problem is in general difficult to analyze,\nwe consider the special case when the two servers are identical. For such a\nduopoly system, we obtain the necessary condition for existence of symmetric\nNash equilibrium of the admission prices. The knowledge of the distribution $F$\ncharacterizing the heterogeneity of the customers is necessary to solve the\nmonopoly and the duopoly problem. However, for most practical scenarios, the\nfunctional form of $F$ may not be known to the system operator and in such\ncases, the revenue maximizing prices cannot be determined. In the last part of\nthe paper, we provide a simple method to estimate the distribution $F$ by\nsuitably varying the admission prices. We illustrate the method with some\nnumerical examples. \n\n"}
{"id": "1605.07753", "contents": "Title: Deciding Maxmin Reachability in Half-Blind Stochastic Games Abstract: Two-player, turn-based, stochastic games with reachability conditions are\nconsidered, where the maximizer has no information (he is blind) and is\nrestricted to deterministic strategies whereas the minimizer is perfectly\ninformed. We ask the question of whether the game has maxmin 1, in other words\nwe ask whether for all $\\epsilon>0$ there exists a deterministic strategy for\nthe (blind) maximizer such that against all the strategies of the minimizer, it\nis possible to reach the set of final states with probability larger than\n$1-\\epsilon$. This problem is undecidable in general, but we define a class of\ngames, called leaktight half-blind games where the problem becomes decidable.\nWe also show that mixed strategies in general are stronger for both players and\nthat optimal strategies for the minimizer might require infinite-memory. \n\n"}
{"id": "1606.00898", "contents": "Title: Factoring Polynomials over Finite Fields using Drinfeld Modules with\n  Complex Multiplication Abstract: We present novel algorithms to factor polynomials over a finite field $\\F_q$\nof odd characteristic using rank $2$ Drinfeld modules with complex\nmultiplication. The main idea is to compute a lift of the Hasse invariant\n(modulo the polynomial $f(x) \\in \\F_q[x]$ to be factored) with respect to a\nDrinfeld module $\\phi$ with complex multiplication. Factors of $f(x)$ supported\non prime ideals with supersingular reduction at $\\phi$ have vanishing Hasse\ninvariant and can be separated from the rest. A Drinfeld module analogue of\nDeligne's congruence plays a key role in computing the Hasse invariant lift. We\npresent two algorithms based on this idea. The first algorithm chooses Drinfeld\nmodules with complex multiplication at random and has a quadratic expected run\ntime. The second is a deterministic algorithm with $O(\\sqrt{p})$ run time\ndependence on the characteristic $p$ of $\\F_q$. \n\n"}
{"id": "1606.01831", "contents": "Title: Window Parity Games: An Alternative Approach Toward Parity Games with\n  Time Bounds Abstract: Classical objectives in two-player zero-sum games played on graphs often deal\nwith limit behaviors of infinite plays: e.g., mean-payoff and total-payoff in\nthe quantitative setting, or parity in the qualitative one (a canonical way to\nencode omega-regular properties). Those objectives offer powerful abstraction\nmechanisms and often yield nice properties such as memoryless determinacy.\nHowever, their very nature provides no guarantee on time bounds within which\nsomething good can be witnessed. In this work, we consider two approaches\ntoward inclusion of time bounds in parity games. The first one, parity-response\ngames, is based on the notion of finitary parity games [CHH09] and parity games\nwith costs [FZ14,WZ16]. The second one, window parity games, is inspired by\nwindow mean-payoff games [CDRR15]. We compare the two approaches and show that\nwhile they prove to be equivalent in some contexts, window parity games offer a\nmore tractable alternative when the time bound is given as a parameter (P-c.\nvs. PSPACE-c.). In particular, it provides a conservative approximation of\nparity games computable in polynomial time. Furthermore, we extend both\napproaches to the multi-dimension setting. We give the full picture for both\ntypes of games with regard to complexity and memory bounds.\n  [CHH09] K. Chatterjee, T.A. Henzinger, F. Horn (2009): Finitary winning in\nomega-regular games. ACM Trans. Comput. Log. 11(1). [FZ14] N. Fijalkow, M.\nZimmermann (2014): Parity and Streett Games with Costs. LMCS 10(2). [WZ16] A.\nWeinert, M. Zimmermann (2016): Easy to Win, Hard to Master: Optimal Strategies\nin Parity Games with Costs. Proc. of CSL, LIPIcs, Schloss Dagstuhl - LZI. To\nappear. [CDRR15] K. Chatterjee, L. Doyen, M. Randour, J.-F. Raskin (2015):\nLooking at mean-payoff and total-payoff through windows. Information and\nComputation 242, pp. 25-52. \n\n"}
{"id": "1606.02194", "contents": "Title: The Price of Anarchy in Transportation Networks: Data-Driven Evaluation\n  and Reduction Strategies Abstract: Among the many functions a Smart City must support, transportation dominates\nin terms of resource consumption, strain on the environment, and frustration of\nits citizens. We study transportation networks under two different routing\npolicies, the commonly assumed selfish user-centric routing policy and a\nsocially-optimal system-centric one. We consider a performance metric of\nefficiency - the Price of Anarchy (PoA) - defined as the ratio of the total\ntravel latency cost under selfish routing over the corresponding quantity under\nsocially-optimal routing. We develop a data-driven approach to estimate the\nPoA, which we subsequently use to conduct a case study using extensive actual\ntraffic data from the Eastern Massachusetts road network. To estimate the PoA,\nour approach learns from data a complete model of the transportation network,\nincluding origin-destination demand and user preferences. We leverage this\nmodel to propose possible strategies to reduce the PoA and increase efficiency. \n\n"}
{"id": "1606.02221", "contents": "Title: Multi-resource defensive strategies for patrolling games with alarm\n  systems Abstract: Security Games employ game theoretical tools to derive resource allocation\nstrategies in security domains. Recent works considered the presence of alarm\nsystems, even suffering various forms of uncertainty, and showed that\ndisregarding alarm signals may lead to arbitrarily bad strategies. The central\nproblem with an alarm system, unexplored in other Security Games, is finding\nthe best strategy to respond to alarm signals for each mobile defensive\nresource. The literature provides results for the basic single-resource case,\nshowing that even in that case the problem is computationally hard. In this\npaper, we focus on the challenging problem of designing algorithms scaling with\nmultiple resources. First, we focus on finding the minimum number of resources\nassuring non-null protection to every target. Then, we deal with the\ncomputation of multi-resource strategies with different degrees of coordination\namong resources. For each considered problem, we provide a computational\nanalysis and propose algorithmic methods. \n\n"}
{"id": "1606.03062", "contents": "Title: Procrastination with variable present bias Abstract: Individuals working towards a goal often exhibit time inconsistent behavior,\nmaking plans and then failing to follow through. One well-known model of such\nbehavioral anomalies is present-bias discounting: individuals over-weight\npresent costs by a bias factor. This model explains many time-inconsistent\nbehaviors, but can make stark predictions in many settings: individuals either\nfollow the most efficient plan for reaching their goal or procrastinate\nindefinitely.\n  We propose a modification in which the present-bias parameter can vary over\ntime, drawn independently each step from a fixed distribution. Following\nKleinberg and Oren (2014), we use a weighted task graph to model task planning,\nand measure the cost of procrastination as the relative expected cost of the\nchosen path versus the optimal path. We use a novel connection to optimal\npricing theory to describe the structure of the worst-case task graph for any\npresent-bias distribution. We then leverage this structure to derive conditions\non the bias distribution under which the worst-case ratio is exponential (in\ntime) or constant. We also examine conditions on the task graph that lead to\nimproved procrastination ratios: graphs with a uniformly bounded distance to\nthe goal, and graphs in which the distance to the goal monotonically decreases\non any path. \n\n"}
{"id": "1606.03898", "contents": "Title: The flow network method Abstract: In this paper we propose an in-depth analysis of a method, called the flow\nnetwork method, which associates with any network a complete and\nquasi-transitive binary relation on its vertices. Such a method, originally\nproposed by Gvozdik (1987), is based on the concept of maximum flow. Given a\ncompetition involving two or more teams, the flow network method can be used to\nbuild a relation on the set of teams which establishes, for every ordered pair\nof teams, if the first one did at least as good as the second one in the\ncompetition. Such a relation naturally induces procedures for ranking teams and\nselecting the best $k$ teams of a competition. Those procedures are proved to\nsatisfy many desirable properties. \n\n"}
{"id": "1606.04145", "contents": "Title: Sample Complexity of Automated Mechanism Design Abstract: The design of revenue-maximizing combinatorial auctions, i.e. multi-item\nauctions over bundles of goods, is one of the most fundamental problems in\ncomputational economics, unsolved even for two bidders and two items for sale.\nIn the traditional economic models, it is assumed that the bidders' valuations\nare drawn from an underlying distribution and that the auction designer has\nperfect knowledge of this distribution. Despite this strong and oftentimes\nunrealistic assumption, it is remarkable that the revenue-maximizing\ncombinatorial auction remains unknown. In recent years, automated mechanism\ndesign has emerged as one of the most practical and promising approaches to\ndesigning high-revenue combinatorial auctions. The most scalable automated\nmechanism design algorithms take as input samples from the bidders' valuation\ndistribution and then search for a high-revenue auction in a rich auction\nclass. In this work, we provide the first sample complexity analysis for the\nstandard hierarchy of deterministic combinatorial auction classes used in\nautomated mechanism design. In particular, we provide tight sample complexity\nbounds on the number of samples needed to guarantee that the empirical revenue\nof the designed mechanism on the samples is close to its expected revenue on\nthe underlying, unknown distribution over bidder valuations, for each of the\nauction classes in the hierarchy. In addition to helping set automated\nmechanism design on firm foundations, our results also push the boundaries of\nlearning theory. In particular, the hypothesis functions used in our contexts\nare defined through multi-stage combinatorial optimization procedures, rather\nthan simple decision boundaries, as are common in machine learning. \n\n"}
{"id": "1606.04592", "contents": "Title: Algebraic Problems Equivalent to Beating Exponent 3/2 for Polynomial\n  Factorization over Finite Fields Abstract: The fastest known algorithm for factoring univariate polynomials over finite\nfields is the Kedlaya-Umans (fast modular composition) implementation of the\nKaltofen-Shoup algorithm. It is randomized and takes $\\widetilde{O}(n^{3/2}\\log\nq + n \\log^2 q)$ time to factor polynomials of degree $n$ over the finite field\n$\\mathbb{F}_q$ with $q$ elements. A significant open problem is if the $3/2$\nexponent can be improved. We study a collection of algebraic problems and\nestablish a web of reductions between them. A consequence is that an algorithm\nfor any one of these problems with exponent better than $3/2$ would yield an\nalgorithm for polynomial factorization with exponent better than $3/2$. \n\n"}
{"id": "1606.06244", "contents": "Title: Learning in Games: Robustness of Fast Convergence Abstract: We show that learning algorithms satisfying a $\\textit{low approximate\nregret}$ property experience fast convergence to approximate optimality in a\nlarge class of repeated games. Our property, which simply requires that each\nlearner has small regret compared to a $(1+\\epsilon)$-multiplicative\napproximation to the best action in hindsight, is ubiquitous among learning\nalgorithms; it is satisfied even by the vanilla Hedge forecaster. Our results\nimprove upon recent work of Syrgkanis et al. [SALS15] in a number of ways. We\nrequire only that players observe payoffs under other players' realized\nactions, as opposed to expected payoffs. We further show that convergence\noccurs with high probability, and show convergence under bandit feedback.\nFinally, we improve upon the speed of convergence by a factor of $n$, the\nnumber of players. Both the scope of settings and the class of algorithms for\nwhich our analysis provides fast convergence are considerably broader than in\nprevious work.\n  Our framework applies to dynamic population games via a low approximate\nregret property for shifting experts. Here we strengthen the results of\nLykouris et al. [LST16] in two ways: We allow players to select learning\nalgorithms from a larger class, which includes a minor variant of the basic\nHedge algorithm, and we increase the maximum churn in players for which\napproximate optimality is achieved.\n  In the bandit setting we present a new algorithm which provides a \"small\nloss\"-type bound with improved dependence on the number of actions in utility\nsettings, and is both simple and efficient. This result may be of independent\ninterest. \n\n"}
{"id": "1606.06271", "contents": "Title: Dynamic Programming for One-Sided Partially Observable Pursuit-Evasion\n  Games Abstract: Pursuit-evasion scenarios appear widely in robotics, security domains, and\nmany other real-world situations. We focus on two-player pursuit-evasion games\nwith concurrent moves, infinite horizon, and discounted rewards. We assume that\nthe players have a partial observability, however, the evader is given an\nadvantage of knowing the current position of the units of the pursuer. This\nsetting is particularly interesting for security domains where a robust\nstrategy, designed to maximize the utility in the worst-case scenario, is often\ndesirable. We provide, to the best of our knowledge, the first algorithm that\nprovably converges to the value of a partially observable pursuit-evasion game\nwith infinite horizon. Our algorithm extends well-known value iteration\nalgorithm by exploiting that (1) the value functions of our game depend only on\nposition of the pursuer and the belief he has about the current position of the\nevader, and (2) that these functions are piecewise linear and convex in the\nbelief space. \n\n"}
{"id": "1606.06510", "contents": "Title: Opportunities for Price Manipulation by Aggregators in Electricity\n  Markets Abstract: Aggregators are playing an increasingly crucial role in the integration of\nrenewable generation in power systems. However, the intermittent nature of\nrenewable generation makes market interactions of aggregators difficult to\nmonitor and regulate, raising concerns about potential market manipulation by\naggregators. In this paper, we study this issue by quantifying the profit an\naggregator can obtain through strategic curtailment of generation in an\nelectricity market. We show that, while the problem of maximizing the benefit\nfrom curtailment is hard in general, efficient algorithms exist when the\ntopology of the network is radial (acyclic). Further, we highlight that\nsignificant increases in profit are possible via strategic curtailment in\npractical settings. \n\n"}
{"id": "1606.07517", "contents": "Title: Coordination Games on Directed Graphs Abstract: We study natural strategic games on directed graphs, which capture the idea\nof coordination in the absence of globally common strategies. We show that\nthese games do not need to have a pure Nash equilibrium and that the problem of\ndetermining their existence is NP-complete. The same holds for strong\nequilibria. We also exhibit some classes of games for which strong equilibria\nexist and prove that a strong equilibrium can then be found in linear time. \n\n"}
{"id": "1606.08971", "contents": "Title: Joint Millimeter Wave and Microwave Resources Allocation in Cellular\n  Networks with Dual-Mode Base Stations Abstract: In this paper, a novel dual-mode scheduling framework is proposed that\njointly performs user applications (UA) selection and scheduling over microwave\n($\\mu$W) and millimeter wave (mmW) bands. The proposed scheduling framework\nutilizes a set of context information, including the channel state information,\nthe delay tolerance and required load per UA, and the uncertainty of mmW\nchannels, to maximize the quality-of-service (QoS) per UA. The scheduling\nproblem is formulated as an optimization with minimum unsatisfied relations\n(min-UR) problem which is shown to be challenging to solve. Consequently, a\nlong-term scheduling framework, consisting of two stages, is proposed. Within\nthis framework, first, the scheduling over $\\mu$W band is formulated as a\nmatching game and to solve this problem, a novel algorithm is proposed and\nshown to yield a two-sided stable resource allocation. Second, over the mmW\nband, the scheduling problem is formulated as a 0-1 Knapsack problem and a\nnovel algorithm is proposed to solve it. Furthermore, it is shown that the\nproposed framework can find an effective scheduling solution, over both $\\mu$W\nand mmW, in polynomial time. Simulation results show that, compared with\nconventional scheduling schemes, the proposed approach significantly increases\nthe number of satisfied UAs and enhances the users' quality-of-experience. \n\n"}
{"id": "1606.09424", "contents": "Title: Variance Allocation and Shapley Value Abstract: Motivated by the problem of utility allocation in a portfolio under a\nMarkowitz mean-variance choice paradigm, we propose an allocation criterion for\nthe variance of the sum of $n$ possibly dependent random variables. This\ncriterion, the Shapley value, requires to translate the problem into a\ncooperative game. The Shapley value has nice properties, but, in general, is\ncomputationally demanding. The main result of this paper shows that in our\nparticular case the Shapley value has a very simple form that can be easily\ncomputed. The same criterion is used also to allocate the standard deviation of\nthe sum of $n$ random variables and a conjecture about the relation of the\nvalues in the two games is formulated. \n\n"}
{"id": "1606.09449", "contents": "Title: Clique-Width and Directed Width Measures for Answer-Set Programming Abstract: Disjunctive Answer Set Programming (ASP) is a powerful declarative\nprogramming paradigm whose main decision problems are located on the second\nlevel of the polynomial hierarchy. Identifying tractable fragments and\ndeveloping efficient algorithms for such fragments are thus important\nobjectives in order to complement the sophisticated ASP systems available to\ndate. Hard problems can become tractable if some problem parameter is bounded\nby a fixed constant; such problems are then called fixed-parameter tractable\n(FPT). While several FPT results for ASP exist, parameters that relate to\ndirected or signed graphs representing the program at hand have been neglected\nso far. In this paper, we first give some negative observations showing that\ndirected width measures on the dependency graph of a program do not lead to FPT\nresults. We then consider the graph parameter of signed clique-width and\npresent a novel dynamic programming algorithm that is FPT w.r.t. this\nparameter. Clique-width is more general than the well-known treewidth, and, to\nthe best of our knowledge, ours is the first FPT algorithm for bounded\nclique-width for reasoning problems beyond SAT. \n\n"}
{"id": "1607.00537", "contents": "Title: Badge System Analysis and Design Abstract: To incentivize users' participations and steer their online activities,\nonline social networks start to provide users with various kinds of rewards for\ntheir contributions to the sites. The most frequently distributed rewards\ninclude account levels, reputation scores, different kinds of badges, and even\nmaterial awards like small gifts and cash back, etc. Attracted by these\nrewards, users will spend more time using the network services. In this paper,\nwe will mainly focus on \"badges reward systems\" but the proposed models can be\napplied to other reward systems as well. Badges are small icons attached to\nusers' homepages and profiles denoting their achievements. People like to\naccumulate badge for various reasons and different badges can have specific\nvalues for them. Meanwhile, to get badges, they also need to exert efforts to\nfinish the required tasks, which can lead to certain costs. To understand and\nmodel users' motivations in badge achievement activities, we will study an\nexisting badge system launched inside a real-world online social network,\nFoursquare, in this paper. At the same time, to maximize users' contributions\nto online social networks, social network system designers need to determine\nthe optimal badge system mechanism carefully. Badge system mechanism describes\nvarious detailed aspects of the system and can involve many parameters, e.g.,\ncategories of existing badges, number of badges available as well as the\nminimum contributions required to obtain the badges, which all need to be\ndesigned with meticulous investigations. Based on the model of users' badges\naccumulating activities, in this paper, we will also study how to design the\nbadge system that can incentivize the maximum users' contributions to the\nsocial networks. \n\n"}
{"id": "1607.01569", "contents": "Title: Nash Social Welfare Approximation for Strategic Agents Abstract: The fair division of resources is an important age-old problem that has led\nto a rich body of literature. At the center of this literature lies the\nquestion of whether there exist fair mechanisms despite strategic behavior of\nthe agents. A fundamental objective function used for measuring fair outcomes\nis the Nash social welfare, defined as the geometric mean of the agent\nutilities. This objective function is maximized by widely known solution\nconcepts such as Nash bargaining and the competitive equilibrium with equal\nincomes. In this work we focus on the question of (approximately) implementing\nthe Nash social welfare. The starting point of our analysis is the Fisher\nmarket, a fundamental model of an economy, whose benchmark is precisely the\n(weighted) Nash social welfare. We begin by studying two extreme classes of\nvaluations functions, namely perfect substitutes and perfect complements, and\nfind that for perfect substitutes, the Fisher market mechanism has a constant\napproximation: at most 2 and at least e1e. However, for perfect complements,\nthe Fisher market does not work well, its bound degrading linearly with the\nnumber of players.\n  Strikingly, the Trading Post mechanism---an indirect market mechanism also\nknown as the Shapley-Shubik game---has significantly better performance than\nthe Fisher market on its own benchmark. Not only does Trading Post achieve an\napproximation of 2 for perfect substitutes, but this bound holds for all\nconcave utilities and becomes arbitrarily close to optimal for Leontief\nutilities (perfect complements), where it reaches $(1+\\epsilon)$ for every\n$\\epsilon > 0$. Moreover, all the Nash equilibria of the Trading Post mechanism\nare pure for all concave utilities and satisfy an important notion of fairness\nknown as proportionality. \n\n"}
{"id": "1607.01718", "contents": "Title: Graphons, mergeons, and so on! Abstract: In this work we develop a theory of hierarchical clustering for graphs. Our\nmodeling assumption is that graphs are sampled from a graphon, which is a\npowerful and general model for generating graphs and analyzing large networks.\nGraphons are a far richer class of graph models than stochastic blockmodels,\nthe primary setting for recent progress in the statistical theory of graph\nclustering. We define what it means for an algorithm to produce the \"correct\"\nclustering, give sufficient conditions in which a method is statistically\nconsistent, and provide an explicit algorithm satisfying these properties. \n\n"}
{"id": "1607.01872", "contents": "Title: Downlink Cell Association and Load Balancing for Joint Millimeter\n  Wave-Microwave Cellular Networks Abstract: The integration of millimeter-wave base stations (mmW-BSs) with conventional\nmicrowave base stations ($\\mu$W-BSs) is a promising solution for enhancing the\nquality-of-service (QoS) of emerging 5G networks. However, the significant\ndifferences in the signal propagation characteristics over the mmW and $\\mu$W\nfrequency bands will require novel cell association schemes cognizant of both\nmmW and $\\mu$W systems. In this paper, a novel cell association framework is\nproposed that considers both the blockage probability and the achievable rate\nto assign user equipments (UEs) to mmW-BSs or $\\mu$W-BSs. The problem is\nformulated as a one-to-many matching problem with minimum quota constraints for\nthe BSs that provides an efficient way to balance the load over the mmW and\n$\\mu$W frequency bands. To solve the problem, a distributed algorithm is\nproposed that is guaranteed to yield a Pareto optimal and two-sided stable\nsolution. Simulation results show that the proposed matching with minimum quota\n(MMQ) algorithm outperforms the conventional max-RSSI and max-SINR cell\nassociation schemes. In addition, it is shown that the proposed MMQ algorithm\ncan effectively balance the number of UEs associated with the $\\mu$W-BSs and\nmmW-BSs and achieve further gains, in terms of the average sum rate. \n\n"}
{"id": "1607.02071", "contents": "Title: On Selfish Creation of Robust Networks Abstract: Robustness is one of the key properties of nowadays networks. However,\nrobustness cannot be simply enforced by design or regulation since many\nimportant networks, most prominently the Internet, are not created and\ncontrolled by a central authority. Instead, Internet-like networks emerge from\nstrategic decisions of many selfish agents. Interestingly, although lacking a\ncoordinating authority, such naturally grown networks are surprisingly robust\nwhile at the same time having desirable properties like a small diameter.\n  To investigate this phenomenon we present the first simple model for selfish\nnetwork creation which explicitly incorporates agents striving for a central\nposition in the network while at the same time protecting themselves against\nrandom edge-failure. We show that networks in our model are diverse and we\nprove the versatility of our model by adapting various properties and\ntechniques from the non-robust versions which we then use for establishing\nbounds on the Price of Anarchy. Moreover, we analyze the computational hardness\nof finding best possible strategies and investigate the game dynamics of our\nmodel. \n\n"}
{"id": "1607.02420", "contents": "Title: Blockchain Mining Games Abstract: We study the strategic considerations of miners participating in the\nbitcoin's protocol. We formulate and study the stochastic game that underlies\nthese strategic considerations. The miners collectively build a tree of blocks,\nand they are paid when they create a node (mine a block) which will end up in\nthe path of the tree that is adopted by all. Since the miners can hide newly\nmined nodes, they play a game with incomplete information. Here we consider two\nsimplified forms of this game in which the miners have complete information. In\nthe simplest game the miners release every mined block immediately, but are\nstrategic on which blocks to mine. In the second more complicated game, when a\nblock is mined it is announced immediately, but it may not be released so that\nother miners cannot continue mining from it. A miner not only decides which\nblocks to mine, but also when to release blocks to other miners. In both games,\nwe show that when the computational power of each miner is relatively small,\ntheir best response matches the expected behavior of the bitcoin designer.\nHowever, when the computational power of a miner is large, he deviates from the\nexpected behavior, and other Nash equilibria arise. \n\n"}
{"id": "1607.03356", "contents": "Title: Extending Finite Memory Determinacy to Multiplayer Games Abstract: We show that under some general conditions the finite memory determinacy of a\nclass of two-player win/lose games played on finite graphs implies the\nexistence of a Nash equilibrium built from finite memory strategies for the\ncorresponding class of multi-player multi-outcome games. This generalizes a\nprevious result by Brihaye, De Pril and Schewe. For most of our conditions we\nprovide counterexamples showing that they cannot be dispensed with.\n  Our proofs are generally constructive, that is, provide upper bounds for the\nmemory required, as well as algorithms to compute the relevant winning\nstrategies. \n\n"}
{"id": "1607.03685", "contents": "Title: On Solutions for the Maximum Revenue Multi-item Auction under\n  Dominant-Strategy and Bayesian Implementations Abstract: Very few exact solutions are known for the monopolist's $k$-item $n$-buyer\nmaximum revenue problem with additive valuation in which $k, n >1$ and the\nbuyers $i$ have independent private distributions $F^j_i$ on items $j$. In this\npaper we derive exact formulas for the maximum revenue when $k=2$ and $F^j_i$\nare any IID distributions on support of size 2, for both the dominant-strategy\n(DIC) and the Bayesian (BIC) implementations. The formulas lead to the simple\ncharacterization that, the two implementations have identical maximum revenue\nif and only if selling-separately is optimal for the distribution. Our results\nalso give the first demonstration, in this setting, of revenue gaps between the\ntwo implementations. For instance, if $k=n=2$ and\n$Pr\\{X_F=1\\}=Pr\\{X_F=2\\}=\\frac{1}{2}$, then the maximum revenue in the Bayesian\nimplementation exceeds that in the dominant-strategy by exactly $2\\%$; the same\ngap exists for the continuous uniform distribution $X_F$ over $[a, a+1]\\cup[2a,\n2a+1]$ for all large $a$. \n\n"}
{"id": "1607.03760", "contents": "Title: Distributed Games and Strategies Abstract: A summary of work on distributed games and strategies done within the first\nthree years of the ERC project ECSYM is presented. \n\n"}
{"id": "1607.04229", "contents": "Title: Improving Viterbi is Hard: Better Runtimes Imply Faster Clique\n  Algorithms Abstract: The classic algorithm of Viterbi computes the most likely path in a Hidden\nMarkov Model (HMM) that results in a given sequence of observations. It runs in\ntime $O(Tn^2)$ given a sequence of $T$ observations from a HMM with $n$ states.\nDespite significant interest in the problem and prolonged effort by different\ncommunities, no known algorithm achieves more than a polylogarithmic speedup.\n  In this paper, we explain this difficulty by providing matching conditional\nlower bounds. We show that the Viterbi algorithm runtime is optimal up to\nsubpolynomial factors even when the number of distinct observations is small.\nOur lower bounds are based on assumptions that the best known algorithms for\nthe All-Pairs Shortest Paths problem (APSP) and for the Max-Weight $k$-Clique\nproblem in edge-weighted graphs are essentially tight.\n  Finally, using a recent algorithm by Green Larsen and Williams for online\nBoolean matrix-vector multiplication, we get a $2^{\\Omega(\\sqrt {\\log n})}$\nspeedup for the Viterbi algorithm when there are few distinct transition\nprobabilities in the HMM. \n\n"}
{"id": "1607.04336", "contents": "Title: The Decision Tree Complexity for $k$-SUM is at most Nearly Quadratic Abstract: Following a recent improvement of Cardinal et al. on the complexity of a\nlinear decision tree for $k$-SUM, resulting in $O(n^3 \\log^3{n})$ linear\nqueries, we present a further improvement to $O(n^2 \\log^2{n})$ such queries. \n\n"}
{"id": "1607.04710", "contents": "Title: A Descending Price Auction for Matching Markets Abstract: This work presents a descending-price-auction algorithm to obtain the maximum\nmarket-clearing price vector (MCP) in unit-demand matching markets with m items\nby exploiting the combinatorial structure. With a shrewd choice of goods for\nwhich the prices are reduced in each step, the algorithm only uses the\ncombinatorial structure, which avoids solving LPs and enjoys a strongly\npolynomial runtime of $O(m^4)$. Critical to the algorithm is determining the\nset of under-demanded goods for which we reduce the prices simultaneously in\neach step of the algorithm. This we accomplish by choosing the subset of goods\nthat maximize a skewness function, which makes the bipartite graph series\nconverges to the combinatorial structure at the maximum MCP in $O(m^2)$ steps.\nA graph coloring algorithm is proposed to find the set of goods with the\nmaximal skewness value that yields $O(m^4)$ complexity. \n\n"}
{"id": "1607.05133", "contents": "Title: Improved Hardness for Cut, Interdiction, and Firefighter Problems Abstract: We study variants of the classic $s$-$t$ cut problem and prove the following\nimproved hardness results assuming the Unique Games Conjecture (UGC).\n  - For any constant $k \\geq 2$ and $\\epsilon > 0$, we show that Directed\nMulticut with $k$ source-sink pairs is hard to approximate within a factor $k -\n\\epsilon$. This matches the trivial $k$-approximation algorithm. By a simple\nreduction, our result for $k = 2$ implies that Directed Multiway Cut with two\nterminals (also known as $s$-$t$ Bicut) is hard to approximate within a factor\n$2 - \\epsilon$, matching the trivial $2$-approximation algorithm. Previously,\nthe best hardness factor for these problems (for constant $k$) was $1.5 -\n\\epsilon$ under the UGC.\n  - For Length-Bounded Cut and Shortest Path Interdiction, we show that both\nproblems are hard to approximate within any constant factor, even if we allow\nbicriteria approximation. If we want to cut vertices or the graph is directed,\nour hardness factor for Length-Bounded Cut matches the best approximation ratio\nup to a constant. Previously, the best hardness factor was $1.1377$ for\nLength-Bounded Cut and $2$ for Shortest Path Interdiction.\n  - Assuming a variant of the UGC (implied by another variant of Bansal and\nKhot), we prove that it is hard to approximate Resource Minimization Fire\nContainment within any constant factor. Previously, the best hardness factor\nwas $2$.\n  Our results are based on a general method of converting an integrality gap\ninstance to a length-control dictatorship test for variants of the $s$-$t$ cut\nproblem, which may be useful for other problems. \n\n"}
{"id": "1607.06711", "contents": "Title: Algorithmic and optimization aspects of Brascamp-Lieb inequalities, via\n  Operator Scaling Abstract: The celebrated Brascamp-Lieb (BL) inequalities (and their extensions) are an\nimportant mathematical tool, unifying and generalizing numerous inequalities in\nanalysis, convex geometry and information theory. While their structural theory\nis very well understood, far less is known about computing their main\nparameters.\n  We give polynomial time algorithms to compute feasibility of BL-datum, the\noptimal BL-constant and a weak separation oracle for the BL-polytope. The same\nresult holds for the so-called Reverse BL inequalities of Barthe. The best\nknown algorithms for any of these tasks required at least exponential time.\n  The algorithms are obtained by a simple efficient reduction of a given\nBL-datum to an instance of the Operator Scaling problem defined by Gurvits, for\nwhich the present authors have provided a polynomial time algorithm. This\nreduction implies algorithmic versions of many of the known structural results,\nand in some cases provide proofs that are different or simpler than existing\nones.\n  Of particular interest is the fact that the operator scaling algorithm is\ncontinuous in its input. Thus as a simple corollary of our reduction we obtain\nexplicit bounds on the magnitude and continuity of the BL-constant in terms of\nthe BL-data. To the best of our knowledge no such bounds were known, as past\narguments relied on compactness. The continuity of BL-constants is important\nfor developing non-linear BL inequalities that have recently found so many\napplications. \n\n"}
{"id": "1607.06847", "contents": "Title: Decentralized Bayesian learning in dynamic games: A framework for\n  studying informational cascades Abstract: We study the problem of Bayesian learning in a dynamical system involving\nstrategic agents with asymmetric information. In a series of seminal papers in\nthe literature, this problem has been investigated under a simplifying model\nwhere myopically selfish players appear sequentially and act once in the game,\nbased on private noisy observations of the system state and public observation\nof past players' actions. It has been shown that there exist information\ncascades where users discard their private information and mimic the action of\ntheir predecessor. In this paper, we provide a framework for studying Bayesian\nlearning dynamics in a more general setting than the one described above. In\nparticular, our model incorporates cases where players are non-myopic and\nstrategically participate for the whole duration of the game, and cases where\nan endogenous process selects which subset of players will act at each time\ninstance. The proposed framework hinges on a sequential decomposition\nmethodology for finding structured perfect Bayesian equilibria (PBE) of a\ngeneral class of dynamic games with asymmetric information, where user-specific\nstates evolve as conditionally independent Markov processes and users make\nindependent noisy observations of their states. Using this methodology, we\nstudy a specific dynamic learning model where players make decisions about\npublic investment based on their estimates of everyone's types. We characterize\na set of informational cascades for this problem where learning stops for the\nteam as a whole. We show that in such cascades, all players' estimates of other\nplayers' types freeze even though each individual player asymptotically learns\nits own true type. \n\n"}
{"id": "1607.07200", "contents": "Title: Approximating Multicut and the Demand Graph Abstract: In the minimum Multicut problem, the input is an edge-weighted supply graph\n$G=(V,E)$ and a simple demand graph $H=(V,F)$. Either $G$ and $H$ are directed\n(DMulC) or both are undirected (UMulC). The goal is to remove a minimum weight\nset of edges in $G$ such that there is no path from $s$ to $t$ in the remaining\ngraph for any $(s,t) \\in F$. UMulC admits an $O(\\log k)$-approximation where\n$k$ is the vertex cover size of $H$ while the best known approximation for\nDMulC is $\\min\\{k, \\tilde{O}(n^{11/23})\\}$. These approximations are obtained\nby proving corresponding results on the multicommodity flow-cut gap. In\ncontrast to these results some special cases of Multicut, such as the\nwell-studied Multiway Cut problem, admit a constant factor approximation in\nboth undirected and directed graphs. Motivated by both concrete instances from\napplications and abstract considerations, we consider the role that the\nstructure of the demand graph $H$ plays in determining the approximability of\nMulticut.\n  In undirected graphs our main result is a $2$-approximation in $n^{O(t)}$\ntime when the demand graph $H$ excludes an induced matching of size $t$. This\ngives a constant factor approximation for a specific demand graph that\nmotivated this work.\n  In contrast to undirected graphs, we prove that in directed graphs such\napproximation algorithms can not exist. Assuming the Unique Games Conjecture\n(UGC), for a large class of fixed demand graphs DMulC cannot be approximated to\na factor better than worst-case flow-cut gap. As a consequence we prove that\nfor any fixed $k$, assuming UGC, DMulC with $k$ demand pairs is hard to\napproximate to within a factor better than $k$. On the positive side, we prove\nan approximation of $k$ when the demand graph excludes certain graphs as an\ninduced subgraph. This generalizes the Multiway Cut result to a much larger\nclass of demand graphs. \n\n"}
{"id": "1607.07306", "contents": "Title: The Costs and Benefits of Sharing: Sequential Individual Rationality and\n  Sequential Fairness Abstract: In designing dynamic shared service systems that incentivize customers to opt\nfor shared rather than exclusive service, the traditional notion of individual\nrationality may be insufficient, as a customer's estimated utility could\nfluctuate arbitrarily during their time in the shared system, as long as their\nrealized utility at service completion is not worse than that for exclusive\nservice. In this work, within a model that explicitly considers the\n\"inconvenience costs\" incurred by customers due to sharing, we introduce the\nnotion of sequential individual rationality (SIR) that requires that the\ndisutility of existing customers is nonincreasing as the system state changes\ndue to new customer arrivals. Next, under SIR, we observe that cost sharing can\nalso be viewed as benefit sharing, which inspires a natural definition of\nsequential fairness (SF) - the total incremental benefit due to a new customer\nis shared among existing customers in proportion to the incremental\ninconvenience suffered.\n  We demonstrate the effectiveness of these notions by applying them to a\nridesharing system, where unexpected detours to pick up subsequent passengers\ninconvenience the existing passengers. Imposing SIR and SF reveals interesting\nand surprising results, including: (a) natural limits on the incremental\ndetours permissible, (b) exact characterization of \"SIR-feasible\" routes, which\nboast sublinear upper and lower bounds on the fractional detours, (c) exact\ncharacterization of sequentially fair cost sharing schemes, which includes a\nstrong requirement that passengers must compensate each other for the detour\ninconveniences that they cause, and (d) new algorithmic problems related to and\nmotivated by SIR. \n\n"}
{"id": "1607.07684", "contents": "Title: The Price of Anarchy in Auctions Abstract: This survey outlines a general and modular theory for proving approximation\nguarantees for equilibria of auctions in complex settings. This theory\ncomplements traditional economic techniques, which generally focus on exact and\noptimal solutions and are accordingly limited to relatively stylized settings.\n  We highlight three user-friendly analytical tools: smoothness-type\ninequalities, which immediately yield approximation guarantees for many auction\nformats of interest in the special case of complete information and\ndeterministic strategies; extension theorems, which extend such guarantees to\nrandomized strategies, no-regret learning outcomes, and incomplete-information\nsettings; and composition theorems, which extend such guarantees from simpler\nto more complex auctions. Combining these tools yields tight worst-case\napproximation guarantees for the equilibria of many widely-used auction\nformats. \n\n"}
{"id": "1607.08192", "contents": "Title: Counting matchings with k unmatched vertices in planar graphs Abstract: We consider the problem of counting matchings in planar graphs. While perfect\nmatchings in planar graphs can be counted by a classical polynomial-time\nalgorithm, the problem of counting all matchings (possibly containing unmatched\nvertices, also known as defects) is known to be #P-complete on planar graphs.\nTo interpolate between the hard case of counting matchings and the easy case of\ncounting perfect matchings, we study the parameterized problem of counting\nmatchings with exactly k unmatched vertices in a planar graph G, on input G and\nk. This setting has a natural interpretation in statistical physics, and it is\na special case of counting perfect matchings in k-apex graphs (graphs that can\nbe turned planar by removing at most k vertices).\n  Starting from a recent #W[1]-hardness proof for counting perfect matchings on\nk-apex graphs, we obtain that counting matchings with k unmatched vertices in\nplanar graphs is #W[1]-hard. In contrast, given a plane graph G with s\ndistinguished faces, there is an $O(2^s \\cdot n^3)$ time algorithm for counting\nthose matchings with k unmatched vertices such that all unmatched vertices lie\non the distinguished faces. This implies an $f(k,s)\\cdot n^{O(1)}$ time\nalgorithm for counting perfect matchings in k-apex graphs whose apex\nneighborhood is covered by s faces. \n\n"}
{"id": "1608.00497", "contents": "Title: From Weak to Strong LP Gaps for all CSPs Abstract: We study the approximability of constraint satisfaction problems (CSPs) by\nlinear programming (LP) relaxations. We show that for every CSP, the\napproximation obtained by a basic LP relaxation, is no weaker than the\napproximation obtained using relaxations given by $\\Omega\\left(\\frac{\\log\nn}{\\log \\log n}\\right)$ levels of the Sherali-Adams hierarchy on instances of\nsize $n$.\n  It was proved by Chan et al. [FOCS 2013] that any polynomial size LP extended\nformulation is no stronger than relaxations obtained by a super-constant levels\nof the Sherali-Adams hierarchy.. Combining this with our result also implies\nthat any polynomial size LP extended formulation is no stronger than the basic\nLP.\n  Using our techniques, we also simplify and strengthen the result by Khot et\nal. [STOC 2014] on (strong) approximation resistance for LPs. They provided a\nnecessary and sufficient condition under which $\\Omega(\\log \\log n)$ levels of\nthe Sherali-Adams hierarchy cannot achieve an approximation better than a\nrandom assignment. We simplify their proof and strengthen the bound to\n$\\Omega\\left(\\frac{\\log n}{\\log \\log n}\\right)$ levels. \n\n"}
{"id": "1608.01540", "contents": "Title: Dividing goods or bads under additive utilities Abstract: We compare the Egalitarian Equivalent and the Competitive Equilibrium with\nEqual Incomes rules to divide a bundle of goods (heirlooms) or a bundle of bads\n(chores).\n  For goods the Competitive division fares better, as it is Resource Monotonic,\nand makes it harder to strategically misreport preferences. But for bads, the\nCompetitive rule, unlike the Egalitarian one, is multivalued, harder to\ncompute, and admits no continuous selection.\n  We also provide an axiomatic characterization of the Competitive rule based\non the simple formulation of Maskin Monotonicity under additive utilities. \n\n"}
{"id": "1608.03475", "contents": "Title: Data Collection and Wireless Communication in Internet of Things (IoT)\n  Using Economic Analysis and Pricing Models: A Survey Abstract: This paper provides a state-of-the-art literature review on economic analysis\nand pricing models for data collection and wireless communication in Internet\nof Things (IoT). Wireless Sensor Networks (WSNs) are the main component of IoT\nwhich collect data from the environment and transmit the data to the sink\nnodes. For long service time and low maintenance cost, WSNs require adaptive\nand robust designs to address many issues, e.g., data collection, topology\nformation, packet forwarding, resource and power optimization, coverage\noptimization, efficient task allocation, and security. For these issues,\nsensors have to make optimal decisions from current capabilities and available\nstrategies to achieve desirable goals. This paper reviews numerous applications\nof the economic and pricing models, known as intelligent rational\ndecision-making methods, to develop adaptive algorithms and protocols for WSNs.\nBesides, we survey a variety of pricing strategies in providing incentives for\nphone users in crowdsensing applications to contribute their sensing data.\nFurthermore, we consider the use of some pricing models in Machine-to-Machine\n(M2M) communication. Finally, we highlight some important open research issues\nas well as future research directions of applying economic and pricing models\nto IoT. \n\n"}
{"id": "1608.03574", "contents": "Title: Inapproximability Results for Approximate Nash Equilibria Abstract: We study the problem of finding approximate Nash equilibria that satisfy\ncertain conditions, such as providing good social welfare. In particular, we\nstudy the problem $\\epsilon$-NE $\\delta$-SW: find an $\\epsilon$-approximate\nNash equilibrium ($\\epsilon$-NE) that is within $\\delta$ of the best social\nwelfare achievable by an $\\epsilon$-NE. Our main result is that, if the\nexponential-time hypothesis (ETH) is true, then solving $\\left(\\frac{1}{8} -\n\\mathrm{O}(\\delta)\\right)$-NE $\\mathrm{O}(\\delta)$-SW for an $n\\times n$\nbimatrix game requires $n^{\\mathrm{\\widetilde \\Omega}(\\log n)}$ time. Building\non this result, we show similar conditional running time lower bounds on a\nnumber of decision problems for approximate Nash equilibria that do not involve\nsocial welfare, including maximizing or minimizing a certain player's payoff,\nor finding approximate equilibria contained in a given pair of supports. We\nshow quasi-polynomial lower bounds for these problems assuming that ETH holds,\nwhere these lower bounds apply to $\\epsilon$-Nash equilibria for all $\\epsilon\n< \\frac{1}{8}$. The hardness of these other decision problems has so far only\nbeen studied in the context of exact equilibria. \n\n"}
{"id": "1608.06819", "contents": "Title: Pricing and Optimization in Shared Vehicle Systems: An Approximation\n  Framework Abstract: Optimizing shared vehicle systems (bike/scooter/car/ride-sharing) is more\nchallenging compared to traditional resource allocation settings due to the\npresence of \\emph{complex network externalities} -- changes in the\ndemand/supply at any location affect future supply throughout the system within\nshort timescales. These externalities are well captured by steady-state\nMarkovian models, which are therefore widely used to analyze such systems.\nHowever, using such models to design pricing and other control policies is\ncomputationally difficult since the resulting optimization problems are\nhigh-dimensional and non-convex.\n  To this end, we develop a \\emph{rigorous approximation framework} for shared\nvehicle systems, providing a unified approach for a wide range of controls\n(pricing, matching, rebalancing), objective functions (throughput, revenue,\nwelfare), and system constraints (travel-times, welfare benchmarks,\nposted-price constraints). Our approach is based on the analysis of natural\nconvex relaxations, and obtains as special cases existing approximate-optimal\npolicies for limited settings, asymptotic-optimality results, and heuristic\npolicies. The resulting guarantees are non-asymptotic and parametric, and\nprovide operational insights into the design of real-world systems. In\nparticular, for any shared vehicle system with $n$ stations and $m$ vehicles,\nour framework obtains an approximation ratio of $1+(n-1)/m$, which is\nparticularly meaningful when $m/n$, the average number of vehicles per station,\nis large, as is often the case in practice. \n\n"}
{"id": "1608.07264", "contents": "Title: Quantum Game Application to Spectrum Scarcity Problems Abstract: Recent spectrum-sharing research has produced a strategy to address spectrum\nscarcity problems. This novel idea, named cognitive radio, considers that\nsecondary users can opportunistically exploit spectrum holes left temporarily\nunused by primary users. This presents a competitive scenario among cognitive\nusers, making it suitable for game theory treatment. In this work, we show that\nthe spectrum-sharing benefits of cognitive radio can be increased by designing\na medium access control based on quantum game theory. In this context, we\npropose a model to manage spectrum fairly and effectively, based on a\nmultiple-users multiple-choice quantum minority game. By taking advantage of\nquantum entanglement and quantum interference, it is possible to reduce the\nprobability of collision problems commonly associated with classic algorithms.\nCollision avoidance is an essential property for classic and quantum\ncommunications systems. In our model, two different scenarios are considered,\nto meet the requirements of different user strategies. The first considers\nsensor networks where the rational use of energy is a cornerstone; the second\nfocuses on installations where the quality of service of the entire network is\na priority. \n\n"}
{"id": "1608.07492", "contents": "Title: Mechanism Design Approach for Energy Efficiency Abstract: In this work we deploy a mechanism design approach for allocating a divisible\ncommodity (electricity in our example) among consumers. We consider each\nconsumer with an associated personal valuation function of the energy resource\nduring a certain time interval. We aim to select the optimal consumption\nprofile for every user avoiding consumption peaks when the total required\nenergy could exceed the energy production. The mechanism will be able to drive\nusers in shifting energy consumptions in different hours of the day. We start\nby presenting a very basic Vickrey-Clarke-Groves mechanism, we discuss its\nweakness and propose several more complex variants. \n\n"}
{"id": "1608.08521", "contents": "Title: Profitable Task Allocation in Mobile Cloud Computing Abstract: We propose a game theoretic framework for task allocation in mobile cloud\ncomputing that corresponds to offloading of compute tasks to a group of nearby\nmobile devices. Specifically, in our framework, a distributor node holds a\nmultidimensional auction for allocating the tasks of a job among nearby mobile\nnodes based on their computational capabilities and also the cost of\ncomputation at these nodes, with the goal of reducing the overall job\ncompletion time. Our proposed auction also has the desired incentive\ncompatibility property that ensures that mobile devices truthfully reveal their\ncapabilities and costs and that those devices benefit from the task allocation.\nTo deal with node mobility, we perform multiple auctions over adaptive time\nintervals. We develop a heuristic approach to dynamically find the best time\nintervals between auctions to minimize unnecessary auctions and the\naccompanying overheads. We evaluate our framework and methods using both real\nworld and synthetic mobility traces. Our evaluation results show that our game\ntheoretic framework improves the job completion time by a factor of 2-5 in\ncomparison to the time taken for executing the job locally, while minimizing\nthe number of auctions and the accompanying overheads. Our approach is also\nprofitable for the nearby nodes that execute the distributor's tasks with these\nnodes receiving a compensation higher than their actual costs. \n\n"}
{"id": "1609.00368", "contents": "Title: Ten Steps of EM Suffice for Mixtures of Two Gaussians Abstract: The Expectation-Maximization (EM) algorithm is a widely used method for\nmaximum likelihood estimation in models with latent variables. For estimating\nmixtures of Gaussians, its iteration can be viewed as a soft version of the\nk-means clustering algorithm. Despite its wide use and applications, there are\nessentially no known convergence guarantees for this method. We provide global\nconvergence guarantees for mixtures of two Gaussians with known covariance\nmatrices. We show that the population version of EM, where the algorithm is\ngiven access to infinitely many samples from the mixture, converges\ngeometrically to the correct mean vectors, and provide simple, closed-form\nexpressions for the convergence rate. As a simple illustration, we show that,\nin one dimension, ten steps of the EM algorithm initialized at infinity result\nin less than 1\\% error estimation of the means. In the finite sample regime, we\nshow that, under a random initialization, $\\tilde{O}(d/\\epsilon^2)$ samples\nsuffice to compute the unknown vectors to within $\\epsilon$ in Mahalanobis\ndistance, where $d$ is the dimension. In particular, the error rate of the EM\nbased estimator is $\\tilde{O}\\left(\\sqrt{d \\over n}\\right)$ where $n$ is the\nnumber of samples, which is optimal up to logarithmic factors. \n\n"}
{"id": "1609.00852", "contents": "Title: Joint Caching and Pricing Strategies for Popular Content in Information\n  Centric Networks Abstract: We develop an analytical framework for distribution of popular content in an\nInformation Centric Network (ICN) that comprises of Access ICNs, a Transit ICN\nand a Content Provider. Using a generalized Zipf distribution to model content\npopularity, we devise a game theoretic approach to jointly determine caching\nand pricing strategies in such an ICN. Under the assumption that the caching\ncost of the access and transit ICNs is inversely proportional to popularity, we\nshow that the Nash caching strategies in the ICN are 0-1 (all or nothing)\nstrategies. Further, for the case of symmetric Access ICNs, we show that the\nNash equilibrium is unique and the caching policy (0 or 1) is determined by a\nthreshold on the popularity of the content (reflected by the Zipf probability\nmetric), i.e., all content more popular than the threshold value is cached. We\nalso show that the resulting threshold of the Access and Transit ICNs, as well\nas all prices can be obtained by a decomposition of the joint caching and\npricing problem into two independent caching only and pricing only problems. \n\n"}
{"id": "1609.01961", "contents": "Title: Auction-Based Coopetition between LTE Unlicensed and Wi-Fi Abstract: Motivated by the recent efforts in extending LTE to the unlicensed spectrum,\nwe propose a novel spectrum sharing framework for the coopetition (i.e.,\ncooperation and competition) between LTE and Wi-Fi in the unlicensed band.\nBasically, the LTE network can choose to work in one of the two modes: in the\ncompetition mode, it randomly accesses an unlicensed channel, and interferes\nwith the Wi-Fi access point using the same channel; in the cooperation mode, it\ndelivers traffic for the Wi-Fi users in exchange for the exclusive access of\nthe corresponding channel. Because the LTE network works in an\ninterference-free manner in the cooperation mode, it can achieve a much larger\ndata rate than that in the competition mode, which allows it to effectively\nserve both its own users and the Wi-Fi users. We design a second-price reverse\nauction mechanism, which enables the LTE provider and the Wi-Fi access point\nowners (APOs) to effectively negotiate the operation mode. Specifically, the\nLTE provider is the auctioneer (buyer), and the APOs are the bidders (sellers)\nwho compete to sell their channel access opportunities to the LTE provider. In\nStage I of the auction, the LTE provider announces a reserve rate. In Stage II\nof the auction, the APOs submit their bids. We show that the auction involves\nallocative externalities, i.e., the cooperation between the LTE provider and\none APO benefits other APOs who are not directly involved in this cooperation.\nAs a result, a particular APO's willingness to cooperate is affected by its\nbelief about other APOs' willingness to cooperate. This makes our analysis much\nmore challenging than that of the conventional second-price auction, where\nbidding truthfully is a weakly dominant strategy. We show that the APOs have a\nunique form of the equilibrium bidding strategies in Stage II, based on which\nwe analyze the LTE provider's optimal reserve rate in Stage I. \n\n"}
{"id": "1609.02596", "contents": "Title: A Stackelberg Game for Incentive Proactive Caching Mechanisms in\n  Wireless Networks Abstract: In this paper, an incentive proactive cache mechanism in cache-enabled small\ncell networks (SCNs) is proposed, in order to motivate the content providers\n(CPs) to participate in the caching procedure. A network composed of a single\nmobile network operator (MNO) and multiple CPs is considered. The MNO aims to\ndefine the price it charges the CPs to maximize its revenue while the CPs\ncompete to determine the number of files they cache at the MNO's small base\nstations (SBSs) to improve the quality of service (QoS) of their users. This\nproblem is formulated as a Stackelberg game where a single MNO is considered as\nthe leader and the multiple CPs willing to cache files are the followers. The\nfollowers game is modeled as a non-cooperative game and both the existence and\nuniqueness of a Nash equilibrium (NE) are proved. The closed-form expression of\nthe NE which corresponds to the amount of storage each CP requests from the MNO\nis derived. An optimization problem is formulated at the MNO side to determine\nthe optimal price that the MNO should charge the CPs. Simulation results show\nthat at the equilibrium, the MNO and CPs can all achieve a utility that is up\nto 50% higher than the cases in which the prices and storage quantities are\nrequested arbitrarily. \n\n"}
{"id": "1609.05136", "contents": "Title: Hardness Results for Consensus-Halving Abstract: We study the consensus-halving problem of dividing an object into two\nportions, such that each of $n$ agents has equal valuation for the two\nportions. The $\\epsilon$-approximate consensus-halving problem allows each\nagent to have an $\\epsilon$ discrepancy on the values of the portions. We prove\nthat computing $\\epsilon$-approximate consensus-halving solution using $n$ cuts\nis in PPA, and is PPAD-hard, where $\\epsilon$ is some positive constant; the\nproblem remains PPAD-hard when we allow a constant number of additional cuts.\nIt is NP-hard to decide whether a solution with $n-1$ cuts exists for the\nproblem. As a corollary of our results, we obtain that the approximate\ncomputational version of the Continuous Necklace Splitting Problem is PPAD-hard\nwhen the number of portions $t$ is two. \n\n"}
{"id": "1609.05537", "contents": "Title: Quantum Speed-ups for Semidefinite Programming Abstract: We give a quantum algorithm for solving semidefinite programs (SDPs). It has\nworst-case running time $n^{\\frac{1}{2}} m^{\\frac{1}{2}} s^2\n\\text{poly}(\\log(n), \\log(m), R, r, 1/\\delta)$, with $n$ and $s$ the dimension\nand row-sparsity of the input matrices, respectively, $m$ the number of\nconstraints, $\\delta$ the accuracy of the solution, and $R, r$ a upper bounds\non the size of the optimal primal and dual solutions. This gives a square-root\nunconditional speed-up over any classical method for solving SDPs both in $n$\nand $m$. We prove the algorithm cannot be substantially improved (in terms of\n$n$ and $m$) giving a $\\Omega(n^{\\frac{1}{2}}+m^{\\frac{1}{2}})$ quantum lower\nbound for solving semidefinite programs with constant $s, R, r$ and $\\delta$.\n  The quantum algorithm is constructed by a combination of quantum Gibbs\nsampling and the multiplicative weight method. In particular it is based on a\nclassical algorithm of Arora and Kale for approximately solving SDPs. We\npresent a modification of their algorithm to eliminate the need for solving an\ninner linear program which may be of independent interest. \n\n"}
{"id": "1609.05952", "contents": "Title: Window Parity Games: An Alternative Approach Toward Parity Games with\n  Time Bounds (Full Version) Abstract: Classical objectives in two-player zero-sum games played on graphs often deal\nwith limit behaviors of infinite plays: e.g., mean-payoff and total-payoff in\nthe quantitative setting, or parity in the qualitative one (a canonical way to\nencode omega-regular properties). Those objectives offer powerful abstraction\nmechanisms and often yield nice properties such as memoryless determinacy.\nHowever, their very nature provides no guarantee on time bounds within which\nsomething good can be witnessed. In this work, we consider two approaches\ntoward inclusion of time bounds in parity games. The first one, parity-response\ngames, is based on the notion of finitary parity games [CHH09] and parity games\nwith costs [FZ14,WZ16]. The second one, window parity games, is inspired by\nwindow mean-payoff games [CDRR15]. We compare the two approaches and show that\nwhile they prove to be equivalent in some contexts, window parity games offer a\nmore tractable alternative when the time bound is given as a parameter (P-c.\nvs. PSPACE-C.). In particular, it provides a conservative approximation of\nparity games computable in polynomial time. Furthermore, we extend both\napproaches to the multi-dimension setting. We give the full picture for both\ntypes of games with regard to complexity and memory bounds.\n  [CHH09] K. Chatterjee, T.A. Henzinger, F. Horn (2009): Finitary winning in\nomega-regular games. ACM Trans. Comput. Log. 11(1). [FZ14] N. Fijalkow, M.\nZimmermann (2014): Parity and Streett Games with Costs. LMCS 10(2). [WZ16] A.\nWeinert, M. Zimmermann (2016): Easy to Win, Hard to Master: Optimal Strategies\nin Parity Games with Costs. Proc. of CSL, LIPIcs 62, pp. 31:1-31:17, Schloss\nDagstuhl - LZI. [CDRR15] K. Chatterjee, L. Doyen, M. Randour, J.-F. Raskin\n(2015): Looking at mean-payoff and total-payoff through windows. Information\nand Computation 242, pp. 25-52. \n\n"}
{"id": "1609.06736", "contents": "Title: Improving and extending the testing of distributions for\n  shape-restricted properties Abstract: Distribution testing deals with what information can be deduced about an\nunknown distribution over $\\{1,\\ldots,n\\}$, where the algorithm is only allowed\nto obtain a relatively small number of independent samples from the\ndistribution. In the extended conditional sampling model, the algorithm is also\nallowed to obtain samples from the restriction of the original distribution on\nsubsets of $\\{1,\\ldots,n\\}$.\n  In 2015, Canonne, Diakonikolas, Gouleakis and Rubinfeld unified several\nprevious results, and showed that for any property of distributions satisfying\na \"decomposability\" criterion, there exists an algorithm (in the basic model)\nthat can distinguish with high probability distributions satisfying the\nproperty from distributions that are far from it in the variation distance.\n  We present here a more efficient yet simpler algorithm for the basic model,\nas well as very efficient algorithms for the conditional model, which until now\nwas not investigated under the umbrella of decomposable properties.\nAdditionally, we provide an algorithm for the conditional model that handles a\nmuch larger class of properties.\n  Our core mechanism is a way of efficiently producing an interval-partition of\n$\\{1,\\ldots,n\\}$ that satisfies a \"fine-grain\" quality. We show that with such\na partition at hand we can directly move forward with testing individual\nintervals, instead of first searching for the \"correct\" partition of\n$\\{1,\\ldots,n\\}$. \n\n"}
{"id": "1609.06844", "contents": "Title: Posted Pricing sans Discrimination Abstract: In the quest for market mechanisms that are easy to implement, yet close to\noptimal, few seem as viable as posted pricing. Despite the growing body of\nimpressive results, the performance of most posted price mechanisms however,\nrely crucially on price discrimination when multiple copies of a good are\navailable. For the more general case with non-linear production costs on each\ngood, hardly anything is known for general multi-good markets. With this in\nmind, we study a Bayesian setting where the seller can produce any number of\ncopies of a good but faces convex production costs for the same, and buyers\narrive sequentially. Our main contribution is a framework for\nnon-discriminatory pricing in the presence of production costs: the framework\nyields posted price mechanisms with O(1)-approximation factors for fractionally\nsubadditive (XoS) buyers, logarithmic approximations for subadditive buyers,\nand also extends to settings where the seller is oblivious to buyer valuations.\nOur work presents the first known results for Bayesian settings with production\ncosts and is among the few posted price mechanisms that do not charge buyers\ndifferently for the same good. \n\n"}
{"id": "1609.08253", "contents": "Title: On the Group and Color Isomorphism Problems Abstract: In this paper, we prove results on the relationship between the complexity of\nthe group and color isomorphism problems. The difficulty of color isomorphism\nproblems is known to be closely linked to the the composition factors of the\npermutation group involved. Previous works are primarily concerned with\napplying color isomorphism to bou nded degree graph isomorphism, and have\ntherefore focused on the alternating composit ion factors, since those are the\nbottleneck in the case of graph isomorphism.\n  We consider the color isomorphism problem with composition factors restricted\nto those other than the alternating group, show that group isomorphism reduces\nin n^(O(log log n)) time to this problem, and, conversely, that a special case\nof this color isomorphism problem reduces to a slight generalization of group\nisomorphism. We then sharpen our results by identifying the projective special\nlinear group as the main obstacle to faster algorithms for group isomorphism\nand prove that the aforementioned reduc tion from group isomorphism to color\nisomorphism in fact produces only cyclic and projective special linear factors.\nOur results demonstrate that, just as the alternatin g group was a barrier to\nfaster algorithms for graph isomorphism for three decades, the projective\nspecial linear group is an obstacle to faster algorithms for group isomorphism. \n\n"}
{"id": "1609.08923", "contents": "Title: Models of Level-0 Behavior for Predicting Human Behavior in Games Abstract: Behavioral game theory seeks to describe the way actual people (as compared\nto idealized, \"rational\" agents) act in strategic situations. Our own recent\nwork has identified iterative models (such as quantal cognitive hierarchy) as\nthe state of the art for predicting human play in unrepeated, simultaneous-move\ngames (Wright & Leyton-Brown 2012, 2016). Iterative models predict that agents\nreason iteratively about their opponents, building up from a specification of\nnonstrategic behavior called level-0. The modeler is in principle free to\nchoose any description of level-0 behavior that makes sense for the setting.\nHowever, almost all existing work specifies this behavior as a uniform\ndistribution over actions. In most games it is not plausible that even\nnonstrategic agents would choose an action uniformly at random, nor that other\nagents would expect them to do so. A more accurate model for level-0 behavior\nhas the potential to dramatically improve predictions of human behavior, since\na substantial fraction of agents may play level-0 strategies directly, and\nfurthermore since iterative models ground all higher-level strategies in\nresponses to the level-0 strategy. Our work considers models of the way in\nwhich level-0 agents construct a probability distribution over actions, given\nan arbitrary game. Using a Bayesian optimization package called SMAC (Hutter,\nHoos, & Leyton-Brown, 2010, 2011, 2012), we systematically evaluated a large\nspace of such models, each of which makes its prediction based only on general\nfeatures that can be computed from any normal form game. In the end, we\nrecommend a model that achieved excellent performance across the board: a\nlinear weighting of features that requires the estimation of four weights. We\nevaluated the effects of combining this new level-0 model with several\niterative models, and observed large improvements in the models' predictive\naccuracies. \n\n"}
{"id": "1610.01443", "contents": "Title: Efficiency and Budget Balance in General Quasi-linear Domains Abstract: We study efficiency and budget balance for designing mechanisms in general\nquasi-linear domains. Green and Laffont (1979) proved that one cannot\ngenerically achieve both. We consider strategyproof budget-balanced mechanisms\nthat are approximately efficient. For deterministic mechanisms, we show that a\nstrategyproof and budget-balanced mechanism must have a sink agent whose\nvaluation function is ignored in selecting an alternative, and she is\ncompensated with the payments made by the other agents. We assume the\nvaluations of the agents come from a bounded open interval. Using this result,\nwe find a tight lower bound on the inefficiencies of strategyproof,\nbudget-balanced mechanisms in this domain. The bound shows that the\ninefficiency asymptotically disappears when the number of agents is large---a\nresult close in spirit to Green and Laffont (1979, Theorem 9.4). However, our\nresults provide worst-case bounds and the best possible rate of convergence.\n  Next, we consider minimizing any convex combination of inefficiency and\nbudget imbalance. We show that if the valuations are unrestricted, no\ndeterministic mechanism can do asymptotically better than minimizing\ninefficiency alone.\n  Finally, we investigate randomized mechanisms and provide improved lower\nbounds on expected inefficiency. We give a tight lower bound for an interesting\nclass of strategyproof, budget-balanced, randomized mechanisms. We also use an\noptimization-based approach---in the spirit of automated mechanism design---to\nprovide a lower bound on the minimum achievable inefficiency of any randomized\nmechanism.\n  Experiments with real data from two applications show that the inefficiency\nfor a simple randomized mechanism is 5--100 times smaller than the worst case.\nThis relative difference increases with the number of agents. \n\n"}
{"id": "1610.01900", "contents": "Title: Distance rationalization of anonymous and homogeneous voting rules Abstract: The concept of distance rationalizability of voting rules has been explored\nin recent years by several authors. Roughly speaking, we first choose a\nconsensus set of elections (defined via preferences of voters over candidates)\nfor which the result is specified a priori (intuitively, these are elections on\nwhich all voters can easily agree on the result). We also choose a measure of\ndistance between elections. The result of an election outside the consensus set\nis defined to be the result of the closest consensual election under the\ndistance measure.\n  Most previous work has dealt with a definition in terms of preference\nprofiles. However, most voting rules in common use are anonymous and\nhomogeneous. In this case there is a much more succinct representation (using\nthe voting simplex) of the inputs to the rule. This representation has been\nwidely used in the voting literature, but rarely in the context of distance\nrationalizability.\n  We show exactly how to connect distance rationalizability on profiles for\nanonymous and homogeneous rules to geometry in the simplex. We develop the\nconnection for the important special case of votewise distances, recently\nintroduced and studied by Elkind, Faliszewski and Slinko in several papers.\nThis yields a direct interpretation in terms of well-developed mathematical\nconcepts not seen before in the voting literature, namely Kantorovich (also\ncalled Wasserstein) distances and the geometry of Minkowski spaces.\n  As an application of this approach, we prove some positive and some negative\nresults about the decisiveness of distance rationalizable anonymous and\nhomogeneous rules. The positive results connect with the recent theory of\nhyperplane rules, while the negative ones deal with distances that are not\nmetrics, controversial notions of consensus, and the fact that the\n$\\ell^1$-norm is not strictly convex. \n\n"}
{"id": "1610.01929", "contents": "Title: Trial-Offer Markets with Continuation Abstract: Trial-offer markets, where customers can sample a product before deciding\nwhether to buy it, are ubiquitous in the online experience. Their static and\ndynamic properties are often studied by assuming that consumers follow a\nmultinomial logit model and try exactly one product. In this paper, we study\nhow to generalize existing results to a more realistic setting where consumers\ncan try multiple products. We show that a multinomial logit model with\ncontinuation can be reduced to a standard multinomial logit model with\ndifferent appeal and product qualities. We examine the consequences of this\nreduction on the performance and predictability of the market, the role of\nsocial influence, and the ranking policies. \n\n"}
{"id": "1610.03013", "contents": "Title: Display Advertising with Real-Time Bidding (RTB) and Behavioural\n  Targeting Abstract: The most significant progress in recent years in online display advertising\nis what is known as the Real-Time Bidding (RTB) mechanism to buy and sell ads.\nRTB essentially facilitates buying an individual ad impression in real time\nwhile it is still being generated from a user's visit. RTB not only scales up\nthe buying process by aggregating a large amount of available inventories\nacross publishers but, most importantly, enables direct targeting of individual\nusers. As such, RTB has fundamentally changed the landscape of digital\nmarketing. Scientifically, the demand for automation, integration and\noptimisation in RTB also brings new research opportunities in information\nretrieval, data mining, machine learning and other related fields. In this\nmonograph, an overview is given of the fundamental infrastructure, algorithms,\nand technical solutions of this new frontier of computational advertising. The\ncovered topics include user response prediction, bid landscape forecasting,\nbidding algorithms, revenue optimisation, statistical arbitrage, dynamic\npricing, and ad fraud detection. \n\n"}
{"id": "1610.03474", "contents": "Title: The Core of the Participatory Budgeting Problem Abstract: In participatory budgeting, communities collectively decide on the allocation\nof public tax dollars for local public projects. In this work, we consider the\nquestion of fairly aggregating the preferences of community members to\ndetermine an allocation of funds to projects. This problem is different from\nstandard fair resource allocation because of public goods: The allocated goods\nbenefit all users simultaneously. Fairness is crucial in participatory decision\nmaking, since generating equitable outcomes is an important goal of these\nprocesses. We argue that the classic game theoretic notion of core captures\nfairness in the setting. To compute the core, we first develop a novel\ncharacterization of a public goods market equilibrium called the Lindahl\nequilibrium, which is always a core solution. We then provide the first (to our\nknowledge) polynomial time algorithm for computing such an equilibrium for a\nbroad set of utility functions; our algorithm also generalizes (in a\nnon-trivial way) the well-known concept of proportional fairness. We use our\ntheoretical insights to perform experiments on real participatory budgeting\nvoting data. We empirically show that the core can be efficiently computed for\nutility functions that naturally model our practical setting, and examine the\nrelation of the core with the familiar welfare objective. Finally, we address\nconcerns of incentives and mechanism design by developing a randomized\napproximately dominant-strategy truthful mechanism building on the exponential\nmechanism from differential privacy. \n\n"}
{"id": "1610.03745", "contents": "Title: Dividing goods and bads under additive utilities Abstract: When utilities are additive, we uncovered in our previous paper (Bogomolnaia\net al. \"Dividing Goods or Bads under Additive Utilities\") many similarities but\nalso surprising differences in the behavior of the familiar Competitive rule\n(with equal incomes), when we divide (private) goods or bads. The rule picks in\nboth cases the critical points of the product of utilities (or disutilities) on\nthe efficiency frontier, but there is only one such point if we share goods,\nwhile there can be exponentially many in the case of bads.\n  We extend this analysis to the fair division of mixed items: each item can be\nviewed by some participants as a good and by others as a bad, with\ncorresponding positive or negative marginal utilities. We find that the\ndivision of mixed items boils down, normatively as well as computationally, to\na variant of an all goods problem, or of an all bads problem: in particular the\ntask of dividing the non disposable items must be either good news for\neveryone, or bad news for everyone.\n  If at least one feasible utility profile is positive, the Competitive rule\npicks the unique maximum of the product of (positive) utilities. If no feasible\nutility profile is positive, this rule picks all critical points of the product\nof disutilities on the efficient frontier. \n\n"}
{"id": "1610.07858", "contents": "Title: Bounding Average-energy Games Abstract: We consider average-energy games, where the goal is to minimize the long-run\naverage of the accumulated energy. While several results have been obtained on\nthese games recently, decidability of average-energy games with a lower-bound\nconstraint on the energy level (but no upper bound) remained open; in\nparticular, so far there was no known upper bound on the memory that is\nrequired for winning strategies.\n  By reducing average-energy games with lower-bounded energy to infinite-state\nmean-payoff games and analyzing the density of low-energy configurations, we\nshow an almost tight doubly-exponential upper bound on the necessary memory,\nand that the winner of average-energy games with lower-bounded energy can be\ndetermined in doubly-exponential time. We also prove EXPSPACE-hardness of this\nproblem.\n  Finally, we consider multi-dimensional extensions of all types of\naverage-energy games: without bounds, with only a lower bound, and with both a\nlower and an upper bound on the energy. We show that the fully-bounded version\nis the only case to remain decidable in multiple dimensions. \n\n"}
{"id": "1610.08906", "contents": "Title: Logarithmic Query Complexity for Approximate Nash Computation in Large\n  Games Abstract: We investigate the problem of equilibrium computation for \"large\" $n$-player\ngames. Large games have a Lipschitz-type property that no single player's\nutility is greatly affected by any other individual player's actions. In this\npaper, we mostly focus on the case where any change of strategy by a player\ncauses other players' payoffs to change by at most $\\frac{1}{n}$. We study\nalgorithms having query access to the game's payoff function, aiming to find\n$\\epsilon$-Nash equilibria. We seek algorithms that obtain $\\epsilon$ as small\nas possible, in time polynomial in $n$.\n  Our main result is a randomised algorithm that achieves $\\epsilon$\napproaching $\\frac{1}{8}$ for 2-strategy games in a {\\em completely uncoupled}\nsetting, where each player observes her own payoff to a query, and adjusts her\nbehaviour independently of other players' payoffs/actions. $O(\\log n)$\nrounds/queries are required. We also show how to obtain a slight improvement\nover $\\frac{1}{8}$, by introducing a small amount of communication between the\nplayers.\n  Finally, we give extension of our results to large games with more than two\nstrategies per player, and alternative largeness parameters. \n\n"}
{"id": "1610.09221", "contents": "Title: Revenue Maximizing Envy-Free Pricing in Matching Markets with Budgets Abstract: We study envy-free pricing mechanisms in matching markets with $m$ items and\n$n$ budget constrained buyers. Each buyer is interested in a subset of the\nitems on sale, and she appraises at some single-value every item in her\npreference-set. Moreover, each buyer has a budget that constraints the maximum\naffordable payment, while she aims to obtain as many items as possible of her\npreference-set. Our goal is to compute an envy-free pricing allocation that\nmaximizes the revenue, i.e., the total payment charged to the buyers. This\npricing problem is hard to approximate better than $\\Omega({\\rm min}\n\\{n,m\\}^{1/2-\\epsilon})$ for any $\\epsilon>0$, unless $P=NP$. The hardness\nresult is due to the presence of the matching constraints given that the\nsimpler multi-unit case can be approximated up to a constant factor of $2$. The\ngoal of this paper is to circumvent the hardness result by restricting\nourselves to specific settings of valuations and budgets. Two particularly\nsignificant scenarios are: each buyer has a budget that is greater than her\nsingle-value valuation, and each buyer has a budget that is lower than her\nsingle-value valuation. Surprisingly, in both scenarios we are able to achieve\na $1/4$-approximation to the optimal envy-free revenue. The algorithms utilize\na novel version of the Ausebel ascending price auction. These results may\nsuggest that, although it is difficult to approximate the optimal revenue in\ngeneral, ascending price auctions could achieve relatively good revenue in most\nof the practical settings. \n\n"}
{"id": "1611.02442", "contents": "Title: Price Doubling and Item Halving: Robust Revenue Guarantees for Item\n  Pricing Abstract: We study approximation algorithms for revenue maximization based on static\nitem pricing, where a seller chooses prices for various goods in the market,\nand then the buyers purchase utility-maximizing bundles at these given prices.\nWe formulate two somewhat general techniques for designing good pricing\nalgorithms for this setting: Price Doubling and Item Halving. Using these\ntechniques, we unify many of the existing results in the item pricing\nliterature under a common framework, as well as provide several new item\npricing algorithms for approximating both revenue and social welfare. More\nspecifically, for a variety of settings with item pricing, we show that it is\npossible to deterministically obtain a log-approximation for revenue and a\nconstant-approximation for social welfare simultaneously: thus one need not\nsacrifice revenue if the goal is to still have decent welfare guarantees. %In\naddition, we provide a new black-box reduction from revenue to welfare based on\nitem pricing, which immediately gives us new revenue-approximation algorithms\n(e.g., for gross substitutes valuations).\n  The main technical contribution of this paper is a $O((\\log m + \\log\nk)^2)$-approximation algorithm for revenue maximization based on the Item\nHalving technique, for settings where buyers have XoS valuations, where $m$ is\nthe number of goods and $k$ is the average supply. Surprisingly, ours is the\nfirst known item pricing algorithm with polylogarithmic approximations for such\ngeneral classes of valuations, and partially resolves an important open\nquestion from the algorithmic pricing literature about the existence of item\npricing algorithms with logarithmic factors for general valuations. We also use\nthe Item Halving framework to form envy-free item pricing mechanisms for the\npopular setting of multi-unit markets, providing a log-approximation to revenue\nin this case. \n\n"}
{"id": "1611.04034", "contents": "Title: Fair Public Decision Making Abstract: We generalize the classic problem of fairly allocating indivisible goods to\nthe problem of \\emph{fair public decision making}, in which a decision must be\nmade on several social issues simultaneously, and, unlike the classic setting,\na decision can provide positive utility to multiple players. We extend the\npopular fairness notion of proportionality (which is not guaranteeable) to our\nmore general setting, and introduce three novel relaxations ---\n\\emph{proportionality up to one issue, round robin share, and pessimistic\nproportional share} --- that are also interesting in the classic goods\nallocation setting. We show that the Maximum Nash Welfare solution, which is\nknown to satisfy appealing fairness properties in the classic setting,\nsatisfies or approximates all three relaxations in our framework. We also\nprovide polynomial time algorithms and hardness results for finding allocations\nsatisfying these axioms, with or without insisting on Pareto optimality. \n\n"}
{"id": "1611.05656", "contents": "Title: Swap Equilibria under Link and Vertex Destruction Abstract: We initiate the study of the \\emph{destruction model} (\\aka \\emph{adversary\nmodel}) introduced by Kliemann (2010), using the stability concept of\n\\emph{swap equilibrium} introduced by Alon et. al (2010). The destruction model\nis a network formation game incorporating the robustness of a network under a\nmore or less targeted attack. In addition to bringing in the swap equilibrium\n(SE) concept, we extend the model from an attack on the edges of the network to\nan attack on its vertices. Vertex destruction can generally cause more harm and\ntends to be more difficult to analyze.\n  We prove structural results and linear upper bounds or super-linear lower\nbounds on the social cost of SE under different attack scenarios. The most\ncomplex case is when the vertex to be destroyed is chosen uniformly at random\nfrom the set of those vertices where each causes a maximum number of player\npairs to be separated (called a max-sep vertex). We prove a lower bound on the\nsocial cost of $\\Omega(n^{3/2})$ for this case and initiate an understanding of\nthe structural properties of SE in this scenario. Namely, we prove that there\nis no SE that is a tree and has only one max-sep vertex. We conjecture that\nthis result can be generalized, in particular we conjecture that there is no SE\nthat is a tree. On the other hand, we prove that if the vertex to be destroyed\nis chosen uniformly at random from the set of \\emph{all} vertices, then each SE\nis a tree (unless it is two-connected). Our conjecture would imply that moving\nfrom the uniform probability measure to a measure concentrated on the max-sep\nvertices, means moving from no SE having a cycle (unless two-connected) to each\nSE having a cycle. This would ask for a more detailed study of this transition\nin future work. \n\n"}
{"id": "1611.08326", "contents": "Title: Detecting communities is hard, and counting them is even harder Abstract: We consider the algorithmic problem of community detection in networks. Given\nan undirected friendship graph $G=\\left(V,E\\right)$, a subset $S\\subseteq V$ is\nan $\\left(\\alpha,\\beta\\right)$-community if:\n  * Every member of the community is friends with an $\\alpha$-fraction of the\ncommunity;\n  * Every non-member is friends with at most a $\\beta$-fraction of the\ncommunity.\n  Arora et al [AGSS12] gave a quasi-polynomial time algorithm for enumerating\nall the $\\left(\\alpha,\\beta\\right)$-communities for any constants\n$\\alpha>\\beta$.\n  Here, we prove that, assuming the Exponential Time Hypothesis (ETH),\nquasi-polynomial time is in fact necessary - and even for a much weaker\napproximation desideratum. Namely, distinguishing between:\n  * $G$ contains an $\\left(1,o\\left(1\\right)\\right)$-community; and\n  * $G$ does not contain an\n$\\left(\\beta+o\\left(1\\right),\\beta\\right)$-community for any\n$\\beta\\in\\left[0,1\\right]$.\n  We also prove that counting the number of\n$\\left(1,o\\left(1\\right)\\right)$-communities requires quasi-polynomial time\nassuming the weaker #ETH. \n\n"}
{"id": "1611.08691", "contents": "Title: Multiwinner Approval Rules as Apportionment Methods Abstract: We establish a link between multiwinner elections and apportionment problems\nby showing how approval-based multiwinner election rules can be interpreted as\nmethods of apportionment. We consider several multiwinner rules and observe\nthat they induce apportionment methods that are well-established in the\nliterature on proportional representation. For instance, we show that\nProportional Approval Voting induces the D'Hondt method and that Monroe's rule\ninduces the largest reminder method. We also consider properties of\napportionment methods and exhibit multiwinner rules that induce apportionment\nmethods satisfying these properties. \n\n"}
{"id": "1611.09613", "contents": "Title: A Note on the Ratio of Revenues Between Selling in a Bundle and\n  Separately Abstract: We consider the problem of maximizing revenue when selling k items to a\nsingle buyer with known valuation distributions. We show that for a single,\nadditive buyer whose valuations for for the items are distributed according to\ni.i.d. distributions which are known to the seller, the ratio of revenue from\nselling in a bundle to selling separately is at least 55.9% and this gap is\nattainable. \n\n"}
{"id": "1611.09928", "contents": "Title: Proportional Justified Representation Abstract: The goal of multi-winner elections is to choose a fixed-size committee based\non voters' preferences. An important concern in this setting is representation:\nlarge groups of voters with cohesive preferences should be adequately\nrepresented by the election winners. Recently, Aziz et al. (2015a;2017)\nproposed two axioms that aim to capture this idea: justified representation\n(JR) and its strengthening extended justified representation (EJR). In this\npaper, we extend the work of Aziz et al. in several directions. First, we\nanswer an open question of Aziz et al., by showing that Reweighted Approval\nVoting satisfies JR for $k=3, 4, 5$, but fails it for $k\\ge 6$. Second, we\nobserve that EJR is incompatible with the Perfect Representation criterion,\nwhich is important for many applications of multi-winner voting, and propose a\nrelaxation of EJR, which we call Proportional Justified Representation (PJR).\nPJR is more demanding than JR, but, unlike EJR, it is compatible with perfect\nrepresentation, and a committee that provides PJR can be computed in polynomial\ntime if the committee size divides the number of voters. Moreover, just like\nEJR, PJR can be used to characterize the classic PAV rule in the class of\nweighted PAV rules. On the other hand, we show that EJR provides stronger\nguarantees with respect to average voter satisfaction than PJR does. \n\n"}
{"id": "1611.10191", "contents": "Title: Is Non-Neutrality Profitable for the Stakeholders of the Internet\n  Market? Abstract: Net neutrality on the Internet is perceived as the policy that mandates\nInternet Service Providers (ISPs) to treat all data equally, regardless of the\nsource, destination, or type of transmitted data. In this work, we consider a\nscheme in which the decision makers of the market are two ISPs, one \"big\"\nContent Provider (CP), and a continuum of end-users. One of the ISPs is neutral\nand the other is non-neutral, i.e. she offers a premium quality to a CP in\nexchange for a side-payment. In addition, we assume that the CP can\ndifferentiate between ISPs by controlling the quality of the content she is\noffering on each one. In this part of the paper, we consider a scenario in\nwhich end-users are not locked in with the ISPs and can switch between ISPs\neasily. We formulate a sequential game, and show that there exists a unique\nSub-game Perfect Nash Equilibrium (SPNE) for the game, where the CP pays the\nside-payment to the non-neutral ISP and offers her content with the premium\nquality. In addition, the CP does not offer her content on the neutral ISP.\nThus, driving this ISP out of the market. \n\n"}
{"id": "1612.00106", "contents": "Title: Menu-Based Pricing for Charging of Electric Vehicles with\n  Vehicle-to-Grid Service Abstract: The paper considers a bidirectional power flow model of the electric vehicles\n(EVs) in a charging station. The EVs can inject energies by discharging via a\nVehicle-to-Grid (V2G) service which can enhance the profits of the charging\nstation. However, frequent charging and discharging degrade battery life. A\nproper compensation needs to be paid to the users to participate in the V2G\nservice. We propose a menu-based pricing scheme, where the charging station\nselects a price for each arriving user for the amount of battery utilization,\nthe total energy, and the time (deadline) that the EV will stay. The user can\naccept one of the contracts or rejects all depending on their utilities. The\ncharging station can serve users using a combination of the renewable energy\nand the conventional energy bought from the grid. We show that though there\nexists a profit maximizing price which maximizes the social welfare, it\nprovides no surplus to the users if the charging station is aware of the\nutilities of the users. If the charging station is not aware of the exact\nutilities, the social welfare maximizing price may not maximize the expected\nprofit. In fact, it can give a zero profit. We propose a pricing strategy which\nprovides a guaranteed fixed profit to the charging station and it also\nmaximizes the expected profit for a wide range of utility functions. Our\nanalysis shows that when the harvested renewable energy is small the users have\nhigher incentives for the V2G service. We, numerically, show that the charging\nstation's profit and the user's surplus both increase as V2G service is\nefficiently utilized by the pricing mechanism. \n\n"}
{"id": "1612.00190", "contents": "Title: Equilibrium Computation in Resource Allocation Games Abstract: We study the equilibrium computation problem for two classical resource\nallocation games: atomic splittable congestion games and multimarket Cournot\noligopolies. For atomic splittable congestion games with singleton strategies\nand player-specific affine cost functions, we devise the first polynomial time\nalgorithm computing a pure Nash equilibrium. Our algorithm is combinatorial and\ncomputes the exact equilibrium assuming rational input. The idea is to compute\nan equilibrium for an associated integrally-splittable singleton congestion\ngame in which the players can only split their demands in integral multiples of\na common packet size. While integral games have been considered in the\nliterature before, no polynomial time algorithm computing an equilibrium was\nknown. Also for this class, we devise the first polynomial time algorithm and\nuse it as a building block for our main algorithm.\n  We then develop a polynomial time computable transformation mapping a\nmultimarket Cournot competition game with firm-specific affine price functions\nand quadratic costs to an associated atomic splittable congestion game as\ndescribed above. The transformation preserves equilibria in either games and,\nthus, leads -- via our first algorithm -- to a polynomial time algorithm\ncomputing Cournot equilibria. Finally, our analysis for integrally-splittable\ngames implies new bounds on the difference between real and integral Cournot\nequilibria. The bounds can be seen as a generalization of the recent bounds for\nsingle market oligopolies obtained by Todd [2016]. \n\n"}
{"id": "1612.01527", "contents": "Title: Matrix multiplication algorithms from group orbits Abstract: We show how to construct highly symmetric algorithms for matrix\nmultiplication. In particular, we consider algorithms which decompose the\nmatrix multiplication tensor into a sum of rank-1 tensors, where the\ndecomposition itself consists of orbits under some finite group action. We show\nhow to use the representation theory of the corresponding group to derive\nsimple constraints on the decomposition, which we solve by hand for n=2,3,4,5,\nrecovering Strassen's algorithm (in a particularly symmetric form) and new\nalgorithms for larger n. While these new algorithms do not improve the known\nupper bounds on tensor rank or the matrix multiplication exponent, they are\nbeautiful in their own right, and we point out modifications of this idea that\ncould plausibly lead to further improvements. Our constructions also suggest\nfurther patterns that could be mined for new algorithms, including a\ntantalizing connection with lattices. In particular, using lattices we give the\nmost transparent proof to date of Strassen's algorithm; the same proof works\nfor all n, to yield a decomposition with $n^3 - n + 1$ terms. \n\n"}
{"id": "1612.02916", "contents": "Title: Solida: A Blockchain Protocol Based on Reconfigurable Byzantine\n  Consensus Abstract: The decentralized cryptocurrency Bitcoin has experienced great success but\nalso encountered many challenges. One of the challenges has been the long\nconfirmation time. Another challenge is the lack of incentives at certain steps\nof the protocol, raising concerns for transaction withholding, selfish mining,\netc. To address these challenges, we propose Solida, a decentralized blockchain\nprotocol based on reconfigurable Byzantine consensus augmented by\nproof-of-work. Solida improves on Bitcoin in confirmation time, and provides\nsafety and liveness assuming the adversary control less than (roughly)\none-third of the total mining power. \n\n"}
{"id": "1612.06335", "contents": "Title: Coding against deletions in oblivious and online models Abstract: We consider binary error correcting codes when errors are deletions. A basic\nchallenge concerning deletion codes is determining $p_0^{(adv)}$, the zero-rate\nthreshold of adversarial deletions, defined to be the supremum of all $p$ for\nwhich there exists a code family with rate bounded away from 0 capable of\ncorrecting a fraction $p$ of adversarial deletions. A recent construction of\ndeletion-correcting codes [Bukh et al 17] shows that $p_0^{(adv)} \\ge\n\\sqrt{2}-1$, and the trivial upper bound, $p_0^{(adv)}\\le\\frac{1}{2}$, is the\nbest known. Perhaps surprisingly, we do not know whether or not $p_0^{(adv)} =\n1/2$.\n  In this work, to gain further insight into deletion codes, we explore two\nrelated error models: oblivious deletions and online deletions, which are in\nbetween random and adversarial deletions in power. In the oblivious model, the\nchannel can inflict an arbitrary pattern of $pn$ deletions, picked without\nknowledge of the codeword. We prove the existence of binary codes of positive\nrate that can correct any fraction $p < 1$ of oblivious deletions, establishing\nthat the associated zero-rate threshold $p_0^{(obliv)}$ equals $1$.\n  For online deletions, where the channel decides whether to delete bit $x_i$\nbased only on knowledge of bits $x_1x_2\\dots x_i$, define the deterministic\nzero-rate threshold for online deletions $p_0^{(on,d)}$ to be the supremum of\n$p$ for which there exist deterministic codes against an online channel causing\n$pn$ deletions with low average probability of error. That is, the probability\nthat a randomly chosen codeword is decoded incorrectly is small. We prove\n$p_0^{(adv)}=\\frac{1}{2}$ if and only if $p_0^{(on,d)}=\\frac{1}{2}$. \n\n"}
{"id": "1612.06340", "contents": "Title: Computing Human-Understandable Strategies Abstract: Algorithms for equilibrium computation generally make no attempt to ensure\nthat the computed strategies are understandable by humans. For instance the\nstrategies for the strongest poker agents are represented as massive binary\nfiles. In many situations, we would like to compute strategies that can\nactually be implemented by humans, who may have computational limitations and\nmay only be able to remember a small number of features or components of the\nstrategies that have been computed. We study poker games where private\ninformation distributions can be arbitrary. We create a large training set of\ngame instances and solutions, by randomly selecting the information\nprobabilities, and present algorithms that learn from the training instances in\norder to perform well in games with unseen information distributions. We are\nable to conclude several new fundamental rules about poker strategy that can be\neasily implemented by humans. \n\n"}
{"id": "1612.06476", "contents": "Title: Computational Complexity of Testing Proportional Justified\n  Representation Abstract: We consider a committee voting setting in which each voter approves of a\nsubset of candidates and based on the approvals, a target number of candidates\nare selected. Aziz et al. (2015) proposed two representation axioms called\njustified representation and extended justified representation. Whereas the\nformer can be tested as well as achieved in polynomial time, the latter\nproperty is coNP-complete to test and no polynomial-time algorithm is known to\nachieve it. Interestingly, S{\\'a}nchez-Fern{\\'a}ndez et~al. (2016) proposed an\nintermediate property called proportional justified representation that admits\na polynomial-time algorithm to achieve. The complexity of testing proportional\njustified representation has remained an open problem. In this paper, we settle\nthe complexity by proving that testing proportional justified representation is\ncoNP-complete. We complement the complexity result by showing that the problem\nadmits efficient algorithms if any of the following parameters are bounded: (1)\nnumber of voters (2) number of candidates (3) maximum number of candidates\napproved by a voter (4) maximum number of voters approving a given candidate. \n\n"}
{"id": "1701.00529", "contents": "Title: Truthful Facility Location with Additive Errors Abstract: We address the problem of locating facilities on the $[0,1]$ interval based\non reports from strategic agents. The cost of each agent is her distance to the\nclosest facility, and the global objective is to minimize either the maximum\ncost of an agent or the social cost.\n  As opposed to the extensive literature on facility location which considers\nthe multiplicative error, we focus on minimizing the worst-case additive error.\nMinimizing the additive error incentivizes mechanisms to adapt to the size of\nthe instance. I.e., mechanisms can sacrifice little efficiency in small\ninstances (location profiles in which all agents are relatively close to one\nanother), in order to gain more [absolute] efficiency in large instances. We\nargue that this measure is better suited for many manifestations of the\nfacility location problem in various domains.\n  We present tight bounds for mechanisms locating a single facility in both\ndeterministic and randomized cases. We further provide several extensions for\nlocating multiple facilities. \n\n"}
{"id": "1701.01963", "contents": "Title: Resource Management in Cloud Networking Using Economic Analysis and\n  Pricing Models: A Survey Abstract: This paper presents a comprehensive literature review on applications of\neconomic and pricing models for resource management in cloud networking. To\nachieve sustainable profit advantage, cost reduction, and flexibility in\nprovisioning of cloud resources, resource management in cloud networking\nrequires adaptive and robust designs to address many issues, e.g., resource\nallocation, bandwidth reservation, request allocation, and workload allocation.\nEconomic and pricing models have received a lot of attention as they can lead\nto desirable performance in terms of social welfare, fairness, truthfulness,\nprofit, user satisfaction, and resource utilization. This paper reviews\napplications of the economic and pricing models to develop adaptive algorithms\nand protocols for resource management in cloud networking. Besides, we survey a\nvariety of incentive mechanisms using the pricing strategies in sharing\nresources in edge computing. In addition, we consider using pricing models in\ncloud-based Software Defined Wireless Networking (cloud-based SDWN). Finally,\nwe highlight important challenges, open issues and future research directions\nof applying economic and pricing models to cloud networking \n\n"}
{"id": "1701.02193", "contents": "Title: Toward Quantum Combinatorial Games Abstract: In this paper, we propose a Quantum variation of combinatorial games,\ngeneralizing the Quantum Tic-Tac-Toe proposed by Allan Goff. A combinatorial\ngame is a two-player game with no chance and no hidden information, such as Go\nor Chess. In this paper, we consider the possibility of playing superpositions\nof moves in such games. We propose different rulesets depending on when\nsuperposed moves should be played, and prove that all these rulesets may lead\nsimilar games to different outcomes. We then consider Quantum variations of the\ngame of Nim. We conclude with some discussion on the relative interest of the\ndifferent rulesets. \n\n"}
{"id": "1701.02490", "contents": "Title: Real-Time Bidding by Reinforcement Learning in Display Advertising Abstract: The majority of online display ads are served through real-time bidding (RTB)\n--- each ad display impression is auctioned off in real-time when it is just\nbeing generated from a user visit. To place an ad automatically and optimally,\nit is critical for advertisers to devise a learning algorithm to cleverly bid\nan ad impression in real-time. Most previous works consider the bid decision as\na static optimization problem of either treating the value of each impression\nindependently or setting a bid price to each segment of ad volume. However, the\nbidding for a given ad campaign would repeatedly happen during its life span\nbefore the budget runs out. As such, each bid is strategically correlated by\nthe constrained budget and the overall effectiveness of the campaign (e.g., the\nrewards from generated clicks), which is only observed after the campaign has\ncompleted. Thus, it is of great interest to devise an optimal bidding strategy\nsequentially so that the campaign budget can be dynamically allocated across\nall the available impressions on the basis of both the immediate and future\nrewards. In this paper, we formulate the bid decision process as a\nreinforcement learning problem, where the state space is represented by the\nauction information and the campaign's real-time parameters, while an action is\nthe bid price to set. By modeling the state transition via auction competition,\nwe build a Markov Decision Process framework for learning the optimal bidding\npolicy to optimize the advertising performance in the dynamic real-time bidding\nenvironment. Furthermore, the scalability problem from the large real-world\nauction volume and campaign budget is well handled by state value approximation\nusing neural networks. \n\n"}
{"id": "1701.03990", "contents": "Title: Quantum algorithm for multivariate polynomial interpolation Abstract: How many quantum queries are required to determine the coefficients of a\ndegree-$d$ polynomial in $n$ variables? We present and analyze quantum\nalgorithms for this multivariate polynomial interpolation problem over the\nfields $\\mathbb{F}_q$, $\\mathbb{R}$, and $\\mathbb{C}$. We show that\n$k_{\\mathbb{C}}$ and $2k_{\\mathbb{C}}$ queries suffice to achieve probability\n$1$ for $\\mathbb{C}$ and $\\mathbb{R}$, respectively, where\n$k_{\\mathbb{C}}=\\smash{\\lceil\\frac{1}{n+1}{n+d\\choose d}\\rceil}$ except for\n$d=2$ and four other special cases. For $\\mathbb{F}_q$, we show that\n$\\smash{\\lceil\\frac{d}{n+d}{n+d\\choose d}\\rceil}$ queries suffice to achieve\nprobability approaching $1$ for large field order $q$. The classical query\ncomplexity of this problem is $\\smash{n+d\\choose d}$, so our result provides a\nspeedup by a factor of $n+1$, $\\frac{n+1}{2}$, and $\\frac{n+d}{d}$ for\n$\\mathbb{C}$, $\\mathbb{R}$, and $\\mathbb{F}_q$, respectively. Thus we find a\nmuch larger gap between classical and quantum algorithms than the univariate\ncase, where the speedup is by a factor of $2$. For the case of $\\mathbb{F}_q$,\nwe conjecture that $2k_{\\mathbb{C}}$ queries also suffice to achieve\nprobability approaching $1$ for large field order $q$, although we leave this\nas an open problem. \n\n"}
{"id": "1701.05633", "contents": "Title: Strong isomorphism in Eisert-Wilkens-Lewenstein type quantum games Abstract: The aim of this paper is to bring together the notions of quantum game and\ngame isomorphism. The work is intended as an attempt to introduce a new\ncriterion for quantum game schemes. The generally accepted requirement forces a\nquantum scheme to generate the classical game in a particular case. Now, given\na quantum game scheme and two isomorphic classical games, we additionally\nrequire the resulting quantum games to be isomorphic as well. We are concerned\nwith the Eisert-Wilkens-Lewenstein quantum game scheme and the strong\nisomorphism between games in strategic form. \n\n"}
{"id": "1701.06064", "contents": "Title: On Recoverable and Two-Stage Robust Selection Problems with Budgeted\n  Uncertainty Abstract: In this paper the problem of selecting $p$ out of $n$ available items is\ndiscussed, such that their total cost is minimized. We assume that costs are\nnot known exactly, but stem from a set of possible outcomes.\n  Robust recoverable and two-stage models of this selection problem are\nanalyzed. In the two-stage problem, up to $p$ items is chosen in the first\nstage, and the solution is completed once the scenario becomes revealed in the\nsecond stage. In the recoverable problem, a set of $p$ items is selected in the\nfirst stage, and can be modified by exchanging up to $k$ items in the second\nstage, after a scenario reveals.\n  We assume that uncertain costs are modeled through bounded uncertainty sets,\ni.e., the interval uncertainty sets with an additional linear (budget)\nconstraint, in their discrete and continuous variants. Polynomial algorithms\nfor recoverable and two-stage selection problems with continuous bounded\nuncertainty, and compact mixed integer formulations in the case of discrete\nbounded uncertainty are constructed. \n\n"}
{"id": "1701.08573", "contents": "Title: Is the essence of a quantum game captured completely in the original\n  classical game? Abstract: S. J. van Enk and R. Pike in PRA 66, 024306 (2002) argue that the equilibrium\nsolution to a quantum game isn't unique but is already present in the classical\ngame itself. In this work, we contest this assertion by showing that a random\nstrategy in a particular quantum (Hawk-Dove) game is unique to the quantum\ngame. In other words, one cannot obtain the equilibrium solution of the quantum\nHawk-Dove game in the classical Hawk-Dove game. Moreover, we provide an\nanalytical solution to the quantum $2\\times2$ strategic form Hawk-Dove game\nusing randomly mixed strategies. The random strategy which we describe is\nPareto optimal with their payoff classically unobtainable. We compare quantum\nstrategies to correlated strategies and find that correlated strategies in the\nquantum Hawk-Dove game or quantum Prisoner's dilemma yield the Nash equilibrium\nsolution. \n\n"}
{"id": "1701.08644", "contents": "Title: Security Game with Non-additive Utilities and Multiple Attacker\n  Resources Abstract: There has been significant interest in studying security games for modeling\nthe interplay of attacks and defenses on various systems involving critical\ninfrastructure, financial system security, political campaigns, and civil\nsafeguarding. However, existing security game models typically either assume\nadditive utility functions, or that the attacker can attack only one target.\nSuch assumptions lead to tractable analysis, but miss key inherent dependencies\nthat exist among different targets in current complex networks. In this paper,\nwe generalize the classical security game models to allow for non-additive\nutility functions. We also allow attackers to be able to attack multiple\ntargets. We examine such a general security game from a theoretical perspective\nand provide a unified view. In particular, we show that each security game is\nequivalent to a combinatorial optimization problem over a set system\n$\\varepsilon$, which consists of defender's pure strategy space. The key\ntechnique we use is based on the transformation, projection of a polytope, and\nthe elipsoid method. This work settles several open questions in security game\ndomain and significantly extends the state of-the-art of both the polynomial\nsolvable and NP-hard class of the security game. \n\n"}
{"id": "1702.01017", "contents": "Title: Emergence of Distributed Coordination in the Kolkata Paise Restaurant\n  Problem with Finite Information Abstract: In this paper, we study a large-scale distributed coordination problem and\npropose efficient adaptive strategies to solve the problem. The basic problem\nis to allocate finite number of resources to individual agents such that there\nis as little congestion as possible and the fraction of unutilized resources is\nreduced as far as possible. In the absence of a central planner and global\ninformation, agents can employ adaptive strategies that uses only finite\nknowledge about the competitors. In this paper, we show that a combination of\nfinite information sets and reinforcement learning can increase the utilization\nrate of resources substantially. \n\n"}
{"id": "1702.01454", "contents": "Title: Property Testing of Joint Distributions using Conditional Samples Abstract: In this paper, we consider the problem of testing properties of joint\ndistributions under the Conditional Sampling framework. In the standard\nsampling model, the sample complexity of testing properties of joint\ndistributions is exponential in the dimension, resulting in inefficient\nalgorithms for practical use. While recent results achieve efficient algorithms\nfor product distributions with significantly smaller sample complexity, no\nefficient algorithm is expected when the marginals are not independent.\n  We initialize the study of conditional sampling in the multidimensional\nsetting. We propose a subcube conditional sampling model where the tester can\ncondition on an (adaptively) chosen subcube of the domain. Due to its\nsimplicity, this model is potentially implementable in many practical\napplications, particularly when the distribution is a joint distribution over\n$\\Sigma^n$ for some set $\\Sigma$.\n  We present algorithms for various fundamental properties of distributions in\nthe subcube-conditioning model and prove that the sample complexity is\npolynomial in the dimension $n$ (and not exponential as in the traditional\nmodel). We present an algorithm for testing identity to a known distribution\nusing $\\tilde{\\mathcal{O}}(n^2)$-subcube-conditional samples, an algorithm for\ntesting identity between two unknown distributions using\n$\\tilde{\\mathcal{O}}(n^5)$-subcube-conditional samples and an algorithm for\ntesting identity to a product distribution using\n$tilde{\\mathcal{O}}(n^5)$-subcube-conditional samples.\n  The central concept of our technique involves an elegant chain rule which can\nbe proved using basic techniques of probability theory yet powerful enough to\navoid the curse of dimensionality. \n\n"}
{"id": "1702.02113", "contents": "Title: Rare Nash Equilibria and the Price of Anarchy in Large Static Games Abstract: We study a static game played by a finite number of agents, in which agents\nare assigned independent and identically distributed random types and each\nagent minimizes its objective function by choosing from a set of admissible\nactions that depends on its type. The game is anonymous in the sense that the\nobjective function of each agent depends on the actions of other agents only\nthrough the empirical distribution of their type-action pairs. We study the\nasymptotic behavior of Nash equilibria, as the number of agents tends to\ninfinity, first by deriving laws of large numbers characterizes almost sure\nlimit points of Nash equilibria in terms of so-called Cournot-Nash equilibria\nof an associated nonatomic game. Our main results are large deviation\nprinciples that characterize the probability of rare Nash equilibria and\nassociated conditional limit theorems describing the behavior of equilibria\nconditioned on a rare event. The results cover situations when neither the\nfinite-player game nor the associated nonatomic game has a unique equilibrium.\nIn addition, we study the asymptotic behavior of the price of anarchy,\ncomplementing existing worst-case bounds with new probabilistic bounds in the\ncontext of congestion games, which are used to model traffic routing in\nnetworks. \n\n"}
{"id": "1702.03037", "contents": "Title: Multi-agent Reinforcement Learning in Sequential Social Dilemmas Abstract: Matrix games like Prisoner's Dilemma have guided research on social dilemmas\nfor decades. However, they necessarily treat the choice to cooperate or defect\nas an atomic action. In real-world social dilemmas these choices are temporally\nextended. Cooperativeness is a property that applies to policies, not\nelementary actions. We introduce sequential social dilemmas that share the\nmixed incentive structure of matrix game social dilemmas but also require\nagents to learn policies that implement their strategic intentions. We analyze\nthe dynamics of policies learned by multiple self-interested independent\nlearning agents, each using its own deep Q-network, on two Markov games we\nintroduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We\ncharacterize how learned behavior in each domain changes as a function of\nenvironmental factors including resource abundance. Our experiments show how\nconflict can emerge from competition over shared resources and shed light on\nhow the sequential nature of real world social dilemmas affects cooperation. \n\n"}
{"id": "1702.04849", "contents": "Title: Theoretical and Practical Advances on Smoothing for Extensive-Form Games Abstract: Sparse iterative methods, in particular first-order methods, are known to be\namong the most effective in solving large-scale two-player zero-sum\nextensive-form games. The convergence rates of these methods depend heavily on\nthe properties of the distance-generating function that they are based on. We\ninvestigate the acceleration of first-order methods for solving extensive-form\ngames through better design of the dilated entropy function---a class of\ndistance-generating functions related to the domains associated with the\nextensive-form games. By introducing a new weighting scheme for the dilated\nentropy function, we develop the first distance-generating function for the\nstrategy spaces of sequential games that has no dependence on the branching\nfactor of the player. This result improves the convergence rate of several\nfirst-order methods by a factor of $\\Omega(b^dd)$, where $b$ is the branching\nfactor of the player, and $d$ is the depth of the game tree.\n  Thus far, counterfactual regret minimization methods have been faster in\npractice, and more popular, than first-order methods despite their\ntheoretically inferior convergence rates. Using our new weighting scheme and\npractical tuning we show that, for the first time, the excessive gap technique\ncan be made faster than the fastest counterfactual regret minimization\nalgorithm, CFR+, in practice. \n\n"}
{"id": "1702.05355", "contents": "Title: How Much Does Users' Psychology Matter in Engineering Mean-Field-Type\n  Games Abstract: Until now mean-field-type game theory was not focused on\ncognitively-plausible models of choices in humans, animals, machines, robots,\nsoftware-defined and mobile devices strategic interactions. This work presents\nsome effects of users' psychology in mean-field-type games. In addition to the\ntraditional \"material\" payoff modelling, psychological patterns are introduced\nin order to better capture and understand behaviors that are observed in\nengineering practice or in experimental settings. The psychological payoff\nvalue depends upon choices, mean-field states, mean-field actions, empathy and\nbeliefs. It is shown that the affective empathy enforces mean-field equilibrium\npayoff equity and improves fairness between the players. It establishes\nequilibrium systems for such interactive decision-making problems. Basic\nempathy concepts are illustrated in several important problems in engineering\nincluding resource sharing, packet collision minimization, energy markets, and\nforwarding in Device-to-Device communications. The work conducts also an\nexperiment with 47 people who have to decide whether to cooperate or not. The\nbasic Interpersonal Reactivity Index of empathy metrics were used to measure\nthe empathy distribution of each participant. Android app called Empathizer is\ndeveloped to analyze systematically the data obtained from the participants.\nThe experimental results reveal that the dominated strategies of the classical\ngame theory are not dominated any more when users' psychology is involved, and\na significant level of cooperation is observed among the users who are\npositively partially empathetic. \n\n"}
{"id": "1702.05456", "contents": "Title: LCL problems on grids Abstract: LCLs or locally checkable labelling problems (e.g. maximal independent set,\nmaximal matching, and vertex colouring) in the LOCAL model of computation are\nvery well-understood in cycles (toroidal 1-dimensional grids): every problem\nhas a complexity of $O(1)$, $\\Theta(\\log^* n)$, or $\\Theta(n)$, and the design\nof optimal algorithms can be fully automated.\n  This work develops the complexity theory of LCL problems for toroidal\n2-dimensional grids. The complexity classes are the same as in the\n1-dimensional case: $O(1)$, $\\Theta(\\log^* n)$, and $\\Theta(n)$. However, given\nan LCL problem it is undecidable whether its complexity is $\\Theta(\\log^* n)$\nor $\\Theta(n)$ in 2-dimensional grids.\n  Nevertheless, if we correctly guess that the complexity of a problem is\n$\\Theta(\\log^* n)$, we can completely automate the design of optimal\nalgorithms. For any problem we can find an algorithm that is of a normal form\n$A' \\circ S_k$, where $A'$ is a finite function, $S_k$ is an algorithm for\nfinding a maximal independent set in $k$th power of the grid, and $k$ is a\nconstant.\n  Finally, partially with the help of automated design tools, we classify the\ncomplexity of several concrete LCL problems related to colourings and\norientations. \n\n"}
{"id": "1702.05860", "contents": "Title: Robust Sparse Estimation Tasks in High Dimensions Abstract: In this paper we initiate the study of whether or not sparse estimation tasks\ncan be performed efficiently in high dimensions, in the robust setting where an\n$\\eps$-fraction of samples are corrupted adversarially. We study the natural\nrobust version of two classical sparse estimation problems, namely, sparse mean\nestimation and sparse PCA in the spiked covariance model. For both of these\nproblems, we provide the first efficient algorithms that provide non-trivial\nerror guarantees in the presence of noise, using only a number of samples which\nis similar to the number required for these problems without noise. In\nparticular, our sample complexities are sublinear in the ambient dimension $d$.\nOur work also suggests evidence for new computational-vs-statistical gaps for\nthese problems (similar to those for sparse PCA without noise) which only arise\nin the presence of noise. \n\n"}
{"id": "1702.06017", "contents": "Title: CLS: New Problems and Completeness Abstract: The complexity class CLS was introduced by Daskalakis and Papadimitriou with\nthe goal of capturing the complexity of some well-known problems in\nPPAD$~\\cap~$PLS that have resisted, in some cases for decades, attempts to put\nthem in polynomial time. No complete problem was known for CLS, and in previous\nwork, the problems ContractionMap, i.e., the problem of finding an approximate\nfixpoint of a contraction map, and PLCP, i.e., the problem of solving a\nP-matrix Linear Complementarity Problem, were identified as prime candidates.\n  First, we present a new CLS-complete problem MetaMetricContractionMap, which\nis closely related to the ContractionMap. Second, we introduce\nEndOfPotentialLine, which captures aspects of PPAD and PLS directly via a\nmonotonic directed path, and show that EndOfPotentialLine is in CLS via a\ntwo-way reduction to EndOfMeteredLine. The latter was defined to keep track of\nhow far a vertex is on the PPAD path via a restricted potential function.\nThird, we reduce PLCP to EndOfPotentialLine, thus making EndOfPotentialLine and\nEndOfMeteredLine at least as likely to be hard for CLS as PLCP. This last\nresult leverages the monotonic structure of Lemke paths for PLCP problems,\nmaking EndOfPotentialLine a likely candidate to capture the exact complexity of\nPLCP; we note that the structure of Lemke-Howson paths for finding a Nash\nequilibrium in a two-player game very directly motivated the definition of the\ncomplexity class PPAD, which eventually ended up capturing this problem's\ncomplexity exactly. \n\n"}
{"id": "1702.06237", "contents": "Title: Exact tensor completion with sum-of-squares Abstract: We obtain the first polynomial-time algorithm for exact tensor completion\nthat improves over the bound implied by reduction to matrix completion. The\nalgorithm recovers an unknown 3-tensor with $r$ incoherent, orthogonal\ncomponents in $\\mathbb R^n$ from $r\\cdot \\tilde O(n^{1.5})$ randomly observed\nentries of the tensor. This bound improves over the previous best one of\n$r\\cdot \\tilde O(n^{2})$ by reduction to exact matrix completion. Our bound\nalso matches the best known results for the easier problem of approximate\ntensor completion (Barak & Moitra, 2015).\n  Our algorithm and analysis extends seminal results for exact matrix\ncompletion (Candes & Recht, 2009) to the tensor setting via the sum-of-squares\nmethod. The main technical challenge is to show that a small number of randomly\nchosen monomials are enough to construct a degree-3 polynomial with precisely\nplanted orthogonal global optima over the sphere and that this fact can be\ncertified within the sum-of-squares proof system. \n\n"}
{"id": "1702.06503", "contents": "Title: When can Graph Hyperbolicity be computed in Linear Time? Abstract: Hyperbolicity measures, in terms of (distance) metrics, how close a given\ngraph is to being a tree. Due to its relevance in modeling real-world networks,\nhyperbolicity has seen intensive research over the last years. Unfortunately,\nthe best known algorithms for computing the hyperbolicity number of a graph\n(the smaller, the more tree-like) have running time $O(n^4)$, where $n$ is the\nnumber of graph vertices. Exploiting the framework of parameterized complexity\nanalysis, we explore possibilities for \"linear-time FPT\" algorithms to compute\nhyperbolicity. For instance, we show that hyperbolicity can be computed in time\n$O(2^{O(k)} + n +m)$ ($m$ being the number of graph edges) while at the same\ntime, unless the SETH fails, there is no $2^{o(k)}n^2$-time algorithm. \n\n"}
{"id": "1702.06645", "contents": "Title: Resource Sharing Among mmWave Cellular Service Providers in a Vertically\n  Differentiated Duopoly Abstract: With the increasing interest in the use of millimeter wave bands for 5G\ncellular systems comes renewed interest in resource sharing. Properties of\nmillimeter wave bands such as massive bandwidth, highly directional antennas,\nhigh penetration loss, and susceptibility to shadowing, suggest technical\nadvantages to spectrum and infrastructure sharing in millimeter wave cellular\nnetworks. However, technical advantages do not necessarily translate to\nincreased profit for service providers, or increased consumer surplus. In this\npaper, detailed network simulations are used to better understand the economic\nimplications of resource sharing in a vertically differentiated duopoly market\nfor cellular service. The results suggest that resource sharing is less often\nprofitable for millimeter wave service providers compared to microwave cellular\nservice providers, and does not necessarily increase consumer surplus. \n\n"}
{"id": "1702.07031", "contents": "Title: Proactive Resource Management for LTE in Unlicensed Spectrum: A Deep\n  Learning Perspective Abstract: LTE in unlicensed spectrum using licensed assisted access LTE (LTE-LAA) is a\npromising approach to overcome the wireless spectrum scarcity. However, to reap\nthe benefits of LTE-LAA, a fair coexistence mechanism with other incumbent WiFi\ndeployments is required. In this paper, a novel deep learning approach is\nproposed for modeling the resource allocation problem of LTE-LAA small base\nstations (SBSs). The proposed approach enables multiple SBSs to proactively\nperform dynamic channel selection, carrier aggregation, and fractional spectrum\naccess while guaranteeing fairness with existing WiFi networks and other\nLTE-LAA operators. Adopting a proactive coexistence mechanism enables future\ndelay-tolerant LTE-LAA data demands to be served within a given prediction\nwindow ahead of their actual arrival time thus avoiding the underutilization of\nthe unlicensed spectrum during off-peak hours while maximizing the total served\nLTE-LAA traffic load. To this end, a noncooperative game model is formulated in\nwhich SBSs are modeled as Homo Egualis agents that aim at predicting a sequence\nof future actions and thus achieving long-term equal weighted fairness with\nWLAN and other LTE-LAA operators over a given time horizon. The proposed deep\nlearning algorithm is then shown to reach a mixed-strategy Nash equilibrium\n(NE), when it converges. Simulation results using real data traces show that\nthe proposed scheme can yield up to 28% and 11% gains over a conventional\nreactive approach and a proportional fair coexistence mechanism, respectively.\nThe results also show that the proposed framework prevents WiFi performance\ndegradation for a densely deployed LTE-LAA network. \n\n"}
{"id": "1702.07311", "contents": "Title: ERA: A Framework for Economic Resource Allocation for the Cloud Abstract: Cloud computing has reached significant maturity from a systems perspective,\nbut currently deployed solutions rely on rather basic economics mechanisms that\nyield suboptimal allocation of the costly hardware resources. In this paper we\npresent Economic Resource Allocation (ERA), a complete framework for scheduling\nand pricing cloud resources, aimed at increasing the efficiency of cloud\nresources usage by allocating resources according to economic principles. The\nERA architecture carefully abstracts the underlying cloud infrastructure,\nenabling the development of scheduling and pricing algorithms independently of\nthe concrete lower-level cloud infrastructure and independently of its\nconcerns. Specifically, ERA is designed as a flexible layer that can sit on top\nof any cloud system and interfaces with both the cloud resource manager and\nwith the users who reserve resources to run their jobs. The jobs are scheduled\nbased on prices that are dynamically calculated according to the predicted\ndemand. Additionally, ERA provides a key internal API to pluggable algorithmic\nmodules that include scheduling, pricing and demand prediction. We provide a\nproof-of-concept software and demonstrate the effectiveness of the architecture\nby testing ERA over both public and private cloud systems -- Azure Batch of\nMicrosoft and Hadoop/YARN. A broader intent of our work is to foster\ncollaborations between economics and system communities. To that end, we have\ndeveloped a simulation platform via which economics and system experts can test\ntheir algorithmic implementations. \n\n"}
{"id": "1702.07444", "contents": "Title: Bandits with Movement Costs and Adaptive Pricing Abstract: We extend the model of Multi-armed Bandit with unit switching cost to\nincorporate a metric between the actions. We consider the case where the metric\nover the actions can be modeled by a complete binary tree, and the distance\nbetween two leaves is the size of the subtree of their least common ancestor,\nwhich abstracts the case that the actions are points on the continuous interval\n$[0,1]$ and the switching cost is their distance. In this setting, we give a\nnew algorithm that establishes a regret of $\\widetilde{O}(\\sqrt{kT} + T/k)$,\nwhere $k$ is the number of actions and $T$ is the time horizon. When the set of\nactions corresponds to whole $[0,1]$ interval we can exploit our method for the\ntask of bandit learning with Lipschitz loss functions, where our algorithm\nachieves an optimal regret rate of $\\widetilde{\\Theta}(T^{2/3})$, which is the\nsame rate one obtains when there is no penalty for movements. As our main\napplication, we use our new algorithm to solve an adaptive pricing problem.\nSpecifically, we consider the case of a single seller faced with a stream of\npatient buyers. Each buyer has a private value and a window of time in which\nthey are interested in buying, and they buy at the lowest price in the window,\nif it is below their value. We show that with an appropriate discretization of\nthe prices, the seller can achieve a regret of $\\widetilde{O}(T^{2/3})$\ncompared to the best fixed price in hindsight, which outperform the previous\nregret bound of $\\widetilde{O}(T^{3/4})$ for the problem. \n\n"}
{"id": "1702.07902", "contents": "Title: Approval Voting with Intransitive Preferences Abstract: We extend Approval voting to the settings where voters may have intransitive\npreferences. The major obstacle to applying Approval voting in these settings\nis that voters are not able to clearly determine who they should approve or\ndisapprove, due to the intransitivity of their preferences. An approach to\naddress this issue is to apply tournament solutions to help voters make the\ndecision. We study a class of voting systems where first each voter casts a\nvote defined as a tournament, then a well-defined tournament solution is\napplied to select the candidates who are assumed to be approved by the voter.\nWinners are the ones receiving the most approvals. We study axiomatic\nproperties of this class of voting systems and complexity of control and\nbribery problems for these voting systems. \n\n"}
{"id": "1702.08862", "contents": "Title: Proportional Representation in Vote Streams Abstract: We consider elections where the voters come one at a time, in a streaming\nfashion, and devise space-efficient algorithms which identify an approximate\nwinning committee with respect to common multiwinner proportional\nrepresentation voting rules; specifically, we consider the Approval-based and\nthe Borda-based variants of both the Chamberlin-- ourant rule and the Monroe\nrule. We complement our algorithms with lower bounds. Somewhat surprisingly,\nour results imply that, using space which does not depend on the number of\nvoters it is possible to efficiently identify an approximate representative\ncommittee of fixed size over vote streams with huge number of voters. \n\n"}
{"id": "1703.00320", "contents": "Title: Investigating the Characteristics of One-Sided Matching Mechanisms Under\n  Various Preferences and Risk Attitudes Abstract: One-sided matching mechanisms are fundamental for assigning a set of\nindivisible objects to a set of self-interested agents when monetary transfers\nare not allowed. Two widely-studied randomized mechanisms in multiagent\nsettings are the Random Serial Dictatorship (RSD) and the Probabilistic Serial\nRule (PS). Both mechanisms require only that agents specify ordinal preferences\nand have a number of desirable economic and computational properties. However,\nthe induced outcomes of the mechanisms are often incomparable and thus there\nare challenges when it comes to deciding which mechanism to adopt in practice.\nIn this paper, we first consider the space of general ordinal preferences and\nprovide empirical results on the (in)comparability of RSD and PS. We analyze\ntheir respective economic properties under general and lexicographic\npreferences. We then instantiate utility functions with the goal of gaining\ninsights on the manipulability, efficiency, and envyfreeness of the mechanisms\nunder different risk-attitude models. Our results hold under various preference\ndistribution models, which further confirm the broad use of RSD in most\npractical applications. \n\n"}
{"id": "1703.00893", "contents": "Title: Being Robust (in High Dimensions) Can Be Practical Abstract: Robust estimation is much more challenging in high dimensions than it is in\none dimension: Most techniques either lead to intractable optimization problems\nor estimators that can tolerate only a tiny fraction of errors. Recent work in\ntheoretical computer science has shown that, in appropriate distributional\nmodels, it is possible to robustly estimate the mean and covariance with\npolynomial time algorithms that can tolerate a constant fraction of\ncorruptions, independent of the dimension. However, the sample and time\ncomplexity of these algorithms is prohibitively large for high-dimensional\napplications. In this work, we address both of these issues by establishing\nsample complexity bounds that are optimal, up to logarithmic factors, as well\nas giving various refinements that allow the algorithms to tolerate a much\nlarger fraction of corruptions. Finally, we show on both synthetic and real\ndata that our algorithms have state-of-the-art performance and suddenly make\nhigh-dimensional robust estimation a realistic possibility. \n\n"}
{"id": "1703.00941", "contents": "Title: On the Fine-grained Complexity of One-Dimensional Dynamic Programming Abstract: In this paper, we investigate the complexity of one-dimensional dynamic\nprogramming, or more specifically, of the Least-Weight Subsequence (LWS)\nproblem: Given a sequence of $n$ data items together with weights for every\npair of the items, the task is to determine a subsequence $S$ minimizing the\ntotal weight of the pairs adjacent in $S$. A large number of natural problems\ncan be formulated as LWS problems, yielding obvious $O(n^2)$-time solutions.\n  In many interesting instances, the $O(n^2)$-many weights can be succinctly\nrepresented. Yet except for near-linear time algorithms for some specific\nspecial cases, little is known about when an LWS instantiation admits a\nsubquadratic-time algorithm and when it does not. In particular, no lower\nbounds for LWS instantiations have been known before. In an attempt to remedy\nthis situation, we provide a general approach to study the fine-grained\ncomplexity of succinct instantiations of the LWS problem. In particular, given\nan LWS instantiation we identify a highly parallel core problem that is\nsubquadratically equivalent. This provides either an explanation for the\napparent hardness of the problem or an avenue to find improved algorithms as\nthe case may be.\n  More specifically, we prove subquadratic equivalences between the following\npairs (an LWS instantiation and the corresponding core problem) of problems: a\nlow-rank version of LWS and minimum inner product, finding the longest chain of\nnested boxes and vector domination, and a coin change problem which is closely\nrelated to the knapsack problem and (min,+)-convolution. Using these\nequivalences and known SETH-hardness results for some of the core problems, we\ndeduce tight conditional lower bounds for the corresponding LWS instantiations.\nWe also establish the (min,+)-convolution-hardness of the knapsack problem. \n\n"}
{"id": "1703.01138", "contents": "Title: Multiplicative Weights Update with Constant Step-Size in Congestion\n  Games: Convergence, Limit Cycles and Chaos Abstract: The Multiplicative Weights Update (MWU) method is a ubiquitous meta-algorithm\nthat works as follows: A distribution is maintained on a certain set, and at\neach step the probability assigned to element $\\gamma$ is multiplied by $(1\n-\\epsilon C(\\gamma))>0$ where $C(\\gamma)$ is the \"cost\" of element $\\gamma$ and\nthen rescaled to ensure that the new values form a distribution. We analyze MWU\nin congestion games where agents use \\textit{arbitrary admissible constants} as\nlearning rates $\\epsilon$ and prove convergence to \\textit{exact Nash\nequilibria}. Our proof leverages a novel connection between MWU and the\nBaum-Welch algorithm, the standard instantiation of the\nExpectation-Maximization (EM) algorithm for hidden Markov models (HMM).\nInterestingly, this convergence result does not carry over to the nearly\nhomologous MWU variant where at each step the probability assigned to element\n$\\gamma$ is multiplied by $(1 -\\epsilon)^{C(\\gamma)}$ even for the most\ninnocuous case of two-agent, two-strategy load balancing games, where such\ndynamics can provably lead to limit cycles or even chaotic behavior. \n\n"}
{"id": "1703.01380", "contents": "Title: Internalization of Externalities in Interdependent Security: Large\n  Network Cases Abstract: With increasing connectivity among comprising agents or (sub-)systems in\nlarge, complex systems, there is a growing interest in understanding\ninterdependent security and dealing with inefficiency in security investments.\nMaking use of a population game model and the well-known Chung-Lu random graph\nmodel, we study how one could encourage selfish agents to invest more in\nsecurity by internalizing the externalities produced by their security\ninvestments.\n  To this end, we first establish an interesting relation between the local\nminimizers of social cost and the Nash equilibria of a population game with\nslightly altered costs. Secondly, under a mild technical assumption, we\ndemonstrate that there exists a unique minimizer of social cost and it\ncoincides with the unique Nash equilibrium of the population game. This finding\ntells us how to modify the private cost functions of selfish agents in order to\nenhance the overall security and reduce social cost. In addition, it reveals\nhow the sensitivity of overall security to security investments of agents\ninfluences their externalities and, consequently, penalties or taxes that\nshould be imposed for internalization of externalities. Finally, we illustrate\nhow the degree distribution of agents influences their security investments and\noverall security at both the NEs of population games and social optima. \n\n"}
{"id": "1703.01649", "contents": "Title: Fair Allocation of Indivisible Goods to Asymmetric Agents Abstract: We study fair allocation of indivisible goods to agents with unequal\nentitlements. Fair allocation has been the subject of many studies in both\ndivisible and indivisible settings. Our emphasis is on the case where the goods\nare indivisible and agents have unequal entitlements. This problem is a\ngeneralization of the work by Procaccia and Wang wherein the agents are assumed\nto be symmetric with respect to their entitlements. Although Procaccia and Wang\nshow an almost fair (constant approximation) allocation exists in their\nsetting, our main result is in sharp contrast to their observation. We show\nthat, in some cases with $n$ agents, no allocation can guarantee better than\n$1/n$ approximation of a fair allocation when the entitlements are not\nnecessarily equal. Furthermore, we devise a simple algorithm that ensures a\n$1/n$ approximation guarantee. Our second result is for a restricted version of\nthe problem where the valuation of every agent for each good is bounded by the\ntotal value he wishes to receive in a fair allocation. Although this assumption\nmight seem w.l.o.g, we show it enables us to find a $1/2$ approximation fair\nallocation via a greedy algorithm. Finally, we run some experiments on\nreal-world data and show that, in practice, a fair allocation is likely to\nexist. We also support our experiments by showing positive results for two\nstochastic variants of the problem, namely stochastic agents and stochastic\nitems. \n\n"}
{"id": "1703.01851", "contents": "Title: Approximation Algorithms for Maximin Fair Division Abstract: We consider the problem of allocating indivisible goods fairly among n agents\nwho have additive and submodular valuations for the goods. Our fairness\nguarantees are in terms of the maximin share, that is defined to be the maximum\nvalue that an agent can ensure for herself, if she were to partition the goods\ninto n bundles, and then receive a minimum valued bundle. Since maximin fair\nallocations (i.e., allocations in which each agent gets at least her maximin\nshare) do not always exist, prior work has focused on approximation results\nthat aim to find allocations in which the value of the bundle allocated to each\nagent is (multiplicatively) as close to her maximin share as possible. In\nparticular, Procaccia and Wang (2014) along with Amanatidis et al. (2015) have\nshown that under additive valuations a 2/3-approximate maximin fair allocation\nalways exists and can be found in polynomial time. We complement these results\nby developing a simple and efficient algorithm that achieves the same\napproximation guarantee.\n  Furthermore, we initiate the study of approximate maximin fair division under\nsubmodular valuations. Specifically, we show that when the valuations of the\nagents are nonnegative, monotone, and submodular, then a 0.21-approximate\nmaximin fair allocation is guaranteed to exist. In fact, we show that such an\nallocation can be efficiently found by using a simple round-robin algorithm. A\ntechnical contribution of the paper is to analyze the performance of this\ncombinatorial algorithm by employing the concept of multilinear extensions. \n\n"}
{"id": "1703.03111", "contents": "Title: Statistical Cost Sharing Abstract: We study the cost sharing problem for cooperative games in situations where\nthe cost function $C$ is not available via oracle queries, but must instead be\nderived from data, represented as tuples $(S, C(S))$, for different subsets $S$\nof players. We formalize this approach, which we call statistical cost sharing,\nand consider the computation of the core and the Shapley value, when the tuples\nare drawn from some distribution $\\mathcal{D}$.\n  Previous work by Balcan et al. in this setting showed how to compute cost\nshares that satisfy the core property with high probability for limited classes\nof functions. We expand on their work and give an algorithm that computes such\ncost shares for any function with a non-empty core. We complement these results\nby proving an inapproximability lower bound for a weaker relaxation.\n  We then turn our attention to the Shapley value. We first show that when cost\nfunctions come from the family of submodular functions with bounded curvature,\n$\\kappa$, the Shapley value can be approximated from samples up to a $\\sqrt{1 -\n\\kappa}$ factor, and that the bound is tight. We then define statistical\nanalogues of the Shapley axioms, and derive a notion of statistical Shapley\nvalue. We show that these can always be approximated arbitrarily well for\ngeneral functions over any distribution $\\mathcal{D}$. \n\n"}
{"id": "1703.03741", "contents": "Title: Opinion-Based Centrality in Multiplex Networks: A Convex Optimization\n  Approach Abstract: Most people simultaneously belong to several distinct social networks, in\nwhich their relations can be different. They have opinions about certain\ntopics, which they share and spread on these networks, and are influenced by\nthe opinions of other persons. In this paper, we build upon this observation to\npropose a new nodal centrality measure for multiplex networks. Our measure,\ncalled Opinion centrality, is based on a stochastic model representing opinion\npropagation dynamics in such a network. We formulate an optimization problem\nconsisting in maximizing the opinion of the whole network when controlling an\nexternal influence able to affect each node individually. We find a\nmathematical closed form of this problem, and use its solution to derive our\ncentrality measure. According to the opinion centrality, the more a node is\nworth investing external influence, and the more it is central. We perform an\nempirical study of the proposed centrality over a toy network, as well as a\ncollection of real-world networks. Our measure is generally negatively\ncorrelated with existing multiplex centrality measures, and highlights\ndifferent types of nodes, accordingly to its definition. \n\n"}
{"id": "1703.04143", "contents": "Title: Bernoulli Factories and Black-Box Reductions in Mechanism Design Abstract: We provide a polynomial time reduction from Bayesian incentive compatible\nmechanism design to Bayesian algorithm design for welfare maximization\nproblems. Unlike prior results, our reduction achieves exact incentive\ncompatibility for problems with multi-dimensional and continuous type spaces.\nThe key technical barrier preventing exact incentive compatibility in prior\nblack-box reductions is that repairing violations of incentive constraints\nrequires understanding the distribution of the mechanism's output, which is\ntypically #P-hard to compute. Reductions that instead estimate the output\ndistribution by sampling inevitably suffer from sampling error, which typically\nprecludes exact incentive compatibility.\n  We overcome this barrier by employing and generalizing the computational\nmodel in the literature on $\\textit{Bernoulli factories}$. In a Bernoulli\nfactory problem, one is given a function mapping the bias of an \"input coin\" to\nthat of an \"output coin\", and the challenge is to efficiently simulate the\noutput coin given only sample access to the input coin. This is the key\ningredient in designing an incentive compatible mechanism for bipartite\nmatching, which can be used to make the approximately incentive compatible\nreduction of Hartline et al. (2015) exactly incentive compatible. \n\n"}
{"id": "1703.05388", "contents": "Title: A distributed primal-dual algorithm for computation of generalized Nash\n  equilibria with shared affine coupling constraints via operator splitting\n  methods Abstract: In this paper, we propose a distributed primal-dual algorithm for computation\nof a generalized Nash equilibrium (GNE) in noncooperative games over network\nsystems. In the considered game, not only each player's local objective\nfunction depends on other players' decisions, but also the feasible decision\nsets of all the players are coupled together with a globally shared affine\ninequality constraint. Adopting the variational GNE, that is the solution of a\nvariational inequality, as a refinement of GNE, we introduce a primal-dual\nalgorithm that players can use to seek it in a distributed manner. Each player\nonly needs to know its local objective function, local feasible set, and a\nlocal block of the affine constraint. Meanwhile, each player only needs to\nobserve the decisions on which its local objective function explicitly depends\nthrough the interference graph and share information related to multipliers\nwith its neighbors through a multiplier graph. Through a primal-dual analysis\nand an augmentation of variables, we reformulate the problem as finding the\nzeros of a sum of monotone operators. Our distributed primal-dual algorithm is\nbased on forward-backward operator splitting methods. We prove its convergence\nto the variational GNE for fixed step-sizes under some mild assumptions. Then a\ndistributed algorithm with inertia is also introduced and analyzed for\nvariational GNE seeking. Finally, numerical simulations for network Cournot\ncompetition are given to illustrate the algorithm efficiency and performance. \n\n"}
{"id": "1703.08139", "contents": "Title: Optimal lower bounds for universal relation, samplers, and finding\n  duplicates Abstract: In the communication problem $\\mathbf{UR}$ (universal relation) [KRW95],\nAlice and Bob respectively receive $x$ and $y$ in $\\{0,1\\}^n$ with the promise\nthat $x\\neq y$. The last player to receive a message must output an index $i$\nsuch that $x_i\\neq y_i$. We prove that the randomized one-way communication\ncomplexity of this problem in the public coin model is exactly $\\Theta(\\min\\{n,\n\\log(1/\\delta)\\log^2(\\frac{n}{\\log(1/\\delta)})\\})$ bits for failure probability\n$\\delta$. Our lower bound holds even if promised $\\mathop{support}(y)\\subset\n\\mathop{support}(x)$. As a corollary, we obtain optimal lower bounds for\n$\\ell_p$-sampling in strict turnstile streams for $0\\le p < 2$, as well as for\nthe problem of finding duplicates in a stream. Our lower bounds do not need to\nuse large weights, and hold even if it is promised that $x\\in\\{0,1\\}^n$ at all\npoints in the stream.\n  Our lower bound demonstrates that any algorithm $\\mathcal{A}$ solving\nsampling problems in turnstile streams in low memory can be used to encode\nsubsets of $[n]$ of certain sizes into a number of bits below the information\ntheoretic minimum. Our encoder makes adaptive queries to $\\mathcal{A}$\nthroughout its execution, but done carefully so as to not violate correctness.\nThis is accomplished by injecting random noise into the encoder's interactions\nwith $\\mathcal{A}$, which is loosely motivated by techniques in differential\nprivacy. Our correctness analysis involves understanding the ability of\n$\\mathcal{A}$ to correctly answer adaptive queries which have positive but\nbounded mutual information with $\\mathcal{A}$'s internal randomness, and may be\nof independent interest in the newly emerging area of adaptive data analysis\nwith a theoretical computer science lens. \n\n"}
{"id": "1703.08509", "contents": "Title: Generalized Nash Equilibrium Problem by the Alternating Direction Method\n  of Multipliers Abstract: In this paper, the problem of finding a generalized Nash equilibrium (GNE) of\na networked game is studied. Players are only able to choose their decisions\nfrom a feasible action set. The feasible set is considered to be a private\nlinear equality constraint that is coupled through decisions of the other\nplayers. We consider that each player has his own private constraint and it has\nnot to be shared with the other players. This general case also embodies the\none with shared constraints between players and it can be also simply extended\nto the case with inequality constraints. Since the players don't have access to\nother players' actions, they need to exchange estimates of others' actions and\na local copy of the Lagrangian multiplier with their neighbors over a connected\ncommunication graph. We develop a relatively fast algorithm by reformulating\nthe conservative GNE problem within the framework of inexact-ADMM. The\nconvergence of the algorithm is guaranteed under a few mild assumptions on cost\nfunctions. Finally, the algorithm is simulated for a wireless ad-hoc network. \n\n"}
{"id": "1703.08750", "contents": "Title: Game-Theoretic Vaccination Against Networked SIS Epidemics and Impacts\n  of Human Decision-Making Abstract: We study decentralized protection strategies against\nSusceptible-Infected-Susceptible (SIS) epidemics on networks. We consider a\npopulation game framework where nodes choose whether or not to vaccinate\nthemselves, and the epidemic risk is defined as the infection probability at\nthe endemic state of the epidemic under a degree-based mean-field\napproximation. Motivated by studies in behavioral economics showing that humans\nperceive probabilities and risks in a nonlinear fashion, we specifically\nexamine the impacts of such misperceptions on the Nash equilibrium protection\nstrategies. We first establish the existence and uniqueness of a threshold\nequilibrium where nodes with degrees larger than a certain threshold vaccinate.\nWhen the vaccination cost is sufficiently high, we show that behavioral biases\ncause fewer players to vaccinate, and vice versa. We quantify this effect for a\nclass of networks with power-law degree distributions by proving tight bounds\non the ratio of equilibrium thresholds under behavioral and true perceptions of\nprobabilities. We further characterize the socially optimal vaccination policy\nand investigate the inefficiency of Nash equilibrium. \n\n"}
{"id": "1703.09177", "contents": "Title: Nash Equilibrium in Social Media Abstract: In this work, we investigate an application of a Nash equilibrium seeking\nalgorithm in a social network. In a networked game each player (user) takes\naction in response to other players' actions in order to decrease (increase)\nhis cost (profit) in the network. We assume that the players' cost functions\nare not necessarily dependent on the actions of all players. This is due to\nbetter mimicking the standard social media rules. A communication graph is\ndefined for the game through which players are able to share their information\nwith only their neighbors. We assume that the communication neighbors\nnecessarily affect the players' cost functions while the reverse is not always\ntrue. In this game, the players are only aware of their own cost functions and\nactions. Thus, each of them maintains an estimate of the others' actions and\nshare it with the neighbors to update his action and estimates. \n\n"}
{"id": "1703.09279", "contents": "Title: Online Market Intermediation Abstract: We study a dynamic market setting where an intermediary interacts with an\nunknown large sequence of agents that can be either sellers or buyers: their\nidentities, as well as the sequence length $n$, are decided in an adversarial,\nonline way. Each agent is interested in trading a single item, and all items in\nthe market are identical. The intermediary has some prior, incomplete knowledge\nof the agents' values for the items: all seller values are independently drawn\nfrom the same distribution $F_S$, and all buyer values from $F_B$. The two\ndistributions may differ, and we make standard regularity assumptions, namely\nthat $F_B$ is MHR and $F_S$ is log-concave.\n  We study online, posted-price mechanisms, and analyse two objectives: that of\nmaximizing the intermediary's profit and that of maximizing the social welfare,\nunder a competitive analysis benchmark. First, on the negative side, for\ngeneral agent sequences we prove tight competitive ratios of\n$\\varTheta(\\sqrt{n})$ and $\\varTheta(\\ln n)$, respectively for the two\nobjectives. On the other hand, under the extra assumption that the intermediary\nknows some bound $\\alpha$ on the ratio between the number of sellers and\nbuyers, we design asymptotically optimal online mechanisms with competitive\nratios of $1+o(1)$ and $4$, respectively. Additionally, we study the model were\nthe number of items that can be stored in stock throughout the execution is\nbounded, in which case the competitive ratio for the profit is improved to\n$O(\\ln n)$. \n\n"}
{"id": "1703.09471", "contents": "Title: Adversarial Image Perturbation for Privacy Protection -- A Game Theory\n  Perspective Abstract: Users like sharing personal photos with others through social media. At the\nsame time, they might want to make automatic identification in such photos\ndifficult or even impossible. Classic obfuscation methods such as blurring are\nnot only unpleasant but also not as effective as one would expect. Recent\nstudies on adversarial image perturbations (AIP) suggest that it is possible to\nconfuse recognition systems effectively without unpleasant artifacts. However,\nin the presence of counter measures against AIPs, it is unclear how effective\nAIP would be in particular when the choice of counter measure is unknown. Game\ntheory provides tools for studying the interaction between agents with\nuncertainties in the strategies. We introduce a general game theoretical\nframework for the user-recogniser dynamics, and present a case study that\ninvolves current state of the art AIP and person recognition techniques. We\nderive the optimal strategy for the user that assures an upper bound on the\nrecognition rate independent of the recogniser's counter measure. Code is\navailable at https://goo.gl/hgvbNK. \n\n"}
{"id": "1704.00633", "contents": "Title: Optimal lower bounds for universal relation, and for samplers and\n  finding duplicates in streams Abstract: In the communication problem $\\mathbf{UR}$ (universal relation) [KRW95],\nAlice and Bob respectively receive $x, y \\in\\{0,1\\}^n$ with the promise that\n$x\\neq y$. The last player to receive a message must output an index $i$ such\nthat $x_i\\neq y_i$. We prove that the randomized one-way communication\ncomplexity of this problem in the public coin model is exactly\n$\\Theta(\\min\\{n,\\log(1/\\delta)\\log^2(\\frac n{\\log(1/\\delta)})\\})$ for failure\nprobability $\\delta$. Our lower bound holds even if promised\n$\\mathop{support}(y)\\subset \\mathop{support}(x)$. As a corollary, we obtain\noptimal lower bounds for $\\ell_p$-sampling in strict turnstile streams for\n$0\\le p < 2$, as well as for the problem of finding duplicates in a stream. Our\nlower bounds do not need to use large weights, and hold even if promised\n$x\\in\\{0,1\\}^n$ at all points in the stream.\n  We give two different proofs of our main result. The first proof demonstrates\nthat any algorithm $\\mathcal A$ solving sampling problems in turnstile streams\nin low memory can be used to encode subsets of $[n]$ of certain sizes into a\nnumber of bits below the information theoretic minimum. Our encoder makes\nadaptive queries to $\\mathcal A$ throughout its execution, but done carefully\nso as to not violate correctness. This is accomplished by injecting random\nnoise into the encoder's interactions with $\\mathcal A$, which is loosely\nmotivated by techniques in differential privacy. Our second proof is via a\nnovel randomized reduction from Augmented Indexing [MNSW98] which needs to\ninteract with $\\mathcal A$ adaptively. To handle the adaptivity we identify\ncertain likely interaction patterns and union bound over them to guarantee\ncorrect interaction on all of them. To guarantee correctness, it is important\nthat the interaction hides some of its randomness from $\\mathcal A$ in the\nreduction. \n\n"}
{"id": "1704.00765", "contents": "Title: Quantum Algorithms for Graph Connectivity and Formula Evaluation Abstract: We give a new upper bound on the quantum query complexity of deciding\n$st$-connectivity on certain classes of planar graphs, and show the bound is\nsometimes exponentially better than previous results. We then show Boolean\nformula evaluation reduces to deciding connectivity on just such a class of\ngraphs. Applying the algorithm for $st$-connectivity to Boolean formula\nevaluation problems, we match the $O(\\sqrt{N})$ bound on the quantum query\ncomplexity of evaluating formulas on $N$ variables, give a quadratic speed-up\nover the classical query complexity of a certain class of promise Boolean\nformulas, and show this approach can yield superpolynomial quantum/classical\nseparations. These results indicate that this $st$-connectivity-based approach\nmay be the \"right\" way of looking at quantum algorithms for formula evaluation. \n\n"}
{"id": "1704.01195", "contents": "Title: Statistical Estimation with Strategic Data Sources in Competitive\n  Settings Abstract: In this paper, we introduce a preliminary model for interactions in the data\nmarket. Recent research has shown ways in which a data aggregator can design\nmechanisms for users to ensure the quality of data, even in situations where\nthe users are effort-averse (i.e. prefer to submit lower-quality estimates) and\nthe data aggregator cannot observe the effort exerted by the users (i.e. the\ncontract suffers from the principal-agent problem). However, we have shown that\nthese mechanisms often break down in more realistic models, where multiple data\naggregators are in competition. Under minor assumptions on the properties of\nthe statistical estimators in use by data aggregators, we show that there is\neither no Nash equilibrium, or there is an infinite number of Nash equilibrium.\nIn the latter case, there is a fundamental ambiguity in who bears the burden of\nincentivizing different data sources. We are also able to calculate the price\nof anarchy, which measures how much social welfare is lost between the Nash\nequilibrium and the social optimum, i.e. between non-cooperative strategic play\nand cooperation. \n\n"}
{"id": "1704.01983", "contents": "Title: A Characterization of Undirected Graphs Admitting Optimal Cost Shares Abstract: In a seminal paper, Chen, Roughgarden and Valiant studied cost sharing\nprotocols for network design with the objective to implement a low-cost Steiner\nforest as a Nash equilibrium of an induced cost-sharing game. One of the most\nintriguing open problems to date is to understand the power of budget-balanced\nand separable cost sharing protocols in order to induce low-cost Steiner\nforests. In this work, we focus on undirected networks and analyze topological\nproperties of the underlying graph so that an optimal Steiner forest can be\nimplemented as a Nash equilibrium (by some separable cost sharing protocol)\nindependent of the edge costs. We term a graph efficient if the above stated\nproperty holds. As our main result, we give a complete characterization of\nefficient undirected graphs for two-player network design games: an undirected\ngraph is efficient if and only if it does not contain (at least) one out of few\nforbidden subgraphs. Our characterization implies that several graph classes\nare efficient: generalized series-parallel graphs, fan and wheel graphs and\ngraphs with small cycles. \n\n"}
{"id": "1704.02438", "contents": "Title: Coordination game in bidirectional flow Abstract: We have introduced evolutionary game dynamics to a one-dimensional\ncellular-automaton to investigate evolution and maintenance of cooperative\navoiding behavior of self-driven particles in bidirectional flow. In our model,\nthere are two kinds of particles, which are right-going particles and\nleft-going particles. They often face opponent particles, so that they swerve\nto the right or left stochastically in order to avoid conflicts. The particles\nreinforce their preferences of the swerving direction after their successful\navoidance. The preference is also weakened by memory-loss effect.\n  Result of our simulation indicates that cooperative avoiding behavior is\nachieved, i.e., swerving directions of the particles are unified, when the\ndensity of particles is close to 1/2 and the memory-loss rate is small.\nFurthermore, when the right-going particles occupy the majority of the system,\nwe observe that their flow increases when the number of left-going particles,\nwhich prevent the smooth movement of right-going particles, becomes large. It\nis also investigated that the critical memory-loss rate of the cooperative\navoiding behavior strongly depends on the size of the system. Small system can\nprolong the cooperative avoiding behavior in wider range of memory-loss rate\nthan large system. \n\n"}
{"id": "1704.02453", "contents": "Title: Consistent Approval-Based Multi-Winner Rules Abstract: This paper is an axiomatic study of consistent approval-based multi-winner\nrules, i.e., voting rules that select a fixed-size group of candidates based on\napproval ballots. We introduce the class of counting rules and provide an\naxiomatic characterization of this class based on the consistency axiom.\nBuilding upon this result, we axiomatically characterize three important\nconsistent multi-winner rules: Proportional Approval Voting, Multi-Winner\nApproval Voting and the Approval Chamberlin--Courant rule. Our results\ndemonstrate the variety of multi-winner rules and illustrate three different,\northogonal principles that multi-winner voting rules may represent: individual\nexcellence, diversity, and proportionality. \n\n"}
{"id": "1704.02657", "contents": "Title: Solving Zero-sum Games using Best Response Oracles with Applications to\n  Search Games Abstract: We present efficient algorithms for computing optimal or approximately\noptimal strategies in a zero-sum game for which Player I has n pure strategies\nand Player II has an arbitrary number of pure strategies. We assume that for\nany given mixed strategy of Player I, a best response or \"approximate\" best\nresponse of Player II can be found by an oracle in time polynomial in n. We\nthen show how our algorithms may be applied to several search games with\napplications to security and counter-terrorism. We evaluate our main algorithm\nexperimentally on a prototypical search game. Our results show it performs well\ncompared to an existing, well-known algorithm for solving zero-sum games that\ncan also be used to solve search games, given a best response oracle. \n\n"}
{"id": "1704.02958", "contents": "Title: On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel\n  Methods and Neural Networks Abstract: Empirical risk minimization (ERM) is ubiquitous in machine learning and\nunderlies most supervised learning methods. While there has been a large body\nof work on algorithms for various ERM problems, the exact computational\ncomplexity of ERM is still not understood. We address this issue for multiple\npopular ERM problems including kernel SVMs, kernel ridge regression, and\ntraining the final layer of a neural network. In particular, we give\nconditional hardness results for these problems based on complexity-theoretic\nassumptions such as the Strong Exponential Time Hypothesis. Under these\nassumptions, we show that there are no algorithms that solve the aforementioned\nERM problems to high accuracy in sub-quadratic time. We also give similar\nhardness results for computing the gradient of the empirical loss, which is the\nmain computational burden in many non-convex learning tasks. \n\n"}
{"id": "1704.03864", "contents": "Title: A Matrix Expander Chernoff Bound Abstract: We prove a Chernoff-type bound for sums of matrix-valued random variables\nsampled via a random walk on an expander, confirming a conjecture due to\nWigderson and Xiao. Our proof is based on a new multi-matrix extension of the\nGolden-Thompson inequality which improves in some ways the inequality of\nSutter, Berta, and Tomamichel, and may be of independent interest, as well as\nan adaptation of an argument for the scalar case due to Healy. Secondarily, we\nalso provide a generic reduction showing that any concentration inequality for\nvector-valued martingales implies a concentration inequality for the\ncorresponding expander walk, with a weakening of parameters proportional to the\nsquared mixing time. \n\n"}
{"id": "1704.04546", "contents": "Title: SETH-Based Lower Bounds for Subset Sum and Bicriteria Path Abstract: Subset-Sum and k-SAT are two of the most extensively studied problems in\ncomputer science, and conjectures about their hardness are among the\ncornerstones of fine-grained complexity. One of the most intriguing open\nproblems in this area is to base the hardness of one of these problems on the\nother.\n  Our main result is a tight reduction from k-SAT to Subset-Sum on dense\ninstances, proving that Bellman's 1962 pseudo-polynomial $O^{*}(T)$-time\nalgorithm for Subset-Sum on $n$ numbers and target $T$ cannot be improved to\ntime $T^{1-\\varepsilon}\\cdot 2^{o(n)}$ for any $\\varepsilon>0$, unless the\nStrong Exponential Time Hypothesis (SETH) fails. This is one of the strongest\nknown connections between any two of the core problems of fine-grained\ncomplexity.\n  As a corollary, we prove a \"Direct-OR\" theorem for Subset-Sum under SETH,\noffering a new tool for proving conditional lower bounds: It is now possible to\nassume that deciding whether one out of $N$ given instances of Subset-Sum is a\nYES instance requires time $(N T)^{1-o(1)}$. As an application of this\ncorollary, we prove a tight SETH-based lower bound for the classical Bicriteria\ns,t-Path problem, which is extensively studied in Operations Research. We\nseparate its complexity from that of Subset-Sum: On graphs with $m$ edges and\nedge lengths bounded by $L$, we show that the $O(Lm)$ pseudo-polynomial time\nalgorithm by Joksch from 1966 cannot be improved to $\\tilde{O}(L+m)$, in\ncontrast to a recent improvement for Subset Sum (Bringmann, SODA 2017). \n\n"}
{"id": "1704.04720", "contents": "Title: Understanding Norm Change: An Evolutionary Game-Theoretic Approach\n  (Extended Version) Abstract: Human societies around the world interact with each other by developing and\nmaintaining social norms, and it is critically important to understand how such\nnorms emerge and change. In this work, we define an evolutionary game-theoretic\nmodel to study how norms change in a society, based on the idea that different\nstrength of norms in societies translate to different game-theoretic\ninteraction structures and incentives. We use this model to study, both\nanalytically and with extensive agent-based simulations, the evolutionary\nrelationships of the need for coordination in a society (which is related to\nits norm strength) with two key aspects of norm change: cultural inertia\n(whether or how quickly the population responds when faced with conditions that\nmake a norm change desirable), and exploration rate (the willingness of agents\nto try out new strategies). Our results show that a high need for coordination\nleads to both high cultural inertia and a low exploration rate, while a low\nneed for coordination leads to low cultural inertia and high exploration rate.\nThis is the first work, to our knowledge, on understanding the evolutionary\ncausal relationships among these factors. \n\n"}
{"id": "1704.04888", "contents": "Title: Envy-free Matchings with Lower Quotas Abstract: While every instance of the Hospitals/Residents problem admits a stable\nmatching, the problem with lower quotas (HR-LQ) has instances with no stable\nmatching. For such an instance, we expect the existence of an envy-free\nmatching, which is a relaxation of a stable matching preserving a kind of\nfairness property. In this paper, we investigate the existence of an envy-free\nmatching in several settings, in which hospitals have lower quotas and not all\ndoctor-hospital pairs are acceptable. We first show that, for an HR-LQ\ninstance, we can efficiently decide the existence of an envy-free matching.\nThen, we consider envy-freeness in the Classified Stable Matching model due to\nHuang (2010), i.e., each hospital has lower and upper quotas on subsets of\ndoctors. We show that, for this model, deciding the existence of an envy-free\nmatching is NP-hard in general, but solvable in polynomial time if quotas are\nparamodular. \n\n"}
{"id": "1704.05027", "contents": "Title: Optimal Multi-Unit Mechanisms with Private Demands Abstract: In the multi-unit pricing problem, multiple units of a single item are for\nsale. A buyer's valuation for $n$ units of the item is $v \\min \\{ n, d\\} $,\nwhere the per unit valuation $v$ and the capacity $d$ are private information\nof the buyer. We consider this problem in the Bayesian setting, where the pair\n$(v,d)$ is drawn jointly from a given probability distribution. In the\n\\emph{unlimited supply} setting, the optimal (revenue maximizing) mechanism is\na pricing problem, i.e., it is a menu of lotteries. In this paper we show that\nunder a natural regularity condition on the probability distributions, which we\ncall \\emph{decreasing marginal revenue}, the optimal pricing is in fact\n\\emph{deterministic}. It is a price curve, offering $i$ units of the item for a\nprice of $p_i$, for every integer $i$. Further, we show that the revenue as a\nfunction of the prices $p_i$ is a \\emph{concave} function, which implies that\nthe optimum price curve can be found in polynomial time. This gives a rare\nexample of a natural multi-parameter setting where we can show such a clean\ncharacterization of the optimal mechanism. We also give a more detailed\ncharacterization of the optimal prices for the case where there are only two\npossible demands. \n\n"}
{"id": "1704.05303", "contents": "Title: The Robot Routing Problem for Collecting Aggregate Stochastic Rewards Abstract: We propose a new model for formalizing reward collection problems on graphs\nwith dynamically generated rewards which may appear and disappear based on a\nstochastic model. The *robot routing problem* is modeled as a graph whose nodes\nare stochastic processes generating potential rewards over discrete time. The\nrewards are generated according to the stochastic process, but at each step, an\nexisting reward disappears with a given probability. The edges in the graph\nencode the (unit-distance) paths between the rewards' locations. On visiting a\nnode, the robot collects the accumulated reward at the node at that time, but\ntraveling between the nodes takes time. The optimization question asks to\ncompute an optimal (or epsilon-optimal) path that maximizes the expected\ncollected rewards.\n  We consider the finite and infinite-horizon robot routing problems. For\nfinite-horizon, the goal is to maximize the total expected reward, while for\ninfinite horizon we consider limit-average objectives. We study the\ncomputational and strategy complexity of these problems, establish NP-lower\nbounds and show that optimal strategies require memory in general. We also\nprovide an algorithm for computing epsilon-optimal infinite paths for arbitrary\nepsilon > 0. \n\n"}
{"id": "1704.06297", "contents": "Title: A Time Hierarchy Theorem for the LOCAL Model Abstract: The celebrated Time Hierarchy Theorem for Turing machines states, informally,\nthat more problems can be solved given more time. The extent to which a time\nhierarchy-type theorem holds in the distributed LOCAL model has been open for\nmany years. It is consistent with previous results that all natural problems in\nthe LOCAL model can be classified according to a small constant number of\ncomplexities, such as $O(1),O(\\log^* n), O(\\log n), 2^{O(\\sqrt{\\log n})}$, etc.\n  In this paper we establish the first time hierarchy theorem for the LOCAL\nmodel and prove that several gaps exist in the LOCAL time hierarchy.\n  1. We define an infinite set of simple coloring problems called Hierarchical\n$2\\frac{1}{2}$-Coloring}. A correctly colored graph can be confirmed by simply\nchecking the neighborhood of each vertex, so this problem fits into the class\nof locally checkable labeling (LCL) problems. However, the complexity of the\n$k$-level Hierarchical $2\\frac{1}{2}$-Coloring problem is $\\Theta(n^{1/k})$,\nfor $k\\in\\mathbb{Z}^+$. The upper and lower bounds hold for both general graphs\nand trees, and for both randomized and deterministic algorithms.\n  2. Consider any LCL problem on bounded degree trees. We prove an\nautomatic-speedup theorem that states that any randomized $n^{o(1)}$-time\nalgorithm solving the LCL can be transformed into a deterministic $O(\\log\nn)$-time algorithm. Together with a previous result, this establishes that on\ntrees, there are no natural deterministic complexities in the ranges\n$\\omega(\\log^* n)$---$o(\\log n)$ or $\\omega(\\log n)$---$n^{o(1)}$.\n  3. We expose a gap in the randomized time hierarchy on general graphs. Any\nrandomized algorithm that solves an LCL problem in sublogarithmic time can be\nsped up to run in $O(T_{LLL})$ time, which is the complexity of the distributed\nLovasz local lemma problem, currently known to be $\\Omega(\\log\\log n)$ and\n$O(\\log n)$. \n\n"}
{"id": "1704.08868", "contents": "Title: Structural Parameters, Tight Bounds, and Approximation for (k,r)-Center Abstract: In $(k,r)$-Center we are given a (possibly edge-weighted) graph and are asked\nto select at most $k$ vertices (centers), so that all other vertices are at\ndistance at most $r$ from a center. In this paper we provide a number of tight\nfine-grained bounds on the complexity of this problem with respect to various\nstandard graph parameters. Specifically:\n  - For any $r\\ge 1$, we show an algorithm that solves the problem in\n$O^*((3r+1)^{\\textrm{cw}})$ time, where $\\textrm{cw}$ is the clique-width of\nthe input graph, as well as a tight SETH lower bound matching this algorithm's\nperformance. As a corollary, for $r=1$, this closes the gap that previously\nexisted on the complexity of Dominating Set parameterized by $\\textrm{cw}$.\n  - We strengthen previously known FPT lower bounds, by showing that\n$(k,r)$-Center is W[1]-hard parameterized by the input graph's vertex cover (if\nedge weights are allowed), or feedback vertex set, even if $k$ is an additional\nparameter. Our reductions imply tight ETH-based lower bounds. Finally, we\ndevise an algorithm parameterized by vertex cover for unweighted graphs.\n  - We show that the complexity of the problem parameterized by tree-depth is\n$2^{\\Theta(\\textrm{td}^2)}$ by showing an algorithm of this complexity and a\ntight ETH-based lower bound.\n  We complement these mostly negative results by providing FPT approximation\nschemes parameterized by clique-width or treewidth which work efficiently\nindependently of the values of $k,r$. In particular, we give algorithms which,\nfor any $\\epsilon>0$, run in time\n$O^*((\\textrm{tw}/\\epsilon)^{O(\\textrm{tw})})$,\n$O^*((\\textrm{cw}/\\epsilon)^{O(\\textrm{cw})})$ and return a\n$(k,(1+\\epsilon)r)$-center, if a $(k,r)$-center exists, thus circumventing the\nproblem's W-hardness. \n\n"}
{"id": "1705.01644", "contents": "Title: Combinatorial Auctions Do Need Modest Interaction Abstract: We study the necessity of interaction for obtaining efficient allocations in\nsubadditive combinatorial auctions. This problem was originally introduced by\nDobzinski, Nisan, and Oren (STOC'14) as the following simple market scenario:\n$m$ items are to be allocated among $n$ bidders in a distributed setting where\nbidders valuations are private and hence communication is needed to obtain an\nefficient allocation. The communication happens in rounds: in each round, each\nbidder, simultaneously with others, broadcasts a message to all parties\ninvolved and the central planner computes an allocation solely based on the\ncommunicated messages. Dobzinski et.al. showed that no non-interactive\n($1$-round) protocol with polynomial communication (in the number of items and\nbidders) can achieve approximation ratio better than $\\Omega(m^{{1}/{4}})$,\nwhile for any $r \\geq 1$, there exists $r$-round protocols that achieve\n$\\widetilde{O}(r \\cdot m^{{1}/{r+1}})$ approximation with polynomial\ncommunication; in particular, $O(\\log{m})$ rounds of interaction suffice to\nobtain an (almost) efficient allocation.\n  A natural question at this point is to identify the \"right\" level of\ninteraction (i.e., number of rounds) necessary to obtain an efficient\nallocation. In this paper, we resolve this question by providing an almost\ntight round-approximation tradeoff for this problem: we show that for any $r\n\\geq 1$, any $r$-round protocol that uses polynomial communication can only\napproximate the social welfare up to a factor of $\\Omega(\\frac{1}{r} \\cdot\nm^{{1}/{2r+1}})$. This in particular implies that\n$\\Omega(\\frac{\\log{m}}{\\log\\log{m}})$ rounds of interaction are necessary for\nobtaining any efficient allocation in these markets. Our work builds on the\nrecent multi-party round-elimination technique of Alon, Nisan, Raz, and\nWeinstein (FOCS'15) and settles an open question posed by Dobzinski et.al. and\nAlon et. al. \n\n"}
{"id": "1705.01736", "contents": "Title: Of the People: Voting Is More Effective with Representative Candidates Abstract: In light of the classic impossibility results of Arrow and Gibbard and\nSatterthwaite regarding voting with ordinal rules, there has been recent\ninterest in characterizing how well common voting rules approximate the social\noptimum. In order to quantify the quality of approximation, it is natural to\nconsider the candidates and voters as embedded within a common metric space,\nand to ask how much further the chosen candidate is from the population as\ncompared to the socially optimal one. We use this metric preference model to\nexplore a fundamental and timely question: does the social welfare of a\npopulation improve when candidates are representative of the population? If so,\nthen by how much, and how does the answer depend on the complexity of the\nmetric space?\n  We restrict attention to the most fundamental and common social choice\nsetting: a population of voters, two independently drawn candidates, and a\nmajority rule election. When candidates are not representative of the\npopulation, it is known that the candidate selected by the majority rule can be\nthrice as far from the population as the socially optimal one. We examine how\nthis ratio improves when candidates are drawn independently from the population\nof voters. Our results are two-fold: When the metric is a line, the ratio\nimproves from $3$ to $4-2\\sqrt{2}$, roughly $1.1716$; this bound is tight. When\nthe metric is arbitrary, we show a lower bound of $1.5$ and a constant upper\nbound strictly better than $2$ on the approximation ratio of the majority rule.\n  The positive result depends in part on the assumption that candidates are\nindependent and identically distributed. However, we show that independence\nalone is not enough to achieve the upper bound: even when candidates are drawn\nindependently, if the population of candidates can be different from the\nvoters, then an upper bound of $2$ on the approximation is tight. \n\n"}
{"id": "1705.01821", "contents": "Title: On Optimal Mechanisms in the Two-Item Single-Buyer Unit-Demand Setting Abstract: We consider the problem of designing a revenue-optimal mechanism in the\ntwo-item, single-buyer, unit-demand setting when the buyer's valuations, $(z_1,\nz_2)$, are uniformly distributed in an arbitrary rectangle\n$[c,c+b_1]\\times[c,c+b_2]$ in the positive quadrant. We provide a complete and\nexplicit solution for arbitrary nonnegative values of $(c,b_1,b_2)$. We\nidentify five simple structures, each with at most five (possibly stochastic)\nmenu items, and prove that the optimal mechanism has one of the five\nstructures. We also characterize the optimal mechanism as a function of $b_1,\nb_2$, and $c$. When $c$ is low, the optimal mechanism is a posted price\nmechanism with an exclusion region; when $c$ is high, it is a posted price\nmechanism without an exclusion region. Our results are the first to show the\nexistence of optimal mechanisms with no exclusion region, to the best of our\nknowledge. \n\n"}
{"id": "1705.01843", "contents": "Title: Quantum SDP-Solvers: Better upper and lower bounds Abstract: Brand\\~ao and Svore very recently gave quantum algorithms for approximately\nsolving semidefinite programs, which in some regimes are faster than the\nbest-possible classical algorithms in terms of the dimension $n$ of the problem\nand the number $m$ of constraints, but worse in terms of various other\nparameters. In this paper we improve their algorithms in several ways, getting\nbetter dependence on those other parameters. To this end we develop new\ntechniques for quantum algorithms, for instance a general way to efficiently\nimplement smooth functions of sparse Hamiltonians, and a generalized\nminimum-finding procedure.\n  We also show limits on this approach to quantum SDP-solvers, for instance for\ncombinatorial optimizations problems that have a lot of symmetry. Finally, we\nprove some general lower bounds showing that in the worst case, the complexity\nof every quantum LP-solver (and hence also SDP-solver) has to scale linearly\nwith $mn$ when $m\\approx n$, which is the same as classical. \n\n"}
{"id": "1705.02266", "contents": "Title: Computing Constrained Approximate Equilibria in Polymatrix Games Abstract: This paper is about computing constrained approximate Nash equilibria in\npolymatrix games, which are succinctly represented many-player games defined by\nan interaction graph between the players. In a recent breakthrough, Rubinstein\nshowed that there exists a small constant $\\epsilon$, such that it is\nPPAD-complete to find an (unconstrained) $\\epsilon$-Nash equilibrium of a\npolymatrix game. In the first part of the paper, we show that is NP-hard to\ndecide if a polymatrix game has a constrained approximate equilibrium for 9\nnatural constraints and any non-trivial approximation guarantee. These results\nhold even for planar bipartite polymatrix games with degree 3 and at most 7\nstrategies per player, and all non-trivial approximation guarantees. These\nresults stand in contrast to similar results for bimatrix games, which\nobviously need a non-constant number of actions, and which rely on stronger\ncomplexity-theoretic conjectures such as the exponential time hypothesis. In\nthe second part, we provide a deterministic QPTAS for interaction graphs with\nbounded treewidth and with logarithmically many actions per player that can\ncompute constrained approximate equilibria for a wide family of constraints\nthat cover many of the constraints dealt with in the first part. \n\n"}
{"id": "1705.02944", "contents": "Title: Hardness Results for Structured Linear Systems Abstract: We show that if the nearly-linear time solvers for Laplacian matrices and\ntheir generalizations can be extended to solve just slightly larger families of\nlinear systems, then they can be used to quickly solve all systems of linear\nequations over the reals. This result can be viewed either positively or\nnegatively: either we will develop nearly-linear time algorithms for solving\nall systems of linear equations over the reals, or progress on the families we\ncan solve in nearly-linear time will soon halt. \n\n"}
{"id": "1705.02955", "contents": "Title: Safe and Nested Subgame Solving for Imperfect-Information Games Abstract: In imperfect-information games, the optimal strategy in a subgame may depend\non the strategy in other, unreached subgames. Thus a subgame cannot be solved\nin isolation and must instead consider the strategy for the entire game as a\nwhole, unlike perfect-information games. Nevertheless, it is possible to first\napproximate a solution for the whole game and then improve it by solving\nindividual subgames. This is referred to as subgame solving. We introduce\nsubgame-solving techniques that outperform prior methods both in theory and\npractice. We also show how to adapt them, and past subgame-solving techniques,\nto respond to opponent actions that are outside the original action\nabstraction; this significantly outperforms the prior state-of-the-art\napproach, action translation. Finally, we show that subgame solving can be\nrepeated as the game progresses down the game tree, leading to far lower\nexploitability. These techniques were a key component of Libratus, the first AI\nto defeat top humans in heads-up no-limit Texas hold'em poker. \n\n"}
{"id": "1705.03283", "contents": "Title: An exponential lower bound for Individualization-Refinement algorithms\n  for Graph Isomorphism Abstract: The individualization-refinement paradigm provides a strong toolbox for\ntesting isomorphism of two graphs and indeed, the currently fastest\nimplementations of isomorphism solvers all follow this approach. While these\nsolvers are fast in practice, from a theoretical point of view, no general\nlower bounds concerning the worst case complexity of these tools are known. In\nfact, it is an open question whether individualization-refinement algorithms\ncan achieve upper bounds on the running time similar to the more theoretical\ntechniques based on a group theoretic approach.\n  In this work we give a negative answer to this question and construct a\nfamily of graphs on which algorithms based on the individualization-refinement\nparadigm require exponential time. Contrary to a previous construction of\nMiyazaki, that only applies to a specific implementation within the\nindividualization-refinement framework, our construction is immune to changing\nthe cell selector, or adding various heuristic invariants to the algorithm.\nFurthermore, our graphs also provide exponential lower bounds in the case when\nthe $k$-dimensional Weisfeiler-Leman algorithm is used to replace the standard\ncolor refinement operator and the arguments even work when the entire\nautomorphism group of the inputs is initially provided to the algorithm. \n\n"}
{"id": "1705.03501", "contents": "Title: Socially Trusted Collaborative Edge Computing in Ultra Dense Networks Abstract: Small cell base stations (SBSs) endowed with cloud-like computing\ncapabilities are considered as a key enabler of edge computing (EC), which\nprovides ultra-low latency and location-awareness for a variety of emerging\nmobile applications and the Internet of Things. However, due to the limited\ncomputation resources of an individual SBS, providing computation services of\nhigh quality to its users faces significant challenges when it is overloaded\nwith an excessive amount of computation workload. In this paper, we propose\ncollaborative edge computing among SBSs by forming SBS coalitions to share\ncomputation resources with each other, thereby accommodating more computation\nworkload in the edge system and reducing reliance on the remote cloud. A novel\nSBS coalition formation algorithm is developed based on the coalitional game\ntheory to cope with various new challenges in small-cell-based edge systems,\nincluding the co-provisioning of radio access and computing services,\ncooperation incentives, and potential security risks. To address these\nchallenges, the proposed method (1) allows collaboration at both the user-SBS\nassociation stage and the SBS peer offloading stage by exploiting the ultra\ndense deployment of SBSs, (2) develops a payment-based incentive mechanism that\nimplements proportionally fair utility division to form stable SBS coalitions,\nand (3) builds a social trust network for managing security risks among SBSs\ndue to collaboration. Systematic simulations in practical scenarios are carried\nout to evaluate the efficacy and performance of the proposed method, which\nshows that tremendous edge computing performance improvement can be achieved. \n\n"}
{"id": "1705.04212", "contents": "Title: Competitive Equilibrium For Almost All Incomes: Existence and Fairness Abstract: Competitive equilibrium (CE) is a fundamental concept in market economics.\nIts efficiency and fairness properties make it particularly appealing as a rule\nfor fair allocation of resources among agents with possibly different\nentitlements. However, when the resources are indivisible, a CE might not exist\neven when there is one resource and two agents with equal incomes. Recently,\nBabaioff and Nisan and Talgam-Cohen (2017) have suggested to consider the\nentire space of possible incomes, and check whether there exists a competitive\nequilibrium for almost all income-vectors --- all income-space except a subset\nof measure zero. They proved various existence and non-existence results, but\nleft open the cases of four goods and three or four agents with\nmonotonically-increasing preferences.\n  This paper proves non-existence in both these cases, thus completing the\ncharacterization of CE existence for almost all incomes in the domain of\nmonotonically increasing preferences. Additionally, the paper provides a\ncomplete characterization of CE existence in the domain of monotonically\ndecreasing preferences, corresponding to allocation of chores.\n  On the positive side, the paper proves that CE exists for almost all incomes\nwhen there are four goods and three agents with additive preferences. The proof\nuses a new tool for describing a CE, as a subgame-perfect equilibrium of a\nspecific sequential game. The same tool also enables substantially simpler\nproofs to the cases already proved by Babaioff et al.\n  Additionally, this paper proves several strong fairness properties that are\nsatisfied by any CE allocation, illustrating its usefulness for fair allocation\namong agents with different entitlements. \n\n"}
{"id": "1705.05030", "contents": "Title: Information Leakage Games Abstract: We consider a game-theoretic setting to model the interplay between attacker\nand defender in the context of information flow, and to reason about their\noptimal strategies. In contrast with standard game theory, in our games the\nutility of a mixed strategy is a convex function of the distribution on the\ndefender's pure actions, rather than the expected value of their utilities.\nNevertheless, the important properties of game theory, notably the existence of\na Nash equilibrium, still hold for our (zero-sum) leakage games, and we provide\nalgorithms to compute the corresponding optimal strategies. As typical in\n(simultaneous) game theory, the optimal strategy is usually mixed, i.e.,\nprobabilistic, for both the attacker and the defender. From the point of view\nof information flow, this was to be expected in the case of the defender, since\nit is well known that randomization at the level of the system design may help\nto reduce information leaks. Regarding the attacker, however, this seems the\nfirst work (w.r.t. the literature in information flow) proving formally that in\ncertain cases the optimal attack strategy is necessarily probabilistic. \n\n"}
{"id": "1705.06840", "contents": "Title: The Conference Paper Assignment Problem: Using Order Weighted Averages\n  to Assign Indivisible Goods Abstract: Motivated by the common academic problem of allocating papers to referees for\nconference reviewing we propose a novel mechanism for solving the assignment\nproblem when we have a two sided matching problem with preferences from one\nside (the agents/reviewers) over the other side (the objects/papers) and both\nsides have capacity constraints. The assignment problem is a fundamental\nproblem in both computer science and economics with application in many areas\nincluding task and resource allocation. We draw inspiration from multi-criteria\ndecision making and voting and use order weighted averages (OWAs) to propose a\nnovel and flexible class of algorithms for the assignment problem. We show an\nalgorithm for finding a $\\Sigma$-OWA assignment in polynomial time, in contrast\nto the NP-hardness of finding an egalitarian assignment. Inspired by this\nsetting we observe an interesting connection between our model and the classic\nproportional multi-winner election problem in social choice. \n\n"}
{"id": "1705.07215", "contents": "Title: On Convergence and Stability of GANs Abstract: We propose studying GAN training dynamics as regret minimization, which is in\ncontrast to the popular view that there is consistent minimization of a\ndivergence between real and generated distributions. We analyze the convergence\nof GAN training from this new point of view to understand why mode collapse\nhappens. We hypothesize the existence of undesirable local equilibria in this\nnon-convex game to be responsible for mode collapse. We observe that these\nlocal equilibria often exhibit sharp gradients of the discriminator function\naround some real data points. We demonstrate that these degenerate local\nequilibria can be avoided with a gradient penalty scheme called DRAGAN. We show\nthat DRAGAN enables faster training, achieves improved stability with fewer\nmode collapses, and leads to generator networks with better modeling\nperformance across a variety of architectures and objective functions. \n\n"}
{"id": "1705.07993", "contents": "Title: Fair Allocation based on Diminishing Differences Abstract: Ranking alternatives is a natural way for humans to explain their\npreferences. It is being used in many settings, such as school choice, course\nallocations and residency matches. In some cases, several `items' are given to\neach participant. Without having any information on the underlying cardinal\nutilities, arguing about fairness of allocation requires extending the ordinal\nitem ranking to ordinal bundle ranking. The most commonly used such extension\nis stochastic dominance (SD), where a bundle X is preferred over a bundle Y if\nits score is better according to all additive score functions. SD is a very\nconservative extension, by which few allocations are necessarily fair while\nmany allocations are possibly fair. We propose to make a natural assumption on\nthe underlying cardinal utilities of the players, namely that the difference\nbetween two items at the top is larger than the difference between two items at\nthe bottom. This assumption implies a preference extension which we call\ndiminishing differences (DD), where X is preferred over Y if its score is\nbetter according to all additive score functions satisfying the DD assumption.\nWe give a full characterization of allocations that are\nnecessarily-proportional or possibly-proportional according to this assumption.\nBased on this characterization, we present a polynomial-time algorithm for\nfinding a necessarily-DD-proportional allocation if it exists. Using\nsimulations, we show that with high probability, a necessarily-proportional\nallocation does not exist but a necessarily-DD-proportional allocation exists,\nand moreover, that allocation is proportional according to the underlying\ncardinal utilities. We also consider chore allocation under the analogous\ncondition --- increasing-differences. \n\n"}
{"id": "1705.08033", "contents": "Title: Social Integration in Two-Sided Matching Markets Abstract: When several two-sided matching markets merge into one, it is inevitable that\nsome agents will become worse off if the matching mechanism used is stable. I\nformalize this observation by defining the property of integration\nmonotonicity, which requires that every agent becomes better off after any\nnumber of matching markets merge. Integration monotonicity is also incompatible\nwith the weaker efficiency property of Pareto optimality.\n  Nevertheless, I obtain two possibility results. First, stable matching\nmechanisms never hurt more than one-half of the society after the integration\nof several matching markets occurs. Second, in random matching markets there\nare positive expected gains from integration for both sides of the market,\nwhich I quantify. \n\n"}
{"id": "1705.08430", "contents": "Title: Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues Abstract: In this work we derive a variant of the classic Glivenko-Cantelli Theorem,\nwhich asserts uniform convergence of the empirical Cumulative Distribution\nFunction (CDF) to the CDF of the underlying distribution. Our variant allows\nfor tighter convergence bounds for extreme values of the CDF.\n  We apply our bound in the context of revenue learning, which is a\nwell-studied problem in economics and algorithmic game theory. We derive\nsample-complexity bounds on the uniform convergence rate of the empirical\nrevenues to the true revenues, assuming a bound on the $k$th moment of the\nvaluations, for any (possibly fractional) $k>1$.\n  For uniform convergence in the limit, we give a complete characterization and\na zero-one law: if the first moment of the valuations is finite, then uniform\nconvergence almost surely occurs; conversely, if the first moment is infinite,\nthen uniform convergence almost never occurs. \n\n"}
{"id": "1705.08563", "contents": "Title: Simple Pricing Schemes for the Cloud Abstract: The problem of pricing the cloud has attracted much recent attention due to\nthe widespread use of cloud computing and cloud services. From a theoretical\nperspective, several mechanisms that provide strong efficiency or fairness\nguarantees and desirable incentive properties have been designed. However,\nthese mechanisms often rely on a rigid model, with several parameters needing\nto be precisely known in order for the guarantees to hold. In this paper, we\nconsider a stochastic model and show that it is possible to obtain good welfare\nand revenue guarantees with simple mechanisms that do not make use of the\ninformation on some of these parameters. In particular, we prove that a\nmechanism that sets the same price per time step for jobs of any length\nachieves at least 50% of the welfare and revenue obtained by a mechanism that\ncan set different prices for jobs of different lengths, and the ratio can be\nimproved if we have more specific knowledge of some parameters. Similarly, a\nmechanism that sets the same price for all servers even though the servers may\nreceive different kinds of jobs can provide a reasonable welfare and revenue\napproximation compared to a mechanism that is allowed to set different prices\nfor different servers. \n\n"}
{"id": "1705.09566", "contents": "Title: Rational Fair Consensus in the GOSSIP Model Abstract: The \\emph{rational fair consensus problem} can be informally defined as\nfollows. Consider a network of $n$ (selfish) \\emph{rational agents}, each of\nthem initially supporting a \\emph{color} chosen from a finite set $ \\Sigma$.\nThe goal is to design a protocol that leads the network to a stable\nmonochromatic configuration (i.e. a consensus) such that the probability that\nthe winning color is $c$ is equal to the fraction of the agents that initially\nsupport $c$, for any $c \\in \\Sigma$. Furthermore, this fairness property must\nbe guaranteed (with high probability) even in presence of any fixed\n\\emph{coalition} of rational agents that may deviate from the protocol in order\nto increase the winning probability of their supported colors. A protocol\nhaving this property, in presence of coalitions of size at most $t$, is said to\nbe a \\emph{whp\\,-$t$-strong equilibrium}. We investigate, for the first time,\nthe rational fair consensus problem in the GOSSIP communication model where, at\nevery round, every agent can actively contact at most one neighbor via a\n\\emph{push$/$pull} operation. We provide a randomized GOSSIP protocol that,\nstarting from any initial color configuration of the complete graph, achieves\nrational fair consensus within $O(\\log n)$ rounds using messages of\n$O(\\log^2n)$ size, w.h.p. More in details, we prove that our protocol is a\nwhp\\,-$t$-strong equilibrium for any $t = o(n/\\log n)$ and, moreover, it\ntolerates worst-case permanent faults provided that the number of non-faulty\nagents is $\\Omega(n)$. As far as we know, our protocol is the first solution\nwhich avoids any all-to-all communication, thus resulting in $o(n^2)$ message\ncomplexity. \n\n"}
{"id": "1705.09700", "contents": "Title: Multi-scale Online Learning and its Applications to Online Auctions Abstract: We consider revenue maximization in online auction/pricing problems. A seller\nsells an identical item in each period to a new buyer, or a new set of buyers.\nFor the online posted pricing problem, we show regret bounds that scale with\nthe best fixed price, rather than the range of the values. We also show regret\nbounds that are almost scale free, and match the offline sample complexity,\nwhen comparing to a benchmark that requires a lower bound on the market share.\nThese results are obtained by generalizing the classical learning from experts\nand multi-armed bandit problems to their multi-scale versions. In this version,\nthe reward of each action is in a different range, and the regret w.r.t. a\ngiven action scales with its own range, rather than the maximum range. \n\n"}
{"id": "1706.00652", "contents": "Title: Computer aided synthesis: a game theoretic approach Abstract: In this invited contribution, we propose a comprehensive introduction to game\ntheory applied in computer aided synthesis. In this context, we give some\nclassical results on two-player zero-sum games and then on multi-player non\nzero-sum games. The simple case of one-player games is strongly related to\nautomata theory on infinite words. All along the article, we focus on general\napproaches to solve the studied problems, and we provide several illustrative\nexamples as well as intuitions on the proofs. \n\n"}
{"id": "1706.00849", "contents": "Title: A Game of Nontransitive Dice Abstract: We consider a two player simultaneous-move game where the two players each\nselect any permissible $n$-sided die for a fixed integer $n$. A player wins if\nthe outcome of his roll is greater than that of his opponent. Remarkably, for\n$n>3$, there is a unique Nash Equilibrium in pure strategies. The unique Nash\nEquilibrium is for each player to throw the Standard $n$-sided die, where each\nside has a different number. Our proof of uniqueness is constructive. We\nintroduce an algorithm with which, for any nonstandard die, we may generate\nanother die that beats it. \n\n"}
{"id": "1706.02205", "contents": "Title: Compression, inversion, and approximate PCA of dense kernel matrices at\n  near-linear computational complexity Abstract: Dense kernel matrices $\\Theta \\in \\mathbb{R}^{N \\times N}$ obtained from\npoint evaluations of a covariance function $G$ at locations $\\{ x_{i} \\}_{1\n\\leq i \\leq N} \\subset \\mathbb{R}^{d}$ arise in statistics, machine learning,\nand numerical analysis. For covariance functions that are Green's functions of\nelliptic boundary value problems and homogeneously-distributed sampling points,\nwe show how to identify a subset $S \\subset \\{ 1 , \\dots , N \\}^2$, with $\\# S\n= O ( N \\log (N) \\log^{d} ( N /\\epsilon ) )$, such that the zero fill-in\nincomplete Cholesky factorisation of the sparse matrix $\\Theta_{ij} 1_{( i, j )\n\\in S}$ is an $\\epsilon$-approximation of $\\Theta$. This factorisation can\nprovably be obtained in complexity $O ( N \\log( N ) \\log^{d}( N /\\epsilon) )$\nin space and $O ( N \\log^{2}( N ) \\log^{2d}( N /\\epsilon) )$ in time, improving\nupon the state of the art for general elliptic operators; we further present\nnumerical evidence that $d$ can be taken to be the intrinsic dimension of the\ndata set rather than that of the ambient space. The algorithm only needs to\nknow the spatial configuration of the $x_{i}$ and does not require an analytic\nrepresentation of $G$. Furthermore, this factorization straightforwardly\nprovides an approximate sparse PCA with optimal rate of convergence in the\noperator norm. Hence, by using only subsampling and the incomplete Cholesky\nfactorization, we obtain, at nearly linear complexity, the compression,\ninversion and approximate PCA of a large class of covariance matrices. By\ninverting the order of the Cholesky factorization we also obtain a solver for\nelliptic PDE with complexity $O ( N \\log^{d}( N /\\epsilon) )$ in space and $O (\nN \\log^{2d}( N /\\epsilon) )$ in time, improving upon the state of the art for\ngeneral elliptic operators. \n\n"}
{"id": "1706.03459", "contents": "Title: Optimal Auctions through Deep Learning: Advances in Differentiable\n  Economics Abstract: Designing an incentive compatible auction that maximizes expected revenue is\nan intricate task. The single-item case was resolved in a seminal piece of work\nby Myerson in 1981, but more than 40 years later a full analytical\nunderstanding of the optimal design still remains elusive for settings with two\nor more items. In this work, we initiate the exploration of the use of tools\nfrom deep learning for the automated design of optimal auctions. We model an\nauction as a multi-layer neural network, frame optimal auction design as a\nconstrained learning problem, and show how it can be solved using standard\nmachine learning pipelines. In addition to providing generalization bounds, we\npresent extensive experimental results, recovering essentially all known\nsolutions that come from the theoretical analysis of optimal auction design\nproblems and obtaining novel mechanisms for settings in which the optimal\nmechanism is unknown. \n\n"}
{"id": "1706.04634", "contents": "Title: A distributed algorithm for average aggregative games with coupling\n  constraints Abstract: We consider the framework of average aggregative games, where the cost\nfunction of each agent depends on his own strategy and on the average\npopulation strategy. We focus on the case in which the agents are coupled not\nonly via their cost functions, but also via constraints coupling their\nstrategies. We propose a distributed algorithm that achieves an almost Nash\nequilibrium by requiring only local communications of the agents, as specified\nby a sparse communication network. The proof of convergence of the algorithm\nrelies on the auxiliary class of network aggregative games and exploits a novel\nresult of parametric convergence of variational inequalities, which is\napplicable beyond the context of games. We apply our theoretical findings to a\nmulti-market Cournot game with transportation costs and maximum market\ncapacity. \n\n"}
{"id": "1706.04637", "contents": "Title: Approximating Gains from Trade in Two-sided Markets via Simple\n  Mechanisms Abstract: We design simple mechanisms to approximate the Gains from Trade (GFT) in\ntwo-sided markets with multiple unit-supply sellers and multiple unit-demand\nbuyers. A classical impossibility result by Myerson and Satterthwaite showed\nthat even with only one seller and one buyer, no Individually Rational (IR),\nBayesian Incentive Compatible (BIC) and Budget-Balanced (BB) mechanism can\nachieve full GFT (trade whenever buyer's value is higher than the seller's\ncost). On the other hand, they proposed the \"second-best\" mechanism that\nmaximizes the GFT subject to IR, BIC and BB constraints, which is unfortunately\nrather complex for even the single-seller single-buyer case. Our mechanism is\nsimple, IR, BIC and BB, and achieves $\\frac{1}{2}$ of the optimal GFT among all\nIR, BIC and BB mechanisms. Our result holds for arbitrary distributions of the\nbuyers' and sellers' values and can accommodate any downward-closed feasibility\nconstraints over the allocations. The analysis of our mechanism is facilitated\nby extending the Cai-Weinberg-Devanur duality framework to two-sided markets. \n\n"}
{"id": "1706.05081", "contents": "Title: Approximate Best-Response Dynamics in Random Interference Games Abstract: In this paper we develop a novel approach to the convergence of Best-Response\nDynamics for the family of interference games. Interference games represent the\nfundamental resource allocation conflict between users of the radio spectrum.\nIn contrast to congestion games, interference games are generally not potential\ngames. Therefore, proving the convergence of the best-response dynamics to a\nNash equilibrium in these games requires new techniques. We suggest a model for\nrandom interference games, based on the long term fading governed by the\nplayers' geometry. Our goal is to prove convergence of the approximate\nbest-response dynamics with high probability with respect to the randomized\ngame. We embrace the asynchronous model in which the acting player is chosen at\neach stage at random. In our approximate best-response dynamics, the action of\na deviating player is chosen at random among all the approximately best ones.\nWe show that with high probability, with respect to the players' geometry and\nasymptotically with the number of players, each action increases the expected\nsocial-welfare (sum of achievable rates). Hence, the induced sum-rate process\nis a submartingale. Based on the Martingale Convergence Theorem, we prove\nconvergence of the strategy profile to an approximate Nash equilibrium with\ngood performance for asymptotically almost all interference games. We use the\nMarkovity of the induced sum-rate process to provide probabilistic bounds on\nthe convergence time. Finally, we demonstrate our results in simulated\nexamples. \n\n"}
{"id": "1706.07181", "contents": "Title: Equilibria, information and frustration in heterogeneous network games\n  with conflicting preferences Abstract: Interactions between people are the basis on which the structure of our\nsociety arises as a complex system and, at the same time, are the starting\npoint of any physical description of it. In the last few years, much\ntheoretical research has addressed this issue by combining the physics of\ncomplex networks with a description of interactions in terms of evolutionary\ngame theory. We here take this research a step further by introducing a most\nsalient societal factor such as the individuals' preferences, a characteristic\nthat is key to understand much of the social phenomenology these days. We\nconsider a heterogeneous, agent-based model in which agents interact\nstrategically with their neighbors but their preferences and payoffs for the\npossible actions differ. We study how such a heterogeneous network behaves\nunder evolutionary dynamics and different strategic interactions, namely\ncoordination games and best shot games. With this model we study the emergence\nof the equilibria predicted analytically in random graphs under best response\ndynamics, and we extend this test to unexplored contexts like proportional\nimitation and scale free networks. We show that some theoretically predicted\nequilibria do not arise in simulations with incomplete Information, and we\ndemonstrate the importance of the graph topology and the payoff function\nparameters for some games. Finally, we discuss our results with available\nexperimental evidence on coordination games, showing that our model agrees\nbetter with the experiment that standard economic theories, and draw hints as\nto how to maximize social efficiency in situations of conflicting preferences. \n\n"}
{"id": "1706.07917", "contents": "Title: Online Participatory Sensing in Double Auction Environment with Location\n  Information Abstract: As mobile devices have been ubiquitous, participatory sensing emerges as a\npowerful tool to solve many contemporary real life problems. Here, we\ncontemplate the participatory sensing in online double auction environment by\nconsidering the location information of the participating agents. In this\npaper, we propose a truthful mechanism in this setting and the mechanism also\nsatisfies the other economic properties such as budget balance and individual\nrationality. \n\n"}
{"id": "1706.08479", "contents": "Title: A Partial Solution to Continuous Blotto Abstract: This paper analyzes the structure of mixed-strategy equilibria for Colonel\nBlotto games, where the outcome on each battlefield is a polynomial function of\nthe difference between the two players' allocations. This paper severely\nreduces the set of strategies that needs to be searched to find a Nash\nequilibrium. It finds that there exists a Nash equilibrium where both players'\nmixed strategies are discrete distributions, and it places an upper bound on\nthe number of points in the supports of these discrete distributions. \n\n"}
{"id": "1706.09763", "contents": "Title: Dynamical selection of Nash equilibria using Experience Weighted\n  Attraction Learning: emergence of heterogeneous mixed equilibria Abstract: We study the distribution of strategies in a large game that models how\nagents choose among different double auction markets. We classify the possible\nmean field Nash equilibria, which include potentially segregated states where\nan agent population can split into subpopulations adopting different\nstrategies. As the game is aggregative, the actual equilibrium strategy\ndistributions remain undetermined, however. We therefore compare with the\nresults of Experience-Weighted Attraction (EWA) learning, which at long times\nleads to Nash equilibria in the appropriate limits of large intensity of\nchoice, low noise (long agent memory) and perfect imputation of missing scores\n(fictitious play). The learning dynamics breaks the indeterminacy of the Nash\nequilibria. Non-trivially, depending on how the relevant limits are taken, more\nthan one type of equilibrium can be selected. These include the standard\nhomogeneous mixed and heterogeneous pure states, but also \\emph{heterogeneous\nmixed} states where different agents play different strategies that are not all\npure. The analysis of the EWA learning involves Fokker-Planck modeling combined\nwith large deviation methods. The theoretical results are confirmed by\nmulti-agent simulations. \n\n"}
{"id": "1707.01068", "contents": "Title: Maintaining cooperation in complex social dilemmas using deep\n  reinforcement learning Abstract: Social dilemmas are situations where individuals face a temptation to\nincrease their payoffs at a cost to total welfare. Building artificially\nintelligent agents that achieve good outcomes in these situations is important\nbecause many real world interactions include a tension between selfish\ninterests and the welfare of others. We show how to modify modern reinforcement\nlearning methods to construct agents that act in ways that are simple to\nunderstand, nice (begin by cooperating), provokable (try to avoid being\nexploited), and forgiving (try to return to mutual cooperation). We show both\ntheoretically and experimentally that such agents can maintain cooperation in\nMarkov social dilemmas. Our construction does not require training methods\nbeyond a modification of self-play, thus if an environment is such that good\nstrategies can be constructed in the zero-sum case (eg. Atari) then we can\nconstruct agents that solve social dilemmas in this environment. \n\n"}
{"id": "1707.01496", "contents": "Title: Group Strategyproof Pareto-Stable Marriage with Indifferences via the\n  Generalized Assignment Game Abstract: We study the variant of the stable marriage problem in which the preferences\nof the agents are allowed to include indifferences. We present a mechanism for\nproducing Pareto-stable matchings in stable marriage markets with indifferences\nthat is group strategyproof for one side of the market. Our key technique\ninvolves modeling the stable marriage market as a generalized assignment game.\nWe also show that our mechanism can be implemented efficiently. These results\ncan be extended to the college admissions problem with indifferences. \n\n"}
{"id": "1707.01590", "contents": "Title: Fairness at Equilibrium in the Labor Market Abstract: Recent literature on computational notions of fairness has been broadly\ndivided into two distinct camps, supporting interventions that address either\nindividual-based or group-based fairness. Rather than privilege a single\ndefinition, we seek to resolve both within the particular domain of employment\ndiscrimination. To this end, we construct a dual labor market model composed of\na Temporary Labor Market, in which firm strategies are constrained to ensure\ngroup-level fairness, and a Permanent Labor Market, in which individual worker\nfairness is guaranteed. We show that such restrictions on hiring practices\ninduces an equilibrium that Pareto-dominates those arising from strategies that\nemploy statistical discrimination or a \"group-blind\" criterion. Individual\nworker reputations produce externalities for collective reputation, generating\na feedback loop termed a \"self-fulfilling prophecy.\" Our model produces its own\nfeedback loop, raising the collective reputation of an initially disadvantaged\ngroup via a fairness intervention that need not be permanent. Moreover, we show\nthat, contrary to popular assumption, the asymmetric equilibria resulting from\nhiring practices that disregard group-fairness may be immovable without\ntargeted intervention. The enduring nature of such equilibria that are both\ninequitable and Pareto inefficient suggest that fairness interventions are of\ncritical importance in moving the labor market to be more socially just and\nefficient. \n\n"}
{"id": "1707.02058", "contents": "Title: Controlling a Population Abstract: We introduce a new setting where a population of agents, each modelled by a\nfinite-state system, are controlled uniformly: the controller applies the same\naction to every agent. The framework is largely inspired by the control of a\nbiological system, namely a population of yeasts, where the controller may only\nchange the environment common to all cells. We study a synchronisation problem\nfor such populations: no matter how individual agents react to the actions of\nthe controller , the controller aims at driving all agents synchronously to a\ntarget state. The agents are naturally represented by a non-deterministic\nfinite state automaton (NFA), the same for every agent, and the whole system is\nencoded as a 2-player game. The first player (Controller) chooses actions, and\nthe second player (Agents) resolves non-determinism for each agent. The game\nwith m agents is called the m-population game. This gives rise to a\nparameterized control problem (where control refers to 2 player games), namely\nthe population control problem: can Controller control the m-population game\nfor all $m $\\in$ N$ whatever Agents does? In this paper, we prove that the\npopulation control problem is decidable, and it is a EXPTIME-complete problem.\nAs far as we know, this is one of the first results on parameterized control.\nOur algorithm, not based on cutoff techniques, produces winning strategies\nwhich are symbolic, that is, they do not need to count precisely how the\npopulation is spread between states. We also show that if there is no winning\nstrategy, then there is a population size M such that Controller wins the\nm-population game if and only if $m $\\le$ M$. Surprisingly, M can be doubly\nexponential in the number of states of the NFA, with tight upper and lower\nbounds. \n\n"}
{"id": "1707.04316", "contents": "Title: How hard is it to satisfy (almost) all roommates? Abstract: The classic Stable Roommates problem (which is the non-bipartite\ngeneralization of the well-known Stable Marriage problem) asks whether there is\na stable matching for a given set of agents, i.e. a partitioning of the agents\ninto disjoint pairs such that no two agents induce a blocking pair. Herein,\neach agent has a preference list denoting who it prefers to have as a partner,\nand two agents are blocking if they prefer to be with each other rather than\nwith their assigned partners. Since stable matchings may not be unique, we\nstudy an NP-hard optimization variant of Stable Roommates, called Egal Stable\nRoommates, which seeks to find a stable matching with a minimum egalitarian\ncost {\\gamma}, i.e. the sum of the dissatisfaction of the agents is minimum.\nThe dissatisfaction of an agent is the number of agents that this agent prefers\nover its partner if it is matched; otherwise it is the length of its preference\nlist. We also study almost stable matchings, called Min-Block-Pair Stable\nRoommates, which seeks to find a matching with a minimum number {\\beta} of\nblocking pairs. Our main result is that Egal Stable Roommates parameterized by\n{\\gamma} is fixed-parameter tractable, while Min-Block-Pair Stable Roommates\nparameterized by {\\beta} is W[1]-hard, even if the length of each preference\nlist is at most five. \n\n"}
{"id": "1707.04428", "contents": "Title: Satiation in Fisher Markets and Approximation of Nash Social Welfare Abstract: We study linear Fisher markets with satiation. In these markets, sellers have\nearning limits and buyers have utility limits. Beyond natural applications in\neconomics, these markets arise in the context of maximizing Nash social welfare\nwhen allocating indivisible items to agents. In contrast to markets with either\nearning or utility limits, markets with both limits have not been studied\nbefore. They turn out to have fundamentally different properties.\n  In general, the existence of competitive equilibria is not guaranteed. We\nidentify a natural property of markets (termed money clearing) that implies\nexistence. We show that the set of equilibria is not always convex, answering a\nquestion of Cole et al. [EC'17]. We design an FPTAS to compute an approximate\nequilibrium and prove that the problem of computing an exact equilibrium lies\nin the intersection of complexity classes PLS and PPAD. For a constant number\nof buyers or goods, we give a polynomial-time algorithm to compute an exact\nequilibrium.\n  We show how (approximate) equilibria can be rounded and provide the first\nconstant-factor approximation algorithm (with a factor of 2.404) for maximizing\nNash social welfare when agents have budget-additive valuations. Finally, we\nsignificantly improve the approximation hardness for additive valuations to\n\\sqrt{8/7} > 1.069 (over 1.00008 by Lee [IPL'17]). \n\n"}
{"id": "1707.07919", "contents": "Title: Mean Field Equilibria for Resource Competition in Spatial Settings Abstract: We study a model of competition among nomadic agents for time-varying and\nlocation-specific resources, arising in crowd-sourced transportation services,\nonline communities, and traditional location-based economic activity. This\nmodel comprises a group of agents and a single location endowed with a dynamic\nstochastic resource process. Periodically, each agent derives a reward\ndetermined by the location's resource level and the number of other agents\nthere, and has to decide whether to stay at the location or move. Upon moving,\nthe agent arrives at a different location whose dynamics are independent and\nidentical to the original location. Using the methodology of mean field\nequilibrium, we study the equilibrium behavior of the agents as a function of\nthe dynamics of the stochastic resource process and the nature of the\ncompetition among co-located agents. We show that an equilibrium exists, where\neach agent decides whether to switch locations based only on their current\nlocation's resource level and the number of other agents there. We additionally\nshow that when an agent's payoff is decreasing in the number of other agents at\nher location, equilibrium strategies obey a simple threshold structure. We show\nhow to exploit this structure to compute equilibria numerically, and use these\nnumerical techniques to study how system structure affects the agents'\ncollective ability to explore their domain to find and effectively utilize\nresource-rich areas. \n\n"}
{"id": "1707.08092", "contents": "Title: Restricted Eigenvalue from Stable Rank with Applications to Sparse\n  Linear Regression Abstract: High-dimensional settings, where the data dimension ($d$) far exceeds the\nnumber of observations ($n$), are common in many statistical and machine\nlearning applications. Methods based on $\\ell_1$-relaxation, such as Lasso, are\nvery popular for sparse recovery in these settings. Restricted Eigenvalue (RE)\ncondition is among the weakest, and hence the most general, condition in\nliterature imposed on the Gram matrix that guarantees nice statistical\nproperties for the Lasso estimator. It is natural to ask: what families of\nmatrices satisfy the RE condition? Following a line of work in this area, we\nconstruct a new broad ensemble of dependent random design matrices that have an\nexplicit RE bound. Our construction starts with a fixed (deterministic) matrix\n$X \\in \\mathbb{R}^{n \\times d}$ satisfying a simple stable rank condition, and\nwe show that a matrix drawn from the distribution $X \\Phi^\\top \\Phi$, where\n$\\Phi \\in \\mathbb{R}^{m \\times d}$ is a subgaussian random matrix, with high\nprobability, satisfies the RE condition. This construction allows incorporating\na fixed matrix that has an easily {\\em verifiable} condition into the design\nprocess, and allows for generation of {\\em compressed} design matrices that\nhave a lower storage requirement than a standard design matrix. We give two\napplications of this construction to sparse linear regression problems,\nincluding one to a compressed sparse regression setting where the regression\nalgorithm only has access to a compressed representation of a fixed design\nmatrix $X$. \n\n"}
{"id": "1707.08730", "contents": "Title: A Quantum Approach to Subset-Sum and Similar Problems Abstract: In this paper, we study the subset-sum problem by using a quantum heuristic\napproach similar to the verification circuit of quantum Arthur-Merlin games.\nUnder described certain assumptions, we show that the exact solution of the\nsubset sum problem my be obtained in polynomial time and the exponential\nspeed-up over the classical algorithms may be possible. We give a numerical\nexample and discuss the complexity of the approach and its further application\nto the knapsack problem. \n\n"}
{"id": "1708.00043", "contents": "Title: Pricing for Online Resource Allocation: Intervals and Paths Abstract: We present pricing mechanisms for several online resource allocation problems\nwhich obtain tight or nearly tight approximations to social welfare. In our\nsettings, buyers arrive online and purchase bundles of items; buyers' values\nfor the bundles are drawn from known distributions. This problem is closely\nrelated to the so-called prophet-inequality of Krengel and Sucheston and its\nextensions in recent literature. Motivated by applications to cloud economics,\nwe consider two kinds of buyer preferences. In the first, items correspond to\ndifferent units of time at which a resource is available; the items are\narranged in a total order and buyers desire intervals of items. The second\ncorresponds to bandwidth allocation over a tree network; the items are edges in\nthe network and buyers desire paths.\n  Because buyers' preferences have complementarities in the settings we\nconsider, recent constant-factor approximations via item prices do not apply,\nand indeed strong negative results are known. We develop static, anonymous\nbundle pricing mechanisms.\n  For the interval preferences setting, we show that static, anonymous bundle\npricings achieve a sublogarithmic competitive ratio, which is optimal (within\nconstant factors) over the class of all online allocation algorithms, truthful\nor not. For the path preferences setting, we obtain a nearly-tight logarithmic\ncompetitive ratio. Both of these results exhibit an exponential improvement\nover item pricings for these settings. Our results extend to settings where the\nseller has multiple copies of each item, with the competitive ratio decreasing\nlinearly with supply. Such a gradual tradeoff between supply and the\ncompetitive ratio for welfare was previously known only for the single item\nprophet inequality. \n\n"}
{"id": "1708.04699", "contents": "Title: Mechanism Redesign Abstract: This paper develops the theory of mechanism redesign by which an auctioneer\ncan reoptimize an auction based on bid data collected from previous iterations\nof the auction on bidders from the same market. We give a direct method for\nestimation of the revenue of a counterfactual auction from the bids in the\ncurrent auction. The estimator is a simple weighted order statistic of the bids\nand has the optimal error rate. Two applications of our estimator are A/B\ntesting (a.k.a., randomized controlled trials) and instrumented optimization\n(i.e., revenue optimization subject to being able to do accurate inference of\nany counterfactual auction revenue). \n\n"}
{"id": "1708.07580", "contents": "Title: The Expanding Approvals Rule: Improving Proportional Representation and\n  Monotonicity Abstract: Proportional representation (PR) is often discussed in voting settings as a\nmajor desideratum. For the past century or so, it is common both in practice\nand in the academic literature to jump to single transferable vote (STV) as the\nsolution for achieving PR. Some of the most prominent electoral reform\nmovements around the globe are pushing for the adoption of STV. It has been\ntermed a major open problem to design a voting rule that satisfies the same PR\nproperties as STV and better monotonicity properties. In this paper, we first\npresent a taxonomy of proportional representation axioms for general weak order\npreferences, some of which generalise and strengthen previously introduced\nconcepts. We then present a rule called Expanding Approvals Rule (EAR) that\nsatisfies properties stronger than the central PR axiom satisfied by STV, can\nhandle indifferences in a convenient and computationally efficient manner, and\nalso satisfies better candidate monotonicity properties. In view of this, our\nproposed rule seems to be a compelling solution for achieving proportional\nrepresentation in voting settings. \n\n"}
{"id": "1708.09455", "contents": "Title: An algorithm to simulate alternating Turing machine in signal machine Abstract: Geometrical Computation as a new model of computation is the counterpart of\nCellular Automata that has Turing computing ability. In this paper we provide\nan algorithm to simulate Alternating Turing Machine in the context of Signal\nMachine using techniques adopted from the features of Signal Machine to set up\nand manage the copies/branches of Alternating Turing Machine. We show that our\nalgorithm can simulate Alternating Turing Machine in Signal Machine as same\nfunctionality as classic family of Turing Machines. Time complexity of the\nalgorithm is linear as ordinary simulated Turing Machines. Depending on the\ncomputation tree space complexity is exponential order of d, where d is the\ndepth of the computation tree. \n\n"}
{"id": "1708.09708", "contents": "Title: Sketching the order of events Abstract: We introduce features for massive data streams. These stream features can be\nthought of as \"ordered moments\" and generalize stream sketches from \"moments of\norder one\" to \"ordered moments of arbitrary order\". In analogy to classic\nmoments, they have theoretical guarantees such as universality that are\nimportant for learning algorithms. \n\n"}
{"id": "1709.00228", "contents": "Title: Learning Multi-item Auctions with (or without) Samples Abstract: We provide algorithms that learn simple auctions whose revenue is\napproximately optimal in multi-item multi-bidder settings, for a wide range of\nvaluations including unit-demand, additive, constrained additive, XOS, and\nsubadditive. We obtain our learning results in two settings. The first is the\ncommonly studied setting where sample access to the bidders' distributions over\nvaluations is given, for both regular distributions and arbitrary distributions\nwith bounded support. Our algorithms require polynomially many samples in the\nnumber of items and bidders. The second is a more general max-min learning\nsetting that we introduce, where we are given \"approximate distributions,\" and\nwe seek to compute an auction whose revenue is approximately optimal\nsimultaneously for all \"true distributions\" that are close to the given ones.\nThese results are more general in that they imply the sample-based results, and\nare also applicable in settings where we have no sample access to the\nunderlying distributions but have estimated them indirectly via market research\nor by observation of previously run, potentially non-truthful auctions.\n  Our results hold for valuation distributions satisfying the standard (and\nnecessary) independence-across-items property. They also generalize and improve\nupon recent works, which have provided algorithms that learn approximately\noptimal auctions in more restricted settings with additive, subadditive and\nunit-demand valuations using sample access to distributions. We generalize\nthese results to the complete unit-demand, additive, and XOS setting, to i.i.d.\nsubadditive bidders, and to the max-min setting.\n  Our results are enabled by new uniform convergence bounds for hypotheses\nclasses under product measures. Our bounds result in exponential savings in\nsample complexity compared to bounds derived by bounding the VC dimension, and\nare of independent interest. \n\n"}
{"id": "1709.00358", "contents": "Title: Adversarial Task Allocation Abstract: The problem of allocating tasks to workers is of long standing fundamental\nimportance. Examples of this include the classical problem of assigning\ncomputing tasks to nodes in a distributed computing environment, as well as the\nmore recent problem of crowdsourcing where a broad array of tasks are slated to\nbe completed by human workers. Extensive research into this problem generally\naddresses important issues such as uncertainty and, in crowdsourcing,\nincentives. However, the problem of adversarial tampering with the task\nallocation process has not received as much attention. We are concerned with a\nparticular adversarial setting in task allocation where an attacker may target\na specific worker in order to prevent the tasks assigned to this worker from\nbeing completed. We consider two attack models: one in which the adversary\nobserves only the allocation policy (which may be randomized), and the second\nin which the attacker observes the actual allocation decision. For the case\nwhen all tasks are homogeneous, we provide polynomial-time algorithms for both\nsettings. When tasks are heterogeneous, however, we show the adversarial\nallocation problem to be NP-Hard, and present algorithms for solving it when\nthe defender is restricted to assign only a single worker per task. Our\nexperiments show, surprisingly, that the difference between the two attack\nmodels is minimal: deterministic allocation can achieve nearly as much utility\nas randomized. \n\n"}
{"id": "1709.02556", "contents": "Title: Game Theory Models for the Verification of the Collective Behaviour of\n  Autonomous Cars Abstract: The collective of autonomous cars is expected to generate almost optimal\ntraffic. In this position paper we discuss the multi-agent models and the\nverification results of the collective behaviour of autonomous cars. We argue\nthat non-cooperative autonomous adaptation cannot guarantee optimal behaviour.\nThe conjecture is that intention aware adaptation with a constraint on\nsimultaneous decision making has the potential to avoid unwanted behaviour. The\nonline routing game model is expected to be the basis to formally prove this\nconjecture. \n\n"}
{"id": "1709.02865", "contents": "Title: Prosocial learning agents solve generalized Stag Hunts better than\n  selfish ones Abstract: Deep reinforcement learning has become an important paradigm for constructing\nagents that can enter complex multi-agent situations and improve their policies\nthrough experience. One commonly used technique is reactive training - applying\nstandard RL methods while treating other agents as a part of the learner's\nenvironment. It is known that in general-sum games reactive training can lead\ngroups of agents to converge to inefficient outcomes. We focus on one such\nclass of environments: Stag Hunt games. Here agents either choose a risky\ncooperative policy (which leads to high payoffs if both choose it but low\npayoffs to an agent who attempts it alone) or a safe one (which leads to a safe\npayoff no matter what). We ask how we can change the learning rule of a single\nagent to improve its outcomes in Stag Hunts that include other reactive\nlearners. We extend existing work on reward-shaping in multi-agent\nreinforcement learning and show that that making a single agent prosocial, that\nis, making them care about the rewards of their partners can increase the\nprobability that groups converge to good outcomes. Thus, even if we control a\nsingle agent in a group making that agent prosocial can increase our agent's\nlong-run payoff. We show experimentally that this result carries over to a\nvariety of more complex environments with Stag Hunt-like dynamics including\nones where agents must learn from raw input pixels. \n\n"}
{"id": "1709.03374", "contents": "Title: When to arrive at a queue with earliness, tardiness and waiting costs Abstract: We consider a queueing facility where customers decide when to arrive. All\ncustomers have the same desired arrival time (w.l.o.g.\\ time zero). There is\none server, and the service times are independent and exponentially\ndistributed. The total number of customers that demand service is random, and\nfollows the Poisson distribution. Each customer wishes to minimize the sum of\nthree costs: earliness, tardiness and waiting. We assume that all three costs\nare linear with time and are defined as follows. Earliness is the time between\narrival and time zero, if there is any. Tardiness is simply the time of\nentering service, if it is after time zero. Waiting time is the time from\narrival until entering service. We focus on customers' rational behaviour,\nassuming that each customer wants to minimize his total cost, and in\nparticular, we seek a symmetric Nash equilibrium strategy. We show that such a\nstrategy is mixed, unless trivialities occur. We construct a set of equations\nthat its solution provides the symmetric Nash equilibrium. The solution is a\ncontinuous distribution on the real line. We also compare the socially optimal\nsolution (that is, the one that minimizes total cost across all customers) to\nthe overall cost resulting from the Nash equilibrium. \n\n"}
{"id": "1709.03926", "contents": "Title: Certified Computation from Unreliable Datasets Abstract: A wide range of learning tasks require human input in labeling massive data.\nThe collected data though are usually low quality and contain inaccuracies and\nerrors. As a result, modern science and business face the problem of learning\nfrom unreliable data sets.\n  In this work, we provide a generic approach that is based on\n\\textit{verification} of only few records of the data set to guarantee high\nquality learning outcomes for various optimization objectives. Our method,\nidentifies small sets of critical records and verifies their validity. We show\nthat many problems only need $\\text{poly}(1/\\varepsilon)$ verifications, to\nensure that the output of the computation is at most a factor of $(1 \\pm\n\\varepsilon)$ away from the truth. For any given instance, we provide an\n\\textit{instance optimal} solution that verifies the minimum possible number of\nrecords to approximately certify correctness. Then using this instance optimal\nformulation of the problem we prove our main result: \"every function that\nsatisfies some Lipschitz continuity condition can be certified with a small\nnumber of verifications\". We show that the required Lipschitz continuity\ncondition is satisfied even by some NP-complete problems, which illustrates the\ngenerality and importance of this theorem.\n  In case this certification step fails, an invalid record will be identified.\nRemoving these records and repeating until success, guarantees that the result\nwill be accurate and will depend only on the verified records. Surprisingly, as\nwe show, for several computation tasks more efficient methods are possible.\nThese methods always guarantee that the produced result is not affected by the\ninvalid records, since any invalid record that affects the output will be\ndetected and verified. \n\n"}
{"id": "1709.04176", "contents": "Title: Computing the Shapley Value in Allocation Problems: Approximations and\n  Bounds, with an Application to the Italian VQR Research Assessment Program Abstract: In allocation problems, a given set of goods are assigned to agents in such a\nway that the social welfare is maximised, that is, the largest possible global\nworth is achieved. When goods are indivisible, it is possible to use money\ncompensation to perform a fair allocation taking into account the actual\ncontribution of all agents to the social welfare. Coalitional games provide a\nformal mathematical framework to model such problems, in particular the Shapley\nvalue is a solution concept widely used for assigning worths to agents in a\nfair way. Unfortunately, computing this value is a $\\#{\\rm P}$-hard problem, so\nthat applying this good theoretical notion is often quite difficult in\nreal-world problems.\n  We describe useful properties that allow us to greatly simplify the instances\nof allocation problems, without affecting the Shapley value of any player.\nMoreover, we propose algorithms for computing lower bounds and upper bounds of\nthe Shapley value, which in some cases provide the exact result and that can be\ncombined with approximation algorithms.\n  The proposed techniques have been implemented and tested on a real-world\napplication of allocation problems, namely, the Italian research assessment\nprogram, known as VQR. For the large university considered in the experiments,\nthe problem involves thousands of agents and goods (here, researchers and their\nresearch products). The algorithms described in the paper are able to compute\nthe Shapley value for most of those agents, and to get a good approximation of\nthe Shapley value for all of them. \n\n"}
{"id": "1709.04326", "contents": "Title: Learning with Opponent-Learning Awareness Abstract: Multi-agent settings are quickly gathering importance in machine learning.\nThis includes a plethora of recent work on deep multi-agent reinforcement\nlearning, but also can be extended to hierarchical RL, generative adversarial\nnetworks and decentralised optimisation. In all these settings the presence of\nmultiple learning agents renders the training problem non-stationary and often\nleads to unstable training or undesired final results. We present Learning with\nOpponent-Learning Awareness (LOLA), a method in which each agent shapes the\nanticipated learning of the other agents in the environment. The LOLA learning\nrule includes a term that accounts for the impact of one agent's policy on the\nanticipated parameter update of the other agents. Results show that the\nencounter of two LOLA agents leads to the emergence of tit-for-tat and\ntherefore cooperation in the iterated prisoners' dilemma, while independent\nlearning does not. In this domain, LOLA also receives higher payouts compared\nto a naive learner, and is robust against exploitation by higher order\ngradient-based methods. Applied to repeated matching pennies, LOLA agents\nconverge to the Nash equilibrium. In a round robin tournament we show that LOLA\nagents successfully shape the learning of a range of multi-agent learning\nalgorithms from literature, resulting in the highest average returns on the\nIPD. We also show that the LOLA update rule can be efficiently calculated using\nan extension of the policy gradient estimator, making the method suitable for\nmodel-free RL. The method thus scales to large parameter and input spaces and\nnonlinear function approximators. We apply LOLA to a grid world task with an\nembedded social dilemma using recurrent policies and opponent modelling. By\nexplicitly considering the learning of the other agent, LOLA agents learn to\ncooperate out of self-interest. The code is at github.com/alshedivat/lola. \n\n"}
{"id": "1709.04569", "contents": "Title: REMOTEGATE: Incentive-Compatible Remote Configuration of Security\n  Gateways Abstract: Imagine that a malicious hacker is trying to attack a server over the\nInternet and the server wants to block the attack packets as close to their\npoint of origin as possible. However, the security gateway ahead of the source\nof attack is untrusted. How can the server block the attack packets through\nthis gateway? In this paper, we introduce REMOTEGATE, a trustworthy mechanism\nfor allowing any party (server) on the Internet to configure a security gateway\nowned by a second party, at a certain agreed upon reward that the former pays\nto the latter for its service. We take an interactive incentive-compatible\napproach, for the case when both the server and the gateway are rational, to\ndevise a protocol that will allow the server to help the security gateway\ngenerate and deploy a policy rule that filters the attack packets before they\nreach the server. The server will reward the gateway only when the latter can\nsuccessfully verify that it has generated and deployed the correct rule for the\nissue. This mechanism will enable an Internet-scale approach to improving\nsecurity and privacy, backed by digital payment incentives. \n\n"}
{"id": "1709.04854", "contents": "Title: Synthesizing Optimally Resilient Controllers Abstract: Recently, Dallal, Neider, and Tabuada studied a generalization of the\nclassical game-theoretic model used in program synthesis, which additionally\naccounts for unmodeled intermittent disturbances. In this extended framework,\none is interested in computing optimally resilient strategies, i.e., strategies\nthat are resilient against as many disturbances as possible. Dallal, Neider,\nand Tabuada showed how to compute such strategies for safety specifications. In\nthis work, we compute optimally resilient strategies for a much wider range of\nwinning conditions and show that they do not require more memory than winning\nstrategies in the classical model. Our algorithms only have a polynomial\noverhead in comparison to the ones computing winning strategies. In particular,\nfor parity conditions, optimally resilient strategies are positional and can be\ncomputed in quasipolynomial time. \n\n"}
{"id": "1709.05282", "contents": "Title: On the Difference Between Closest, Furthest, and Orthogonal Pairs:\n  Nearly-Linear vs Barely-Subquadratic Complexity in Computational Geometry Abstract: Point location problems for $n$ points in $d$-dimensional Euclidean space\n(and $\\ell_p$ spaces more generally) have typically had two kinds of\nrunning-time solutions:\n  * (Nearly-Linear) less than $d^{poly(d)} \\cdot n \\log^{O(d)} n$ time, or\n  * (Barely-Subquadratic) $f(d) \\cdot n^{2-1/\\Theta(d)}$ time, for various $f$.\n  For small $d$ and large $n$, \"nearly-linear\" running times are generally\nfeasible, while \"barely-subquadratic\" times are generally infeasible. For\nexample, in the Euclidean metric, finding a Closest Pair among $n$ points in\n${\\mathbb R}^d$ is nearly-linear, solvable in $2^{O(d)} \\cdot n \\log^{O(1)} n$\ntime, while known algorithms for Furthest Pair (the diameter of the point set)\nare only barely-subquadratic, requiring $\\Omega(n^{2-1/\\Theta(d)})$ time. Why\ndo these proximity problems have such different time complexities? Is there a\nbarrier to obtaining nearly-linear algorithms for problems which are currently\nonly barely-subquadratic?\n  We give a novel exact and deterministic self-reduction for the Orthogonal\nVectors problem on $n$ vectors in $\\{0,1\\}^d$ to $n$ vectors in ${\\mathbb\nZ}^{\\omega(\\log d)}$ that runs in $2^{o(d)}$ time. As a consequence,\nbarely-subquadratic problems such as Euclidean diameter, Euclidean bichromatic\nclosest pair, ray shooting, and incidence detection do not have\n$O(n^{2-\\epsilon})$ time algorithms (in Turing models of computation) for\ndimensionality $d = \\omega(\\log \\log n)^2$, unless the popular Orthogonal\nVectors Conjecture and the Strong Exponential Time Hypothesis are false. That\nis, while poly-log-log-dimensional Closest Pair is in $n^{1+o(1)}$ time, the\nanalogous case of Furthest Pair can encode larger-dimensional problems\nconjectured to require $n^{2-o(1)}$ time. We also show that the All-Nearest\nNeighbors problem in $\\omega(\\log n)$ dimensions requires $n^{2-o(1)}$ time to\nsolve, assuming either of the above conjectures. \n\n"}
{"id": "1709.05294", "contents": "Title: The Orthogonal Vectors Conjecture for Branching Programs and Formulas Abstract: In the Orthogonal Vectors (OV) problem, we wish to determine if there is an\northogonal pair of vectors among $n$ Boolean vectors in $d$ dimensions. The OV\nConjecture (OVC) posits that OV requires $n^{2-o(1)}$ time to solve, for all\n$d=\\omega(\\log n)$. Assuming the OVC, optimal time lower bounds have been\nproved for many prominent problems in $P$.\n  We prove that OVC is true in several computational models of interest:\n  * For all sufficiently large $n$ and $d$, OV for $n$ vectors in $\\{0,1\\}^d$\nhas branching program complexity $\\tilde{\\Theta}(n\\cdot \\min(n,2^d))$. In\nparticular, the lower bounds match the upper bounds up to polylog factors.\n  * OV has Boolean formula complexity $\\tilde{\\Theta}(n\\cdot \\min(n,2^d))$,\nover all complete bases of $O(1)$ fan-in.\n  * OV requires $\\tilde{\\Theta}(n\\cdot \\min(n,2^d))$ wires, in formulas\ncomprised of gates computing arbitrary symmetric functions of unbounded fan-in.\n  Our lower bounds basically match the best known (quadratic) lower bounds for\nany explicit function in those models. Analogous lower bounds hold for many\nrelated problems shown to be hard under OVC, such as Batch Partial Match, Batch\nSubset Queries, and Batch Hamming Nearest Neighbors, all of which have very\nsuccinct reductions to OV.\n  The proofs use a certain kind of input restriction that is different from\ntypical random restrictions where variables are assigned independently. We give\na sense in which independent random restrictions cannot be used to show\nhardness, in that OVC is false in the \"average case\" even for $AC^0$ formulas:\n  * For every fixed $p \\in (0,1)$ there is an $\\epsilon_p > 0$ such that for\nevery $n$ and $d$, OV instances where input bits are independently set to $1$\nwith probability $p$ (and $0$ otherwise) can be solved with $AC^0$ formulas of\nsize $O(n^{2-\\epsilon_p})$, on all but a $o_n(1)$ fraction of instances. \n\n"}
{"id": "1709.06367", "contents": "Title: Selfish Jobs with Favorite Machines: Price of Anarchy vs Strong Price of\n  Anarchy Abstract: We consider the well-studied game-theoretic version of machine scheduling in\nwhich jobs correspond to self-interested users and machines correspond to\nresources. Here each user chooses a machine trying to minimize her own cost,\nand such selfish behavior typically results in some equilibrium which is not\nglobally optimal: An equilibrium is an allocation where no user can reduce her\nown cost by moving to another machine, which in general need not minimize the\nmakespan, i.e., the maximum load over the machines.\n  We provide tight bounds on two well-studied notions in algorithmic game\ntheory, namely, the price of anarchy and the strong price of anarchy on machine\nscheduling setting which lies in between the related and the unrelated machine\ncase. Both notions study the social cost (makespan) of the worst equilibrium\ncompared to the optimum, with the strong price of anarchy restricting to a\nstronger form of equilibria. Our results extend a prior study comparing the\nprice of anarchy to the strong price of anarchy for two related machines\n(Epstein, Acta Informatica 2010), thus providing further insights on the\nrelation between these concepts. Our exact bounds give a qualitative and\nquantitative comparison between the two models. The bounds also show that the\nsetting is indeed easier than the two unrelated machines: In the latter, the\nstrong price of anarchy is $2$, while in ours it is strictly smaller. \n\n"}
{"id": "1709.08318", "contents": "Title: Hodge decomposition and the Shapley value of a cooperative game Abstract: We show that a cooperative game may be decomposed into a sum of component\ngames, one for each player, using the combinatorial Hodge decomposition on a\ngraph. This decomposition is shown to satisfy certain efficiency, null-player,\nsymmetry, and linearity properties. Consequently, we obtain a new\ncharacterization of the classical Shapley value as the value of the grand\ncoalition in each player's component game. We also relate this decomposition to\na least-squares problem involving inessential games (in a similar spirit to\nprevious work on least-squares and minimum-norm solution concepts) and to the\ngraph Laplacian. Finally, we generalize this approach to games with weights\nand/or constraints on coalition formation. \n\n"}
{"id": "1709.08441", "contents": "Title: Uncertainty in Multi-Commodity Routing Networks: When does it help? Abstract: We study the equilibrium behavior in a multi-commodity selfish routing game\nwith many types of uncertain users where each user over- or under-estimates\ntheir congestion costs by a multiplicative factor. Surprisingly, we find that\nuncertainties in different directions have qualitatively distinct impacts on\nequilibria. Namely, contrary to the usual notion that uncertainty increases\ninefficiencies, network congestion actually decreases when users over-estimate\ntheir costs. On the other hand, under-estimation of costs leads to increased\ncongestion. We apply these results to urban transportation networks, where\ndrivers have different estimates about the cost of congestion. In light of the\ndynamic pricing policies aimed at tackling congestion, our results indicate\nthat users' perception of these prices can significantly impact the policy's\nefficacy, and \"caution in the face of uncertainty\" leads to favorable network\nconditions. \n\n"}
{"id": "1709.10122", "contents": "Title: Mechanism Design for Demand Response Programs with financial and\n  non-monetary (social) Incentives Abstract: Most demand management approaches with non-mandatory policies assume full\nusers' cooperation, which may not be the case given users' beliefs, needs and\npreferences. In this paper we propose a mechanism for demand management\nincluding incentives both with and without money. The mechanism is validated by\nmeans of simulation, modeling the consumers as a finite multiagent system which\nevolves until a stable state, and social incentives diffusion using opinion\ndynamics. \n\n"}
{"id": "1709.10258", "contents": "Title: An improved algorithm for recognizing matroids Abstract: Let $M$ be a matroid defined on a finite set $E$ and $L\\subset E$. $L$ is\nlocked in $M$ if $M|L$ and $M^*|(E\\backslash L)$ are 2-connected, and\n$min\\{r(L), r^*(E\\backslash L)\\} \\geq 2$. Locked subsets characterize\nnontrivial facets of the bases polytope. In this paper, we give a new axiom\nsystem for matroids based on locked subsets. We deduce an algorithm for\nrecognizing matroids improving the running time complexity of the best known\ntill today. This algorithm induces a polynomial time algorithm for recognizing\nuniform matroids. This latter problem is intractable if we use an independence\noracle. \n\n"}
{"id": "1710.00996", "contents": "Title: Equilibrium Computation and Robust Optimization in Zero Sum Games with\n  Submodular Structure Abstract: We define a class of zero-sum games with combinatorial structure, where the\nbest response problem of one player is to maximize a submodular function. For\nexample, this class includes security games played on networks, as well as the\nproblem of robustly optimizing a submodular function over the worst case from a\nset of scenarios. The challenge in computing equilibria is that both players'\nstrategy spaces can be exponentially large. Accordingly, previous algorithms\nhave worst-case exponential runtime and indeed fail to scale up on practical\ninstances. We provide a pseudopolynomial-time algorithm which obtains a\nguaranteed $(1 - 1/e)^2$-approximate mixed strategy for the maximizing player.\nOur algorithm only requires access to a weakened version of a best response\noracle for the minimizing player which runs in polynomial time. Experimental\nresults for network security games and a robust budget allocation problem\nconfirm that our algorithm delivers near-optimal solutions and scales to much\nlarger instances than was previously possible. \n\n"}
{"id": "1710.01567", "contents": "Title: Cloud/fog computing resource management and pricing for blockchain\n  networks Abstract: The mining process in blockchain requires solving a proof-of-work puzzle,\nwhich is resource expensive to implement in mobile devices due to the high\ncomputing power and energy needed. In this paper, we, for the first time,\nconsider edge computing as an enabler for mobile blockchain. In particular, we\nstudy edge computing resource management and pricing to support mobile\nblockchain applications in which the mining process of miners can be offloaded\nto an edge computing service provider. We formulate a two-stage Stackelberg\ngame to jointly maximize the profit of the edge computing service provider and\nthe individual utilities of the miners. In the first stage, the service\nprovider sets the price of edge computing nodes. In the second stage, the\nminers decide on the service demand to purchase based on the observed prices.\nWe apply the backward induction to analyze the sub-game perfect equilibrium in\neach stage for both uniform and discriminatory pricing schemes. For the uniform\npricing where the same price is applied to all miners, the existence and\nuniqueness of Stackelberg equilibrium are validated by identifying the best\nresponse strategies of the miners. For the discriminatory pricing where the\ndifferent prices are applied to different miners, the Stackelberg equilibrium\nis proved to exist and be unique by capitalizing on the Variational Inequality\ntheory. Further, the real experimental results are employed to justify our\nproposed model. \n\n"}
{"id": "1710.03091", "contents": "Title: On Variants of Network Flow Stability Abstract: We consider a general stable flow problem in a directed and capacitated\nnetwork, where each vertex has a strict preference list over the incoming and\noutgoing edges. A flow is stable if no group of vertices forming a path can\nmutually benefit by rerouting the flow. Motivated by applications in supply\nchain networks, we generalize the traditional Kirchhoff's law, requiring the\noutflow is equal to the inflow at every nonterminal node, to a monotone\npiecewise linear relationship between the inflows and the outflows. We show the\nexistence of a stable flow using Scarf's Lemma, and provide a polynomial time\nalgorithm to find such a stable flow. We further show that finding a minimum\ncost generalized stable network is NP-hard, while the problem is polynomial\ntime solvable for the traditional stable flow satisfying Kirchhoff's law. \n\n"}
{"id": "1710.03830", "contents": "Title: Inference on Auctions with Weak Assumptions on Information Abstract: Given a sample of bids from independent auctions, this paper examines the\nquestion of inference on auction fundamentals (e.g. valuation distributions,\nwelfare measures) under weak assumptions on information structure. The question\nis important as it allows us to learn about the valuation distribution in a\nrobust way, i.e., without assuming that a particular information structure\nholds across observations. We leverage the recent contributions of\n\\cite{Bergemann2013} in the robust mechanism design literature that exploit the\nlink between Bayesian Correlated Equilibria and Bayesian Nash Equilibria in\nincomplete information games to construct an econometrics framework for\nlearning about auction fundamentals using observed data on bids. We showcase\nour construction of identified sets in private value and common value auctions.\nOur approach for constructing these sets inherits the computational simplicity\nof solving for correlated equilibria: checking whether a particular valuation\ndistribution belongs to the identified set is as simple as determining whether\na {\\it linear} program is feasible. A similar linear program can be used to\nconstruct the identified set on various welfare measures and counterfactual\nobjects. For inference and to summarize statistical uncertainty, we propose\nnovel finite sample methods using tail inequalities that are used to construct\nconfidence regions on sets. We also highlight methods based on Bayesian\nbootstrap and subsampling. A set of Monte Carlo experiments show adequate\nfinite sample properties of our inference procedures. We illustrate our methods\nusing data from OCS auctions. \n\n"}
{"id": "1710.04246", "contents": "Title: Monotonicity axioms in approval-based multi-winner voting rules Abstract: In this paper we study several monotonicity axioms in approval-based\nmulti-winner voting rules. We consider monotonicity with respect to the support\nreceived by the winners and also monotonicity in the size of the committee.\nMonotonicity with respect to the support is studied when the set of voters does\nnot change and when new voters enter the election. For each of these two cases\nwe consider a strong and a weak version of the axiom. We observe certain\nincompatibilities between the monotonicity axioms and well-known representation\naxioms (extended/proportional justified representation) for the voting rules\nthat we analyze and provide formal proofs of incompatibility between some of\nthese axioms and perfect representation. \n\n"}
{"id": "1710.04677", "contents": "Title: Game-Theoretic Design of Secure and Resilient Distributed Support Vector\n  Machines with Adversaries Abstract: With a large number of sensors and control units in networked systems,\ndistributed support vector machines (DSVMs) play a fundamental role in scalable\nand efficient multi-sensor classification and prediction tasks. However, DSVMs\nare vulnerable to adversaries who can modify and generate data to deceive the\nsystem to misclassification and misprediction. This work aims to design defense\nstrategies for DSVM learner against a potential adversary. We establish a\ngame-theoretic framework to capture the conflicting interests between the DSVM\nlearner and the attacker. The Nash equilibrium of the game allows predicting\nthe outcome of learning algorithms in adversarial environments, and enhancing\nthe resilience of the machine learning through dynamic distributed learning\nalgorithms. We show that the DSVM learner is less vulnerable when he uses a\nbalanced network with fewer nodes and higher degree. We also show that adding\nmore training samples is an efficient defense strategy against an attacker. We\npresent secure and resilient DSVM algorithms with verification method and\nrejection method, and show their resiliency against adversary with numerical\nexperiments. \n\n"}
{"id": "1710.07328", "contents": "Title: Online Monotone Games Abstract: Algorithmic game theory (AGT) focuses on the design and analysis of\nalgorithms for interacting agents, with interactions rigorously formalized\nwithin the framework of games. Results from AGT find applications in domains\nsuch as online bidding auctions for web advertisements and network routing\nprotocols. Monotone games are games where agent strategies naturally converge\nto an equilibrium state. Previous results in AGT have been obtained for convex,\nsocially-convex, or smooth games, but not monotone games. Our primary\ntheoretical contributions are defining the monotone game setting and its\nextension to the online setting, a new notion of regret for this setting, and\naccompanying algorithms that achieve sub-linear regret. We demonstrate the\nutility of online monotone game theory on a variety of problem domains\nincluding variational inequalities, reinforcement learning, and generative\nadversarial networks. \n\n"}
{"id": "1710.07365", "contents": "Title: An extension of the Moran process using type-specific connection graphs Abstract: The Moran process, as studied by [Lieberman, E., Hauert, C. and Nowak, M.\nEvolutionary dynamics on graphs. Nature 433, pp. 312-316 (2005)], is a\nstochastic process modeling the spread of genetic mutations in populations. In\nthis process, agents of a two-type population (i.e. mutants and residents) are\nassociated with the vertices of a graph. Initially, only one vertex chosen\nuniformly at random is a mutant, with fitness $r > 0$, while all other\nindividuals are residents, with fitness $1$. In every step, an individual is\nchosen with probability proportional to its fitness, and its state (mutant or\nresident) is passed on to a neighbor which is chosen uniformly at random. In\nthis paper, we introduce and study a generalization of the model of Lieberman\net al. by assuming that different types of individuals perceive the population\nthrough different graphs, namely $G_R(V,E_R)$ for residents and $G_M(V,E_M)$\nfor mutants. In this model, we study the fixation probability, i.e. the\nprobability that eventually only mutants remain in the population, for various\npairs of graphs. First, we transfer known results from the original\nsingle-graph model of Lieberman et al. to our 2-graph model. Among them, we\nprovide a generalization of the Isothermal Theorem of Lieberman et al., that\ngives sufficient conditions for a pair of graphs to have the same fixation\nprobability as a pair of cliques. Next, we give a 2-player strategic game view\nof the process where player payoffs correspond to fixation and/or extinction\nprobabilities. In this setting, we attempt to identify best responses for each\nplayer and give evidence that the clique is the most beneficial graph for both\nplayers. Finally, we examine the possibility of efficient approximation of the\nfixation probability and provide a FPRAS for the special case where the mutant\ngraph is complete. \n\n"}
{"id": "1710.07460", "contents": "Title: The Importance of System-Level Information in Multiagent Systems Design:\n  Cardinality and Covering Problems Abstract: A fundamental challenge in multiagent systems is to design local control\nalgorithms to ensure a desirable collective behaviour. The information\navailable to the agents, gathered either through communication or sensing,\nnaturally restricts the achievable performance. Hence, it is fundamental to\nidentify what piece of information is valuable and can be exploited to design\ncontrol laws with enhanced performance guarantees. This paper studies the case\nwhen such information is uncertain or inaccessible for a class of submodular\nresource allocation problems termed covering problems. In the first part of\nthis work we pinpoint a fundamental risk-reward tradeoff faced by the system\noperator when conditioning the control design on a valuable but uncertain piece\nof information, which we refer to as the cardinality, that represents the\nmaximum number of agents that can simultaneously select any given resource.\nBuilding on this analysis, we propose a distributed algorithm that allows\nagents to learn the cardinality while adjusting their behaviour over time. This\nalgorithm is proved to perform on par or better to the optimal design obtained\nwhen the exact cardinality is known a priori. \n\n"}
{"id": "1710.08394", "contents": "Title: Fixed Price Approximability of the Optimal Gain From Trade Abstract: Bilateral trade is a fundamental economic scenario comprising a strategically\nacting buyer and seller, each holding valuations for the item, drawn from\npublicly known distributions. A mechanism is supposed to facilitate trade\nbetween these agents, if such trade is beneficial. It was recently shown that\nthe only mechanisms that are simultaneously DSIC, SBB, and ex-post IR, are\nfixed price mechanisms, i.e., mechanisms that are parametrised by a price p,\nand trade occurs if and only if the valuation of the buyer is at least p and\nthe valuation of the seller is at most p. The gain from trade is the increase\nin welfare that results from applying a mechanism; here we study the gain from\ntrade achievable by fixed price mechanisms. We explore this question for both\nthe bilateral trade setting, and a double auction setting where there are\nmultiple buyers and sellers. We first identify a fixed price mechanism that\nachieves a gain from trade of at least 2/r times the optimum, where r is the\nprobability that the seller's valuation does not exceed the buyer's valuation.\nThis extends a previous result by McAfee. Subsequently, we improve this\napproximation factor in an asymptotic sense, by showing that a more\nsophisticated rule for setting the fixed price results in an expected gain from\ntrade within a factor O(log(1/r)) of the optimal gain from trade. This is\nasymptotically the best approximation factor possible. Lastly, we extend our\nstudy of fixed price mechanisms to the double auction setting defined by a set\nof multiple i.i.d. unit demand buyers, and i.i.d. unit supply sellers. We\npresent a fixed price mechanism that achieves a gain from trade that achieves\nfor all epsilon > 0 a gain from trade of at least (1-epsilon) times the\nexpected optimal gain from trade with probability 1 - 2/e^{#T epsilon^2 /2},\nwhere #T is the expected number of trades resulting from the double auction. \n\n"}
{"id": "1710.09595", "contents": "Title: Quantum versus Classical Online Streaming Algorithms with Logarithmic\n  Size of Memory Abstract: We consider online algorithms with respect to the competitive ratio. Here, we\ninvestigate quantum and classical one-way automata with non-constant size of\nmemory (streaming algorithms) as a model for online algorithms. We construct\nproblems that can be solved by quantum online streaming algorithms better than\nby classical ones in a case of logarithmic or sublogarithmic size of memory. \n\n"}
{"id": "1710.10753", "contents": "Title: Computational Social Choice and Computational Complexity: BFFs? Abstract: We discuss the connection between computational social choice (comsoc) and\ncomputational complexity. We stress the work so far on, and urge continued\nfocus on, two less-recognized aspects of this connection. Firstly, this is very\nmuch a two-way street: Everyone knows complexity classification is used in\ncomsoc, but we also highlight benefits to complexity that have arisen from its\nuse in comsoc. Secondly, more subtle, less-known complexity tools often can be\nvery productively used in comsoc. \n\n"}
{"id": "1711.00141", "contents": "Title: Training GANs with Optimism Abstract: We address the issue of limit cycling behavior in training Generative\nAdversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for\ntraining Wasserstein GANs. Recent theoretical results have shown that\noptimistic mirror decent (OMD) can enjoy faster regret rates in the context of\nzero-sum games. WGANs is exactly a context of solving a zero-sum game with\nsimultaneous no-regret dynamics. Moreover, we show that optimistic mirror\ndecent addresses the limit cycling problem in training WGANs. We formally show\nthat in the case of bi-linear zero-sum games the last iterate of OMD dynamics\nconverges to an equilibrium, in contrast to GD dynamics which are bound to\ncycle. We also portray the huge qualitative difference between GD and OMD\ndynamics with toy examples, even when GD is modified with many adaptations\nproposed in the recent literature, such as gradient penalty or momentum. We\napply OMD WGAN training to a bioinformatics problem of generating DNA\nsequences. We observe that models trained with OMD achieve consistently smaller\nKL divergence with respect to the true underlying distribution, than models\ntrained with GD variants. Finally, we introduce a new algorithm, Optimistic\nAdam, which is an optimistic variant of Adam. We apply it to WGAN training on\nCIFAR10 and observe improved performance in terms of inception score as\ncompared to Adam. \n\n"}
{"id": "1711.00609", "contents": "Title: Security Against Impersonation Attacks in Distributed Systems Abstract: In a multi-agent system, transitioning from a centralized to a distributed\ndecision-making strategy can introduce vulnerability to adversarial\nmanipulation. We study the potential for adversarial manipulation in a class of\ngraphical coordination games where the adversary can pose as a friendly agent\nin the game, thereby influencing the decision-making rules of a subset of\nagents. The adversary's influence can cascade throughout the system, indirectly\ninfluencing other agents' behavior and significantly impacting the emergent\ncollective behavior. The main results in this paper focus on characterizing\nconditions under which the adversary's local influence can dramatically impact\nthe emergent global behavior, e.g., destabilize efficient Nash equilibria. \n\n"}
{"id": "1711.00832", "contents": "Title: A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning Abstract: To achieve general intelligence, agents must learn how to interact with\nothers in a shared environment: this is the challenge of multiagent\nreinforcement learning (MARL). The simplest form is independent reinforcement\nlearning (InRL), where each agent treats its experience as part of its\n(non-stationary) environment. In this paper, we first observe that policies\nlearned using InRL can overfit to the other agents' policies during training,\nfailing to sufficiently generalize during execution. We introduce a new metric,\njoint-policy correlation, to quantify this effect. We describe an algorithm for\ngeneral MARL, based on approximate best responses to mixtures of policies\ngenerated using deep reinforcement learning, and empirical game-theoretic\nanalysis to compute meta-strategies for policy selection. The algorithm\ngeneralizes previous ones such as InRL, iterated best response, double oracle,\nand fictitious play. Then, we present a scalable implementation which reduces\nthe memory requirement using decoupled meta-solvers. Finally, we demonstrate\nthe generality of the resulting policies in two partially observable settings:\ngridworld coordination games and poker. \n\n"}
{"id": "1711.01049", "contents": "Title: Optimal Pricing-Based Edge Computing Resource Management in Mobile\n  Blockchain Abstract: As the core issue of blockchain, the mining requires solving a proof-of-work\npuzzle, which is resource expensive to implement in mobile devices due to high\ncomputing power needed. Thus, the development of blockchain in mobile\napplications is restricted. In this paper, we consider the edge computing as\nthe network enabler for mobile blockchain. In particular, we study optimal\npricing-based edge computing resource management to support mobile blockchain\napplications where the mining process can be offloaded to an Edge computing\nService Provider (ESP). We adopt a two-stage Stackelberg game to jointly\nmaximize the profit of the ESP and the individual utilities of different\nminers. In Stage I, the ESP sets the price of edge computing services. In Stage\nII, the miners decide on the service demand to purchase based on the observed\nprices. We apply the backward induction to analyze the sub-game perfect\nequilibrium in each stage for uniform and discriminatory pricing schemes.\nFurther, the existence and uniqueness of Stackelberg game are validated for\nboth pricing schemes. At last, the performance evaluation shows that the ESP\nintends to set the maximum possible value as the optimal price for profit\nmaximization under uniform pricing. In addition, the discriminatory pricing\nhelps the ESP encourage higher total service demand from miners and achieve\ngreater profit correspondingly. \n\n"}
{"id": "1711.01295", "contents": "Title: Optimal Data Acquisition for Statistical Estimation Abstract: We consider a data analyst's problem of purchasing data from strategic agents\nto compute an unbiased estimate of a statistic of interest. Agents incur\nprivate costs to reveal their data and the costs can be arbitrarily correlated\nwith their data. Once revealed, data are verifiable. This paper focuses on\nlinear unbiased estimators. We design an individually rational and incentive\ncompatible mechanism that optimizes the worst-case mean-squared error of the\nestimation, where the worst-case is over the unknown correlation between costs\nand data, subject to a budget constraint in expectation. We characterize the\nform of the optimal mechanism in closed-form. We further extend our results to\nacquiring data for estimating a parameter in regression analysis, where private\ncosts can correlate with the values of the dependent variable but not with the\nvalues of the independent variables. \n\n"}
{"id": "1711.01333", "contents": "Title: Learning to Bid Without Knowing your Value Abstract: We address online learning in complex auction settings, such as sponsored\nsearch auctions, where the value of the bidder is unknown to her, evolving in\nan arbitrary manner and observed only if the bidder wins an allocation. We\nleverage the structure of the utility of the bidder and the partial feedback\nthat bidders typically receive in auctions, in order to provide algorithms with\nregret rates against the best fixed bid in hindsight, that are exponentially\nfaster in convergence in terms of dependence on the action space, than what\nwould have been derived by applying a generic bandit algorithm and almost\nequivalent to what would have been achieved in the full information setting.\nOur results are enabled by analyzing a new online learning setting with\noutcome-based feedback, which generalizes learning with feedback graphs. We\nprovide an online learning algorithm for this setting, of independent interest,\nwith regret that grows only logarithmically with the number of actions and\nlinearly only in the number of potential outcomes (the latter being very small\nin most auction settings). Last but not least, we show that our algorithm\noutperforms the bandit approach experimentally and that this performance is\nrobust to dropping some of our theoretical assumptions or introducing noise in\nthe feedback that the bidder receives. \n\n"}
{"id": "1711.02165", "contents": "Title: The menu complexity of \"one-and-a-half-dimensional\" mechanism design Abstract: We study the menu complexity of optimal and approximately-optimal auctions in\nthe context of the \"FedEx\" problem, a so-called \"one-and-a-half-dimensional\"\nsetting where a single bidder has both a value and a deadline for receiving an\n[FGKK16]. The menu complexity of an auction is equal to the number of distinct\n(allocation, price) pairs that a bidder might receive [HN13]. We show the\nfollowing when the bidder has $n$ possible deadlines:\n  - Exponential menu complexity is necessary to be exactly optimal: There exist\ninstances where the optimal mechanism has menu complexity is $2^n-1$. This\nmatches exactly the upper bound provided by Fiat et al.'s algorithm, and\nresolves one of their open questions [FGKK16].\n  - Fully polynomial menu complexity is necessary and sufficient for\napproximation: For all instances, there exists a mechanism guaranteeing a\nmultiplicative (1-\\epsilon)-approximation to the optimal revenue with menu\ncomplexity $O(n^{3/2}\\sqrt{\\frac{\\min\\{n/\\epsilon,\\ln(v_{\\max})\\}}{\\epsilon}})\n= O(n^2/\\epsilon)$, where $v_{\\max}$ denotes the largest value in the support\nof integral distributions.\n  - There exist instances where any mechanism guaranteeing a multiplicative\n$(1-O(1/n^2))$-approximation to the optimal revenue requires menu complexity\n$\\Omega(n^2)$.\n  Our main technique is the polygon approximation of concave functions\n[Rote19], and our results here should be of independent interest. We further\nshow how our techniques can be used to resolve an open question of [DW17] on\nthe menu complexity of optimal auctions for a budget-constrained buyer. \n\n"}
{"id": "1711.02211", "contents": "Title: Social welfare and profit maximization from revealed preferences Abstract: Consider the seller's problem of finding optimal prices for her $n$\n(divisible) goods when faced with a set of $m$ consumers, given that she can\nonly observe their purchased bundles at posted prices, i.e., revealed\npreferences. We study both social welfare and profit maximization with revealed\npreferences. Although social welfare maximization is a seemingly non-convex\noptimization problem in prices, we show that (i) it can be reduced to a dual\nconvex optimization problem in prices, and (ii) the revealed preferences can be\ninterpreted as supergradients of the concave conjugate of valuation, with which\nsubgradients of the dual function can be computed. We thereby obtain a simple\nsubgradient-based algorithm for strongly concave valuations and convex cost,\nwith query complexity $O(m^2/\\epsilon^2)$, where $\\epsilon$ is the additive\ndifference between the social welfare induced by our algorithm and the optimum\nsocial welfare. We also study social welfare maximization under the online\nsetting, specifically the random permutation model, where consumers arrive\none-by-one in a random order. For the case where consumer valuations can be\narbitrary continuous functions, we propose a price posting mechanism that\nachieves an expected social welfare up to an additive factor of $O(\\sqrt{mn})$\nfrom the maximum social welfare. Finally, for profit maximization (which may be\nnon-convex in simple cases), we give nearly matching upper and lower bounds on\nthe query complexity for separable valuations and cost (i.e., each good can be\ntreated independently). \n\n"}
{"id": "1711.02308", "contents": "Title: Security Strategies of Both Players in Asymmetric Information Zero-Sum\n  Stochastic Games with an Informed Controller Abstract: This paper considers a zero-sum two-player asymmetric information stochastic\ngame where only one player knows the system state, and the transition law is\ncontrolled by the informed player only. For the informed player, it has been\nshown that the security strategy only depends on the belief and the current\nstage. We provide LP formulations whose size is only linear in the size of the\nuninformed player's action set to compute both history based and belief based\nsecurity strategies. For the uninformed player, we focus on the regret, the\ndifference between 0 and the future payoff guaranteed by the uninformed player\nin every possible state. Regret is a real vector of the same size as the\nbelief, and depends only on the action of the informed player and the strategy\nof the uninformed player. This paper shows that the uninformed player has a\nsecurity strategy that only depends on the regret and the current stage. LP\nformulations are then given to compute the history based security strategy, the\nregret at every stage, and the regret based security strategy. The size of the\nLP formulations are again linear in the size of the uninformed player action\nset. Finally, an intrusion detection problem is studied to demonstrate the main\nresults in this paper. \n\n"}
{"id": "1711.02844", "contents": "Title: Optimal Auction For Edge Computing Resource Management in Mobile\n  Blockchain Networks: A Deep Learning Approach Abstract: Blockchain has recently been applied in many applications such as bitcoin,\nsmart grid, and Internet of Things (IoT) as a public ledger of transactions.\nHowever, the use of blockchain in mobile environments is still limited because\nthe mining process consumes too much computing and energy resources on mobile\ndevices. Edge computing offered by the Edge Computing Service Provider can be\nadopted as a viable solution for offloading the mining tasks from the mobile\ndevices, i.e., miners, in the mobile blockchain environment. However, a\nmechanism needs to be designed for edge resource allocation to maximize the\nrevenue for the Edge Computing Service Provider and to ensure incentive\ncompatibility and individual rationality is still open. In this paper, we\ndevelop an optimal auction based on deep learning for the edge resource\nallocation. Specifically, we construct a multi-layer neural network\narchitecture based on an analytical solution of the optimal auction. The neural\nnetworks first perform monotone transformations of the miners' bids. Then, they\ncalculate allocation and conditional payment rules for the miners. We use\nvaluations of the miners as the data training to adjust parameters of the\nneural networks so as to optimize the loss function which is the expected,\nnegated revenue of the Edge Computing Service Provider. We show the\nexperimental results to confirm the benefits of using the deep learning for\nderiving the optimal auction for mobile blockchain with high revenue \n\n"}
{"id": "1711.03417", "contents": "Title: A Further Analysis of The Role of Heterogeneity in Coevolutionary\n  Spatial Games Abstract: Heterogeneity has been studied as one of the most common explanations of the\npuzzle of cooperation in social dilemmas. A large number of papers have been\npublished discussing the effects of increasing heterogeneity in structured\npopulations of agents, where it has been established that heterogeneity may\nfavour cooperative behaviour if it supports agents to locally coordinate their\nstrategies. In this paper, assuming an existing model of a heterogeneous\nweighted network, we aim to further this analysis by exploring the relationship\n(if any) between heterogeneity and cooperation. We adopt a weighted network\nwhich is fully populated by agents playing both the Prisoner's Dilemma or the\nOptional Prisoner's Dilemma games with coevolutionary rules, i.e., not only the\nstrategies but also the link weights evolve over time. Surprisingly, results\nshow that the heterogeneity of link weights (states) on their own does not\nalways promote cooperation; rather cooperation is actually favoured by the\nincrease in the number of overlapping states and not by the heterogeneity\nitself. We believe that these results can guide further research towards a more\naccurate analysis of the role of heterogeneity in social dilemmas. \n\n"}
{"id": "1711.03466", "contents": "Title: On Strong Equilibria and Improvement Dynamics in Network Creation Games Abstract: We study strong equilibria in network creation games. These form a classical\nand well-studied class of games where a set of players form a network by buying\nedges to their neighbors at a cost of a fixed parameter $\\alpha$. The cost of a\nplayer is defined to be the cost of the bought edges plus the sum of distances\nto all the players in the resulting graph.\n  We identify and characterize various structural properties of strong\nequilibria, which lead to a characterization of the set of strong equilibria\nfor all $\\alpha$ in the range $(0,2)$. For $\\alpha > 2$, Andelman et al. (2009)\nprove that a star graph in which every leaf buys one edge to the center node is\na strong equilibrium, and conjecture that in fact any star is a strong\nequilibrium. We resolve this conjecture in the affirmative. Additionally, we\nshow that when $\\alpha$ is large enough ($\\geq 2n$) there exist non-star trees\nthat are strong equilibria. For the strong price of anarchy, we provide precise\nexpressions when $\\alpha$ is in the range $(0,2)$, and we prove a lower bound\nof $3/2$ when $\\alpha \\geq 2$.\n  Lastly, we aim to characterize under which conditions (coalitional)\nimprovement dynamics may converge to a strong equilibrium. To this end, we\nstudy the (coalitional) finite improvement property and (coalitional) weak\nacyclicity property. We prove various conditions under which these properties\ndo and do not hold. Some of these results also hold for the class of pure Nash\nequilibria. \n\n"}
{"id": "1711.04208", "contents": "Title: Practical Scalability for Stackelberg Security Games Abstract: Stackelberg Security Games (SSGs) have been adopted widely for modeling\nadversarial interactions. With increasing size of the applications of SSGs,\nscalability of equilibrium computation is an important research problem. While\nprior research has made progress with regards to scalability, many real world\nproblems cannot be solved satisfactorily yet as per current requirements; these\ninclude the deployed federal air marshals (FAMS) application and the threat\nscreening (TSG) problem at airports. Further, these problem domains are\ninherently limited by NP hardness shown in prior literature. We initiate a\nprincipled study of approximations in zero-sum SSGs. Our contribution includes\nthe following: (1) a unified model of SSGs called adversarial randomized\nallocation (ARA) games that allows studying most SSGs in one model, (2)\nhardness of approximation results for zero-sum ARA, as well as for the\nsub-problem of allocating federal air marshal (FAMS) and threat screening\nproblem (TSG) at airports, (3) an approximation framework for zero-sum ARA with\nprovable approximation guarantees and (4) experiments demonstrating the\nsignificant scalability of up to 1000x improvement in runtime with an\nacceptable 5% solution quality loss. \n\n"}
{"id": "1711.04503", "contents": "Title: Consensus Halving is PPA-Complete Abstract: We show that the computational problem CONSENSUS-HALVING is PPA-complete, the\nfirst PPA-completeness result for a problem whose definition does not involve\nan explicit circuit. We also show that an approximate version of this problem\nis polynomial-time equivalent to NECKLACE SPLITTING, which establishes\nPPAD-hardness for NECKLACE SPLITTING, and suggests that it is also\nPPA-complete. \n\n"}
{"id": "1711.04520", "contents": "Title: Fair Knapsack Abstract: We study the following multiagent variant of the knapsack problem. We are\ngiven a set of items, a set of voters, and a value of the budget; each item is\nendowed with a cost and each voter assigns to each item a certain value. The\ngoal is to select a subset of items with the total cost not exceeding the\nbudget, in a way that is consistent with the voters' preferences. Since the\npreferences of the voters over the items can vary significantly, we need a way\nof aggregating these preferences, in order to select the socially best valid\nknapsack. We study three approaches to aggregating voters' preferences, which\nare motivated by the literature on multiwinner elections and fair allocation.\nThis way we introduce the concepts of individually best, diverse, and fair\nknapsack. We study the computational complexity (including parameterized\ncomplexity, and complexity under restricted domains) of the aforementioned\nmultiagent variants of knapsack. \n\n"}
{"id": "1711.05144", "contents": "Title: Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup\n  Fairness Abstract: The most prevalent notions of fairness in machine learning are statistical\ndefinitions: they fix a small collection of pre-defined groups, and then ask\nfor parity of some statistic of the classifier across these groups. Constraints\nof this form are susceptible to intentional or inadvertent \"fairness\ngerrymandering\", in which a classifier appears to be fair on each individual\ngroup, but badly violates the fairness constraint on one or more structured\nsubgroups defined over the protected attributes. We propose instead to demand\nstatistical notions of fairness across exponentially (or infinitely) many\nsubgroups, defined by a structured class of functions over the protected\nattributes. This interpolates between statistical definitions of fairness and\nrecently proposed individual notions of fairness, but raises several\ncomputational challenges. It is no longer clear how to audit a fixed classifier\nto see if it satisfies such a strong definition of fairness. We prove that the\ncomputational problem of auditing subgroup fairness for both equality of false\npositive rates and statistical parity is equivalent to the problem of weak\nagnostic learning, which means it is computationally hard in the worst case,\neven for simple structured subclasses.\n  We then derive two algorithms that provably converge to the best fair\nclassifier, given access to oracles which can solve the agnostic learning\nproblem. The algorithms are based on a formulation of subgroup fairness as a\ntwo-player zero-sum game between a Learner and an Auditor. Our first algorithm\nprovably converges in a polynomial number of steps. Our second algorithm enjoys\nonly provably asymptotic convergence, but has the merit of simplicity and\nfaster per-step computation. We implement the simpler algorithm using linear\nregression as a heuristic oracle, and show that we can effectively both audit\nand learn fair classifiers on real datasets. \n\n"}
{"id": "1711.05157", "contents": "Title: A note on the complexity of Feedback Vertex Set parameterized by\n  mim-width Abstract: We complement the recent algorithmic result that Feedback Vertex Set is\nXP-time solvable parameterized by the mim-width of a given branch decomposition\nof the input graph [3] by showing that the problem is W[1]-hard in this\nparameterization. The hardness holds even for linear mim-width, as well as for\nH-graphs, where the parameter is the number of edges in H. To obtain this\nresult, we adapt a reduction due to Fomin, Golovach and Raymond [2], following\nthe same line of reasoning but adding a new gadget. \n\n"}
{"id": "1711.06030", "contents": "Title: Sub-committee Approval Voting and Generalised Justified Representation\n  Axioms Abstract: Social choice is replete with various settings including single-winner\nvoting, multi-winner voting, probabilistic voting, multiple referenda, and\npublic decision making. We study a general model of social choice called\nSub-Committee Voting (SCV) that simultaneously generalizes these settings. We\nthen focus on sub-committee voting with approvals and propose extensions of the\njustified representation axioms that have been considered for proportional\nrepresentation in approval-based committee voting. We study the properties and\nrelations of these axioms. For each of the axioms, we analyse whether a\nrepresentative committee exists and also examine the complexity of computing\nand verifying such a committee. \n\n"}
{"id": "1711.06740", "contents": "Title: Information Gathering with Peers: Submodular Optimization with\n  Peer-Prediction Constraints Abstract: We study a problem of optimal information gathering from multiple data\nproviders that need to be incentivized to provide accurate information. This\nproblem arises in many real world applications that rely on crowdsourced data\nsets, but where the process of obtaining data is costly. A notable example of\nsuch a scenario is crowd sensing. To this end, we formulate the problem of\noptimal information gathering as maximization of a submodular function under a\nbudget constraint, where the budget represents the total expected payment to\ndata providers. Contrary to the existing approaches, we base our payments on\nincentives for accuracy and truthfulness, in particular, {\\em peer-prediction}\nmethods that score each of the selected data providers against its best peer,\nwhile ensuring that the minimum expected payment is above a given threshold. We\nfirst show that the problem at hand is hard to approximate within a constant\nfactor that is not dependent on the properties of the payment function.\nHowever, for given topological and analytical properties of the instance, we\nconstruct two greedy algorithms, respectively called PPCGreedy and\nPPCGreedyIter, and establish theoretical bounds on their performance w.r.t. the\noptimal solution. Finally, we evaluate our methods using a realistic crowd\nsensing testbed. \n\n"}
{"id": "1711.07600", "contents": "Title: On the Distortion of Voting with Multiple Representative Candidates Abstract: We study positional voting rules when candidates and voters are embedded in a\ncommon metric space, and cardinal preferences are naturally given by distances\nin the metric space. In a positional voting rule, each candidate receives a\nscore from each ballot based on the ballot's rank order; the candidate with the\nhighest total score wins the election. The cost of a candidate is his sum of\ndistances to all voters, and the distortion of an election is the ratio between\nthe cost of the elected candidate and the cost of the optimum candidate. We\nconsider the case when candidates are representative of the population, in the\nsense that they are drawn i.i.d. from the population of the voters, and analyze\nthe expected distortion of positional voting rules.\n  Our main result is a clean and tight characterization of positional voting\nrules that have constant expected distortion (independent of the number of\ncandidates and the metric space). Our characterization result immediately\nimplies constant expected distortion for Borda Count and elections in which\neach voter approves a constant fraction of all candidates. On the other hand,\nwe obtain super-constant expected distortion for Plurality, Veto, and approving\na constant number of candidates. These results contrast with previous results\non voting with metric preferences: When the candidates are chosen\nadversarially, all of the preceding voting rules have distortion linear in the\nnumber of candidates or voters. Thus, the model of representative candidates\nallows us to distinguish voting rules which seem equally bad in the worst case. \n\n"}
{"id": "1711.07621", "contents": "Title: Groupwise Maximin Fair Allocation of Indivisible Goods Abstract: We study the problem of allocating indivisible goods among n agents in a fair\nmanner. For this problem, maximin share (MMS) is a well-studied solution\nconcept which provides a fairness threshold. Specifically, maximin share is\ndefined as the minimum utility that an agent can guarantee for herself when\nasked to partition the set of goods into n bundles such that the remaining\n(n-1) agents pick their bundles adversarially. An allocation is deemed to be\nfair if every agent gets a bundle whose valuation is at least her maximin\nshare.\n  Even though maximin shares provide a natural benchmark for fairness, it has\nits own drawbacks and, in particular, it is not sufficient to rule out\nunsatisfactory allocations. Motivated by these considerations, in this work we\ndefine a stronger notion of fairness, called groupwise maximin share guarantee\n(GMMS). In GMMS, we require that the maximin share guarantee is achieved not\njust with respect to the grand bundle, but also among all the subgroups of\nagents. Hence, this solution concept strengthens MMS and provides an ex-post\nfairness guarantee. We show that in specific settings, GMMS allocations always\nexist. We also establish the existence of approximate GMMS allocations under\nadditive valuations, and develop a polynomial-time algorithm to find such\nallocations. Moreover, we establish a scale of fairness wherein we show that\nGMMS implies approximate envy freeness.\n  Finally, we empirically demonstrate the existence of GMMS allocations in a\nlarge set of randomly generated instances. For the same set of instances, we\nadditionally show that our algorithm achieves an approximation factor better\nthan the established, worst-case bound. \n\n"}
{"id": "1711.08365", "contents": "Title: Budget Allocation in Binary Opinion Dynamics Abstract: In this article we study the allocation of a budget to promote an opinion in\na group of agents. We assume that their opinion dynamics are based on the\nwell-known voter model. We are interested in finding the most efficient use of\na budget over time in order to manipulate a social network. We address the\nproblem using the theory of discounted Markov decision processes. Our\ncontributions can be summarized as follows: (i) we introduce the discounted\nMarkov decision process in our cases, (ii) we present the corresponding Bellman\nequations, and, (iii) we solve the Bellman equations via backward programming.\nThis work is a step towards providing a solid formulation of the budget\nallocation in social networks. \n\n"}
{"id": "1711.10102", "contents": "Title: A Game-theoretic Framework for Revenue Sharing in Edge-Cloud Computing\n  System Abstract: We introduce a game-theoretic framework to ex- plore revenue sharing in an\nEdge-Cloud computing system, in which computing service providers at the edge\nof the Internet (edge providers) and computing service providers at the cloud\n(cloud providers) co-exist and collectively provide computing resources to\nclients (e.g., end users or applications) at the edge. Different from\ntraditional cloud computing, the providers in an Edge-Cloud system are\nindependent and self-interested. To achieve high system-level efficiency, the\nmanager of the system adopts a task distribution mechanism to maximize the\ntotal revenue received from clients and also adopts a revenue sharing mechanism\nto split the received revenue among computing servers (and hence service\nproviders). Under those system-level mechanisms, service providers attempt to\ngame with the system in order to maximize their own utilities, by strategically\nallocating their resources (e.g., computing servers).\n  Our framework models the competition among the providers in an Edge-Cloud\nsystem as a non-cooperative game. Our simulations and experiments on an\nemulation system have shown the existence of Nash equilibrium in such a game.\nWe find that revenue sharing mechanisms have a significant impact on the\nsystem-level efficiency at Nash equilibria, and surprisingly the revenue\nsharing mechanism based directly on actual contributions can result in\nsignificantly worse system efficiency than Shapley value sharing mechanism and\nOrtmann proportional sharing mechanism. Our framework provides an effective\neconomics approach to understanding and designing efficient Edge-Cloud\ncomputing systems. \n\n"}
{"id": "1712.00270", "contents": "Title: Together or Alone: The Price of Privacy in Collaborative Learning Abstract: Machine learning algorithms have reached mainstream status and are widely\ndeployed in many applications. The accuracy of such algorithms depends\nsignificantly on the size of the underlying training dataset; in reality a\nsmall or medium sized organization often does not have the necessary data to\ntrain a reasonably accurate model. For such organizations, a realistic solution\nis to train their machine learning models based on their joint dataset (which\nis a union of the individual ones). Unfortunately, privacy concerns prevent\nthem from straightforwardly doing so. While a number of privacy-preserving\nsolutions exist for collaborating organizations to securely aggregate the\nparameters in the process of training the models, we are not aware of any work\nthat provides a rational framework for the participants to precisely balance\nthe privacy loss and accuracy gain in their collaboration. In this paper, by\nfocusing on a two-player setting, we model the collaborative training process\nas a two-player game where each player aims to achieve higher accuracy while\npreserving the privacy of its own dataset. We introduce the notion of Price of\nPrivacy, a novel approach for measuring the impact of privacy protection on the\naccuracy in the proposed framework. Furthermore, we develop a game-theoretical\nmodel for different player types, and then either find or prove the existence\nof a Nash Equilibrium with regard to the strength of privacy protection for\neach player. Using recommendation systems as our main use case, we demonstrate\nhow two players can make practical use of the proposed theoretical framework,\nincluding setting up the parameters and approximating the non-trivial Nash\nEquilibrium. \n\n"}
{"id": "1712.00679", "contents": "Title: GANGs: Generative Adversarial Network Games Abstract: Generative Adversarial Networks (GAN) have become one of the most successful\nframeworks for unsupervised generative modeling. As GANs are difficult to train\nmuch research has focused on this. However, very little of this research has\ndirectly exploited game-theoretic techniques. We introduce Generative\nAdversarial Network Games (GANGs), which explicitly model a finite zero-sum\ngame between a generator ($G$) and classifier ($C$) that use mixed strategies.\nThe size of these games precludes exact solution methods, therefore we define\nresource-bounded best responses (RBBRs), and a resource-bounded Nash\nEquilibrium (RB-NE) as a pair of mixed strategies such that neither $G$ or $C$\ncan find a better RBBR. The RB-NE solution concept is richer than the notion of\n`local Nash equilibria' in that it captures not only failures of escaping local\noptima of gradient descent, but applies to any approximate best response\ncomputations, including methods with random restarts. To validate our approach,\nwe solve GANGs with the Parallel Nash Memory algorithm, which provably\nmonotonically converges to an RB-NE. We compare our results to standard GAN\nsetups, and demonstrate that our method deals well with typical GAN problems\nsuch as mode collapse, partial mode coverage and forgetting. \n\n"}
{"id": "1712.02027", "contents": "Title: Evolutionary Game for Mining Pool Selection in Blockchain Networks Abstract: In blockchain networks adopting the proof-of-work schemes, the monetary\nincentive is introduced by the Nakamoto consensus protocol to guide the\nbehaviors of the full nodes (i.e., block miners) in the process of maintaining\nthe consensus about the blockchain state. The block miners have to devote their\ncomputation power measured in hash rate in a crypto-puzzle solving competition\nto win the reward of publishing (a.k.a., mining) new blocks. Due to the\nexponentially increasing difficulty of the crypto-puzzle, individual block\nminers tends to join mining pools, i.e., the coalitions of miners, in order to\nreduce the income variance and earn stable profits. In this paper, we study the\ndynamics of mining pool selection in a blockchain network, where mining pools\nmay choose arbitrary block mining strategies. We identify the hash rate and the\nblock propagation delay as two major factors determining the outcomes of mining\ncompetition, and then model the strategy evolution of the individual miners as\nan evolutionary game. We provide the theoretical analysis of the evolutionary\nstability for the pool selection dynamics in a case study of two mining pools.\nThe numerical simulations provide the evidence to support our theoretical\ndiscoveries as well as demonstrating the stability in the evolution of miners'\nstrategies in a general case. \n\n"}
{"id": "1712.02447", "contents": "Title: On Colouring $(2P_2,H)$-Free and $(P_5,H)$-Free Graphs Abstract: The Colouring problem asks whether the vertices of a graph can be coloured\nwith at most $k$ colours for a given integer $k$ in such a way that no two\nadjacent vertices receive the same colour. A graph is $(H_1,H_2)$-free if it\nhas no induced subgraph isomorphic to $H_1$ or $H_2$. A connected graph $H_1$\nis almost classified if Colouring on $(H_1,H_2)$-free graphs is known to be\npolynomial-time solvable or NP-complete for all but finitely many connected\ngraphs $H_2$. We show that every connected graph $H_1$ apart from the claw\n$K_{1,3}$ and the $5$-vertex path $P_5$ is almost classified. We also prove a\nnumber of new hardness results for Colouring on $(2P_2,H)$-free graphs. This\nenables us to list all graphs $H$ for which the complexity of Colouring is open\non $(2P_2,H)$-free graphs and all graphs $H$ for which the complexity of\nColouring is open on $(P_5,H)$-free graphs. In fact we show that these two\nlists coincide. Moreover, we show that the complexities of Colouring for\n$(2P_2,H)$-free graphs and for $(P_5,H)$-free graphs are the same for all known\ncases. \n\n"}
{"id": "1712.02712", "contents": "Title: Group Activity Selection on Social Networks Abstract: We propose a new variant of the group activity selection problem (GASP),\nwhere the agents are placed on a social network and activities can only be\nassigned to connected subgroups (gGASP). We show that if multiple groups can\nsimultaneously engage in the same activity, finding a stable outcome is easy as\nlong as the network is acyclic. In contrast, if each activity can be assigned\nto a single group only, finding stable outcomes becomes computationally\nintractable, even if the underlying network is very simple: the problem of\ndetermining whether a given instance of a gGASP admits a Nash stable outcome\nturns out to be NP-hard when the social network is a path or a star, or if the\nsize of each connected component is bounded by a constant. We then study the\nparameterized complexity of finding outcomes of gGASP that are Nash stable,\nindividually stable or core stable. For the parameter `number of activities',\nwe propose an FPT algorithm for Nash stability for the case where the social\nnetwork is acyclic and obtain a W[1]-hardness result for cliques (i.e., for\nstandard GASP); similar results hold for individual stability. In contrast,\nfinding a core stable outcome is hard even if the number of activities is\nbounded by a small constant, both for standard GASP and when the social network\nis a star. For the parameter `number of players', all problems we consider are\nin XP for arbitrary social networks; on the other hand, we prove W[1]-hardness\nresults with respect to the parameter `number of players' for the case where\nthe social network is a clique. \n\n"}
{"id": "1712.03141", "contents": "Title: Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning Abstract: Learning-based pattern classifiers, including deep networks, have shown\nimpressive performance in several application domains, ranging from computer\nvision to cybersecurity. However, it has also been shown that adversarial input\nperturbations carefully crafted either at training or at test time can easily\nsubvert their predictions. The vulnerability of machine learning to such wild\npatterns (also referred to as adversarial examples), along with the design of\nsuitable countermeasures, have been investigated in the research field of\nadversarial machine learning. In this work, we provide a thorough overview of\nthe evolution of this research area over the last ten years and beyond,\nstarting from pioneering, earlier work on the security of non-deep learning\nalgorithms up to more recent work aimed to understand the security properties\nof deep learning algorithms, in the context of computer vision and\ncybersecurity tasks. We report interesting connections between these\napparently-different lines of work, highlighting common misconceptions related\nto the security evaluation of machine-learning algorithms. We review the main\nthreat models and attacks defined to this end, and discuss the main limitations\nof current work, along with the corresponding future challenges towards the\ndesign of more secure learning algorithms. \n\n"}
{"id": "1712.04427", "contents": "Title: Small-Scale Markets for Bilateral Resource Trading in the Sharing\n  Economy Abstract: We consider a general small-scale market for agent-to-agent resource sharing,\nin which each agent could either be a server (seller) or a client (buyer) in\neach time period. In every time period, a server has a certain amount of\nresources that any client could consume, and randomly gets matched with a\nclient. Our target is to maximize the resource utilization in such an\nagent-to-agent market, where the agents are strategic. During each transaction,\nthe server gets money and the client gets resources. Hence, trade ratio\nmaximization implies efficiency maximization of our system. We model the\nproposed market system through a Mean Field Game approach and prove the\nexistence of the Mean Field Equilibrium, which can achieve an almost 100% trade\nratio. Finally, we carry out a simulation study motivated by an agent-to-agent\ncomputing market, and a case study on a proposed photovoltaic market, and show\nthe designed market benefits both individuals and the system as a whole. \n\n"}
{"id": "1712.05385", "contents": "Title: Equilibria in the Tangle Abstract: We analyse the Tangle --- a DAG-valued stochastic process where new vertices\nget attached to the graph at Poissonian times, and the attachment's locations\nare chosen by means of random walks on that graph. These new vertices, also\nthought of as \"transactions\", are issued by many players (which are the nodes\nof the network), independently. The main application of this model is that it\nis used as a base for the IOTA cryptocurrency system (www.iota.org). We prove\nexistence of \"almost symmetric\" Nash equilibria for the system where a part of\nplayers tries to optimize their attachment strategies. Then, we also present\nsimulations that show that the \"selfish\" players will nevertheless cooperate\nwith the network by choosing attachment strategies that are similar to the\n\"recommended\" one. \n\n"}
{"id": "1712.05723", "contents": "Title: Perfect Prediction in Normal Form: Superrational Thinking Extended to\n  Non-Symmetric Games Abstract: This paper introduces a new solution concept for non-cooperative games in\nnormal form with no ties and pure strategies: the Perfectly Transparent\nEquilibrium. The players are rational in all possible worlds and know each\nother's strategies in all possible worlds, which together we refer to as\nPerfect Prediction. The anticipation of a player's decision by their opponents\nis counterfactually dependent on the decision, unlike in Nash Equilibra where\nthe decisions are made independently. The equilibrium, when it exists, is\nunique and is Pareto optimal.\n  This equilibrium is the normal-form counterpart of the Perfect Prediction\nEquilibrium; the prediction happens \"in another room\" rather than in the past.\nThe equilibrium can also be seen as a natural extension of Hofstadter's\nsuperrationality to non-symmetric games.\n  Algorithmically, an iterated elimination of non-individually-rational\nstrategy profiles is performed until at most one remains. An equilibrium is a\nstrategy profile that is immune against knowledge of strategies in all possible\nworlds and rationality in all possible worlds, a stronger concept than common\nknowledge of rationality but also than common counterfactual belief of\nrationality.\n  We formalize and contrast the Non-Nashian Decision Theory paradigm, common to\nthis and several other papers, with Causal Decision Theory and Evidential\nDecision Theory. We define the Perfectly Transparent Equilibrium\nalgorithmically and, when it exists, prove its uniqueness, its\nPareto-optimality, and that it coincides with the Hofstadter's Superrationality\non symmetric games.\n  We also relate to concepts found in literature such as Individual\nRationality, Minimax-Rationalizability, the Correlated Equilibrium.\n  Finally, we specifically discuss inclusion relationships on the special case\nof symmetric games and contrast them with asymmetric games. \n\n"}
{"id": "1712.06309", "contents": "Title: FPT-algorithms for some problems related to integer programming Abstract: In this paper, we present FPT-algorithms for special cases of the shortest\nlattice vector, integer linear programming, and simplex width computation\nproblems, when matrices included in the problems' formulations are near square.\nThe parameter is the maximum absolute value of rank minors of the corresponding\nmatrices. Additionally, we present FPT-algorithms with respect to the same\nparameter for the problems, when the matrices have no singular rank\nsub-matrices. \n\n"}
{"id": "1712.06358", "contents": "Title: The Saga of KPR: Theoretical and Experimental developments Abstract: In this article, we present a brief narration of the origin and the overview\nof the recent developments done on the Kolkata Paise Restaurant (KPR) problem,\nwhich can serve as a prototype for a broader class of resource allocation\nproblems in the presence of a large number of competing agents, typically\nstudied using coordination and anti-coordination games. We discuss the KPR and\nits several extensions, as well as its applications in many economic and social\nphenomena. We end the article with some discussions on our ongoing experimental\nanalysis of the same problem. We demonstrate that this provides an interesting\npicture of how people analyze complex situations, and design their strategies\nor react to them. \n\n"}
{"id": "1712.06488", "contents": "Title: Invincible Strategies of Iterated Prisoner's Dilemma Abstract: Iterated Prisoner's Dilemma(IPD) is a well-known benchmark for studying the\nlong term behaviors of rational agents, such as how cooperation can emerge\namong selfish and unrelated agents that need to co-exist over long term. Many\nwell-known strategies have been studied, from the simple tit-for-tat(TFT)\nstrategy made famous by Axelrod after his influential tournaments to more\ninvolved ones like zero determinant strategies studied recently by Press and\nDyson. In this paper, following Press and Dyson, we consider one memory\nprobabilistic strategies. We consider that we call invincible strategies: a\nstrategy is invincible if it never loses against any other strategy in terms of\naverage payoff in the limit, if the limit exists. We show a strategy is\ninvincible for infinite repeated iterated prisoner's dilemma. \n\n"}
{"id": "1712.06706", "contents": "Title: Fast Algorithms for Delta-Separated Sparsity Projection Abstract: We describe a fast approximation algorithm for the $\\Delta$-separated\nsparsity projection problem. The $\\Delta$-separated sparsity model was\nintroduced by Hegde, Duarte and Cevher (2009) to capture the firing process of\na single Poisson neuron with absolute refractoriness. The running time of our\nprojection algorithm is linear for an arbitrary (but fixed) precision and it is\nboth a head and a tail approximation. This solves a problem of Hegde, Indyk and\nSchmidt (2015). We also describe how our algorithm fits into the approximate\nmodel iterative hard tresholding framework of Hegde, Indyk and Schmidt (2014)\nthat allows to recover $\\Delta$-separated sparse signals from noisy random\nlinear measurements. The resulting recovery algorithm is substantially faster\nthan the existing one, at least for large data sets. \n\n"}
{"id": "1712.06713", "contents": "Title: Game-Theoretic Electric Vehicle Charging Management Resilient to\n  Non-Ideal User Behavior Abstract: In this paper, an electric vehicle (EV) charging competition, among EV\naggregators that perform coordinated EV charging, is explored while taking into\nconsideration potential non-ideal actions of the aggregators. In the\ncoordinated EV charging strategy presented in this paper, each aggregator\ndetermines EV charging start time and charging energy profiles to minimize\noverall EV charging energy cost by including consideration of the actions of\nthe neighboring aggregators. The competitive interactions of the aggregators\nare modeled by developing a two-stage non-cooperative game among the\naggregators. The game is then studied under prospect theory to examine the\nimpacts of non-ideal actions of the aggregators in selecting EV charging start\ntimes according to subjectively evaluating their opponents' actions. It is\nshown that the non-cooperative interactions among the aggregators lead to a\nsubgame perfect $\\epsilon$-Nash equilibrium when the game is played with either\nideal, or non-ideal, actions of the aggregators. A case study presented\ndemonstrates that the benefits of the coordinated EV charging strategy, in\nterms of energy cost savings and peak-to-average ratio reductions, are\nsignificantly resilient to non-ideal actions of the aggregators. \n\n"}
{"id": "1712.07246", "contents": "Title: Further limitations of the known approaches for matrix multiplication Abstract: We consider the techniques behind the current best algorithms for matrix\nmultiplication. Our results are threefold.\n  (1) We provide a unifying framework, showing that all known matrix\nmultiplication running times since 1986 can be achieved from a single very\nnatural tensor - the structural tensor $T_q$ of addition modulo an integer $q$.\n  (2) We show that if one applies a generalization of the known techniques\n(arbitrary zeroing out of tensor powers to obtain independent matrix products\nin order to use the asymptotic sum inequality of Sch\\\"{o}nhage) to an arbitrary\nmonomial degeneration of $T_q$, then there is an explicit lower bound,\ndepending on $q$, on the bound on the matrix multiplication exponent $\\omega$\nthat one can achieve. We also show upper bounds on the value $\\alpha$ that one\ncan achieve, where $\\alpha$ is such that $n\\times n^\\alpha \\times n$ matrix\nmultiplication can be computed in $n^{2+o(1)}$ time.\n  (3) We show that our lower bound on $\\omega$ approaches $2$ as $q$ goes to\ninfinity. This suggests a promising approach to improving the bound on\n$\\omega$: for variable $q$, find a monomial degeneration of $T_q$ which, using\nthe known techniques, produces an upper bound on $\\omega$ as a function of $q$.\nThen, take $q$ to infinity. It is not ruled out, and hence possible, that one\ncan obtain $\\omega=2$ in this way. \n\n"}
{"id": "1712.07742", "contents": "Title: Mechanism Design for Demand Response Programs Abstract: Demand Response (DR) programs serve to reduce the consumption of electricity\nat times when the supply is scarce and expensive. The utility informs the\naggregator of an anticipated DR event. The aggregator calls on a subset of its\npool of recruited agents to reduce their electricity use. Agents are paid for\nreducing their energy consumption from contractually established baselines.\nBaselines are counter-factual consumption estimates of the energy an agent\nwould have consumed if they were not participating in the DR program. Baselines\nare used to determine payments to agents. This creates an incentive for agents\nto inflate their baselines. We propose a novel self-reported baseline mechanism\n(SRBM) where each agent reports its baseline and marginal utility. These\nreports are strategic and need not be truthful. Based on the reported\ninformation, the aggregator selects or calls on agents to meet the load\nreduction target. Called agents are paid for observed reductions from their\nself-reported baselines. Agents who are not called face penalties for\nconsumption shortfalls below their baselines. The mechanism is specified by the\nprobability with which agents are called, reward prices for called agents, and\npenalty prices for agents who are not called. Under SRBM, we show that truthful\nreporting of baseline consumption and marginal utility is a dominant strategy.\nThus, SRBM eliminates the incentive for agents to inflate baselines. SRBM is\nassured to meet the load reduction target. SRBM is also nearly efficient since\nit selects agents with the smallest marginal utilities, and each called agent\ncontributes maximally to the load reduction target. Finally, we show that SRBM\nis almost optimal in the metric of average cost of DR provision faced by the\naggregator. \n\n"}
{"id": "1712.08622", "contents": "Title: Analysis and Implementation of a Hourly Billing Mechanism for Demand\n  Response Management Abstract: An important part of the Smart Grid literature on residential Demand Response\ndeals with game-theoretic consumption models. Among those papers, the hourly\nbilling model is of special interest as an intuitive and fair mechanism. We\nfocus on this model and answer to several theoretical and practical questions.\nFirst, we prove the uniqueness of the consumption profile corresponding to the\nNash equilibrium, and we analyze its efficiency by providing a bound on the\nPrice of Anarchy. Next, we address the computational issue of the equilibrium\nprofile by providing two algorithms: the cycling best response dynamics and a\nprojected gradient descent method, and by giving an upper bound on their\nconvergence rate to the equilibrium. Last, we simulate this demand response\nframework in a stochastic environment where the parameters depend on forecasts.\nWe show numerically the relevance of an online demand response procedure, which\nreduces the impact of inaccurate forecasts. \n\n"}
{"id": "1712.08973", "contents": "Title: The Better Half of Selling Separately Abstract: Separate selling of two independent goods is shown to yield at least 62% of\nthe optimal revenue, and at least 73% when the goods satisfy the Myerson\nregularity condition. This improves the 50% result of Hart and Nisan (2017,\noriginally circulated in 2012). \n\n"}
{"id": "1801.00218", "contents": "Title: Game-theoretic Network Centrality: A Review Abstract: Game-theoretic centrality is a flexible and sophisticated approach to\nidentify the most important nodes in a network. It builds upon the methods from\ncooperative game theory and network theory. The key idea is to treat nodes as\nplayers in a cooperative game, where the value of each coalition is determined\nby certain graph-theoretic properties. Using solution concepts from cooperative\ngame theory, it is then possible to measure how responsible each node is for\nthe worth of the network.\n  The literature on the topic is already quite large, and is scattered among\ngame-theoretic and computer science venues. We review the main game-theoretic\nnetwork centrality measures from both bodies of literature and organize them\ninto two categories: those that are more focused on the connectivity of nodes,\nand those that are more focused on the synergies achieved by nodes in groups.\nWe present and explain each centrality, with a focus on algorithms and\ncomplexity. \n\n"}
{"id": "1801.02042", "contents": "Title: Learning from Neighbors about a Changing State Abstract: Agents learn about a changing state using private signals and their\nneighbors' past estimates of the state. We present a model in which Bayesian\nagents in equilibrium use neighbors' estimates simply by taking weighted sums\nwith time-invariant weights. The dynamics thus parallel those of the tractable\nDeGroot model of learning in networks, but arise as an equilibrium outcome\nrather than a behavioral assumption. We examine whether information aggregation\nis nearly optimal as neighborhoods grow large. A key condition for this is\nsignal diversity: each individual's neighbors have private signals that not\nonly contain independent information, but also have sufficiently different\ndistributions. Without signal diversity $\\unicode{x2013}$ e.g., if private\nsignals are i.i.d. $\\unicode{x2013}$ learning is suboptimal in all networks and\nhighly inefficient in some. Turning to social influence, we find it is much\nmore sensitive to one's signal quality than to one's number of neighbors, in\ncontrast to standard models with exogenous updating rules. \n\n"}
{"id": "1801.02044", "contents": "Title: Multilabeled versions of Sperner's and Fan's lemmas and applications Abstract: We propose a general technique related to the polytopal Sperner lemma for\nproving old and new multilabeled versions of Sperner's lemma. A notable\napplication of this technique yields a cake-cutting theorem where the number of\nplayers and the number of pieces can be independently chosen. We also prove\nmultilabeled versions of Fan's lemma, a combinatorial analogue of the\nBorsuk-Ulam theorem, and exhibit applications to fair division and graph\ncoloring. \n\n"}
{"id": "1801.02161", "contents": "Title: Metastability in Stochastic Replicator Dynamics Abstract: We consider a novel model of stochastic replicator dynamics for potential\ngames that converts to a Langevin equation on a sphere after a change of\nvariables. This is distinct from the models studied earlier. In particular, it\nis ill-posed due to non-uniqueness of solutions, but is amenable to a natural\nselection principle that picks a unique solution. The model allows us to make\nspecific statements regarding metastable states such as small noise asymptotics\nfor mean exit times from their domain of attraction, and quasi-stationary\nmeasures. We illustrate the general results by specializing them to replicator\ndynamics on graphs and demonstrate that the numerical experiments support\ntheoretical predictions. \n\n"}
{"id": "1801.02263", "contents": "Title: Seasonal Goods and Spoiled Milk: Pricing for a Limited Shelf-Life Abstract: We examine the case of items with a limited shelf-life where storing an item\n(before consumption) may carry a cost to a buyer (or distributor). For example,\neggs, milk, or Groupon coupons have a fixed expiry date, and seasonal goods can\nsuffer a decrease in value. We show how this setting contrasts with recent\nresults by Berbeglia et al (arXiv:1509.07330(v5)) for items with infinite\nshelf-life.\n  We prove tight bounds on the seller's profits showing how they relate to the\nitems' shelf-life. We show, counterintuitively, that in our limited shelf-life\nsetting, increasing storage costs can sometimes lead to less profit for the\nseller which cannot happen when items have unlimited shelf-life. We also\nprovide an algorithm that calculates optimal prices. Finally, we examine\nempirically the relationship between profits and buyer utility as the storage\ncost and shelf-life duration change, and observe properties, some of which are\nunique to the limited shelf-life setting. \n\n"}
{"id": "1801.03459", "contents": "Title: Sequential decomposition of repeated games with asymmetric information\n  and dependent states Abstract: We consider a finite horizon repeated game with $N$ selfish players who\nobserve their types privately and take actions, which are publicly observed.\nTheir actions and types jointly determine their instantaneous rewards. In each\nperiod, players jointly observe actions of each other with delay 1, and private\nobservations of the state of the system, and get an instantaneous reward which\nis a function of the state and everyone's actions. The players' types are\nstatic and are potentially correlated among players.\n  An appropriate notion of equilibrium for such games is Perfect Bayesian\nEquilibrium (PBE) which consists of a strategy and a belief profile of the\nplayers which is coupled across time and as a result, the complexity of finding\nsuch equilibria grows double-exponentially in time. We present a sequential\ndecomposition methodology to compute \\emph{structured perfect Bayesian\nequilibria} (SPBE) of this game, introduced in~\\cite{VaAn15arxiv}, where\nequilibrium policy of a player is a function of a common belief and a private\nstate. This methodology computes SPBE in linear time. In general, the SPBE of\nthe game problem exhibit \\textit{signaling} behavior, i.e. players' actions\nreveal part of their private information that is payoff relevant to other\nplayers. \n\n"}
{"id": "1801.04403", "contents": "Title: Social Advantage with Mixed Entangled States Abstract: It has been extensively shown in past literature that Bayesian Game Theory\nand Quantum Non-locality have strong ties between them. Pure Entangled States\nhave been used, in both common and conflict interest games, to gain\nadvantageous payoffs, both at the individual and social level. In this paper we\nconstruct a game for a Mixed Entangled State such that this state gives higher\npayoffs than classically possible, both at the individual level and the social\nlevel. Also, we use the I-3322 inequality so that states that aren't helpful as\nadvice for Bell-CHSH inequality can also be used. Finally, the measurement\nsetting we use is a Restricted Social Welfare Strategy (given this particular\nstate). \n\n"}
{"id": "1801.04497", "contents": "Title: Near-optimal approximation algorithm for simultaneous Max-Cut Abstract: In the simultaneous Max-Cut problem, we are given $k$ weighted graphs on the\nsame set of $n$ vertices, and the goal is to find a cut of the vertex set so\nthat the minimum, over the $k$ graphs, of the cut value is as large as\npossible. Previous work [BKS15] gave a polynomial time algorithm which achieved\nan approximation factor of $1/2 - o(1)$ for this problem (and an approximation\nfactor of $1/2 + \\epsilon_k$ in the unweighted case, where $\\epsilon_k\n\\rightarrow 0$ as $k \\rightarrow \\infty$).\n  In this work, we give a polynomial time approximation algorithm for\nsimultaneous Max-Cut with an approximation factor of $0.8780$ (for all constant\n$k$). The natural SDP formulation for simultaneous Max-Cut was shown to have an\nintegrality gap of $1/2+\\epsilon_k$ in [BKS15]. In achieving the better\napproximation guarantee, we use a stronger Sum-of-Squares hierarchy SDP\nrelaxation and a rounding algorithm based on Raghavendra-Tan [RT12], in\naddition to techniques from [BKS15]. \n\n"}
{"id": "1801.04544", "contents": "Title: Hire the Experts: Combinatorial Auction Based Scheme for Experts\n  Selection in E-Healthcare Abstract: During the last decade, scheduling the healthcare services (such as staffs\nand OTs) inside the hospitals have assumed a central role in healthcare.\nRecently, some works are addressed in the direction of hiring the expert\nconsultants (mainly doctors) for the critical healthcare scenarios from outside\nof the medical unit, in both strategic and non-strategic settings under\nmonetary and non-monetary perspectives. In this paper, we have tried to\ninvestigate the experts hiring problem with multiple patients and multiple\nexperts; where each patient reports a preferred set of experts which is private\ninformation alongwith their private cost for consultancy. To the best of our\nknowledge, this is the first step in the direction of modeling the experts\nhiring problem in the combinatorial domain. In this paper, the combinatorial\nauction based scheme is proposed for hiring experts from outside of the\nhospitals to have expertise by the preferred doctors set to the patients. \n\n"}
{"id": "1801.05500", "contents": "Title: Cellular-Connected UAVs over 5G: Deep Reinforcement Learning for\n  Interference Management Abstract: In this paper, an interference-aware path planning scheme for a network of\ncellular-connected unmanned aerial vehicles (UAVs) is proposed. In particular,\neach UAV aims at achieving a tradeoff between maximizing energy efficiency and\nminimizing both wireless latency and the interference level caused on the\nground network along its path. The problem is cast as a dynamic game among\nUAVs. To solve this game, a deep reinforcement learning algorithm, based on\necho state network (ESN) cells, is proposed. The introduced deep ESN\narchitecture is trained to allow each UAV to map each observation of the\nnetwork state to an action, with the goal of minimizing a sequence of\ntime-dependent utility functions. Each UAV uses ESN to learn its optimal path,\ntransmission power level, and cell association vector at different locations\nalong its path. The proposed algorithm is shown to reach a subgame perfect Nash\nequilibrium (SPNE) upon convergence. Moreover, an upper and lower bound for the\naltitude of the UAVs is derived thus reducing the computational complexity of\nthe proposed algorithm. Simulation results show that the proposed scheme\nachieves better wireless latency per UAV and rate per ground user (UE) while\nrequiring a number of steps that is comparable to a heuristic baseline that\nconsiders moving via the shortest distance towards the corresponding\ndestinations. The results also show that the optimal altitude of the UAVs\nvaries based on the ground network density and the UE data rate requirements\nand plays a vital role in minimizing the interference level on the ground UEs\nas well as the wireless transmission delay of the UAV. \n\n"}
{"id": "1801.07215", "contents": "Title: Get Your Workload in Order: Game Theoretic Prioritization of Database\n  Auditing Abstract: For enhancing the privacy protections of databases, where the increasing\namount of detailed personal data is stored and processed, multiple mechanisms\nhave been developed, such as audit logging and alert triggers, which notify\nadministrators about suspicious activities; however, the two main limitations\nin common are: 1) the volume of such alerts is often substantially greater than\nthe capabilities of resource-constrained organizations, and 2) strategic\nattackers may disguise their actions or carefully choosing which records they\ntouch, making incompetent the statistical detection models. For solving them,\nwe introduce a novel approach to database auditing that explicitly accounts for\nadversarial behavior by 1) prioritizing the order in which types of alerts are\ninvestigated and 2) providing an upper bound on how much resource to allocate\nfor each type. We model the interaction between a database auditor and\npotential attackers as a Stackelberg game in which the auditor chooses an\nauditing policy and attackers choose which records to target. A corresponding\napproach combining linear programming, column generation, and heuristic search\nis proposed to derive an auditing policy. For testing the policy-searching\nperformance, a publicly available credit card application dataset are adopted,\non which it shows that our methods produce high-quality mixed strategies as\ndatabase audit policies, and our general approach significantly outperforms\nnon-game-theoretic baselines. \n\n"}
{"id": "1801.08341", "contents": "Title: The Price of Indivisibility in Cake Cutting Abstract: We consider the problem of envy-free cake cutting, which is the distribution\nof a continuous heterogeneous resource among self interested players such that\nnobody prefers what somebody else receives to what they get. Existing work has\nfocused on two distinct classes of solutions to this problem - allocations\nwhich give each player a continuous piece of cake and allocations which give\neach player arbitrarily many disjoint pieces of cake. Our aim is to investigate\nallocations between these two extremes by parameterizing the maximum number of\ndisjoint pieces each player may receive. We characterize the Price of\nIndivisibility (POI) as the gain achieved in social welfare (utilitarian and\negalitarian), by moving from allocations which give each player a continuous\npiece of cake to allocations that may give each player up to $k$ disjoint\npieces of cake. Our results contain bounds for the Price of Indivisibility for\nutilitarian as well as egalitarian social welfare, and for envy-free cake\ncutting as well as cake cutting without any fairness constraints. \n\n"}
{"id": "1801.08709", "contents": "Title: Adaptive Lower Bound for Testing Monotonicity on the Line Abstract: In the property testing model, the task is to distinguish objects possessing\nsome property from the objects that are far from it. One of such properties is\nmonotonicity, when the objects are functions from one poset to another. This is\nan active area of research. In this paper we study query complexity of\n$\\epsilon$-testing monotonicity of a function $f\\colon [n]\\to[r]$. All our\nlower bounds are for adaptive two-sided testers.\n  * We prove a nearly tight lower bound for this problem in terms of $r$. The\nbound is $\\Omega(\\frac{\\log r}{\\log \\log r})$ when $\\epsilon = 1/2$. No\nprevious satisfactory lower bound in terms of $r$ was known.\n  * We completely characterise query complexity of this problem in terms of $n$\nfor smaller values of $\\epsilon$. The complexity is $\\Theta(\\epsilon^{-1} \\log\n(\\epsilon n))$. Apart from giving the lower bound, this improves on the best\nknown upper bound.\n  Finally, we give an alternative proof of the $\\Omega(\\epsilon^{-1}d\\log n -\n\\epsilon^{-1}\\log\\epsilon^{-1})$ lower bound for testing monotonicity on the\nhypergrid $[n]^d$ due to Chakrabarty and Seshadhri (RANDOM'13). \n\n"}
{"id": "1801.08808", "contents": "Title: Learning Optimal Redistribution Mechanisms through Neural Networks Abstract: We consider a setting where $p$ public resources are to be allocated among\n$n$ competing and strategic agents so as to maximize social welfare (the\nobjects should be allocated to those who value them the most). This is called\nallocative efficiency (AE). We need the agents to report their valuations for\nobtaining these resources, truthfully referred to as dominant strategy\nincentive compatibility (DSIC). We use auction-based mechanisms to achieve AE\nand DSIC yet budget balance cannot be ensured, due to Green-Laffont\nImpossibility Theorem. That is, the net transfer of money cannot be zero. This\nproblem has been addressed by designing a redistribution mechanism so as to\nensure a minimum surplus of money as well as AE and DSIC. The objective could\nbe to minimize surplus in expectation or in the worst case and these $p$\nobjects could be homogeneous or heterogeneous. Designing redistribution\nmechanisms which perform well in expectation becomes analytically challenging\nfor heterogeneous settings. In this paper, we take a completely different,\ndata-driven approach. We train a neural network to determine an optimal\nredistribution mechanism based on given settings with both the objectives,\noptimal in expectation and optimal in the worst case. We also propose a loss\nfunction to train a neural network to optimize worst case. We design neural\nnetworks with the underlying rebate functions being linear as well as nonlinear\nin terms of bids of the agents. Our networks' performances are same as the\ntheoretical guarantees for the cases where it has been solved. We observe that\na neural network based redistribution mechanism for homogeneous settings which\nuses nonlinear rebate functions outperforms linear rebate functions when the\nobjective is optimal in expectation. Our approach also yields an optimal in\nexpectation redistribution mechanism for heterogeneous settings. \n\n"}
{"id": "1801.09346", "contents": "Title: Representing the Insincere: Strategically Robust Proportional\n  Representation Abstract: Proportional representation (PR) is a fundamental principle of many\ndemocracies world-wide which employ PR-based voting rules to elect their\nrepresentatives. The normative properties of these voting rules however, are\noften only understood in the context of sincere voting.\n  In this paper we consider PR in the presence of strategic voters. We\nconstruct a voting rule such that for every preference profile there exists at\nleast one costly voting equilibrium satisfying PR with respect to voters'\nprivate and unrevealed preferences - such a voting rule is said to be\nstrategically robust. In contrast, a commonly applied voting rule is shown not\nbe strategically robust. Furthermore, we prove a limit on `how strategically\nrobust' a PR-based voting rule can be; we show that there is no PR-based voting\nrule which ensures that every equilibrium satisfies PR. Collectively, our\nresults highlight the possibility and limit of achieving PR in the presence of\nstrategic voters and a positive role for mechanisms, such as pre-election\npolls, which coordinate voter behaviour towards equilibria which satisfy PR. \n\n"}
{"id": "1802.02050", "contents": "Title: Optimal Data Reduction for Graph Coloring Using Low-Degree Polynomials Abstract: The theory of kernelization can be used to rigorously analyze data reduction\nfor graph coloring problems. Here, the aim is to reduce a q-Coloring input to\nan equivalent but smaller input whose size is provably bounded in terms of\nstructural properties, such as the size of a minimum vertex cover. In this\npaper we settle two open problems about data reduction for q-Coloring.\n  First, we obtain a kernel of bitsize $O(k^{q-1}\\log{k})$ for q-Coloring\nparameterized by Vertex Cover, for any q >= 3. This size bound is optimal up to\n$k^{o(1)}$ factors assuming NP is not a subset of coNP/poly, and improves on\nthe previous-best kernel of size $O(k^q)$. We generalize this result for\ndeciding q-colorability of a graph G, to deciding the existence of a\nhomomorphism from G to an arbitrary fixed graph H. Furthermore, we can replace\nthe parameter vertex cover by the less restrictive parameter twin-cover. We\nprove that H-Coloring parameterized by Twin-Cover has a kernel of size\n$O(k^{\\Delta(H)}\\log k)$.\n  Our second result shows that 3-Coloring does not admit non-trivial\nsparsification: assuming NP is not a subset of coNP/poly, the parameterization\nby the number of vertices n admits no (generalized) kernel of size $O(n^{2-e})$\nfor any e > 0. Previously, such a lower bound was only known for coloring with\nq >= 4 colors. \n\n"}
{"id": "1802.02618", "contents": "Title: A Diversity-based Substation Cyber Defense Strategy utilizing Coloring\n  Games Abstract: Growing cybersecurity risks in the power grid require that utilities\nimplement a variety of security mechanism (SM) composed mostly of VPNs,\nfirewalls, or other custom security components. While they provide some\nprotection, they might contain software vulnerabilities which can lead to a\ncyber-attack. In this paper, the severity of a cyber-attack has been decreased\nby employing a diverse set of SM that reduce repetition of a single\nvulnerability. This paper focuses on the allocation of diverse SM and tries to\nincrease the security of the cyber assets located within the electronic\nsecurity perimeter(ESP) of a substation. We have used a graph-based coloring\ngame in a distributed manner to allocate diverse SM for protecting the cyber\nassets. The vulnerability assessment for power grid network is also analyzed\nusing this game theoretic method. An improved, diversified SMs for worst-case\nscenario has been demonstrated by reaching the Nash equilibrium of graph\ncoloring game. As a case study, we analyze the IEEE-14 and IEEE-118 bus system,\nobserve the different distributed coloring algorithm for allocating diverse SM\nand calculating the overall network criticality. \n\n"}
{"id": "1802.02751", "contents": "Title: Monopoly pricing with buyer search Abstract: In many shopping scenarios, e.g., in online shopping, customers have a large\nmenu of options to choose from. However, most of the buyers do not browse all\nthe options and make decision after considering only a small part of the menu.\nTo study such buyer's behavior we consider the standard Bayesian monopoly\nproblem for a unit-demand buyer, where the monopolist displays the menu\ndynamically page after a page to the buyer. The seller aims to maximize the\nexpected revenue over the distribution of buyer's values which we assume are\ni.i.d. The buyer incurs a fixed cost for browsing through one menu page and\nwould stop if that cost exceeds the increase in her utility. We observe that\nthe optimal posted price mechanism in our dynamic setting may have quite\ndifferent structure than in the classic static scenario. We find a (relatively)\nsimple and approximately optimal mechanism, that uses part of the items as a\n\"bait\" to keep the buyer interested for multiple rounds with low prices, while\nat the same time showing many other expensive items. \n\n"}
{"id": "1802.02850", "contents": "Title: Detection Games Under Fully Active Adversaries Abstract: We study a binary hypothesis testing problem in which a defender must decide\nwhether or not a test sequence has been drawn from a given memoryless source\n$P_0$ whereas, an attacker strives to impede the correct detection. With\nrespect to previous works, the adversarial setup addressed in this paper\nconsiders an attacker who is active under both hypotheses, namely, a fully\nactive attacker, as opposed to a partially active attacker who is active under\none hypothesis only. In the fully active setup, the attacker distorts sequences\ndrawn both from $P_0$ and from an alternative memoryless source $P_1$, up to a\ncertain distortion level, which is possibly different under the two hypotheses,\nin order to maximize the confusion in distinguishing between the two sources,\ni.e., to induce both false positive and false negative errors at the detector,\nalso referred to as the defender. We model the defender-attacker interaction as\na game and study two versions of this game, the Neyman-Pearson game and the\nBayesian game. Our main result is in the characterization of an attack strategy\nthat is asymptotically both dominant (i.e., optimal no matter what the\ndefender's strategy is) and universal, i.e., independent of $P_0$ and $P_1$.\nFrom the analysis of the equilibrium payoff, we also derive the best achievable\nperformance of the defender, by relaxing the requirement on the exponential\ndecay rate of the false positive error probability in the Neyman--Pearson setup\nand the tradeoff between the error exponents in the Bayesian setup. Such\nanalysis permits to characterize the conditions for the distinguishability of\nthe two sources given the distortion levels. \n\n"}
{"id": "1802.04670", "contents": "Title: Equilibrium solutions of three player Kuhn poker with $N>3$ cards: A new\n  numerical method using regularization and arc-length continuation Abstract: We study the equilibrium solutions of three player Kuhn poker with $N>3$\ncards. We compute these solutions as a function of the initial pot size, $P$,\nusing a novel method based on regularizing the system of polynomial equations\nand inequalities that defines the solutions, and solving the resulting system\nof nonlinear, algebraic equations using a combination of Newton's method and\narc-length continuation. We find that the structure of the equilibrium solution\ncurve is very complex, even for games with a small number of cards. Standard\nthree player Kuhn poker, which is played with $N=4$ cards, is qualitatively\ndifferent from the game with $N>4$ cards because of the simplicity of the\nstructure of the value betting and bluffing ranges of each player. When $N>5$,\nwe find that there is a new type of equilibrium bet with midrange cards that\nacts as a bluff against one player and a value bet against the other. \n\n"}
{"id": "1802.04966", "contents": "Title: Destination Choice Game: A Spatial Interaction Theory on Human Mobility Abstract: With remarkable significance in migration prediction, global disease\nmitigation, urban planning and many others, an arresting challenge is to\npredict human mobility fluxes between any two locations. A number of methods\nhave been proposed against the above challenge, including the gravity model,\nthe intervening opportunity model, the radiation model, the population-weighted\nopportunity model, and so on. Despite their theoretical elegance, all models\nignored an intuitive and important ingredient in individual decision about\nwhere to go, that is, the possible congestion on the way and the possible\ncrowding in the destination. Here we propose a microscopic mechanism underlying\nmobility decisions, named destination choice game (DCG), which takes into\naccount the crowding effects resulted from spatial interactions among\nindividuals. In comparison with the state-of-the-art models, the present one\nshows more accurate prediction on mobility fluxes across wide scales from\nintracity trips to intercity travels, and further to internal migrations. The\nwell-known gravity model is proved to be the equilibrium solution of a\ndegenerated DCG neglecting the crowding effects in the destinations. \n\n"}
{"id": "1802.06132", "contents": "Title: Interaction Matters: A Note on Non-asymptotic Local Convergence of\n  Generative Adversarial Networks Abstract: Motivated by the pursuit of a systematic computational and algorithmic\nunderstanding of Generative Adversarial Networks (GANs), we present a simple\nyet unified non-asymptotic local convergence theory for smooth two-player\ngames, which subsumes several discrete-time gradient-based saddle point\ndynamics. The analysis reveals the surprising nature of the off-diagonal\ninteraction term as both a blessing and a curse. On the one hand, this\ninteraction term explains the origin of the slow-down effect in the convergence\nof Simultaneous Gradient Ascent (SGA) to stable Nash equilibria. On the other\nhand, for the unstable equilibria, exponential convergence can be proved thanks\nto the interaction term, for four modified dynamics proposed to stabilize GAN\ntraining: Optimistic Mirror Descent (OMD), Consensus Optimization (CO),\nImplicit Updates (IU) and Predictive Method (PM). The analysis uncovers the\nintimate connections among these stabilizing techniques, and provides detailed\ncharacterization on the choice of learning rate. As a by-product, we present a\nnew analysis for OMD proposed in Daskalakis, Ilyas, Syrgkanis, and Zeng [2017]\nwith improved rates. \n\n"}
{"id": "1802.06440", "contents": "Title: Capacitated Dynamic Programming: Faster Knapsack and Graph Algorithms Abstract: One of the most fundamental problems in Computer Science is the Knapsack\nproblem. Given a set of n items with different weights and values, it asks to\npick the most valuable subset whose total weight is below a capacity threshold\nT. Despite its wide applicability in various areas in Computer Science,\nOperations Research, and Finance, the best known running time for the problem\nis O(Tn). The main result of our work is an improved algorithm running in time\nO(TD), where D is the number of distinct weights. Previously, faster runtimes\nfor Knapsack were only possible when both weights and values are bounded by M\nand V respectively, running in time O(nMV) [Pisinger'99]. In comparison, our\nalgorithm implies a bound of O(nM^2) without any dependence on V, or O(nV^2)\nwithout any dependence on M. Additionally, for the unbounded Knapsack problem,\nwe provide an algorithm running in time O(M^2) or O(V^2). Both our algorithms\nmatch recent conditional lower bounds shown for the Knapsack problem [Cygan et\nal'17, K\\\"unnemann et al'17].\n  We also initiate a systematic study of general capacitated dynamic\nprogramming, of which Knapsack is a core problem. This problem asks to compute\nthe maximum weight path of length k in an edge- or node-weighted directed\nacyclic graph. In a graph with m edges, these problems are solvable by dynamic\nprogramming in time O(km), and we explore under which conditions the dependence\non k can be eliminated. We identify large classes of graphs where this is\npossible and apply our results to obtain linear time algorithms for the problem\nof k-sparse Delta-separated sequences. The main technical innovation behind our\nresults is identifying and exploiting concavity that appears in relaxations and\nsubproblems of the tasks we consider. \n\n"}
{"id": "1802.06483", "contents": "Title: Committee Scoring Rules: Axiomatic Characterization and Hierarchy Abstract: Committee scoring voting rules are multiwinner analogues of positional\nscoring rules which constitute an important subclass of single-winner voting\nrules. We identify several natural subclasses of committee scoring rules,\nnamely, weakly separable, representation-focused, top-$k$-counting, OWA-based,\nand decomposable rules. We characterize SNTV, Bloc, and $k$-Approval\nChamberlin--Courant as the only nontrivial rules in pairwise intersections of\nthese classes. We provide some axiomatic characterizations for these classes,\nwhere monotonicity properties appear to be especially useful. The class of\ndecomposable rules is new to the literature. We show that it strictly contains\nthe class of OWA-based rules and describe some of the applications of\ndecomposable rules. \n\n"}
{"id": "1802.07187", "contents": "Title: Leader-follower based Coalition Formation in Large-scale UAV Networks, A\n  Quantum Evolutionary Approach Abstract: The problem of decentralized multiple Point of Interests (PoIs) detection and\nassociated task completion in an unknown environment with multiple\nresource-constrained and self-interested Unmanned Aerial Vehicles (UAVs) is\nstudied. The UAVs form several coalitions to efficiently complete the compound\ntasks which are impossible to be performed individually. The objectives of such\ncoalition formation are to firstly minimize resource consumption in completing\nthe encountered tasks on time, secondly to enhance the reliability of the\ncoalitions, and lastly in segregating the most trusted UAVs amid the self\ninterested of them. As many previous publications have merely focused on\nminimizing costs, this study considers a multi-objective optimization coalition\nformation problem that considers the three aforementioned objectives. In doing\nso, a leader-follower- inspired coalition formation algorithm amalgamating the\nthree objectives to address the problem of the computational complexity of\ncoalition formation in large-scale UAV networks is proposed. This algorithm\nattempts to form the coalitions with minimally exceeding the required resources\nfor the encountered tasks while maximizing the number of completed tasks. The\nproposed algorithm is based on Quantum Evolutionary Algorithms(QEA) which are a\ncombination of quantum computing and evolutionary algorithms. Results from\nsimulations show that the proposed algorithm significantly outperforms the\nexisting coalition formation algorithms such as merge-and-split and a famous\nmulti-objective genetic algorithm called NSGA-II. \n\n"}
{"id": "1802.07407", "contents": "Title: Third-Party Data Providers Ruin Simple Mechanisms Abstract: Motivated by the growing prominence of third-party data providers in online\nmarketplaces, this paper studies the impact of the presence of third-party data\nproviders on mechanism design. When no data provider is present, it has been\nshown that simple mechanisms are \"good enough\" -- they can achieve a constant\nfraction of the revenue of optimal mechanisms. The results in this paper\ndemonstrate that this is no longer true in the presence of a third-party data\nprovider who can provide the bidder with a signal that is correlated with the\nitem type. Specifically, even with a single seller, a single bidder, and a\nsingle item of uncertain type for sale, the strategies of pricing each\nitem-type separately (the analog of item pricing for multi-item auctions) and\nbundling all item-types under a single price (the analog of grand bundling) can\nboth simultaneously be a logarithmic factor worse than the optimal revenue.\nFurther, in the presence of a data provider, item-type partitioning\nmechanisms---a more general class of mechanisms which divide item-types into\ndisjoint groups and offer prices for each group---still cannot achieve within a\n$\\log \\log$ factor of the optimal revenue. Thus, our results highlight that the\npresence of a data-provider forces the use of more complicated mechanisms in\norder to achieve a constant fraction of the optimal revenue. \n\n"}
{"id": "1802.08351", "contents": "Title: A Cut-And-Choose Mechanism to Prevent Gerrymandering Abstract: This paper presents a novel mechanism to endogenously determine the fair\ndivision of a state into electoral districts in a two-party setting. No\ngeometric constraints are imposed on voter distributions or district shapes;\ninstead, it is assumed that any partition of the population into districts of\nequal population is feasible. One party divides the map, then the other party\nchooses a minimum threshold level of support needed to win a district.\nDistricts in which neither party meets this threshold are awarded randomly.\nDespite the inherent asymmetry, the equilibria of this mechanism always yield\nfair outcomes, up to integer rounding. \n\n"}
{"id": "1802.08563", "contents": "Title: The Parameterized Hardness of the k-Center Problem in Transportation\n  Networks Abstract: In this paper we study the hardness of the $k$-Center problem on inputs that\nmodel transportation networks. For the problem, a graph $G=(V,E)$ with edge\nlengths and an integer $k$ are given and a center set $C\\subseteq V$ needs to\nbe chosen such that $|C|\\leq k$. The aim is to minimize the maximum distance of\nany vertex in the graph to the closest center. This problem arises in many\napplications of logistics, and thus it is natural to consider inputs that model\ntransportation networks. Such inputs are often assumed to be planar graphs, low\ndoubling metrics, or bounded highway dimension graphs. For each of these\nmodels, parameterized approximation algorithms have been shown to exist. We\ncomplement these results by proving that the $k$-Center problem is W[1]-hard on\nplanar graphs of constant doubling dimension, where the parameter is the\ncombination of the number of centers $k$, the highway dimension $h$, and the\npathwidth $p$. Moreover, under the Exponential Time Hypothesis there is no\n$f(k,p,h)\\cdot n^{o(p+\\sqrt{k+h})}$ time algorithm for any computable function\n$f$. Thus it is unlikely that the optimum solution to $k$-Center can be found\nefficiently, even when assuming that the input graph abides to all of the above\nmodels for transportation networks at once!\n  Additionally we give a simple parameterized $(1+\\varepsilon)$-approximation\nalgorithm for inputs of doubling dimension $d$ with runtime\n$(k^k/\\varepsilon^{O(kd)})\\cdot n^{O(1)}$. This generalizes a previous result,\nwhich considered inputs in $D$-dimensional $L_q$ metrics. \n\n"}
{"id": "1802.09001", "contents": "Title: The Complexity of the Possible Winner Problem over Partitioned\n  Preferences Abstract: The Possible-Winner problem asks, given an election where the voters'\npreferences over the set of candidates is partially specified, whether a\ndistinguished candidate can become a winner. In this work, we consider the\ncomputational complexity of Possible-Winner under the assumption that the voter\npreferences are $partitioned$. That is, we assume that every voter provides a\ncomplete order over sets of incomparable candidates (e.g., candidates are\nranked by their level of education). We consider elections with partitioned\nprofiles over positional scoring rules, with an unbounded number of candidates,\nand unweighted voters. Our first result is a polynomial time algorithm for\nvoting rules with $2$ distinct values, which include the well-known\n$k$-approval voting rule. We then go on to prove NP-hardness for a class of\nrules that contain all voting rules that produce scoring vectors with at least\n$4$ distinct values. \n\n"}
{"id": "1802.09158", "contents": "Title: Surrogate Scoring Rules Abstract: Strictly proper scoring rules (SPSR) are incentive compatible for eliciting\ninformation about random variables from strategic agents when the principal can\nreward agents after the realization of the random variables. They also quantify\nthe quality of elicited information, with more accurate predictions receiving\nhigher scores in expectation. In this paper, we extend such scoring rules to\nsettings where a principal elicits private probabilistic beliefs but only has\naccess to agents' reports. We name our solution \\emph{Surrogate Scoring Rules}\n(SSR). SSR build on a bias correction step and an error rate estimation\nprocedure for a reference answer defined using agents' reports. We show that,\nwith a single bit of information about the prior distribution of the random\nvariables, SSR in a multi-task setting recover SPSR in expectation, as if\nhaving access to the ground truth. Therefore, a salient feature of SSR is that\nthey quantify the quality of information despite the lack of ground truth, just\nas SPSR do for the setting \\emph{with} ground truth. As a by-product, SSR\ninduce \\emph{dominant truthfulness} in reporting. Our method is verified both\ntheoretically and empirically using data collected from real human forecasters. \n\n"}
{"id": "1802.09396", "contents": "Title: Attraction versus Persuasion: Information Provision in Search Markets Abstract: We consider a model of oligopolistic competition in a market with search\nfrictions, in which competing firms with products of unknown quality advertise\nhow much information a consumer's visit will glean. In the unique symmetric\nequilibrium of this game, the countervailing incentives of attraction and\npersuasion yield a payoff function for each firm that is linear in the firm's\nrealized effective value. If the expected quality of the products is\nsufficiently high (or competition is sufficiently fierce), this corresponds to\nfull information--firms provide the first-best level of information. If not,\nthis corresponds to information dispersion--firms randomize over signals. \n\n"}
{"id": "1802.09490", "contents": "Title: Controlling Human Utilization of Failure-Prone Systems via Taxes Abstract: We consider a game-theoretic model where individuals compete over a shared\nfailure-prone system or resource. We investigate the effectiveness of a\ntaxation mechanism in controlling the utilization of the resource at the Nash\nequilibrium when the decision-makers have behavioral risk preferences, captured\nby prospect theory. We first observe that heterogeneous prospect-theoretic risk\npreferences can lead to counter-intuitive outcomes. In particular, for\nresources that exhibit network effects, utilization can increase under taxation\nand there may not exist a tax rate that achieves the socially optimal level of\nutilization. We identify conditions under which utilization is monotone and\ncontinuous, and then characterize the range of utilizations that can be\nachieved by a suitable choice of tax rate. We further show that resource\nutilization is higher when players are charged differentiated tax rates\ncompared to the case when all players are charged an identical tax rate, under\nsuitable assumptions. \n\n"}
{"id": "1803.00162", "contents": "Title: Towards Cooperation in Sequential Prisoner's Dilemmas: a Deep Multiagent\n  Reinforcement Learning Approach Abstract: The Iterated Prisoner's Dilemma has guided research on social dilemmas for\ndecades. However, it distinguishes between only two atomic actions: cooperate\nand defect. In real-world prisoner's dilemmas, these choices are temporally\nextended and different strategies may correspond to sequences of actions,\nreflecting grades of cooperation. We introduce a Sequential Prisoner's Dilemma\n(SPD) game to better capture the aforementioned characteristics. In this work,\nwe propose a deep multiagent reinforcement learning approach that investigates\nthe evolution of mutual cooperation in SPD games. Our approach consists of two\nphases. The first phase is offline: it synthesizes policies with different\ncooperation degrees and then trains a cooperation degree detection network. The\nsecond phase is online: an agent adaptively selects its policy based on the\ndetected degree of opponent cooperation. The effectiveness of our approach is\ndemonstrated in two representative SPD 2D games: the Apple-Pear game and the\nFruit Gathering game. Experimental results show that our strategy can avoid\nbeing exploited by exploitative opponents and achieve cooperation with\ncooperative opponents. \n\n"}
{"id": "1803.00607", "contents": "Title: Optimization-Based Algorithm for Evolutionarily Stable Strategies\n  against Pure Mutations Abstract: Evolutionarily stable strategy (ESS) is an important solution concept in game\ntheory which has been applied frequently to biological models. Informally an\nESS is a strategy that if followed by the population cannot be taken over by a\nmutation strategy that is initially rare. Finding such a strategy has been\nshown to be difficult from a theoretical complexity perspective. We present an\nalgorithm for the case where mutations are restricted to pure strategies, and\npresent experiments on several game classes including random and a\nrecently-proposed cancer model. Our algorithm is based on a mixed-integer\nnon-convex feasibility program formulation, which constitutes the first general\noptimization formulation for this problem. It turns out that the vast majority\nof the games included in the experiments contain ESS with small support, and\nour algorithm is outperformed by a support-enumeration based approach. However\nwe suspect our algorithm may be useful in the future as games are studied that\nhave ESS with potentially larger and unknown support size. \n\n"}
{"id": "1803.00796", "contents": "Title: Fine-Grained Complexity of Analyzing Compressed Data: Quantifying\n  Improvements over Decompress-And-Solve Abstract: Can we analyze data without decompressing it? As our data keeps growing,\nunderstanding the time complexity of problems on compressed inputs, rather than\nin convenient uncompressed forms, becomes more and more relevant. Suppose we\nare given a compression of size $n$ of data that originally has size $N$, and\nwe want to solve a problem with time complexity $T(\\cdot)$. The naive strategy\nof \"decompress-and-solve\" gives time $T(N)$, whereas \"the gold standard\" is\ntime $T(n)$: to analyze the compression as efficiently as if the original data\nwas small.\n  We restrict our attention to data in the form of a string (text, files,\ngenomes, etc.) and study the most ubiquitous tasks. While the challenge might\nseem to depend heavily on the specific compression scheme, most methods of\npractical relevance (Lempel-Ziv-family, dictionary methods, and others) can be\nunified under the elegant notion of Grammar Compressions. A vast literature,\nacross many disciplines, established this as an influential notion for\nAlgorithm design.\n  We introduce a framework for proving (conditional) lower bounds in this\nfield, allowing us to assess whether decompress-and-solve can be improved, and\nby how much. Our main results are:\n  - The $O(nN\\sqrt{\\log{N/n}})$ bound for LCS and the $O(\\min\\{N \\log N, nM\\})$\nbound for Pattern Matching with Wildcards are optimal up to $N^{o(1)}$ factors,\nunder the Strong Exponential Time Hypothesis. (Here, $M$ denotes the\nuncompressed length of the compressed pattern.)\n  - Decompress-and-solve is essentially optimal for Context-Free Grammar\nParsing and RNA Folding, under the $k$-Clique conjecture.\n  - We give an algorithm showing that decompress-and-solve is not optimal for\nDisjointness. \n\n"}
{"id": "1803.00916", "contents": "Title: Deep Learning for Signal Authentication and Security in Massive Internet\n  of Things Systems Abstract: Secure signal authentication is arguably one of the most challenging problems\nin the Internet of Things (IoT) environment, due to the large-scale nature of\nthe system and its susceptibility to man-in-the-middle and eavesdropping\nattacks. In this paper, a novel deep learning method is proposed for dynamic\nauthentication of IoT signals to detect cyber attacks. The proposed learning\nframework, based on a long short-term memory (LSTM) structure, enables the IoT\ndevices (IoTDs) to extract a set of stochastic features from their generated\nsignal and dynamically watermark these features into the signal. This method\nenables the cloud, which collects signals from the IoT devices, to effectively\nauthenticate the reliability of the signals. Moreover, in massive IoT\nscenarios, since the cloud cannot authenticate all the IoTDs simultaneously due\nto computational limitations, a game-theoretic framework is proposed to improve\nthe cloud's decision making process by predicting vulnerable IoTDs. The\nmixed-strategy Nash equilibrium (MSNE) for this game is derived and the\nuniqueness of the expected utility at the equilibrium is proven. In the massive\nIoT system, due to a large set of available actions for the cloud, it is shown\nthat analytically deriving the MSNE is challenging and, thus, a learning\nalgorithm proposed that converges to the MSNE. Moreover, in order to cope with\nthe incomplete information case in which the cloud cannot access the state of\nthe unauthenticated IoTDs, a deep reinforcement learning algorithm is proposed\nto dynamically predict the state of unauthenticated IoTDs and allow the cloud\nto decide on which IoTDs to authenticate. Simulation results show that, with an\nattack detection delay of under 1 second the messages can be transmitted from\nIoT devices with an almost 100% reliability. \n\n"}
{"id": "1803.02194", "contents": "Title: Bidding Machine: Learning to Bid for Directly Optimizing Profits in\n  Display Advertising Abstract: Real-time bidding (RTB) based display advertising has become one of the key\ntechnological advances in computational advertising. RTB enables advertisers to\nbuy individual ad impressions via an auction in real-time and facilitates the\nevaluation and the bidding of individual impressions across multiple\nadvertisers. In RTB, the advertisers face three main challenges when optimizing\ntheir bidding strategies, namely (i) estimating the utility (e.g., conversions,\nclicks) of the ad impression, (ii) forecasting the market value (thus the cost)\nof the given ad impression, and (iii) deciding the optimal bid for the given\nauction based on the first two. Previous solutions assume the first two are\nsolved before addressing the bid optimization problem. However, these\nchallenges are strongly correlated and dealing with any individual problem\nindependently may not be globally optimal. In this paper, we propose Bidding\nMachine, a comprehensive learning to bid framework, which consists of three\noptimizers dealing with each challenge above, and as a whole, jointly optimizes\nthese three parts. We show that such a joint optimization would largely\nincrease the campaign effectiveness and the profit. From the learning\nperspective, we show that the bidding machine can be updated smoothly with both\noffline periodical batch or online sequential training schemes. Our extensive\noffline empirical study and online A/B testing verify the high effectiveness of\nthe proposed bidding machine. \n\n"}
{"id": "1803.02409", "contents": "Title: On the parameterized complexity of manipulating Top Trading Cycles Abstract: We study the problem of exchange when 1) agents are endowed with\nheterogeneous indivisible objects, and 2) there is no money. In general, no\nrule satisfies the three central properties Pareto-efficiency, individual\nrationality, and strategy-proofness \\cite{Sonmez1999}. Recently, it was shown\nthat Top Trading Cycles is $\\NP$-hard to manipulate \\cite{FujitaEA2015}, a\nrelaxation of strategy-proofness. However, parameterized complexity is a more\nappropriate framework for this and other economic settings. Certain aspects of\nthe problem - number of objects each agent brings to the table, goods up for\nauction, candidates in an election \\cite{consandlang2007}, legislative figures\nto influence \\cite{christian2007complexity} - may face natural bounds or are\nfixed as the problem grows. We take a parameterized complexity approach to\nindivisible goods exchange for the first time. Our results represent good and\nbad news for TTC. When the size of the endowments $k$ is a fixed constant, we\nshow that the computational task of manipulating TTC can be performed in\npolynomial time. On the other hand, we show that this parameterized problem is\n$\\W[1]$-hard, and therefore unlikely to be \\emph{fixed parameter tractable}. \n\n"}
{"id": "1803.02751", "contents": "Title: Aspiration-based Perturbed Learning Automata Abstract: This paper introduces a novel payoff-based learning scheme for distributed\noptimization in repeatedly-played strategic-form games. Standard\nreinforcement-based learning exhibits several limitations with respect to their\nasymptotic stability. For example, in two-player coordination games,\npayoff-dominant (or efficient) Nash equilibria may not be stochastically\nstable. In this work, we present an extension of perturbed learning automata,\nnamely aspiration-based perturbed learning automata (APLA) that overcomes these\nlimitations. We provide a stochastic stability analysis of APLA in multi-player\ncoordination games. We further show that payoff-dominant Nash equilibria are\nthe only stochastically stable states. \n\n"}
{"id": "1803.03239", "contents": "Title: Fairness Through Computationally-Bounded Awareness Abstract: We study the problem of fair classification within the versatile framework of\nDwork et al. [ITCS '12], which assumes the existence of a metric that measures\nsimilarity between pairs of individuals. Unlike earlier work, we do not assume\nthat the entire metric is known to the learning algorithm; instead, the learner\ncan query this arbitrary metric a bounded number of times. We propose a new\nnotion of fairness called metric multifairness and show how to achieve this\nnotion in our setting. Metric multifairness is parameterized by a similarity\nmetric $d$ on pairs of individuals to classify and a rich collection ${\\cal C}$\nof (possibly overlapping) \"comparison sets\" over pairs of individuals. At a\nhigh level, metric multifairness guarantees that similar subpopulations are\ntreated similarly, as long as these subpopulations are identified within the\nclass ${\\cal C}$. \n\n"}
{"id": "1803.03451", "contents": "Title: Comparative Statics via Stochastic Orderings in a Two-Echelon Market\n  with Upstream Demand Uncertainty Abstract: We revisit the classic Cournot model and extend it to a two-echelon supply\nchain with an upstream supplier who operates under demand uncertainty and\nmultiple downstream retailers who compete over quantity. The supplier's belief\nabout retail demand is modeled via a continuous probability distribution\nfunction F. If F has the decreasing generalized mean residual life (DGMRL)\nproperty, then the supplier's optimal pricing policy exists and is the unique\nfixed point of the mean residual life (MRL) function. This closed form\nrepresentation of the supplier's equilibrium strategy facilitates a transparent\ncomparative statics and sensitivity analysis. We utilize the theory of\nstochastic orderings to study the response of the equilibrium fundamentals -\nwholesale price, retail price and quantity - to different demand distribution\nparameters. We examine supply chain performance, in terms of the distribution\nof profits, supply chain efficiency, in terms of the Price of Anarchy, and\ncomplement our findings with numerical results. \n\n"}
{"id": "1803.04244", "contents": "Title: The generalized stochastic preference choice model Abstract: We propose a new discrete choice model, called the generalized stochastic\npreference (GSP) model, that incorporates non-rationality into the stochastic\npreference (SP) choice model, also known as the rank- based choice model. Our\nmodel can explain several choice phenomena that cannot be represented by any SP\nmodel such as the compromise and attraction effects, but still subsumes the SP\nmodel class. The GSP model is defined as a distribution over consumer types,\nwhere each type extends the choice behavior of rational types in the SP model.\nWe build on existing methods for estimating the SP model and propose an\niterative estimation algorithm for the GSP model that finds new types by\nsolving a integer linear program in each iteration. We further show that our\nproposed notion of non-rationality can be incorporated into other choice\nmodels, like the random utility maximization (RUM) model class as well as any\nof its subclasses. As a concrete example, we introduce the non-rational\nextension of the classical MNL model, which we term the generalized MNL (GMNL)\nmodel and present an efficient expectation-maximization (EM) algorithm for\nestimating the GMNL model. Numerical evaluation on real choice data shows that\nthe GMNL and GSP models can outperform their rational counterparts in\nout-of-sample prediction accuracy. \n\n"}
{"id": "1803.04744", "contents": "Title: On Integer Programming, Discrepancy, and Convolution Abstract: Integer programs with m constraints are solvable in pseudo-polynomial time in\n$\\Delta$, the largest coefficient in a constraint, when m is a fixed constant.\nWe give a new algorithm with a running time of $O(\\sqrt{m}\\Delta)^{2m} +\nO(nm)$, which improves on the state-of-the-art. Moreover, we show that\nimproving on our algorithm for any $m$ is equivalent to improving over the\nquadratic time algorithm for $(\\min,~+)$-convolution. This is a strong evidence\nthat our algorithm's running time is the best possible. We also present a\nspecialized algorithm with running time $O(\\sqrt{m} \\Delta)^{(1 + o(1))m} +\nO(nm)$ for testing feasibility of an integer program and also give a tight\nlower bound, which is based on the SETH in this case. \n\n"}
{"id": "1803.05361", "contents": "Title: Approximating Generalized Network Design under (Dis)economies of Scale\n  with Applications to Energy Efficiency Abstract: In a generalized network design (GND) problem, a set of resources are\nassigned to multiple communication requests. Each request contributes its\nweight to the resources it uses and the total load on a resource is then\ntranslated to the cost it incurs via a resource specific cost function. For\nexample, a request may be to establish a virtual circuit, thus contributing to\nthe load on each edge in the circuit. Motivated by energy efficiency\napplications, recently, there is a growing interest in GND using cost functions\nthat exhibit (dis)economies of scale ((D)oS), namely, cost functions that\nappear subadditive for small loads and superadditive for larger loads.\n  The current paper advances the existing literature on approximation\nalgorithms for GND problems with (D)oS cost functions in various aspects: (1)\nwe present a generic approximation framework that yields approximation results\nfor a much wider family of requests in both directed and undirected graphs; (2)\nour framework allows for unrelated weights, thus providing the first\nnon-trivial approximation for the problem of scheduling unrelated parallel\nmachines with (D)oS cost functions; (3) our framework is fully combinatorial\nand runs in strongly polynomial time; (4) the family of (D)oS cost functions\nconsidered in the current paper is more general than the one considered in the\nexisting literature, providing a more accurate abstraction for practical energy\nconservation scenarios; and (5) we obtain the first approximation ratio for GND\nwith (D)oS cost functions that depends only on the parameters of the resources'\ntechnology and does not grow with the number of resources, the number of\nrequests, or their weights. The design of our framework relies heavily on\nRoughgarden's smoothness toolbox (JACM 2015), thus demonstrating the possible\nusefulness of this toolbox in the area of approximation algorithms. \n\n"}
{"id": "1803.05392", "contents": "Title: Automated Construction of Bounded-Loss Imperfect-Recall Abstractions in\n  Extensive-Form Games Abstract: Extensive-form games (EFGs) model finite sequential interactions between\nplayers. The amount of memory required to represent these games is the main\nbottleneck of algorithms for computing optimal strategies and the size of these\nstrategies is often impractical for real-world applications. A common approach\nto tackle the memory bottleneck is to use information abstraction that removes\nparts of information available to players thus reducing the number of decision\npoints in the game. However, existing information-abstraction techniques are\neither specific for a particular domain, they do not provide any quality\nguarantees, or they are applicable to very small subclasses of EFGs. We present\ndomain-independent abstraction methods for creating imperfect recall\nabstractions in extensive-form games that allow computing strategies that are\n(near) optimal in the original game. To this end, we introduce two novel\nalgorithms, FPIRA and CFR+IRA, based on fictitious play and counterfactual\nregret minimization. These algorithms can start with an arbitrary domain\nspecific, or the coarsest possible, abstraction of the original game. The\nalgorithms iteratively detect the missing information they require for\ncomputing a strategy for the abstract game that is (near) optimal in the\noriginal game. This information is then included back into the abstract game.\nMoreover, our algorithms are able to exploit imperfect-recall abstractions that\nallow players to forget even history of their own actions. However, the\nalgorithms require traversing the complete unabstracted game tree. We\nexperimentally show that our algorithms can closely approximate Nash\nequilibrium of large games using abstraction with as little as 0.9% of\ninformation sets of the original game. Moreover, the results suggest that\nmemory savings increase with the increasing size of the original games. \n\n"}
{"id": "1803.05501", "contents": "Title: Max-Min Greedy Matching Abstract: A bipartite graph $G(U,V;E)$ that admits a perfect matching is given. One\nplayer imposes a permutation $\\pi$ over $V$, the other player imposes a\npermutation $\\sigma$ over $U$. In the greedy matching algorithm, vertices of\n$U$ arrive in order $\\sigma$ and each vertex is matched to the lowest (under\n$\\pi$) yet unmatched neighbor in $V$ (or left unmatched, if all its neighbors\nare already matched). The obtained matching is maximal, thus matches at least a\nhalf of the vertices. The max-min greedy matching problem asks: suppose the\nfirst (max) player reveals $\\pi$, and the second (min) player responds with the\nworst possible $\\sigma$ for $\\pi$, does there exist a permutation $\\pi$\nensuring to match strictly more than a half of the vertices? Can such a\npermutation be computed in polynomial time?\n  The main result of this paper is an affirmative answer for this question: we\nshow that there exists a polytime algorithm to compute $\\pi$ for which for\nevery $\\sigma$ at least $\\rho > 0.51$ fraction of the vertices of $V$ are\nmatched. We provide additional lower and upper bounds for special families of\ngraphs, including regular and Hamiltonian. Interestingly, even for regular\ngraphs with arbitrarily large degree (implying a large number of disjoint\nperfect matchings), there is no $\\pi$ ensuring to match more than a fraction\n$8/9$ of the vertices.\n  The max-min greedy matching problem solves an open problem regarding the\nwelfare guarantees attainable by pricing in sequential markets with binary\nunit-demand valuations. In addition, it has implications for the size of the\nunique stable matching in markets with global preferences, subject to the graph\nstructure. \n\n"}
{"id": "1803.06878", "contents": "Title: Parameterized Complexity of Fair Vertex Evaluation Problems Abstract: A prototypical graph problem is centered around a graph-theoretic property\nfor a set of vertices and a solution to it is a set of vertices for which the\ndesired property holds. The task is to decide whether, in the given graph,\nthere exists a solution of a certain quality, where we use size as a quality\nmeasure. In this work, we are changing the measure to the fair measure\n[Lin&Sahni: Fair edge deletion problems. IEEE Trans. Comput. 89]. The measure\nis k if the number of solution neighbors does not exceed k for any vertex in\nthe graph. One possible way to study graph problems is by defining the property\nin a certain logic. For a given objective an evaluation problem is to find a\nset (of vertices) that simultaneously minimizes the assumed measure and\nsatisfies an appropriate formula.\n  In the presented paper we show that there is an FPT algorithm for the MSO\nFair Vertex Evaluation problem for formulas with one free variable\nparameterized by the twin cover number of the input graph. Here, the free\nvariable corresponds to the solution sought. One may define an extended variant\nof MSO Fair Vertex Evaluation for formulas with l free variables; here we\nmeasure a maximum number of neighbors in each of the l sets. However, such\nvariant is W[1]-hard for parameter l even on graphs with twin cover one.\nFurthermore, we study the Fair Vertex Cover (Fair VC) problem. Fair VC is among\nthe simplest problems with respect to the demanded property (i.e., the rest\nforms an edgeless graph). On the negative side, Fair VC is W[1]-hard when\nparameterized by both treedepth and feedback vertex set of the input graph. On\nthe positive side, we provide an FPT algorithm for the parameter modular width. \n\n"}
{"id": "1803.07164", "contents": "Title: Adversarial Generalized Method of Moments Abstract: We provide an approach for learning deep neural net representations of models\ndescribed via conditional moment restrictions. Conditional moment restrictions\nare widely used, as they are the language by which social scientists describe\nthe assumptions they make to enable causal inference. We formulate the problem\nof estimating the underling model as a zero-sum game between a modeler and an\nadversary and apply adversarial training. Our approach is similar in nature to\nGenerative Adversarial Networks (GAN), though here the modeler is learning a\nrepresentation of a function that satisfies a continuum of moment conditions\nand the adversary is identifying violating moments. We outline ways of\nconstructing effective adversaries in practice, including kernels centered by\nk-means clustering, and random forests. We examine the practical performance of\nour approach in the setting of non-parametric instrumental variable regression. \n\n"}
{"id": "1803.07484", "contents": "Title: Collective Schedules: Scheduling Meets Computational Social Choice Abstract: When scheduling public works or events in a shared facility one needs to\naccommodate preferences of a population. We formalize this problem by\nintroducing the notion of a collective schedule. We show how to extend\nfundamental tools from social choice theory---positional scoring rules, the\nKemeny rule and the Condorcet principle---to collective scheduling. We study\nthe computational complexity of finding collective schedules. We also\nexperimentally demonstrate that optimal collective schedules can be found for\ninstances with realistic sizes. \n\n"}
{"id": "1803.08884", "contents": "Title: Inequity aversion improves cooperation in intertemporal social dilemmas Abstract: Groups of humans are often able to find ways to cooperate with one another in\ncomplex, temporally extended social dilemmas. Models based on behavioral\neconomics are only able to explain this phenomenon for unrealistic stateless\nmatrix games. Recently, multi-agent reinforcement learning has been applied to\ngeneralize social dilemma problems to temporally and spatially extended Markov\ngames. However, this has not yet generated an agent that learns to cooperate in\nsocial dilemmas as humans do. A key insight is that many, but not all, human\nindividuals have inequity averse social preferences. This promotes a particular\nresolution of the matrix game social dilemma wherein inequity-averse\nindividuals are personally pro-social and punish defectors. Here we extend this\nidea to Markov games and show that it promotes cooperation in several types of\nsequential social dilemma, via a profitable interaction with policy\nlearnability. In particular, we find that inequity aversion improves temporal\ncredit assignment for the important class of intertemporal social dilemmas.\nThese results help explain how large-scale cooperation may emerge and persist. \n\n"}
{"id": "1803.09353", "contents": "Title: Stochastic bandits robust to adversarial corruptions Abstract: We introduce a new model of stochastic bandits with adversarial corruptions\nwhich aims to capture settings where most of the input follows a stochastic\npattern but some fraction of it can be adversarially changed to trick the\nalgorithm, e.g., click fraud, fake reviews and email spam. The goal of this\nmodel is to encourage the design of bandit algorithms that (i) work well in\nmixed adversarial and stochastic models, and (ii) whose performance\ndeteriorates gracefully as we move from fully stochastic to fully adversarial\nmodels.\n  In our model, the rewards for all arms are initially drawn from a\ndistribution and are then altered by an adaptive adversary. We provide a simple\nalgorithm whose performance gracefully degrades with the total corruption the\nadversary injected in the data, measured by the sum across rounds of the\nbiggest alteration the adversary made in the data in that round; this total\ncorruption is denoted by $C$. Our algorithm provides a guarantee that retains\nthe optimal guarantee (up to a logarithmic term) if the input is stochastic and\nwhose performance degrades linearly to the amount of corruption $C$, while\ncrucially being agnostic to it. We also provide a lower bound showing that this\nlinear degradation is necessary if the algorithm achieves optimal performance\nin the stochastic setting (the lower bound works even for a known amount of\ncorruption, a special case in which our algorithm achieves optimal performance\nwithout the extra logarithm). \n\n"}
{"id": "1803.09370", "contents": "Title: Popular Matching in Roommates Setting is NP-hard Abstract: An input to the Popular Matching problem, in the roommates setting, consists\nof a graph $G$ and each vertex ranks its neighbors in strict order, known as\nits preference. In the Popular Matching problem the objective is to test\nwhether there exists a matching $M^\\star$ such that there is no matching $M$\nwhere more people are happier with $M$ than with $M^\\star$. In this paper we\nsettle the computational complexity of the Popular Matching problem in the\nroommates setting by showing that the problem is NP-complete. Thus, we resolve\nan open question that has been repeatedly, explicitly asked over the last\ndecade. \n\n"}
{"id": "1803.10131", "contents": "Title: The algebra of predicting agents Abstract: The category of open games, which provides a strongly compositional\nfoundation of economic game theory, is intermediate between symmetric monoidal\nand compact closed. More precisely it has counits with no corresponding units,\nand a partially defined duality. There exist open games with the same types as\nunit maps, given by agents with the strategic goal of predicting a future\nvalue. Such agents appear in earlier work on selection functions. We explore\nthe algebraic properties of these agents via the symmetric monoidal bicategory\nwhose 2-cells are morphisms between open games, and show how the resulting\nstructure approximates a compact closed category with a family of lax\ncommutative bialgebras. \n\n"}
{"id": "1803.10184", "contents": "Title: A New Optimal Algorithm for Computing the Visibility Area of a simple\n  Polygon from a Viewpoint Abstract: Given a simple polygon $ \\mathcal {P} $ of $ n $ vertices in the Plane. We\nstudy the problem of computing the visibility area from a given viewpoint $ q $\ninside $ \\mathcal {P} $ where only sub-linear variables are allowed for working\nspace. Without any memory-constrained, this problem was previously solved in $\nO(n) $-time and $ O(n) $-variables space. In a newer research, the visibility\narea of a point be computed in $ O(n) $-time, using $ O(\\sqrt{n}) $ variables\nfor working space. In this paper, we present an optimal-time algorithm, using $\nO(c/\\log n) $ variables space for computing visibility area, where $ c<n $ is\nthe number of critical vertices. We keep the algorithm in the linear-time and\nreduce space as much as possible. \n\n"}
{"id": "1803.11030", "contents": "Title: Exploiting Weak Supermodularity for Coalition-Proof Mechanisms Abstract: Under the incentive-compatible Vickrey-Clarke-Groves mechanism, coalitions of\nparticipants can influence the auction outcome to obtain higher collective\nprofit. These manipulations were proven to be eliminated if and only if the\nmarket objective is supermodular. Nevertheless, several auctions do not satisfy\nthe stringent conditions for supermodularity. These auctions include\nelectricity markets, which are the main motivation of our study. To\ncharacterize nonsupermodular functions, we introduce the supermodularity ratio\nand the weak supermodularity. We show that these concepts provide us with tight\nbounds on the profitability of collusion and shill bidding. We then derive an\nanalytical lower bound on the supermodularity ratio. Our results are verified\nwith case studies based on the IEEE test systems. \n\n"}
{"id": "1803.11130", "contents": "Title: Incentive Design in a Distributed Problem with Strategic Agents Abstract: In this paper, we consider a general distributed system with multiple agents\nwho select and then implement actions in the system. The system has an operator\nwith a centralized objective. The agents, on the other hand, are selfinterested\nand strategic in the sense that each agent optimizes its own individual\nobjective. The operator aims to mitigate this misalignment by designing an\nincentive scheme for the agents. The problem is difficult due to the cost\nfunctions of the agents being coupled, the objective of the operator not being\nsocial welfare, and the operator having no direct control over actions being\nimplemented by the agents. This problem has been studied in many fields,\nparticularly in mechanism design and cost allocation. However, mechanism design\ntypically assumes that the operator has knowledge of the cost functions of the\nagents and the actions being implemented by the operator. On the other hand,\ncost allocation classically assumes that agents do not anticipate the effect of\ntheir actions on the incentive that they obtain. We remove these assumptions\nand present an incentive rule for this setup by bridging the gap between\nmechanism design and classical cost allocation. We analyze whether the proposed\ndesign satisfies various desirable properties such as social optimality, budget\nbalance, participation constraint, and so on. We also analyze which of these\nproperties can be satisfied if the assumptions of cost functions of the agents\nbeing private and the agents being anticipatory are relaxed. \n\n"}
{"id": "1804.00308", "contents": "Title: Manipulating Machine Learning: Poisoning Attacks and Countermeasures for\n  Regression Learning Abstract: As machine learning becomes widely used for automated decisions, attackers\nhave strong incentives to manipulate the results and models generated by\nmachine learning algorithms. In this paper, we perform the first systematic\nstudy of poisoning attacks and their countermeasures for linear regression\nmodels. In poisoning attacks, attackers deliberately influence the training\ndata to manipulate the results of a predictive model. We propose a\ntheoretically-grounded optimization framework specifically designed for linear\nregression and demonstrate its effectiveness on a range of datasets and models.\nWe also introduce a fast statistical attack that requires limited knowledge of\nthe training process. Finally, we design a new principled defense method that\nis highly resilient against all poisoning attacks. We provide formal guarantees\nabout its convergence and an upper bound on the effect of poisoning attacks\nwhen the defense is deployed. We evaluate extensively our attacks and defenses\non three realistic datasets from health care, loan assessment, and real estate\ndomains. \n\n"}
{"id": "1804.00480", "contents": "Title: Tight Revenue Gaps among Simple Mechanisms Abstract: We consider a fundamental problem in microeconomics: selling a single item to\na number of potential buyers, whose values are drawn from known independent and\nregular (not necessarily identical) distributions. There are four widely-used\nand widely-studied mechanisms in the literature: {\\sf Myerson Auction}~({\\sf\nOPT}), {\\sf Sequential Posted-Pricing}~({\\sf SPM}), {\\sf Second-Price Auction\nwith Anonymous Reserve}~({\\sf AR}), and {\\sf Anonymous Pricing}~({\\sf AP}).\n  {\\sf OPT} is revenue-optimal but complicated, which also experiences several\nissues in practice such as fairness; {\\sf AP} is the simplest mechanism, but\nalso generates the lowest revenue among these four mechanisms; {\\sf SPM} and\n{\\sf AR} are of intermediate complexity and revenue. We explore revenue gaps\namong these mechanisms, each of which is defined as the largest ratio between\nrevenues from a pair of mechanisms. We establish two tight bounds and one\nimproved bound:\n  1. {\\sf SPM} vs.\\ {\\sf AP}: this ratio studies the power of discrimination in\npricing schemes. We obtain the tight ratio of $\\mathcal{C^*} \\approx 2.62$,\nclosing the gap between $\\big[\\frac{e}{e - 1}, e\\big]$ left before.\n  2. {\\sf AR} vs.\\ {\\sf AP}: this ratio measures the relative power of auction\nscheme vs.\\ pricing scheme, when no discrimination is allowed. We attain the\ntight ratio of $\\frac{\\pi^2}{6} \\approx 1.64$, closing the previously known\nbounds $\\big[\\frac{e}{e - 1}, e\\big]$.\n  3. {\\sf OPT} vs.\\ {\\sf AR}: this ratio quantifies the power of discrimination\nin auction schemes, and is previously known to be somewhere between $\\big[2,\ne\\big]$. The lower-bound of $2$ was conjectured to be tight by Hartline and\nRoughgarden (2009) and Alaei et al.\\ (2015). We acquire a better lower-bound of\n$2.15$, and thus disprove this conjecture. \n\n"}
{"id": "1804.01023", "contents": "Title: Attracting Tangles to Solve Parity Games Abstract: Parity games have important practical applications in formal verification and\nsynthesis, especially to solve the model-checking problem of the modal\nmu-calculus. They are also interesting from the theory perspective, because\nthey are widely believed to admit a polynomial solution, but so far no such\nalgorithm is known.\n  We propose a new algorithm to solve parity games based on learning tangles,\nwhich are strongly connected subgraphs for which one player has a strategy to\nwin all cycles in the subgraph. We argue that tangles play a fundamental role\nin the prominent parity game solving algorithms. We show that tangle learning\nis competitive in practice and the fastest solver for large random games. \n\n"}
{"id": "1804.01567", "contents": "Title: On-line Chain Partitioning Approach to Scheduling Abstract: An on-line chain partitioning algorithm receives the points of the poset from\nsome externally determined list. Being presented with a new point the algorithm\nlearns the comparability status of this new point to all previously presented\nones. As each point is received, the algorithm assigns this new point to a\nchain in an irrevocable manner and this assignment is made without knowledge of\nfuture points. Kierstead presented an algorithm using $(5^w-1)/4$ chains to\ncover each poset of width $w$. Felsner proved that width $2$ posets can be\npartitioned on-line into $5$ chains. We present an algorithm using $16$ chains\non posets of width $3$. This result significantly narrows down the previous\nbound of $31$. Moreover, we address the on-line chain partitioning problem for\ninterval orders. Kierstead and Trotter presented an algorithm using $3w-2$\nchains. We deal with an up-growing version of an on-line chain partition of\ninterval orders, i.e. we restrict possible inputs by the rule that each new\npoint is maximal at the moment of its arrival. We present an algorithm using\n$2w-1$ chains and show that there is no better one. These problems come from a\nneed for better algorithms that can be applied to scheduling. Each on-line\nchain partitioning algorithm schedules tasks in a multiprocessor environment,\nand therefore can be applied in order to minimize number of processors. \n\n"}
{"id": "1804.02394", "contents": "Title: An Accelerated Directional Derivative Method for Smooth Stochastic\n  Convex Optimization Abstract: We consider smooth stochastic convex optimization problems in the context of\nalgorithms which are based on directional derivatives of the objective\nfunction. This context can be considered as an intermediate one between\nderivative-free optimization and gradient-based optimization. We assume that at\nany given point and for any given direction, a stochastic approximation for the\ndirectional derivative of the objective function at this point and in this\ndirection is available with some additive noise. The noise is assumed to be of\nan unknown nature, but bounded in the absolute value. We underline that we\nconsider directional derivatives in any direction, as opposed to coordinate\ndescent methods which use only derivatives in coordinate directions. For this\nsetting, we propose a non-accelerated and an accelerated directional derivative\nmethod and provide their complexity bounds. Our non-accelerated algorithm has a\ncomplexity bound which is similar to the gradient-based algorithm, that is,\nwithout any dimension-dependent factor. Our accelerated algorithm has a\ncomplexity bound which coincides with the complexity bound of the accelerated\ngradient-based algorithm up to a factor of square root of the problem\ndimension. We extend these results to strongly convex problems. \n\n"}
{"id": "1804.02806", "contents": "Title: Prior Independent Equilibria and Linear Multi-dimensional Bayesian Games Abstract: We show that a Bayesian strategy map profile is a Bayesian Nash Equilibrium\nindependent of any prior if and only if the Bayesian strategy map profile,\nevaluated at any type profile, is the Nash equilibrium of the so-called local\ndeterministic game corresponding to that type profile. We call such a Bayesian\ngame type-regular. We then show that an m-dimensional n-agent Bayesian game\nwhose utilities are linearly dependent on the types of the agents is\nequivalent, following a normalisation of the type space of each agent into the\n(m-1)-simplex, to a simultaneous competition in nm so-called basic n-agent\ngames. If the game is own-type-linear, i.e., the utility of each player only\ndepends linearly on its own type, then the Bayesian game is equivalent to a\nsimultaneous competition in m basic n-agent games, called a multi-game. We then\nprove that an own-type-linear Bayesian game is type-regular if it is\ntype-regular on the vertices of the (m-1)-simplex, a result which provides a\nlarge class of type-regular Bayesian maps. The class of m-dimensional\nown-type-linear Bayesian games can model, via their equivalence with\nmulti-games, simultaneous decision-making in m different environments. We show\nthat a two dimensional own-type-linear Bayesian game can be used to give a new\nmodel of the Prisoner's Dilemma (PD) in which the prosocial tendencies of the\nagents are considered as their types and the two agents play simultaneously in\nthe PD as well as in a prosocial game. This Bayesian game addresses the\nmaterialistic and the prosocial tendencies of the agents. Similarly, we present\na new two dimensional Bayesian model of the Trust game in which the type of the\ntwo agents reflect their prosocial tendency or trustfulness, which leads to\nmore reasonable Nash equilibria. We finally consider an example of such\nmulti-environment decision making in production by several companies in\nmulti-markets. \n\n"}
{"id": "1804.03450", "contents": "Title: End of Potential Line Abstract: We introduce the problem EndOfPotentialLine and the corresponding complexity\nclass EOPL of all problems that can be reduced to it in polynomial time. This\nclass captures problems that admit a single combinatorial proof of their joint\nmembership in the complexity classes PPAD of fixpoint problems and PLS of local\nsearch problems. EOPL is a combinatorially-defined alternative to the class CLS\n(for Continuous Local Search), which was introduced in with the goal of\ncapturing the complexity of some well-known problems in PPAD $\\cap$ PLS that\nhave resisted, in some cases for decades, attempts to put them in polynomial\ntime. Two of these are Contraction, the problem of finding a fixpoint of a\ncontraction map, and P-LCP, the problem of solving a P-matrix Linear\nComplementarity Problem.\n  We show that EndOfPotentialLine is in CLS via a two-way reduction to\nEndOfMeteredLine. The latter was defined in to show query and cryptographic\nlower bounds for CLS. Our two main results are to show that both PL-Contraction\n(Piecewise-Linear Contraction) and P-LCP are in EOPL. Our reductions imply that\nthe promise versions of PL-Contraction and P-LCP are in the promise class\nUniqueEOPL, which corresponds to the case of a single potential line. This also\nshows that simple-stochastic, discounted, mean-payoff, and parity games are in\nEOPL.\n  Using the insights from our reduction for PL-Contraction, we obtain the first\npolynomial-time algorithms for finding fixed points of contraction maps in\nfixed dimension for any $\\ell_p$ norm, where previously such algorithms were\nonly known for the $\\ell_2$ and $\\ell_\\infty$ norms. Our reduction from P-LCP\nto EndOfPotentialLine allows a technique of Aldous to be applied, which in turn\ngives the fastest-known randomized algorithm for the P-LCP. \n\n"}
{"id": "1804.03833", "contents": "Title: Don't cry to be the first!Symmetric fair division exist Abstract: In this article we study a cake cutting problem. More precisely, we study\nsymmetric fair division algorithms, that is to say we study algorithms where\nthe order of the players do not influence the value obtained by each player. In\nthe first part of the article, we give a symmetric and envy-free fair division\nalgorithm. More precisely, we show how to get a symmetric and envy-free fair\ndivision algorithm from an envy-free division algorithm. In the second part, we\ngive a proportional and symmetric fair division algorithm with a complexity in\nO(n 3) in the Robertson-Webb model of complexity. This algorithm is based on\nKuhn's algorithm. Furthermore, our study has led us to introduce a new notion:\naristotelian fair division. This notion is an interpretation of Aristotle's\nprinciple: give equal shares to equal people. We conclude this article with a\ndiscussion and some questions about the Robertson-Webb model of computation. \n\n"}
{"id": "1804.03894", "contents": "Title: Computing Shapley values in the plane Abstract: We consider the problem of computing Shapley values for points in the plane,\nwhere each point is interpreted as a player, and the value of a coalition is\ndefined by the area of usual geometric objects, such as the convex hull or the\nminimum axis-parallel bounding box.\n  For sets of $n$ points in the plane, we show how to compute in roughly\n$O(n^{3/2})$ time the Shapley values for the area of the minimum axis-parallel\nbounding box and the area of the union of the rectangles spanned by the origin\nand the input points. When the points form an increasing or decreasing chain,\nthe running time can be improved to near-linear. In all these cases, we use\nlinearity of the Shapley values and algebraic methods.\n  We also show that Shapley values for the area and the perimeter of the convex\nhull or the minimum enclosing disk can be computed in $O(n^2)$ and $O(n^3)$\ntime, respectively. In this case the computation is closely related to the\nmodel of stochastic point sets considered in computational geometry, but here\nwe have to consider random insertion orders of the points instead of a\nprobabilistic existence of points. \n\n"}
{"id": "1804.05083", "contents": "Title: Incentive design for learning in user-recommendation systems with\n  time-varying states Abstract: We consider the problem of how strategic users with asymmetric information\ncan learn an underlying time varying state in a user-recommendation system.\nUsers who observe private signals about the state, sequentially make a decision\nabout buying a product whose value varies with time in an ergodic manner. We\nformulate the team problem as an instance of decentralized stochastic control\nproblem and characterize its optimal policies. With strategic users, we design\nincentives such that users reveal their true private signals, so that the gap\nbetween the strategic and team objective is small and the overall expected\nincentive payments are also small. \n\n"}
{"id": "1804.05560", "contents": "Title: Deep Bayesian Trust : A Dominant and Fair Incentive Mechanism for Crowd Abstract: An important class of game-theoretic incentive mechanisms for eliciting\neffort from a crowd are the peer based mechanisms, in which workers are paid by\nmatching their answers with one another. The other classic mechanism is to have\nthe workers solve some gold standard tasks and pay them according to their\naccuracy on gold tasks. This mechanism ensures stronger incentive compatibility\nthan the peer based mechanisms but assigning gold tasks to all workers becomes\ninefficient at large scale. We propose a novel mechanism that assigns gold\ntasks to only a few workers and exploits transitivity to derive accuracy of the\nrest of the workers from their peers' accuracy. We show that the resulting\nmechanism ensures a dominant notion of incentive compatibility and fairness. \n\n"}
{"id": "1804.06465", "contents": "Title: Triggers for cooperative behavior in the thermodynamic limit: a case\n  study in Public goods game Abstract: In this work, we aim to answer the question: what triggers cooperative\nbehavior in the thermodynamic limit by taking recourse to the Public goods\ngame. Using the idea of mapping the 1D Ising model Hamiltonian with nearest\nneighbor coupling to payoffs in the game theory we calculate the Magnetisation\nof the game in the thermodynamic limit. We see a phase transition in the\nthermodynamic limit of the two player Public goods game. We observe that\npunishment acts as an external field for the two player Public goods game\ntriggering cooperation or provide strategy, while cost can be a trigger for\nsuppressing cooperation or free riding. Finally, reward also acts as a trigger\nfor providing while the role of inverse temperature (fluctuations in choices)\nis to introduce randomness in strategic choices. \n\n"}
{"id": "1804.06836", "contents": "Title: Delayed Blockchain Protocols Abstract: Given the parallels between game theory and consensus, it makes sense to\nintelligently design blockchain or DAG protocols with an\nincentive-compatible-first mentality. To that end, we propose a new blockchain\nor DAG protocol enhancement based on delayed rewards. We devise a new method\nfor imposing slashing conditions on miner behavior, using their delayed rewards\nas stake in a Proof of Work system. Using fraud proofs, we can slash malicious\nminer behavior and reward long-lived, honest behavior. \n\n"}
{"id": "1804.06867", "contents": "Title: Optimal Deterministic Mechanisms for an Additive Buyer Abstract: We study revenue maximization by deterministic mechanisms for the simplest\ncase for which Myerson's characterization does not hold: a single seller\nselling two items, with independently distributed values, to a single additive\nbuyer. We prove that optimal mechanisms are submodular and hence monotone.\nFurthermore, we show that in the IID case, optimal mechanisms are symmetric.\nOur characterizations are surprisingly non-trivial, and we show that they fail\nto extend in several natural ways, e.g. for correlated distributions or more\nthan two items. In particular, this shows that the optimality of symmetric\nmechanisms does not follow from the symmetry of the IID distribution. \n\n"}
{"id": "1804.06923", "contents": "Title: Truthful Fair Division without Free Disposal Abstract: We study the problem of fairly dividing a heterogeneous resource, commonly\nknown as cake cutting and chore division, in the presence of strategic agents.\nWhile a number of results in this setting have been established in previous\nworks, they rely crucially on the free disposal assumption, meaning that the\nmechanism is allowed to throw away part of the resource at no cost. In the\npresent work, we remove this assumption and focus on mechanisms that always\nallocate the entire resource. We exhibit a truthful and envy-free mechanism for\ncake cutting and chore division for two agents with piecewise uniform\nvaluations, and we complement our result by showing that such a mechanism does\nnot exist when certain additional constraints are imposed on the mechanisms.\nMoreover, we provide bounds on the efficiency of mechanisms satisfying various\nproperties, and give truthful mechanisms for multiple agents with restricted\nclasses of valuations. \n\n"}
{"id": "1804.07451", "contents": "Title: Bayesian Auctions with Efficient Queries Abstract: Generating good revenue is one of the most important problems in Bayesian\nauction design, and many (approximately) optimal dominant-strategy incentive\ncompatible (DSIC) Bayesian mechanisms have been constructed for various auction\nsettings. However, most existing studies do not consider the complexity for the\nseller to carry out the mechanism. It is assumed that the seller knows \"each\nsingle bit\" of the distributions and is able to optimize perfectly based on the\nentire distributions. Unfortunately, this is a strong assumption and may not\nhold in reality: for example, when the value distributions have exponentially\nlarge supports or do not have succinct representations.\n  In this work we consider, for the first time, the query complexity of\nBayesian mechanisms. We only allow the seller to have limited oracle accesses\nto the players' value distributions, via quantile queries and value queries.\nFor a large class of auction settings, we prove logarithmic lower-bounds for\nthe query complexity for any DSIC Bayesian mechanism to be of any constant\napproximation to the optimal revenue. For single-item auctions and multi-item\nauctions with unit-demand or additive valuation functions, we prove tight\nupper-bounds via efficient query schemes, without requiring the distributions\nto be regular or have monotone hazard rate. Thus, in those auction settings the\nseller needs to access much less than the full distributions in order to\nachieve approximately optimal revenue. \n\n"}
{"id": "1804.07605", "contents": "Title: Distributed, Private, and Derandomized Allocation Algorithm for EV\n  Charging Abstract: Efficient resource allocation is challenging when privacy of users is\nimportant. Distributed approaches have recently been used extensively to find a\nsolution for such problems. In this work, the efficiency of distributed AIMD\nalgorithm for allocation of subsidized goods is studied. First, a suitable\nutility function is assigned to each user describing the amount of satisfaction\nthat it has from allocated resource. Then the resource allocation is defined as\na total utilitarianism problem that is an optimization problem of sum of users\nutility functions subjected to capacity constraint. Recently, a stochastic\nstate-dependent variant of AIMD algorithm is used for allocation of common\ngoods among users with strictly increasing and concave utility functions. Here,\nthe stochastic AIMD algorithm is derandomized and its efficiency is compared\nwith the stochastic version. Moreover, the algorithm is improved to allocate\nsubsidized goods to users with concave and non-monotone utility functions as\nwell as users with Sigmoidal utility functions. To illustrate the effectiveness\nof the proposed solutions, simulation results is presented for a public\nrenewable-energy powered charging station in which the electric vehicles (EV)\ncompete to be recharged. \n\n"}
{"id": "1804.08005", "contents": "Title: Learning in Games with Cumulative Prospect Theoretic Preferences Abstract: We consider repeated games where the players behave according to cumulative\nprospect theory (CPT). We show that, when the players have calibrated\nstrategies and behave according to CPT, the natural analog of the notion of\ncorrelated equilibrium in the CPT case, as defined by Keskin, is not enough to\ncapture all subsequential limits of the empirical distribution of action play.\nWe define the notion of a mediated CPT correlated equilibrium via an extension\nof the stage game to a so-called mediated game. We then show, along the lines\nof the result of Foster and Vohra about convergence to the set of correlated\nequilibria when the players behave according to expected utility theory that,\nin the CPT case, under calibrated learning the empirical distribution of action\nplay converges to the set of all mediated CPT correlated equilibria. We also\nshow that, in general, the set of CPT correlated equilibria is not approachable\nin the Blackwell approachability sense. We observe that a mediated game is a\nspecific type of a game with communication, as introduced by Myerson, and as a\nconsequence we get that the revelation principle does not hold under CPT. \n\n"}
{"id": "1804.08885", "contents": "Title: Polynomial Kernels for Hitting Forbidden Minors under Structural\n  Parameterizations Abstract: We investigate polynomial-time preprocessing for the problem of hitting\nforbidden minors in a graph, using the framework of kernelization. For a fixed\nfinite set of connected graphs F, the F-Deletion problem is the following:\ngiven a graph G and integer k, is it possible to delete k vertices from G to\nensure the resulting graph does not contain any graph from F as a minor?\nEarlier work by Fomin, Lokshtanov, Misra, and Saurabh [FOCS'12] showed that\nwhen F contains a planar graph, an instance (G,k) can be reduced in polynomial\ntime to an equivalent one of size $k^{O(1)}$. In this work we focus on\nstructural measures of the complexity of an instance, with the aim of giving\nnontrivial preprocessing guarantees for instances whose solutions are large.\nMotivated by several impossibility results, we parameterize the F-Deletion\nproblem by the size of a vertex modulator whose removal results in a graph of\nconstant treedepth $\\eta$.\n  We prove that for each set F of connected graphs and constant $\\eta$, the\nF-Deletion problem parameterized by the size of a treedepth-$\\eta$ modulator\nhas a polynomial kernel. Our kernelization is fully explicit and does not\ndepend on protrusion reduction or well-quasi-ordering, which are sources of\nalgorithmic non-constructivity in earlier works on F-Deletion. Our main\ntechnical contribution is to analyze how models of a forbidden minor in a graph\nG with modulator X, interact with the various connected components of G-X. By\nbounding the number of different types of behavior that can occur by a\npolynomial in |X|, we obtain a polynomial kernel using a recursive\npreprocessing strategy. Our results extend earlier work for specific instances\nof F-Deletion such as Vertex Cover and Feedback Vertex Set. It also generalizes\nearlier preprocessing results for F-Deletion parameterized by a vertex cover,\nwhich is a treedepth-one modulator. \n\n"}
{"id": "1804.09521", "contents": "Title: Fair Division Under Cardinality Constraints Abstract: We consider the problem of fairly allocating indivisible goods, among agents,\nunder cardinality constraints and additive valuations. In this setting, we are\ngiven a partition of the entire set of goods---i.e., the goods are\ncategorized---and a limit is specified on the number of goods that can be\nallocated from each category to any agent. The objective here is to find a fair\nallocation in which the subset of goods assigned to any agent satisfies the\ngiven cardinality constraints. This problem naturally captures a number of\nresource-allocation applications, and is a generalization of the well-studied\n(unconstrained) fair division problem.\n  The two central notions of fairness, in the context of fair division of\nindivisible goods, are envy freeness up to one good (EF1) and the (approximate)\nmaximin share guarantee (MMS). We show that the existence and algorithmic\nguarantees established for these solution concepts in the unconstrained setting\ncan essentially be achieved under cardinality constraints. Specifically, we\ndevelop efficient algorithms which compute EF1 and approximately MMS\nallocations in the constrained setting.\n  Furthermore, focusing on the case wherein all the agents have the same\nadditive valuation, we establish that EF1 allocations exist and can be computed\nefficiently even under laminar matroid constraints. \n\n"}
{"id": "1804.10412", "contents": "Title: On Cyber Risk Management of Blockchain Networks: A Game Theoretic\n  Approach Abstract: Open-access blockchains based on proof-of-work protocols have gained\ntremendous popularity for their capabilities of providing decentralized\ntamper-proof ledgers and platforms for data-driven autonomous organization.\nNevertheless, the proof-of-work based consensus protocols are vulnerable to\ncyber-attacks such as double-spending. In this paper, we propose a novel\napproach of cyber risk management for blockchain-based service. In particular,\nwe adopt the cyber-insurance as an economic tool for neutralizing cyber risks\ndue to attacks in blockchain networks. We consider a blockchain service market,\nwhich is composed of the infrastructure provider, the blockchain provider, the\ncyber-insurer, and the users. The blockchain provider purchases from the\ninfrastructure provider, e.g., a cloud, the computing resources to maintain the\nblockchain consensus, and then offers blockchain services to the users. The\nblockchain provider strategizes its investment in the infrastructure and the\nservice price charged to the users, in order to improve the security of the\nblockchain and thus optimize its profit. Meanwhile, the blockchain provider\nalso purchases a cyber-insurance from the cyber-insurer to protect itself from\nthe potential damage due to the attacks. In return, the cyber-insurer adjusts\nthe insurance premium according to the perceived risk level of the blockchain\nservice. Based on the assumption of rationality for the market entities, we\nmodel the interaction among the blockchain provider, the users, and the\ncyber-insurer as a two-level Stackelberg game. Namely, the blockchain provider\nand the cyber-insurer lead to set their pricing/investment strategies, and then\nthe users follow to determine their demand of the blockchain service.\nSpecifically, we consider the scenario of double-spending attacks and provide a\nseries of analytical results about the Stackelberg equilibrium in the market\ngame. \n\n"}
{"id": "1804.10512", "contents": "Title: Probabilistic Verification for Obviously Strategyproof Mechanisms Abstract: Obviously strategyproof (OSP) mechanisms maintain the incentive compatibility\nof agents that are not fully rational. They have been object of a number of\nstudies since their recent definition. A research agenda, initiated in\n[Ferraioli&Ventre, AAAI 2017], is to find a small (possibly, the smallest) set\nof conditions allowing to implement an OSP mechanism. To this aim, we define a\nmodel of probabilistic verification wherein agents are caught misbehaving with\na certain probability, and show how OSP mechanisms can implement every social\nchoice function at the cost of either imposing very large fines for lies or\nverifying a linear number of agents. \n\n"}
{"id": "1804.10586", "contents": "Title: Approximating Nash Equilibria for Black-Box Games: A Bayesian\n  Optimization Approach Abstract: Game theory has emerged as a powerful framework for modeling a large range of\nmulti-agent scenarios. Many algorithmic solutions require discrete, finite\ngames with payoffs that have a closed-form specification. In contrast, many\nreal-world applications require modeling with continuous action spaces and\nblack-box utility functions where payoff information is available only in the\nform of empirical (often expensive and/or noisy) observations of strategy\nprofiles. To the best of our knowledge, few tools exist for solving the class\nof expensive, black-box continuous games. In this paper, we develop a method to\nfind equilibria for such games in a sequential decision-making framework using\nBayesian Optimization. The proposed approach is validated on a collection of\nsynthetic game problems with varying degree of noise and action space\ndimensions. The results indicate that it is capable of improving the maximum\nregret in noisy and high dimensions to a greater extent than hierarchical or\ndiscretized methods. \n\n"}
{"id": "1804.11196", "contents": "Title: A Feature Selection Method Based on Shapley Value to False Alarm\n  Reduction in ICUs, A Genetic-Algorithm Approach Abstract: High false alarm rate in intensive care units (ICUs) has been identified as\none of the most critical medical challenges in recent years. This often results\nin overwhelming the clinical staff by numerous false or unurgent alarms and\ndecreasing the quality of care through enhancing the probability of missing\ntrue alarms as well as causing delirium, stress, sleep deprivation and\ndepressed immune systems for patients. One major cause of false alarms in\nclinical practice is that the collected signals from different devices are\nprocessed individually to trigger an alarm, while there exists a considerable\nchance that the signal collected from one device is corrupted by noise or\nmotion artifacts. In this paper, we propose a low-computational complexity yet\naccurate game-theoretic feature selection method which is based on a genetic\nalgorithm that identifies the most informative biomarkers across the signals\ncollected from various monitoring devices and can considerably reduce the rate\nof false alarms. \n\n"}
{"id": "1804.11221", "contents": "Title: Adversarial Task Assignment Abstract: The problem of assigning tasks to workers is of long-standing fundamental\nimportance. Examples of this include the classical problem of assigning\ncomputing tasks to nodes in a distributed computing environment, assigning jobs\nto robots, and crowdsourcing. Extensive research into this problem generally\naddresses important issues such as uncertainty and incentives. However, the\nproblem of adversarial tampering with the task assignment process has not\nreceived as much attention.\n  We are concerned with a particular adversarial setting where an attacker may\ntarget a set of workers in order to prevent the tasks assigned to these workers\nfrom being completed. When all tasks are homogeneous, we provide an efficient\nalgorithm for computing the optimal assignment. When tasks are heterogeneous,\nwe show that the adversarial assignment problem is NP-Hard, and present an\nalgorithm for solving it approximately. Our theoretical results are accompanied\nby extensive experiments showing the effectiveness of our algorithms. \n\n"}
{"id": "1805.01074", "contents": "Title: Lower Bounds for Tolerant Junta and Unateness Testing via Rejection\n  Sampling of Graphs Abstract: We introduce a new model for testing graph properties which we call the\n\\emph{rejection sampling model}. We show that testing bipartiteness of\n$n$-nodes graphs using rejection sampling queries requires complexity\n$\\widetilde{\\Omega}(n^2)$. Via reductions from the rejection sampling model, we\ngive three new lower bounds for tolerant testing of Boolean functions of the\nform $f\\colon\\{0,1\\}^n\\to \\{0,1\\}$:\n  $\\bullet$Tolerant $k$-junta testing with \\emph{non-adaptive} queries requires\n$\\widetilde{\\Omega}(k^2)$ queries.\n  $\\bullet$Tolerant unateness testing requires $\\widetilde{\\Omega}(n)$ queries.\n  $\\bullet$Tolerant unateness testing with \\emph{non-adaptive} queries requires\n$\\widetilde{\\Omega}(n^{3/2})$ queries.\n  Given the $\\widetilde{O}(k^{3/2})$-query non-adaptive junta tester of Blais\n\\cite{B08}, we conclude that non-adaptive tolerant junta testing requires more\nqueries than non-tolerant junta testing. In addition, given the\n$\\widetilde{O}(n^{3/4})$-query unateness tester of Chen, Waingarten, and Xie\n\\cite{CWX17b} and the $\\widetilde{O}(n)$-query non-adaptive unateness tester of\nBaleshzar, Chakrabarty, Pallavoor, Raskhodnikova, and Seshadhri \\cite{BCPRS17},\nwe conclude that tolerant unateness testing requires more queries than\nnon-tolerant unateness testing, in both adaptive and non-adaptive settings.\nThese lower bounds provide the first separation between tolerant and\nnon-tolerant testing for a natural property of Boolean functions. \n\n"}
{"id": "1805.02351", "contents": "Title: Fine-grained Complexity Meets IP = PSPACE Abstract: In this paper we study the fine-grained complexity of finding exact and\napproximate solutions to problems in P. Our main contribution is showing\nreductions from exact to approximate solution for a host of such problems.\n  As one (notable) example, we show that the Closest-LCS-Pair problem (Given\ntwo sets of strings $A$ and $B$, compute exactly the maximum $\\textsf{LCS}(a,\nb)$ with $(a, b) \\in A \\times B$) is equivalent to its approximation version\n(under near-linear time reductions, and with a constant approximation factor).\nMore generally, we identify a class of problems, which we call BP-Pair-Class,\ncomprising both exact and approximate solutions, and show that they are all\nequivalent under near-linear time reductions.\n  Exploring this class and its properties, we also show:\n  $\\bullet$ Under the NC-SETH assumption (a significantly more relaxed\nassumption than SETH), solving any of the problems in this class requires\nessentially quadratic time.\n  $\\bullet$ Modest improvements on the running time of known algorithms\n(shaving log factors) would imply that NEXP is not in non-uniform\n$\\textsf{NC}^1$.\n  $\\bullet$ Finally, we leverage our techniques to show new barriers for\ndeterministic approximation algorithms for LCS.\n  At the heart of these new results is a deep connection between interactive\nproof systems for bounded-space computations and the fine-grained complexity of\nexact and approximate solutions to problems in P. In particular, our results\nbuild on the proof techniques from the classical IP = PSPACE result. \n\n"}
{"id": "1805.04436", "contents": "Title: Capturing Complementarity in Set Functions by Going Beyond\n  Submodularity/Subadditivity Abstract: We introduce two new \"degree of complementarity\" measures, which we refer to,\nrespectively, as supermodular width and superadditive width. Both are\nformulated based on natural witnesses of complementarity. We show that both\nmeasures are robust by proving that they, respectively, characterize the gap of\nmonotone set functions from being submodular and subadditive. Thus, they define\ntwo new hierarchies over monotone set functions, which we will refer to as\nSupermodular Width (SMW) hierarchy and Superadditive Width (SAW) hierarchy,\nwith level 0 of the hierarchies resting exactly on submodular and subadditive\nfunctions, respectively.\n  We present a comprehensive comparative analysis of the SMW hierarchy and the\nSupermodular Degree (SD) hierarchy, defined by Feige and Izsak. We prove that\nthe SMW hierarchy is strictly more expressive than the SD hierarchy. We show\nthat previous results regarding approximation guarantees for welfare and\nconstrained maximization as well as regarding the Price of Anarchy (PoA) of\nsimple auctions can be extended without any loss from the SD hierarchy to the\nSMW hierarchy. We also establish almost matching information-theoretical lower\nbounds. The combination of these approximation and hardness results illustrate\nthat the SMW hierarchy provides an accurate characterization of \"near\nsubmodularity\" needed for maximization approximation. While SD and SMW\nhierarchies support nontrivial bounds on the PoA of simple auctions, we show\nthat our SAW hierarchy seems to capture more intrinsic properties needed to\nrealize the efficiency of simple auctions. So far, the SAW hierarchy provides\nthe best dependency for the PoA of Single-bid Auction, and is nearly as\ncompetitive as the Maximum over Positive Hypergraphs (MPH) hierarchy for\nSimultaneous Item First Price Auction (SIA). We provide almost tight lower\nbounds for the PoA of both auctions with respect to the SAW hierarchy. \n\n"}
{"id": "1805.05305", "contents": "Title: Transforming graph states using single-qubit operations Abstract: Stabilizer states form an important class of states in quantum information,\nand are of central importance in quantum error correction. Here, we provide an\nalgorithm for deciding whether one stabilizer (target) state can be obtained\nfrom another stabilizer (source) state by single-qubit Clifford operations\n(LC), single-qubit Pauli measurements (LPM), and classical communication (CC)\nbetween sites holding the individual qubits. What's more, we provide a recipe\nto obtain the sequence of LC+LPM+CC operations which prepare the desired target\nstate from the source state, and show how these operations can be applied in\nparallel to reach the target state in constant time. Our algorithm has\napplications in quantum networks, quantum computing, and can also serve as a\ndesign tool - for example, to find transformations between quantum error\ncorrecting codes. We provide a software implementation of our algorithm that\nmakes this tool easier to apply.\n  A key insight leading to our algorithm is to show that the problem is\nequivalent to one in graph theory, which is to decide whether some graph G' is\na vertex-minor of another graph G. Here we show that the vertex-minor problem\ncan be solved in time O(|G|^3) where |G| is the size of the graph G, whenever\nthe rank-width of G and the size of G' are bounded. Our algorithm is based on\ntechniques by Courcelle for solving fixed parameter tractable problems, where\nhere the relevant fixed parameter is the rank width. The second half of this\npaper serves as an accessible but far from exhausting introduction to these\nconcepts, that could be useful for many other problems in quantum information. \n\n"}
{"id": "1805.05306", "contents": "Title: How to transform graph states using single-qubit operations:\n  computational complexity and algorithms Abstract: Graph states are ubiquitous in quantum information with diverse applications\nranging from quantum network protocols to measurement based quantum computing.\nHere we consider the question whether one graph (source) state can be\ntransformed into another graph (target) state, using a specific set of quantum\noperations (LC+LPM+CC): single-qubit Clifford operations (LC), single-qubit\nPauli measurements (LPM) and classical communication (CC) between sites holding\nthe individual qubits. We first show that deciding whether a graph state |G>\ncan be transformed into another graph state |G'> using LC+LPM+CC is\nNP-Complete, even if |G'> is restricted to be the GHZ-state. However, we also\nprovide efficient algorithms for two situations of practical interest:\n  1. |G> has Schmidt-rank width one and |G'> is a GHZ-state. The Schmidt-rank\nwidth is an entanglement measure of quantum states, meaning this algorithm is\nefficient if the original state has little entanglement. Our algorithm has\nruntime O(|V(G')||V(G)|^3), and is also efficient in practice even on small\ninstances as further showcased by a freely available software implementation.\n  2. |G> is in a certain class of states with unbounded Schmidt-rank width, and\n|G'> is a GHZ-state of a constant size. Here the runtime is O(poly(|V(G)|)),\nshowing that more efficient algorithms can in principle be found even for\nstates holding a large amount of entanglement, as long as the output state has\nconstant size.\n  Our results make use of the insight that deciding whether a graph state |G>\ncan be transformed to another graph state |G'> is equivalent to a known\ndecision problem in graph theory, namely the problem of deciding whether a\ngraph G' is a vertex-minor of a graph G. Many of the technical tools developed\nto obtain our results may be of independent interest. \n\n"}
{"id": "1805.06191", "contents": "Title: Fair Allocation of Indivisible Items With Externalities Abstract: One of the important yet insufficiently studied subjects in fair allocation\nis the externality effect among agents. For a resource allocation problem,\nexternalities imply that a bundle allocated to an agent may affect the\nutilities of other agents.\n  In this paper, we conduct a study of fair allocation of indivisible goods\nwhen the externalities are not negligible. We present a simple and natural\nmodel, namely \\emph{network externalities}, to capture the externalities. To\nevaluate fairness in the network externalities model, we generalize the idea\nbehind the notion of maximin-share ($\\MMS$) to achieve a new criterion, namely,\n\\emph{extended-maximin-share} ($\\EMMS$). Next, we consider two problems\nconcerning our model.\n  First, we discuss the computational aspects of finding the value of $\\EMMS$\nfor every agent. For this, we introduce a generalized form of partitioning\nproblem that includes many famous partitioning problems such as maximin,\nminimax, and leximin partitioning problems. We show that a $1/2$-approximation\nalgorithm exists for this partitioning problem.\n  Next, we investigate on finding approximately optimal $\\EMMS$ allocations.\nThat is, allocations that guarantee every agent a utility of at least a\nfraction of his extended-maximin-share. We show that under a natural assumption\nthat the agents are $\\alpha$-self-reliant, an $\\alpha/2$-$\\EMMS$ allocation\nalways exists. The combination of this with the former result yields a\npolynomial-time $\\alpha/4$-$\\EMMS$ allocation algorithm. \n\n"}
{"id": "1805.06387", "contents": "Title: Near-Optimal Communication Lower Bounds for Approximate Nash Equilibria Abstract: We prove an $N^{2-o(1)}$ lower bound on the randomized communication\ncomplexity of finding an $\\epsilon$-approximate Nash equilibrium (for constant\n$\\epsilon>0$) in a two-player $N\\times N$ game. \n\n"}
{"id": "1805.07762", "contents": "Title: Selfishness need not be bad: a general proof Abstract: This article studies the user behavior in non-atomic congestion games. We\nconsider non-atomic congestion games with continuous and non-decreasing\nfunctions and investigate the limit of the price of anarchy when the total user\nvolume approaches infinity. We deepen the knowledge on {\\em asymptotically well\ndesigned games} \\cite{Wu2017Selfishness}, {\\em limit games}\n\\cite{Wu2017Selfishness}, {\\em scalability} \\cite{Wu2017Selfishness} and {\\em\ngaugeability} \\cite{Colini2017b} that were recently used in the limit analyses\nof the price of anarchy for non-atomic congestion games. We develop a unified\nframework and derive new techniques that allow a general limit analysis of the\nprice of anarchy. With these new techniques, we are able to prove a global\nconvergence on the price of anarchy for non-atomic congestion games with\narbitrary polynomial price functions and arbitrary user volume vector\nsequences. Moreover, we show that these new techniques are very flexible and\nrobust and apply also to non-atomic congestion games with price functions of\nother types. In particular, we prove that non-atomic congestion games with\nregularly varying price functions are also asymptotically well designed,\nprovided that the price functions are slightly restricted. Our results greatly\ngeneralize recent results. In particular, our results further support the view\nwith a general proof that selfishness need not be bad for non-atomic congestion\ngames. \n\n"}
{"id": "1805.08013", "contents": "Title: Incentive-Compatible Diffusion Abstract: Our work bridges the literature on incentive-compatible mechanism design and\nthe literature on diffusion algorithms. We introduce the study of finding an\nincentive-compatible (strategy-proof) mechanism for selecting an influential\nvertex in a directed graph (e.g. Twitter's network). The goal is to devise a\nmechanism with a bounded ratio between the maximal influence and the influence\nof the selected user, and in which no user can improve its probability of being\nselected by following or unfollowing other users. We introduce the `Two Path'\nmechanism which is based on the idea of selecting the vertex that is the first\nintersection of two independent random walks in the network. The Two Path\nmechanism is incentive compatible on directed acyclic graphs (DAGs), and has a\nfinite approximation ratio on natural subfamilies of DAGs. Simulations indicate\nthat this mechanism is suitable for practical uses. \n\n"}
{"id": "1805.08125", "contents": "Title: A Marketplace for Data: An Algorithmic Solution Abstract: In this work, we aim to design a data marketplace; a robust real-time\nmatching mechanism to efficiently buy and sell training data for Machine\nLearning tasks. While the monetization of data and pre-trained models is an\nessential focus of industry today, there does not exist a market mechanism to\nprice training data and match buyers to sellers while still addressing the\nassociated (computational and other) complexity. The challenge in creating such\na market stems from the very nature of data as an asset: (i) it is freely\nreplicable; (ii) its value is inherently combinatorial due to correlation with\nsignal in other data; (iii) prediction tasks and the value of accuracy vary\nwidely; (iv) usefulness of training data is difficult to verify a priori\nwithout first applying it to a prediction task. As our main contributions we:\n(i) propose a mathematical model for a two-sided data market and formally\ndefine the key associated challenges; (ii) construct algorithms for such a\nmarket to function and analyze how they meet the challenges defined. We\nhighlight two technical contributions: (i) a new notion of 'fairness' required\nfor cooperative games with freely replicable goods; (ii) a truthful, zero\nregret mechanism to auction a class of combinatorial goods based on utilizing\nMyerson's payment function and the Multiplicative Weights algorithm. These\nmight be of independent interest. \n\n"}
{"id": "1805.08195", "contents": "Title: Depth-Limited Solving for Imperfect-Information Games Abstract: A fundamental challenge in imperfect-information games is that states do not\nhave well-defined values. As a result, depth-limited search algorithms used in\nsingle-agent settings and perfect-information games do not apply. This paper\nintroduces a principled way to conduct depth-limited solving in\nimperfect-information games by allowing the opponent to choose among a number\nof strategies for the remainder of the game at the depth limit. Each one of\nthese strategies results in a different set of values for leaf nodes. This\nforces an agent to be robust to the different strategies an opponent may\nemploy. We demonstrate the effectiveness of this approach by building a\nmaster-level heads-up no-limit Texas hold'em poker AI that defeats two prior\ntop agents using only a 4-core CPU and 16 GB of memory. Developing such a\npowerful agent would have previously required a supercomputer. \n\n"}
{"id": "1805.08832", "contents": "Title: The Impact of Uncle Rewards on Selfish Mining in Ethereum Abstract: Many of today's crypto currencies use blockchains as decentralized ledgers\nand secure them with proof of work. In case of a fork of the chain, Bitcoin's\nrule for achieving consensus is selecting the longest chain and discarding the\nother chain as stale. It has been demonstrated that this consensus rule has a\nweakness against selfish mining in which the selfish miner exploits the\nvariance in block generation by partially withholding blocks. In Ethereum,\nhowever, under certain conditions stale blocks don't have to be discarded but\ncan be referenced from the main chain as uncle blocks yielding a partial\nreward. This concept limits the impact of network delays on the expected\nrevenue for miners. But the concept also reduces the risk for a selfish miner\nto gain no rewards from withholding a freshly minted block. This paper uses a\nMonte Carlo simulation to quantify the effect of uncle blocks both to the\nprofitability of selfish mining and the blockchain's security in Ethereum\n(ETH). A brief outlook about a recent Ethereum Classic (ETC) improvement\nproposal that weighs uncle blocks during the selection of the main chain will\nbe given. \n\n"}
{"id": "1805.09480", "contents": "Title: Optimal Algorithms for Continuous Non-monotone Submodular and\n  DR-Submodular Maximization Abstract: In this paper we study the fundamental problems of maximizing a continuous\nnon-monotone submodular function over the hypercube, both with and without\ncoordinate-wise concavity. This family of optimization problems has several\napplications in machine learning, economics, and communication systems. Our\nmain result is the first $\\frac{1}{2}$-approximation algorithm for continuous\nsubmodular function maximization; this approximation factor of $\\frac{1}{2}$ is\nthe best possible for algorithms that only query the objective function at\npolynomially many points. For the special case of DR-submodular maximization,\ni.e. when the submodular functions is also coordinate wise concave along all\ncoordinates, we provide a different $\\frac{1}{2}$-approximation algorithm that\nruns in quasilinear time. Both of these results improve upon prior work [Bian\net al, 2017, Soma and Yoshida, 2017].\n  Our first algorithm uses novel ideas such as reducing the guaranteed\napproximation problem to analyzing a zero-sum game for each coordinate, and\nincorporates the geometry of this zero-sum game to fix the value at this\ncoordinate. Our second algorithm exploits coordinate-wise concavity to identify\na monotone equilibrium condition sufficient for getting the required\napproximation guarantee, and hunts for the equilibrium point using binary\nsearch. We further run experiments to verify the performance of our proposed\nalgorithms in related machine learning applications. \n\n"}
{"id": "1805.10115", "contents": "Title: On incremental deployability Abstract: Motivated by the difficulty of effecting fundamental change in the\narchitecture of the Internet, in this paper, we study from a theoretical\nperspective the question of how individuals can join forces toward collective\nventures. To that end, we draw on an elementary concept in Internet systems\nengineering, namely, that of incremental deployability, which we study\nmathematically and computationally. For example, we show that incremental\ndeployability is at least as general a concept as the Nash equilibrium (in that\nthe latter can be derived from the former). We then draw on this foundation to\ndesign and analyze institutional mechanisms that are not only promising to\nbootstrap emerging Internet architectures but they also have broader\napplications in social organization beyond its predominant market (and\nfinance)-based character. \n\n"}
{"id": "1805.10693", "contents": "Title: Strategyproof Linear Regression in High Dimensions Abstract: This paper is part of an emerging line of work at the intersection of machine\nlearning and mechanism design, which aims to avoid noise in training data by\ncorrectly aligning the incentives of data sources. Specifically, we focus on\nthe ubiquitous problem of linear regression, where strategyproof mechanisms\nhave previously been identified in two dimensions. In our setting, agents have\nsingle-peaked preferences and can manipulate only their response variables. Our\nmain contribution is the discovery of a family of group strategyproof linear\nregression mechanisms in any number of dimensions, which we call generalized\nresistant hyperplane mechanisms. The game-theoretic properties of these\nmechanisms -- and, in fact, their very existence -- are established through a\nconnection to a discrete version of the Ham Sandwich Theorem. \n\n"}
{"id": "1805.10913", "contents": "Title: Combinatorial Auctions with Endowment Effect Abstract: We study combinatorial auctions with bidders that exhibit endowment effect.\nIn most of the previous work on cognitive biases in algorithmic game theory\n(e.g., [Kleinberg and Oren, EC'14] and its follow-ups) the focus was on\nanalyzing the implications and mitigating their negative consequences. In\ncontrast, in this paper we show how in some cases cognitive biases can be\nharnessed to obtain better outcomes.\n  Specifically, we study Walrasian equilibria in combinatorial markets. It is\nwell known that Walrasian equilibria exist only in limited settings, e.g., when\nall valuations are gross substitutes, but fails to exist in more general\nsettings, e.g., when the valuations are submodular. We consider combinatorial\nsettings in which bidders exhibit the endowment effect, that is, their value\nfor items increases with ownership.\n  Our main result shows that when the valuations are submodular, even a mild\ndegree of endowment effect is sufficient to guarantee the existence of\nWalrasian equilibria. In fact, we show that in contrast to Walrasian equilibria\nwith standard utility maximizing bidders -- in which the equilibrium allocation\nmust be efficient -- when bidders exhibit endowment effect any local optimum\ncan be an equilibrium allocation.\n  Our techniques reveal interesting connections between the LP relaxation of\ncombinatorial auctions and local maxima. We also provide lower bounds on the\nintensity of the endowment effect that the bidders must have in order to\nguarantee the existence of a Walrasian equilibrium in various settings. \n\n"}
{"id": "1805.11450", "contents": "Title: Model-based Pricing for Machine Learning in a Data Marketplace Abstract: Data analytics using machine learning (ML) has become ubiquitous in science,\nbusiness intelligence, journalism and many other domains. While a lot of work\nfocuses on reducing the training cost, inference runtime and storage cost of ML\nmodels, little work studies how to reduce the cost of data acquisition, which\npotentially leads to a loss of sellers' revenue and buyers' affordability and\nefficiency.\n  In this paper, we propose a model-based pricing (MBP) framework, which\ninstead of pricing the data, directly prices ML model instances. We first\nformally describe the desired properties of the MBP framework, with a focus on\navoiding arbitrage. Next, we show a concrete realization of the MBP framework\nvia a noise injection approach, which provably satisfies the desired formal\nproperties. Based on the proposed framework, we then provide algorithmic\nsolutions on how the seller can assign prices to models under different market\nscenarios (such as to maximize revenue). Finally, we conduct extensive\nexperiments, which validate that the MBP framework can provide high revenue to\nthe seller, high affordability to the buyer, and also operate on low runtime\ncost. \n\n"}
{"id": "1806.00040", "contents": "Title: Efficient Algorithms and Lower Bounds for Robust Linear Regression Abstract: We study the problem of high-dimensional linear regression in a robust model\nwhere an $\\epsilon$-fraction of the samples can be adversarially corrupted. We\nfocus on the fundamental setting where the covariates of the uncorrupted\nsamples are drawn from a Gaussian distribution $\\mathcal{N}(0, \\Sigma)$ on\n$\\mathbb{R}^d$. We give nearly tight upper bounds and computational lower\nbounds for this problem. Specifically, our main contributions are as follows:\n  For the case that the covariance matrix is known to be the identity, we give\na sample near-optimal and computationally efficient algorithm that outputs a\ncandidate hypothesis vector $\\widehat{\\beta}$ which approximates the unknown\nregression vector $\\beta$ within $\\ell_2$-norm $O(\\epsilon \\log(1/\\epsilon)\n\\sigma)$, where $\\sigma$ is the standard deviation of the random observation\nnoise. An error of $\\Omega (\\epsilon \\sigma)$ is information-theoretically\nnecessary, even with infinite sample size. Prior work gave an algorithm for\nthis problem with sample complexity $\\tilde{\\Omega}(d^2/\\epsilon^2)$ whose\nerror guarantee scales with the $\\ell_2$-norm of $\\beta$.\n  For the case of unknown covariance, we show that we can efficiently achieve\nthe same error guarantee as in the known covariance case using an additional\n$\\tilde{O}(d^2/\\epsilon^2)$ unlabeled examples. On the other hand, an error of\n$O(\\epsilon \\sigma)$ can be information-theoretically attained with\n$O(d/\\epsilon^2)$ samples. We prove a Statistical Query (SQ) lower bound\nproviding evidence that this quadratic tradeoff in the sample size is inherent.\nMore specifically, we show that any polynomial time SQ learning algorithm for\nrobust linear regression (in Huber's contamination model) with estimation\ncomplexity $O(d^{2-c})$, where $c>0$ is an arbitrarily small constant, must\nincur an error of $\\Omega(\\sqrt{\\epsilon} \\sigma)$. \n\n"}
{"id": "1806.00955", "contents": "Title: A Game-Theoretic Approach to Recommendation Systems with Strategic\n  Content Providers Abstract: We introduce a game-theoretic approach to the study of recommendation systems\nwith strategic content providers. Such systems should be fair and stable.\nShowing that traditional approaches fail to satisfy these requirements, we\npropose the Shapley mediator. We show that the Shapley mediator fulfills the\nfairness and stability requirements, runs in linear time, and is the only\neconomically efficient mechanism satisfying these properties. \n\n"}
{"id": "1806.01072", "contents": "Title: A rational decentralized generalized Nash equilibrium seeking for energy\n  markets Abstract: We propose a method to design a decentralized energy market which guarantees\nindividual rationality (IR) in expectation, in the presence of system-level\ngrid constraints. We formulate the market as a welfare maximization problem\nsubject to IR constraints, and we make use of Lagrangian duality to model the\nproblem as a n-person non-cooperative game with a unique generalized Nash\nequilibrium (GNE). We provide a distributed algorithm which converges to the\nGNE. The convergence and properties of the algorithm are investigated by means\nof numerical simulations. \n\n"}
{"id": "1806.02643", "contents": "Title: Re-evaluating Evaluation Abstract: Progress in machine learning is measured by careful evaluation on problems of\noutstanding common interest. However, the proliferation of benchmark suites and\nenvironments, adversarial attacks, and other complications has diluted the\nbasic evaluation model by overwhelming researchers with choices. Deliberate or\naccidental cherry picking is increasingly likely, and designing well-balanced\nevaluation suites requires increasing effort. In this paper we take a step back\nand propose Nash averaging. The approach builds on a detailed analysis of the\nalgebraic structure of evaluation in two basic scenarios: agent-vs-agent and\nagent-vs-task. The key strength of Nash averaging is that it automatically\nadapts to redundancies in evaluation data, so that results are not biased by\nthe incorporation of easy tasks or weak agents. Nash averaging thus encourages\nmaximally inclusive evaluation -- since there is no harm (computational cost\naside) from including all available tasks and agents. \n\n"}
{"id": "1806.02661", "contents": "Title: New mechanism for repeated posted price auction with a strategic buyer\n  without discounting Abstract: On ad exchange platforms the place for advertisement is sold through\ndifferent kinds of auctions. However, it is not uncommon the situation where\nthe seller repeatedly encounters only one buyer, thus the posted price auction\ndegenerates into a monopoly-monopsony game with asymmetric information and\nnearly an infinite number of rounds; on each round the seller proposes the\nprice and the buyer accepts or rejects it.\n  I learned this problem from a discussion with members of Yandex research team\nand my main motivation was to find an incentive-compatible seller's strategy.\nIn this short paper such a strategy is proposed and a corresponding distortion\nat the top type lower bound (Spence-Mirrlees property, actually) for the\nsurplus of the buyer is established; this shows that the proposed strategy is\nthe best possible.\n  The key ingredients are the following. The main leash that the buyer has is\nthe frequency of accepted deals. Once this frequency (as a function on the\nbuyer's type) is fixed, the strategy randomly chooses between the {\\it\nrewarding} price which incentivises the buyer to reveal his type (the higher\nthe type, the more average surplus the buyer has), the {\\it adaptation} price\nwhich allows the buyer to communicate that his type is higher then the current\nguess of the cook, and the {\\it type confirmation} price which disincentivises\nthe buyer to pretend that his type is higher than it is. \n\n"}
{"id": "1806.02771", "contents": "Title: Structural Rounding: Approximation Algorithms for Graphs Near an\n  Algorithmically Tractable Class Abstract: We develop a new framework for generalizing approximation algorithms from the\nstructural graph algorithm literature so that they apply to graphs somewhat\nclose to that class (a scenario we expect is common when working with\nreal-world networks) while still guaranteeing approximation ratios. The idea is\nto $\\textit{edit}$ a given graph via vertex- or edge-deletions to put the graph\ninto an algorithmically tractable class, apply known approximation algorithms\nfor that class, and then $\\textit{lift}$ the solution to apply to the original\ngraph. We give a general characterization of when an optimization problem is\namenable to this approach, and show that it includes many well-studied graph\nproblems, such as Independent Set, Vertex Cover, Feedback Vertex Set, Minimum\nMaximal Matching, Chromatic Number, ($\\ell$-)Dominating Set, Edge\n($\\ell$-)Dominating Set, and Connected Dominating Set.\n  To enable this framework, we develop new editing algorithms that find the\napproximately-fewest edits required to bring a given graph into one of several\nimportant graph classes (in some cases, also approximating the target parameter\nof the family). For bounded degeneracy, we obtain a bicriteria\n$(4,4)$-approximation which also extends to a smoother bicriteria trade-off.\nFor bounded treewidth, we obtain a bicriteria $(O(\\log^{1.5} n), O(\\sqrt{\\log\nw}))$-approximation, and for bounded pathwidth, we obtain a bicriteria\n$(O(\\log^{1.5} n), O(\\sqrt{\\log w} \\cdot \\log n))$-approximation. For treedepth\n$2$ (also related to bounded expansion), we obtain a $4$-approximation. We also\nprove complementary hardness-of-approximation results assuming $\\mathrm{P} \\neq\n\\mathrm{NP}$: in particular, these problems are all log-factor inapproximable,\nexcept the last which is not approximable below some constant factor ($2$\nassuming UGC). \n\n"}
{"id": "1806.03907", "contents": "Title: Reachability for Branching Concurrent Stochastic Games Abstract: We give polynomial time algorithms for deciding almost-sure and limit-sure\nreachability in Branching Concurrent Stochastic Games (BCSGs). These are a\nclass of infinite-state imperfect-information stochastic games that generalize\nboth finite-state concurrent stochastic reachability games, as well as\nbranching simple stochastic reachability games. \n\n"}
{"id": "1806.04067", "contents": "Title: Adaptive Mechanism Design: Learning to Promote Cooperation Abstract: In the future, artificial learning agents are likely to become increasingly\nwidespread in our society. They will interact with both other learning agents\nand humans in a variety of complex settings including social dilemmas. We\nconsider the problem of how an external agent can promote cooperation between\nartificial learners by distributing additional rewards and punishments based on\nobserving the learners' actions. We propose a rule for automatically learning\nhow to create right incentives by considering the players' anticipated\nparameter updates. Using this learning rule leads to cooperation with high\nsocial welfare in matrix games in which the agents would otherwise learn to\ndefect with high probability. We show that the resulting cooperative outcome is\nstable in certain games even if the planning agent is turned off after a given\nnumber of episodes, while other games require ongoing intervention to maintain\nmutual cooperation. However, even in the latter case, the amount of necessary\nadditional incentives decreases over time. \n\n"}
{"id": "1806.05544", "contents": "Title: Constrained existence problem for weak subgame perfect equilibria with\n  omega-regular Boolean objectives (full version) Abstract: We study multiplayer turn-based games played on a finite directed graph such\nthat each player aims at satisfying an omega-regular Boolean objective. Instead\nof the well-known notions of Nash equilibrium (NE) and subgame perfect\nequilibrium (SPE), we focus on the recent notion of weak subgame perfect\nequilibrium (weak SPE), a refinement of SPE. In this setting, players who\ndeviate can only use the subclass of strategies that differ from the original\none on a finite number of histories. We are interested in the constrained\nexistence problem for weak SPEs. We provide a complete characterization of the\ncomputational complexity of this problem: it is P-complete for Explicit Muller\nobjectives, NP-complete for Co-B\\\"uchi, Parity, Muller, Rabin, and Streett\nobjectives, and PSPACE-complete for Reachability and Safety objectives (we only\nprove NP-membership for B\\\"uchi objectives). We also show that the constrained\nexistence problem is fixed parameter tractable and is polynomial when the\nnumber of players is fixed. All these results are based on a fine analysis of a\nfixpoint algorithm that computes the set of possible payoff profiles underlying\nweak SPEs. \n\n"}
{"id": "1806.05799", "contents": "Title: CIA-Towards a Unified Marketing Optimization Framework for e-Commerce\n  Sponsored Search Abstract: As the largest e-commerce platform, Taobao helps advertisers reach billions\nof search queries each day via sponsored search, which has also contributed\nconsiderable revenue to the platform. An efficient bidding strategy to cater to\ndiverse advertiser demands while balancing platform revenue and consumer\nexperience is significant to a healthy and sustainable marketing ecosystem. In\nthis paper we propose \\emph{Customer Intelligent Agent (CIA)}, a bidding\noptimization framework which implements an impression-level bidding to reflect\nadvertisers' conversion willingness and budget control. In this way, CIA is\ncapable of fulfilling various e-commerce advertiser demands on different\nlevels, such as Gross Merchandise Volume optimization, style comparison etc.\nAdditionally, a replay based simulation system is designed to predict the\nperformance of different take-rate. CIA unifies the benefits of three parties\nin the marketing ecosystem without changing the Generalized Second Price\nmechanism. Our extensive offline simulations and large-scale online experiments\non \\emph{Taobao Search Advertising (TSA)} platform verify the high\neffectiveness of the CIA framework. Moreover, CIA has been deployed online as a\nmajor bidding tool in TSA. \n\n"}
{"id": "1806.06173", "contents": "Title: On the Complexity of Detecting Convexity over a Box Abstract: It has recently been shown that the problem of testing global convexity of\npolynomials of degree four is {strongly} NP-hard, answering an open question of\nN.Z. Shor. This result is minimal in the degree of the polynomial when global\nconvexity is of concern. In a number of applications however, one is interested\nin testing convexity only over a compact region, most commonly a box (i.e.,\nhyper-rectangle). In this paper, we show that this problem is also strongly\nNP-hard, in fact for polynomials of degree as low as three. This result is\nminimal in the degree of the polynomial and in some sense justifies why\nconvexity detection in nonlinear optimization solvers is limited to quadratic\nfunctions or functions with special structure. As a byproduct, our proof shows\nthat the problem of testing whether all matrices in an interval family are\npositive semidefinite is strongly NP-hard. This problem, which was previously\nshown to be (weakly) NP-hard by Nemirovski, is of independent interest in the\ntheory of robust control. \n\n"}
{"id": "1806.06230", "contents": "Title: Nonsmooth Aggregative Games with Coupling Constraints and Infinitely\n  Many Classes of Players Abstract: After defining a pure-action profile in a nonatomic aggregative game, where\nplayers have specific compact convex pure-action sets and nonsmooth convex cost\nfunctions, as a square-integrable function, we characterize a Wardrop\nequilibrium as a solution to an infinite-dimensional generalized variational\ninequality. We show the existence of Wardrop equilibrium and variational\nWardrop equilibrium, a concept of equilibrium adapted to the presence of\ncoupling constraints, in monotone nonatomic aggregative games. The uniqueness\nof (variational) Wardrop equilibrium is proved for strictly or aggregatively\nstrictly monotone nonatomic aggregative games. We then show that, for a\nsequence of finite-player aggregative games with aggregative constraints, if\nthe players' pure-action sets converge to those of a strongly (resp.\naggregatively strongly) monotone nonatomic aggregative game, and the\naggregative constraints in the finite-player games converge to the aggregative\nconstraint of the nonatomic game, then a sequence of so-called variational Nash\nequilibria in these finite-player games converge to the variational Wardrop\nequilibrium in pure-action profile (resp. aggregate-action profile). In\nparticular, it allows the construction of an auxiliary sequence of games with\nfinite-dimensional equilibria to approximate the infinite-dimensional\nequilibrium in such a nonatomic game. Finally, we show how to construct\nauxiliary finite-player games for two general classes of nonatomic games. \n\n"}
{"id": "1806.06266", "contents": "Title: On Strategyproof Conference Peer Review Abstract: We consider peer review in a conference setting where there is typically an\noverlap between the set of reviewers and the set of authors. This overlap can\nincentivize strategic reviews to influence the final ranking of one's own\npapers. In this work, we address this problem through the lens of social\nchoice, and present a theoretical framework for strategyproof and efficient\npeer review. We first present and analyze an algorithm for reviewer-assignment\nand aggregation that guarantees strategyproofness and a natural efficiency\nproperty called unanimity, when the authorship graph satisfies a simple\nproperty. Our algorithm is based on the so-called partitioning method, and can\nbe thought as a generalization of this method to conference peer review\nsettings. We then empirically show that the requisite property on the\nauthorship graph is indeed satisfied in the submission data from the ICLR\nconference, and further demonstrate a simple trick to make the partitioning\nmethod more practically appealing for conference peer review. Finally, we\ncomplement our positive results with negative theoretical results where we\nprove that under various ways of strengthening the requirements, it is\nimpossible for any algorithm to be strategyproof and efficient. \n\n"}
{"id": "1806.06996", "contents": "Title: Optimization over Nonnegative and Convex Polynomials With and Without\n  Semidefinite Programming Abstract: The problem of optimizing over the cone of nonnegative polynomials is a\nfundamental problem in computational mathematics, with applications to\npolynomial optimization, control, machine learning, game theory, and\ncombinatorics, among others. A number of breakthrough papers in the early 2000s\nshowed that this problem, long thought to be out of reach, could be tackled by\nusing sum of squares programming. This technique however has proved to be\nexpensive for large-scale problems, as it involves solving large semidefinite\nprograms (SDPs).\n  In the first part of this thesis, we present two methods for approximately\nsolving large-scale sum of squares programs that dispense altogether with\nsemidefinite programming and only involve solving a sequence of linear or\nsecond order cone programs generated in an adaptive fashion. We then focus on\nthe problem of finding tight lower bounds on polynomial optimization problems\n(POPs), a fundamental task in this area that is most commonly handled through\nthe use of SDP-based sum of squares hierarchies (e.g., due to Lasserre and\nParrilo). In contrast to previous approaches, we provide the first theoretical\nframework for constructing converging hierarchies of lower bounds on POPs whose\ncomputation simply requires the ability to multiply certain fixed polynomials\ntogether and to check nonnegativity of the coefficients of their product.\n  In the second part of this thesis, we focus on the theory and applications of\nthe problem of optimizing over convex polynomials, a subcase of the problem of\noptimizing over nonnegative polynomials. (See manuscript for the rest of the\nabstract.) \n\n"}
{"id": "1806.07268", "contents": "Title: Beyond Local Nash Equilibria for Adversarial Networks Abstract: Save for some special cases, current training methods for Generative\nAdversarial Networks (GANs) are at best guaranteed to converge to a `local Nash\nequilibrium` (LNE). Such LNEs, however, can be arbitrarily far from an actual\nNash equilibrium (NE), which implies that there are no guarantees on the\nquality of the found generator or classifier. This paper proposes to model GANs\nexplicitly as finite games in mixed strategies, thereby ensuring that every LNE\nis an NE. With this formulation, we propose a solution method that is proven to\nmonotonically converge to a resource-bounded Nash equilibrium (RB-NE): by\nincreasing computational resources we can find better solutions. We empirically\ndemonstrate that our method is less prone to typical GAN problems such as mode\ncollapse, and produces solutions that are less exploitable than those produced\nby GANs and MGANs, and closely resemble theoretical predictions about NEs. \n\n"}
{"id": "1806.07508", "contents": "Title: Reducibility and Computational Lower Bounds for Problems with Planted\n  Sparse Structure Abstract: The prototypical high-dimensional statistics problem entails finding a\nstructured signal in noise. Many of these problems exhibit an intriguing\nphenomenon: the amount of data needed by all known computationally efficient\nalgorithms far exceeds what is needed for inefficient algorithms that search\nover all possible structures. A line of work initiated by Berthet and Rigollet\nin 2013 has aimed to explain these statistical-computational gaps by reducing\nfrom conjecturally hard average-case problems in computer science. However, the\ndelicate nature of average-case reductions has limited the applicability of\nthis approach. In this work we introduce several new techniques to give a web\nof average-case reductions showing strong computational lower bounds based on\nthe planted clique conjecture using natural problems as intermediates. These\ninclude tight lower bounds for Planted Independent Set, Planted Dense Subgraph,\nSparse Spiked Wigner, Sparse PCA, a subgraph variant of the Stochastic Block\nModel and a biased variant of Sparse PCA. We also give algorithms matching our\nlower bounds and identify the information-theoretic limits of the models we\nconsider. \n\n"}
{"id": "1806.10071", "contents": "Title: Learning Existing Social Conventions via Observationally Augmented\n  Self-Play Abstract: In order for artificial agents to coordinate effectively with people, they\nmust act consistently with existing conventions (e.g. how to navigate in\ntraffic, which language to speak, or how to coordinate with teammates). A\ngroup's conventions can be viewed as a choice of equilibrium in a coordination\ngame. We consider the problem of an agent learning a policy for a coordination\ngame in a simulated environment and then using this policy when it enters an\nexisting group. When there are multiple possible conventions we show that\nlearning a policy via multi-agent reinforcement learning (MARL) is likely to\nfind policies which achieve high payoffs at training time but fail to\ncoordinate with the real group into which the agent enters. We assume access to\na small number of samples of behavior from the true convention and show that we\ncan augment the MARL objective to help it find policies consistent with the\nreal group's convention. In three environments from the literature - traffic,\ncommunication, and team coordination - we observe that augmenting MARL with a\nsmall amount of imitation learning greatly increases the probability that the\nstrategy found by MARL fits well with the existing social convention. We show\nthat this works even in an environment where standard training methods very\nrarely find the true convention of the agent's partners. \n\n"}
{"id": "1806.10952", "contents": "Title: Amortized Analysis of Asynchronous Price Dynamics Abstract: We extend a recently developed framework for analyzing asynchronous\ncoordinate descent algorithms to show that an asynchronous version of\ntatonnement, a fundamental price dynamic widely studied in general equilibrium\ntheory, converges toward a market equilibrium for Fisher markets with CES\nutilities or Leontief utilities, for which tatonnement is equivalent to\ncoordinate descent. \n\n"}
{"id": "1806.11282", "contents": "Title: Approximation Algorithms for Complex-Valued Ising Models on Bounded\n  Degree Graphs Abstract: We study the problem of approximating the Ising model partition function with\ncomplex parameters on bounded degree graphs. We establish a deterministic\npolynomial-time approximation scheme for the partition function when the\ninteractions and external fields are absolutely bounded close to zero.\nFurthermore, we prove that for this class of Ising models the partition\nfunction does not vanish. Our algorithm is based on an approach due to Barvinok\nfor approximating evaluations of a polynomial based on the location of the\ncomplex zeros and a technique due to Patel and Regts for efficiently computing\nthe leading coefficients of graph polynomials on bounded degree graphs.\nFinally, we show how our algorithm can be extended to approximate certain\noutput probability amplitudes of quantum circuits. \n\n"}
{"id": "1807.00304", "contents": "Title: Quality of local equilibria in discrete exchange economies Abstract: This paper defines the notion of a local equilibrium of quality $(r , s)$, $0\n\\leq r , s$, in a discrete exchange economy: a partial allocation and item\nprices that guarantee certain stability properties parametrized by the numbers\n$r$ and $s$. The quality $( r , s )$ measures the fit between the allocation\nand the prices: the larger $r$ and $s$ the closer the fit. For $r , s \\leq 1$\nthis notion provides a graceful degradation for the conditional equilibria of\n[10] which are exactly the local equilibria of quality $( 1 , 1 )$. For $1 < r\n, s $ the local equilibria of quality $( r , s )$ are {\\em more stable} than\nconditional equilibria. Any local equilibrium of quality $( r , s )$ provides,\nwithout any assumption on the type of the agents' valuations, an allocation\nwhose value is at least $\\frac{r s} { 1 + r s }$ the optimal fractional\nallocation. In any economy in which all agents' valuations are $a$-submodular,\ni.e., exhibit complementarity bounded by $a \\: \\geq \\: 1$, there is a local\nequilibrium of quality $( \\frac{1} {a} , \\frac{1}{a} )$. In such an economy any\ngreedy allocation provides a local equilibrium of quality $( 1 , \\frac{1}{a} )\n$. Walrasian equilibria are not amenable to such graceful degradation. \n\n"}
{"id": "1807.00472", "contents": "Title: Linear algebraic structure of zero-determinant strategies in repeated\n  games Abstract: Zero-determinant (ZD) strategies, a recently found novel class of strategies\nin repeated games, has attracted much attention in evolutionary game theory. A\nZD strategy unilaterally enforces a linear relation between average payoffs of\nplayers. Although existence and evolutional stability of ZD strategies have\nbeen studied in simple games, their mathematical properties have not been\nwell-known yet. For example, what happens when more than one players employ ZD\nstrategies have not been clarified. In this paper, we provide a general\nframework for investigating situations where more than one players employ ZD\nstrategies in terms of linear algebra. First, we theoretically prove that a set\nof linear relations of average payoffs enforced by ZD strategies always has\nsolutions, which implies that incompatible linear relations are impossible.\nSecond, we prove that linear payoff relations are independent of each other\nunder some conditions. These results hold for general games with public\nmonitoring including perfect-monitoring games. Furthermore, we provide a simple\nexample of a two-player game in which one player can simultaneously enforce two\nlinear relations, that is, simultaneously control her and her opponent's\naverage payoffs. All of these results elucidate general mathematical properties\nof ZD strategies. \n\n"}
{"id": "1807.01343", "contents": "Title: Utility Design for Distributed Resource Allocation -- Part II:\n  Applications to Submodular, Covering, and Supermodular Problems Abstract: A fundamental component of the game theoretic approach to distributed control\nis the design of local utility functions.Relative to resource allocation\nproblems that are additive over the resources, Part I showed how to design\nlocal utilities so as to maximize the associated performance guarantees\n[Paccagnan et al., TAC 2019] which we measure by the price of anarchy. The\npurpose of the present manuscript is to specialize these results to the case of\nsubmodular, covering, and supermodular problems. In all these cases we obtain\ntight expressions for the price of anarchy that often match or improve the\nguarantees associated to state-of-the-art approximation algorithms. Two\napplications and corresponding numerics are presented: the vehicle-target\nassignment problem and a coverage problem arising in wireless data caching. \n\n"}
{"id": "1807.01389", "contents": "Title: Efficient Rational Proofs with Strong Utility-Gap Guarantees Abstract: As modern computing moves towards smaller devices and powerful cloud\nplatforms, more and more computation is being delegated to powerful service\nproviders. Interactive proofs are a widely-used model to design efficient\nprotocols for verifiable computation delegation. Rational proofs are\npayment-based interactive proofs. The payments are designed to incentivize the\nprovers to give correct answers. If the provers misreport the answer then they\nincur a payment loss of at least 1/u, where u is the utility gap of the\nprotocol.\n  In this work, we tightly characterize the power of rational proofs that are\nsuper efficient, that is, require only logarithmic time and communication for\nverification. We also characterize the power of single-round rational protocols\nthat require only logarithmic space and randomness for verification. Our\nprotocols have strong (that is, polynomial, logarithmic, and even constant)\nutility gap. Finally, we show when and how rational protocols can be converted\nto give the completeness and soundness guarantees of classical interactive\nproofs. \n\n"}
{"id": "1807.01732", "contents": "Title: Recommendation Systems and Self Motivated Users Abstract: Modern recommendation systems rely on the wisdom of the crowd to learn the\noptimal course of action. This induces an inherent mis-alignment of incentives\nbetween the system's objective to learn (explore) and the individual users'\nobjective to take the contemporaneous optimal action (exploit). The design of\nsuch systems must account for this and also for additional information\navailable to the users. A prominent, yet simple, example is when agents arrive\nsequentially and each agent observes the action and reward of his predecessor.\nWe provide an incentive compatible and asymptotically optimal mechanism for\nthat setting. The complexity of the mechanism suggests that the design of such\nsystems for general settings is a challenging task. \n\n"}
{"id": "1807.05194", "contents": "Title: An Algorithmic Blend of LPs and Ring Equations for Promise CSPs Abstract: Promise CSPs are a relaxation of constraint satisfaction problems where the\ngoal is to find an assignment satisfying a relaxed version of the constraints.\nSeveral well-known problems can be cast as promise CSPs including approximate\ngraph coloring, discrepancy minimization, and interesting variants of\nsatisfiability. Similar to CSPs, the tractability of promise CSPs can be tied\nto the structure of operations on the solution space called polymorphisms,\nthough in the promise world these operations are much less constrained. Under\nthe thesis that non-trivial polymorphisms govern tractability, promise CSPs\ntherefore provide a fertile ground for the discovery of novel algorithms.\n  In previous work, we classified Boolean promise CSPs when the constraint\npredicates are symmetric. In this work, we vastly generalize these algorithmic\nresults. Specifically, we show that promise CSPs that admit a family of\n\"regional-periodic\" polymorphisms are in P, assuming that determining which\nregion a point is in can be computed in polynomial time. Such polymorphisms are\nquite general and are obtained by gluing together several functions that are\nperiodic in the Hamming weights in different blocks of the input.\n  Our algorithm is based on a novel combination of linear programming and\nsolving linear systems over rings. We also abstract a framework based on\nreducing a promise CSP to a CSP over an infinite domain, solving it there, and\nthen rounding the solution to an assignment for the promise CSP instance. The\nrounding step is intimately tied to the family of polymorphisms and clarifies\nthe connection between polymorphisms and algorithms in this context. As a key\ningredient, we introduce the technique of finding a solution to a linear\nprogram with integer coefficients that lies in a different ring (such as\n$\\mathbb Z[\\sqrt{2}]$) to bypass ad-hoc adjustments for lying on a rounding\nboundary. \n\n"}
{"id": "1807.06456", "contents": "Title: Quantum Chebyshev's Inequality and Applications Abstract: In this paper we provide new quantum algorithms with polynomial speed-up for\na range of problems for which no such results were known, or we improve\nprevious algorithms. First, we consider the approximation of the frequency\nmoments $F_k$ of order $k \\geq 3$ in the multi-pass streaming model with\nupdates (turnstile model). We design a $P$-pass quantum streaming algorithm\nwith memory $M$ satisfying a tradeoff of $P^2 M = \\tilde{O}(n^{1-2/k})$,\nwhereas the best classical algorithm requires $P M = \\Theta(n^{1-2/k})$. Then,\nwe study the problem of estimating the number $m$ of edges and the number $t$\nof triangles given query access to an $n$-vertex graph. We describe optimal\nquantum algorithms that perform $\\tilde{O}(\\sqrt{n}/m^{1/4})$ and\n$\\tilde{O}(\\sqrt{n}/t^{1/6} + m^{3/4}/\\sqrt{t})$ queries respectively. This is\na quadratic speed-up compared to the classical complexity of these problems.\n  For this purpose we develop a new quantum paradigm that we call Quantum\nChebyshev's inequality. Namely we demonstrate that, in a certain model of\nquantum sampling, one can approximate with relative error the mean of any\nrandom variable with a number of quantum samples that is linear in the ratio of\nthe square root of the variance to the mean. Classically the dependency is\nquadratic. Our algorithm subsumes a previous result of Montanaro [Mon15]. This\nnew paradigm is based on a refinement of the Amplitude Estimation algorithm of\nBrassard et al. [BHMT02] and of previous quantum algorithms for the mean\nestimation problem. We show that this speed-up is optimal, and we identify\nanother common model of quantum sampling where it cannot be obtained. For our\napplications, we also adapt the variable-time amplitude amplification technique\nof Ambainis [Amb10] into a variable-time amplitude estimation algorithm. \n\n"}
{"id": "1807.06666", "contents": "Title: Payoff Control in the Iterated Prisoner's Dilemma Abstract: Repeated game has long been the touchstone model for agents' long-run\nrelationships. Previous results suggest that it is particularly difficult for a\nrepeated game player to exert an autocratic control on the payoffs since they\nare jointly determined by all participants. This work discovers that the scale\nof a player's capability to unilaterally influence the payoffs may have been\nmuch underestimated. Under the conventional iterated prisoner's dilemma, we\ndevelop a general framework for controlling the feasible region where the\nplayers' payoff pairs lie. A control strategy player is able to confine the\npayoff pairs in her objective region, as long as this region has feasible\nlinear boundaries. With this framework, many well-known existing strategies can\nbe categorized and various new strategies with nice properties can be further\nidentified. We show that the control strategies perform well either in a\ntournament or against a human-like opponent. \n\n"}
{"id": "1807.07170", "contents": "Title: How Consumer Empathy Assist Power Grid in Demand Response Abstract: This paper investigates interaction among residential electricity users and\nutility company in a distribution network with the capability of two-way\ncommunication provided by smart grid. The energy consumption scheduling of\nelectricity users is formulated as a game-theoretic problem within a group\nwhere all players are not totally selfish. Considering altruistic behavior of\nhuman decision-making, altruistic player action to other players actions can be\ninfluenced by recognizing the well being of others. The proposed model captures\nthe empathy of electricity users in energy consumption scheduling and how this\nbehavior affect peak demand and electricity prices. Numerical results\ndemonstrate that both residential users and utility company can benefit through\nthe channel of empathy. \n\n"}
{"id": "1807.07794", "contents": "Title: Kripke Semantics of the Perfectly Transparent Equilibrium Abstract: The Perfectly Transparent Equilibrium is algorithmically defined, for any\ngame in normal form with perfect information and no ties, as the iterated\ndeletion of non-individually-rational strategy profiles until at most one\nremains. In this paper, we characterize the Perfectly Transparent Equilibrium\nwith adapted Kripke models having necessary rationality, necessary knowledge of\nstrategies as well as eventual logical omniscience.\n  Eventual logical omniscience is introduced as a weaker version of perfect\nlogical omniscience, with logical omniscience being quantized and fading away\ncounterfactually. It is the price to pay for necessary factual omniscience and\nnecessary rationality: we conjecture that epistemic omniscience, logical\nomniscience and necessary rationality form an impossibility triangle.\n  We consider multimodal classes of Kripke structures, with respect to agents,\nbut also in the sense that we have both epistemic and logical accessibility\nrelations. Knowledge is defined in terms of the former, while necessity is\ndefined in terms of the latter. Lewisian closest-state functions, which are not\nrestricted to unilateral deviations, model counterfactuals.\n  We use impossible possible worlds \\`a la Rantala to model that some strategy\nprofiles cannot possibly be reached in some situations. Eventual logical\nomniscience is then bootstrapped with the agents' considering that, at\nlogically possible, but non-normal worlds \\`a la Kripke, any world is logically\naccessible and thus any deviation of strategy is possible. As in known in\nliterature, under rationality and knowledge of strategies, these worlds\ncharacterize individual rationality. Then, in normal worlds, higher levels of\nlogical omniscience characterize higher levels of individual rationality, and a\nhigh-enough level of logical omniscience characterizes, when it exists, the\nPerfectly Transparent Equilibrium. \n\n"}
{"id": "1807.08949", "contents": "Title: A Note on Clustering Aggregation for Binary Clusterings Abstract: We consider the clustering aggregation problem in which we are given a set of\nclusterings and want to find an aggregated clustering which minimizes the sum\nof mismatches to the input clusterings. In the binary case (each clustering is\na bipartition) this problem was known to be NP-hard under Turing reductions. We\nstrengthen this result by providing a polynomial-time many-one reduction. Our\nresult also implies that no $2^{o(n)}\\cdot |I'|^{O(1)}$-time algorithm exists\nthat solves any given clustering instance $I'$ with $n$ elements, unless the\n\\ETH{} fails. On the positive side, we show that the problem is fixed-parameter\ntractable with respect to the number of input clusterings and we give an\ninteger linear programming formulation. \n\n"}
{"id": "1807.10546", "contents": "Title: Universal trees grow inside separating automata: Quasi-polynomial lower\n  bounds for parity games Abstract: Several distinct techniques have been proposed to design quasi-polynomial\nalgorithms for solving parity games since the breakthrough result of Calude,\nJain, Khoussainov, Li, and Stephan (2017): play summaries, progress measures\nand register games. We argue that all those techniques can be viewed as\ninstances of the separation approach to solving parity games, a key technical\ncomponent of which is constructing (explicitly or implicitly) an automaton that\nseparates languages of words encoding plays that are (decisively) won by either\nof the two players. Our main technical result is a quasi-polynomial lower bound\non the size of such separating automata that nearly matches the current best\nupper bounds. This forms a barrier that all existing approaches must overcome\nin the ongoing quest for a polynomial-time algorithm for solving parity games.\nThe key and fundamental concept that we introduce and study is a universal\nordered tree. The technical highlights are a quasi-polynomial lower bound on\nthe size of universal ordered trees and a proof that every separating safety\nautomaton has a universal tree hidden in its state space. \n\n"}
{"id": "1807.10684", "contents": "Title: Fair allocation of combinations of indivisible goods and chores Abstract: We consider the problem of fairly dividing a set of items. Much of the fair\ndivision literature assumes that the items are `goods' i.e., they yield\npositive utility for the agents. There is also some work where the items are\n`chores' that yield negative utility for the agents. In this paper, we consider\na more general scenario where an agent may have negative or positive utility\nfor each item. This framework captures, e.g., fair task assignment, where\nagents can have both positive and negative utilities for each task. We show\nthat whereas some of the positive axiomatic and computational results extend to\nthis more general setting, others do not. We present several new and efficient\nalgorithms for finding fair allocations in this general setting. We also point\nout several gaps in the literature regarding the existence of allocations\nsatisfying certain fairness and efficiency properties and further study the\ncomplexity of computing such allocations. \n\n"}
{"id": "1807.10878", "contents": "Title: Revealed Preference Dimension via Matrix Sign Rank Abstract: Given a data-set of consumer behaviour, the Revealed Preference Graph\nsuccinctly encodes inferred relative preferences between observed outcomes as a\ndirected graph. Not all graphs can be constructed as revealed preference graphs\nwhen the market dimension is fixed. This paper solves the open problem of\ndetermining exactly which graphs are attainable as revealed preference graphs\nin $d$-dimensional markets. This is achieved via an exact characterization\nwhich closely ties the feasibility of the graph to the Matrix Sign Rank of its\nsigned adjacency matrix. The paper also shows that when the preference\nrelations form a partially ordered set with order-dimension $k$, the graph is\nattainable as a revealed preference graph in a $k$-dimensional market. \n\n"}
{"id": "1807.11518", "contents": "Title: Parameterized Orientable Deletion Abstract: A graph is $d$-orientable if its edges can be oriented so that the maximum\nin-degree of the resulting digraph is at most $d$. $d$-orientability is a\nwell-studied concept with close connections to fundamental graph-theoretic\nnotions and applications as a load balancing problem. In this paper we consider\nthe d-ORIENTABLE DELETION problem: given a graph $G=(V,E)$, delete the minimum\nnumber of vertices to make $G$ $d$-orientable. We contribute a number of\nresults that improve the state of the art on this problem. Specifically:\n  - We show that the problem is W[2]-hard and $\\log n$-inapproximable with\nrespect to $k$, the number of deleted vertices. This closes the gap in the\nproblem's approximability.\n  - We completely characterize the parameterized complexity of the problem on\nchordal graphs: it is FPT parameterized by $d+k$, but W-hard for each of the\nparameters $d,k$ separately.\n  - We show that, under the SETH, for all $d,\\epsilon$, the problem does not\nadmit a $(d+2-\\epsilon)^{tw}$, algorithm where $tw$ is the graph's treewidth,\nresolving as a special case an open problem on the complexity of PSEUDOFOREST\nDELETION.\n  - We show that the problem is W-hard parameterized by the input graph's\nclique-width. Complementing this, we provide an algorithm running in time\n$d^{O(d\\cdot cw)}$, showing that the problem is FPT by $d+cw$, and improving\nthe previously best known algorithm for this case. \n\n"}
{"id": "1808.00422", "contents": "Title: Almost Envy Freeness and Welfare Efficiency in Fair Division with Goods\n  or Bads Abstract: We consider two models of fair division with indivisible items: one for goods\nand one for bads. For goods, we study two generalized envy freeness proxies\n(EF1 and EFX for goods) and three common welfare (utilitarian, egalitarian and\nNash) efficiency notions. For bads, we study two generalized envy freeness\nproxies (1EF and XEF for goods) and two less common diswelfare (egalitarian and\nNash) efficiency notions. Some existing algorithms for goods do not work for\nbads. We thus propose several new algorithms for the model with bads. Our new\nalgorithms exhibit many nice properties. For example, with additive identical\nvaluations, an allocation that maximizes the egalitarian diswelfare or Nash\ndiswelfare is XEF and PE. Finally, we also give simple and tractable cases when\nthese envy freeness proxies and welfare efficiency are attainable in\ncombination (e.g. binary valuations, house allocations). \n\n"}
{"id": "1808.02359", "contents": "Title: On the Computational Complexity of Length- and Neighborhood-Constrained\n  Path Problems Abstract: Finding paths in graphs is a fundamental graph-theoretic task. In this work,\nwe we are concerned with finding a path with some constraints on its length and\nthe number of vertices neighboring the path, that is, being outside of and\nincident with the path. Herein, we consider short and long paths on the one\nside, and small and large neighborhoods on the other side---yielding four\ndecision problems. We show that all four problems are NP-complete, even in\nplanar graphs with small maximum degree. Moreover, we study all four variants\nwhen parameterized by a bound $k$ on the length of the path, by a bound $\\ell$\non the size of neighborhood, and by $k + \\ell$. \n\n"}
{"id": "1808.02696", "contents": "Title: The roll call interpretation of the Shapley value Abstract: The Shapley value is commonly illustrated by roll call votes in which players\nsupport or reject a proposal in sequence. If all sequences are equiprobable, a\nvoter's Shapley value can be interpreted as the probability of being pivotal,\ni.e., to bring about the required majority or to make this impossible for\nothers. We characterize the joint probability distributions over cooperation\npatterns that permit this roll call interpretation: individual votes may be\ninterdependent but must be exchangeable. \n\n"}
{"id": "1808.03494", "contents": "Title: On the Complexity of Solving Subtraction Games Abstract: We study algorithms for solving Subtraction games, which sometimes are\nreferred to as one-heap Nim games. We describe a quantum algorithm which is\napplicable to any game on DAG, and show that its query compexity for solving an\narbitrary Subtraction game of $n$ stones is $O(n^{3/2}\\log n)$. The best known\ndeterministic algorithms for solving such games are based on the dynamic\nprogramming approach. We show that this approach is asymptotically optimal and\nthat classical query complexity for solving a Subtraction game is generally\n$\\Theta(n^2)$. This paper perhaps is the first explicit \"quantum\" contribution\nto algorithmic game theory. \n\n"}
{"id": "1808.04067", "contents": "Title: Game Theoretic Analysis for Joint Sponsored and Edge Caching Content\n  Service Market Abstract: With a sponsored content scheme in a wireless network, a sponsored content\nservice provider can pay to a network operator on behalf of the mobile\nusers/subscribers to lower down the network subscription fees at the reasonable\ncost in terms of receiving some amount of advertisements. As such, content\nproviders, network operators and mobile users are all actively motivated to\nparticipate in the sponsored content ecosystem. Meanwhile, in 5G cellular\nnetworks, caching technique is employed to improve content service quality,\nwhich stores potentially popular contents on edge networks nodes to serve\nmobile users. In this work, we propose the joint sponsored and edge caching\ncontent service market model. We investigate an interplay between the sponsored\ncontent service provider and the edge caching content service provider under\nthe non-cooperative game framework. Furthermore, a three-stage Stackelberg game\nis formulated to model the interactions among the network operator, content\nservice provider, and mobile users. Sub-game perfect equilibrium in each stage\nis analyzed by backward induction. The existence of Stackelberg equilibrium is\nvalidated by employing the bilevel optimization programming. Based on the game\nproperties, we propose a sub-gradient based iterative algorithm, which ensures\nto converge to the Stackelberg equilibrium. \n\n"}
{"id": "1808.04701", "contents": "Title: Measuring Market Performance with Stochastic Demand: Price of Anarchy\n  and Price of Uncertainty Abstract: Globally operating suppliers face the rising challenge of wholesale pricing\nunder scarce data about retail demand, in contrast to better informed, locally\noperating retailers. At the same time, as local businesses proliferate, markets\ncongest and retail competition increases. To capture these strategic\nconsiderations, we employ the classic Cournot model and extend it to a\ntwo-stage supply chain with an upstream supplier who operates under demand\nuncertainty and multiple downstream retailers who compete over quantity. The\nsupplier's belief about retail demand is modeled via a continuous probability\ndistribution function F. If F has the decreasing generalized mean residual life\nproperty, then the supplier's optimal pricing policy exists and is the unique\nfixed point of the mean residual life function. We evaluate the realized Price\nof Uncertainty and show that there exist demand levels for which market\nperforms better when the supplier prices under demand uncertainty. In general,\nperformance worsens for lower values of realized demand. We examine the effects\nof increasing competition on supply chain efficiency via the realized Price of\nAnarchy and complement our findings with numerical results. \n\n"}
{"id": "1808.06015", "contents": "Title: Ultra Reliable, Low Latency Vehicle-to-Infrastructure Wireless\n  Communications with Edge Computing Abstract: Ultra reliable, low latency vehicle-to-infrastructure (V2I) communications is\na key requirement for seamless operation of autonomous vehicles (AVs) in future\nsmart cities. To this end, cellular small base stations (SBSs) with edge\ncomputing capabilities can reduce the end-to-end (E2E) service delay by\nprocessing requested tasks from AVs locally, without forwarding the tasks to a\nremote cloud server. Nonetheless, due to the limited computational capabilities\nof the SBSs, coupled with the scarcity of the wireless bandwidth resources,\nminimizing the E2E latency for AVs and achieving a reliable V2I network is\nchallenging. In this paper, a novel algorithm is proposed to jointly optimize\nAVs-to-SBSs association and bandwidth allocation to maximize the reliability of\nthe V2I network. By using tools from labor matching markets, the proposed\nframework can effectively perform distributed association of AVs to SBSs, while\naccounting for the latency needs of AVs as well as the limited computational\nand bandwidth resources of SBSs. Moreover, the convergence of the proposed\nalgorithm to a core allocation between AVs and SBSs is proved and its ability\nto capture interdependent computational and transmission latencies for AVs in a\nV2I network is characterized. Simulation results show that by optimizing the\nE2E latency, the proposed algorithm substantially outperforms conventional cell\nassociation schemes, in terms of service reliability and latency. \n\n"}
{"id": "1808.06979", "contents": "Title: Thresholding at the monopoly price: an agnostic way to improve bidding\n  strategies in revenue-maximizing auctions Abstract: We address the problem of improving bidders' strategies in prior-dependent\nrevenue-maximizing auctions and introduce a simple and generic method to design\nnovel bidding strategies if the seller uses past bids to optimize her\nmechanism. We propose a simple and agnostic strategy, independent of the\ndistribution of the competition, that is robust to mechanism changes and local\n(as opposed to global) optimization of e.g. reserve prices by the seller. This\nstrategy guarantees an increase in utility compared to the truthful strategy\nfor any distribution of the competition. In textbook-style examples, for\ninstance with uniform [0,1] value distributions and two bidders, this\nno-side-information and mechanism-independent strategy yields an enormous 57%\nincrease in buyer utility for lazy second price auctions with monopoly\nreserves. When the bidder knows the distribution of the highest bid of the\ncompetition, we show how to optimize the tradeoff between reducing the reserve\nprice and beating the competition. Our formulation enables to study some\nimportant robustness properties of the strategies, showing their impact even\nwhen the seller is using a data-driven approach to set the reserve prices. In\nthis sample-size setting, we prove under what conditions, thresholding bidding\nstrategies can still improve the buyer's utility. The gist of our approach is\nto see optimal auctions in practice as a Stackelberg game where the buyer is\nthe leader, as he is the first one to move (here bid) when the seller is the\nfollower as she has no prior information on the bidder. \n\n"}
{"id": "1808.08905", "contents": "Title: Fair redistricting is hard Abstract: Gerrymandering is a long-standing issue within the U.S. political system, and\nit has received scrutiny recently by the U.S. Supreme Court. In this note, we\nprove that deciding whether there exists a fair redistricting among legal maps\nis NP-hard. To make this precise, we use simplified notions of \"legal\" and\n\"fair\" that account for desirable traits such as geographic compactness of\ndistricts and sufficient representation of voters. The proof of our result is\ninspired by the work of Mahanjan, Minbhorkar and Varadarajan that proves that\nplanar k-means is NP-hard. \n\n"}
{"id": "1808.09057", "contents": "Title: Loss Functions, Axioms, and Peer Review Abstract: It is common to see a handful of reviewers reject a highly novel paper,\nbecause they view, say, extensive experiments as far more important than\nnovelty, whereas the community as a whole would have embraced the paper. More\ngenerally, the disparate mapping of criteria scores to final recommendations by\ndifferent reviewers is a major source of inconsistency in peer review. In this\npaper we present a framework inspired by empirical risk minimization (ERM) for\nlearning the community's aggregate mapping. The key challenge that arises is\nthe specification of a loss function for ERM. We consider the class of $L(p,q)$\nloss functions, which is a matrix-extension of the standard class of $L_p$\nlosses on vectors; here the choice of the loss function amounts to choosing the\nhyperparameters $p, q \\in [1,\\infty]$. To deal with the absence of ground truth\nin our problem, we instead draw on computational social choice to identify\ndesirable values of the hyperparameters $p$ and $q$. Specifically, we\ncharacterize $p=q=1$ as the only choice of these hyperparameters that satisfies\nthree natural axiomatic properties. Finally, we implement and apply our\napproach to reviews from IJCAI 2017. \n\n"}
{"id": "1808.09226", "contents": "Title: A Note on the Complexity of Manipulating Weighted Schulze Voting Abstract: We prove that the constructive weighted coalitional manipulation problem for\nthe Schulze voting rule can be solved in polynomial time for an unbounded\nnumber of candidates and an unbounded number of manipulators. \n\n"}
{"id": "1808.09406", "contents": "Title: Almost Envy-Free Allocations with Connected Bundles Abstract: We study the existence of allocations of indivisible goods that are envy-free\nup to one good (EF1), under the additional constraint that each bundle needs to\nbe connected in an underlying item graph. If the graph is a path and the\nutility functions are monotonic over bundles, we show the existence of EF1\nallocations for at most four agents, and the existence of EF2 allocations for\nany number of agents; our proofs involve discrete analogues of the Stromquist's\nmoving-knife protocol and the Su--Simmons argument based on Sperner's lemma.\nFor identical utilities, we provide a polynomial-time algorithm that computes\nan EF1 allocation for any number of agents. For the case of two agents, we\ncharacterize the class of graphs that guarantee the existence of EF1\nallocations as those whose biconnected components are arranged in a path; this\nproperty can be checked in linear time. \n\n"}
{"id": "1809.01130", "contents": "Title: Nash equilibrium in asymmetric multi-players zero-sum game with two\n  strategic variables and only one alien Abstract: We consider a partially asymmetric multi-players zero-sum game with two\nstrategic variables. All but one players have the same payoff functions, and\none player (Player $n$) does not. Two strategic variables are $t_i$'s and\n$s_i$'s for each player $i$. Mainly we will show the following results. 1) The\nequilibrium when all players choose $t_i$'s is equivalent to the equilibrium\nwhen all but one players choose $t_i$'s and Player $n$ chooses $s_n$ as their\nstrategic variables. 2) The equilibrium when all players choose $s_i$'s is\nequivalent to the equilibrium when all but one players choose $s_i$'s and\nPlayer $n$ chooses $t_n$ as their strategic variables. The equilibrium when all\nplayers choose $t_i$'s and the equilibrium when all players choose $s_i$'s are\nnot equivalent although they are equivalent in a symmetric game in which all\nplayers have the same payoff functions. \n\n"}
{"id": "1809.01803", "contents": "Title: A Bridge between Liquid and Social Welfare in Combinatorial Auctions\n  with Submodular Bidders Abstract: We study incentive compatible mechanisms for Combinatorial Auctions where the\nbidders have submodular (or XOS) valuations and are budget-constrained. Our\nobjective is to maximize the \\emph{liquid welfare}, a notion of efficiency for\nbudget-constrained bidders introduced by Dobzinski and Paes Leme (2014). We\nshow that some of the known truthful mechanisms that best-approximate the\nsocial welfare for Combinatorial Auctions with submodular bidders through\ndemand query oracles can be adapted, so that they retain truthfulness and\nachieve asymptotically the same approximation guarantees for the liquid\nwelfare. More specifically, for the problem of optimizing the liquid welfare in\nCombinatorial Auctions with submodular bidders, we obtain a universally\ntruthful randomized $O(\\log m)$-approximate mechanism, where $m$ is the number\nof items, by adapting the mechanism of Krysta and V\\\"ocking (2012).\n  Additionally, motivated by large market assumptions often used in mechanism\ndesign, we introduce a notion of competitive markets and show that in such\nmarkets, liquid welfare can be approximated within a constant factor by a\nrandomized universally truthful mechanism. Finally, in the Bayesian setting, we\nobtain a truthful $O(1)$-approximate mechanism for the case where bidder\nvaluations are generated as independent samples from a known distribution, by\nadapting the results of Feldman, Gravin and Lucier (2014). \n\n"}
{"id": "1809.02931", "contents": "Title: From Recommendation Systems to Facility Location Games Abstract: Recommendation systems are extremely popular tools for matching users and\ncontents. However, when content providers are strategic, the basic principle of\nmatching users to the closest content, where both users and contents are\nmodeled as points in some semantic space, may yield low social welfare. This is\ndue to the fact that content providers are strategic and optimize their offered\ncontent to be recommended to as many users as possible. Motivated by modern\napplications, we propose the widely studied framework of facility location\ngames to study recommendation systems with strategic content providers. Our\nconceptual contribution is the introduction of a $\\textit{mediator}$ to\nfacility location models, in the pursuit of better social welfare. We aim at\ndesigning mediators that a) induce a game with high social welfare in\nequilibrium, and b) intervene as little as possible. In service of the latter,\nwe introduce the notion of $\\textit{intervention cost}$, which quantifies how\nmuch damage a mediator may cause to the social welfare when an off-equilibrium\nprofile is adopted. As a case study in high-welfare low-intervention mediator\ndesign, we consider the one-dimensional segment as the user domain. We propose\na mediator that implements the socially optimal strategy profile as the unique\nequilibrium profile, and show a tight bound on its intervention cost.\nUltimately, we consider some extensions, and highlight open questions for the\ngeneral agenda. \n\n"}
{"id": "1809.03057", "contents": "Title: Variance Reduction in Monte Carlo Counterfactual Regret Minimization\n  (VR-MCCFR) for Extensive Form Games using Baselines Abstract: Learning strategies for imperfect information games from samples of\ninteraction is a challenging problem. A common method for this setting, Monte\nCarlo Counterfactual Regret Minimization (MCCFR), can have slow long-term\nconvergence rates due to high variance. In this paper, we introduce a variance\nreduction technique (VR-MCCFR) that applies to any sampling variant of MCCFR.\nUsing this technique, per-iteration estimated values and updates are\nreformulated as a function of sampled values and state-action baselines,\nsimilar to their use in policy gradient reinforcement learning. The new\nformulation allows estimates to be bootstrapped from other estimates within the\nsame episode, propagating the benefits of baselines along the sampled\ntrajectory; the estimates remain unbiased even when bootstrapping from other\nestimates. Finally, we show that given a perfect baseline, the variance of the\nvalue estimates can be reduced to zero. Experimental evaluation shows that\nVR-MCCFR brings an order of magnitude speedup, while the empirical variance\ndecreases by three orders of magnitude. The decreased variance allows for the\nfirst time CFR+ to be used with sampling, increasing the speedup to two orders\nof magnitude. \n\n"}
{"id": "1809.03066", "contents": "Title: Multi-agent online learning in time-varying games Abstract: We examine the long-run behavior of multi-agent online learning in games that\nevolve over time. Specifically, we focus on a wide class of policies based on\nmirror descent, and we show that the induced sequence of play (a) converges to\nNash equilibrium in time-varying games that stabilize in the long run to a\nstrictly monotone limit; and (b) it stays asymptotically close to the evolving\nequilibrium of the sequence of stage games (assuming they are strongly\nmonotone). Our results apply to both gradient-based and payoff-based feedback -\ni.e., the \"bandit feedback\" case where players only get to observe the payoffs\nof their chosen actions. \n\n"}
{"id": "1809.03887", "contents": "Title: Quantitative Reductions and Vertex-Ranked Infinite Games Abstract: We introduce quantitative reductions, a novel technique for structuring the\nspace of quantitative games and solving them that does not rely on a reduction\nto qualitative games. We show that such reductions exhibit the same desirable\nproperties as their qualitative counterparts and additionally retain the\noptimality of solutions. Moreover, we introduce vertex-ranked games as a\ngeneral-purpose target for quantitative reductions and show how to solve them.\nIn such games, the value of a play is determined only by a qualitative winning\ncondition and a ranking of the vertices.\n  We provide quantitative reductions of quantitative request-response games to\nvertex-ranked games, thus showing ExpTime-completeness of solving the former\ngames. Furthermore, we exhibit the usefulness and flexibility of vertex-ranked\ngames by showing how to use such games to compute fault-resilient strategies\nfor safety specifications. This work lays the foundation for a general study of\nfault-resilient strategies for more complex winning conditions \n\n"}
{"id": "1809.03888", "contents": "Title: Constrained Existence Problem for Weak Subgame Perfect Equilibria with\n  $\\omega$-Regular Boolean Objectives Abstract: We study multiplayer turn-based games played on a finite directed graph such\nthat each player aims at satisfying an omega-regular Boolean objective. Instead\nof the well-known notions of Nash equilibrium (NE) and subgame perfect\nequilibrium (SPE), we focus on the recent notion of weak subgame perfect\nequilibrium (weak SPE), a refinement of SPE. In this setting, players who\ndeviate can only use the subclass of strategies that differ from the original\none on a finite number of histories. We are interested in the constrained\nexistence problem for weak SPEs. We provide a complete characterization of the\ncomputational complexity of this problem: it is P-complete for Explicit Muller\nobjectives, NP-complete for Co-B\\\"uchi, Parity, Muller, Rabin, and Streett\nobjectives, and PSPACE-complete for Reachability and Safety objectives (we only\nprove NP-membership for B\\\"uchi objectives). We also show that the constrained\nexistence problem is fixed parameter tractable and is polynomial when the\nnumber of players is fixed. All these results are based on a fine analysis of a\nfixpoint algorithm that computes the set of possible payoff profiles underlying\nweak SPEs. \n\n"}
{"id": "1809.04040", "contents": "Title: Solving Imperfect-Information Games via Discounted Regret Minimization Abstract: Counterfactual regret minimization (CFR) is a family of iterative algorithms\nthat are the most popular and, in practice, fastest approach to approximately\nsolving large imperfect-information games. In this paper we introduce novel CFR\nvariants that 1) discount regrets from earlier iterations in various ways (in\nsome cases differently for positive and negative regrets), 2) reweight\niterations in various ways to obtain the output strategies, 3) use a\nnon-standard regret minimizer and/or 4) leverage \"optimistic regret matching\".\nThey lead to dramatically improved performance in many settings. For one, we\nintroduce a variant that outperforms CFR+, the prior state-of-the-art\nalgorithm, in every game tested, including large-scale realistic settings. CFR+\nis a formidable benchmark: no other algorithm has been able to outperform it.\nFinally, we show that, unlike CFR+, many of the important new variants are\ncompatible with modern imperfect-information-game pruning techniques and one is\nalso compatible with sampling in the game tree. \n\n"}
{"id": "1809.04136", "contents": "Title: Randomized Wagering Mechanisms Abstract: Wagering mechanisms are one-shot betting mechanisms that elicit agents'\npredictions of an event. For deterministic wagering mechanisms, an existing\nimpossibility result has shown incompatibility of some desirable theoretical\nproperties. In particular, Pareto optimality (no profitable side bet before\nallocation) can not be achieved together with weak incentive compatibility,\nweak budget balance and individual rationality. In this paper, we expand the\ndesign space of wagering mechanisms to allow randomization and ask whether\nthere are randomized wagering mechanisms that can achieve all previously\nconsidered desirable properties, including Pareto optimality. We answer this\nquestion positively with two classes of randomized wagering mechanisms: i) one\nsimple randomized lottery-type implementation of existing deterministic\nwagering mechanisms, and ii) another family of simple and randomized wagering\nmechanisms which we call surrogate wagering mechanisms, which are robust to\nnoisy ground truth. This family of mechanisms builds on the idea of learning\nwith noisy labels (Natarajan et al. 2013) as well as a recent extension of this\nidea to the information elicitation without verification setting (Liu and Chen\n2018). We show that a broad family of randomized wagering mechanisms satisfy\nall desirable theoretical properties. \n\n"}
{"id": "1809.04198", "contents": "Title: Optimization with Non-Differentiable Constraints with Applications to\n  Fairness, Recall, Churn, and Other Goals Abstract: We show that many machine learning goals, such as improved fairness metrics,\ncan be expressed as constraints on the model's predictions, which we call rate\nconstraints. We study the problem of training non-convex models subject to\nthese rate constraints (or any non-convex and non-differentiable constraints).\nIn the non-convex setting, the standard approach of Lagrange multipliers may\nfail. Furthermore, if the constraints are non-differentiable, then one cannot\noptimize the Lagrangian with gradient-based methods. To solve these issues, we\nintroduce the proxy-Lagrangian formulation. This new formulation leads to an\nalgorithm that produces a stochastic classifier by playing a two-player\nnon-zero-sum game solving for what we call a semi-coarse correlated\nequilibrium, which in turn corresponds to an approximately optimal and feasible\nsolution to the constrained optimization problem. We then give a procedure\nwhich shrinks the randomized solution down to one that is a mixture of at most\n$m+1$ deterministic solutions, given $m$ constraints. This culminates in\nalgorithms that can solve non-convex constrained optimization problems with\npossibly non-differentiable and non-convex constraints with theoretical\nguarantees. We provide extensive experimental results enforcing a wide range of\npolicy goals including different fairness metrics, and other goals on accuracy,\ncoverage, recall, and churn. \n\n"}
{"id": "1809.04224", "contents": "Title: Access to Population-Level Signaling as a Source of Inequality Abstract: We identify and explore differential access to population-level signaling\n(also known as information design) as a source of unequal access to\nopportunity. A population-level signaler has potentially noisy observations of\na binary type for each member of a population and, based on this, produces a\nsignal about each member. A decision-maker infers types from signals and\naccepts those individuals whose type is high in expectation. We assume the\nsignaler of the disadvantaged population reveals her observations to the\ndecision-maker, whereas the signaler of the advantaged population forms signals\nstrategically. We study the expected utility of the populations as measured by\nthe fraction of accepted members, as well as the false positive rates (FPR) and\nfalse negative rates (FNR).\n  We first show the intuitive results that for a fixed environment, the\nadvantaged population has higher expected utility, higher FPR, and lower FNR,\nthan the disadvantaged one (despite having identical population quality), and\nthat more accurate observations improve the expected utility of the advantaged\npopulation while harming that of the disadvantaged one. We next explore the\nintroduction of a publicly-observable signal, such as a test score, as a\npotential intervention. Our main finding is that this natural intervention,\nintended to reduce the inequality between the populations' utilities, may\nactually exacerbate it in settings where observations and test scores are\nnoisy. \n\n"}
{"id": "1809.04374", "contents": "Title: A Collaborative Multi-agent Reinforcement Learning Anti-jamming\n  Algorithm in Wireless Networks Abstract: In this letter, we investigate the anti-jamming defense problem in multi-user\nscenarios, where the coordination among users is taken into consideration. The\nMarkov game framework is employed to model and analyze the anti-jamming defense\nproblem, and a collaborative multi-agent anti-jamming algorithm (CMAA) is\nproposed to obtain the optimal anti-jamming strategy. In sweep jamming\nscenarios, on the one hand, the proposed CMAA can tackle the external malicious\njamming. On the other hand, it can effectively cope with the mutual\ninterference among users. Simulation results show that the proposed CMAA is\nsuperior to both sensing based method and independent Q-learning method, and\nhas the highest normalized rate. \n\n"}
{"id": "1809.05161", "contents": "Title: An Incentive Mechanism for Crowd Sensing with Colluding Agents Abstract: Vehicular mobile crowd sensing is a fast-emerging paradigm to collect data\nabout the environment by mounting sensors on vehicles such as taxis. An\nimportant problem in vehicular crowd sensing is to design payment mechanisms to\nincentivize drivers (agents) to collect data, with the overall goal of\nobtaining the maximum amount of data (across multiple vehicles) for a given\nbudget. Past works on this problem consider a setting where each agent operates\nin isolation---an assumption which is frequently violated in practice. In this\npaper, we design an incentive mechanism to incentivize agents who can engage in\narbitrary collusions. We then show that in a \"homogeneous\" setting, our\nmechanism is optimal, and can do as well as any mechanism which knows the\nagents' preferences a priori. Moreover, if the agents are non-colluding, then\nour mechanism automatically does as well as any other non-colluding mechanism.\nWe also show that our proposed mechanism has strong (and asymptotically\noptimal) guarantees for a more general \"heterogeneous\" setting. Experiments\nbased on synthesized data and real-world data reveal gains of over 30\\%\nattained by our mechanism compared to past literature. \n\n"}
{"id": "1809.06171", "contents": "Title: Best-case and Worst-case Sparsifiability of Boolean CSPs Abstract: We continue the investigation of polynomial-time sparsification for\nNP-complete Boolean Constraint Satisfaction Problems (CSPs). The goal in\nsparsification is to reduce the number of constraints in a problem instance\nwithout changing the answer, such that a bound on the number of resulting\nconstraints can be given in terms of the number of variables n. We investigate\nhow the worst-case sparsification size depends on the types of constraints\nallowed in the problem formulation (the constraint language). Two algorithmic\nresults are presented. The first result essentially shows that for any arity k,\nthe only constraint type for which no nontrivial sparsification is possible has\nexactly one falsifying assignment, and corresponds to logical OR (up to\nnegations). Our second result concerns linear sparsification, that is, a\nreduction to an equivalent instance with O(n) constraints. Using linear algebra\nover rings of integers modulo prime powers, we give an elegant necessary and\nsufficient condition for a constraint type to be captured by a degree-1\npolynomial over such a ring, which yields linear sparsifications. The\ncombination of these algorithmic results allows us to prove two\ncharacterizations that capture the optimal sparsification sizes for a range of\nBoolean CSPs. For NP-complete Boolean CSPs whose constraints are symmetric (the\nsatisfaction depends only on the number of 1 values in the assignment, not on\ntheir positions), we give a complete characterization of which constraint\nlanguages allow for a linear sparsification. For Boolean CSPs in which every\nconstraint has arity at most three, we characterize the optimal size of\nsparsifications in terms of the largest OR that can be expressed by the\nconstraint language. \n\n"}
{"id": "1809.08027", "contents": "Title: On the Constant Price of Anarchy Conjecture Abstract: We study Nash equilibria and the price of anarchy in the classic model of\nNetwork Creation Games introduced by Fabrikant et al. In this model every agent\n(node) buys links at a prefixed price $\\alpha > 0$ in order to get connected to\nthe network formed by all the $n$ agents. In this setting, the reformulated\ntree conjecture states that for $\\alpha > n$, every Nash equilibrium network is\na tree. Moreover, Demaine et al. conjectured that the price of anarchy for this\nmodel is constant. Since it was shown that the price of anarchy for trees is\nconstant, if the tree conjecture were true, then the price of anarchy would be\nconstant for $\\alpha > n$.\n  Up to now it has been proved that the \\PoA is constant $(i)$ in the\n\\emph{lower range}, for $\\alpha = O(n^{1-\\delta})$ with $\\delta \\geq\n\\frac{1}{\\log n}$ and $(ii)$ in the \\emph{upper range}, for $\\alpha > 4n-13$.\nIn contrast, the best upper bound known for the price of anarchy for the\nremaining range is $2^{O(\\sqrt{\\log n})}$.\n  In this paper we give new insights into the structure of the Nash equilibria\nfor $\\alpha > n$ and we enlarge the range of the parameter $\\alpha$ for which\nthe price of anarchy is constant. Specifically, we prove that the price of\nanarchy is constant for $\\alpha > n(1+\\epsilon)$ by showing that every\nequilibrium of diameter greater than some prefixed constant is a tree. \n\n"}
{"id": "1809.08387", "contents": "Title: Towards Secure Blockchain-enabled Internet of Vehicles: Optimizing\n  Consensus Management Using Reputation and Contract Theory Abstract: In Internet of Vehicles (IoV), data sharing among vehicles is essential to\nimprove driving safety and enhance vehicular services. To ensure data sharing\nsecurity and traceability, highefficiency Delegated Proof-of-Stake consensus\nscheme as a hard security solution is utilized to establish blockchain-enabled\nIoV (BIoV). However, as miners are selected from miner candidates by\nstake-based voting, it is difficult to defend against voting collusion between\nthe candidates and compromised high-stake vehicles, which introduces serious\nsecurity challenges to the BIoV. To address such challenges, we propose a soft\nsecurity enhancement solution including two stages: (i) miner selection and\n(ii) block verification. In the first stage, a reputation-based voting scheme\nfor the blockchain is proposed to ensure secure miner selection. This scheme\nevaluates candidates' reputation by using both historical interactions and\nrecommended opinions from other vehicles. The candidates with high reputation\nare selected to be active miners and standby miners. In the second stage, to\nprevent internal collusion among the active miners, a newly generated block is\nfurther verified and audited by the standby miners. To incentivize the standby\nminers to participate in block verification, we formulate interactions between\nthe active miners and the standby miners by using contract theory, which takes\nblock verification security and delay into consideration. Numerical results\nbased on a real-world dataset indicate that our schemes are secure and\nefficient for data sharing in BIoV. \n\n"}
{"id": "1809.09315", "contents": "Title: A Budget Feasible Peer Graded Mechanism For IoT-Based Crowdsourcing Abstract: We develop and extend a line of recent works on the design of mechanisms for\nheterogeneous tasks assignment problem in 'crowdsourcing'. The budgeted market\nwe consider consists of multiple task requesters and multiple IoT devices as\ntask executers; where each task requester is endowed with a single distinct\ntask along with the publicly known budget. Also, each IoT device has valuations\nas the cost for executing the tasks and quality, which are private. Given such\nscenario, the objective is to select a subset of IoT devices for each task,\nsuch that the total payment made is within the allotted quota of the budget\nwhile attaining a threshold quality. For the purpose of determining the unknown\nquality of the IoT devices, we have utilized the concept of peer grading. In\nthis paper, we have carefully crafted a truthful budget feasible mechanism;\nnamely TUBE-TAP for the problem under investigation that also allows us to have\nthe true information about the quality of the IoT devices. The simulations are\nperformed in order to measure the efficacy of our proposed mechanism. \n\n"}
{"id": "1809.10007", "contents": "Title: Learning through Probing: a decentralized reinforcement learning\n  architecture for social dilemmas Abstract: Multi-agent reinforcement learning has received significant interest in\nrecent years notably due to the advancements made in deep reinforcement\nlearning which have allowed for the developments of new architectures and\nlearning algorithms. Using social dilemmas as the training ground, we present a\nnovel learning architecture, Learning through Probing (LTP), where agents\nutilize a probing mechanism to incorporate how their opponent's behavior\nchanges when an agent takes an action. We use distinct training phases and\nadjust rewards according to the overall outcome of the experiences accounting\nfor changes to the opponents behavior. We introduce a parameter eta to\ndetermine the significance of these future changes to opponent behavior. When\napplied to the Iterated Prisoner's Dilemma (IPD), LTP agents demonstrate that\nthey can learn to cooperate with each other, achieving higher average\ncumulative rewards than other reinforcement learning methods while also\nmaintaining good performance in playing against static agents that are present\nin Axelrod tournaments. We compare this method with traditional reinforcement\nlearning algorithms and agent-tracking techniques to highlight key differences\nand potential applications. We also draw attention to the differences between\nsolving games and societal-like interactions and analyze the training of\nQ-learning agents in makeshift societies. This is to emphasize how cooperation\nmay emerge in societies and demonstrate this using environments where\ninteractions with opponents are determined through a random encounter format of\nthe IPD. \n\n"}
{"id": "1809.10469", "contents": "Title: Probabilistic Analysis of Edge Elimination for Euclidean TSP Abstract: One way to speed up the calculation of optimal TSP tours in practice is\neliminating edges that are certainly not in the optimal tour as a preprocessing\nstep. In order to do so several edge elimination approaches have been proposed\nin the past. In this work we investigate two of them in the scenario where the\ninput consists of $n$ independently distributed random points in the\n2-dimensional unit square with bounded density function from above and below by\narbitrary positive constants. We show that after the edge elimination procedure\nof Hougardy and Schroeder the expected number of remaining edges is\n$\\Theta(n)$, while after that the non-recursive part of Jonker and Volgenant\nthe expected number of remaining edges is $\\Theta(n^2)$. \n\n"}
{"id": "1810.00800", "contents": "Title: Optimal Pricing For MHR and $\\lambda$-Regular Distributions Abstract: We study the performance of anonymous posted-price selling mechanisms for a\nstandard Bayesian auction setting, where $n$ bidders have i.i.d. valuations for\na single item. We show that for the natural class of Monotone Hazard Rate (MHR)\ndistributions, offering the same, take-it-or-leave-it price to all bidders can\nachieve an (asymptotically) optimal revenue. In particular, the approximation\nratio is shown to be $1+O(\\ln \\ln n/\\ln n)$, matched by a tight lower bound for\nthe case of exponential distributions. This improves upon the previously\nbest-known upper bound of $e/(e-1)\\approx 1.58$ for the slightly more general\nclass of regular distributions. In the worst case (over $n$), we still show a\nglobal upper bound of $1.35$. We give a simple, closed-form description of our\nprices which, interestingly enough, relies only on minimal knowledge of the\nprior distribution, namely just the expectation of its second-highest order\nstatistic.\n  Furthermore, we extend our techniques to handle the more general class of\n$\\lambda$-regular distributions that interpolate between MHR ($\\lambda=0$) and\nregular ($\\lambda=1$). Our anonymous pricing rule now results in an asymptotic\napproximation ratio that ranges smoothly, with respect to $\\lambda$, from $1$\n(MHR distributions) to $e/(e-1)$ (regular distributions). Finally, we\nexplicitly give a class of continuous distributions that provide matching lower\nbounds, for every $\\lambda$. \n\n"}
{"id": "1810.01436", "contents": "Title: Efficient Estimation of Equilibria of Large Congestion Games with\n  Heterogeneous Players Abstract: Computing an equilibrium in congestion games can be challenging when the\nnumber of players is large. Yet, it is a problem to be addressed in practice,\nfor instance to forecast the state of the system and be able to control it. In\nthis work, we analyze the case of generalized atomic congestion games, with\ncoupling constraints, and with players that are heterogeneous through their\naction sets and their utility functions. We obtain an approximation of the\nvariational Nash equilibria---a notion generalizing Nash equilibria in the\npresence of coupling constraints---of a large atomic congestion game by an\nequilibrium of an auxiliary population game, where each population corresponds\nto a group of atomic players of the initial game. Because the variational\ninequalities characterizing the equilibrium of the auxiliary game have smaller\ndimension than the original problem, this approach enables the fast computation\nof an estimation of equilibria in a large congestion game with thousands of\nheterogeneous players. \n\n"}
{"id": "1810.02066", "contents": "Title: Turning Lemons into Peaches using Secure Computation Abstract: In many cases, assessing the quality of goods is hard. For example, when\npurchasing a car, it is hard to measure how pollutant the car is since there\nare infinitely many driving conditions to be tested. Typically, these\nsituations are considered under the umbrella of information asymmetry and as\nAkelrof showed may lead to a market of lemons. However, we argue that in many\nof these situations, the problem is not the missing information but the\ncomputational challenge of obtaining it. In a nut-shell, if verifying the value\nof goods requires a large amount of computation or even infinite amounts of\ncomputation, the buyer is forced to use a finite test that samples, in some\nsense, the quality of the goods. However, if the seller knows the test, then\nthe seller can over-fit the test and create goods that pass the quality test\ndespite not having the desired quality. We show different solutions to this\nsituation including a novel approach that uses secure computation to hide the\ntest from the seller to prevent over-fitting. \n\n"}
{"id": "1810.02304", "contents": "Title: Polynomial-time Recognition of 4-Steiner Powers Abstract: The $k^{th}$-power of a given graph $G=(V,E)$ is obtained from $G$ by adding\nan edge between every two distinct vertices at a distance at most $k$ in $G$.\nWe call $G$ a $k$-Steiner power if it is an induced subgraph of the\n$k^{th}$-power of some tree. Our main contribution is a polynomial-time\nrecognition algorithm of $4$-Steiner powers, thereby extending the\ndecade-year-old results of (Lin, Kearney and Jiang, ISAAC'00) for $k=1,2$ and\n(Chang and Ko, WG'07) for $k=3$.\n  A graph $G$ is termed $k$-leaf power if there is some tree $T$ such that: all\nvertices in $V(G)$ are leaf-nodes of $T$, and $G$ is an induced subgraph of the\n$k^{th}$-power of $T$. As a byproduct of our main result, we give the first\nknown polynomial-time recognition algorithm for $6$-leaf powers. \n\n"}
{"id": "1810.03063", "contents": "Title: Solving Large Sequential Games with the Excessive Gap Technique Abstract: There has been tremendous recent progress on equilibrium-finding algorithms\nfor zero-sum imperfect-information extensive-form games, but there has been a\npuzzling gap between theory and practice. First-order methods have\nsignificantly better theoretical convergence rates than any\ncounterfactual-regret minimization (CFR) variant. Despite this, CFR variants\nhave been favored in practice. Experiments with first-order methods have only\nbeen conducted on small- and medium-sized games because those methods are\ncomplicated to implement in this setting, and because CFR variants have been\nenhanced extensively for over a decade they perform well in practice. In this\npaper we show that a particular first-order method, a state-of-the-art variant\nof the excessive gap technique---instantiated with the dilated entropy distance\nfunction---can efficiently solve large real-world problems competitively with\nCFR and its variants. We show this on large endgames encountered by the\nLibratus poker AI, which recently beat top human poker specialist professionals\nat no-limit Texas hold'em. We show experimental results on our variant of the\nexcessive gap technique as well as a prior version. We introduce a numerically\nfriendly implementation of the smoothed best response computation associated\nwith first-order methods for extensive-form game solving. We present, to our\nknowledge, the first GPU implementation of a first-order method for\nextensive-form games. We present comparisons of several excessive gap technique\nand CFR variants. \n\n"}
{"id": "1810.05106", "contents": "Title: Parity games and universal graphs Abstract: This paper is a contribution to the study of parity games and the recent\nconstructions of three quasipolynomial time algorithms for solving them. We\nrevisit a result of Czerwi\\'nski, Daviaud, Fijalkow, Jurdzi\\'nski, Lazi\\'c, and\nParys witnessing a quasipolynomial barrier for all three quasipolynomial time\nalgorithms. The argument is that all three algorithms can be understood as\nconstructing a so-called separating automaton, and to give a quasipolynomial\nlower bond on the size of separating automata.\n  We give an alternative proof of this result. The key innovations of this\npaper are the notion of universal graphs and the idea of saturation. \n\n"}
{"id": "1810.05921", "contents": "Title: Two Can Play That Game: An Adversarial Evaluation of a Cyber-alert\n  Inspection System Abstract: Cyber-security is an important societal concern. Cyber-attacks have increased\nin numbers as well as in the extent of damage caused in every attack. Large\norganizations operate a Cyber Security Operation Center (CSOC), which form the\nfirst line of cyber-defense. The inspection of cyber-alerts is a critical part\nof CSOC operations. A recent work, in collaboration with Army Research Lab, USA\nproposed a reinforcement learning (RL) based approach to prevent the\ncyber-alert queue length from growing large and overwhelming the defender.\nGiven the potential deployment of this approach to CSOCs run by US defense\nagencies, we perform a red team (adversarial) evaluation of this approach.\nFurther, with the recent attacks on learning systems, it is even more important\nto test the limits of this RL approach. Towards that end, we learn an\nadversarial alert generation policy that is a best response to the defender\ninspection policy. Surprisingly, we find the defender policy to be quite robust\nto the best response of the attacker. In order to explain this observation, we\nextend the earlier RL model to a game model and show that there exists defender\npolicies that can be robust against any adversarial policy. We also derive a\ncompetitive baseline from the game theory model and compare it to the RL\napproach. However, we go further to exploit assumptions made in the MDP in the\nRL model and discover an attacker policy that overwhelms the defender. We use a\ndouble oracle approach to retrain the defender with episodes from this\ndiscovered attacker policy. This made the defender robust to the discovered\nattacker policy and no further harmful attacker policies were discovered.\nOverall, the adversarial RL and double oracle approach in RL are general\ntechniques that are applicable to other RL usage in adversarial environments. \n\n"}
{"id": "1810.08345", "contents": "Title: A Matrix Chernoff Bound for Strongly Rayleigh Distributions and Spectral\n  Sparsifiers from a few Random Spanning Trees Abstract: Strongly Rayleigh distributions are a class of negatively dependent\ndistributions of binary-valued random variables [Borcea, Branden, Liggett JAMS\n09]. Recently, these distributions have played a crucial role in the analysis\nof algorithms for fundamental graph problems, e.g. Traveling Salesman Problem\n[Gharan, Saberi, Singh FOCS 11]. We prove a new matrix Chernoff bound for\nStrongly Rayleigh distributions.\n  As an immediate application, we show that adding together the Laplacians of\n$\\epsilon^{-2} \\log^2 n$ random spanning trees gives an $(1\\pm \\epsilon)$\nspectral sparsifiers of graph Laplacians with high probability. Thus, we\npositively answer an open question posed in [Baston, Spielman, Srivastava, Teng\nJACM 13]. Our number of spanning trees for spectral sparsifier matches the\nnumber of spanning trees required to obtain a cut sparsifier in [Fung,\nHariharan, Harvey, Panigraphi STOC 11]. The previous best result was by naively\napplying a classical matrix Chernoff bound which requires $\\epsilon^{-2} n \\log\nn$ spanning trees. For the tree averaging procedure to agree with the original\ngraph Laplacian in expectation, each edge of the tree should be reweighted by\nthe inverse of the edge leverage score in the original graph. We also show that\nwhen using this reweighting of the edges, the Laplacian of single random tree\nis bounded above in the PSD order by the original graph Laplacian times a\nfactor $\\log n$ with high probability, i.e. $L_T \\preceq O(\\log n) L_G$.\n  We show a lower bound that almost matches our last result, namely that in\nsome graphs, with high probability, the random spanning tree is $\\it{not}$\nbounded above in the spectral order by $\\frac{\\log n}{\\log\\log n}$ times the\noriginal graph Laplacian. We also show a lower bound that in $\\epsilon^{-2}\n\\log n$ spanning trees are necessary to get a $(1\\pm \\epsilon)$ spectral\nsparsifier. \n\n"}
{"id": "1810.08671", "contents": "Title: Limits on All Known (and Some Unknown) Approaches to Matrix\n  Multiplication Abstract: We study the known techniques for designing Matrix Multiplication algorithms.\nThe two main approaches are the Laser method of Strassen, and the Group\ntheoretic approach of Cohn and Umans. We define a generalization based on\nzeroing outs which subsumes these two approaches, which we call the Solar\nmethod, and an even more general method based on monomial degenerations, which\nwe call the Galactic method.\n  We then design a suite of techniques for proving lower bounds on the value of\n$\\omega$, the exponent of matrix multiplication, which can be achieved by\nalgorithms using many tensors $T$ and the Galactic method. Some of our\ntechniques exploit `local' properties of $T$, like finding a sub-tensor of $T$\nwhich is so `weak' that $T$ itself couldn't be used to achieve a good bound on\n$\\omega$, while others exploit `global' properties, like $T$ being a monomial\ndegeneration of the structural tensor of a group algebra.\n  Our main result is that there is a universal constant $\\ell>2$ such that a\nlarge class of tensors generalizing the Coppersmith-Winograd tensor $CW_q$\ncannot be used within the Galactic method to show a bound on $\\omega$ better\nthan $\\ell$, for any $q$. We give evidence that previous lower-bounding\ntechniques were not strong enough to show this. We also prove a number of\ncomplementary results along the way, including that for any group $G$, the\nstructural tensor of $\\mathbb{C}[G]$ can be used to recover the best bound on\n$\\omega$ which the Coppersmith-Winograd approach gets using $CW_{|G|-2}$ as\nlong as the asymptotic rank of the structural tensor is not too large. \n\n"}
{"id": "1810.08810", "contents": "Title: The Frontiers of Fairness in Machine Learning Abstract: The last few years have seen an explosion of academic and popular interest in\nalgorithmic fairness. Despite this interest and the volume and velocity of work\nthat has been produced recently, the fundamental science of fairness in machine\nlearning is still in a nascent state. In March 2018, we convened a group of\nexperts as part of a CCC visioning workshop to assess the state of the field,\nand distill the most promising research directions going forward. This report\nsummarizes the findings of that workshop. Along the way, it surveys recent\ntheoretical work in the field and points towards promising directions for\nresearch. \n\n"}
{"id": "1810.09026", "contents": "Title: Actor-Critic Policy Optimization in Partially Observable Multiagent\n  Environments Abstract: Optimization of parameterized policies for reinforcement learning (RL) is an\nimportant and challenging problem in artificial intelligence. Among the most\ncommon approaches are algorithms based on gradient ascent of a score function\nrepresenting discounted return. In this paper, we examine the role of these\npolicy gradient and actor-critic algorithms in partially-observable multiagent\nenvironments. We show several candidate policy update rules and relate them to\na foundation of regret minimization and multiagent learning techniques for the\none-shot and tabular cases, leading to previously unknown convergence\nguarantees. We apply our method to model-free multiagent reinforcement learning\nin adversarial sequential decision problems (zero-sum imperfect information\ngames), using RL-style function approximation. We evaluate on commonly used\nbenchmark Poker domains, showing performance against fixed policies and\nempirical convergence to approximate Nash equilibria in self-play with rates\nsimilar to or better than a baseline model-free algorithm for zero sum games,\nwithout any domain-specific state space reductions. \n\n"}
{"id": "1810.09832", "contents": "Title: Mechanism Design for Social Good Abstract: Across various domains--such as health, education, and housing--improving\nsocietal welfare involves allocating resources, setting policies, targeting\ninterventions, and regulating activities. These solutions have an immense\nimpact on the day-to-day lives of individuals, whether in the form of access to\nquality healthcare, labor market outcomes, or how votes are accounted for in a\ndemocratic society. Problems that can have an out-sized impact on individuals\nwhose opportunities have historically been limited often pose conceptual and\ntechnical challenges, requiring insights from many disciplines. Conversely, the\nlack of interdisciplinary approach can leave these urgent needs unaddressed and\ncan even exacerbate underlying socioeconomic inequalities. To realize the\nopportunities in these domains, we need to correctly set objectives and reason\nabout human behavior and actions. Doing so requires a deep grounding in the\nfield of interest and collaboration with domain experts who understand the\nsocietal implications and feasibility of proposed solutions. These insights can\nplay an instrumental role in proposing algorithmically-informed policies.\n  In this article, we describe the Mechanism Design for Social Good (MD4SG)\nresearch agenda, which involves using insights from algorithms, optimization,\nand mechanism design to improve access to opportunity. The MD4SG research\ncommunity takes an interdisciplinary, multi-stakeholder approach to improve\nsocietal welfare. We discuss three exciting research avenues within MD4SG\nrelated to improving access to opportunity in the developing world, labor\nmarkets and discrimination, and housing. For each of these, we showcase ongoing\nwork, underline new directions, and discuss potential for implementing existing\nwork in practice. \n\n"}
{"id": "1810.10900", "contents": "Title: On Policies for Single-leg Revenue Management with Limited Demand\n  Information Abstract: In this paper we study the single-item revenue management problem, with no\ninformation given about the demand trajectory over time. When the item is sold\nthrough accepting/rejecting different fare classes, Ball and Queyranne (2009)\nhave established the tight competitive ratio for this problem using booking\nlimit policies, which raise the acceptance threshold as the remaining inventory\ndwindles. However, when the item is sold through dynamic pricing instead, there\nis the additional challenge that offering a low price may entice high-paying\ncustomers to substitute down. We show that despite this challenge, the same\ncompetitive ratio can still be achieved using a randomized dynamic pricing\npolicy. Our policy incorporates the price-skimming technique from Eren and\nMaglaras (2010), but importantly we show how the randomized price distribution\nshould be stochastically-increased as the remaining inventory dwindles. A key\ntechnical ingredient in our policy is a new \"valuation tracking\" subroutine,\nwhich tracks the possible values for the optimum, and follows the most\n\"inventory-conservative\" control which maintains the desired competitive ratio.\nFinally, we demonstrate the empirical effectiveness of our policy in\nsimulations, where its average-case performance surpasses all naive\nmodifications of the existing policies. \n\n"}
{"id": "1810.11216", "contents": "Title: Packing Returning Secretaries Abstract: We study online secretary problems with returns in combinatorial packing\ndomains with $n$ candidates that arrive sequentially over time in random order.\nThe goal is to accept a feasible packing of candidates of maximum total value.\nIn the first variant, each candidate arrives exactly twice. All $2n$ arrivals\noccur in random order. We propose a simple 0.5-competitive algorithm that can\nbe combined with arbitrary approximation algorithms for the packing domain,\neven when the total value of candidates is a subadditive function. For\nbipartite matching, we obtain an algorithm with competitive ratio at least\n$0.5721 - o(1)$ for growing $n$, and an algorithm with ratio at least $0.5459$\nfor all $n \\ge 1$. We extend all algorithms and ratios to $k \\ge 2$ arrivals\nper candidate.\n  In the second variant, there is a pool of undecided candidates. In each\nround, a random candidate from the pool arrives. Upon arrival a candidate can\nbe either decided (accept/reject) or postponed (returned into the pool). We\nmainly focus on minimizing the expected number of postponements when computing\nan optimal solution. An expected number of $\\Theta(n \\log n)$ is always\nsufficient. For matroids, we show that the expected number can be reduced to\n$O(r \\log (n/r))$, where $r \\le n/2$ is the minimum of the ranks of matroid and\ndual matroid. For bipartite matching, we show a bound of $O(r \\log n)$, where\n$r$ is the size of the optimum matching. For general packing, we show a lower\nbound of $\\Omega(n \\log \\log n)$, even when the size of the optimum is $r =\n\\Theta(\\log n)$. \n\n"}
{"id": "1810.11700", "contents": "Title: Minimum Reload Cost Graph Factors Abstract: The concept of Reload cost in a graph refers to the cost that occurs while\ntraversing a vertex via two of its incident edges. This cost is uniquely\ndetermined by the colors of the two edges. This concept has various\napplications in transportation networks, communication networks, and energy\ndistribution networks. Various problems using this model are defined and\nstudied in the literature. The problem of finding a spanning tree whose\ndiameter with respect to the reload costs is the smallest possible, the\nproblems of finding a path, trail or walk with minimum total reload cost\nbetween two given vertices, problems about finding a proper edge coloring of a\ngraph such that the total reload cost is minimized, the problem of finding a\nspanning tree such that the sum of the reload costs of all paths between all\npairs of vertices is minimized, and the problem of finding a set of cycles of\nminimum reload cost, that cover all the vertices of a graph, are examples of\nsuch problems. % In this work we focus on the last problem. Noting that a cycle\ncover of a graph is a 2-factor of it, we generalize the problem to that of\nfinding an $r$-factor of minimum reload cost of an edge colored graph. We prove\nseveral NP-hardness results for special cases of the problem. Namely, bounded\ndegree graphs, planar graphs, bounded total cost, and bounded number of\ndistinct costs. For the special case of $r=2$, our results imply an improved\nNP-hardness result. On the positive side, we present a polynomial-time solvable\nspecial case which provides a tight boundary between the polynomial and hard\ncases in terms of $r$ and the maximum degree of the graph. We then investigate\nthe parameterized complexity of the problem, prove W[1]-hardness results and\npresent an FPT algorithm. \n\n"}
{"id": "1810.12030", "contents": "Title: Simon's problem for linear functions Abstract: Simon's problem asks the following: determine if a function $f: \\{0,1\\}^n\n\\rightarrow \\{0,1\\}^n$ is one-to-one or if there exists a unique $s \\in\n\\{0,1\\}^n$ such that $f(x) = f(x \\oplus s)$ for all $x \\in \\{0,1\\}^n$, given\nthe promise that exactly one of the two holds. A classical algorithm that can\nsolve this problem for every $f$ requires $2^{\\Omega(n)}$ queries to $f$. Simon\nshowed that there is a quantum algorithm that can solve this promise problem\nfor every $f$ using only $\\mathcal O(n)$ quantum queries to $f$. A matching\nlower bound on the number of quantum queries was given by Koiran et al., even\nfor functions $f: {\\mathbb{F}_p^n} \\to {\\mathbb{F}_p^n}$. We give a short proof\nthat $\\mathcal O(n)$ quantum queries is optimal even when we are additionally\npromised that $f$ is linear. This is somewhat surprising because for linear\nfunctions there even exists a classical $n$-query algorithm. \n\n"}
{"id": "1811.00763", "contents": "Title: Tight Approximation Ratio of Anonymous Pricing Abstract: We consider two canonical Bayesian mechanism design settings. In the\nsingle-item setting, we prove tight approximation ratio for anonymous pricing:\ncompared with Myerson Auction, it extracts at least $\\frac{1}{2.62}$-fraction\nof revenue; there is a matching lower-bound example.\n  In the unit-demand single-buyer setting, we prove tight approximation ratio\nbetween the simplest and optimal deterministic mechanisms: in terms of revenue,\nuniform pricing admits a $2.62$-approximation of item pricing; we further\nvalidate the tightness of this ratio.\n  These results settle two open problems asked\nin~\\cite{H13,CD15,AHNPY15,L17,JLTX18}. As an implication, in the single-item\nsetting: we improve the approximation ratio of the second-price auction with\nanonymous reserve to $2.62$, which breaks the state-of-the-art upper bound of\n$e \\approx 2.72$. \n\n"}
{"id": "1811.02351", "contents": "Title: An Incentive Analysis of some Bitcoin Fee Designs Abstract: In the Bitcoin system, miners are incentivized to join the system and\nvalidate transactions through fees paid by the users. A simple \"pay your bid\"\nauction has been employed to determine the transaction fees. Recently, Lavi,\nSattath and Zohar [LSZ17] proposed an alternative fee design, called the\nmonopolistic price (MP) mechanism, aimed at improving the revenue for the\nminers. Although MP is not strictly incentive compatible (IC), they studied how\nclose to IC the mechanism is for iid distributions, and conjectured that it is\nnearly IC asymptotically based on extensive simulations and some analysis. In\nthis paper, we prove that the MP mechanism is nearly incentive compatible for\nany iid distribution as the number of users grows large. This holds true with\nrespect to other attacks such as splitting bids. We also prove a conjecture in\n[LSZ17] that MP dominates the RSOP auction in revenue (originally defined in\nGoldberg et al. [GHKSW06] for digital goods). These results lend support to MP\nas a Bitcoin fee design candidate. Additionally, we explore some possible\nintrinsic correlations between incentive compatibility and revenue in general. \n\n"}
{"id": "1811.02446", "contents": "Title: Knowledge and Blameworthiness Abstract: Blameworthiness of an agent or a coalition of agents is often defined in\nterms of the principle of alternative possibilities: for the coalition to be\nresponsible for an outcome, the outcome must take place and the coalition\nshould have had a strategy to prevent it. In this article we argue that in the\nsettings with imperfect information, not only should the coalition have had a\nstrategy, but it also should have known that it had a strategy, and it should\nhave known what the strategy was. The main technical result of the article is a\nsound and complete bimodal logic that describes the interplay between knowledge\nand blameworthiness in strategic games with imperfect information. \n\n"}
{"id": "1811.04063", "contents": "Title: On convexity and solution concepts in cooperative interval games Abstract: Cooperative interval game is a cooperative game in which every coalition gets\nassigned some closed real interval. This models uncertainty about how much the\nmembers of a coalition get for cooperating together.\n  In this paper we study convexity, core and the Shapley value of games with\ninterval uncertainty.\n  Our motivation to do so is twofold. First, we want to capture which\nproperties are preserved when we generalize concepts from classical cooperative\ngame theory to interval games. Second, since these generalizations can be done\nin different ways, mainly with regard to the resulting level of uncertainty, we\ntry to compare them and show their relation to each other. \n\n"}
{"id": "1811.04753", "contents": "Title: Sliding Window Temporal Graph Coloring Abstract: Graph coloring is one of the most famous computational problems with\napplications in a wide range of areas such as planning and scheduling, resource\nallocation, and pattern matching. So far coloring problems are mostly studied\non static graphs, which often stand in stark contrast to practice where data is\ninherently dynamic and subject to discrete changes over time. A temporal graph\nis a graph whose edges are assigned a set of integer time labels, indicating at\nwhich discrete time steps the edge is active. In this paper we present a\nnatural temporal extension of the classical graph coloring problem. Given a\ntemporal graph and a natural number $\\Delta$, we ask for a coloring sequence\nfor each vertex such that (i) in every sliding time window of $\\Delta$\nconsecutive time steps, in which an edge is active, this edge is properly\ncolored (i.e. its endpoints are assigned two different colors) at least once\nduring that time window, and (ii) the total number of different colors is\nminimized. This sliding window temporal coloring problem abstractly captures\nmany realistic graph coloring scenarios in which the underlying network changes\nover time, such as dynamically assigning communication channels to moving\nagents. We present a thorough investigation of the computational complexity of\nthis temporal coloring problem. More specifically, we prove strong\ncomputational hardness results, complemented by efficient exact and\napproximation algorithms. Some of our algorithms are linear-time\nfixed-parameter tractable with respect to appropriate parameters, while others\nare asymptotically almost optimal under the Exponential Time Hypothesis (ETH). \n\n"}
{"id": "1811.06126", "contents": "Title: Cooperation Enforcement and Collusion Resistance in Repeated Public\n  Goods Games Abstract: Enforcing cooperation among substantial agents is one of the main objectives\nfor multi-agent systems. However, due to the existence of inherent social\ndilemmas in many scenarios, the free-rider problem may arise during agents'\nlong-run interactions and things become even severer when self-interested\nagents work in collusion with each other to get extra benefits. It is commonly\naccepted that in such social dilemmas, there exists no simple strategy for an\nagent whereby she can simultaneously manipulate on the utility of each of her\nopponents and further promote mutual cooperation among all agents. Here, we\nshow that such strategies do exist. Under the conventional repeated public\ngoods game, we novelly identify them and find that, when confronted with such\nstrategies, a single opponent can maximize his utility only via global\ncooperation and any colluding alliance cannot get the upper hand. Since a full\ncooperation is individually optimal for any single opponent, a stable\ncooperation among all players can be achieved. Moreover, we experimentally show\nthat these strategies can still promote cooperation even when the opponents are\nboth self-learning and collusive. \n\n"}
{"id": "1811.06736", "contents": "Title: Learning Approximately Optimal Contracts Abstract: In principal-agent models, a principal offers a contract to an agent to\nperform a certain task. The agent exerts a level of effort that maximizes her\nutility. The principal is oblivious to the agent's chosen level of effort, and\nconditions her wage only on possible outcomes. In this work, we consider a\nmodel in which the principal is unaware of the agent's utility and action\nspace: she sequentially offers contracts to identical agents, and observes the\nresulting outcomes. We present an algorithm for learning the optimal contract\nunder mild assumptions. We bound the number of samples needed for the principal\nto obtain a contract that is within $\\eps$ of her optimal net profit for every\n$\\eps>0$. Our results are robust even when considering risk-averse agents.\nFurthermore, we show that when there are only two possible outcomes or the\nagent is risk-neutral, the algorithm's outcome approximates the optimal\ncontract described in the classical theory. \n\n"}
{"id": "1811.07515", "contents": "Title: Classical Algorithms from Quantum and Arthur-Merlin Communication\n  Protocols Abstract: The polynomial method from circuit complexity has been applied to several\nfundamental problems and obtains the state-of-the-art running times. As\nobserved in [Alman and Williams, STOC 2017], almost all applications of the\npolynomial method in algorithm design ultimately rely on certain low-rank\ndecompositions of the computation matrices corresponding to key subroutines.\nThey suggest that making use of low-rank decompositions directly could lead to\nmore powerful algorithms, as the polynomial method is just one way to derive\nsuch a decomposition. Inspired by their observation, in this paper, we study\nanother way of systematically constructing low-rank decompositions of matrices\nwhich could be used by algorithms. It is known that various types of\ncommunication protocols lead to certain low-rank decompositions (e.g.,\n$\\mathsf{P}$ protocols/rank, $\\mathsf{BQP}$ protocols/approximate rank). These\nare usually interpreted as approaches for proving communication lower bounds,\nwhile in this work we explore the other direction.\n  We have the two generic algorithmic applications of communication protocols.\nThe first connection is that a fast $\\mathsf{BQP}$ communication protocol for a\nfunction $f$ implies a fast deterministic additive approximate counting\nalgorithm for a related pair counting problem. The second connection is that a\nfast $\\mathsf{AM}^{\\mathsf{cc}}$ protocol for a function $f$ implies a\nfaster-than-bruteforce algorithm for $f\\textsf{-Satisfying-Pair}$.\n  We also apply our second connection to shed some light on long-standing open\nproblems in communication complexity. We show that if the Longest Common\nSubsequence problem admits an efficient $\\mathsf{AM}^{\\mathsf{cc}}$ protocol,\nthen polynomial-size Formula-$\\textsf{SAT}$ admits a $2^{n - n^{1-\\delta}}$\ntime algorithm for any constant $\\delta > 0$. \n\n"}
{"id": "1811.08506", "contents": "Title: Tight Approximation Ratio for Minimum Maximal Matching Abstract: We study a combinatorial problem called Minimum Maximal Matching, where we\nare asked to find in a general graph the smallest that can not be extended. We\nshow that this problem is hard to approximate with a constant smaller than 2,\nassuming the Unique Games Conjecture.\n  As a corollary we show, that Minimum Maximal Matching in bipartite graphs is\nhard to approximate with constant smaller than $\\frac{4}{3}$, with the same\nassumption. With a stronger variant of the Unique Games Conjecture --- that is\nSmall Set Expansion Hypothesis --- we are able to improve the hardness result\nup to the factor of $\\frac{3}{2}$. \n\n"}
{"id": "1811.08673", "contents": "Title: On the Proximity of Markets with Integral Equilibria Abstract: We study Fisher markets that admit equilibria wherein each good is integrally\nassigned to some agent. While strong existence and computational guarantees are\nknown for equilibria of Fisher markets with additive valuations, such\nequilibria, in general, assign goods fractionally to agents. Hence, Fisher\nmarkets are not directly applicable in the context of indivisible goods. In\nthis work we show that one can always bypass this hurdle and, up to a bounded\nchange in agents' budgets, obtain markets that admit an integral equilibrium.\nWe refer to such markets as pure markets and show that, for any given Fisher\nmarket (with additive valuations), one can efficiently compute a \"near-by,\"\npure market with an accompanying integral equilibrium.\n  Our work on pure markets leads to novel algorithmic results for fair division\nof indivisible goods. Prior work in discrete fair division has shown that,\nunder additive valuations, there always exist allocations that simultaneously\nachieve the seemingly incompatible properties of fairness and efficiency; here\nfairness refers to envy-freeness up to one good (EF1) and efficiency\ncorresponds to Pareto efficiency. However, polynomial-time algorithms are not\nknown for finding such allocations. Considering relaxations of proportionality\nand EF1, respectively, as our notions of fairness, we show that fair and Pareto\nefficient allocations can be computed in strongly polynomial time. \n\n"}
{"id": "1811.09646", "contents": "Title: Core-Selecting Mechanisms in Electricity Markets Abstract: Due to its theoretical virtues, several recent works propose the use of the\nincentive-compatible Vickrey-Clarke-Groves (VCG) mechanism for electricity\nmarkets. Coalitions of participants, however, can influence the VCG outcome to\nobtain higher collective profit. To address this issue, we propose\ncore-selecting mechanisms for their coalition-proofness. We show that\ncore-selecting mechanisms generalize the economic rationale of the locational\nmarginal pricing (LMP) mechanism. Namely, these mechanisms are the exact class\nof mechanisms that ensure the existence of a competitive equilibrium in\nlinear/nonlinear prices. This implies that the LMP mechanism is also\ncore-selecting, and hence coalition-proof. In contrast to the LMP mechanism,\ncore-selecting mechanisms exist for a broad class of electricity markets, such\nas ones involving nonconvex costs and nonconvex constraint sets. In addition,\nthey can approximate truthfulness without the price-taking assumption of the\nLMP mechanism. Finally, we show that they are also budget-balanced. Our results\nare verified with case studies based on optimal power flow test systems and the\nSwiss reserve market. \n\n"}
{"id": "1811.09680", "contents": "Title: Enhancing Engagement in Token-Curated Registries via an Inflationary\n  Mechanism Abstract: Token Curated Registries (TCR) are decentralized recommendation systems that\ncan be implemented using Blockchain smart contracts. They allow participants to\nvote for or against adding items to a list through a process that involves\nstaking tokens intrinsic to the registry, with winners receiving the staked\ntokens for each vote. A TCR aims to provide incentives to create a well-curated\nlist. In this work, we consider a challenge for these systems - incentivizing\ntoken-holders to actually engage and participate in the voting process. We\npropose a novel token-inflation mechanism for enhancing engagement, whereby\nonly voting participants see their token supply increased by a pre-defined\nmultiple after each round of voting. To evaluate this proposal, we propose a\nsimple 4-class model of voters that captures all possible combinations of two\nkey dimensions: whether they are engaged (likely to vote at all for a given\nitem) or disengaged, and whether they are informed (likely to vote in a way\nthat increases the quality of the list) or uninformed, and a simple metric to\nevaluate the quality of the list as a function of the vote outcomes. We conduct\nsimulations using this model of voters and show that implementing\ntoken-inflation results in greater wealth accumulation for engaged voters. In\nparticular, when the number of informed voters is sufficiently high, our\nsimulations show that voters that are both informed and engaged see the\ngreatest benefits from participating in the registry when our proposed\ntoken-inflation mechanism is employed. We further validate this finding using a\nsimplified mathematical analysis. \n\n"}
{"id": "1811.09992", "contents": "Title: Externalities in Endogenous Sharing Economy Networks Abstract: This paper investigates the impact of link formation between a pair of agents\non the resource availability of other agents (that is, externalities) in a\nsocial cloud network, a special case of endogenous sharing economy networks.\nSpecifically, we study how the closeness between agents and the network size\naffect externalities.\n  We conjecture, and experimentally support, that for an agent to experience\npositive externalities, an increase in its closeness is necessary. The\ncondition is not sufficient though. We, then, show that for populated ring\nnetworks, one or more agents experience positive externalities due to an\nincrease in the closeness of agents. Further, the initial distance between\nagents forming a link has a direct bearing on the number of beneficiaries, and\nthe number of beneficiaries is always less than that of non-beneficiaries. \n\n"}
{"id": "1811.12082", "contents": "Title: Joint Service Pricing and Cooperative Relay Communication for Federated\n  Learning Abstract: For the sake of protecting data privacy and due to the rapid development of\nmobile devices, e.g., powerful central processing unit (CPU) and nascent neural\nprocessing unit (NPU), collaborative machine learning on mobile devices, e.g.,\nfederated learning, has been envisioned as a new AI approach with broad\napplication prospects. However, the learning process of the existing federated\nlearning platforms rely on the direct communication between the model owner,\ne.g., central cloud or edge server, and the mobile devices for transferring the\nmodel update. Such a direct communication may be energy inefficient or even\nunavailable in mobile environments. In this paper, we consider adopting the\nrelay network to construct a cooperative communication platform for supporting\nmodel update transfer and trading. In the system, the mobile devices generate\nmodel updates based on their training data. The model updates are then\nforwarded to the model owner through the cooperative relay network. The model\nowner enjoys the learning service provided by the mobile devices. In return,\nthe mobile devices charge the model owner certain prices. Due to the coupled\ninterference of wireless transmission among the mobile devices that use the\nsame relay node, the rational mobile devices have to choose their relay nodes\nas well as deciding on their transmission powers. Thus, we formulate a\nStackelberg game model to investigate the interaction among the mobile devices\nand that between the mobile devices and the model owner. The Stackelberg\nequilibrium is investigated by capitalizing on the exterior point method.\nMoreover, we provide a series of insightful analytical and numerical results on\nthe equilibrium of the Stackelberg game. \n\n"}
{"id": "1811.12253", "contents": "Title: Unifying the stochastic and the adversarial Bandits with Knapsack Abstract: This paper investigates the adversarial Bandits with Knapsack (BwK) online\nlearning problem, where a player repeatedly chooses to perform an action, pays\nthe corresponding cost, and receives a reward associated with the action. The\nplayer is constrained by the maximum budget $B$ that can be spent to perform\nactions, and the rewards and the costs of the actions are assigned by an\nadversary. This problem has only been studied in the restricted setting where\nthe reward of an action is greater than the cost of the action, while we\nprovide a solution in the general setting. Namely, we propose EXP3.BwK, a novel\nalgorithm that achieves order optimal regret. We also propose EXP3++.BwK, which\nis order optimal in the adversarial BwK setup, and incurs an almost optimal\nexpected regret with an additional factor of $\\log(B)$ in the stochastic BwK\nsetup. Finally, we investigate the case of having large costs for the actions\n(i.e., they are comparable to the budget size $B$), and show that for the\nadversarial setting, achievable regret bounds can be significantly worse,\ncompared to the case of having costs bounded by a constant, which is a common\nassumption within the BwK literature. \n\n"}
{"id": "1811.12459", "contents": "Title: Smoothed Analysis of Multi-Item Auctions with Correlated Values Abstract: Consider a seller with m heterogeneous items for sale to a single additive\nbuyer whose values for the items are arbitrarily correlated. It was previously\nshown that, in such settings, distributions exist for which the seller's\noptimal revenue is infinite, but the best \"simple\" mechanism achieves revenue\nat most one ([Briest et. al 15], [Hart and Nisan 13]), even when $m=2$. This\nresult has long served as a cautionary tale discouraging the study of\nmulti-item auctions without some notion of \"independent items\".\n  In this work we initiate a smoothed analysis of such multi-item auction\nsettings. We consider a buyer whose item values are drawn from an arbitrarily\ncorrelated multi-dimensional distribution then randomly perturbed with\nmagnitude $\\delta$ under several natural perturbation models. On one hand, we\nprove that the ([Briest et. al 15], [Hart and Nisan 13]) construction is\nsurprisingly robust to certain natural perturbations of this form, and the\ninfinite gap remains.\n  On the other hand, we provide a smoothed model such that the approximation\nguarantee of simple mechanisms is smoothed-finite. We show that when the\nperturbation has magnitude $\\delta$, pricing only the grand bundle guarantees\nan $O(1/\\delta)$-approximation to the optimal revenue. That is, no matter the\n(worst-case) initially correlated distribution, these tiny perturbations\nsuffice to bring the gap down from infinite to finite. We further show that the\nsame guarantees hold when $n$ buyers have values drawn from an arbitrarily\ncorrelated $mn$-dimensional distribution (without any dependence on $n$).\n  Taken together, these analyses further pin down key properties of correlated\ndistributions that result in large gaps between simplicity and optimality. \n\n"}
{"id": "1811.12655", "contents": "Title: Prior-free Data Acquisition for Accurate Statistical Estimation Abstract: We study a data analyst's problem of acquiring data from self-interested\nindividuals to obtain an accurate estimation of some statistic of a population,\nsubject to an expected budget constraint. Each data holder incurs a cost, which\nis unknown to the data analyst, to acquire and report his data. The cost can be\narbitrarily correlated with the data. The data analyst has an expected budget\nthat she can use to incentivize individuals to provide their data. The goal is\nto design a joint acquisition-estimation mechanism to optimize the performance\nof the produced estimator, without any prior information on the underlying\ndistribution of cost and data. We investigate two types of estimations:\nunbiased point estimation and confidence interval estimation.\n  Unbiased estimators: We design a truthful, individually rational, online\nmechanism to acquire data from individuals and output an unbiased estimator of\nthe population mean when the data analyst has no prior information on the\ncost-data distribution and individuals arrive in a random order. The\nperformance of this mechanism matches that of the optimal mechanism, which\nknows the true cost distribution, within a constant factor. The performance of\nan estimator is evaluated by its variance under the worst-case cost-data\ncorrelation.\n  Confidence intervals: We characterize an approximately optimal (within a\nfactor $2$) mechanism for obtaining a confidence interval of the population\nmean when the data analyst knows the true cost distribution at the beginning.\nThis mechanism is efficiently computable. We then design a truthful,\nindividually rational, online algorithm that is only worse than the\napproximately optimal mechanism by a constant factor. The performance of an\nestimator is evaluated by its expected length under the worst-case cost-data\ncorrelation. \n\n"}
{"id": "1812.01132", "contents": "Title: A Unified Approach to Dynamic Decision Problems with Asymmetric\n  Information - Part II: Strategic Agents Abstract: We study a general class of dynamic games with asymmetric information where\nagents' beliefs are strategy dependent, i.e. signaling occurs. We show that the\nnotion of sufficient information, introduced in the companion paper team, can\nbe used to effectively compress the agents' information in a mutually\nconsistent manner that is sufficient for decision-making purposes. We present\ninstances of dynamic games with asymmetric information where we can\ncharacterize a time-invariant information state for each agent. Based on the\nnotion of sufficient information, we define a class of equilibria for dynamic\ngames called Sufficient Information Based Perfect Bayesian Equilibrium\n(SIB-PBE). Utilizing the notion of SIB-PBE, we provide a sequential\ndecomposition of dynamic games with asymmetric information over time; this\ndecomposition leads to a dynamic program that determines SIB-PBE of dynamic\ngames. Furthermore, we provide conditions under which we can guarantee the\nexistence of SIB-PBE. \n\n"}
{"id": "1812.01267", "contents": "Title: A Game-Theoretic Learning Framework for Multi-Agent Intelligent Wireless\n  Networks Abstract: In this article, we introduce a game-theoretic learning framework for the\nmulti-agent wireless network. By combining learning in artificial intelligence\n(AI) with game theory, several promising properties emerge such as obtaining\nhigh payoff in the unknown and dynamic environment, coordinating the actions of\nagents and making the adversarial decisions with the existence of malicious\nusers. Unfortunately, there is no free lunch. To begin with, we discuss the\nconnections between learning in AI and game theory mainly in three levels,\ni.e., pattern recognition, prediction and decision making. Then, we discuss the\nchallenges and requirements of the combination for the intelligent wireless\nnetwork, such as constrained capabilities of agents, incomplete information\nobtained from the environment and the distributed, dynamically scalable and\nheterogeneous characteristics of wireless network. To cope with these, we\npropose a game-theoretic learning framework for the wireless network, including\nthe internal coordination (resource optimization) and external adversarial\ndecision-making (anti-jamming). Based on the framework, we introduce several\nattractive game-theoretic learning methods combining with the typical\napplications that we have proposed. What's more, we developed a real-life\ntestbed for the multi-agent anti-jamming problem based on the game-theoretic\nlearning framework. The experiment results verify the effectiveness of the\nproposed game-theoretic learning method. \n\n"}
{"id": "1812.01482", "contents": "Title: Target Set Selection parameterized by vertex cover and more Abstract: Given a simple, undirected graph $G$ with a threshold function $\\tau:V(G)\n\\rightarrow \\mathbb{N}$, the \\textsc{Target Set Selection} (TSS) problem is\nabout choosing a minimum cardinality set, say $S \\subseteq V(G)$, such that\nstarting a diffusion process with $S$ as its seed set will eventually result in\nactivating all the nodes in $G$. For any non-negative integer $i$, we say a set\n$T\\subseteq V(G)$ is a \"degree-$i$ modulator\" of $G$ if the degree of any\nvertex in the graph $G-T$ is at most $i$. Degree-$0$ modulators of a graph are\nprecisely its vertex covers. Consider a graph $G$ on $n$ vertices and $m$\nedges. We have the following results on the TSS problem:\n  -> It was shown by Nichterlein et al. [Social Network Analysis and Mining,\n2013] that it is possible to compute an optimal-sized target set in\n$O(2^{(2^{t}+1)t}\\cdot m)$ time, where $t$ denotes the cardinality of a minimum\ndegree-$0$ modulator of $G$. We improve this result by designing an algorithm\nrunning in time $2^{O(t\\log t)}n^{O(1)}$.\n  -> We design a $2^{2^{O(t)}}n^{O(1)}$ time algorithm to compute an optimal\ntarget set for $G$, where $t$ is the size of a minimum degree-$1$ modulator of\n$G$. \n\n"}
{"id": "1812.01789", "contents": "Title: Hard combinatorial problems and minor embeddings on lattice graphs Abstract: Today, hardware constraints are an important limitation on quantum adiabatic\noptimization algorithms. Firstly, computational problems must be formulated as\nquadratic unconstrained binary optimization (QUBO) in the presence of noisy\ncoupling constants. Secondly, the interaction graph of the QUBO must have an\neffective minor embedding into a two-dimensional nonplanar lattice graph. We\ndescribe new strategies for constructing QUBOs for NP-complete/hard\ncombinatorial problems that address both of these challenges. Our results\ninclude asymptotically improved embeddings for number partitioning, filling\nknapsacks, graph coloring, and finding Hamiltonian cycles. These embeddings can\nbe also be found with reduced computational effort. Our new embedding for\nnumber partitioning may be more effective on next-generation hardware. \n\n"}
{"id": "1812.02878", "contents": "Title: Solving Non-Convex Non-Concave Min-Max Games Under Polyak-{\\L}ojasiewicz\n  Condition Abstract: In this short note, we consider the problem of solving a min-max zero-sum\ngame. This problem has been extensively studied in the convex-concave regime\nwhere the global solution can be computed efficiently. Recently, there have\nalso been developments for finding the first order stationary points of the\ngame when one of the player's objective is concave or (weakly) concave. This\nwork focuses on the non-convex non-concave regime where the objective of one of\nthe players satisfies Polyak-{\\L}ojasiewicz (PL) Condition. For such a game, we\nshow that a simple multi-step gradient descent-ascent algorithm finds an\n$\\varepsilon$--first order stationary point of the problem in\n$\\widetilde{\\mathcal{O}}(\\varepsilon^{-2})$ iterations. \n\n"}
{"id": "1812.05821", "contents": "Title: Partial Function Extension with Applications to Learning and Property\n  Testing Abstract: In partial function extension, we are given a partial function consisting of\n$n$ points from a domain and a function value at each point. Our objective is\nto determine if this partial function can be extended to a function defined on\nthe domain, that additionally satisfies a given property, such as convexity.\nThis basic problem underlies research questions in many areas, such as\nlearning, property testing, and game theory. We formally study the problem of\nextending partial functions to satisfy fundamental properties in combinatorial\noptimization, focusing on upper and lower bounds for extension and applications\nto learning and property testing.\n  (1) For subadditive functions, we show the extension problem is\ncoNP-complete, and we give tight bounds on the approximability. We also give an\nimproved lower bound for learning subadditive functions, and give the first\nnontrivial testers for subadditive and XOS functions.\n  (2) For submodular functions, we show that if a partial function can be\nextended to a submodular function on the lattice closure (the minimal set that\ncontains the partial function and is closed under union and intersection) of\nthe partial function, it can be extended to a submodular function on the entire\ndomain. We obtain algorithms for determining extendibility in a number of\ncases, including if $n$ is a constant, or the points are nearly the same size.\nThe complexity of extendibility is in general unresolved.\n  (3) Lastly, for convex functions in $\\mathbb{R}^m$, we show an interesting\njuxtaposition: while we can determine the existence of an extension\nefficiently, computing the value of a widely-studied convex extension at a\ngiven point is strongly NP-hard. \n\n"}
{"id": "1812.10563", "contents": "Title: The Prophet Inequality Can Be Solved Optimally with a Single Set of\n  Samples Abstract: The setting of the classic prophet inequality is as follows: a gambler is\nshown the probability distributions of $n$ independent, non-negative random\nvariables with finite expectations. In their indexed order, a value is drawn\nfrom each distribution, and after every draw the gambler may choose to accept\nthe value and end the game, or discard the value permanently and continue the\ngame. What is the best performance that the gambler can achieve in comparison\nto a prophet who can always choose the highest value? Krengel, Sucheston, and\nGarling solved this problem in 1978, showing that there exists a strategy for\nwhich the gambler can achieve half as much reward as the prophet in\nexpectation. Furthermore, this result is tight.\n  In this work, we consider a setting in which the gambler is allowed much less\ninformation. Suppose that the gambler can only take one sample from each of the\ndistributions before playing the game, instead of knowing the full\ndistributions. We provide a simple and intuitive algorithm that recovers the\noriginal approximation of $\\frac{1}{2}$. Our algorithm works against even an\nalmighty adversary who always chooses a worst-case ordering, rather than the\nstandard offline adversary. The result also has implications for mechanism\ndesign -- there is much interest in designing competitive auctions with a\nfinite number of samples from value distributions rather than full\ndistributional knowledge. \n\n"}
{"id": "1812.10620", "contents": "Title: Evasive path planning under surveillance uncertainty Abstract: The classical setting of optimal control theory assumes full knowledge of the\nprocess dynamics and the costs associated with every control strategy. The\nproblem becomes much harder if the controller only knows a finite set of\npossible running cost functions, but has no way of checking which of these\nrunning costs is actually in place. In this paper we address this challenge for\na class of evasive path planning problems on a continuous domain, in which an\nEvader needs to reach a target while minimizing his exposure to an enemy\nObserver, who is in turn selecting from a finite set of known surveillance\nplans. Our key assumption is that both the evader and the observer need to\ncommit to their (possibly probabilistic) strategies in advance and cannot\nimmediately change their actions based on any newly discovered information\nabout the opponent's current position. We consider two types of evader\nbehavior: in the first one, a completely risk-averse evader seeks a trajectory\nminimizing his {\\em worst-case} cumulative observability, and in the second,\nthe evader is concerned with minimizing the {\\em average-case} cumulative\nobservability. The latter version is naturally interpreted as a semi-infinite\nstrategic game, and we provide an efficient method for approximating its Nash\nequilibrium. The proposed approach draws on methods from game theory, convex\noptimization, optimal control, and multiobjective dynamic programming. We\nillustrate our algorithm using numerical examples and discuss the computational\ncomplexity, including for the generalized version with multiple evaders. \n\n"}
{"id": "1812.11712", "contents": "Title: On the Complexity of the Inverse Semivalue Problem for Weighted Voting\n  Games Abstract: Weighted voting games are a family of cooperative games, typically used to\nmodel voting situations where a number of agents (players) vote against or for\na proposal. In such games, a proposal is accepted if an appropriately weighted\nsum of the votes exceeds a prespecified threshold. As the influence of a player\nover the voting outcome is not in general proportional to her assigned weight,\nvarious power indices have been proposed to measure each player's influence.\nThe inverse power index problem is the problem of designing a weighted voting\ngame that achieves a set of target influences according to a predefined power\nindex. In this work, we study the computational complexity of the inverse\nproblem when the power index belongs to the class of semivalues. We prove that\nthe inverse problem is computationally intractable for a broad family of\nsemivalues, including all regular semivalues. As a special case of our general\nresult, we establish computational hardness of the inverse problem for the\nBanzhaf indices and the Shapley values, arguably the most popular power\nindices. \n\n"}
{"id": "1901.01030", "contents": "Title: Multi-Product Dynamic Pricing in High-Dimensions with Heterogeneous\n  Price Sensitivity Abstract: We consider the problem of multi-product dynamic pricing, in a contextual\nsetting, for a seller of differentiated products. In this environment, the\ncustomers arrive over time and products are described by high-dimensional\nfeature vectors. Each customer chooses a product according to the widely used\nMultinomial Logit (MNL) choice model and her utility depends on the product\nfeatures as well as the prices offered. The seller a-priori does not know the\nparameters of the choice model but can learn them through interactions with\ncustomers. The seller's goal is to design a pricing policy that maximizes her\ncumulative revenue. This model is motivated by online marketplaces such as\nAirbnb platform and online advertising. We measure the performance of a pricing\npolicy in terms of regret, which is the expected revenue loss with respect to a\nclairvoyant policy that knows the parameters of the choice model in advance and\nalways sets the revenue-maximizing prices. We propose a pricing policy, named\nM3P, that achieves a $T$-period regret of $O(\\log(Td) ( \\sqrt{T}+ d\\log(T)))$\nunder heterogeneous price sensitivity for products with features of dimension\n$d$. We also use tools from information theory to prove that no policy can\nachieve worst-case $T$-regret better than $\\Omega(\\sqrt{T})$. \n\n"}
{"id": "1901.01598", "contents": "Title: Toward a Theory of Cyber Attacks Abstract: We provide a general methodology for analyzing defender-attacker based\n\"games\" in which we model such games as Markov models and introduce a capacity\nregion to analyze how defensive and adversarial strategies impact security.\nSuch a framework allows us to analyze under what kind of conditions we can\nprove statements (about an attack objective $k$) of the form \"if the attacker\nhas a time budget $T_{bud}$, then the probability that the attacker can reach\nan attack objective $\\geq k$ is at most $poly(T_{bud})negl(k)$\". We are\ninterested in such rigorous cryptographic security guarantees (that describe\nworst-case guarantees) as these shed light on the requirements of a defender's\nstrategy for preventing more and more the progress of an attack, in terms of\nthe \"learning rate\" of a defender's strategy. We explain the damage an attacker\ncan achieve by a \"containment parameter\" describing the maximally reached\nattack objective within a specific time window. \n\n"}
{"id": "1901.01861", "contents": "Title: On the Parameterized Complexity of $k$-Edge Colouring Abstract: For every fixed integer $k \\geq 1$, we prove that $k$-Edge Colouring is\nfixed-parameter-tractable when parameterized by the number of vertices of\nmaximum degree. \n\n"}
{"id": "1901.03571", "contents": "Title: Life is Random, Time is Not: Markov Decision Processes with Window\n  Objectives Abstract: The window mechanism was introduced by Chatterjee et al. to strengthen\nclassical game objectives with time bounds. It permits to synthesize system\ncontrollers that exhibit acceptable behaviors within a configurable time frame,\nall along their infinite execution, in contrast to the traditional objectives\nthat only require correctness of behaviors in the limit. The window concept has\nproved its interest in a variety of two-player zero-sum games because it\nenables reasoning about such time bounds in system specifications, but also\nthanks to the increased tractability that it usually yields.\n  In this work, we extend the window framework to stochastic environments by\nconsidering Markov decision processes. A fundamental problem in this context is\nthe threshold probability problem: given an objective it aims to synthesize\nstrategies that guarantee satisfying runs with a given probability. We solve it\nfor the usual variants of window objectives, where either the time frame is set\nas a parameter, or we ask if such a time frame exists. We develop a generic\napproach for window-based objectives and instantiate it for the classical\nmean-payoff and parity objectives, already considered in games. Our work paves\nthe way to a wide use of the window mechanism in stochastic models. \n\n"}
{"id": "1901.05168", "contents": "Title: How Will the Presence of Autonomous Vehicles Affect the Equilibrium\n  State of Traffic Networks? Abstract: It is known that connected and autonomous vehicles are capable of maintaining\nshorter headways and distances when they form platoons of vehicles. Thus, such\ntechnologies can result in increases in the capacities of traffic networks.\nConsequently, it is envisioned that their deployment will boost the network\nmobility. In this paper, we verify the validity of this impact under selfish\nrouting behavior of drivers in traffic networks with mixed autonomy, i.e.\ntraffic networks with both regular and autonomous vehicles. We consider a\nnonatomic routing game on a network with inelastic (fixed) demands for the set\nof network O/D pairs, and study how replacing a fraction of regular vehicles by\nautonomous vehicles will affect the mobility of the network. Using the well\nknown US bureau of public roads (BPR) traffic delay models, we show that the\nresulting Wardrop equilibrium is not necessarily unique even in its weak sense\nfor networks with mixed autonomy. We state the conditions under which the total\nnetwork delay is guaranteed not to increase as a result of autonomy increase.\nHowever, we show that when these conditions do not hold, counter intuitive\nbehaviors may occur: the total delay can grow by increasing the network\nautonomy. In particular, we prove that for networks with a single O/D pair, if\nthe road degrees of asymmetry are homogeneous, the total delay is 1) unique,\nand 2) a nonincreasing continuous function of network autonomy fraction. We\nshow that for heterogeneous degrees of asymmetry, the total delay is not\nunique, and it can further grow with autonomy increase. We demonstrate that\nsimilar behaviors may be observed in networks with multiple O/D pairs. We\nfurther bound such performance degradations due to the introduction of autonomy\nin homogeneous networks. \n\n"}
{"id": "1901.06230", "contents": "Title: Computing large market equilibria using abstractions Abstract: Computing market equilibria is an important practical problem for market\ndesign, for example in fair division of items. However, computing equilibria\nrequires large amounts of information (typically the valuation of every buyer\nfor every item) and computing power. We consider ameliorating these issues by\napplying a method used for solving complex games: constructing a coarsened\nabstraction of a given market, solving for the equilibrium in the abstraction,\nand lifting the prices and allocations back to the original market. We show how\nto bound important quantities such as regret, envy, Nash social welfare, Pareto\noptimality, and maximin share/proportionality when the abstracted prices and\nallocations are used in place of the real equilibrium. We then study two\nabstraction methods of interest for practitioners: (1) filling in unknown\nvaluations using techniques from matrix completion, (2) reducing the problem\nsize by aggregating groups of buyers/items into smaller numbers of\nrepresentative buyers/items and solving for equilibrium in this coarsened\nmarket. We find that in real data allocations/prices that are relatively close\nto equilibria can be computed from even very coarse abstractions. \n\n"}
{"id": "1901.07333", "contents": "Title: Multi-agent Reinforcement Learning Embedded Game for the Optimization of\n  Building Energy Control and Power System Planning Abstract: Most of the current game-theoretic demand-side management methods focus\nprimarily on the scheduling of home appliances, and the related numerical\nexperiments are analyzed under various scenarios to achieve the corresponding\nNash-equilibrium (NE) and optimal results. However, not much work is conducted\nfor academic or commercial buildings. The methods for optimizing\nacademic-buildings are distinct from the optimal methods for home appliances.\nIn my study, we address a novel methodology to control the operation of\nheating, ventilation, and air conditioning system (HVAC). With the development\nof Artificial Intelligence and computer technologies, reinforcement learning\n(RL) can be implemented in multiple realistic scenarios and help people to\nsolve thousands of real-world problems. Reinforcement Learning, which is\nconsidered as the art of future AI, builds the bridge between agents and\nenvironments through Markov Decision Chain or Neural Network and has seldom\nbeen used in power system. The art of RL is that once the simulator for a\nspecific environment is built, the algorithm can keep learning from the\nenvironment. Therefore, RL is capable of dealing with constantly changing\nsimulator inputs such as power demand, the condition of power system and\noutdoor temperature, etc. Compared with the existing distribution power system\nplanning mechanisms and the related game theoretical methodologies, our\nproposed algorithm can plan and optimize the hourly energy usage, and have the\nability to corporate with even shorter time window if needed. \n\n"}
{"id": "1901.07621", "contents": "Title: Single Deep Counterfactual Regret Minimization Abstract: Counterfactual Regret Minimization (CFR) is the most successful algorithm for\nfinding approximate Nash equilibria in imperfect information games. However,\nCFR's reliance on full game-tree traversals limits its scalability. For this\nreason, the game's state- and action-space is often abstracted (i.e.\nsimplified) for CFR, and the resulting strategy is then translated back to the\nfull game, which requires extensive expert-knowledge and often converges to\nhighly exploitable policies. A recently proposed method, Deep CFR, applies deep\nlearning directly to CFR, allowing the agent to intrinsically abstract and\ngeneralize over the state-space from samples, without requiring expert\nknowledge. In this paper, we introduce Single Deep CFR (SD-CFR), a simplified\nvariant of Deep CFR that has a lower overall approximation error by avoiding\nthe training of an average strategy network. We show that SD-CFR is more\nattractive from a theoretical perspective and empirically outperforms Deep CFR\nwith respect to exploitability and one-on-one play in poker. \n\n"}
{"id": "1901.08463", "contents": "Title: Almost Envy-Freeness in Group Resource Allocation Abstract: We study the problem of fairly allocating indivisible goods between groups of\nagents using the recently introduced relaxations of envy-freeness. We consider\nthe existence of fair allocations under different assumptions on the valuations\nof the agents. In particular, our results cover cases of arbitrary monotonic,\nresponsive, and additive valuations, while for the case of binary valuations we\nfully characterize the cardinalities of two groups of agents for which a fair\nallocation can be guaranteed with respect to both envy-freeness up to one good\n(EF1) and envy-freeness up to any good (EFX). Moreover, we introduce a new\nmodel where the agents are not partitioned into groups in advance, but instead\nthe partition can be chosen in conjunction with the allocation of the goods. In\nthis model, we show that for agents with arbitrary monotonic valuations, there\nis always a partition of the agents into two groups of any given sizes along\nwith an EF1 allocation of the goods. We also provide an extension of this\nresult to any number of groups. \n\n"}
{"id": "1901.10059", "contents": "Title: A Regulation Enforcement Solution for Multi-agent Reinforcement Learning Abstract: Human behaviors are regularized by a variety of norms or regulations, either\nto maintain orders or to enhance social welfare. If artificially intelligent\n(AI) agents make decisions on behalf of human beings, we would hope they can\nalso follow established regulations while interacting with humans or other AI\nagents. However, it is possible that an AI agent can opt to disobey the\nregulations (being defective) for self-interests. In this paper, we aim to\nanswer the following question: Consider a multi-agent decentralized\nenvironment. Agents make decisions in complete isolation of other agents. Each\nagent knows the state of its own MDP and its own actions but it does not know\nthe states and the actions taken by other players. There is a set of\nregulations for all agents to follow. Although most agents are benign and will\ncomply to regulations but not all agents are compliant at first, can we develop\na framework such that it is in the self-interest of non-compliant agents to\ncomply after all?. We first introduce the problem as Regulation Enforcement and\nformulate it using reinforcement learning and game theory under the scenario\nwhere agents make decisions in complete isolation of other agents. We then\npropose a solution based on the key idea that although we could not alter how\ndefective agents choose to behave, we can, however, leverage the aggregated\npower of compliant agents to boycott the defective ones. We conducted simulated\nexperiments on two scenarios: Replenishing Resource Management Dilemma and\nDiminishing Reward Shaping Enforcement, using deep multi-agent reinforcement\nlearning algorithms. We further use empirical game-theoretic analysis to show\nthat the method alters the resulting empirical payoff matrices in a way that\npromotes compliance (making mutual compliant a Nash Equilibrium). \n\n"}
{"id": "1901.10064", "contents": "Title: Committee Selection with Attribute Level Preferences Abstract: We consider the problem of committee selection from a fixed set of candidates\nwhere each candidate has multiple quantifiable attributes. To select the best\npossible committee, instead of voting for a candidate, a voter is allowed to\napprove the preferred attributes of a given candidate. Though attribute based\npreference is addressed in several contexts, committee selection problem with\nattribute approval of voters has not been attempted earlier. A committee formed\non attribute preferences is more likely to be a better representative of the\nqualities desired by the voters and is less likely to be susceptible to\ncollusion or manipulation. In this work, we provide a formal study of the\ndifferent aspects of this problem and define properties of weak unanimity,\nstrong unanimity, simple justified representations and compound justified\nrepresentation, that are required to be satisfied by the selected committee. We\nshow that none of the existing vote/approval aggregation rules satisfy these\nnew properties for attribute aggregation. We describe a greedy approach for\nattribute aggregation that satisfies the first three properties, but not the\nfourth, i.e., compound justified representation, which we prove to be\nNP-complete. Furthermore, we prove that finding a committee with justified\nrepresentation and the highest approval voting score is NP-complete. \n\n"}
{"id": "1901.10450", "contents": "Title: Toward Controlling Discrimination in Online Ad Auctions Abstract: Online advertising platforms are thriving due to the customizable audiences\nthey offer advertisers. However, recent studies show that advertisements can be\ndiscriminatory with respect to the gender or race of the audience that sees the\nad, and may inadvertently cross ethical and/or legal boundaries. To prevent\nthis, we propose a constrained ad auction framework that maximizes the\nplatform's revenue conditioned on ensuring that the audience seeing an\nadvertiser's ad is distributed appropriately across sensitive types such as\ngender or race. Building upon Myerson's classic work, we first present an\noptimal auction mechanism for a large class of fairness constraints. Finding\nthe parameters of this optimal auction, however, turns out to be a non-convex\nproblem. We show that this non-convex problem can be reformulated as a more\nstructured non-convex problem with no saddle points or local-maxima; this\nallows us to develop a gradient-descent-based algorithm to solve it. Our\nempirical results on the A1 Yahoo! dataset demonstrate that our algorithm can\nobtain uniform coverage across different user types for each advertiser at a\nminor loss to the revenue of the platform, and a small change to the size of\nthe audience each advertiser reaches. \n\n"}
{"id": "cs/0204041", "contents": "Title: Trust Brokerage Systems for the Internet Abstract: This thesis addresses the problem of providing trusted individuals with\nconfidential information about other individuals, in particular, granting\naccess to databases of personal records using the World-Wide Web. It proposes\nan access rights management system for distributed databases which aims to\ncreate and implement organisation structures based on the wishes of the owners\nand of demands of the users of the databases. The dissertation describes how\ncurrent software components could be used to implement this system; it\nre-examines the theory of collective choice to develop mechanisms for\ngenerating hierarchies of authorities; it analyses organisational processes for\nstability and develops a means of measuring the similarity of their\nhierarchies. \n\n"}
{"id": "cs/0309033", "contents": "Title: Lower bounds for predecessor searching in the cell probe model Abstract: We consider a fundamental problem in data structures, static predecessor\nsearching: Given a subset S of size n from the universe [m], store S so that\nqueries of the form \"What is the predecessor of x in S?\" can be answered\nefficiently. We study this problem in the cell probe model introduced by Yao.\nRecently, Beame and Fich obtained optimal bounds on the number of probes needed\nby any deterministic query scheme if the associated storage scheme uses only\nn^{O(1)} cells of word size (\\log m)^{O(1)} bits. We give a new lower bound\nproof for this problem that matches the bounds of Beame and Fich. Our lower\nbound proof has the following advantages: it works for randomised query schemes\ntoo, while Beame and Fich's proof works for deterministic query schemes only.\nIt also extends to `quantum address-only' query schemes that we define in this\npaper, and is simpler than Beame and Fich's proof. We prove our lower bound\nusing the round elimination approach of Miltersen, Nisan, Safra and Wigderson.\nUsing tools from information theory, we prove a strong round elimination lemma\nfor communication complexity that enables us to obtain a tight lower bound for\nthe predecessor problem. Our strong round elimination lemma also extends to\nquantum communication complexity. We also use our round elimination lemma to\nobtain a rounds versus communication tradeoff for the `greater-than' problem,\nimproving on the tradeoff in Miltersen et al. We believe that our round\nelimination lemma is of independent interest and should have other\napplications. \n\n"}
{"id": "cs/0406037", "contents": "Title: Propositional Computability Logic II Abstract: Computability logic is a formal theory of computational tasks and resources.\nIts formulas represent interactive computational problems, logical operators\nstand for operations on computational problems, and validity of a formula is\nunderstood as being a scheme of problems that always have algorithmic\nsolutions. A comprehensive online source on the subject is available at\nhttp://www.cis.upenn.edu/~giorgi/cl.html . The earlier article \"Propositional\ncomputability logic I\" proved soundness and completeness for the (in a sense)\nminimal nontrivial fragment CL1 of computability logic. The present paper\nextends that result to the significantly more expressive propositional system\nCL2. What makes CL2 more expressive than CL1 is the presence of two sorts of\natoms in its language: elementary atoms, representing elementary computational\nproblems (i.e. predicates), and general atoms, representing arbitrary\ncomputational problems. CL2 conservatively extends CL1, with the latter being\nnothing but the general-atom-free fragment of the former. \n\n"}
{"id": "cs/0602090", "contents": "Title: On the Approximation and Smoothed Complexity of Leontief Market\n  Equilibria Abstract: We show that the problem of finding an \\epsilon-approximate Nash equilibrium\nof an n by n two-person games can be reduced to the computation of an\n(\\epsilon/n)^2-approximate market equilibrium of a Leontief economy. Together\nwith a recent result of Chen, Deng and Teng, this polynomial reduction implies\nthat the Leontief market exchange problem does not have a fully polynomial-time\napproximation scheme, that is, there is no algorithm that can compute an\n\\epsilon-approximate market equilibrium in time polynomial in m, n, and\n1/\\epsilon, unless PPAD is not in P, We also extend the analysis of our\nreduction to show, unless PPAD is not in RP, that the smoothed complexity of\nthe Scarf's general fixed-point approximation algorithm (when applying to solve\nthe approximate Leontief market exchange problem) or of any algorithm for\ncomputing an approximate market equilibrium of Leontief economies is not\npolynomial in n and 1/\\sigma, under Gaussian or uniform perturbations with\nmagnitude \\sigma. \n\n"}
{"id": "cs/0603043", "contents": "Title: Time-Space Trade-Offs for Predecessor Search Abstract: We develop a new technique for proving cell-probe lower bounds for static\ndata structures. Previous lower bounds used a reduction to communication games,\nwhich was known not to be tight by counting arguments. We give the first lower\nbound for an explicit problem which breaks this communication complexity\nbarrier. In addition, our bounds give the first separation between polynomial\nand near linear space. Such a separation is inherently impossible by\ncommunication complexity.\n  Using our lower bound technique and new upper bound constructions, we obtain\ntight bounds for searching predecessors among a static set of integers. Given a\nset Y of n integers of l bits each, the goal is to efficiently find\npredecessor(x) = max{y in Y | y <= x}, by representing Y on a RAM using space\nS.\n  In external memory, it follows that the optimal strategy is to use either\nstandard B-trees, or a RAM algorithm ignoring the larger block size. In the\nimportant case of l = c*lg n, for c>1 (i.e. polynomial universes), and near\nlinear space (such as S = n*poly(lg n)), the optimal search time is Theta(lg\nl). Thus, our lower bound implies the surprising conclusion that van Emde Boas'\nclassic data structure from [FOCS'75] is optimal in this case. Note that for\nspace n^{1+eps}, a running time of O(lg l / lglg l) was given by Beame and Fich\n[STOC'99]. \n\n"}
{"id": "cs/0603084", "contents": "Title: Random 3CNF formulas elude the Lovasz theta function Abstract: Let $\\phi$ be a 3CNF formula with n variables and m clauses. A simple\nnonconstructive argument shows that when m is sufficiently large compared to n,\nmost 3CNF formulas are not satisfiable. It is an open question whether there is\nan efficient refutation algorithm that for most such formulas proves that they\nare not satisfiable. A possible approach to refute a formula $\\phi$ is: first,\ntranslate it into a graph $G_{\\phi}$ using a generic reduction from 3-SAT to\nmax-IS, then bound the maximum independent set of $G_{\\phi}$ using the Lovasz\n$\\vartheta$ function. If the $\\vartheta$ function returns a value $< m$, this\nis a certificate for the unsatisfiability of $\\phi$. We show that for random\nformulas with $m < n^{3/2 -o(1)}$ clauses, the above approach fails, i.e. the\n$\\vartheta$ function is likely to return a value of m. \n\n"}
{"id": "cs/0608081", "contents": "Title: How Hard Is Bribery in Elections? Abstract: We study the complexity of influencing elections through bribery: How\ncomputationally complex is it for an external actor to determine whether by a\ncertain amount of bribing voters a specified candidate can be made the\nelection's winner? We study this problem for election systems as varied as\nscoring protocols and Dodgson voting, and in a variety of settings regarding\nhomogeneous-vs.-nonhomogeneous electorate bribability,\nbounded-size-vs.-arbitrary-sized candidate sets, weighted-vs.-unweighted\nvoters, and succinct-vs.-nonsuccinct input specification. We obtain both\npolynomial-time bribery algorithms and proofs of the intractability of bribery,\nand indeed our results show that the complexity of bribery is extremely\nsensitive to the setting. For example, we find settings in which bribery is\nNP-complete but manipulation (by voters) is in P, and we find settings in which\nbribing weighted voters is NP-complete but bribing voters with individual bribe\nthresholds is in P. For the broad class of elections (including plurality,\nBorda, k-approval, and veto) known as scoring protocols, we prove a dichotomy\nresult for bribery of weighted voters: We find a simple-to-evaluate condition\nthat classifies every case as either NP-complete or in P. \n\n"}
{"id": "cs/0609056", "contents": "Title: Matrix Games, Linear Programming, and Linear Approximation Abstract: The following four classes of computational problems are equivalent: solving\nmatrix games, solving linear programs, best $l^{\\infty}$ linear approximation,\nbest $l^1$ linear approximation. \n\n"}
{"id": "cs/0612089", "contents": "Title: On the time complexity of 2-tag systems and small universal Turing\n  machines Abstract: We show that 2-tag systems efficiently simulate Turing machines. As a\ncorollary we find that the small universal Turing machines of Rogozhin, Minsky\nand others simulate Turing machines in polynomial time. This is an exponential\nimprovement on the previously known simulation time overhead and improves a\nforty year old result in the area of small universal Turing machines. \n\n"}
{"id": "cs/0703148", "contents": "Title: Computer Science and Game Theory: A Brief Survey Abstract: There has been a remarkable increase in work at the interface of computer\nscience and game theory in the past decade. In this article I survey some of\nthe main themes of work in the area, with a focus on the work in computer\nscience. Given the length constraints, I make no attempt at being\ncomprehensive, especially since other surveys are also available, and a\ncomprehensive survey book will appear shortly. \n\n"}
{"id": "quant-ph/0211191", "contents": "Title: An invitation to Quantum Game Theory Abstract: Recent development in quantum computation and quantum information theory\nallows to extend the scope of game theory for the quantum world. The paper\npresents the history, basic ideas and recent development in quantum game\ntheory. In this context, a new application of the Ising chain model is\nproposed. \n\n"}
{"id": "quant-ph/0309033", "contents": "Title: Correlated Equilibria of Classical Strategic Games with Quantum Signals Abstract: Correlated equilibria are sometimes more efficient than the Nash equilibria\nof a game without signals. We investigate whether the availability of quantum\nsignals in the context of a classical strategic game may allow the players to\nachieve even better efficiency than in any correlated equilibrium with\nclassical signals, and find the answer to be positive. \n\n"}
{"id": "quant-ph/0504012", "contents": "Title: Quantum search algorithms Abstract: We review some of quantum algorithms for search problems: Grover's search\nalgorithm, its generalization to amplitude amplification, the applications of\namplitude amplification to various problems and the recent quantum algorithms\nbased on quantum walks. \n\n"}
{"id": "quant-ph/0608026", "contents": "Title: Search via Quantum Walk Abstract: We propose a new method for designing quantum search algorithms for finding a\n\"marked\" element in the state space of a classical Markov chain. The algorithm\nis based on a quantum walk \\'a la Szegedy (2004) that is defined in terms of\nthe Markov chain. The main new idea is to apply quantum phase estimation to the\nquantum walk in order to implement an approximate reflection operator. This\noperator is then used in an amplitude amplification scheme. As a result we\nconsiderably expand the scope of the previous approaches of Ambainis (2004) and\nSzegedy (2004). Our algorithm combines the benefits of these approaches in\nterms of being able to find marked elements, incurring the smaller cost of the\ntwo, and being applicable to a larger class of Markov chains. In addition, it\nis conceptually simple and avoids some technical difficulties in the previous\nanalyses of several algorithms based on quantum walk. \n\n"}
